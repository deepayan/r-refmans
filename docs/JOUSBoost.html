<!DOCTYPE html><html><head><title>Help for package JOUSBoost</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {JOUSBoost}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#JOUSBoost'><p>JOUSBoost: A package for probability estimation</p></a></li>
<li><a href='#adaboost'><p>AdaBoost Classifier</p></a></li>
<li><a href='#circle_data'><p>Simulate data from the circle model.</p></a></li>
<li><a href='#friedman_data'><p>Simulate data from the Friedman model</p></a></li>
<li><a href='#grid_probs'><p>Function to compute predicted quantiles</p></a></li>
<li><a href='#index_over'><p>Return indices to be used for jittered data in oversampling</p></a></li>
<li><a href='#index_under'><p>Return indices to be used in original data for undersampling</p></a></li>
<li><a href='#jous'><p>Jittering with Over/Under Sampling</p></a></li>
<li><a href='#predict.adaboost'><p>Create predictions from AdaBoost fit</p></a></li>
<li><a href='#predict.jous'><p>Create predictions</p></a></li>
<li><a href='#print.adaboost'><p>Print a summary of adaboost fit.</p></a></li>
<li><a href='#print.jous'><p>Print a summary of <code>jous</code> fit.</p></a></li>
<li><a href='#sonar'><p>Dataset of sonar measurements of rocks and mines</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Implements Under/Oversampling for Probability Estimation</td>
</tr>
<tr>
<td>Version:</td>
<td>2.1.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements under/oversampling for probability estimation.  To be
    used with machine learning methods such as AdaBoost, random forests, etc.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr, rmarkdown</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, rpart, stats, doParallel, foreach</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2017-07-12 18:16:22 UTC; matthewolson</td>
</tr>
<tr>
<td>Author:</td>
<td>Matthew Olson [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Matthew Olson &lt;maolson@wharton.upenn.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2017-07-12 19:13:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='JOUSBoost'>JOUSBoost: A package for probability estimation</h2><span id='topic+JOUSBoost'></span><span id='topic+JOUSBoost-package'></span>

<h3>Description</h3>

<p>JOUSBoost implements under/oversampling with jittering for probability estimation.
Its intent is to be used to improve probability estimates that come from
boosting algorithms (such as AdaBoost), but is modular enough to be used with
virtually any classification algorithm from machine learning.
</p>


<h3>Details</h3>

<p>For more theoretical background, consult Mease (2007).
</p>


<h3>References</h3>

<p>Mease, D., Wyner, A. and Buja, A. (2007). Costweighted
boosting with jittering and over/under-sampling:
JOUS-boost. J. Machine Learning Research 8 409-439.
</p>

<hr>
<h2 id='adaboost'>AdaBoost Classifier</h2><span id='topic+adaboost'></span>

<h3>Description</h3>

<p>An implementation of the AdaBoost algorithm from Freund and Shapire (1997)
applied to decision tree classifiers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adaboost(X, y, tree_depth = 3, n_rounds = 100, verbose = FALSE,
  control = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adaboost_+3A_x">X</code></td>
<td>
<p>A matrix of continuous predictors.</p>
</td></tr>
<tr><td><code id="adaboost_+3A_y">y</code></td>
<td>
<p>A vector of responses with entries in <code>c(-1, 1)</code>.</p>
</td></tr>
<tr><td><code id="adaboost_+3A_tree_depth">tree_depth</code></td>
<td>
<p>The depth of the base tree classifier to use.</p>
</td></tr>
<tr><td><code id="adaboost_+3A_n_rounds">n_rounds</code></td>
<td>
<p>The number of rounds of boosting to use.</p>
</td></tr>
<tr><td><code id="adaboost_+3A_verbose">verbose</code></td>
<td>
<p>Whether to print the number of iterations.</p>
</td></tr>
<tr><td><code id="adaboost_+3A_control">control</code></td>
<td>
<p>A <code>rpart.control</code> list that controls properties of fitted
decision trees.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an object of class <code>adaboost</code> containing the following values:
</p>
<table>
<tr><td><code>alphas</code></td>
<td>
<p>Weights computed in the adaboost fit.</p>
</td></tr>
<tr><td><code>trees</code></td>
<td>
<p>The trees constructed in each round of boosting.  Storing trees
allows one to make predictions on new data.</p>
</td></tr>
<tr><td><code>confusion_matrix</code></td>
<td>
<p>A confusion matrix for the in-sample fits.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Trees are grown using the CART algorithm implemented in the <code>rpart</code>
package.  In order to conserve memory, the only parts of the fitted
tree objects that are retained are those essential to making predictions.
In practice, the number of rounds of boosting to use is chosen by
cross-validation.
</p>


<h3>References</h3>

<p>Freund, Y. and Schapire, R. (1997). A decision-theoretic
generalization of online learning and an application to boosting, Journal of
Computer and System Sciences 55: 119-139.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Generate data from the circle model
set.seed(111)
dat = circle_data(n = 500)
train_index = sample(1:500, 400)

ada = adaboost(dat$X[train_index,], dat$y[train_index], tree_depth = 2,
               n_rounds = 200, verbose = TRUE)
print(ada)
yhat_ada = predict(ada, dat$X[-train_index,])

# calculate misclassification rate
mean(dat$y[-train_index] != yhat_ada)

## End(Not run)
</code></pre>

<hr>
<h2 id='circle_data'>Simulate data from the circle model.</h2><span id='topic+circle_data'></span>

<h3>Description</h3>

<p>Simulate draws from a bernoulli distribution over <code>c(-1,1)</code>.  First, the
predictors <code class="reqn">x</code> are drawn i.i.d. uniformly over the square in the two dimensional
plane centered at the origin with side length <code>2*outer_r</code>, and then the
response is drawn according to <code class="reqn">p(y=1|x)</code>, which depends
on <code class="reqn">r(x)</code>, the euclidean norm of <code class="reqn">x</code>.  If
<code class="reqn">r(x) \le inner_r</code>, then <code class="reqn">p(y=1|x) = 1</code>, if <code class="reqn">r(x) \ge outer_r</code>
then <code class="reqn">p(y=1|x) = 1</code>, and <code class="reqn">p(y=1|x) = (outer_r - r(x))/(outer_r - inner_r)</code>
when <code class="reqn">inner_r &lt;= r(x) &lt;= outer_r</code>.  See Mease (2008).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>circle_data(n = 500, inner_r = 8, outer_r = 28)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="circle_data_+3A_n">n</code></td>
<td>
<p>Number of points to simulate.</p>
</td></tr>
<tr><td><code id="circle_data_+3A_inner_r">inner_r</code></td>
<td>
<p>Inner radius of annulus.</p>
</td></tr>
<tr><td><code id="circle_data_+3A_outer_r">outer_r</code></td>
<td>
<p>Outer radius of annulus.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with the following components:
</p>
<table>
<tr><td><code>y</code></td>
<td>
<p>Vector of simulated response in <code>c(-1,1)</code>.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>An <code>n</code>x<code>2</code> matrix of simulated predictors.</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>The true conditional probability <code class="reqn">p(y=1|x)</code>.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Mease, D., Wyner, A. and Buha, A. (2007). Costweighted
boosting with jittering and over/under-sampling:
JOUS-boost. J. Machine Learning Research 8 409-439.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate data from the circle model
set.seed(111)
dat = circle_data(n = 500, inner_r = 1, outer_r = 5)

## Not run: 
# Visualization of conditional probability p(y=1|x)
inner_r = 0.5
outer_r = 1.5
x = seq(-outer_r, outer_r, by=0.02)
radius = sqrt(outer(x^2, x^2, "+"))
prob = ifelse(radius &gt;= outer_r, 0, ifelse(radius &lt;= inner_r, 1,
             (outer_r-radius)/(outer_r-inner_r)))
image(x, x, prob, main='Probability Density: Circle Example')

## End(Not run)

</code></pre>

<hr>
<h2 id='friedman_data'>Simulate data from the Friedman model</h2><span id='topic+friedman_data'></span>

<h3>Description</h3>

<p>Simulate draws from a bernoulli distribution over <code>c(-1,1)</code>, where the
log-odds is defined according to:
</p>
<p style="text-align: center;"><code class="reqn">log{p(y=1|x)/p(y=-1|x)} = gamma*(1 - x_1 + x_2 - ... + x_6)*(x_1 + x_2 + ... + x_6)</code>
</p>

<p>and <code class="reqn">x</code> is distributed as N(0, I_<code>d</code>x<code>d</code>).  See Friedman (2000).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>friedman_data(n = 500, d = 10, gamma = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="friedman_data_+3A_n">n</code></td>
<td>
<p>Number of points to simulate.</p>
</td></tr>
<tr><td><code id="friedman_data_+3A_d">d</code></td>
<td>
<p>The dimension of the predictor variable <code class="reqn">x</code>.</p>
</td></tr>
<tr><td><code id="friedman_data_+3A_gamma">gamma</code></td>
<td>
<p>A parameter controlling the Bayes error, with higher values of
<code>gamma</code> corresponding to lower error rates.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with the following components:
</p>
<table>
<tr><td><code>y</code></td>
<td>
<p>Vector of simulated response in <code>c(-1,1)</code>.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>An <code>n</code>x<code>d</code> matrix of simulated predictors.</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>The true conditional probability <code class="reqn">p(y=1|x)</code>.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Friedman, J., Hastie, T. and Tibshirani, R. (2000). Additive logistic
regression: a statistical view of boosting (with discussion), Annals of
Statistics 28: 337-307.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(111)
dat = friedman_data(n = 500, gamma = 0.5)

</code></pre>

<hr>
<h2 id='grid_probs'>Function to compute predicted quantiles</h2><span id='topic+grid_probs'></span>

<h3>Description</h3>

<p>Find predicted quantiles given classification results at different quantiles.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>grid_probs(X, q, delta, median_loc)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="grid_probs_+3A_x">X</code></td>
<td>
<p>Matrix of class predictions, where each column gives the predictions
for a given quantile in q.</p>
</td></tr>
<tr><td><code id="grid_probs_+3A_q">q</code></td>
<td>
<p>The quantiles for which the columns of X are predictions.</p>
</td></tr>
<tr><td><code id="grid_probs_+3A_delta">delta</code></td>
<td>
<p>The number of quantiles used.</p>
</td></tr>
<tr><td><code id="grid_probs_+3A_median_loc">median_loc</code></td>
<td>
<p>Location of median quantile (0-based indexing).</p>
</td></tr>
</table>

<hr>
<h2 id='index_over'>Return indices to be used for jittered data in oversampling</h2><span id='topic+index_over'></span>

<h3>Description</h3>

<p>Return indices to be used for jittered data in oversampling
</p>


<h3>Usage</h3>

<pre><code class='language-R'>index_over(ix_pos, ix_neg, q)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="index_over_+3A_ix_pos">ix_pos</code></td>
<td>
<p>Indices for positive examples in data.</p>
</td></tr>
<tr><td><code id="index_over_+3A_ix_neg">ix_neg</code></td>
<td>
<p>Indices for negative examples in data.</p>
</td></tr>
<tr><td><code id="index_over_+3A_q">q</code></td>
<td>
<p>Quantiles for which to construct tilted datasets.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a list, each of element of which gives indices to be used on
a particular cut (note: will be of length delta - 1)
</p>

<hr>
<h2 id='index_under'>Return indices to be used in original data for undersampling</h2><span id='topic+index_under'></span>

<h3>Description</h3>

<p>(note: sampling is done without replacement)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>index_under(ix_pos, ix_neg, q, delta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="index_under_+3A_ix_pos">ix_pos</code></td>
<td>
<p>Indices for positive examples in data.</p>
</td></tr>
<tr><td><code id="index_under_+3A_ix_neg">ix_neg</code></td>
<td>
<p>Indices for negative examples in data.</p>
</td></tr>
<tr><td><code id="index_under_+3A_q">q</code></td>
<td>
<p>Quantiles for which to construct tilted datasets.</p>
</td></tr>
<tr><td><code id="index_under_+3A_delta">delta</code></td>
<td>
<p>Number of quantiles.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a list, each of element of which gives indices to be used on
a particular cut (note: will be of length delta - 1)
</p>

<hr>
<h2 id='jous'>Jittering with Over/Under Sampling</h2><span id='topic+jous'></span>

<h3>Description</h3>

<p>Perform probability estimation using jittering with over or undersampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jous(X, y, class_func, pred_func, type = c("under", "over"), delta = 10,
  nu = 1, X_pred = NULL, keep_models = FALSE, verbose = FALSE,
  parallel = FALSE, packages = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jous_+3A_x">X</code></td>
<td>
<p>A matrix of continuous predictors.</p>
</td></tr>
<tr><td><code id="jous_+3A_y">y</code></td>
<td>
<p>A vector of responses with entries in <code>c(-1, 1)</code>.</p>
</td></tr>
<tr><td><code id="jous_+3A_class_func">class_func</code></td>
<td>
<p>Function to perform classification.  This function definition must be
exactly of the form <code>class_func(X, y)</code> where X is a matrix and y is a
vector with entries in <code>c(-1, 1)</code>, and it must return an object on which
<code>pred_func</code> can create predictions.  See examples.</p>
</td></tr>
<tr><td><code id="jous_+3A_pred_func">pred_func</code></td>
<td>
<p>Function to create predictions.  This function definition must be
exactly of the form <code>pred_func(fit_obj, X)</code> where <code>fit_obj</code>
is an object returned by class_func and X is a matrix of new data
values, and it must return a vector with entries in <code>c(-1, 1)</code>.
See examples.</p>
</td></tr>
<tr><td><code id="jous_+3A_type">type</code></td>
<td>
<p>Type of sampling: &quot;over&quot; for oversampling,  or &quot;under&quot; for
undersampling.</p>
</td></tr>
<tr><td><code id="jous_+3A_delta">delta</code></td>
<td>
<p>An integer (greater than 3) to control the number of quantiles to
estimate:</p>
</td></tr>
<tr><td><code id="jous_+3A_nu">nu</code></td>
<td>
<p>The amount of noise to apply to predictors when oversampling data.
The noise level is controlled by <code>nu * sd(X[,j])</code> for each
predictor - the default of <code>nu = 1</code> works well.  Such &quot;jittering&quot;
of the predictors is essential when applying <code>jous</code> to boosting
type methods.</p>
</td></tr>
<tr><td><code id="jous_+3A_x_pred">X_pred</code></td>
<td>
<p>A matrix of predictors for which to form probability estimates.</p>
</td></tr>
<tr><td><code id="jous_+3A_keep_models">keep_models</code></td>
<td>
<p>Whether to store all of the models used to create
the probability estimates.  If <code>type=FALSE</code>, the user will need
to re-run <code>jous</code> when creating probability estimates for test data.</p>
</td></tr>
<tr><td><code id="jous_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code>, print the function's progress to the terminal.</p>
</td></tr>
<tr><td><code id="jous_+3A_parallel">parallel</code></td>
<td>
<p>If <code>TRUE</code>, use parallel <code>foreach</code> to fit models.
Must register parallel before hand, such as <code>doParallel</code>.  See
examples below.</p>
</td></tr>
<tr><td><code id="jous_+3A_packages">packages</code></td>
<td>
<p>If <code>parallel = TRUE</code>, a vector of strings containing the
names of any packages used in <code>class_func</code> or <code>pred_func</code>.
See examples below.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list containing information about the
parameters used in the <code>jous</code> function call, as well as the following
additional components:
</p>
<table>
<tr><td><code>q</code></td>
<td>
<p>The vector of target quantiles estimated by <code>jous</code>.  Note that
the estimated probabilities will be located at the midpoints of the values in
<code>q</code>.</p>
</td></tr>
<tr><td><code>phat_train</code></td>
<td>
<p>The in-sample probability estimates <code class="reqn">p(y=1|x)</code>.</p>
</td></tr>
<tr><td><code>phat_test</code></td>
<td>
<p>Probability estimates for the optional test data in <code>X_test</code></p>
</td></tr>
<tr><td><code>models</code></td>
<td>
<p>If <code>keep_models=TRUE</code>, a list of models fitted to
the resampled data sets.</p>
</td></tr>
<tr><td><code>confusion_matrix</code></td>
<td>
<p>A confusion matrix for the in-sample fits.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The <code>jous</code> function runs the classifier <code>class_func</code> a total
of <code>delta</code> times on the data, which can be computationally expensive.
Also,<code>jous</code> cannot yet be applied to categorical predictors - in the
oversampling case, it is not clear how to &quot;jitter&quot; a categorical variable.
</p>


<h3>References</h3>

<p>Mease, D., Wyner, A. and Buja, A. (2007). Costweighted
boosting with jittering and over/under-sampling:
JOUS-boost. J. Machine Learning Research 8 409-439.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Generate data from Friedman model #
set.seed(111)
dat = friedman_data(n = 500, gamma = 0.5)
train_index = sample(1:500, 400)

# Apply jous to adaboost classifier
class_func = function(X, y) adaboost(X, y, tree_depth = 2, n_rounds = 200)
pred_func = function(fit_obj, X_test) predict(fit_obj, X_test)

jous_fit = jous(dat$X[train_index,], dat$y[train_index], class_func,
                pred_func, keep_models = TRUE)
# get probability
phat_jous = predict(jous_fit, dat$X[-train_index, ], type = "prob")

# compare with probability from AdaBoost
ada = adaboost(dat$X[train_index,], dat$y[train_index], tree_depth = 2,
               n_rounds = 200)
phat_ada = predict(ada, dat$X[train_index,], type = "prob")

mean((phat_jous - dat$p[-train_index])^2)
mean((phat_ada - dat$p[-train_index])^2)

## Example using parallel option

library(doParallel)
cl &lt;- makeCluster(4)
registerDoParallel(cl)

# n.b. the packages='rpart' is not really needed here since it gets
# exported automatically by JOUSBoost, but for illustration
jous_fit = jous(dat$X[train_index,], dat$y[train_index], class_func,
                pred_func, keep_models = TRUE, parallel = TRUE,
                packages = 'rpart')
phat = predict(jous_fit, dat$X[-train_index,], type = 'prob')
stopCluster(cl)

## Example using SVM

library(kernlab)
class_func = function(X, y) ksvm(X, as.factor(y), kernel = 'rbfdot')
pred_func = function(obj, X) as.numeric(as.character(predict(obj, X)))
jous_obj = jous(dat$X[train_index,], dat$y[train_index], class_func = class_func,
           pred_func = pred_func, keep_models = TRUE)
jous_pred = predict(jous_obj, dat$X[-train_index,], type = 'prob')

## End(Not run)
</code></pre>

<hr>
<h2 id='predict.adaboost'>Create predictions from AdaBoost fit</h2><span id='topic+predict.adaboost'></span>

<h3>Description</h3>

<p>Makes a prediction on new data for a given fitted <code>adaboost</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'adaboost'
predict(object, X, type = c("response", "prob"),
  n_tree = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.adaboost_+3A_object">object</code></td>
<td>
<p>An object of class <code>adaboost</code> returned by the <code>adaboost</code> function.</p>
</td></tr>
<tr><td><code id="predict.adaboost_+3A_x">X</code></td>
<td>
<p>A design matrix of predictors.</p>
</td></tr>
<tr><td><code id="predict.adaboost_+3A_type">type</code></td>
<td>
<p>The type of prediction to return.  If <code>type="response"</code>, a
class label of -1 or 1 is returned.  If <code>type="prob"</code>, the
probability <code class="reqn">p(y = 1 | x)</code> is returned.</p>
</td></tr>
<tr><td><code id="predict.adaboost_+3A_n_tree">n_tree</code></td>
<td>
<p>The number of trees to use in the prediction (by default, all
them).</p>
</td></tr>
<tr><td><code id="predict.adaboost_+3A_...">...</code></td>
<td>
<p>...</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a vector of class predictions if <code>type="response"</code>, or a
vector of class probabilities <code class="reqn">p(y=1|x)</code> if <code>type="prob"</code>.
</p>


<h3>Note</h3>

<p>Probabilities are estimated according to the formula:
</p>
<p style="text-align: center;"><code class="reqn">p(y=1| x) = 1/(1 + exp(-2*f(x)))</code>
</p>

<p>where <code class="reqn">f(x)</code> is the score function produced by AdaBoost.  See
Friedman (2000).
</p>


<h3>References</h3>

<p>Friedman, J., Hastie, T. and Tibshirani, R. (2000). Additive logistic
regression: a statistical view of boosting (with discussion), Annals of
Statistics 28: 337-307.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Generate data from the circle model
set.seed(111)
dat = circle_data(n = 500)
train_index = sample(1:500, 400)

ada = adaboost(dat$X[train_index,], dat$y[train_index], tree_depth = 2,
               n_rounds = 100, verbose = TRUE)
# get class prediction
yhat = predict(ada, dat$X[-train_index, ])
# get probability estimate
phat = predict(ada, dat$X[-train_index, ], type="prob")

## End(Not run)

</code></pre>

<hr>
<h2 id='predict.jous'>Create predictions</h2><span id='topic+predict.jous'></span>

<h3>Description</h3>

<p>Makes a prediction on new data for a given fitted <code>jous</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'jous'
predict(object, X, type = c("response", "prob"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.jous_+3A_object">object</code></td>
<td>
<p>An object of class <code>jous</code> returned by the <code>jous</code> function.</p>
</td></tr>
<tr><td><code id="predict.jous_+3A_x">X</code></td>
<td>
<p>A design matrix of predictors.</p>
</td></tr>
<tr><td><code id="predict.jous_+3A_type">type</code></td>
<td>
<p>The type of prediction to return.  If <code>type="response"</code>, a
class label of -1 or 1 is returned.  If <code>type="prob"</code>, the
probability <code class="reqn">p(y=1|x)</code> is returned.</p>
</td></tr>
<tr><td><code id="predict.jous_+3A_...">...</code></td>
<td>
<p>...</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a vector of class predictions if <code>type="response"</code>, or a
vector of class probabilities <code class="reqn">p(y=1|x)</code> if <code>type="prob"</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Generate data from Friedman model #
set.seed(111)
dat = friedman_data(n = 500, gamma = 0.5)
train_index = sample(1:500, 400)

# Apply jous to adaboost classifier
class_func = function(X, y) adaboost(X, y, tree_depth = 2, n_rounds = 100)
pred_func = function(fit_obj, X_test) predict(fit_obj, X_test)

jous_fit = jous(dat$X[train_index,], dat$y[train_index], class_func,
                pred_func, keep_models=TRUE)
# get class prediction
yhat = predict(jous_fit, dat$X[-train_index, ])
# get probability estimate
phat = predict(jous_fit, dat$X[-train_index, ], type="prob")

## End(Not run)
</code></pre>

<hr>
<h2 id='print.adaboost'>Print a summary of adaboost fit.</h2><span id='topic+print.adaboost'></span>

<h3>Description</h3>

<p>Print a summary of adaboost fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'adaboost'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.adaboost_+3A_x">x</code></td>
<td>
<p>An adaboost object fit using the <code>adaboost</code> function.</p>
</td></tr>
<tr><td><code id="print.adaboost_+3A_...">...</code></td>
<td>
<p>...</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Printed summary of the fit, including information about the tree
depth and number of boosting rounds used.
</p>

<hr>
<h2 id='print.jous'>Print a summary of <code>jous</code> fit.</h2><span id='topic+print.jous'></span>

<h3>Description</h3>

<p>Print a summary of <code>jous</code> fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'jous'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.jous_+3A_x">x</code></td>
<td>
<p>A <code>jous</code> object.</p>
</td></tr>
<tr><td><code id="print.jous_+3A_...">...</code></td>
<td>
<p>...</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Printed summary of the fit
</p>

<hr>
<h2 id='sonar'>Dataset of sonar measurements of rocks and mines</h2><span id='topic+sonar'></span>

<h3>Description</h3>

<p>A dataset containing sonar measurements used to discriminate rocks from mines.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(sonar)
</code></pre>


<h3>Format</h3>

<p>A data frame with 208 observations on 61 variables.  The variables
V1-V60 represent the energy within a certain frequency band, and
are to be used as predictors.  The variable y is a class label, 1
for 'rock' and -1 for 'mine'.</p>


<h3>Source</h3>

<p><a href="http://archive.ics.uci.edu/ml/">http://archive.ics.uci.edu/ml/</a>
</p>


<h3>References</h3>

<p>Gorman, R. P., and Sejnowski, T. J. (1988). &quot;Analysis of Hidden
Units in a Layered Network
Trained to Classify Sonar Targets&quot; in Neural Networks, Vol. 1, pp. 75-89.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
