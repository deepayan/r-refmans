<!DOCTYPE html><html><head><title>Help for package npsf</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {npsf}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#npsf-package'>
<p>Introduction to Nonparametric and Stochastic Frontier Analysis</p></a></li>
<li><a href='#banks00_07'><p>U.S. Commercial Banks Data</p></a></li>
<li><a href='#banks05'><p>U.S. Commercial Banks Data</p></a></li>
<li><a href='#ccr81'><p>Program Follow Through at Primary Schools</p></a></li>
<li><a href='#coef.npsf'>
<p>'coef' method for class 'npsf'</p></a></li>
<li><a href='#halton'>
<p>'halton' method for class 'npsf'</p></a></li>
<li><a href='#mroz'><p>Female labor force participation</p></a></li>
<li><a href='#nobs.npsf'>
<p>'nobs' method for class 'npsf'</p></a></li>
<li><a href='#nptestind'>
<p>Nonparametric Test of Independence</p></a></li>
<li><a href='#nptestrts'>
<p>Nonparametric Test of Returns to Scale</p></a></li>
<li><a href='#primes'>
<p>'primes' method for class 'npsf'</p></a></li>
<li><a href='#pwt56'><p>Penn World Tables 5.6 (compiled in 1995)</p></a></li>
<li><a href='#rescale'>
<p>'rescale' method for class 'npsf'</p></a></li>
<li><a href='#sf'>
<p>Stochastic Frontier Models Using Cross-Sectional and Panel Data</p></a></li>
<li><a href='#summary.npsf'>
<p>'summary' method for class 'npsf'</p></a></li>
<li><a href='#tenonradial'>
<p>Nonradial Measure of Technical Efficiency, the Russell Measure</p></a></li>
<li><a href='#tenonradialbc'>
<p>Statistical Inference Regarding the Russell Measure of Technical Efficiency</p></a></li>
<li><a href='#teradial'>
<p>Radial Measure of Technical Efficiency, the Debrue-Farrell Measure</p></a></li>
<li><a href='#teradialbc'>
<p>Statistical Inference Regarding the Radial Measure of Technical Efficiency</p></a></li>
<li><a href='#truncreg'>
<p>Parametric truncated regression for cross-sectional data</p></a></li>
<li><a href='#usmanuf'><p>US Manufacturing Industry Data</p></a></li>
<li><a href='#vcov.npsf'>
<p>'vcov' method for class 'npsf'</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Nonparametric and Stochastic Efficiency and Productivity
Analysis</td>
</tr>
<tr>
<td>Version:</td>
<td>0.8.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2020-11-22</td>
</tr>
<tr>
<td>Author:</td>
<td>Oleg Badunenko [aut, cre],
 Pavlo Mozharovskyi [aut],
 Yaryna Kolomiytseva [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Oleg Badunenko &lt;oleg.badunenko@brunel.ac.uk&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Nonparametric efficiency measurement and statistical inference via DEA type estimators (see FÃ¤re, Grosskopf, and Lovell (1994) &lt;<a href="https://doi.org/10.1017%2FCBO9780511551710">doi:10.1017/CBO9780511551710</a>&gt;, Kneip, Simar, and Wilson (2008) &lt;<a href="https://doi.org/10.1017%2FS0266466608080651">doi:10.1017/S0266466608080651</a>&gt; and Badunenko and Mozharovskyi (2020) &lt;<a href="https://doi.org/10.1080%2F01605682.2019.1599778">doi:10.1080/01605682.2019.1599778</a>&gt;) as well as Stochastic Frontier estimators for both cross-sectional data and 1st, 2nd, and 4th generation models for panel data (see Kumbhakar and Lovell (2003) &lt;<a href="https://doi.org/10.1017%2FCBO9781139174411">doi:10.1017/CBO9781139174411</a>&gt;, Badunenko and Kumbhakar (2016) &lt;<a href="https://doi.org/10.1016%2Fj.ejor.2016.04.049">doi:10.1016/j.ejor.2016.04.049</a>&gt;). The stochastic frontier estimators can handle both half-normal and truncated normal models with conditional mean and heteroskedasticity. The marginal effects of determinants can be obtained.</td>
</tr>
<tr>
<td>Depends:</td>
<td>Formula</td>
</tr>
<tr>
<td>Suggests:</td>
<td>snowFT, Rmpi</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-11-22 21:47:22 UTC; boo</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-11-22 22:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='npsf-package'>
Introduction to Nonparametric and Stochastic Frontier Analysis
</h2><span id='topic+npsf-package'></span><span id='topic+npsf'></span>

<h3>Description</h3>

<p>This package provides a variety of tools for nonparametric and parametric efficiency measurement. 
</p>


<h3>Details</h3>

<p>The nonparametric models in <code>npsf</code> comprise nonradial efficiency measurement (<code><a href="#topic+tenonradial">tenonradial</a></code>), where non-proportional reductions (expansions) in each positive input (output) are allowed, as well as popular radial efficiency measurement (<code><a href="#topic+teradial">teradial</a></code>), where movements to the frontier are proportional.
</p>
<p>Using bootstrapping techniques,  <code><a href="#topic+teradialbc">teradialbc</a></code>, <code><a href="#topic+tenonradialbc">tenonradialbc</a></code>, <code><a href="#topic+nptestrts">nptestrts</a></code>, <code><a href="#topic+nptestind">nptestind</a></code> deal with statistical inference about the radial efficiency measurement. <code><a href="#topic+nptestind">nptestind</a></code> helps in deciding which type of the bootstrap to employ. Global return to scale and individual scale efficiency is tested by <code><a href="#topic+nptestrts">nptestrts</a></code>.  Finally, <code><a href="#topic+teradialbc">teradialbc</a></code> and <code><a href="#topic+tenonradialbc">tenonradialbc</a></code>, performs bias correction of the radial Debrue-Farrell and nonradial Russell input- or output-based measure of technical efficiency, computes bias and constructs confidence intervals.
</p>
<p>Computer intensive functions <code><a href="#topic+teradialbc">teradialbc</a></code> and <code><a href="#topic+nptestrts">nptestrts</a></code> allow making use of parallel computing, even on a single machine with multiple cores. Help files contain examples that are intended to introduce the usage.
</p>
<p>The parametric stochastic frontier models in <code>npsf</code> can be estimated by <code><a href="#topic+sf">sf</a></code>, which performs maximum likelihood estimation of the frontier parameters and technical or cost efficiencies.  Inefficiency error component can be assumed to be have either half-normal or truncated normal distribution.  <code><a href="#topic+sf">sf</a></code> allows modelling multiplicative heteroskedasticity of either inefficiency or random noise component, or both.  Additionally, marginal effects of determinants on the expected value of inefficiency term can be computed.
</p>
<p>For details of the respective method please see the reference at the end of this introduction and of the respective help file.
</p>
<p>All function in <code>npsf</code> accept formula with either names of variables in the data set, or names of the matrices.  Except for <code><a href="#topic+nptestind">nptestind</a></code>, all function return <code>esample</code>, a logical vector length of which is determined by <code>data</code> and <code>subset</code> (if specified) or number of rows in matrix <code>outputs</code>. <code>esample</code> equals <code>TRUE</code> if this data point parted in estimation procedure, and <code>FALSE</code> otherwise.
</p>
<p>Results can be summarized using <code><a href="#topic+summary.npsf">summary.npsf</a></code>.
</p>


<h3>Author(s)</h3>

<p>Oleg Badunenko, &lt;oleg.badunenko@brunel.ac.uk&gt;
</p>
<p>Pavlo Mozharovskyi, &lt;pavlo.mozharovskyi@telecom-paris.fr&gt;
</p>
<p>Yaryna Kolomiytseva, &lt;kolomiytseva@wiso.uni-koeln.de&gt;
</p>
<p>Maintainer: Oleg Badunenko &lt;oleg.badunenko@brunel.ac.uk&gt;
</p>


<h3>References</h3>

<p>Badunenko, O. and Kumbhakar, S.C. (2016), When, Where and How to Estimate Persistent and Transient Efficiency in Stochastic Frontier Panel Data Models, <em>European Journal of Operational Research</em>, <b>255</b>(1), 272&ndash;287, doi: <a href="https://doi.org/10.1016/j.ejor.2016.04.049">10.1016/j.ejor.2016.04.049</a>
</p>
<p>Badunenko, O. and Mozharovskyi, P. (2016), Nonparametric Frontier Analysis using Stata, <em>Stata Journal</em>, <b>16</b>3, 550&ndash;89, doi: <a href="https://doi.org/10.1177/1536867X1601600302">10.1177/1536867X1601600302</a>
</p>
<p>Badunenko, O. and Mozharovskyi, P. (2020), Statistical inference for the Russell measure of technical efficiency, <em>Journal of the Operational Research Society</em>, <b>71</b>3, 517&ndash;527, doi: <a href="https://doi.org/10.1080/01605682.2019.1599778">10.1080/01605682.2019.1599778</a>
</p>
<p>Bartelsman, E.J. and Gray, W. (1996), The NBER Manufacturing Productivity Database, <em>National Bureau of Economic Research</em>, Technical Working Paper Series, doi: <a href="https://doi.org/10.3386/t0205">10.3386/t0205</a>
</p>
<p>Battese, G., Coelli, T. (1988), Prediction of firm-level technical effiiencies with a generalized frontier production function and panel data. <em>Journal of Econometrics</em>, <b>38</b>, 387&ndash;399
</p>
<p>Battese, G., Coelli, T. (1992), Frontier production functions, technical efficiency and panel data: With application to paddy farmers in India. <em>Journal of Productivity Analysis</em>, <b>3</b>, 153&ndash;169
</p>
<p>Charnes, A., W. W. Cooper, and E. Rhodes. 1981. Evaluating Program and Managerial Efficiency: An Application of Data Envelopment Analysis to Program Follow Through. <em>Management Science</em> <b>27</b>: 668&ndash;697
</p>
<p>Caudill, S., Ford, J., Gropper, D. (1995), Frontier estimation and firm-specific inefficiency measures in the presence of heteroscedasticity. <em>Journal of Business and Economic Statistics</em>, <b>13</b>, 105&ndash;111
</p>
<p>Debreu, G. 1951. The Coefficient of Resource Utilization. <em>Econometrica</em> <b>19</b>: 273&ndash;292
</p>
<p>FÃ¤re, R. and Lovell, C. A. K. (1978), Measuring the technical efficiency of production, <em>Journal of Economic Theory</em>, <b>19</b>, 150&ndash;162, doi: <a href="https://doi.org/10.1016/0022-0531(78)90060-1">10.1016/0022-0531(78)90060-1</a>
</p>
<p>FÃ¤re, R., Grosskopf, S. and Lovell, C. A. K. (1994), <em>Production Frontiers</em>, Cambridge U.K.: Cambridge University Press, doi: <a href="https://doi.org/10.1017/CBO9780511551710">10.1017/CBO9780511551710</a>
</p>
<p>Farrell, M. J. 1957. The Measurement of Productive Efficiency. <em>Journal of the Royal Statistical Society. Series A (General)</em> <b>120</b>(3): 253&ndash;290
</p>
<p>Heston, A., and R. Summers. 1991. The Penn World Table (Mark 5): An Expanded Set of International Comparisons, 1950-1988. <em>The Quarterly Journal of Economics</em> <b>106</b>: 327&ndash;368
</p>
<p>Horrace, W. and Schmidt, P. (1996), On ranking and selection from independent truncated normal distributions. <em>Journal of Productivity Analysis</em>, <b>7</b>, 257&ndash;282
</p>
<p>Jondrow, J., Lovell, C., Materov, I., Schmidt, P. (1982), On estimation of technical inefficiency in the stochastic frontier production function model. <em>Journal of Econometrics</em>, <b>19</b>, 233&ndash;238
</p>
<p>Kneip, A., Simar L., and P.W. Wilson (2008), Asymptotics and Consistent Bootstraps for DEA Estimators in Nonparametric Frontier Models, <em>Econometric Theory</em>, <b>24</b>, 1663&ndash;1697, doi: <a href="https://doi.org/10.1017/S0266466608080651">10.1017/S0266466608080651</a>
</p>
<p>Koetter, M., Kolari, J., and Spierdijk, L. (2012), Enjoying the quiet life under deregulation? Evidence from adjusted Lerner indices for U.S. banks. <em>Review of Economics and Statistics</em>, <b>94</b>, 2, 462&ndash;480
</p>
<p>Kumbhakar, S. (1990),  Production Frontiers, Panel Data, and Time-varying Technical Inefficiency. <em>Journal of Econometrics</em>, <b>46</b>, 201&ndash;211
</p>
<p>Kumbhakar, S. and Lovell, C. (2003), <em>Stochastic Frontier Analysis.</em> Cambridge: Cambridge University Press, doi: <a href="https://doi.org/10.1017/CBO9781139174411">10.1017/CBO9781139174411</a>
</p>
<p>Restrepo-Tobon, D. and Kumbhakar, S. (2014), Enjoying the quiet life under deregulation? Not Quite. <em>Journal of Applied Econometrics</em>, <b>29</b>, 2, 333&ndash;343
</p>
<p>Simar, L. and P.W. Wilson (1998), Sensitivity Analysis of Efficiency Scores: How to Bootstrap in Nonparametric Frontier Models, <em>Management Science</em>, <b>44</b>, 49&ndash;61, doi: <a href="https://doi.org/10.1287/mnsc.44.1.49">10.1287/mnsc.44.1.49</a>
</p>
<p>Simar, L. and P.W. Wilson (2000), A General Methodology for Bootstrapping in Nonparametric Frontier Models, <em>Journal of Applied Statistics</em>, <b>27</b>, 779&ndash;802, doi: <a href="https://doi.org/10.1080/02664760050081951">10.1080/02664760050081951</a>
</p>
<p>Simar, L. and P.W. Wilson (2002), Nonparametric Tests of Return to Scale, <em>European Journal of Operational Research</em>, <b>139</b>, 115&ndash;132
</p>
<p>Wang, H.-J. (2002), Heteroskedasticity and non-monotonic efficiency effects of a stochastic frontier model. <em>Journal of Productivity Analysis</em>, <b>18</b>, 241&ndash;253
</p>
<p>Wilson P.W.  (2003), Testing Independence in Models of Productive Efficiency, <em>Journal of Productivity Analysis</em>, <b>20</b>, 361&ndash;390, doi: <a href="https://doi.org/10.1023/A:1027355917855">10.1023/A:1027355917855</a>
</p>

<hr>
<h2 id='banks00_07'>U.S. Commercial Banks Data</h2><span id='topic+banks00_07'></span>

<h3>Description</h3>

<p><code>banks00_07</code> data frame contains selected variables for 500 (randomly sampled from around 5000) U.S. commercial banks from data of Koetter et al. (2012) for years 2000-2007.  This data are used for illustrution purposes and are not suitable for research purposes. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(banks00_07)</code></pre>


<h3>Format</h3>

<p>This data frame contains the following variables (columns):
</p>

<dl>
<dt><code>year</code></dt><dd><p>Year.</p>
</dd>
<dt><code>id</code></dt><dd><p>Entity ID.</p>
</dd>
<dt><code>TA</code></dt><dd><p>Gross total assets.</p>
</dd>
<dt><code>LLP</code></dt><dd><p>Loan loss provisions.</p>
</dd>
<dt><code>Y1</code></dt><dd><p>Total securities (in thousands of US dollars).</p>
</dd>
<dt><code>Y2</code></dt><dd><p>Total loans and leases (in thousands of US dollars).</p>
</dd>
<dt><code>W1</code></dt><dd><p>Cost of fixed assets divided by the cost of borrowed funds.</p>
</dd>
<dt><code>W2</code></dt><dd><p>Cost of labor (in thousands of US dollars) divided by the cost of borrowed funds.</p>
</dd>
<dt><code>ER</code></dt><dd><p>Gross total equity to gross total assets ratio.</p>
</dd>
<dt><code>TC</code></dt><dd><p>Total operating cost.</p>
</dd>
<dt><code>LA</code></dt><dd><p>Total loans and leases to gross total assets ratio.</p>
</dd>
<dt><code>Ti</code></dt><dd><p>Times bank is observed.</p>
</dd>
<dt><code>TA_ave</code></dt><dd><p>Mean value of TA.</p>
</dd>
<dt><code>TA_initial</code></dt><dd><p>Value of TA in the first observed year.</p>
</dd>
<dt><code>LLP_ave</code></dt><dd><p>Mean value of LLP.</p>
</dd>
<dt><code>LLP_initial</code></dt><dd><p>Value of LLP in the first observed year.</p>
</dd>
<dt><code>ER_ave</code></dt><dd><p>Mean value of ER.</p>
</dd>
<dt><code>ER_initial</code></dt><dd><p>Value of ER in the first observed year.</p>
</dd>
<dt><code>LA_ave</code></dt><dd><p>Mean value of LA.</p>
</dd>
<dt><code>LA_initial</code></dt><dd><p>Value of LA in the first observed year.</p>
</dd>
</dl>



<h3>Details</h3>

<p>The data were sampled and generated as shown in section &quot;Examples&quot;.
</p>


<h3>Source</h3>

<p><a href="http://qed.econ.queensu.ca/jae/2014-v29.2/restrepo-tobon-kumbhakar/">http://qed.econ.queensu.ca/jae/2014-v29.2/restrepo-tobon-kumbhakar/</a>.
</p>


<h3>References</h3>

<p>Koetter, M., Kolari, J., and Spierdijk, L. (2012), Enjoying the quiet life under deregulation? Evidence from adjusted Lerner indices for U.S. banks. <em>Review of Economics and Statistics</em>, <b>94</b>, 2, 462&ndash;480.
</p>
<p>Restrepo-Tobon, D. and Kumbhakar, S. (2014), Enjoying the quiet life under deregulation? Not Quite. <em>Journal of Applied Econometrics</em>, <b>29</b>, 2, 333&ndash;343.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

# Download data from the link in "Source"

banks00_07 &lt;- read.delim("2b_QLH.txt")

# rename 'entity' to 'id'
colnames(banks00_07) [colnames(banks00_07) == "entity"] &lt;- "id"

table(banks00_07$year)
# keep if 2000 -- 2007

banks00_07 &lt;- 
 banks00_07[(banks00_07$year &gt;= 2000 &amp; banks00_07$year &lt;= 2007),]
dim(banks00_07)

q1q3 &lt;- quantile(banks00_07$TA, probs = c(.25,.75))

banks00_07 &lt;- 
 banks00_07[(banks00_07$TA &gt;= q1q3[1] &amp; banks00_07$TA &lt;= q1q3[2]),]
dim(banks00_07)


# generate required variables
banks00_07$TC &lt;-banks00_07$TOC
banks00_07$ER &lt;- banks00_07$Z / banks00_07$TA
banks00_07$LA &lt;- banks00_07$Y2 / banks00_07$TA

banks00_07 &lt;- 
 banks00_07[, colnames(banks00_07) 
 c("id", "year", "Ti", "TC", "Y1", "Y2", "W1","W2", "ER", "LA", "TA", "LLP")]
dim(banks00_07)

t0 &lt;- as.vector( by(data = banks00_07$id, 
                    INDICES = banks00_07$id, 
                    FUN = function(qq) length(qq)) )
banks00_07$Ti &lt;- rep(t0, times = t0)
banks00_07 &lt;- banks00_07[banks00_07$Ti &gt; 4,]

# complete observations
banks00_07 &lt;- banks00_07[complete.cases(banks00_07),]
dim(banks00_07)

id_names &lt;- unique(banks00_07$id)
N_total &lt;- length(id_names)
set.seed(816376586)
ids_n2choose &lt;- sample(1:N_total, 500)
ids2choose &lt;- id_names[ids_n2choose]
banks00_07 &lt;- banks00_07[banks00_07$id 
dim(banks00_07)

t0 &lt;- as.vector( by(data = banks00_07$id, 
                    INDICES = banks00_07$id, 
                    FUN = function(qq) length(qq)) )
length(rep(t0, times = t0))

banks00_07$Ti &lt;- rep(t0, times = t0)

banks00_07[1:50,c("id","year","Ti")]

# keep if Ti &gt; 4

banks00_07 &lt;- banks00_07[banks00_07$Ti &gt; 4,]
dim(banks00_07)

# sort
banks00_07 &lt;- banks00_07[order(banks00_07$id, banks00_07$year),]

# TC = TOC
#
# ER = Z / TA
# Gross total equity to gross total assets ratio.
#
# LA = Y2 / TA
# Total loans and leases to gross total assets ratio.

banks00_07$TA_ave &lt;- 
 rep(as.vector( by(data = banks00_07$TA,
                   INDICES = banks00_07$id, 
                   FUN = function(qq) mean(qq))), times = t0)

banks00_07$TA_initial &lt;- 
 rep(as.vector( by(data = banks00_07$TA, 
                   INDICES = banks00_07$id,
                   FUN = function(qq) qq[1])), times = t0)

banks00_07$LLP_ave &lt;- 
 rep(as.vector( by(data = banks00_07$LLP,
                   INDICES = banks00_07$id,
                   FUN = function(qq) mean(qq))), times = t0)

banks00_07$LLP_initial &lt;- 
 rep(as.vector( by(data = banks00_07$LLP, 
                   INDICES = banks00_07$id, 
                   FUN = function(qq) qq[1])), times = t0)

banks00_07$ER_ave &lt;- 
 rep(as.vector( by(data = banks00_07$ER, 
                   INDICES = banks00_07$id, 
                   FUN = function(qq) mean(qq))), times = t0)

banks00_07$ER_initial &lt;- 
 rep(as.vector( by(data = banks00_07$ER, 
                   INDICES = banks00_07$id, 
                   FUN = function(qq) qq[1])), times = t0)

banks00_07$LA_ave &lt;- 
 rep(as.vector( by(data = banks00_07$LA, 
                   INDICES = banks00_07$id, 
                   FUN = function(qq) mean(qq))), times = t0)

banks00_07$LA_initial &lt;- 
 rep(as.vector( by(data = banks00_07$LA, 
                   INDICES = banks00_07$id, 
                   FUN = function(qq) qq[1])), times = t0)

cols2export &lt;- c("id","year","Ti","TA","TA_ave",
                 "TA_initial","LLP","LLP_ave",
                 "LLP_initial","ER_ave","ER_initial","LA_ave","LA_initial")

write.table(x = banks00_07, file = "banks00_07.txt", row.names = FALSE)


## End(Not run)

</code></pre>

<hr>
<h2 id='banks05'>U.S. Commercial Banks Data</h2><span id='topic+banks05'></span>

<h3>Description</h3>

<p><code>banks05</code> data frame contains selected variables from the U.S. commercial banks data of Koetter et al. (2012) for year 2005 and 500 banks randomly sampled from around 5000. Dependent variable was randomly generated, as described under 'Details', to satisfy the assumptions of doubly heteroskedastic stochastic cost frontier model. This data is, therefore, not suitable for research purposes. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(banks05)</code></pre>


<h3>Format</h3>

<p>This data frame contains the following variables (columns):
</p>

<dl>
<dt><code>lnC</code></dt><dd><p>Randomly generated total operating costs.</p>
</dd>
<dt><code>lnw1</code></dt><dd><p>Logarithm of the cost of fixed assets divided by the cost of borrowed funds.</p>
</dd>
<dt><code>lnw2</code></dt><dd><p>Logarithm of the cost of labor (in thousands of US dollars) divided by the cost of borrowed funds.</p>
</dd>
<dt><code>lny1</code></dt><dd><p>Logarithm of total securities (in thousands of US dollars).</p>
</dd>
<dt><code>lny2</code></dt><dd><p>Logarithm of total loans and leases (in thousands of US dollars).</p>
</dd>
<dt><code>ER</code></dt><dd><p>Gross total equity to gross total assets ratio.</p>
</dd>
<dt><code>LA</code></dt><dd><p>Total loans and leases to gross total assets ratio.</p>
</dd>
</dl>



<h3>Details</h3>

<p>The variable representing total operating costs was generated as follows: </p>
<p style="text-align: center;"><code class="reqn">lnC = \beta0 + \beta1*lnw1 + \beta2*lnw2 + \gamma1*lny1 + \gamma2*lny2 + \nu + u,</code>
</p>
<p> where <code class="reqn">\nu ~ N(0, exp(\alpha0 + \alpha1*LA))</code> and <code class="reqn">u ~ N+(\delta0 + \delta1*ER, exp(\omega0 + \omega1*ER)).</code> More detailed description of input prices, outputs, and exogenous variables is provided in Koetter et al. (2012). See also related study of Restrepo-Tobon and Kumbhakar (2014).
</p>


<h3>Source</h3>

<p><a href="http://qed.econ.queensu.ca/jae/2014-v29.2/restrepo-tobon-kumbhakar/">http://qed.econ.queensu.ca/jae/2014-v29.2/restrepo-tobon-kumbhakar/</a>.
</p>


<h3>References</h3>

<p>Koetter, M., Kolari, J., and Spierdijk, L. (2012), Enjoying the quiet life under deregulation? Evidence from adjusted Lerner indices for U.S. banks. <em>Review of Economics and Statistics</em>, <b>94</b>, 2, 462&ndash;480.
</p>
<p>Restrepo-Tobon, D. and Kumbhakar, S. (2014), Enjoying the quiet life under deregulation? Not Quite. <em>Journal of Applied Econometrics</em>, <b>29</b>, 2, 333&ndash;343.
</p>

<hr>
<h2 id='ccr81'>Program Follow Through at Primary Schools</h2><span id='topic+ccr81'></span>

<h3>Description</h3>

<p>The data set is from an US federally sponsored program for providing remedial assistance to disadvantaged primary school students. The data comprises 70 school sites.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data( ccr81 )</code></pre>


<h3>Format</h3>

<p>This data frame contains the following variables (columns):
</p>

<dl>
<dt><code>nu</code></dt><dd><p>School Site Number</p>
</dd>    		<dt><code>y1</code></dt><dd><p>Total Reading Score</p>
</dd>
<dt><code>y2</code></dt><dd><p>Total Math Score</p>
</dd>
<dt><code>y3</code></dt><dd><p>Total Coopersmith Score</p>
</dd>
<dt><code>x1</code></dt><dd><p>Education Level of Mother</p>
</dd>
<dt><code>x2</code></dt><dd><p>Occupation Index</p>
</dd>
<dt><code>x3</code></dt><dd><p>Parental Visit Index</p>
</dd>
<dt><code>x4</code></dt><dd><p>Counseling Index</p>
</dd>
<dt><code>x5</code></dt><dd><p>Number of Teachers</p>
</dd>
</dl>



<h3>Details</h3>

<p>The data were originally used to evaluate the efficiency of public programs and their management.
</p>


<h3>Source</h3>

<p>A. Charnes, W. W. Cooper and E. Rhodes (1981), Evaluating Program and Managerial Efficiency: An Application of Data Envelopment Analysis to Program Follow Through, <em>Management Science</em>, <b>27</b>, 668&ndash;697.
</p>


<h3>References</h3>

<p>Charnes, A., W. W. Cooper, and E. Rhodes. 1981. Evaluating Program and Managerial Efficiency: An Application of Data Envelopment Analysis to Program Follow Through. <em>Management Science</em> <b>27</b>: 668&ndash;697
</p>

<hr>
<h2 id='coef.npsf'>
'coef' method for class 'npsf'
</h2><span id='topic+coef.npsf'></span>

<h3>Description</h3>

<p>Extracts the ML parameters of a stochastic frontier model estimated by <code><a href="#topic+sf">sf</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> ## S3 method for class 'npsf'
coef( object, ... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.npsf_+3A_object">object</code></td>
<td>

<p>an object of class <code>npsf</code> returned by the function <code><a href="#topic+sf">sf</a></code>.
</p>
</td></tr>
<tr><td><code id="coef.npsf_+3A_...">...</code></td>
<td>

<p>currently unused.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>coef.npsf</code> returns a named vector of the ML parameters of a stochastic frontier model.
</p>


<h3>Author(s)</h3>

<p>Oleg Badunenko &lt;oleg.badunenko@brunel.ac.uk&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcov.npsf">vcov.npsf</a></code>, <code><a href="#topic+nobs.npsf">nobs.npsf</a></code>, <code><a href="#topic+summary.npsf">summary.npsf</a></code>, and <code><a href="#topic+sf">sf</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require( npsf )
 
# Load Penn World Tables 5.6 dataset
 
data( pwt56 )
head( pwt56 )
 
# Create some missing values
 
pwt56 [4, "K"] &lt;- NA 
 
# Stochastic production frontier model with 
# homoskedastic error components (half-normal)
 
# Use subset of observations - for year 1965
 
m1 &lt;- sf(log(Y) ~ log(L) + log(K), data = pwt56, 
 subset = year == 1965, distribution = "h")
coef( m1 )
</code></pre>

<hr>
<h2 id='halton'>
'halton' method for class 'npsf'
</h2><span id='topic+halton'></span>

<h3>Description</h3>

<p>Provides Halton draws, deviates from a uniform distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  halton(n = 1, n.bases = 1, bases = NULL, 
                   start = 0, random.primes = FALSE, seed = 7, 
                   scale.coverage = FALSE, shuffle = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="halton_+3A_n">n</code></td>
<td>

<p>number of prime numbers to be returned (the row number in the value).
</p>
</td></tr>
<tr><td><code id="halton_+3A_n.bases">n.bases</code></td>
<td>

<p>numeric. number of bases used.  (the column number in the value).
</p>
</td></tr>
<tr><td><code id="halton_+3A_bases">bases</code></td>
<td>

<p>numeric. Supply specific order numbers for getting primes, see <code><a href="#topic+primes">primes</a></code>.  See examples.
</p>
</td></tr>
<tr><td><code id="halton_+3A_start">start</code></td>
<td>

<p>numeric. from which value in the halton sequence to start.  Default is 0, which is actually 0.
</p>
</td></tr>
<tr><td><code id="halton_+3A_random.primes">random.primes</code></td>
<td>

<p>logical. if <code>TRUE</code>, the <code>n.bases</code> primes are chosen on a random basis from 100008 available prime numbers. See <code><a href="#topic+primes">primes</a></code>.
</p>
</td></tr>
<tr><td><code id="halton_+3A_seed">seed</code></td>
<td>

<p>set seed for replicability. Default is 17345168.
</p>
</td></tr>
<tr><td><code id="halton_+3A_scale.coverage">scale.coverage</code></td>
<td>

<p>logical. at larger primes not whole [0,1] interval is covered. if <code>TRUE</code>, <code>rescale</code> is used to fill the coverage.
</p>
</td></tr>
<tr><td><code id="halton_+3A_shuffle">shuffle</code></td>
<td>

<p>logical. if <code>TRUE</code>, each column in the value is randomly reshuffled (<code>seed</code> is used).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>halton</code> returns Halton draws.
</p>


<h3>Author(s)</h3>

<p>Oleg Badunenko &lt;oleg.badunenko@brunel.ac.uk&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sf">sf</a></code> and <code><a href="#topic+primes">primes</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  require( npsf )
  
  # obtain first 10 x 7 matrix with the first 7 primes as bases
  npsf::halton(10, n.bases = 7)
  
  # obtain first 10 x 7 matrix with the randomly chosen 7 primes as bases
  npsf::halton(10, n.bases = 7, random.primes = TRUE, seed = 17345168)
  
  # just one column with desired prime
  npsf::halton(10, bases = 1)
  
  # or 2 columns
  npsf::halton(10, bases = c(1,7))
  
  # if bases are large
  npsf::halton(10, bases = c(1,7)*1000)
  
  # the coverage is not great
  npsf::halton(10, bases = c(1,7)*1000, scale.coverage = TRUE)
  
  # reshuffle them, use seed for replicability
  npsf::halton(10, bases = c(1,7)*1000, scale.coverage = TRUE, shuffle = TRUE, seed = 17345168)
  
</code></pre>

<hr>
<h2 id='mroz'>Female labor force participation</h2><span id='topic+mroz'></span>

<h3>Description</h3>

<p>Instructional dataset, N=753, cross-sectional labor force participation data Accompanying Introductory Econometrics: A Modern Approach, Jeffrey M. Wooldridge, South-Western College Publishing, (c) 2000 and Jeffrey M. Wooldridge, Econometric Analysis of Cross Section and Panel Data, MIT Press,(c) 2001.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data( mroz )</code></pre>


<h3>Format</h3>

<p>This data frame contains the following variables (columns):
</p>

<dl>
<dt><code>inlf</code></dt><dd><p>=1 if in labor force, 1975</p>
</dd>
<dt><code>hours</code></dt><dd><p>hours worked, 1975</p>
</dd>
<dt><code>kidslt6</code></dt><dd><p># kids &lt; 6 years</p>
</dd>
<dt><code>kidsge6</code></dt><dd><p># kids 6-18</p>
</dd>
<dt><code>age</code></dt><dd><p>woman's age in yrs</p>
</dd>
<dt><code>educ</code></dt><dd><p>years of schooling</p>
</dd>
<dt><code>wage</code></dt><dd><p>estimated wage from earns., hours</p>
</dd>
<dt><code>repwage</code></dt><dd><p>reported wage at interview in 1976</p>
</dd>
<dt><code>hushrs</code></dt><dd><p>hours worked by husband, 1975</p>
</dd>
<dt><code>husage</code></dt><dd><p>husband's age</p>
</dd>
<dt><code>huseduc</code></dt><dd><p>husband's years of schooling</p>
</dd>
<dt><code>huswage</code></dt><dd><p>husband's hourly wage, 1975</p>
</dd>
<dt><code>faminc</code></dt><dd><p>family income, 1975</p>
</dd>
<dt><code>mtr</code></dt><dd><p>fed. marginal tax rate facing woman</p>
</dd>
<dt><code>motheduc</code></dt><dd><p>mother's years of schooling</p>
</dd>
<dt><code>fatheduc</code></dt><dd><p>father's years of schooling</p>
</dd>
<dt><code>unem</code></dt><dd><p>unem. rate in county of resid.</p>
</dd>
<dt><code>city</code></dt><dd><p>=1 if live in SMSA</p>
</dd>
<dt><code>exper</code></dt><dd><p>actual labor mkt exper</p>
</dd>
<dt><code>nwifeinc</code></dt><dd><p>(faminc - wage*hours)/1000</p>
</dd>
</dl>



<h3>Details</h3>

<p>Instructional dataset, N=753, cross-sectional labor force participation data Accompanying Introductory Econometrics: A Modern Approach, Jeffrey M. Wooldridge, South-Western College Publishing, (c) 2000 and Jeffrey M. Wooldridge, Econometric Analysis of Cross Section and Panel Data, MIT Press,(c) 2001. 
</p>


<h3>Source</h3>

<p>Datasets accessible from http://wooldridge.swcollege.com, http://courses.bus.msu.edu/econ/821/001/index.cfm?action=mat, and http://www.cengage.com/aise/economics/wooldridge_3e_datasets/
</p>


<h3>References</h3>

<p>Mroz, T.A. 1987. The Sensitiviy of an Empirical Model of Married Women's Hours of Work to Economic and Statistical Assumptions. <em>Econometrica</em> <b>55</b>: 765-799
</p>

<hr>
<h2 id='nobs.npsf'>
'nobs' method for class 'npsf'
</h2><span id='topic+nobs.npsf'></span>

<h3>Description</h3>

<p>Extracts the number of observations for which efficiencies are estimated 
by SF or DEA model estimated by <code><a href="#topic+sf">sf</a></code>, <code><a href="#topic+teradial">teradial</a></code>, 
<code><a href="#topic+tenonradial">tenonradial</a></code>, or <code><a href="#topic+teradialbc">teradialbc</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> ## S3 method for class 'npsf'
nobs( object, ... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nobs.npsf_+3A_object">object</code></td>
<td>

<p>an object of class <code>npsf</code> returned by the function <code><a href="#topic+sf">sf</a></code>).
</p>
</td></tr>
<tr><td><code id="nobs.npsf_+3A_...">...</code></td>
<td>

<p>currently unused.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>nobs.npsf</code> returns the number of observations for which efficiencies are estimated by SF or DEA model.
</p>


<h3>Author(s)</h3>

<p>Oleg Badunenko &lt;oleg.badunenko@brunel.ac.uk&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcov.npsf">vcov.npsf</a></code>, <code><a href="#topic+coef.npsf">coef.npsf</a></code>, <code><a href="#topic+summary.npsf">summary.npsf</a></code>, and <code><a href="#topic+sf">sf</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require( npsf )
 
# Load Penn World Tables 5.6 dataset
 
data( pwt56 )
head( pwt56 )
 
# Create some missing values
 
pwt56 [4, "K"] &lt;- NA 
 
# Stochastic production frontier model with 
# homoskedastic error components (half-normal)
 
# Use subset of observations - for year 1965
 
m1 &lt;- sf(log(Y) ~ log(L) + log(K), data = pwt56, 
 subset = year == 1965, distribution = "h")
nobs( m1 )

# DEA

t1 &lt;- teradialbc ( Y ~ K + L, data = pwt56, subset = Nu &lt; 10)
nobs(t1)
</code></pre>

<hr>
<h2 id='nptestind'>
Nonparametric Test of Independence
</h2><span id='topic+nptestind'></span>

<h3>Description</h3>

<p>In output based efficiency measurement, routine <code>nptestind</code> perform test that radial (Debreu-Farrell) output-based measure of technical efficiency under chosen assumption about the technology and mix of outputs are independent. In input-based efficiency measurement, routine <code>nptestind</code> perform test that radial (Debreu-Farrell) input-based measure of technical efficiency under chosen assumption about the technology and mix of inputs are independent. Testing is performed using bootstrap technique.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nptestind(formula, data, subset,
 rts = c("C", "NI", "V"), base = c("output", "input"),
 reps = 999, alpha = 0.05,
 print.level = 1, dots = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nptestind_+3A_formula">formula</code></td>
<td>

<p>an object of class &ldquo;formula&rdquo; (or one that can be coerced to that class): a symbolic description of the model. The details of model specification are given under &lsquo;Details&rsquo;.
</p>
</td></tr>
<tr><td><code id="nptestind_+3A_data">data</code></td>
<td>

<p>an optional data frame containing the variables in the model. If not found in data, the variables are taken from environment (<code>formula</code>), typically the environment from which <code>teradial</code> is called.
</p>
</td></tr>
<tr><td><code id="nptestind_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations for which technical efficiency is to be computed.
</p>
</td></tr>
<tr><td><code id="nptestind_+3A_rts">rts</code></td>
<td>

<p>character or numeric. string: first letter of the word &ldquo;c&rdquo; for constant, &ldquo;n&rdquo; for non-increasing, or &ldquo;v&rdquo; for variable returns to scale assumption. numeric: 3 for constant, 2 for non-increasing, or 1 for variable returns to scale assumption. 
</p>
</td></tr>
<tr><td><code id="nptestind_+3A_base">base</code></td>
<td>

<p>character or numeric. string: first letter of the word &ldquo;o&rdquo; for computing output-based or &ldquo;i&rdquo; for computing input-based technical efficiency measure. string: 2 for computing output-based or 1 for computing input-based technical efficiency measure
</p>
</td></tr>
<tr><td><code id="nptestind_+3A_reps">reps</code></td>
<td>

<p>specifies the number of bootstrap replications to be performed.  The default is 999.  The minimum is 100.  Adequate estimates of confidence intervals using bias-corrected methods typically require 1,000 or more replications.
</p>
</td></tr>
<tr><td><code id="nptestind_+3A_alpha">alpha</code></td>
<td>

<p>sets significance level; default is <code>alpha=0.05</code>.
</p>
</td></tr>
<tr><td><code id="nptestind_+3A_dots">dots</code></td>
<td>

<p>logical. Relevant if <code>print.level&gt;=1</code>. If TRUE, one dot character is displayed for each successful replication; if FALSE,  display of the replication dots is suppressed.
</p>
</td></tr>
<tr><td><code id="nptestind_+3A_print.level">print.level</code></td>
<td>

<p>numeric. 0 - nothing is printed; 1 - print summary of the model and data. 2 - print summary of technical efficiency measures. 3 - print estimation results observation by observation. Default is 1.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In output based efficiency measurement, routine <code>nptestind</code> perform test that radial (Debreu-Farrell) output-based measure of technical efficiency under chosen assumption about the technology and mix of outputs are independent. In input-based efficiency measurement, routine <code>nptestind</code> perform test that radial (Debreu-Farrell) input-based measure of technical efficiency under chosen assumption about the technology and mix of inputs are independent.
</p>
<p>Testing is performed using bootstrap technique (see Wilson, 2003).
</p>
<p>Results can be summarized using <code><a href="#topic+summary.npsf">summary.npsf</a></code>.
</p>


<h3>Value</h3>

<p><code>nptestrts</code> returns a list of class <code>npsf</code> containing the following elements:
</p>
<table>
<tr><td><code>K</code></td>
<td>

<p>numeric: number of data points.
</p>
</td></tr>
<tr><td><code>M</code></td>
<td>

<p>numeric: number of outputs.
</p>
</td></tr>
<tr><td><code>N</code></td>
<td>

<p>numeric: number of inputs.
</p>
</td></tr>
<tr><td><code>rts</code></td>
<td>

<p>string: RTS assumption.
</p>
</td></tr>
<tr><td><code>base</code></td>
<td>

<p>string: base for efficiency measurement.
</p>
</td></tr>
<tr><td><code>reps</code></td>
<td>

<p>numeric: number of bootstrap replications.
</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>

<p>numeric: significance level.
</p>
</td></tr>
<tr><td><code>t4n</code></td>
<td>

<p>numeric: value of the T4n statistic.
</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>

<p>numeric: p-value of the test of independence.
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Results can be summarized using <code><a href="#topic+summary.npsf">summary.npsf</a></code>.
</p>


<h3>Author(s)</h3>

<p>Oleg Badunenko &lt;oleg.badunenko@brunel.ac.uk&gt;
</p>


<h3>References</h3>

<p>FÃ¤re, R. and Lovell, C. A. K. (1978), Measuring the technical efficiency of production, <em>Journal of Economic Theory</em>, <b>19</b>, 150&ndash;162, doi: <a href="https://doi.org/10.1016/0022-0531(78)90060-1">10.1016/0022-0531(78)90060-1</a>
</p>
<p>FÃ¤re, R., Grosskopf, S. and Lovell, C. A. K. (1994), <em>Production Frontiers</em>, Cambridge U.K.: Cambridge University Press
</p>
<p>Wilson P.W. (2003), Testing Independence in Models of Productive Efficiency, <em>Journal of Productivity Analysis</em>, <b>20</b>, 361&ndash;390, doi: <a href="https://doi.org/10.1023/A:1027355917855">10.1023/A:1027355917855</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+teradial">teradial</a></code>, <code><a href="#topic+tenonradial">tenonradial</a></code>, <code><a href="#topic+teradialbc">teradialbc</a></code>, <code><a href="#topic+tenonradialbc">tenonradialbc</a></code>, <code><a href="#topic+nptestrts">nptestrts</a></code>, <code><a href="#topic+sf">sf</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

require( npsf )

# Prepare data and matrices

data( ccr81 )
head( ccr81 )

# Create some missing values

ccr81 [64, "x4"] &lt;- NA # just to create missing
ccr81 [68, "y2"] &lt;- NA # just to create missing

Y2 &lt;- as.matrix( ccr81[ , c("y1", "y2", "y3"), drop = FALSE] )
X2 &lt;- as.matrix( ccr81[ , c("x1", "x2", "x3", "x4", "x5"), drop = FALSE] )

# Perform nonparametric test that radial (Debreu-Farrell) 
# output-based measure of technical efficiency under assumption of 
# NIRS technology and mix of outputs are independent. Test is 
# performed based on 999 replications at the 5

t1 &lt;- nptestind ( y1 + y2 + y3 ~ x1 + x2 + x3 + x4 + x5, 
	data = ccr81, base = "o", rts = "n", 
	reps = 999, dots = TRUE)


# Really large data-set

data(usmanuf)
head(usmanuf)

nrow(usmanuf)
table(usmanuf$year)

# This will take some time depending on computer power

data(usmanuf)
head(usmanuf)

t2 &lt;- nptestind ( Y ~ K + L + M, data = usmanuf, 
	subset = year &gt;= 1999 &amp; year &lt;= 2000, 
	reps = 999, dots = TRUE, base = "i", rts = "v")


## End(Not run)

</code></pre>

<hr>
<h2 id='nptestrts'>
Nonparametric Test of Returns to Scale
</h2><span id='topic+nptestrts'></span>

<h3>Description</h3>

<p>Routine <code>nptestrts</code> performs nonparametric tests the returns to scale of the underlying technology via bootstrapping techniques.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nptestrts(formula, data, subset,
 base = c("output", "input"),
 homogeneous = TRUE, test.two = TRUE,
 reps = 999, alpha = 0.05,
 core.count = 1, cl.type = c("SOCK", "MPI"),
 print.level = 1, dots = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nptestrts_+3A_formula">formula</code></td>
<td>

<p>an object of class &ldquo;formula&rdquo; (or one that can be coerced to that class): a symbolic description of the model. The details of model specification are given under &lsquo;Details&rsquo;.
</p>
</td></tr>
<tr><td><code id="nptestrts_+3A_data">data</code></td>
<td>

<p>an optional data frame containing the variables in the model. If not found in data, the variables are taken from environment (<code>formula</code>), typically the environment from which <code>teradial</code> is called.
</p>
</td></tr>
<tr><td><code id="nptestrts_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations for which technical efficiency is to be computed.
</p>
</td></tr>
<tr><td><code id="nptestrts_+3A_base">base</code></td>
<td>

<p>character or numeric. string: first letter of the word &ldquo;o&rdquo; for computing output-based or &ldquo;i&rdquo; for computing input-based technical efficiency measure. string: 2 for computing output-based or 1 for computing input-based technical efficiency measure
</p>
</td></tr>
<tr><td><code id="nptestrts_+3A_homogeneous">homogeneous</code></td>
<td>

<p>logical. If TRUE, the reference set is bootstrapped with homogeneous smoothing; if FALSE, the reference set is bootstrapped with heterogeneous smoothing.
</p>
</td></tr>
<tr><td><code id="nptestrts_+3A_test.two">test.two</code></td>
<td>

<p>logical. If TRUE, test 2, where efficiency measures under assumption of non-increasing and variable returns to scale technology are compared; if FALSE, <code>nptestrts</code> stops after test 1 is completed.
</p>
</td></tr>
<tr><td><code id="nptestrts_+3A_reps">reps</code></td>
<td>

<p>specifies the number of bootstrap replications to be performed.  The default is 999.  The minimum is 100.  Adequate estimates of confidence intervals using bias-corrected methods typically require 1,000 or more replications.
</p>
</td></tr>
<tr><td><code id="nptestrts_+3A_alpha">alpha</code></td>
<td>

<p>sets significance level; default is <code>alpha=0.05</code>.
</p>
</td></tr>
<tr><td><code id="nptestrts_+3A_core.count">core.count</code></td>
<td>

<p>positive integer. Number of cluster nodes. If <code>core.count=1</code>, the process runs sequentially. See  <code>performParallel</code> in package <code>snowFT</code> for more details.
</p>
</td></tr>
<tr><td><code id="nptestrts_+3A_cl.type">cl.type</code></td>
<td>

<p>Character string that specifies cluster type (see <code>makeClusterFT</code> in package <code>snowFT</code>). Possible values are 'MPI' and 'SOCK' ('PVM' is currently not available). See <code>performParallel</code> in package <code>snowFT</code> for more details.
</p>
</td></tr>
<tr><td><code id="nptestrts_+3A_dots">dots</code></td>
<td>

<p>logical. Relevant if <code>print.level&gt;=1</code>. If TRUE, one dot character is displayed for each successful replication; if FALSE,  display of the replication dots is suppressed.
</p>
</td></tr>
<tr><td><code id="nptestrts_+3A_print.level">print.level</code></td>
<td>

<p>numeric. 0 - nothing is printed; 1 - print summary of the model and data. 2 - print summary of technical efficiency measures. 3 - print estimation results observation by observation. Default is 1.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Routine <code>nptestrts</code> performs nonparametric tests the returns to scale of the underlying technology (see Simar L. and P.W. Wilson (2002), Nonparametric Tests of Return to Scale, <em>European Journal of Operational Research</em>, <b>139</b>, 115&ndash;132, doi: <a href="https://doi.org/10.1016/S0377-2217(01)00167-9">10.1016/S0377-2217(01)00167-9</a>).
</p>
<p>If <code>test.two</code> is not specified, <code>nptestrts</code> performs only Test #1, which consists of two parts. First, the null hypothesis that the technology is globally CRS (vs VRS) is tested. Second, the null hypothesis that the data point is scale efficient is tested.
</p>
<p>If <code>test.two</code> is specified, <code>nptestrts</code> may perform Test #2. If the null hypothesis that the technology is CRS is rejected, <code>test.two</code> requests that <code>nptestrts</code> tests the null hypothesis that the technology is NIRS (vs VRS). If not all data points are scale efficient, <code>nptestrts</code> tests that the reason for scale inefficiency is DRS. If the null hypothesis that the technology is CRS is not rejected and all data points are scale efficient, <code>nptestrts</code> will not perform Test #2 even if <code>test.two</code> is specified.
</p>
<p>Models for <code>nptestrts</code> are specified symbolically. A typical model has the form <code>outputs ~ inputs</code>, where <code>outputs</code> (<code>inputs</code>) is a series of (numeric) terms which specifies outputs (inputs).  Refer to the examples.
</p>
<p>If <code>core.count&gt;=1</code>, <code>nptestrts</code> will perform bootstrap on multiple cores.  Parallel computing requires package <code>snowFT</code>. By the default cluster type is defined by option <code>cl.type="SOCK"</code>.  Specifying <code>cl.type="MPI"</code> requires package <code>Rmpi</code>.
</p>
<p>On some systems, specifying option <code>cl.type="SOCK"</code> results in much quicker execution than specifying option <code>cl.type="MPI"</code>.  Option <code>cl.type="SOCK"</code> might be problematic on Mac system.
</p>
<p>Parallel computing make a difference for large data sets.  Specifying option <code>dots=TRUE</code> will indicate at what speed the bootstrap actually proceeds.  Specify <code>reps=100</code> and compare two runs with option <code>core.count=1</code> and <code>core.count&gt;1</code> to see if parallel computing speeds up the bootstrap.  For small samples, parallel computing may actually slow down the <code>nptestrts</code>.
</p>
<p>Results can be summarized using <code><a href="#topic+summary.npsf">summary.npsf</a></code>.
</p>


<h3>Value</h3>

<p><code>nptestrts</code> returns a list of class <code>npsf</code> containing the following elements:
</p>
<table>
<tr><td><code>K</code></td>
<td>

<p>numeric: number of data points.
</p>
</td></tr>
<tr><td><code>M</code></td>
<td>

<p>numeric: number of outputs.
</p>
</td></tr>
<tr><td><code>N</code></td>
<td>

<p>numeric: number of inputs.
</p>
</td></tr>
<tr><td><code>rts</code></td>
<td>

<p>string: RTS assumption.
</p>
</td></tr>
<tr><td><code>base</code></td>
<td>

<p>string: base for efficiency measurement.
</p>
</td></tr>
<tr><td><code>reps</code></td>
<td>

<p>numeric: number of bootstrap replications.
</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>

<p>numeric: significance level.
</p>
</td></tr>
<tr><td><code>teCrs</code></td>
<td>

<p>numeric: measures of technical efficiency under the assumption of CRS.
</p>
</td></tr>
<tr><td><code>teNrs</code></td>
<td>

<p>numeric: measures of technical efficiency under the assumption of NiRS.
</p>
</td></tr>
<tr><td><code>teVrs</code></td>
<td>

<p>numeric: measures of technical efficiency under the assumption of VRS.
</p>
</td></tr>
<tr><td><code>sefficiency</code></td>
<td>

<p>numeric: scale efficiency.
</p>
</td></tr>
<tr><td><code>sefficiencyMean</code></td>
<td>

<p>numeric: ratio of means of technical efficiency measures under CRS and VRS.
</p>
</td></tr>
<tr><td><code>pGlobalCRS</code></td>
<td>

<p>numeric: p-value of the test that the technology is globally CRS.
</p>
</td></tr>
<tr><td><code>psefficient</code></td>
<td>

<p>numeric: p-value of the test that data point is statistically scale efficient.
</p>
</td></tr>
<tr><td><code>sefficient</code></td>
<td>

<p>logical: returns <code>TRUE</code>, if statistically scale efficient; <code>FALSE</code> otherwise.
</p>
</td></tr>
<tr><td><code>nsefficient</code></td>
<td>

<p>numeric: number of statistically scale efficient.
</p>
</td></tr>
<tr><td><code>nrsOVERvrsMean</code></td>
<td>

<p>numeric: ratio of means of technical efficiency measures under NIRS and VRS (if <code>test.two=TRUE</code>).
</p>
</td></tr>
<tr><td><code>pGlobalNRS</code></td>
<td>

<p>numeric: p-value of the test the technology is globally NIRS (if <code>test.two=TRUE</code>).
</p>
</td></tr>
<tr><td><code>sineffdrs</code></td>
<td>

<p>logical: returns <code>TRUE</code> if statistically scale inefficient due to DRS and <code>FALSE</code> if statistically scale inefficient due to IRS (if <code>test.two=TRUE</code> and not all data points are statistically scale efficient <code>nsefficient&lt;K</code>).
</p>
</td></tr>
<tr><td><code>pineffdrs</code></td>
<td>

<p>numeric: p-value of the test that data point is scale inefficient due to DRS (if <code>test.two=TRUE</code> and not all data points are statistically scale efficient <code>nsefficient&lt;K</code>).
</p>
</td></tr>
<tr><td><code>nrsOVERvrs</code></td>
<td>

<p>numeric: ratio of measures of technical efficiency under NiRS and VRS (if <code>test.two=TRUE</code> and not all data points are statistically scale efficient <code>nsefficient&lt;K</code>).
</p>
</td></tr>
<tr><td><code>esample</code></td>
<td>

<p>logical: returns TRUE if the observation in user supplied data is in the estimation subsample and FALSE otherwise.
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Before specifying option <code>homogeneous</code> it is advised to preform the test of independence (see <code><a href="#topic+nptestind">nptestind</a></code>).
</p>
<p>Results can be summarized using <code><a href="#topic+summary.npsf">summary.npsf</a></code>.
</p>


<h3>Author(s)</h3>

<p>Oleg Badunenko &lt;oleg.badunenko@brunel.ac.uk&gt;, Pavlo Mozharovskyi &lt;pavlo.mozharovskyi@telecom-paris.fr&gt;
</p>


<h3>References</h3>

<p>Badunenko, O. and Mozharovskyi, P. (2016), Nonparametric Frontier Analysis using Stata, <em>Stata Journal</em>, <b>16</b>3, 550&ndash;89, doi: <a href="https://doi.org/10.1177/1536867X1601600302">10.1177/1536867X1601600302</a>
</p>
<p>FÃ¤re, R. and Lovell, C. A. K. (1978), Measuring the technical efficiency of production, <em>Journal of Economic Theory</em>, <b>19</b>, 150&ndash;162, doi: <a href="https://doi.org/10.1016/0022-0531(78)90060-1">10.1016/0022-0531(78)90060-1</a>
</p>
<p>FÃ¤re, R., Grosskopf, S. and Lovell, C. A. K. (1994), <em>Production Frontiers</em>, Cambridge U.K.: Cambridge University Press, doi: <a href="https://doi.org/10.1017/CBO9780511551710">10.1017/CBO9780511551710</a>
</p>
<p>Simar L. and P.W. Wilson (2002), Nonparametric Tests of Return to Scale, <em>European Journal of Operational Research</em>, <b>139</b>, 115&ndash;132, doi: <a href="https://doi.org/10.1016/S0377-2217(01)00167-9">10.1016/S0377-2217(01)00167-9</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+teradial">teradial</a></code>, <code><a href="#topic+tenonradial">tenonradial</a></code>, <code><a href="#topic+teradialbc">teradialbc</a></code>, <code><a href="#topic+tenonradialbc">tenonradialbc</a></code>, <code><a href="#topic+nptestind">nptestind</a></code>, <code><a href="#topic+sf">sf</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

require( npsf )

# Prepare data and matrices

data( ccr81 )
head( ccr81 )

# Create some missing values

ccr81 [64, "x4"] &lt;- NA # just to create missing
ccr81 [68, "y2"] &lt;- NA # just to create missing

Y2 &lt;- as.matrix( ccr81[ , c("y1", "y2", "y3"), drop = FALSE] )
X2 &lt;- as.matrix( ccr81[ , c("x1", "x2", "x3", "x4", "x5"), drop = FALSE] )

# Perform output-based test of returns to scale smoothed 
# homogeneous bootstrap with 999 replications at the 5
# significance level.  Also perform Test #2

t1 &lt;- nptestrts(y1 + y2 + y3 ~ x1 + x2 + x3 + x4 + x5,
	data = ccr81, homogeneous = TRUE,
	reps = 999, dots = TRUE, base = "o")

# suppress printing replication dots
t2 &lt;- nptestrts(Y2 ~ X2,
	homogeneous = TRUE,
	reps = 100, dots = FALSE, base = "o")


# heterogeneous
t3 &lt;- nptestrts(Y2 ~ X2,
	homogeneous = FALSE,
	reps = 100, dots = TRUE, base = "o")


# ===========================
# ===  Parallel computing ===
# ===========================

# Perform previous test but use 8 cores and
# cluster type SOCK

t3 &lt;-  nptestrts(y1 + y2 + y3 ~ x1 + x2 + x3 + x4 + x5,
	data = ccr81, homogeneous = FALSE,
	reps = 100, dots = TRUE, base = "o",
	core.count = 8, cl.type = "SOCK")


# Really large data-set

data(usmanuf)
head(usmanuf)

nrow(usmanuf)
table(usmanuf$year)

# Figure industries to include in the sample (first quarter)

summary(usmanuf[usmanuf$year &gt;= 1999 &amp; usmanuf$year &lt; 2000, "naics"])

# This test is quite demanding and it will take some time
# depending on computer power

t4 &lt;- nptestrts(Y ~ K + L + M, data = usmanuf,
	subset = year &gt;= 1999 &amp; year &lt; 2000 &amp; naics &lt; 321900,
	homogeneous = FALSE, reps = 100, dots = TRUE, base = "o",
	core.count = 8, cl.type = "SOCK")

# This is very computer intensive task

t5 &lt;- nptestrts(Y ~ K + L + M, data = usmanuf,
	subset = year &gt;= 1999 &amp; year &lt; 2000,
	homogeneous = FALSE, reps = 100, dots = TRUE, base = "o",
	core.count = 8, cl.type = "SOCK")


## End(Not run)

</code></pre>

<hr>
<h2 id='primes'>
'primes' method for class 'npsf'
</h2><span id='topic+primes'></span>

<h3>Description</h3>

<p>Provides prime numbers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  primes(n = NULL, which = NULL, random.primes = FALSE, seed = 7)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="primes_+3A_n">n</code></td>
<td>

<p>number of prime numbers to be returned. Should be smaller than 100008.
</p>
</td></tr>
<tr><td><code id="primes_+3A_which">which</code></td>
<td>

<p>numeric. if specific prime numbers are requred.  See examples.
</p>
</td></tr>
<tr><td><code id="primes_+3A_random.primes">random.primes</code></td>
<td>

<p>logical. if <code>n</code> is supplied and <code>random.primes = TRUE</code>, the  <code>n</code> primes are chosen on a random basis from 100008 available prime numbers.
</p>
</td></tr>
<tr><td><code id="primes_+3A_seed">seed</code></td>
<td>

<p>set seed for replicability. Default is 17345168.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>primes</code> just returns prime numbers, which come from <a href="https://primes.utm.edu/lists/small/100000.txt">https://primes.utm.edu/lists/small/100000.txt</a>, see <a href="https://primes.utm.edu">https://primes.utm.edu</a>
</p>


<h3>Value</h3>

<p><code>primes</code> returns prime numbers.
</p>


<h3>Author(s)</h3>

<p>Oleg Badunenko &lt;oleg.badunenko@brunel.ac.uk&gt;
</p>


<h3>Source</h3>

<p><a href="https://primes.utm.edu/lists/small/100000.txt">https://primes.utm.edu/lists/small/100000.txt</a> and <a href="https://primes.utm.edu">https://primes.utm.edu</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sf">sf</a></code> and <code><a href="#topic+halton">halton</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  require( npsf )
  
  # obtain first 30 prime numbers
  npsf::primes( 30 )
  
  # the same as
  npsf::primes( n = 30 )
  
  # the result in both case above are 30 prime numbers
  
  # if we use
  npsf::primes( which = 30 )
  
  # the 30th prime is returns, just a scalar
  
  # both cannot be used
  # npsf::primes(n = 30, which = 30, random.primes = FALSE, seed = 17345168)
  # will give a mistake
  
  # you can get random 30 primes, use seed for replicability
  npsf::primes(n = 30, random.primes = TRUE, seed = 17345168)
  
  # obtain specific primes: which take order number(s)
  npsf::primes(which = c(3,67,30, 100008))

</code></pre>

<hr>
<h2 id='pwt56'>Penn World Tables 5.6 (compiled in 1995)</h2><span id='topic+pwt56'></span>

<h3>Description</h3>

<p>The data set is from Penn World Tables (PWT) 5.6.  This data set provides only selected variables for years 1965 and 1990.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data( pwt56 )</code></pre>


<h3>Format</h3>

<p>This data frame contains the following variables (columns):
</p>

<dl>
<dt><code>Nu</code></dt><dd><p>Order Number</p>
</dd>
<dt><code>Country</code></dt><dd><p>Country Name</p>
</dd>
<dt><code>year</code></dt><dd><p>1965 or 1990</p>
</dd>
<dt><code>Y</code></dt><dd><p>Real GDP chain, international prices of 1985</p>
</dd>
<dt><code>K</code></dt><dd><p>Capital stock, international prices of 1985</p>
</dd>
<dt><code>L</code></dt><dd><p>Number of workers, in thousands</p>
</dd>
</dl>



<h3>Details</h3>

<p>The Penn World Table was developed by Robert Summers and Alan Heston (and others) to facilitate consistent national accounts comparisons across countries as well as over time. The data can be used to evaluate the efficiency of economies of various countries in years 1965 and 1990.
</p>


<h3>Source</h3>

<p><a href="http://www.rug.nl/research/ggdc/data/pwt/pwt-5.6">http://www.rug.nl/research/ggdc/data/pwt/pwt-5.6</a>. These data were originally hosted on the website of the Center for International Comparisons at the University of Pennsylvania.
</p>


<h3>References</h3>

<p>Heston, A. and Summers, R. (1991), The Penn World Table (Mark 5): An Expanded Set of International Comparisons, 1950-1988, <em>The Quarterly Journal of Economics</em>, <b>106</b>, 327&ndash;368.
</p>

<hr>
<h2 id='rescale'>
'rescale' method for class 'npsf'
</h2><span id='topic+rescale'></span>

<h3>Description</h3>

<p>rescales a vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  rescale(x, lb = min(x), ub = max(x))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rescale_+3A_x">x</code></td>
<td>

<p>a numeric vector.
</p>
</td></tr>
<tr><td><code id="rescale_+3A_lb">lb</code></td>
<td>

<p>numeric. lower bound.
</p>
</td></tr>
<tr><td><code id="rescale_+3A_ub">ub</code></td>
<td>

<p>numeric. upper bound.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>rescale</code> returns rescaled vector.
</p>


<h3>Author(s)</h3>

<p>Oleg Badunenko &lt;oleg.badunenko@brunel.ac.uk&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sf">sf</a></code>, <code><a href="#topic+primes">primes</a></code>, and <code><a href="#topic+halton">halton</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  require( npsf )
  
  # obtain first 30 prime numbers
  set.seed(8265897)
  t1 &lt;- runif(10, min = 1, max = 2)
  summary(t1)
  summary(rescale(t1, 0, 10))
  
</code></pre>

<hr>
<h2 id='sf'>
Stochastic Frontier Models Using Cross-Sectional and Panel Data
</h2><span id='topic+sf'></span>

<h3>Description</h3>

<p><code>sf</code> performs maximum likelihood estimation of the parameters and technical or cost efficiencies in cross-sectional stochastic (production or cost) frontier models with half-normal or truncated normal distributional assumption imposed on inefficiency error component. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sf(formula, data, it = NULL, subset,
 prod = TRUE, model = "K1990", distribution = c("h"),
 eff.time.invariant = FALSE, 
 mean.u.0i.zero     = FALSE,
 mean.u.0i          = NULL,
 ln.var.u.0i        = NULL,
 ln.var.v.0i        = NULL,
 ln.var.v.it        = NULL,  
 simtype = c("halton", "rnorm"), halton.base = NULL, R = 500,
 simtype_GHK = c("halton", "runif"), R_GHK = 500,
 random.primes = FALSE,
 cost.eff.less.one  = FALSE, level = 95, marg.eff = FALSE,
 start.val = NULL, maxit = 199, report.ll.optim = 10, 
 reltol = 1e-8, lmtol = sqrt(.Machine$double.eps),
 digits = 4, print.level = 4, seed = 17345168,
 only.data = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sf_+3A_formula">formula</code></td>
<td>

<p>an object of class &ldquo;formula&rdquo; (or one that can be coerced to that class): a symbolic description of the model. The details of model specification are given under &lsquo;Details&rsquo;.
</p>
</td></tr>
<tr><td><code id="sf_+3A_data">data</code></td>
<td>

<p>an optional data frame containing variables in the model. If not found in data, the variables are taken from environment (<code>formula</code>), typically the environment from which <code>sf</code> is called.
</p>
</td></tr>
<tr><td><code id="sf_+3A_it">it</code></td>
<td>

<p>vector with two character entries.  E.g., c(&quot;ID&quot;, &quot;TIME&quot;), where &quot;ID&quot; defines individuals that are observed in time periods defined by &quot;TIME&quot;. The default is <code>NULL</code>.  At default, cross-sectional model will be estimated. 
</p>
</td></tr>
<tr><td><code id="sf_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations for which technical or cost efficiencies are to be computed.
</p>
</td></tr>
<tr><td><code id="sf_+3A_prod">prod</code></td>
<td>

<p>logical. If <code>TRUE</code>, the estimates of parameters of stochastic production frontier model and of technical efficiencies are returned; if <code>FALSE</code>, the estimates of parameters of stochastic cost frontier model and of cost efficiencies are returned.
</p>
</td></tr>
<tr><td><code id="sf_+3A_model">model</code></td>
<td>

<p>character. Five panel data models are estimated for now. &quot;K1990&quot; and &quot;K1990modified&quot; (see Kumbhakar, 1990), &quot;BC1992&quot; (see Battese and Coelli, 1992), &quot;4comp&quot; (see Badunenko and Kumbhakar (2016) and Filippini and Greene, 2016). They specify common evolution of inefficiency.  Deffault is &quot;K1990&quot;. The time functions are &quot;( 1 + exp(b*t + c*t^2) )^-1&quot;, &quot;1 + d*(t-T_i) + e*(t-T_i)^2&quot;, and &quot;exp( -f*(t-T_i) )&quot;, respectively.
</p>
</td></tr>
<tr><td><code id="sf_+3A_distribution">distribution</code></td>
<td>

<p>either <code>"h"</code> (half-normal), <code>"t"</code> (truncated normal), or <code>"e"</code> (exponential, only crosssectional models), specifying the distribution of inefficiency term.
</p>
</td></tr>
<tr><td><code id="sf_+3A_eff.time.invariant">eff.time.invariant</code></td>
<td>

<p>logical. If <code>TRUE</code>, the 1st generation of panel data models is estimated, otherwise, the 2nd generation or 4 components panel data model is estimated.
</p>
</td></tr>
<tr><td><code id="sf_+3A_mean.u.0i.zero">mean.u.0i.zero</code></td>
<td>

<p>logical. If <code>TRUE</code>, normal-half normal model is estimated, otherwise, normal-truncated model is estimated.
</p>
</td></tr>
<tr><td><code id="sf_+3A_mean.u.0i">mean.u.0i</code></td>
<td>

<p>one-sided formula; e.g. <code>mean.u.0i ~ z1 + z2</code>. Specifies whether the mean of pre-truncated normal distribution of inefficiency term is a linear function of exogenous variables. In cross-sectional models, used only when <code>distribution = "t"</code>. If <code>NULL</code>, mean is assumed to be constant for all ids. 
</p>
</td></tr>
<tr><td><code id="sf_+3A_ln.var.u.0i">ln.var.u.0i</code></td>
<td>

<p>one-sided formula; e.g. <code>ln.var.u.0i ~ z1 + z2</code>. Specifies exogenous variables entering the expression for the log of variance of inefficiency error component. If <code>NULL</code>, inefficiency term is assumed to be homoskedastic, i.e. <code class="reqn">\sigma_u^2 = exp(\gamma[0])</code>. Time invariant variables are expected.
</p>
</td></tr>
<tr><td><code id="sf_+3A_ln.var.v.0i">ln.var.v.0i</code></td>
<td>

<p>one-sided formula; e.g. <code>ln.var.v.0i ~ z1 + z2</code>. Specifies exogenous variables entering the expression for variance of random noise error component. If <code>NULL</code>, random noise component is assumed to be homoskedastic, i.e. <code class="reqn">\sigma_v^2 = exp(\gamma[0])</code>. Time invariant variables are expected.
</p>
</td></tr>
<tr><td><code id="sf_+3A_ln.var.v.it">ln.var.v.it</code></td>
<td>

<p>one-sided formula; e.g. <code>ln.var.v.it ~ z1 + z2</code>. Specifies exogenous variables entering the expression for variance of random noise error component. If <code>NULL</code>, random noise component is assumed to be homoskedastic, i.e. <code class="reqn">\sigma_v^2 = exp(\gamma[0])</code>. Time invariant variables are expected.
</p>
</td></tr>
<tr><td><code id="sf_+3A_simtype">simtype</code></td>
<td>

<p>character. Type of random deviates for the 4 components model. 'halton' draws are default.  One can specify 'rnorm.'
</p>
</td></tr>
<tr><td><code id="sf_+3A_halton.base">halton.base</code></td>
<td>

<p>numeric.  The prime number which is the base for the Halton draws. If not used, different bases are used for each id.
</p>
</td></tr>
<tr><td><code id="sf_+3A_r">R</code></td>
<td>

<p>numeric.  Number of draws.  Default is 500.  Can be time consuming.
</p>
</td></tr>
<tr><td><code id="sf_+3A_simtype_ghk">simtype_GHK</code></td>
<td>

<p>character. Type of random deviates for use in GHK for efficiency estimating by approximation. 'halton' draws are default.  One can specify 'runif.'
</p>
</td></tr>
<tr><td><code id="sf_+3A_r_ghk">R_GHK</code></td>
<td>

<p>numeric.  Number of draws for GHK.  Default is 500.  Can be time consuming.
</p>
</td></tr>
<tr><td><code id="sf_+3A_random.primes">random.primes</code></td>
<td>

<p>logical. If <code>TRUE</code>, and <code>halton.base = NULL</code>, the primes are chosen on a random basis for each ID from 100008 available prime numbers.
</p>
</td></tr>
<tr><td><code id="sf_+3A_cost.eff.less.one">cost.eff.less.one</code></td>
<td>

<p>logical. If <code>TRUE</code>, and <code>prod = FALSE</code>, reported cost efficiencies are larger than one. Interpretation: by what factor is total cost larger than the potential total cost.
</p>
</td></tr>
<tr><td><code id="sf_+3A_level">level</code></td>
<td>

<p>numeric. Defines level% two-sided prediction interval for technical or cost efficiencies (see Horrace and Schmidt 1996). Default is 95.
</p>
</td></tr>
<tr><td><code id="sf_+3A_marg.eff">marg.eff</code></td>
<td>

<p>logical. If <code>TRUE</code>, unit-specific marginal effects of exogenous variables on the mean of distribution of inefficiency term are returned.
</p>
</td></tr>
<tr><td><code id="sf_+3A_start.val">start.val</code></td>
<td>

<p>numeric. Starting values to be supplied to the optimization routine. If <code>NULL</code>, OLS and method of moments estimates are used (see Kumbhakar and Lovell 2000).
</p>
</td></tr>
<tr><td><code id="sf_+3A_maxit">maxit</code></td>
<td>

<p>numeric. Maximum number of iterations. Default is 199.
</p>
</td></tr>
<tr><td><code id="sf_+3A_report.ll.optim">report.ll.optim</code></td>
<td>

<p>numeric. Not used for now.
</p>
</td></tr> 
<tr><td><code id="sf_+3A_reltol">reltol</code></td>
<td>

<p>numeric. One of convergence criteria. Not used for now.
</p>
</td></tr>
<tr><td><code id="sf_+3A_lmtol">lmtol</code></td>
<td>

<p>numeric. Tolerance for the scaled gradient in ML optimization. Default is sqrt(.Machine$double.eps).
</p>
</td></tr>
<tr><td><code id="sf_+3A_digits">digits</code></td>
<td>

<p>numeric. Number of digits to be displayed in estimation results and for efficiency estimates. Default is 4.
</p>
</td></tr>
<tr><td><code id="sf_+3A_print.level">print.level</code></td>
<td>

<p>numeric. 1 - print estimation results. 2 - print optimization details. 3 - print summary of point estimates of technical or cost efficiencies. 7 - print unit-specific point and interval estimates of technical or cost efficiencies. Default is 4.
</p>
</td></tr>
<tr><td><code id="sf_+3A_seed">seed</code></td>
<td>

<p>set seed for replicability. Default is 17345168.
</p>
</td></tr>
<tr><td><code id="sf_+3A_only.data">only.data</code></td>
<td>

<p>logical. If <code>TRUE</code>, only data are returned. Default is <code>FALSE</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Models for <code>sf</code> are specified symbolically. A typical model has the form <code>y ~ x1 + ...</code>, where <code>y</code> represents the logarithm of outputs or total costs and <code>{x1,...}</code> is a series of inputs or outputs and input prices (in logs).  
</p>
<p>Options <code>ln.var.u.0i</code> and <code>ln.var.v.0i</code> can be used if multiplicative heteroskedasticity of either inefficiency or random noise component (or both) is assumed; i.e. if their variances can be expressed as exponential functions of (e.g. size-related) exogenous variables (including intercept) (see Caudill et al. 1995).
</p>
<p>If <code>marg.eff = TRUE</code> and <code>distribution = "h"</code>, the marginal effect of <em>k</em>th exogenous variable on the expected value of inefficiency term of unit <em>i</em> is computed as: <code class="reqn">\gamma[k]\sigma[i]/\sqrt2\pi</code>, where <code class="reqn">\sigma_u[i] = \sqrt exp(z[i]'\gamma)</code>. If <code>distribution = "t"</code>, marginal effects are returned if either <code>mean.u.0i</code> or <code>ln.var.u.0i</code> are not <code>NULL</code>. If the same exogenous variables are specified under both options, (non-monotonic) marginal effects are computed as explained in Wang (2002).
</p>
<p>See references and links below.
</p>


<h3>Value</h3>

<p><code>sf</code> returns a list of class <code>npsf</code> containing the following elements:
</p>
<table>
<tr><td><code>coef</code></td>
<td>

<p>numeric. Named vector of ML parameter estimates.
</p>
</td></tr>
<tr><td><code>vcov</code></td>
<td>

<p>matrix. Estimated covariance matrix of ML estimator. 
</p>
</td></tr>
<tr><td><code>ll</code></td>
<td>

<p>numeric. Value of log-likelihood at ML estimates.
</p>
</td></tr>
<tr><td><code>efficiencies</code></td>
<td>

<p>data frame. Contains point estimates of unit-specific technical or cost efficiencies:  exp(-E(u|e)) of Jondrow et al. (1982), E(exp(-u)|e) of Battese and Coelli (1988), and exp(-M(u|e)), where M(u|e) is the mode of conditional distribution of inefficiency term. In addition, estimated lower and upper bounds of (1-<code class="reqn">\alpha</code>)100% two-sided prediction intervals are returned. 
</p>
</td></tr>
<tr><td><code>marg.effects</code></td>
<td>

<p>data frame. Contains unit-specific marginal effects of exogenous variables on the expected value of inefficiency term. 
</p>
</td></tr>
<tr><td><code>sigmas_u</code></td>
<td>

<p>matrix. Estimated unit-specific variances of inefficiency term. Returned if <code>ln.var.u.0i</code> is not <code>NULL</code>.
</p>
</td></tr>
<tr><td><code>sigmas_v</code></td>
<td>

<p>matrix. Estimated unit-specific variances of random noise component. Returned if <code>ln.var.v.0i</code> is not <code>NULL</code>.
</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>

<p>matrix. Estimated unit-specific means of pre-truncated normal distribution of inefficiency term. Returned if <code>mean.u.0i</code> is not <code>NULL</code>.
</p>
</td></tr>
<tr><td><code>esample</code></td>
<td>

<p>logical. Returns TRUE if the observation in user supplied data is in the estimation subsample and FALSE otherwise.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Oleg Badunenko &lt;oleg.badunenko@brunel.ac.uk&gt;
</p>


<h3>References</h3>

<p>Badunenko, O. and Kumbhakar, S.C. (2016), When, Where and How to Estimate Persistent and Transient Efficiency in Stochastic Frontier Panel Data Models, <em>European Journal of Operational Research</em>, <b>255</b>(1), 272&ndash;287, doi: <a href="https://doi.org/10.1016/j.ejor.2016.04.049">10.1016/j.ejor.2016.04.049</a>.
</p>
<p>Battese, G., Coelli, T. (1988), Prediction of firm-level technical effiiencies with a generalized frontier production function and panel data. <em>Journal of Econometrics</em>, <b>38</b>, 387&ndash;399.
</p>
<p>Battese, G., Coelli, T. (1992), Frontier production functions, technical efficiency and panel data: With application to paddy farmers in India. <em>Journal of Productivity Analysis</em>, <b>3</b>, 153&ndash;169.
</p>
<p>Caudill, S., Ford, J., Gropper, D. (1995), Frontier estimation and firm-specific inefficiency measures in the presence of heteroscedasticity. <em>Journal of Business and Economic Statistics</em>, <b>13</b>, 105&ndash;111.
</p>
<p>Filippini, M. and Greene, W.H. (2016), Persistent and transient productive inefficiency: A maximum simulated likelihood approach. <em>Journal of Productivity Analysis</em>, <b>45</b> (2), 187&ndash;196.
</p>
<p>Horrace, W. and Schmidt, P. (1996), On ranking and selection from independent truncated normal distributions. <em>Journal of Productivity Analysis</em>, <b>7</b>, 257&ndash;282.
</p>
<p>Jondrow, J., Lovell, C., Materov, I., Schmidt, P. (1982), On estimation of technical inefficiency in the stochastic frontier production function model. <em>Journal of Econometrics</em>, <b>19</b>, 233&ndash;238.
</p>
<p>Kumbhakar, S. (1990),  Production Frontiers, Panel Data, and Time-varying Technical Inefficiency. <em>Journal of Econometrics</em>, <b>46</b>, 201&ndash;211.
</p>
<p>Kumbhakar, S. and Lovell, C. (2003), <em>Stochastic Frontier Analysis.</em> Cambridge: Cambridge University Press, doi: <a href="https://doi.org/10.1017/CBO9781139174411">10.1017/CBO9781139174411</a>.
</p>
<p>Wang, H.-J. (2002), Heteroskedasticity and non-monotonic efficiency effects of a stochastic frontier model. <em>Journal of Productivity Analysis</em>, <b>18</b>, 241&ndash;253.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+teradial">teradial</a></code>, <code><a href="#topic+tenonradial">tenonradial</a></code>, <code><a href="#topic+teradialbc">teradialbc</a></code>, <code><a href="#topic+tenonradialbc">tenonradialbc</a></code>, <code><a href="#topic+nptestrts">nptestrts</a></code>, <code><a href="#topic+nptestind">nptestind</a></code>, <code><a href="#topic+halton">halton</a></code>, <code><a href="#topic+primes">primes</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
require( npsf )

# Cross-sectional examples begin ------------------------------------------

# Load Penn World Tables 5.6 dataset

data( pwt56 )
head( pwt56 )

# Create some missing values

pwt56 [4, "K"] &lt;- NA 

# Stochastic production frontier model with 
# homoskedastic error components (half-normal)

# Use subset of observations - for year 1965

m1 &lt;- sf(log(Y) ~ log(L) + log(K), data = pwt56, 
         subset = year == 1965, distribution = "h")

m1 &lt;- sf(log(Y) ~ log(L) + log(K), data = pwt56, 
         subset = year == 1965, distribution = "e")

# Test CRS: 'car' package in required for that
## Not run: linearHypothesis(m1, "log(L) + log(K) = 1")

# Write efficiencies to the data frame using 'esample':

pwt56$BC[ m1$esample ] &lt;- m1$efficiencies$BC
## Not run: View(pwt56)

# Computation using matrices

Y1 &lt;- as.matrix(log(pwt56[pwt56$year == 1965, 
                          c("Y"), drop = FALSE]))
X1 &lt;- as.matrix(log(pwt56[pwt56$year == 1965,
                          c("K", "L"), drop = FALSE]))

X1 [51, 2] &lt;- NA # create missing
X1 [49, 1] &lt;- NA # create missing

m2 &lt;- sf(Y1 ~ X1, distribution = "h")

# Load U.S. commercial banks dataset

data(banks05)
head(banks05)

# Doubly heteroskedastic stochastic cost frontier 
# model (truncated normal)

# Print summaries of cost efficiencies' estimates

m3 &lt;- sf(lnC ~ lnw1 + lnw2 + lny1 + lny2, ln.var.u.0i = ~ ER, 
         ln.var.v.0i = ~ LA, data = banks05, distribution = "t", 
         prod = FALSE, print.level = 3)

m3 &lt;- sf(lnC ~ lnw1 + lnw2 + lny1 + lny2, ln.var.u.0i = ~ ER, 
         ln.var.v.0i = ~ LA, data = banks05, distribution = "e", 
         prod = FALSE, print.level = 3)
         
# Non-monotonic marginal effects of equity ratio on 
# the mean of distribution of inefficiency term

m4 &lt;- sf(lnC ~ lnw1 + lnw2 + lny1 + lny2, ln.var.u.0i = ~ ER,
         mean.u.0i = ~ ER, data = banks05, distribution = "t", 
         prod = FALSE, marg.eff = TRUE)

summary(m4$marg.effects)


# Cross-sectional examples end --------------------------------------------

## Not run: 

# Panel data examples begin -----------------------------------------------

# The only way to differentiate between cross-sectional and panel-data
# models is by specifying "it".
# If "it" is not specified, cross-sectional model will be estimated.
# Example is below.

# Read data ---------------------------------------------------------------

# Load U.S. commercial banks dataset

data(banks00_07)
head(banks00_07, 5)

banks00_07$trend &lt;- banks00_07$year - min(banks00_07$year) + 1

# Model specification -----------------------------------------------------

my.prod     &lt;- FALSE
my.it       &lt;- c("id","year")

# my.model = "BC1992"
# my.model = "K1990modified"
# my.model = "K1990"

# translog ----------------------------------------------------------------
formu &lt;- log(TC) ~ (log(Y1) + log(Y2) + log(W1) + log(W2) + trend)^2 +
 I(0.5*log(Y1)^2) + I(0.5*log(Y2)^2) + I(0.5*log(W1)^2) + I(0.5*log(W2)^2) +
 trend + I(0.5*trend^2)

# Cobb-Douglas ------------------------------------------------------------
# formu &lt;- log(TC) ~ log(Y1) + log(Y2) + log(W1) + log(W2) + trend + I(trend^2)

ols &lt;- lm(formu, data = banks00_07)
# just to mark the results of the OLS model
summary(ols)

# Components specifications -----------------------------------------------

ln.var.v.it &lt;- ~ log(TA)
ln.var.u.0i &lt;- ~ ER_ave
mean.u.0i_1 &lt;- ~ LLP_ave + LA_ave
mean.u.0i_2 &lt;- ~ LLP_ave + LA_ave - 1

# Suppose "it" is not specified
# Then it is a cross-sectional model

t0_1st_0 &lt;- sf(formu, data = banks00_07, subset = year &gt; 2000,
               prod = my.prod,
               ln.var.v.it = ln.var.v.it,
               ln.var.u.0i = ln.var.u.0i,
               eff.time.invariant = TRUE,
               mean.u.0i.zero = TRUE)

# 1st generation models ---------------------------------------------------

# normal-half normal ------------------------------------------------------

# the same as above but "it" is specified

t0_1st_0 &lt;- sf(formu, data = banks00_07, it = my.it, subset = year &gt; 2000,
               prod = my.prod,
               ln.var.v.it = ln.var.v.it,
               ln.var.u.0i = ln.var.u.0i, 
               eff.time.invariant = TRUE, 
               mean.u.0i.zero = TRUE)

# Note efficiencies are time-invariant

# confidence intervals for efficiencies -----------------------------------

head(t0_1st_0$efficiencies, 20)


# normal-truncated normal -------------------------------------------------

# truncation point is constant (for all ids) ------------------------------

t0_1st_1 &lt;- sf(formu, data = banks00_07, it = my.it, subset = year &gt; 2000,
               prod = my.prod,
               eff.time.invariant = TRUE,
               mean.u.0i.zero = FALSE,
               ln.var.v.it = ln.var.v.it,
               ln.var.u.0i = ln.var.u.0i,
               mean.u.0i = NULL,
               cost.eff.less.one = TRUE)



# truncation point is determined by variables -----------------------------

t0_1st_2 &lt;- sf(formu, data = banks00_07, it = my.it, subset = year &gt; 2000,
               prod = my.prod,
               eff.time.invariant = TRUE,
               mean.u.0i.zero = FALSE,
               mean.u.0i = mean.u.0i_1,
               ln.var.v.it = ln.var.v.it,
               ln.var.u.0i = ln.var.u.0i,
               cost.eff.less.one = TRUE)



# the same, but without intercept in mean.u.0i

t0_1st_3 &lt;- sf(formu, data = banks00_07, it = my.it, subset = year &gt; 2000,
               prod = my.prod,
               eff.time.invariant = TRUE,
               mean.u.0i.zero = FALSE,
               mean.u.0i = mean.u.0i_2,
               ln.var.v.it = ln.var.v.it,
               ln.var.u.0i = ln.var.u.0i,
               cost.eff.less.one = TRUE)

# 2nd generation models ---------------------------------------------------

# normal-half normal ------------------------------------------------------

# Kumbhakar (1990) model --------------------------------------------------

t_nhn_K1990 &lt;- sf(formu, data = banks00_07, it = my.it, subset = year &gt; 2000,
                  prod = my.prod,
                  eff.time.invariant = FALSE,
                  mean.u.0i.zero = TRUE, 
                  ln.var.v.it = ln.var.v.it,
                  ln.var.u.0i = ln.var.u.0i, 
                  cost.eff.less.one = FALSE)


# Kumbhakar (1990) modified model -----------------------------------------

t_nhn_K1990modified &lt;- sf(formu, data = banks00_07, it = my.it, subset = year &gt; 2000,
                          prod = my.prod, model = "K1990modified",
                          eff.time.invariant = FALSE,
                          mean.u.0i.zero = TRUE, 
                          ln.var.v.it = ln.var.v.it,
                          ln.var.u.0i = ln.var.u.0i, 
                          cost.eff.less.one = FALSE)


# Battese and Coelli (1992) model -----------------------------------------

t_nhn_BC1992 &lt;- sf(formu, data = banks00_07, it = my.it, subset = year &gt; 2000,
                   prod = my.prod, model = "BC1992",
                   eff.time.invariant = FALSE,
                   mean.u.0i.zero = TRUE, 
                   ln.var.v.it = ln.var.v.it,
                   ln.var.u.0i = ln.var.u.0i, 
                   cost.eff.less.one = FALSE)

# normal-truncated normal -------------------------------------------------

# Kumbhakar (1990) model --------------------------------------------------

# truncation point is constant (for all ids) ------------------------------

t_ntn_K1990_0 &lt;- sf(formu, data = banks00_07, it = my.it, subset = year &gt; 2000,
                    prod = my.prod,
                    eff.time.invariant = FALSE, 
                    mean.u.0i.zero = FALSE,
                    ln.var.v.it = ln.var.v.it, 
                    ln.var.u.0i = ln.var.u.0i,
                    cost.eff.less.one = FALSE)


# truncation point is determined by variables -----------------------------

t_ntn_K1990_1 &lt;- sf(formu, data = banks00_07, it = my.it, subset = year &gt; 2000,
                    prod = my.prod,
                    eff.time.invariant = FALSE, 
                    mean.u.0i.zero = FALSE, 
                    mean.u.0i = mean.u.0i_1,
                    ln.var.v.it = ln.var.v.it, 
                    ln.var.u.0i = ln.var.u.0i,
                    cost.eff.less.one = FALSE)

# Efficiencies are tiny, since empirically truncation points are quite big.
# Try withouth an intercept in conditional mean f-n

t_ntn_K1990_2 &lt;- sf(formu, data = banks00_07, it = my.it, subset = year &gt; 2000,
                    prod = my.prod,
                    eff.time.invariant = FALSE, 
                    mean.u.0i.zero = FALSE, 
                    mean.u.0i = mean.u.0i_2,
                    ln.var.v.it = ln.var.v.it, 
                    ln.var.u.0i = ln.var.u.0i,
                    cost.eff.less.one = FALSE)

# Kumbhakar (1990) modified model -----------------------------------------

t_ntn_K1990modified &lt;- sf(formu, data = banks00_07, it = my.it, subset = year &gt; 2000,
                          prod = my.prod, model = "K1990modified",
                          eff.time.invariant = FALSE, 
                          mean.u.0i.zero = FALSE, 
                          mean.u.0i = mean.u.0i_1,
                          ln.var.v.it = ln.var.v.it, 
                          ln.var.u.0i = ln.var.u.0i,
                          cost.eff.less.one = FALSE)

# Battese and Coelli (1992) model -----------------------------------------


t_ntn_BC1992 &lt;- sf(formu, data = banks00_07, it = my.it, subset = year &gt; 2000,
                   prod = my.prod, model = "BC1992",
                   eff.time.invariant = FALSE, 
                   mean.u.0i.zero = FALSE, 
                   mean.u.0i = mean.u.0i_1,
                   ln.var.v.it = ln.var.v.it, 
                   ln.var.u.0i = ln.var.u.0i,
                   cost.eff.less.one = FALSE)

# The next one (without "subset = year &gt; 2000" option) converges OK

t_ntn_BC1992 &lt;- sf(formu, data = banks00_07, it = my.it,
                   prod = my.prod, model = "BC1992",
                   eff.time.invariant = FALSE, 
                   mean.u.0i.zero = FALSE, 
                   mean.u.0i = mean.u.0i_1,
                   ln.var.v.it = ln.var.v.it, 
                   ln.var.u.0i = ln.var.u.0i,
                   cost.eff.less.one = FALSE)

# 4 component model ------------------------------------------------------

# Note, R should better be more than 200, this is just for illustration.
# This is the model that takes long to be estimated.  
# For the following example, 'mlmaximize' required 357 iterations and
# took 8 minutes.  
# The time will increase with more data and more parameters.

formu &lt;- log(TC) ~ log(Y1) + log(Y2) + log(W1) + log(W2) + trend

t_4comp &lt;- sf(formu, data = banks00_07, it = my.it, 
              subset = year &gt;= 2001 &amp; year &lt; 2006,
              prod = my.prod, model = "4comp",
              R = 500, initialize.halton = TRUE, 
              lmtol = 1e-5, maxit = 500, print.level = 4)

# With R = 500, 'mlmaximize' required 124 iterations and
# took 7 minutes.  
# The time will increase with more data and more parameters.

formu &lt;- log(TC) ~ log(Y1) + log(Y2) + log(W1) + log(W2) + trend

t_4comp_500 &lt;- sf(formu, data = banks00_07, it = my.it, 
              subset = year &gt;= 2001 &amp; year &lt; 2006,
              prod = my.prod, model = "4comp",
              R = 500, initialize.halton = TRUE, 
              lmtol = 1e-5, maxit = 500, print.level = 4)
              
# @e_i0, @e_it, and @e_over give efficiencies, 
# where @ is either 'c' or 't' for cost or production function.
# e.g., t_ntn_4comp$ce_i0 from last model, gives persistent cost efficiencies

# Panel data examples end -------------------------------------------------


## End(Not run)

</code></pre>

<hr>
<h2 id='summary.npsf'>
'summary' method for class 'npsf'
</h2><span id='topic+summary.npsf'></span><span id='topic+print.summary.npsf'></span>

<h3>Description</h3>

<p>Prints summary of SF or DEA model estimated by <code><a href="#topic+sf">sf</a></code>, 
<code><a href="#topic+teradial">teradial</a></code>, <code><a href="#topic+tenonradial">tenonradial</a></code>, and 
<code><a href="#topic+teradialbc">teradialbc</a></code>, or 
testing procedures <code><a href="#topic+nptestrts">nptestrts</a></code> and <code><a href="#topic+nptestind">nptestind</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> ## S3 method for class 'npsf'
summary( object, ... )
 ## S3 method for class 'summary.npsf'
print( x, digits = NULL, print.level = NULL, ... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.npsf_+3A_object">object</code></td>
<td>

<p>an object of class <code>npsf</code> returned by one of the functions 
<code><a href="#topic+sf">sf</a></code>, <code><a href="#topic+teradial">teradial</a></code>, <code><a href="#topic+tenonradial">tenonradial</a></code>, 
<code><a href="#topic+teradialbc">teradialbc</a></code>, <code><a href="#topic+nptestrts">nptestrts</a></code> or <code><a href="#topic+nptestind">nptestind</a></code>.
</p>
</td></tr>
<tr><td><code id="summary.npsf_+3A_x">x</code></td>
<td>

<p>an object of class <code>npsf</code> returned by one of the functions 
<code><a href="#topic+sf">sf</a></code>, <code><a href="#topic+teradial">teradial</a></code>, <code><a href="#topic+tenonradial">tenonradial</a></code>, 
<code><a href="#topic+teradialbc">teradialbc</a></code>, <code><a href="#topic+nptestrts">nptestrts</a></code> or <code><a href="#topic+nptestind">nptestind</a></code>.
</p>
</td></tr>
<tr><td><code id="summary.npsf_+3A_digits">digits</code></td>
<td>

<p>numeric. Number of digits to be displayed in estimation results and for efficiency estimates. Default is 4.
</p>
</td></tr>
<tr><td><code id="summary.npsf_+3A_print.level">print.level</code></td>
<td>

<p>numeric. 0 - nothing is printed; 1 - print summary of the model and data. 2 - print summary of technical efficiency measures. 3 - print estimation results observation by observation (for DEA models). Default is 1.
</p>
</td></tr>
<tr><td><code id="summary.npsf_+3A_...">...</code></td>
<td>

<p>currently unused.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The summary depends on the model or testing procedure that is being estimated
</p>


<h3>Value</h3>

<p>Currently no value is returned
</p>


<h3>Author(s)</h3>

<p>Oleg Badunenko &lt;oleg.badunenko@brunel.ac.uk&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sf">sf</a></code>, <code><a href="#topic+teradial">teradial</a></code>, <code><a href="#topic+tenonradial">tenonradial</a></code>, 
<code><a href="#topic+teradialbc">teradialbc</a></code>, <code><a href="#topic+tenonradialbc">tenonradialbc</a></code>, <code><a href="#topic+nptestrts">nptestrts</a></code>, and <code><a href="#topic+nptestind">nptestind</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require( npsf )
 
# Load Penn World Tables 5.6 dataset
 
data( pwt56 )
 
# Stochastic production frontier model with 
# homoskedastic error components (half-normal)
 
# Use subset of observations - for year 1965

# DEA

t1 &lt;- teradialbc ( Y ~ K + L, data = pwt56, subset = Nu &lt; 10, 
 reps = 199, print.level = 0)
summary(t1)

# SFA
 
m1 &lt;- sf(log(Y) ~ log(L) + log(K), data = pwt56, 
 subset = year == 1965, distribution = "h",
 print.level = 0)
summary( m1 )

# Load U.S. commercial banks dataset
 
data(banks05)

m3 &lt;- sf(lnC ~ lnw1 + lnw2 + lny1 + lny2, ln.var.u.0i = ~ ER, 
         ln.var.v.0i = ~ LA, data = banks05, distribution = "t", 
         prod = FALSE, print.level = 3)
 
summary(m3)

</code></pre>

<hr>
<h2 id='tenonradial'>
Nonradial Measure of Technical Efficiency, the Russell Measure
</h2><span id='topic+tenonradial'></span>

<h3>Description</h3>

<p>Routine <code>tenonradial</code> uses reduced linear programming to compute the nonradial output- or input-based measure of technical efficiency, which is known as the Russell measure. In input-based nonradial efficiency measurement, this measure allows for non-proportional/different reductions in each positive input, and this is what permits it to shrink an input vector all the way back to the efficient subset. In output-based nonradial efficiency measurement, the Russell measure allows for non-proportional/different expansions of each positive output. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tenonradial(formula, data, subset,
 rts = c("C", "NI", "V"), 
 base = c("output", "input"), 
 ref = NULL, data.ref = NULL, subset.ref = NULL,
 full.solution = TRUE,
 print.level = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tenonradial_+3A_formula">formula</code></td>
<td>

<p>an object of class &ldquo;formula&rdquo; (or one that can be coerced to that class): a symbolic description of the model. The details of model specification are given under &lsquo;Details&rsquo;.
</p>
</td></tr>
<tr><td><code id="tenonradial_+3A_data">data</code></td>
<td>

<p>an optional data frame containing the variables in the model. If not found in data, the variables are taken from environment (<code>formula</code>), typically the environment from which <code>tenonradial</code> is called.
</p>
</td></tr>
<tr><td><code id="tenonradial_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations for which technical efficiency is to be computed.
</p>
</td></tr>
<tr><td><code id="tenonradial_+3A_rts">rts</code></td>
<td>

<p>character or numeric. string: first letter of the word &ldquo;c&rdquo; for constant, &ldquo;n&rdquo; for non-increasing, or &ldquo;v&rdquo; for variable returns to scale assumption. numeric: 3 for constant, 2 for non-increasing, or 1 for variable returns to scale assumption.  
</p>
</td></tr>
<tr><td><code id="tenonradial_+3A_base">base</code></td>
<td>

<p>character or numeric. string: first letter of the word &ldquo;o&rdquo; for computing output-based or &ldquo;i&rdquo; for computing input-based technical efficiency measure. string: 2 for computing output-based or 1 for computing input-based technical efficiency measure
</p>
</td></tr>
<tr><td><code id="tenonradial_+3A_ref">ref</code></td>
<td>

<p>an object of class &ldquo;formula&rdquo; (or one that can be coerced to that class): a symbolic description of inputs and outputs that are used to define the technology reference set. The details of technology reference set specification are given under &lsquo;Details&rsquo;. If reference is not provided, the technical efficiency measures for data points are computed relative to technology based on data points themselves.
</p>
</td></tr>
<tr><td><code id="tenonradial_+3A_data.ref">data.ref</code></td>
<td>

<p>an optional data frame containing the variables in the technology reference set. If not found in <code>data.ref</code>, the variables are taken from environment(ref), typically the environment from which <code>tenonradial</code> is called.
</p>
</td></tr>
<tr><td><code id="tenonradial_+3A_subset.ref">subset.ref</code></td>
<td>

<p>an optional vector specifying a subset of observations to define the technology reference set.
</p>
</td></tr>
<tr><td><code id="tenonradial_+3A_full.solution">full.solution</code></td>
<td>

<p>logical. The detailed solution is returned.  See <code>value</code> section.
</p>
</td></tr>
<tr><td><code id="tenonradial_+3A_print.level">print.level</code></td>
<td>

<p>numeric. 0 - nothing is printed; 1 - print summary of the model and data. 2 - print summary of technical efficiency measures. 3 - print estimation results observation by observation. Default is 1.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Routine <code>tenonradial</code> computes the nonradial output- or input-based measure of technical efficiency under assumption of constant, non-increasing, or variable returns to scale technology. The details of the estimator can be found e.g., in FÃ¤re, Grosskopf, and Lovell (1994) or Badunenko and Mozharovskyi (2020).
</p>
<p>Models for <code>tenonradial</code> are specified symbolically. A typical model has the form <code>outputs ~ inputs</code>, where <code>outputs</code> (<code>inputs</code>) is a series of (numeric) terms which specifies outputs (inputs). The same goes for reference set.  Refer to the examples.
</p>
<p>Results can be summarized using <code><a href="#topic+summary.npsf">summary.npsf</a></code>.
</p>


<h3>Value</h3>

<p><code>tenonradial</code> returns a list of class <code>npsf</code> containing the following elements:
</p>
<table>
<tr><td><code>model</code></td>
<td>

<p>string: model name.
</p>
</td></tr>
<tr><td><code>K</code></td>
<td>

<p>numeric: number of data points for which efficiency is estimated.
</p>
</td></tr>
<tr><td><code>M</code></td>
<td>

<p>numeric: number of outputs.
</p>
</td></tr>
<tr><td><code>N</code></td>
<td>

<p>numeric: number of inputs.
</p>
</td></tr>
<tr><td><code>Kref</code></td>
<td>

<p>numeric: number of data points in the reference.
</p>
</td></tr>
<tr><td><code>rts</code></td>
<td>

<p>string: RTS assumption.
</p>
</td></tr>
<tr><td><code>base</code></td>
<td>

<p>string: base for efficiency measurement.
</p>
</td></tr>
<tr><td><code>te</code></td>
<td>

<p>numeric: nonradial measure (Russell) of technical efficiency.
</p>
</td></tr>
<tr><td><code>te.detail</code></td>
<td>

<p>numeric: <code>K x ncol</code> matrix containing thetas or lambdas for <code>ncol</code> outputs (output-based) or inputs (input-based). <code>ncol</code> is <code>M</code> for output- and <code>N</code> for input-based efficiency measurement.
</p>
</td></tr>
<tr><td><code>intensity</code></td>
<td>

<p>numeric: <code>K x Kref</code> matrix containing the intensity variables <code>z</code>. These can be used to identify peers.
</p>
</td></tr>
<tr><td><code>esample</code></td>
<td>

<p>logical: returns TRUE if the observation in user supplied data is in the estimation subsample and FALSE otherwise.
</p>
</td></tr>
<tr><td><code>esample.ref</code></td>
<td>

<p>logical: returns TRUE if the observation in the user supplied reference is in the reference subsample and FALSE otherwise.
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>In case of one input (output), the input (output)-based Russell measure is equal to Debrue-Farrell (<code><a href="#topic+teradial">teradial</a></code>) measure of technical efficiency.
</p>
<p>Results can be summarized using <code><a href="#topic+summary.npsf">summary.npsf</a></code>.
</p>


<h3>Author(s)</h3>

<p>Oleg Badunenko &lt;oleg.badunenko@brunel.ac.uk&gt;, Pavlo Mozharovskyi &lt;pavlo.mozharovskyi@telecom-paris.fr&gt;
</p>


<h3>References</h3>

<p>Badunenko, O. and Mozharovskyi, P. (2016), Nonparametric Frontier Analysis using Stata, <em>Stata Journal</em>, <b>16</b>3, 550&ndash;89, doi: <a href="https://doi.org/10.1177/1536867X1601600302">10.1177/1536867X1601600302</a>
</p>
<p>Badunenko, O. and Mozharovskyi, P. (2020), Statistical inference for the Russell measure of technical efficiency, <em>Journal of the Operational Research Society</em>, <b>71</b>3, 517&ndash;527, doi: <a href="https://doi.org/10.1080/01605682.2019.1599778">10.1080/01605682.2019.1599778</a>
</p>
<p>FÃ¤re, R. and Lovell, C. A. K. (1978), Measuring the technical efficiency of production, <em>Journal of Economic Theory</em>, <b>19</b>, 150&ndash;162, doi: <a href="https://doi.org/10.1016/0022-0531(78)90060-1">10.1016/0022-0531(78)90060-1</a>
</p>
<p>FÃ¤re, R., Grosskopf, S. and Lovell, C. A. K. (1994), <em>Production Frontiers</em>, Cambridge U.K.: Cambridge University Press, doi: <a href="https://doi.org/10.1017/CBO9780511551710">10.1017/CBO9780511551710</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+teradial">teradial</a></code>, <code><a href="#topic+teradialbc">teradialbc</a></code>, <code><a href="#topic+tenonradialbc">tenonradialbc</a></code>, <code><a href="#topic+nptestrts">nptestrts</a></code>, <code><a href="#topic+nptestind">nptestind</a></code>, <code><a href="#topic+sf">sf</a></code>, <code><a href="#topic+summary.npsf">summary.npsf</a></code> for printing summary results
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
require( npsf )

# Prepare data and matrices

data( pwt56 )
head( pwt56 )

# Create some missing values

pwt56 [49, "K"] &lt;- NA # create missing

Y1 &lt;- as.matrix ( pwt56[ pwt56$year == 1965, c("Y"), drop = FALSE] )
X1 &lt;- as.matrix ( pwt56[ pwt56$year == 1965, c("K", "L"), drop = FALSE] )

X1 [51, 2] &lt;- NA # create missing
X1 [49, 1] &lt;- NA # create missing

data( ccr81 )
head( ccr81 )

# Create some missing values

ccr81 [64, "x4"] &lt;- NA # create missing
ccr81 [68, "y2"] &lt;- NA # create missing

Y2 &lt;- as.matrix( ccr81[ , c("y1", "y2", "y3"), drop = FALSE] )
X2 &lt;- as.matrix( ccr81[ , c("x1", "x2", "x3", "x4", "x5"), drop = FALSE] )

# Computing without reference set

# Using formula

# Country is a categorical variable, so nonradial gives error message

# t1 &lt;- tenonradial ( Country ~ K + L, data = pwt56 )

# for computing the efficiencies of countries in 1965 
# with technology reference set is defined by observations in 1965
# (that same sample of countries)

t2 &lt;- tenonradial ( Y ~ K + L, data = pwt56, rts = "v", 
base = "in", print.level = 2)

# Using a subset

t3 &lt;- tenonradial ( Y ~ K + L, data = pwt56, subset = year == 1965,
	rts = "VRS", base = "in", print.level = 3 )

t4 &lt;- tenonradial ( Y ~ K + L, data = pwt56, subset = Nu &lt; 10,
	rts = "vrs", base = "I" )

t5 &lt;- tenonradial ( Y ~ L, data = pwt56, subset = Nu &lt; 10, rts = "v" )

# Multiple outputs

t8 &lt;- tenonradial ( y1 + y2 + y3 ~ x1 + x2 + x3 + x4 + x5, data = ccr81,
	rts = "v", base = "i" )

# Using a subset

t7 &lt;- tenonradial ( y1 + y2 + y3 ~ x1 + x2 + x3 + x4 + x5, data = ccr81,
	subset = x5 != 22, rts = "n", base = "o" )

# Computation using matrices

t9 &lt;- tenonradial ( Y1 ~ X1, rts = "v", base = "i" )

# Define subsets on a fly

t10 &lt;- tenonradial ( Y1[-1,] ~ X1[-2,1] )
t11 &lt;- tenonradial ( Y1[-3,] ~ X1[-1,], rts = "v", base = "o" )

# Multiple outputs

t12 &lt;- tenonradial ( Y2 ~ X2 )
t13 &lt;- tenonradial ( Y2[-66,] ~ X2[-1, -c(1,3)] )


# Computing with reference set

# Using formula

# For computing the efficiencies of countries with order number
# less than 10 with technology reference set defined by countries
# with order number larger than 10 and smaller than 11 (in effect 
# no reference set, hence warning) type

t14 &lt;- tenonradial ( Y ~ K + L, data = pwt56, subset = Nu &lt; 10, 
	ref = Y ~ K + L, data.ref = pwt56,
	subset.ref = Nu &gt; 10 &amp; Nu &lt; 11 ) # warning

# For computing the efficiencies of countries with order number
# less than 10 with technology reference set defined by countries 
# with order number larger than 10 and smaller than 15 type

t15 &lt;- tenonradial ( Y ~ K + L, data = pwt56, subset = Nu &lt; 10, ref = Y ~ K + L, 
	data.ref = pwt56, subset.ref = Nu &gt; 10 &amp; Nu &lt; 15 )

# For computing the efficiencies of countries in 1965 
# with technology reference set is defined by observations in both
# 1965 and 1990 (all) type
	
t16 &lt;- tenonradial ( Y ~ K + L, data = pwt56, subset = year == 1965,
	rts = "v", base = "i", 
	ref = Y ~ K + L, data.ref = pwt56 )

# For computing the efficiencies of countries in 1990
# with technology reference set is defined by observations in 1965
# type

t17 &lt;- tenonradial ( Y ~ K + L, data = pwt56, subset = year == 1990, 
	ref = Y ~ K + L, data.ref = pwt56, subset.ref = year == 1965 )

# Using matrices

t18 &lt;- tenonradial ( Y1[-1,] ~ X1[-2,], ref = Y1[-2,] ~ X1[-1,] )

# error: not equal number of observations in outputs and inputs

# t19 &lt;- tenonradial ( Y1[-1,] ~ X1[-(1:2),], 
# ref = Y1[-2,] ~ X1[-1,1] )

# Combined formula and matrix

# error: not equal number of inputs in data and reference set

# t20 &lt;- tenonradial ( Y ~ K + L, data = pwt56, subset = Nu &lt; 10,
# ref = Y1[-2,] ~ X1[-1,1] )

t21 &lt;- tenonradial ( Y ~ K + L, data = pwt56, subset = Nu &lt; 10, 
	ref = Y1[-2,] ~ X1[-1,] )

## Not run: 

# Really large data-set

data(usmanuf)
head(usmanuf)

nrow(usmanuf)
table(usmanuf$year)

# This will take some time depending on computer power

t22 &lt;- tenonradial ( Y ~ K + L + M, data = usmanuf, 
	subset = year &gt;= 1995 &amp; year &lt;= 2000 ) 

# Summary

summary ( t22$te )

# Write efficiencies to the data frame:

usmanuf$te_nonrad_crs_out[ t22$esample ] &lt;- t22$te

head(usmanuf, 17)


## End(Not run)


</code></pre>

<hr>
<h2 id='tenonradialbc'>
Statistical Inference Regarding the Russell Measure of Technical Efficiency
</h2><span id='topic+tenonradialbc'></span>

<h3>Description</h3>

<p>Routine <code>tenonradialbc</code> performs bias correction of the nonradial Russell input- or output-based measure of technical efficiency, computes bias and constructs confidence intervals via bootstrapping techniques.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> tenonradialbc(formula, data, subset,
            ref = NULL, data.ref = NULL, subset.ref = NULL,
            rts = c("C", "NI", "V"), base = c("output", "input"),
            homogeneous = TRUE, smoothed = TRUE, kappa = NULL,
            reps = 999, level = 95,
            print.level = 1, show.progress = TRUE, seed = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tenonradialbc_+3A_formula">formula</code></td>
<td>

<p>an object of class &ldquo;formula&rdquo; (or one that can be coerced to that class): a symbolic description of the model. The details of model specification are given under &lsquo;Details&rsquo;.
</p>
</td></tr>
<tr><td><code id="tenonradialbc_+3A_data">data</code></td>
<td>

<p>an optional data frame containing the variables in the model. If not found in data, the variables are taken from environment (<code>formula</code>), typically the environment from which <code>teradial</code> is called.
</p>
</td></tr>
<tr><td><code id="tenonradialbc_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations for which technical efficiency is to be computed.
</p>
</td></tr>
<tr><td><code id="tenonradialbc_+3A_rts">rts</code></td>
<td>

<p>character or numeric. string: first letter of the word &ldquo;c&rdquo; for constant, &ldquo;n&rdquo; for non-increasing, or &ldquo;v&rdquo; for variable returns to scale assumption. numeric: 3 for constant, 2 for non-increasing, or 1 for variable returns to scale assumption. 
</p>
</td></tr>
<tr><td><code id="tenonradialbc_+3A_base">base</code></td>
<td>

<p>character or numeric. string: first letter of the word &ldquo;o&rdquo; for computing output-based or &ldquo;i&rdquo; for computing input-based technical efficiency measure. string: 2 for computing output-based or 1 for computing input-based technical efficiency measure
</p>
</td></tr>
<tr><td><code id="tenonradialbc_+3A_ref">ref</code></td>
<td>

<p>an object of class &ldquo;formula&rdquo; (or one that can be coerced to that class): a symbolic description of inputs and outputs that are used to define the technology reference set. The details of technology reference set specification are given under &lsquo;Details&rsquo;. If reference is not provided, the technical efficiency measures for data points are computed relative to technology based on data points themselves.
</p>
</td></tr>
<tr><td><code id="tenonradialbc_+3A_data.ref">data.ref</code></td>
<td>

<p>an optional data frame containing the variables in the technology reference set. If not found in <code>data.ref</code>, the variables are taken from environment(ref), typically the environment from which <code>teradial</code> is called.
</p>
</td></tr>
<tr><td><code id="tenonradialbc_+3A_subset.ref">subset.ref</code></td>
<td>

<p>an optional vector specifying a subset of observations to define the technology reference set.
</p>
</td></tr>
<tr><td><code id="tenonradialbc_+3A_smoothed">smoothed</code></td>
<td>

<p>logical. If TRUE, the reference set is bootstrapped with smoothing; if FALSE, the reference set is bootstrapped with subsampling.
</p>
</td></tr>
<tr><td><code id="tenonradialbc_+3A_homogeneous">homogeneous</code></td>
<td>

<p>logical. Relevant if <code>smoothed=TRUE</code>. If TRUE, the reference set is bootstrapped with homogeneous smoothing; if FALSE, the reference set is bootstrapped with heterogeneous subsampling.
</p>
</td></tr>
<tr><td><code id="tenonradialbc_+3A_kappa">kappa</code></td>
<td>

<p>relevant if <code>smoothed=TRUE</code>. 'kappa' sets the size of the subsample as K^kappa, where K is the number of data points in the original reference set. The default value is 0.7. 'kappa' may be between 0.5 and 1.
</p>
</td></tr>
<tr><td><code id="tenonradialbc_+3A_reps">reps</code></td>
<td>

<p>specifies the number of bootstrap replications to be performed.  The default is 999.  The minimum is 100.  Adequate estimates of confidence intervals using bias-corrected methods typically require 1,000 or more replications.
</p>
</td></tr>
<tr><td><code id="tenonradialbc_+3A_level">level</code></td>
<td>

<p>sets confidence level for confidence intervals; default is <code>level=95</code>.
</p>
</td></tr>
<tr><td><code id="tenonradialbc_+3A_show.progress">show.progress</code></td>
<td>

<p>logical. Relevant if <code>print.level&gt;=1</code>. If TRUE, progress of the bootstrap is displayed; if FALSE, display of the bootstrap progress is suppressed.
</p>
</td></tr>
<tr><td><code id="tenonradialbc_+3A_print.level">print.level</code></td>
<td>

<p>numeric. 0 - nothing is printed; 1 - print summary of the model and data. 2 - print summary of technical efficiency measures. 3 - print estimation results observation by observation. Default is 1.
</p>
</td></tr>
<tr><td><code id="tenonradialbc_+3A_seed">seed</code></td>
<td>

<p>numeric. The seed (for replication purposes).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Routine <code>tenonradialbc</code> performs bias correction of the nonradial Russell input- or output-based measure of technical efficiency, computes bias and constructs confidence intervals via bootstrapping techniques (see Badunenko and Mozharovskyi (2020), doi: <a href="https://doi.org/10.1080/01605682.2019.1599778">10.1080/01605682.2019.1599778</a>).
</p>
<p>Models for <code>tenonradialbc</code> are specified symbolically. A typical model has the form <code>outputs ~ inputs</code>, where <code>outputs</code> (<code>inputs</code>) is a series of (numeric) terms which specifies outputs (inputs). The same goes for reference set.  Refer to the examples.
</p>
<p>Results can be summarized using <code><a href="#topic+summary.npsf">summary.npsf</a></code>.
</p>


<h3>Value</h3>

<p><code>tenonradialbc</code> returns a list of class <code>npsf</code> containing the following elements:
</p>
<table>
<tr><td><code>K</code></td>
<td>

<p>numeric: number of data points.
</p>
</td></tr>
<tr><td><code>M</code></td>
<td>

<p>numeric: number of outputs.
</p>
</td></tr>
<tr><td><code>N</code></td>
<td>

<p>numeric: number of inputs.
</p>
</td></tr>
<tr><td><code>rts</code></td>
<td>

<p>string: RTS assumption.
</p>
</td></tr>
<tr><td><code>base</code></td>
<td>

<p>string: base for efficiency measurement.
</p>
</td></tr>
<tr><td><code>reps</code></td>
<td>

<p>numeric: number of bootstrap replications.
</p>
</td></tr>
<tr><td><code>level</code></td>
<td>

<p>numeric: confidence level for confidence intervals.
</p>
</td></tr>
<tr><td><code>te</code></td>
<td>

<p>numeric: radial measure (Russell) of technical efficiency.
</p>
</td></tr>
<tr><td><code>tebc</code></td>
<td>

<p>numeric: bias-corrected radial measures of technical efficiency.
</p>
</td></tr>
<tr><td><code>biasboot</code></td>
<td>

<p>numeric: bootstrap bias estimate for the original radial measures of technical efficiency.
</p>
</td></tr>
<tr><td><code>varboot</code></td>
<td>

<p>numeric: bootstrap variance estimate for the radial measures of technical efficiency.
</p>
</td></tr>
<tr><td><code>biassqvar</code></td>
<td>

<p>numeric:  one-third of the ratio  of bias squared to variance for radial measures of technical efficiency.
</p>
</td></tr>
<tr><td><code>realreps</code></td>
<td>

<p>numeric: actual number of replications used for statistical inference.
</p>
</td></tr>
<tr><td><code>telow</code></td>
<td>

<p>numeric: lower bound estimate for radial measures of technical efficiency.
</p>
</td></tr>
<tr><td><code>teupp</code></td>
<td>

<p>numeric: upper bound estimate for radial measures of technical efficiency.
</p>
</td></tr>
<tr><td><code>teboot</code></td>
<td>

<p>numeric: <code>reps x K</code> matrix containing bootstrapped measures of technical efficiency from each of <code>reps</code> bootstrap replications.
</p>
</td></tr>
<tr><td><code>esample</code></td>
<td>

<p>logical: returns TRUE if the observation in user supplied data is in the estimation subsample and FALSE otherwise.
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Before specifying option <code>homogeneous</code> it is advised to preform the test of independence (see <code><a href="#topic+nptestind">nptestind</a></code>). Routine <code><a href="#topic+nptestrts">nptestrts</a></code> may help deciding regarding option <code>rts</code>.
</p>
<p>Results can be summarized using <code><a href="#topic+summary.npsf">summary.npsf</a></code>.
</p>


<h3>Author(s)</h3>

<p>Oleg Badunenko &lt;oleg.badunenko@brunel.ac.uk&gt;, Pavlo Mozharovskyi &lt;pavlo.mozharovskyi@telecom-paris.fr&gt;
</p>


<h3>References</h3>

<p>Badunenko, O. and Mozharovskyi, P. (2020), Statistical inference for the Russell measure of technical efficiency, <em>Journal of the Operational Research Society</em>, <b>71</b>3, 517&ndash;527, doi: <a href="https://doi.org/10.1080/01605682.2019.1599778">10.1080/01605682.2019.1599778</a>
</p>
<p>FÃ¤re, R., Grosskopf, S. and Lovell, C. A. K. (1994), <em>Production Frontiers</em>, Cambridge U.K.: Cambridge University Press, doi: <a href="https://doi.org/10.1017/CBO9780511551710">10.1017/CBO9780511551710</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+teradial">teradial</a></code>, <code><a href="#topic+tenonradial">tenonradial</a></code>, <code><a href="#topic+teradialbc">teradialbc</a></code>, <code><a href="#topic+nptestrts">nptestrts</a></code>, <code><a href="#topic+nptestind">nptestind</a></code>, <code><a href="#topic+sf">sf</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
 ## Not run: 
  
  data( ccr81 )
  head( ccr81 )
  
  # Subsampling

  t9 &lt;- tenonradialbc(y1 + y2 + y3 ~ x1 + x2 + x3 + x4 + x5, data = ccr81,
                          ref = NULL, data.ref = NULL, subset.ref = NULL,
                          rts = "v", base = "i",
                          homogeneous = FALSE, smoothed = TRUE, kappa = .6,
                          reps = 999, level = 95,
                          print.level = 1, show.progress = TRUE, seed = NULL)
  # display the results

  cbind(te = t9$te, telow = t9$telow, tebc = t9$tebc, teupp = t9$teupp, 
        biasboot = t9$biasboot, varboot = t9$varboot, biassqvar = t9$biassqvar)
  
  # Smoothing

  t10 &lt;- tenonradialbc(y1 + y2 + y3 ~ x1 + x2 + x3 + x4 + x5, data = ccr81,
                          ref = NULL, data.ref = NULL, subset.ref = NULL,
                          rts = "v", base = "i",
                          homogeneous = TRUE, smoothed = TRUE, kappa = .6,
                          reps = 999, level = 95,
                          print.level = 1, show.progress = TRUE, seed = NULL)
  # display the results

  cbind(te = t10$te, telow = t10$telow, tebc = t10$tebc, teupp = t10$teupp, 
        biasboot = t10$biasboot, varboot = t10$varboot, biassqvar = t10$biassqvar)
  
 
## End(Not run)
 
</code></pre>

<hr>
<h2 id='teradial'>
Radial Measure of Technical Efficiency, the Debrue-Farrell Measure
</h2><span id='topic+teradial'></span>

<h3>Description</h3>

<p>Routine <code>teradial</code> computes radial Debrue-Farrell input- or output-based measure of efficiency via reduced linear programing. In input-based radial efficiency measurement, this measure allows for proportional reductions in each positive input, and this is what permits it to shrink an input vector all the way back to the efficient subset. In output-based radial efficiency measurement, the Debrue-Farrell measure allows for proportional expansions of each positive output. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>teradial(formula, data, subset,
 rts = c("C", "NI", "V"),
 base = c("output", "input"),
 ref = NULL, data.ref = NULL, subset.ref = NULL,
 intensity = FALSE,
 print.level = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="teradial_+3A_formula">formula</code></td>
<td>

<p>an object of class &ldquo;formula&rdquo; (or one that can be coerced to that class): a symbolic description of the model. The details of model specification are given under &lsquo;Details&rsquo;.
</p>
</td></tr>
<tr><td><code id="teradial_+3A_data">data</code></td>
<td>

<p>an optional data frame containing the variables in the model. If not found in data, the variables are taken from environment (<code>formula</code>), typically the environment from which <code>teradial</code> is called.
</p>
</td></tr>
<tr><td><code id="teradial_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations for which technical efficiency is to be computed.
</p>
</td></tr>
<tr><td><code id="teradial_+3A_rts">rts</code></td>
<td>

<p>character or numeric. string: first letter of the word &ldquo;c&rdquo; for constant, &ldquo;n&rdquo; for non-increasing, or &ldquo;v&rdquo; for variable returns to scale assumption. numeric: 3 for constant, 2 for non-increasing, or 1 for variable returns to scale assumption.
</p>
</td></tr>
<tr><td><code id="teradial_+3A_base">base</code></td>
<td>

<p>character or numeric. string: first letter of the word &ldquo;o&rdquo; for computing output-based or &ldquo;i&rdquo; for computing input-based technical efficiency measure. string: 2 for computing output-based or 1 for computing input-based technical efficiency measure
</p>
</td></tr>
<tr><td><code id="teradial_+3A_ref">ref</code></td>
<td>

<p>an object of class &ldquo;formula&rdquo; (or one that can be coerced to that class): a symbolic description of inputs and outputs that are used to define the technology reference set. The details of technology reference set specification are given under &lsquo;Details&rsquo;. If reference is not provided, the technical efficiency measures for data points are computed relative to technology based on data points themselves.
</p>
</td></tr>
<tr><td><code id="teradial_+3A_data.ref">data.ref</code></td>
<td>

<p>an optional data frame containing the variables in the technology reference set. If not found in <code>data.ref</code>, the variables are taken from environment(ref), typically the environment from which <code>teradial</code> is called.
</p>
</td></tr>
<tr><td><code id="teradial_+3A_subset.ref">subset.ref</code></td>
<td>

<p>an optional vector specifying a subset of observations to define the technology reference set.
</p>
</td></tr>
<tr><td><code id="teradial_+3A_intensity">intensity</code></td>
<td>

<p>logical.  If set to TRUE, the value <code>intensity</code> will contain <code>K x Kref</code> matrix with intensity variables, which can be used to for example identify the peers. Default is <code>FALSE</code> as the matris is potentially large.
</p>
</td></tr>
<tr><td><code id="teradial_+3A_print.level">print.level</code></td>
<td>

<p>numeric. 0 - nothing is printed; 1 - print summary of the model and data. 2 - print summary of technical efficiency measures. 3 - print estimation results observation by observation. Default is 1.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Routine <code>teradial</code> computes the radial output- or input-based measure of technical efficiency under assumption of constant, non-increasing, or variable returns to scale technology. The details of the estimator can be found e.g., in FÃ¤re, Grosskopf, and Lovell (1994, especially section 3.1 on p.62 fot input-based and section 4.1 on p.96 for output-based efficiency measurement) or Badunenko and Mozharovskyi (2016).
</p>
<p>Models for <code>teradial</code> are specified symbolically. A typical model has the form <code>outputs ~ inputs</code>, where <code>outputs</code> (<code>inputs</code>) is a series of (numeric) terms which specifies outputs (inputs). The same goes for reference set.  Refer to the examples.
</p>
<p>Results can be summarized using <code><a href="#topic+summary.npsf">summary.npsf</a></code>.
</p>


<h3>Value</h3>

<p><code>teradial</code> returns a list of class <code>npsf</code> containing the following elements:
</p>
<table>
<tr><td><code>K</code></td>
<td>

<p>numeric: number of data points.
</p>
</td></tr>
<tr><td><code>M</code></td>
<td>

<p>numeric: number of outputs.
</p>
</td></tr>
<tr><td><code>N</code></td>
<td>

<p>numeric: number of inputs.
</p>
</td></tr>
<tr><td><code>rts</code></td>
<td>

<p>string: RTS assumption.
</p>
</td></tr>
<tr><td><code>base</code></td>
<td>

<p>string: base for efficiency measurement.
</p>
</td></tr>
<tr><td><code>te</code></td>
<td>

<p>numeric: radial measure (Debrue-Farrell) of technical efficiency.
</p>
</td></tr>
<tr><td><code>intensity</code></td>
<td>

<p>numeric: if the option <code>intensity</code> is set to TRUE, the value <code>intensity</code> will contain <code>K x Kref</code> matrix with intensity variables, which can be used to for example identify the peers (see example with <code>t3</code> in the example section). Is <code>NULL</code> if option <code>intensity</code> is set to <code>FALSE</code>, which is a default.
</p>
</td></tr>
<tr><td><code>esample</code></td>
<td>

<p>logical: returns TRUE if the observation in user supplied data is in the estimation subsample and FALSE otherwise.
</p>
</td></tr>
<tr><td><code>esample.ref</code></td>
<td>

<p>logical: returns TRUE if the observation in the user supplied reference is in the reference subsample and FALSE otherwise.
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>In case of one input (output), the input (output)-based Debrue-Farrell measure is equal to  Russell  measure of technical efficiency (see <code><a href="#topic+tenonradial">tenonradial</a></code>).
</p>
<p>Results can be summarized using <code><a href="#topic+summary.npsf">summary.npsf</a></code>.
</p>


<h3>Author(s)</h3>

<p>Oleg Badunenko &lt;oleg.badunenko@brunel.ac.uk&gt;, Pavlo Mozharovskyi &lt;pavlo.mozharovskyi@telecom-paris.fr&gt;
</p>


<h3>References</h3>

<p>Badunenko, O. and Mozharovskyi, P. (2016), Nonparametric Frontier Analysis using Stata, <em>Stata Journal</em>, <b>16</b>3, 550&ndash;89, doi: <a href="https://doi.org/10.1177/1536867X1601600302">10.1177/1536867X1601600302</a>
</p>
<p>FÃ¤re, R. and Lovell, C. A. K. (1978), Measuring the technical efficiency of production, <em>Journal of Economic Theory</em>, <b>19</b>, 150&ndash;162, doi: <a href="https://doi.org/10.1016/0022-0531(78)90060-1">10.1016/0022-0531(78)90060-1</a>
</p>
<p>FÃ¤re, R., Grosskopf, S. and Lovell, C. A. K. (1994), <em>Production Frontiers</em>, Cambridge U.K.: Cambridge University Press, doi: <a href="https://doi.org/10.1017/CBO9780511551710">10.1017/CBO9780511551710</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tenonradial">tenonradial</a></code>, <code><a href="#topic+teradialbc">teradialbc</a></code>, <code><a href="#topic+tenonradialbc">tenonradialbc</a></code>, <code><a href="#topic+nptestrts">nptestrts</a></code>, <code><a href="#topic+nptestind">nptestind</a></code>, <code><a href="#topic+sf">sf</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
require( npsf )

# Prepare data and matrices

data( pwt56 )
head( pwt56 )

# Create some missing values

pwt56 [49, "K"] &lt;- NA # create missing

Y1 &lt;- as.matrix ( pwt56[ pwt56$year == 1965, c("Y"), drop = FALSE] )
X1 &lt;- as.matrix ( pwt56[ pwt56$year == 1965, c("K", "L"), drop = FALSE] )

X1 [51, 2] &lt;- NA # create missing
X1 [49, 1] &lt;- NA # create missing

data( ccr81 )
head( ccr81 )

# Create some missing values

ccr81 [64, "x4"] &lt;- NA # create missing
ccr81 [68, "y2"] &lt;- NA # create missing

Y2 &lt;- as.matrix( ccr81[ , c("y1", "y2", "y3"), drop = FALSE] )
X2 &lt;- as.matrix( ccr81[ , c("x1", "x2", "x3", "x4", "x5"), drop = FALSE] )

# Computing without reference set

# Using formula

# Country is a categorical variable, so nonradial gives error message

# t1 &lt;- teradial ( Country ~ K + L, data = pwt56 )

# for computing the efficiencies of countries in 1965 
# with technology reference set is defined by observations in 1965
# (that same sample of countries)

t2 &lt;- teradial ( Y ~ K + L, data = pwt56, rts = "v", 
base = "in", print.level = 2)

# Using a subset

t3 &lt;- teradial ( Y ~ K + L, data = pwt56, subset = year == 1965,
	rts = "VRS", base = "in", print.level = 3, intensity = TRUE )

# VRS constraint is satisfied, which is easy to varify 
# by checking the sums of intensity variables
rowSums(t3$intensity)

# to obtain peers create a list that will contain order numers of peers
t3.peers &lt;- list()
# now fill this list
for(i in seq.int(sum(t3$esample))){
  t3.peers[[i]] &lt;- which( t3$intensity[i,] != 0 )
}

t4 &lt;- teradial ( Y ~ K + L, data = pwt56, subset = Nu &lt; 10,
	rts = "vrs", base = "I" )

t5 &lt;- teradial ( Y ~ L, data = pwt56, subset = Nu &lt; 10, rts = "v" )

# Multiple outputs

t8 &lt;- teradial ( y1 + y2 + y3 ~ x1 + x2 + x3 + x4 + x5, data = ccr81,
	rts = "v", base = "i" )

# Using a subset

t7 &lt;- teradial ( y1 + y2 + y3 ~ x1 + x2 + x3 + x4 + x5, data = ccr81,
	subset = x5 != 22, rts = "n", base = "o" )

# Computation using matrices

t9 &lt;- teradial ( Y1 ~ X1, rts = "v", base = "i" )

# Define subsets on a fly

t10 &lt;- teradial ( Y1[-1,] ~ X1[-2,1] )
t11 &lt;- teradial ( Y1[-3,] ~ X1[-1,], rts = "v", base = "o" )

# Multiple outputs

t12 &lt;- teradial ( Y2 ~ X2 )
t13 &lt;- teradial ( Y2[-66,] ~ X2[-1, -c(1,3)] )


# Computing with reference set

# Using formula

# For computing the efficiencies of countries with order number
# less than 10 with technology reference set defined by countries
# with order number larger than 10 and smaller than 11 (in effect 
# no reference set, hence warning) type

t14 &lt;- teradial ( Y ~ K + L, data = pwt56, subset = Nu &lt; 10, 
	ref = Y ~ K + L, data.ref = pwt56,
	subset.ref = Nu &gt; 10 &amp; Nu &lt; 11 ) # warning

# For computing the efficiencies of countries with order number
# less than 10 with technology reference set defined by countries 
# with order number larger than 10 and smaller than 15 type

t15 &lt;- teradial ( Y ~ K + L, data = pwt56, subset = Nu &lt; 10, ref = Y ~ K + L, 
	data.ref = pwt56, subset.ref = Nu &gt; 10 &amp; Nu &lt; 15 )

# For computing the efficiencies of countries in 1965 
# with technology reference set is defined by observations in both
# 1965 and 1990 (all) type
	
t16 &lt;- teradial ( Y ~ K + L, data = pwt56, subset = year == 1965,
	rts = "v", base = "i", 
	ref = Y ~ K + L, data.ref = pwt56 )

# For computing the efficiencies of countries in 1990
# with technology reference set is defined by observations in 1965
# type

t17 &lt;- teradial ( Y ~ K + L, data = pwt56, subset = year == 1990, 
	ref = Y ~ K + L, data.ref = pwt56, subset.ref = year == 1965 )

# Using matrices

t18 &lt;- teradial ( Y1[-1,] ~ X1[-2,], ref = Y1[-2,] ~ X1[-1,] )

# error: not equal number of observations in outputs and inputs

# t19 &lt;- teradial ( Y1[-1,] ~ X1[-(1:2),], 
# ref = Y1[-2,] ~ X1[-1,1] )

# Combined formula and matrix

# error: not equal number of inputs in data and reference set

# t20 &lt;- teradial ( Y ~ K + L, data = pwt56, subset = Nu &lt; 10,
# ref = Y1[-2,] ~ X1[-1,1] )

t21 &lt;- teradial ( Y ~ K + L, data = pwt56, subset = Nu &lt; 10, 
	ref = Y1[-2,] ~ X1[-1,] )
	
## Not run: 

# Really large data-set

data(usmanuf)
head(usmanuf)

nrow(usmanuf)
table(usmanuf$year)

# This will take some time depending on computer power

t22 &lt;- teradial ( Y ~ K + L + M, data = usmanuf, 
	subset = year &gt;= 1995 &amp; year &lt;= 2000 ) 

# Summary

summary ( t22$te )

# Write efficiencies to the data frame:

usmanuf$te_nonrad_crs_out[ t22$esample ] &lt;- t22$te

head(usmanuf, 17)


## End(Not run)

</code></pre>

<hr>
<h2 id='teradialbc'>
Statistical Inference Regarding the Radial Measure of Technical Efficiency
</h2><span id='topic+teradialbc'></span>

<h3>Description</h3>

<p>Routine <code>teradialbc</code> performs bias correction of the radial Debrue-Farrell input- or output-based measure of technical efficiency, computes bias and constructs confidence intervals via bootstrapping techniques.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>teradialbc(formula, data, subset,
 ref = NULL, data.ref = NULL, subset.ref = NULL,
 rts = c("C", "NI", "V"), base = c("output", "input"),
 homogeneous = TRUE, smoothed = TRUE, kappa = NULL,
 reps = 999, level = 95,
 core.count = 1, cl.type = c("SOCK", "MPI"),
 print.level = 1, dots = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="teradialbc_+3A_formula">formula</code></td>
<td>

<p>an object of class &ldquo;formula&rdquo; (or one that can be coerced to that class): a symbolic description of the model. The details of model specification are given under &lsquo;Details&rsquo;.
</p>
</td></tr>
<tr><td><code id="teradialbc_+3A_data">data</code></td>
<td>

<p>an optional data frame containing the variables in the model. If not found in data, the variables are taken from environment (<code>formula</code>), typically the environment from which <code>teradial</code> is called.
</p>
</td></tr>
<tr><td><code id="teradialbc_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations for which technical efficiency is to be computed.
</p>
</td></tr>
<tr><td><code id="teradialbc_+3A_rts">rts</code></td>
<td>

<p>character or numeric. string: first letter of the word &ldquo;c&rdquo; for constant, &ldquo;n&rdquo; for non-increasing, or &ldquo;v&rdquo; for variable returns to scale assumption. numeric: 3 for constant, 2 for non-increasing, or 1 for variable returns to scale assumption. 
</p>
</td></tr>
<tr><td><code id="teradialbc_+3A_base">base</code></td>
<td>

<p>character or numeric. string: first letter of the word &ldquo;o&rdquo; for computing output-based or &ldquo;i&rdquo; for computing input-based technical efficiency measure. string: 2 for computing output-based or 1 for computing input-based technical efficiency measure
</p>
</td></tr>
<tr><td><code id="teradialbc_+3A_ref">ref</code></td>
<td>

<p>an object of class &ldquo;formula&rdquo; (or one that can be coerced to that class): a symbolic description of inputs and outputs that are used to define the technology reference set. The details of technology reference set specification are given under &lsquo;Details&rsquo;. If reference is not provided, the technical efficiency measures for data points are computed relative to technology based on data points themselves.
</p>
</td></tr>
<tr><td><code id="teradialbc_+3A_data.ref">data.ref</code></td>
<td>

<p>an optional data frame containing the variables in the technology reference set. If not found in <code>data.ref</code>, the variables are taken from environment(ref), typically the environment from which <code>teradial</code> is called.
</p>
</td></tr>
<tr><td><code id="teradialbc_+3A_subset.ref">subset.ref</code></td>
<td>

<p>an optional vector specifying a subset of observations to define the technology reference set.
</p>
</td></tr>
<tr><td><code id="teradialbc_+3A_smoothed">smoothed</code></td>
<td>

<p>logical. If TRUE, the reference set is bootstrapped with smoothing; if FALSE, the reference set is bootstrapped with subsampling.
</p>
</td></tr>
<tr><td><code id="teradialbc_+3A_homogeneous">homogeneous</code></td>
<td>

<p>logical. Relevant if <code>smoothed=TRUE</code>. If TRUE, the reference set is bootstrapped with homogeneous smoothing; if FALSE, the reference set is bootstrapped with heterogeneous smoothing.
</p>
</td></tr>
<tr><td><code id="teradialbc_+3A_kappa">kappa</code></td>
<td>

<p>relevant if <code>smoothed=TRUE</code>. 'kappa' sets the size of the subsample as K^kappa, where K is the number of data points in the original reference set. The default value is 0.7. 'kappa' may be between 0.5 and 1.
</p>
</td></tr>
<tr><td><code id="teradialbc_+3A_reps">reps</code></td>
<td>

<p>specifies the number of bootstrap replications to be performed.  The default is 999.  The minimum is 100.  Adequate estimates of confidence intervals using bias-corrected methods typically require 1,000 or more replications.
</p>
</td></tr>
<tr><td><code id="teradialbc_+3A_level">level</code></td>
<td>

<p>sets confidence level for confidence intervals; default is <code>level = 95</code>.
</p>
</td></tr>
<tr><td><code id="teradialbc_+3A_core.count">core.count</code></td>
<td>

<p>positive integer. Number of cluster nodes. If <code>core.count=1</code>, the process runs sequentially. See  <code>performParallel</code> in package <code>snowFT</code> for more details.
</p>
</td></tr>
<tr><td><code id="teradialbc_+3A_cl.type">cl.type</code></td>
<td>

<p>Character string that specifies cluster type (see <code>makeClusterFT</code> in package <code>snowFT</code>). Possible values are 'MPI' and 'SOCK' ('PVM' is currently not available). See <code>performParallel</code> in package <code>snowFT</code> for more details.
</p>
</td></tr>
<tr><td><code id="teradialbc_+3A_dots">dots</code></td>
<td>

<p>logical. Relevant if <code>print.level&gt;=1</code>. If TRUE, one dot character is displayed for each successful replication; if FALSE,  display of the replication dots is suppressed.
</p>
</td></tr>
<tr><td><code id="teradialbc_+3A_print.level">print.level</code></td>
<td>

<p>numeric. 0 - nothing is printed; 1 - print summary of the model and data. 2 - print summary of technical efficiency measures. 3 - print estimation results observation by observation. Default is 1.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Routine <code>teradialbc</code> performs bias correction of the radial Debrue-Farrell input- or output-based measure of technical efficiency, computes bias and constructs confidence intervals via bootstrapping techniques. See Simar and Wilson (1998) doi: <a href="https://doi.org/10.1287/mnsc.44.1.49">10.1287/mnsc.44.1.49</a>, Simar and Wilson (2000) doi: <a href="https://doi.org/10.1080/02664760050081951">10.1080/02664760050081951</a>, Kneip, Simar, and Wilson (2008) doi: <a href="https://doi.org/10.1017/S0266466608080651">10.1017/S0266466608080651</a>, and references with links below.
</p>
<p>Models for <code>teradialbc</code> are specified symbolically. A typical model has the form <code>outputs ~ inputs</code>, where <code>outputs</code> (<code>inputs</code>) is a series of (numeric) terms which specifies outputs (inputs). The same goes for reference set.  Refer to the examples.
</p>
<p>If <code>core.count&gt;=1</code>, <code>teradialbc</code> will perform bootstrap on multiple cores.  Parallel computing requires package <code>snowFT</code>. By the default cluster type is defined by option <code>cl.type="SOCK"</code>.  Specifying <code>cl.type="MPI"</code> requires package <code>Rmpi</code>.
</p>
<p>On some systems, specifying option <code>cl.type="SOCK"</code> results in much quicker execution than specifying option <code>cl.type="MPI"</code>.  Option <code>cl.type="SOCK"</code> might be problematic on Mac system.
</p>
<p>Parallel computing make a difference for large data sets.  Specifying option <code>dots=TRUE</code> will indicate at what speed the bootstrap actually proceeds.  Specify <code>reps=100</code> and compare two runs with option <code>core.count=1</code> and <code>core.count&gt;1</code> to see if parallel computing speeds up the bootstrap.  For small samples, parallel computing may actually slow down the <code>teradialbc</code>.
</p>
<p>Results can be summarized using <code><a href="#topic+summary.npsf">summary.npsf</a></code>.
</p>


<h3>Value</h3>

<p><code>teradialbc</code> returns a list of class <code>npsf</code> containing the following elements:
</p>
<table>
<tr><td><code>K</code></td>
<td>

<p>numeric: number of data points.
</p>
</td></tr>
<tr><td><code>M</code></td>
<td>

<p>numeric: number of outputs.
</p>
</td></tr>
<tr><td><code>N</code></td>
<td>

<p>numeric: number of inputs.
</p>
</td></tr>
<tr><td><code>rts</code></td>
<td>

<p>string: RTS assumption.
</p>
</td></tr>
<tr><td><code>base</code></td>
<td>

<p>string: base for efficiency measurement.
</p>
</td></tr>
<tr><td><code>reps</code></td>
<td>

<p>numeric: number of bootstrap replications.
</p>
</td></tr>
<tr><td><code>level</code></td>
<td>

<p>numeric: confidence level for confidence intervals.
</p>
</td></tr>
<tr><td><code>te</code></td>
<td>

<p>numeric: radial measure (Russell) of technical efficiency.
</p>
</td></tr>
<tr><td><code>tebc</code></td>
<td>

<p>numeric: bias-corrected radial measures of technical efficiency.
</p>
</td></tr>
<tr><td><code>biasboot</code></td>
<td>

<p>numeric: bootstrap bias estimate for the original radial measures of technical efficiency.
</p>
</td></tr>
<tr><td><code>varboot</code></td>
<td>

<p>numeric: bootstrap variance estimate for the radial measures of technical efficiency.
</p>
</td></tr>
<tr><td><code>biassqvar</code></td>
<td>

<p>numeric:  one-third of the ratio  of bias squared to variance for radial measures of technical efficiency.
</p>
</td></tr>
<tr><td><code>realreps</code></td>
<td>

<p>numeric: actual number of replications used for statistical inference.
</p>
</td></tr>
<tr><td><code>telow</code></td>
<td>

<p>numeric: lower bound estimate for radial measures of technical efficiency.
</p>
</td></tr>
<tr><td><code>teupp</code></td>
<td>

<p>numeric: upper bound estimate for radial measures of technical efficiency.
</p>
</td></tr>
<tr><td><code>teboot</code></td>
<td>

<p>numeric: <code>reps x K</code> matrix containing bootstrapped measures of technical efficiency from each of <code>reps</code> bootstrap replications.
</p>
</td></tr>
<tr><td><code>esample</code></td>
<td>

<p>logical: returns TRUE if the observation in user supplied data is in the estimation subsample and FALSE otherwise.
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Before specifying option <code>homogeneous</code> it is advised to preform the test of independence (see <code><a href="#topic+nptestind">nptestind</a></code>). Routine <code><a href="#topic+nptestrts">nptestrts</a></code> may help deciding regarding option <code>rts</code>.
</p>
<p>Results can be summarized using <code><a href="#topic+summary.npsf">summary.npsf</a></code>.
</p>


<h3>Author(s)</h3>

<p>Oleg Badunenko &lt;oleg.badunenko@brunel.ac.uk&gt;, Pavlo Mozharovskyi &lt;pavlo.mozharovskyi@telecom-paris.fr&gt;
</p>


<h3>References</h3>

<p>Badunenko, O. and Mozharovskyi, P. (2016), Nonparametric Frontier Analysis using Stata, <em>Stata Journal</em>, <b>16</b>3, 550&ndash;89, doi: <a href="https://doi.org/10.1177/1536867X1601600302">10.1177/1536867X1601600302</a>
</p>
<p>FÃ¤re, R. and Lovell, C. A. K. (1978), Measuring the technical efficiency of production, <em>Journal of Economic Theory</em>, <b>19</b>, 150&ndash;162, doi: <a href="https://doi.org/10.1016/0022-0531(78)90060-1">10.1016/0022-0531(78)90060-1</a>
</p>
<p>FÃ¤re, R., Grosskopf, S. and Lovell, C. A. K. (1994), <em>Production Frontiers</em>, Cambridge U.K.: Cambridge University Press, doi: <a href="https://doi.org/10.1017/CBO9780511551710">10.1017/CBO9780511551710</a>
</p>
<p>Kneip, A., Simar L., and P.W. Wilson (2008), Asymptotics and Consistent Bootstraps for DEA Estimators in Nonparametric Frontier Models, <em>Econometric Theory</em>, <b>24</b>, 1663&ndash;1697, doi: <a href="https://doi.org/10.1017/S0266466608080651">10.1017/S0266466608080651</a>
</p>
<p>Simar, L. and P.W. Wilson (1998), Sensitivity Analysis of Efficiency Scores: How to Bootstrap in Nonparametric Frontier Models, <em>Management Science</em>, <b>44</b>, 49&ndash;61, doi: <a href="https://doi.org/10.1287/mnsc.44.1.49">10.1287/mnsc.44.1.49</a>
</p>
<p>Simar, L. and P.W. Wilson (2000), A General Methodology for Bootstrapping in Nonparametric Frontier Models, <em>Journal of Applied Statistics</em>, <b>27</b>, 779&ndash;802, doi: <a href="https://doi.org/10.1080/02664760050081951">10.1080/02664760050081951</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+teradial">teradial</a></code>, <code><a href="#topic+tenonradial">tenonradial</a></code>, <code><a href="#topic+tenonradialbc">tenonradialbc</a></code>, <code><a href="#topic+nptestrts">nptestrts</a></code>, <code><a href="#topic+nptestind">nptestind</a></code>, <code><a href="#topic+sf">sf</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

require( npsf )

# Prepare data and matrices

data( pwt56 )
head( pwt56 )

# Create some missing values

pwt56 [49, "K"] &lt;- NA # just to create missing

Y1 &lt;- as.matrix ( pwt56[ pwt56$year == 1965, c("Y"), drop = FALSE] )
X1 &lt;- as.matrix ( pwt56[ pwt56$year == 1965, c("K", "L"), drop = FALSE] )

X1 [51, 2] &lt;- NA # just to create missing
X1 [49, 1] &lt;- NA # just to create missing

data( ccr81 )
head( ccr81 )

# Create some missing values

ccr81 [64, "x4"] &lt;- NA # just to create missing
ccr81 [68, "y2"] &lt;- NA # just to create missing

Y2 &lt;- as.matrix( ccr81[ , c("y1", "y2", "y3"), drop = FALSE] )
X2 &lt;- as.matrix( ccr81[ , c("x1", "x2", "x3", "x4", "x5"), drop = FALSE] )

# Compute output-based measures of technical efficiency under 
# the assumption of CRS (the default) and perform bias-correctiion
# using smoothed homogeneous bootstrap (the default) with 999
# replications (the default).

t1 &lt;- teradialbc ( y1 + y2 + y3 ~ x1 + x2 + x3 + x4 + x5, 
	data = ccr81)

# or just

t2 &lt;- teradialbc ( Y2 ~ X2)

# Combined formula and matrix

t3 &lt;- teradialbc ( Y ~ K + L, data = pwt56, subset = Nu &lt; 10, 
	ref = Y1[-2,] ~ X1[-1,] )

# Compute input-based measures of technical efficiency under 
# the assumption of VRS and perform bias-correctiion using
# subsampling heterogenous bootstrap with 1999 replications.
# Choose to report 99
# formed by data points where x5 is not equal 10. 
# Suppress printing dots.

t4 &lt;- teradialbc ( y1 + y2 + y3 ~ x1 + x2 + x3 + x4 + x5, 
	data = ccr81, ref = y1 + y2 + y3 ~ x1 + x2 + x3 + x4 + x5, 
	subset.ref = x5 != 10, data.ref = ccr81, reps = 1999, 
	smoothed = FALSE, kappa = 0.7, dots = FALSE, 
	base = "i", rts = "v", level = 99)

# Compute input-based measures of technical efficiency under
# the assumption of NRS and perform bias-correctiion using 
# smoothed heterogenous bootstrap with 499 replications for 
# all data points. The reference set formed by data points 
# where x5 is not equal 10.

t5 &lt;- teradialbc ( y1 + y2 + y3 ~ x1 + x2 + x3 + x4 + x5, 
	data = ccr81, ref = y1 + y2 + y3 ~ x1 + x2 + x3 + x4 + x5, 
	subset.ref = x5 != 10, data.ref = ccr81, homogeneous = FALSE, 
	reps = 999, smoothed = TRUE, dots = TRUE, base = "i", rts = "n")


# ===========================
# ===  Parallel computing ===
# ===========================

# Perform previous bias-correction but use 8 cores and 
# cluster type SOCK

t51 &lt;-  teradialbc ( y1 + y2 + y3 ~ x1 + x2 + x3 + x4 + x5, 
	data = ccr81, ref = y1 + y2 + y3 ~ x1 + x2 + x3 + x4 + x5, 
	subset.ref = x5 != 10, data.ref = ccr81, homogeneous = FALSE, 
	reps = 999, smoothed = TRUE, dots = TRUE, base = "i", rts = "n", 
	core.count = 8, cl.type = "SOCK")


# Really large data-set

data(usmanuf)
head(usmanuf)

nrow(usmanuf)
table(usmanuf$year)

# This will take some time depending on computer power

data(usmanuf)
head(usmanuf)

t6 &lt;- teradialbc ( Y ~ K + L + M, data = usmanuf, 
	subset = year &gt;= 1999 &amp; year &lt;= 2000, homogeneous = FALSE, 
	base = "o", reps = 100, 
	core.count = 8, cl.type = "SOCK")


## End(Not run)

</code></pre>

<hr>
<h2 id='truncreg'>
Parametric truncated regression for cross-sectional data
</h2><span id='topic+truncreg'></span>

<h3>Description</h3>

<p><code>truncreg</code> performs maximum likelihood estimation of the parameters in cross-sectional truncated regression. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>truncreg(formula, data, subset, 
 ll = -Inf, ul = Inf, 
 lmtol = .Machine$double.eps, maxiter = 150, 
 marg.eff = FALSE, 
 print.level = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="truncreg_+3A_formula">formula</code></td>
<td>

<p>an object of class &ldquo;formula&rdquo; (or one that can be coerced to that class): a symbolic description of the model. The details of model specification are given under &lsquo;Details&rsquo;.
</p>
</td></tr>
<tr><td><code id="truncreg_+3A_data">data</code></td>
<td>

<p>an optional data frame containing variables in the model. If not found in data, the variables are taken from environment (<code>formula</code>), typically the environment from which <code>sf</code> is called.
</p>
</td></tr>
<tr><td><code id="truncreg_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations for which technical or cost efficiencies are to be computed.
</p>
</td></tr>
<tr><td><code id="truncreg_+3A_ll">ll</code></td>
<td>

<p>scalar or one-sided formula for lower limit for left-truncation; e.g. <code>ll = ~ 1</code> or <code>ll = ~ z1</code>.
</p>
</td></tr>
<tr><td><code id="truncreg_+3A_ul">ul</code></td>
<td>

<p>scalar or one-sided formula for upper limit for right-truncation; e.g. <code>ul = ~ 800</code> or <code>ul = ~ z1</code>.
</p>
</td></tr>
<tr><td><code id="truncreg_+3A_lmtol">lmtol</code></td>
<td>

<p>numeric. Tolerance for the scaled gradient in ML optimization. Default is .Machine$double.eps.
</p>
</td></tr>
<tr><td><code id="truncreg_+3A_maxiter">maxiter</code></td>
<td>

<p>numeric. maximum number of iteration for maximization of the log likelihood function.
</p>
</td></tr>
<tr><td><code id="truncreg_+3A_marg.eff">marg.eff</code></td>
<td>

<p>logical. If <code>TRUE</code>, unit-specific marginal effects of exogenous variables on the mean of distribution of inefficiency term are returned.
</p>
</td></tr>
<tr><td><code id="truncreg_+3A_print.level">print.level</code></td>
<td>

<p>numeric. 0 - nothing is printed. 1 - optimization steps and print estimation results. 2 - detailed optimization steps and print estimation results. Default is 1.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>truncreg</code> performs a regression from a sample drawn from a restricted part of the population.  Under the assumption that the error term of the whole population is normal, the error terms in the truncated regression model have a truncated normal distribution.
</p>
<p>Both lower limit for left-truncation and upper limit for right-truncation can be specified simultaneously.
</p>
<p>Models for <code>truncreg</code> are specified symbolically. A typical model has the form <code>y ~ x1 + ...</code>, where <code>y</code> represents the left hand side variable and <code>{x1,...}</code> right hand side variables.  
</p>
<p>If <code>marg.eff = TRUE</code>, the marginal effects are computed.
</p>


<h3>Value</h3>

<p><code>truncreg</code> returns a list of class <code>npsf</code> containing the following elements:
</p>
<table>
<tr><td><code>call</code></td>
<td>

<p>call. 'truncreg.cs'.
</p>
</td></tr>
<tr><td><code>model</code></td>
<td>

<p>character. Unevaluated call to function <code>truncreg</code>.
</p>
</td></tr>
<tr><td><code>coef</code></td>
<td>

<p>numeric. Named vector of ML parameter estimates.
</p>
</td></tr>
<tr><td><code>table</code></td>
<td>

<p>matrix. Table with results.
</p>
</td></tr>
<tr><td><code>vcov</code></td>
<td>

<p>matrix. Estimated covariance matrix of ML estimator. 
</p>
</td></tr>
<tr><td><code>ll</code></td>
<td>

<p>numeric. Value of log-likelihood at ML estimates.
</p>
</td></tr>
<tr><td><code>lmtol</code></td>
<td>

<p>numeric. Convergence criterion: tolerance for the scaled gradient.
</p>
</td></tr>
<tr><td><code>LM</code></td>
<td>

<p>numeric. Value of the scaled gradient.
</p>
</td></tr>
<tr><td><code>esttime</code></td>
<td>

<p>numeric. Estimation time.
</p>
</td></tr>
<tr><td><code>marg.effects</code></td>
<td>

<p>data frame. Contains unit-specific marginal effects of exogenous variables. 
</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>

<p>numeric. estimate of sigma.
</p>
</td></tr>
<tr><td><code>LL</code></td>
<td>

<p>numeric. The lower limit for left-truncation
</p>
</td></tr>
<tr><td><code>UL</code></td>
<td>

<p>numeric. The upper limit for left-truncation
</p>
</td></tr>
<tr><td><code>n</code></td>
<td>

<p>numeric. Number of observations (used in regression).
</p>
</td></tr>
<tr><td><code>n.full</code></td>
<td>

<p>numeric. Number of observations (used and not used in regression).
</p>
</td></tr>
<tr><td><code>nontruncsample</code></td>
<td>

<p>logical. Returns TRUE if the observation in user supplied data is in the estimation subsample and in non-truncated part of the sample, and FALSE otherwise.
</p>
</td></tr>
<tr><td><code>esample</code></td>
<td>

<p>logical. Returns TRUE if the observation in user supplied data is in the estimation subsample and FALSE otherwise.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Oleg Badunenko &lt;oleg.badunenko@brunel.ac.uk&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+teradial">teradial</a></code>, <code><a href="#topic+tenonradial">tenonradial</a></code>, <code><a href="#topic+teradialbc">teradialbc</a></code>, <code><a href="#topic+tenonradialbc">tenonradialbc</a></code>, <code><a href="#topic+nptestrts">nptestrts</a></code>, <code><a href="#topic+nptestind">nptestind</a></code>, <code><a href="#topic+sf">sf</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
require( npsf )
 
# Female labor force participation dataset
 
data(mroz)
head(mroz)

t1 &lt;- npsf::truncreg(hours ~ kidslt6 + kidsge6 + age*educ, 
 data = mroz, ll = 0, lmtol = 1e-16, print.level = 2)
 
# matrices also can be used
myY &lt;- mroz$hours
myX &lt;- cbind(mroz$kidslt6, mroz$kidsge6, mroz$age, mroz$educ, mroz$age*mroz$educ)

t1.m &lt;- truncreg(myY ~ myX, ll = 0)

# gives identical result to `t1':
# compare summary(t1) and summary(t1.m)

# using variable for limits is admissible
# we obtain the same result as before

mroz$myll &lt;- 0
t11 &lt;- npsf::truncreg(hours ~ kidslt6 + kidsge6 + age*educ, 
 data = mroz, ll = ~ myll, lmtol = 1e-16, print.level = 0)
summary(t11)

# if you believe that the sample is additionally truncted from above at say 3500

t12 &lt;- update(t1, ul = 3500, print.level = 1)

# for obtaining marginal effects

t13 &lt;- update(t12, marg.eff = TRUE)

summary(t13$marg.effects)

</code></pre>

<hr>
<h2 id='usmanuf'>US Manufacturing Industry Data</h2><span id='topic+usmanuf'></span>

<h3>Description</h3>

<p>This data come from the National Bureau of Economic Research Center for Economic Studies manufacturing industry database.  This data set provides only selected variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data( usmanuf )</code></pre>


<h3>Format</h3>

<p>This data frame contains the following variables (columns):
</p>

<dl>
<dt><code>naics</code></dt><dd><p>NAICS 6-digit Codes</p>
</dd>
<dt><code>year</code></dt><dd><p>Year ranges from 1990 to 2009</p>
</dd>
<dt><code>Y</code></dt><dd><p>Total value of shipments in $1m divided by the deflator for VSHIP 1997=1.000 (vship/piship)</p>
</dd>
<dt><code>K</code></dt><dd><p>Total real capital stock in $1m (cap)</p>
</dd>
<dt><code>L</code></dt><dd><p>Total employment in thousands (emp)</p>
</dd>
<dt><code>M</code></dt><dd><p>Total cost of materials in $1m divided by the deflator for MATCOST 1997=1.000 plus oost of electric &amp; fuels in $1m divided by the deflator for ENERGY 1997=1.000 (matcost/pimat +  energy/pien)</p>
</dd>
</dl>



<h3>Details</h3>

<p>These data come from the National Bureau of Economic Research Center for Economic Studies manufacturing industry database, which was compiled by Randy A. Becker and Wayne B. Gray. This database is a joint effort between the National Bureau of Economic Research (NBER) and U.S. Census Bureau's Center for Economic Studies (CES), containing annual industry-level data from 1958-2009 on output, employment, payroll and other input costs, investment, capital stocks, TFP, and various industry-specific price indexes. Because of the change from SIC to NAICS industry definitions in 1997, the database is provided in two versions: one with 459 four-digit 1987 SIC industries and the other with 473 six-digit 1997 NAICS industries.
</p>


<h3>Source</h3>

<p><a href="https://www.nber.org/nberces/">https://www.nber.org/nberces/</a>.
</p>


<h3>References</h3>

<p>Bartelsman, E.J. and Gray, W. (1996), The NBER Manufacturing Productivity Database, <em>National Bureau of Economic Research</em>, Technical Working Paper Series, doi: <a href="https://doi.org/10.3386/t0205">10.3386/t0205</a>
</p>

<hr>
<h2 id='vcov.npsf'>
'vcov' method for class 'npsf'
</h2><span id='topic+vcov.npsf'></span>

<h3>Description</h3>

<p>Extracts the variance-covariance matrix of the ML parameters of a stochastic frontier model estimated by <code><a href="#topic+sf">sf</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> ## S3 method for class 'npsf'
vcov( object, ... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vcov.npsf_+3A_object">object</code></td>
<td>

<p>an object of class <code>npsf</code> returned by the function <code><a href="#topic+sf">sf</a></code>.
</p>
</td></tr>
<tr><td><code id="vcov.npsf_+3A_...">...</code></td>
<td>

<p>currently unused.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The estimated variance-covariance matrix of the ML parameters 
is the inverse of the negative Hessian evaluated at the MLE.
</p>


<h3>Value</h3>

<p><code>vcov.npsf</code> returns the estimated variance-covariance matrix of the ML parameters.
</p>


<h3>Author(s)</h3>

<p>Oleg Badunenko &lt;oleg.badunenko@brunel.ac.uk&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coef.npsf">coef.npsf</a></code>, <code><a href="#topic+nobs.npsf">nobs.npsf</a></code>, <code><a href="#topic+summary.npsf">summary.npsf</a></code>, and <code><a href="#topic+sf">sf</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require( npsf )
 
# Load Penn World Tables 5.6 dataset
 
data( pwt56 )
head( pwt56 )
 
# Create some missing values
 
pwt56 [4, "K"] &lt;- NA 
 
# Stochastic production frontier model with 
# homoskedastic error components (half-normal)
 
# Use subset of observations - for year 1965
 
m1 &lt;- sf(log(Y) ~ log(L) + log(K), data = pwt56, 
 subset = year == 1965, distribution = "h")
vcov( m1 )
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
