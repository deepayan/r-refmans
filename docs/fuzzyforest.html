<!DOCTYPE html><html><head><title>Help for package fuzzyforest</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {fuzzyforest}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#fuzzyforest'><p>fuzzyforest: an implementation of the fuzzy forest algorithm in R.</p></a></li>
<li><a href='#ctg'><p>Cardiotocography Data Set</p></a></li>
<li><a href='#example_ff'><p>Fuzzy Forest Example</p></a></li>
<li><a href='#ff'><p>Fuzzy forests algorithm</p></a></li>
<li><a href='#ff.formula'><p>Fuzzy forests algorithm</p></a></li>
<li><a href='#fuzzy_forest'><p>Fuzzy Forest Object</p></a></li>
<li><a href='#iterative_RF'><p>Fits iterative random forest algorithm.</p></a></li>
<li><a href='#Liver_Expr'><p>Liver Expression Data from Female Mice</p></a></li>
<li><a href='#modplot'><p>Plots relative importance of modules.</p></a></li>
<li><a href='#multi_class_lr'><p>Multinomial Logistic Regression</p></a></li>
<li><a href='#predict.fuzzy_forest'><p>Predict method for fuzzy_forest object.</p>
Obtains predictions from fuzzy forest algorithm.</a></li>
<li><a href='#print.fuzzy_forest'><p>Print fuzzy_forest object.</p>
Prints output from fuzzy forests algorithm.</a></li>
<li><a href='#screen_control'><p>Set Parameters for Screening Step of</p>
Fuzzy Forests</a></li>
<li><a href='#select_control'><p>Set Parameters for Selection Step of</p>
Fuzzy Forests</a></li>
<li><a href='#select_RF'><p>Carries out the selection step of fuzzyforest algorithm.</p></a></li>
<li><a href='#wff'><p>WGCNA based fuzzy forest algorithm</p></a></li>
<li><a href='#wff.formula'><p>WGCNA based fuzzy forest algorithm</p></a></li>
<li><a href='#WGCNA_control'><p>Set Parameters for WGCNA Step of</p>
Fuzzy Forests</a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Fuzzy Forests</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.8</td>
</tr>
<tr>
<td>Description:</td>
<td>Fuzzy forests, a new algorithm based on random forests,
    is designed to reduce the bias seen in random forest feature selection
    caused by the presence of correlated features.  Fuzzy forests uses
    recursive feature elimination random forests to select
    features from separate blocks of correlated features where the
    correlation within each block of features is high
    and the correlation between blocks of features is low.
    One final random forest is fit using the surviving features.
    This package fits random forests using the 'randomForest' package and
    allows for easy use of 'WGCNA' to split features into distinct blocks.
    See D. Conn, Ngun, T., C. Ramirez, and G. Li (2019) &lt;<a href="https://doi.org/10.18637%2Fjss.v091.i09">doi:10.18637/jss.v091.i09</a>&gt;
    for further details.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2.1)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Imports:</td>
<td>randomForest, foreach, doParallel, parallel, ggplot2, mvtnorm</td>
</tr>
<tr>
<td>Suggests:</td>
<td>WGCNA, testthat</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-03-23 18:48:28 UTC; danielconn</td>
</tr>
<tr>
<td>Author:</td>
<td>Daniel Conn [aut, cre],
  Tuck Ngun [aut],
  Christina M. Ramirez [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Daniel Conn &lt;djconn17@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-03-25 16:40:16 UTC</td>
</tr>
</table>
<hr>
<h2 id='fuzzyforest'>fuzzyforest: an implementation of the fuzzy forest algorithm in R.</h2><span id='topic+fuzzyforest'></span><span id='topic+fuzzyforest-package'></span>

<h3>Description</h3>

<p>This package implements fuzzy forests and integrates the fuzzy
forests algorithm with the package, <span class="pkg">WGCNA</span>.
</p>


<h3>Note</h3>

<p>This work was partially funded by NSF IIS 1251151 and AMFAR 8721SC.
</p>

<hr>
<h2 id='ctg'>Cardiotocography Data Set</h2><span id='topic+ctg'></span>

<h3>Description</h3>

<p>A data set containing measurements of fetal heart rate and uterine
contraction from cardiotocograms.  This data set was obtained from
the [UCI machine learning repository](https://archive.ics.uci.edu/ml/index.html)
For our examples we extract a random sub sample of 100 observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(ctg)
</code></pre>


<h3>Format</h3>

<p>A data frame with 100 rows and 21.</p>

<hr>
<h2 id='example_ff'>Fuzzy Forest Example</h2><span id='topic+example_ff'></span>

<h3>Description</h3>

<p>An example of a fuzzy_forest object derived from fitting fuzzy forests
on the ctg data set.  The source code used to produce example_ff can be
seen in the vignette &quot;fuzzyforest_introduction&quot;.
</p>


<h3>Format</h3>

<p>.RData</p>

<hr>
<h2 id='ff'>Fuzzy forests algorithm</h2><span id='topic+ff'></span><span id='topic+ff.default'></span>

<h3>Description</h3>

<p>Fits the fuzzy forests algorithm. Note that a formula interface for
fuzzy forests also exists: <code><a href="#topic+ff.formula">ff.formula</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
ff(X, y, Z = NULL, module_membership,
  screen_params = screen_control(min_ntree = 500),
  select_params = select_control(min_ntree = 500), final_ntree = 5000,
  num_processors = 1, nodesize, test_features = NULL, test_y = NULL,
  ...)

ff(X, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ff_+3A_x">X</code></td>
<td>
<p>A data.frame.
Each column corresponds to a feature vectors.</p>
</td></tr>
<tr><td><code id="ff_+3A_y">y</code></td>
<td>
<p>Response vector.  For classification, y should be a
factor.  For regression, y should be
numeric.</p>
</td></tr>
<tr><td><code id="ff_+3A_z">Z</code></td>
<td>
<p>A data.frame. Additional features that are not to be
screened out at the screening step.</p>
</td></tr>
<tr><td><code id="ff_+3A_module_membership">module_membership</code></td>
<td>
<p>A character vector giving the module membership of
each feature.</p>
</td></tr>
<tr><td><code id="ff_+3A_screen_params">screen_params</code></td>
<td>
<p>Parameters for screening step of fuzzy forests.
See <code><a href="#topic+screen_control">screen_control</a></code> for
details. <code>screen_params</code> is an object of type
<code>screen_control</code>.</p>
</td></tr>
<tr><td><code id="ff_+3A_select_params">select_params</code></td>
<td>
<p>Parameters for selection step of fuzzy forests.
See <code><a href="#topic+select_control">select_control</a></code> for details.
<code>select_params</code> is an object of type
<code>select_control</code>.</p>
</td></tr>
<tr><td><code id="ff_+3A_final_ntree">final_ntree</code></td>
<td>
<p>Number of trees grown in the final random forest.
This random forest contains all selected features.</p>
</td></tr>
<tr><td><code id="ff_+3A_num_processors">num_processors</code></td>
<td>
<p>Number of processors used to fit random forests.</p>
</td></tr>
<tr><td><code id="ff_+3A_nodesize">nodesize</code></td>
<td>
<p>Minimum terminal nodesize. 1 if classification.
5 if regression.  If the sample size is very large,
the trees will be grown extremely deep.
This may lead to issues with memory usage and may
lead to significant increases in the time it takes
the algorithm to run.  In this case,
it may be useful to increase <code>nodesize</code>.</p>
</td></tr>
<tr><td><code id="ff_+3A_test_features">test_features</code></td>
<td>
<p>A data.frame containing features from a test set.
The data.frame should contain the features in both
X and Z.</p>
</td></tr>
<tr><td><code id="ff_+3A_test_y">test_y</code></td>
<td>
<p>The responses for the test set.</p>
</td></tr>
<tr><td><code id="ff_+3A_...">...</code></td>
<td>
<p>Additional arguments currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of type <code><a href="#topic+fuzzy_forest">fuzzy_forest</a></code>.  This
object is a list containing useful output of fuzzy forests.
In particular it contains a data.frame with a list of selected the features.
It also includes a random forest fit using the selected features.
</p>


<h3>Note</h3>

<p>This work was partially funded by NSF IIS 1251151 and AMFAR 8721SC.
</p>


<h3>References</h3>

<p>Conn, D., Ngun, T., Ramirez C.M., Li, G. (2019).
&quot;Fuzzy Forests: Extending Random Forest Feature Selection for Correlated, High-Dimensional Data.&quot;
<em>Journal of Statistical Software</em>, <strong>91</strong>(9).
doi: <a href="https://doi.org/10.18637/jss.v091.i09">10.18637/jss.v091.i09</a>
</p>
<p>Breiman, L. (2001).
&quot;Random Forests.&quot;
<em>Machine Learning</em>, <strong>45</strong>(1), 5-32.
doi: <a href="https://doi.org/10.1023/A:1010933404324">10.1023/A:1010933404324</a>
</p>
<p>Zhang, B. and Horvath, S. (2005).
&quot;A General Framework for Weighted Gene Co-Expression Network Analysis.&quot;
<em>Statistical Applications in Genetics and Molecular Biology</em>, <strong>4</strong>(1).
doi: <a href="https://doi.org/10.2202/1544-6115.1128">10.2202/1544-6115.1128</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ff.formula">ff.formula</a></code>,
<code><a href="#topic+print.fuzzy_forest">print.fuzzy_forest</a></code>,
<code><a href="#topic+predict.fuzzy_forest">predict.fuzzy_forest</a></code>,
<code><a href="#topic+modplot">modplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#ff requires that the partition of the covariates be previously determined.
#ff is also handy if the user wants to test out multiple settings of WGCNA
#prior to running fuzzy forests.

library(mvtnorm)
gen_mod &lt;- function(n, p, corr) {
  sigma &lt;- matrix(corr, nrow=p, ncol=p)
  diag(sigma) &lt;- 1
  X &lt;- rmvnorm(n, sigma=sigma)
  return(X)
}

gen_X &lt;- function(n, mod_sizes, corr){
  m &lt;- length(mod_sizes)
  X_list &lt;- vector("list", length = m)
  for(i in 1:m){
    X_list[[i]] &lt;- gen_mod(n, mod_sizes[i], corr[i])
  }
  X &lt;- do.call("cbind", X_list)
  return(X)
}

err_sd &lt;- .5
n &lt;- 500
mod_sizes &lt;- rep(25, 4)
corr &lt;- rep(.8, 4)
X &lt;- gen_X(n, mod_sizes, corr)
beta &lt;- rep(0, 100)
beta[c(1:4, 76:79)] &lt;- 5
y &lt;- X%*%beta + rnorm(n, sd=err_sd)
X &lt;- as.data.frame(X)

Xtest &lt;- gen_X(n, mod_sizes, corr)
ytest &lt;- Xtest%*%beta + rnorm(n, sd=err_sd)
Xtest &lt;- as.data.frame(Xtest)

cdist &lt;- as.dist(1 - cor(X))
hclust_fit &lt;- hclust(cdist, method="ward.D")
groups &lt;- cutree(hclust_fit, k=4)

screen_c &lt;- screen_control(keep_fraction = .25,
                           ntree_factor = 1,
                           min_ntree = 250)
select_c &lt;- select_control(number_selected = 10,
                           ntree_factor = 1,
                           min_ntree = 250)

ff_fit &lt;- ff(X, y, module_membership = groups,
             screen_params = screen_c,
             select_params = select_c,
             final_ntree = 250)
#extract variable importance rankings
vims &lt;- ff_fit$feature_list

#plot results
modplot(ff_fit)

#obtain predicted values for a new test set
preds &lt;- predict(ff_fit, new_data=Xtest)

#estimate test set error
test_err &lt;- sqrt(sum((ytest - preds)^2)/n)

</code></pre>

<hr>
<h2 id='ff.formula'>Fuzzy forests algorithm</h2><span id='topic+ff.formula'></span>

<h3>Description</h3>

<p>Implements formula interface for <code><a href="#topic+ff">ff</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'formula'
ff(formula, data = NULL, module_membership, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ff.formula_+3A_formula">formula</code></td>
<td>
<p>Formula object.</p>
</td></tr>
<tr><td><code id="ff.formula_+3A_data">data</code></td>
<td>
<p>data used in the analysis.</p>
</td></tr>
<tr><td><code id="ff.formula_+3A_module_membership">module_membership</code></td>
<td>
<p>A character vector giving the module membership
of each feature.</p>
</td></tr>
<tr><td><code id="ff.formula_+3A_...">...</code></td>
<td>
<p>Additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of type <code><a href="#topic+fuzzy_forest">fuzzy_forest</a></code>.  This
object is a list containing useful output of fuzzy forests.
In particular it contains a data.frame with list of selected features.
It also includes the random forest fit using the selected features.
</p>


<h3>Note</h3>

<p>See <code><a href="#topic+ff">ff</a></code> for additional arguments.
Note that the matrix, <code>Z</code>, of features that do not go through
the screening step must specified separately from the formula.
<code>test_features</code> and <code>test_y</code> are not supported in formula
interface.  As in the <code>randomForest</code> package, for large data sets
the formula interface may be substantially slower.
</p>
<p>This work was partially funded by NSF IIS 1251151 and AMFAR 8721SC.
</p>


<h3>References</h3>

<p>Conn, D., Ngun, T., Ramirez C.M., Li, G. (2019).
&quot;Fuzzy Forests: Extending Random Forest Feature Selection for Correlated, High-Dimensional Data.&quot;
<em>Journal of Statistical Software</em>, <strong>91</strong>(9).
doi: <a href="https://doi.org/10.18637/jss.v091.i09">10.18637/jss.v091.i09</a>
</p>
<p>Breiman, L. (2001).
&quot;Random Forests.&quot;
<em>Machine Learning</em>, <strong>45</strong>(1), 5-32.
doi: <a href="https://doi.org/10.1023/A:1010933404324">10.1023/A:1010933404324</a>
</p>
<p>Zhang, B. and Horvath, S. (2005).
&quot;A General Framework for Weighted Gene Co-Expression Network Analysis.&quot;
<em>Statistical Applications in Genetics and Molecular Biology</em>, <strong>4</strong>(1).
doi: <a href="https://doi.org/10.2202/1544-6115.1128">10.2202/1544-6115.1128</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ff">ff</a></code>,
<code><a href="#topic+print.fuzzy_forest">print.fuzzy_forest</a></code>,
<code><a href="#topic+predict.fuzzy_forest">predict.fuzzy_forest</a></code>,
<code><a href="#topic+modplot">modplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#ff requires that the partition of the covariates be previously determined.
#ff is also handy if the user wants to test out multiple settings of WGCNA
#prior to running fuzzy forests.
library(mvtnorm)
gen_mod &lt;- function(n, p, corr) {
  sigma &lt;- matrix(corr, nrow=p, ncol=p)
  diag(sigma) &lt;- 1
  X &lt;- rmvnorm(n, sigma=sigma)
  return(X)
}

gen_X &lt;- function(n, mod_sizes, corr){
  m &lt;- length(mod_sizes)
  X_list &lt;- vector("list", length = m)
  for(i in 1:m){
    X_list[[i]] &lt;- gen_mod(n, mod_sizes[i], corr[i])
  }
  X &lt;- do.call("cbind", X_list)
  return(X)
}

err_sd &lt;- .5
n &lt;- 500
mod_sizes &lt;- rep(25, 4)
corr &lt;- rep(.8, 4)
X &lt;- gen_X(n, mod_sizes, corr)
beta &lt;- rep(0, 100)
beta[c(1:4, 76:79)] &lt;- 5
y &lt;- X%*%beta + rnorm(n, sd=err_sd)
X &lt;- as.data.frame(X)
dat &lt;- as.data.frame(cbind(y, X))

Xtest &lt;- gen_X(n, mod_sizes, corr)
ytest &lt;- Xtest%*%beta + rnorm(n, sd=err_sd)
Xtest &lt;- as.data.frame(Xtest)

cdist &lt;- as.dist(1 - cor(X))
hclust_fit &lt;- hclust(cdist, method="ward.D")
groups &lt;- cutree(hclust_fit, k=4)

screen_c &lt;- screen_control(keep_fraction = .25,
                           ntree_factor = 1,
                           min_ntree = 250)
select_c &lt;- select_control(number_selected = 10,
                           ntree_factor = 1,
                           min_ntree = 250)

ff_fit &lt;- ff(y ~ ., data=dat,
             module_membership = groups,
             screen_params = screen_c,
             select_params = select_c,
             final_ntree = 250)
#extract variable importance rankings
vims &lt;- ff_fit$feature_list

#plot results
modplot(ff_fit)

#obtain predicted values for a new test set
preds &lt;- predict(ff_fit, new_data=Xtest)

#estimate test set error
test_err &lt;- sqrt(sum((ytest - preds)^2)/n)

</code></pre>

<hr>
<h2 id='fuzzy_forest'>Fuzzy Forest Object</h2><span id='topic+fuzzy_forest'></span>

<h3>Description</h3>

<p>Fuzzy forests returns an object of type
fuzzyforest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fuzzy_forest(feature_list, final_rf, module_membership,
  WGCNA_object = NULL, survivor_list, selection_list)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fuzzy_forest_+3A_feature_list">feature_list</code></td>
<td>
<p>List of selected features along with variable
importance measures.</p>
</td></tr>
<tr><td><code id="fuzzy_forest_+3A_final_rf">final_rf</code></td>
<td>
<p>A final random forest fit using the features
selected by fuzzy forests.</p>
</td></tr>
<tr><td><code id="fuzzy_forest_+3A_module_membership">module_membership</code></td>
<td>
<p>Module membership of each feature.</p>
</td></tr>
<tr><td><code id="fuzzy_forest_+3A_wgcna_object">WGCNA_object</code></td>
<td>
<p>If applicable, output of WGCNA analysis.</p>
</td></tr>
<tr><td><code id="fuzzy_forest_+3A_survivor_list">survivor_list</code></td>
<td>
<p>List of features that have survived screening step.</p>
</td></tr>
<tr><td><code id="fuzzy_forest_+3A_selection_list">selection_list</code></td>
<td>
<p>List of features retained at each iteration of
selection step.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of type fuzzy_forest.
</p>


<h3>Note</h3>

<p>This work was partially funded by NSF IIS 1251151 and AMFAR 8721SC.
</p>

<hr>
<h2 id='iterative_RF'>Fits iterative random forest algorithm.</h2><span id='topic+iterative_RF'></span>

<h3>Description</h3>

<p>Fits iterative random forest algorithm.  Returns
data.frame with variable importances and top rated features.
For now this is an internal function that I've used to explore how
recursive feature elimination works in simulations.  It may be exported at
a later time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>iterative_RF(X, y, drop_fraction, keep_fraction, mtry_factor,
  ntree_factor = 10, min_ntree = 5000, num_processors = 1, nodesize)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="iterative_RF_+3A_x">X</code></td>
<td>
<p>A data.frame.
Each column corresponds to a feature vectors.</p>
</td></tr>
<tr><td><code id="iterative_RF_+3A_y">y</code></td>
<td>
<p>Response vector.</p>
</td></tr>
<tr><td><code id="iterative_RF_+3A_drop_fraction">drop_fraction</code></td>
<td>
<p>A number between 0 and 1.  Percentage of features
dropped at each iteration.</p>
</td></tr>
<tr><td><code id="iterative_RF_+3A_keep_fraction">keep_fraction</code></td>
<td>
<p>A number between 0 and 1. Proportion features
from each module to retain at screening step.</p>
</td></tr>
<tr><td><code id="iterative_RF_+3A_mtry_factor">mtry_factor</code></td>
<td>
<p>A positive number.  Mtry for each random forest
is set to
<code>ceiling</code>(<code class="reqn">\sqrt{p}</code><code>mtry_factor</code>)
where <code>p</code> is the current number of features.</p>
</td></tr>
<tr><td><code id="iterative_RF_+3A_ntree_factor">ntree_factor</code></td>
<td>
<p>A number greater than 1.  <code>ntree</code> for each
random is <code>ntree_factor</code> times the number
of features.  For each random forest, <code>ntree</code>
is set to <code>max</code>(<code>min_ntree</code>,
<code>ntree_factor</code>*<code>p</code>).</p>
</td></tr>
<tr><td><code id="iterative_RF_+3A_min_ntree">min_ntree</code></td>
<td>
<p>Minimum number of trees grown in each random forest.</p>
</td></tr>
<tr><td><code id="iterative_RF_+3A_num_processors">num_processors</code></td>
<td>
<p>Number of processors used to fit random forests.</p>
</td></tr>
<tr><td><code id="iterative_RF_+3A_nodesize">nodesize</code></td>
<td>
<p>Minimum nodesize.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame with the top ranked features.
</p>


<h3>Note</h3>

<p>This work was partially funded by NSF IIS 1251151 and AMFAR 8721SC.
</p>

<hr>
<h2 id='Liver_Expr'>Liver Expression Data from Female Mice</h2><span id='topic+Liver_Expr'></span>

<h3>Description</h3>

<p>A data set containing gene expression levels in liver tissue from female
mice. This data set is a subset of the liver expression data set
from the WGCNA tutorial <a href="https://horvath.genetics.ucla.edu/html/CoexpressionNetwork/Rpackages/WGCNA/Tutorials/">https://horvath.genetics.ucla.edu/html/CoexpressionNetwork/Rpackages/WGCNA/Tutorials/</a>.
The tutorial contains further information about the data set as well as
extensive examples of WGCNA.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Liver_Expr)
</code></pre>


<h3>Format</h3>

<p>A data frame with 66 rows and 3601</p>


<h3>Details</h3>


<ul>
<li><p> The first column contains weight (g) for the 66 mice.
</p>
</li>
<li><p> The other 3600 columns contain the liver expression levels.
</p>
</li></ul>


<hr>
<h2 id='modplot'>Plots relative importance of modules.</h2><span id='topic+modplot'></span>

<h3>Description</h3>

<p>The plot is designed
to depict the size of each module and what percentage of selected
features fall into each module.  In particular, it is easy to
determine which module is over-represented in the group of selected
features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>modplot(object, main = NULL, xlab = NULL, ylab = NULL,
  module_labels = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="modplot_+3A_object">object</code></td>
<td>
<p>A fuzzy_forest object.</p>
</td></tr>
<tr><td><code id="modplot_+3A_main">main</code></td>
<td>
<p>Title of plot.</p>
</td></tr>
<tr><td><code id="modplot_+3A_xlab">xlab</code></td>
<td>
<p>Title for the x axis.</p>
</td></tr>
<tr><td><code id="modplot_+3A_ylab">ylab</code></td>
<td>
<p>Title for the y axis.</p>
</td></tr>
<tr><td><code id="modplot_+3A_module_labels">module_labels</code></td>
<td>
<p>Labels for the modules.  A data.frame
or character matrix with first column giving
the current name of module and second column giving
the assigned name of each module.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This work was partially funded by NSF IIS 1251151 and AMFAR 8721SC.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ff">ff</a></code>,
<code><a href="#topic+wff">wff</a></code>,
<code><a href="#topic+ff.formula">ff.formula</a></code>,
<code><a href="#topic+wff.formula">wff.formula</a></code>
</p>

<hr>
<h2 id='multi_class_lr'>Multinomial Logistic Regression</h2><span id='topic+multi_class_lr'></span>

<h3>Description</h3>

<p>Function to generate multi-class data from a multinomial logistic
regression.  Assumes there are 5 classes.  Only supports two modules for now.
Currently this function is used for testing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multi_class_lr(n, mod1_size = 10, mod2_size = 10, rho = 0.8,
  beta = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multi_class_lr_+3A_n">n</code></td>
<td>
<p>Sample size.</p>
</td></tr>
<tr><td><code id="multi_class_lr_+3A_mod1_size">mod1_size</code></td>
<td>
<p>Size of first module.</p>
</td></tr>
<tr><td><code id="multi_class_lr_+3A_mod2_size">mod2_size</code></td>
<td>
<p>Size of second module.</p>
</td></tr>
<tr><td><code id="multi_class_lr_+3A_rho">rho</code></td>
<td>
<p>Correlation of covariates.</p>
</td></tr>
<tr><td><code id="multi_class_lr_+3A_beta">beta</code></td>
<td>
<p>A matrix of parameters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with design matrix X, outcome y, and beta.
</p>


<h3>Note</h3>

<p>This work was partially funded by NSF IIS 1251151 and AMFAR 8721SC.
</p>

<hr>
<h2 id='predict.fuzzy_forest'>Predict method for fuzzy_forest object.
Obtains predictions from fuzzy forest algorithm.</h2><span id='topic+predict.fuzzy_forest'></span>

<h3>Description</h3>

<p>Predict method for fuzzy_forest object.
Obtains predictions from fuzzy forest algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fuzzy_forest'
predict(object, new_data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.fuzzy_forest_+3A_object">object</code></td>
<td>
<p>A fuzzy_forest object.</p>
</td></tr>
<tr><td><code id="predict.fuzzy_forest_+3A_new_data">new_data</code></td>
<td>
<p>A matrix or data.frame containing new_data.
Pay close attention to ensure feature names
match between training set and test set
data.frame.</p>
</td></tr>
<tr><td><code id="predict.fuzzy_forest_+3A_...">...</code></td>
<td>
<p>Additional arguments not in use.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of predictions
</p>


<h3>Note</h3>

<p>This work was partially funded by NSF IIS 1251151 and AMFAR 8721SC.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ff">ff</a></code>,
<code><a href="#topic+wff">wff</a></code>,
<code><a href="#topic+ff.formula">ff.formula</a></code>,
<code><a href="#topic+wff.formula">wff.formula</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mvtnorm)
gen_mod &lt;- function(n, p, corr) {
  sigma &lt;- matrix(corr, nrow=p, ncol=p)
  diag(sigma) &lt;- 1
  X &lt;- rmvnorm(n, sigma=sigma)
  return(X)
}

gen_X &lt;- function(n, mod_sizes, corr){
  m &lt;- length(mod_sizes)
  X_list &lt;- vector("list", length = m)
  for(i in 1:m){
    X_list[[i]] &lt;- gen_mod(n, mod_sizes[i], corr[i])
  }
  X &lt;- do.call("cbind", X_list)
  return(X)
}

err_sd &lt;- .5
n &lt;- 500
mod_sizes &lt;- rep(25, 4)
corr &lt;- rep(.8, 4)
X &lt;- gen_X(n, mod_sizes, corr)
beta &lt;- rep(0, 100)
beta[c(1:4, 76:79)] &lt;- 5
y &lt;- X%*%beta + rnorm(n, sd=err_sd)
X &lt;- as.data.frame(X)

Xtest &lt;- gen_X(n, mod_sizes, corr)
ytest &lt;- Xtest%*%beta + rnorm(n, sd=err_sd)
Xtest &lt;- as.data.frame(Xtest)

cdist &lt;- as.dist(1 - cor(X))
hclust_fit &lt;- hclust(cdist, method="ward.D")
groups &lt;- cutree(hclust_fit, k=4)

screen_c &lt;- screen_control(keep_fraction = .25,
                           ntree_factor = 1,
                           min_ntree = 250)
select_c &lt;- select_control(number_selected = 10,
                           ntree_factor = 1,
                           min_ntree = 250)

ff_fit &lt;- ff(X, y, module_membership = groups,
             screen_params = screen_c,
             select_params = select_c,
             final_ntree = 250)
#extract variable importance rankings
vims &lt;- ff_fit$feature_list

#plot results
modplot(ff_fit)

#obtain predicted values for a new test set
preds &lt;- predict(ff_fit, new_data=Xtest)

#estimate test set error
test_err &lt;- sqrt(sum((ytest - preds)^2)/n)

</code></pre>

<hr>
<h2 id='print.fuzzy_forest'>Print fuzzy_forest object.
Prints output from fuzzy forests algorithm.</h2><span id='topic+print.fuzzy_forest'></span>

<h3>Description</h3>

<p>Print fuzzy_forest object.
Prints output from fuzzy forests algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fuzzy_forest'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.fuzzy_forest_+3A_x">x</code></td>
<td>
<p>A fuzzy_forest object.</p>
</td></tr>
<tr><td><code id="print.fuzzy_forest_+3A_...">...</code></td>
<td>
<p>Additional arguments not in use.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame with list of selected features and variable
importance measures.
</p>


<h3>Note</h3>

<p>This work was partially funded by NSF IIS 1251151 and AMFAR 8721SC.
</p>

<hr>
<h2 id='screen_control'>Set Parameters for Screening Step of
Fuzzy Forests</h2><span id='topic+screen_control'></span>

<h3>Description</h3>

<p>Creates <code>screen_control</code> object for
controlling how feature selection
will be carried out on each module.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>screen_control(drop_fraction = 0.25, keep_fraction = 0.05,
  mtry_factor = 1, min_ntree = 500, ntree_factor = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="screen_control_+3A_drop_fraction">drop_fraction</code></td>
<td>
<p>A number between 0 and 1.  Percentage of features
dropped at each iteration.</p>
</td></tr>
<tr><td><code id="screen_control_+3A_keep_fraction">keep_fraction</code></td>
<td>
<p>A number between 0 and 1. Proportion of features
from each module that are retained from screening step.</p>
</td></tr>
<tr><td><code id="screen_control_+3A_mtry_factor">mtry_factor</code></td>
<td>
<p>In the case of regression, <code>mtry</code> is set to
<code>ceiling</code>(<code class="reqn">\sqrt(p)</code>*<code>mtry_factor</code>).
In the case of classification, <code>mtry</code> is set to
<code>ceiling</code>((p/3)*<code>mtry_factor</code>).  If either
of these numbers is greater than p, <code>mtry</code> is
set to p.</p>
</td></tr>
<tr><td><code id="screen_control_+3A_min_ntree">min_ntree</code></td>
<td>
<p>Minimum number of trees grown in each random forest.</p>
</td></tr>
<tr><td><code id="screen_control_+3A_ntree_factor">ntree_factor</code></td>
<td>
<p>A number greater than 1.  <code>ntree</code> for each
random forest is <code>ntree_factor</code> times the number
of features.  For each random forest, <code>ntree</code>
is set to <code>max</code>(<code>min_ntree</code>,
<code>ntree_factor</code>*p).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of type screen_control.
</p>


<h3>Note</h3>

<p>This work was partially funded by NSF IIS 1251151 and AMFAR 8721SC.
</p>


<h3>References</h3>

<p>Conn, D., Ngun, T., Ramirez C.M., Li, G. (2019).
&quot;Fuzzy Forests: Extending Random Forest Feature Selection for Correlated, High-Dimensional Data.&quot;
<em>Journal of Statistical Software</em>, <strong>91</strong>(9).
doi: <a href="https://doi.org/10.18637/jss.v091.i09">10.18637/jss.v091.i09</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>drop_fraction &lt;- .25
keep_fraction &lt;- .1
mtry_factor &lt;- 1
min_ntree &lt;- 5000
ntree_factor &lt;- 5
screen_params &lt;- screen_control(drop_fraction=drop_fraction,
                                keep_fraction=keep_fraction,
                                mtry_factor=mtry_factor,
                                min_ntree=min_ntree,
                                ntree_factor=ntree_factor)
</code></pre>

<hr>
<h2 id='select_control'>Set Parameters for Selection Step of
Fuzzy Forests</h2><span id='topic+select_control'></span>

<h3>Description</h3>

<p>Creates <code>selection_control</code> object for
controlling how feature selection
will be carried out after features from different
modules have been combined.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>select_control(drop_fraction = 0.25, number_selected = 5,
  mtry_factor = 1, min_ntree = 500, ntree_factor = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="select_control_+3A_drop_fraction">drop_fraction</code></td>
<td>
<p>A number between 0 and 1.  Percentage of features
dropped at each iteration.</p>
</td></tr>
<tr><td><code id="select_control_+3A_number_selected">number_selected</code></td>
<td>
<p>A positive number. Number of features
that will be selected by fuzzyforests.</p>
</td></tr>
<tr><td><code id="select_control_+3A_mtry_factor">mtry_factor</code></td>
<td>
<p>In the case of regression, <code>mtry</code> is set to
<code>ceiling</code>(<code class="reqn">\sqrt(p)</code>*<code>mtry_factor</code>).
In the case of classification, <code>mtry</code> is set to
<code>ceiling</code>((p/3)*<code>mtry_factor</code>).  If either
of these numbers is greater than p, <code>mtry</code> is
set to p.</p>
</td></tr>
<tr><td><code id="select_control_+3A_min_ntree">min_ntree</code></td>
<td>
<p>Minimum number of trees grown in each random forest.</p>
</td></tr>
<tr><td><code id="select_control_+3A_ntree_factor">ntree_factor</code></td>
<td>
<p>A number greater than 1.  <code>ntree</code> for each
random forest is <code>ntree_factor</code> times the number
of features.  For each random forest, <code>ntree</code>
is set to <code>max</code>(<code>min_ntree</code>,
<code>ntree_factor</code>*<code>p</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of type selection_control.
</p>


<h3>Note</h3>

<p>This work was partially funded by NSF IIS 1251151 and AMFAR 8721SC.
</p>


<h3>References</h3>

<p>Conn, D., Ngun, T., Ramirez C.M., Li, G. (2019).
&quot;Fuzzy Forests: Extending Random Forest Feature Selection for Correlated, High-Dimensional Data.&quot;
<em>Journal of Statistical Software</em>, <strong>91</strong>(9).
doi: <a href="https://doi.org/10.18637/jss.v091.i09">10.18637/jss.v091.i09</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>drop_fraction &lt;- .25
number_selected &lt;- 10
mtry_factor &lt;- 1
min_ntree &lt;- 5000
ntree_factor &lt;- 5
select_params &lt;- select_control(drop_fraction=drop_fraction,
                                number_selected=number_selected,
                                mtry_factor=mtry_factor,
                                min_ntree=min_ntree,
                                ntree_factor=ntree_factor)
</code></pre>

<hr>
<h2 id='select_RF'>Carries out the selection step of fuzzyforest algorithm.</h2><span id='topic+select_RF'></span>

<h3>Description</h3>

<p>Carries out the selection step of fuzzyforest algorithm.  Returns
data.frame with variable importances and top rated features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>select_RF(X, y, drop_fraction, number_selected, mtry_factor, ntree_factor,
  min_ntree, num_processors, nodesize)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="select_RF_+3A_x">X</code></td>
<td>
<p>A data.frame.
Each column corresponds to a feature vectors.
Could include additional covariates not a part of
the original modules.</p>
</td></tr>
<tr><td><code id="select_RF_+3A_y">y</code></td>
<td>
<p>Response vector.</p>
</td></tr>
<tr><td><code id="select_RF_+3A_drop_fraction">drop_fraction</code></td>
<td>
<p>A number between 0 and 1.  Percentage of features
dropped at each iteration.</p>
</td></tr>
<tr><td><code id="select_RF_+3A_number_selected">number_selected</code></td>
<td>
<p>Number of features selected by fuzzyforest.</p>
</td></tr>
<tr><td><code id="select_RF_+3A_mtry_factor">mtry_factor</code></td>
<td>
<p>In the case of regression, <code>mtry</code> is set to
<code>ceiling</code>(<code class="reqn">\sqrt(p)</code>*<code>mtry_factor</code>).
In the case of classification, <code>mtry</code> is set to
<code>ceiling</code>((p/3)*<code>mtry_factor</code>).  If either
of these numbers is greater than p, <code>mtry</code> is
set to p.</p>
</td></tr>
<tr><td><code id="select_RF_+3A_ntree_factor">ntree_factor</code></td>
<td>
<p>A number greater than 1.  <code>ntree</code> for each
random is <code>ntree_factor</code> times the number
of features.  For each random forest, <code>ntree</code>
is set to <code>max</code>(<code>min_ntree</code>,
<code>ntree_factor</code>*<code>p</code>).</p>
</td></tr>
<tr><td><code id="select_RF_+3A_min_ntree">min_ntree</code></td>
<td>
<p>Minimum number of trees grown in each random forest.</p>
</td></tr>
<tr><td><code id="select_RF_+3A_num_processors">num_processors</code></td>
<td>
<p>Number of processors used to fit random forests.</p>
</td></tr>
<tr><td><code id="select_RF_+3A_nodesize">nodesize</code></td>
<td>
<p>Minimum nodesize</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame with the top ranked features.
</p>


<h3>Note</h3>

<p>This work was partially funded by NSF IIS 1251151 and AMFAR 8721SC.
</p>

<hr>
<h2 id='wff'>WGCNA based fuzzy forest algorithm</h2><span id='topic+wff'></span><span id='topic+wff.default'></span>

<h3>Description</h3>

<p>Fits fuzzy forests using WGCNA to cluster features into
distinct modules.  Requires installation of WGCNA package. Note that a formula interface for
WGCNA based fuzzy forests also exists: <code><a href="#topic+wff.formula">wff.formula</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
wff(X, y, Z = NULL,
  WGCNA_params = WGCNA_control(power = 6),
  screen_params = screen_control(min_ntree = 500),
  select_params = select_control(min_ntree = 500), final_ntree = 5000,
  num_processors = 1, nodesize, test_features = NULL, test_y = NULL,
  ...)

wff(X, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wff_+3A_x">X</code></td>
<td>
<p>A data.frame. Each column corresponds to a feature
vector.  WGCNA will be used to cluster the
features in X.  As a result, the features should be
all be numeric.  Non-numeric features may be input
via Z.</p>
</td></tr>
<tr><td><code id="wff_+3A_y">y</code></td>
<td>
<p>Response vector.  For classification, y should be a
factor.  For regression, y should be
numeric.</p>
</td></tr>
<tr><td><code id="wff_+3A_z">Z</code></td>
<td>
<p>Additional features that are not to be screened out
at the screening step.  WGCNA is not carried out on
features in Z.</p>
</td></tr>
<tr><td><code id="wff_+3A_wgcna_params">WGCNA_params</code></td>
<td>
<p>Parameters for WGCNA.
See blockwiseModules function from WGCNA and
<code><a href="#topic+WGCNA_control">WGCNA_control</a></code> for details.
<code>WGCNA_params</code> is an object of type
<code>WGCNA_control</code>.</p>
</td></tr>
<tr><td><code id="wff_+3A_screen_params">screen_params</code></td>
<td>
<p>Parameters for screening step of fuzzy forests.
See <code><a href="#topic+screen_control">screen_control</a></code> for details.
<code>screen_params</code> is an object of type
<code>screen_control</code>.</p>
</td></tr>
<tr><td><code id="wff_+3A_select_params">select_params</code></td>
<td>
<p>Parameters for selection step of fuzzy forests.
See <code><a href="#topic+select_control">select_control</a></code> for details.
<code>select_params</code> is an object of type
<code>select_control</code>.</p>
</td></tr>
<tr><td><code id="wff_+3A_final_ntree">final_ntree</code></td>
<td>
<p>Number of trees grown in the final random forest.
This random forest contains all selected features.</p>
</td></tr>
<tr><td><code id="wff_+3A_num_processors">num_processors</code></td>
<td>
<p>Number of processors used to fit random forests.</p>
</td></tr>
<tr><td><code id="wff_+3A_nodesize">nodesize</code></td>
<td>
<p>Minimum terminal nodesize. 1 if classification.
5 if regression.  If the sample size is very large,
the trees will be grown extremely deep.
This may lead to issues with memory usage and may
lead to significant increases in the time it takes
the algorithm to run.  In this case,
it may be useful to increase <code>nodesize</code>.</p>
</td></tr>
<tr><td><code id="wff_+3A_test_features">test_features</code></td>
<td>
<p>A data.frame containing features from a test set.
The data.frame should contain the features in both
X and Z.</p>
</td></tr>
<tr><td><code id="wff_+3A_test_y">test_y</code></td>
<td>
<p>The responses for the test set.</p>
</td></tr>
<tr><td><code id="wff_+3A_...">...</code></td>
<td>
<p>Additional arguments currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of type <code><a href="#topic+fuzzy_forest">fuzzy_forest</a></code>.  This
object is a list containing useful output of fuzzy forests.
In particular it contains a data.frame with list of selected features.
It also includes the random forest fit using the selected features.
</p>


<h3>Note</h3>

<p>This work was partially funded by NSF IIS 1251151 and AMFAR 8721SC.
</p>


<h3>References</h3>

<p>Conn, D., Ngun, T., Ramirez C.M., Li, G. (2019).
&quot;Fuzzy Forests: Extending Random Forest Feature Selection for Correlated, High-Dimensional Data.&quot;
<em>Journal of Statistical Software</em>, <strong>91</strong>(9).
doi: <a href="https://doi.org/10.18637/jss.v091.i09">10.18637/jss.v091.i09</a>
</p>
<p>Breiman, L. (2001).
&quot;Random Forests.&quot;
<em>Machine Learning</em>, <strong>45</strong>(1), 5-32.
doi: <a href="https://doi.org/10.1023/A:1010933404324">10.1023/A:1010933404324</a>
</p>
<p>Zhang, B. and Horvath, S. (2005).
&quot;A General Framework for Weighted Gene Co-Expression Network Analysis.&quot;
<em>Statistical Applications in Genetics and Molecular Biology</em>, <strong>4</strong>(1).
doi: <a href="https://doi.org/10.2202/1544-6115.1128">10.2202/1544-6115.1128</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+wff.formula">wff.formula</a></code>,
<code><a href="#topic+print.fuzzy_forest">print.fuzzy_forest</a></code>,
<code><a href="#topic+predict.fuzzy_forest">predict.fuzzy_forest</a></code>,
<code><a href="#topic+modplot">modplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ctg)
y &lt;- ctg$NSP
X &lt;- ctg[, 2:22]
WGCNA_params &lt;- WGCNA_control(p = 6, minModuleSize = 1, nThreads = 1)
mtry_factor &lt;- 1; min_ntree &lt;- 500;  drop_fraction &lt;- .5; ntree_factor &lt;- 1
screen_params &lt;- screen_control(drop_fraction = drop_fraction,
                                keep_fraction = .25, min_ntree = min_ntree,
                                ntree_factor = ntree_factor,
                                mtry_factor = mtry_factor)
select_params &lt;- select_control(drop_fraction = drop_fraction,
                                number_selected = 5,
                                min_ntree = min_ntree,
                                ntree_factor = ntree_factor,
                                mtry_factor = mtry_factor)

library(WGCNA)
wff_fit &lt;- wff(X, y, WGCNA_params = WGCNA_params,
                screen_params = screen_params,
                select_params = select_params,
                final_ntree = 500)

#extract variable importance rankings
vims &lt;- wff_fit$feature_list

#plot results
modplot(wff_fit)

</code></pre>

<hr>
<h2 id='wff.formula'>WGCNA based fuzzy forest algorithm</h2><span id='topic+wff.formula'></span>

<h3>Description</h3>

<p>Implements formula interface for <code><a href="#topic+wff">wff</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'formula'
wff(formula, data = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wff.formula_+3A_formula">formula</code></td>
<td>
<p>Formula object.</p>
</td></tr>
<tr><td><code id="wff.formula_+3A_data">data</code></td>
<td>
<p>data used in the analysis.</p>
</td></tr>
<tr><td><code id="wff.formula_+3A_...">...</code></td>
<td>
<p>Additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of type <code><a href="#topic+fuzzy_forest">fuzzy_forest</a></code>.  This
object is a list containing useful output of fuzzy forests.
In particular it contains a data.frame with list of selected features.
It also includes the random forest fit using the selected features.
</p>


<h3>Note</h3>

<p>See <code><a href="#topic+ff">ff</a></code> for additional arguments.
Note that the matrix, <code>Z</code>, of features that do not go through
the screening step must specified separately from the formula.
<code>test_features</code> and <code>test_y</code> are not supported in formula
interface.  As in the <code>randomForest</code> package, for large data sets
the formula interface may be substantially slower.
</p>
<p>This work was partially funded by NSF IIS 1251151 and AMFAR 8721SC.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+wff">wff</a></code>,
<code><a href="#topic+print.fuzzy_forest">print.fuzzy_forest</a></code>,
<code><a href="#topic+predict.fuzzy_forest">predict.fuzzy_forest</a></code>,
<code><a href="#topic+modplot">modplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(ctg)
y &lt;- ctg$NSP
X &lt;- ctg[, 2:22]
dat &lt;- as.data.frame(cbind(y, X))
WGCNA_params &lt;- WGCNA_control(p = 6, minModuleSize = 1, nThreads = 1)
mtry_factor &lt;- 1; min_ntree &lt;- 500;  drop_fraction &lt;- .5; ntree_factor &lt;- 1
screen_params &lt;- screen_control(drop_fraction = drop_fraction,
                                keep_fraction = .25, min_ntree = min_ntree,
                                ntree_factor = ntree_factor,
                                mtry_factor = mtry_factor)
select_params &lt;- select_control(drop_fraction = drop_fraction,
                                number_selected = 5,
                                min_ntree = min_ntree,
                                ntree_factor = ntree_factor,
                                mtry_factor = mtry_factor)

library(WGCNA)
wff_fit &lt;- wff(y ~ ., data=dat,
               WGCNA_params = WGCNA_params,
               screen_params = screen_params,
               select_params = select_params,
               final_ntree = 500)

#extract variable importance rankings
vims &lt;- wff_fit$feature_list

#plot results
modplot(wff_fit)

</code></pre>

<hr>
<h2 id='WGCNA_control'>Set Parameters for WGCNA Step of
Fuzzy Forests</h2><span id='topic+WGCNA_control'></span>

<h3>Description</h3>

<p>Creates <code>WGCNA_control</code> object for
controlling WGCNA will be carried out.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>WGCNA_control(power = 6, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="WGCNA_control_+3A_power">power</code></td>
<td>
<p>Power of adjacency function.</p>
</td></tr>
<tr><td><code id="WGCNA_control_+3A_...">...</code></td>
<td>
<p>Additional arguments.
See blockwiseModules from the WGCNA package for
details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of type WGCNA_control.
</p>


<h3>Note</h3>

<p>This work was partially funded by NSF IIS 1251151.
</p>


<h3>References</h3>

<p>Conn, D., Ngun, T., Ramirez C.M., Li, G. (2019).
&quot;Fuzzy Forests: Extending Random Forest Feature Selection for Correlated, High-Dimensional Data.&quot;
<em>Journal of Statistical Software</em>, <strong>91</strong>(9).
doi: <a href="https://doi.org/10.18637/jss.v091.i09">10.18637/jss.v091.i09</a>
</p>
<p>Zhang, B. and Horvath, S. (2005).
&quot;A General Framework for Weighted Gene Co-Expression Network Analysis.&quot;
<em>Statistical Applications in Genetics and Molecular Biology</em>, <strong>4</strong>(1).
doi: <a href="https://doi.org/10.2202/1544-6115.1128">10.2202/1544-6115.1128</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>WGCNA_params &lt;- WGCNA_control(p=7, minModuleSize=30, TOMType = "unsigned",
                              reassignThreshold = 0, mergeCutHeight = 0.25,
                              numericLabels = TRUE, pamRespectsDendro = FALSE)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
