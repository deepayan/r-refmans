<!DOCTYPE html><html><head><title>Help for package tram</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tram}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Aareg'>
<p>Aalen Additive Hazards Model</p></a></li>
<li><a href='#BoxCox'>
<p>(Similar to) Box-Cox Models</p></a></li>
<li><a href='#Colr'>
<p>Continuous Outcome Logistic Regression</p></a></li>
<li><a href='#Compris'>
<p>Competing Risk Regression</p></a></li>
<li><a href='#Coxph'>
<p>Cox Proportional Hazards Model</p></a></li>
<li><a href='#Lehmann'>
<p>Proportional Reverse Time Hazards Linear Regression</p></a></li>
<li><a href='#Lm'>
<p>Normal Linear Model</p></a></li>
<li><a href='#mmlt'>
<p>Multivariate Conditional Transformation Models</p></a></li>
<li><a href='#mtram'>
<p>Transformation Models for Clustered Data</p></a></li>
<li><a href='#perm_test'>
<p>Permutation Transformation Tests</p></a></li>
<li><a href='#Polr'>
<p>Ordered Categorical Regression</p></a></li>
<li><a href='#score_test'>
<p>Transformation Score Tests and Confidence Intervals</p></a></li>
<li><a href='#Survreg'>
<p>Parametric Survival Models</p></a></li>
<li><a href='#tram'>
<p>Stratified Linear Transformation Models</p></a></li>
<li><a href='#tram-methods'>
<p>Methods for Stratified Linear Transformation Models</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Transformation Models</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0-2</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-01-30</td>
</tr>
<tr>
<td>Description:</td>
<td>Formula-based user-interfaces to specific transformation models
  implemented in package 'mlt'. Available models include Cox models, some parametric 
  survival models (Weibull, etc.), models for ordered categorical variables,
  normal and non-normal (Box-Cox type) linear models, and continuous outcome logistic regression
  (Lohse et al., 2017, &lt;<a href="https://doi.org/10.12688%2Ff1000research.12934.1">doi:10.12688/f1000research.12934.1</a>&gt;). The underlying theory
  is described in Hothorn et al. (2018) &lt;<a href="https://doi.org/10.1111%2Fsjos.12291">doi:10.1111/sjos.12291</a>&gt;. An extension to
  transformation models for clustered data is provided (Barbanti and Hothorn, 2022,
  &lt;<a href="https://doi.org/10.1093%2Fbiostatistics%2Fkxac048">doi:10.1093/biostatistics/kxac048</a>&gt;). Multivariate conditional transformation models 
  (Klein et al, 2022, &lt;<a href="https://doi.org/10.1111%2Fsjos.12501">doi:10.1111/sjos.12501</a>&gt;) and shift-scale transformation models (Siegfried et al, 2023,
  &lt;<a href="https://doi.org/10.1080%2F00031305.2023.2203177">doi:10.1080/00031305.2023.2203177</a>&gt;) can be fitted as well.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0), mlt (&ge; 1.5-0), mvtnorm (&ge; 1.2-0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Formula, multcomp, variables (&ge; 1.0-4), basefun (&ge; 1.1-2),
sandwich, stats, survival, graphics, Matrix, methods</td>
</tr>
<tr>
<td>Suggests:</td>
<td>MASS, TH.data, trtf (&ge; 0.3-3), mlbench, knitr, quantreg,
colorspace, ATR, lme4, merDeriv, SparseGrid, alabama, numDeriv,
gridExtra, lattice, latticeExtra, HSAUR3, ordinalCont, coxme,
mlt.docreg, ordinal, coin, asht, gamlss</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://ctm.R-forge.R-project.org">http://ctm.R-forge.R-project.org</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-30 09:32:50 UTC; hothorn</td>
</tr>
<tr>
<td>Author:</td>
<td>Torsten Hothorn <a href="https://orcid.org/0000-0001-8301-0471"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Luisa Barbanti <a href="https://orcid.org/0000-0001-5352-5802"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Sandra Siegfried <a href="https://orcid.org/0000-0002-7312-1001"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Brian Ripley [ctb],
  Bill Venables [ctb],
  Douglas M. Bates [ctb],
  Nadja Klein [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Torsten Hothorn &lt;Torsten.Hothorn@R-project.org&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-30 11:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='Aareg'>
Aalen Additive Hazards Model
</h2><span id='topic+Aareg'></span>

<h3>Description</h3>

<p>Aalen model with fully parameterised hazard function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Aareg(formula, data, subset, weights, offset, cluster, na.action = na.omit, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Aareg_+3A_formula">formula</code></td>
<td>
<p>an object of class <code>"formula"</code>: a symbolic description 
of the model structure to be
fitted.  The details of model specification are given under
<code><a href="#topic+tram">tram</a></code> and in the package vignette.
</p>
</td></tr>
<tr><td><code id="Aareg_+3A_data">data</code></td>
<td>
<p>an optional data frame, list or environment (or object
coercible by <code>as.data.frame</code> to a data frame) containing the
variables in the model.  If not found in <code>data</code>, the
variables are taken from <code>environment(formula)</code>.
</p>
</td></tr>
<tr><td><code id="Aareg_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations to be
used in the fitting process.
</p>
</td></tr>
<tr><td><code id="Aareg_+3A_weights">weights</code></td>
<td>
<p>an optional vector of weights to be used in the fitting
process.  Should be <code>NULL</code> or a numeric vector. If present,
the weighted log-likelihood is maximised.
</p>
</td></tr>
<tr><td><code id="Aareg_+3A_offset">offset</code></td>
<td>
<p>this can be used to specify an _a priori_ known component to
be included in the linear predictor during fitting.  This
should be <code>NULL</code> or a numeric vector of length equal to the
number of cases.
</p>
</td></tr>
<tr><td><code id="Aareg_+3A_cluster">cluster</code></td>
<td>
<p>optional factor with a cluster ID employed for computing
clustered covariances.
</p>
</td></tr>
<tr><td><code id="Aareg_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when the data
contain <code>NA</code>s.  The default is set to <code>na.omit</code>.
</p>
</td></tr>
<tr><td><code id="Aareg_+3A_...">...</code></td>
<td>
<p>additional arguments to <code><a href="#topic+tram">tram</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function allows simultaneous
estimation of the cumulative hazard parameterised by a Bernstein polynomial. 
The model is typically fitted with time-varying coefficients, all types of random
censoring and trunction are allowed.
</p>
<p>The responses is bounded (<code>bounds = c(0, Inf)</code>) when specified as a
<code>Surv</code> object. Otherwise, <code>bounds</code> can be specified via
<code>...</code>.
</p>


<h3>Value</h3>

<p>An object of class <code>Aareg</code>, with corresponding <code>coef</code>,
<code>vcov</code>, <code>logLik</code>, <code>estfun</code>, <code>summary</code>, 
<code>print</code>, <code>plot</code> and <code>predict</code> methods.
</p>


<h3>References</h3>

<p>Torsten Hothorn, Lisa Moest, Peter Buehlmann (2018), Most Likely
Transformations, <em>Scandinavian Journal of Statistics</em>, <b>45</b>(1),
110&ndash;134, <a href="https://doi.org/10.1111/sjos.12291">doi:10.1111/sjos.12291</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  data("GBSG2", package = "TH.data")
  library("survival")
  GBSG2$time &lt;- as.numeric(GBSG2$time)
  GBSG2$y &lt;- with(GBSG2, Surv(time, cens))

  ### Cox proportional hazards model
  m1 &lt;- Coxph(y ~ horTh, data = GBSG2, support = c(1, 1500))
  logLik(m1)

  ### Aalen additive hazards model with time-varying effects
  m2 &lt;- Aareg(y | horTh ~ 1, data = GBSG2, support = c(1, 1500))
  logLik(m2)

  ### compare the hazard functions
  nd &lt;- data.frame(horTh = unique(GBSG2$horTh))
  col &lt;- 1:2
  lty &lt;- 1:2
  plot(as.mlt(m1), newdata = nd, type = "hazard", 
       col = col, lty = lty[1], xlab = "time")
  plot(as.mlt(m2), newdata = nd, type = "hazard", 
       col = col, lty = 2, add = TRUE)
  legend("topright", col = rep(col, each = 2), 
         lty = rep(1:2), bty = "n",
         legend = paste(rep(paste("horTh:", 
                                  levels(nd$horTh)), each = 2),
                        rep(c("Cox", "Aalen"), 2)))

</code></pre>

<hr>
<h2 id='BoxCox'>
(Similar to) Box-Cox Models
</h2><span id='topic+BoxCox'></span>

<h3>Description</h3>

<p>Non-normal linear regression inspired by Box-Cox models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BoxCox(formula, data, subset, weights, offset, cluster, na.action = na.omit, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BoxCox_+3A_formula">formula</code></td>
<td>
<p>an object of class <code>"formula"</code>: a symbolic description 
of the model structure to be
fitted.  The details of model specification are given under
<code><a href="#topic+tram">tram</a></code> and in the package vignette.
</p>
</td></tr>
<tr><td><code id="BoxCox_+3A_data">data</code></td>
<td>
<p>an optional data frame, list or environment (or object
coercible by <code>as.data.frame</code> to a data frame) containing the
variables in the model.  If not found in <code>data</code>, the
variables are taken from <code>environment(formula)</code>.
</p>
</td></tr>
<tr><td><code id="BoxCox_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations to be
used in the fitting process.
</p>
</td></tr>
<tr><td><code id="BoxCox_+3A_weights">weights</code></td>
<td>
<p>an optional vector of weights to be used in the fitting
process.  Should be <code>NULL</code> or a numeric vector. If present,
the weighted log-likelihood is maximised.
</p>
</td></tr>
<tr><td><code id="BoxCox_+3A_offset">offset</code></td>
<td>
<p>this can be used to specify an _a priori_ known component to
be included in the linear predictor during fitting.  This
should be <code>NULL</code> or a numeric vector of length equal to the
number of cases.
</p>
</td></tr>
<tr><td><code id="BoxCox_+3A_cluster">cluster</code></td>
<td>
<p>optional factor with a cluster ID employed for computing
clustered covariances.
</p>
</td></tr>
<tr><td><code id="BoxCox_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when the data
contain <code>NA</code>s.  The default is set to <code>na.omit</code>.
</p>
</td></tr>
<tr><td><code id="BoxCox_+3A_...">...</code></td>
<td>
<p>additional arguments to <code><a href="#topic+tram">tram</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A normal model for transformed responses, where the transformation is
estimated from the data simultaneously with the regression coefficients.
This is similar to a Box-Cox transformation, but the technical details
differ. Examples can be found in the package vignette.
</p>
<p>The model is defined with a negative shift term. Large values of the
linear predictor correspond to large values of the conditional 
expectation response (but this relationship is potentially nonlinear).
</p>


<h3>Value</h3>

<p>An object of class <code>BoxCox</code>, with corresponding <code>coef</code>,
<code>vcov</code>, <code>logLik</code>, <code>estfun</code>, <code>summary</code>, 
<code>print</code>, <code>plot</code> and <code>predict</code> methods.
</p>


<h3>References</h3>

<p>Torsten Hothorn, Lisa Moest, Peter Buehlmann (2018), Most Likely
Transformations, <em>Scandinavian Journal of Statistics</em>, <b>45</b>(1),
110&ndash;134, <a href="https://doi.org/10.1111/sjos.12291">doi:10.1111/sjos.12291</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  data("BostonHousing2", package = "mlbench")

  lm(cmedv ~ crim + zn + indus + chas + nox + rm + age + dis + 
             rad + tax + ptratio + b + lstat, data = BostonHousing2)

  BoxCox(cmedv ~ chas + crim + zn + indus + nox + 
                 rm + age + dis + rad + tax + ptratio + b + lstat, 
                 data = BostonHousing2)
</code></pre>

<hr>
<h2 id='Colr'>
Continuous Outcome Logistic Regression
</h2><span id='topic+Colr'></span>

<h3>Description</h3>

<p>A proportional-odds model for continuous variables
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Colr(formula, data, subset, weights, offset, cluster, na.action = na.omit, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Colr_+3A_formula">formula</code></td>
<td>
<p>an object of class <code>"formula"</code>: a symbolic description 
of the model structure to be
fitted.  The details of model specification are given under
<code><a href="#topic+tram">tram</a></code> and in the package vignette.
</p>
</td></tr>
<tr><td><code id="Colr_+3A_data">data</code></td>
<td>
<p>an optional data frame, list or environment (or object
coercible by <code>as.data.frame</code> to a data frame) containing the
variables in the model.  If not found in <code>data</code>, the
variables are taken from <code>environment(formula)</code>.
</p>
</td></tr>
<tr><td><code id="Colr_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations to be
used in the fitting process.
</p>
</td></tr>
<tr><td><code id="Colr_+3A_weights">weights</code></td>
<td>
<p>an optional vector of weights to be used in the fitting
process.  Should be <code>NULL</code> or a numeric vector. If present,
the weighted log-likelihood is maximised.
</p>
</td></tr>
<tr><td><code id="Colr_+3A_offset">offset</code></td>
<td>
<p>this can be used to specify an _a priori_ known component to
be included in the linear predictor during fitting.  This
should be <code>NULL</code> or a numeric vector of length equal to the
number of cases.
</p>
</td></tr>
<tr><td><code id="Colr_+3A_cluster">cluster</code></td>
<td>
<p>optional factor with a cluster ID employed for computing
clustered covariances.
</p>
</td></tr>
<tr><td><code id="Colr_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when the data
contain <code>NA</code>s.  The default is set by the <code>na.action</code> setting
of <code>options</code>, and is <code>na.fail</code> if that is unset.
</p>
</td></tr>
<tr><td><code id="Colr_+3A_...">...</code></td>
<td>
<p>additional arguments to <code><a href="#topic+tram">tram</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Simultaneous estimation of all possible binary logistic models obtained 
by dichotomisation of a continuous response. The regression coefficients can
be constant allowing for an interpretation as log-odds ratios.
</p>
<p>The model is defined with a positive shift term, thus <code>exp(coef())</code> is
the multiplicative change of the odds ratio (conditional odds of treatment
or for a one unit increase in a numeric variable divided by conditional odds
of reference).  Large values of the linear predictor correspond to small
values of the conditional expectation response (but this relationship is
nonlinear).
</p>


<h3>Value</h3>

<p>An object of class <code>Colr</code>, with corresponding <code>coef</code>,
<code>vcov</code>, <code>logLik</code>, <code>estfun</code>, <code>summary</code>, 
<code>print</code>, <code>plot</code> and <code>predict</code> methods.
</p>


<h3>References</h3>

<p>Tina Lohse, Sabine Rohrmann, David Faeh and Torsten Hothorn (2017),
Continuous Outcome Logistic Regression for Analyzing Body Mass Index
Distributions, <em>F1000Research</em>, <b>6</b>(1933),
<a href="https://doi.org/10.12688/f1000research.12934.1">doi:10.12688/f1000research.12934.1</a>.
</p>
<p>Torsten Hothorn, Lisa Moest, Peter Buehlmann (2018), Most Likely
Transformations, <em>Scandinavian Journal of Statistics</em>, <b>45</b>(1),
110&ndash;134, <a href="https://doi.org/10.1111/sjos.12291">doi:10.1111/sjos.12291</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  data("BostonHousing2", package = "mlbench")

  lm(cmedv ~ crim + zn + indus + chas + nox + rm + age + dis + 
             rad + tax + ptratio + b + lstat, data = BostonHousing2)

  Colr(cmedv ~ chas + crim + zn + indus + nox + 
               rm + age + dis + rad + tax + ptratio + b + lstat, 
               data = BostonHousing2)
</code></pre>

<hr>
<h2 id='Compris'>
Competing Risk Regression
</h2><span id='topic+Compris'></span>

<h3>Description</h3>

<p>An alternative approach to competing risk regression via multivariate
transformation models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Compris(formula, data, subset, weights, na.action, offset, 
        primary = c("Coxph", "Colr", "BoxCox"), 
        competing = switch(primary, Coxph = "weibull", 
                                    Colr = "loglogistic", 
                                    BoxCox = "lognormal"), 
        NPlogLik = FALSE,
        optim = mmltoptim(), args = list(seed = 1, M = 1000), 
        scale = FALSE, tol = 0.001, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Compris_+3A_formula">formula</code></td>
<td>
<p>an object of class <code>"formula"</code>: a symbolic description 
of the model structure to be
fitted.  The details of model specification are given under
Details and in the package vignette.
</p>
</td></tr>
<tr><td><code id="Compris_+3A_data">data</code></td>
<td>
<p>an optional data frame, list or environment (or object
coercible by <code>as.data.frame</code> to a data frame) containing the
variables in the model.  If not found in <code>data</code>, the
variables are taken from <code>environment(formula)</code>.
</p>
</td></tr>
<tr><td><code id="Compris_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations to be
used in the fitting process.
</p>
</td></tr>
<tr><td><code id="Compris_+3A_weights">weights</code></td>
<td>
<p>an optional vector of case weights to be used in the fitting
process.  Should be <code>NULL</code> or a numeric vector. If present,
the weighted log-likelihood is maximised.
</p>
</td></tr>
<tr><td><code id="Compris_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when the data
contain <code>NA</code>s.  The default is set to <code>na.omit</code>.
</p>
</td></tr>
<tr><td><code id="Compris_+3A_offset">offset</code></td>
<td>
<p>this can be used to specify an _a priori_ known component to
be included in the linear predictor during fitting.  This
should be <code>NULL</code> or a numeric vector of length equal to the
number of cases.
</p>
</td></tr>
<tr><td><code id="Compris_+3A_primary">primary</code></td>
<td>
<p>a character defining the marginal model for the primary
event of interest, that is, the first status level.
</p>
</td></tr>
<tr><td><code id="Compris_+3A_competing">competing</code></td>
<td>
<p>a character defining the marginal models for the
remaining competing events.
</p>
</td></tr>
<tr><td><code id="Compris_+3A_nploglik">NPlogLik</code></td>
<td>
<p>logical, optimise nonparametric likelihood defined in
terms of multivariate probabilities.</p>
</td></tr>
<tr><td><code id="Compris_+3A_optim">optim</code></td>
<td>
<p>see <code>mmlt</code>.
</p>
</td></tr>
<tr><td><code id="Compris_+3A_args">args</code></td>
<td>
<p>a list of arguments for <code><a href="mvtnorm.html#topic+lpmvnorm">lpmvnorm</a></code>.
</p>
</td></tr>
<tr><td><code id="Compris_+3A_scale">scale</code></td>
<td>
<p>logical defining if variables in the linear predictor shall
be scaled. Scaling is internally used for model estimation,
rescaled coefficients are reported in model output.
</p>
</td></tr>
<tr><td><code id="Compris_+3A_tol">tol</code></td>
<td>
<p>a tolerance for faking interval censoring.
</p>
</td></tr>
<tr><td><code id="Compris_+3A_...">...</code></td>
<td>
<p>addition arguments.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a highly experimental approach to an alternative competing risk
regression framework described by Czado and Van Keilegom (2023) and Deresa
and Van Keilegom (2023).
</p>


<h3>Value</h3>

<p>An object of class <code>mmlt</code>, allowing to derive marginal time-to-event
distributions for the primary event of interest and all competing events.
</p>


<h3>References</h3>

<p>Claudia Czado and Ingrid Van Keilegom (2023). Dependent Censoring Based on Parametric Copulas.
<em>Biometrika</em>, <b>110</b>(3), 721&ndash;738, <a href="https://doi.org/10.1093/biomet/asac067">doi:10.1093/biomet/asac067</a>.
</p>
<p>Negera Wakgari Deresa and Ingrid Van Keilegom (2023).  Copula Based Cox Proportional Hazards 
Models for Dependent Censoring. <em>Journal of the American Statistical
Association</em>, <a href="https://doi.org/10.1080/01621459.2022.2161387">doi:10.1080/01621459.2022.2161387</a>.
</p>

<hr>
<h2 id='Coxph'>
Cox Proportional Hazards Model
</h2><span id='topic+Coxph'></span>

<h3>Description</h3>

<p>Cox model with fully parameterised baseline hazard function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Coxph(formula, data, subset, weights, offset, cluster, na.action = na.omit, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Coxph_+3A_formula">formula</code></td>
<td>
<p>an object of class <code>"formula"</code>: a symbolic description 
of the model structure to be
fitted.  The details of model specification are given under
<code><a href="#topic+tram">tram</a></code> and in the package vignette.
</p>
</td></tr>
<tr><td><code id="Coxph_+3A_data">data</code></td>
<td>
<p>an optional data frame, list or environment (or object
coercible by <code>as.data.frame</code> to a data frame) containing the
variables in the model.  If not found in <code>data</code>, the
variables are taken from <code>environment(formula)</code>.
</p>
</td></tr>
<tr><td><code id="Coxph_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations to be
used in the fitting process.
</p>
</td></tr>
<tr><td><code id="Coxph_+3A_weights">weights</code></td>
<td>
<p>an optional vector of weights to be used in the fitting
process.  Should be <code>NULL</code> or a numeric vector. If present,
the weighted log-likelihood is maximised.
</p>
</td></tr>
<tr><td><code id="Coxph_+3A_offset">offset</code></td>
<td>
<p>this can be used to specify an _a priori_ known component to
be included in the linear predictor during fitting.  This
should be <code>NULL</code> or a numeric vector of length equal to the
number of cases.
</p>
</td></tr>
<tr><td><code id="Coxph_+3A_cluster">cluster</code></td>
<td>
<p>optional factor with a cluster ID employed for computing
clustered covariances.
</p>
</td></tr>
<tr><td><code id="Coxph_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when the data
contain <code>NA</code>s.  The default is set to <code>na.omit</code>.
</p>
</td></tr>
<tr><td><code id="Coxph_+3A_...">...</code></td>
<td>
<p>additional arguments to <code><a href="#topic+tram">tram</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The original implementation of Cox models via the partial likelihood,
treating the baseline hazard function as a nuisance parameter, is available
in <code><a href="survival.html#topic+coxph">coxph</a></code>. This function allows simultaneous
estimation of the log-hazard ratios and the log-cumulative baseline hazard,
the latter parameterised by a Bernstein polynomial. The model can be fitted
under stratification (time-varying coefficients), all types of random
censoring and trunction. An early reference to this parameterisation is
McLain and Ghosh (2013).
</p>
<p>The response is bounded (<code>bounds = c(0, Inf)</code>) when specified as a
<code>Surv</code> object. Otherwise, <code>bounds</code> can be specified via
<code>...</code>.
</p>
<p>Parameters are log-hazard ratios comparing treatment (or a one unit increase
in a numeric variable) with a reference.
</p>


<h3>Value</h3>

<p>An object of class <code>Coxph</code>, with corresponding <code>coef</code>,
<code>vcov</code>, <code>logLik</code>, <code>estfun</code>, <code>summary</code>, 
<code>print</code>, <code>plot</code> and <code>predict</code> methods.
</p>


<h3>References</h3>

<p>Alexander C. McLain and Sujit K. Ghosh (2013).
Efficient Sieve Maximum Likelihood Estimation of
Time-Transformation Models, <em>Journal of Statistical Theory and Practice</em>,
<b>7</b>(2), 285&ndash;303, <a href="https://doi.org/10.1080/15598608.2013.772835">doi:10.1080/15598608.2013.772835</a>.
</p>
<p>Torsten Hothorn, Lisa Moest, Peter Buehlmann (2018), Most Likely
Transformations, <em>Scandinavian Journal of Statistics</em>, <b>45</b>(1),
110&ndash;134, <a href="https://doi.org/10.1111/sjos.12291">doi:10.1111/sjos.12291</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  data("GBSG2", package = "TH.data")

  library("survival")
  (m1 &lt;- coxph(Surv(time, cens) ~ horTh, data = GBSG2))

  (m2 &lt;- Coxph(Surv(time, cens) ~ horTh, data = GBSG2))

  ### McLain &amp; Ghosh (2013)
  (m3 &lt;- Coxph(Surv(time, cens) ~ horTh, data = GBSG2, 
               frailty = "Gamma"))

  ### Wald intervals
  confint(m1)
  confint(m2)
  ### profile likelihood interval
  confint(profile(m2))
  ### score interval
  confint(score_test(m2))
  ### permutation score interval; uses permutation distribution
  ### see coin::independence_test
  ## Not run: confint(perm_test(m2))

</code></pre>

<hr>
<h2 id='Lehmann'>
Proportional Reverse Time Hazards Linear Regression
</h2><span id='topic+Lehmann'></span>

<h3>Description</h3>

<p>Non-normal linear regression for Lehmann-alternatives
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Lehmann(formula, data, subset, weights, offset, cluster, na.action = na.omit, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Lehmann_+3A_formula">formula</code></td>
<td>
<p>an object of class <code>"formula"</code>: a symbolic description 
of the model structure to be
fitted.  The details of model specification are given under
<code><a href="#topic+tram">tram</a></code> and in the package vignette.
</p>
</td></tr>
<tr><td><code id="Lehmann_+3A_data">data</code></td>
<td>
<p>an optional data frame, list or environment (or object
coercible by <code>as.data.frame</code> to a data frame) containing the
variables in the model.  If not found in <code>data</code>, the
variables are taken from <code>environment(formula)</code>.
</p>
</td></tr>
<tr><td><code id="Lehmann_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations to be
used in the fitting process.
</p>
</td></tr>
<tr><td><code id="Lehmann_+3A_weights">weights</code></td>
<td>
<p>an optional vector of weights to be used in the fitting
process.  Should be <code>NULL</code> or a numeric vector. If present,
the weighted log-likelihood is maximised.
</p>
</td></tr>
<tr><td><code id="Lehmann_+3A_offset">offset</code></td>
<td>
<p>this can be used to specify an _a priori_ known component to
be included in the linear predictor during fitting.  This
should be <code>NULL</code> or a numeric vector of length equal to the
number of cases.
</p>
</td></tr>
<tr><td><code id="Lehmann_+3A_cluster">cluster</code></td>
<td>
<p>optional factor with a cluster ID employed for computing
clustered covariances.
</p>
</td></tr>
<tr><td><code id="Lehmann_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when the data
contain <code>NA</code>s.  The default is set to <code>na.omit</code>.
</p>
</td></tr>
<tr><td><code id="Lehmann_+3A_...">...</code></td>
<td>
<p>additional arguments to <code><a href="#topic+tram">tram</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This transformation model uses the cumulative distribution function for the
standard Gumbel maximum extreme value distribution to map the shifted
transformation function into probabilities. The exponential of the shift
paramater can be interpreted as a Lehmann-alternative or 
reverse time hazard ratio.
</p>


<h3>Value</h3>

<p>An object of class <code>Lehmann</code>, with corresponding <code>coef</code>,
<code>vcov</code>, <code>logLik</code>, <code>estfun</code>, <code>summary</code>, 
<code>print</code>, <code>plot</code> and <code>predict</code> methods.
</p>


<h3>References</h3>

<p>Erich L. Lehmann (1953), The Power of Rank Tests,
<em>The Annals of Mathematical Statistics</em>, <b>24</b>(1),
23-43.
</p>
<p>Torsten Hothorn, Lisa Moest, Peter Buehlmann (2018), Most Likely
Transformations, <em>Scandinavian Journal of Statistics</em>, <b>45</b>(1),
110&ndash;134, <a href="https://doi.org/10.1111/sjos.12291">doi:10.1111/sjos.12291</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  data("BostonHousing2", package = "mlbench")

  lm(cmedv ~ crim + zn + indus + chas + nox + rm + age + dis + 
             rad + tax + ptratio + b + lstat, data = BostonHousing2)

  Lehmann(cmedv ~ chas + crim + zn + indus + nox + 
                  rm + age + dis + rad + tax + ptratio + b + lstat, 
                  data = BostonHousing2)
</code></pre>

<hr>
<h2 id='Lm'>
Normal Linear Model
</h2><span id='topic+Lm'></span>

<h3>Description</h3>

<p>Normal linear model with benefits
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Lm(formula, data, subset, weights, offset, cluster, na.action = na.omit, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Lm_+3A_formula">formula</code></td>
<td>
<p>an object of class <code>"formula"</code>: a symbolic description 
of the model structure to be
fitted.  The details of model specification are given under
<code><a href="#topic+tram">tram</a></code> and in the package vignette.
</p>
</td></tr>
<tr><td><code id="Lm_+3A_data">data</code></td>
<td>
<p>an optional data frame, list or environment (or object
coercible by <code>as.data.frame</code> to a data frame) containing the
variables in the model.  If not found in <code>data</code>, the
variables are taken from <code>environment(formula)</code>.
</p>
</td></tr>
<tr><td><code id="Lm_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations to be
used in the fitting process.
</p>
</td></tr>
<tr><td><code id="Lm_+3A_weights">weights</code></td>
<td>
<p>an optional vector of weights to be used in the fitting
process.  Should be <code>NULL</code> or a numeric vector. If present,
the weighted log-likelihood is maximised.
</p>
</td></tr>
<tr><td><code id="Lm_+3A_offset">offset</code></td>
<td>
<p>this can be used to specify an _a priori_ known component to
be included in the linear predictor during fitting.  This
should be <code>NULL</code> or a numeric vector of length equal to the
number of cases.
</p>
</td></tr>
<tr><td><code id="Lm_+3A_cluster">cluster</code></td>
<td>
<p>optional factor with a cluster ID employed for computing
clustered covariances.
</p>
</td></tr>
<tr><td><code id="Lm_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when the data
contain <code>NA</code>s.  The default is set to <code>na.omit</code>.
</p>
</td></tr>
<tr><td><code id="Lm_+3A_...">...</code></td>
<td>
<p>additional arguments to <code><a href="#topic+tram">tram</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A normal linear model with simulaneous estimation of regression coefficients
and scale parameter(s). This function also allows for stratum-specific
intercepts and variances as well as censoring and truncation in the
response.
</p>
<p>Note that the scale of the parameters is different from what is reported by
<code><a href="stats.html#topic+lm">lm</a></code>; the discrepancies are explained in the package
vignette.
</p>
<p>The model is defined with a negative shift term. Large values of the
linear predictor correspond to large values of the conditional 
expectation response.
</p>


<h3>Value</h3>

<p>An object of class <code>Lm</code>, with corresponding <code>coef</code>,
<code>vcov</code>, <code>logLik</code>, <code>estfun</code>, <code>summary</code>, 
<code>print</code>, <code>plot</code> and <code>predict</code> methods.
</p>


<h3>References</h3>

<p>Torsten Hothorn, Lisa Moest, Peter Buehlmann (2018), Most Likely
Transformations, <em>Scandinavian Journal of Statistics</em>, <b>45</b>(1),
110&ndash;134, <a href="https://doi.org/10.1111/sjos.12291">doi:10.1111/sjos.12291</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  data("BostonHousing2", package = "mlbench")

  lm(cmedv ~ crim + zn + indus + chas + nox + rm + age + dis + 
             rad + tax + ptratio + b + lstat, data = BostonHousing2)

  Lm(cmedv ~ chas + crim + zn + indus + nox + 
             rm + age + dis + rad + tax + ptratio + b + lstat, 
             data = BostonHousing2)
</code></pre>

<hr>
<h2 id='mmlt'>
Multivariate Conditional Transformation Models
</h2><span id='topic+mmlt'></span><span id='topic+mmltoptim'></span><span id='topic+coef.cmmlt'></span><span id='topic+coef.mmmlt'></span><span id='topic+predict.mmlt'></span><span id='topic+simulate.mmlt'></span>

<h3>Description</h3>

<p>Conditional transformation models for multivariate continuous, discrete,
or a mix of continuous and discrete outcomes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mmlt(..., formula = ~ 1, data, conditional = FALSE, theta = NULL, fixed = NULL,
     scale = FALSE, optim = mmltoptim(), args = list(seed = 1, M = 1000), 
     dofit = TRUE, domargins = TRUE)
mmltoptim(auglag = list(maxtry = 5), ...)
## S3 method for class 'cmmlt'
coef(object, newdata, 
     type = c("all", "conditional", "Lambda", "Lambdainv", 
              "Precision", "PartialCorr", "Sigma", "Corr", 
              "Spearman", "Kendall"), 
     ...)
## S3 method for class 'mmmlt'
coef(object, newdata, 
     type = c("all", "marginal", "Lambda", "Lambdainv", 
              "Precision", "PartialCorr", "Sigma", "Corr", 
              "Spearman", "Kendall"), 
     ...)
## S3 method for class 'mmlt'
predict(object, newdata, margins = 1:J, 
        type = c("trafo", "distribution", "survivor", "density", "hazard"), 
                 log = FALSE, args = object$args, ...)
## S3 method for class 'mmlt'
simulate(object, nsim = 1L, seed = NULL, newdata, K = 50, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mmlt_+3A_...">...</code></td>
<td>
<p>marginal transformation models, one for each response, for
<code>mmlt</code>. Additional arguments for the methods.
</p>
</td></tr>
<tr><td><code id="mmlt_+3A_formula">formula</code></td>
<td>
<p>a model formula describing a model for the dependency
structure via the lambda parameters. The default is set to <code>~ 1</code> for constant lambdas.
</p>
</td></tr>
<tr><td><code id="mmlt_+3A_data">data</code></td>
<td>
<p>a data.frame.
</p>
</td></tr>
<tr><td><code id="mmlt_+3A_conditional">conditional</code></td>
<td>
<p>logical; parameters are defined conditionally (only
possible when all models are probit models). This is the default as
described by Klein et al. (2022). If <code>FALSE</code>, parameters can be
directly interpreted marginally, this is explained in Section 2.6 by Klein
et al. (2022). Using <code>conditional = FALSE</code> with probit-only models
gives the same likelihood but different parameter estimates.
</p>
</td></tr>
<tr><td><code id="mmlt_+3A_theta">theta</code></td>
<td>
<p>an optional vector of starting values.
</p>
</td></tr>
<tr><td><code id="mmlt_+3A_fixed">fixed</code></td>
<td>
<p>an optional named numeric vector of predefined parameter values.
</p>
</td></tr>
<tr><td><code id="mmlt_+3A_scale">scale</code></td>
<td>
<p>a logical indicating if (internal) scaling shall be applied
to the model coefficients.
</p>
</td></tr>
<tr><td><code id="mmlt_+3A_optim">optim</code></td>
<td>
<p>a list of optimisers as returned by <code><a href="#topic+mmltoptim">mmltoptim</a></code>
</p>
</td></tr>
<tr><td><code id="mmlt_+3A_args">args</code></td>
<td>
<p>a list of arguments for <code><a href="mvtnorm.html#topic+lpmvnorm">lpmvnorm</a></code>.</p>
</td></tr>
<tr><td><code id="mmlt_+3A_dofit">dofit</code></td>
<td>
<p>logical; parameters are fitted by default, otherwise a list
with log-likelihood and score function is returned.
</p>
</td></tr>
<tr><td><code id="mmlt_+3A_domargins">domargins</code></td>
<td>
<p>logical; all model parameters are fitted by default, 
including the parameters of marginal models.
</p>
</td></tr>
<tr><td><code id="mmlt_+3A_auglag">auglag</code></td>
<td>
<p>a list of arguments to <code><a href="alabama.html#topic+auglag">auglag</a></code> ensuring
a numerically determined Hessian is returned.</p>
</td></tr>
<tr><td><code id="mmlt_+3A_object">object</code></td>
<td>
<p>an object of class <code>mmlt</code>.
</p>
</td></tr>
<tr><td><code id="mmlt_+3A_newdata">newdata</code></td>
<td>
<p>an optional data.frame coefficients and predictions shall
be computed for.
</p>
</td></tr>
<tr><td><code id="mmlt_+3A_type">type</code></td>
<td>
<p>type of coefficient or prediction to be returned.
</p>
</td></tr>
<tr><td><code id="mmlt_+3A_margins">margins</code></td>
<td>
<p>indices defining marginal models to be evaluated. Can be
single integers giving the marginal distribution of the corresponding
variable, or multiple integers (currently only <code>1:j</code> implemented).
</p>
</td></tr>
<tr><td><code id="mmlt_+3A_log">log</code></td>
<td>
<p>logical; return log-probabilities or log-densities of
<code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="mmlt_+3A_nsim">nsim</code></td>
<td>
<p>number of samples to generate.</p>
</td></tr>
<tr><td><code id="mmlt_+3A_seed">seed</code></td>
<td>
<p>optional seed for the random number generator.</p>
</td></tr>
<tr><td><code id="mmlt_+3A_k">K</code></td>
<td>
<p>number of grid points to generate.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function implements multivariate conditional transformation models
as described by Klein et al (2020). 
Below is a simple example for an unconditional bivariate distribution.
See <code>demo("undernutrition", package = "tram")</code> for a conditional
three-variate example.
</p>


<h3>Value</h3>

<p>An object of class <code>mmlt</code> with <code>coef</code> and <code>predict</code>
methods.
</p>


<h3>References</h3>

<p>Nadja Klein, Torsten Hothorn, Luisa Barbanti, Thomas Kneib (2022),
Multivariate Conditional Transformation Models. <em>Scandinavian Journal
of Statistics</em>, <b>49</b>, 116&ndash;142, <a href="https://doi.org/10.1111/sjos.12501">doi:10.1111/sjos.12501</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  data("cars")

  ### fit unconditional bivariate distribution of speed and distance to stop
  ## fit unconditional marginal transformation models
  m_speed &lt;- BoxCox(speed ~ 1, data = cars, support = ss &lt;- c(4, 25), 
                    add = c(-5, 5))
  m_dist &lt;- BoxCox(dist ~ 1, data = cars, support = sd &lt;- c(0, 120), 
                   add = c(-5, 5))

  ## fit multivariate unconditional transformation model
  m_speed_dist &lt;- mmlt(m_speed, m_dist, formula = ~ 1, data = cars)

  ## log-likelihood
  logLik(m_speed_dist)
  sum(predict(m_speed_dist, newdata = cars, type = "density", log = TRUE))

  ## Wald test of independence of speed and dist (the "dist.sped.(Intercept)"
  ## coefficient)
  summary(m_speed_dist)

  ## LR test comparing to independence model
  LR &lt;- 2 * (logLik(m_speed_dist) - logLik(m_speed) - logLik(m_dist))
  pchisq(LR, df = 1, lower.tail = FALSE)

  ## constrain lambda to zero and fit independence model
  ## =&gt; log-likelihood is the sum of the marginal log-likelihoods
  mI &lt;- mmlt(m_speed, m_dist, formula = ~1, data = cars, 
             fixed = c("dist.sped.(Intercept)" = 0))
  logLik(m_speed) + logLik(m_dist)
  logLik(mI)

  ## linear correlation, ie Pearson correlation of speed and dist after
  ## transformation to bivariate normality
  (r &lt;- coef(m_speed_dist, type = "Corr"))
  
  ## Spearman's rho (rank correlation) of speed and dist on original scale
  (rs &lt;- coef(m_speed_dist, type = "Spearman"))

  ## evaluate joint and marginal densities (needs to be more user-friendly)
  nd &lt;- expand.grid(c(nd_s &lt;- mkgrid(m_speed, 100), nd_d &lt;- mkgrid(m_dist, 100)))
  nd$d &lt;- predict(m_speed_dist, newdata = nd, type = "density")

  ## compute marginal densities
  nd_s &lt;- as.data.frame(nd_s)
  nd_s$d &lt;- predict(m_speed_dist, newdata = nd_s, margins = 1L,
                    type = "density")
  nd_d &lt;- as.data.frame(nd_d)
  nd_d$d &lt;- predict(m_speed_dist, newdata = nd_d, margins = 2L, 
                    type = "density")

  ## plot bivariate and marginal distribution
  col1 &lt;- rgb(.1, .1, .1, .9)
  col2 &lt;- rgb(.1, .1, .1, .5)
  w &lt;- c(.8, .2)
  layout(matrix(c(2, 1, 4, 3), nrow = 2), width = w, height = rev(w))
  par(mai = c(1, 1, 0, 0) * par("mai"))
  sp &lt;- unique(nd$speed)
  di &lt;- unique(nd$dist)
  d &lt;- matrix(nd$d, nrow = length(sp))
  contour(sp, di, d, xlab = "Speed (in mph)", ylab = "Distance (in ft)", xlim = ss, ylim = sd,
          col = col1)
  points(cars$speed, cars$dist, pch = 19, col = col2)
  mai &lt;- par("mai")
  par(mai = c(0, 1, 0, 1) * mai)
  plot(d ~ speed, data = nd_s, xlim = ss, type = "n", axes = FALSE, 
       xlab = "", ylab = "")
  polygon(nd_s$speed, nd_s$d, col = col2, border = FALSE)
  par(mai = c(1, 0, 1, 0) * mai)
  plot(dist ~ d, data = nd_d, ylim = sd, type = "n", axes = FALSE, 
       xlab = "", ylab = "")
  polygon(nd_d$d, nd_d$dist, col = col2, border = FALSE)

  ### NOTE: marginal densities are NOT normal, nor is the joint
  ### distribution. The non-normal shape comes from the data-driven 
  ### transformation of both variables to joint normality in this model.

</code></pre>

<hr>
<h2 id='mtram'>
Transformation Models for Clustered Data
</h2><span id='topic+mtram'></span>

<h3>Description</h3>

<p>Marginally interpretable transformation models for clustered data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mtram(object, formula, data,
      grd = SparseGrid::createSparseGrid(type = "KPU", 
                dimension = length(rt$cnms[[1]]), k = 10), 
      Hessian = FALSE,  tol = .Machine$double.eps, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mtram_+3A_object">object</code></td>
<td>

<p>A <code>tram</code> object.
</p>
</td></tr>
<tr><td><code id="mtram_+3A_formula">formula</code></td>
<td>

<p>A formula specifying the random effects.
</p>
</td></tr>
<tr><td><code id="mtram_+3A_data">data</code></td>
<td>

<p>A data frame. 
</p>
</td></tr>
<tr><td><code id="mtram_+3A_grd">grd</code></td>
<td>

<p>A sparse grid used for numerical integration to get the likelihood.
</p>
</td></tr>
<tr><td><code id="mtram_+3A_hessian">Hessian</code></td>
<td>

<p>A logical, if <code>TRUE</code>, the hessian is computed and returned.
</p>
</td></tr>
<tr><td><code id="mtram_+3A_tol">tol</code></td>
<td>
<p>numerical tolerance.</p>
</td></tr>
<tr><td><code id="mtram_+3A_...">...</code></td>
<td>

<p>Additional argument.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A Gaussian copula with a correlation structure obtained from a random
intercept or random intercept / random slope model (that is, clustered or
longitudinal data can by modelled only) is used to capture the
correlations whereas the marginal distributions are described by a 
transformation model. The methodology is described in Barbanti and Hothorn
(2022) and examples are given in the <code>mtram</code> package vignette.
</p>
<p>Only <code>coef()</code> and <code>logLik()</code> methods are available at the
moment, see <code>vignette("mtram", package = "tram")</code> for worked
examples.
</p>


<h3>Value</h3>

<p>An object of class <code>tram</code> with <code>coef()</code> and <code>logLik()</code>
methods.
</p>


<h3>References</h3>

<p>Luisa Barbanti and Torsten Hothorn (2023). A Transformation Perspective on 
Marginal and Conditional Models, <em>Biostatistics</em>, <a href="https://doi.org/10.1093/biostatistics/kxac048">doi:10.1093/biostatistics/kxac048</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  ### For illustrations see
  ## vignette("mtram", package = "tram")
  ## or
  ## demo("mtram", package = "tram")

</code></pre>

<hr>
<h2 id='perm_test'>
Permutation Transformation Tests 
</h2><span id='topic+perm_test'></span><span id='topic+perm_test.tram'></span>

<h3>Description</h3>

<p>P-values for a parameter in a linear transformation
model and corresponding confidence intervals 
obtained from by the permutation principle
</p>


<h3>Usage</h3>

<pre><code class='language-R'>perm_test(object, ...)
## S3 method for class 'tram'
perm_test(object, parm = names(coef(object)), 
    statistic = c("Score", "Likelihood", "Wald"),
    alternative = c("two.sided", "less", "greater"), 
    nullvalue = 0, confint = TRUE, level = .95, 
    Taylor = FALSE, block_permutation = TRUE, maxsteps = 25, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="perm_test_+3A_object">object</code></td>
<td>
<p>an object of class <code><a href="#topic+tram">tram</a></code></p>
</td></tr>
<tr><td><code id="perm_test_+3A_parm">parm</code></td>
<td>
<p>a vector of names of parameters to be tested.
These parameters must be present in <code>object</code>.</p>
</td></tr>
<tr><td><code id="perm_test_+3A_statistic">statistic</code></td>
<td>
<p>a character string specifying the statistic to be
permuted. The default <code>Score</code> is the classical permutation
test for the esiduals of a model excluding the parameter <code>parm</code>.
Only available for <code>nullvalue = 0</code>, confidence intervals are
not available. Permuting the likelihood or the model coefficients
under the nullvalue is highly expermimental as are the
corresponding confidence intervals.</p>
</td></tr>
<tr><td><code id="perm_test_+3A_alternative">alternative</code></td>
<td>
<p>a character string specifying the alternative hypothesis,
must be one of <code>"two.sided"</code> (default), <code>"greater"</code> 
or <code>"less"</code>.</p>
</td></tr>
<tr><td><code id="perm_test_+3A_nullvalue">nullvalue</code></td>
<td>
<p>a number specifying an optional parameter used to form the
null hypothesis.</p>
</td></tr>
<tr><td><code id="perm_test_+3A_confint">confint</code></td>
<td>
<p>a logical indicating whether a confidence interval should be
computed. Score confidence intervals are computed by default. A
1st order Taylor approximation to the Score statistc is used with
<code>Taylor = TRUE</code> (in case numerical inversion of the score
statistic fails, Wald-type confidence intervals relying from this approximation are
returned) . For the remaining likelihood and Wald statistics, confidence
intervals are highly experimental (and probably not worth looking
at).</p>
</td></tr>
<tr><td><code id="perm_test_+3A_level">level</code></td>
<td>
<p>the confidence level.</p>
</td></tr>
<tr><td><code id="perm_test_+3A_block_permutation">block_permutation</code></td>
<td>
<p>a logical indicating wheather stratifying
variables shall be interpreted as blocks defining admissible
permutations.</p>
</td></tr>
<tr><td><code id="perm_test_+3A_taylor">Taylor</code></td>
<td>
<p>a logical requesting the use of a 1st order Taylor
approximation when inverting the score statistic.</p>
</td></tr>
<tr><td><code id="perm_test_+3A_maxsteps">maxsteps</code></td>
<td>
<p>number of function evaluations when inverting the score statistic
for computing confidence intervals.</p>
</td></tr>
<tr><td><code id="perm_test_+3A_...">...</code></td>
<td>
<p>additional arguments to <code><a href="coin.html#topic+independence_test">independence_test</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Permutation test for one single parameters in the linear
predictor of <code>object</code> is computed. This parameters must be present
in <code>object</code>. This is somewhat experimental and not recommended for
serious practical use (yet!).
</p>


<h3>Value</h3>

<p>An object of class <code>htest</code> or a list thereof. See <code><a href="#topic+Coxph">Coxph</a></code>
for an example.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  ## Tritiated Water Diffusion Across Human Chorioamnion
  ## Hollander and Wolfe (1999, p. 110, Tab. 4.1)
  diffusion &lt;- data.frame(
      pd = c(0.80, 0.83, 1.89, 1.04, 1.45, 1.38, 1.91, 1.64, 0.73, 1.46,
             1.15, 0.88, 0.90, 0.74, 1.21),
      age = factor(rep(c("At term", "12-26 Weeks"), c(10, 5)))
  )

  ### plot the two quantile functions
  boxplot(pd ~ age, data = diffusion)

  ### the Wilcoxon rank sum test, with a confidence interval
  ### for a median shift
  wilcox.test(pd ~ age, data = diffusion, conf.int = TRUE, exact = TRUE)

  ### a corresponding parametric transformation model with a log-odds ratio
  ### difference parameter, ie a difference on the log-odds scale
  md &lt;- Colr(pd ~ age, data = diffusion)

  ### assess model fit by plotting estimated distribution fcts
  agef &lt;- sort(unique(diffusion$age))
  col &lt;- c("black", "darkred")
  plot(as.mlt(md), newdata = data.frame(age = agef),
       type = "distribution", col = col)
  legend("bottomright", col = col, lty = 1, legend = levels(agef), 
         bty = "n", pch = 19)
  ## compare with ECDFs: not too bad (but not good, either)
  npfit &lt;- with(diffusion, tapply(pd, age, ecdf))
  lines(npfit[[1]], col = col[1])
  lines(npfit[[2]], col = col[2])

  ### Wald confidence interval
  confint(md)

  ### Likelihood confidence interval
  confint(profile(md))

  ### Score confidence interval
  confint(score_test(md))
  confint(score_test(md, Taylor = TRUE))

  ### exact permutation score test
  (pt &lt;- perm_test(md, confint = TRUE, distribution = "exact"))
  (pt &lt;- perm_test(md, confint = TRUE, distribution = "exact", 
                   Taylor = TRUE))

  ### compare with probabilistic indices obtained from asht::wmwTest
  if (require("asht", warn.conflicts = FALSE)) {
      print(wt2 &lt;- wmwTest(pd ~ I(relevel(age, "At term")), 
                      data = diffusion, method = "exact.ce"))
      ### as log-odds ratios
      print(PI(prob = wt2$conf.int))
      print(PI(prob = wt2$estimate))
  }
</code></pre>

<hr>
<h2 id='Polr'>
Ordered Categorical Regression
</h2><span id='topic+Polr'></span>

<h3>Description</h3>

<p>Some regression models for ordered categorical responses
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Polr(formula, data, subset, weights, offset, cluster, na.action = na.omit, 
     method = c("logistic", "probit", "loglog", "cloglog", "cauchit"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Polr_+3A_formula">formula</code></td>
<td>
<p>an object of class <code>"formula"</code>: a symbolic description 
of the model structure to be
fitted.  The details of model specification are given under
<code><a href="#topic+tram">tram</a></code> and in the package vignette.
</p>
</td></tr>
<tr><td><code id="Polr_+3A_data">data</code></td>
<td>
<p>an optional data frame, list or environment (or object
coercible by <code>as.data.frame</code> to a data frame) containing the
variables in the model.  If not found in <code>data</code>, the
variables are taken from <code>environment(formula)</code>.
</p>
</td></tr>
<tr><td><code id="Polr_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations to be
used in the fitting process.
</p>
</td></tr>
<tr><td><code id="Polr_+3A_weights">weights</code></td>
<td>
<p>an optional vector of weights to be used in the fitting
process.  Should be <code>NULL</code> or a numeric vector. If present,
the weighted log-likelihood is maximised.
</p>
</td></tr>
<tr><td><code id="Polr_+3A_offset">offset</code></td>
<td>
<p>this can be used to specify an _a priori_ known component to
be included in the linear predictor during fitting.  This
should be <code>NULL</code> or a numeric vector of length equal to the
number of cases.
</p>
</td></tr>
<tr><td><code id="Polr_+3A_cluster">cluster</code></td>
<td>
<p>optional factor with a cluster ID employed for computing
clustered covariances.
</p>
</td></tr>
<tr><td><code id="Polr_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when the data
contain <code>NA</code>s.  The default is set by the <code>na.action</code> setting
of <code>options</code>, and is <code>na.fail</code> if that is unset.
</p>
</td></tr>
<tr><td><code id="Polr_+3A_method">method</code></td>
<td>
<p>a character describing the link function.
</p>
</td></tr>
<tr><td><code id="Polr_+3A_...">...</code></td>
<td>
<p>additional arguments to <code><a href="#topic+tram">tram</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Models for ordered categorical responses reusing the interface of
<code><a href="MASS.html#topic+polr">polr</a></code>. Allows for stratification, censoring and
trunction.
</p>
<p>The model is defined with a negative shift term, thus <code>exp(coef())</code>
is the multiplicative change of the odds ratio (conditional odds for 
reference divided by conditional odds of treatment or for a one unit
increase in a numeric variable). Large values of the
linear predictor correspond to large values of the conditional 
expectation response (but this relationship is nonlinear).
</p>


<h3>Value</h3>

<p>An object of class <code>Polr</code>, with corresponding <code>coef</code>,
<code>vcov</code>, <code>logLik</code>, <code>estfun</code>, <code>summary</code>, 
<code>print</code>, <code>plot</code> and <code>predict</code> methods.
</p>


<h3>References</h3>

<p>Torsten Hothorn, Lisa Moest, Peter Buehlmann (2018), Most Likely
Transformations, <em>Scandinavian Journal of Statistics</em>, <b>45</b>(1),
110&ndash;134, <a href="https://doi.org/10.1111/sjos.12291">doi:10.1111/sjos.12291</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  data("wine", package = "ordinal")

  library("MASS")
  polr(rating ~ temp + contact, data = wine)

  Polr(rating ~ temp + contact, data = wine)

</code></pre>

<hr>
<h2 id='score_test'>
Transformation Score Tests and Confidence Intervals
</h2><span id='topic+score_test'></span><span id='topic+score_test.tram'></span>

<h3>Description</h3>

<p>P-values and confidence intervals for parameters in linear transformation
models obtained from by the score test principle
</p>


<h3>Usage</h3>

<pre><code class='language-R'>score_test(object, ...)
## S3 method for class 'tram'
score_test(object, parm = names(coef(object)), 
    alternative = c("two.sided", "less", "greater"), nullvalue = 0, 
    confint = TRUE, level = .95, Taylor = FALSE, maxsteps = 25, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="score_test_+3A_object">object</code></td>
<td>
<p>an object of class <code><a href="#topic+tram">tram</a></code></p>
</td></tr>
<tr><td><code id="score_test_+3A_parm">parm</code></td>
<td>
<p>a vector of names of parameters to be tested.
These parameters must be present in <code>object</code>.</p>
</td></tr>
<tr><td><code id="score_test_+3A_alternative">alternative</code></td>
<td>
<p>a character string specifying the alternative hypothesis,
must be one of <code>"two.sided"</code> (default), <code>"greater"</code> 
or <code>"less"</code>.</p>
</td></tr>
<tr><td><code id="score_test_+3A_nullvalue">nullvalue</code></td>
<td>
<p>a number specifying an optional parameter used to form the
null hypothesis.</p>
</td></tr>
<tr><td><code id="score_test_+3A_confint">confint</code></td>
<td>
<p>a logical indicating whether a confidence interval should be
computed. Score confidence intervals are computed by default. A
1st order Taylor approximation to the Score statistc is used with
<code>Taylor = TRUE</code> (in case numerical inversion of the score
statistic fails, Wald confidence intervals relying from this approximation are
returned).</p>
</td></tr>
<tr><td><code id="score_test_+3A_level">level</code></td>
<td>
<p>the confidence level.</p>
</td></tr>
<tr><td><code id="score_test_+3A_taylor">Taylor</code></td>
<td>
<p>a logical requesting the use of a 1st order Taylor
approximation when inverting the score statistic.</p>
</td></tr>
<tr><td><code id="score_test_+3A_maxsteps">maxsteps</code></td>
<td>
<p>number of function evaluations when inverting the score statistic
for computing confidence intervals.</p>
</td></tr>
<tr><td><code id="score_test_+3A_...">...</code></td>
<td>
<p>additional arguments, currently ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Score tests and confidence intervals for the parameters in the linear
predictor of <code>object</code> are computed. These parameters must be present
in <code>object</code>.
</p>


<h3>Value</h3>

<p>An object of class <code>htest</code> or a list thereof. See <code><a href="#topic+Coxph">Coxph</a></code>
for an example. A corresponding permutation test for parameters in a
transformation models is available in
<code><a href="#topic+perm_test">perm_test</a></code>.
</p>

<hr>
<h2 id='Survreg'>
Parametric Survival Models
</h2><span id='topic+Survreg'></span>

<h3>Description</h3>

<p>Weibull, log-normal, log-logistic and other parametric models (not exclusively) for survival analysis
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Survreg(formula, data, subset, weights, offset, cluster, na.action = na.omit, 
        dist = c("weibull", "logistic", "gaussian", "exponential", "rayleigh", 
                 "loggaussian", "lognormal", "loglogistic"), scale = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Survreg_+3A_formula">formula</code></td>
<td>
<p>an object of class <code>"formula"</code>: a symbolic description 
of the model structure to be
fitted.  The details of model specification are given under
<code><a href="#topic+tram">tram</a></code> and in the package vignette.
</p>
</td></tr>
<tr><td><code id="Survreg_+3A_data">data</code></td>
<td>
<p>an optional data frame, list or environment (or object
coercible by <code>as.data.frame</code> to a data frame) containing the
variables in the model.  If not found in <code>data</code>, the
variables are taken from <code>environment(formula)</code>.
</p>
</td></tr>
<tr><td><code id="Survreg_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations to be
used in the fitting process.
</p>
</td></tr>
<tr><td><code id="Survreg_+3A_weights">weights</code></td>
<td>
<p>an optional vector of weights to be used in the fitting
process.  Should be <code>NULL</code> or a numeric vector. If present,
the weighted log-likelihood is maximised.
</p>
</td></tr>
<tr><td><code id="Survreg_+3A_offset">offset</code></td>
<td>
<p>this can be used to specify an _a priori_ known component to
be included in the linear predictor during fitting.  This
should be <code>NULL</code> or a numeric vector of length equal to the
number of cases.
</p>
</td></tr>
<tr><td><code id="Survreg_+3A_cluster">cluster</code></td>
<td>
<p>optional factor with a cluster ID employed for computing
clustered covariances.
</p>
</td></tr>
<tr><td><code id="Survreg_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when the data
contain <code>NA</code>s.  The default is set by the <code>na.action</code> setting
of <code>options</code>, and is <code>na.fail</code> if that is unset.
</p>
</td></tr>
<tr><td><code id="Survreg_+3A_dist">dist</code></td>
<td>
<p>character defining the conditional distribution of the
(not necessarily positive) response, current choices include 
Weibull, logistic, normal, exponential, Rayleigh, log-normal (same as
log-gaussian), or log-logistic.
</p>
</td></tr>
<tr><td><code id="Survreg_+3A_scale">scale</code></td>
<td>
<p>a fixed value for the scale parameter(s).
</p>
</td></tr>
<tr><td><code id="Survreg_+3A_...">...</code></td>
<td>
<p>additional arguments to <code><a href="#topic+tram">tram</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Parametric survival models reusing the interface of
<code><a href="survival.html#topic+survreg">survreg</a></code>. The parameterisation is, however, a little
different, see the package vignette.
</p>
<p>The model is defined with a negative shift term. Large values of the
linear predictor correspond to large values of the conditional 
expectation response (but this relationship is nonlinear).
Parameters are log-hazard ratios comparing a reference with 
treatment (or a one unit increase in a numeric variable).
</p>


<h3>Value</h3>

<p>An object of class <code>Survreg</code>,  with corresponding <code>coef</code>,
<code>vcov</code>, <code>logLik</code>, <code>estfun</code>, <code>summary</code>, 
<code>print</code>, <code>plot</code> and <code>predict</code> methods.
</p>


<h3>References</h3>

<p>Torsten Hothorn, Lisa Moest, Peter Buehlmann (2018), Most Likely
Transformations, <em>Scandinavian Journal of Statistics</em>, <b>45</b>(1),
110&ndash;134, <a href="https://doi.org/10.1111/sjos.12291">doi:10.1111/sjos.12291</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  data("GBSG2", package = "TH.data")

  library("survival")
  survreg(Surv(time, cens) ~ horTh, data = GBSG2)

  Survreg(Surv(time, cens) ~ horTh, data = GBSG2)

</code></pre>

<hr>
<h2 id='tram'>
Stratified Linear Transformation Models
</h2><span id='topic+tram'></span><span id='topic+tram_data'></span>

<h3>Description</h3>

<p>Likelihood-inference for stratified linear transformation models, including
linear shift-scale transformation models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tram(formula, data, subset, weights, offset, cluster, na.action = na.omit, 
     distribution = c("Normal", "Logistic", "MinExtrVal", "MaxExtrVal",
                      "Exponential", "Cauchy", "Laplace"), 
     frailty = c("None", "Gamma", "InvGauss", "PositiveStable"),
     transformation = c("discrete", "linear", "logarithmic", "smooth"), 
     LRtest = TRUE, prob = c(0.1, 0.9), support = NULL, 
     bounds = NULL, add = c(0, 0), order = 6, 
     negative = TRUE, remove_intercept = TRUE, 
     scale = TRUE, scale_shift = FALSE, extrapolate = FALSE, 
     log_first = FALSE, sparse_nlevels = Inf,
     model_only = FALSE, constraints = NULL, ...)
tram_data(formula, data, subset, weights, offset, cluster, na.action = na.omit) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tram_+3A_formula">formula</code></td>
<td>
<p>an object of class <code>"formula"</code>: a symbolic description 
of the model structure to be
fitted.  The details of model specification are given under
Details and in the package vignette.
</p>
</td></tr>
<tr><td><code id="tram_+3A_data">data</code></td>
<td>
<p>an optional data frame, list or environment (or object
coercible by <code>as.data.frame</code> to a data frame) containing the
variables in the model.  If not found in <code>data</code>, the
variables are taken from <code>environment(formula)</code>.
</p>
</td></tr>
<tr><td><code id="tram_+3A_subset">subset</code></td>
<td>
<p>an optional vector specifying a subset of observations to be
used in the fitting process.
</p>
</td></tr>
<tr><td><code id="tram_+3A_weights">weights</code></td>
<td>
<p>an optional vector of case weights to be used in the fitting
process.  Should be <code>NULL</code> or a numeric vector. If present,
the weighted log-likelihood is maximised.
</p>
</td></tr>
<tr><td><code id="tram_+3A_offset">offset</code></td>
<td>
<p>this can be used to specify an _a priori_ known component to
be included in the linear predictor during fitting.  This
should be <code>NULL</code> or a numeric vector of length equal to the
number of cases.
</p>
</td></tr>
<tr><td><code id="tram_+3A_cluster">cluster</code></td>
<td>
<p>optional factor with a cluster ID employed for computing
clustered covariances.
</p>
</td></tr>
<tr><td><code id="tram_+3A_na.action">na.action</code></td>
<td>
<p>a function which indicates what should happen when the data
contain <code>NA</code>s.  The default is set to <code>na.omit</code>.
</p>
</td></tr>
<tr><td><code id="tram_+3A_distribution">distribution</code></td>
<td>
<p>character specifying how the transformation function
is mapped into probabilities. Available choices
include the cumulative distribution functions of the standard normal, the
standard logistic and the standard minimum extreme value distribution.
</p>
</td></tr>
<tr><td><code id="tram_+3A_frailty">frailty</code></td>
<td>
<p>character specifying the addition of a frailty term, that
is, a random component added to the linear predictor of the
model, with specific distribution (Gamma, inverse Gaussian,
positive stable).</p>
</td></tr>
<tr><td><code id="tram_+3A_transformation">transformation</code></td>
<td>
<p>character specifying the complexity of the
response-transformation. For discrete responses, one parameter is assigned
to each level (except the last one), for continuous responses linear,
log-linear and smooth (parameterised as a Bernstein polynomial) function
are implemented.
</p>
</td></tr>
<tr><td><code id="tram_+3A_lrtest">LRtest</code></td>
<td>
<p>logical specifying if a likelihood-ratio test for the null
of all coefficients in the linear predictor being zero shall be performed.
</p>
</td></tr>
<tr><td><code id="tram_+3A_prob">prob</code></td>
<td>
<p>two probabilities giving quantiles of the response defining the support of a smooth
Bernstein polynomial (if <code>transformation = "smooth"</code>).
</p>
</td></tr>
<tr><td><code id="tram_+3A_support">support</code></td>
<td>
<p>a vector of two elements; the support of a smooth
Bernstein polynomial (if <code>transformation = "smooth"</code>).
</p>
</td></tr>
<tr><td><code id="tram_+3A_bounds">bounds</code></td>
<td>
<p>an interval defining the bounds of a real sample space.
</p>
</td></tr>
<tr><td><code id="tram_+3A_add">add</code></td>
<td>
<p>add these values to the support before generating a grid via
<code><a href="variables.html#topic+mkgrid">mkgrid</a></code>.
</p>
</td></tr>
<tr><td><code id="tram_+3A_order">order</code></td>
<td>
<p>integer &gt;= 1 defining the order of the Bernstein polynomial 
(if <code>transformation = "smooth"</code>).
</p>
</td></tr>
<tr><td><code id="tram_+3A_negative">negative</code></td>
<td>
<p>logical defining the sign of the linear predictor.
</p>
</td></tr>
<tr><td><code id="tram_+3A_remove_intercept">remove_intercept</code></td>
<td>
<p>logical defining if the intercept shall be removed
from the linear shift predictor in favour of an (typically implicit) intercept
in the baseline transformation. If <code>FALSE</code>, the linear shift predictor has
an intercept (unless <code>-1</code> is added to the formula) but the baseline
transformation is centered. For linear transformation models, this does
not change the in-sample log-likelihood. For shift-scale transformation
models, using <code>FALSE</code> ensures that centering of variables in the
linear shift predictor does not affect the corresponding estimates and
standard errors. Note that linear scale predictors are always fitted
without intercept.
</p>
</td></tr>
<tr><td><code id="tram_+3A_scale">scale</code></td>
<td>
<p>logical defining if variables in the linear predictor shall
be scaled. Scaling is internally used for model estimation,
rescaled coefficients are reported in model output.
</p>
</td></tr>
<tr><td><code id="tram_+3A_scale_shift">scale_shift</code></td>
<td>
<p>a logical choosing between two different model types in
the presence of a <code>scaling</code> term, see <code><a href="mlt.html#topic+ctm">ctm</a></code>.
</p>
</td></tr>
<tr><td><code id="tram_+3A_extrapolate">extrapolate</code></td>
<td>
<p>logical defining the behaviour of the Bernstein transformation
function outside <code>support</code>. The default
<code>FALSE</code> is to extrapolate linearily without requiring the second
derivative of the transformation function to be zero at <code>support</code>.
If <code>TRUE</code>, this additional constraint is respected.
</p>
</td></tr>
<tr><td><code id="tram_+3A_sparse_nlevels">sparse_nlevels</code></td>
<td>
<p>integer; use a sparse model matrix if the number
of levels of an ordered factor is at least as large as
<code>sparse_nlevels</code>.</p>
</td></tr>
<tr><td><code id="tram_+3A_log_first">log_first</code></td>
<td>
<p>logical; if <code>TRUE</code>, a Bernstein polynomial is
defined on the log-scale.</p>
</td></tr>
<tr><td><code id="tram_+3A_model_only">model_only</code></td>
<td>
<p>logical, if <code>TRUE</code> the unfitted model is returned.
</p>
</td></tr>
<tr><td><code id="tram_+3A_constraints">constraints</code></td>
<td>
<p>additional constraints on regression coefficients in
the linear predictor of the form <code>lhs %*% coef(object) &gt;= rhs</code>,
where <code>lhs</code> and <code>rhs</code> can be specified as a character (as in
<code><a href="multcomp.html#topic+glht">glht</a></code>) or by a matrix <code>lhs</code> (assuming
<code>rhs = 0</code>), or as a list containing the two elements <code>lhs</code> and
<code>rhs</code>.</p>
</td></tr>
<tr><td><code id="tram_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The model formula is of the form <code>y | s ~ x | z</code> where <code>y</code> is an at
least ordered response variable, <code>s</code> are the variables defining strata
and <code>x</code> defines the linear predictor. Optionally, <code>z</code> defines a
scaling term (see <code><a href="mlt.html#topic+ctm">ctm</a></code>). <code>y ~ x</code> defines a model
without strata (but response-varying intercept function) and <code>y | s ~
0</code> sets-up response-varying coefficients for all variables in <code>s</code>.
</p>
<p>The two functions <code>tram</code> and <code>tram_data</code> are not intended 
to be called directly by users. Instead,
functions <code><a href="#topic+Coxph">Coxph</a></code> (Cox proportional hazards models), 
<code><a href="#topic+Survreg">Survreg</a></code> (parametric survival models), 
<code><a href="#topic+Polr">Polr</a></code> (models for ordered categorical responses), 
<code><a href="#topic+Lm">Lm</a></code> (normal linear models),
<code><a href="#topic+BoxCox">BoxCox</a></code> (non-normal linear models) or 
<code><a href="#topic+Colr">Colr</a></code> (continuous outcome logistic regression) allow
direct access to the corresponding models.
</p>
<p>The model class and the specific models implemented in <span class="pkg">tram</span> are 
explained in the package vignette of package <span class="pkg">tram</span>.
The underlying theory of most likely transformations 
is presented in Hothorn et al. (2018), computational
and modelling aspects in more complex situations 
are discussed by Hothorn (2018).
</p>


<h3>Value</h3>

<p>An object of class <code>tram</code> inheriting from <code>mlt</code>.
</p>


<h3>References</h3>

<p>Torsten Hothorn, Lisa Moest, Peter Buehlmann (2018), Most Likely
Transformations, <em>Scandinavian Journal of Statistics</em>, <b>45</b>(1),
110&ndash;134, <a href="https://doi.org/10.1111/sjos.12291">doi:10.1111/sjos.12291</a>.
</p>
<p>Torsten Hothorn (2020), Most Likely Transformations: The mlt Package,
<em>Journal of Statistical Software</em>, <b>92</b>(1), <a href="https://doi.org/10.18637/jss.v092.i01">doi:10.18637/jss.v092.i01</a>.
</p>
<p>Sandra Siegfried, Lucas Kook, Torsten Hothorn (2023),
Distribution-Free Location-Scale Regression, <em>The American Statistician</em>,
<a href="https://doi.org/10.1080/00031305.2023.2203177">doi:10.1080/00031305.2023.2203177</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  data("BostonHousing2", package = "mlbench")

  ### unconstrained regression coefficients
  ### BoxCox calls tram internally
  m1 &lt;- BoxCox(cmedv ~ chas + crim + zn + indus + nox + 
               rm + age + dis + rad + tax + ptratio + b + lstat, 
               data = BostonHousing2)

  ### now with two constraints on regression coefficients
  m2 &lt;- BoxCox(cmedv ~ chas + crim + zn + indus + nox + 
               rm + age + dis + rad + tax + ptratio + b + lstat, 
               data = BostonHousing2, 
               constraints = c("crim &gt;= 0", "chas1 + rm &gt;= 1.5"))
  coef(m1)
  coef(m2)

  K &lt;- matrix(0, nrow = 2, ncol = length(coef(m2)))
  colnames(K) &lt;- names(coef(m2))
  K[1, "crim"] &lt;- 1
  K[2, c("chas1", "rm")] &lt;- 1
  m3 &lt;- BoxCox(cmedv ~ chas + crim + zn + indus + nox + 
               rm + age + dis + rad + tax + ptratio + b + lstat, 
               data = BostonHousing2, 
               constraints = list(K, c(0, 1.5)))
  all.equal(coef(m2), coef(m3))

</code></pre>

<hr>
<h2 id='tram-methods'>
Methods for Stratified Linear Transformation Models
</h2><span id='topic+as.mlt.tram'></span><span id='topic+model.frame.tram'></span><span id='topic+model.matrix.tram'></span><span id='topic+model.matrix.stram'></span><span id='topic+coef.tram'></span><span id='topic+coef.Lm'></span><span id='topic+coef.Survreg'></span><span id='topic+vcov.tram'></span><span id='topic+logLik.tram'></span><span id='topic+estfun.tram'></span><span id='topic+predict.tram'></span><span id='topic+predict.stram'></span><span id='topic+residuals.tram'></span><span id='topic+plot.tram'></span><span id='topic+plot.ROCtram'></span><span id='topic+PI'></span><span id='topic+PI.tram'></span><span id='topic+PI.default'></span><span id='topic+OVL'></span><span id='topic+OVL.tram'></span><span id='topic+OVL.default'></span><span id='topic+TV'></span><span id='topic+TV.tram'></span><span id='topic+TV.default'></span><span id='topic+L1'></span><span id='topic+L1.tram'></span><span id='topic+L1.default'></span><span id='topic+ROC'></span><span id='topic+ROC.tram'></span><span id='topic+ROC.default'></span>

<h3>Description</h3>

<p>Methods for objects inheriting from class tram
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tram'
as.mlt(object)
## S3 method for class 'tram'
model.frame(formula, ...)
## S3 method for class 'tram'
model.matrix(object, data = object$data, with_baseline = FALSE, ...) 
## S3 method for class 'stram'
model.matrix(object, data = object$data, with_baseline = FALSE, 
       what = c("shifting", "scaling"), ...) 
## S3 method for class 'tram'
coef(object, with_baseline = FALSE, ...) 
## S3 method for class 'Lm'
coef(object, as.lm = FALSE, ...)
## S3 method for class 'Survreg'
coef(object, as.survreg = FALSE, ...)
## S3 method for class 'tram'
vcov(object, with_baseline = FALSE, complete = FALSE, ...) 
## S3 method for class 'tram'
logLik(object, parm = coef(as.mlt(object), fixed = FALSE), ...)
## S3 method for class 'tram'
estfun(x, parm = coef(as.mlt(x), fixed = FALSE), ...)
## S3 method for class 'tram'
predict(object, newdata = model.frame(object), 
        type = c("lp", "trafo", "distribution", "logdistribution", 
             "survivor", "logsurvivor", "density", "logdensity", 
             "hazard", "loghazard", "cumhazard", "logcumhazard", 
             "odds", "logodds", "quantile"), ...) 
## S3 method for class 'stram'
predict(object, newdata = model.frame(object), 
        type = c("lp", "trafo", "distribution", "logdistribution", 
             "survivor", "logsurvivor", "density", "logdensity", 
             "hazard", "loghazard", "cumhazard", "logcumhazard", 
             "odds", "logodds", "quantile"), 
        what = c("shifting", "scaling"), ...)
## S3 method for class 'tram'
plot(x, newdata = model.frame(x), 
     which = c("QQ-PIT", "baseline only", "distribution"), 
     confidence = c("none", "interval", "band"), level = 0.95, 
     K = 50, cheat = K, col = "black", fill = "lightgrey", lwd = 1, ...)
## S3 method for class 'tram'
residuals(object, ...)
## S3 method for class 'tram'
PI(object, newdata = model.frame(object), reference = 0,
                  one2one = FALSE, ...)
## Default S3 method:
PI(object, prob, link = "logistic", ...)
## S3 method for class 'tram'
OVL(object, newdata = model.frame(object), reference = 0,
                  one2one = FALSE, ...)
## Default S3 method:
OVL(object, link = "logistic", ...)
## S3 method for class 'tram'
TV(object, newdata = model.frame(object), reference = 0,
                  one2one = FALSE, ...)
## Default S3 method:
TV(object, link = "logistic", ...)
## S3 method for class 'tram'
L1(object, newdata = model.frame(object), reference = 0,
                  one2one = FALSE, ...)
## Default S3 method:
L1(object, link = "logistic", ...)
## S3 method for class 'tram'
ROC(object, newdata = model.frame(object), reference = 0,
                   prob = 1:99 / 100, one2one = FALSE, ...)
## Default S3 method:
ROC(object, prob = 1:99 / 100, link = "logistic", ...)
## S3 method for class 'ROCtram'
plot(x, lty = 1:ncol(x), col = "black", 
     fill = "lightgrey", lwd = 1, ...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tram-methods_+3A_object">object</code>, <code id="tram-methods_+3A_formula">formula</code>, <code id="tram-methods_+3A_x">x</code></td>
<td>
<p>a fitted stratified linear transformation model inheriting
from class <code>tram</code>. <code>PI</code> also takes a numeric
vector in the default method.</p>
</td></tr>
<tr><td><code id="tram-methods_+3A_data">data</code></td>
<td>
<p>an optional data frame.</p>
</td></tr>
<tr><td><code id="tram-methods_+3A_with_baseline">with_baseline</code></td>
<td>
<p>logical, if <code>TRUE</code> all model parameters
are returned, otherwise parameters describing the
baseline transformation are ignored.</p>
</td></tr>
<tr><td><code id="tram-methods_+3A_as.lm">as.lm</code></td>
<td>
<p>logical, return parameters in the <code><a href="stats.html#topic+lm">lm</a></code>
parameterisation if <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="tram-methods_+3A_as.survreg">as.survreg</code></td>
<td>
<p>logical, return parameters in the <code><a href="survival.html#topic+survreg">survreg</a></code>
parameterisation in <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="tram-methods_+3A_parm">parm</code></td>
<td>
<p>model parameters, including baseline parameters.</p>
</td></tr>
<tr><td><code id="tram-methods_+3A_complete">complete</code></td>
<td>
<p>currently ignored</p>
</td></tr>
<tr><td><code id="tram-methods_+3A_newdata">newdata</code></td>
<td>
<p>an optional data frame of new observations.</p>
</td></tr>
<tr><td><code id="tram-methods_+3A_reference">reference</code></td>
<td>
<p>an optional data frame of reference observations, or 
a numeric vector of reference values.</p>
</td></tr>
<tr><td><code id="tram-methods_+3A_type">type</code></td>
<td>
<p>type of prediction, current options include
linear predictors (<code>"lp"</code>, of <code>x</code> variables in the
formula <code>y | s ~ x</code>), transformation functions
(<code>"trafo"</code>) or distribution functions on the
scale of the cdf (<code>"distribution"</code>),
survivor function, density function, log-density
function, hazard function, log-hazard function, cumulative
hazard function or quantile function.</p>
</td></tr>
<tr><td><code id="tram-methods_+3A_which">which</code></td>
<td>
<p>type of plot, either a QQ plot of the probability-integral
transformed observations (<code>"QQ-PIT"</code>), of the
baseline transformation of the whole distribution.</p>
</td></tr>
<tr><td><code id="tram-methods_+3A_what">what</code></td>
<td>
<p>type of model matrix / linear predictor: <code>shifting</code> returns model
model matrix / linear predictor for shift term, <code>scaling</code> for the scale term.</p>
</td></tr>
<tr><td><code id="tram-methods_+3A_confidence">confidence</code></td>
<td>
<p>type of uncertainty assessment.</p>
</td></tr>
<tr><td><code id="tram-methods_+3A_level">level</code></td>
<td>
<p>confidence level.</p>
</td></tr>
<tr><td><code id="tram-methods_+3A_k">K</code></td>
<td>
<p>number of grid points in the response, see
<code><a href="mlt.html#topic+plot.ctm">plot.ctm</a></code>.</p>
</td></tr>
<tr><td><code id="tram-methods_+3A_cheat">cheat</code></td>
<td>
<p>reduced number of grid points for the computation
of confidence bands, see <code><a href="mlt.html#topic+confband">confband</a></code>.</p>
</td></tr>
<tr><td><code id="tram-methods_+3A_col">col</code></td>
<td>
<p>line color.</p>
</td></tr>
<tr><td><code id="tram-methods_+3A_fill">fill</code></td>
<td>
<p>fill color.</p>
</td></tr>
<tr><td><code id="tram-methods_+3A_lwd">lwd</code></td>
<td>
<p>line width.</p>
</td></tr>
<tr><td><code id="tram-methods_+3A_lty">lty</code></td>
<td>
<p>line type.</p>
</td></tr>
<tr><td><code id="tram-methods_+3A_prob">prob</code></td>
<td>
<p>a numeric vector of probabilities..</p>
</td></tr>
<tr><td><code id="tram-methods_+3A_link">link</code></td>
<td>
<p>a character identifying a link function.</p>
</td></tr>
<tr><td><code id="tram-methods_+3A_one2one">one2one</code></td>
<td>
<p>logical, compute the ROC curve (and derived measures) 
comparing each row in <code>newdata</code> with each row in
<code>reference</code> (<code>FALSE</code>, the default),
or compare observations rowwise (<code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="tram-methods_+3A_...">...</code></td>
<td>
<p>additional arguments to the underlying methods for class
<code>mlt</code>, see <code><a href="mlt.html#topic+mlt-methods">mlt-methods</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>coef</code> can be used to get (and set) model parameters, 
<code>logLik</code> evaluates the log-likelihood (also for
parameters other than the maximum likelihood estimate); 
<code>vcov</code> returns the estimated variance-covariance matrix (possibly
taking <code>cluster</code> into account) and 
and <code>estfun</code> gives the score contribution by each observation.
<code>predict</code> and <code>plot</code> can be used to inspect the model on
different scales.
</p>
<p><code>PI</code> computes the probabilistic index (or concordance probability or
AUC) for all observations in <code>newdata</code>, relative to <code>reference</code>,
ie the probability </p>
<p style="text-align: center;"><code class="reqn">P(Y_1 \le Y_0 \mid x_0, x_1)</code>
</p>

<p>of observing a smaller value of a randomly sampled observation conditional
on <code class="reqn">x_1</code> compared to a randomly sampled reference observation, which
is conditional on <code class="reqn">x_0</code>. This is equivalent to the area under the
receiver operating curve (ROC). The probability only applies within
strata, response-varying coefficients are not allowed.
</p>
<p>Under the same setup, <code>OVL</code> gives the overlap coefficient, which is
one minus the total variation and one minus half the <code class="reqn">L_1</code> distance
between the two conditional densities. The overlap coefficient is
identical to the Youden index and the Smirnov statistic. 
</p>
<p><code>PI</code> and friends also accept an argument <code>conf.level</code> which
triggers computation of simultaneous 
Wald confidence intervals for these measures.
Arguments in ... are forwarded to <code><a href="multcomp.html#topic+glht">glht</a></code>.
</p>


<h3>References</h3>

<p>Torsten Hothorn, Lisa Moest, Peter Buehlmann (2018), Most Likely
Transformations, <em>Scandinavian Journal of Statistics</em>, <b>45</b>(1),
110&ndash;134, <a href="https://doi.org/10.1111/sjos.12291">doi:10.1111/sjos.12291</a>.
</p>


<h3>See Also</h3>

<p><code><a href="mlt.html#topic+mlt-methods">mlt-methods</a></code>,  <code><a href="mlt.html#topic+plot.ctm">plot.ctm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
    data("BostonHousing2", package = "mlbench")

    ### fit non-normal Box-Cox type linear model with two
    ### baseline functions (for houses near and off Charles River)
    BC_BH_2 &lt;- BoxCox(cmedv | 0 + chas ~ crim + zn + indus + nox + 
                      rm + age + dis + rad + tax + ptratio + b + lstat,
                      data = BostonHousing2)
    logLik(BC_BH_2)

    ### classical likelihood inference
    summary(BC_BH_2)

    ### coefficients of the linear predictor
    coef(BC_BH_2)

    ### plot linear predictor (mean of _transformed_ response) 
    ### vs. observed values
    plot(predict(BC_BH_2, type = "lp"), BostonHousing2$cmedv)

    ### all coefficients
    coef(BC_BH_2, with_baseline = TRUE)

    ### compute predicted median along with 10% and 90% quantile for the first
    ### observations
    predict(BC_BH_2, newdata = BostonHousing2[1:3,], type = "quantile",
            prob = c(.1, .5, .9))

    ### plot the predicted density for these observations
    plot(BC_BH_2, newdata = BostonHousing2[1:3, -1],
         which = "distribution", type = "density", K = 1000)

    ### evaluate the two baseline transformations, with confidence intervals
    nd &lt;- model.frame(BC_BH_2)[1:2, -1]
    nd$chas &lt;- factor(c("0", "1"))
    library("colorspace")
    col &lt;- diverge_hcl(2, h = c(246, 40), c = 96, l = c(65, 90))
    fill &lt;- diverge_hcl(2, h = c(246, 40), c = 96, l = c(65, 90), alpha = .3)
    plot(BC_BH_2, which = "baseline only", newdata = nd, col = col,
         confidence = "interval", fill = fill, lwd = 2,
         xlab = "Median Value", ylab = expression(h[Y]))
    legend("bottomright", lty = 1, col = col, 
            title = "Near Charles River", legend = c("no", "yes"), bty = "n")

    ### cars data; with quantile functions
    plot(dist ~ speed, data = cars)
    m &lt;- Colr(dist ~ speed, data = cars)
    q &lt;- predict(as.mlt(m), newdata = data.frame(speed = s &lt;- 6:25),
                 type = "quantile", prob = c(1, 5, 9) / 10)
    lines(s, q[1,])
    lines(s, q[2,])
    lines(s, q[3,])

    nd &lt;- data.frame(speed = s &lt;- as.double(1:5 * 5))
    
    # Prob(dist at speed s &gt; dist at speed 0)
    # speed 0 is reference, not a good choice here
    PI(m, newdata = nd)

    # Prob(dist at speed s &gt; dist at speed 15)
    lp15 &lt;- c(predict(m, newdata = data.frame(speed = 15)))
    PI(m, newdata = nd, reference = lp15)
    PI(m, newdata = nd, reference = nd[3,,drop = FALSE])

    # Prob(dist at speed s' &gt; dist at speed s)
    PI(m, newdata = nd, reference = nd)
    # essentially:
    lp &lt;- predict(m, newdata = nd)
    PI(object = dist(lp))
    # same, with simultaneous confidence intervals
    PI(m, newdata = nd, reference = nd, conf.level = .95)

    # plot ROC curves + confidence bands
    # compare speed 20 and 25 to speed 15
    plot(ROC(m, newdata = nd[4:5,,drop = FALSE],
             reference = nd[3,,drop = FALSE],
             conf.level = 0.95))

    # Overlap of conditional densities at speed s' and s
    OVL(m, newdata = nd, reference = nd)

    ### ROC analysis (takes too long for CRAN Windows)
    if (require("mlbench") &amp;&amp; .Platform$OS.type != "windows") {

        layout(matrix(1:4, nrow = 2))
        data("PimaIndiansDiabetes2", package = "mlbench")
        dia &lt;- sort(unique(PimaIndiansDiabetes2$diabetes))
        nd &lt;- data.frame(diabetes = dia, 
                         age = 29, mass = 32) ### median values

        ### unconditional ROC analysis: glucose tolerance test
        m0 &lt;- Colr(glucose ~ diabetes, data = PimaIndiansDiabetes2)
        # ROC curve + confidence band
        plot(ROC(m0, newdata = nd[2,,drop = FALSE], conf.level = .95)) 
        # Wald interval for AUC
        PI(m0, newdata = nd[2,,drop = FALSE], conf.level = .95)
        # score interval for AUC
        PI(-c(coef(m0), score_test(m0)$conf.int[2:1]))

        ### adjusted ROC analysis for age and mass
        m1 &lt;- Colr(glucose ~ diabetes + age + mass, data = PimaIndiansDiabetes2)
        # ROC curve + confidence band (this is the same for all ages /
        # masses)
        plot(ROC(m1, newdata = nd[2,,drop = FALSE], 
                     reference = nd[1,,drop = FALSE], 
                 conf.level = .95))
        # Wald interval for adjusted AUC
        PI(m1, newdata = nd[2,,drop = FALSE], reference = nd[1,,drop = FALSE], 
           conf.level = .95)
        # Score interval for adjusted AUC
        PI(-c(coef(m1)[1], score_test(m1, names(coef(m1))[1])$conf.int[2:1]))

        ### conditional ROC analysis: AUC regression ~ age + mass
        m2 &lt;- Colr(glucose ~ diabetes * (age + mass), data = PimaIndiansDiabetes2)
        # ROC curve for a person with age = 29 and mass = 32
        plot(ROC(m2, newdata = nd[2,,drop = FALSE], 
                     reference = nd[1,,drop = FALSE], 
                 conf.level = .95))
        # AUC for persons ages 21:81, all with mass = 32
        nd1 &lt;- data.frame(diabetes = nd[1,"diabetes"], age = 21:81, mass = 32)
        nd2 &lt;- data.frame(diabetes = nd[2,"diabetes"], age = 21:81, mass = 32)
        auc &lt;- PI(m2, newdata = nd2, reference = nd1, one2one = TRUE,
                  conf.level = 0.95)
        plot(nd1$age, auc[, "Estimate"], xlab = "Age (in years)", ylab =
             "AUC", ylim = c(0, 1), type = "l")
        lines(nd1$age, auc[, "lwr"], lty = 3)
        lines(nd1$age, auc[, "upr"], lty = 3)
    }
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
