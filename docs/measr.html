<!DOCTYPE html><html><head><title>Help for package measr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {measr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#%&gt;%'><p>Pipe operator</p></a></li>
<li><a href='#c.measrprior'><p>Combine multiple measrprior objects into one measrprior</p></a></li>
<li><a href='#create_profiles'><p>Generate mastery profiles</p></a></li>
<li><a href='#default_dcm_priors'><p>Default priors for diagnostic classification models</p></a></li>
<li><a href='#ecpe_data'><p>Examination for the Certificate of Proficiency in English (ECPE)</p></a></li>
<li><a href='#fit_m2.measrdcm'><p>Estimate the M<sub>2</sub> fit statistic for</p>
diagnostic classification models</a></li>
<li><a href='#fit_ppmc'><p>Posterior predictive model checks for assessing model fit</p></a></li>
<li><a href='#get_parameters'><p>Get a list of possible parameters</p></a></li>
<li><a href='#is.measrprior'><p>Checks if argument is a <code>measrprior</code> object</p></a></li>
<li><a href='#loo_compare.measrfit'><p>Relative model fit comparisons</p></a></li>
<li><a href='#loo.measrfit'><p>Efficient approximate leave-one-out cross-validation (LOO)</p></a></li>
<li><a href='#mdm_data'><p>MacReady &amp; Dayton (1977) Multiplication Data</p></a></li>
<li><a href='#measr_dcm'><p>Fit Bayesian diagnostic classification models</p></a></li>
<li><a href='#measr_examples'><p>Determine if code is executed interactively or in pkgdown</p></a></li>
<li><a href='#measr_extract'><p>Extract components of a <code>measrfit</code> object.</p></a></li>
<li><a href='#measr-package'><p>measr: Bayesian Psychometric Measurement Using 'Stan'</p></a></li>
<li><a href='#measrfit-class'><p>Class <code>measrfit</code> of models fitted with the <strong>measr</strong> package</p></a></li>
<li><a href='#measrprior'><p>Prior definitions for <strong>measr</strong> models</p></a></li>
<li><a href='#model_evaluation'><p>Add model evaluation metrics model objects</p></a></li>
<li><a href='#predict.measrdcm'><p>Posterior draws of respondent proficiency</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#reliability'><p>Estimate the reliability of psychometric models</p></a></li>
<li><a href='#tidyeval'><p>Tidy eval helpers</p></a></li>
<li><a href='#waic.measrfit'><p>Widely applicable information criterion (WAIC)</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Bayesian Psychometric Measurement Using 'Stan'</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Estimate diagnostic classification models (also called cognitive
    diagnostic models) with 'Stan'. Diagnostic classification models are 
    confirmatory latent class models, as described by Rupp et al.
    (2010, ISBN: 978-1-60623-527-0). Automatically generate 'Stan' code for the
    general loglinear cognitive diagnostic diagnostic model proposed by
    Henson et al. (2009) &lt;<a href="https://doi.org/10.1007%2Fs11336-008-9089-5">doi:10.1007/s11336-008-9089-5</a>&gt; and other subtypes that
    introduce additional model constraints. Using the generated 'Stan' code,
    estimate the model evaluate the model's performance using model fit indices,
    information criteria, and reliability metrics.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://measr.info">https://measr.info</a>, <a href="https://github.com/wjakethompson/measr">https://github.com/wjakethompson/measr</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/wjakethompson/measr/issues">https://github.com/wjakethompson/measr/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>dcm2, dplyr (&ge; 1.1.1), fs, glue, loo, magrittr, methods,
posterior, psych, Rcpp (&ge; 0.12.0), RcppParallel (&ge; 5.0.1),
rlang (&ge; 0.4.11), rstan (&ge; 2.26.0), rstantools (&ge; 2.3.0),
stats, tibble, tidyr (&ge; 1.3.0)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>BH (&ge; 1.66.0), Rcpp (&ge; 0.12.0), RcppEigen (&ge; 0.3.3.3.0),
RcppParallel (&ge; 5.0.1), rstan (&ge; 2.26.0), StanHeaders (&ge;
2.26.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>cli, cmdstanr (&ge; 0.4.0), crayon, knitr, rmarkdown, roxygen2,
spelling, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Additional_repositories:</td>
<td><a href="https://mc-stan.org/r-packages/">https://mc-stan.org/r-packages/</a></td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Config/Needs/website:</td>
<td>wjakethompson/wjake, showtext, ggdist, english</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Biarch:</td>
<td>true</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>GNU make</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-29 17:09:31 UTC; jakethompson</td>
</tr>
<tr>
<td>Author:</td>
<td>W. Jake Thompson <a href="https://orcid.org/0000-0001-7339-0300"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Nathan Jones <a href="https://orcid.org/0000-0001-6177-7161"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb],
  Matthew Johnson [cph] (Provided code adapted for
    reliability.measrdcm()),
  Paul-Christian BÃ¼rkner [cph] (Author of eval_silent()),
  University of Kansas [cph],
  Institute of Education Sciences [fnd]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>W. Jake Thompson &lt;wjakethompson@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-30 08:50:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='+25+26gt+3B+25'>Pipe operator</h2><span id='topic++25+3E+25'></span>

<h3>Description</h3>

<p>See <code>magrittr::<a href="magrittr.html#topic+pipe">%&gt;%</a></code> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lhs %&gt;% rhs
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B25+2B26gt+2B3B+2B25_+3A_lhs">lhs</code></td>
<td>
<p>A value or the magrittr placeholder.</p>
</td></tr>
<tr><td><code id="+2B25+2B26gt+2B3B+2B25_+3A_rhs">rhs</code></td>
<td>
<p>A function call using the magrittr semantics.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The result of calling <code>rhs(lhs)</code>.
</p>

<hr>
<h2 id='c.measrprior'>Combine multiple measrprior objects into one measrprior</h2><span id='topic+c.measrprior'></span>

<h3>Description</h3>

<p>Combine multiple measrprior objects into one measrprior
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'measrprior'
c(x, ..., replace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="c.measrprior_+3A_x">x</code></td>
<td>
<p>A <code>measrprior</code> object.</p>
</td></tr>
<tr><td><code id="c.measrprior_+3A_...">...</code></td>
<td>
<p>Additional <code>measrprior</code> objects to be combined.</p>
</td></tr>
<tr><td><code id="c.measrprior_+3A_replace">replace</code></td>
<td>
<p>Should only unique priors be kept? If <code>TRUE</code>, the first prior
specified is kept.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>measrprior</code> object.
</p>

<hr>
<h2 id='create_profiles'>Generate mastery profiles</h2><span id='topic+create_profiles'></span>

<h3>Description</h3>

<p>Given the number of attributes, generate all possible patterns of attribute
mastery.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_profiles(attributes)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_profiles_+3A_attributes">attributes</code></td>
<td>
<p>Positive integer. The number of attributes being measured.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="tibble.html#topic+tibble-package">tibble</a> with all possible attribute
mastery profiles. Each row is a profile, and each column indicates whether
the attribute in that column was mastered (1) or not mastered (0). Thus,
the tibble will have <code>2^attributes</code> rows, and <code>attributes</code> columns.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>create_profiles(3L)
create_profiles(5)
</code></pre>

<hr>
<h2 id='default_dcm_priors'>Default priors for diagnostic classification models</h2><span id='topic+default_dcm_priors'></span>

<h3>Description</h3>

<p>Default priors for diagnostic classification models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>default_dcm_priors(type = "lcdm", attribute_structure = "unconstrained")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="default_dcm_priors_+3A_type">type</code></td>
<td>
<p>Type of DCM to estimate. Must be one of
lcdm, dina, dino, or crum.</p>
</td></tr>
<tr><td><code id="default_dcm_priors_+3A_attribute_structure">attribute_structure</code></td>
<td>
<p>Structural model specification. Must be one of
unconstrained, or independent.
<code>unconstrained</code> makes no assumptions about the relationships between
attributes, whereas <code>independent</code> assumes that proficiency statuses on
attributes are independent of each other.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>measrprior</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>default_dcm_priors(type = "lcdm")
</code></pre>

<hr>
<h2 id='ecpe_data'>Examination for the Certificate of Proficiency in English (ECPE)</h2><span id='topic+ecpe_data'></span><span id='topic+ecpe_qmatrix'></span>

<h3>Description</h3>

<p>This is data from the grammar section of the ECPE, administered annually by
the English Language Institute at the University of Michigan. This data
contains responses to 28 questions from 2,922 respondents, which ask
respondents to complete a sentence with the correct word. This data set has
been used by Templin &amp; Hoffman (2013) and Templin &amp; Bradshaw (2014) for
demonstrating the log-linear cognitive diagnosis model (LCDM) and the
hierarchical diagnostic classification model (HDCM), respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ecpe_data

ecpe_qmatrix
</code></pre>


<h3>Format</h3>

<p><code>ecpe_data</code> is a <a href="tibble.html#topic+tibble-package">tibble</a> containing ECPE
response data with 2,922 rows and 29 variables.
</p>

<ul>
<li> <p><code>resp_id</code>: Respondent identifier
</p>
</li>
<li> <p><code>E1</code>-<code>E28</code>: Dichotomous item responses to the 28 ECPE items
</p>
</li></ul>

<p><code>ecpe_qmatrix</code> is a <a href="tibble.html#topic+tibble-package">tibble</a> that identifies
which skills are measured by each ECPE item. This section of the ECPE
contains 28 items measuring 3 skills. The <code>ecpe_qmatrix</code> correspondingly is
made up of 28 rows and 4 variables.
</p>

<ul>
<li> <p><code>item_id</code>: Item identifier, corresponds to <code>E1</code>-<code>E28</code> in <code><a href="#topic+ecpe_data">ecpe_data</a></code>
</p>
</li>
<li> <p><code>morphosyntactic</code>, <code>cohesive</code>, and <code>lexical</code>: Dichotomous indicator for
whether or not the skill is measured by each item. A value of <code>1</code> indicates
the skill is measured by the item and a value of <code>0</code> indicates the skill is
not measured by the item.
</p>
</li></ul>



<h3>Details</h3>

<p>The skills correspond to knowledge of:
</p>

<ol>
<li><p> Morphosyntactic rules
</p>
</li>
<li><p> Cohesive rules
</p>
</li>
<li><p> Lexical rules
</p>
</li></ol>

<p>For more details, see Buck &amp; Tatsuoka (1998) and Henson &amp; Templin (2007).
</p>


<h3>References</h3>

<p>Buck, G., &amp; Tatsuoka, K. K. (1998). Application of the rule-space
procedure to language testing: Examining attributes of a free response
listening test. <em>Language Testing, 15</em>(2), 119-157.
<a href="https://doi.org/10.1177/026553229801500201">doi:10.1177/026553229801500201</a>
</p>
<p>Henson, R., &amp; Templin, J. (2007, April). <em>Large-scale language
assessment using cognitive diagnosis models.</em> Paper presented at the Annual
meeting of the National Council on Measurement in Education, Chicago, IL.
</p>
<p>Templin, J., &amp; Hoffman, L. (2013). Obtaining diagnostic
classification model estimates using Mplus. <em>Educational Measurement:
Issues and Practice, 32</em>(2), 37-50.
<a href="https://doi.org/10.1111/emip.12010">doi:10.1111/emip.12010</a>
</p>
<p>Templin, J., &amp; Bradshaw, L. (2014). Hierarchical diagnostic
classification models: A family of models for estimating and testing
attribute hierarchies. <em>Psychometrika, 79</em>(2), 317-339.
<a href="https://doi.org/10.1007/s11336-013-9362-0">doi:10.1007/s11336-013-9362-0</a>
</p>

<hr>
<h2 id='fit_m2.measrdcm'>Estimate the M<sub>2</sub> fit statistic for
diagnostic classification models</h2><span id='topic+fit_m2.measrdcm'></span>

<h3>Description</h3>

<p>For diagnostic classification models, the
M<sub>2</sub> statistic is calculated as
described by Hansen et al. (2016) and Liu et al. (2016).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'measrdcm'
fit_m2(model, ..., ci = 0.9, force = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_m2.measrdcm_+3A_model">model</code></td>
<td>
<p>An estimated diagnostic classification model.</p>
</td></tr>
<tr><td><code id="fit_m2.measrdcm_+3A_...">...</code></td>
<td>
<p>Unused, for extensibility.</p>
</td></tr>
<tr><td><code id="fit_m2.measrdcm_+3A_ci">ci</code></td>
<td>
<p>The confidence interval for the RMSEA.</p>
</td></tr>
<tr><td><code id="fit_m2.measrdcm_+3A_force">force</code></td>
<td>
<p>If the M<sub>2</sub> has already
been saved to the model object with <code><a href="#topic+add_fit">add_fit()</a></code>, should it be recalculated.
Default is <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame created by <code><a href="dcm2.html#topic+fit_m2">dcm2::fit_m2()</a></code>.
</p>


<h3>Methods (by class)</h3>


<ul>
<li> <p><code>fit_m2(measrdcm)</code>: M<sub>2</sub> for
diagnostic classification models.
</p>
</li></ul>


<h3>References</h3>

<p>Hansen, M., Cai, L., Monroe, S., &amp; Li, Z. (2016).
Limited-information goodness-of-fit testing of diagnostic classification
item response models. <em>British Journal of Mathematical and Statistical
Psychology, 69</em>(3), 225-252. <a href="https://doi.org/10.1111/bmsp.12074">doi:10.1111/bmsp.12074</a>
</p>
<p>Liu, Y., Tian, W., &amp; Xin, T. (2016). An application of
M<sub>2</sub> statistic to evaluate the fit
of cognitive diagnostic models. <em>Journal of Educational and Behavioral
Statistics, 41</em>(1), 3-26. <a href="https://doi.org/10.3102/1076998615621293">doi:10.3102/1076998615621293</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
rstn_mdm_lcdm &lt;- measr_dcm(
  data = mdm_data, missing = NA, qmatrix = mdm_qmatrix,
  resp_id = "respondent", item_id = "item", type = "lcdm",
  method = "optim", seed = 63277, backend = "rstan"
)

fit_m2(rstn_mdm_lcdm)

</code></pre>

<hr>
<h2 id='fit_ppmc'>Posterior predictive model checks for assessing model fit</h2><span id='topic+fit_ppmc'></span>

<h3>Description</h3>

<p>For models estimated with <code>method = "mcmc"</code>, use the posterior distributions
to compute expected distributions for fit statistics and compare to values
in the observed data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit_ppmc(
  model,
  ndraws = NULL,
  probs = c(0.025, 0.975),
  return_draws = 0,
  model_fit = c("raw_score"),
  item_fit = c("conditional_prob", "odds_ratio"),
  force = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_ppmc_+3A_model">model</code></td>
<td>
<p>A <a href="#topic+measrfit">measrfit</a> object.</p>
</td></tr>
<tr><td><code id="fit_ppmc_+3A_ndraws">ndraws</code></td>
<td>
<p>The number of posterior draws to base the checks on. Must be
less than or equal to the total number of posterior draws retained in the
estimated model. If <code>NULL</code> (the default) the total number from the
estimated model is used.</p>
</td></tr>
<tr><td><code id="fit_ppmc_+3A_probs">probs</code></td>
<td>
<p>The percentiles to be computed by the <code style="white-space: pre;">&#8288;[stats::quantile()]&#8288;</code>
function for summarizing the posterior distributions of the specified fit
statistics.</p>
</td></tr>
<tr><td><code id="fit_ppmc_+3A_return_draws">return_draws</code></td>
<td>
<p>Proportion of posterior draws for each specified fit
statistic to be returned. This does not affect the calculation of the
posterior predictive checks, but can be useful for visualizing the fit
statistics. For example, if <code>ndraws = 500</code>, <code>return_draws = 0.2</code>, and
<code>model_fit = "raw_score"</code>, then the raw score chi-square will be computed
500 times (once for each draw) and 100 of those values (0.2 * 500) will be
returned. If <code>0</code> (the default), only summaries of the posterior are
returned (no individual samples).</p>
</td></tr>
<tr><td><code id="fit_ppmc_+3A_model_fit">model_fit</code></td>
<td>
<p>The posterior predictive model checks to compute for an
evaluation of model-level fit. If <code>NULL</code>, no model-level checks are
computed. See details.</p>
</td></tr>
<tr><td><code id="fit_ppmc_+3A_item_fit">item_fit</code></td>
<td>
<p>The posterior predictive model checks to compute for an
evaluation of item-level fit. If <code>NULL</code>, no item-level checks are computed.
Multiple checks can be provided in order to calculate more than one check
simultaneously (e.g., <code>item_fit = c("conditional_prob", "odds_ratio")</code>).
See details.</p>
</td></tr>
<tr><td><code id="fit_ppmc_+3A_force">force</code></td>
<td>
<p>If all requested PPMCs have already been added to the model
object using <code><a href="#topic+add_fit">add_fit()</a></code>, should they be recalculated. Default is <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Posterior predictive model checks (PPMCs) use the posterior distribution of
an estimated model to compute different statistics. This creates an expected
distribution of the given statistic, <em>if our estimated parameters are
correct</em>. We then compute the statistic in our observed data and compare the
observed value to the expected distribution. Observed values that fall
outside of the expected distributions indicate incompatibility between the
estimated model and the observed data.
</p>
<p>We currently support PPMCs at the model and item level. At the model level,
we calculate the expected raw score distribution (<code>model_fit = "raw_score"</code>),
as described by Thompson (2019) and Park et al. (2015).
</p>
<p>At the item level, we can calculate the conditional probability that a
respondent in each class provides a correct response (<code>item_fit = "conditional_prob"</code>) as described by Thompson (2019) and Sinharay &amp; Almond
(2007). We can also calculate the odds ratio for each pair of items
(<code>item_fit = "odds_ratio"</code>) as described by Park et al. (2015) and Sinharay
et al. (2006).
</p>


<h3>Value</h3>

<p>A list with two elements, &quot;model_fit&quot; and &quot;item_fit&quot;. If either
<code>model_fit = NULL</code> or <code>item_fit = NULL</code> in the function call, this will be
a one-element list, with the null criteria excluded. Each list element, is
itself a list with one element for each specified PPMC containing a
<a href="tibble.html#topic+tibble-package">tibble</a>. For example if
<code>item_fit = c("conditional_prob", "odds_ratio")</code>, the &quot;item_fit&quot; element
will be a list of length two, where each element is a tibble containing the
results of the PPMC. All tibbles follow the same general structure:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;obs_{ppmc}&#8288;</code>: The value of the relevant statistic in the observed data.
</p>
</li>
<li> <p><code>ppmc_mean</code>: The mean of the <code>ndraws</code> posterior samples calculated for
the given statistic.
</p>
</li>
<li><p> Quantile columns: 1 column for each value of <code>probs</code>, providing the
corresponding quantiles of the <code>ndraws</code> posterior samples calculated for
the given statistic.
</p>
</li>
<li> <p><code>samples</code>: A list column, where each element contains a vector of length
<code>(ndraws * return_draws)</code>, representing samples from the posterior
distribution of the calculated statistic. This column is excluded if
<code>return_draws = 0</code>.
</p>
</li>
<li> <p><code>ppp</code>: The posterior predictive p-value. This is the proportion of
posterior samples for calculated statistic that are greater than the
observed value. Values very close to 0 or 1 indicate incompatibility
between the fitted model and the observed data.
</p>
</li></ul>



<h3>References</h3>

<p>Park, J. Y., Johnson, M. S., Lee, Y-S. (2015). Posterior
predictive model checks for cognitive diagnostic models. <em>International
Journal of Quantitative Research in Education, 2</em>(3-4), 244-264.
<a href="https://doi.org/10.1504/IJQRE.2015.071738">doi:10.1504/IJQRE.2015.071738</a>
</p>
<p>Sinharay, S., &amp; Almond, R. G. (2007). Assessing fit of cognitive
diagnostic models. <em>Educational and Psychological Measurement, 67</em>(2),
239-257. <a href="https://doi.org/10.1177/0013164406292025">doi:10.1177/0013164406292025</a>
</p>
<p>Sinharay, S., Johnson, M. S., &amp; Stern, H. S. (2006). Posterior
predictive assessment of item response theory models. <em>Applied
Psychological Measurement, 30</em>(4), 298-321.
<a href="https://doi.org/10.1177/0146621605285517">doi:10.1177/0146621605285517</a>
</p>
<p>Thompson, W. J. (2019). <em>Bayesian psychometrics for diagnostic
assessments: A proof of concept</em> (Research Report No. 19-01). University
of Kansas; Accessible Teaching, Learning, and Assessment Systems.
<a href="https://doi.org/10.35542/osf.io/jzqs8">doi:10.35542/osf.io/jzqs8</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
mdm_dina &lt;- measr_dcm(
  data = mdm_data, missing = NA, qmatrix = mdm_qmatrix,
  resp_id = "respondent", item_id = "item", type = "dina",
  method = "mcmc", seed = 63277, backend = "rstan",
  iter = 700, warmup = 500, chains = 2, refresh = 0
)

fit_ppmc(mdm_dina, model_fit = "raw_score", item_fit = NULL)

</code></pre>

<hr>
<h2 id='get_parameters'>Get a list of possible parameters</h2><span id='topic+get_parameters'></span>

<h3>Description</h3>

<p>When specifying prior distributions, it is often useful to see which
parameters are included in a given model. Using the Q-matrix and type of
diagnostic model to estimated, we can create a list of all included
parameters for which a prior can be specified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_parameters(
  qmatrix,
  item_id = NULL,
  rename_att = FALSE,
  rename_item = FALSE,
  type = c("lcdm", "dina", "dino", "crum"),
  attribute_structure = c("unconstrained", "independent")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_parameters_+3A_qmatrix">qmatrix</code></td>
<td>
<p>The Q-matrix. A data frame with 1 row per item and 1 column
per attribute. All cells should be either 0 (item does not measure the
attribute) or 1 (item does measure the attribute).</p>
</td></tr>
<tr><td><code id="get_parameters_+3A_item_id">item_id</code></td>
<td>
<p>Optional. Variable name of a column in <code>qmatrix</code> that contains
item identifiers. <code>NULL</code> (the default) indicates that no identifiers are
present in the Q-matrix.</p>
</td></tr>
<tr><td><code id="get_parameters_+3A_rename_att">rename_att</code></td>
<td>
<p>Should attribute names from the <code>qmatrix</code> be replaced with
generic, but consistent names (e.g., &quot;att1&quot;, &quot;att2&quot;, &quot;att3&quot;).</p>
</td></tr>
<tr><td><code id="get_parameters_+3A_rename_item">rename_item</code></td>
<td>
<p>Should item names from the <code>qmatrix</code> be replaced with
generic, but consistent names (e.g., 1, 2, 3).</p>
</td></tr>
<tr><td><code id="get_parameters_+3A_type">type</code></td>
<td>
<p>Type of DCM to estimate. Must be one of
lcdm, dina, dino, or crum.</p>
</td></tr>
<tr><td><code id="get_parameters_+3A_attribute_structure">attribute_structure</code></td>
<td>
<p>Structural model specification. Must be one of
unconstrained, or independent.
<code>unconstrained</code> makes no assumptions about the relationships between
attributes, whereas <code>independent</code> assumes that proficiency statuses on
attributes are independent of each other.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="tibble.html#topic+tibble-package">tibble</a> with one row per parameter.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>get_parameters(ecpe_qmatrix, item_id = "item_id", type = "lcdm")

get_parameters(ecpe_qmatrix, item_id = "item_id", type = "lcdm",
               rename_att = TRUE)
</code></pre>

<hr>
<h2 id='is.measrprior'>Checks if argument is a <code>measrprior</code> object</h2><span id='topic+is.measrprior'></span>

<h3>Description</h3>

<p>Checks if argument is a <code>measrprior</code> object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.measrprior(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.measrprior_+3A_x">x</code></td>
<td>
<p>An object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A logical indicating if <code>x</code> is a <code>measrprior</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prior1 &lt;- prior(lognormal(0, 1), class = maineffect)
is.measrprior(prior1)

prior2 &lt;- 3
is.measrprior(prior2)
</code></pre>

<hr>
<h2 id='loo_compare.measrfit'>Relative model fit comparisons</h2><span id='topic+loo_compare.measrfit'></span>

<h3>Description</h3>

<p>A <code><a href="loo.html#topic+loo_compare">loo::loo_compare()</a></code> method that is customized for <code>measrfit</code> objects. See
the <strong>loo</strong> package <a href="https://mc-stan.org/loo/articles/">vignettes</a> for
details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'measrfit'
loo_compare(x, ..., criterion = c("loo", "waic"), model_names = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loo_compare.measrfit_+3A_x">x</code></td>
<td>
<p>A <a href="#topic+measrfit">measrfit</a> object.</p>
</td></tr>
<tr><td><code id="loo_compare.measrfit_+3A_...">...</code></td>
<td>
<p>Additional objects of class <a href="#topic+measrfit">measrfit</a>.</p>
</td></tr>
<tr><td><code id="loo_compare.measrfit_+3A_criterion">criterion</code></td>
<td>
<p>The name of the criterion to be extracted from the
<a href="#topic+measrfit">measrfit</a> object for comparison.</p>
</td></tr>
<tr><td><code id="loo_compare.measrfit_+3A_model_names">model_names</code></td>
<td>
<p>Names given to each provided model in the comparison
output. If <code>NULL</code> (the default), the names will be parsed from the names of
the objects passed for comparison.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned by <code><a href="loo.html#topic+loo_compare">loo::loo_compare()</a></code>.
</p>

<hr>
<h2 id='loo.measrfit'>Efficient approximate leave-one-out cross-validation (LOO)</h2><span id='topic+loo.measrfit'></span>

<h3>Description</h3>

<p>A <code><a href="loo.html#topic+loo">loo::loo()</a></code> method that is customized for <code>measrfit</code> objects. This is a
simple wrapper around <code><a href="loo.html#topic+loo">loo::loo.array()</a></code>. See the <strong>loo</strong> package
<a href="https://mc-stan.org/loo/articles/">vignettes</a> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'measrfit'
loo(x, ..., r_eff = NA, force = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loo.measrfit_+3A_x">x</code></td>
<td>
<p>A <a href="#topic+measrfit">measrfit</a> object.</p>
</td></tr>
<tr><td><code id="loo.measrfit_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="loo.html#topic+loo">loo::loo.array()</a></code>.</p>
</td></tr>
<tr><td><code id="loo.measrfit_+3A_r_eff">r_eff</code></td>
<td>
<p>Vector of relative effective sample size estimates for the
likelihood (<code>exp(log_lik)</code>) of each observation. This is related to
the relative efficiency of estimating the normalizing term in
self-normalizing importance sampling when using posterior draws obtained
with MCMC. If MCMC draws are used and <code>r_eff</code> is not provided then
the reported PSIS effective sample sizes and Monte Carlo error estimates
will be over-optimistic. If the posterior draws are independent then
<code>r_eff=1</code> and can be omitted. The warning message thrown when <code>r_eff</code> is
not specified can be disabled by setting <code>r_eff</code> to <code>NA</code>. See the
<code><a href="loo.html#topic+relative_eff">relative_eff()</a></code> helper functions for computing <code>r_eff</code>.</p>
</td></tr>
<tr><td><code id="loo.measrfit_+3A_force">force</code></td>
<td>
<p>If the LOO criterion has already been added to the model object
with <code><a href="#topic+add_criterion">add_criterion()</a></code>, should it be recalculated. Default is <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned by <code><a href="loo.html#topic+loo">loo::loo.array()</a></code>.
</p>

<hr>
<h2 id='mdm_data'>MacReady &amp; Dayton (1977) Multiplication Data</h2><span id='topic+mdm_data'></span><span id='topic+mdm_qmatrix'></span>

<h3>Description</h3>

<p>This is a small data set of multiplication item responses. This data contains
responses to 4 items from 142 respondents, which ask respondents to complete
an integer multiplication problem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdm_data

mdm_qmatrix
</code></pre>


<h3>Format</h3>

<p><code>mdm_data</code> is a <a href="tibble.html#topic+tibble-package">tibble</a> containing responses
to multiplication items, as described in MacReady &amp; Dayton (1977). There are
142 rows and 5 variables.
</p>

<ul>
<li> <p><code>respondent</code>: Respondent identifier
</p>
</li>
<li> <p><code>mdm1</code>-<code>mdm4</code>: Dichotomous item responses to the 4 multiplication items
</p>
</li></ul>

<p><code>mdm_qmatrix</code> is a <a href="tibble.html#topic+tibble-package">tibble</a> that identifies
which skills are measured by each MDM item. This MDM data contains 4 items,
all of which measure the skill of multiplication. The <code>mdm_qmatrix</code>
correspondingly is made up of 4 rows and 2 variables.
</p>

<ul>
<li> <p><code>item</code>: Item identifier, corresponds to <code>mdm1</code>-<code>mdm4</code> in <code><a href="#topic+mdm_data">mdm_data</a></code>
</p>
</li>
<li> <p><code>multiplication</code>: Dichotomous indicator for whether or not the
multiplication skill is measured by each item. A value of <code>1</code> indicates the
skill is measured by the item and a value of <code>0</code> indicates the skill is not
measured by the item.
</p>
</li></ul>



<h3>References</h3>

<p>MacReady, G. B., &amp; Dayton, C. M. (1977). The use of probabilistic
models in the assessment of mastery. <em>Journal of Educational Statistics,
2</em>(2), 99-120. <a href="https://doi.org/10.2307/1164802">doi:10.2307/1164802</a>
</p>

<hr>
<h2 id='measr_dcm'>Fit Bayesian diagnostic classification models</h2><span id='topic+measr_dcm'></span>

<h3>Description</h3>

<p>Estimate diagnostic classification models (DCMs; also known as cognitive
diagnostic models) using 'Stan'. Models can be estimated using Stan's
optimizer, or full Markov chain Monte Carlo (MCMC).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>measr_dcm(
  data,
  missing = NA,
  qmatrix,
  resp_id = NULL,
  item_id = NULL,
  type = c("lcdm", "dina", "dino", "crum"),
  max_interaction = Inf,
  attribute_structure = c("unconstrained", "independent"),
  method = c("mcmc", "optim"),
  prior = NULL,
  backend = getOption("measr.backend", "rstan"),
  file = NULL,
  file_refit = getOption("measr.file_refit", "never"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="measr_dcm_+3A_data">data</code></td>
<td>
<p>Response data. A data frame with 1 row per respondent and 1
column per item.</p>
</td></tr>
<tr><td><code id="measr_dcm_+3A_missing">missing</code></td>
<td>
<p>An R expression specifying how missing data in <code>data</code> is coded
(e.g., <code>NA</code>, <code>"."</code>, <code>-99</code>, etc.). The default is <code>NA</code>.</p>
</td></tr>
<tr><td><code id="measr_dcm_+3A_qmatrix">qmatrix</code></td>
<td>
<p>The Q-matrix. A data frame with 1 row per item and 1 column
per attribute. All cells should be either 0 (item does not measure the
attribute) or 1 (item does measure the attribute).</p>
</td></tr>
<tr><td><code id="measr_dcm_+3A_resp_id">resp_id</code></td>
<td>
<p>Optional. Variable name of a column in <code>data</code> that
contains respondent identifiers. <code>NULL</code> (the default) indicates that no
identifiers are present in the data, and row numbers will be used as
identifiers.</p>
</td></tr>
<tr><td><code id="measr_dcm_+3A_item_id">item_id</code></td>
<td>
<p>Optional. Variable name of a column in <code>qmatrix</code> that contains
item identifiers. <code>NULL</code> (the default) indicates that no identifiers are
present in the Q-matrix. In this case, the column names of <code>data</code>
(excluding any column specified in <code>resp_id</code>) will be used as the item
identifiers. <code>NULL</code> also assumes that the order of the rows in the Q-matrix
is the same as the order of the columns in <code>data</code> (i.e., the item in row 1
of <code>qmatrix</code> is the item in column 1 of <code>data</code>, excluding <code>resp_id</code>).</p>
</td></tr>
<tr><td><code id="measr_dcm_+3A_type">type</code></td>
<td>
<p>Type of DCM to estimate. Must be one of
lcdm, dina, dino, or crum.</p>
</td></tr>
<tr><td><code id="measr_dcm_+3A_max_interaction">max_interaction</code></td>
<td>
<p>If <code>type = "lcdm"</code>, the highest level of interaction
to estimate. The default is to estimate all possible interactions. For
example, an item that measures 4 attributes would have 4 main effects,
6 two-way interactions, 4 three-way interactions, and 1 four-way
interaction. Setting <code>max_interaction = 2</code> would result in only estimating
the main effects and two-way interactions, excluding the three- and four-
way interactions.</p>
</td></tr>
<tr><td><code id="measr_dcm_+3A_attribute_structure">attribute_structure</code></td>
<td>
<p>Structural model specification. Must be one of
unconstrained, or independent.
<code>unconstrained</code> makes no assumptions about the relationships between
attributes, whereas <code>independent</code> assumes that proficiency statuses on
attributes are independent of each other.</p>
</td></tr>
<tr><td><code id="measr_dcm_+3A_method">method</code></td>
<td>
<p>Estimation method. Options are <code>"mcmc"</code>, which uses Stan's
sampling method, or <code>"optim"</code>, which uses Stan's optimizer.</p>
</td></tr>
<tr><td><code id="measr_dcm_+3A_prior">prior</code></td>
<td>
<p>A <a href="#topic+measrprior">measrprior</a> object. If <code>NULL</code>, default priors
are used, as specified by <code><a href="#topic+default_dcm_priors">default_dcm_priors()</a></code>.</p>
</td></tr>
<tr><td><code id="measr_dcm_+3A_backend">backend</code></td>
<td>
<p>Character string naming the package to use as the backend for
fitting the Stan model. Options are <code>"rstan"</code> (the default) or
<code>"cmdstanr"</code>. Can be set globally for the current R session via the
&quot;measr.backend&quot; option (see <code><a href="base.html#topic+options">options()</a></code>). Details on the <strong>rstan</strong> and
<strong>cmdstanr</strong> packages are available at <a href="https://mc-stan.org/rstan/">https://mc-stan.org/rstan/</a> and
<a href="https://mc-stan.org/cmdstanr/">https://mc-stan.org/cmdstanr/</a>, respectively.</p>
</td></tr>
<tr><td><code id="measr_dcm_+3A_file">file</code></td>
<td>
<p>Either <code>NULL</code> (the default) or a character string. If a character
string, the fitted model object is saved as an <code>.rds</code> object using
<code><a href="base.html#topic+saveRDS">saveRDS()</a></code> using the supplied character string. The <code>.rds</code> extension
is automatically added. If the specified file already exists, <strong>measr</strong>
will load the previously saved model. Unless <code>file_refit</code> is specified, the
model will not be refit.</p>
</td></tr>
<tr><td><code id="measr_dcm_+3A_file_refit">file_refit</code></td>
<td>
<p>Controls when a saved model is refit. Options are
<code>"never"</code>, <code>"always"</code>, and <code>"on_change"</code>. Can be set globally for the
current R session via the &quot;measr.file_refit&quot; option (see <code><a href="base.html#topic+options">options()</a></code>).
</p>

<ul>
<li><p> For <code>"never"</code> (the default), the fitted model is always loaded if the
<code>file</code> exists, and model fitting is skipped.
</p>
</li>
<li><p> For <code>"always"</code>, the model is always refitted, regardless of whether or
not <code>file</code> exists.
</p>
</li>
<li><p> For <code>"on_change"</code>, the model will be refit if the <code>data</code>, <code>prior</code>, or
<code>method</code> specified are different from that in the saved <code>file</code>.
</p>
</li></ul>
</td></tr>
<tr><td><code id="measr_dcm_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to Stan.
</p>

<ul>
<li><p> For <code>backend = "rstan"</code>, arguments are passed to <code><a href="rstan.html#topic+stanmodel-method-sampling">rstan::sampling()</a></code>
or <code><a href="rstan.html#topic+stanmodel-method-optimizing">rstan::optimizing()</a></code>.
</p>
</li>
<li><p> For <code>backend = "cmdstanr"</code>, arguments are passed to the
<a href="https://mc-stan.org/cmdstanr/reference/model-method-sample.html">sample</a>
or
<a href="https://mc-stan.org/cmdstanr/reference/model-method-optimize.html">optimize</a>
methods of the
<a href="https://mc-stan.org/cmdstanr/reference/CmdStanModel.html">CmdStanModel</a>
class.
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+measrfit">measrfit</a> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
rstn_mdm_lcdm &lt;- measr_dcm(
  data = mdm_data, missing = NA, qmatrix = mdm_qmatrix,
  resp_id = "respondent", item_id = "item", type = "lcdm",
  method = "optim", seed = 63277, backend = "rstan"
)

</code></pre>

<hr>
<h2 id='measr_examples'>Determine if code is executed interactively or in pkgdown</h2><span id='topic+measr_examples'></span>

<h3>Description</h3>

<p>Used for determining examples that shouldn't be run on CRAN, but can be run
for the pkgdown website.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>measr_examples()
</code></pre>


<h3>Value</h3>

<p>A logical value indicating whether or not the examples should be run.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>measr_examples()
</code></pre>

<hr>
<h2 id='measr_extract'>Extract components of a <code>measrfit</code> object.</h2><span id='topic+measr_extract'></span><span id='topic+measr_extract.measrdcm'></span>

<h3>Description</h3>

<p>Extract components of a <code>measrfit</code> object.
</p>
<p>Extract components of an estimated diagnostic classification model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>measr_extract(model, ...)

## S3 method for class 'measrdcm'
measr_extract(model, what, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="measr_extract_+3A_model">model</code></td>
<td>
<p>The estimated to extract information from.</p>
</td></tr>
<tr><td><code id="measr_extract_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to each extract method.
</p>

<ul>
<li> <p><code>ppmc_interval</code>:
</p>
<p>For <code>what = "odds_ratio_flags"</code> and
<code>what = "conditional_prob_flags"</code>, the compatibility interval used for
determining model fit flags to return. For example, a <code>ppmc_interval</code> of
0.95 (the default) will return any PPMCs where the posterior predictive
<em>p</em>-value (ppp) is less than 0.025 or greater than 0.975.
</p>
</li>
<li> <p><code>agreement</code>:
</p>
<p>For <code>what = "classification_reliability"</code>, additional
measures of agreement to include. By default, the classification
accuracy and consistency metrics defined Johnson &amp; Sinharay (2018) are
returned. Additional metrics that can be specified to <code>agreement</code> are
Goodman &amp; Kruskal's lambda (<code>lambda</code>), Cohen's kappa (<code>kappa</code>), Youden's
statistic (<code>youden</code>), the tetrachoric correlation (<code>tetra</code>), true
positive rate (<code>tp</code>), and the true negative rate (<code>tn</code>).
</p>
<p>For <code>what = "probability_reliability"</code>, additional measures of agreement
to include. By default, the informational reliability index defined by
Johnson &amp; Sinharay (2020) is returned. Additional metrics that can be
specified to <code>agreement</code> are the point biserial reliability index (<code>bs</code>),
parallel forms reliability index (<code>pf</code>), and the tetrachoric reliability
index (<code>tb</code>), which was originally defined by Templin &amp; Bradshaw (2013).
</p>
</li></ul>
</td></tr>
<tr><td><code id="measr_extract_+3A_what">what</code></td>
<td>
<p>Character string. The information to be extracted. See details
for available options.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For diagnostic classification models, we can extract the following
information:
</p>

<ul>
<li> <p><code>item_param</code>: The estimated item parameters. This shows the name of the
parameter, the class of the parameter, and the estimated value.
</p>
</li>
<li> <p><code>strc_param</code>: The estimated structural parameters. This is the base rate
of membership in each class. This shows the class pattern and the
estimated proportion of respondents in each class.
</p>
</li>
<li> <p><code>prior</code>: The priors used when estimating the model.
</p>
</li>
<li> <p><code>classes</code>: The possible classes or profile patterns. This will show the
class label (i.e., the pattern of proficiency) and the attributes
included in each class.
</p>
</li>
<li> <p><code>class_prob</code>: The probability that each respondent belongs to class
(i.e., has the given pattern of proficiency).
</p>
</li>
<li> <p><code>attribute_prob</code>: The proficiency probability for each respondent and
attribute.
</p>
</li>
<li> <p><code>m2</code>: The M<sub>2</sub> fit statistic.
See <code><a href="#topic+fit_m2">fit_m2()</a></code> for details. Model fit information must first be added to
the model using <code><a href="#topic+add_fit">add_fit()</a></code>.
</p>
</li>
<li> <p><code>rmsea</code>: The root mean square error of approximation (RMSEA) fit
statistic and associated confidence interval. See <code><a href="#topic+fit_m2">fit_m2()</a></code> for details.
Model fit information must first be added to the model using <code><a href="#topic+add_fit">add_fit()</a></code>.
</p>
</li>
<li> <p><code>srmsr</code>: The standardized root mean square residual (SRMSR) fit
statistic. See <code><a href="#topic+fit_m2">fit_m2()</a></code> for details. Model fit information must first
be added to the model using <code><a href="#topic+add_fit">add_fit()</a></code>.
</p>
</li>
<li> <p><code>ppmc_raw_score</code>: The observed and posterior predicted chi-square
statistic for the raw score distribution. See <code><a href="#topic+fit_ppmc">fit_ppmc()</a></code> for details.
Model fit information must first be added to the model using <code><a href="#topic+add_fit">add_fit()</a></code>.
</p>
</li>
<li> <p><code>ppmc_conditional_prob</code>: The observed and posterior predicted conditional
probabilities of each class providing a correct response to each item.
See <code><a href="#topic+fit_ppmc">fit_ppmc()</a></code> for details.
Model fit information must first be added to the model using <code><a href="#topic+add_fit">add_fit()</a></code>.
</p>
</li>
<li> <p><code>ppmc_conditional_prob_flags</code>: A subset of the PPMC conditional
probabilities where the <em>ppp</em> is outside the specified <code>ppmc_interval</code>.
</p>
</li>
<li> <p><code>ppmc_odds_ratio</code>: The observed and posterior predicted odds ratios of
each item pair. See <code><a href="#topic+fit_ppmc">fit_ppmc()</a></code> for details.
Model fit information must first be added to the model using <code><a href="#topic+add_fit">add_fit()</a></code>.
</p>
</li>
<li> <p><code>ppmc_odds_ratio_flags</code>: A subset of the PPMC odds ratios where the <em>ppp</em>
is outside the specified <code>ppmc_interval</code>.
</p>
</li>
<li> <p><code>loo</code>: The leave-one-out cross validation results. See <code><a href="loo.html#topic+loo">loo::loo()</a></code> for
details. The information criterion must first be added to the model using
<code><a href="#topic+add_criterion">add_criterion()</a></code>.
</p>
</li>
<li> <p><code>waic</code>: The widely applicable information criterion results. See
<code><a href="loo.html#topic+waic">loo::waic()</a></code> for details. The information criterion must first be added
to the model using <code><a href="#topic+add_criterion">add_criterion()</a></code>.
</p>
</li>
<li> <p><code>pattern_reliability</code>: The accuracy and consistency of the overall
attribute profile classification, as described by Cui et al. (2012).
Reliability information must first be added to the model using
<code><a href="#topic+add_reliability">add_reliability()</a></code>.
</p>
</li>
<li> <p><code>classification_reliability</code>: The classification accuracy and consistency
for each attribute, using the metrics described by Johnson &amp; Sinharay
(2018). Reliability information must first be added to the model using
<code><a href="#topic+add_reliability">add_reliability()</a></code>.
</p>
</li>
<li> <p><code>probability_reliability</code>: Reliability estimates for the probability of
proficiency on each attribute, as described by Johnson &amp; Sinharay (2020).
Reliability information must first be added to the model using
<code><a href="#topic+add_reliability">add_reliability()</a></code>.
</p>
</li></ul>



<h3>Value</h3>

<p>The extracted information. The specific structure will vary depending
on what is being extracted, but usually the returned object is a
<a href="tibble.html#topic+tibble-package">tibble</a> with the requested information.
</p>


<h3>Methods (by class)</h3>


<ul>
<li> <p><code>measr_extract(measrdcm)</code>: Extract components of an estimated diagnostic
classification model.
</p>
</li></ul>


<h3>References</h3>

<p>Cui, Y., Gierl, M. J., &amp; Chang, H.-H. (2012). Estimating
classification consistency and accuracy for cognitive diagnostic
assessment. <em>Journal of Educational Measurement, 49</em>(1), 19-38.
<a href="https://doi.org/10.1111/j.1745-3984.2011.00158.x">doi:10.1111/j.1745-3984.2011.00158.x</a>
</p>
<p>Johnson, M. S., &amp; Sinharay, S. (2018). Measures of agreement to
assess attribute-level classification accuracy and consistency for
cognitive diagnostic assessments. <em>Journal of Educational Measurement,
55</em>(4), 635-664. <a href="https://doi.org/10.1111/jedm.12196">doi:10.1111/jedm.12196</a>
</p>
<p>Johnson, M. S., &amp; Sinharay, S. (2020). The reliability of the
posterior probability of skill attainment in diagnostic classification
models. <em>Journal of Educational and Behavioral Statistics, 45</em>(1), 5-31.
<a href="https://doi.org/10.3102/1076998619864550">doi:10.3102/1076998619864550</a>
</p>
<p>Templin, J., &amp; Bradshaw, L. (2013). Measuring the reliability of
diagnostic classification model examinee estimates. <em>Journal of
Classification, 30</em>(2), 251-275. <a href="https://doi.org/10.1007/s00357-013-9129-4">doi:10.1007/s00357-013-9129-4</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
rstn_mdm_lcdm &lt;- measr_dcm(
  data = mdm_data, missing = NA, qmatrix = mdm_qmatrix,
  resp_id = "respondent", item_id = "item", type = "lcdm",
  method = "optim", seed = 63277, backend = "rstan"
)

measr_extract(rstn_mdm_lcdm, "strc_param")

</code></pre>

<hr>
<h2 id='measr-package'>measr: Bayesian Psychometric Measurement Using 'Stan'</h2><span id='topic+measr'></span><span id='topic+measr-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>Estimate diagnostic classification models (also called cognitive diagnostic models) with 'Stan'. Diagnostic classification models are confirmatory latent class models, as described by Rupp et al. (2010, ISBN: 978-1-60623-527-0). Automatically generate 'Stan' code for the general loglinear cognitive diagnostic diagnostic model proposed by Henson et al. (2009) <a href="https://doi.org/10.1007/s11336-008-9089-5">doi:10.1007/s11336-008-9089-5</a> and other subtypes that introduce additional model constraints. Using the generated 'Stan' code, estimate the model evaluate the model's performance using model fit indices, information criteria, and reliability metrics.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: W. Jake Thompson <a href="mailto:wjakethompson@gmail.com">wjakethompson@gmail.com</a> (<a href="https://orcid.org/0000-0001-7339-0300">ORCID</a>)
</p>
<p>Other contributors:
</p>

<ul>
<li><p> Nathan Jones <a href="mailto:jonesnateb@gmail.com">jonesnateb@gmail.com</a> (<a href="https://orcid.org/0000-0001-6177-7161">ORCID</a>) [contributor]
</p>
</li>
<li><p> Matthew Johnson (Provided code adapted for reliability.measrdcm()) [copyright holder]
</p>
</li>
<li><p> Paul-Christian BÃ¼rkner (Author of eval_silent()) [copyright holder]
</p>
</li>
<li><p> University of Kansas [copyright holder]
</p>
</li>
<li><p> Institute of Education Sciences [funder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://measr.info">https://measr.info</a>
</p>
</li>
<li> <p><a href="https://github.com/wjakethompson/measr">https://github.com/wjakethompson/measr</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/wjakethompson/measr/issues">https://github.com/wjakethompson/measr/issues</a>
</p>
</li></ul>


<hr>
<h2 id='measrfit-class'>Class <code>measrfit</code> of models fitted with the <strong>measr</strong> package</h2><span id='topic+measrfit-class'></span><span id='topic+measrfit'></span>

<h3>Description</h3>

<p>Models fitted with the <strong>measr</strong> package are represented as a <code>measrfit</code>
object, which contains the posterior draws, Stan code, priors, and other
relevant information.
</p>


<h3>Slots</h3>


<dl>
<dt><code>data</code></dt><dd><p>The data and Q-matrix used to estimate the model.</p>
</dd>
<dt><code>type</code></dt><dd><p>The type of DCM that was estimated.</p>
</dd>
<dt><code>prior</code></dt><dd><p>A <a href="#topic+measrprior">measrprior</a> object containing information on the
priors used in the model.</p>
</dd>
<dt><code>stancode</code></dt><dd><p>The model code in <strong>Stan</strong> language.</p>
</dd>
<dt><code>method</code></dt><dd><p>The method used to fit the model.</p>
</dd>
<dt><code>algorithm</code></dt><dd><p>The name of the algorithm used to fit the model.</p>
</dd>
<dt><code>backend</code></dt><dd><p>The name of the backend used to fit the model.</p>
</dd>
<dt><code>model</code></dt><dd><p>The fitted Stan model. This will object of class
<a href="rstan.html#topic+stanfit-class">rstan::stanfit</a> if <code>backend = "rstan"</code> and
<a href="https://mc-stan.org/cmdstanr/reference/CmdStanMCMC.html"><code>CmdStanMCMC</code></a>
if <code>backend = "cmdstanr"</code> was specified when fitting the model.</p>
</dd>
<dt><code>respondent_estimates</code></dt><dd><p>An empty list for adding estimated person
parameters after fitting the model.</p>
</dd>
<dt><code>fit</code></dt><dd><p>An empty list for adding model fit information after fitting the
model.</p>
</dd>
<dt><code>criteria</code></dt><dd><p>An empty list for adding information criteria after fitting
the model.</p>
</dd>
<dt><code>reliability</code></dt><dd><p>An empty list for adding reliability information after
fitting the model.</p>
</dd>
<dt><code>file</code></dt><dd><p>Optional name of a file which the model objects was saved to
or loaded from.</p>
</dd>
<dt><code>version</code></dt><dd><p>The versions of <strong>measr</strong>, <strong>Stan</strong>, <strong>rstan</strong> and/or
<strong>cmdstanr</strong> that were used to fit the model.</p>
</dd>
</dl>


<h3>See Also</h3>

<p><code><a href="#topic+measr_dcm">measr_dcm()</a></code>
</p>

<hr>
<h2 id='measrprior'>Prior definitions for <strong>measr</strong> models</h2><span id='topic+measrprior'></span><span id='topic+prior'></span><span id='topic+prior_'></span><span id='topic+prior_string'></span>

<h3>Description</h3>

<p>Create prior definitions for classes of parameters, or specific parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>measrprior(
  prior,
  class = c("structural", "intercept", "maineffect", "interaction", "slip", "guess"),
  coef = NA,
  lb = NA,
  ub = NA
)

prior(prior, ...)

prior_(prior, ...)

prior_string(prior, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="measrprior_+3A_prior">prior</code></td>
<td>
<p>A character string defining a distribution in <strong>Stan</strong> language.
A list of all distributions supported by <strong>Stan</strong> can be found in <em>Stan
Language Functions Reference</em> at
<a href="https://mc-stan.org/users/documentation/">https://mc-stan.org/users/documentation/</a>.</p>
</td></tr>
<tr><td><code id="measrprior_+3A_class">class</code></td>
<td>
<p>The parameter class. Defaults to <code>"intercept"</code>. Must be one of
<code>"intercept"</code>, <code>"maineffect"</code>, <code>"interaction"</code> for the LCDM, or one of
<code>"slip"</code> or <code>"guess"</code> for DINA or DINO models.</p>
</td></tr>
<tr><td><code id="measrprior_+3A_coef">coef</code></td>
<td>
<p>Name of a specific parameter within the defined class. If not
defined, the prior is applied to all parameters within the class.</p>
</td></tr>
<tr><td><code id="measrprior_+3A_lb">lb</code></td>
<td>
<p>Lower bound for parameter restriction. Defaults to no restriction.</p>
</td></tr>
<tr><td><code id="measrprior_+3A_ub">ub</code></td>
<td>
<p>Upper bound for parameter restriction. Defaults to no restriction.</p>
</td></tr>
<tr><td><code id="measrprior_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>measrprior()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="tibble.html#topic+tibble-package">tibble</a> of class <code>measrprior</code>.
</p>


<h3>Functions</h3>


<ul>
<li> <p><code>prior()</code>: Alias of <code>measrprior()</code> which allows arguments to be
specified as expressions without quotation marks.
</p>
</li>
<li> <p><code>prior_()</code>: Alias of <code>measrprior()</code> which allows arguments to be
specified as one-sided formulas or wrapped in <code><a href="base.html#topic+substitute">base::quote()</a></code>.
</p>
</li>
<li> <p><code>prior_string()</code>: Alias of <code>measrprior()</code> which allows arguments to be
specified as character strings.
</p>
</li></ul>


<h3>Examples</h3>

<pre><code class='language-R'># Use alias functions to define priors without quotes, as formulas,
# or as character strings.
(prior1 &lt;- prior(lognormal(0, 1), class = maineffect))

(prior2 &lt;- prior_(~lognormal(0, 1), class = ~maineffect))

(prior3 &lt;- prior_string("lognormal(0, 1)", class = "maineffect"))

identical(prior1, prior2)
identical(prior1, prior3)
identical(prior2, prior3)

# Define a prior for an entire class of parameters
prior(beta(5, 25), class = "slip")

# Or for a specific item (e.g., just the slipping parameter for item 7)
prior(beta(5, 25), class = "slip", coef = "slip[7]")
</code></pre>

<hr>
<h2 id='model_evaluation'>Add model evaluation metrics model objects</h2><span id='topic+model_evaluation'></span><span id='topic+add_criterion'></span><span id='topic+add_reliability'></span><span id='topic+add_fit'></span><span id='topic+add_respondent_estimates'></span>

<h3>Description</h3>

<p>Add model evaluation metrics to fitted model objects. These functions are
wrappers around other functions that compute the metrics. The benefit of
using these wrappers is that the model evaluation metrics are saved as part
of the model object so that time-intensive calculations do not need to be
repeated. See Details for specifics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_criterion(
  x,
  criterion = c("loo", "waic"),
  overwrite = FALSE,
  save = TRUE,
  ...,
  r_eff = NA
)

add_reliability(x, overwrite = FALSE, save = TRUE)

add_fit(
  x,
  method = c("m2", "ppmc"),
  overwrite = FALSE,
  save = TRUE,
  ...,
  ci = 0.9
)

add_respondent_estimates(
  x,
  probs = c(0.025, 0.975),
  overwrite = FALSE,
  save = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model_evaluation_+3A_x">x</code></td>
<td>
<p>A <a href="#topic+measrfit">measrfit</a> object.</p>
</td></tr>
<tr><td><code id="model_evaluation_+3A_criterion">criterion</code></td>
<td>
<p>A vector of criteria to calculate and add to the model
object.</p>
</td></tr>
<tr><td><code id="model_evaluation_+3A_overwrite">overwrite</code></td>
<td>
<p>Logical. Indicates whether specified elements that have
already been added to the estimated model should be overwritten. Default is
<code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="model_evaluation_+3A_save">save</code></td>
<td>
<p>Logical. Only relevant if a file was specified in the
<a href="#topic+measrfit">measrfit</a> object passed to <code>x</code>. If <code>TRUE</code> (the default), the model is
re-saved to the specified file when new criteria are added to the R object.
If <code>FALSE</code>, the new criteria will be added to the R object, but the saved
file will not be updated.</p>
</td></tr>
<tr><td><code id="model_evaluation_+3A_...">...</code></td>
<td>
<p>Additional arguments passed relevant methods. See Details.</p>
</td></tr>
<tr><td><code id="model_evaluation_+3A_r_eff">r_eff</code></td>
<td>
<p>Vector of relative effective sample size estimates for the
likelihood (<code>exp(log_lik)</code>) of each observation. This is related to
the relative efficiency of estimating the normalizing term in
self-normalizing importance sampling when using posterior draws obtained
with MCMC. If MCMC draws are used and <code>r_eff</code> is not provided then
the reported PSIS effective sample sizes and Monte Carlo error estimates
will be over-optimistic. If the posterior draws are independent then
<code>r_eff=1</code> and can be omitted. The warning message thrown when <code>r_eff</code> is
not specified can be disabled by setting <code>r_eff</code> to <code>NA</code>. See the
<code><a href="loo.html#topic+relative_eff">relative_eff()</a></code> helper functions for computing <code>r_eff</code>.</p>
</td></tr>
<tr><td><code id="model_evaluation_+3A_method">method</code></td>
<td>
<p>A vector of model fit methods to evaluate and add to the model
object.</p>
</td></tr>
<tr><td><code id="model_evaluation_+3A_ci">ci</code></td>
<td>
<p>The confidence interval for the RMSEA, computed from the M2</p>
</td></tr>
<tr><td><code id="model_evaluation_+3A_probs">probs</code></td>
<td>
<p>The percentiles to be computed by the <code style="white-space: pre;">&#8288;[stats::quantile()]&#8288;</code>
function to summarize the posterior distributions of each person parameter.
Only relevant if <code>method = "mcmc"</code> was used to estimate the model.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>add_respondent_estimates()</code>, estimated person parameters are added to
the <code style="white-space: pre;">&#8288;$respondent_estimates&#8288;</code> element of the fitted model.
</p>
<p>For <code>add_fit()</code>, model and item fit information are added to the <code style="white-space: pre;">&#8288;$fit&#8288;</code>
element of the fitted model. This function wraps <code><a href="#topic+fit_m2">fit_m2()</a></code> to calculate the
M<sub>2</sub> statistic (Hansen et al., 2016;
Liu et al., 2016) and/or <code><a href="#topic+fit_ppmc">fit_ppmc()</a></code> to calculate posterior predictive model
checks (Park et al., 2015; Sinharay &amp; Almond, 2007; Sinharay et al., 2006;
Thompson, 2019), depending on which methods are specified. Additional
arguments supplied to <code>...</code> are passed to <code><a href="#topic+fit_ppmc">fit_ppmc()</a></code>.
</p>
<p>For <code>add_criterion()</code>, relative fit criteria are added to the <code style="white-space: pre;">&#8288;$criteria&#8288;</code>
element of the fitted model. This function wraps <code><a href="#topic+loo">loo()</a></code> and/or <code><a href="#topic+waic">waic()</a></code>,
depending on which criteria are specified, to calculate the leave-one-out
(LOO; Vehtari et al., 2017) and/or widely applicable information criteria
(WAIC; Watanabe, 2010) to fitted model objects. Additional arguments supplied
to <code>...</code> are passed to <code><a href="loo.html#topic+loo">loo::loo.array()</a></code> or <code><a href="loo.html#topic+waic">loo::waic.array()</a></code>.
</p>
<p>For <code>add_reliability()</code>, reliability information is added to the
<code style="white-space: pre;">&#8288;$reliability&#8288;</code> element of the fitted model. Pattern level reliability is
described by Cui et al. (2012). Classification reliability and posterior
probability reliability are described by Johnson &amp; Sinharay (2018, 2020),
respectively. This function wraps <code><a href="#topic+reliability">reliability()</a></code>.
</p>


<h3>Value</h3>

<p>A modified <a href="#topic+measrfit">measrfit</a> object with the corresponding slot populated
with the specified information.
</p>


<h3>References</h3>

<p>Cui, Y., Gierl, M. J., &amp; Chang, H.-H. (2012). Estimating
classification consistency and accuracy for cognitive diagnostic
assessment. <em>Journal of Educational Measurement, 49</em>(1), 19-38.
<a href="https://doi.org/10.1111/j.1745-3984.2011.00158.x">doi:10.1111/j.1745-3984.2011.00158.x</a>
</p>
<p>Hansen, M., Cai, L., Monroe, S., &amp; Li, Z. (2016).
Limited-information goodness-of-fit testing of diagnostic classification
item response models. <em>British Journal of Mathematical and Statistical
Psychology, 69</em>(3), 225-252. <a href="https://doi.org/10.1111/bmsp.12074">doi:10.1111/bmsp.12074</a>
</p>
<p>Johnson, M. S., &amp; Sinharay, S. (2018). Measures of agreement to
assess attribute-level classification accuracy and consistency for
cognitive diagnostic assessments. <em>Journal of Educational Measurement,
55</em>(4), 635-664. <a href="https://doi.org/10.1111/jedm.12196">doi:10.1111/jedm.12196</a>
</p>
<p>Johnson, M. S., &amp; Sinharay, S. (2020). The reliability of the
posterior probability of skill attainment in diagnostic classification
models. <em>Journal of Educational and Behavioral Statistics, 45</em>(1), 5-31.
<a href="https://doi.org/10.3102/1076998619864550">doi:10.3102/1076998619864550</a>
</p>
<p>Liu, Y., Tian, W., &amp; Xin, T. (2016). An application of
M<sub>2</sub> statistic to evaluate the fit
of cognitive diagnostic models. <em>Journal of Educational and Behavioral
Statistics, 41</em>(1), 3-26. <a href="https://doi.org/10.3102/1076998615621293">doi:10.3102/1076998615621293</a>
</p>
<p>Park, J. Y., Johnson, M. S., Lee, Y-S. (2015). Posterior
predictive model checks for cognitive diagnostic models. <em>International
Journal of Quantitative Research in Education, 2</em>(3-4), 244-264.
<a href="https://doi.org/10.1504/IJQRE.2015.071738">doi:10.1504/IJQRE.2015.071738</a>
</p>
<p>Sinharay, S., &amp; Almond, R. G. (2007). Assessing fit of cognitive
diagnostic models. <em>Educational and Psychological Measurement, 67</em>(2),
239-257. <a href="https://doi.org/10.1177/0013164406292025">doi:10.1177/0013164406292025</a>
</p>
<p>Sinharay, S., Johnson, M. S., &amp; Stern, H. S. (2006). Posterior
predictive assessment of item response theory models. <em>Applied
Psychological Measurement, 30</em>(4), 298-321.
<a href="https://doi.org/10.1177/0146621605285517">doi:10.1177/0146621605285517</a>
</p>
<p>Thompson, W. J. (2019). <em>Bayesian psychometrics for diagnostic
assessments: A proof of concept</em> (Research Report No. 19-01). University
of Kansas; Accessible Teaching, Learning, and Assessment Systems.
<a href="https://doi.org/10.35542/osf.io/jzqs8">doi:10.35542/osf.io/jzqs8</a>
</p>
<p>Vehtari, A., Gelman, A., &amp; Gabry, J. (2017). Practical Bayesian
model evaluation using leave-one-out cross-validation and WAIC.
<em>Statistics and Computing, 27</em>(5), 1413-1432.
<a href="https://doi.org/10.1007/s11222-016-9696-4">doi:10.1007/s11222-016-9696-4</a>
</p>
<p>Watanabe, S. (2010). Asymptotic equivalence of Bayes cross
validation and widely applicable information criterion in singular learning
theory. <em>Journal of Machine Learning Research, 11</em>(116), 3571-3594.
<a href="https://jmlr.org/papers/v11/watanabe10a.html">https://jmlr.org/papers/v11/watanabe10a.html</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
cmds_mdm_dina &lt;- measr_dcm(
  data = mdm_data, missing = NA, qmatrix = mdm_qmatrix,
  resp_id = "respondent", item_id = "item", type = "dina",
  method = "optim", seed = 63277, backend = "rstan",
  prior = c(prior(beta(5, 17), class = "slip"),
            prior(beta(5, 17), class = "guess"))
)

cmds_mdm_dina &lt;- add_reliability(cmds_mdm_dina)
cmds_mdm_dina &lt;- add_fit(cmds_mdm_dina, method = "m2")
cmds_mdm_dina &lt;- add_respondent_estimates(cmds_mdm_dina)

</code></pre>

<hr>
<h2 id='predict.measrdcm'>Posterior draws of respondent proficiency</h2><span id='topic+predict.measrdcm'></span>

<h3>Description</h3>

<p>Calculate posterior draws of respondent proficiency. Optionally retain all
posterior draws or return only summaries of the distribution for each
respondent.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'measrdcm'
predict(
  object,
  newdata = NULL,
  resp_id = NULL,
  missing = NA,
  summary = TRUE,
  probs = c(0.025, 0.975),
  force = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.measrdcm_+3A_object">object</code></td>
<td>
<p>An object of class <code>measrdcm</code>. Generated from <code><a href="#topic+measr_dcm">measr_dcm()</a></code>.</p>
</td></tr>
<tr><td><code id="predict.measrdcm_+3A_newdata">newdata</code></td>
<td>
<p>Optional new data. If not provided, the data used to estimate
the model is scored. If provided, <code>newdata</code> should be a data frame with 1
row per respondent and 1 column per item. All items that appear in
<code>newdata</code> should appear in the data used to estimate <code>object</code>.</p>
</td></tr>
<tr><td><code id="predict.measrdcm_+3A_resp_id">resp_id</code></td>
<td>
<p>Optional. Variable name of a column in <code>newdata</code> that
contains respondent identifiers. <code>NULL</code> (the default) indicates that no
identifiers are present in the data, and row numbers will be used as
identifiers. If <code>newdata</code> is not specified and the data used to estimate
the model is scored, the <code>resp_id</code> is taken from the original data.</p>
</td></tr>
<tr><td><code id="predict.measrdcm_+3A_missing">missing</code></td>
<td>
<p>An R expression specifying how missing data in <code>data</code> is coded
(e.g., <code>NA</code>, <code>"."</code>, <code>-99</code>, etc.). The default is <code>NA</code>.</p>
</td></tr>
<tr><td><code id="predict.measrdcm_+3A_summary">summary</code></td>
<td>
<p>Should summary statistics be returned instead of the raw
posterior draws? Only relevant if the model was estimated with
<code>method = "mcmc"</code>. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="predict.measrdcm_+3A_probs">probs</code></td>
<td>
<p>The percentiles to be computed by the <code style="white-space: pre;">&#8288;[stats::quantile()]&#8288;</code>
function. Only relevant if the model was estimated with <code>method = "mcmc"</code>.
Only used if <code>summary</code> is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="predict.measrdcm_+3A_force">force</code></td>
<td>
<p>If respondent estimates have already been added to the model
object with <code><a href="#topic+add_respondent_estimates">add_respondent_estimates()</a></code>, should they be recalculated.
Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="predict.measrdcm_+3A_...">...</code></td>
<td>
<p>Unused.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with two elements: <code>class_probabilities</code> and
<code>attribute_probabilities</code>.
</p>
<p>If summary is <code>FALSE</code>, each element is a tibble with the number of rows
equal to the number of draws in <code>object</code> with columns: <code>.chain</code>,
<code>.iteration</code>, <code>.draw</code>, the respondent identifier, and one column of
probabilities for each of the possible classes.
</p>
<p>If summary is <code>TRUE</code>, each element is a tibble with one row per respondent
and class or attribute, and columns of the respondent identifier, <code>class</code>
or <code>attribute</code>, <code>mean</code>, and one column for every value specified in
<code>probs</code>.
</p>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+loo'></span><span id='topic+waic'></span><span id='topic+loo_compare'></span><span id='topic+fit_m2'></span><span id='topic+as_draws'></span><span id='topic+E'></span><span id='topic+Pr'></span><span id='topic+rvar_median'></span><span id='topic+rvar_sum'></span><span id='topic+rvar_prod'></span><span id='topic+rvar_min'></span><span id='topic+rvar_max'></span><span id='topic+rvar_mean'></span><span id='topic+rvar_sd'></span><span id='topic+rvar_var'></span><span id='topic+rvar_mad'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>dcm2</dt><dd><p><code><a href="dcm2.html#topic+fit_m2">fit_m2</a></code></p>
</dd>
<dt>loo</dt><dd><p><code><a href="loo.html#topic+loo">loo</a></code>, <code><a href="loo.html#topic+loo_compare">loo_compare</a></code>, <code><a href="loo.html#topic+waic">waic</a></code></p>
</dd>
<dt>posterior</dt><dd><p><code><a href="posterior.html#topic+draws">as_draws</a></code>, <code><a href="posterior.html#topic+rvar-summaries-over-draws">E</a></code>, <code><a href="posterior.html#topic+rvar-summaries-over-draws">Pr</a></code>, <code><a href="posterior.html#topic+rvar-summaries-within-draws">rvar_mad</a></code>, <code><a href="posterior.html#topic+rvar-summaries-within-draws">rvar_max</a></code>, <code><a href="posterior.html#topic+rvar-summaries-within-draws">rvar_mean</a></code>, <code><a href="posterior.html#topic+rvar-summaries-within-draws">rvar_median</a></code>, <code><a href="posterior.html#topic+rvar-summaries-within-draws">rvar_min</a></code>, <code><a href="posterior.html#topic+rvar-summaries-within-draws">rvar_prod</a></code>, <code><a href="posterior.html#topic+rvar-summaries-within-draws">rvar_sd</a></code>, <code><a href="posterior.html#topic+rvar-summaries-within-draws">rvar_sum</a></code>, <code><a href="posterior.html#topic+rvar-summaries-within-draws">rvar_var</a></code></p>
</dd>
</dl>

<hr>
<h2 id='reliability'>Estimate the reliability of psychometric models</h2><span id='topic+reliability'></span><span id='topic+reliability.measrdcm'></span>

<h3>Description</h3>

<p>For diagnostic classification models, reliability can be estimated at the
pattern or attribute level. Pattern-level reliability represents the
classification consistency and accuracy of placing students into an overall
mastery profile. Rather than an overall profile, attributes can also be
scored individually. In this case, classification consistency and accuracy
should be evaluated for each individual attribute, rather than the overall
profile. This is referred to as the <em>maximum a posteriori</em> (MAP) reliability.
Finally, it may be desirable to report results as the probability of
proficiency or mastery on each attribute instead of a proficient/not
proficient classification. In this case, the reliability of the posterior
probability should be reported. This is the <em>expected a posteriori</em> (EAP)
reliability.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reliability(model, ...)

## S3 method for class 'measrdcm'
reliability(model, ..., force = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reliability_+3A_model">model</code></td>
<td>
<p>The estimated model to be evaluated.</p>
</td></tr>
<tr><td><code id="reliability_+3A_...">...</code></td>
<td>
<p>Unused. For future extensions.</p>
</td></tr>
<tr><td><code id="reliability_+3A_force">force</code></td>
<td>
<p>If reliability information has already been added to the model
object with <code><a href="#topic+add_reliability">add_reliability()</a></code>, should it be recalculated. Default is
<code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The pattern-level reliability (<code>pattern_reliability</code>) statistics are
described in Cui et al. (2012). Attribute-level classification reliability
statistics (<code>map_reliability</code>) are described in Johnson &amp; Sinharay (2018).
Reliability statistics for the posterior mean of the skill indicators (i.e.,
the mastery or proficiency probabilities; <code>eap_reliability</code>) are described in
Johnson &amp; Sinharay (2019).
</p>


<h3>Value</h3>

<p>For class <code>measrdcm</code>, a list with 3 elements:
</p>

<ul>
<li> <p><code>pattern_reliability</code>: The pattern-level accuracy (<code>p_a</code>) and consistency
(<code>p_c</code>) described by Cui et al. (2012).
</p>
</li>
<li> <p><code>map_reliability</code>: A list with 2 elements: <code>accuracy</code> and <code>consistency</code>,
which include the attribute-level classification reliability statistics
described by Johnson &amp; Sinharay (2018).
</p>
</li>
<li> <p><code>eap_reliability</code>: The attribute-level posterior probability reliability
statistics described by Johnson &amp; Sinharay (2020).
</p>
</li></ul>



<h3>Methods (by class)</h3>


<ul>
<li> <p><code>reliability(measrdcm)</code>: Reliability measures for diagnostic classification
models.
</p>
</li></ul>


<h3>References</h3>

<p>Cui, Y., Gierl, M. J., &amp; Chang, H.-H. (2012). Estimating
classification consistency and accuracy for cognitive diagnostic
assessment. <em>Journal of Educational Measurement, 49</em>(1), 19-38.
<a href="https://doi.org/10.1111/j.1745-3984.2011.00158.x">doi:10.1111/j.1745-3984.2011.00158.x</a>
</p>
<p>Johnson, M. S., &amp; Sinharay, S. (2018). Measures of agreement to
assess attribute-level classification accuracy and consistency for
cognitive diagnostic assessments. <em>Journal of Educational Measurement,
55</em>(4), 635-664. <a href="https://doi.org/10.1111/jedm.12196">doi:10.1111/jedm.12196</a>
</p>
<p>Johnson, M. S., &amp; Sinharay, S. (2020). The reliability of the
posterior probability of skill attainment in diagnostic classification
models. <em>Journal of Educational and Behavioral Statistics, 45</em>(1), 5-31.
<a href="https://doi.org/10.3102/1076998619864550">doi:10.3102/1076998619864550</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
rstn_mdm_lcdm &lt;- measr_dcm(
  data = mdm_data, missing = NA, qmatrix = mdm_qmatrix,
  resp_id = "respondent", item_id = "item", type = "lcdm",
  method = "optim", seed = 63277, backend = "rstan"
)

reliability(rstn_mdm_lcdm)

</code></pre>

<hr>
<h2 id='tidyeval'>Tidy eval helpers</h2><span id='topic+tidyeval'></span><span id='topic+enquo'></span><span id='topic+enquos'></span><span id='topic+.data'></span><span id='topic++3A+3D'></span><span id='topic+as_name'></span><span id='topic+as_label'></span>

<h3>Description</h3>

<p>This page lists the tidy eval tools reexported in this package from
rlang. To learn about using tidy eval in scripts and packages at a
high level, see the <a href="https://dplyr.tidyverse.org/articles/programming.html">dplyr programming vignette</a>
and the <a href="https://ggplot2.tidyverse.org/articles/ggplot2-in-packages.html">ggplot2 in packages vignette</a>.
The <a href="https://adv-r.hadley.nz/metaprogramming.html">Metaprogramming section</a> of <a href="https://adv-r.hadley.nz">Advanced R</a> may also be useful for a deeper dive.
</p>

<ul>
<li><p> The tidy eval operators <code style="white-space: pre;">&#8288;{{&#8288;</code>, <code style="white-space: pre;">&#8288;!!&#8288;</code>, and <code style="white-space: pre;">&#8288;!!!&#8288;</code> are syntactic
constructs which are specially interpreted by tidy eval functions.
You will mostly need <code style="white-space: pre;">&#8288;{{&#8288;</code>, as <code style="white-space: pre;">&#8288;!!&#8288;</code> and <code style="white-space: pre;">&#8288;!!!&#8288;</code> are more advanced
operators which you should not have to use in simple cases.
</p>
<p>The curly-curly operator <code style="white-space: pre;">&#8288;{{&#8288;</code> allows you to tunnel data-variables
passed from function arguments inside other tidy eval functions.
<code style="white-space: pre;">&#8288;{{&#8288;</code> is designed for individual arguments. To pass multiple
arguments contained in dots, use <code>...</code> in the normal way.
</p>
<div class="sourceCode"><pre>my_function &lt;- function(data, var, ...) {
  data %&gt;%
    group_by(...) %&gt;%
    summarise(mean = mean({{ var }}))
}
</pre></div>
</li>
<li> <p><code><a href="#topic+enquo">enquo()</a></code> and <code><a href="#topic+enquos">enquos()</a></code> delay the execution of one or several
function arguments. The former returns a single expression, the
latter returns a list of expressions. Once defused, expressions
will no longer evaluate on their own. They must be injected back
into an evaluation context with <code style="white-space: pre;">&#8288;!!&#8288;</code> (for a single expression) and
<code style="white-space: pre;">&#8288;!!!&#8288;</code> (for a list of expressions).
</p>
<div class="sourceCode"><pre>my_function &lt;- function(data, var, ...) {
  # Defuse
  var &lt;- enquo(var)
  dots &lt;- enquos(...)

  # Inject
  data %&gt;%
    group_by(!!!dots) %&gt;%
    summarise(mean = mean(!!var))
}
</pre></div>
<p>In this simple case, the code is equivalent to the usage of <code style="white-space: pre;">&#8288;{{&#8288;</code>
and <code>...</code> above. Defusing with <code>enquo()</code> or <code>enquos()</code> is only
needed in more complex cases, for instance if you need to inspect
or modify the expressions in some way.
</p>
</li>
<li><p> The <code>.data</code> pronoun is an object that represents the current
slice of data. If you have a variable name in a string, use the
<code>.data</code> pronoun to subset that variable with <code>[[</code>.
</p>
<div class="sourceCode"><pre>my_var &lt;- "disp"
mtcars %&gt;% summarise(mean = mean(.data[[my_var]]))
</pre></div>
</li>
<li><p> Another tidy eval operator is <code style="white-space: pre;">&#8288;:=&#8288;</code>. It makes it possible to use
glue and curly-curly syntax on the LHS of <code>=</code>. For technical
reasons, the R language doesn't support complex expressions on
the left of <code>=</code>, so we use <code style="white-space: pre;">&#8288;:=&#8288;</code> as a workaround.
</p>
<div class="sourceCode"><pre>my_function &lt;- function(data, var, suffix = "foo") {
  # Use `{{` to tunnel function arguments and the usual glue
  # operator `{` to interpolate plain strings.
  data %&gt;%
    summarise("{{ var }}_mean_{suffix}" := mean({{ var }}))
}
</pre></div>
</li>
<li><p> Many tidy eval functions like <code>dplyr::mutate()</code> or
<code>dplyr::summarise()</code> give an automatic name to unnamed inputs. If
you need to create the same sort of automatic names by yourself,
use <code>as_label()</code>. For instance, the glue-tunnelling syntax above
can be reproduced manually with:
</p>
<div class="sourceCode"><pre>my_function &lt;- function(data, var, suffix = "foo") {
  var &lt;- enquo(var)
  prefix &lt;- as_label(var)
  data %&gt;%
    summarise("{prefix}_mean_{suffix}" := mean(!!var))
}
</pre></div>
<p>Expressions defused with <code>enquo()</code> (or tunnelled with <code style="white-space: pre;">&#8288;{{&#8288;</code>) need
not be simple column names, they can be arbitrarily complex.
<code>as_label()</code> handles those cases gracefully. If your code assumes
a simple column name, use <code>as_name()</code> instead. This is safer
because it throws an error if the input is not a name as expected.
</p>
</li></ul>



<h3>Value</h3>

<p>See documentation for specific functions in
<a href="rlang.html#topic+rlang-package">rlang</a>.
</p>

<hr>
<h2 id='waic.measrfit'>Widely applicable information criterion (WAIC)</h2><span id='topic+waic.measrfit'></span>

<h3>Description</h3>

<p>A <code><a href="loo.html#topic+waic">loo::waic()</a></code> method that is customized for <code>measrfit</code> objects. This is a
simple wrapper around <code><a href="loo.html#topic+waic">loo::waic.array()</a></code>. See the <strong>loo</strong> package
<a href="https://mc-stan.org/loo/articles/">vignettes</a> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'measrfit'
waic(x, ..., force = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="waic.measrfit_+3A_x">x</code></td>
<td>
<p>A <a href="#topic+measrfit">measrfit</a> object.</p>
</td></tr>
<tr><td><code id="waic.measrfit_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="loo.html#topic+waic">loo::waic.array()</a></code>.</p>
</td></tr>
<tr><td><code id="waic.measrfit_+3A_force">force</code></td>
<td>
<p>If the WAIC criterion has already been added to the model object
with <code><a href="#topic+add_criterion">add_criterion()</a></code>, should it be recalculated. Default is <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The object returned by <code><a href="loo.html#topic+waic">loo::waic.array()</a></code>.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
