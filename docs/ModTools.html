<!DOCTYPE html><html><head><title>Help for package ModTools</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ModTools}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#BestCut'><p>Best Cutpoint for a ROC Curve</p>
</a></li>
<li><a href='#bioChemists'><p>article production by graduate students in biochemistry Ph.D. programs</p></a></li>
<li><a href='#BreuschPaganTest'><p>Breusch-Pagan Test</p></a></li>
<li><a href='#CoeffDiffCI'><p>Confidence Interval for the Difference of Two Coefficients in a Linear Model</p>
</a></li>
<li><a href='#CP'><p>Complexity Parameter of an rpart Model</p>
</a></li>
<li><a href='#d.glass'>
<p>Measurements of Forensic Glass Fragments</p></a></li>
<li><a href='#d.pima'><p>Diabetes survey on Pima Indians</p></a></li>
<li><a href='#FitMod'><p>Wrapper for Several Model Functions</p></a></li>
<li><a href='#LeafRates'><p>Leafrates for the Nodes of an 'rpart' Tree</p>
</a></li>
<li><a href='#LogitBoost'><p>LogitBoost Classification Algorithm</p></a></li>
<li><a href='#ModTools-package'><p>Regression and Classification Tools</p>
</p></a></li>
<li><a href='#Node'><p>Nodes and Splits in an rpart Tree</p>
</a></li>
<li><a href='#Over-/Undersample'><p>Oversample and Undersample</p>
</a></li>
<li><a href='#PlotLift'><p>Lift Charts to Compare Binary Predictive Models</p></a></li>
<li><a href='#predict.zeroinfl'><p>Methods for zeroinfl Objects</p></a></li>
<li><a href='#PredictCI'><p>Confidence Intervals for Predictions of a GLM</p>
</a></li>
<li><a href='#RefLevel'><p>Used Reference Levels in a Linear Model</p>
</a></li>
<li><a href='#Response'><p>Extract the Response from Several Models</p>
</a></li>
<li><a href='#RobSummary'><p>Robust Summary for Linear Models</p>
</a></li>
<li><a href='#ROC'><p>Build a ROC curve</p>
</a></li>
<li><a href='#Rules'><p>Extract Rules from 'rpart' Object</p>
</a></li>
<li><a href='#SplitTrainTest'><p>Split DataFrame in Train an Test Sample</p>
</a></li>
<li><a href='#TModC'><p>Compare Classification Models</p></a></li>
<li><a href='#Tobit'><p>Tobit Regression</p></a></li>
<li><a href='#Tune'><p>Tune Classificators</p>
</a></li>
<li><a href='#VarImp'><p>Variable Importance for Regression and Classification Models</p></a></li>
<li><a href='#zeroinfl'><p>Zero-inflated Count Data Regression</p></a></li>
<li><a href='#zeroinfl.control'><p>Control Parameters for Zero-inflated Count Data Regression</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Tools for Building Regression and Classification Models</td>
</tr>
<tr>
<td>Version:</td>
<td>0.9.6</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-01-25</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Description:</td>
<td>Collection of tools for regression and classification tasks. The package implements a consistent user interface to the most popular regression and classification algorithms, such as random forest, neural networks, C5 trees and support vector machines, and complements it with a handful of auxiliary functions, such as variable importance and a tuning function for the parameters.</td>
</tr>
<tr>
<td>Depends:</td>
<td>DescTools, MASS, nnet, survival, R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Imports:</td>
<td>e1071, C50, rpart, randomForest, pROC, methods, relaimpo,
rpart.plot, lattice, lmtest, car, robustbase, class,
NeuralNetTools, naivebayes, sandwich, AER</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-24 00:33:53 UTC; andri</td>
</tr>
<tr>
<td>Author:</td>
<td>Andri Signorell [aut, cre],
  Bernhard Compton [ctb],
  Marcel Dettling [ctb],
  Alexandre Hainard [ctb],
  Max Kuhn [ctb],
  Frédérique Lisacek [ctb],
  Michal Majka [ctb],
  Markus Müller [ctb],
  Dan Putler [ctb],
  Jean-Charles Sanchez [ctb],
  Natalia Tiberti [ctb],
  Natacha Turck [ctb],
  Jarek Tuszynski [ctb],
  Robin Xavier [ctb],
  Achim Zeileis [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Andri Signorell &lt;andri@signorell.net&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-24 00:52:47 UTC</td>
</tr>
</table>
<hr>
<h2 id='BestCut'>Best Cutpoint for a ROC Curve

</h2><span id='topic+BestCut'></span>

<h3>Description</h3>

<p>Returns the best cutpoint for a given classification model.

</p>


<h3>Usage</h3>

<pre><code class='language-R'>BestCut(x, method = c("youden", "closest.topleft"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BestCut_+3A_x">x</code></td>
<td>
<p>a roc object from the roc function

</p>
</td></tr>
<tr><td><code id="BestCut_+3A_method">method</code></td>
<td>
<p>one of <code>"youden"</code> or <code>"closest.topleft"</code>, controls how the
optimal threshold is determined. See details.

</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>method</code> argument controls how the
optimal threshold is determined.
</p>

<dl>
<dt>'<code>youden</code>'</dt><dd>
<p>Youden's J statistic (Youden, 1950) is employed. The optimal cut-off is the threshold
that maximizes the distance to the identity (diagonal) line. Can
be shortened to &ldquo;y&rdquo;.
</p>
<p>The optimality criterion is:
</p>
<p style="text-align: center;"><code class="reqn">max(sensitivities + specificities)</code>
</p>

</dd>
<dt>'<code>closest.topleft</code>'</dt><dd>
<p>The optimal threshold is the point closest to the top-left part of
the plot with perfect sensitivity or specificity. Can be shortened
to &ldquo;c&rdquo; or &ldquo;t&rdquo;.
</p>
<p>The optimality criterion is:
</p>
<p style="text-align: center;"><code class="reqn">min((1 - sensitivities)^2 + (1- specificities)^2)</code>
</p>

</dd>
</dl>




<h3>Value</h3>

<p>the threshold value





</p>


<h3>Author(s)</h3>

<p>Robin Xavier &lt;pROC-cran@xavier.robin.name&gt;,
Andri Signorell &lt;andri@signorell.net&gt; (interface)

</p>


<h3>References</h3>

<p>Xavier Robin, Natacha Turck, Alexandre Hainard, <em>et al.</em>
(2011) &ldquo;pROC: an open-source package for R and S+ to analyze and
compare ROC curves&rdquo;. <em>BMC Bioinformatics</em>, <b>7</b>, 77.
<a href="https://doi.org/10.1186/1471-2105-12-77">doi:10.1186/1471-2105-12-77</a>.

</p>


<h3>See Also</h3>

<p><code><a href="#topic+ROC">ROC</a></code>

</p>


<h3>Examples</h3>

<pre><code class='language-R'>r.glm &lt;- FitMod(diabetes ~ ., data = d.pima, fitfn="logit")

ROC(r.glm)
BestCut(ROC(r.glm))
</code></pre>

<hr>
<h2 id='bioChemists'>article production by graduate students in biochemistry Ph.D. programs</h2><span id='topic+bioChemists'></span>

<h3>Description</h3>

<p>A sample of 915 biochemistry graduate students.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(bioChemists)</code></pre>


<h3>Format</h3>


<dl>
<dt><code>art</code></dt><dd><p>count of articles produced during last 3 years of Ph.D.</p>
</dd>
<dt><code>fem</code></dt><dd><p>factor indicating gender of student, with levels Men and Women</p>
</dd>
<dt><code>mar</code></dt><dd><p>factor indicating marital status of student, with levels
Single and Married</p>
</dd>
<dt><code>kid5</code></dt><dd><p>number of children aged 5 or younger</p>
</dd>
<dt><code>phd</code></dt><dd><p>prestige of Ph.D. department</p>
</dd>
<dt><code>ment</code></dt><dd><p>count of articles produced by Ph.D. mentor during last 3 years</p>
</dd>
</dl>



<h3>References</h3>

<p>Long, J. Scott. 1990.  The origins of sex differences in
science. <em>Social Forces</em>. 68(3):1297-1316.
</p>
<p>Long, J. Scott. 1997.  <em>Regression Models for Categorical and
Limited Dependent Variables</em>. Thousand Oaks, California: Sage.
</p>

<hr>
<h2 id='BreuschPaganTest'>Breusch-Pagan Test</h2><span id='topic+BreuschPaganTest'></span>

<h3>Description</h3>

<p>Performs the Breusch-Pagan test against heteroskedasticity.</p>


<h3>Usage</h3>

<pre><code class='language-R'>BreuschPaganTest(formula, varformula = NULL, studentize = TRUE, data = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BreuschPaganTest_+3A_formula">formula</code></td>
<td>
<p>a symbolic description for the model to be tested
(or a fitted <code>"lm"</code> object).</p>
</td></tr>
<tr><td><code id="BreuschPaganTest_+3A_varformula">varformula</code></td>
<td>
<p>a formula describing only the potential explanatory variables
for the variance (no dependent variable needed). By default the same
explanatory variables are taken as in the main regression model.</p>
</td></tr>
<tr><td><code id="BreuschPaganTest_+3A_studentize">studentize</code></td>
<td>
<p>logical. If set to <code>TRUE</code> Koenker's studentized
version of the test statistic will be used.</p>
</td></tr>
<tr><td><code id="BreuschPaganTest_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables in the model.
By default the variables are taken from the environment which <code>BreuschPaganTest</code> is
called from.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Breusch-Pagan test fits a linear regression model to the residuals
of a linear regression model
(by default the same explanatory variables are taken as in the main regression
model) and rejects if too much of the variance
is explained by the additional explanatory variables.
</p>
<p>Under <code class="reqn">H_0</code> the test statistic of the Breusch-Pagan test follows a
chi-squared distribution with <code>parameter</code> (the number of regressors without
the constant in the model) degrees of freedom.
</p>
<p>Examples can not only be found on this page, but also on the help pages of the
data sets <code><a href="lmtest.html#topic+bondyield">bondyield</a></code>, <code><a href="lmtest.html#topic+currencysubstitution">currencysubstitution</a></code>,
<code><a href="lmtest.html#topic+growthofmoney">growthofmoney</a></code>, <code><a href="lmtest.html#topic+moneydemand">moneydemand</a></code>,
<code><a href="lmtest.html#topic+unemployment">unemployment</a></code>, <code><a href="lmtest.html#topic+wages">wages</a></code>.
</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>degrees of freedom.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>a character string indicating what type of test was
performed.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>a character string giving the name(s) of the data.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Achim Zeileis &lt;Achim.Zeileis@R-project.org&gt;</p>


<h3>References</h3>

<p>T.S. Breusch &amp; A.R. Pagan (1979),
A Simple Test for Heteroscedasticity and Random Coefficient Variation.
<em>Econometrica</em> <b>47</b>, 1287&ndash;1294
</p>
<p>R. Koenker (1981), A Note on Studentizing a Test for Heteroscedasticity.
<em>Journal of Econometrics</em> <b>17</b>, 107&ndash;112.
</p>
<p>W. Kraemer &amp; H. Sonnberger (1986),
<em>The Linear Regression Model under Test</em>. Heidelberg: Physica
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+lm">lm</a></code>, <code><a href="car.html#topic+ncvTest">ncvTest</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate a regressor
x &lt;- rep(c(-1,1), 50)

## generate heteroskedastic and homoskedastic disturbances
err1 &lt;- rnorm(100, sd=rep(c(1,2), 50))
err2 &lt;- rnorm(100)

## generate a linear relationship
y1 &lt;- 1 + x + err1
y2 &lt;- 1 + x + err2

## perform Breusch-Pagan test
BreuschPaganTest(y1 ~ x)
BreuschPaganTest(y2 ~ x)
</code></pre>

<hr>
<h2 id='CoeffDiffCI'>Confidence Interval for the Difference of Two Coefficients in a Linear Model

</h2><span id='topic+CoeffDiffCI'></span>

<h3>Description</h3>

<p>Calculate the confidence interval for the difference of two coefficients in a linear model.

</p>


<h3>Usage</h3>

<pre><code class='language-R'>CoeffDiffCI(x, coeff, conf.level = 0.95, sides = c("two.sided", "left", "right"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CoeffDiffCI_+3A_x">x</code></td>
<td>
<p>the linear model object

</p>
</td></tr>
<tr><td><code id="CoeffDiffCI_+3A_coeff">coeff</code></td>
<td>
<p>a vector of length two, containing either the names or the index of the two coefficients whose difference should be used

</p>
</td></tr>
<tr><td><code id="CoeffDiffCI_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level of the interval.

</p>
</td></tr>
<tr><td><code id="CoeffDiffCI_+3A_sides">sides</code></td>
<td>
<p>a character string specifying the side of the confidence interval, must be one of <code>"two.sided"</code> (default), <code>"left"</code> or <code>"right"</code>. You can specify just the initial letter. <code>"left"</code> would be analogue to a hypothesis of <code>"greater"</code> in a <code>t.test</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is quite useful in the course of the modelling process.

</p>


<h3>Value</h3>

<p>a numeric vector with 3 elements:
</p>
<table>
<tr><td><code>mean</code></td>
<td>
<p>mean</p>
</td></tr>
<tr><td><code>lwr.ci</code></td>
<td>
<p>lower bound of the confidence interval</p>
</td></tr>
<tr><td><code>upr.ci</code></td>
<td>
<p>upper bound of the confidence interval</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Andri Signorell &lt;andri@signorell.net&gt;</p>


<h3>See Also</h3>

<p><code><a href="car.html#topic+linearHypothesis">linearHypothesis</a>()</code>

</p>


<h3>Examples</h3>

<pre><code class='language-R'># get some model first...
r.lm &lt;- FitMod(Fertility ~ ., data=swiss, fitfn="lm")

# calculate the confidence interval for the difference of the
# coefficients Examination and Education
CoeffDiffCI(r.lm, c("Examination", "Education"))

# the test could be calculated as
car::linearHypothesis(r.lm, "Education = Examination")
</code></pre>

<hr>
<h2 id='CP'>Complexity Parameter of an rpart Model

</h2><span id='topic+CP'></span><span id='topic+print.CP'></span><span id='topic+plot.CP'></span>

<h3>Description</h3>

<p>Extracts, prints and plots the complexity table of an <code>rpart</code> model.

</p>


<h3>Usage</h3>

<pre><code class='language-R'>CP(x, ...)

## S3 method for class 'CP'
print(x, digits = getOption("digits") - 2L, ...)
## S3 method for class 'CP'
plot(x, minline = TRUE, lty = 3, col = 1,
     upper = c("size", "splits", "none"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CP_+3A_x">x</code></td>
<td>
<p>fitted model object of class <code>"rpart"</code>. This is assumed to be the result of some function that produces an object with the same named components as that returned by the <code>rpart</code> function.
</p>
</td></tr>
<tr><td><code id="CP_+3A_digits">digits</code></td>
<td>
<p>the number of digits of numbers to print.</p>
</td></tr>
<tr><td><code id="CP_+3A_minline">minline</code></td>
<td>
<p>whether a horizontal line is drawn 1SE above the minimum of the curve.</p>
</td></tr>
<tr><td><code id="CP_+3A_lty">lty</code></td>
<td>
<p>line type for this line</p>
</td></tr>
<tr><td><code id="CP_+3A_col">col</code></td>
<td>
<p>colour for this line</p>
</td></tr>
<tr><td><code id="CP_+3A_upper">upper</code></td>
<td>
<p>what is plotted on the top axis: the size of the tree (the number of leaves) (&quot;<code>size</code>&quot;), the number of splits (&quot;<code>splits</code>&quot;) or nothing (&quot;<code>none</code>&quot;).</p>
</td></tr>
<tr><td><code id="CP_+3A_...">...</code></td>
<td>
<p>further arguments passed to <code>print</code> and <code>plot</code>

</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The complexity parameter table is hidden deep in the entrails of the <code>rpart</code> result object, it is convenient to have a function to extract it.

</p>


<h3>Value</h3>

<p>A list containing the following components:

</p>
<table>
<tr><td><code>cp</code></td>
<td>
<p>the complexity table</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>the <code>rpart</code> object</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Andri Signorell &lt;andri@signorell.net&gt;

</p>


<h3>See Also</h3>

<p><code><a href="rpart.html#topic+printcp">printcp</a></code>, <code><a href="rpart.html#topic+plotcp">plotcp</a></code>

</p>


<h3>Examples</h3>

<pre><code class='language-R'>r.rp &lt;- FitMod(diabetes ~ ., d.pima, fitfn="rpart")

CP(r.rp)
plot(CP(r.rp))
</code></pre>

<hr>
<h2 id='d.glass'>
Measurements of Forensic Glass Fragments
</h2><span id='topic+d.glass'></span>

<h3>Description</h3>

<p>The <code>d.glass</code> data frame has 214 rows and 10 columns.
It was collected by B. German on fragments of glass
collected in forensic work.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>d.glass
</code></pre>


<h3>Format</h3>

<p>This data frame contains the following columns:
</p>

<dl>
<dt><code>RI</code></dt><dd>
<p>refractive index; more precisely the refractive index is 1.518xxxx.
</p>
<p>The next 8 measurements are percentages by weight of oxides.
</p>
</dd>
<dt><code>Na</code></dt><dd><p>sodium.</p>
</dd>
<dt><code>Mg</code></dt><dd><p>manganese.</p>
</dd>
<dt><code>Al</code></dt><dd><p>aluminium.</p>
</dd>
<dt><code>Si</code></dt><dd><p>silicon.</p>
</dd>
<dt><code>K</code></dt><dd><p>potassium.</p>
</dd>
<dt><code>Ca</code></dt><dd><p>calcium.</p>
</dd>
<dt><code>Ba</code></dt><dd><p>barium.</p>
</dd>
<dt><code>Fe</code></dt><dd><p>iron.</p>
</dd>
<dt><code>Type</code></dt><dd>
<p>The fragments were originally classed into seven types, one of which
was absent in this dataset.  The categories which occur are
window float glass (<code>WinF</code>: 70),
window non-float glass (<code>WinNF</code>: 76),
vehicle window glass (<code>Veh</code>: 17),
containers (<code>Con</code>: 13),
tableware (<code>Tabl</code>: 9) and
vehicle headlamps (<code>Head</code>: 29).
</p>
</dd>
</dl>



<h3>References</h3>

<p>Venables, W. N. and Ripley, B. D. (2002)
<em>Modern Applied Statistics with S.</em> Fourth edition.  Springer.
</p>

<hr>
<h2 id='d.pima'>Diabetes survey on Pima Indians</h2><span id='topic+d.pima'></span><span id='topic+d.pima2'></span>

<h3>Description</h3>

<p>The National Institute of Diabetes and
Digestive and Kidney Diseases conducted a study on 768 adult female
Pima Indians living near Phoenix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(d.pima)
data(d.pima2)
</code></pre>


<h3>Format</h3>

<p>The dataset contains the following variables
</p>

<dl>
<dt><code>pregnant</code></dt><dd><p> Number of times pregnant</p>
</dd>
<dt><code>glucose</code></dt><dd><p> Plasma glucose concentration at 2 hours in an
oral glucose tolerance test</p>
</dd>
<dt><code>diastolic</code></dt><dd><p> Diastolic blood pressure (mm Hg)</p>
</dd>
<dt><code>triceps</code></dt><dd><p> Triceps skin fold thickness (mm)</p>
</dd>
<dt><code>insulin</code></dt><dd><p> 2-Hour serum insulin (mu U/ml)</p>
</dd>
<dt><code>bmi</code></dt><dd><p> Body mass index (weight in kg/(height in metres
squared))</p>
</dd>
<dt><code>diabetes</code></dt><dd><p> Diabetes pedigree function</p>
</dd>
<dt><code>age</code></dt><dd><p> Age (years)</p>
</dd>
<dt><code>test</code></dt><dd><p> test whether the patient shows signs of
diabetes (coded 0 if negative, 1 if positive)</p>
</dd>
</dl>



<h3>Details</h3>

<p><code>d.pima2</code> is the same dataset as <code>d.pima</code> with the only change, that invalid 0-values are replaced by <code>NA</code>s.</p>


<h3>Note</h3>

<p>This dataset has been borrowed from Julian Faraway's package:<br /> <em>faraway</em>: Functions and datasets for books by Julian Faraway, 2015</p>


<h3>Source</h3>

<p> The data may
be obtained from the package <code>MASS</code>.
</p>

<hr>
<h2 id='FitMod'>Wrapper for Several Model Functions
</h2><span id='topic+FitMod'></span><span id='topic+print.FitMod'></span><span id='topic+plot.FitMod'></span><span id='topic+predict.FitMod'></span><span id='topic+summary.FitMod'></span><span id='topic+drop1.FitMod'></span>

<h3>Description</h3>

<p>Popular implementations of algorithms are characterized by partly unconventional implementations of the operating standards in R. For example, the function <code>e1071::SVM()</code> returns the predicted values as attributes! <br />
<code>FitMod()</code> is designed as a wrapping function to offer a consistent interface for a selection of most often used classification and regression models.

</p>


<h3>Usage</h3>

<pre><code class='language-R'>FitMod(formula, data, ..., subset, na.action = na.pass, fitfn = NULL)

## S3 method for class 'FitMod'
predict(object, ...)
## S3 method for class 'FitMod'
plot(x, ...)
## S3 method for class 'FitMod'
summary(object, ...)
## S3 method for class 'FitMod'
drop1(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FitMod_+3A_x">x</code></td>
<td>
<p>a fitted object of class <code>"FitMod"</code>.</p>
</td></tr>
<tr><td><code id="FitMod_+3A_formula">formula</code></td>
<td>
<p>a formula expression as for classification and regression models, of the form <code>response ~ predictors</code>. The response should be a factor or a matrix with K columns, which will be interpreted as counts for each of K classes. See the documentation of <code><a href="stats.html#topic+formula">formula</a>()</code> for other details.

</p>
</td></tr>
<tr><td><code id="FitMod_+3A_data">data</code></td>
<td>
<p>an optional data frame in which to interpret the variables occurring in formula.

</p>
</td></tr>
<tr><td><code id="FitMod_+3A_subset">subset</code></td>
<td>
<p>expression saying which subset of the rows of the data should be used in the fit. All observations are included by default.

</p>
</td></tr>
<tr><td><code id="FitMod_+3A_na.action">na.action</code></td>
<td>
<p>a function to filter missing data.

</p>
</td></tr>
<tr><td><code id="FitMod_+3A_fitfn">fitfn</code></td>
<td>
<p>code for the fitting function to be used for regression or classifying. So far implemented are: <code>lm</code>, <code>lmrob</code>, <code>poisson</code>, <code>quasipoisson</code>, <code>gamma</code>, <code>negbin</code>, <code>poisson</code>, <code>polr</code>, <code>tobit</code>, <code>zeroinfl</code>, <code>multinom</code>, <code>poisson</code>, <code>rpart</code>, <code>randomForest</code>, <code>logit</code>, <code>nnet</code>, <code>C5.0</code>, <code>lda</code>, <code>qda</code>, <code>svm</code>, <code>naive_bayes</code>, <code>lb</code>.

</p>
</td></tr>
<tr><td><code id="FitMod_+3A_object">object</code></td>
<td>
<p>the model object.</p>
</td></tr>
<tr><td><code id="FitMod_+3A_...">...</code></td>
<td>
<p>further arguments passed to the underlying functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function will in general return the original object, extended by a further class <code>FitMod</code>, which allows to capture the output and plot routines.
</p>
<p>The classifying algorithms will at the minimum offer the predicting options <code>type = c("class", "prob")</code> additionally to those implemented by the underlying function.
</p>


<h3>Value</h3>

<p>model object as returned by the calculating function extended with the <code>FitMod</code> class.





</p>


<h3>Author(s)</h3>

<p>Andri Signorell &lt;andri@signorell.net&gt;
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+lm">lm</a></code>, <code><a href="rpart.html#topic+rpart">rpart</a></code>

</p>


<h3>Examples</h3>

<pre><code class='language-R'>r.lm &lt;- FitMod(Fertility ~ ., data=swiss, fitfn="lm")

r.logit &lt;- FitMod(diabetes ~ glucose + pressure + mass + age,
                  data=d.pima, fitfn="logit")
r.svm &lt;- FitMod(diabetes ~ glucose + pressure + mass + age,
                  data=d.pima, fitfn="svm")
</code></pre>

<hr>
<h2 id='LeafRates'>Leafrates for the Nodes of an 'rpart' Tree

</h2><span id='topic+Purity'></span><span id='topic+LeafRates'></span><span id='topic+plot.LeafRates'></span>

<h3>Description</h3>

<p>Return the frequencies of correct and wrong classifications in given node(s) in tabular form. The 'purity', denoting the relative frequency of correctly classified elements, is a useful information for the interpretation of regression and classification trees and a measure for its quality.

</p>


<h3>Usage</h3>

<pre><code class='language-R'>LeafRates(x)

## S3 method for class 'LeafRates'
plot(x, col = NULL, which = c("rel", "abs"),
                         layout = NULL, ylim = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LeafRates_+3A_x">x</code></td>
<td>
<p>fitted model object of class <code>rpart</code>.</p>
</td></tr>
<tr><td><code id="LeafRates_+3A_col">col</code></td>
<td>
<p>color for the bars in the plot</p>
</td></tr>
<tr><td><code id="LeafRates_+3A_which">which</code></td>
<td>
<p>one out of <code>"rel"</code> or <code>"abs"</code>, denoting whether relative or absolute frequencies should be used for the plot.</p>
</td></tr>
<tr><td><code id="LeafRates_+3A_layout">layout</code></td>
<td>
<p>vector defining the layout</p>
</td></tr>
<tr><td><code id="LeafRates_+3A_ylim">ylim</code></td>
<td>
<p>the y limits of the plot.</p>
</td></tr>
<tr><td><code id="LeafRates_+3A_...">...</code></td>
<td>
<p>further arguments (not used).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The result comprises absolute and relative frequencies per leaf.
</p>


<h3>Value</h3>

<p>A list with 5 elements consisting of:
</p>
<table>
<tr><td><code>node</code></td>
<td>
<p>the node id (of the leaf)</p>
</td></tr>
<tr><td><code>freq</code></td>
<td>
<p>the absolute frequency of correct and wrong classifications</p>
</td></tr>
<tr><td><code>p.row</code></td>
<td>
<p>the relative frequency of correct and wrong classifications</p>
</td></tr>
<tr><td><code>mfreq</code></td>
<td>
<p>the total number of cases</p>
</td></tr>
<tr><td><code>mperc</code></td>
<td>
<p>the percentage of the sample in the leaf</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Andri Signorell &lt;andri@signorell.net&gt;

</p>


<h3>See Also</h3>

<p><code><a href="#topic+Node">Node</a></code>, <code><a href="#topic+Rules">Rules</a></code>

</p>


<h3>Examples</h3>

<pre><code class='language-R'>r.rp &lt;- FitMod(Species ~ ., data=iris, fitfn="rpart")
LeafRates(r.rp)

plot(LeafRates(r.rp))
</code></pre>

<hr>
<h2 id='LogitBoost'>LogitBoost Classification Algorithm</h2><span id='topic+LogitBoost'></span><span id='topic+LogitBoost.default'></span><span id='topic+LogitBoost.formula'></span>

<h3>Description</h3>

<p>Train logitboost classification algorithm using decision
stumps (one node decision trees) as weak learners.  </p>


<h3>Usage</h3>

<pre><code class='language-R'>LogitBoost(x, ...)

## S3 method for class 'formula'
LogitBoost(formula, data, ..., subset, na.action)

## Default S3 method:
LogitBoost(x, y, nIter=ncol(x), ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LogitBoost_+3A_formula">formula</code></td>
<td>
<p>a formula expression as for regression models, of the form <code>response ~ predictors</code>. The response should be a factor or a matrix with K columns, which will be interpreted as counts for each of K classes. See the documentation of <code><a href="stats.html#topic+formula">formula</a>()</code> for other details.

</p>
</td></tr>
<tr><td><code id="LogitBoost_+3A_data">data</code></td>
<td>
<p>an optional data frame in which to interpret the variables occurring in formula.

</p>
</td></tr>
<tr><td><code id="LogitBoost_+3A_...">...</code></td>
<td>
<p>additional arguments for nnet

</p>
</td></tr>
<tr><td><code id="LogitBoost_+3A_subset">subset</code></td>
<td>
<p>expression saying which subset of the rows of the data should be used in the fit. All observations are included by default.

</p>
</td></tr>
<tr><td><code id="LogitBoost_+3A_na.action">na.action</code></td>
<td>
<p>a function to filter missing data.

</p>
</td></tr>
<tr><td><code id="LogitBoost_+3A_x">x</code></td>
<td>
<p>A matrix or data frame with training data. Rows contain samples
and columns contain features</p>
</td></tr>
<tr><td><code id="LogitBoost_+3A_y">y</code></td>
<td>
<p>Class labels for the training data samples.
A response vector with one label for each row/component of <code>xlearn</code>.
Can be either a factor, string or a numeric vector.</p>
</td></tr>
<tr><td><code id="LogitBoost_+3A_niter">nIter</code></td>
<td>
<p>An integer, describing the number of iterations for
which boosting should be run, or number of decision stumps that will be
used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function was adapted from logitboost.R function written by Marcel
Dettling. See references and &quot;See Also&quot; section. The code was modified in
order to make it much faster for very large data sets. The speed-up was
achieved by implementing a internal version of decision stump classifier
instead of using calls to <code><a href="rpart.html#topic+rpart">rpart</a></code>. That way, some of the most time
consuming operations were precomputed once, instead of performing them at
each iteration. Another difference is that training and testing phases of the
classification process were split into separate functions.
</p>


<h3>Value</h3>

<p>An object of class &quot;LogitBoost&quot; including components:
</p>
<table>
<tr><td><code>Stump</code></td>
<td>
<p>List of decision stumps (one node decision trees) used:
</p>

<ul>
<li><p> column 1: feature numbers or each stump, or which column each stump
operates on
</p>
</li>
<li><p> column 2: threshold to be used for that column
</p>
</li>
<li><p> column 3: bigger/smaller info: 1 means that if values in the column
are above threshold than corresponding samples will be labeled as
<code>lablist[1]</code>. Value &quot;-1&quot; means the opposite.
</p>
</li></ul>

<p>If there are more than two classes, than several &quot;Stumps&quot; will be
<code>cbind</code>'ed
</p>
</td></tr>
<tr><td><code>lablist</code></td>
<td>
<p>names of each class</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jarek Tuszynski (SAIC) <a href="mailto:jaroslaw.w.tuszynski@saic.com">jaroslaw.w.tuszynski@saic.com</a></p>


<h3>References</h3>

<p>Dettling and Buhlmann (2002), <em>Boosting for Tumor Classification of Gene
Expression Data</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># basic interface
r.lb &lt;- LogitBoost(Species ~ ., data=iris, nIter=20)
pred &lt;- predict(r.lb)
prob &lt;- predict(r.lb, type="prob")
d.res &lt;- data.frame(pred, prob)
d.res[1:10, ]

# accuracy increases with nIter (at least for train set)
table(predict(r.lb, iris, type="class", nIter= 2), iris$Species)
table(predict(r.lb, iris, type="class", nIter=10), iris$Species)
table(predict(r.lb, iris, type="class"),           iris$Species)

# example of spliting the data into train and test set
d.set &lt;- SplitTrainTest(iris)
r.lb &lt;- LogitBoost(Species ~ ., data=d.set$train, nIter=10)
table(predict(r.lb, d.set$test, type="class", nIter=2), d.set$test$Species)
table(predict(r.lb, d.set$test, type="class"),          d.set$test$Species)
</code></pre>

<hr>
<h2 id='ModTools-package'>Regression and Classification Tools
</h2><span id='topic+ModTools-package'></span><span id='topic+ModTools'></span>

<h3>Description</h3>

<p>There is a rich selection of R packages implementing algorithms for classification and regression tasks out there. The authors legitimately take the liberty to tailor the function interfaces according to their own taste  and needs. For us other users, however, this often results in struggling with user interfaces, some of which are rather weird - to put it mildly - and almost always different in terms of arguments and result structures.
<b>ModTools</b> pursues the goal of offering uniform handling for the most important regression and classification models in applied data analyses.<br />
The function <code>FitMod()</code> is designed as a simple and consistent interface to these original functions while maintaining the flexibility to pass on all possible arguments. <code>print</code>, <code>plot</code>, <code>summary</code> and <code>predict</code> operations can so be carried out following the same logic. The results will again be reshaped to a reasonable standard.
</p>
<p>For all the functions of this package Google styleguides are used as naming rules (in absence of convincing alternatives). The 'BigCamelCase' style has been consequently applied to functions borrowed from contributed R packages as well.
</p>
<p>As always: Feedback, feature requests, bugreports and other suggestions are welcome!
</p>


<h3>Details</h3>

<p>The <code>ModTools::<a href="#topic+FitMod">FitMod</a>())</code> function comprises interfaces to the following models:
</p>

<table>
<tr>
 <td style="text-align: left;">
<b>Regression</b>:</td><td style="text-align: left;"></td>
</tr>
<tr>
 <td style="text-align: left;">
 <code><a href="stats.html#topic+lm">lm</a>()</code>  </td><td style="text-align: left;"> 	 Linear model OLS (<b>base</b>)</td>
</tr>
<tr>
 <td style="text-align: left;">
 <code><a href="robustbase.html#topic+lmrob">lmrob</a>()</code>  </td><td style="text-align: left;"> 	 Robust linear model (<b>robustbase</b>)</td>
</tr>
<tr>
 <td style="text-align: left;">
 <code>poisson()</code>  </td><td style="text-align: left;"> 	GLM model with family <code>poisson</code> (<b>base</b>)</td>
</tr>
<tr>
 <td style="text-align: left;">
 <code>negbin()</code>  </td><td style="text-align: left;"> 	GLM model with family <code>negative.binomial</code> (<b>MASS</b>)</td>
</tr>
<tr>
 <td style="text-align: left;">
 <code>gamma()</code>  </td><td style="text-align: left;"> 		GLM model with family <code>gamma</code> (<b>base</b>)</td>
</tr>
<tr>
 <td style="text-align: left;">
 <code>tobit()</code>  </td><td style="text-align: left;"> 	Tobit model for censored responses (package  <b>AER</b>)</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>
<tr>
 <td style="text-align: left;"></td>
</tr>
<tr>
 <td style="text-align: left;"></td>
</tr>
<tr>
 <td style="text-align: left;">
<b>Classification</b>:	</td><td style="text-align: left;"></td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="MASS.html#topic+lda">lda</a>()</code>  </td><td style="text-align: left;"> 	 Linear discriminant analysis (<b>MASS</b>)</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="MASS.html#topic+qda">qda</a>()</code>  </td><td style="text-align: left;"> 	 Quadratic discriminant analysis (<b>MASS</b>)</td>
</tr>
<tr>
 <td style="text-align: left;">
<code>logit()</code>  </td><td style="text-align: left;"> 	 Logistic Regression model <code><a href="stats.html#topic+glm">glm</a></code>, family <code>binomial(logit)</code>(<b>base</b>)</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="mgcv.html#topic+multinom">multinom</a>()</code>  </td><td style="text-align: left;"> 	 Multinomial Regression model (<b>nnet</b>)</td>
</tr>
<tr>
 <td style="text-align: left;">
 <code><a href="MASS.html#topic+polr">polr</a>()</code>  </td><td style="text-align: left;"> 	 Proportional odds model (<b>MASS</b>)</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="rpart.html#topic+rpart">rpart</a>()</code>  </td><td style="text-align: left;"> 	 Regression and classification trees (<b>rpart</b>)</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="nnet.html#topic+nnet">nnet</a>()</code>  </td><td style="text-align: left;"> 	 Neuronal networks (<b>nnet</b>)</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="randomForest.html#topic+randomForest">randomForest</a>()</code>   </td><td style="text-align: left;"> 	 Random forests (<b>randomForest</b>)</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="C50.html#topic+C5.0">C5.0</a>()</code>   </td><td style="text-align: left;"> 	 C5.0 tree (<b>C50</b>)</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="e1071.html#topic+svm">svm</a>()</code>   </td><td style="text-align: left;"> 	 Support vector machines (<b>e1071</b>)</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="naivebayes.html#topic+naive_bayes">naive_bayes</a>()</code>   </td><td style="text-align: left;"> 	 Naive Bayes classificator (<b>naivebayes</b>)</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+LogitBoost">LogitBoost</a>()</code>  </td><td style="text-align: left;"> 	 Logit boost (using decision
  stumps as weak learners) (<b>ModTools</b>)</td>
</tr>
<tr>
 <td style="text-align: left;">


















</td>
</tr>
<tr>
 <td style="text-align: left;"></td>
</tr>
<tr>
 <td style="text-align: left;"></td>
</tr>
<tr>
 <td style="text-align: left;"> <b>Preprocess</b>:</td><td style="text-align: left;"></td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+SplitTrainTest">SplitTrainTest</a>()</code>  </td><td style="text-align: left;"> 	Splits a data frame or index vector into a training and a test sample</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+OverSample">OverSample</a>()</code>  </td><td style="text-align: left;"> Get balanced datasets by sampling with replacement.</td>
</tr>
<tr>
 <td style="text-align: left;">

</td>
</tr>
<tr>
 <td style="text-align: left;"></td>
</tr>
<tr>
 <td style="text-align: left;"></td>
</tr>
<tr>
 <td style="text-align: left;"> <b>Manipulating <code>rpart</code> objects</b>:</td><td style="text-align: left;"></td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+CP">CP</a>()</code>  </td><td style="text-align: left;"> Extract and plot complexity table of an rpart tree.</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+Node">Node</a>()</code>  </td><td style="text-align: left;"> Accessor to the most important properties of a node, being a split or a leaf.</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+Rules">Rules</a>()</code>  </td><td style="text-align: left;"> Extract the decision rules from top to the end node of an rpart tree.</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+LeafRates">LeafRates</a>()</code>  </td><td style="text-align: left;"> Returns the misclassification rates in all end nodes.</td>
</tr>
<tr>
 <td style="text-align: left;">

</td>
</tr>
<tr>
 <td style="text-align: left;"></td>
</tr>
<tr>
 <td style="text-align: left;"></td>
</tr>
<tr>
 <td style="text-align: left;"> <b>Prediction and Validation</b>:</td><td style="text-align: left;"></td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+Response">Response</a>()</code>  </td><td style="text-align: left;"> Extract the response variable of any model.</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="stats.html#topic+predict">predict</a>()</code>  </td><td style="text-align: left;"> 	 Consistent predict for <code>FitMod</code> models</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+VarImp">VarImp</a>()</code>  </td><td style="text-align: left;"> 	 Variable importance for most <code>FitMod</code> models</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+ROC">ROC</a>()</code>  </td><td style="text-align: left;"> ROC curves for all dichotomous classification <code>FitMod</code> models	 </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+BestCut">BestCut</a>()</code>  </td><td style="text-align: left;"> 	Find the optimal cut for a classification based on the ROC curve.</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+PlotLift">PlotLift</a>()</code>  </td><td style="text-align: left;"> Produces a lift chart for a binary classification model	 </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+TModC">TModC</a>()</code>  </td><td style="text-align: left;"> Aggregated results for multiple <code>FitMod</code> classification models </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+Tune">Tune</a>()</code>  </td><td style="text-align: left;"> Tuning approaches to find optimal parameters for <code>FitMod</code> classification models.	 </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+RobSummary">RobSummary</a>()</code>  </td><td style="text-align: left;"> Robust summary for GLM models (poisson).</td>
</tr>
<tr>
 <td style="text-align: left;">

</td>
</tr>
<tr>
 <td style="text-align: left;"></td>
</tr>
<tr>
 <td style="text-align: left;"></td>
</tr>
<tr>
 <td style="text-align: left;">
<b>Tests</b>:	</td><td style="text-align: left;"></td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+BreuschPaganTest">BreuschPaganTest</a>()</code>  </td><td style="text-align: left;"> 	 Breusch-Pagan test against heteroskedasticity.</td>
</tr>
<tr>
 <td style="text-align: left;">

</td>
</tr>

</table>













<h3>Warning</h3>

<p>This package is still under development. You should be aware that everything in the package might be subject to change. Backward compatibility is not yet guaranteed. Functions may be deleted or renamed and new syntax may be inconsistent with earlier versions. By release of version 1.0 the &quot;deprecated-defunct process&quot; will be installed.
</p>


<h3>Author(s)</h3>

<p>Andri Signorell<br />
Helsana Versicherungen AG, Health Sciences, Zurich<br />
HWZ University of Applied Sciences in Business Administration Zurich.<br />
</p>
<p>Includes R source code and/or documentation previously published by (in alphabetical order): <br />
Bernhard Compton, Marcel Dettling, Max Kuhn, Michal Majka, Dan Putler, Jarek Tuszynski, Robin Xavier, Achim Zeileis
</p>
<p>The good things come from all these guys, any problems are likely due to my tweaking.
Thank you all! <br />
</p>
<p>Maintainer: Andri Signorell &lt;andri@signorell.net&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
r.swiss &lt;- FitMod(Fertility ~ ., swiss, fitfn="lm")
r.swiss
# PlotTA(r.swiss)
# PlotQQNorm(r.swiss)


## Count models

data(housing, package="MASS")

# poisson count
r.pois &lt;- FitMod(Freq ~ Infl*Type*Cont + Sat, family=poisson, data=housing, fitfn="poisson")

# negative binomial count
r.nb &lt;- FitMod(Freq ~ Infl*Type*Cont + Sat, data=housing, fitfn="negbin")
summary(r.nb)

r.log &lt;- FitMod(log(Freq) ~ Infl*Type*Cont + Sat, data=housing, fitfn="lm")
summary(r.log)

r.ols &lt;- FitMod(Freq ~ Infl*Type*Cont + Sat, data=housing, fitfn="lm")
summary(r.ols)

r.gam &lt;- FitMod(Freq ~ Infl*Type*Cont + Sat, data=housing, fitfn="gamma")
summary(r.gam)

r.gami &lt;- FitMod(Freq ~ Infl*Type*Cont + Sat, data=housing, fitfn="gamma", link="identity")
summary(r.gami)

old &lt;-options(digits=3)
TMod(r.pois, r.nb, r.log, r.ols, r.gam, r.gami)
options(old)


## Ordered Regression

r.polr &lt;- FitMod(Sat ~ Infl + Type + Cont, data=housing, fitfn="polr", weights = Freq)

# multinomial Regression
# r.mult &lt;- FitMod(factor(Sat, ordered=FALSE) ~ Infl + Type + Cont, data=housing,
#                  weights = housing$Freq, fitfn="multinom")


# Regression tree
r.rp &lt;- FitMod(factor(Sat, ordered=FALSE) ~ Infl + Type + Cont, data=housing,
                 weights = housing$Freq, fitfn="rpart")

# compare predictions
d.p &lt;- expand.grid(Infl=levels(housing$Infl), Type=levels(housing$Type), Cont=levels(housing$Cont))
d.p$polr &lt;- predict(r.polr, newdata=d.p)
# ??
# d.p$ols &lt;- factor(round(predict(r.ols, newdata=d.p)^2), labels=levels(housing$Sat))
# d.p$mult &lt;- predict(r.mult, newdata=d.p)
d.p$rp &lt;- predict(r.rp, newdata=d.p, type="class")

d.p


# Classification with 2 classes  ***************

r.pima &lt;- FitMod(diabetes ~ ., d.pima, fitfn="logit")
r.pima
Conf(r.pima)
plot(ROC(r.pima))
OddsRatio(r.pima)


# rpart tree
rp.pima &lt;- FitMod(diabetes ~ ., d.pima, fitfn="rpart")
rp.pima
Conf(rp.pima)
lines(ROC(rp.pima), col=hblue)
# to be improved
plot(rp.pima, col=SetAlpha(c("blue","red"), 0.4), cex=0.7)


# Random Forest
rf.pima &lt;- FitMod(diabetes ~ ., d.pima, method="class", fitfn="randomForest")
rf.pima
Conf(rf.pima)
lines(ROC(r.pima), col=hred)



# more models to compare

d.pim &lt;- SplitTrainTest(d.pima, p = 0.2)
mdiab &lt;- formula(diabetes ~ pregnant + glucose + pressure + triceps
                            + insulin + mass + pedigree + age)

r.glm &lt;- FitMod(mdiab, data=d.pim$train, fitfn="logit")
r.rp &lt;- FitMod(mdiab, data=d.pim$train, fitfn="rpart")
r.rf &lt;- FitMod(mdiab, data=d.pim$train, fitfn="randomForest")
r.svm &lt;- FitMod(mdiab, data=d.pim$train, fitfn="svm")
r.c5 &lt;- FitMod(mdiab, data=d.pim$train, fitfn="C5.0")
r.nn &lt;- FitMod(mdiab, data=d.pim$train, fitfn="nnet")
r.nb &lt;- FitMod(mdiab, data=d.pim$train, fitfn="naive_bayes")
r.lda &lt;- FitMod(mdiab, data=d.pim$train, fitfn="lda")
r.qda &lt;- FitMod(mdiab, data=d.pim$train, fitfn="qda")
r.lb &lt;- FitMod(mdiab, data=d.pim$train, fitfn="lb")

mods &lt;- list(glm=r.glm, rp=r.rp, rf=r.rf, svm=r.svm, c5=r.c5
             , nn=r.nn, nb=r.nb, lda=r.lda, qda=r.qda, lb=r.lb)

# insight in the Regression tree
plot(r.rp, box.palette = as.list(Pal("Helsana", alpha = 0.5)))

# Insample accuracy ...
TModC(mods, ord="auc")
# ... is substantially different from the out-of-bag:
TModC(mods, newdata=d.pim$test, reference=d.pim$test$diabetes, ord="bs")
# C5 and SVM turn out to be show-offs! They overfit quite ordinary
# whereas randomforest and logit keep their promises. ...

sapply(mods, function(z) VarImp(z))


# Multinomial classification problem with n classes  ***************

d.gl &lt;- SplitTrainTest(d.glass, p = 0.2)
mglass &lt;- formula(Type ~ RI + Na + Mg + Al + Si + K + Ca + Ba + Fe)

# *** raises an unclear error in CRAN-Debian tests *** ??
# r.mult &lt;- FitMod(mglass, data=d.gl$train, maxit=600, fitfn="multinom")
r.rp &lt;- FitMod(mglass, data=d.gl$train, fitfn="rpart")
r.rf &lt;- FitMod(mglass, data=d.gl$train, fitfn="randomForest")
r.svm &lt;- FitMod(mglass, data=d.gl$train, fitfn="svm")
r.c5 &lt;- FitMod(mglass, data=d.gl$train, fitfn="C5.0")
r.nn &lt;- FitMod(mglass, data=d.gl$train, fitfn="nnet")
r.nbay &lt;- FitMod(mglass, data=d.gl$train, fitfn="naive_bayes")
r.lda &lt;- FitMod(mglass, data=d.gl$train, fitfn="lda")
# r.qda &lt;- FitMod(mglass, data=d.glass, fitfn="qda")
r.lb &lt;- FitMod(mglass, data=d.gl$train, fitfn="lb")

mods &lt;- list(rp=r.rp, rf=r.rf, svm=r.svm, c5=r.c5,
             nn=r.nn, nbay=r.nbay, lda=r.lda, lb=r.lb)

# confusion matrix and other quality measures can be calculated with Conf()
Conf(r.rf)

# we only extract the general accuracy
sapply(lapply(mods, function(z) Conf(z)), "[[", "acc")

# let's compare r.mult with a model without RI as predictor
# Conf(r.mult)
# Conf(update(r.mult, . ~ . -RI))
</code></pre>

<hr>
<h2 id='Node'>Nodes and Splits in an rpart Tree

</h2><span id='topic+Node'></span><span id='topic+Splits'></span>

<h3>Description</h3>

<p>The <code>rpart</code> result object has a complex and compact design. This can make practical use tedious for occasional users as it is difficult to figure out how to access some specific information. The function <code>Node()</code> is designed as accessor to the most important properties of a node, being a 'split' or a 'leaf' (aka. 'endnode'). It also serves as base for further convenience functions as e.g. <code><a href="#topic+LeafRates">LeafRates</a>()</code>.

</p>


<h3>Usage</h3>

<pre><code class='language-R'>Node(x, node = NULL, type = c("all", "split", "leaf"), digits = 3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Node_+3A_x">x</code></td>
<td>

<p>fitted model object of class <code>rpart</code>.

</p>
</td></tr>
<tr><td><code id="Node_+3A_node">node</code></td>
<td>
<p>integer vector, defining the nodes whose details are required.

</p>
</td></tr>
<tr><td><code id="Node_+3A_type">type</code></td>
<td>
<p>one out of <code>"all"</code> (default), <code>"split"</code>, <code>"leaf"</code>, where the latter two restrict the result set to splits or end nodes only. Can be abbreviated.

</p>
</td></tr>
<tr><td><code id="Node_+3A_digits">digits</code></td>
<td>
<p>the number of digits for numeric values</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>Node()</code> returns detailed information for a single node in the tree. It reports all the data in the summary of a node, but with the option to provide a nodelist. The structure of the result is organised as a list.
</p>


<h3>Value</h3>

<p>A list containing:


</p>
<table>
<tr><td><code>id</code></td>
<td>
<p>int, id of the node</p>
</td></tr>
<tr><td><code>vname</code></td>
<td>
<p>character, one out of <code>'leaf'</code> or <code>'split'</code></p>
</td></tr>
<tr><td><code>isleaf</code></td>
<td>
<p>logical, <code>TRUE</code> for leaves <code>FALSE</code> else</p>
</td></tr>
<tr><td><code>nobs</code></td>
<td>
<p>integer, number of observation in the node</p>
</td></tr>
<tr><td><code>group</code></td>
<td>
<p>character, the predicted class for the node</p>
</td></tr>
<tr><td><code>ycount</code></td>
<td>
<p>numeric, the number of observation per class in the node</p>
</td></tr>
<tr><td><code>yprob</code></td>
<td>
<p>numeric, the relative frequencies for the each class</p>
</td></tr>
<tr><td><code>nodeprob</code></td>
<td>
<p>the global probability for an observation to fall in the node</p>
</td></tr>
<tr><td><code>complexity</code></td>
<td>
<p>numeric, the complexity parameter for the node</p>
</td></tr>
<tr><td><code>tprint</code></td>
<td>
<p>character, the text to be printed</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Andri Signorell &lt;andri@signorell.net&gt;

</p>


<h3>See Also</h3>

<p><code><a href="#topic+LeafRates">LeafRates</a></code>, <code><a href="#topic+Rules">Rules</a></code>

</p>


<h3>Examples</h3>

<pre><code class='language-R'>r.rpart &lt;- FitMod(Species ~ ., data=iris, fitfn="rpart")
# return Node nr. 3
Node(r.rpart, node=3)

r.rp &lt;- FitMod(Type ~ ., data = d.glass, fitfn="rpart")
# return all the splits
Node(r.rpart, type="split")
</code></pre>

<hr>
<h2 id='Over-+2FUndersample'>Oversample and Undersample

</h2><span id='topic+OverSample'></span><span id='topic+UnderSample'></span>

<h3>Description</h3>

<p>For classification purposes we might want to have balanced datasets. If the response variable has not a prevalence of 50%, we can sample records for getting as much response A cases as response B. This is called oversample. Undersample means to sample the (lower) number of cases A from the records of case B.

</p>


<h3>Usage</h3>

<pre><code class='language-R'>OverSample(x, vname)
UnderSample(x, vname)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Over-+2B2FUndersample_+3A_x">x</code></td>
<td>
<p>a data frame containing predictors and response

</p>
</td></tr>
<tr><td><code id="Over-+2B2FUndersample_+3A_vname">vname</code></td>
<td>
<p>the name of the response variable to be used to over/undersample

</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data frame with balanced response variable





</p>


<h3>Author(s)</h3>

<p>Andri Signorell &lt;andri@signorell.net&gt;

</p>


<h3>See Also</h3>

 <p><code><a href="#topic+BestCut">BestCut</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>OverSample(d.pima2, "diabetes")

UnderSample(d.pima2, "diabetes")
</code></pre>

<hr>
<h2 id='PlotLift'>Lift Charts to Compare Binary Predictive Models</h2><span id='topic+PlotLift'></span>

<h3>Description</h3>

<p>Provides either a total cumulative response or incremental response rate lift
chart for the purposes of comparing the predictive capability of different
binary predictive models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PlotLift(modelList, data, targLevel, trueResp, type = "cumulative", sub = "")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PlotLift_+3A_modellist">modelList</code></td>
<td>
<p>A character vector containing the names of the different
models to be compared. The selected models must have the same y variable that
must be a binary factor, and have been estimated using the same data set.</p>
</td></tr>
<tr><td><code id="PlotLift_+3A_data">data</code></td>
<td>
<p>The dataframe that constitues the comparison sample. If this
dataframe is not the same as the dataframe used to estimated models, the
dataframe must contain all the variables used in the models to be compared.</p>
</td></tr>
<tr><td><code id="PlotLift_+3A_targlevel">targLevel</code></td>
<td>
<p>The label for the level of the binary factor of interest.
For example, in a database marketing application, this level could be &quot;Yes&quot;
for a variable that takes on the values &quot;Yes&quot; and &quot;No&quot; to indicate if a
customer responded favorably to a promotion offer.</p>
</td></tr>
<tr><td><code id="PlotLift_+3A_trueresp">trueResp</code></td>
<td>
<p>The true rate of the target level for the master database
the estimation and comparison dataframes were originally drawn from.</p>
</td></tr>
<tr><td><code id="PlotLift_+3A_type">type</code></td>
<td>
<p>A character string that must either have the value of
&quot;cummulative&quot; (to produce a total cummaltive response chart) or &quot;incremental&quot;
(to produce an incremental response rate chart).</p>
</td></tr>
<tr><td><code id="PlotLift_+3A_sub">sub</code></td>
<td>
<p>A sub-title for the plot, typically to identify the sample used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Lift charts are a commonly used tool in business data mining
applications. They are used to assess how well a model is able to predict a
desirable (from an organization's point-of-view) response on the part of a
customer compared to alternative estimated models and a benchmark model of
approaching customers randomly. The total cummulative response chart shows the
percentage of the total response the organization would receive from only
contacting a given percentage (grouped by deciles) of its entire customer base.
This chart is best for selecting between alternative models, and in predicting
the revenues the organization will receive by contacting a given percentage of
their customers that the model predicts are most likely to favorably respond.
The incremental response rate chart provides the response rate among each of ten
decile groups of the organization's customers, with the decile groups ordered by
their estimated likelihood of a favorable response.</p>


<h3>Value</h3>

<p>The function returns the sample response invisibly.
</p>


<h3>Author(s)</h3>

<p>original Dan Putler, tweaks Andri Signorell &lt;andri@signorell.net&gt;</p>


<h3>Examples</h3>

<pre><code class='language-R'>d.pim &lt;- SplitTrainTest(d.pima, p = 0.2)

r.rp &lt;- FitMod(diabetes ~ pregnant + glucose + pressure + triceps
               + insulin + mass + pedigree + age
               , data=d.pim$train, fitfn="rpart")

r.glm &lt;- FitMod(diabetes ~ pregnant + glucose + pressure + triceps
               + insulin + mass + pedigree + age
               , data=d.pim$train, fitfn="logit")

r.nn &lt;- FitMod(diabetes ~ pregnant + glucose + pressure + triceps
                + insulin + mass + pedigree + age
                , data=d.pim$train, fitfn="nnet")

oldpar &lt;- par(mfrow=c(1,2))
PlotLift(c("r.rp", "r.glm", "r.nn"), data = d.pim$train,
              targLevel = "pos", trueResp =0.34, type = "cumulative")
PlotLift(c("r.rp", "r.glm", "r.nn"), data = d.pim$train,
              targLevel = "pos", trueResp =0.34, type = "incremental")
par(oldpar)
</code></pre>

<hr>
<h2 id='predict.zeroinfl'>Methods for zeroinfl Objects</h2><span id='topic+predict.zeroinfl'></span><span id='topic+residuals.zeroinfl'></span><span id='topic+terms.zeroinfl'></span><span id='topic+model.matrix.zeroinfl'></span><span id='topic+coef.zeroinfl'></span><span id='topic+vcov.zeroinfl'></span><span id='topic+summary.zeroinfl'></span><span id='topic+print.summary.zeroinfl'></span><span id='topic+logLik.zeroinfl'></span><span id='topic+fitted.zeroinfl'></span><span id='topic+predprob.zeroinfl'></span><span id='topic+extractAIC.zeroinfl'></span>

<h3>Description</h3>

<p>Methods for extracting information from fitted zero-inflated
regression model objects of class <code>"zeroinfl"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'zeroinfl'
predict(object, newdata,
  type = c("response", "prob", "count", "zero"), na.action = na.pass,
  at = NULL, ...)
## S3 method for class 'zeroinfl'
residuals(object, type = c("pearson", "response"), ...)

## S3 method for class 'zeroinfl'
coef(object, model = c("full", "count", "zero"), ...)
## S3 method for class 'zeroinfl'
vcov(object, model = c("full", "count", "zero"), ...)

## S3 method for class 'zeroinfl'
terms(x, model = c("count", "zero"), ...)
## S3 method for class 'zeroinfl'
model.matrix(object, model = c("count", "zero"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.zeroinfl_+3A_object">object</code>, <code id="predict.zeroinfl_+3A_x">x</code></td>
<td>
<p>an object of class <code>"zeroinfl"</code> as returned by
<code><a href="#topic+zeroinfl">zeroinfl</a></code>.</p>
</td></tr>
<tr><td><code id="predict.zeroinfl_+3A_newdata">newdata</code></td>
<td>
<p>optionally, a data frame in which to look for variables with
which to predict. If omitted, the original observations are used.</p>
</td></tr>
<tr><td><code id="predict.zeroinfl_+3A_type">type</code></td>
<td>
<p>character specifying the type of predictions or residuals,
respectively. For details see below.</p>
</td></tr>
<tr><td><code id="predict.zeroinfl_+3A_na.action">na.action</code></td>
<td>
<p>function determining what should be done with missing values
in <code>newdata</code>. The default is to predict <code>NA</code>.</p>
</td></tr>
<tr><td><code id="predict.zeroinfl_+3A_at">at</code></td>
<td>
<p>optionally, if <code>type = "prob"</code>, a numeric vector at which
the probabilities are evaluated. By default <code>0:max(y)</code> is used
where <code>y</code> is the original observed response.</p>
</td></tr>
<tr><td><code id="predict.zeroinfl_+3A_model">model</code></td>
<td>
<p>character specifying for which component of the model the
terms or model matrix should be extracted.</p>
</td></tr>
<tr><td><code id="predict.zeroinfl_+3A_...">...</code></td>
<td>
<p>currently not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A set of standard extractor functions for fitted model objects is available for
objects of class <code>"zeroinfl"</code>, including methods to the generic functions
<code><a href="base.html#topic+print">print</a></code> and <code><a href="base.html#topic+summary">summary</a></code> which print the estimated
coefficients along with some further information. The <code>summary</code> in particular
supplies partial Wald tests based on the coefficients and the covariance matrix
(estimated from the Hessian in the numerical optimization of the log-likelihood).
As usual, the <code>summary</code> method returns an object of class <code>"summary.zeroinfl"</code>
containing the relevant summary statistics which can subsequently be printed
using the associated <code>print</code> method.
</p>
<p>The methods for <code><a href="stats.html#topic+coef">coef</a></code> and <code><a href="stats.html#topic+vcov">vcov</a></code> by default
return a single vector of coefficients and their associated covariance matrix,
respectively, i.e., all coefficients are concatenated. By setting the <code>model</code>
argument, the estimates for the corresponding model components can be extracted.
</p>
<p>Both the <code><a href="stats.html#topic+fitted">fitted</a></code> and <code><a href="stats.html#topic+predict">predict</a></code> methods can
compute fitted responses. The latter additionally provides the predicted density
(i.e., probabilities for the observed counts), the predicted mean from the count
component (without zero inflation) and the predicted probability for the zero
component. The <code><a href="stats.html#topic+residuals">residuals</a></code> method can compute
raw residuals (observed - fitted) and Pearson residuals (raw residuals scaled by
square root of variance function).
</p>
<p>The <code><a href="stats.html#topic+terms">terms</a></code> and <code><a href="stats.html#topic+model.matrix">model.matrix</a></code> extractors can
be used to extract the relevant information for either component of the model.
</p>
<p>A <code><a href="stats.html#topic+logLik">logLik</a></code> method is provided, hence <code><a href="stats.html#topic+AIC">AIC</a></code>
can be called to compute information criteria.
</p>


<h3>Author(s)</h3>

<p>Achim Zeileis &lt;Achim.Zeileis@R-project.org&gt;</p>


<h3>See Also</h3>

<p><code><a href="#topic+zeroinfl">zeroinfl</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data("bioChemists", package = "ModTools")

fm_zip &lt;- zeroinfl(art ~ ., data = bioChemists)
plot(residuals(fm_zip) ~ fitted(fm_zip))

coef(fm_zip)
coef(fm_zip, model = "count")

summary(fm_zip)
logLik(fm_zip)
</code></pre>

<hr>
<h2 id='PredictCI'>Confidence Intervals for Predictions of a GLM

</h2><span id='topic+PredictCI'></span>

<h3>Description</h3>

<p>Provides confidence intervals for predictions of a GLM.

</p>


<h3>Usage</h3>

<pre><code class='language-R'>PredictCI(mod, newdata, conf.level = 0.95)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PredictCI_+3A_mod">mod</code></td>
<td>
<p>the binomial model

</p>
</td></tr>
<tr><td><code id="PredictCI_+3A_newdata">newdata</code></td>
<td>
<p>the data to be predicted

</p>
</td></tr>
<tr><td><code id="PredictCI_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level of the interval. Default is 0.95.

</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The confidence intervals for predictions are calculated with the se of the model and the normal quantile.

</p>


<h3>Value</h3>

<p>a matrix with 3 columns for the fit, the lower confidence interval and the upper confidence interval





</p>


<h3>Author(s)</h3>

<p>Andri Signorell &lt;andri@signorell.net&gt;

</p>


<h3>References</h3>

<p><a href="https://stackoverflow.com/questions/14423325/confidence-intervals-for-predictions-from-logistic-regression">https://stackoverflow.com/questions/14423325/confidence-intervals-for-predictions-from-logistic-regression</a>

</p>


<h3>See Also</h3>

<p><code><a href="#topic+FitMod">FitMod</a></code>

</p>


<h3>Examples</h3>

<pre><code class='language-R'>r.logit &lt;- FitMod(diabetes ~ age, d.pima, fitfn = "logit")
head(PredictCI(r.logit, newdata=d.pima))
</code></pre>

<hr>
<h2 id='RefLevel'>Used Reference Levels in a Linear Model

</h2><span id='topic+RefLevel'></span>

<h3>Description</h3>

<p>Returns all the reference levels in the factors used in a linear model. It is customer friendly to report also the reference level in lm summaries, which normally are suppressed.

</p>


<h3>Usage</h3>

<pre><code class='language-R'>RefLevel(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RefLevel_+3A_x">x</code></td>
<td>
<p>lm object, linear model with factors as predictors.

</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For reporting tables of linear models we might want to include an information about the used reference levels, which remain uncommented in the default <code>lm</code> result output. <code>RefLevel()</code> allows to add a footnote or integrate the reference levels in the coefficient table.

</p>


<h3>Value</h3>

<p>a named vector containing the reference levels of all factors</p>


<h3>Note</h3>

<p>It's not clear how general the used algorithm is for more exotic models. <code><a href="stats.html#topic+dummy.coef">dummy.coef</a></code> could in such cases be an alternative.
</p>


<h3>Author(s)</h3>

<p>Andri Signorell &lt;andri@signorell.net&gt;

</p>


<h3>See Also</h3>

<p><code>dummy.coef</code>, <code><a href="#topic+Response">Response</a></code>, <code><a href="stats.html#topic+relevel">relevel</a></code>, <code><a href="stats.html#topic+lm">lm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>RefLevel(lm(breaks ~ wool + tension, data = warpbreaks))
</code></pre>

<hr>
<h2 id='Response'>Extract the Response from Several Models

</h2><span id='topic+Response'></span>

<h3>Description</h3>

<p>Time after time, in the course of our daily work, we experience that the response variable is hidden very deeply in the object. This again leads to superfluous consultation of the documentation.
<code>Reponse()</code> relieves us of this work.

</p>


<h3>Usage</h3>

<pre><code class='language-R'>Response(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Response_+3A_x">x</code></td>
<td>
<p>the model to use

</p>
</td></tr>
<tr><td><code id="Response_+3A_...">...</code></td>
<td>
<p>more arguments

</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function implements the extraction of the response variables for all the models listed in the package's help text.

</p>


<h3>Value</h3>

<p>the response of model x





</p>


<h3>Author(s)</h3>

<p>Andri Signorell &lt;andri@signorell.net&gt;

</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+model.frame">model.frame</a></code>, <code><a href="stats.html#topic+model.response">model.response</a></code>, <code><a href="#topic+RefLevel">RefLevel</a></code>

</p>


<h3>Examples</h3>

<pre><code class='language-R'>r.rpart &lt;- FitMod(diabetes ~ ., d.pima, fitfn="rpart")
Response(r.rpart)

# up to the attribute "response" this is the same
identical(StripAttr(Response(r.rpart), "response"),
          model.response(model.frame(r.rpart)))
</code></pre>

<hr>
<h2 id='RobSummary'>Robust Summary for Linear Models

</h2><span id='topic+RobSummary'></span>

<h3>Description</h3>

<p>For poisson models with mild violation of the distribution assumption that the variance equals the mean, Cameron and Trivedi (2009) recommended using robust standard errors for the parameter estimates. The function uses the function <code>vcovHC</code> from the package <span class="pkg">sandwich</span> to obtain the robust standard errors and calculate the p-values accordingly.
It returns a matrix containing the usual results in the model summary, comprising the parameter estimates,  their robust standard errors, p-values,  extended with the 95% confidence interval.

</p>


<h3>Usage</h3>

<pre><code class='language-R'>RobSummary(mod, conf.level = 0.95, type = "HC0")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RobSummary_+3A_mod">mod</code></td>
<td>
<p>the model for which robust standard errors should be calculated

</p>
</td></tr>
<tr><td><code id="RobSummary_+3A_conf.level">conf.level</code></td>
<td>
<p>the confidence level, default is 95%.

</p>
</td></tr>
<tr><td><code id="RobSummary_+3A_type">type</code></td>
<td>
<p>a character string specifying the estimation type. Details in <code><a href="sandwich.html#topic+vcovHC">vcovHC</a>()</code>. </p>
</td></tr>

</table>


<h3>Details</h3>

<p>Further details in <a href="https://stats.oarc.ucla.edu/r/dae/poisson-regression/">https://stats.oarc.ucla.edu/r/dae/poisson-regression/</a>

</p>


<h3>Value</h3>

<p>a <em>p x 6</em> matrix with columns for the estimated coefficient, its standard error, t- or z-statistic, the corresponding (two-sided) p-value, the lower and upper confidence interval.





</p>


<h3>Author(s)</h3>

<p>Andri Signorell &lt;andri@signorell.net&gt;

</p>


<h3>References</h3>

<p>Cameron, A. C. and Trivedi, P. K. (2009) Microeconometrics Using Stata. College Station, TX: Stata Press.
</p>



<h3>See Also</h3>

<p><code><a href="stats.html#topic+summary.lm">summary.lm</a></code>, <code><a href="stats.html#topic+summary.glm">summary.glm</a></code>

</p>


<h3>Examples</h3>

<pre><code class='language-R'>r.lm &lt;- lm(Fertility ~ ., swiss)
RobSummary(r.lm)
</code></pre>

<hr>
<h2 id='ROC'>Build a ROC curve

</h2><span id='topic+ROC'></span>

<h3>Description</h3>

<p>This is a wrapper to the main function <code><a href="pROC.html#topic+pROC">pROC</a></code> of the <span class="pkg">pROC</span> package (by Xavier Robin et al.). It builds a ROC curve and returns a <code>"roc"</code> object, a list of class <code>"roc"</code>.

</p>


<h3>Usage</h3>

<pre><code class='language-R'>ROC(x, resp = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ROC_+3A_x">x</code></td>
<td>
<p>a model object, or the predicted probabilities, when resp is not <code>NULL</code>. </p>
</td></tr>
<tr><td><code id="ROC_+3A_resp">resp</code></td>
<td>
<p>the response</p>
</td></tr>
<tr><td><code id="ROC_+3A_...">...</code></td>
<td>
<p>all arguments are passed to <code>roc()</code>.

</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Partial ROC is calculated following Peterson et al.
(2008; <a href="https://doi.org/10.1016/j.ecolmodel.2007.11.008">doi:10.1016/j.ecolmodel.2007.11.008</a>). This function is a modification
of the PartialROC funcion, available at <a href="https://github.com/narayanibarve/ENMGadgets">https://github.com/narayanibarve/ENMGadgets</a>.
</p>


<h3>Value</h3>

<p>A data.frame containing the AUC values and AUC ratios calculated for each iteration.
</p>


<h3>Author(s)</h3>

<p>Andri Signorell &lt;andri@signorell.net&gt;

</p>


<h3>References</h3>

<p>Peterson, A.T. et al. (2008) Rethinking receiver operating characteristic analysis applications in ecological niche modeling. Ecol. Modell., 213, 63-72.
</p>


<h3>See Also</h3>

<p><code><a href="pROC.html#topic+pROC">pROC</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>r.glm &lt;- FitMod(diabetes ~ ., data = d.pima, fitfn="logit")
ROC(r.glm)

# plot ROC curves for a list of models
r.rp &lt;- FitMod(diabetes ~ ., data = d.pima, fitfn="rpart")

# combine models to a list
mlst &lt;- list(r.glm, r.rp)

# do the plot
for(i in seq_along(mlst))
  if(i==1){
    plot(ROC(mlst[[i]], grid=TRUE, col=c(hred, hblue)[i]))
  } else {
    lines(ROC(mlst[[i]], col=c(hred, hblue)[i]))
  }
</code></pre>

<hr>
<h2 id='Rules'>Extract Rules from 'rpart' Object

</h2><span id='topic+Rules'></span>

<h3>Description</h3>

<p>Extract rules from an rpart object. This can be useful, if the rules must be implemented in another system. The rules contain all the criteria for the binary splits of an rpart tree from the root node down to the specified leaf.

</p>


<h3>Usage</h3>

<pre><code class='language-R'>Rules(x, node = NULL, leafonly = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Rules_+3A_x">x</code></td>
<td>
<p>the rpart object to extract the rules from

</p>
</td></tr>
<tr><td><code id="Rules_+3A_node">node</code></td>
<td>
<p>integer vector, defining the nodes whose details are required.</p>
</td></tr>
<tr><td><code id="Rules_+3A_leafonly">leafonly</code></td>
<td>
<p>boolean, defining if only the rules leading to end nodes (&quot;leafs&quot;) should be returned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function builds upon the original function <code><a href="rpart.html#topic+path.rpart">path.rpart</a></code>, which is bulky in some situations.
</p>


<h3>Value</h3>

<p>a list with the rules
</p>
<table>
<tr><td><code>frame</code></td>
<td>
<p>the frame of the rpart</p>
</td></tr>
<tr><td><code>ylevels</code></td>
<td>
<p>the y values of the node</p>
</td></tr>
<tr><td><code>ds.size</code></td>
<td>
<p>the size of the dataset</p>
</td></tr>
<tr><td><code>path</code></td>
<td>
<p>a list of character vecotrs containing the rules</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Andri Signorell &lt;andri@signorell.net&gt;

</p>


<h3>See Also</h3>

<p><code><a href="rpart.html#topic+rpart">rpart</a></code>, <code><a href="rpart.html#topic+path.rpart">path.rpart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>r.rp &lt;- FitMod(diabetes ~ ., data=d.pima, fitfn="rpart")
Rules(r.rp)
</code></pre>

<hr>
<h2 id='SplitTrainTest'>Split DataFrame in Train an Test Sample

</h2><span id='topic+SplitTrainTest'></span>

<h3>Description</h3>

<p>For modeling we usually split our data frame in a train sample, where we train our model on, and a test sample, where we test, how good it works. This function splits a given data frame in two parts, one being the training sample and the other the test sample in form of a list with two elements.

</p>


<h3>Usage</h3>

<pre><code class='language-R'>SplitTrainTest(x, p = 0.1, seed = NULL, logical = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SplitTrainTest_+3A_x">x</code></td>
<td>
<p>data.frame

</p>
</td></tr>
<tr><td><code id="SplitTrainTest_+3A_p">p</code></td>
<td>
<p>proportion for test sample. Default is 10%.

</p>
</td></tr>
<tr><td><code id="SplitTrainTest_+3A_seed">seed</code></td>
<td>
<p>initialization for random number generator.
</p>
</td></tr>
<tr><td><code id="SplitTrainTest_+3A_logical">logical</code></td>
<td>
<p>logical, defining if a logical vector should be returned or the list with train and test data. Default is <code>FALSE</code>. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>In order to obtain reasonable models, we should ensure two points. The dataset must be large enough to yield statistically meaningful results and it should be representative of the data set as a whole. Assuming that our test set meets the preceding two conditions, our goal is to create a model that generalizes well to new data. We are aiming for a model that equally well predicts training and test data. We should never train on test data. If we are seeing surprisingly good results on the evaluation metrics, it might be a sign that we're accidentally training on the test set.

</p>


<h3>Value</h3>

<p>If <code>logical</code> is <code>FALSE</code> a list with two data frames, <code>train</code> and <code>test</code>, of the same structure as the given data in <code>x</code>
<br /> if <code>logical</code> is <code>TRUE</code> a logical vector containing <code>nrow</code> elements of <code>TRUE</code> and <code>FALSE</code>





</p>


<h3>Author(s)</h3>

<p>Andri Signorell &lt;andri@signorell.net&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>SplitTrainTest(d.pima)
</code></pre>

<hr>
<h2 id='TModC'>Compare Classification Models
</h2><span id='topic+TModC'></span><span id='topic+plot.TModC'></span>

<h3>Description</h3>

<p>For the comparison of several classification models, the AUC values and BrierScore values of the models are determined and tabulated. Both the absolute values and the relative values are reported, each related to the model with the highest corresponding value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TModC(..., newdata = NULL, reference = NULL, ord = NULL)

## S3 method for class 'TModC'
plot(x, col = NULL, args.legend = NULL,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TModC_+3A_...">...</code></td>
<td>
<p>the models to be compared</p>
</td></tr>
<tr><td><code id="TModC_+3A_x">x</code></td>
<td>
<p><code>TModC</code> object to plot</p>
</td></tr>
<tr><td><code id="TModC_+3A_newdata">newdata</code></td>
<td>
<p>the data to use for predicting. If not provided, the <code>model.frame</code> will be used.</p>
</td></tr>
<tr><td><code id="TModC_+3A_reference">reference</code></td>
<td>
<p>the reference values</p>
</td></tr>
<tr><td><code id="TModC_+3A_ord">ord</code></td>
<td>
<p>character defining the order of the results table, can be any of <code>"auc"</code>, <code>"bs"</code>,
<code>"auc_p"</code>, <code>"bs_p"</code>, <code>"bs_rnk"</code>, <code>"auc_rnk"</code>, <code>"ensemble"</code> (using the mean of <code>"auc_p"</code> and <code>"bs_p"</code> for the ranking).</p>
</td></tr>
<tr><td><code id="TModC_+3A_col">col</code></td>
<td>
<p>the color for the lines in the ROC plot</p>
</td></tr>
<tr><td><code id="TModC_+3A_args.legend">args.legend</code></td>
<td>
<p>the legend to be placed in the ROC plot</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix with the columns
</p>
<table>
<tr><td><code>auc</code></td>
<td>
<p>absolute value of area under the ROC curve (AUC) </p>
</td></tr>
<tr><td><code>auc_p</code></td>
<td>
<p>percentage of the auc based on the best observerd AUC</p>
</td></tr>
<tr><td><code>auc_rnk</code></td>
<td>
<p>the rank of the auc</p>
</td></tr>
<tr><td><code>bs</code></td>
<td>
<p>absolute value of the Brier score</p>
</td></tr>
<tr><td><code>bs_p</code></td>
<td>
<p>percentage of the Brier score based on the best observed BS</p>
</td></tr>
<tr><td><code>bs_rnk</code></td>
<td>
<p>the rank of the BS</p>
</td></tr>
<tr><td><code>auc_grnk</code></td>
<td>
<p>character representation of the AUC rank</p>
</td></tr>
<tr><td><code>bs_grnk</code></td>
<td>
<p>character representation of the BS rank</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Andri Signorell &lt;andri@signorell.net&gt;

</p>


<h3>See Also</h3>

<p><code><a href="DescTools.html#topic+TMod">TMod</a></code>, <code><a href="DescTools.html#topic+BrierScore">BrierScore</a></code>, <code><a href="DescTools.html#topic+AUC">AUC</a></code>, <code><a href="#topic+ROC">ROC</a></code>

</p>


<h3>Examples</h3>

<pre><code class='language-R'>d.pim &lt;- SplitTrainTest(d.pima, p = 0.2)
mdiab &lt;- formula(diabetes ~ pregnant + glucose + pressure + triceps +
                            insulin + mass + pedigree + age)

r.glm &lt;- FitMod(mdiab, data=d.pim$train, fitfn="logit")
r.rp &lt;- FitMod(mdiab, data=d.pim$train, fitfn="rpart")
mods &lt;- list(glm=r.glm, rp=r.rp)

# the table with the measures
(tm &lt;- TModC(mods, ord="auc"))

# plotting the ROC curves
plot(tm, col=c("darkmagenta", "dodgerblue"))
</code></pre>

<hr>
<h2 id='Tobit'>Tobit Regression</h2><span id='topic+Tobit'></span>

<h3>Description</h3>

<p>Fitting and testing Tobit regression models for censored data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Tobit(formula, left = 0, right = Inf, dist = "gaussian",
      subset = NULL, data = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Tobit_+3A_formula">formula</code></td>
<td>
<p>a symbolic description of a regression model of type
<code>y ~ x1 + x2 + ...</code>.</p>
</td></tr>
<tr><td><code id="Tobit_+3A_left">left</code></td>
<td>
<p>left limit for the censored dependent variable <code>y</code>.
If set to <code>-Inf</code>, <code>y</code> is assumed not to be left-censored.</p>
</td></tr>
<tr><td><code id="Tobit_+3A_right">right</code></td>
<td>
<p>right limit for the censored dependent variable <code>y</code>.
If set to <code>Inf</code>, the default, <code>y</code> is assumed not to be right-censored.</p>
</td></tr>
<tr><td><code id="Tobit_+3A_dist">dist</code></td>
<td>
<p>assumed distribution for the dependent variable <code>y</code>.
This is passed to <code><a href="survival.html#topic+survreg">survreg</a></code>, see the respective man page for
more details.</p>
</td></tr>
<tr><td><code id="Tobit_+3A_subset">subset</code></td>
<td>
<p>a specification of the rows to be used.</p>
</td></tr>
<tr><td><code id="Tobit_+3A_data">data</code></td>
<td>
<p>a data frame containing the variables in the model.</p>
</td></tr>
<tr><td><code id="Tobit_+3A_...">...</code></td>
<td>
<p>further arguments passed to <code><a href="survival.html#topic+survreg">survreg</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>Tobit</code> is an alias for the <span class="pkg">AER</span> function <code><a href="AER.html#topic+tobit">tobit</a></code> (Achim Zeileis &lt;Achim.Zeileis@R-project.org&gt;).
All details can be found there.
</p>


<h3>Value</h3>

<p>An object of class <code>"Tobit"</code> inheriting from class <code>"survreg"</code>.
</p>


<h3>Author(s)</h3>

<p>Andri Signorell</p>


<h3>Examples</h3>

<pre><code class='language-R'># still to do
</code></pre>

<hr>
<h2 id='Tune'>Tune Classificators

</h2><span id='topic+Tune'></span>

<h3>Description</h3>

<p>Some classifiers benefit more from adjusted parameters to a particular dataset than others. However, it is often not clear from the beginning how the parameters have to be determined.  What often only remains is a grid search when several parameters have to be found in combination. The present function uses a grid search approch for the decisive arguments (typically for a neural network, a random forest or a classification tree). However it's not restricted to these models, any model fulfilling weak interface standards could be provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Tune(x, ..., testset = NULL, keepmod = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Tune_+3A_x">x</code></td>
<td>
<p>the model to be tuned, best (but not necessarily) trained with <code><a href="#topic+FitMod">FitMod</a></code>.
</p>
</td></tr>
<tr><td><code id="Tune_+3A_...">...</code></td>
<td>
<p>a list of parameters, containing the values to be used for a grid search.</p>
</td></tr>
<tr><td><code id="Tune_+3A_testset">testset</code></td>
<td>
<p>a testset containing all variables required in the model to be used for calculating independently the accuracy (normally a subset of the original dataset).</p>
</td></tr>
<tr><td><code id="Tune_+3A_keepmod">keepmod</code></td>
<td>
<p>logical, defining if all fitted models should be returned in the result set. Default is <code>TRUE</code>. (Keep an eye on your RAM!)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function creates a n-dimensional grid according to the given parameters and calculates the model with the combinations of all the parameters. The accuracy for the models are calculated insample and on a test set, if one has been provided.
</p>
<p>It makes sense to avoid overfitting to provide a test set to also be evaluated.
A matrix with all combination of the values for the given parameters, sorted by accuracy, either by the accuracy achieved in the test set or the insample accuracy is returned.
</p>


<h3>Value</h3>

<p>a matrix with all supplied parameters and a column <code>"acc"</code> and <code>"test_acc"</code> (if a test set has been provided) </p>


<h3>Author(s)</h3>

<p>Andri Signorell &lt;andri@signorell.net&gt;</p>


<h3>Examples</h3>

<pre><code class='language-R'>d.pim &lt;- SplitTrainTest(d.pima, p = 0.2)
mdiab &lt;- formula(diabetes ~ pregnant + glucose + pressure + triceps
                 + insulin + mass + pedigree + age)

# tune a neural network for size and decay
r.nn &lt;- FitMod(mdiab, data=d.pim$train, fitfn="nnet")
(tu &lt;- Tune(r.nn, size=12:17, decay = 10^(-4:-1), testset=d.pim$test))

# tune a random forest
r.rf &lt;- FitMod(mdiab, data=d.pim$train, fitfn="randomForest")
(tu &lt;- Tune(r.rf, mtry=seq(2, 20, 2), testset=d.pim$test))

# tune a SVM model
r.svm &lt;- FitMod(mdiab, data=d.pim$train, fitfn="svm")

tu &lt;- Tune(r.svm,
           kernel=c("radial", "sigmoid"),
           cost=c(0.1,1,10,100,1000),
           gamma=c(0.5,1,2,3,4), testset=d.pim$test)

# let's get some more quality measures
tu$modpar$Sens &lt;- sapply(tu$mods, Sens)     # Sensitivity
tu$modpar$Spec &lt;- sapply(tu$mods, Spec)     # Specificity
Sort(tu$modpar, ord="test_acc", decreasing=TRUE)

</code></pre>

<hr>
<h2 id='VarImp'>Variable Importance for Regression and Classification Models</h2><span id='topic+GarsonWeights'></span><span id='topic+VarImp'></span><span id='topic+VarImp.default'></span><span id='topic+VarImp.FitMod'></span><span id='topic+plot.VarImp'></span><span id='topic+print.VarImp'></span>

<h3>Description</h3>

<p>Variable importance is an expression of the desire to know how important a variable is within a group of predictors for a particular model. But in general it is not a well defined concept, say there is no theoretically defined variable importance metric. Nevertheless, there are some approaches that have been established in practice for some regression and classification algorithms.
The present function provides an interface for calculating variable importance for some of the models produced by <code>FitMod</code>, comprising linear models, classification trees, random forests, C5 trees and neural networks. The intention here is to provide reasonably homogeneous output and plot routines.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>VarImp(x, scale = FALSE, sort = TRUE, ...)

## S3 method for class 'FitMod'
VarImp(x, scale = FALSE, sort = TRUE, type=NULL, ...)
## Default S3 method:
VarImp(x, scale = FALSE, sort = TRUE, ...)


## S3 method for class 'VarImp'
plot(x, sort = TRUE, maxrows = NULL,
           main = "Variable importance", ...)

## S3 method for class 'VarImp'
print(x, digits = 3, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="VarImp_+3A_x">x</code></td>
<td>
<p>the fitted model</p>
</td></tr>
<tr><td><code id="VarImp_+3A_scale">scale</code></td>
<td>
<p>logical, should the importance values be scaled to 0 and 100?</p>
</td></tr>
<tr><td><code id="VarImp_+3A_...">...</code></td>
<td>
<p>parameters to pass to the specific <code>VarImp</code> methods</p>
</td></tr>
<tr><td><code id="VarImp_+3A_sort">sort</code></td>
<td>
<p>the name of the column, the importance table should be ordered after</p>
</td></tr>
<tr><td><code id="VarImp_+3A_maxrows">maxrows</code></td>
<td>
<p>the maximum number of rows to be reported</p>
</td></tr>
<tr><td><code id="VarImp_+3A_main">main</code></td>
<td>
<p>the main title for the plot </p>
</td></tr>
<tr><td><code id="VarImp_+3A_type">type</code></td>
<td>
<p>some models have more than one type available to produce a variable importance. Linear models accept one of <code>"lmg"</code>, <code>"pmvd"</code>, <code>"first"</code>, <code>"last"</code>, <code>"betasq"</code>, <code>"pratt"</code>.</p>
</td></tr>
<tr><td><code id="VarImp_+3A_digits">digits</code></td>
<td>
<p> the number of digits for printing the &quot;VarImp&quot; table </p>
</td></tr>
</table>


<h3>Details</h3>

<p><b>Linear Models</b>:  <code style="white-space: pre;">&#8288;   &#8288;</code> For linear models there's a fine package <span class="pkg">relaimpo</span> available on CRAN containing several interesting approaches for quantifying the variable importance. See the original documentation.
</p>
<p><b>rpart</b>, <b>Random Forest</b>:  <code style="white-space: pre;">&#8288;   &#8288;</code> <code>VarImp.rpart</code> and <code>VarImp.randomForest</code> are wrappers around the importance functions from the <span class="pkg">rpart</span> or <span class="pkg">randomForest</span> packages, respectively.
</p>
<p><b>C5.0</b>:  <code style="white-space: pre;">&#8288;   &#8288;</code> C5.0 measures predictor importance by determining the
percentage of training set samples that fall into all the terminal
nodes after the split. For example, the predictor in the first split
automatically has an importance measurement of 100 percent since all
samples are affected by this split. Other predictors may be used
frequently in splits, but if the terminal nodes cover only a handful
of training set samples, the importance scores may be close to
zero. The same strategy is applied to rule-based models and boosted
versions of the model. The underlying function can also return the
number of times each predictor was involved in a split by using the
option <code>metric="usage"</code>.
</p>
<p><b>Neural Networks</b>:  <code style="white-space: pre;">&#8288;   &#8288;</code> The method used here is &quot;Garson weights&quot;.
</p>
<p><b>SVM, GLM, Multinom</b>:  <code style="white-space: pre;">&#8288;   &#8288;</code> There are no implementations for these models so far.
</p>


<h3>Value</h3>

<p>A data frame with class <code>c("VarImp.train", "data.frame")</code> for
<code>VarImp.train</code> or a matrix for other models.
</p>


<h3>Author(s)</h3>

<p>Andri Signorell &lt;andri@signorell.net&gt;</p>


<h3>References</h3>

<p>Quinlan, J. (1992). Learning with continuous classes. Proceedings of
the 5th Australian Joint Conference On Artificial Intelligence,
343-348.</p>

<hr>
<h2 id='zeroinfl'>Zero-inflated Count Data Regression</h2><span id='topic+zeroinfl'></span><span id='topic+print.zeroinfl'></span>

<h3>Description</h3>

<p>Fit zero-inflated regression models for count data via maximum likelihood.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>zeroinfl(formula, data, subset, na.action, weights, offset,
  dist = c("poisson", "negbin", "geometric"),
  link = c("logit", "probit", "cloglog", "cauchit", "log"),
  control = zeroinfl.control(...),
  model = TRUE, y = TRUE, x = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="zeroinfl_+3A_formula">formula</code></td>
<td>
<p>symbolic description of the model, see details.</p>
</td></tr>
<tr><td><code id="zeroinfl_+3A_data">data</code>, <code id="zeroinfl_+3A_subset">subset</code>, <code id="zeroinfl_+3A_na.action">na.action</code></td>
<td>
<p>arguments controlling formula processing
via <code><a href="stats.html#topic+model.frame">model.frame</a></code>.</p>
</td></tr>
<tr><td><code id="zeroinfl_+3A_weights">weights</code></td>
<td>
<p>optional numeric vector of weights.</p>
</td></tr>
<tr><td><code id="zeroinfl_+3A_offset">offset</code></td>
<td>
<p>optional numeric vector with an a priori known component to be
included in the linear predictor of the count model. See below for more
information on offsets.</p>
</td></tr>
<tr><td><code id="zeroinfl_+3A_dist">dist</code></td>
<td>
<p>character specification of count model family (a log link is
always used).</p>
</td></tr>
<tr><td><code id="zeroinfl_+3A_link">link</code></td>
<td>
<p>character specification of link function in the binary
zero-inflation model (a binomial family is always used).</p>
</td></tr>
<tr><td><code id="zeroinfl_+3A_control">control</code></td>
<td>
<p>a list of control arguments specified via
<code><a href="#topic+zeroinfl.control">zeroinfl.control</a></code>.</p>
</td></tr>
<tr><td><code id="zeroinfl_+3A_model">model</code>, <code id="zeroinfl_+3A_y">y</code>, <code id="zeroinfl_+3A_x">x</code></td>
<td>
<p>logicals. If <code>TRUE</code> the corresponding components
of the fit (model frame, response, model matrix) are returned.</p>
</td></tr>
<tr><td><code id="zeroinfl_+3A_...">...</code></td>
<td>
<p>arguments passed to <code><a href="#topic+zeroinfl.control">zeroinfl.control</a></code> in the
default setup.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Zero-inflated count models are two-component mixture models
combining a point mass at zero with a proper count distribution.
Thus, there are two sources of zeros: zeros may come from
both the point mass and from the count component. Usually the count model
is a Poisson or negative binomial regression (with log link).
The geometric distribution is a special case of the negative binomial
with size parameter equal to 1.
For modeling the unobserved state (zero vs. count), a binary model is used
that captures the probability of zero inflation.
in the simplest case only with an intercept but potentially containing regressors.
For this zero-inflation model, a binomial model with different links can be
used, typically logit or probit.
</p>
<p>The <code>formula</code> can be used to specify both components of the model:
If a <code>formula</code> of type <code>y ~ x1 + x2</code> is supplied, then the same
regressors are employed in both components. This is equivalent to
<code>y ~ x1 + x2 | x1 + x2</code>. Of course, a different set of regressors
could be specified for the count and zero-inflation component, e.g.,
<code>y ~ x1 + x2 | z1 + z2 + z3</code> giving the count data model <code>y ~ x1 + x2</code>
conditional on (<code>|</code>) the zero-inflation model <code>y ~ z1 + z2 + z3</code>.
A simple inflation model where all zero counts have the same
probability of belonging to the zero component can by specified by the formula
<code>y ~ x1 + x2 | 1</code>.
</p>
<p>Offsets can be specified in both components of the model pertaining to count and
zero-inflation model: <code>y ~ x1 + offset(x2) | z1 + z2 + offset(z3)</code>, where
<code>x2</code> is used as an offset (i.e., with coefficient fixed to 1) in the
count component and <code>z3</code> analogously in the zero-inflation component. By the rule
stated above <code>y ~ x1 + offset(x2)</code> is expanded to
<code>y ~ x1 + offset(x2) | x1 + offset(x2)</code>. Instead of using the
<code>offset()</code> wrapper within the <code>formula</code>, the <code>offset</code> argument
can also be employed which sets an offset only for the count model. Thus,
<code>formula = y ~ x1</code> and <code>offset = x2</code> is equivalent to
<code>formula = y ~ x1 + offset(x2) | x1</code>.
</p>
<p>All parameters are estimated by maximum likelihood using <code><a href="stats.html#topic+optim">optim</a></code>,
with control options set in <code><a href="#topic+zeroinfl.control">zeroinfl.control</a></code>.
Starting values can be supplied, estimated by the EM (expectation maximization)
algorithm, or by <code><a href="stats.html#topic+glm.fit">glm.fit</a></code> (the default). Standard errors
are derived numerically using the Hessian matrix returned by <code><a href="stats.html#topic+optim">optim</a></code>.
See <code><a href="#topic+zeroinfl.control">zeroinfl.control</a></code> for details.
</p>
<p>The returned fitted model object is of class <code>"zeroinfl"</code> and is similar
to fitted <code>"glm"</code> objects. For elements such as <code>"coefficients"</code> or
<code>"terms"</code> a list is returned with elements for the zero and count component,
respectively. For details see below.
</p>
<p>A set of standard extractor functions for fitted model objects is available for
objects of class <code>"zeroinfl"</code>, including methods to the generic functions
<code><a href="base.html#topic+print">print</a></code>, <code><a href="base.html#topic+summary">summary</a></code>, <code><a href="stats.html#topic+coef">coef</a></code>,
<code><a href="stats.html#topic+vcov">vcov</a></code>, <code><a href="stats.html#topic+logLik">logLik</a></code>, <code><a href="stats.html#topic+residuals">residuals</a></code>,
<code><a href="stats.html#topic+predict">predict</a></code>, <code><a href="stats.html#topic+fitted">fitted</a></code>, <code><a href="stats.html#topic+terms">terms</a></code>,
<code><a href="stats.html#topic+model.matrix">model.matrix</a></code>. See <code><a href="#topic+predict.zeroinfl">predict.zeroinfl</a></code> for more details
on all methods.
</p>


<h3>Value</h3>

<p>An object of class <code>"zeroinfl"</code>, i.e., a list with components including
</p>
<table>
<tr><td><code>coefficients</code></td>
<td>
<p>a list with elements <code>"count"</code> and <code>"zero"</code>
containing the coefficients from the respective models,</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>a vector of raw residuals (observed - fitted),</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>a vector of fitted means,</p>
</td></tr>
<tr><td><code>optim</code></td>
<td>
<p>a list with the output from the <code>optim</code> call for
minimizing the negative log-likelihood,</p>
</td></tr>
<tr><td><code>control</code></td>
<td>
<p>the control arguments passed to the <code>optim</code> call,</p>
</td></tr>
<tr><td><code>start</code></td>
<td>
<p>the starting values for the parameters passed to the <code>optim</code> call,</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>the case weights used,</p>
</td></tr>
<tr><td><code>offset</code></td>
<td>
<p>a list with elements <code>"count"</code> and <code>"zero"</code>
containing the offset vectors (if any) from the respective models,</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>number of observations (with weights &gt; 0),</p>
</td></tr>
<tr><td><code>df.null</code></td>
<td>
<p>residual degrees of freedom for the null model (= <code>n - 2</code>),</p>
</td></tr>
<tr><td><code>df.residual</code></td>
<td>
<p>residual degrees of freedom for fitted model,</p>
</td></tr>
<tr><td><code>terms</code></td>
<td>
<p>a list with elements <code>"count"</code>, <code>"zero"</code> and
<code>"full"</code> containing the terms objects for the respective models,</p>
</td></tr>
<tr><td><code>theta</code></td>
<td>
<p>estimate of the additional <code class="reqn">\theta</code> parameter of the
negative binomial model (if a negative binomial regression is used),</p>
</td></tr>
<tr><td><code>SE.logtheta</code></td>
<td>
<p>standard error for <code class="reqn">\log(\theta)</code>,</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>log-likelihood of the fitted model,</p>
</td></tr>
<tr><td><code>vcov</code></td>
<td>
<p>covariance matrix of all coefficients in the model (derived from the
Hessian of the <code>optim</code> output),</p>
</td></tr>
<tr><td><code>dist</code></td>
<td>
<p>character string describing the count distribution used,</p>
</td></tr>
<tr><td><code>link</code></td>
<td>
<p>character string describing the link of the zero-inflation model,</p>
</td></tr>
<tr><td><code>linkinv</code></td>
<td>
<p>the inverse link function corresponding to <code>link</code>,</p>
</td></tr>
<tr><td><code>converged</code></td>
<td>
<p>logical indicating successful convergence of <code>optim</code>,</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the original function call,</p>
</td></tr>
<tr><td><code>formula</code></td>
<td>
<p>the original formula,</p>
</td></tr>
<tr><td><code>levels</code></td>
<td>
<p>levels of the categorical regressors,</p>
</td></tr>
<tr><td><code>contrasts</code></td>
<td>
<p>a list with elements <code>"count"</code> and <code>"zero"</code>
containing the contrasts corresponding to <code>levels</code> from the
respective models,</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>the full model frame (if <code>model = TRUE</code>),</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the response count vector (if <code>y = TRUE</code>),</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>a list with elements <code>"count"</code> and <code>"zero"</code>
containing the model matrices from the respective models
(if <code>x = TRUE</code>),</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Achim Zeileis &lt;Achim.Zeileis@R-project.org&gt;</p>


<h3>References</h3>

<p>Cameron, A. Colin and Pravin K. Trevedi. 1998. <em>Regression Analysis of Count
Data.</em> New York: Cambridge University Press.
</p>
<p>Cameron, A. Colin and Pravin K. Trivedi. 2005. <em>Microeconometrics: Methods and Applications</em>.
Cambridge: Cambridge University Press.
</p>
<p>Lambert, Diane. 1992. &ldquo;Zero-Inflated Poisson Regression,
with an Application to Defects in Manufacturing.&rdquo; <em>Technometrics</em>. <b>34</b>(1):1-14
</p>
<p>Zeileis, Achim, Christian Kleiber and Simon Jackman 2008.
&ldquo;Regression Models for Count Data in R.&rdquo;
<em>Journal of Statistical Software</em>, <b>27</b>(8).
URL <a href="https://www.jstatsoft.org/v27/i08/">https://www.jstatsoft.org/v27/i08/</a>.
</p>


<h3>See Also</h3>

<p><code><a href="pscl.html#topic+zeroinfl.control">zeroinfl.control</a></code>, <code><a href="stats.html#topic+glm">glm</a></code>,
<code><a href="stats.html#topic+glm.fit">glm.fit</a></code>, <code><a href="MASS.html#topic+glm.nb">glm.nb</a></code>,
<code><a href="pscl.html#topic+hurdle">hurdle</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## data
data("bioChemists", package = "ModTools")

## without inflation
## ("art ~ ." is "art ~ fem + mar + kid5 + phd + ment")
fm_pois &lt;- glm(art ~ ., data = bioChemists, family = poisson)
fm_qpois &lt;- glm(art ~ ., data = bioChemists, family = quasipoisson)
fm_nb &lt;- MASS::glm.nb(art ~ ., data = bioChemists)

## with simple inflation (no regressors for zero component)
fm_zip &lt;- zeroinfl(art ~ . | 1, data = bioChemists)
fm_zinb &lt;- zeroinfl(art ~ . | 1, data = bioChemists, dist = "negbin")

## inflation with regressors
## ("art ~ . | ." is "art ~ fem + mar + kid5 + phd + ment | fem + mar + kid5 + phd + ment")
fm_zip2 &lt;- zeroinfl(art ~ . | ., data = bioChemists)
fm_zinb2 &lt;- zeroinfl(art ~ . | ., data = bioChemists, dist = "negbin")
</code></pre>

<hr>
<h2 id='zeroinfl.control'>Control Parameters for Zero-inflated Count Data Regression</h2><span id='topic+zeroinfl.control'></span>

<h3>Description</h3>

<p>Various parameters that control fitting of zero-inflated regression models
using <code><a href="#topic+zeroinfl">zeroinfl</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>zeroinfl.control(method = "BFGS", maxit = 10000, trace = FALSE,
  EM = FALSE, start = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="zeroinfl.control_+3A_method">method</code></td>
<td>
<p>characters string specifying the <code>method</code> argument
passed to <code><a href="stats.html#topic+optim">optim</a></code>.</p>
</td></tr>
<tr><td><code id="zeroinfl.control_+3A_maxit">maxit</code></td>
<td>
<p>integer specifying the <code>maxit</code> argument (maximal number
of iterations) passed to <code><a href="stats.html#topic+optim">optim</a></code>.</p>
</td></tr>
<tr><td><code id="zeroinfl.control_+3A_trace">trace</code></td>
<td>
<p>logical or integer controlling whether tracing information on 
the progress of the optimization should be produced (passed to <code><a href="stats.html#topic+optim">optim</a></code>).</p>
</td></tr>
<tr><td><code id="zeroinfl.control_+3A_em">EM</code></td>
<td>
<p>logical. Should starting values be estimated by the EM (expectation
maximization) algorithm? See details.</p>
</td></tr>
<tr><td><code id="zeroinfl.control_+3A_start">start</code></td>
<td>
<p>an optional list with elements <code>"count"</code> and <code>"zero"</code>
(and potentially <code>"theta"</code>) containing the coefficients for the corresponding component.</p>
</td></tr>
<tr><td><code id="zeroinfl.control_+3A_...">...</code></td>
<td>
<p>arguments passed to <code><a href="stats.html#topic+optim">optim</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>All parameters in <code><a href="#topic+zeroinfl">zeroinfl</a></code> are estimated by maximum likelihood
using <code><a href="stats.html#topic+optim">optim</a></code> with control options set in <code><a href="#topic+zeroinfl.control">zeroinfl.control</a></code>.
Most arguments are passed on directly to <code>optim</code>, only <code>trace</code> is also
used within <code>zeroinfl</code> and <code>EM</code>/<code>start</code> control the choice
of starting values for calling <code>optim</code>.
</p>
<p>Starting values can be supplied, estimated by the EM (expectation maximization)
algorithm, or by <code><a href="stats.html#topic+glm.fit">glm.fit</a></code> (the default). Standard errors are
derived numerically using
the Hessian matrix returned by <code><a href="stats.html#topic+optim">optim</a></code>. To supply starting 
values, <code>start</code> should be a list with elements <code>"count"</code> and <code>"zero"</code>
and potentially <code>"theta"</code> (for negative binomial components only) containing
the starting values for the coefficients of the corresponding component of the
model.
</p>


<h3>Value</h3>

<p>A list with the arguments specified.
</p>


<h3>Author(s)</h3>

<p>Achim Zeileis &lt;Achim.Zeileis@R-project.org&gt;</p>


<h3>See Also</h3>

<p><code><a href="#topic+zeroinfl">zeroinfl</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data("bioChemists", package = "pscl")

## default start values
fm1 &lt;- zeroinfl(art ~ ., data = bioChemists)

## use EM algorithm for start values
fm2 &lt;- zeroinfl(art ~ ., data = bioChemists, EM = TRUE)

## user-supplied start values
fm3 &lt;- zeroinfl(art ~ ., data = bioChemists,
  start = list(count = c(0.7, -0.2, 0.1, -0.2, 0, 0), zero = -1.7))

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
