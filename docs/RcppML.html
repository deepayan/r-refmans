<!DOCTYPE html><html><head><title>Help for package RcppML</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {RcppML}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#RcppML'><p>RcppML: Rcpp Machine Learning Library</p></a></li>
<li><a href='#bipartition'><p>Bipartition a sample set</p></a></li>
<li><a href='#dclust'><p>Divisive clustering</p></a></li>
<li><a href='#getRcppMLthreads'><p>Get the number of threads RcppML should use</p></a></li>
<li><a href='#mse'><p>Mean Squared Error loss of a factor model</p></a></li>
<li><a href='#nmf'><p>Non-negative matrix factorization</p></a></li>
<li><a href='#nnls'><p>Non-negative least squares</p></a></li>
<li><a href='#project'><p>Project a linear factor model</p></a></li>
<li><a href='#setRcppMLthreads'><p>Set the number of threads RcppML should use</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Rcpp Machine Learning Library</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.7</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-09-21</td>
</tr>
<tr>
<td>Description:</td>
<td>Fast machine learning algorithms including matrix factorization 
    and divisive clustering for large sparse and dense matrices.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, Matrix, methods, stats</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppEigen</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/zdebruine/RcppML">https://github.com/zdebruine/RcppML</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/zdebruine/RcppML/issues">https://github.com/zdebruine/RcppML/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-09-21 18:39:44 UTC; Owner</td>
</tr>
<tr>
<td>Author:</td>
<td>Zachary DeBruine <a href="https://orcid.org/0000-0003-2234-4827"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Zachary DeBruine &lt;zacharydebruine@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-09-21 19:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='RcppML'>RcppML: Rcpp Machine Learning Library</h2><span id='topic+RcppML'></span><span id='topic+RcppML-package'></span>

<h3>Description</h3>

<p>High-performance non-negative matrix factorization and linear model projection for sparse matrices, and fast non-negative least squares implementations
</p>


<h3>Author(s)</h3>

<p>Zach DeBruine
</p>

<hr>
<h2 id='bipartition'>Bipartition a sample set</h2><span id='topic+bipartition'></span>

<h3>Description</h3>

<p>Spectral biparitioning by rank-2 matrix factorization
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bipartition(
  A,
  tol = 1e-05,
  maxit = 100,
  nonneg = TRUE,
  samples = 1:ncol(A),
  seed = NULL,
  verbose = FALSE,
  calc_dist = FALSE,
  diag = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bipartition_+3A_a">A</code></td>
<td>
<p>matrix of features-by-samples in dense or sparse format (preferred classes are &quot;matrix&quot; or &quot;Matrix::dgCMatrix&quot;, respectively). Prefer sparse storage when more than half of all values are zero.</p>
</td></tr>
<tr><td><code id="bipartition_+3A_tol">tol</code></td>
<td>
<p>stopping criteria, the correlation distance between <code class="reqn">w</code> across consecutive iterations, <code class="reqn">1 - cor(w_i, w_{i-1})</code></p>
</td></tr>
<tr><td><code id="bipartition_+3A_maxit">maxit</code></td>
<td>
<p>stopping criteria, maximum number of alternating updates of <code class="reqn">w</code> and <code class="reqn">h</code></p>
</td></tr>
<tr><td><code id="bipartition_+3A_nonneg">nonneg</code></td>
<td>
<p>enforce non-negativity</p>
</td></tr>
<tr><td><code id="bipartition_+3A_samples">samples</code></td>
<td>
<p>samples to include in bipartition, numbered from 1 to <code>ncol(A)</code>. Default is <code>NULL</code> for all samples.</p>
</td></tr>
<tr><td><code id="bipartition_+3A_seed">seed</code></td>
<td>
<p>random seed for model initialization</p>
</td></tr>
<tr><td><code id="bipartition_+3A_verbose">verbose</code></td>
<td>
<p>print model tolerances between iterations</p>
</td></tr>
<tr><td><code id="bipartition_+3A_calc_dist">calc_dist</code></td>
<td>
<p>calculate the relative cosine distance of samples within a cluster to either cluster centroid. If <code>TRUE</code>, centers for clusters will also be calculated.</p>
</td></tr>
<tr><td><code id="bipartition_+3A_diag">diag</code></td>
<td>
<p>scale factors in <code class="reqn">w</code> and <code class="reqn">h</code> to sum to 1 by introducing a diagonal, <code class="reqn">d</code>. This should generally never be set to <code>FALSE</code>. Diagonalization enables symmetry of models in factorization of symmetric matrices, convex L1 regularization, and consistent factor scalings.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Spectral bipartitioning is a popular subroutine in divisive clustering. The sign of the difference between sample loadings in factors of a rank-2 matrix factorization
gives a bipartition that is nearly identical to an SVD.
</p>
<p>Rank-2 matrix factorization by alternating least squares is faster than rank-2-truncated SVD (i.e. <em>irlba</em>).
</p>
<p>This function is a specialization of rank-2 <code><a href="#topic+nmf">nmf</a></code> with support for factorization of only a subset of samples, and with additional calculations on the factorization model relevant to bipartitioning. See <code><a href="#topic+nmf">nmf</a></code> for details regarding rank-2 factorization.
</p>


<h3>Value</h3>

<p>A list giving the bipartition and useful statistics:
</p>

<ul>
<li><p> v       : vector giving difference between sample loadings between factors in a rank-2 factorization
</p>
</li>
<li><p> dist    : relative cosine distance of samples within a cluster to centroids of assigned vs. not-assigned cluster
</p>
</li>
<li><p> size1   : number of samples in first cluster (positive loadings in 'v')
</p>
</li>
<li><p> size2   : number of samples in second cluster (negative loadings in 'v')
</p>
</li>
<li><p> samples1: indices of samples in first cluster
</p>
</li>
<li><p> samples2: indices of samples in second cluster
</p>
</li>
<li><p> center1 : mean feature loadings across samples in first cluster
</p>
</li>
<li><p> center2 : mean feature loadings across samples in second cluster
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Zach DeBruine
</p>


<h3>References</h3>

<p>Kuang, D, Park, H. (2013). &quot;Fast rank-2 nonnegative matrix factorization for hierarchical document clustering.&quot; Proc. 19th ACM SIGKDD intl. conf. on Knowledge discovery and data mining.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nmf">nmf</a></code>, <code><a href="#topic+dclust">dclust</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(Matrix)
data(iris)
A &lt;- as(as.matrix(iris[,1:4]), "dgCMatrix")
bipartition(A, calc_dist = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='dclust'>Divisive clustering</h2><span id='topic+dclust'></span>

<h3>Description</h3>

<p>Recursive bipartitioning by rank-2 matrix factorization with an efficient modularity-approximate stopping criteria
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dclust(
  A,
  min_samples,
  min_dist = 0,
  verbose = TRUE,
  tol = 1e-05,
  maxit = 100,
  nonneg = TRUE,
  seed = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dclust_+3A_a">A</code></td>
<td>
<p>matrix of features-by-samples in sparse format (preferred class is &quot;Matrix::dgCMatrix&quot;)</p>
</td></tr>
<tr><td><code id="dclust_+3A_min_samples">min_samples</code></td>
<td>
<p>stopping criteria giving the minimum number of samples permitted in a cluster</p>
</td></tr>
<tr><td><code id="dclust_+3A_min_dist">min_dist</code></td>
<td>
<p>stopping criteria giving the minimum cosine distance of samples within a cluster to the center of their assigned vs. unassigned cluster. If <code>0</code>, neither this distance nor cluster centroids will be calculated.</p>
</td></tr>
<tr><td><code id="dclust_+3A_verbose">verbose</code></td>
<td>
<p>print number of divisions in each generation</p>
</td></tr>
<tr><td><code id="dclust_+3A_tol">tol</code></td>
<td>
<p>in rank-2 NMF, the correlation distance (<code class="reqn">1 - R^2</code>) between <code class="reqn">w</code> across consecutive iterations at which to stop factorization</p>
</td></tr>
<tr><td><code id="dclust_+3A_maxit">maxit</code></td>
<td>
<p>stopping criteria, maximum number of alternating updates of <code class="reqn">w</code> and <code class="reqn">h</code></p>
</td></tr>
<tr><td><code id="dclust_+3A_nonneg">nonneg</code></td>
<td>
<p>in rank-2 NMF, enforce non-negativity</p>
</td></tr>
<tr><td><code id="dclust_+3A_seed">seed</code></td>
<td>
<p>random seed for rank-2 NMF model initialization</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Divisive clustering is a sensitive and fast method for sample classification. Samples are recursively partitioned into two groups until a stopping criteria is satisfied and prevents successful partitioning.
</p>
<p>See <code><a href="#topic+nmf">nmf</a></code> and <code><a href="#topic+bipartition">bipartition</a></code> for technical considerations and optimizations relevant to bipartitioning.
</p>
<p><strong>Stopping criteria</strong>. Two stopping criteria are used to prevent indefinite division of clusters and tune the clustering resolution to a desirable range:
</p>

<ul>
<li> <p><code>min_samples</code>: Minimum number of samples permitted in a cluster
</p>
</li>
<li> <p><code>min_dist</code>: Minimum cosine distance of samples to their cluster center relative to their unassigned cluster center (an approximation of Newman-Girvan modularity)
</p>
</li></ul>

<p>Newman-Girvan modularity (<code class="reqn">Q</code>) is an interpretable and widely used measure of modularity for a bipartition. However, it requires the calculation of distance between all within-cluster and between-cluster sample pairs. This is computationally intensive, especially for large sample sets.
</p>
<p><code>dclust</code> uses a measure which linearly approximates Newman-Girvan modularity, and simply requires the calculation of distance between all samples in a cluster and both cluster centers (the assigned and unassigned center), which is orders of magnitude faster to compute. Cosine distance is used instead of Euclidean distance since it handles outliers and sparsity well.
</p>
<p>A bipartition is rejected if either of the two clusters contains fewer than <code>min_samples</code> or if the mean relative cosine distance of the bipartition is less than <code>min_dist</code>.
</p>
<p>A bipartition will only be attempted if there are more than <code>2 * min_samples</code> samples in the cluster, meaning that <code>dist</code> may not be calculated for some clusters.
</p>
<p><strong>Reproducibility.</strong> Because rank-2 NMF is approximate and requires random initialization, results may vary slightly across restarts. Therefore, specify a <code>seed</code> to guarantee absolute reproducibility.
</p>
<p>Other than setting the seed, reproducibility may be improved by setting <code>tol</code> to a smaller number to increase the exactness of each bipartition.
</p>


<h3>Value</h3>

<p>A list of lists corresponding to individual clusters:
</p>

<ul>
<li><p> id      : character sequence of &quot;0&quot; and &quot;1&quot; giving position of clusters along splitting hierarchy
</p>
</li>
<li><p> samples : indices of samples in the cluster
</p>
</li>
<li><p> center  : mean feature expression of all samples in the cluster
</p>
</li>
<li><p> dist    : if applicable, relative cosine distance of samples in cluster to assigned/unassigned cluster center.
</p>
</li>
<li><p> leaf    : is cluster a leaf node
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Zach DeBruine
</p>


<h3>References</h3>

<p>Schwartz, G. et al. &quot;TooManyCells identifies and visualizes relationships of single-cell clades&quot;. Nature Methods (2020).
</p>
<p>Newman, MEJ. &quot;Modularity and community structure in networks&quot;. PNAS (2006)
</p>
<p>Kuang, D, Park, H. (2013). &quot;Fast rank-2 nonnegative matrix factorization for hierarchical document clustering.&quot; Proc. 19th ACM SIGKDD intl. conf. on Knowledge discovery and data mining.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bipartition">bipartition</a></code>, <code><a href="#topic+nmf">nmf</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(Matrix)
data(USArrests)
A &lt;- as(as.matrix(t(USArrests)), "dgCMatrix")
clusters &lt;- dclust(A, min_samples = 2, min_dist = 0.001)
str(clusters)

## End(Not run)
</code></pre>

<hr>
<h2 id='getRcppMLthreads'>Get the number of threads RcppML should use</h2><span id='topic+getRcppMLthreads'></span>

<h3>Description</h3>

<p>Get the number of threads that will be used by RcppML functions supporting parallelization with OpenMP. Use <code><a href="#topic+setRcppMLthreads">setRcppMLthreads</a></code> to set the number of threads to be used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getRcppMLthreads()
</code></pre>


<h3>Value</h3>

<p>integer giving number of threads to be used by RcppML functions. <code>0</code> corresponds to all available threads, as determined by OpenMP.
</p>


<h3>Author(s)</h3>

<p>Zach DeBruine
</p>


<h3>See Also</h3>

<p><code><a href="#topic+setRcppMLthreads">setRcppMLthreads</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# set serial configuration
setRcppMLthreads(1)
getRcppMLthreads()

# restore default parallel configuration, 
# letting OpenMP decide how many threads to use
setRcppMLthreads(0)
getRcppMLthreads()

## End(Not run)
</code></pre>

<hr>
<h2 id='mse'>Mean Squared Error loss of a factor model</h2><span id='topic+mse'></span>

<h3>Description</h3>

<p>MSE of factor models <code>w</code> and <code>h</code> given sparse matrix <code class="reqn">A</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mse(A, w, d = NULL, h, mask_zeros = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mse_+3A_a">A</code></td>
<td>
<p>matrix of features-by-samples in dense or sparse format (preferred classes are &quot;matrix&quot; or &quot;Matrix::dgCMatrix&quot;, respectively). Prefer sparse storage when more than half of all values are zero.</p>
</td></tr>
<tr><td><code id="mse_+3A_w">w</code></td>
<td>
<p>dense matrix of class <code>matrix</code> with factors (columns) by features (rows)</p>
</td></tr>
<tr><td><code id="mse_+3A_d">d</code></td>
<td>
<p>diagonal scaling vector of rank length</p>
</td></tr>
<tr><td><code id="mse_+3A_h">h</code></td>
<td>
<p>dense matrix of class <code>matrix</code> with samples (columns) by factors (rows)</p>
</td></tr>
<tr><td><code id="mse_+3A_mask_zeros">mask_zeros</code></td>
<td>
<p>handle zeros as missing values, available only when <code>A</code> is sparse</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mean squared error of a matrix factorization of the form <code class="reqn">A = wdh</code> is given by </p>
<p style="text-align: center;"><code class="reqn">\frac{\sum_{i,j}{(A - wdh)^2}}{ij}</code>
</p>
<p> where <code class="reqn">i</code> and <code class="reqn">j</code> are the number of rows and columns in <code class="reqn">A</code>.
</p>
<p>Thus, this function simply calculates the cross-product of <code class="reqn">wh</code> or <code class="reqn">wdh</code> (if <code class="reqn">d</code> is specified),
subtracts that from <code class="reqn">A</code>, squares the result, and calculates the mean of all values.
</p>
<p>If no diagonal scaling vector is present in the model, input <code>d = rep(1, k)</code> where <code>k</code> is the rank of the model.
</p>
<p><strong>Parallelization.</strong> Calculation of mean squared error is performed in parallel across columns in <code>A</code> using the number of threads set by <code><a href="#topic+setRcppMLthreads">setRcppMLthreads</a></code>.
By default, all available threads are used, see <code><a href="#topic+getRcppMLthreads">getRcppMLthreads</a></code>.
</p>


<h3>Value</h3>

<p>mean squared error of the factorization model
</p>


<h3>Author(s)</h3>

<p>Zach DeBruine
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(Matrix)
A &lt;- Matrix::rsparsematrix(1000, 1000, 0.1)
model &lt;- nmf(A, k = 10, tol = 0.01)
c_mse &lt;- mse(A, model$w, model$d, model$h)
R_mse &lt;- mean((A - model$w %*% Diagonal(x = model$d) %*% model$h)^2)
all.equal(c_mse, R_mse)

## End(Not run)
</code></pre>

<hr>
<h2 id='nmf'>Non-negative matrix factorization</h2><span id='topic+nmf'></span>

<h3>Description</h3>

<p>Sparse matrix factorization of the form <code class="reqn">A = wdh</code> by alternating least squares with optional non-negativity constraints.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nmf(
  A,
  k,
  tol = 1e-04,
  maxit = 100,
  verbose = TRUE,
  L1 = c(0, 0),
  seed = NULL,
  mask_zeros = FALSE,
  diag = TRUE,
  nonneg = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nmf_+3A_a">A</code></td>
<td>
<p>matrix of features-by-samples in dense or sparse format (preferred classes are &quot;matrix&quot; or &quot;Matrix::dgCMatrix&quot;, respectively). Prefer sparse storage when more than half of all values are zero.</p>
</td></tr>
<tr><td><code id="nmf_+3A_k">k</code></td>
<td>
<p>rank</p>
</td></tr>
<tr><td><code id="nmf_+3A_tol">tol</code></td>
<td>
<p>stopping criteria, the correlation distance between <code class="reqn">w</code> across consecutive iterations, <code class="reqn">1 - cor(w_i, w_{i-1})</code></p>
</td></tr>
<tr><td><code id="nmf_+3A_maxit">maxit</code></td>
<td>
<p>stopping criteria, maximum number of alternating updates of <code class="reqn">w</code> and <code class="reqn">h</code></p>
</td></tr>
<tr><td><code id="nmf_+3A_verbose">verbose</code></td>
<td>
<p>print model tolerances between iterations</p>
</td></tr>
<tr><td><code id="nmf_+3A_l1">L1</code></td>
<td>
<p>L1/LASSO penalties between 0 and 1, array of length two for <code>c(w, h)</code></p>
</td></tr>
<tr><td><code id="nmf_+3A_seed">seed</code></td>
<td>
<p>random seed for model initialization</p>
</td></tr>
<tr><td><code id="nmf_+3A_mask_zeros">mask_zeros</code></td>
<td>
<p>handle zeros as missing values, available only when <code>A</code> is sparse</p>
</td></tr>
<tr><td><code id="nmf_+3A_diag">diag</code></td>
<td>
<p>scale factors in <code class="reqn">w</code> and <code class="reqn">h</code> to sum to 1 by introducing a diagonal, <code class="reqn">d</code>. This should generally never be set to <code>FALSE</code>. Diagonalization enables symmetry of models in factorization of symmetric matrices, convex L1 regularization, and consistent factor scalings.</p>
</td></tr>
<tr><td><code id="nmf_+3A_nonneg">nonneg</code></td>
<td>
<p>enforce non-negativity</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This fast non-negative matrix factorization (NMF) implementation decomposes a matrix <code class="reqn">A</code> into lower-rank
non-negative matrices <code class="reqn">w</code> and <code class="reqn">h</code>, with factors scaled to sum to 1 via multiplication by a diagonal, <code class="reqn">d</code>: </p>
<p style="text-align: center;"><code class="reqn">A = wdh</code>
</p>

<p>The scaling diagonal enables symmetric factorization, convex L1 regularization, and consistent factor scalings regardless of random initialization.
</p>
<p>The factorization model is randomly initialized, and <code class="reqn">w</code> and <code class="reqn">h</code> are updated alternately using least squares.
Given <code class="reqn">A</code> and <code class="reqn">w</code>, <code class="reqn">h</code> is updated according to the equation: </p>
<p style="text-align: center;"><code class="reqn">w^Twh = wA_j</code>
</p>

<p>This equation is in the form <code class="reqn">ax = b</code> where <code class="reqn">a = w^Tw</code>, <code class="reqn">x = h</code>, and <code class="reqn">b = wA_j</code> for all columns <code class="reqn">j</code> in <code class="reqn">A</code>.
</p>
<p>The corresponding update for <code class="reqn">w</code> is </p>
<p style="text-align: center;"><code class="reqn">hh^Tw = hA^T_j</code>
</p>

<p><strong>Stopping criteria.</strong> Alternating least squares projections (see <code><a href="#topic+project">project</a></code> subroutine) are repeated until a stopping criteria is satisfied, which is either a maximum number of
iterations or a tolerance based on the correlation distance between models (<code class="reqn">1 - cor(w_i, w_{i-1})</code>) across consecutive iterations. Use the <code>tol</code> parameter to control the stopping criteria for alternating updates:
</p>

<ul>
<li> <p><code>tol = 1e-2</code> is appropriate for approximate mean squared error determination and coarse cross-validation, useful for rank determination.
</p>
</li>
<li> <p><code>tol = 1e-3</code> to <code>1e-4</code> are suitable for rapid expermentation, cross-validation, and preliminary analysis.
</p>
</li>
<li> <p><code>tol = 1e-5</code> and smaller for publication-quality runs
</p>
</li>
<li> <p><code>tol = 1e-10</code> and smaller for robust factorizations at or near machine-precision
</p>
</li></ul>

<p><strong>Parallelization.</strong> Least squares projections in factorizations of rank-3 and greater are parallelized using the number of threads set by <code><a href="#topic+setRcppMLthreads">setRcppMLthreads</a></code>.
By default, all available threads are used, see <code><a href="#topic+getRcppMLthreads">getRcppMLthreads</a></code>.
The overhead of parallization is too great to benefit rank-1 and rank-2 factorization.
</p>
<p><strong>Specializations.</strong> There are specializations for symmetric matrices, and for rank-1 and rank-2 factorization.
</p>
<p><strong>L1 regularization</strong>. L1 penalization increases the sparsity of factors, but does not change the information content of the model
or the relative contributions of the leading coefficients in each factor to the model. L1 regularization only slightly increases the error of a model.
L1 penalization should be used to aid interpretability. Penalty values should range from 0 to 1, where 1 gives complete sparsity. In this implementation of NMF,
a scaling diagonal ensures that the L1 penalty is equally applied across all factors regardless of random initialization and the distribution of the model.
Many other implementations of matrix factorization claim to apply L1, but the magnitude of the penalty is at the mercy of the random distribution and
more significantly affects factors with lower overall contribution to the model. L1 regularization of rank-1 and rank-2 factorizations has no effect.
</p>
<p><strong>Rank-2 factorization.</strong> When <code class="reqn">k = 2</code>, a very fast optimized algorithm is used. Two-variable least squares solutions to the problem <code class="reqn">ax = b</code> are found by direct substitution:
</p>
<p style="text-align: center;"><code class="reqn">x_1 = \frac{a_{22}b_1 - a_{12}b_2}{a_{11}a_{22} - a_{12}^2}</code>
</p>

<p style="text-align: center;"><code class="reqn">x_2 = \frac{a_{11}b_2 - a_{12}b_1}{a_{11}a_{22} - a_{12}^2}</code>
</p>

<p>In the above equations, the denominator is constant and thus needs to be calculated only once. Additionally, if non-negativity constraints are to be imposed,
if <code class="reqn">x_1 &lt; 0</code> then <code class="reqn">x_1 = 0</code> and <code class="reqn">x_2 = \frac{b_1}{a_{11}}</code>.
Similarly, if <code class="reqn">x_2 &lt; 0</code> then <code class="reqn">x_2 = 0</code> and <code class="reqn">x_1 = \frac{b_2}{a_{22}}</code>.
</p>
<p>Rank-2 NMF is useful for bipartitioning, and is a subroutine in <code><a href="#topic+bipartition">bipartition</a></code>, where the sign of the difference between sample loadings
in both factors gives the partitioning.
</p>
<p><strong>Rank-1 factorization.</strong> Rank-1 factorization by alternating least squares gives vectors equivalent to the first singular vectors in an SVD. It is a normalization of the data to a middle point,
and may be useful for ordering samples based on the most significant axis of variation (i.e. pseudotime trajectories). Diagonal scaling guarantees consistent linear
scaling of the factor across random restarts.
</p>
<p><strong>Random seed and reproducibility.</strong> Results of a rank-1 and rank-2 factorizations should be reproducible regardless of random seed. For higher-rank models,
results across random restarts should, in theory, be comparable at very high tolerances (i.e. machine precision for <em>double</em>, corresponding to about <code>tol = 1e-10</code>). However, to guarantee
reproducibility without such low tolerances, set the <code>seed</code> argument. Note that <code>set.seed()</code> will not work. Only random initialization is supported, as other methods
incur unnecessary overhead and sometimes trap updates into local minima.
</p>
<p><strong>Rank determination.</strong> This function does not attempt to provide a method for rank determination. Like any clustering algorithm or dimensional reduction,
finding the optimal rank can be subjective. An easy way to
estimate rank uses the &quot;elbow method&quot;, where the inflection point on a plot of Mean Squared Error loss (MSE) vs. rank
gives a good idea of the rank at which most of the signal has been captured in the model. Unfortunately, this inflection point
is not often as obvious for NMF as it is for SVD or PCA.
</p>
<p>k-fold cross-validation is a better method. Missing value of imputation has previously been proposed, but is arguably no less subjective
than test-training splits and requires computationally slower factorization updates using missing values, which are not supported here.
</p>
<p><strong>Symmetric factorization.</strong> Special optimization for symmetric matrices is automatically applied. Specifically, alternating updates of <code>w</code> and <code>h</code>
require transposition of <code>A</code>, but <code>A == t(A)</code> when <code>A</code> is symmetric, thus no up-front transposition is performed.
</p>
<p><strong>Zero-masking</strong>. When zeros in a data structure can be regarded as &quot;missing&quot;, <code>mask_zeros = TRUE</code> may be set. However, this requires a slower
algorithm, and tolerances will fluctuate more dramatically.
</p>
<p><strong>Publication reference.</strong> For theoretical and practical considerations, please see our manuscript: &quot;DeBruine ZJ, Melcher K, Triche TJ (2021)
High-performance non-negative matrix factorization for large single cell data.&quot; on BioRXiv.
</p>


<h3>Value</h3>

<p>A list giving the factorization model:
</p>

<ul>
<li><p> w    : feature factor matrix
</p>
</li>
<li><p> d    : scaling diagonal vector
</p>
</li>
<li><p> h    : sample factor matrix
</p>
</li>
<li><p> tol  : tolerance between models at final update
</p>
</li>
<li><p> iter : number of alternating updates run
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Zach DeBruine
</p>


<h3>References</h3>

<p>DeBruine, ZJ, Melcher, K, and Triche, TJ. (2021). &quot;High-performance non-negative matrix factorization for large single-cell data.&quot; BioRXiv.
</p>
<p>Lin, X, and Boutros, PC (2020). &quot;Optimization and expansion of non-negative matrix factorization.&quot; BMC Bioinformatics.
</p>
<p>Lee, D, and Seung, HS (1999). &quot;Learning the parts of objects by non-negative matrix factorization.&quot; Nature.
</p>
<p>Franc, VC, Hlavac, VC, Navara, M. (2005). &quot;Sequential Coordinate-Wise Algorithm for the Non-negative Least Squares Problem&quot;. Proc. Int'l Conf. Computer Analysis of Images and Patterns. Lecture Notes in Computer Science.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nnls">nnls</a></code>, <code><a href="#topic+project">project</a></code>, <code><a href="#topic+mse">mse</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(Matrix)
# basic NMF
model &lt;- nmf(rsparsematrix(1000, 100, 0.1), k = 10)

# compare rank-2 NMF to second left vector in an SVD
data(iris)
A &lt;- as(as.matrix(iris[,1:4]), "dgCMatrix")
nmf_model &lt;- nmf(A, 2, tol = 1e-5)
bipartitioning_vector &lt;- apply(nmf_model$w, 1, diff)
second_left_svd_vector &lt;- base::svd(A, 2)$u[,2]
abs(cor(bipartitioning_vector, second_left_svd_vector))

# compare rank-1 NMF with first singular vector in an SVD
abs(cor(nmf(A, 1)$w[,1], base::svd(A, 2)$u[,1]))

# symmetric NMF
A &lt;- crossprod(rsparsematrix(100, 100, 0.02))
model &lt;- nmf(A, 10, tol = 1e-5, maxit = 1000)
plot(model$w, t(model$h))
# see package vignette for more examples

## End(Not run)
</code></pre>

<hr>
<h2 id='nnls'>Non-negative least squares</h2><span id='topic+nnls'></span>

<h3>Description</h3>

<p>Solves the equation <code>a %*% x = b</code> for <code>x</code> subject to <code class="reqn">x &gt; 0</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nnls(a, b, cd_maxit = 100L, cd_tol = 1e-08, fast_nnls = FALSE, L1 = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nnls_+3A_a">a</code></td>
<td>
<p>symmetric positive definite matrix giving coefficients of the linear system</p>
</td></tr>
<tr><td><code id="nnls_+3A_b">b</code></td>
<td>
<p>matrix giving the right-hand side(s) of the linear system</p>
</td></tr>
<tr><td><code id="nnls_+3A_cd_maxit">cd_maxit</code></td>
<td>
<p>maximum number of coordinate descent iterations</p>
</td></tr>
<tr><td><code id="nnls_+3A_cd_tol">cd_tol</code></td>
<td>
<p>stopping criteria, difference in <code class="reqn">x</code> across consecutive solutions over the sum of <code class="reqn">x</code></p>
</td></tr>
<tr><td><code id="nnls_+3A_fast_nnls">fast_nnls</code></td>
<td>
<p>initialize coordinate descent with a FAST NNLS approximation</p>
</td></tr>
<tr><td><code id="nnls_+3A_l1">L1</code></td>
<td>
<p>L1/LASSO penalty to be subtracted from <code>b</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a very fast implementation of non-negative least squares (NNLS), suitable for very small or very large systems.
</p>
<p><strong>Algorithm</strong>. Sequential coordinate descent (CD) is at the core of this implementation, and requires an initialization of <code class="reqn">x</code>. There are two supported methods for initialization of <code class="reqn">x</code>:
</p>

<ol>
<li> <p><strong>Zero-filled initialization</strong> when <code>fast_nnls = FALSE</code> and <code>cd_maxit &gt; 0</code>. This is generally very efficient for well-conditioned and small systems.
</p>
</li>
<li> <p><strong>Approximation with FAST</strong> when <code>fast_nnls = TRUE</code>. Forward active set tuning (FAST), described below, finds an approximate active set using unconstrained least squares solutions found by Cholesky decomposition and substitution. To use only FAST approximation, set <code>cd_maxit = 0</code>.
</p>
</li></ol>

<p><code>a</code> must be symmetric positive definite if FAST NNLS is used, but this is not checked.
</p>
<p>See our BioRXiv manuscript (references) for benchmarking against Lawson-Hanson NNLS and for a more technical introduction to these methods.
</p>
<p><strong>Coordinate Descent NNLS</strong>. Least squares by <strong>sequential coordinate descent</strong> is used to ensure the solution returned is exact. This algorithm was
introduced by Franc et al. (2005), and our implementation is a vectorized and optimized rendition of that found in the NNLM R package by Xihui Lin (2020).
</p>
<p><strong>FAST NNLS.</strong> Forward active set tuning (FAST) is an exact or near-exact NNLS approximation initialized by an unconstrained
least squares solution. Negative values in this unconstrained solution are set to zero (the &quot;active set&quot;), and all
other values are added  to a &quot;feasible set&quot;. An unconstrained least squares solution is then solved for the
&quot;feasible set&quot;, any negative values in the resulting solution are set to zero, and the process is repeated until
the feasible set solution is strictly positive.
</p>
<p>The FAST algorithm has a definite convergence guarantee because the
feasible set will either converge or become smaller with each iteration. The result is generally exact or nearly
exact for small well-conditioned systems (&lt; 50 variables) within 2 iterations and thus sets up coordinate
descent for very rapid convergence. The FAST method is similar to the first phase of the so-called &quot;TNT-NN&quot; algorithm (Myre et al., 2017),
but the latter half of that method relies heavily on heuristics to refine the approximate active set, which we avoid by using
coordinate descent instead.
</p>


<h3>Value</h3>

<p>vector or matrix giving solution for <code>x</code>
</p>


<h3>Author(s)</h3>

<p>Zach DeBruine
</p>


<h3>References</h3>

<p>DeBruine, ZJ, Melcher, K, and Triche, TJ. (2021). &quot;High-performance non-negative matrix factorization for large single-cell data.&quot; BioRXiv.
</p>
<p>Franc, VC, Hlavac, VC, and Navara, M. (2005). &quot;Sequential Coordinate-Wise Algorithm for the Non-negative Least Squares Problem. Proc. Int'l Conf. Computer Analysis of Images and Patterns.&quot;
</p>
<p>Lin, X, and Boutros, PC (2020). &quot;Optimization and expansion of non-negative matrix factorization.&quot; BMC Bioinformatics.
</p>
<p>Myre, JM, Frahm, E, Lilja DJ, and Saar, MO. (2017) &quot;TNT-NN: A Fast Active Set Method for Solving Large Non-Negative Least Squares Problems&quot;. Proc. Computer Science.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nmf">nmf</a></code>, <code><a href="#topic+project">project</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# compare solution to base::solve for a random system
X &lt;- matrix(runif(100), 10, 10)
a &lt;- crossprod(X)
b &lt;- crossprod(X, runif(10))
unconstrained_soln &lt;- solve(a, b)
nonneg_soln &lt;- nnls(a, b)
unconstrained_err &lt;- mean((a %*% unconstrained_soln - b)^2)
nonnegative_err &lt;- mean((a %*% nonneg_soln - b)^2)
unconstrained_err
nonnegative_err
all.equal(solve(a, b), nnls(a, b))

# example adapted from multiway::fnnls example 1
X &lt;- matrix(1:100,50,2)
y &lt;- matrix(101:150,50,1)
beta &lt;- solve(crossprod(X)) %*% crossprod(X, y)
beta
beta &lt;- nnls(crossprod(X), crossprod(X, y))
beta

## End(Not run)
</code></pre>

<hr>
<h2 id='project'>Project a linear factor model</h2><span id='topic+project'></span>

<h3>Description</h3>

<p>Solves the equation <code class="reqn">A = wh</code> for either <code class="reqn">h</code> or <code class="reqn">w</code> given either <code class="reqn">w</code> or <code class="reqn">h</code> and <code class="reqn">A</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>project(A, w = NULL, h = NULL, nonneg = TRUE, L1 = 0, mask_zeros = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="project_+3A_a">A</code></td>
<td>
<p>matrix of features-by-samples in dense or sparse format (preferred classes are &quot;matrix&quot; or &quot;Matrix::dgCMatrix&quot;, respectively). Prefer sparse storage when more than half of all values are zero.</p>
</td></tr>
<tr><td><code id="project_+3A_w">w</code></td>
<td>
<p>dense matrix of factors x features giving the linear model to be projected (if <code>h = NULL</code>)</p>
</td></tr>
<tr><td><code id="project_+3A_h">h</code></td>
<td>
<p>dense matrix of factors x samples giving the linear model to be projected (if <code>w = NULL</code>)</p>
</td></tr>
<tr><td><code id="project_+3A_nonneg">nonneg</code></td>
<td>
<p>enforce non-negativity</p>
</td></tr>
<tr><td><code id="project_+3A_l1">L1</code></td>
<td>
<p>L1/LASSO penalty to be applied. No scaling is performed. See details.</p>
</td></tr>
<tr><td><code id="project_+3A_mask_zeros">mask_zeros</code></td>
<td>
<p>handle zeros as missing values, available only when <code>A</code> is sparse</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the classical alternating least squares matrix factorization update problem <code class="reqn">A = wh</code>, the updates
(or projection) of <code class="reqn">h</code> is given by the equation: </p>
<p style="text-align: center;"><code class="reqn">w^Twh = wA_j</code>
</p>

<p>which is in the form <code class="reqn">ax = b</code> where <code class="reqn">a = w^Tw</code> <code class="reqn">x = h</code> and <code class="reqn">b = wA_j</code> for all columns <code class="reqn">j</code> in <code class="reqn">A</code>.
</p>
<p>Given <code class="reqn">A</code>, <code>project</code> can solve for either <code class="reqn">w</code> or <code class="reqn">h</code> given the other:
</p>

<ul>
<li><p> When given <code class="reqn">A</code> and <code class="reqn">w</code>, <code class="reqn">h</code> is found using a highly efficient parallelization scheme.
</p>
</li>
<li><p> When given <code class="reqn">A</code> and <code class="reqn">h</code>, <code class="reqn">w</code> is found without transposition of <code class="reqn">A</code> (as would be the case in traditional block-pivoting matrix factorization) by accumulating the right-hand sides of linear systems in-place in <code class="reqn">A</code>, then solving the systems. Note that <code class="reqn">w</code> may also be found by inputting the transpose of <code class="reqn">A</code> and <code class="reqn">h</code> in place of <code class="reqn">w</code>, (i.e. <code>A = t(A), w = h, h = NULL</code>). However, for most applications, the cost of a single projection in-place is less than transposition of <code class="reqn">A</code>. However, for matrix factorization, it is desirable to transpose <code class="reqn">A</code> if possible because many projections are needed.
</p>
</li></ul>

<p><strong>Parallelization.</strong> Least squares projections in factorizations of rank-3 and greater are parallelized using the number of threads set by <code><a href="#topic+setRcppMLthreads">setRcppMLthreads</a></code>.
By default, all available threads are used, see <code><a href="#topic+getRcppMLthreads">getRcppMLthreads</a></code>. The overhead of parallization is too great for rank-1 and -2 factorization.
</p>
<p><strong>L1 Regularization.</strong> Any L1 penalty is subtracted from <code class="reqn">b</code> and should generally be scaled to <code>max(b)</code>, where <code class="reqn">b = WA_j</code> for all columns <code class="reqn">j</code> in <code class="reqn">A</code>. An easy way to properly scale an L1 penalty is to normalize all columns in <code class="reqn">w</code> to sum to 1. No scaling is applied in this function. Such scaling guarantees that <code>L1 = 1</code> gives a completely sparse solution.
</p>
<p><strong>Specializations.</strong> There are specializations for symmetric input matrices, and for rank-1 and rank-2 projections. See documentation for <code><a href="#topic+nmf">nmf</a></code> for theoretical details and guidance.
</p>
<p><strong>Publication reference.</strong> For theoretical and practical considerations, please see our manuscript: &quot;DeBruine ZJ, Melcher K, Triche TJ (2021)
High-performance non-negative matrix factorization for large single cell data.&quot; on BioRXiv.
</p>


<h3>Value</h3>

<p>matrix <code class="reqn">h</code> or <code class="reqn">w</code>
</p>


<h3>Author(s)</h3>

<p>Zach DeBruine
</p>


<h3>References</h3>

<p>DeBruine, ZJ, Melcher, K, and Triche, TJ. (2021). &quot;High-performance non-negative matrix factorization for large single-cell data.&quot; BioRXiv.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nnls">nnls</a></code>, <code><a href="#topic+nmf">nmf</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(Matrix)
w &lt;- matrix(runif(1000 * 10), 1000, 10)
h_true &lt;- matrix(runif(10 * 100), 10, 100)
# A is the crossproduct of "w" and "h" with 10% signal dropout
A &lt;- (w %*% h_true) * (rsparsematrix(1000, 100, 0.9) &gt; 0)
h &lt;- project(A, w)
cor(as.vector(h_true), as.vector(h))

# alternating projections refine solution (like NMF)
mse_bad &lt;- mse(A, w, rep(1, ncol(w)), h) # mse before alternating updates
h &lt;- project(A, w = w)
w &lt;- project(A, h = h)
h &lt;- project(A, w)
w &lt;- project(A, h = h)
h &lt;- project(A, w)
w &lt;- t(project(A, h = h))
mse_better &lt;- mse(A, w, rep(1, ncol(w)), h) # mse after alternating updates
mse_better &lt; mse_bad

# two ways to solve for "w" that give the same solution
w &lt;- project(A, h = h)
w2 &lt;- project(t(A), w = t(h))
all.equal(w, w2)

## End(Not run)
</code></pre>

<hr>
<h2 id='setRcppMLthreads'>Set the number of threads RcppML should use</h2><span id='topic+setRcppMLthreads'></span>

<h3>Description</h3>

<p>The number of threads is 0 by default (corresponding to all available threads), but can be set manually using this function. If you clear environment variables or affect the &quot;RcppMLthreads&quot; environment variable specifically, you will need to set your number of preferred threads again.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setRcppMLthreads(threads)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="setRcppMLthreads_+3A_threads">threads</code></td>
<td>
<p>number of threads to be used in RcppML functions that are parallelized with OpenMP.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The number of threads set affects OpenMP parallelization only for functions in the RcppML package. It does not affect other R packages that use OpenMP. Parallelization is used for projection of linear factor models with rank &gt; 2, calculation of mean squared error for linear factor models, and for divisive clustering.
</p>


<h3>Author(s)</h3>

<p>Zach DeBruine
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getRcppMLthreads">getRcppMLthreads</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# set serial configuration
setRcppMLthreads(1)
getRcppMLthreads()

# restore default parallel configuration, 
# letting OpenMP decide how many threads to use
setRcppMLthreads(0)
getRcppMLthreads()

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
