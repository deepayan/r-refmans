<!DOCTYPE html><html lang="en"><head><title>Help for package ari</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ari}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ari_burn_subtitles'><p>Burn Subtitles into a video</p></a></li>
<li><a href='#ari_example'><p>Get the path to an ari example file</p></a></li>
<li><a href='#ari_narrate'><p>Create a video from slides and a script</p></a></li>
<li><a href='#ari_spin'><p>Create a video from images and text</p></a></li>
<li><a href='#ari_stitch'><p>Create a video from images and audio</p></a></li>
<li><a href='#ari_talk'><p>Create spoken audio files</p></a></li>
<li><a href='#ffmpeg_codecs'><p>Get Codecs for ffmpeg</p></a></li>
<li><a href='#ffmpeg_exec'><p>Get Path to ffmpeg Executable</p></a></li>
<li><a href='#set_audio_codec'><p>Set Default Audio and Video Codecs</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Automated R Instructor</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.5</td>
</tr>
<tr>
<td>Description:</td>
<td>Create videos from 'R Markdown' documents, or images and audio
    files. These images can come from image files or HTML slides, and the audio
    files can be provided by the user or computer voice narration can be created
    using 'Amazon Polly'. The purpose of this package is to allow users to create
    accessible, translatable, and reproducible lecture videos. See
    <a href="https://aws.amazon.com/polly/">https://aws.amazon.com/polly/</a> for more information.</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>ffmpeg (&gt;= 3.2.4)</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>text2speech (&ge; 0.2.8), tuneR, webshot, purrr, rmarkdown,
xml2, rvest, tools, progress, hms</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, grDevices, xaringan, knitr</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://github.com/seankross/ari">http://github.com/seankross/ari</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="http://github.com/seankross/ari/issues">http://github.com/seankross/ari/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.0.2</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-02-08 05:58:11 UTC; sean</td>
</tr>
<tr>
<td>Author:</td>
<td>Sean Kross [aut, cre],
  John Muschelli [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Sean Kross &lt;sean@seankross.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-02-08 19:10:13 UTC</td>
</tr>
</table>
<hr>
<h2 id='ari_burn_subtitles'>Burn Subtitles into a video</h2><span id='topic+ari_burn_subtitles'></span>

<h3>Description</h3>

<p>Burn Subtitles into a video
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ari_burn_subtitles(video, srt, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ari_burn_subtitles_+3A_video">video</code></td>
<td>
<p>Video in <code>mp4</code> format</p>
</td></tr>
<tr><td><code id="ari_burn_subtitles_+3A_srt">srt</code></td>
<td>
<p>Subtitle file in <code>srt</code> format</p>
</td></tr>
<tr><td><code id="ari_burn_subtitles_+3A_verbose">verbose</code></td>
<td>
<p>print diagnostic messages.  If &gt; 1, 
then more are printed</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Name of output video
</p>


<h3>Note</h3>

<p>This needs <code>ffmpeg</code> that was compiled with 
<code>--enable-libass</code> as per 
<a href="https://trac.ffmpeg.org/wiki/HowToBurnSubtitlesIntoVideo">https://trac.ffmpeg.org/wiki/HowToBurnSubtitlesIntoVideo</a>
</p>

<hr>
<h2 id='ari_example'>Get the path to an ari example file</h2><span id='topic+ari_example'></span>

<h3>Description</h3>

<p>This function allows you to quickly access files that are used in
the ari documentation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ari_example(path = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ari_example_+3A_path">path</code></td>
<td>
<p>The name of the file. If no argument is provided then
all of the example files will be listed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character string
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ari_example("ari_intro.Rmd")
</code></pre>

<hr>
<h2 id='ari_narrate'>Create a video from slides and a script</h2><span id='topic+ari_narrate'></span>

<h3>Description</h3>

<p><code>ari_narrate</code> creates a video from a script written in markdown and HTML
slides created with <code><a href="rmarkdown.html#topic+rmarkdown">rmarkdown</a></code> or a similar package.
This function uses <a href="https://aws.amazon.com/polly/">Amazon Polly</a> 
via <code><a href="#topic+ari_spin">ari_spin</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ari_narrate(
  script,
  slides,
  output = tempfile(fileext = ".mp4"),
  voice = text2speech::tts_default_voice(service = service),
  service = "amazon",
  capture_method = c("vectorized", "iterative"),
  subtitles = FALSE,
  ...,
  verbose = FALSE,
  audio_codec = get_audio_codec(),
  video_codec = get_video_codec(),
  cleanup = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ari_narrate_+3A_script">script</code></td>
<td>
<p>Either a markdown file where every paragraph will be read over
a corresponding slide, or an <code>.Rmd</code> file where each HTML comment will
be used for narration.</p>
</td></tr>
<tr><td><code id="ari_narrate_+3A_slides">slides</code></td>
<td>
<p>A path or URL for an HTML slideshow created with 
<code><a href="rmarkdown.html#topic+rmarkdown">rmarkdown</a></code>, <code>xaringan</code>, or a 
similar package.</p>
</td></tr>
<tr><td><code id="ari_narrate_+3A_output">output</code></td>
<td>
<p>The path to the video file which will be created.</p>
</td></tr>
<tr><td><code id="ari_narrate_+3A_voice">voice</code></td>
<td>
<p>The voice you want to use. See 
<code><a href="text2speech.html#topic+tts_voices">tts_voices</a></code> for more information 
about what voices are available.</p>
</td></tr>
<tr><td><code id="ari_narrate_+3A_service">service</code></td>
<td>
<p>speech synthesis service to use,
passed to <code><a href="text2speech.html#topic+tts">tts</a></code>. 
Either <code>"amazon"</code> or <code>"google"</code>.</p>
</td></tr>
<tr><td><code id="ari_narrate_+3A_capture_method">capture_method</code></td>
<td>
<p>Either <code>"vectorized"</code> or <code>"iterative"</code>.
The vectorized mode is faster though it can cause screens to repeat. If
making a video from an <code><a href="rmarkdown.html#topic+ioslides_presentation">ioslides_presentation</a></code>
you should use <code>"iterative"</code>.</p>
</td></tr>
<tr><td><code id="ari_narrate_+3A_subtitles">subtitles</code></td>
<td>
<p>Should a <code>.srt</code> file be created with subtitles? The
default value is <code>FALSE</code>. If <code>TRUE</code> then a file with the same name
as the <code>output</code> argument will be created, but with the file extension
<code>.srt</code>.</p>
</td></tr>
<tr><td><code id="ari_narrate_+3A_...">...</code></td>
<td>
<p>Arguments that will be passed to <code><a href="webshot.html#topic+webshot">webshot</a></code>.</p>
</td></tr>
<tr><td><code id="ari_narrate_+3A_verbose">verbose</code></td>
<td>
<p>print diagnostic messages.  If &gt; 1, then more are printed</p>
</td></tr>
<tr><td><code id="ari_narrate_+3A_audio_codec">audio_codec</code></td>
<td>
<p>The audio encoder for the splicing.  If this
fails, try <code>copy</code>.</p>
</td></tr>
<tr><td><code id="ari_narrate_+3A_video_codec">video_codec</code></td>
<td>
<p>The video encoder for the splicing.  If this
fails, see <code>ffmpeg -codecs</code></p>
</td></tr>
<tr><td><code id="ari_narrate_+3A_cleanup">cleanup</code></td>
<td>
<p>If <code>TRUE</code>, interim files are deleted</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The output from <code><a href="#topic+ari_spin">ari_spin</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

# 
ari_narrate(system.file("test", "ari_intro_script.md", package = "ari"),
            system.file("test", "ari_intro.html", package = "ari"),
            voice = "Joey")


## End(Not run)
</code></pre>

<hr>
<h2 id='ari_spin'>Create a video from images and text</h2><span id='topic+ari_spin'></span>

<h3>Description</h3>

<p>Given equal length vectors of paths to images (preferably <code>.jpg</code>s
or <code>.png</code>s) and strings which will be synthesized by
<a href="https://aws.amazon.com/polly/">Amazon Polly</a> or 
any other synthesizer available in
<code><a href="text2speech.html#topic+tts">tts</a></code>, this function creates an
<code>.mp4</code> video file where each image is shown with
its corresponding narration. This function uses <code><a href="#topic+ari_stitch">ari_stitch</a></code> to
create the video.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ari_spin(
  images,
  paragraphs,
  output = tempfile(fileext = ".mp4"),
  voice = text2speech::tts_default_voice(service = service),
  service = "amazon",
  subtitles = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ari_spin_+3A_images">images</code></td>
<td>
<p>A vector of paths to images.</p>
</td></tr>
<tr><td><code id="ari_spin_+3A_paragraphs">paragraphs</code></td>
<td>
<p>A vector strings that will be spoken by Amazon Polly.</p>
</td></tr>
<tr><td><code id="ari_spin_+3A_output">output</code></td>
<td>
<p>A path to the video file which will be created.</p>
</td></tr>
<tr><td><code id="ari_spin_+3A_voice">voice</code></td>
<td>
<p>The voice you want to use. See 
<code><a href="text2speech.html#topic+tts_voices">tts_voices</a></code> for more information 
about what voices are available.</p>
</td></tr>
<tr><td><code id="ari_spin_+3A_service">service</code></td>
<td>
<p>speech synthesis service to use,
passed to <code><a href="text2speech.html#topic+tts">tts</a></code>.
Either <code>"amazon"</code> or <code>"google"</code>.</p>
</td></tr>
<tr><td><code id="ari_spin_+3A_subtitles">subtitles</code></td>
<td>
<p>Should a <code>.srt</code> file be created with subtitles? The
default value is <code>FALSE</code>. If <code>TRUE</code> then a file with the same name
as the <code>output</code> argument will be created, but with the file extension
<code>.srt</code>.</p>
</td></tr>
<tr><td><code id="ari_spin_+3A_...">...</code></td>
<td>
<p>additional arguments to <code><a href="#topic+ari_stitch">ari_stitch</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function needs to connect to
<a href="https://aws.amazon.com/">Amazon Web Services</a> in order to create the
narration. You can find a guide for accessing AWS from R
<a href="http://seankross.com/2017/05/02/Access-Amazon-Web-Services-in-R.html">here</a>.
For more information about how R connects
to Amazon Polly see the <code>aws.polly]</code> documentation 
<a href="https://github.com/cloudyr/aws.polly">here</a>.
</p>


<h3>Value</h3>

<p>The output from <code><a href="#topic+ari_stitch">ari_stitch</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

slides &lt;- system.file("test", c("mab2.png", "mab1.png"),
package = "ari")
sentences &lt;- c("Welome to my very interesting lecture.",
               "Here are some fantastic equations I came up with.")
ari_spin(slides, sentences, voice = "Joey")


## End(Not run)

</code></pre>

<hr>
<h2 id='ari_stitch'>Create a video from images and audio</h2><span id='topic+ari_stitch'></span>

<h3>Description</h3>

<p>Given a vector of paths to images (preferably <code>.jpg</code>s
or <code>.png</code>s) and a flat list of <code><a href="tuneR.html#topic+Wave">Wave</a></code>s of equal
length this function will create an <code>.mp4</code> video file where each image 
is shown with its corresponding audio. Take a look at the
<code><a href="tuneR.html#topic+readWave">readWave</a></code> function if you want to import your audio 
files into R. Please be sure that all images have the same dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ari_stitch(
  images,
  audio,
  output = tempfile(fileext = ".mp4"),
  verbose = FALSE,
  cleanup = TRUE,
  ffmpeg_opts = "",
  divisible_height = TRUE,
  audio_codec = get_audio_codec(),
  video_codec = get_video_codec(),
  video_sync_method = "2",
  audio_bitrate = NULL,
  video_bitrate = NULL,
  pixel_format = "yuv420p",
  fast_start = TRUE,
  deinterlace = TRUE,
  stereo_audio = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ari_stitch_+3A_images">images</code></td>
<td>
<p>A vector of paths to images.</p>
</td></tr>
<tr><td><code id="ari_stitch_+3A_audio">audio</code></td>
<td>
<p>A list of <code>Wave</code>s from tuneR.</p>
</td></tr>
<tr><td><code id="ari_stitch_+3A_output">output</code></td>
<td>
<p>A path to the video file which will be created.</p>
</td></tr>
<tr><td><code id="ari_stitch_+3A_verbose">verbose</code></td>
<td>
<p>print diagnostic messages.  If &gt; 1, then more are printed</p>
</td></tr>
<tr><td><code id="ari_stitch_+3A_cleanup">cleanup</code></td>
<td>
<p>If <code>TRUE</code>, interim files are deleted</p>
</td></tr>
<tr><td><code id="ari_stitch_+3A_ffmpeg_opts">ffmpeg_opts</code></td>
<td>
<p>additional options to send to <code>ffmpeg</code>.
This is an advanced option, use at your own risk</p>
</td></tr>
<tr><td><code id="ari_stitch_+3A_divisible_height">divisible_height</code></td>
<td>
<p>Make height divisible by 2, which may 
be required if getting &quot;height not divisible by 2&quot; error.</p>
</td></tr>
<tr><td><code id="ari_stitch_+3A_audio_codec">audio_codec</code></td>
<td>
<p>The audio encoder for the splicing.  If this
fails, try <code>copy</code>.</p>
</td></tr>
<tr><td><code id="ari_stitch_+3A_video_codec">video_codec</code></td>
<td>
<p>The video encoder for the splicing.  If this
fails, see <code>ffmpeg -codecs</code></p>
</td></tr>
<tr><td><code id="ari_stitch_+3A_video_sync_method">video_sync_method</code></td>
<td>
<p>Video sync method.  Should be 
&quot;auto&quot; or '&quot;vfr&quot;' or a numeric.  See <a href="https://ffmpeg.org/ffmpeg.html">https://ffmpeg.org/ffmpeg.html</a>.</p>
</td></tr>
<tr><td><code id="ari_stitch_+3A_audio_bitrate">audio_bitrate</code></td>
<td>
<p>Bit rate for audio. Passed to <code>-b:a</code>.</p>
</td></tr>
<tr><td><code id="ari_stitch_+3A_video_bitrate">video_bitrate</code></td>
<td>
<p>Bit rate for video. Passed to <code>-b:v</code>.</p>
</td></tr>
<tr><td><code id="ari_stitch_+3A_pixel_format">pixel_format</code></td>
<td>
<p>pixel format to encode for 'ffmpeg'.</p>
</td></tr>
<tr><td><code id="ari_stitch_+3A_fast_start">fast_start</code></td>
<td>
<p>Adding 'faststart' flags for YouTube and other sites,
see <a href="https://trac.ffmpeg.org/wiki/Encode/YouTube">https://trac.ffmpeg.org/wiki/Encode/YouTube</a></p>
</td></tr>
<tr><td><code id="ari_stitch_+3A_deinterlace">deinterlace</code></td>
<td>
<p>should the video be de-interlaced, 
see <a href="https://ffmpeg.org/ffmpeg-filters.html">https://ffmpeg.org/ffmpeg-filters.html</a>, generally for 
YouTube</p>
</td></tr>
<tr><td><code id="ari_stitch_+3A_stereo_audio">stereo_audio</code></td>
<td>
<p>should the audio be forced to stereo,
corresponds to '-ac 2'</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses <a href="https://ffmpeg.org/">FFmpeg</a>
which you should be sure is installed before using this function. If running
<code>Sys.which("ffmpeg")</code> in your R console returns an empty string after
installing FFmpeg then you should set the path to FFmpeg on you computer to
an environmental variable using <code>Sys.setenv(ffmpeg = "path/to/ffmpeg")</code>.
The environmental variable will always override the result of
<code>Sys.which("ffmpeg")</code>.
</p>


<h3>Value</h3>

<p>A logical value, with the attribute <code>outfile</code> for the
output file.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
if (ffmpeg_version_sufficient()) {
result = ari_stitch(
ari_example(c("mab1.png", "mab2.png")),
list(tuneR::noise(), tuneR::noise()))
} 

## End(Not run)
</code></pre>

<hr>
<h2 id='ari_talk'>Create spoken audio files</h2><span id='topic+ari_talk'></span>

<h3>Description</h3>

<p>A simple function for demoing how spoken text will sound.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ari_talk(
  paragraphs,
  output = tempfile(fileext = ".wav"),
  voice = text2speech::tts_default_voice(service = service),
  service = "amazon"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ari_talk_+3A_paragraphs">paragraphs</code></td>
<td>
<p>A vector strings that will be spoken by Amazon Polly.</p>
</td></tr>
<tr><td><code id="ari_talk_+3A_output">output</code></td>
<td>
<p>A path to the audio file which will be created.</p>
</td></tr>
<tr><td><code id="ari_talk_+3A_voice">voice</code></td>
<td>
<p>The  voice you want to use. See 
<code><a href="text2speech.html#topic+tts_voices">tts_voices</a></code> for more information 
about what voices are available.</p>
</td></tr>
<tr><td><code id="ari_talk_+3A_service">service</code></td>
<td>
<p>speech synthesis service to use,
passed to <code><a href="text2speech.html#topic+tts">tts</a></code>
Either <code>"amazon"</code> or <code>"google"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>Wave</code> output object, with the attribute <code>outfile</code>
of the output file name.
</p>

<hr>
<h2 id='ffmpeg_codecs'>Get Codecs for ffmpeg</h2><span id='topic+ffmpeg_codecs'></span><span id='topic+ffmpeg_video_codecs'></span><span id='topic+ffmpeg_audio_codecs'></span><span id='topic+ffmpeg_muxers'></span><span id='topic+ffmpeg_version'></span><span id='topic+ffmpeg_version_sufficient'></span><span id='topic+check_ffmpeg_version'></span>

<h3>Description</h3>

<p>Get Codecs for ffmpeg
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ffmpeg_codecs()

ffmpeg_video_codecs()

ffmpeg_audio_codecs()

ffmpeg_muxers()

ffmpeg_version()

ffmpeg_version_sufficient()

check_ffmpeg_version()
</code></pre>


<h3>Value</h3>

<p>A 'data.frame' of codec names and capabilities
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
if (ffmpeg_version_sufficient()) {
ffmpeg_codecs()
ffmpeg_video_codecs()
ffmpeg_audio_codecs()
}

## End(Not run)
</code></pre>

<hr>
<h2 id='ffmpeg_exec'>Get Path to ffmpeg Executable</h2><span id='topic+ffmpeg_exec'></span><span id='topic+have_ffmpeg_exec'></span>

<h3>Description</h3>

<p>Get Path to ffmpeg Executable
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ffmpeg_exec(quote = FALSE)

have_ffmpeg_exec()
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ffmpeg_exec_+3A_quote">quote</code></td>
<td>
<p>should <code><a href="base.html#topic+shQuote">shQuote</a></code> be run before returning?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The path to the <code>ffmpeg</code> executable, or an error.
</p>


<h3>Note</h3>

<p>This looks using 'Sys.getenv(&quot;ffmpeg&quot;)' and 'Sys.which(&quot;ffmpeg&quot;)'
to find 'ffmpeg'.  If 'ffmpeg' is not in your PATH, then please set the
path to 'ffmpeg' using 'Sys.setenv(ffmpeg = &quot;/path/to/ffmpeg&quot;)'
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
if (have_ffmpeg_exec()) { 
ffmpeg_exec()
}

## End(Not run)
</code></pre>

<hr>
<h2 id='set_audio_codec'>Set Default Audio and Video Codecs</h2><span id='topic+set_audio_codec'></span><span id='topic+set_video_codec'></span><span id='topic+get_audio_codec'></span><span id='topic+get_video_codec'></span><span id='topic+audio_codec_encode'></span><span id='topic+video_codec_encode'></span>

<h3>Description</h3>

<p>Set Default Audio and Video Codecs
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_audio_codec(codec)

set_video_codec(codec = "libx264")

get_audio_codec()

get_video_codec()

audio_codec_encode(codec)

video_codec_encode(codec)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="set_audio_codec_+3A_codec">codec</code></td>
<td>
<p>The codec to use or get for audio/video.  Uses the
'ffmpeg_audio_codec' and 'ffmpeg_video_codec' options
to store this information.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A 'NULL' output
</p>


<h3>See Also</h3>

<p>[ffmpeg_codecs()] for options
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
if (have_ffmpeg_exec()) {
print(ffmpeg_version())
get_audio_codec()
set_audio_codec(codec = "libfdk_aac")
get_audio_codec()
set_audio_codec(codec = "aac")
get_audio_codec()
}
if (have_ffmpeg_exec()) {
get_video_codec()
set_video_codec(codec = "libx265") 
get_video_codec()
set_video_codec(codec = "libx264")
get_video_codec()
}
## empty thing
if (have_ffmpeg_exec()) {
video_codec_encode("libx264")

audio_codec_encode("aac")
}

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
