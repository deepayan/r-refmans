<!DOCTYPE html><html><head><title>Help for package textreuse</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {textreuse}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#align_local'><p>Local alignment of natural language texts</p></a></li>
<li><a href='#as.matrix.textreuse_candidates'><p>Convert candidates data frames to other formats</p></a></li>
<li><a href='#filenames'><p>Filenames from paths</p></a></li>
<li><a href='#hash_string'><p>Hash a string to an integer</p></a></li>
<li><a href='#lsh'><p>Locality sensitive hashing for minhash</p></a></li>
<li><a href='#lsh_candidates'><p>Candidate pairs from LSH comparisons</p></a></li>
<li><a href='#lsh_compare'><p>Compare candidates identified by LSH</p></a></li>
<li><a href='#lsh_probability'><p>Probability that a candidate pair will be detected with LSH</p></a></li>
<li><a href='#lsh_query'><p>Query a LSH cache for matches to a single document</p></a></li>
<li><a href='#lsh_subset'><p>List of all candidates in a corpus</p></a></li>
<li><a href='#minhash_generator'><p>Generate a minhash function</p></a></li>
<li><a href='#pairwise_candidates'><p>Candidate pairs from pairwise comparisons</p></a></li>
<li><a href='#pairwise_compare'><p>Pairwise comparisons among documents in a corpus</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#rehash'><p>Recompute the hashes for a document or corpus</p></a></li>
<li><a href='#similarity-functions'><p>Measure similarity/dissimilarity in documents</p></a></li>
<li><a href='#textreuse-package'><p>textreuse: Detect Text Reuse and Document Similarity</p></a></li>
<li><a href='#TextReuseCorpus'><p>TextReuseCorpus</p></a></li>
<li><a href='#TextReuseTextDocument'><p>TextReuseTextDocument</p></a></li>
<li><a href='#TextReuseTextDocument-accessors'><p>Accessors for TextReuse objects</p></a></li>
<li><a href='#tokenize'><p>Recompute the tokens for a document or corpus</p></a></li>
<li><a href='#tokenizers'><p>Split texts into tokens</p></a></li>
<li><a href='#wordcount'><p>Count words</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Detect Text Reuse and Document Similarity</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.5</td>
</tr>
<tr>
<td>Date:</td>
<td>2020-05-14</td>
</tr>
<tr>
<td>Description:</td>
<td>Tools for measuring similarity among documents and detecting
    passages which have been reused. Implements shingled n-gram, skip n-gram,
    and other tokenizers; similarity/dissimilarity functions; pairwise
    comparisons; minhash and locality sensitive hashing algorithms; and a
    version of the Smith-Waterman local alignment algorithm suitable for
    natural language.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://docs.ropensci.org/textreuse">https://docs.ropensci.org/textreuse</a>,
<a href="https://github.com/ropensci/textreuse">https://github.com/ropensci/textreuse</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ropensci/textreuse/issues">https://github.com/ropensci/textreuse/issues</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.1)</td>
</tr>
<tr>
<td>Imports:</td>
<td>assertthat (&ge; 0.1), digest (&ge; 0.6.8), dplyr (&ge; 0.8.0), NLP
(&ge; 0.1.8), Rcpp (&ge; 0.12.0), RcppProgress (&ge; 0.1), stringr
(&ge; 1.0.0), tibble (&ge; 3.0.1), tidyr (&ge; 0.3.1)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 0.11.0), knitr (&ge; 1.11), rmarkdown (&ge; 0.8),
covr</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>BH, Rcpp, RcppProgress</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.0</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-05-15 14:43:54 UTC; lmullen</td>
</tr>
<tr>
<td>Author:</td>
<td>Lincoln Mullen <a href="https://orcid.org/0000-0001-5103-6917"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Lincoln Mullen &lt;lincoln@lincolnmullen.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-05-15 15:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='align_local'>Local alignment of natural language texts</h2><span id='topic+align_local'></span>

<h3>Description</h3>

<p>This function takes two texts, either as strings or as
<code>TextReuseTextDocument</code> objects, and finds the optimal local alignment
of those texts. A local alignment finds the best matching subset of the two
documents. This function adapts the
<a href="https://en.wikipedia.org/wiki/Smith-Waterman_algorithm">Smith-Waterman
algorithm</a>, used for genetic sequencing, for use with natural language. It
compare the texts word by word (the comparison is case-insensitive) and
scores them according to a set of parameters. These parameters define the
score for a <code>match</code>, and the penalties for a <code>mismatch</code> and for
opening a <code>gap</code> (i.e., the first mismatch in a potential sequence). The
function then reports the optimal local alignment. Only the subset of the
documents that is a match is included. Insertions or deletions in the text
are reported with the <code>edit_mark</code> character.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>align_local(
  a,
  b,
  match = 2L,
  mismatch = -1L,
  gap = -1L,
  edit_mark = "#",
  progress = interactive()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="align_local_+3A_a">a</code></td>
<td>
<p>A character vector of length one, or a
<code><a href="#topic+TextReuseTextDocument">TextReuseTextDocument</a></code>.</p>
</td></tr>
<tr><td><code id="align_local_+3A_b">b</code></td>
<td>
<p>A character vector of length one, or a
<code><a href="#topic+TextReuseTextDocument">TextReuseTextDocument</a></code>.</p>
</td></tr>
<tr><td><code id="align_local_+3A_match">match</code></td>
<td>
<p>The score to assign a matching word. Should be a positive
integer.</p>
</td></tr>
<tr><td><code id="align_local_+3A_mismatch">mismatch</code></td>
<td>
<p>The score to assign a mismatching word. Should be a negative
integer or zero.</p>
</td></tr>
<tr><td><code id="align_local_+3A_gap">gap</code></td>
<td>
<p>The penalty for opening a gap in the sequence. Should be a
negative integer or zero.</p>
</td></tr>
<tr><td><code id="align_local_+3A_edit_mark">edit_mark</code></td>
<td>
<p>A single character used for displaying for displaying
insertions/deletions in the documents.</p>
</td></tr>
<tr><td><code id="align_local_+3A_progress">progress</code></td>
<td>
<p>Display a progress bar and messages while computing the
alignment.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The compute time of this function is proportional to the product of the
lengths of the two documents. Thus, longer documents will take considerably
more time to compute. This function has been tested with pairs of documents
containing about 25 thousand words each.
</p>
<p>If the function reports that there were multiple optimal alignments, then it
is likely that there is no strong match in the document.
</p>
<p>The score reported for the local alignment is dependent on both the size of
the documents and on the strength of the match, as well as on the parameters
for match, mismatch, and gap penalties, so the scores are not directly
comparable.
</p>


<h3>Value</h3>

<p>A list with the class <code>textreuse_alignment</code>. This list contains
several elements: </p>
 <ul>
<li> <p><code>a_edit</code> and <code>b_edit</code>:
Character vectors of the sequences with edits marked. </p>
</li>
<li> <p><code>score</code>:
The score of the optimal alignment. </p>
</li></ul>



<h3>References</h3>

<p>For a useful description of the algorithm, see
<a href="http://etherealbits.com/2013/04/string-alignment-dynamic-programming-dna/">this
post</a>. For the application of the Smith-Waterman algorithm to natural
language, see David A. Smith, Ryan Cordell, and Elizabeth Maddock Dillon,
&quot;Infectious Texts: Modeling Text Reuse in Nineteenth-Century Newspapers.&quot;
IEEE International Conference on Big Data, 2013,
<a href="http://hdl.handle.net/2047/d20004858">http://hdl.handle.net/2047/d20004858</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>align_local("The answer is blowin' in the wind.",
            "As the Bob Dylan song says, the answer is blowing in the wind.")

# Example of matching documents from a corpus
dir &lt;- system.file("extdata/legal", package = "textreuse")
corpus &lt;- TextReuseCorpus(dir = dir, progress = FALSE)
alignment &lt;- align_local(corpus[["ca1851-match"]], corpus[["ny1850-match"]])
str(alignment)

</code></pre>

<hr>
<h2 id='as.matrix.textreuse_candidates'>Convert candidates data frames to other formats</h2><span id='topic+as.matrix.textreuse_candidates'></span>

<h3>Description</h3>

<p>These S3 methods convert a <code>textreuse_candidates</code> object to a matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'textreuse_candidates'
as.matrix(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.matrix.textreuse_candidates_+3A_x">x</code></td>
<td>
<p>An object of class <code><a href="#topic+lsh_compare">textreuse_candidates</a></code>.</p>
</td></tr>
<tr><td><code id="as.matrix.textreuse_candidates_+3A_...">...</code></td>
<td>
<p>Additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A similarity matrix with row and column names containing document IDs.
</p>

<hr>
<h2 id='filenames'>Filenames from paths</h2><span id='topic+filenames'></span>

<h3>Description</h3>

<p>This function takes a character vector of paths and returns just the file
name, by default without the extension. A <code><a href="#topic+TextReuseCorpus">TextReuseCorpus</a></code> uses
the paths to the files in the corpus as the names of the list. This function
is intended to turn those paths into more manageable identifiers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>filenames(paths, extension = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="filenames_+3A_paths">paths</code></td>
<td>
<p>A character vector of paths.</p>
</td></tr>
<tr><td><code id="filenames_+3A_extension">extension</code></td>
<td>
<p>Should the file extension be preserved?</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="base.html#topic+basename">basename</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>paths &lt;- c("corpus/one.txt", "corpus/two.md", "corpus/three.text")
filenames(paths)
filenames(paths, extension = TRUE)
</code></pre>

<hr>
<h2 id='hash_string'>Hash a string to an integer</h2><span id='topic+hash_string'></span>

<h3>Description</h3>

<p>Hash a string to an integer
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hash_string(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hash_string_+3A_x">x</code></td>
<td>
<p>A character vector to be hashed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of integer hashes.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>s &lt;- c("How", "many", "roads", "must", "a", "man", "walk", "down")
hash_string(s)
</code></pre>

<hr>
<h2 id='lsh'>Locality sensitive hashing for minhash</h2><span id='topic+lsh'></span>

<h3>Description</h3>

<p>Locality sensitive hashing (LSH) discovers potential matches among a corpus of
documents quickly, so that only likely pairs can be compared.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lsh(x, bands, progress = interactive())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lsh_+3A_x">x</code></td>
<td>
<p>A <code><a href="#topic+TextReuseCorpus">TextReuseCorpus</a></code> or
<code><a href="#topic+TextReuseTextDocument">TextReuseTextDocument</a></code>.</p>
</td></tr>
<tr><td><code id="lsh_+3A_bands">bands</code></td>
<td>
<p>The number of bands to use for locality sensitive hashing. The
number of hashes in the documents in the corpus must be evenly divisible by
the number of bands. See <code><a href="#topic+lsh_threshold">lsh_threshold</a></code> and
<code><a href="#topic+lsh_probability">lsh_probability</a></code> for guidance in selecting the number of bands
and hashes.</p>
</td></tr>
<tr><td><code id="lsh_+3A_progress">progress</code></td>
<td>
<p>Display a progress bar while comparing documents.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Locality sensitive hashing is a technique for detecting document
similarity that does not require pairwise comparisons. When comparing pairs
of documents, the number of pairs grows rapidly, so that only the smallest
corpora can be compared pairwise in a reasonable amount of computation time.
Locality sensitive hashing, on the other hand, takes a document which has
been tokenized and hashed using a minhash algorithm. (See
<code><a href="#topic+minhash_generator">minhash_generator</a></code>.) Each set of minhash signatures is then
broken into bands comprised of a certain number of rows. (For example, 200
minhash signatures might be broken down into 20 bands each containing 10
rows.) Each band is then hashed to a bucket. Documents with identical rows
in a band will be hashed to the same bucket. The likelihood that a document
will be marked as a potential duplicate is proportional to the number of
bands and inversely proportional to the number of rows in each band.
</p>
<p>This function returns a data frame with the additional class
<code>lsh_buckets</code>. The LSH technique only requires that the signatures for
each document be calculated once. So it is possible, as long as one uses the
same minhash function and the same number of bands, to combine the outputs
from this function at different times. The output can thus be treated as a
kind of cache of LSH signatures.
</p>
<p>To extract pairs of documents from the output of this function, see
<code><a href="#topic+lsh_candidates">lsh_candidates</a></code>.
</p>


<h3>Value</h3>

<p>A data frame (with the additional class <code>lsh_buckets</code>),
containing a column with the document IDs and a column with their LSH
signatures, or buckets.
</p>


<h3>References</h3>

<p>Jure Leskovec, Anand Rajaraman, and Jeff Ullman,
<a href="http://www.mmds.org/#book"><em>Mining of Massive Datasets</em></a>
(Cambridge University Press, 2011), ch. 3. See also Matthew Casperson,
&quot;<a href="http://matthewcasperson.blogspot.com/2013/11/minhash-for-dummies.html">Minhash
for Dummies</a>&quot; (November 14, 2013).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+minhash_generator">minhash_generator</a></code>, <code><a href="#topic+lsh_candidates">lsh_candidates</a></code>,
<code><a href="#topic+lsh_query">lsh_query</a></code>, <code><a href="#topic+lsh_probability">lsh_probability</a></code>,
<code><a href="#topic+lsh_threshold">lsh_threshold</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dir &lt;- system.file("extdata/legal", package = "textreuse")
minhash &lt;- minhash_generator(200, seed = 235)
corpus &lt;- TextReuseCorpus(dir = dir,
                          tokenizer = tokenize_ngrams, n = 5,
                          minhash_func = minhash)
buckets &lt;- lsh(corpus, bands = 50)
buckets
</code></pre>

<hr>
<h2 id='lsh_candidates'>Candidate pairs from LSH comparisons</h2><span id='topic+lsh_candidates'></span>

<h3>Description</h3>

<p>Given a data frame of LSH buckets returned from <code><a href="#topic+lsh">lsh</a></code>, this
function returns the potential candidates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lsh_candidates(buckets)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lsh_candidates_+3A_buckets">buckets</code></td>
<td>
<p>A data frame returned from <code><a href="#topic+lsh">lsh</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame of candidate pairs.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dir &lt;- system.file("extdata/legal", package = "textreuse")
minhash &lt;- minhash_generator(200, seed = 234)
corpus &lt;- TextReuseCorpus(dir = dir,
                          tokenizer = tokenize_ngrams, n = 5,
                          minhash_func = minhash)
buckets &lt;- lsh(corpus, bands = 50)
lsh_candidates(buckets)

</code></pre>

<hr>
<h2 id='lsh_compare'>Compare candidates identified by LSH</h2><span id='topic+lsh_compare'></span>

<h3>Description</h3>

<p>The <code><a href="#topic+lsh_candidates">lsh_candidates</a></code> only identifies potential matches, but
cannot estimate the actual similarity of the documents. This function takes a
data frame returned by <code><a href="#topic+lsh_candidates">lsh_candidates</a></code> and applies a comparison
function to each of the documents in a corpus, thereby calculating the
document similarity score. Note that since your corpus will have minhash
signatures rather than hashes for the tokens itself, you will probably wish
to use <code><a href="#topic+tokenize">tokenize</a></code> to calculate new hashes. This can be done for
just the potentially similar documents. See the package vignettes for
details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lsh_compare(candidates, corpus, f, progress = interactive())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lsh_compare_+3A_candidates">candidates</code></td>
<td>
<p>A data frame returned by <code><a href="#topic+lsh_candidates">lsh_candidates</a></code>.</p>
</td></tr>
<tr><td><code id="lsh_compare_+3A_corpus">corpus</code></td>
<td>
<p>The same <code><a href="#topic+TextReuseCorpus">TextReuseCorpus</a></code> corpus which was used to generate the candidates.</p>
</td></tr>
<tr><td><code id="lsh_compare_+3A_f">f</code></td>
<td>
<p>A comparison function such as <code><a href="#topic+jaccard_similarity">jaccard_similarity</a></code>.</p>
</td></tr>
<tr><td><code id="lsh_compare_+3A_progress">progress</code></td>
<td>
<p>Display a progress bar while comparing documents.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with values calculated for <code>score</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dir &lt;- system.file("extdata/legal", package = "textreuse")
minhash &lt;- minhash_generator(200, seed = 234)
corpus &lt;- TextReuseCorpus(dir = dir,
                          tokenizer = tokenize_ngrams, n = 5,
                          minhash_func = minhash)
buckets &lt;- lsh(corpus, bands = 50)
candidates &lt;- lsh_candidates(buckets)
lsh_compare(candidates, corpus, jaccard_similarity)
</code></pre>

<hr>
<h2 id='lsh_probability'>Probability that a candidate pair will be detected with LSH</h2><span id='topic+lsh_probability'></span><span id='topic+lsh_threshold'></span>

<h3>Description</h3>

<p>Functions to help choose the correct parameters for the <code><a href="#topic+lsh">lsh</a></code> and
<code><a href="#topic+minhash_generator">minhash_generator</a></code> functions. Use <code>lsh_threshold</code> to
determine the minimum Jaccard similarity for two documents for them to likely
be considered a match. Use <code>lsh_probability</code> to determine the
probability that a pair of documents with a known Jaccard similarity will be
detected.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lsh_probability(h, b, s)

lsh_threshold(h, b)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lsh_probability_+3A_h">h</code></td>
<td>
<p>The number of minhash signatures.</p>
</td></tr>
<tr><td><code id="lsh_probability_+3A_b">b</code></td>
<td>
<p>The number of LSH bands.</p>
</td></tr>
<tr><td><code id="lsh_probability_+3A_s">s</code></td>
<td>
<p>The Jaccard similarity.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Locality sensitive hashing returns a list of possible matches for
similar documents. How likely is it that a pair of documents will be detected
as a possible match? If <code>h</code> is the number of minhash signatures,
<code>b</code> is the number of bands in the LSH function (implying then that the
number of rows <code>r = h / b</code>), and <code>s</code> is the actual Jaccard
similarity of the two documents, then the probability <code>p</code> that the two
documents will be marked as a candidate pair is given by this equation.
</p>
<p style="text-align: center;"><code class="reqn">p = 1 - (1 - s^{r})^{b}</code>
</p>

<p>According to <a href="http://infolab.stanford.edu/~ullman/mmds/book.pdf">MMDS</a>,
that equation approximates an S-curve. This implies that there is a threshold
(<code>t</code>) for <code>s</code> approximated by this equation.
</p>
<p style="text-align: center;"><code class="reqn">t = \frac{1}{b}^{\frac{1}{r}}</code>
</p>



<h3>References</h3>

<p>Jure Leskovec, Anand Rajaraman, and Jeff Ullman,
<a href="http://www.mmds.org/#book"><em>Mining of Massive Datasets</em></a>
(Cambridge University Press, 2011), ch. 3.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Threshold for default values
lsh_threshold(h = 200, b = 40)

# Probability for varying values of s
lsh_probability(h = 200, b = 40, s = .25)
lsh_probability(h = 200, b = 40, s = .50)
lsh_probability(h = 200, b = 40, s = .75)
</code></pre>

<hr>
<h2 id='lsh_query'>Query a LSH cache for matches to a single document</h2><span id='topic+lsh_query'></span>

<h3>Description</h3>

<p>This function retrieves the matches for a single document from an <code>lsh_buckets</code> object created by <code><a href="#topic+lsh">lsh</a></code>. See <code><a href="#topic+lsh_candidates">lsh_candidates</a></code> to retrieve all pairs of matches.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lsh_query(buckets, id)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lsh_query_+3A_buckets">buckets</code></td>
<td>
<p>An <code>lsh_buckets</code> object created by <code><a href="#topic+lsh">lsh</a></code>.</p>
</td></tr>
<tr><td><code id="lsh_query_+3A_id">id</code></td>
<td>
<p>The document ID to find matches for.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code>lsh_candidates</code> data frame with matches to the document specified.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lsh">lsh</a></code>, <code><a href="#topic+lsh_candidates">lsh_candidates</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dir &lt;- system.file("extdata/legal", package = "textreuse")
minhash &lt;- minhash_generator(200, seed = 235)
corpus &lt;- TextReuseCorpus(dir = dir,
                          tokenizer = tokenize_ngrams, n = 5,
                          minhash_func = minhash)
buckets &lt;- lsh(corpus, bands = 50)
lsh_query(buckets, "ny1850-match")

</code></pre>

<hr>
<h2 id='lsh_subset'>List of all candidates in a corpus</h2><span id='topic+lsh_subset'></span>

<h3>Description</h3>

<p>List of all candidates in a corpus
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lsh_subset(candidates)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lsh_subset_+3A_candidates">candidates</code></td>
<td>
<p>A data frame of candidate pairs from
<code><a href="#topic+lsh_candidates">lsh_candidates</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector of document IDs from the candidate pairs, to be
used to subset the <code><a href="#topic+TextReuseCorpus">TextReuseCorpus</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dir &lt;- system.file("extdata/legal", package = "textreuse")
minhash &lt;- minhash_generator(200, seed = 234)
corpus &lt;- TextReuseCorpus(dir = dir,
                          tokenizer = tokenize_ngrams, n = 5,
                          minhash_func = minhash)
buckets &lt;- lsh(corpus, bands = 50)
candidates &lt;- lsh_candidates(buckets)
lsh_subset(candidates)
corpus[lsh_subset(candidates)]
</code></pre>

<hr>
<h2 id='minhash_generator'>Generate a minhash function</h2><span id='topic+minhash_generator'></span>

<h3>Description</h3>

<p>A minhash value is calculated by hashing the strings in a character vector to
integers and then selecting the minimum value. Repeated minhash values are
generated by using different hash functions: these different hash functions
are created by using performing a bitwise <code>XOR</code> operation
(<code><a href="base.html#topic+bitwXor">bitwXor</a></code>) with a vector of random integers. Since it is vital
that the same random integers be used for each document, this function
generates another function which will always use the same integers. The
returned function is intended to be passed to the <code>hash_func</code> parameter
of <code><a href="#topic+TextReuseTextDocument">TextReuseTextDocument</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>minhash_generator(n = 200, seed = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="minhash_generator_+3A_n">n</code></td>
<td>
<p>The number of minhashes that the returned function should generate.</p>
</td></tr>
<tr><td><code id="minhash_generator_+3A_seed">seed</code></td>
<td>
<p>An option parameter to set the seed used in generating the random
numbers to ensure that the same minhash function is used on repeated
applications.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A function which will take a character vector and return <code>n</code>
minhashes.
</p>


<h3>References</h3>

<p>Jure Leskovec, Anand Rajaraman, and Jeff Ullman,
<a href="http://www.mmds.org/#book"><em>Mining of Massive Datasets</em></a>
(Cambridge University Press, 2011), ch. 3. See also Matthew Casperson,
&quot;<a href="http://matthewcasperson.blogspot.com/2013/11/minhash-for-dummies.html">Minhash
for Dummies</a>&quot; (November 14, 2013).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lsh">lsh</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(253)
minhash &lt;- minhash_generator(10)

# Example with a TextReuseTextDocument
file &lt;- system.file("extdata/legal/ny1850-match.txt", package = "textreuse")
doc &lt;- TextReuseTextDocument(file = file, hash_func = minhash,
                             keep_tokens = TRUE)
hashes(doc)

# Example with a character vector
is.character(tokens(doc))
minhash(tokens(doc))
</code></pre>

<hr>
<h2 id='pairwise_candidates'>Candidate pairs from pairwise comparisons</h2><span id='topic+pairwise_candidates'></span>

<h3>Description</h3>

<p>Converts a comparison matrix generated by <code><a href="#topic+pairwise_compare">pairwise_compare</a></code> into a
data frame of candidates for matches.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pairwise_candidates(m, directional = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pairwise_candidates_+3A_m">m</code></td>
<td>
<p>A matrix from <code><a href="#topic+pairwise_compare">pairwise_compare</a></code>.</p>
</td></tr>
<tr><td><code id="pairwise_candidates_+3A_directional">directional</code></td>
<td>
<p>Should be set to the same value as in
<code><a href="#topic+pairwise_compare">pairwise_compare</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame containing all the non-<code>NA</code> values from <code>m</code>.
Columns <code>a</code> and <code>b</code> are the IDs from the original corpus as
passed to the comparison function. Column <code>score</code> is the score
returned by the comparison function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dir &lt;- system.file("extdata/legal", package = "textreuse")
corpus &lt;- TextReuseCorpus(dir = dir)

m1 &lt;- pairwise_compare(corpus, ratio_of_matches, directional = TRUE)
pairwise_candidates(m1, directional = TRUE)

m2 &lt;- pairwise_compare(corpus, jaccard_similarity)
pairwise_candidates(m2)
</code></pre>

<hr>
<h2 id='pairwise_compare'>Pairwise comparisons among documents in a corpus</h2><span id='topic+pairwise_compare'></span>

<h3>Description</h3>

<p>Given a <code><a href="#topic+TextReuseCorpus">TextReuseCorpus</a></code> containing documents of class
<code><a href="#topic+TextReuseTextDocument">TextReuseTextDocument</a></code>, this function applies a comparison
function to every pairing of documents, and returns a matrix with the
comparison scores.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pairwise_compare(corpus, f, ..., directional = FALSE, progress = interactive())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pairwise_compare_+3A_corpus">corpus</code></td>
<td>
<p>A <code><a href="#topic+TextReuseCorpus">TextReuseCorpus</a></code>.</p>
</td></tr>
<tr><td><code id="pairwise_compare_+3A_f">f</code></td>
<td>
<p>The function to apply to <code>x</code> and <code>y</code>.</p>
</td></tr>
<tr><td><code id="pairwise_compare_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>f</code>.</p>
</td></tr>
<tr><td><code id="pairwise_compare_+3A_directional">directional</code></td>
<td>
<p>Some comparison functions are commutative, so that
<code>f(a, b) == f(b, a)</code> (e.g., <code><a href="#topic+jaccard_similarity">jaccard_similarity</a></code>). Other
functions are directional, so that <code>f(a, b)</code> measures <code>a</code>'s
borrowing from <code>b</code>, which may not be the same as <code>f(b, a)</code> (e.g.,
<code><a href="#topic+ratio_of_matches">ratio_of_matches</a></code>). If <code>directional</code> is <code>FALSE</code>,
then only the minimum number of comparisons will be made, i.e., the upper
triangle of the matrix. If <code>directional</code> is <code>TRUE</code>, then both
directional comparisons will be measured. In no case, however, will
documents be compared to themselves, i.e., the diagonal of the matrix.</p>
</td></tr>
<tr><td><code id="pairwise_compare_+3A_progress">progress</code></td>
<td>
<p>Display a progress bar while comparing documents.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A square matrix with dimensions equal to the length of the corpus,
and row and column names set by the names of the documents in the corpus. A
value of <code>NA</code> in the matrix indicates that a comparison was not made.
In cases of directional comparisons, then the comparison reported is
<code>f(row, column)</code>.
</p>


<h3>See Also</h3>

<p>See these document comparison functions,
<code><a href="#topic+jaccard_similarity">jaccard_similarity</a></code>, <code><a href="#topic+ratio_of_matches">ratio_of_matches</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dir &lt;- system.file("extdata/legal", package = "textreuse")
corpus &lt;- TextReuseCorpus(dir = dir)
names(corpus) &lt;- filenames(names(corpus))

# A non-directional comparison
pairwise_compare(corpus, jaccard_similarity)

# A directional comparison
pairwise_compare(corpus, ratio_of_matches, directional = TRUE)
</code></pre>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+meta'></span><span id='topic+meta+3C-'></span><span id='topic+content'></span><span id='topic+content+3C-'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>NLP</dt><dd><p><code><a href="NLP.html#topic+content">content</a></code>, <code><a href="NLP.html#topic+content+3C-">content&lt;-</a></code>, <code><a href="NLP.html#topic+meta">meta</a></code>, <code><a href="NLP.html#topic+meta+3C-">meta&lt;-</a></code></p>
</dd>
</dl>

<hr>
<h2 id='rehash'>Recompute the hashes for a document or corpus</h2><span id='topic+rehash'></span>

<h3>Description</h3>

<p>Given a <code><a href="#topic+TextReuseTextDocument">TextReuseTextDocument</a></code> or a
<code><a href="#topic+TextReuseCorpus">TextReuseCorpus</a></code>, this function recomputes either the hashes or
the minhashes with the function specified. This implies that you have
retained the tokens with the <code>keep_tokens = TRUE</code> parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rehash(x, func, type = c("hashes", "minhashes"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rehash_+3A_x">x</code></td>
<td>
<p>A <code><a href="#topic+TextReuseTextDocument">TextReuseTextDocument</a></code> or
<code><a href="#topic+TextReuseCorpus">TextReuseCorpus</a></code>.</p>
</td></tr>
<tr><td><code id="rehash_+3A_func">func</code></td>
<td>
<p>A function to either hash the tokens or to generate the minhash
signature. See <code><a href="#topic+hash_string">hash_string</a></code>, <code><a href="#topic+minhash_generator">minhash_generator</a></code>.</p>
</td></tr>
<tr><td><code id="rehash_+3A_type">type</code></td>
<td>
<p>Recompute the <code>hashes</code> or <code>minhashes</code>?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The modified <code><a href="#topic+TextReuseTextDocument">TextReuseTextDocument</a></code> or
<code><a href="#topic+TextReuseCorpus">TextReuseCorpus</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dir &lt;- system.file("extdata/legal", package = "textreuse")
minhash1 &lt;- minhash_generator(seed = 1)
corpus &lt;- TextReuseCorpus(dir = dir, minhash_func = minhash1, keep_tokens = TRUE)
head(minhashes(corpus[[1]]))
minhash2 &lt;- minhash_generator(seed = 2)
corpus &lt;- rehash(corpus, minhash2, type = "minhashes")
head(minhashes(corpus[[2]]))

</code></pre>

<hr>
<h2 id='similarity-functions'>Measure similarity/dissimilarity in documents</h2><span id='topic+similarity-functions'></span><span id='topic+jaccard_similarity'></span><span id='topic+jaccard_dissimilarity'></span><span id='topic+jaccard_bag_similarity'></span><span id='topic+ratio_of_matches'></span>

<h3>Description</h3>

<p>A set of functions which take two sets or bag of words and measure their
similarity or dissimilarity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jaccard_similarity(a, b)

jaccard_dissimilarity(a, b)

jaccard_bag_similarity(a, b)

ratio_of_matches(a, b)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="similarity-functions_+3A_a">a</code></td>
<td>
<p>The first set (or bag) to be compared. The origin bag for
directional comparisons.</p>
</td></tr>
<tr><td><code id="similarity-functions_+3A_b">b</code></td>
<td>
<p>The second set (or bag) to be compared. The destination bag for
directional comparisons.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functions <code>jaccard_similarity</code> and
<code>jaccard_dissimilarity</code> provide the Jaccard measures of similarity or
dissimilarity for two sets. The coefficients will be numbers between
<code>0</code> and <code>1</code>. For the similarity coefficient, the higher the
number the more similar the two sets are. When applied to two documents of
class <code><a href="#topic+TextReuseTextDocument">TextReuseTextDocument</a></code>, the hashes in those documents
are compared. But this function can be passed objects of any class accepted
by the set functions in base R. So it is possible, for instance, to pass
this function two character vectors comprised of word, line, sentence, or
paragraph tokens, or those character vectors hashed as integers.
</p>
<p>The Jaccard similarity coeffecient is defined as follows:
</p>
<p style="text-align: center;"><code class="reqn">J(A, B) = \frac{ | A \cap B | }{ | A \cup B | }</code>
</p>

<p>The Jaccard dissimilarity is simply
</p>
<p style="text-align: center;"><code class="reqn">1 - J(A, B)</code>
</p>

<p>The function <code>jaccard_bag_similarity</code> treats <code>a</code> and <code>b</code> as
bags rather than sets, so that the result is a fraction where the numerator
is the sum of each matching element counted the minimum number of times it
appears in each bag, and the denominator is the sum of the lengths of both
bags. The maximum value for the Jaccard bag similarity is <code>0.5</code>.
</p>
<p>The function <code>ratio_of_matches</code> finds the ratio between the number of
items in <code>b</code> that are also in <code>a</code> and the total number of items
in <code>b</code>. Note that this similarity measure is directional: it measures
how much <code>b</code> borrows from <code>a</code>, but says nothing about how much of
<code>a</code> borrows from <code>b</code>.
</p>


<h3>References</h3>

<p>Jure Leskovec, Anand Rajaraman, and Jeff Ullman,
<a href="http://www.mmds.org/#book"><em>Mining of Massive Datasets</em></a>
(Cambridge University Press, 2011).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>jaccard_similarity(1:6, 3:10)
jaccard_dissimilarity(1:6, 3:10)

a &lt;- c("a", "a", "a", "b")
b &lt;- c("a", "a", "b", "b", "c")
jaccard_similarity(a, b)
jaccard_bag_similarity(a, b)
ratio_of_matches(a, b)
ratio_of_matches(b, a)

ny         &lt;- system.file("extdata/legal/ny1850-match.txt", package = "textreuse")
ca_match   &lt;- system.file("extdata/legal/ca1851-match.txt", package = "textreuse")
ca_nomatch &lt;- system.file("extdata/legal/ca1851-nomatch.txt", package = "textreuse")

ny         &lt;- TextReuseTextDocument(file = ny,
                                    meta = list(id = "ny"))
ca_match   &lt;- TextReuseTextDocument(file = ca_match,
                                    meta = list(id = "ca_match"))
ca_nomatch &lt;- TextReuseTextDocument(file = ca_nomatch,
                                    meta = list(id = "ca_nomatch"))

# These two should have higher similarity scores
jaccard_similarity(ny, ca_match)
ratio_of_matches(ny, ca_match)

# These two should have lower similarity scores
jaccard_similarity(ny, ca_nomatch)
ratio_of_matches(ny, ca_nomatch)

</code></pre>

<hr>
<h2 id='textreuse-package'>textreuse: Detect Text Reuse and Document Similarity</h2><span id='topic+textreuse'></span><span id='topic+textreuse-package'></span>

<h3>Description</h3>

<p>Tools for measuring similarity among documents and detecting
passages which have been reused. Implements shingled n-gram, skip n-gram,
and other tokenizers; similarity/dissimilarity functions; pairwise
comparisons; minhash and locality sensitive hashing algorithms; and a
version of the Smith-Waterman local alignment algorithm suitable for
natural language.
</p>


<h3>Details</h3>

<p>The best place to begin with this package in the introductory vignette.
</p>
<p><code>vignette("textreuse-introduction", package = "textreuse")</code>
</p>
<p>After reading that vignette, the &quot;pairwise&quot; and &quot;minhash&quot; vignettes introduce
specific paths for working with the package.
</p>
<p><code>vignette("textreuse-pairwise", package = "textreuse")</code>
</p>
<p><code>vignette("textreuse-minhash", package = "textreuse")</code>
</p>
<p><code>vignette("textreuse-alignment", package = "textreuse")</code>
</p>
<p>Another good place to begin with the package is the documentation for loading
documents (<code><a href="#topic+TextReuseTextDocument">TextReuseTextDocument</a></code> and
<code><a href="#topic+TextReuseCorpus">TextReuseCorpus</a></code>), for <a href="#topic+tokenizers">tokenizers</a>,
<a href="#topic+similarity-functions">similarity functions</a>, and
<a href="#topic+lsh">locality-sensitive hashing</a>.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Lincoln Mullen <a href="mailto:lincoln@lincolnmullen.com">lincoln@lincolnmullen.com</a> (<a href="https://orcid.org/0000-0001-5103-6917">ORCID</a>)
</p>


<h3>References</h3>

<p>The sample data provided in the <code>extdata/legal</code> directory is
taken from a
<a href="http://lincolnmullen.com/blog/corpus-of-american-tract-society-publications/">corpus
of American Tract Society publications</a> from the nineteen-century,
gathered from the <a href="https://archive.org/">Internet Archive</a>.
</p>
<p>The sample data provided in the <code>extdata/legal</code> directory, are taken
from the following nineteenth-century codes of civil procedure from
California and New York.
</p>
<p><em>Final Report of the Commissioners on Practice and Pleadings</em>, in 2
<em>Documents of the Assembly of New York</em>, 73rd Sess., No. 16, (1850):
243-250, sections 597-613.
<a href="http://books.google.com/books?id=9HEbAQAAIAAJ&amp;pg=PA243#v=onepage&amp;q&amp;f=false">Google
Books</a>.
</p>
<p><em>An Act To Regulate Proceedings in Civil Cases</em>, 1851 <em>California
Laws</em> 51, 51-53 sections 4-17; 101, sections 313-316.
<a href="http://books.google.com/books?id=4PHEAAAAIAAJ&amp;pg=PA51#v=onepage&amp;q&amp;f=false">Google
Books</a>.
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://docs.ropensci.org/textreuse">https://docs.ropensci.org/textreuse</a>
</p>
</li>
<li> <p><a href="https://github.com/ropensci/textreuse">https://github.com/ropensci/textreuse</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/ropensci/textreuse/issues">https://github.com/ropensci/textreuse/issues</a>
</p>
</li></ul>


<hr>
<h2 id='TextReuseCorpus'>TextReuseCorpus</h2><span id='topic+TextReuseCorpus'></span><span id='topic+is.TextReuseCorpus'></span><span id='topic+skipped'></span>

<h3>Description</h3>

<p>This is the constructor function for a <code>TextReuseCorpus</code>, modeled on the
virtual S3 class <code><a href="tm.html#topic+Corpus">Corpus</a></code> from the <code>tm</code> package. The
object is a <code>TextReuseCorpus</code>, which is basically a list containing
objects of class <code><a href="#topic+TextReuseTextDocument">TextReuseTextDocument</a></code>. Arguments are passed
along to that constructor function. To create the corpus, you can pass either
a character vector of paths to text files using the <code>paths =</code> parameter,
a directory containing text files (with any extension) using the <code>dir =</code>
parameter, or a character vector of documents using the <code>text = </code>
parameter, where each element in the characer vector is a document. If the
character vector passed to <code>text = </code> has names, then those names will be
used as the document IDs. Otherwise, IDs will be assigned to the documents.
Only one of the <code>paths</code>, <code>dir</code>, or <code>text</code> parameters should be
specified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TextReuseCorpus(
  paths,
  dir = NULL,
  text = NULL,
  meta = list(),
  progress = interactive(),
  tokenizer = tokenize_ngrams,
  ...,
  hash_func = hash_string,
  minhash_func = NULL,
  keep_tokens = FALSE,
  keep_text = TRUE,
  skip_short = TRUE
)

is.TextReuseCorpus(x)

skipped(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TextReuseCorpus_+3A_paths">paths</code></td>
<td>
<p>A character vector of paths to files to be opened.</p>
</td></tr>
<tr><td><code id="TextReuseCorpus_+3A_dir">dir</code></td>
<td>
<p>The path to a directory of text files.</p>
</td></tr>
<tr><td><code id="TextReuseCorpus_+3A_text">text</code></td>
<td>
<p>A character vector (possibly named) of documents.</p>
</td></tr>
<tr><td><code id="TextReuseCorpus_+3A_meta">meta</code></td>
<td>
<p>A list with named elements for the metadata associated with this
corpus.</p>
</td></tr>
<tr><td><code id="TextReuseCorpus_+3A_progress">progress</code></td>
<td>
<p>Display a progress bar while loading files.</p>
</td></tr>
<tr><td><code id="TextReuseCorpus_+3A_tokenizer">tokenizer</code></td>
<td>
<p>A function to split the text into tokens. See
<code><a href="#topic+tokenizers">tokenizers</a></code>. If value is <code>NULL</code>, then tokenizing and
hashing will be skipped.</p>
</td></tr>
<tr><td><code id="TextReuseCorpus_+3A_...">...</code></td>
<td>
<p>Arguments passed on to the <code>tokenizer</code>.</p>
</td></tr>
<tr><td><code id="TextReuseCorpus_+3A_hash_func">hash_func</code></td>
<td>
<p>A function to hash the tokens. See
<code><a href="#topic+hash_string">hash_string</a></code>.</p>
</td></tr>
<tr><td><code id="TextReuseCorpus_+3A_minhash_func">minhash_func</code></td>
<td>
<p>A function to create minhash signatures of the document.
See <code><a href="#topic+minhash_generator">minhash_generator</a></code>.</p>
</td></tr>
<tr><td><code id="TextReuseCorpus_+3A_keep_tokens">keep_tokens</code></td>
<td>
<p>Should the tokens be saved in the documents that are
returned or discarded?</p>
</td></tr>
<tr><td><code id="TextReuseCorpus_+3A_keep_text">keep_text</code></td>
<td>
<p>Should the text be saved in the documents that are returned
or discarded?</p>
</td></tr>
<tr><td><code id="TextReuseCorpus_+3A_skip_short">skip_short</code></td>
<td>
<p>Should short documents be skipped? (See details.)</p>
</td></tr>
<tr><td><code id="TextReuseCorpus_+3A_x">x</code></td>
<td>
<p>An R object to check.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>skip_short = TRUE</code>, this function will skip very short or
empty documents. A very short document is one where there are two few words
to create at least two n-grams. For example, if five-grams are desired,
then a document must be at least six words long. If no value of <code>n</code> is
provided, then the function assumes a value of <code>n = 3</code>. A warning will
be printed with the document ID of each skipped document. Use
<code>skipped()</code> to get the IDs of skipped documents.
</p>
<p>This function will use multiple cores on non-Windows machines if the
<code>"mc.cores"</code> option is set. For example, to use four cores:
<code>options("mc.cores" = 4L)</code>.
</p>


<h3>See Also</h3>

<p><a href="#topic+TextReuseTextDocument-accessors">Accessors for TextReuse
objects</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dir &lt;- system.file("extdata/legal", package = "textreuse")
corpus &lt;- TextReuseCorpus(dir = dir, meta = list("description" = "Field Codes"))
# Subset by position or file name
corpus[[1]]
names(corpus)
corpus[["ca1851-match"]]

</code></pre>

<hr>
<h2 id='TextReuseTextDocument'>TextReuseTextDocument</h2><span id='topic+TextReuseTextDocument'></span><span id='topic+is.TextReuseTextDocument'></span><span id='topic+has_content'></span><span id='topic+has_tokens'></span><span id='topic+has_hashes'></span><span id='topic+has_minhashes'></span>

<h3>Description</h3>

<p>This is the constructor function for <code>TextReuseTextDocument</code> objects.
This class is used for comparing documents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TextReuseTextDocument(
  text,
  file = NULL,
  meta = list(),
  tokenizer = tokenize_ngrams,
  ...,
  hash_func = hash_string,
  minhash_func = NULL,
  keep_tokens = FALSE,
  keep_text = TRUE,
  skip_short = TRUE
)

is.TextReuseTextDocument(x)

has_content(x)

has_tokens(x)

has_hashes(x)

has_minhashes(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TextReuseTextDocument_+3A_text">text</code></td>
<td>
<p>A character vector containing the text of the document. This
argument can be skipped if supplying <code>file</code>.</p>
</td></tr>
<tr><td><code id="TextReuseTextDocument_+3A_file">file</code></td>
<td>
<p>The path to a text file, if <code>text</code> is not provided.</p>
</td></tr>
<tr><td><code id="TextReuseTextDocument_+3A_meta">meta</code></td>
<td>
<p>A list with named elements for the metadata associated with this
document. If a document is created using the <code>text</code> parameter, then
you must provide an <code>id</code> field, e.g., <code>meta = list(id =
"my_id")</code>. If the document is created using <code>file</code>, then the ID will
be created from the file name.</p>
</td></tr>
<tr><td><code id="TextReuseTextDocument_+3A_tokenizer">tokenizer</code></td>
<td>
<p>A function to split the text into tokens. See
<code><a href="#topic+tokenizers">tokenizers</a></code>. If value is <code>NULL</code>, then tokenizing and
hashing will be skipped.</p>
</td></tr>
<tr><td><code id="TextReuseTextDocument_+3A_...">...</code></td>
<td>
<p>Arguments passed on to the <code>tokenizer</code>.</p>
</td></tr>
<tr><td><code id="TextReuseTextDocument_+3A_hash_func">hash_func</code></td>
<td>
<p>A function to hash the tokens. See
<code><a href="#topic+hash_string">hash_string</a></code>.</p>
</td></tr>
<tr><td><code id="TextReuseTextDocument_+3A_minhash_func">minhash_func</code></td>
<td>
<p>A function to create minhash signatures of the document.
See <code><a href="#topic+minhash_generator">minhash_generator</a></code>.</p>
</td></tr>
<tr><td><code id="TextReuseTextDocument_+3A_keep_tokens">keep_tokens</code></td>
<td>
<p>Should the tokens be saved in the document that is
returned or discarded?</p>
</td></tr>
<tr><td><code id="TextReuseTextDocument_+3A_keep_text">keep_text</code></td>
<td>
<p>Should the text be saved in the document that is returned or
discarded?</p>
</td></tr>
<tr><td><code id="TextReuseTextDocument_+3A_skip_short">skip_short</code></td>
<td>
<p>Should short documents be skipped? (See details.)</p>
</td></tr>
<tr><td><code id="TextReuseTextDocument_+3A_x">x</code></td>
<td>
<p>An R object to check.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This constructor function follows a three-step process. It reads in
the text, either from a file or from memory. It then tokenizes that text.
Then it hashes the tokens. Most of the comparison functions in this package
rely only on the hashes to make the comparison. By passing <code>FALSE</code> to
<code>keep_tokens</code> and <code>keep_text</code>, you can avoid saving those
objects, which can result in significant memory savings for large corpora.
</p>
<p>If <code>skip_short = TRUE</code>, this function will return <code>NULL</code> for very
short or empty documents. A very short document is one where there are two
few words to create at least two n-grams. For example, if five-grams are
desired, then a document must be at least six words long. If no value of
<code>n</code> is provided, then the function assumes a value of <code>n = 3</code>. A
warning will be printed with the document ID of a skipped document.
</p>


<h3>Value</h3>

<p>An object of class <code>TextReuseTextDocument</code>. This object inherits
from the virtual S3 class <code><a href="NLP.html#topic+TextDocument">TextDocument</a></code> in the NLP
package. It contains the following elements: </p>
 <dl>
<dt>content</dt><dd><p>The
text of the document.</p>
</dd> <dt>tokens</dt><dd><p>The tokens created from the text.</p>
</dd>
<dt>hashes</dt><dd><p>Hashes created from the tokens.</p>
</dd> <dt>minhashes</dt><dd><p>The minhash
signature of the document.</p>
</dd> <dt>metadata</dt><dd><p>The document metadata,
including the filename (if any) in <code>file</code>.</p>
</dd> </dl>



<h3>See Also</h3>

<p><a href="#topic+TextReuseTextDocument-accessors">Accessors for TextReuse
objects</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>file &lt;- system.file("extdata/legal/ny1850-match.txt", package = "textreuse")
doc  &lt;- TextReuseTextDocument(file = file, meta = list(id = "ny1850"))
print(doc)
meta(doc)
head(tokens(doc))
head(hashes(doc))
## Not run: 
content(doc)

## End(Not run)
</code></pre>

<hr>
<h2 id='TextReuseTextDocument-accessors'>Accessors for TextReuse objects</h2><span id='topic+TextReuseTextDocument-accessors'></span><span id='topic+tokens'></span><span id='topic+tokens+3C-'></span><span id='topic+hashes'></span><span id='topic+hashes+3C-'></span><span id='topic+minhashes'></span><span id='topic+minhashes+3C-'></span>

<h3>Description</h3>

<p>Accessor functions to read and write components of
<code><a href="#topic+TextReuseTextDocument">TextReuseTextDocument</a></code> and <code><a href="#topic+TextReuseCorpus">TextReuseCorpus</a></code>
objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokens(x)

tokens(x) &lt;- value

hashes(x)

hashes(x) &lt;- value

minhashes(x)

minhashes(x) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TextReuseTextDocument-accessors_+3A_x">x</code></td>
<td>
<p>The object to access.</p>
</td></tr>
<tr><td><code id="TextReuseTextDocument-accessors_+3A_value">value</code></td>
<td>
<p>The value to assign.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Either a vector or a named list of vectors.
</p>

<hr>
<h2 id='tokenize'>Recompute the tokens for a document or corpus</h2><span id='topic+tokenize'></span>

<h3>Description</h3>

<p>Given a <code><a href="#topic+TextReuseTextDocument">TextReuseTextDocument</a></code> or a
<code><a href="#topic+TextReuseCorpus">TextReuseCorpus</a></code>, this function recomputes the tokens and hashes
with the functions specified. Optionally, it can also recompute the minhash signatures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokenize(
  x,
  tokenizer,
  ...,
  hash_func = hash_string,
  minhash_func = NULL,
  keep_tokens = FALSE,
  keep_text = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokenize_+3A_x">x</code></td>
<td>
<p>A <code><a href="#topic+TextReuseTextDocument">TextReuseTextDocument</a></code> or
<code><a href="#topic+TextReuseCorpus">TextReuseCorpus</a></code>.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_tokenizer">tokenizer</code></td>
<td>
<p>A function to split the text into tokens. See
<code><a href="#topic+tokenizers">tokenizers</a></code>.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_...">...</code></td>
<td>
<p>Arguments passed on to the <code>tokenizer</code>.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_hash_func">hash_func</code></td>
<td>
<p>A function to hash the tokens. See
<code><a href="#topic+hash_string">hash_string</a></code>.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_minhash_func">minhash_func</code></td>
<td>
<p>A function to create minhash signatures. See
<code><a href="#topic+minhash_generator">minhash_generator</a></code>.</p>
</td></tr>
<tr><td><code id="tokenize_+3A_keep_tokens">keep_tokens</code></td>
<td>
<p>Should the tokens be saved in the document that is
returned or discarded?</p>
</td></tr>
<tr><td><code id="tokenize_+3A_keep_text">keep_text</code></td>
<td>
<p>Should the text be saved in the document that is returned or
discarded?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The modified <code><a href="#topic+TextReuseTextDocument">TextReuseTextDocument</a></code> or
<code><a href="#topic+TextReuseCorpus">TextReuseCorpus</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dir &lt;- system.file("extdata/legal", package = "textreuse")
corpus &lt;- TextReuseCorpus(dir = dir, tokenizer = NULL)
corpus &lt;- tokenize(corpus, tokenize_ngrams)
head(tokens(corpus[[1]]))
</code></pre>

<hr>
<h2 id='tokenizers'>Split texts into tokens</h2><span id='topic+tokenizers'></span><span id='topic+tokenize_words'></span><span id='topic+tokenize_sentences'></span><span id='topic+tokenize_ngrams'></span><span id='topic+tokenize_skip_ngrams'></span>

<h3>Description</h3>

<p>These functions each turn a text into tokens. The <code>tokenize_ngrams</code>
functions returns shingled n-grams.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokenize_words(string, lowercase = TRUE)

tokenize_sentences(string, lowercase = TRUE)

tokenize_ngrams(string, lowercase = TRUE, n = 3)

tokenize_skip_ngrams(string, lowercase = TRUE, n = 3, k = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokenizers_+3A_string">string</code></td>
<td>
<p>A character vector of length 1 to be tokenized.</p>
</td></tr>
<tr><td><code id="tokenizers_+3A_lowercase">lowercase</code></td>
<td>
<p>Should the tokens be made lower case?</p>
</td></tr>
<tr><td><code id="tokenizers_+3A_n">n</code></td>
<td>
<p>For n-gram tokenizers, the number of words in each n-gram.</p>
</td></tr>
<tr><td><code id="tokenizers_+3A_k">k</code></td>
<td>
<p>For the skip n-gram tokenizer, the maximum skip distance between
words. The function will compute all skip n-grams between <code>0</code> and
<code>k</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions will strip all punctuation.
</p>


<h3>Value</h3>

<p>A character vector containing the tokens.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dylan &lt;- "How many roads must a man walk down? The answer is blowin' in the wind."
tokenize_words(dylan)
tokenize_sentences(dylan)
tokenize_ngrams(dylan, n = 2)
tokenize_skip_ngrams(dylan, n = 3, k = 2)
</code></pre>

<hr>
<h2 id='wordcount'>Count words</h2><span id='topic+wordcount'></span>

<h3>Description</h3>

<p>This function counts words in a text, for example, a character vector, a
<code><a href="#topic+TextReuseTextDocument">TextReuseTextDocument</a></code>, some other object that inherits from
<code><a href="NLP.html#topic+TextDocument">TextDocument</a></code>, or a all the documents in a
<code><a href="#topic+TextReuseCorpus">TextReuseCorpus</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wordcount(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="wordcount_+3A_x">x</code></td>
<td>
<p>The object containing a text.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An integer vector for the word count.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
