<!DOCTYPE html><html><head><title>Help for package tfprobability</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tfprobability}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#glm_families'><p>GLM families</p></a></li>
<li><a href='#glm_fit'><p>Runs multiple Fisher scoring steps</p></a></li>
<li><a href='#glm_fit_one_step'><p>Runs one Fisher scoring step</p></a></li>
<li><a href='#glm_fit_one_step.tensorflow.tensor'><p>Runs one Fisher Scoring step</p></a></li>
<li><a href='#glm_fit.tensorflow.tensor'><p>Runs multiple Fisher scoring steps</p></a></li>
<li><a href='#initializer_blockwise'><p>Blockwise Initializer</p></a></li>
<li><a href='#install_tfprobability'><p>Installs TensorFlow Probability</p></a></li>
<li><a href='#layer_autoregressive'><p>Masked Autoencoder for Distribution Estimation</p></a></li>
<li><a href='#layer_autoregressive_transform'><p>An autoregressive normalizing flow layer, given a <code>layer_autoregressive</code>.</p></a></li>
<li><a href='#layer_categorical_mixture_of_one_hot_categorical'><p>A OneHotCategorical mixture Keras layer from <code>k * (1 + d)</code> params.</p></a></li>
<li><a href='#layer_conv_1d_flipout'><p>1D convolution layer (e.g. temporal convolution) with Flipout</p></a></li>
<li><a href='#layer_conv_1d_reparameterization'><p>1D convolution layer (e.g. temporal convolution).</p></a></li>
<li><a href='#layer_conv_2d_flipout'><p>2D convolution layer (e.g. spatial convolution over images) with Flipout</p></a></li>
<li><a href='#layer_conv_2d_reparameterization'><p>2D convolution layer (e.g. spatial convolution over images)</p></a></li>
<li><a href='#layer_conv_3d_flipout'><p>3D convolution layer (e.g. spatial convolution over volumes) with Flipout</p></a></li>
<li><a href='#layer_conv_3d_reparameterization'><p>3D convolution layer (e.g. spatial convolution over volumes)</p></a></li>
<li><a href='#layer_dense_flipout'><p>Densely-connected layer class with Flipout estimator.</p></a></li>
<li><a href='#layer_dense_local_reparameterization'><p>Densely-connected layer class with local reparameterization estimator.</p></a></li>
<li><a href='#layer_dense_reparameterization'><p>Densely-connected layer class with reparameterization estimator.</p></a></li>
<li><a href='#layer_dense_variational'><p>Dense Variational Layer</p></a></li>
<li><a href='#layer_distribution_lambda'><p>Keras layer enabling plumbing TFP distributions through Keras models</p></a></li>
<li><a href='#layer_independent_bernoulli'><p>An Independent-Bernoulli Keras layer from prod(event_shape) params</p></a></li>
<li><a href='#layer_independent_logistic'><p>An independent Logistic Keras layer.</p></a></li>
<li><a href='#layer_independent_normal'><p>An independent Normal Keras layer.</p></a></li>
<li><a href='#layer_independent_poisson'><p>An independent Poisson Keras layer.</p></a></li>
<li><a href='#layer_kl_divergence_add_loss'><p>Pass-through layer that adds a KL divergence penalty to the model loss</p></a></li>
<li><a href='#layer_kl_divergence_regularizer'><p>Regularizer that adds a KL divergence penalty to the model loss</p></a></li>
<li><a href='#layer_mixture_logistic'><p>A mixture distribution Keras layer, with independent logistic components.</p></a></li>
<li><a href='#layer_mixture_normal'><p>A mixture distribution Keras layer, with independent normal components.</p></a></li>
<li><a href='#layer_mixture_same_family'><p>A mixture (same-family) Keras layer.</p></a></li>
<li><a href='#layer_multivariate_normal_tri_l'><p>A d-variate Multivariate Normal TriL Keras layer from <code>d+d*(d+1)/ 2</code> params</p></a></li>
<li><a href='#layer_one_hot_categorical'><p>A <code>d</code>-variate OneHotCategorical Keras layer from <code>d</code> params.</p></a></li>
<li><a href='#layer_variable'><p>Variable Layer</p></a></li>
<li><a href='#layer_variational_gaussian_process'><p>A Variational Gaussian Process Layer.</p></a></li>
<li><a href='#mcmc_dual_averaging_step_size_adaptation'><p>Adapts the inner kernel's <code>step_size</code> based on <code>log_accept_prob</code>.</p></a></li>
<li><a href='#mcmc_effective_sample_size'><p>Estimate a lower bound on effective sample size for each independent chain.</p></a></li>
<li><a href='#mcmc_hamiltonian_monte_carlo'><p>Runs one step of Hamiltonian Monte Carlo.</p></a></li>
<li><a href='#mcmc_metropolis_adjusted_langevin_algorithm'><p>Runs one step of Metropolis-adjusted Langevin algorithm.</p></a></li>
<li><a href='#mcmc_metropolis_hastings'><p>Runs one step of the Metropolis-Hastings algorithm.</p></a></li>
<li><a href='#mcmc_no_u_turn_sampler'><p>Runs one step of the No U-Turn Sampler</p></a></li>
<li><a href='#mcmc_potential_scale_reduction'><p>Gelman and Rubin (1992)'s potential scale reduction for chain convergence.</p></a></li>
<li><a href='#mcmc_random_walk_metropolis'><p>Runs one step of the RWM algorithm with symmetric proposal.</p></a></li>
<li><a href='#mcmc_replica_exchange_mc'><p>Runs one step of the Replica Exchange Monte Carlo</p></a></li>
<li><a href='#mcmc_sample_annealed_importance_chain'><p>Runs annealed importance sampling (AIS) to estimate normalizing constants.</p></a></li>
<li><a href='#mcmc_sample_chain'><p>Implements Markov chain Monte Carlo via repeated <code>TransitionKernel</code> steps.</p></a></li>
<li><a href='#mcmc_sample_halton_sequence'><p>Returns a sample from the <code>dim</code> dimensional Halton sequence.</p></a></li>
<li><a href='#mcmc_simple_step_size_adaptation'><p>Adapts the inner kernel's <code>step_size</code> based on <code>log_accept_prob</code>.</p></a></li>
<li><a href='#mcmc_slice_sampler'><p>Runs one step of the slice sampler using a hit and run approach</p></a></li>
<li><a href='#mcmc_transformed_transition_kernel'><p>Applies a bijector to the MCMC's state space</p></a></li>
<li><a href='#mcmc_uncalibrated_hamiltonian_monte_carlo'><p>Runs one step of Uncalibrated Hamiltonian Monte Carlo</p></a></li>
<li><a href='#mcmc_uncalibrated_langevin'><p>Runs one step of Uncalibrated Langevin discretized diffusion.</p></a></li>
<li><a href='#mcmc_uncalibrated_random_walk'><p>Generate proposal for the Random Walk Metropolis algorithm.</p></a></li>
<li><a href='#params_size_categorical_mixture_of_one_hot_categorical'><p>number of <code>params</code> needed to create a CategoricalMixtureOfOneHotCategorical distribution</p></a></li>
<li><a href='#params_size_independent_bernoulli'><p>number of <code>params</code> needed to create an IndependentBernoulli distribution</p></a></li>
<li><a href='#params_size_independent_logistic'><p>number of <code>params</code> needed to create an IndependentLogistic distribution</p></a></li>
<li><a href='#params_size_independent_normal'><p>number of <code>params</code> needed to create an IndependentNormal distribution</p></a></li>
<li><a href='#params_size_independent_poisson'><p>number of <code>params</code> needed to create an IndependentPoisson distribution</p></a></li>
<li><a href='#params_size_mixture_logistic'><p>number of <code>params</code> needed to create a MixtureLogistic distribution</p></a></li>
<li><a href='#params_size_mixture_normal'><p>number of <code>params</code> needed to create a MixtureNormal distribution</p></a></li>
<li><a href='#params_size_mixture_same_family'><p>number of <code>params</code> needed to create a MixtureSameFamily distribution</p></a></li>
<li><a href='#params_size_multivariate_normal_tri_l'><p>number of <code>params</code> needed to create a MultivariateNormalTriL distribution</p></a></li>
<li><a href='#params_size_one_hot_categorical'><p>number of <code>params</code> needed to create a OneHotCategorical distribution</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#sts_additive_state_space_model'><p>A state space model representing a sum of component state space models.</p></a></li>
<li><a href='#sts_autoregressive'><p>Formal representation of an autoregressive model.</p></a></li>
<li><a href='#sts_autoregressive_state_space_model'><p>State space model for an autoregressive process.</p></a></li>
<li><a href='#sts_build_factored_surrogate_posterior'><p>Build a variational posterior that factors over model parameters.</p></a></li>
<li><a href='#sts_build_factored_variational_loss'><p>Build a loss function for variational inference in STS models.</p></a></li>
<li><a href='#sts_constrained_seasonal_state_space_model'><p>Seasonal state space model with effects constrained to sum to zero.</p></a></li>
<li><a href='#sts_decompose_by_component'><p>Decompose an observed time series into contributions from each component.</p></a></li>
<li><a href='#sts_decompose_forecast_by_component'><p>Decompose a forecast distribution into contributions from each component.</p></a></li>
<li><a href='#sts_dynamic_linear_regression'><p>Formal representation of a dynamic linear regression model.</p></a></li>
<li><a href='#sts_dynamic_linear_regression_state_space_model'><p>State space model for a dynamic linear regression from provided covariates.</p></a></li>
<li><a href='#sts_fit_with_hmc'><p>Draw posterior samples using Hamiltonian Monte Carlo (HMC)</p></a></li>
<li><a href='#sts_forecast'><p>Construct predictive distribution over future observations</p></a></li>
<li><a href='#sts_linear_regression'><p>Formal representation of a linear regression from provided covariates.</p></a></li>
<li><a href='#sts_local_level'><p>Formal representation of a local level model</p></a></li>
<li><a href='#sts_local_level_state_space_model'><p>State space model for a local level</p></a></li>
<li><a href='#sts_local_linear_trend'><p>Formal representation of a local linear trend model</p></a></li>
<li><a href='#sts_local_linear_trend_state_space_model'><p>State space model for a local linear trend</p></a></li>
<li><a href='#sts_one_step_predictive'><p>Compute one-step-ahead predictive distributions for all timesteps</p></a></li>
<li><a href='#sts_sample_uniform_initial_state'><p>Initialize from a uniform <code style="white-space: pre;">&#8288;[-2, 2]&#8288;</code> distribution in unconstrained space.</p></a></li>
<li><a href='#sts_seasonal'><p>Formal representation of a seasonal effect model.</p></a></li>
<li><a href='#sts_seasonal_state_space_model'><p>State space model for a seasonal effect.</p></a></li>
<li><a href='#sts_semi_local_linear_trend'><p>Formal representation of a semi-local linear trend model.</p></a></li>
<li><a href='#sts_semi_local_linear_trend_state_space_model'><p>State space model for a semi-local linear trend.</p></a></li>
<li><a href='#sts_smooth_seasonal'><p>Formal representation of a smooth seasonal effect model</p></a></li>
<li><a href='#sts_smooth_seasonal_state_space_model'><p>State space model for a smooth seasonal effect</p></a></li>
<li><a href='#sts_sparse_linear_regression'><p>Formal representation of a sparse linear regression.</p></a></li>
<li><a href='#sts_sum'><p>Sum of structural time series components.</p></a></li>
<li><a href='#tfb_absolute_value'><p>Computes<code>Y = g(X) = Abs(X)</code>, element-wise</p></a></li>
<li><a href='#tfb_affine'><p>Affine bijector</p></a></li>
<li><a href='#tfb_affine_linear_operator'><p>Computes<code style="white-space: pre;">&#8288;Y = g(X; shift, scale) = scale @ X + shift&#8288;</code></p></a></li>
<li><a href='#tfb_affine_scalar'><p>AffineScalar bijector (Deprecated)</p></a></li>
<li><a href='#tfb_ascending'><p>Maps unconstrained R^n to R^n in ascending order.</p></a></li>
<li><a href='#tfb_batch_normalization'><p>Computes<code>Y = g(X)</code> s.t. <code>X = g^-1(Y) = (Y - mean(Y)) / std(Y)</code></p></a></li>
<li><a href='#tfb_blockwise'><p>Bijector which applies a list of bijectors to blocks of a Tensor</p></a></li>
<li><a href='#tfb_chain'><p>Bijector which applies a sequence of bijectors</p></a></li>
<li><a href='#tfb_cholesky_outer_product'><p>Computes<code>g(X) = X @ X.T</code> where <code>X</code> is lower-triangular, positive-diagonal matrix</p></a></li>
<li><a href='#tfb_cholesky_to_inv_cholesky'><p>Maps the Cholesky factor of M to the Cholesky factor of <code>M^{-1}</code></p></a></li>
<li><a href='#tfb_correlation_cholesky'><p>Maps unconstrained reals to Cholesky-space correlation matrices.</p></a></li>
<li><a href='#tfb_cumsum'><p>Computes the cumulative sum of a tensor along a specified axis.</p></a></li>
<li><a href='#tfb_discrete_cosine_transform'><p>Computes<code>Y = g(X) = DCT(X)</code>, where DCT type is indicated by the type arg</p></a></li>
<li><a href='#tfb_exp'><p>Computes<code>Y=g(X)=exp(X)</code></p></a></li>
<li><a href='#tfb_expm1'><p>Computes<code>Y = g(X) = exp(X) - 1</code></p></a></li>
<li><a href='#tfb_ffjord'><p>Implements a continuous normalizing flow X-&gt;Y defined via an ODE.</p></a></li>
<li><a href='#tfb_fill_scale_tri_l'><p>Transforms unconstrained vectors to TriL matrices with positive diagonal</p></a></li>
<li><a href='#tfb_fill_triangular'><p>Transforms vectors to triangular</p></a></li>
<li><a href='#tfb_forward'><p>Returns the forward Bijector evaluation, i.e., <code>X = g(Y)</code>.</p></a></li>
<li><a href='#tfb_forward_log_det_jacobian'><p>Returns the result of the forward evaluation of the log determinant of the Jacobian</p></a></li>
<li><a href='#tfb_glow'><p>Implements the Glow Bijector from Kingma &amp; Dhariwal (2018).</p></a></li>
<li><a href='#tfb_gompertz_cdf'><p>Compute <code style="white-space: pre;">&#8288;Y = g(X) = 1 - exp(-c * (exp(rate * X) - 1)&#8288;</code>, the Gompertz CDF.</p></a></li>
<li><a href='#tfb_gumbel'><p>Computes<code>Y = g(X) = exp(-exp(-(X - loc) / scale))</code></p></a></li>
<li><a href='#tfb_gumbel_cdf'><p>Compute <code>Y = g(X) = exp(-exp(-(X - loc) / scale))</code>, the Gumbel CDF.</p></a></li>
<li><a href='#tfb_identity'><p>Computes<code>Y = g(X) = X</code></p></a></li>
<li><a href='#tfb_inline'><p>Bijector constructed from custom functions</p></a></li>
<li><a href='#tfb_inverse'><p>Returns the inverse Bijector evaluation, i.e., <code>X = g^{-1}(Y)</code>.</p></a></li>
<li><a href='#tfb_inverse_log_det_jacobian'><p>Returns the result of the inverse evaluation of the log determinant of the Jacobian</p></a></li>
<li><a href='#tfb_invert'><p>Bijector which inverts another Bijector</p></a></li>
<li><a href='#tfb_iterated_sigmoid_centered'><p>Bijector which applies a Stick Breaking procedure.</p></a></li>
<li><a href='#tfb_kumaraswamy'><p>Computes<code>Y = g(X) = (1 - (1 - X)**(1 / b))**(1 / a)</code>, with X in <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code></p></a></li>
<li><a href='#tfb_kumaraswamy_cdf'><p>Computes<code>Y = g(X) = (1 - (1 - X)**(1 / b))**(1 / a)</code>, with X in <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code></p></a></li>
<li><a href='#tfb_lambert_w_tail'><p>LambertWTail transformation for heavy-tail Lambert W x F random variables.</p></a></li>
<li><a href='#tfb_masked_autoregressive_default_template'><p>Masked Autoregressive Density Estimator</p></a></li>
<li><a href='#tfb_masked_autoregressive_flow'><p>Affine MaskedAutoregressiveFlow bijector</p></a></li>
<li><a href='#tfb_masked_dense'><p>Autoregressively masked dense layer</p></a></li>
<li><a href='#tfb_matrix_inverse_tri_l'><p>Computes <code>g(L) = inv(L)</code>, where L is a lower-triangular matrix</p></a></li>
<li><a href='#tfb_matvec_lu'><p>Matrix-vector multiply using LU decomposition</p></a></li>
<li><a href='#tfb_normal_cdf'><p>Computes<code>Y = g(X) = NormalCDF(x)</code></p></a></li>
<li><a href='#tfb_ordered'><p>Bijector which maps a tensor x_k that has increasing elements in the last dimension to an unconstrained tensor y_k</p></a></li>
<li><a href='#tfb_pad'><p>Pads a value to the <code>event_shape</code> of a <code>Tensor</code>.</p></a></li>
<li><a href='#tfb_permute'><p>Permutes the rightmost dimension of a Tensor</p></a></li>
<li><a href='#tfb_power_transform'><p>Computes<code>Y = g(X) = (1 + X * c)**(1 / c)</code>, where <code>X &gt;= -1 / c</code></p></a></li>
<li><a href='#tfb_rational_quadratic_spline'><p>A piecewise rational quadratic spline, as developed in Conor et al.(2019).</p></a></li>
<li><a href='#tfb_rayleigh_cdf'><p>Compute <code style="white-space: pre;">&#8288;Y = g(X) = 1 - exp( -(X/scale)**2 / 2 ), X &gt;= 0&#8288;</code>.</p></a></li>
<li><a href='#tfb_real_nvp'><p>RealNVP affine coupling layer for vector-valued events</p></a></li>
<li><a href='#tfb_real_nvp_default_template'><p>Build a scale-and-shift function using a multi-layer neural network</p></a></li>
<li><a href='#tfb_reciprocal'><p>A Bijector that computes <code>b(x) = 1. / x</code></p></a></li>
<li><a href='#tfb_reshape'><p>Reshapes the event_shape of a Tensor</p></a></li>
<li><a href='#tfb_scale'><p>Compute <code style="white-space: pre;">&#8288;Y = g(X; scale) = scale * X&#8288;</code>.</p></a></li>
<li><a href='#tfb_scale_matvec_diag'><p>Compute <code style="white-space: pre;">&#8288;Y = g(X; scale) = scale @ X&#8288;</code></p></a></li>
<li><a href='#tfb_scale_matvec_linear_operator'><p>Compute <code style="white-space: pre;">&#8288;Y = g(X; scale) = scale @ X&#8288;</code>.</p></a></li>
<li><a href='#tfb_scale_matvec_lu'><p>Matrix-vector multiply using LU decomposition.</p></a></li>
<li><a href='#tfb_scale_matvec_tri_l'><p>Compute <code style="white-space: pre;">&#8288;Y = g(X; scale) = scale @ X&#8288;</code>.</p></a></li>
<li><a href='#tfb_scale_tri_l'><p>Transforms unconstrained vectors to TriL matrices with positive diagonal</p></a></li>
<li><a href='#tfb_shift'><p>Compute <code style="white-space: pre;">&#8288;Y = g(X; shift) = X + shift&#8288;</code>.</p></a></li>
<li><a href='#tfb_shifted_gompertz_cdf'><p>Compute <code>Y = g(X) = (1 - exp(-rate * X)) * exp(-c * exp(-rate * X))</code></p></a></li>
<li><a href='#tfb_sigmoid'><p>Computes<code>Y = g(X) = 1 / (1 + exp(-X))</code></p></a></li>
<li><a href='#tfb_sinh'><p>Bijector that computes <code>Y = sinh(X)</code>.</p></a></li>
<li><a href='#tfb_sinh_arcsinh'><p>Computes<code>Y = g(X) = Sinh( (Arcsinh(X) + skewness) * tailweight )</code></p></a></li>
<li><a href='#tfb_softmax_centered'><p>Computes <code style="white-space: pre;">&#8288;Y = g(X) = exp([X 0]) / sum(exp([X 0]))&#8288;</code></p></a></li>
<li><a href='#tfb_softplus'><p>Computes <code>Y = g(X) = Log[1 + exp(X)]</code></p></a></li>
<li><a href='#tfb_softsign'><p>Computes <code style="white-space: pre;">&#8288;Y = g(X) = X / (1 + |X|)&#8288;</code></p></a></li>
<li><a href='#tfb_split'><p>Split a <code>Tensor</code> event along an axis into a list of <code>Tensor</code>s.</p></a></li>
<li><a href='#tfb_square'><p>Computes<code>g(X) = X^2</code>; X is a positive real number.</p></a></li>
<li><a href='#tfb_tanh'><p>Computes <code>Y = tanh(X)</code></p></a></li>
<li><a href='#tfb_transform_diagonal'><p>Applies a Bijector to the diagonal of a matrix</p></a></li>
<li><a href='#tfb_transpose'><p>Computes<code>Y = g(X) = transpose_rightmost_dims(X, rightmost_perm)</code></p></a></li>
<li><a href='#tfb_weibull'><p>Computes<code>Y = g(X) = 1 - exp((-X / scale) ** concentration)</code> where X &gt;= 0</p></a></li>
<li><a href='#tfb_weibull_cdf'><p>Compute <code style="white-space: pre;">&#8288;Y = g(X) = 1 - exp((-X / scale) ** concentration), X &gt;= 0&#8288;</code>.</p></a></li>
<li><a href='#tfd_autoregressive'><p>Autoregressive distribution</p></a></li>
<li><a href='#tfd_batch_reshape'><p>Batch-Reshaping distribution</p></a></li>
<li><a href='#tfd_bates'><p>Bates distribution.</p></a></li>
<li><a href='#tfd_bernoulli'><p>Bernoulli distribution</p></a></li>
<li><a href='#tfd_beta'><p>Beta distribution</p></a></li>
<li><a href='#tfd_beta_binomial'><p>Beta-Binomial compound distribution</p></a></li>
<li><a href='#tfd_binomial'><p>Binomial distribution</p></a></li>
<li><a href='#tfd_blockwise'><p>Blockwise distribution</p></a></li>
<li><a href='#tfd_categorical'><p>Categorical distribution over integers</p></a></li>
<li><a href='#tfd_cauchy'><p>Cauchy distribution with location <code>loc</code> and scale <code>scale</code></p></a></li>
<li><a href='#tfd_cdf'><p>Cumulative distribution function.</p>
Given random variable X, the cumulative distribution function cdf is:
<code>cdf(x) := P[X &lt;= x]</code></a></li>
<li><a href='#tfd_chi'><p>Chi distribution</p></a></li>
<li><a href='#tfd_chi2'><p>Chi Square distribution</p></a></li>
<li><a href='#tfd_cholesky_lkj'><p>The CholeskyLKJ distribution on cholesky factors of correlation matrices</p></a></li>
<li><a href='#tfd_continuous_bernoulli'><p>Continuous Bernoulli distribution.</p></a></li>
<li><a href='#tfd_covariance'><p>Covariance.</p></a></li>
<li><a href='#tfd_cross_entropy'><p>Computes the (Shannon) cross entropy.</p></a></li>
<li><a href='#tfd_deterministic'><p>Scalar <code>Deterministic</code> distribution on the real line</p></a></li>
<li><a href='#tfd_dirichlet'><p>Dirichlet distribution</p></a></li>
<li><a href='#tfd_dirichlet_multinomial'><p>Dirichlet-Multinomial compound distribution</p></a></li>
<li><a href='#tfd_doublesided_maxwell'><p>Double-sided Maxwell distribution.</p></a></li>
<li><a href='#tfd_empirical'><p>Empirical distribution</p></a></li>
<li><a href='#tfd_entropy'><p>Shannon entropy in nats.</p></a></li>
<li><a href='#tfd_exp_gamma'><p>ExpGamma distribution.</p></a></li>
<li><a href='#tfd_exp_inverse_gamma'><p>ExpInverseGamma distribution.</p></a></li>
<li><a href='#tfd_exp_relaxed_one_hot_categorical'><p>ExpRelaxedOneHotCategorical distribution with temperature and logits.</p></a></li>
<li><a href='#tfd_exponential'><p>Exponential distribution</p></a></li>
<li><a href='#tfd_finite_discrete'><p>The finite discrete distribution.</p></a></li>
<li><a href='#tfd_gamma'><p>Gamma distribution</p></a></li>
<li><a href='#tfd_gamma_gamma'><p>Gamma-Gamma distribution</p></a></li>
<li><a href='#tfd_gaussian_process'><p>Marginal distribution of a Gaussian process at finitely many points.</p></a></li>
<li><a href='#tfd_gaussian_process_regression_model'><p>Posterior predictive distribution in a conjugate GP regression model.</p></a></li>
<li><a href='#tfd_generalized_normal'><p>The Generalized Normal distribution.</p></a></li>
<li><a href='#tfd_generalized_pareto'><p>The Generalized Pareto distribution.</p></a></li>
<li><a href='#tfd_geometric'><p>Geometric distribution</p></a></li>
<li><a href='#tfd_gumbel'><p>Scalar Gumbel distribution with location <code>loc</code> and <code>scale</code> parameters</p></a></li>
<li><a href='#tfd_half_cauchy'><p>Half-Cauchy distribution</p></a></li>
<li><a href='#tfd_half_normal'><p>Half-Normal distribution with scale <code>scale</code></p></a></li>
<li><a href='#tfd_hidden_markov_model'><p>Hidden Markov model distribution</p></a></li>
<li><a href='#tfd_horseshoe'><p>Horseshoe distribution</p></a></li>
<li><a href='#tfd_independent'><p>Independent distribution from batch of distributions</p></a></li>
<li><a href='#tfd_inverse_gamma'><p>InverseGamma distribution</p></a></li>
<li><a href='#tfd_inverse_gaussian'><p>Inverse Gaussian distribution</p></a></li>
<li><a href='#tfd_johnson_s_u'><p>Johnson's SU-distribution.</p></a></li>
<li><a href='#tfd_joint_distribution_named'><p>Joint distribution parameterized by named distribution-making functions.</p></a></li>
<li><a href='#tfd_joint_distribution_named_auto_batched'><p>Joint distribution parameterized by named distribution-making functions.</p></a></li>
<li><a href='#tfd_joint_distribution_sequential'><p>Joint distribution parameterized by distribution-making functions</p></a></li>
<li><a href='#tfd_joint_distribution_sequential_auto_batched'><p>Joint distribution parameterized by distribution-making functions.</p></a></li>
<li><a href='#tfd_kl_divergence'><p>Computes the Kullback&ndash;Leibler divergence.</p></a></li>
<li><a href='#tfd_kumaraswamy'><p>Kumaraswamy distribution</p></a></li>
<li><a href='#tfd_laplace'><p>Laplace distribution with location <code>loc</code> and <code>scale</code> parameters</p></a></li>
<li><a href='#tfd_linear_gaussian_state_space_model'><p>Observation distribution from a linear Gaussian state space model</p></a></li>
<li><a href='#tfd_lkj'><p>LKJ distribution on correlation matrices</p></a></li>
<li><a href='#tfd_log_cdf'><p>Log cumulative distribution function.</p></a></li>
<li><a href='#tfd_log_logistic'><p>The log-logistic distribution.</p></a></li>
<li><a href='#tfd_log_normal'><p>Log-normal distribution</p></a></li>
<li><a href='#tfd_log_prob'><p>Log probability density/mass function.</p></a></li>
<li><a href='#tfd_log_survival_function'><p>Log survival function.</p></a></li>
<li><a href='#tfd_logistic'><p>Logistic distribution with location <code>loc</code> and <code>scale</code> parameters</p></a></li>
<li><a href='#tfd_logit_normal'><p>The Logit-Normal distribution</p></a></li>
<li><a href='#tfd_mean'><p>Mean.</p></a></li>
<li><a href='#tfd_mixture'><p>Mixture distribution</p></a></li>
<li><a href='#tfd_mixture_same_family'><p>Mixture (same-family) distribution</p></a></li>
<li><a href='#tfd_mode'><p>Mode.</p></a></li>
<li><a href='#tfd_multinomial'><p>Multinomial distribution</p></a></li>
<li><a href='#tfd_multivariate_normal_diag'><p>Multivariate normal distribution on <code>R^k</code></p></a></li>
<li><a href='#tfd_multivariate_normal_diag_plus_low_rank'><p>Multivariate normal distribution on <code>R^k</code></p></a></li>
<li><a href='#tfd_multivariate_normal_full_covariance'><p>Multivariate normal distribution on <code>R^k</code></p></a></li>
<li><a href='#tfd_multivariate_normal_linear_operator'><p>The multivariate normal distribution on <code>R^k</code></p></a></li>
<li><a href='#tfd_multivariate_normal_tri_l'><p>The multivariate normal distribution on <code>R^k</code></p></a></li>
<li><a href='#tfd_multivariate_student_t_linear_operator'><p>Multivariate Student's t-distribution on <code>R^k</code></p></a></li>
<li><a href='#tfd_negative_binomial'><p>NegativeBinomial distribution</p></a></li>
<li><a href='#tfd_normal'><p>Normal distribution with loc and scale parameters</p></a></li>
<li><a href='#tfd_one_hot_categorical'><p>OneHotCategorical distribution</p></a></li>
<li><a href='#tfd_pareto'><p>Pareto distribution</p></a></li>
<li><a href='#tfd_pert'><p>Modified PERT distribution for modeling expert predictions.</p></a></li>
<li><a href='#tfd_pixel_cnn'><p>The Pixel CNN++ distribution</p></a></li>
<li><a href='#tfd_plackett_luce'><p>Plackett-Luce distribution over permutations.</p></a></li>
<li><a href='#tfd_poisson'><p>Poisson distribution</p></a></li>
<li><a href='#tfd_poisson_log_normal_quadrature_compound'><p><code>PoissonLogNormalQuadratureCompound</code> distribution</p></a></li>
<li><a href='#tfd_power_spherical'><p>The Power Spherical distribution over unit vectors on <code>S^{n-1}</code>.</p></a></li>
<li><a href='#tfd_prob'><p>Probability density/mass function.</p></a></li>
<li><a href='#tfd_probit_bernoulli'><p>ProbitBernoulli distribution.</p></a></li>
<li><a href='#tfd_quantile'><p>Quantile function. Aka &quot;inverse cdf&quot; or &quot;percent point function&quot;.</p></a></li>
<li><a href='#tfd_quantized'><p>Distribution representing the quantization <code>Y = ceiling(X)</code></p></a></li>
<li><a href='#tfd_relaxed_bernoulli'><p>RelaxedBernoulli distribution with temperature and logits parameters</p></a></li>
<li><a href='#tfd_relaxed_one_hot_categorical'><p>RelaxedOneHotCategorical distribution with temperature and logits</p></a></li>
<li><a href='#tfd_sample'><p>Generate samples of the specified shape.</p></a></li>
<li><a href='#tfd_sample_distribution'><p>Sample distribution via independent draws.</p></a></li>
<li><a href='#tfd_sinh_arcsinh'><p>The SinhArcsinh transformation of a distribution on <code style="white-space: pre;">&#8288;(-inf, inf)&#8288;</code></p></a></li>
<li><a href='#tfd_skellam'><p>Skellam distribution.</p></a></li>
<li><a href='#tfd_spherical_uniform'><p>The uniform distribution over unit vectors on <code>S^{n-1}</code>.</p></a></li>
<li><a href='#tfd_stddev'><p>Standard deviation.</p></a></li>
<li><a href='#tfd_student_t'><p>Student's t-distribution</p></a></li>
<li><a href='#tfd_student_t_process'><p>Marginal distribution of a Student's T process at finitely many points</p></a></li>
<li><a href='#tfd_survival_function'><p>Survival function.</p></a></li>
<li><a href='#tfd_transformed_distribution'><p>A Transformed Distribution</p></a></li>
<li><a href='#tfd_triangular'><p>Triangular distribution with <code>low</code>, <code>high</code> and <code>peak</code> parameters</p></a></li>
<li><a href='#tfd_truncated_cauchy'><p>The Truncated Cauchy distribution.</p></a></li>
<li><a href='#tfd_truncated_normal'><p>Truncated Normal distribution</p></a></li>
<li><a href='#tfd_uniform'><p>Uniform distribution with <code>low</code> and <code>high</code> parameters</p></a></li>
<li><a href='#tfd_variance'><p>Variance.</p></a></li>
<li><a href='#tfd_variational_gaussian_process'><p>Posterior predictive of a variational Gaussian process</p></a></li>
<li><a href='#tfd_vector_deterministic'><p>Vector Deterministic Distribution</p></a></li>
<li><a href='#tfd_vector_diffeomixture'><p>VectorDiffeomixture distribution</p></a></li>
<li><a href='#tfd_vector_exponential_diag'><p>The vectorization of the Exponential distribution on <code>R^k</code></p></a></li>
<li><a href='#tfd_vector_exponential_linear_operator'><p>The vectorization of the Exponential distribution on <code>R^k</code></p></a></li>
<li><a href='#tfd_vector_laplace_diag'><p>The vectorization of the Laplace distribution on <code>R^k</code></p></a></li>
<li><a href='#tfd_vector_laplace_linear_operator'><p>The vectorization of the Laplace distribution on <code>R^k</code></p></a></li>
<li><a href='#tfd_vector_sinh_arcsinh_diag'><p>The (diagonal) SinhArcsinh transformation of a distribution on <code>R^k</code></p></a></li>
<li><a href='#tfd_von_mises'><p>The von Mises distribution over angles</p></a></li>
<li><a href='#tfd_von_mises_fisher'><p>The von Mises-Fisher distribution over unit vectors on <code>S^{n-1}</code></p></a></li>
<li><a href='#tfd_weibull'><p>The Weibull distribution with 'concentration' and <code>scale</code> parameters.</p></a></li>
<li><a href='#tfd_wishart'><p>The matrix Wishart distribution on positive definite matrices</p></a></li>
<li><a href='#tfd_wishart_linear_operator'><p>The matrix Wishart distribution on positive definite matrices</p></a></li>
<li><a href='#tfd_wishart_tri_l'><p>The matrix Wishart distribution parameterized with Cholesky factors.</p></a></li>
<li><a href='#tfd_zipf'><p>Zipf distribution</p></a></li>
<li><a href='#tfp'><p>Handle to the <code>tensorflow_probability</code> module</p></a></li>
<li><a href='#tfp_version'><p>TensorFlow Probability Version</p></a></li>
<li><a href='#vi_amari_alpha'><p>The Amari-alpha Csiszar-function in log-space</p></a></li>
<li><a href='#vi_arithmetic_geometric'><p>The Arithmetic-Geometric Csiszar-function in log-space</p></a></li>
<li><a href='#vi_chi_square'><p>The chi-square Csiszar-function in log-space</p></a></li>
<li><a href='#vi_csiszar_vimco'><p>Use VIMCO to lower the variance of the gradient of csiszar_function(Avg(logu))</p></a></li>
<li><a href='#vi_dual_csiszar_function'><p>Calculates the dual Csiszar-function in log-space</p></a></li>
<li><a href='#vi_fit_surrogate_posterior'><p>Fit a surrogate posterior to a target (unnormalized) log density</p></a></li>
<li><a href='#vi_jeffreys'><p>The Jeffreys Csiszar-function in log-space</p></a></li>
<li><a href='#vi_jensen_shannon'><p>The Jensen-Shannon Csiszar-function in log-space</p></a></li>
<li><a href='#vi_kl_forward'><p>The forward Kullback-Leibler Csiszar-function in log-space</p></a></li>
<li><a href='#vi_kl_reverse'><p>The reverse Kullback-Leibler Csiszar-function in log-space</p></a></li>
<li><a href='#vi_log1p_abs'><p>The log1p-abs Csiszar-function in log-space</p></a></li>
<li><a href='#vi_modified_gan'><p>The Modified-GAN Csiszar-function in log-space</p></a></li>
<li><a href='#vi_monte_carlo_variational_loss'><p>Monte-Carlo approximation of an f-Divergence variational loss</p></a></li>
<li><a href='#vi_pearson'><p>The Pearson Csiszar-function in log-space</p></a></li>
<li><a href='#vi_squared_hellinger'><p>The Squared-Hellinger Csiszar-function in log-space</p></a></li>
<li><a href='#vi_symmetrized_csiszar_function'><p>Symmetrizes a Csiszar-function in log-space</p></a></li>
<li><a href='#vi_t_power'><p>The T-Power Csiszar-function in log-space</p></a></li>
<li><a href='#vi_total_variation'><p>The Total Variation Csiszar-function in log-space</p></a></li>
<li><a href='#vi_triangular'><p>The Triangular Csiszar-function in log-space</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Interface to 'TensorFlow Probability'</td>
</tr>
<tr>
<td>Version:</td>
<td>0.15.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Interface to 'TensorFlow Probability', a 'Python' library built on 'TensorFlow'
    that makes it easy to combine probabilistic models and deep learning on modern hardware ('TPU', 'GPU').
    'TensorFlow Probability' includes a wide selection of probability distributions and bijectors, probabilistic layers,
    variational inference, Markov chain Monte Carlo, and optimizers such as Nelder-Mead, BFGS, and SGLD.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License (&ge; 2.0)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/rstudio/tfprobability">https://github.com/rstudio/tfprobability</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/rstudio/tfprobability/issues">https://github.com/rstudio/tfprobability/issues</a></td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>TensorFlow Probability
(https://www.tensorflow.org/probability)</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>tensorflow (&ge; 2.4.0), reticulate, keras, magrittr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>tfdatasets, testthat (&ge; 2.1.0), knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-08-31 17:00:12 UTC; tomasz</td>
</tr>
<tr>
<td>Author:</td>
<td>Tomasz Kalinowski [ctb, cre],
  Sigrid Keydana [aut],
  Daniel Falbel [ctb],
  Kevin Kuo <a href="https://orcid.org/0000-0001-7803-7901"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [ctb],
  RStudio [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Tomasz Kalinowski &lt;tomasz.kalinowski@rstudio.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-09-01 09:10:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='glm_families'>GLM families</h2><span id='topic+glm_families'></span>

<h3>Description</h3>

<p>A list of models that can be used as the <code>model</code> argument in <code><a href="#topic+glm_fit">glm_fit()</a></code>:
</p>


<h3>Details</h3>


<ul>
<li> <p><code>Bernoulli</code>: <code>Bernoulli(probs=mean)</code> where <code>mean = sigmoid(matmul(X, weights))</code>
</p>
</li>
<li> <p><code>BernoulliNormalCDF</code>: <code>Bernoulli(probs=mean)</code> where <code style="white-space: pre;">&#8288;mean = Normal(0, 1).cdf(matmul(X, weights))&#8288;</code>
</p>
</li>
<li> <p><code>GammaExp</code>: <code>Gamma(concentration=1, rate=1 / mean)</code> where <code>mean = exp(matmul(X, weights))</code>
</p>
</li>
<li> <p><code>GammaSoftplus</code>: <code>Gamma(concentration=1, rate=1 / mean)</code> where <code>mean = softplus(matmul(X, weights))</code>
</p>
</li>
<li> <p><code>LogNormal</code>: <code>LogNormal(loc=log(mean) - log(2) / 2, scale=sqrt(log(2)))</code> where
<code>mean = exp(matmul(X, weights))</code>.
</p>
</li>
<li> <p><code>LogNormalSoftplus</code>: <code>LogNormal(loc=log(mean) - log(2) / 2, scale=sqrt(log(2)))</code> where
<code>mean = softplus(matmul(X, weights))</code>
</p>
</li>
<li> <p><code>Normal</code>: <code>Normal(loc=mean, scale=1)</code> where <code>mean = matmul(X, weights)</code>.
</p>
</li>
<li> <p><code>NormalReciprocal</code>: <code>Normal(loc=mean, scale=1)</code> where <code>mean = 1 / matmul(X, weights)</code>
</p>
</li>
<li> <p><code>Poisson</code>: <code>Poisson(rate=mean)</code> where <code>mean = exp(matmul(X, weights))</code>.
</p>
</li>
<li> <p><code>PoissonSoftplus</code>: <code>Poisson(rate=mean)</code> where <code>mean = softplus(matmul(X, weights))</code>.
</p>
</li></ul>



<h3>Value</h3>

<p>list of models that can be used as the <code>model</code> argument in <code><a href="#topic+glm_fit">glm_fit()</a></code>
</p>


<h3>See Also</h3>

<p>Other glm_fit: 
<code><a href="#topic+glm_fit.tensorflow.tensor">glm_fit.tensorflow.tensor</a>()</code>,
<code><a href="#topic+glm_fit_one_step.tensorflow.tensor">glm_fit_one_step.tensorflow.tensor</a>()</code>
</p>

<hr>
<h2 id='glm_fit'>Runs multiple Fisher scoring steps</h2><span id='topic+glm_fit'></span>

<h3>Description</h3>

<p>Runs multiple Fisher scoring steps
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glm_fit(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glm_fit_+3A_x">x</code></td>
<td>
<p>float-like, matrix-shaped Tensor where each row represents a sample's
features.</p>
</td></tr>
<tr><td><code id="glm_fit_+3A_...">...</code></td>
<td>
<p>other arguments passed to specific methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>glm_fit</code> object with parameter estimates, number of iterations,
etc.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+glm_fit.tensorflow.tensor">glm_fit.tensorflow.tensor()</a></code>
</p>

<hr>
<h2 id='glm_fit_one_step'>Runs one Fisher scoring step</h2><span id='topic+glm_fit_one_step'></span>

<h3>Description</h3>

<p>Runs one Fisher scoring step
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glm_fit_one_step(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glm_fit_one_step_+3A_x">x</code></td>
<td>
<p>float-like, matrix-shaped Tensor where each row represents a sample's
features.</p>
</td></tr>
<tr><td><code id="glm_fit_one_step_+3A_...">...</code></td>
<td>
<p>other arguments passed to specific methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>glm_fit</code> object with parameter estimates, number of iterations,
etc.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+glm_fit_one_step.tensorflow.tensor">glm_fit_one_step.tensorflow.tensor()</a></code>
</p>

<hr>
<h2 id='glm_fit_one_step.tensorflow.tensor'>Runs one Fisher Scoring step</h2><span id='topic+glm_fit_one_step.tensorflow.tensor'></span>

<h3>Description</h3>

<p>Runs one Fisher Scoring step
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tensorflow.tensor'
glm_fit_one_step(
  x,
  response,
  model,
  model_coefficients_start = NULL,
  predicted_linear_response_start = NULL,
  l2_regularizer = NULL,
  dispersion = NULL,
  offset = NULL,
  learning_rate = NULL,
  fast_unsafe_numerics = TRUE,
  name = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glm_fit_one_step.tensorflow.tensor_+3A_x">x</code></td>
<td>
<p>float-like, matrix-shaped Tensor where each row represents a sample's
features.</p>
</td></tr>
<tr><td><code id="glm_fit_one_step.tensorflow.tensor_+3A_response">response</code></td>
<td>
<p>vector-shaped Tensor where each element represents a sample's
observed response (to the corresponding row of features). Must have same <code>dtype</code>
as <code>x</code>.</p>
</td></tr>
<tr><td><code id="glm_fit_one_step.tensorflow.tensor_+3A_model">model</code></td>
<td>
<p>a string naming the model (see <a href="#topic+glm_families">glm_families</a>) or a <code>tfp$glm$ExponentialFamily-like</code>
instance which implicitly characterizes a negative log-likelihood loss by specifying
the distribuion's mean, gradient_mean, and variance.</p>
</td></tr>
<tr><td><code id="glm_fit_one_step.tensorflow.tensor_+3A_model_coefficients_start">model_coefficients_start</code></td>
<td>
<p>Optional (batch of) vector-shaped Tensor representing
the initial model coefficients, one for each column in <code>x</code>. Must have same <code>dtype</code>
as model_matrix. Default value: Zeros.</p>
</td></tr>
<tr><td><code id="glm_fit_one_step.tensorflow.tensor_+3A_predicted_linear_response_start">predicted_linear_response_start</code></td>
<td>
<p>Optional Tensor with shape, <code>dtype</code> matching
<code>response</code>; represents offset shifted initial linear predictions based on
<code>model_coefficients_start</code>. Default value: offset if model_coefficients is <code>NULL</code>,
and <code>tf$linalg$matvec(x, model_coefficients_start) + offset</code> otherwise.</p>
</td></tr>
<tr><td><code id="glm_fit_one_step.tensorflow.tensor_+3A_l2_regularizer">l2_regularizer</code></td>
<td>
<p>Optional scalar Tensor representing L2 regularization penalty.
Default: <code>NULL</code> ie. no regularization.</p>
</td></tr>
<tr><td><code id="glm_fit_one_step.tensorflow.tensor_+3A_dispersion">dispersion</code></td>
<td>
<p>Optional (batch of) Tensor representing response dispersion.</p>
</td></tr>
<tr><td><code id="glm_fit_one_step.tensorflow.tensor_+3A_offset">offset</code></td>
<td>
<p>Optional Tensor representing constant shift applied to <code>predicted_linear_response</code>.</p>
</td></tr>
<tr><td><code id="glm_fit_one_step.tensorflow.tensor_+3A_learning_rate">learning_rate</code></td>
<td>
<p>Optional (batch of) scalar Tensor used to dampen iterative progress.
Typically only needed if optimization diverges, should be no larger than 1 and typically
very close to 1. Default value: <code>NULL</code> (i.e., 1).</p>
</td></tr>
<tr><td><code id="glm_fit_one_step.tensorflow.tensor_+3A_fast_unsafe_numerics">fast_unsafe_numerics</code></td>
<td>
<p>Optional Python bool indicating if faster, less numerically
accurate methods can be employed for computing the weighted least-squares solution. Default
value: TRUE (i.e., &quot;fast but possibly diminished accuracy&quot;).</p>
</td></tr>
<tr><td><code id="glm_fit_one_step.tensorflow.tensor_+3A_name">name</code></td>
<td>
<p>usesed as name prefix to ops created by this function. Default value: &quot;fit&quot;.</p>
</td></tr>
<tr><td><code id="glm_fit_one_step.tensorflow.tensor_+3A_...">...</code></td>
<td>
<p>other arguments passed to specific methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>glm_fit</code> object with parameter estimates, and
number of required steps.
</p>


<h3>See Also</h3>

<p>Other glm_fit: 
<code><a href="#topic+glm_families">glm_families</a></code>,
<code><a href="#topic+glm_fit.tensorflow.tensor">glm_fit.tensorflow.tensor</a>()</code>
</p>

<hr>
<h2 id='glm_fit.tensorflow.tensor'>Runs multiple Fisher scoring steps</h2><span id='topic+glm_fit.tensorflow.tensor'></span>

<h3>Description</h3>

<p>Runs multiple Fisher scoring steps
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tensorflow.tensor'
glm_fit(
  x,
  response,
  model,
  model_coefficients_start = NULL,
  predicted_linear_response_start = NULL,
  l2_regularizer = NULL,
  dispersion = NULL,
  offset = NULL,
  convergence_criteria_fn = NULL,
  learning_rate = NULL,
  fast_unsafe_numerics = TRUE,
  maximum_iterations = NULL,
  name = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glm_fit.tensorflow.tensor_+3A_x">x</code></td>
<td>
<p>float-like, matrix-shaped Tensor where each row represents a sample's
features.</p>
</td></tr>
<tr><td><code id="glm_fit.tensorflow.tensor_+3A_response">response</code></td>
<td>
<p>vector-shaped Tensor where each element represents a sample's
observed response (to the corresponding row of features). Must have same <code>dtype</code>
as <code>x</code>.</p>
</td></tr>
<tr><td><code id="glm_fit.tensorflow.tensor_+3A_model">model</code></td>
<td>
<p>a string naming the model (see <a href="#topic+glm_families">glm_families</a>) or a <code>tfp$glm$ExponentialFamily-like</code>
instance which implicitly characterizes a negative log-likelihood loss by specifying
the distribuion's mean, gradient_mean, and variance.</p>
</td></tr>
<tr><td><code id="glm_fit.tensorflow.tensor_+3A_model_coefficients_start">model_coefficients_start</code></td>
<td>
<p>Optional (batch of) vector-shaped Tensor representing
the initial model coefficients, one for each column in <code>x</code>. Must have same <code>dtype</code>
as model_matrix. Default value: Zeros.</p>
</td></tr>
<tr><td><code id="glm_fit.tensorflow.tensor_+3A_predicted_linear_response_start">predicted_linear_response_start</code></td>
<td>
<p>Optional Tensor with shape, <code>dtype</code> matching
<code>response</code>; represents offset shifted initial linear predictions based on
<code>model_coefficients_start</code>. Default value: offset if model_coefficients is <code>NULL</code>,
and <code>tf$linalg$matvec(x, model_coefficients_start) + offset</code> otherwise.</p>
</td></tr>
<tr><td><code id="glm_fit.tensorflow.tensor_+3A_l2_regularizer">l2_regularizer</code></td>
<td>
<p>Optional scalar Tensor representing L2 regularization penalty.
Default: <code>NULL</code> ie. no regularization.</p>
</td></tr>
<tr><td><code id="glm_fit.tensorflow.tensor_+3A_dispersion">dispersion</code></td>
<td>
<p>Optional (batch of) Tensor representing response dispersion.</p>
</td></tr>
<tr><td><code id="glm_fit.tensorflow.tensor_+3A_offset">offset</code></td>
<td>
<p>Optional Tensor representing constant shift applied to <code>predicted_linear_response</code>.</p>
</td></tr>
<tr><td><code id="glm_fit.tensorflow.tensor_+3A_convergence_criteria_fn">convergence_criteria_fn</code></td>
<td>
<p>callable taking: <code>is_converged_previous</code>, <code>iter_</code>,
<code>model_coefficients_previous</code>, <code>predicted_linear_response_previous</code>, <code>model_coefficients_next</code>,
<code>predicted_linear_response_next</code>, <code>response</code>, <code>model</code>, <code>dispersion</code> and returning
a logical Tensor indicating that Fisher scoring has converged.</p>
</td></tr>
<tr><td><code id="glm_fit.tensorflow.tensor_+3A_learning_rate">learning_rate</code></td>
<td>
<p>Optional (batch of) scalar Tensor used to dampen iterative progress.
Typically only needed if optimization diverges, should be no larger than 1 and typically
very close to 1. Default value: <code>NULL</code> (i.e., 1).</p>
</td></tr>
<tr><td><code id="glm_fit.tensorflow.tensor_+3A_fast_unsafe_numerics">fast_unsafe_numerics</code></td>
<td>
<p>Optional Python bool indicating if faster, less numerically
accurate methods can be employed for computing the weighted least-squares solution. Default
value: TRUE (i.e., &quot;fast but possibly diminished accuracy&quot;).</p>
</td></tr>
<tr><td><code id="glm_fit.tensorflow.tensor_+3A_maximum_iterations">maximum_iterations</code></td>
<td>
<p>Optional maximum number of iterations of Fisher scoring to run;
&quot;and-ed&quot; with result of <code>convergence_criteria_fn</code>. Default value: <code>NULL</code> (i.e., infinity).</p>
</td></tr>
<tr><td><code id="glm_fit.tensorflow.tensor_+3A_name">name</code></td>
<td>
<p>usesed as name prefix to ops created by this function. Default value: &quot;fit&quot;.</p>
</td></tr>
<tr><td><code id="glm_fit.tensorflow.tensor_+3A_...">...</code></td>
<td>
<p>other arguments passed to specific methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>glm_fit</code> object with parameter estimates, and
number of required steps.
</p>


<h3>See Also</h3>

<p>Other glm_fit: 
<code><a href="#topic+glm_families">glm_families</a></code>,
<code><a href="#topic+glm_fit_one_step.tensorflow.tensor">glm_fit_one_step.tensorflow.tensor</a>()</code>
</p>

<hr>
<h2 id='initializer_blockwise'>Blockwise Initializer</h2><span id='topic+initializer_blockwise'></span>

<h3>Description</h3>

<p>Initializer which concats other intializers
</p>


<h3>Usage</h3>

<pre><code class='language-R'>initializer_blockwise(initializers, sizes, validate_args = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="initializer_blockwise_+3A_initializers">initializers</code></td>
<td>
<p>list of Keras initializers, eg: <code><a href="keras.html#topic+initializer_glorot_uniform">keras::initializer_glorot_uniform()</a></code>
or <code>initializer_constant()</code>.</p>
</td></tr>
<tr><td><code id="initializer_blockwise_+3A_sizes">sizes</code></td>
<td>
<p>list of integers scalars representing the number of elements associated
with each initializer in <code>initializers</code>.</p>
</td></tr>
<tr><td><code id="initializer_blockwise_+3A_validate_args">validate_args</code></td>
<td>
<p>bool indicating we should do (possibly expensive) graph-time
assertions, if necessary.
</p>
<p>@return Initializer which concats other intializers</p>
</td></tr>
</table>

<hr>
<h2 id='install_tfprobability'>Installs TensorFlow Probability</h2><span id='topic+install_tfprobability'></span>

<h3>Description</h3>

<p>Installs TensorFlow Probability
</p>


<h3>Usage</h3>

<pre><code class='language-R'>install_tfprobability(
  method = c("auto", "virtualenv", "conda"),
  conda = "auto",
  version = "default",
  tensorflow = "default",
  extra_packages = NULL,
  ...,
  pip_ignore_installed = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="install_tfprobability_+3A_method">method</code></td>
<td>
<p>Installation method. By default, &quot;auto&quot; automatically finds a
method that will work in the local environment. Change the default to force
a specific installation method. Note that the &quot;virtualenv&quot; method is not
available on Windows.</p>
</td></tr>
<tr><td><code id="install_tfprobability_+3A_conda">conda</code></td>
<td>
<p>The path to a <code>conda</code> executable. Use <code>"auto"</code> to allow
<code>reticulate</code> to automatically find an appropriate <code>conda</code> binary.
See <strong>Finding Conda</strong> and <code><a href="reticulate.html#topic+conda_binary">conda_binary()</a></code> for more details.</p>
</td></tr>
<tr><td><code id="install_tfprobability_+3A_version">version</code></td>
<td>
<p>TensorFlow version to install. Valid values include:
</p>

<ul>
<li> <p><code>"default"</code> installs  2.9
</p>
</li>
<li> <p><code>"release"</code> installs the latest release version of tensorflow (which may
be incompatible with the current version of the R package)
</p>
</li>
<li><p> A version specification like <code>"2.4"</code> or <code>"2.4.0"</code>. Note that if the patch
version is not supplied, the latest patch release is installed (e.g.,
<code>"2.4"</code> today installs version &quot;2.4.2&quot;)
</p>
</li>
<li> <p><code>nightly</code> for the latest available nightly build.
</p>
</li>
<li><p> To any specification, you can append &quot;-cpu&quot; to install the cpu version
only of the package (e.g., <code>"2.4-cpu"</code>)
</p>
</li>
<li><p> The full URL or path to a installer binary or python *.whl file.
</p>
</li></ul>
</td></tr>
<tr><td><code id="install_tfprobability_+3A_tensorflow">tensorflow</code></td>
<td>
<p>Synonym for <code>version</code>. Maintained for backwards.</p>
</td></tr>
<tr><td><code id="install_tfprobability_+3A_extra_packages">extra_packages</code></td>
<td>
<p>Additional Python packages to install along with
TensorFlow.</p>
</td></tr>
<tr><td><code id="install_tfprobability_+3A_...">...</code></td>
<td>
<p>other arguments passed to <code><a href="reticulate.html#topic+conda-tools">reticulate::conda_install()</a></code> or
<code><a href="reticulate.html#topic+virtualenv-tools">reticulate::virtualenv_install()</a></code>, depending on the <code>method</code> used.</p>
</td></tr>
<tr><td><code id="install_tfprobability_+3A_pip_ignore_installed">pip_ignore_installed</code></td>
<td>
<p>Whether pip should ignore installed python
packages and reinstall all already installed python packages. This defaults
to <code>TRUE</code>, to ensure that TensorFlow dependencies like NumPy are compatible
with the prebuilt TensorFlow binaries.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>invisible
</p>

<hr>
<h2 id='layer_autoregressive'>Masked Autoencoder for Distribution Estimation</h2><span id='topic+layer_autoregressive'></span>

<h3>Description</h3>

<p><code>layer_autoregressive</code> takes as input a Tensor of shape <code style="white-space: pre;">&#8288;[..., event_size]&#8288;</code>
and returns a Tensor of shape <code style="white-space: pre;">&#8288;[..., event_size, params]&#8288;</code>.
The output satisfies the autoregressive property.  That is, the layer is
configured with some permutation <code>ord</code> of <code style="white-space: pre;">&#8288;{0, ..., event_size-1}&#8288;</code> (i.e., an
ordering of the input dimensions), and the output <code>output[batch_idx, i, ...]</code>
for input dimension <code>i</code> depends only on inputs <code>x[batch_idx, j]</code> where
<code>ord(j) &lt; ord(i)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_autoregressive(
  object,
  params,
  event_shape = NULL,
  hidden_units = NULL,
  input_order = "left-to-right",
  hidden_degrees = "equal",
  activation = NULL,
  use_bias = TRUE,
  kernel_initializer = "glorot_uniform",
  validate_args = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_autoregressive_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_autoregressive_+3A_params">params</code></td>
<td>
<p>integer specifying the number of parameters to output per input.</p>
</td></tr>
<tr><td><code id="layer_autoregressive_+3A_event_shape">event_shape</code></td>
<td>
<p><code>list</code>-like of positive integers (or a single int),
specifying the shape of the input to this layer, which is also the
event_shape of the distribution parameterized by this layer.  Currently
only rank-1 shapes are supported.  That is, event_shape must be a single
integer.  If not specified, the event shape is inferred when this layer
is first called or built.</p>
</td></tr>
<tr><td><code id="layer_autoregressive_+3A_hidden_units">hidden_units</code></td>
<td>
<p><code>list</code>-like of non-negative integers, specifying
the number of units in each hidden layer.</p>
</td></tr>
<tr><td><code id="layer_autoregressive_+3A_input_order">input_order</code></td>
<td>
<p>Order of degrees to the input units: 'random',
'left-to-right', 'right-to-left', or an array of an explicit order. For
example, 'left-to-right' builds an autoregressive model:
<code style="white-space: pre;">&#8288;p(x) = p(x1) p(x2 | x1) ... p(xD | x&lt;D)&#8288;</code>.  Default: 'left-to-right'.</p>
</td></tr>
<tr><td><code id="layer_autoregressive_+3A_hidden_degrees">hidden_degrees</code></td>
<td>
<p>Method for assigning degrees to the hidden units:
'equal', 'random'.  If 'equal', hidden units in each layer are allocated
equally (up to a remainder term) to each degree.  Default: 'equal'.</p>
</td></tr>
<tr><td><code id="layer_autoregressive_+3A_activation">activation</code></td>
<td>
<p>An activation function.  See <code>keras::layer_dense</code>. Default: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="layer_autoregressive_+3A_use_bias">use_bias</code></td>
<td>
<p>Whether or not the dense layers constructed in this layer
should have a bias term.  See <code>keras::layer_dense</code>.  Default: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="layer_autoregressive_+3A_kernel_initializer">kernel_initializer</code></td>
<td>
<p>Initializer for the kernel weights matrix.  Default: 'glorot_uniform'.</p>
</td></tr>
<tr><td><code id="layer_autoregressive_+3A_validate_args">validate_args</code></td>
<td>
<p><code>logical</code>, default <code>FALSE</code>. When <code>TRUE</code>, layer
parameters are checked for validity despite possibly degrading runtime
performance. When <code>FALSE</code> invalid inputs may silently render incorrect outputs.</p>
</td></tr>
<tr><td><code id="layer_autoregressive_+3A_...">...</code></td>
<td>
<p>Additional keyword arguments passed to the <code>keras::layer_dense</code> constructed by this layer.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The autoregressive property allows us to use
<code>output[batch_idx, i]</code> to parameterize conditional distributions:
<code style="white-space: pre;">&#8288;p(x[batch_idx, i] | x[batch_idx, ] for ord(j) &lt; ord(i))&#8288;</code>
which give us a tractable distribution over input <code>x[batch_idx]</code>:
</p>
<p><code style="white-space: pre;">&#8288;p(x[batch_idx]) = prod_i p(x[batch_idx, ord(i)] | x[batch_idx, ord(0:i)])&#8288;</code>
</p>
<p>For example, when <code>params</code> is 2, the output of the layer can parameterize
the location and log-scale of an autoregressive Gaussian distribution.
</p>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>See Also</h3>

<p>Other layers: 
<code><a href="#topic+layer_conv_1d_flipout">layer_conv_1d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_1d_reparameterization">layer_conv_1d_reparameterization</a>()</code>,
<code><a href="#topic+layer_conv_2d_flipout">layer_conv_2d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_2d_reparameterization">layer_conv_2d_reparameterization</a>()</code>,
<code><a href="#topic+layer_conv_3d_flipout">layer_conv_3d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_3d_reparameterization">layer_conv_3d_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_flipout">layer_dense_flipout</a>()</code>,
<code><a href="#topic+layer_dense_local_reparameterization">layer_dense_local_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_reparameterization">layer_dense_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_variational">layer_dense_variational</a>()</code>,
<code><a href="#topic+layer_variable">layer_variable</a>()</code>
</p>

<hr>
<h2 id='layer_autoregressive_transform'>An autoregressive normalizing flow layer, given a <code>layer_autoregressive</code>.</h2><span id='topic+layer_autoregressive_transform'></span>

<h3>Description</h3>

<p>Following <a href="https://arxiv.org/abs/1705.07057">Papamakarios et al. (2017)</a>, given
an autoregressive model <code class="reqn">p(x)</code> with conditional distributions in the location-scale
family, we can construct a normalizing flow for <code class="reqn">p(x)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_autoregressive_transform(object, made, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_autoregressive_transform_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_autoregressive_transform_+3A_made">made</code></td>
<td>
<p>A <code>Made</code> layer, which must output two parameters for each input.</p>
</td></tr>
<tr><td><code id="layer_autoregressive_transform_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to Keras Layer.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Specifically, suppose made is a <code style="white-space: pre;">&#8288;[layer_autoregressive()]&#8288;</code> &ndash; a layer implementing
a Masked Autoencoder for Distribution Estimation (MADE) &ndash; that computes location
and log-scale parameters <code class="reqn">made(x)[i]</code> for each input <code class="reqn">x[i]</code>. Then we can represent
the autoregressive model <code class="reqn">p(x)</code> as <code class="reqn">x = f(u)</code> where <code class="reqn">u</code> is drawn
from from some base distribution and where <code class="reqn">f</code> is an invertible and
differentiable function (i.e., a Bijector) and <code class="reqn">f^{-1}(x)</code> is defined by:
</p>
<div class="sourceCode"><pre>library(tensorflow)
library(zeallot)
f_inverse &lt;- function(x) {
  c(shift, log_scale) %&lt;-% tf$unstack(made(x), 2, axis = -1L)
  (x - shift) * tf$math$exp(-log_scale)
}
</pre></div>
<p>Given a <code><a href="#topic+layer_autoregressive">layer_autoregressive()</a></code> made, a <code><a href="#topic+layer_autoregressive_transform">layer_autoregressive_transform()</a></code>
transforms an input <code style="white-space: pre;">&#8288;tfd_*&#8288;</code> <code class="reqn">p(u)</code> to an output <code style="white-space: pre;">&#8288;tfd_*&#8288;</code> <code class="reqn">p(x)</code> where
<code class="reqn">x = f(u)</code>.
</p>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>References</h3>

<p><a href="https://arxiv.org/abs/1705.07057">Papamakarios et al. (2017)</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow()</a></code> and <code><a href="#topic+layer_autoregressive">layer_autoregressive()</a></code>
</p>

<hr>
<h2 id='layer_categorical_mixture_of_one_hot_categorical'>A OneHotCategorical mixture Keras layer from <code>k * (1 + d)</code> params.</h2><span id='topic+layer_categorical_mixture_of_one_hot_categorical'></span>

<h3>Description</h3>

<p><code>k</code> (i.e., <code>num_components</code>) represents the number of component
<code>OneHotCategorical</code> distributions and <code>d</code> (i.e., <code>event_size</code>) represents the
number of categories within each <code>OneHotCategorical</code> distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_categorical_mixture_of_one_hot_categorical(
  object,
  event_size,
  num_components,
  convert_to_tensor_fn = tfp$distributions$Distribution$sample,
  sample_dtype = NULL,
  validate_args = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_categorical_mixture_of_one_hot_categorical_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_categorical_mixture_of_one_hot_categorical_+3A_event_size">event_size</code></td>
<td>
<p>Scalar <code>integer</code> representing the size of single draw from this distribution.</p>
</td></tr>
<tr><td><code id="layer_categorical_mixture_of_one_hot_categorical_+3A_num_components">num_components</code></td>
<td>
<p>Scalar <code>integer</code> representing the number of mixture
components. Must be at least 1. (If <code>num_components=1</code>, it's more
efficient to use the <code>OneHotCategorical</code> layer.)</p>
</td></tr>
<tr><td><code id="layer_categorical_mixture_of_one_hot_categorical_+3A_convert_to_tensor_fn">convert_to_tensor_fn</code></td>
<td>
<p>A callable that takes a tfd$Distribution instance and returns a
tf$Tensor-like object. Default value: <code>tfd$distributions$Distribution$sample</code>.</p>
</td></tr>
<tr><td><code id="layer_categorical_mixture_of_one_hot_categorical_+3A_sample_dtype">sample_dtype</code></td>
<td>
<p><code>dtype</code> of samples produced by this distribution.
Default value: <code>NULL</code> (i.e., previous layer's <code>dtype</code>).</p>
</td></tr>
<tr><td><code id="layer_categorical_mixture_of_one_hot_categorical_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="layer_categorical_mixture_of_one_hot_categorical_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>args</code> of <code>keras::create_layer</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Typical choices for <code>convert_to_tensor_fn</code> include:
</p>

<ul>
<li> <p><code>tfp$distributions$Distribution$sample</code>
</p>
</li>
<li> <p><code>tfp$distributions$Distribution$mean</code>
</p>
</li>
<li> <p><code>tfp$distributions$Distribution$mode</code>
</p>
</li></ul>



<h3>Value</h3>

<p>a Keras layer
</p>


<h3>See Also</h3>

<p>For an example how to use in a Keras model, see <code><a href="#topic+layer_independent_normal">layer_independent_normal()</a></code>.
</p>
<p>Other distribution_layers: 
<code><a href="#topic+layer_distribution_lambda">layer_distribution_lambda</a>()</code>,
<code><a href="#topic+layer_independent_bernoulli">layer_independent_bernoulli</a>()</code>,
<code><a href="#topic+layer_independent_logistic">layer_independent_logistic</a>()</code>,
<code><a href="#topic+layer_independent_normal">layer_independent_normal</a>()</code>,
<code><a href="#topic+layer_independent_poisson">layer_independent_poisson</a>()</code>,
<code><a href="#topic+layer_kl_divergence_add_loss">layer_kl_divergence_add_loss</a>()</code>,
<code><a href="#topic+layer_kl_divergence_regularizer">layer_kl_divergence_regularizer</a>()</code>,
<code><a href="#topic+layer_mixture_logistic">layer_mixture_logistic</a>()</code>,
<code><a href="#topic+layer_mixture_normal">layer_mixture_normal</a>()</code>,
<code><a href="#topic+layer_mixture_same_family">layer_mixture_same_family</a>()</code>,
<code><a href="#topic+layer_multivariate_normal_tri_l">layer_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+layer_one_hot_categorical">layer_one_hot_categorical</a>()</code>
</p>

<hr>
<h2 id='layer_conv_1d_flipout'>1D convolution layer (e.g. temporal convolution) with Flipout</h2><span id='topic+layer_conv_1d_flipout'></span>

<h3>Description</h3>

<p>This layer creates a convolution kernel that is convolved
(actually cross-correlated) with the layer input to produce a tensor of
outputs. It may also include a bias addition and activation function
on the outputs. It assumes the <code>kernel</code> and/or <code>bias</code> are drawn from distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_conv_1d_flipout(
  object,
  filters,
  kernel_size,
  strides = 1,
  padding = "valid",
  data_format = "channels_last",
  dilation_rate = 1,
  activation = NULL,
  activity_regularizer = NULL,
  trainable = TRUE,
  kernel_posterior_fn = tfp$layers$util$default_mean_field_normal_fn(),
  kernel_posterior_tensor_fn = function(d) d %&gt;% tfd_sample(),
  kernel_prior_fn = tfp$layers$util$default_multivariate_normal_fn,
  kernel_divergence_fn = function(q, p, ignore) tfd_kl_divergence(q, p),
  bias_posterior_fn = tfp$layers$util$default_mean_field_normal_fn(is_singular = TRUE),
  bias_posterior_tensor_fn = function(d) d %&gt;% tfd_sample(),
  bias_prior_fn = NULL,
  bias_divergence_fn = function(q, p, ignore) tfd_kl_divergence(q, p),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_conv_1d_flipout_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_conv_1d_flipout_+3A_filters">filters</code></td>
<td>
<p>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution).</p>
</td></tr>
<tr><td><code id="layer_conv_1d_flipout_+3A_kernel_size">kernel_size</code></td>
<td>
<p>An integer or list of a single integer, specifying the
length of the 1D convolution window.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_flipout_+3A_strides">strides</code></td>
<td>
<p>An integer or list of a single integer,
specifying the stride length of the convolution.
Specifying any stride value != 1 is incompatible with specifying
any <code>dilation_rate</code> value != 1.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_flipout_+3A_padding">padding</code></td>
<td>
<p>One of <code>"valid"</code> or <code>"same"</code> (case-insensitive).</p>
</td></tr>
<tr><td><code id="layer_conv_1d_flipout_+3A_data_format">data_format</code></td>
<td>
<p>A string, one of <code>channels_last</code> (default) or
<code>channels_first</code>. The ordering of the dimensions in the inputs.
<code>channels_last</code> corresponds to inputs with shape <code style="white-space: pre;">&#8288;(batch, length, channels)&#8288;</code> while <code>channels_first</code> corresponds to inputs with shape
<code style="white-space: pre;">&#8288;(batch, channels, length)&#8288;</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_flipout_+3A_dilation_rate">dilation_rate</code></td>
<td>
<p>An integer or tuple/list of a single integer, specifying
the dilation rate to use for dilated convolution.
Currently, specifying any <code>dilation_rate</code> value != 1 is
incompatible with specifying any <code>strides</code> value != 1.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_flipout_+3A_activation">activation</code></td>
<td>
<p>Activation function. Set it to None to maintain a linear activation.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_flipout_+3A_activity_regularizer">activity_regularizer</code></td>
<td>
<p>Regularizer function for the output.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_flipout_+3A_trainable">trainable</code></td>
<td>
<p>Whether the layer weights will be updated during training.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_flipout_+3A_kernel_posterior_fn">kernel_posterior_fn</code></td>
<td>
<p>Function which creates <code>tfd$Distribution</code> instance representing the surrogate
posterior of the <code>kernel</code> parameter. Default value: <code>default_mean_field_normal_fn()</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_flipout_+3A_kernel_posterior_tensor_fn">kernel_posterior_tensor_fn</code></td>
<td>
<p>Function which takes a <code>tfd$Distribution</code> instance and returns a representative
value. Default value: <code>function(d) d %&gt;% tfd_sample()</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_flipout_+3A_kernel_prior_fn">kernel_prior_fn</code></td>
<td>
<p>Function which creates <code>tfd$Distribution</code> instance. See <code>default_mean_field_normal_fn</code> docstring for required
parameter signature. Default value: <code>tfd_normal(loc = 0, scale = 1)</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_flipout_+3A_kernel_divergence_fn">kernel_divergence_fn</code></td>
<td>
<p>Function which takes the surrogate posterior distribution, prior distribution and random variate
sample(s) from the surrogate posterior and computes or approximates the KL divergence. The
distributions are <code>tfd$Distribution</code>-like instances and the sample is a <code>Tensor</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_flipout_+3A_bias_posterior_fn">bias_posterior_fn</code></td>
<td>
<p>Function which creates a <code>tfd$Distribution</code> instance representing the surrogate
posterior of the <code>bias</code> parameter. Default value:  <code>default_mean_field_normal_fn(is_singular = TRUE)</code> (which creates an
instance of <code>tfd_deterministic</code>).</p>
</td></tr>
<tr><td><code id="layer_conv_1d_flipout_+3A_bias_posterior_tensor_fn">bias_posterior_tensor_fn</code></td>
<td>
<p>Function which takes a <code>tfd$Distribution</code> instance and returns a representative
value. Default value: <code>function(d) d %&gt;% tfd_sample()</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_flipout_+3A_bias_prior_fn">bias_prior_fn</code></td>
<td>
<p>Function which creates <code>tfd</code> instance. See <code>default_mean_field_normal_fn</code> docstring for required parameter
signature. Default value: <code>NULL</code> (no prior, no variational inference)</p>
</td></tr>
<tr><td><code id="layer_conv_1d_flipout_+3A_bias_divergence_fn">bias_divergence_fn</code></td>
<td>
<p>Function which takes the surrogate posterior distribution, prior distribution and random variate sample(s)
from the surrogate posterior and computes or approximates the KL divergence. The
distributions are <code>tfd$Distribution</code>-like instances and the sample is a <code>Tensor</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_flipout_+3A_...">...</code></td>
<td>
<p>Additional keyword arguments passed to the <code>keras::layer_dense</code> constructed by this layer.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This layer implements the Bayesian variational inference analogue to
a dense layer by assuming the <code>kernel</code> and/or the <code>bias</code> are drawn
from distributions.
</p>
<p>By default, the layer implements a stochastic forward pass via sampling from the kernel and bias posteriors,
</p>
<div class="sourceCode"><pre>outputs = f(inputs; kernel, bias), kernel, bias ~ posterior
</pre></div>
<p>where f denotes the layer's calculation. It uses the Flipout
estimator (Wen et al., 2018), which performs a Monte Carlo approximation
of the distribution integrating over the <code>kernel</code> and <code>bias</code>. Flipout uses
roughly twice as many floating point operations as the reparameterization
estimator but has the advantage of significantly lower variance.
</p>
<p>The arguments permit separate specification of the surrogate posterior
(<code>q(W|x)</code>), prior (<code>p(W)</code>), and divergence for both the <code>kernel</code> and <code>bias</code>
distributions.
</p>
<p>Upon being built, this layer adds losses (accessible via the <code>losses</code>
property) representing the divergences of <code>kernel</code> and/or <code>bias</code> surrogate
posteriors and their respective priors. When doing minibatch stochastic
optimization, make sure to scale this loss such that it is applied just once
per epoch (e.g. if <code>kl</code> is the sum of <code>losses</code> for each element of the batch,
you should pass <code>kl / num_examples_per_epoch</code> to your optimizer).
You can access the <code>kernel</code> and/or <code>bias</code> posterior and prior distributions
after the layer is built via the <code>kernel_posterior</code>, <code>kernel_prior</code>,
<code>bias_posterior</code> and <code>bias_prior</code> properties.
</p>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1803.04386">Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse. Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches. In <em>International Conference on Learning Representations</em>, 2018.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other layers: 
<code><a href="#topic+layer_autoregressive">layer_autoregressive</a>()</code>,
<code><a href="#topic+layer_conv_1d_reparameterization">layer_conv_1d_reparameterization</a>()</code>,
<code><a href="#topic+layer_conv_2d_flipout">layer_conv_2d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_2d_reparameterization">layer_conv_2d_reparameterization</a>()</code>,
<code><a href="#topic+layer_conv_3d_flipout">layer_conv_3d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_3d_reparameterization">layer_conv_3d_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_flipout">layer_dense_flipout</a>()</code>,
<code><a href="#topic+layer_dense_local_reparameterization">layer_dense_local_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_reparameterization">layer_dense_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_variational">layer_dense_variational</a>()</code>,
<code><a href="#topic+layer_variable">layer_variable</a>()</code>
</p>

<hr>
<h2 id='layer_conv_1d_reparameterization'>1D convolution layer (e.g. temporal convolution).</h2><span id='topic+layer_conv_1d_reparameterization'></span>

<h3>Description</h3>

<p>This layer creates a convolution kernel that is convolved
(actually cross-correlated) with the layer input to produce a tensor of
outputs. It may also include a bias addition and activation function
on the outputs. It assumes the <code>kernel</code> and/or <code>bias</code> are drawn from distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_conv_1d_reparameterization(
  object,
  filters,
  kernel_size,
  strides = 1,
  padding = "valid",
  data_format = "channels_last",
  dilation_rate = 1,
  activation = NULL,
  activity_regularizer = NULL,
  trainable = TRUE,
  kernel_posterior_fn = tfp$layers$util$default_mean_field_normal_fn(),
  kernel_posterior_tensor_fn = function(d) d %&gt;% tfd_sample(),
  kernel_prior_fn = tfp$layers$util$default_multivariate_normal_fn,
  kernel_divergence_fn = function(q, p, ignore) tfd_kl_divergence(q, p),
  bias_posterior_fn = tfp$layers$util$default_mean_field_normal_fn(is_singular = TRUE),
  bias_posterior_tensor_fn = function(d) d %&gt;% tfd_sample(),
  bias_prior_fn = NULL,
  bias_divergence_fn = function(q, p, ignore) tfd_kl_divergence(q, p),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_conv_1d_reparameterization_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_conv_1d_reparameterization_+3A_filters">filters</code></td>
<td>
<p>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution).</p>
</td></tr>
<tr><td><code id="layer_conv_1d_reparameterization_+3A_kernel_size">kernel_size</code></td>
<td>
<p>An integer or list of a single integer, specifying the
length of the 1D convolution window.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_reparameterization_+3A_strides">strides</code></td>
<td>
<p>An integer or list of a single integer,
specifying the stride length of the convolution.
Specifying any stride value != 1 is incompatible with specifying
any <code>dilation_rate</code> value != 1.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_reparameterization_+3A_padding">padding</code></td>
<td>
<p>One of <code>"valid"</code> or <code>"same"</code> (case-insensitive).</p>
</td></tr>
<tr><td><code id="layer_conv_1d_reparameterization_+3A_data_format">data_format</code></td>
<td>
<p>A string, one of <code>channels_last</code> (default) or
<code>channels_first</code>. The ordering of the dimensions in the inputs.
<code>channels_last</code> corresponds to inputs with shape <code style="white-space: pre;">&#8288;(batch, length, channels)&#8288;</code> while <code>channels_first</code> corresponds to inputs with shape
<code style="white-space: pre;">&#8288;(batch, channels, length)&#8288;</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_reparameterization_+3A_dilation_rate">dilation_rate</code></td>
<td>
<p>An integer or tuple/list of a single integer, specifying
the dilation rate to use for dilated convolution.
Currently, specifying any <code>dilation_rate</code> value != 1 is
incompatible with specifying any <code>strides</code> value != 1.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_reparameterization_+3A_activation">activation</code></td>
<td>
<p>Activation function. Set it to None to maintain a linear activation.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_reparameterization_+3A_activity_regularizer">activity_regularizer</code></td>
<td>
<p>Regularizer function for the output.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_reparameterization_+3A_trainable">trainable</code></td>
<td>
<p>Whether the layer weights will be updated during training.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_reparameterization_+3A_kernel_posterior_fn">kernel_posterior_fn</code></td>
<td>
<p>Function which creates <code>tfd$Distribution</code> instance representing the surrogate
posterior of the <code>kernel</code> parameter. Default value: <code>default_mean_field_normal_fn()</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_reparameterization_+3A_kernel_posterior_tensor_fn">kernel_posterior_tensor_fn</code></td>
<td>
<p>Function which takes a <code>tfd$Distribution</code> instance and returns a representative
value. Default value: <code>function(d) d %&gt;% tfd_sample()</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_reparameterization_+3A_kernel_prior_fn">kernel_prior_fn</code></td>
<td>
<p>Function which creates <code>tfd$Distribution</code> instance. See <code>default_mean_field_normal_fn</code> docstring for required
parameter signature. Default value: <code>tfd_normal(loc = 0, scale = 1)</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_reparameterization_+3A_kernel_divergence_fn">kernel_divergence_fn</code></td>
<td>
<p>Function which takes the surrogate posterior distribution, prior distribution and random variate
sample(s) from the surrogate posterior and computes or approximates the KL divergence. The
distributions are <code>tfd$Distribution</code>-like instances and the sample is a <code>Tensor</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_reparameterization_+3A_bias_posterior_fn">bias_posterior_fn</code></td>
<td>
<p>Function which creates a <code>tfd$Distribution</code> instance representing the surrogate
posterior of the <code>bias</code> parameter. Default value:  <code>default_mean_field_normal_fn(is_singular = TRUE)</code> (which creates an
instance of <code>tfd_deterministic</code>).</p>
</td></tr>
<tr><td><code id="layer_conv_1d_reparameterization_+3A_bias_posterior_tensor_fn">bias_posterior_tensor_fn</code></td>
<td>
<p>Function which takes a <code>tfd$Distribution</code> instance and returns a representative
value. Default value: <code>function(d) d %&gt;% tfd_sample()</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_reparameterization_+3A_bias_prior_fn">bias_prior_fn</code></td>
<td>
<p>Function which creates <code>tfd</code> instance. See <code>default_mean_field_normal_fn</code> docstring for required parameter
signature. Default value: <code>NULL</code> (no prior, no variational inference)</p>
</td></tr>
<tr><td><code id="layer_conv_1d_reparameterization_+3A_bias_divergence_fn">bias_divergence_fn</code></td>
<td>
<p>Function which takes the surrogate posterior distribution, prior distribution and random variate sample(s)
from the surrogate posterior and computes or approximates the KL divergence. The
distributions are <code>tfd$Distribution</code>-like instances and the sample is a <code>Tensor</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_1d_reparameterization_+3A_...">...</code></td>
<td>
<p>Additional keyword arguments passed to the <code>keras::layer_dense</code> constructed by this layer.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This layer implements the Bayesian variational inference analogue to
a dense layer by assuming the <code>kernel</code> and/or the <code>bias</code> are drawn
from distributions.
</p>
<p>By default, the layer implements a stochastic forward pass via sampling from the kernel and bias posteriors,
</p>
<div class="sourceCode"><pre>outputs = f(inputs; kernel, bias), kernel, bias ~ posterior
</pre></div>
<p>where f denotes the layer's calculation. It uses the reparameterization
estimator (Kingma and Welling, 2014), which performs a Monte Carlo
approximation of the distribution integrating over the <code>kernel</code> and <code>bias</code>.
</p>
<p>The arguments permit separate specification of the surrogate posterior
(<code>q(W|x)</code>), prior (<code>p(W)</code>), and divergence for both the <code>kernel</code> and <code>bias</code>
distributions.
</p>
<p>Upon being built, this layer adds losses (accessible via the <code>losses</code>
property) representing the divergences of <code>kernel</code> and/or <code>bias</code> surrogate
posteriors and their respective priors. When doing minibatch stochastic
optimization, make sure to scale this loss such that it is applied just once
per epoch (e.g. if <code>kl</code> is the sum of <code>losses</code> for each element of the batch,
you should pass <code>kl / num_examples_per_epoch</code> to your optimizer).
You can access the <code>kernel</code> and/or <code>bias</code> posterior and prior distributions
after the layer is built via the <code>kernel_posterior</code>, <code>kernel_prior</code>,
<code>bias_posterior</code> and <code>bias_prior</code> properties.
</p>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1312.6114">Diederik Kingma and Max Welling. Auto-Encoding Variational Bayes. In <em>International Conference on Learning Representations</em>, 2014.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other layers: 
<code><a href="#topic+layer_autoregressive">layer_autoregressive</a>()</code>,
<code><a href="#topic+layer_conv_1d_flipout">layer_conv_1d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_2d_flipout">layer_conv_2d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_2d_reparameterization">layer_conv_2d_reparameterization</a>()</code>,
<code><a href="#topic+layer_conv_3d_flipout">layer_conv_3d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_3d_reparameterization">layer_conv_3d_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_flipout">layer_dense_flipout</a>()</code>,
<code><a href="#topic+layer_dense_local_reparameterization">layer_dense_local_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_reparameterization">layer_dense_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_variational">layer_dense_variational</a>()</code>,
<code><a href="#topic+layer_variable">layer_variable</a>()</code>
</p>

<hr>
<h2 id='layer_conv_2d_flipout'>2D convolution layer (e.g. spatial convolution over images) with Flipout</h2><span id='topic+layer_conv_2d_flipout'></span>

<h3>Description</h3>

<p>This layer creates a convolution kernel that is convolved
(actually cross-correlated) with the layer input to produce a tensor of
outputs. It may also include a bias addition and activation function
on the outputs. It assumes the <code>kernel</code> and/or <code>bias</code> are drawn from distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_conv_2d_flipout(
  object,
  filters,
  kernel_size,
  strides = 1,
  padding = "valid",
  data_format = "channels_last",
  dilation_rate = 1,
  activation = NULL,
  activity_regularizer = NULL,
  trainable = TRUE,
  kernel_posterior_fn = tfp$layers$util$default_mean_field_normal_fn(),
  kernel_posterior_tensor_fn = function(d) d %&gt;% tfd_sample(),
  kernel_prior_fn = tfp$layers$util$default_multivariate_normal_fn,
  kernel_divergence_fn = function(q, p, ignore) tfd_kl_divergence(q, p),
  bias_posterior_fn = tfp$layers$util$default_mean_field_normal_fn(is_singular = TRUE),
  bias_posterior_tensor_fn = function(d) d %&gt;% tfd_sample(),
  bias_prior_fn = NULL,
  bias_divergence_fn = function(q, p, ignore) tfd_kl_divergence(q, p),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_conv_2d_flipout_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_conv_2d_flipout_+3A_filters">filters</code></td>
<td>
<p>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution).</p>
</td></tr>
<tr><td><code id="layer_conv_2d_flipout_+3A_kernel_size">kernel_size</code></td>
<td>
<p>An integer or list of a single integer, specifying the
length of the 1D convolution window.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_flipout_+3A_strides">strides</code></td>
<td>
<p>An integer or list of a single integer,
specifying the stride length of the convolution.
Specifying any stride value != 1 is incompatible with specifying
any <code>dilation_rate</code> value != 1.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_flipout_+3A_padding">padding</code></td>
<td>
<p>One of <code>"valid"</code> or <code>"same"</code> (case-insensitive).</p>
</td></tr>
<tr><td><code id="layer_conv_2d_flipout_+3A_data_format">data_format</code></td>
<td>
<p>A string, one of <code>channels_last</code> (default) or
<code>channels_first</code>. The ordering of the dimensions in the inputs.
<code>channels_last</code> corresponds to inputs with shape <code style="white-space: pre;">&#8288;(batch, length, channels)&#8288;</code> while <code>channels_first</code> corresponds to inputs with shape
<code style="white-space: pre;">&#8288;(batch, channels, length)&#8288;</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_flipout_+3A_dilation_rate">dilation_rate</code></td>
<td>
<p>An integer or tuple/list of a single integer, specifying
the dilation rate to use for dilated convolution.
Currently, specifying any <code>dilation_rate</code> value != 1 is
incompatible with specifying any <code>strides</code> value != 1.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_flipout_+3A_activation">activation</code></td>
<td>
<p>Activation function. Set it to None to maintain a linear activation.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_flipout_+3A_activity_regularizer">activity_regularizer</code></td>
<td>
<p>Regularizer function for the output.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_flipout_+3A_trainable">trainable</code></td>
<td>
<p>Whether the layer weights will be updated during training.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_flipout_+3A_kernel_posterior_fn">kernel_posterior_fn</code></td>
<td>
<p>Function which creates <code>tfd$Distribution</code> instance representing the surrogate
posterior of the <code>kernel</code> parameter. Default value: <code>default_mean_field_normal_fn()</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_flipout_+3A_kernel_posterior_tensor_fn">kernel_posterior_tensor_fn</code></td>
<td>
<p>Function which takes a <code>tfd$Distribution</code> instance and returns a representative
value. Default value: <code>function(d) d %&gt;% tfd_sample()</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_flipout_+3A_kernel_prior_fn">kernel_prior_fn</code></td>
<td>
<p>Function which creates <code>tfd$Distribution</code> instance. See <code>default_mean_field_normal_fn</code> docstring for required
parameter signature. Default value: <code>tfd_normal(loc = 0, scale = 1)</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_flipout_+3A_kernel_divergence_fn">kernel_divergence_fn</code></td>
<td>
<p>Function which takes the surrogate posterior distribution, prior distribution and random variate
sample(s) from the surrogate posterior and computes or approximates the KL divergence. The
distributions are <code>tfd$Distribution</code>-like instances and the sample is a <code>Tensor</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_flipout_+3A_bias_posterior_fn">bias_posterior_fn</code></td>
<td>
<p>Function which creates a <code>tfd$Distribution</code> instance representing the surrogate
posterior of the <code>bias</code> parameter. Default value:  <code>default_mean_field_normal_fn(is_singular = TRUE)</code> (which creates an
instance of <code>tfd_deterministic</code>).</p>
</td></tr>
<tr><td><code id="layer_conv_2d_flipout_+3A_bias_posterior_tensor_fn">bias_posterior_tensor_fn</code></td>
<td>
<p>Function which takes a <code>tfd$Distribution</code> instance and returns a representative
value. Default value: <code>function(d) d %&gt;% tfd_sample()</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_flipout_+3A_bias_prior_fn">bias_prior_fn</code></td>
<td>
<p>Function which creates <code>tfd</code> instance. See <code>default_mean_field_normal_fn</code> docstring for required parameter
signature. Default value: <code>NULL</code> (no prior, no variational inference)</p>
</td></tr>
<tr><td><code id="layer_conv_2d_flipout_+3A_bias_divergence_fn">bias_divergence_fn</code></td>
<td>
<p>Function which takes the surrogate posterior distribution, prior distribution and random variate sample(s)
from the surrogate posterior and computes or approximates the KL divergence. The
distributions are <code>tfd$Distribution</code>-like instances and the sample is a <code>Tensor</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_flipout_+3A_...">...</code></td>
<td>
<p>Additional keyword arguments passed to the <code>keras::layer_dense</code> constructed by this layer.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This layer implements the Bayesian variational inference analogue to
a dense layer by assuming the <code>kernel</code> and/or the <code>bias</code> are drawn
from distributions.
</p>
<p>By default, the layer implements a stochastic forward pass via sampling from the kernel and bias posteriors,
</p>
<div class="sourceCode"><pre>outputs = f(inputs; kernel, bias), kernel, bias ~ posterior
</pre></div>
<p>where f denotes the layer's calculation. It uses the Flipout
estimator (Wen et al., 2018), which performs a Monte Carlo approximation
of the distribution integrating over the <code>kernel</code> and <code>bias</code>. Flipout uses
roughly twice as many floating point operations as the reparameterization
estimator but has the advantage of significantly lower variance.
</p>
<p>The arguments permit separate specification of the surrogate posterior
(<code>q(W|x)</code>), prior (<code>p(W)</code>), and divergence for both the <code>kernel</code> and <code>bias</code>
distributions.
</p>
<p>Upon being built, this layer adds losses (accessible via the <code>losses</code>
property) representing the divergences of <code>kernel</code> and/or <code>bias</code> surrogate
posteriors and their respective priors. When doing minibatch stochastic
optimization, make sure to scale this loss such that it is applied just once
per epoch (e.g. if <code>kl</code> is the sum of <code>losses</code> for each element of the batch,
you should pass <code>kl / num_examples_per_epoch</code> to your optimizer).
You can access the <code>kernel</code> and/or <code>bias</code> posterior and prior distributions
after the layer is built via the <code>kernel_posterior</code>, <code>kernel_prior</code>,
<code>bias_posterior</code> and <code>bias_prior</code> properties.
</p>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1803.04386">Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse. Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches. In <em>International Conference on Learning Representations</em>, 2018.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other layers: 
<code><a href="#topic+layer_autoregressive">layer_autoregressive</a>()</code>,
<code><a href="#topic+layer_conv_1d_flipout">layer_conv_1d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_1d_reparameterization">layer_conv_1d_reparameterization</a>()</code>,
<code><a href="#topic+layer_conv_2d_reparameterization">layer_conv_2d_reparameterization</a>()</code>,
<code><a href="#topic+layer_conv_3d_flipout">layer_conv_3d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_3d_reparameterization">layer_conv_3d_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_flipout">layer_dense_flipout</a>()</code>,
<code><a href="#topic+layer_dense_local_reparameterization">layer_dense_local_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_reparameterization">layer_dense_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_variational">layer_dense_variational</a>()</code>,
<code><a href="#topic+layer_variable">layer_variable</a>()</code>
</p>

<hr>
<h2 id='layer_conv_2d_reparameterization'>2D convolution layer (e.g. spatial convolution over images)</h2><span id='topic+layer_conv_2d_reparameterization'></span>

<h3>Description</h3>

<p>This layer creates a convolution kernel that is convolved
(actually cross-correlated) with the layer input to produce a tensor of
outputs. It may also include a bias addition and activation function
on the outputs. It assumes the <code>kernel</code> and/or <code>bias</code> are drawn from distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_conv_2d_reparameterization(
  object,
  filters,
  kernel_size,
  strides = 1,
  padding = "valid",
  data_format = "channels_last",
  dilation_rate = 1,
  activation = NULL,
  activity_regularizer = NULL,
  trainable = TRUE,
  kernel_posterior_fn = tfp$layers$util$default_mean_field_normal_fn(),
  kernel_posterior_tensor_fn = function(d) d %&gt;% tfd_sample(),
  kernel_prior_fn = tfp$layers$util$default_multivariate_normal_fn,
  kernel_divergence_fn = function(q, p, ignore) tfd_kl_divergence(q, p),
  bias_posterior_fn = tfp$layers$util$default_mean_field_normal_fn(is_singular = TRUE),
  bias_posterior_tensor_fn = function(d) d %&gt;% tfd_sample(),
  bias_prior_fn = NULL,
  bias_divergence_fn = function(q, p, ignore) tfd_kl_divergence(q, p),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_conv_2d_reparameterization_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_conv_2d_reparameterization_+3A_filters">filters</code></td>
<td>
<p>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution).</p>
</td></tr>
<tr><td><code id="layer_conv_2d_reparameterization_+3A_kernel_size">kernel_size</code></td>
<td>
<p>An integer or list of a single integer, specifying the
length of the 1D convolution window.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_reparameterization_+3A_strides">strides</code></td>
<td>
<p>An integer or list of a single integer,
specifying the stride length of the convolution.
Specifying any stride value != 1 is incompatible with specifying
any <code>dilation_rate</code> value != 1.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_reparameterization_+3A_padding">padding</code></td>
<td>
<p>One of <code>"valid"</code> or <code>"same"</code> (case-insensitive).</p>
</td></tr>
<tr><td><code id="layer_conv_2d_reparameterization_+3A_data_format">data_format</code></td>
<td>
<p>A string, one of <code>channels_last</code> (default) or
<code>channels_first</code>. The ordering of the dimensions in the inputs.
<code>channels_last</code> corresponds to inputs with shape <code style="white-space: pre;">&#8288;(batch, length, channels)&#8288;</code> while <code>channels_first</code> corresponds to inputs with shape
<code style="white-space: pre;">&#8288;(batch, channels, length)&#8288;</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_reparameterization_+3A_dilation_rate">dilation_rate</code></td>
<td>
<p>An integer or tuple/list of a single integer, specifying
the dilation rate to use for dilated convolution.
Currently, specifying any <code>dilation_rate</code> value != 1 is
incompatible with specifying any <code>strides</code> value != 1.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_reparameterization_+3A_activation">activation</code></td>
<td>
<p>Activation function. Set it to None to maintain a linear activation.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_reparameterization_+3A_activity_regularizer">activity_regularizer</code></td>
<td>
<p>Regularizer function for the output.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_reparameterization_+3A_trainable">trainable</code></td>
<td>
<p>Whether the layer weights will be updated during training.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_reparameterization_+3A_kernel_posterior_fn">kernel_posterior_fn</code></td>
<td>
<p>Function which creates <code>tfd$Distribution</code> instance representing the surrogate
posterior of the <code>kernel</code> parameter. Default value: <code>default_mean_field_normal_fn()</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_reparameterization_+3A_kernel_posterior_tensor_fn">kernel_posterior_tensor_fn</code></td>
<td>
<p>Function which takes a <code>tfd$Distribution</code> instance and returns a representative
value. Default value: <code>function(d) d %&gt;% tfd_sample()</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_reparameterization_+3A_kernel_prior_fn">kernel_prior_fn</code></td>
<td>
<p>Function which creates <code>tfd$Distribution</code> instance. See <code>default_mean_field_normal_fn</code> docstring for required
parameter signature. Default value: <code>tfd_normal(loc = 0, scale = 1)</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_reparameterization_+3A_kernel_divergence_fn">kernel_divergence_fn</code></td>
<td>
<p>Function which takes the surrogate posterior distribution, prior distribution and random variate
sample(s) from the surrogate posterior and computes or approximates the KL divergence. The
distributions are <code>tfd$Distribution</code>-like instances and the sample is a <code>Tensor</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_reparameterization_+3A_bias_posterior_fn">bias_posterior_fn</code></td>
<td>
<p>Function which creates a <code>tfd$Distribution</code> instance representing the surrogate
posterior of the <code>bias</code> parameter. Default value:  <code>default_mean_field_normal_fn(is_singular = TRUE)</code> (which creates an
instance of <code>tfd_deterministic</code>).</p>
</td></tr>
<tr><td><code id="layer_conv_2d_reparameterization_+3A_bias_posterior_tensor_fn">bias_posterior_tensor_fn</code></td>
<td>
<p>Function which takes a <code>tfd$Distribution</code> instance and returns a representative
value. Default value: <code>function(d) d %&gt;% tfd_sample()</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_reparameterization_+3A_bias_prior_fn">bias_prior_fn</code></td>
<td>
<p>Function which creates <code>tfd</code> instance. See <code>default_mean_field_normal_fn</code> docstring for required parameter
signature. Default value: <code>NULL</code> (no prior, no variational inference)</p>
</td></tr>
<tr><td><code id="layer_conv_2d_reparameterization_+3A_bias_divergence_fn">bias_divergence_fn</code></td>
<td>
<p>Function which takes the surrogate posterior distribution, prior distribution and random variate sample(s)
from the surrogate posterior and computes or approximates the KL divergence. The
distributions are <code>tfd$Distribution</code>-like instances and the sample is a <code>Tensor</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_2d_reparameterization_+3A_...">...</code></td>
<td>
<p>Additional keyword arguments passed to the <code>keras::layer_dense</code> constructed by this layer.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This layer implements the Bayesian variational inference analogue to
a dense layer by assuming the <code>kernel</code> and/or the <code>bias</code> are drawn
from distributions.
</p>
<p>By default, the layer implements a stochastic forward pass via sampling from the kernel and bias posteriors,
</p>
<div class="sourceCode"><pre>outputs = f(inputs; kernel, bias), kernel, bias ~ posterior
</pre></div>
<p>where f denotes the layer's calculation. It uses the reparameterization
estimator (Kingma and Welling, 2014), which performs a Monte Carlo
approximation of the distribution integrating over the <code>kernel</code> and <code>bias</code>.
</p>
<p>The arguments permit separate specification of the surrogate posterior
(<code>q(W|x)</code>), prior (<code>p(W)</code>), and divergence for both the <code>kernel</code> and <code>bias</code>
distributions.
</p>
<p>Upon being built, this layer adds losses (accessible via the <code>losses</code>
property) representing the divergences of <code>kernel</code> and/or <code>bias</code> surrogate
posteriors and their respective priors. When doing minibatch stochastic
optimization, make sure to scale this loss such that it is applied just once
per epoch (e.g. if <code>kl</code> is the sum of <code>losses</code> for each element of the batch,
you should pass <code>kl / num_examples_per_epoch</code> to your optimizer).
You can access the <code>kernel</code> and/or <code>bias</code> posterior and prior distributions
after the layer is built via the <code>kernel_posterior</code>, <code>kernel_prior</code>,
<code>bias_posterior</code> and <code>bias_prior</code> properties.
</p>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1312.6114">Diederik Kingma and Max Welling. Auto-Encoding Variational Bayes. In <em>International Conference on Learning Representations</em>, 2014.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other layers: 
<code><a href="#topic+layer_autoregressive">layer_autoregressive</a>()</code>,
<code><a href="#topic+layer_conv_1d_flipout">layer_conv_1d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_1d_reparameterization">layer_conv_1d_reparameterization</a>()</code>,
<code><a href="#topic+layer_conv_2d_flipout">layer_conv_2d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_3d_flipout">layer_conv_3d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_3d_reparameterization">layer_conv_3d_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_flipout">layer_dense_flipout</a>()</code>,
<code><a href="#topic+layer_dense_local_reparameterization">layer_dense_local_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_reparameterization">layer_dense_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_variational">layer_dense_variational</a>()</code>,
<code><a href="#topic+layer_variable">layer_variable</a>()</code>
</p>

<hr>
<h2 id='layer_conv_3d_flipout'>3D convolution layer (e.g. spatial convolution over volumes) with Flipout</h2><span id='topic+layer_conv_3d_flipout'></span>

<h3>Description</h3>

<p>This layer creates a convolution kernel that is convolved
(actually cross-correlated) with the layer input to produce a tensor of
outputs. It may also include a bias addition and activation function
on the outputs. It assumes the <code>kernel</code> and/or <code>bias</code> are drawn from distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_conv_3d_flipout(
  object,
  filters,
  kernel_size,
  strides = 1,
  padding = "valid",
  data_format = "channels_last",
  dilation_rate = 1,
  activation = NULL,
  activity_regularizer = NULL,
  trainable = TRUE,
  kernel_posterior_fn = tfp$layers$util$default_mean_field_normal_fn(),
  kernel_posterior_tensor_fn = function(d) d %&gt;% tfd_sample(),
  kernel_prior_fn = tfp$layers$util$default_multivariate_normal_fn,
  kernel_divergence_fn = function(q, p, ignore) tfd_kl_divergence(q, p),
  bias_posterior_fn = tfp$layers$util$default_mean_field_normal_fn(is_singular = TRUE),
  bias_posterior_tensor_fn = function(d) d %&gt;% tfd_sample(),
  bias_prior_fn = NULL,
  bias_divergence_fn = function(q, p, ignore) tfd_kl_divergence(q, p),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_conv_3d_flipout_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_conv_3d_flipout_+3A_filters">filters</code></td>
<td>
<p>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution).</p>
</td></tr>
<tr><td><code id="layer_conv_3d_flipout_+3A_kernel_size">kernel_size</code></td>
<td>
<p>An integer or list of a single integer, specifying the
length of the 1D convolution window.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_flipout_+3A_strides">strides</code></td>
<td>
<p>An integer or list of a single integer,
specifying the stride length of the convolution.
Specifying any stride value != 1 is incompatible with specifying
any <code>dilation_rate</code> value != 1.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_flipout_+3A_padding">padding</code></td>
<td>
<p>One of <code>"valid"</code> or <code>"same"</code> (case-insensitive).</p>
</td></tr>
<tr><td><code id="layer_conv_3d_flipout_+3A_data_format">data_format</code></td>
<td>
<p>A string, one of <code>channels_last</code> (default) or
<code>channels_first</code>. The ordering of the dimensions in the inputs.
<code>channels_last</code> corresponds to inputs with shape <code style="white-space: pre;">&#8288;(batch, length, channels)&#8288;</code> while <code>channels_first</code> corresponds to inputs with shape
<code style="white-space: pre;">&#8288;(batch, channels, length)&#8288;</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_flipout_+3A_dilation_rate">dilation_rate</code></td>
<td>
<p>An integer or tuple/list of a single integer, specifying
the dilation rate to use for dilated convolution.
Currently, specifying any <code>dilation_rate</code> value != 1 is
incompatible with specifying any <code>strides</code> value != 1.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_flipout_+3A_activation">activation</code></td>
<td>
<p>Activation function. Set it to None to maintain a linear activation.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_flipout_+3A_activity_regularizer">activity_regularizer</code></td>
<td>
<p>Regularizer function for the output.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_flipout_+3A_trainable">trainable</code></td>
<td>
<p>Whether the layer weights will be updated during training.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_flipout_+3A_kernel_posterior_fn">kernel_posterior_fn</code></td>
<td>
<p>Function which creates <code>tfd$Distribution</code> instance representing the surrogate
posterior of the <code>kernel</code> parameter. Default value: <code>default_mean_field_normal_fn()</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_flipout_+3A_kernel_posterior_tensor_fn">kernel_posterior_tensor_fn</code></td>
<td>
<p>Function which takes a <code>tfd$Distribution</code> instance and returns a representative
value. Default value: <code>function(d) d %&gt;% tfd_sample()</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_flipout_+3A_kernel_prior_fn">kernel_prior_fn</code></td>
<td>
<p>Function which creates <code>tfd$Distribution</code> instance. See <code>default_mean_field_normal_fn</code> docstring for required
parameter signature. Default value: <code>tfd_normal(loc = 0, scale = 1)</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_flipout_+3A_kernel_divergence_fn">kernel_divergence_fn</code></td>
<td>
<p>Function which takes the surrogate posterior distribution, prior distribution and random variate
sample(s) from the surrogate posterior and computes or approximates the KL divergence. The
distributions are <code>tfd$Distribution</code>-like instances and the sample is a <code>Tensor</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_flipout_+3A_bias_posterior_fn">bias_posterior_fn</code></td>
<td>
<p>Function which creates a <code>tfd$Distribution</code> instance representing the surrogate
posterior of the <code>bias</code> parameter. Default value:  <code>default_mean_field_normal_fn(is_singular = TRUE)</code> (which creates an
instance of <code>tfd_deterministic</code>).</p>
</td></tr>
<tr><td><code id="layer_conv_3d_flipout_+3A_bias_posterior_tensor_fn">bias_posterior_tensor_fn</code></td>
<td>
<p>Function which takes a <code>tfd$Distribution</code> instance and returns a representative
value. Default value: <code>function(d) d %&gt;% tfd_sample()</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_flipout_+3A_bias_prior_fn">bias_prior_fn</code></td>
<td>
<p>Function which creates <code>tfd</code> instance. See <code>default_mean_field_normal_fn</code> docstring for required parameter
signature. Default value: <code>NULL</code> (no prior, no variational inference)</p>
</td></tr>
<tr><td><code id="layer_conv_3d_flipout_+3A_bias_divergence_fn">bias_divergence_fn</code></td>
<td>
<p>Function which takes the surrogate posterior distribution, prior distribution and random variate sample(s)
from the surrogate posterior and computes or approximates the KL divergence. The
distributions are <code>tfd$Distribution</code>-like instances and the sample is a <code>Tensor</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_flipout_+3A_...">...</code></td>
<td>
<p>Additional keyword arguments passed to the <code>keras::layer_dense</code> constructed by this layer.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This layer implements the Bayesian variational inference analogue to
a dense layer by assuming the <code>kernel</code> and/or the <code>bias</code> are drawn
from distributions.
</p>
<p>By default, the layer implements a stochastic forward pass via sampling from the kernel and bias posteriors,
</p>
<div class="sourceCode"><pre>outputs = f(inputs; kernel, bias), kernel, bias ~ posterior
</pre></div>
<p>where f denotes the layer's calculation. It uses the Flipout
estimator (Wen et al., 2018), which performs a Monte Carlo approximation
of the distribution integrating over the <code>kernel</code> and <code>bias</code>. Flipout uses
roughly twice as many floating point operations as the reparameterization
estimator but has the advantage of significantly lower variance.
</p>
<p>The arguments permit separate specification of the surrogate posterior
(<code>q(W|x)</code>), prior (<code>p(W)</code>), and divergence for both the <code>kernel</code> and <code>bias</code>
distributions.
</p>
<p>Upon being built, this layer adds losses (accessible via the <code>losses</code>
property) representing the divergences of <code>kernel</code> and/or <code>bias</code> surrogate
posteriors and their respective priors. When doing minibatch stochastic
optimization, make sure to scale this loss such that it is applied just once
per epoch (e.g. if <code>kl</code> is the sum of <code>losses</code> for each element of the batch,
you should pass <code>kl / num_examples_per_epoch</code> to your optimizer).
You can access the <code>kernel</code> and/or <code>bias</code> posterior and prior distributions
after the layer is built via the <code>kernel_posterior</code>, <code>kernel_prior</code>,
<code>bias_posterior</code> and <code>bias_prior</code> properties.
</p>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1803.04386">Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse. Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches. In <em>International Conference on Learning Representations</em>, 2018.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other layers: 
<code><a href="#topic+layer_autoregressive">layer_autoregressive</a>()</code>,
<code><a href="#topic+layer_conv_1d_flipout">layer_conv_1d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_1d_reparameterization">layer_conv_1d_reparameterization</a>()</code>,
<code><a href="#topic+layer_conv_2d_flipout">layer_conv_2d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_2d_reparameterization">layer_conv_2d_reparameterization</a>()</code>,
<code><a href="#topic+layer_conv_3d_reparameterization">layer_conv_3d_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_flipout">layer_dense_flipout</a>()</code>,
<code><a href="#topic+layer_dense_local_reparameterization">layer_dense_local_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_reparameterization">layer_dense_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_variational">layer_dense_variational</a>()</code>,
<code><a href="#topic+layer_variable">layer_variable</a>()</code>
</p>

<hr>
<h2 id='layer_conv_3d_reparameterization'>3D convolution layer (e.g. spatial convolution over volumes)</h2><span id='topic+layer_conv_3d_reparameterization'></span>

<h3>Description</h3>

<p>This layer creates a convolution kernel that is convolved
(actually cross-correlated) with the layer input to produce a tensor of
outputs. It may also include a bias addition and activation function
on the outputs. It assumes the <code>kernel</code> and/or <code>bias</code> are drawn from distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_conv_3d_reparameterization(
  object,
  filters,
  kernel_size,
  strides = 1,
  padding = "valid",
  data_format = "channels_last",
  dilation_rate = 1,
  activation = NULL,
  activity_regularizer = NULL,
  trainable = TRUE,
  kernel_posterior_fn = tfp$layers$util$default_mean_field_normal_fn(),
  kernel_posterior_tensor_fn = function(d) d %&gt;% tfd_sample(),
  kernel_prior_fn = tfp$layers$util$default_multivariate_normal_fn,
  kernel_divergence_fn = function(q, p, ignore) tfd_kl_divergence(q, p),
  bias_posterior_fn = tfp$layers$util$default_mean_field_normal_fn(is_singular = TRUE),
  bias_posterior_tensor_fn = function(d) d %&gt;% tfd_sample(),
  bias_prior_fn = NULL,
  bias_divergence_fn = function(q, p, ignore) tfd_kl_divergence(q, p),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_conv_3d_reparameterization_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_conv_3d_reparameterization_+3A_filters">filters</code></td>
<td>
<p>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution).</p>
</td></tr>
<tr><td><code id="layer_conv_3d_reparameterization_+3A_kernel_size">kernel_size</code></td>
<td>
<p>An integer or list of a single integer, specifying the
length of the 1D convolution window.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_reparameterization_+3A_strides">strides</code></td>
<td>
<p>An integer or list of a single integer,
specifying the stride length of the convolution.
Specifying any stride value != 1 is incompatible with specifying
any <code>dilation_rate</code> value != 1.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_reparameterization_+3A_padding">padding</code></td>
<td>
<p>One of <code>"valid"</code> or <code>"same"</code> (case-insensitive).</p>
</td></tr>
<tr><td><code id="layer_conv_3d_reparameterization_+3A_data_format">data_format</code></td>
<td>
<p>A string, one of <code>channels_last</code> (default) or
<code>channels_first</code>. The ordering of the dimensions in the inputs.
<code>channels_last</code> corresponds to inputs with shape <code style="white-space: pre;">&#8288;(batch, length, channels)&#8288;</code> while <code>channels_first</code> corresponds to inputs with shape
<code style="white-space: pre;">&#8288;(batch, channels, length)&#8288;</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_reparameterization_+3A_dilation_rate">dilation_rate</code></td>
<td>
<p>An integer or tuple/list of a single integer, specifying
the dilation rate to use for dilated convolution.
Currently, specifying any <code>dilation_rate</code> value != 1 is
incompatible with specifying any <code>strides</code> value != 1.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_reparameterization_+3A_activation">activation</code></td>
<td>
<p>Activation function. Set it to None to maintain a linear activation.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_reparameterization_+3A_activity_regularizer">activity_regularizer</code></td>
<td>
<p>Regularizer function for the output.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_reparameterization_+3A_trainable">trainable</code></td>
<td>
<p>Whether the layer weights will be updated during training.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_reparameterization_+3A_kernel_posterior_fn">kernel_posterior_fn</code></td>
<td>
<p>Function which creates <code>tfd$Distribution</code> instance representing the surrogate
posterior of the <code>kernel</code> parameter. Default value: <code>default_mean_field_normal_fn()</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_reparameterization_+3A_kernel_posterior_tensor_fn">kernel_posterior_tensor_fn</code></td>
<td>
<p>Function which takes a <code>tfd$Distribution</code> instance and returns a representative
value. Default value: <code>function(d) d %&gt;% tfd_sample()</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_reparameterization_+3A_kernel_prior_fn">kernel_prior_fn</code></td>
<td>
<p>Function which creates <code>tfd$Distribution</code> instance. See <code>default_mean_field_normal_fn</code> docstring for required
parameter signature. Default value: <code>tfd_normal(loc = 0, scale = 1)</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_reparameterization_+3A_kernel_divergence_fn">kernel_divergence_fn</code></td>
<td>
<p>Function which takes the surrogate posterior distribution, prior distribution and random variate
sample(s) from the surrogate posterior and computes or approximates the KL divergence. The
distributions are <code>tfd$Distribution</code>-like instances and the sample is a <code>Tensor</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_reparameterization_+3A_bias_posterior_fn">bias_posterior_fn</code></td>
<td>
<p>Function which creates a <code>tfd$Distribution</code> instance representing the surrogate
posterior of the <code>bias</code> parameter. Default value:  <code>default_mean_field_normal_fn(is_singular = TRUE)</code> (which creates an
instance of <code>tfd_deterministic</code>).</p>
</td></tr>
<tr><td><code id="layer_conv_3d_reparameterization_+3A_bias_posterior_tensor_fn">bias_posterior_tensor_fn</code></td>
<td>
<p>Function which takes a <code>tfd$Distribution</code> instance and returns a representative
value. Default value: <code>function(d) d %&gt;% tfd_sample()</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_reparameterization_+3A_bias_prior_fn">bias_prior_fn</code></td>
<td>
<p>Function which creates <code>tfd</code> instance. See <code>default_mean_field_normal_fn</code> docstring for required parameter
signature. Default value: <code>NULL</code> (no prior, no variational inference)</p>
</td></tr>
<tr><td><code id="layer_conv_3d_reparameterization_+3A_bias_divergence_fn">bias_divergence_fn</code></td>
<td>
<p>Function which takes the surrogate posterior distribution, prior distribution and random variate sample(s)
from the surrogate posterior and computes or approximates the KL divergence. The
distributions are <code>tfd$Distribution</code>-like instances and the sample is a <code>Tensor</code>.</p>
</td></tr>
<tr><td><code id="layer_conv_3d_reparameterization_+3A_...">...</code></td>
<td>
<p>Additional keyword arguments passed to the <code>keras::layer_dense</code> constructed by this layer.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This layer implements the Bayesian variational inference analogue to
a dense layer by assuming the <code>kernel</code> and/or the <code>bias</code> are drawn
from distributions.
</p>
<p>By default, the layer implements a stochastic forward pass via sampling from the kernel and bias posteriors,
</p>
<div class="sourceCode"><pre>outputs = f(inputs; kernel, bias), kernel, bias ~ posterior
</pre></div>
<p>where f denotes the layer's calculation. It uses the reparameterization
estimator (Kingma and Welling, 2014), which performs a Monte Carlo
approximation of the distribution integrating over the <code>kernel</code> and <code>bias</code>.
</p>
<p>The arguments permit separate specification of the surrogate posterior
(<code>q(W|x)</code>), prior (<code>p(W)</code>), and divergence for both the <code>kernel</code> and <code>bias</code>
distributions.
</p>
<p>Upon being built, this layer adds losses (accessible via the <code>losses</code>
property) representing the divergences of <code>kernel</code> and/or <code>bias</code> surrogate
posteriors and their respective priors. When doing minibatch stochastic
optimization, make sure to scale this loss such that it is applied just once
per epoch (e.g. if <code>kl</code> is the sum of <code>losses</code> for each element of the batch,
you should pass <code>kl / num_examples_per_epoch</code> to your optimizer).
You can access the <code>kernel</code> and/or <code>bias</code> posterior and prior distributions
after the layer is built via the <code>kernel_posterior</code>, <code>kernel_prior</code>,
<code>bias_posterior</code> and <code>bias_prior</code> properties.
</p>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1312.6114">Diederik Kingma and Max Welling. Auto-Encoding Variational Bayes. In <em>International Conference on Learning Representations</em>, 2014.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other layers: 
<code><a href="#topic+layer_autoregressive">layer_autoregressive</a>()</code>,
<code><a href="#topic+layer_conv_1d_flipout">layer_conv_1d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_1d_reparameterization">layer_conv_1d_reparameterization</a>()</code>,
<code><a href="#topic+layer_conv_2d_flipout">layer_conv_2d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_2d_reparameterization">layer_conv_2d_reparameterization</a>()</code>,
<code><a href="#topic+layer_conv_3d_flipout">layer_conv_3d_flipout</a>()</code>,
<code><a href="#topic+layer_dense_flipout">layer_dense_flipout</a>()</code>,
<code><a href="#topic+layer_dense_local_reparameterization">layer_dense_local_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_reparameterization">layer_dense_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_variational">layer_dense_variational</a>()</code>,
<code><a href="#topic+layer_variable">layer_variable</a>()</code>
</p>

<hr>
<h2 id='layer_dense_flipout'>Densely-connected layer class with Flipout estimator.</h2><span id='topic+layer_dense_flipout'></span>

<h3>Description</h3>

<p>This layer implements the Bayesian variational inference analogue to
a dense layer by assuming the <code>kernel</code> and/or the <code>bias</code> are drawn
from distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_dense_flipout(
  object,
  units,
  activation = NULL,
  activity_regularizer = NULL,
  trainable = TRUE,
  kernel_posterior_fn = tfp$layers$util$default_mean_field_normal_fn(),
  kernel_posterior_tensor_fn = function(d) d %&gt;% tfd_sample(),
  kernel_prior_fn = tfp$layers$util$default_multivariate_normal_fn,
  kernel_divergence_fn = function(q, p, ignore) tfd_kl_divergence(q, p),
  bias_posterior_fn = tfp$layers$util$default_mean_field_normal_fn(is_singular = TRUE),
  bias_posterior_tensor_fn = function(d) d %&gt;% tfd_sample(),
  bias_prior_fn = NULL,
  bias_divergence_fn = function(q, p, ignore) tfd_kl_divergence(q, p),
  seed = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_dense_flipout_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_dense_flipout_+3A_units">units</code></td>
<td>
<p>integer dimensionality of the output space</p>
</td></tr>
<tr><td><code id="layer_dense_flipout_+3A_activation">activation</code></td>
<td>
<p>Activation function. Set it to None to maintain a linear activation.</p>
</td></tr>
<tr><td><code id="layer_dense_flipout_+3A_activity_regularizer">activity_regularizer</code></td>
<td>
<p>Regularizer function for the output.</p>
</td></tr>
<tr><td><code id="layer_dense_flipout_+3A_trainable">trainable</code></td>
<td>
<p>Whether the layer weights will be updated during training.</p>
</td></tr>
<tr><td><code id="layer_dense_flipout_+3A_kernel_posterior_fn">kernel_posterior_fn</code></td>
<td>
<p>Function which creates <code>tfd$Distribution</code> instance representing the surrogate
posterior of the <code>kernel</code> parameter. Default value: <code>default_mean_field_normal_fn()</code>.</p>
</td></tr>
<tr><td><code id="layer_dense_flipout_+3A_kernel_posterior_tensor_fn">kernel_posterior_tensor_fn</code></td>
<td>
<p>Function which takes a <code>tfd$Distribution</code> instance and returns a representative
value. Default value: <code>function(d) d %&gt;% tfd_sample()</code>.</p>
</td></tr>
<tr><td><code id="layer_dense_flipout_+3A_kernel_prior_fn">kernel_prior_fn</code></td>
<td>
<p>Function which creates <code>tfd$Distribution</code> instance. See <code>default_mean_field_normal_fn</code> docstring for required
parameter signature. Default value: <code>tfd_normal(loc = 0, scale = 1)</code>.</p>
</td></tr>
<tr><td><code id="layer_dense_flipout_+3A_kernel_divergence_fn">kernel_divergence_fn</code></td>
<td>
<p>Function which takes the surrogate posterior distribution, prior distribution and random variate
sample(s) from the surrogate posterior and computes or approximates the KL divergence. The
distributions are <code>tfd$Distribution</code>-like instances and the sample is a <code>Tensor</code>.</p>
</td></tr>
<tr><td><code id="layer_dense_flipout_+3A_bias_posterior_fn">bias_posterior_fn</code></td>
<td>
<p>Function which creates a <code>tfd$Distribution</code> instance representing the surrogate
posterior of the <code>bias</code> parameter. Default value:  <code>default_mean_field_normal_fn(is_singular = TRUE)</code> (which creates an
instance of <code>tfd_deterministic</code>).</p>
</td></tr>
<tr><td><code id="layer_dense_flipout_+3A_bias_posterior_tensor_fn">bias_posterior_tensor_fn</code></td>
<td>
<p>Function which takes a <code>tfd$Distribution</code> instance and returns a representative
value. Default value: <code>function(d) d %&gt;% tfd_sample()</code>.</p>
</td></tr>
<tr><td><code id="layer_dense_flipout_+3A_bias_prior_fn">bias_prior_fn</code></td>
<td>
<p>Function which creates <code>tfd</code> instance. See <code>default_mean_field_normal_fn</code> docstring for required parameter
signature. Default value: <code>NULL</code> (no prior, no variational inference)</p>
</td></tr>
<tr><td><code id="layer_dense_flipout_+3A_bias_divergence_fn">bias_divergence_fn</code></td>
<td>
<p>Function which takes the surrogate posterior distribution, prior distribution and random variate sample(s)
from the surrogate posterior and computes or approximates the KL divergence. The
distributions are <code>tfd$Distribution</code>-like instances and the sample is a <code>Tensor</code>.</p>
</td></tr>
<tr><td><code id="layer_dense_flipout_+3A_seed">seed</code></td>
<td>
<p>scalar <code>integer</code> which initializes the random number generator.
Default value: <code>NULL</code> (i.e., use global seed).</p>
</td></tr>
<tr><td><code id="layer_dense_flipout_+3A_...">...</code></td>
<td>
<p>Additional keyword arguments passed to the <code>keras::layer_dense</code> constructed by this layer.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default, the layer implements a stochastic
forward pass via sampling from the kernel and bias posteriors,
</p>
<div class="sourceCode"><pre>kernel, bias ~ posterior
outputs = activation(matmul(inputs, kernel) + bias)
</pre></div>
<p>It uses the Flipout estimator (Wen et al., 2018), which performs a Monte
Carlo approximation of the distribution integrating over the <code>kernel</code> and
<code>bias</code>. Flipout uses roughly twice as many floating point operations as the
reparameterization estimator but has the advantage of significantly lower
variance.
</p>
<p>The arguments permit separate specification of the surrogate posterior
(<code>q(W|x)</code>), prior (<code>p(W)</code>), and divergence for both the <code>kernel</code> and <code>bias</code>
distributions.
</p>
<p>Upon being built, this layer adds losses (accessible via the <code>losses</code>
property) representing the divergences of <code>kernel</code> and/or <code>bias</code> surrogate
posteriors and their respective priors. When doing minibatch stochastic
optimization, make sure to scale this loss such that it is applied just once
per epoch (e.g. if <code>kl</code> is the sum of <code>losses</code> for each element of the batch,
you should pass <code>kl / num_examples_per_epoch</code> to your optimizer).
</p>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1803.04386">Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse. Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches. In <em>International Conference on Learning Representations</em>, 2018.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other layers: 
<code><a href="#topic+layer_autoregressive">layer_autoregressive</a>()</code>,
<code><a href="#topic+layer_conv_1d_flipout">layer_conv_1d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_1d_reparameterization">layer_conv_1d_reparameterization</a>()</code>,
<code><a href="#topic+layer_conv_2d_flipout">layer_conv_2d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_2d_reparameterization">layer_conv_2d_reparameterization</a>()</code>,
<code><a href="#topic+layer_conv_3d_flipout">layer_conv_3d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_3d_reparameterization">layer_conv_3d_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_local_reparameterization">layer_dense_local_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_reparameterization">layer_dense_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_variational">layer_dense_variational</a>()</code>,
<code><a href="#topic+layer_variable">layer_variable</a>()</code>
</p>

<hr>
<h2 id='layer_dense_local_reparameterization'>Densely-connected layer class with local reparameterization estimator.</h2><span id='topic+layer_dense_local_reparameterization'></span>

<h3>Description</h3>

<p>This layer implements the Bayesian variational inference analogue to
a dense layer by assuming the <code>kernel</code> and/or the <code>bias</code> are drawn
from distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_dense_local_reparameterization(
  object,
  units,
  activation = NULL,
  activity_regularizer = NULL,
  trainable = TRUE,
  kernel_posterior_fn = tfp$layers$util$default_mean_field_normal_fn(),
  kernel_posterior_tensor_fn = function(d) d %&gt;% tfd_sample(),
  kernel_prior_fn = tfp$layers$util$default_multivariate_normal_fn,
  kernel_divergence_fn = function(q, p, ignore) tfd_kl_divergence(q, p),
  bias_posterior_fn = tfp$layers$util$default_mean_field_normal_fn(is_singular = TRUE),
  bias_posterior_tensor_fn = function(d) d %&gt;% tfd_sample(),
  bias_prior_fn = NULL,
  bias_divergence_fn = function(q, p, ignore) tfd_kl_divergence(q, p),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_dense_local_reparameterization_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_dense_local_reparameterization_+3A_units">units</code></td>
<td>
<p>integer dimensionality of the output space</p>
</td></tr>
<tr><td><code id="layer_dense_local_reparameterization_+3A_activation">activation</code></td>
<td>
<p>Activation function. Set it to None to maintain a linear activation.</p>
</td></tr>
<tr><td><code id="layer_dense_local_reparameterization_+3A_activity_regularizer">activity_regularizer</code></td>
<td>
<p>Regularizer function for the output.</p>
</td></tr>
<tr><td><code id="layer_dense_local_reparameterization_+3A_trainable">trainable</code></td>
<td>
<p>Whether the layer weights will be updated during training.</p>
</td></tr>
<tr><td><code id="layer_dense_local_reparameterization_+3A_kernel_posterior_fn">kernel_posterior_fn</code></td>
<td>
<p>Function which creates <code>tfd$Distribution</code> instance representing the surrogate
posterior of the <code>kernel</code> parameter. Default value: <code>default_mean_field_normal_fn()</code>.</p>
</td></tr>
<tr><td><code id="layer_dense_local_reparameterization_+3A_kernel_posterior_tensor_fn">kernel_posterior_tensor_fn</code></td>
<td>
<p>Function which takes a <code>tfd$Distribution</code> instance and returns a representative
value. Default value: <code>function(d) d %&gt;% tfd_sample()</code>.</p>
</td></tr>
<tr><td><code id="layer_dense_local_reparameterization_+3A_kernel_prior_fn">kernel_prior_fn</code></td>
<td>
<p>Function which creates <code>tfd$Distribution</code> instance. See <code>default_mean_field_normal_fn</code> docstring for required
parameter signature. Default value: <code>tfd_normal(loc = 0, scale = 1)</code>.</p>
</td></tr>
<tr><td><code id="layer_dense_local_reparameterization_+3A_kernel_divergence_fn">kernel_divergence_fn</code></td>
<td>
<p>Function which takes the surrogate posterior distribution, prior distribution and random variate
sample(s) from the surrogate posterior and computes or approximates the KL divergence. The
distributions are <code>tfd$Distribution</code>-like instances and the sample is a <code>Tensor</code>.</p>
</td></tr>
<tr><td><code id="layer_dense_local_reparameterization_+3A_bias_posterior_fn">bias_posterior_fn</code></td>
<td>
<p>Function which creates a <code>tfd$Distribution</code> instance representing the surrogate
posterior of the <code>bias</code> parameter. Default value:  <code>default_mean_field_normal_fn(is_singular = TRUE)</code> (which creates an
instance of <code>tfd_deterministic</code>).</p>
</td></tr>
<tr><td><code id="layer_dense_local_reparameterization_+3A_bias_posterior_tensor_fn">bias_posterior_tensor_fn</code></td>
<td>
<p>Function which takes a <code>tfd$Distribution</code> instance and returns a representative
value. Default value: <code>function(d) d %&gt;% tfd_sample()</code>.</p>
</td></tr>
<tr><td><code id="layer_dense_local_reparameterization_+3A_bias_prior_fn">bias_prior_fn</code></td>
<td>
<p>Function which creates <code>tfd</code> instance. See <code>default_mean_field_normal_fn</code> docstring for required parameter
signature. Default value: <code>NULL</code> (no prior, no variational inference)</p>
</td></tr>
<tr><td><code id="layer_dense_local_reparameterization_+3A_bias_divergence_fn">bias_divergence_fn</code></td>
<td>
<p>Function which takes the surrogate posterior distribution, prior distribution and random variate sample(s)
from the surrogate posterior and computes or approximates the KL divergence. The
distributions are <code>tfd$Distribution</code>-like instances and the sample is a <code>Tensor</code>.</p>
</td></tr>
<tr><td><code id="layer_dense_local_reparameterization_+3A_...">...</code></td>
<td>
<p>Additional keyword arguments passed to the <code>keras::layer_dense</code> constructed by this layer.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default, the layer implements a stochastic
forward pass via sampling from the kernel and bias posteriors,
</p>
<div class="sourceCode"><pre>kernel, bias ~ posterior
outputs = activation(matmul(inputs, kernel) + bias)
</pre></div>
<p>It uses the local reparameterization estimator (Kingma et al., 2015),
which performs a Monte Carlo approximation of the distribution on the hidden
units induced by the <code>kernel</code> and <code>bias</code>. The default <code>kernel_posterior_fn</code>
is a normal distribution which factorizes across all elements of the weight
matrix and bias vector. Unlike that paper's multiplicative parameterization, this
distribution has trainable location and scale parameters which is known as
an additive noise parameterization (Molchanov et al., 2017).
</p>
<p>The arguments permit separate specification of the surrogate posterior
(<code>q(W|x)</code>), prior (<code>p(W)</code>), and divergence for both the <code>kernel</code> and <code>bias</code>
distributions.
</p>
<p>Upon being built, this layer adds losses (accessible via the <code>losses</code>
property) representing the divergences of <code>kernel</code> and/or <code>bias</code> surrogate
posteriors and their respective priors. When doing minibatch stochastic
optimization, make sure to scale this loss such that it is applied just once
per epoch (e.g. if <code>kl</code> is the sum of <code>losses</code> for each element of the batch,
you should pass <code>kl / num_examples_per_epoch</code> to your optimizer).
You can access the <code>kernel</code> and/or <code>bias</code> posterior and prior distributions
after the layer is built via the <code>kernel_posterior</code>, <code>kernel_prior</code>,
<code>bias_posterior</code> and <code>bias_prior</code> properties.
</p>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1506.02557">Diederik Kingma, Tim Salimans, and Max Welling. Variational Dropout and the Local Reparameterization Trick. In <em>Neural Information Processing Systems</em>, 2015.</a>
</p>
</li>
<li> <p><a href="https://arxiv.org/abs/1701.05369">Dmitry Molchanov, Arsenii Ashukha, Dmitry Vetrov. Variational Dropout Sparsifies Deep Neural Networks. In <em>International Conference on Machine Learning</em>, 2017.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other layers: 
<code><a href="#topic+layer_autoregressive">layer_autoregressive</a>()</code>,
<code><a href="#topic+layer_conv_1d_flipout">layer_conv_1d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_1d_reparameterization">layer_conv_1d_reparameterization</a>()</code>,
<code><a href="#topic+layer_conv_2d_flipout">layer_conv_2d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_2d_reparameterization">layer_conv_2d_reparameterization</a>()</code>,
<code><a href="#topic+layer_conv_3d_flipout">layer_conv_3d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_3d_reparameterization">layer_conv_3d_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_flipout">layer_dense_flipout</a>()</code>,
<code><a href="#topic+layer_dense_reparameterization">layer_dense_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_variational">layer_dense_variational</a>()</code>,
<code><a href="#topic+layer_variable">layer_variable</a>()</code>
</p>

<hr>
<h2 id='layer_dense_reparameterization'>Densely-connected layer class with reparameterization estimator.</h2><span id='topic+layer_dense_reparameterization'></span>

<h3>Description</h3>

<p>This layer implements the Bayesian variational inference analogue to
a dense layer by assuming the <code>kernel</code> and/or the <code>bias</code> are drawn
from distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_dense_reparameterization(
  object,
  units,
  activation = NULL,
  activity_regularizer = NULL,
  trainable = TRUE,
  kernel_posterior_fn = tfp$layers$util$default_mean_field_normal_fn(),
  kernel_posterior_tensor_fn = function(d) d %&gt;% tfd_sample(),
  kernel_prior_fn = tfp$layers$util$default_multivariate_normal_fn,
  kernel_divergence_fn = function(q, p, ignore) tfd_kl_divergence(q, p),
  bias_posterior_fn = tfp$layers$util$default_mean_field_normal_fn(is_singular = TRUE),
  bias_posterior_tensor_fn = function(d) d %&gt;% tfd_sample(),
  bias_prior_fn = NULL,
  bias_divergence_fn = function(q, p, ignore) tfd_kl_divergence(q, p),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_dense_reparameterization_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_dense_reparameterization_+3A_units">units</code></td>
<td>
<p>integer dimensionality of the output space</p>
</td></tr>
<tr><td><code id="layer_dense_reparameterization_+3A_activation">activation</code></td>
<td>
<p>Activation function. Set it to None to maintain a linear activation.</p>
</td></tr>
<tr><td><code id="layer_dense_reparameterization_+3A_activity_regularizer">activity_regularizer</code></td>
<td>
<p>Regularizer function for the output.</p>
</td></tr>
<tr><td><code id="layer_dense_reparameterization_+3A_trainable">trainable</code></td>
<td>
<p>Whether the layer weights will be updated during training.</p>
</td></tr>
<tr><td><code id="layer_dense_reparameterization_+3A_kernel_posterior_fn">kernel_posterior_fn</code></td>
<td>
<p>Function which creates <code>tfd$Distribution</code> instance representing the surrogate
posterior of the <code>kernel</code> parameter. Default value: <code>default_mean_field_normal_fn()</code>.</p>
</td></tr>
<tr><td><code id="layer_dense_reparameterization_+3A_kernel_posterior_tensor_fn">kernel_posterior_tensor_fn</code></td>
<td>
<p>Function which takes a <code>tfd$Distribution</code> instance and returns a representative
value. Default value: <code>function(d) d %&gt;% tfd_sample()</code>.</p>
</td></tr>
<tr><td><code id="layer_dense_reparameterization_+3A_kernel_prior_fn">kernel_prior_fn</code></td>
<td>
<p>Function which creates <code>tfd$Distribution</code> instance. See <code>default_mean_field_normal_fn</code> docstring for required
parameter signature. Default value: <code>tfd_normal(loc = 0, scale = 1)</code>.</p>
</td></tr>
<tr><td><code id="layer_dense_reparameterization_+3A_kernel_divergence_fn">kernel_divergence_fn</code></td>
<td>
<p>Function which takes the surrogate posterior distribution, prior distribution and random variate
sample(s) from the surrogate posterior and computes or approximates the KL divergence. The
distributions are <code>tfd$Distribution</code>-like instances and the sample is a <code>Tensor</code>.</p>
</td></tr>
<tr><td><code id="layer_dense_reparameterization_+3A_bias_posterior_fn">bias_posterior_fn</code></td>
<td>
<p>Function which creates a <code>tfd$Distribution</code> instance representing the surrogate
posterior of the <code>bias</code> parameter. Default value:  <code>default_mean_field_normal_fn(is_singular = TRUE)</code> (which creates an
instance of <code>tfd_deterministic</code>).</p>
</td></tr>
<tr><td><code id="layer_dense_reparameterization_+3A_bias_posterior_tensor_fn">bias_posterior_tensor_fn</code></td>
<td>
<p>Function which takes a <code>tfd$Distribution</code> instance and returns a representative
value. Default value: <code>function(d) d %&gt;% tfd_sample()</code>.</p>
</td></tr>
<tr><td><code id="layer_dense_reparameterization_+3A_bias_prior_fn">bias_prior_fn</code></td>
<td>
<p>Function which creates <code>tfd</code> instance. See <code>default_mean_field_normal_fn</code> docstring for required parameter
signature. Default value: <code>NULL</code> (no prior, no variational inference)</p>
</td></tr>
<tr><td><code id="layer_dense_reparameterization_+3A_bias_divergence_fn">bias_divergence_fn</code></td>
<td>
<p>Function which takes the surrogate posterior distribution, prior distribution and random variate sample(s)
from the surrogate posterior and computes or approximates the KL divergence. The
distributions are <code>tfd$Distribution</code>-like instances and the sample is a <code>Tensor</code>.</p>
</td></tr>
<tr><td><code id="layer_dense_reparameterization_+3A_...">...</code></td>
<td>
<p>Additional keyword arguments passed to the <code>keras::layer_dense</code> constructed by this layer.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default, the layer implements a stochastic
forward pass via sampling from the kernel and bias posteriors,
</p>
<div class="sourceCode"><pre>kernel, bias ~ posterior
outputs = activation(matmul(inputs, kernel) + bias)
</pre></div>
<p>It uses the reparameterization estimator (Kingma and Welling, 2014)
which performs a Monte Carlo approximation of the distribution integrating
over the <code>kernel</code> and <code>bias</code>.
</p>
<p>The arguments permit separate specification of the surrogate posterior
(<code>q(W|x)</code>), prior (<code>p(W)</code>), and divergence for both the <code>kernel</code> and <code>bias</code>
distributions.
</p>
<p>Upon being built, this layer adds losses (accessible via the <code>losses</code>
property) representing the divergences of <code>kernel</code> and/or <code>bias</code> surrogate
posteriors and their respective priors. When doing minibatch stochastic
optimization, make sure to scale this loss such that it is applied just once
per epoch (e.g. if <code>kl</code> is the sum of <code>losses</code> for each element of the batch,
you should pass <code>kl / num_examples_per_epoch</code> to your optimizer).
You can access the <code>kernel</code> and/or <code>bias</code> posterior and prior distributions
after the layer is built via the <code>kernel_posterior</code>, <code>kernel_prior</code>,
<code>bias_posterior</code> and <code>bias_prior</code> properties.
</p>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1312.6114">Diederik Kingma and Max Welling. Auto-Encoding Variational Bayes. In <em>International Conference on Learning Representations</em>, 2014.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other layers: 
<code><a href="#topic+layer_autoregressive">layer_autoregressive</a>()</code>,
<code><a href="#topic+layer_conv_1d_flipout">layer_conv_1d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_1d_reparameterization">layer_conv_1d_reparameterization</a>()</code>,
<code><a href="#topic+layer_conv_2d_flipout">layer_conv_2d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_2d_reparameterization">layer_conv_2d_reparameterization</a>()</code>,
<code><a href="#topic+layer_conv_3d_flipout">layer_conv_3d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_3d_reparameterization">layer_conv_3d_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_flipout">layer_dense_flipout</a>()</code>,
<code><a href="#topic+layer_dense_local_reparameterization">layer_dense_local_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_variational">layer_dense_variational</a>()</code>,
<code><a href="#topic+layer_variable">layer_variable</a>()</code>
</p>

<hr>
<h2 id='layer_dense_variational'>Dense Variational Layer</h2><span id='topic+layer_dense_variational'></span>

<h3>Description</h3>

<p>This layer uses variational inference to fit a &quot;surrogate&quot; posterior to the
distribution over both the <code>kernel</code> matrix and the <code>bias</code> terms which are
otherwise used in a manner similar to <code>layer_dense()</code>.
This layer fits the &quot;weights posterior&quot; according to the following generative
process:
</p>
<div class="sourceCode none"><pre>[K, b] ~ Prior()
M = matmul(X, K) + b
Y ~ Likelihood(M)
</pre></div>


<h3>Usage</h3>

<pre><code class='language-R'>layer_dense_variational(
  object,
  units,
  make_posterior_fn,
  make_prior_fn,
  kl_weight = NULL,
  kl_use_exact = FALSE,
  activation = NULL,
  use_bias = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_dense_variational_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_dense_variational_+3A_units">units</code></td>
<td>
<p>Positive integer, dimensionality of the output space.</p>
</td></tr>
<tr><td><code id="layer_dense_variational_+3A_make_posterior_fn">make_posterior_fn</code></td>
<td>
<p>function taking <code>tf$size(kernel)</code>,
<code>tf$size(bias)</code>, <code>dtype</code> and returns another callable which takes an
input and produces a <code>tfd$Distribution</code> instance.</p>
</td></tr>
<tr><td><code id="layer_dense_variational_+3A_make_prior_fn">make_prior_fn</code></td>
<td>
<p>function taking <code>tf$size(kernel)</code>, <code>tf$size(bias)</code>,
<code>dtype</code> and returns another callable which takes an input and produces a
<code>tfd$Distribution</code> instance.</p>
</td></tr>
<tr><td><code id="layer_dense_variational_+3A_kl_weight">kl_weight</code></td>
<td>
<p>Amount by which to scale the KL divergence loss between prior
and posterior.</p>
</td></tr>
<tr><td><code id="layer_dense_variational_+3A_kl_use_exact">kl_use_exact</code></td>
<td>
<p>Logical indicating that the analytical KL divergence
should be used rather than a Monte Carlo approximation.</p>
</td></tr>
<tr><td><code id="layer_dense_variational_+3A_activation">activation</code></td>
<td>
<p>An activation function.  See <code>keras::layer_dense</code>. Default: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="layer_dense_variational_+3A_use_bias">use_bias</code></td>
<td>
<p>Whether or not the dense layers constructed in this layer
should have a bias term.  See <code>keras::layer_dense</code>.  Default: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="layer_dense_variational_+3A_...">...</code></td>
<td>
<p>Additional keyword arguments passed to the <code>keras::layer_dense</code> constructed by this layer.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>See Also</h3>

<p>Other layers: 
<code><a href="#topic+layer_autoregressive">layer_autoregressive</a>()</code>,
<code><a href="#topic+layer_conv_1d_flipout">layer_conv_1d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_1d_reparameterization">layer_conv_1d_reparameterization</a>()</code>,
<code><a href="#topic+layer_conv_2d_flipout">layer_conv_2d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_2d_reparameterization">layer_conv_2d_reparameterization</a>()</code>,
<code><a href="#topic+layer_conv_3d_flipout">layer_conv_3d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_3d_reparameterization">layer_conv_3d_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_flipout">layer_dense_flipout</a>()</code>,
<code><a href="#topic+layer_dense_local_reparameterization">layer_dense_local_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_reparameterization">layer_dense_reparameterization</a>()</code>,
<code><a href="#topic+layer_variable">layer_variable</a>()</code>
</p>

<hr>
<h2 id='layer_distribution_lambda'>Keras layer enabling plumbing TFP distributions through Keras models</h2><span id='topic+layer_distribution_lambda'></span>

<h3>Description</h3>

<p>Keras layer enabling plumbing TFP distributions through Keras models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_distribution_lambda(
  object,
  make_distribution_fn,
  convert_to_tensor_fn = tfp$distributions$Distribution$sample,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_distribution_lambda_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_distribution_lambda_+3A_make_distribution_fn">make_distribution_fn</code></td>
<td>
<p>A callable that takes previous layer outputs and returns a <code>tfd$distributions$Distribution</code> instance.</p>
</td></tr>
<tr><td><code id="layer_distribution_lambda_+3A_convert_to_tensor_fn">convert_to_tensor_fn</code></td>
<td>
<p>A callable that takes a tfd$Distribution instance and returns a
tf$Tensor-like object. Default value: <code>tfd$distributions$Distribution$sample</code>.</p>
</td></tr>
<tr><td><code id="layer_distribution_lambda_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>args</code> of <code>keras::create_layer</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>See Also</h3>

<p>For an example how to use in a Keras model, see <code><a href="#topic+layer_independent_normal">layer_independent_normal()</a></code>.
</p>
<p>Other distribution_layers: 
<code><a href="#topic+layer_categorical_mixture_of_one_hot_categorical">layer_categorical_mixture_of_one_hot_categorical</a>()</code>,
<code><a href="#topic+layer_independent_bernoulli">layer_independent_bernoulli</a>()</code>,
<code><a href="#topic+layer_independent_logistic">layer_independent_logistic</a>()</code>,
<code><a href="#topic+layer_independent_normal">layer_independent_normal</a>()</code>,
<code><a href="#topic+layer_independent_poisson">layer_independent_poisson</a>()</code>,
<code><a href="#topic+layer_kl_divergence_add_loss">layer_kl_divergence_add_loss</a>()</code>,
<code><a href="#topic+layer_kl_divergence_regularizer">layer_kl_divergence_regularizer</a>()</code>,
<code><a href="#topic+layer_mixture_logistic">layer_mixture_logistic</a>()</code>,
<code><a href="#topic+layer_mixture_normal">layer_mixture_normal</a>()</code>,
<code><a href="#topic+layer_mixture_same_family">layer_mixture_same_family</a>()</code>,
<code><a href="#topic+layer_multivariate_normal_tri_l">layer_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+layer_one_hot_categorical">layer_one_hot_categorical</a>()</code>
</p>

<hr>
<h2 id='layer_independent_bernoulli'>An Independent-Bernoulli Keras layer from prod(event_shape) params</h2><span id='topic+layer_independent_bernoulli'></span>

<h3>Description</h3>

<p>An Independent-Bernoulli Keras layer from prod(event_shape) params
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_independent_bernoulli(
  object,
  event_shape,
  convert_to_tensor_fn = tfp$distributions$Distribution$sample,
  sample_dtype = NULL,
  validate_args = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_independent_bernoulli_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_independent_bernoulli_+3A_event_shape">event_shape</code></td>
<td>
<p>Scalar integer representing the size of single draw from this distribution.</p>
</td></tr>
<tr><td><code id="layer_independent_bernoulli_+3A_convert_to_tensor_fn">convert_to_tensor_fn</code></td>
<td>
<p>A callable that takes a tfd$Distribution instance and returns a
tf$Tensor-like object. Default value: <code>tfd$distributions$Distribution$sample</code>.</p>
</td></tr>
<tr><td><code id="layer_independent_bernoulli_+3A_sample_dtype">sample_dtype</code></td>
<td>
<p>dtype of samples produced by this distribution.
Default value: NULL (i.e., previous layer's dtype).</p>
</td></tr>
<tr><td><code id="layer_independent_bernoulli_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.
@param ... Additional arguments passed to <code>args</code> of <code>keras::create_layer</code>.</p>
</td></tr>
<tr><td><code id="layer_independent_bernoulli_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>args</code> of <code>keras::create_layer</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>See Also</h3>

<p>For an example how to use in a Keras model, see <code><a href="#topic+layer_independent_normal">layer_independent_normal()</a></code>.
</p>
<p>Other distribution_layers: 
<code><a href="#topic+layer_categorical_mixture_of_one_hot_categorical">layer_categorical_mixture_of_one_hot_categorical</a>()</code>,
<code><a href="#topic+layer_distribution_lambda">layer_distribution_lambda</a>()</code>,
<code><a href="#topic+layer_independent_logistic">layer_independent_logistic</a>()</code>,
<code><a href="#topic+layer_independent_normal">layer_independent_normal</a>()</code>,
<code><a href="#topic+layer_independent_poisson">layer_independent_poisson</a>()</code>,
<code><a href="#topic+layer_kl_divergence_add_loss">layer_kl_divergence_add_loss</a>()</code>,
<code><a href="#topic+layer_kl_divergence_regularizer">layer_kl_divergence_regularizer</a>()</code>,
<code><a href="#topic+layer_mixture_logistic">layer_mixture_logistic</a>()</code>,
<code><a href="#topic+layer_mixture_normal">layer_mixture_normal</a>()</code>,
<code><a href="#topic+layer_mixture_same_family">layer_mixture_same_family</a>()</code>,
<code><a href="#topic+layer_multivariate_normal_tri_l">layer_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+layer_one_hot_categorical">layer_one_hot_categorical</a>()</code>
</p>

<hr>
<h2 id='layer_independent_logistic'>An independent Logistic Keras layer.</h2><span id='topic+layer_independent_logistic'></span>

<h3>Description</h3>

<p>An independent Logistic Keras layer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_independent_logistic(
  object,
  event_shape,
  convert_to_tensor_fn = tfp$distributions$Distribution$sample,
  validate_args = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_independent_logistic_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_independent_logistic_+3A_event_shape">event_shape</code></td>
<td>
<p>Scalar integer representing the size of single draw from this distribution.</p>
</td></tr>
<tr><td><code id="layer_independent_logistic_+3A_convert_to_tensor_fn">convert_to_tensor_fn</code></td>
<td>
<p>A callable that takes a tfd$Distribution instance and returns a
tf$Tensor-like object. Default value: <code>tfd$distributions$Distribution$sample</code>.</p>
</td></tr>
<tr><td><code id="layer_independent_logistic_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.
@param ... Additional arguments passed to <code>args</code> of <code>keras::create_layer</code>.</p>
</td></tr>
<tr><td><code id="layer_independent_logistic_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>args</code> of <code>keras::create_layer</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>See Also</h3>

<p>For an example how to use in a Keras model, see <code><a href="#topic+layer_independent_normal">layer_independent_normal()</a></code>.
</p>
<p>Other distribution_layers: 
<code><a href="#topic+layer_categorical_mixture_of_one_hot_categorical">layer_categorical_mixture_of_one_hot_categorical</a>()</code>,
<code><a href="#topic+layer_distribution_lambda">layer_distribution_lambda</a>()</code>,
<code><a href="#topic+layer_independent_bernoulli">layer_independent_bernoulli</a>()</code>,
<code><a href="#topic+layer_independent_normal">layer_independent_normal</a>()</code>,
<code><a href="#topic+layer_independent_poisson">layer_independent_poisson</a>()</code>,
<code><a href="#topic+layer_kl_divergence_add_loss">layer_kl_divergence_add_loss</a>()</code>,
<code><a href="#topic+layer_kl_divergence_regularizer">layer_kl_divergence_regularizer</a>()</code>,
<code><a href="#topic+layer_mixture_logistic">layer_mixture_logistic</a>()</code>,
<code><a href="#topic+layer_mixture_normal">layer_mixture_normal</a>()</code>,
<code><a href="#topic+layer_mixture_same_family">layer_mixture_same_family</a>()</code>,
<code><a href="#topic+layer_multivariate_normal_tri_l">layer_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+layer_one_hot_categorical">layer_one_hot_categorical</a>()</code>
</p>

<hr>
<h2 id='layer_independent_normal'>An independent Normal Keras layer.</h2><span id='topic+layer_independent_normal'></span>

<h3>Description</h3>

<p>An independent Normal Keras layer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_independent_normal(
  object,
  event_shape,
  convert_to_tensor_fn = tfp$distributions$Distribution$sample,
  validate_args = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_independent_normal_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_independent_normal_+3A_event_shape">event_shape</code></td>
<td>
<p>Scalar integer representing the size of single draw from this distribution.</p>
</td></tr>
<tr><td><code id="layer_independent_normal_+3A_convert_to_tensor_fn">convert_to_tensor_fn</code></td>
<td>
<p>A callable that takes a tfd$Distribution instance and returns a
tf$Tensor-like object. Default value: <code>tfd$distributions$Distribution$sample</code>.</p>
</td></tr>
<tr><td><code id="layer_independent_normal_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.
@param ... Additional arguments passed to <code>args</code> of <code>keras::create_layer</code>.</p>
</td></tr>
<tr><td><code id="layer_independent_normal_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>args</code> of <code>keras::create_layer</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>See Also</h3>

<p>Other distribution_layers: 
<code><a href="#topic+layer_categorical_mixture_of_one_hot_categorical">layer_categorical_mixture_of_one_hot_categorical</a>()</code>,
<code><a href="#topic+layer_distribution_lambda">layer_distribution_lambda</a>()</code>,
<code><a href="#topic+layer_independent_bernoulli">layer_independent_bernoulli</a>()</code>,
<code><a href="#topic+layer_independent_logistic">layer_independent_logistic</a>()</code>,
<code><a href="#topic+layer_independent_poisson">layer_independent_poisson</a>()</code>,
<code><a href="#topic+layer_kl_divergence_add_loss">layer_kl_divergence_add_loss</a>()</code>,
<code><a href="#topic+layer_kl_divergence_regularizer">layer_kl_divergence_regularizer</a>()</code>,
<code><a href="#topic+layer_mixture_logistic">layer_mixture_logistic</a>()</code>,
<code><a href="#topic+layer_mixture_normal">layer_mixture_normal</a>()</code>,
<code><a href="#topic+layer_mixture_same_family">layer_mixture_same_family</a>()</code>,
<code><a href="#topic+layer_multivariate_normal_tri_l">layer_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+layer_one_hot_categorical">layer_one_hot_categorical</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(keras)
input_shape &lt;- c(28, 28, 1)
encoded_shape &lt;- 2
n &lt;- 2
model &lt;- keras_model_sequential(
  list(
    layer_input(shape = input_shape),
    layer_flatten(),
    layer_dense(units = n),
    layer_dense(units = params_size_independent_normal(encoded_shape)),
    layer_independent_normal(event_shape = encoded_shape)
    )
  )

</code></pre>

<hr>
<h2 id='layer_independent_poisson'>An independent Poisson Keras layer.</h2><span id='topic+layer_independent_poisson'></span>

<h3>Description</h3>

<p>An independent Poisson Keras layer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_independent_poisson(
  object,
  event_shape,
  convert_to_tensor_fn = tfp$distributions$Distribution$sample,
  validate_args = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_independent_poisson_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_independent_poisson_+3A_event_shape">event_shape</code></td>
<td>
<p>Scalar integer representing the size of single draw from this distribution.</p>
</td></tr>
<tr><td><code id="layer_independent_poisson_+3A_convert_to_tensor_fn">convert_to_tensor_fn</code></td>
<td>
<p>A callable that takes a tfd$Distribution instance and returns a
tf$Tensor-like object. Default value: <code>tfd$distributions$Distribution$sample</code>.</p>
</td></tr>
<tr><td><code id="layer_independent_poisson_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.
@param ... Additional arguments passed to <code>args</code> of <code>keras::create_layer</code>.</p>
</td></tr>
<tr><td><code id="layer_independent_poisson_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>args</code> of <code>keras::create_layer</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>See Also</h3>

<p>For an example how to use in a Keras model, see <code><a href="#topic+layer_independent_normal">layer_independent_normal()</a></code>.
</p>
<p>Other distribution_layers: 
<code><a href="#topic+layer_categorical_mixture_of_one_hot_categorical">layer_categorical_mixture_of_one_hot_categorical</a>()</code>,
<code><a href="#topic+layer_distribution_lambda">layer_distribution_lambda</a>()</code>,
<code><a href="#topic+layer_independent_bernoulli">layer_independent_bernoulli</a>()</code>,
<code><a href="#topic+layer_independent_logistic">layer_independent_logistic</a>()</code>,
<code><a href="#topic+layer_independent_normal">layer_independent_normal</a>()</code>,
<code><a href="#topic+layer_kl_divergence_add_loss">layer_kl_divergence_add_loss</a>()</code>,
<code><a href="#topic+layer_kl_divergence_regularizer">layer_kl_divergence_regularizer</a>()</code>,
<code><a href="#topic+layer_mixture_logistic">layer_mixture_logistic</a>()</code>,
<code><a href="#topic+layer_mixture_normal">layer_mixture_normal</a>()</code>,
<code><a href="#topic+layer_mixture_same_family">layer_mixture_same_family</a>()</code>,
<code><a href="#topic+layer_multivariate_normal_tri_l">layer_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+layer_one_hot_categorical">layer_one_hot_categorical</a>()</code>
</p>

<hr>
<h2 id='layer_kl_divergence_add_loss'>Pass-through layer that adds a KL divergence penalty to the model loss</h2><span id='topic+layer_kl_divergence_add_loss'></span>

<h3>Description</h3>

<p>Pass-through layer that adds a KL divergence penalty to the model loss
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_kl_divergence_add_loss(
  object,
  distribution_b,
  use_exact_kl = FALSE,
  test_points_reduce_axis = NULL,
  test_points_fn = tf$convert_to_tensor,
  weight = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_kl_divergence_add_loss_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_kl_divergence_add_loss_+3A_distribution_b">distribution_b</code></td>
<td>
<p>Distribution instance corresponding to b as in  <code>KL[a, b]</code>.
The previous layer's output is presumed to be a Distribution instance and is a.</p>
</td></tr>
<tr><td><code id="layer_kl_divergence_add_loss_+3A_use_exact_kl">use_exact_kl</code></td>
<td>
<p>Logical indicating if KL divergence should be
calculated exactly via <code>tfp$distributions$kl_divergence</code> or via Monte Carlo approximation.
Default value: FALSE.</p>
</td></tr>
<tr><td><code id="layer_kl_divergence_add_loss_+3A_test_points_reduce_axis">test_points_reduce_axis</code></td>
<td>
<p>Integer vector or scalar representing dimensions
over which to reduce_mean while calculating the Monte Carlo approximation of the KL divergence.
As is with all tf$reduce_* ops, NULL means reduce over all dimensions;
() means reduce over none of them. Default value: () (i.e., no reduction).</p>
</td></tr>
<tr><td><code id="layer_kl_divergence_add_loss_+3A_test_points_fn">test_points_fn</code></td>
<td>
<p>A callable taking a <code>tfp$distributions$Distribution</code> instance and returning a tensor
used for random test points to approximate the KL divergence.
Default value: tf$convert_to_tensor.</p>
</td></tr>
<tr><td><code id="layer_kl_divergence_add_loss_+3A_weight">weight</code></td>
<td>
<p>Multiplier applied to the calculated KL divergence for each Keras batch member.
Default value: NULL (i.e., do not weight each batch member).</p>
</td></tr>
<tr><td><code id="layer_kl_divergence_add_loss_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>args</code> of <code>keras::create_layer</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>See Also</h3>

<p>For an example how to use in a Keras model, see <code><a href="#topic+layer_independent_normal">layer_independent_normal()</a></code>.
</p>
<p>Other distribution_layers: 
<code><a href="#topic+layer_categorical_mixture_of_one_hot_categorical">layer_categorical_mixture_of_one_hot_categorical</a>()</code>,
<code><a href="#topic+layer_distribution_lambda">layer_distribution_lambda</a>()</code>,
<code><a href="#topic+layer_independent_bernoulli">layer_independent_bernoulli</a>()</code>,
<code><a href="#topic+layer_independent_logistic">layer_independent_logistic</a>()</code>,
<code><a href="#topic+layer_independent_normal">layer_independent_normal</a>()</code>,
<code><a href="#topic+layer_independent_poisson">layer_independent_poisson</a>()</code>,
<code><a href="#topic+layer_kl_divergence_regularizer">layer_kl_divergence_regularizer</a>()</code>,
<code><a href="#topic+layer_mixture_logistic">layer_mixture_logistic</a>()</code>,
<code><a href="#topic+layer_mixture_normal">layer_mixture_normal</a>()</code>,
<code><a href="#topic+layer_mixture_same_family">layer_mixture_same_family</a>()</code>,
<code><a href="#topic+layer_multivariate_normal_tri_l">layer_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+layer_one_hot_categorical">layer_one_hot_categorical</a>()</code>
</p>

<hr>
<h2 id='layer_kl_divergence_regularizer'>Regularizer that adds a KL divergence penalty to the model loss</h2><span id='topic+layer_kl_divergence_regularizer'></span>

<h3>Description</h3>

<p>When using Monte Carlo approximation (e.g., <code>use_exact = FALSE</code>), it is presumed that the input
distribution's concretization (i.e., <code>tf$convert_to_tensor(distribution)</code>) corresponds to a random
sample. To override this behavior, set test_points_fn.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_kl_divergence_regularizer(
  object,
  distribution_b,
  use_exact_kl = FALSE,
  test_points_reduce_axis = NULL,
  test_points_fn = tf$convert_to_tensor,
  weight = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_kl_divergence_regularizer_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_kl_divergence_regularizer_+3A_distribution_b">distribution_b</code></td>
<td>
<p>Distribution instance corresponding to b as in  <code>KL[a, b]</code>.
The previous layer's output is presumed to be a Distribution instance and is a.</p>
</td></tr>
<tr><td><code id="layer_kl_divergence_regularizer_+3A_use_exact_kl">use_exact_kl</code></td>
<td>
<p>Logical indicating if KL divergence should be
calculated exactly via <code>tfp$distributions$kl_divergence</code> or via Monte Carlo approximation.
Default value: FALSE.</p>
</td></tr>
<tr><td><code id="layer_kl_divergence_regularizer_+3A_test_points_reduce_axis">test_points_reduce_axis</code></td>
<td>
<p>Integer vector or scalar representing dimensions
over which to reduce_mean while calculating the Monte Carlo approximation of the KL divergence.
As is with all tf$reduce_* ops, NULL means reduce over all dimensions;
() means reduce over none of them. Default value: () (i.e., no reduction).</p>
</td></tr>
<tr><td><code id="layer_kl_divergence_regularizer_+3A_test_points_fn">test_points_fn</code></td>
<td>
<p>A callable taking a <code>tfp$distributions$Distribution</code> instance and returning a tensor
used for random test points to approximate the KL divergence.
Default value: tf$convert_to_tensor.</p>
</td></tr>
<tr><td><code id="layer_kl_divergence_regularizer_+3A_weight">weight</code></td>
<td>
<p>Multiplier applied to the calculated KL divergence for each Keras batch member.
Default value: NULL (i.e., do not weight each batch member).</p>
</td></tr>
<tr><td><code id="layer_kl_divergence_regularizer_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>args</code> of <code>keras::create_layer</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>See Also</h3>

<p>For an example how to use in a Keras model, see <code><a href="#topic+layer_independent_normal">layer_independent_normal()</a></code>.
</p>
<p>Other distribution_layers: 
<code><a href="#topic+layer_categorical_mixture_of_one_hot_categorical">layer_categorical_mixture_of_one_hot_categorical</a>()</code>,
<code><a href="#topic+layer_distribution_lambda">layer_distribution_lambda</a>()</code>,
<code><a href="#topic+layer_independent_bernoulli">layer_independent_bernoulli</a>()</code>,
<code><a href="#topic+layer_independent_logistic">layer_independent_logistic</a>()</code>,
<code><a href="#topic+layer_independent_normal">layer_independent_normal</a>()</code>,
<code><a href="#topic+layer_independent_poisson">layer_independent_poisson</a>()</code>,
<code><a href="#topic+layer_kl_divergence_add_loss">layer_kl_divergence_add_loss</a>()</code>,
<code><a href="#topic+layer_mixture_logistic">layer_mixture_logistic</a>()</code>,
<code><a href="#topic+layer_mixture_normal">layer_mixture_normal</a>()</code>,
<code><a href="#topic+layer_mixture_same_family">layer_mixture_same_family</a>()</code>,
<code><a href="#topic+layer_multivariate_normal_tri_l">layer_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+layer_one_hot_categorical">layer_one_hot_categorical</a>()</code>
</p>

<hr>
<h2 id='layer_mixture_logistic'>A mixture distribution Keras layer, with independent logistic components.</h2><span id='topic+layer_mixture_logistic'></span>

<h3>Description</h3>

<p>A mixture distribution Keras layer, with independent logistic components.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_mixture_logistic(
  object,
  num_components,
  event_shape = list(),
  convert_to_tensor_fn = tfp$distributions$Distribution$sample,
  validate_args = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_mixture_logistic_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_mixture_logistic_+3A_num_components">num_components</code></td>
<td>
<p>Number of component distributions in the mixture distribution.</p>
</td></tr>
<tr><td><code id="layer_mixture_logistic_+3A_event_shape">event_shape</code></td>
<td>
<p>integer vector <code>Tensor</code> representing the shape of single
draw from this distribution.</p>
</td></tr>
<tr><td><code id="layer_mixture_logistic_+3A_convert_to_tensor_fn">convert_to_tensor_fn</code></td>
<td>
<p>A callable that takes a tfd$Distribution instance and returns a
tf$Tensor-like object. Default value: <code>tfd$distributions$Distribution$sample</code>.</p>
</td></tr>
<tr><td><code id="layer_mixture_logistic_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.
@param ... Additional arguments passed to <code>args</code> of <code>keras::create_layer</code>.</p>
</td></tr>
<tr><td><code id="layer_mixture_logistic_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>args</code> of <code>keras::create_layer</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>See Also</h3>

<p>For an example how to use in a Keras model, see <code><a href="#topic+layer_independent_normal">layer_independent_normal()</a></code>.
</p>
<p>Other distribution_layers: 
<code><a href="#topic+layer_categorical_mixture_of_one_hot_categorical">layer_categorical_mixture_of_one_hot_categorical</a>()</code>,
<code><a href="#topic+layer_distribution_lambda">layer_distribution_lambda</a>()</code>,
<code><a href="#topic+layer_independent_bernoulli">layer_independent_bernoulli</a>()</code>,
<code><a href="#topic+layer_independent_logistic">layer_independent_logistic</a>()</code>,
<code><a href="#topic+layer_independent_normal">layer_independent_normal</a>()</code>,
<code><a href="#topic+layer_independent_poisson">layer_independent_poisson</a>()</code>,
<code><a href="#topic+layer_kl_divergence_add_loss">layer_kl_divergence_add_loss</a>()</code>,
<code><a href="#topic+layer_kl_divergence_regularizer">layer_kl_divergence_regularizer</a>()</code>,
<code><a href="#topic+layer_mixture_normal">layer_mixture_normal</a>()</code>,
<code><a href="#topic+layer_mixture_same_family">layer_mixture_same_family</a>()</code>,
<code><a href="#topic+layer_multivariate_normal_tri_l">layer_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+layer_one_hot_categorical">layer_one_hot_categorical</a>()</code>
</p>

<hr>
<h2 id='layer_mixture_normal'>A mixture distribution Keras layer, with independent normal components.</h2><span id='topic+layer_mixture_normal'></span>

<h3>Description</h3>

<p>A mixture distribution Keras layer, with independent normal components.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_mixture_normal(
  object,
  num_components,
  event_shape = list(),
  convert_to_tensor_fn = tfp$distributions$Distribution$sample,
  validate_args = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_mixture_normal_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_mixture_normal_+3A_num_components">num_components</code></td>
<td>
<p>Number of component distributions in the mixture distribution.</p>
</td></tr>
<tr><td><code id="layer_mixture_normal_+3A_event_shape">event_shape</code></td>
<td>
<p>integer vector <code>Tensor</code> representing the shape of single
draw from this distribution.</p>
</td></tr>
<tr><td><code id="layer_mixture_normal_+3A_convert_to_tensor_fn">convert_to_tensor_fn</code></td>
<td>
<p>A callable that takes a tfd$Distribution instance and returns a
tf$Tensor-like object. Default value: <code>tfd$distributions$Distribution$sample</code>.</p>
</td></tr>
<tr><td><code id="layer_mixture_normal_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.
@param ... Additional arguments passed to <code>args</code> of <code>keras::create_layer</code>.</p>
</td></tr>
<tr><td><code id="layer_mixture_normal_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>args</code> of <code>keras::create_layer</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>See Also</h3>

<p>For an example how to use in a Keras model, see <code><a href="#topic+layer_independent_normal">layer_independent_normal()</a></code>.
</p>
<p>Other distribution_layers: 
<code><a href="#topic+layer_categorical_mixture_of_one_hot_categorical">layer_categorical_mixture_of_one_hot_categorical</a>()</code>,
<code><a href="#topic+layer_distribution_lambda">layer_distribution_lambda</a>()</code>,
<code><a href="#topic+layer_independent_bernoulli">layer_independent_bernoulli</a>()</code>,
<code><a href="#topic+layer_independent_logistic">layer_independent_logistic</a>()</code>,
<code><a href="#topic+layer_independent_normal">layer_independent_normal</a>()</code>,
<code><a href="#topic+layer_independent_poisson">layer_independent_poisson</a>()</code>,
<code><a href="#topic+layer_kl_divergence_add_loss">layer_kl_divergence_add_loss</a>()</code>,
<code><a href="#topic+layer_kl_divergence_regularizer">layer_kl_divergence_regularizer</a>()</code>,
<code><a href="#topic+layer_mixture_logistic">layer_mixture_logistic</a>()</code>,
<code><a href="#topic+layer_mixture_same_family">layer_mixture_same_family</a>()</code>,
<code><a href="#topic+layer_multivariate_normal_tri_l">layer_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+layer_one_hot_categorical">layer_one_hot_categorical</a>()</code>
</p>

<hr>
<h2 id='layer_mixture_same_family'>A mixture (same-family) Keras layer.</h2><span id='topic+layer_mixture_same_family'></span>

<h3>Description</h3>

<p>A mixture (same-family) Keras layer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_mixture_same_family(
  object,
  num_components,
  component_layer,
  convert_to_tensor_fn = tfp$distributions$Distribution$sample,
  validate_args = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_mixture_same_family_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_mixture_same_family_+3A_num_components">num_components</code></td>
<td>
<p>Number of component distributions in the mixture distribution.</p>
</td></tr>
<tr><td><code id="layer_mixture_same_family_+3A_component_layer">component_layer</code></td>
<td>
<p>Function that, given a tensor of shape
<code style="white-space: pre;">&#8288;batch_shape + [num_components, component_params_size]&#8288;</code>, returns a
<code>tfd.Distribution</code>-like instance that implements the component
distribution (with batch shape <code style="white-space: pre;">&#8288;batch_shape + [num_components]&#8288;</code>) &ndash;
e.g., a TFP distribution layer.</p>
</td></tr>
<tr><td><code id="layer_mixture_same_family_+3A_convert_to_tensor_fn">convert_to_tensor_fn</code></td>
<td>
<p>A callable that takes a tfd$Distribution instance and returns a
tf$Tensor-like object. Default value: <code>tfd$distributions$Distribution$sample</code>.</p>
</td></tr>
<tr><td><code id="layer_mixture_same_family_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.
@param ... Additional arguments passed to <code>args</code> of <code>keras::create_layer</code>.</p>
</td></tr>
<tr><td><code id="layer_mixture_same_family_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>args</code> of <code>keras::create_layer</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>See Also</h3>

<p>For an example how to use in a Keras model, see <code><a href="#topic+layer_independent_normal">layer_independent_normal()</a></code>.
</p>
<p>Other distribution_layers: 
<code><a href="#topic+layer_categorical_mixture_of_one_hot_categorical">layer_categorical_mixture_of_one_hot_categorical</a>()</code>,
<code><a href="#topic+layer_distribution_lambda">layer_distribution_lambda</a>()</code>,
<code><a href="#topic+layer_independent_bernoulli">layer_independent_bernoulli</a>()</code>,
<code><a href="#topic+layer_independent_logistic">layer_independent_logistic</a>()</code>,
<code><a href="#topic+layer_independent_normal">layer_independent_normal</a>()</code>,
<code><a href="#topic+layer_independent_poisson">layer_independent_poisson</a>()</code>,
<code><a href="#topic+layer_kl_divergence_add_loss">layer_kl_divergence_add_loss</a>()</code>,
<code><a href="#topic+layer_kl_divergence_regularizer">layer_kl_divergence_regularizer</a>()</code>,
<code><a href="#topic+layer_mixture_logistic">layer_mixture_logistic</a>()</code>,
<code><a href="#topic+layer_mixture_normal">layer_mixture_normal</a>()</code>,
<code><a href="#topic+layer_multivariate_normal_tri_l">layer_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+layer_one_hot_categorical">layer_one_hot_categorical</a>()</code>
</p>

<hr>
<h2 id='layer_multivariate_normal_tri_l'>A d-variate Multivariate Normal TriL Keras layer from <code>d+d*(d+1)/ 2</code> params</h2><span id='topic+layer_multivariate_normal_tri_l'></span>

<h3>Description</h3>

<p>A d-variate Multivariate Normal TriL Keras layer from <code>d+d*(d+1)/ 2</code> params
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_multivariate_normal_tri_l(
  object,
  event_size,
  convert_to_tensor_fn = tfp$distributions$Distribution$sample,
  validate_args = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_multivariate_normal_tri_l_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_multivariate_normal_tri_l_+3A_event_size">event_size</code></td>
<td>
<p>Integer vector tensor representing the shape of single draw from this distribution.</p>
</td></tr>
<tr><td><code id="layer_multivariate_normal_tri_l_+3A_convert_to_tensor_fn">convert_to_tensor_fn</code></td>
<td>
<p>A callable that takes a tfd$Distribution instance and returns a
tf$Tensor-like object. Default value: <code>tfd$distributions$Distribution$sample</code>.</p>
</td></tr>
<tr><td><code id="layer_multivariate_normal_tri_l_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="layer_multivariate_normal_tri_l_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>args</code> of <code>keras::create_layer</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>See Also</h3>

<p>For an example how to use in a Keras model, see <code><a href="#topic+layer_independent_normal">layer_independent_normal()</a></code>.
</p>
<p>Other distribution_layers: 
<code><a href="#topic+layer_categorical_mixture_of_one_hot_categorical">layer_categorical_mixture_of_one_hot_categorical</a>()</code>,
<code><a href="#topic+layer_distribution_lambda">layer_distribution_lambda</a>()</code>,
<code><a href="#topic+layer_independent_bernoulli">layer_independent_bernoulli</a>()</code>,
<code><a href="#topic+layer_independent_logistic">layer_independent_logistic</a>()</code>,
<code><a href="#topic+layer_independent_normal">layer_independent_normal</a>()</code>,
<code><a href="#topic+layer_independent_poisson">layer_independent_poisson</a>()</code>,
<code><a href="#topic+layer_kl_divergence_add_loss">layer_kl_divergence_add_loss</a>()</code>,
<code><a href="#topic+layer_kl_divergence_regularizer">layer_kl_divergence_regularizer</a>()</code>,
<code><a href="#topic+layer_mixture_logistic">layer_mixture_logistic</a>()</code>,
<code><a href="#topic+layer_mixture_normal">layer_mixture_normal</a>()</code>,
<code><a href="#topic+layer_mixture_same_family">layer_mixture_same_family</a>()</code>,
<code><a href="#topic+layer_one_hot_categorical">layer_one_hot_categorical</a>()</code>
</p>

<hr>
<h2 id='layer_one_hot_categorical'>A <code>d</code>-variate OneHotCategorical Keras layer from <code>d</code> params.</h2><span id='topic+layer_one_hot_categorical'></span>

<h3>Description</h3>

<p>Typical choices for <code>convert_to_tensor_fn</code> include:
</p>

<ul>
<li> <p><code>tfp$distributions$Distribution$sample</code>
</p>
</li>
<li> <p><code>tfp$distributions$Distribution$mean</code>
</p>
</li>
<li> <p><code>tfp$distributions$Distribution$mode</code>
</p>
</li>
<li> <p><code>tfp$distributions$OneHotCategorical$logits</code>
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>layer_one_hot_categorical(
  object,
  event_size,
  convert_to_tensor_fn = tfp$distributions$Distribution$sample,
  sample_dtype = NULL,
  validate_args = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_one_hot_categorical_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_one_hot_categorical_+3A_event_size">event_size</code></td>
<td>
<p>Scalar <code>integer</code> representing the size of single draw from this distribution.</p>
</td></tr>
<tr><td><code id="layer_one_hot_categorical_+3A_convert_to_tensor_fn">convert_to_tensor_fn</code></td>
<td>
<p>A callable that takes a tfd$Distribution instance and returns a
tf$Tensor-like object. Default value: <code>tfd$distributions$Distribution$sample</code>.</p>
</td></tr>
<tr><td><code id="layer_one_hot_categorical_+3A_sample_dtype">sample_dtype</code></td>
<td>
<p><code>dtype</code> of samples produced by this distribution.
Default value: <code>NULL</code> (i.e., previous layer's <code>dtype</code>).</p>
</td></tr>
<tr><td><code id="layer_one_hot_categorical_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="layer_one_hot_categorical_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>args</code> of <code>keras::create_layer</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>See Also</h3>

<p>For an example how to use in a Keras model, see <code><a href="#topic+layer_independent_normal">layer_independent_normal()</a></code>.
</p>
<p>Other distribution_layers: 
<code><a href="#topic+layer_categorical_mixture_of_one_hot_categorical">layer_categorical_mixture_of_one_hot_categorical</a>()</code>,
<code><a href="#topic+layer_distribution_lambda">layer_distribution_lambda</a>()</code>,
<code><a href="#topic+layer_independent_bernoulli">layer_independent_bernoulli</a>()</code>,
<code><a href="#topic+layer_independent_logistic">layer_independent_logistic</a>()</code>,
<code><a href="#topic+layer_independent_normal">layer_independent_normal</a>()</code>,
<code><a href="#topic+layer_independent_poisson">layer_independent_poisson</a>()</code>,
<code><a href="#topic+layer_kl_divergence_add_loss">layer_kl_divergence_add_loss</a>()</code>,
<code><a href="#topic+layer_kl_divergence_regularizer">layer_kl_divergence_regularizer</a>()</code>,
<code><a href="#topic+layer_mixture_logistic">layer_mixture_logistic</a>()</code>,
<code><a href="#topic+layer_mixture_normal">layer_mixture_normal</a>()</code>,
<code><a href="#topic+layer_mixture_same_family">layer_mixture_same_family</a>()</code>,
<code><a href="#topic+layer_multivariate_normal_tri_l">layer_multivariate_normal_tri_l</a>()</code>
</p>

<hr>
<h2 id='layer_variable'>Variable Layer</h2><span id='topic+layer_variable'></span>

<h3>Description</h3>

<p>Simply returns a (trainable) variable, regardless of input.
This layer implements the mathematical function <code>f(x) = c</code> where <code>c</code> is a
constant, i.e., unchanged for all <code>x</code>. Like other Keras layers, the constant
is <code>trainable</code>.  This layer can also be interpretted as the special case of
<code>layer_dense()</code> when the <code>kernel</code> is forced to be the zero matrix
(<code>tf$zeros</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_variable(
  object,
  shape,
  dtype = NULL,
  activation = NULL,
  initializer = "zeros",
  regularizer = NULL,
  constraint = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_variable_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_variable_+3A_shape">shape</code></td>
<td>
<p>integer or integer vector specifying the shape of the output of this layer.</p>
</td></tr>
<tr><td><code id="layer_variable_+3A_dtype">dtype</code></td>
<td>
<p>TensorFlow <code>dtype</code> of the variable created by this layer.</p>
</td></tr>
<tr><td><code id="layer_variable_+3A_activation">activation</code></td>
<td>
<p>An activation function.  See <code>keras::layer_dense</code>. Default: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="layer_variable_+3A_initializer">initializer</code></td>
<td>
<p>Initializer for the <code>constant</code> vector.</p>
</td></tr>
<tr><td><code id="layer_variable_+3A_regularizer">regularizer</code></td>
<td>
<p>Regularizer function applied to the <code>constant</code> vector.</p>
</td></tr>
<tr><td><code id="layer_variable_+3A_constraint">constraint</code></td>
<td>
<p>Constraint function applied to the <code>constant</code> vector.</p>
</td></tr>
<tr><td><code id="layer_variable_+3A_...">...</code></td>
<td>
<p>Additional keyword arguments passed to the <code>keras::layer_dense</code> constructed by this layer.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>See Also</h3>

<p>Other layers: 
<code><a href="#topic+layer_autoregressive">layer_autoregressive</a>()</code>,
<code><a href="#topic+layer_conv_1d_flipout">layer_conv_1d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_1d_reparameterization">layer_conv_1d_reparameterization</a>()</code>,
<code><a href="#topic+layer_conv_2d_flipout">layer_conv_2d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_2d_reparameterization">layer_conv_2d_reparameterization</a>()</code>,
<code><a href="#topic+layer_conv_3d_flipout">layer_conv_3d_flipout</a>()</code>,
<code><a href="#topic+layer_conv_3d_reparameterization">layer_conv_3d_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_flipout">layer_dense_flipout</a>()</code>,
<code><a href="#topic+layer_dense_local_reparameterization">layer_dense_local_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_reparameterization">layer_dense_reparameterization</a>()</code>,
<code><a href="#topic+layer_dense_variational">layer_dense_variational</a>()</code>
</p>

<hr>
<h2 id='layer_variational_gaussian_process'>A Variational Gaussian Process Layer.</h2><span id='topic+layer_variational_gaussian_process'></span>

<h3>Description</h3>

<p>Create a Variational Gaussian Process distribution whose <code>index_points</code> are
the inputs to the layer. Parameterized by number of inducing points and a
<code>kernel_provider</code>, which should be a <code>tf.keras.Layer</code> with an @property that
late-binds variable parameters to a <code>tfp.positive_semidefinite_kernel.PositiveSemidefiniteKernel</code>
instance (this requirement has to do with the way that variables must be created
in a keras model). The mean_fn is an optional argument which, if omitted, will
be automatically configured to be a constant function with trainable variable
output.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_variational_gaussian_process(
  object,
  num_inducing_points,
  kernel_provider,
  event_shape = 1,
  inducing_index_points_initializer = NULL,
  unconstrained_observation_noise_variance_initializer = NULL,
  mean_fn = NULL,
  jitter = 1e-06,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_variational_gaussian_process_+3A_object">object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li><p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li><p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li><p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li></ul>
</td></tr>
<tr><td><code id="layer_variational_gaussian_process_+3A_num_inducing_points">num_inducing_points</code></td>
<td>
<p>number of inducing points in the Variational Gaussian
Process distribution.</p>
</td></tr>
<tr><td><code id="layer_variational_gaussian_process_+3A_kernel_provider">kernel_provider</code></td>
<td>
<p>a <code>Layer</code> instance equipped with an <code style="white-space: pre;">&#8288;@property&#8288;</code>, which
yields a <code>PositiveSemidefiniteKernel</code> instance. The latter is used to parametrize
the constructed Variational Gaussian Process distribution returned by calling
the layer.</p>
</td></tr>
<tr><td><code id="layer_variational_gaussian_process_+3A_event_shape">event_shape</code></td>
<td>
<p>the shape of the output of the layer. This translates to a
batch of underlying Variational Gaussian Process distributions. For example,
<code>event_shape = 3</code> means we are modelling a batch of 3 distributions over functions.
We can think oof this as a distribution over 3-dimensional veector-valued
functions.</p>
</td></tr>
<tr><td><code id="layer_variational_gaussian_process_+3A_inducing_index_points_initializer">inducing_index_points_initializer</code></td>
<td>
<p>a <code>tf.keras.initializer.Initializer</code>
used to initialize the trainable <code style="white-space: pre;">&#8288;inducing_index_points variables&#8288;</code>. Training
VGP's is pretty sensitive to choice of initial inducing index point locations.
A reasonable heuristic is to scatter them near the data, not too close to each
other.</p>
</td></tr>
<tr><td><code id="layer_variational_gaussian_process_+3A_unconstrained_observation_noise_variance_initializer">unconstrained_observation_noise_variance_initializer</code></td>
<td>
<p>a <code>tf.keras.initializer.Initializer</code>
used to initialize the unconstrained observation noise variable. The observation
noise variance is computed from this variable via the <code>tf.nn.softplus</code> function.</p>
</td></tr>
<tr><td><code id="layer_variational_gaussian_process_+3A_mean_fn">mean_fn</code></td>
<td>
<p>a callable that maps layer inputs to mean function values.
Passed to the mean_fn parameter of Variational Gaussian Process distribution.
If omitted, defaults to a constant function with trainable variable value.</p>
</td></tr>
<tr><td><code id="layer_variational_gaussian_process_+3A_jitter">jitter</code></td>
<td>
<p>a small term added to the diagonal of various kernel matrices for
numerical stability.</p>
</td></tr>
<tr><td><code id="layer_variational_gaussian_process_+3A_name">name</code></td>
<td>
<p>name to give to this layer and the scope of ops and variables it
contains.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Keras layer
</p>

<hr>
<h2 id='mcmc_dual_averaging_step_size_adaptation'>Adapts the inner kernel's <code>step_size</code> based on <code>log_accept_prob</code>.</h2><span id='topic+mcmc_dual_averaging_step_size_adaptation'></span>

<h3>Description</h3>

<p>The dual averaging policy uses a noisy step size for exploration, while
averaging over tuning steps to provide a smoothed estimate of an optimal
value. It is based on section 3.2 of Hoffman and Gelman (2013), which
modifies the [stochastic convex optimization scheme of Nesterov (2009).
The modified algorithm applies extra weight to recent iterations while
keeping the convergence guarantees of Robbins-Monro, and takes care not
to make the step size too small too quickly when maintaining a constant
trajectory length, to avoid expensive early iterations. A good target
acceptance probability depends on the inner kernel. If this kernel is
<code>HamiltonianMonteCarlo</code>, then 0.6-0.9 is a good range to aim for. For
<code>RandomWalkMetropolis</code> this should be closer to 0.25. See the individual
kernels' docstrings for guidance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc_dual_averaging_step_size_adaptation(
  inner_kernel,
  num_adaptation_steps,
  target_accept_prob = 0.75,
  exploration_shrinkage = 0.05,
  step_count_smoothing = 10,
  decay_rate = 0.75,
  step_size_setter_fn = NULL,
  step_size_getter_fn = NULL,
  log_accept_prob_getter_fn = NULL,
  validate_args = FALSE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc_dual_averaging_step_size_adaptation_+3A_inner_kernel">inner_kernel</code></td>
<td>
<p><code>TransitionKernel</code>-like object.</p>
</td></tr>
<tr><td><code id="mcmc_dual_averaging_step_size_adaptation_+3A_num_adaptation_steps">num_adaptation_steps</code></td>
<td>
<p>Scalar <code>integer</code> <code>Tensor</code> number of initial steps to
during which to adjust the step size. This may be greater, less than, or
equal to the number of burnin steps.</p>
</td></tr>
<tr><td><code id="mcmc_dual_averaging_step_size_adaptation_+3A_target_accept_prob">target_accept_prob</code></td>
<td>
<p>A floating point <code>Tensor</code> representing desired
acceptance probability. Must be a positive number less than 1. This can
either be a scalar, or have shape <code style="white-space: pre;">&#8288;[num_chains]&#8288;</code>. Default value: <code>0.75</code>
(the center of asymptotically optimal rate for HMC).</p>
</td></tr>
<tr><td><code id="mcmc_dual_averaging_step_size_adaptation_+3A_exploration_shrinkage">exploration_shrinkage</code></td>
<td>
<p>Floating point scalar <code>Tensor</code>. How strongly the
exploration rate is biased towards the shrinkage target.</p>
</td></tr>
<tr><td><code id="mcmc_dual_averaging_step_size_adaptation_+3A_step_count_smoothing">step_count_smoothing</code></td>
<td>
<p>Int32 scalar <code>Tensor</code>. Number of &quot;pseudo-steps&quot;
added to the number of steps taken to prevents noisy exploration during
the early samples.</p>
</td></tr>
<tr><td><code id="mcmc_dual_averaging_step_size_adaptation_+3A_decay_rate">decay_rate</code></td>
<td>
<p>Floating point scalar <code>Tensor</code>. How much to favor recent
iterations over earlier ones. A value of 1 gives equal weight to all
history.</p>
</td></tr>
<tr><td><code id="mcmc_dual_averaging_step_size_adaptation_+3A_step_size_setter_fn">step_size_setter_fn</code></td>
<td>
<p>A function with the signature
<code style="white-space: pre;">&#8288;(kernel_results, new_step_size) -&gt; new_kernel_results&#8288;</code> where <code>kernel_results</code> are the
results of the <code>inner_kernel</code>, <code>new_step_size</code> is a <code>Tensor</code> or a nested
collection of <code>Tensor</code>s with the same structure as returned by the
<code>step_size_getter_fn</code>, and <code>new_kernel_results</code> are a copy of
<code>kernel_results</code> with the step size(s) set.</p>
</td></tr>
<tr><td><code id="mcmc_dual_averaging_step_size_adaptation_+3A_step_size_getter_fn">step_size_getter_fn</code></td>
<td>
<p>A callable with the signature
<code>(kernel_results) -&gt; step_size</code> where <code>kernel_results</code> are the results of the <code>inner_kernel</code>,
and <code>step_size</code> is a floating point <code>Tensor</code> or a nested collection of
such <code>Tensor</code>s.</p>
</td></tr>
<tr><td><code id="mcmc_dual_averaging_step_size_adaptation_+3A_log_accept_prob_getter_fn">log_accept_prob_getter_fn</code></td>
<td>
<p>A callable with the signature
<code>(kernel_results) -&gt; log_accept_prob</code> where <code>kernel_results</code> are the results of the
<code>inner_kernel</code>, and <code>log_accept_prob</code> is a floating point <code>Tensor</code>.
<code>log_accept_prob</code> can either be a scalar, or have shape <code style="white-space: pre;">&#8288;[num_chains]&#8288;</code>. If
it's the latter, <code>step_size</code> should also have the same leading
dimension.</p>
</td></tr>
<tr><td><code id="mcmc_dual_averaging_step_size_adaptation_+3A_validate_args">validate_args</code></td>
<td>
<p><code>logical</code>. When <code>TRUE</code> kernel parameters are checked
for validity. When <code>FALSE</code> invalid inputs may silently render incorrect
outputs.</p>
</td></tr>
<tr><td><code id="mcmc_dual_averaging_step_size_adaptation_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this function.
Default value: <code>NULL</code> (i.e., 'dual_averaging_step_size_adaptation').</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In general, adaptation prevents the chain from reaching a stationary
distribution, so obtaining consistent samples requires <code>num_adaptation_steps</code>
be set to a value somewhat smaller than the number of burnin steps.
However, it may sometimes be helpful to set <code>num_adaptation_steps</code> to a larger
value during development in order to inspect the behavior of the chain during
adaptation.
The step size is assumed to broadcast with the chain state, potentially having
leading dimensions corresponding to multiple chains. When there are fewer of
those leading dimensions than there are chain dimensions, the corresponding
dimensions in the <code>log_accept_prob</code> are averaged (in the direct space, rather
than the log space) before being used to adjust the step size. This means that
this kernel can do both cross-chain adaptation, or per-chain step size
adaptation, depending on the shape of the step size.
For example, if your problem has a state with shape <code style="white-space: pre;">&#8288;[S]&#8288;</code>, your chain state
has shape <code style="white-space: pre;">&#8288;[C0, C1, S]&#8288;</code> (meaning that there are <code>C0 * C1</code> total chains) and
<code>log_accept_prob</code> has shape <code style="white-space: pre;">&#8288;[C0, C1]&#8288;</code> (one acceptance probability per chain),
then depending on the shape of the step size, the following will happen:
</p>

<ul>
<li><p> Step size has shape <code style="white-space: pre;">&#8288;[]&#8288;</code>, <code style="white-space: pre;">&#8288;[S]&#8288;</code> or <code style="white-space: pre;">&#8288;[1]&#8288;</code>, the <code>log_accept_prob</code> will be averaged
across its <code>C0</code> and <code>C1</code> dimensions. This means that you will learn a shared
step size based on the mean acceptance probability across all chains. This
can be useful if you don't have a lot of steps to adapt and want to average
away the noise.
</p>
</li>
<li><p> Step size has shape <code style="white-space: pre;">&#8288;[C1, 1]&#8288;</code> or <code style="white-space: pre;">&#8288;[C1, S]&#8288;</code>, the <code>log_accept_prob</code> will be
averaged across its <code>C0</code> dimension. This means that you will learn a shared
step size based on the mean acceptance probability across chains that share
the coordinate across the <code>C1</code> dimension. This can be useful when the <code>C1</code>
dimension indexes different distributions, while <code>C0</code> indexes replicas of a
single distribution, all sampled in parallel.
</p>
</li>
<li><p> Step size has shape <code style="white-space: pre;">&#8288;[C0, C1, 1]&#8288;</code> or <code style="white-space: pre;">&#8288;[C0, C1, S]&#8288;</code>, then no averaging will
happen. This means that each chain will learn its own step size. This can be
useful when all chains are sampling from different distributions. Even when
all chains are for the same distribution, this can help during the initial
warmup period.
</p>
</li>
<li><p> Step size has shape <code style="white-space: pre;">&#8288;[C0, 1, 1]&#8288;</code> or <code style="white-space: pre;">&#8288;[C0, 1, S]&#8288;</code>, the <code>log_accept_prob</code> will be
averaged across its <code>C1</code> dimension. This means that you will learn a shared
step size based on the mean acceptance probability across chains that share
the coordinate across the <code>C0</code> dimension. This can be useful when the <code>C0</code>
dimension indexes different distributions, while <code>C1</code> indexes replicas of a
single distribution, all sampled in parallel.
</p>
</li></ul>



<h3>Value</h3>

<p>a Monte Carlo sampling kernel
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf">Matthew D. Hoffman, Andrew Gelman. The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo. In <em>Journal of Machine Learning Research</em>, 15(1):1593-1623, 2014.</a>
</p>
</li>
<li> <p><a href="https://link.springer.com/article/10.1007/s10107-007-0149-x">Yurii Nesterov. Primal-dual subgradient methods for convex problems. Mathematical programming 120.1 (2009): 221-259</a>
</p>
</li>
<li> <p><a href="https://statmodeling.stat.columbia.edu/2017/12/15/burn-vs-warm-iterative-simulation-algorithms/">https://statmodeling.stat.columbia.edu/2017/12/15/burn-vs-warm-iterative-simulation-algorithms/</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>For an example how to use see <code><a href="#topic+mcmc_no_u_turn_sampler">mcmc_no_u_turn_sampler()</a></code>.
</p>
<p>Other mcmc_kernels: 
<code><a href="#topic+mcmc_hamiltonian_monte_carlo">mcmc_hamiltonian_monte_carlo</a>()</code>,
<code><a href="#topic+mcmc_metropolis_adjusted_langevin_algorithm">mcmc_metropolis_adjusted_langevin_algorithm</a>()</code>,
<code><a href="#topic+mcmc_metropolis_hastings">mcmc_metropolis_hastings</a>()</code>,
<code><a href="#topic+mcmc_no_u_turn_sampler">mcmc_no_u_turn_sampler</a>()</code>,
<code><a href="#topic+mcmc_random_walk_metropolis">mcmc_random_walk_metropolis</a>()</code>,
<code><a href="#topic+mcmc_replica_exchange_mc">mcmc_replica_exchange_mc</a>()</code>,
<code><a href="#topic+mcmc_simple_step_size_adaptation">mcmc_simple_step_size_adaptation</a>()</code>,
<code><a href="#topic+mcmc_slice_sampler">mcmc_slice_sampler</a>()</code>,
<code><a href="#topic+mcmc_transformed_transition_kernel">mcmc_transformed_transition_kernel</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_hamiltonian_monte_carlo">mcmc_uncalibrated_hamiltonian_monte_carlo</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_langevin">mcmc_uncalibrated_langevin</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_random_walk">mcmc_uncalibrated_random_walk</a>()</code>
</p>

<hr>
<h2 id='mcmc_effective_sample_size'>Estimate a lower bound on effective sample size for each independent chain.</h2><span id='topic+mcmc_effective_sample_size'></span>

<h3>Description</h3>

<p>Roughly speaking, &quot;effective sample size&quot; (ESS) is the size of an iid sample
with the same variance as <code>state</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc_effective_sample_size(
  states,
  filter_threshold = 0,
  filter_beyond_lag = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc_effective_sample_size_+3A_states">states</code></td>
<td>
<p><code>Tensor</code> or list of <code>Tensor</code> objects.  Dimension zero should index
identically distributed states.</p>
</td></tr>
<tr><td><code id="mcmc_effective_sample_size_+3A_filter_threshold">filter_threshold</code></td>
<td>
<p><code>Tensor</code> or list of <code>Tensor</code> objects.
Must broadcast with <code>state</code>.  The auto-correlation sequence is truncated
after the first appearance of a term less than <code>filter_threshold</code>.
Setting to <code>NULL</code> means we use no threshold filter.  Since <code style="white-space: pre;">&#8288;|R_k| &lt;= 1&#8288;</code>,
setting to any number less than <code>-1</code> has the same effect.</p>
</td></tr>
<tr><td><code id="mcmc_effective_sample_size_+3A_filter_beyond_lag">filter_beyond_lag</code></td>
<td>
<p><code>Tensor</code> or list of <code>Tensor</code> objects.  Must be
<code>int</code>-like and scalar valued.  The auto-correlation sequence is truncated
to this length.  Setting to <code>NULL</code> means we do not filter based on number of lags.</p>
</td></tr>
<tr><td><code id="mcmc_effective_sample_size_+3A_name">name</code></td>
<td>
<p>name to prepend to created ops.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>More precisely, given a stationary sequence of possibly correlated random
variables <code style="white-space: pre;">&#8288;X_1, X_2,...,X_N&#8288;</code>, each identically distributed ESS is the number
such that
<code style="white-space: pre;">&#8288;Variance{ N**-1 * Sum{X_i} } = ESS**-1 * Variance{ X_1 }.&#8288;</code>
</p>
<p>If the sequence is uncorrelated, <code>ESS = N</code>.  In general, one should expect
<code>ESS &lt;= N</code>, with more highly correlated sequences having smaller <code>ESS</code>.
</p>


<h3>Value</h3>

<p><code>Tensor</code> or list of <code>Tensor</code> objects.  The effective sample size of
each component of <code>states</code>.  Shape will be <code style="white-space: pre;">&#8288;states$shape[1:]&#8288;</code>.
</p>


<h3>See Also</h3>

<p>Other mcmc_functions: 
<code><a href="#topic+mcmc_potential_scale_reduction">mcmc_potential_scale_reduction</a>()</code>,
<code><a href="#topic+mcmc_sample_annealed_importance_chain">mcmc_sample_annealed_importance_chain</a>()</code>,
<code><a href="#topic+mcmc_sample_chain">mcmc_sample_chain</a>()</code>,
<code><a href="#topic+mcmc_sample_halton_sequence">mcmc_sample_halton_sequence</a>()</code>
</p>

<hr>
<h2 id='mcmc_hamiltonian_monte_carlo'>Runs one step of Hamiltonian Monte Carlo.</h2><span id='topic+mcmc_hamiltonian_monte_carlo'></span>

<h3>Description</h3>

<p>Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm
that takes a series of gradient-informed steps to produce a Metropolis
proposal. This class implements one random HMC step from a given
<code>current_state</code>. Mathematical details and derivations can be found in
Neal (2011).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc_hamiltonian_monte_carlo(
  target_log_prob_fn,
  step_size,
  num_leapfrog_steps,
  state_gradients_are_stopped = FALSE,
  step_size_update_fn = NULL,
  seed = NULL,
  store_parameters_in_results = FALSE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc_hamiltonian_monte_carlo_+3A_target_log_prob_fn">target_log_prob_fn</code></td>
<td>
<p>Function which takes an argument like
<code>current_state</code> (if it's a list <code>current_state</code> will be unpacked) and returns its
(possibly unnormalized) log-density under the target distribution.</p>
</td></tr>
<tr><td><code id="mcmc_hamiltonian_monte_carlo_+3A_step_size">step_size</code></td>
<td>
<p><code>Tensor</code> or <code>list</code> of <code>Tensor</code>s representing the step
size for the leapfrog integrator. Must broadcast with the shape of
<code>current_state</code>. Larger step sizes lead to faster progress, but
too-large step sizes make rejection exponentially more likely. When
possible, it's often helpful to match per-variable step sizes to the
standard deviations of the target distribution in each variable.</p>
</td></tr>
<tr><td><code id="mcmc_hamiltonian_monte_carlo_+3A_num_leapfrog_steps">num_leapfrog_steps</code></td>
<td>
<p>Integer number of steps to run the leapfrog integrator
for. Total progress per HMC step is roughly proportional to
<code>step_size * num_leapfrog_steps</code>.</p>
</td></tr>
<tr><td><code id="mcmc_hamiltonian_monte_carlo_+3A_state_gradients_are_stopped">state_gradients_are_stopped</code></td>
<td>
<p><code>logical</code> indicating that the proposed
new state be run through <code>tf$stop_gradient</code>. This is particularly useful
when combining optimization over samples from the HMC chain.
Default value: <code>FALSE</code> (i.e., do not apply <code>stop_gradient</code>).</p>
</td></tr>
<tr><td><code id="mcmc_hamiltonian_monte_carlo_+3A_step_size_update_fn">step_size_update_fn</code></td>
<td>
<p>Function taking current <code>step_size</code>
(typically a <code>tf$Variable</code>) and <code>kernel_results</code> (typically
<code>collections$namedtuple</code>) and returns updated step_size (<code>Tensor</code>s).
Default value: <code>NULL</code> (i.e., do not update <code>step_size</code> automatically).</p>
</td></tr>
<tr><td><code id="mcmc_hamiltonian_monte_carlo_+3A_seed">seed</code></td>
<td>
<p>integer to seed the random number generator.</p>
</td></tr>
<tr><td><code id="mcmc_hamiltonian_monte_carlo_+3A_store_parameters_in_results">store_parameters_in_results</code></td>
<td>
<p>If <code>TRUE</code>, then <code>step_size</code> and
<code>num_leapfrog_steps</code> are written to and read from eponymous fields in
the kernel results objects returned from <code>one_step</code> and
<code>bootstrap_results</code>. This allows wrapper kernels to adjust those
parameters on the fly. This is incompatible with <code>step_size_update_fn</code>,
which must be set to <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="mcmc_hamiltonian_monte_carlo_+3A_name">name</code></td>
<td>
<p>string prefixed to Ops created by this function.
Default value: <code>NULL</code> (i.e., 'hmc_kernel').</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>one_step</code> function can update multiple chains in parallel. It assumes
that all leftmost dimensions of <code>current_state</code> index independent chain states
(and are therefore updated independently). The output of
<code>target_log_prob_fn(current_state)</code> should sum log-probabilities across all
event dimensions. Slices along the rightmost dimensions may have different
target distributions; for example, <code style="white-space: pre;">&#8288;current_state[0, :]&#8288;</code> could have a
different target distribution from <code style="white-space: pre;">&#8288;current_state[1, :]&#8288;</code>. These semantics are
governed by <code>target_log_prob_fn(current_state)</code>. (The number of independent
chains is <code>tf$size(target_log_prob_fn(current_state))</code>.)
</p>


<h3>Value</h3>

<p>a Monte Carlo sampling kernel
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1206.1901">Radford Neal. MCMC Using Hamiltonian Dynamics. <em>Handbook of Markov Chain Monte Carlo</em>, 2011.</a>
</p>
</li>
<li> <p><a href="https://www.jstor.org/stable/120120">Bernard Delyon, Marc Lavielle, Eric, Moulines. <em>Convergence of a stochastic approximation version of the EM algorithm</em>, Ann. Statist. 27 (1999), no. 1, 94&ndash;128.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other mcmc_kernels: 
<code><a href="#topic+mcmc_dual_averaging_step_size_adaptation">mcmc_dual_averaging_step_size_adaptation</a>()</code>,
<code><a href="#topic+mcmc_metropolis_adjusted_langevin_algorithm">mcmc_metropolis_adjusted_langevin_algorithm</a>()</code>,
<code><a href="#topic+mcmc_metropolis_hastings">mcmc_metropolis_hastings</a>()</code>,
<code><a href="#topic+mcmc_no_u_turn_sampler">mcmc_no_u_turn_sampler</a>()</code>,
<code><a href="#topic+mcmc_random_walk_metropolis">mcmc_random_walk_metropolis</a>()</code>,
<code><a href="#topic+mcmc_replica_exchange_mc">mcmc_replica_exchange_mc</a>()</code>,
<code><a href="#topic+mcmc_simple_step_size_adaptation">mcmc_simple_step_size_adaptation</a>()</code>,
<code><a href="#topic+mcmc_slice_sampler">mcmc_slice_sampler</a>()</code>,
<code><a href="#topic+mcmc_transformed_transition_kernel">mcmc_transformed_transition_kernel</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_hamiltonian_monte_carlo">mcmc_uncalibrated_hamiltonian_monte_carlo</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_langevin">mcmc_uncalibrated_langevin</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_random_walk">mcmc_uncalibrated_random_walk</a>()</code>
</p>

<hr>
<h2 id='mcmc_metropolis_adjusted_langevin_algorithm'>Runs one step of Metropolis-adjusted Langevin algorithm.</h2><span id='topic+mcmc_metropolis_adjusted_langevin_algorithm'></span>

<h3>Description</h3>

<p>Metropolis-adjusted Langevin algorithm (MALA) is a Markov chain Monte Carlo
(MCMC) algorithm that takes a step of a discretised Langevin diffusion as a
proposal. This class implements one step of MALA using Euler-Maruyama method
for a given <code>current_state</code> and diagonal preconditioning <code>volatility</code> matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc_metropolis_adjusted_langevin_algorithm(
  target_log_prob_fn,
  step_size,
  volatility_fn = NULL,
  seed = NULL,
  parallel_iterations = 10,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc_metropolis_adjusted_langevin_algorithm_+3A_target_log_prob_fn">target_log_prob_fn</code></td>
<td>
<p>Function which takes an argument like
<code>current_state</code> (if it's a list <code>current_state</code> will be unpacked) and returns its
(possibly unnormalized) log-density under the target distribution.</p>
</td></tr>
<tr><td><code id="mcmc_metropolis_adjusted_langevin_algorithm_+3A_step_size">step_size</code></td>
<td>
<p><code>Tensor</code> or <code>list</code> of <code>Tensor</code>s representing the step
size for the leapfrog integrator. Must broadcast with the shape of
<code>current_state</code>. Larger step sizes lead to faster progress, but
too-large step sizes make rejection exponentially more likely. When
possible, it's often helpful to match per-variable step sizes to the
standard deviations of the target distribution in each variable.</p>
</td></tr>
<tr><td><code id="mcmc_metropolis_adjusted_langevin_algorithm_+3A_volatility_fn">volatility_fn</code></td>
<td>
<p>function which takes an argument like
<code>current_state</code> (or <code style="white-space: pre;">&#8288;*current_state&#8288;</code> if it's a list) and returns
volatility value at <code>current_state</code>. Should return a <code>Tensor</code> or
<code>list</code> of <code>Tensor</code>s that must broadcast with the shape of
<code>current_state</code>. Defaults to the identity function.</p>
</td></tr>
<tr><td><code id="mcmc_metropolis_adjusted_langevin_algorithm_+3A_seed">seed</code></td>
<td>
<p>integer to seed the random number generator.</p>
</td></tr>
<tr><td><code id="mcmc_metropolis_adjusted_langevin_algorithm_+3A_parallel_iterations">parallel_iterations</code></td>
<td>
<p>the number of coordinates for which the gradients of
the volatility matrix <code>volatility_fn</code> can be computed in parallel.</p>
</td></tr>
<tr><td><code id="mcmc_metropolis_adjusted_langevin_algorithm_+3A_name">name</code></td>
<td>
<p>String prefixed to Ops created by this function.
Default value: <code>NULL</code> (i.e., 'mala_kernel').</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical details and derivations can be found in
Roberts and Rosenthal (1998) and Xifara et al. (2013).
</p>
<p>The <code>one_step</code> function can update multiple chains in parallel. It assumes
that all leftmost dimensions of <code>current_state</code> index independent chain states
(and are therefore updated independently). The output of
<code>target_log_prob_fn(current_state)</code> should reduce log-probabilities across
all event dimensions. Slices along the rightmost dimensions may have different
target distributions; for example, <code style="white-space: pre;">&#8288;current_state[0, :]&#8288;</code> could have a
different target distribution from <code style="white-space: pre;">&#8288;current_state[1, :]&#8288;</code>. These semantics are
governed by <code>target_log_prob_fn(current_state)</code>. (The number of independent
chains is <code>tf.size(target_log_prob_fn(current_state))</code>.)
</p>


<h3>References</h3>


<ul>
<li> <p><a href="http://probability.ca/jeff/ftpdir/lang.pdf">Gareth Roberts and Jeffrey Rosenthal. Optimal Scaling of Discrete Approximations to Langevin Diffusions. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 60: 255-268, 1998.</a>
</p>
</li>
<li> <p><a href="https://arxiv.org/abs/1309.2983">T. Xifara et al. Langevin diffusions and the Metropolis-adjusted Langevin algorithm. <em>arXiv preprint arXiv:1309.2983</em>, 2013.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other mcmc_kernels: 
<code><a href="#topic+mcmc_dual_averaging_step_size_adaptation">mcmc_dual_averaging_step_size_adaptation</a>()</code>,
<code><a href="#topic+mcmc_hamiltonian_monte_carlo">mcmc_hamiltonian_monte_carlo</a>()</code>,
<code><a href="#topic+mcmc_metropolis_hastings">mcmc_metropolis_hastings</a>()</code>,
<code><a href="#topic+mcmc_no_u_turn_sampler">mcmc_no_u_turn_sampler</a>()</code>,
<code><a href="#topic+mcmc_random_walk_metropolis">mcmc_random_walk_metropolis</a>()</code>,
<code><a href="#topic+mcmc_replica_exchange_mc">mcmc_replica_exchange_mc</a>()</code>,
<code><a href="#topic+mcmc_simple_step_size_adaptation">mcmc_simple_step_size_adaptation</a>()</code>,
<code><a href="#topic+mcmc_slice_sampler">mcmc_slice_sampler</a>()</code>,
<code><a href="#topic+mcmc_transformed_transition_kernel">mcmc_transformed_transition_kernel</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_hamiltonian_monte_carlo">mcmc_uncalibrated_hamiltonian_monte_carlo</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_langevin">mcmc_uncalibrated_langevin</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_random_walk">mcmc_uncalibrated_random_walk</a>()</code>
</p>

<hr>
<h2 id='mcmc_metropolis_hastings'>Runs one step of the Metropolis-Hastings algorithm.</h2><span id='topic+mcmc_metropolis_hastings'></span>

<h3>Description</h3>

<p>The Metropolis-Hastings algorithm is a Markov chain Monte Carlo (MCMC) technique which uses a proposal distribution
to eventually sample from a target distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc_metropolis_hastings(inner_kernel, seed = NULL, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc_metropolis_hastings_+3A_inner_kernel">inner_kernel</code></td>
<td>
<p><code>TransitionKernel</code>-like object which has <code>collections$namedtuple</code>
<code>kernel_results</code> and which contains a <code>target_log_prob</code> member and optionally a <code>log_acceptance_correction</code> member.</p>
</td></tr>
<tr><td><code id="mcmc_metropolis_hastings_+3A_seed">seed</code></td>
<td>
<p>integer to seed the random number generator.</p>
</td></tr>
<tr><td><code id="mcmc_metropolis_hastings_+3A_name">name</code></td>
<td>
<p>string prefixed to Ops created by this function. Default value: <code>NULL</code> (i.e., &quot;mh_kernel&quot;).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note: <code>inner_kernel$one_step</code> must return <code>kernel_results</code> as a <code>collections$namedtuple</code> which must:
</p>

<ul>
<li><p> have a <code>target_log_prob</code> field,
</p>
</li>
<li><p> optionally have a <code>log_acceptance_correction</code> field, and,
</p>
</li>
<li><p> have only fields which are <code>Tensor</code>-valued.
</p>
</li></ul>

<p>The Metropolis-Hastings log acceptance-probability is computed as:
</p>
<div class="sourceCode"><pre>log_accept_ratio = (current_kernel_results.target_log_prob
                   - previous_kernel_results.target_log_prob
                   + current_kernel_results.log_acceptance_correction)
</pre></div>
<p>If <code>current_kernel_results$log_acceptance_correction</code> does not exist, it is
presumed <code>0</code> (i.e., that the proposal distribution is symmetric).
The most common use-case for <code>log_acceptance_correction</code> is in the
Metropolis-Hastings algorithm, i.e.,
</p>
<div class="sourceCode"><pre>accept_prob(x' | x) = p(x') / p(x) (g(x|x') / g(x'|x))
where,
p  represents the target distribution,
g  represents the proposal (conditional) distribution,
x' is the proposed state, and,
x  is current state
</pre></div>
<p>The log of the parenthetical term is the <code>log_acceptance_correction</code>.
The <code>log_acceptance_correction</code> may not necessarily correspond to the ratio of
proposal distributions, e.g, <code>log_acceptance_correction</code> has a different
interpretation in Hamiltonian Monte Carlo.
</p>


<h3>Value</h3>

<p>a Monte Carlo sampling kernel
</p>


<h3>See Also</h3>

<p>Other mcmc_kernels: 
<code><a href="#topic+mcmc_dual_averaging_step_size_adaptation">mcmc_dual_averaging_step_size_adaptation</a>()</code>,
<code><a href="#topic+mcmc_hamiltonian_monte_carlo">mcmc_hamiltonian_monte_carlo</a>()</code>,
<code><a href="#topic+mcmc_metropolis_adjusted_langevin_algorithm">mcmc_metropolis_adjusted_langevin_algorithm</a>()</code>,
<code><a href="#topic+mcmc_no_u_turn_sampler">mcmc_no_u_turn_sampler</a>()</code>,
<code><a href="#topic+mcmc_random_walk_metropolis">mcmc_random_walk_metropolis</a>()</code>,
<code><a href="#topic+mcmc_replica_exchange_mc">mcmc_replica_exchange_mc</a>()</code>,
<code><a href="#topic+mcmc_simple_step_size_adaptation">mcmc_simple_step_size_adaptation</a>()</code>,
<code><a href="#topic+mcmc_slice_sampler">mcmc_slice_sampler</a>()</code>,
<code><a href="#topic+mcmc_transformed_transition_kernel">mcmc_transformed_transition_kernel</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_hamiltonian_monte_carlo">mcmc_uncalibrated_hamiltonian_monte_carlo</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_langevin">mcmc_uncalibrated_langevin</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_random_walk">mcmc_uncalibrated_random_walk</a>()</code>
</p>

<hr>
<h2 id='mcmc_no_u_turn_sampler'>Runs one step of the No U-Turn Sampler</h2><span id='topic+mcmc_no_u_turn_sampler'></span>

<h3>Description</h3>

<p>The No U-Turn Sampler (NUTS) is an adaptive variant of the Hamiltonian Monte
Carlo (HMC) method for MCMC.  NUTS adapts the distance traveled in response to
the curvature of the target density.  Conceptually, one proposal consists of
reversibly evolving a trajectory through the sample space, continuing until
that trajectory turns back on itself (hence the name, 'No U-Turn').
This class implements one random NUTS step from a given
<code>current_state</code>.  Mathematical details and derivations can be found in
Hoffman &amp; Gelman (2011).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc_no_u_turn_sampler(
  target_log_prob_fn,
  step_size,
  max_tree_depth = 10,
  max_energy_diff = 1000,
  unrolled_leapfrog_steps = 1,
  seed = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc_no_u_turn_sampler_+3A_target_log_prob_fn">target_log_prob_fn</code></td>
<td>
<p>function which takes an argument like
<code>current_state</code> and returns its (possibly unnormalized) log-density under the target
distribution.</p>
</td></tr>
<tr><td><code id="mcmc_no_u_turn_sampler_+3A_step_size">step_size</code></td>
<td>
<p><code>Tensor</code> or <code>list</code> of <code>Tensor</code>s representing the step
size for the leapfrog integrator. Must broadcast with the shape of
<code>current_state</code>. Larger step sizes lead to faster progress, but
too-large step sizes make rejection exponentially more likely. When
possible, it's often helpful to match per-variable step sizes to the
standard deviations of the target distribution in each variable.</p>
</td></tr>
<tr><td><code id="mcmc_no_u_turn_sampler_+3A_max_tree_depth">max_tree_depth</code></td>
<td>
<p>Maximum depth of the tree implicitly built by NUTS. The
maximum number of leapfrog steps is bounded by <code>2**max_tree_depth</code> i.e.
the number of nodes in a binary tree <code>max_tree_depth</code> nodes deep. The
default setting of 10 takes up to 1024 leapfrog steps.</p>
</td></tr>
<tr><td><code id="mcmc_no_u_turn_sampler_+3A_max_energy_diff">max_energy_diff</code></td>
<td>
<p>Scaler threshold of energy differences at each leapfrog,
divergence samples are defined as leapfrog steps that exceed this
threshold. Default to 1000.</p>
</td></tr>
<tr><td><code id="mcmc_no_u_turn_sampler_+3A_unrolled_leapfrog_steps">unrolled_leapfrog_steps</code></td>
<td>
<p>The number of leapfrogs to unroll per tree
expansion step. Applies a direct linear multipler to the maximum
trajectory length implied by max_tree_depth. Defaults to 1.</p>
</td></tr>
<tr><td><code id="mcmc_no_u_turn_sampler_+3A_seed">seed</code></td>
<td>
<p>integer to seed the random number generator.</p>
</td></tr>
<tr><td><code id="mcmc_no_u_turn_sampler_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this function.
Default value: <code>NULL</code> (i.e., 'nuts_kernel').</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>one_step</code> function can update multiple chains in parallel. It assumes
that a prefix of leftmost dimensions of <code>current_state</code> index independent
chain states (and are therefore updated independently).  The output of
<code>target_log_prob_fn(current_state)</code> should sum log-probabilities across all
event dimensions.  Slices along the rightmost dimensions may have different
target distributions; for example, <code>current_state[0][0, ...]</code> could have a
different target distribution from <code>current_state[0][1, ...]</code>.  These
semantics are governed by <code style="white-space: pre;">&#8288;target_log_prob_fn(*current_state)&#8288;</code>.
(The number of independent chains is <code>tf$size(target_log_prob_fn(current_state))</code>.)
</p>


<h3>Value</h3>

<p>a Monte Carlo sampling kernel
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/pdf/1111.4246.pdf">Matthew D. Hoffman, Andrew Gelman.  The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.  2011.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other mcmc_kernels: 
<code><a href="#topic+mcmc_dual_averaging_step_size_adaptation">mcmc_dual_averaging_step_size_adaptation</a>()</code>,
<code><a href="#topic+mcmc_hamiltonian_monte_carlo">mcmc_hamiltonian_monte_carlo</a>()</code>,
<code><a href="#topic+mcmc_metropolis_adjusted_langevin_algorithm">mcmc_metropolis_adjusted_langevin_algorithm</a>()</code>,
<code><a href="#topic+mcmc_metropolis_hastings">mcmc_metropolis_hastings</a>()</code>,
<code><a href="#topic+mcmc_random_walk_metropolis">mcmc_random_walk_metropolis</a>()</code>,
<code><a href="#topic+mcmc_replica_exchange_mc">mcmc_replica_exchange_mc</a>()</code>,
<code><a href="#topic+mcmc_simple_step_size_adaptation">mcmc_simple_step_size_adaptation</a>()</code>,
<code><a href="#topic+mcmc_slice_sampler">mcmc_slice_sampler</a>()</code>,
<code><a href="#topic+mcmc_transformed_transition_kernel">mcmc_transformed_transition_kernel</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_hamiltonian_monte_carlo">mcmc_uncalibrated_hamiltonian_monte_carlo</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_langevin">mcmc_uncalibrated_langevin</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_random_walk">mcmc_uncalibrated_random_walk</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
predictors &lt;- tf$cast( c(201,244, 47,287,203,58,210,202,198,158,165,201,157,
  131,166,160,186,125,218,146),tf$float32)
obs &lt;- tf$cast(c(592,401,583,402,495,173,479,504,510,416,393,442,317,311,400,
  337,423,334,533,344),tf$float32)
y_sigma &lt;- tf$cast(c(61,25,38,15,21,15,27,14,30,16,14,25,52,16,34,31,42,26,
  16,22),tf$float32)

# Robust linear regression model
robust_lm &lt;- tfd_joint_distribution_sequential(
 list(
   tfd_normal(loc = 0, scale = 1, name = "b0"),
   tfd_normal(loc = 0, scale = 1, name = "b1"),
   tfd_half_normal(5, name = "df"),
   function(df, b1, b0)
     tfd_independent(
       tfd_student_t(
         # Likelihood
           df = tf$expand_dims(df, axis = -1L),
           loc = tf$expand_dims(b0, axis = -1L) +
                 tf$expand_dims(b1, axis = -1L) * predictors[tf$newaxis, ],
           scale = y_sigma,
           name = "st"
           ), name = "ind")), validate_args = TRUE)

 log_prob &lt;-function(b0, b1, df) {robust_lm %&gt;%
   tfd_log_prob(list(b0, b1, df, obs))}
 step_size0 &lt;- Map(function(x) tf$cast(x, tf$float32), c(1, .2, .5))

 number_of_steps &lt;- 10
 burnin &lt;- 5
 nchain &lt;- 50

 run_chain &lt;- function() {
 # random initialization of the starting postion of each chain
 samples &lt;- robust_lm %&gt;% tfd_sample(nchain)
 b0 &lt;- samples[[1]]
 b1 &lt;- samples[[2]]
 df &lt;- samples[[3]]

 # bijector to map constrained parameters to real
 unconstraining_bijectors &lt;- list(
   tfb_identity(), tfb_identity(), tfb_exp())

 trace_fn &lt;- function(x, pkr) {
   list(pkr$inner_results$inner_results$step_size,
     pkr$inner_results$inner_results$log_accept_ratio)
 }

 nuts &lt;- mcmc_no_u_turn_sampler(
   target_log_prob_fn = log_prob,
   step_size = step_size0
   ) %&gt;%
   mcmc_transformed_transition_kernel(bijector = unconstraining_bijectors) %&gt;%
   mcmc_dual_averaging_step_size_adaptation(
     num_adaptation_steps = burnin,
     step_size_setter_fn = function(pkr, new_step_size)
       pkr$`_replace`(
         inner_results = pkr$inner_results$`_replace`(step_size = new_step_size)),
     step_size_getter_fn = function(pkr) pkr$inner_results$step_size,
     log_accept_prob_getter_fn = function(pkr) pkr$inner_results$log_accept_ratio
     )

   nuts %&gt;% mcmc_sample_chain(
     num_results = number_of_steps,
     num_burnin_steps = burnin,
     current_state = list(b0, b1, df),
     trace_fn = trace_fn)
   }

   run_chain &lt;- tensorflow::tf_function(run_chain)
   res &lt;- run_chain()

</code></pre>

<hr>
<h2 id='mcmc_potential_scale_reduction'>Gelman and Rubin (1992)'s potential scale reduction for chain convergence.</h2><span id='topic+mcmc_potential_scale_reduction'></span>

<h3>Description</h3>

<p>Given <code>N &gt; 1</code> states from each of <code>C &gt; 1</code> independent chains, the potential
scale reduction factor, commonly referred to as R-hat, measures convergence of
the chains (to the same target) by testing for equality of means.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc_potential_scale_reduction(
  chains_states,
  independent_chain_ndims = 1,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc_potential_scale_reduction_+3A_chains_states">chains_states</code></td>
<td>
<p><code>Tensor</code> or <code>list</code> of <code>Tensor</code>s representing the
state(s) of a Markov Chain at each result step.  The <code>ith</code> state is
assumed to have shape <code style="white-space: pre;">&#8288;[Ni, Ci1, Ci2,...,CiD] + A&#8288;</code>.
Dimension <code>0</code> indexes the <code>Ni &gt; 1</code> result steps of the Markov Chain.
Dimensions <code>1</code> through <code>D</code> index the <code style="white-space: pre;">&#8288;Ci1 x ... x CiD&#8288;</code> independent
chains to be tested for convergence to the same target.
The remaining dimensions, <code>A</code>, can have any shape (even empty).</p>
</td></tr>
<tr><td><code id="mcmc_potential_scale_reduction_+3A_independent_chain_ndims">independent_chain_ndims</code></td>
<td>
<p>Integer type <code>Tensor</code> with value <code style="white-space: pre;">&#8288;&gt;= 1&#8288;</code> giving the
number of giving the number of dimensions, from <code>dim = 1</code> to <code>dim = D</code>,
holding independent chain results to be tested for convergence.</p>
</td></tr>
<tr><td><code id="mcmc_potential_scale_reduction_+3A_name">name</code></td>
<td>
<p>name to prepend to created tf.  Default: <code>potential_scale_reduction</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Specifically, R-hat measures the degree to which variance (of the means)
between chains exceeds what one would expect if the chains were identically
distributed. See Gelman and Rubin (1992), Brooks and Gelman (1998)].
</p>
<p>Some guidelines:
</p>

<ul>
<li><p> The initial state of the chains should be drawn from a distribution overdispersed with respect to the target.
</p>
</li>
<li><p> If all chains converge to the target, then as <code style="white-space: pre;">&#8288;N --&gt; infinity&#8288;</code>, R-hat &ndash;&gt; 1.
Before that, R-hat &gt; 1 (except in pathological cases, e.g. if the chain paths were identical).
</p>
</li>
<li><p> The above holds for any number of chains <code>C &gt; 1</code>.  Increasing <code>C</code> improves effectiveness of the diagnostic.
</p>
</li>
<li><p> Sometimes, R-hat &lt; 1.2 is used to indicate approximate convergence, but of
course this is problem dependent. See Brooks and Gelman (1998).
</p>
</li>
<li><p> R-hat only measures non-convergence of the mean. If higher moments, or
other statistics are desired, a different diagnostic should be used. See Brooks and Gelman (1998).
</p>
</li></ul>

<p>To see why R-hat is reasonable, let <code>X</code> be a random variable drawn uniformly
from the combined states (combined over all chains).  Then, in the limit
<code style="white-space: pre;">&#8288;N, C --&gt; infinity&#8288;</code>, with <code>E</code>, <code>Var</code> denoting expectation and variance,
<code style="white-space: pre;">&#8288;R-hat = ( E[Var[X | chain]] + Var[E[X | chain]] ) / E[Var[X | chain]].&#8288;</code>
Using the law of total variance, the numerator is the variance of the combined
states, and the denominator is the total variance minus the variance of the
the individual chain means.  If the chains are all drawing from the same
distribution, they will have the same mean, and thus the ratio should be one.
</p>


<h3>Value</h3>

<p><code>Tensor</code> or <code>list</code> of <code>Tensor</code>s representing the R-hat statistic for
the state(s).  Same <code>dtype</code> as <code>state</code>, and shape equal to
<code style="white-space: pre;">&#8288;state$shape[1 + independent_chain_ndims:]&#8288;</code>.
</p>


<h3>References</h3>


<ul>
<li><p> Stephen P. Brooks and Andrew Gelman. General Methods for Monitoring Convergence of Iterative Simulations.
<em>Journal of Computational and Graphical Statistics</em>, 7(4), 1998.
</p>
</li>
<li><p> Andrew Gelman and Donald B. Rubin. Inference from Iterative Simulation Using Multiple Sequences.
<em>Statistical Science</em>, 7(4):457-472, 1992.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other mcmc_functions: 
<code><a href="#topic+mcmc_effective_sample_size">mcmc_effective_sample_size</a>()</code>,
<code><a href="#topic+mcmc_sample_annealed_importance_chain">mcmc_sample_annealed_importance_chain</a>()</code>,
<code><a href="#topic+mcmc_sample_chain">mcmc_sample_chain</a>()</code>,
<code><a href="#topic+mcmc_sample_halton_sequence">mcmc_sample_halton_sequence</a>()</code>
</p>

<hr>
<h2 id='mcmc_random_walk_metropolis'>Runs one step of the RWM algorithm with symmetric proposal.</h2><span id='topic+mcmc_random_walk_metropolis'></span>

<h3>Description</h3>

<p>Random Walk Metropolis is a gradient-free Markov chain Monte Carlo
(MCMC) algorithm. The algorithm involves a proposal generating step
<code>proposal_state = current_state + perturb</code> by a random
perturbation, followed by Metropolis-Hastings accept/reject step. For more
details see Section 2.1 of Roberts and Rosenthal (2004).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc_random_walk_metropolis(
  target_log_prob_fn,
  new_state_fn = NULL,
  seed = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc_random_walk_metropolis_+3A_target_log_prob_fn">target_log_prob_fn</code></td>
<td>
<p>Function which takes an argument like
<code>current_state</code> ((if it's a list <code>current_state</code> will be unpacked) and returns its
(possibly unnormalized) log-density under the target distribution.</p>
</td></tr>
<tr><td><code id="mcmc_random_walk_metropolis_+3A_new_state_fn">new_state_fn</code></td>
<td>
<p>Function which takes a list of state parts and a
seed; returns a same-type <code>list</code> of <code>Tensor</code>s, each being a perturbation
of the input state parts. The perturbation distribution is assumed to be
a symmetric distribution centered at the input state part.
Default value: <code>NULL</code> which is mapped to <code>tfp$mcmc$random_walk_normal_fn()</code>.</p>
</td></tr>
<tr><td><code id="mcmc_random_walk_metropolis_+3A_seed">seed</code></td>
<td>
<p>integer to seed the random number generator.</p>
</td></tr>
<tr><td><code id="mcmc_random_walk_metropolis_+3A_name">name</code></td>
<td>
<p>String name prefixed to Ops created by this function.
Default value: <code>NULL</code> (i.e., 'rwm_kernel').</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The current class implements RWM for normal and uniform proposals. Alternatively,
the user can supply any custom proposal generating function.
The function <code>one_step</code> can update multiple chains in parallel. It assumes
that all leftmost dimensions of <code>current_state</code> index independent chain states
(and are therefore updated independently). The output of
<code>target_log_prob_fn(current_state)</code> should sum log-probabilities across all
event dimensions. Slices along the rightmost dimensions may have different
target distributions; for example, <code style="white-space: pre;">&#8288;current_state[0, :]&#8288;</code> could have a
different target distribution from <code style="white-space: pre;">&#8288;current_state[1, :]&#8288;</code>. These semantics
are governed by <code>target_log_prob_fn(current_state)</code>. (The number of
independent chains is <code>tf$size(target_log_prob_fn(current_state))</code>.)
</p>


<h3>Value</h3>

<p>a Monte Carlo sampling kernel
</p>


<h3>See Also</h3>

<p>Other mcmc_kernels: 
<code><a href="#topic+mcmc_dual_averaging_step_size_adaptation">mcmc_dual_averaging_step_size_adaptation</a>()</code>,
<code><a href="#topic+mcmc_hamiltonian_monte_carlo">mcmc_hamiltonian_monte_carlo</a>()</code>,
<code><a href="#topic+mcmc_metropolis_adjusted_langevin_algorithm">mcmc_metropolis_adjusted_langevin_algorithm</a>()</code>,
<code><a href="#topic+mcmc_metropolis_hastings">mcmc_metropolis_hastings</a>()</code>,
<code><a href="#topic+mcmc_no_u_turn_sampler">mcmc_no_u_turn_sampler</a>()</code>,
<code><a href="#topic+mcmc_replica_exchange_mc">mcmc_replica_exchange_mc</a>()</code>,
<code><a href="#topic+mcmc_simple_step_size_adaptation">mcmc_simple_step_size_adaptation</a>()</code>,
<code><a href="#topic+mcmc_slice_sampler">mcmc_slice_sampler</a>()</code>,
<code><a href="#topic+mcmc_transformed_transition_kernel">mcmc_transformed_transition_kernel</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_hamiltonian_monte_carlo">mcmc_uncalibrated_hamiltonian_monte_carlo</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_langevin">mcmc_uncalibrated_langevin</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_random_walk">mcmc_uncalibrated_random_walk</a>()</code>
</p>

<hr>
<h2 id='mcmc_replica_exchange_mc'>Runs one step of the Replica Exchange Monte Carlo</h2><span id='topic+mcmc_replica_exchange_mc'></span>

<h3>Description</h3>

<p><a href="https://en.wikipedia.org/wiki/Parallel_tempering">Replica Exchange Monte Carlo</a>
is a Markov chain Monte Carlo (MCMC) algorithm that is also known as Parallel Tempering.
This algorithm performs multiple sampling with different temperatures in parallel,
and exchanges those samplings according to the Metropolis-Hastings criterion.
The <code>K</code> replicas are parameterized in terms of <code>inverse_temperature</code>'s,
<code style="white-space: pre;">&#8288;(beta[0], beta[1], ..., beta[K-1])&#8288;</code>.  If the target distribution has
probability density <code>p(x)</code>, the <code>kth</code> replica has density <code>p(x)**beta_k</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc_replica_exchange_mc(
  target_log_prob_fn,
  inverse_temperatures,
  make_kernel_fn,
  swap_proposal_fn = tfp$mcmc$replica_exchange_mc$default_swap_proposal_fn(1),
  state_includes_replicas = FALSE,
  seed = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc_replica_exchange_mc_+3A_target_log_prob_fn">target_log_prob_fn</code></td>
<td>
<p>Function which takes an argument like
<code>current_state</code> (if it's a list <code>current_state</code> will be unpacked) and returns its
(possibly unnormalized) log-density under the target distribution.</p>
</td></tr>
<tr><td><code id="mcmc_replica_exchange_mc_+3A_inverse_temperatures">inverse_temperatures</code></td>
<td>
<p><code style="white-space: pre;">&#8288;1D&#8288;</code> Tensor of inverse temperatures to perform
samplings with each replica. Must have statically known <code>shape</code>.
<code>inverse_temperatures[0]</code> produces the states returned by samplers,
and is typically == 1.</p>
</td></tr>
<tr><td><code id="mcmc_replica_exchange_mc_+3A_make_kernel_fn">make_kernel_fn</code></td>
<td>
<p>Function which takes target_log_prob_fn and seed
args and returns a TransitionKernel instance.</p>
</td></tr>
<tr><td><code id="mcmc_replica_exchange_mc_+3A_swap_proposal_fn">swap_proposal_fn</code></td>
<td>
<p>function which take a number of replicas, and
return combinations of replicas for exchange.</p>
</td></tr>
<tr><td><code id="mcmc_replica_exchange_mc_+3A_state_includes_replicas">state_includes_replicas</code></td>
<td>
<p>Boolean indicating whether the leftmost dimension
of each state sample should index replicas. If <code>TRUE</code>, the leftmost
dimension of the <code>current_state</code> kwarg to <code>tfp.mcmc.sample_chain</code> will
be interpreted as indexing replicas.</p>
</td></tr>
<tr><td><code id="mcmc_replica_exchange_mc_+3A_seed">seed</code></td>
<td>
<p>integer to seed the random number generator.</p>
</td></tr>
<tr><td><code id="mcmc_replica_exchange_mc_+3A_name">name</code></td>
<td>
<p>string prefixed to Ops created by this function.
Default value: <code>NULL</code> (i.e., &quot;remc_kernel&quot;).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Typically <code>beta[0] = 1.0</code>, and <code style="white-space: pre;">&#8288;1.0 &gt; beta[1] &gt; beta[2] &gt; ... &gt; 0.0&#8288;</code>.
</p>

<ul>
<li> <p><code>beta[0] == 1</code> ==&gt; First replicas samples from the target density, <code>p</code>.
</p>
</li>
<li> <p><code>beta[k] &lt; 1</code>, for <code style="white-space: pre;">&#8288;k = 1, ..., K-1&#8288;</code> ==&gt; Other replicas sample from
&quot;flattened&quot; versions of <code>p</code> (peak is less high, valley less low).  These
distributions are somewhat closer to a uniform on the support of <code>p</code>.
Samples from adjacent replicas <code>i</code>, <code>i + 1</code> are used as proposals for each
other in a Metropolis step.  This allows the lower <code>beta</code> samples, which
explore less dense areas of <code>p</code>, to occasionally be used to help the
<code>beta == 1</code> chain explore new regions of the support.
Samples from replica 0 are returned, and the others are discarded.
</p>
</li></ul>



<h3>Value</h3>

<p>list of
<code>next_state</code> (Tensor or Python list of <code>Tensor</code>s representing the state(s)
of the Markov chain(s) at each result step. Has same shape as
and <code>current_state</code>.) and
<code>kernel_results</code> (<code>collections$namedtuple</code> of internal calculations used to
'advance the chain).
</p>


<h3>See Also</h3>

<p>Other mcmc_kernels: 
<code><a href="#topic+mcmc_dual_averaging_step_size_adaptation">mcmc_dual_averaging_step_size_adaptation</a>()</code>,
<code><a href="#topic+mcmc_hamiltonian_monte_carlo">mcmc_hamiltonian_monte_carlo</a>()</code>,
<code><a href="#topic+mcmc_metropolis_adjusted_langevin_algorithm">mcmc_metropolis_adjusted_langevin_algorithm</a>()</code>,
<code><a href="#topic+mcmc_metropolis_hastings">mcmc_metropolis_hastings</a>()</code>,
<code><a href="#topic+mcmc_no_u_turn_sampler">mcmc_no_u_turn_sampler</a>()</code>,
<code><a href="#topic+mcmc_random_walk_metropolis">mcmc_random_walk_metropolis</a>()</code>,
<code><a href="#topic+mcmc_simple_step_size_adaptation">mcmc_simple_step_size_adaptation</a>()</code>,
<code><a href="#topic+mcmc_slice_sampler">mcmc_slice_sampler</a>()</code>,
<code><a href="#topic+mcmc_transformed_transition_kernel">mcmc_transformed_transition_kernel</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_hamiltonian_monte_carlo">mcmc_uncalibrated_hamiltonian_monte_carlo</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_langevin">mcmc_uncalibrated_langevin</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_random_walk">mcmc_uncalibrated_random_walk</a>()</code>
</p>

<hr>
<h2 id='mcmc_sample_annealed_importance_chain'>Runs annealed importance sampling (AIS) to estimate normalizing constants.</h2><span id='topic+mcmc_sample_annealed_importance_chain'></span>

<h3>Description</h3>

<p>This function uses an MCMC transition operator (e.g., Hamiltonian Monte Carlo)
to sample from a series of distributions that slowly interpolates between
an initial &quot;proposal&quot; distribution:
<code>exp(proposal_log_prob_fn(x) - proposal_log_normalizer)</code>
and the target distribution:
<code>exp(target_log_prob_fn(x) - target_log_normalizer)</code>,
accumulating importance weights along the way. The product of these
importance weights gives an unbiased estimate of the ratio of the
normalizing constants of the initial distribution and the target
distribution:
<code>E[exp(ais_weights)] = exp(target_log_normalizer - proposal_log_normalizer)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc_sample_annealed_importance_chain(
  num_steps,
  proposal_log_prob_fn,
  target_log_prob_fn,
  current_state,
  make_kernel_fn,
  parallel_iterations = 10,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc_sample_annealed_importance_chain_+3A_num_steps">num_steps</code></td>
<td>
<p>Integer number of Markov chain updates to run. More
iterations means more expense, but smoother annealing between q
and p, which in turn means exponentially lower variance for the
normalizing constant estimator.</p>
</td></tr>
<tr><td><code id="mcmc_sample_annealed_importance_chain_+3A_proposal_log_prob_fn">proposal_log_prob_fn</code></td>
<td>
<p>function that returns the log density of the
initial distribution.</p>
</td></tr>
<tr><td><code id="mcmc_sample_annealed_importance_chain_+3A_target_log_prob_fn">target_log_prob_fn</code></td>
<td>
<p>function which takes an argument like
<code>current_state</code> and returns its
(possibly unnormalized) log-density under the target distribution.</p>
</td></tr>
<tr><td><code id="mcmc_sample_annealed_importance_chain_+3A_current_state">current_state</code></td>
<td>
<p><code>Tensor</code> or <code>list</code> of <code>Tensor</code>s representing the
current state(s) of the Markov chain(s). The first <code>r</code> dimensions index
independent chains, <code>r</code> = <code>tf$rank(target_log_prob_fn(current_state))</code>.</p>
</td></tr>
<tr><td><code id="mcmc_sample_annealed_importance_chain_+3A_make_kernel_fn">make_kernel_fn</code></td>
<td>
<p>function which returns a <code>TransitionKernel</code>-like
object. Must take one argument representing the <code>TransitionKernel</code>'s
<code>target_log_prob_fn</code>. The <code>target_log_prob_fn</code> argument represents the
<code>TransitionKernel</code>'s target log distribution.  Note:
<code>sample_annealed_importance_chain</code> creates a new <code>target_log_prob_fn</code>
which is an interpolation between the supplied <code>target_log_prob_fn</code> and
<code>proposal_log_prob_fn</code>; it is this interpolated function which is used as
an argument to <code>make_kernel_fn</code>.</p>
</td></tr>
<tr><td><code id="mcmc_sample_annealed_importance_chain_+3A_parallel_iterations">parallel_iterations</code></td>
<td>
<p>The number of iterations allowed to run in parallel.
It must be a positive integer. See <code>tf$while_loop</code> for more details.</p>
</td></tr>
<tr><td><code id="mcmc_sample_annealed_importance_chain_+3A_name">name</code></td>
<td>
<p>string prefixed to Ops created by this function.
Default value: <code>NULL</code> (i.e., &quot;sample_annealed_importance_chain&quot;).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note: When running in graph mode, <code>proposal_log_prob_fn</code> and
<code>target_log_prob_fn</code> are called exactly three times (although this may be
reduced to two times in the future).
</p>


<h3>Value</h3>

<p>list of
<code>next_state</code> (<code>Tensor</code> or Python list of <code>Tensor</code>s representing the
state(s) of the Markov chain(s) at the final iteration. Has same shape as
input <code>current_state</code>),
<code>ais_weights</code> (Tensor with the estimated weight(s). Has shape matching
<code>target_log_prob_fn(current_state)</code>), and
<code>kernel_results</code> (<code>collections.namedtuple</code> of internal calculations used to
advance the chain).
</p>


<h3>See Also</h3>

<p>For an example how to use see <code><a href="#topic+mcmc_sample_chain">mcmc_sample_chain()</a></code>.
</p>
<p>Other mcmc_functions: 
<code><a href="#topic+mcmc_effective_sample_size">mcmc_effective_sample_size</a>()</code>,
<code><a href="#topic+mcmc_potential_scale_reduction">mcmc_potential_scale_reduction</a>()</code>,
<code><a href="#topic+mcmc_sample_chain">mcmc_sample_chain</a>()</code>,
<code><a href="#topic+mcmc_sample_halton_sequence">mcmc_sample_halton_sequence</a>()</code>
</p>

<hr>
<h2 id='mcmc_sample_chain'>Implements Markov chain Monte Carlo via repeated <code>TransitionKernel</code> steps.</h2><span id='topic+mcmc_sample_chain'></span>

<h3>Description</h3>

<p>This function samples from an Markov chain at <code>current_state</code> and whose
stationary distribution is governed by the supplied <code>TransitionKernel</code>
instance (<code>kernel</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc_sample_chain(
  kernel = NULL,
  num_results,
  current_state,
  previous_kernel_results = NULL,
  num_burnin_steps = 0,
  num_steps_between_results = 0,
  trace_fn = NULL,
  return_final_kernel_results = FALSE,
  parallel_iterations = 10,
  seed = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc_sample_chain_+3A_kernel">kernel</code></td>
<td>
<p>An instance of <code>tfp$mcmc$TransitionKernel</code> which implements one step
of the Markov chain.</p>
</td></tr>
<tr><td><code id="mcmc_sample_chain_+3A_num_results">num_results</code></td>
<td>
<p>Integer number of Markov chain draws.</p>
</td></tr>
<tr><td><code id="mcmc_sample_chain_+3A_current_state">current_state</code></td>
<td>
<p><code>Tensor</code> or <code>list</code> of <code>Tensor</code>s representing the
current state(s) of the Markov chain(s).</p>
</td></tr>
<tr><td><code id="mcmc_sample_chain_+3A_previous_kernel_results">previous_kernel_results</code></td>
<td>
<p>A <code>Tensor</code> or a nested collection of <code>Tensor</code>s
representing internal calculations made within the previous call to this
function (or as returned by <code>bootstrap_results</code>).</p>
</td></tr>
<tr><td><code id="mcmc_sample_chain_+3A_num_burnin_steps">num_burnin_steps</code></td>
<td>
<p>Integer number of chain steps to take before starting to
collect results. Default value: 0 (i.e., no burn-in).</p>
</td></tr>
<tr><td><code id="mcmc_sample_chain_+3A_num_steps_between_results">num_steps_between_results</code></td>
<td>
<p>Integer number of chain steps between collecting
a result. Only one out of every <code>num_steps_between_samples + 1</code> steps is
included in the returned results.  The number of returned chain states is
still equal to <code>num_results</code>.  Default value: 0 (i.e., no thinning).</p>
</td></tr>
<tr><td><code id="mcmc_sample_chain_+3A_trace_fn">trace_fn</code></td>
<td>
<p>A function that takes in the current chain state and the previous
kernel results and return a <code>Tensor</code> or a nested collection of <code>Tensor</code>s
that is then traced along with the chain state.</p>
</td></tr>
<tr><td><code id="mcmc_sample_chain_+3A_return_final_kernel_results">return_final_kernel_results</code></td>
<td>
<p>If <code>TRUE</code>, then the final kernel results are
returned alongside the chain state and the trace specified by the <code>trace_fn</code>.</p>
</td></tr>
<tr><td><code id="mcmc_sample_chain_+3A_parallel_iterations">parallel_iterations</code></td>
<td>
<p>The number of iterations allowed to run in parallel. It
must be a positive integer. See <code>tf$while_loop</code> for more details.</p>
</td></tr>
<tr><td><code id="mcmc_sample_chain_+3A_seed">seed</code></td>
<td>
<p>Optional, a seed for reproducible sampling.</p>
</td></tr>
<tr><td><code id="mcmc_sample_chain_+3A_name">name</code></td>
<td>
<p>string prefixed to Ops created by this function. Default value: <code>NULL</code>,
(i.e., &quot;mcmc_sample_chain&quot;).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can sample from multiple chains, in parallel. (Whether or not
there are multiple chains is dictated by the <code>kernel</code>.)
</p>
<p>The <code>current_state</code> can be represented as a single <code>Tensor</code> or a <code>list</code> of
<code>Tensors</code> which collectively represent the current state.
Since MCMC states are correlated, it is sometimes desirable to produce
additional intermediate states, and then discard them, ending up with a set of
states with decreased autocorrelation.  See Owen (2017). Such &quot;thinning&quot;
is made possible by setting <code>num_steps_between_results &gt; 0</code>. The chain then
takes <code>num_steps_between_results</code> extra steps between the steps that make it
into the results. The extra steps are never materialized (in calls to
<code>sess$run</code>), and thus do not increase memory requirements.
</p>
<p>Warning: when setting a <code>seed</code> in the <code>kernel</code>, ensure that <code>sample_chain</code>'s
<code>parallel_iterations=1</code>, otherwise results will not be reproducible.
In addition to returning the chain state, this function supports tracing of
auxiliary variables used by the kernel. The traced values are selected by
specifying <code>trace_fn</code>. By default, all kernel results are traced but in the
future the default will be changed to no results being traced, so plan
accordingly. See below for some examples of this feature.
</p>


<h3>Value</h3>

<p>list of:
</p>

<ul>
<li><p> checkpointable_states_and_trace: if <code>return_final_kernel_results</code> is
<code>TRUE</code>. The return value is an instance of <code>CheckpointableStatesAndTrace</code>.
</p>
</li>
<li><p> all_states: if <code>return_final_kernel_results</code> is <code>FALSE</code> and <code>trace_fn</code> is
<code>NULL</code>. The return value is a <code>Tensor</code> or Python list of <code>Tensor</code>s
representing the state(s) of the Markov chain(s) at each result step. Has
same shape as input <code>current_state</code> but with a prepended
<code>num_results</code>-size dimension.
</p>
</li>
<li><p> states_and_trace: if <code>return_final_kernel_results</code> is <code>FALSE</code> and
<code>trace_fn</code> is not <code>NULL</code>. The return value is an instance of
<code>StatesAndTrace</code>.
</p>
</li></ul>



<h3>References</h3>


<ul>
<li> <p><a href="http://statweb.stanford.edu/~owen/reports/bestthinning.pdf">Art B. Owen. Statistically efficient thinning of a Markov chain sampler. <em>Technical Report</em>, 2017.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other mcmc_functions: 
<code><a href="#topic+mcmc_effective_sample_size">mcmc_effective_sample_size</a>()</code>,
<code><a href="#topic+mcmc_potential_scale_reduction">mcmc_potential_scale_reduction</a>()</code>,
<code><a href="#topic+mcmc_sample_annealed_importance_chain">mcmc_sample_annealed_importance_chain</a>()</code>,
<code><a href="#topic+mcmc_sample_halton_sequence">mcmc_sample_halton_sequence</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  dims &lt;- 10
  true_stddev &lt;- sqrt(seq(1, 3, length.out = dims))
  likelihood &lt;- tfd_multivariate_normal_diag(scale_diag = true_stddev)

  kernel &lt;- mcmc_hamiltonian_monte_carlo(
    target_log_prob_fn = likelihood$log_prob,
    step_size = 0.5,
    num_leapfrog_steps = 2
  )

  states &lt;- kernel %&gt;% mcmc_sample_chain(
    num_results = 1000,
    num_burnin_steps = 500,
    current_state = rep(0, dims),
    trace_fn = NULL
  )

  sample_mean &lt;- tf$reduce_mean(states, axis = 0L)
  sample_stddev &lt;- tf$sqrt(
    tf$reduce_mean(tf$math$squared_difference(states, sample_mean), axis = 0L))

</code></pre>

<hr>
<h2 id='mcmc_sample_halton_sequence'>Returns a sample from the <code>dim</code> dimensional Halton sequence.</h2><span id='topic+mcmc_sample_halton_sequence'></span>

<h3>Description</h3>

<p>Warning: The sequence elements take values only between 0 and 1. Care must be
taken to appropriately transform the domain of a function if it differs from
the unit cube before evaluating integrals using Halton samples. It is also
important to remember that quasi-random numbers without randomization are not
a replacement for pseudo-random numbers in every context. Quasi random numbers
are completely deterministic and typically have significant negative
autocorrelation unless randomization is used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc_sample_halton_sequence(
  dim,
  num_results = NULL,
  sequence_indices = NULL,
  dtype = tf$float32,
  randomized = TRUE,
  seed = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc_sample_halton_sequence_+3A_dim">dim</code></td>
<td>
<p>Positive <code>integer</code> representing each sample's <code>event_size.</code> Must
not be greater than 1000.</p>
</td></tr>
<tr><td><code id="mcmc_sample_halton_sequence_+3A_num_results">num_results</code></td>
<td>
<p>(Optional) Positive scalar <code>Tensor</code> of dtype int32. The number
of samples to generate. Either this parameter or sequence_indices must
be specified but not both. If this parameter is None, then the behaviour
is determined by the <code>sequence_indices</code>. Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="mcmc_sample_halton_sequence_+3A_sequence_indices">sequence_indices</code></td>
<td>
<p>(Optional) <code>Tensor</code> of dtype int32 and rank 1. The
elements of the sequence to compute specified by their position in the
sequence. The entries index into the Halton sequence starting with 0 and
hence, must be whole numbers. For example, sequence_indices=<code style="white-space: pre;">&#8288;[0, 5, 6]&#8288;</code> will
produce the first, sixth and seventh elements of the sequence. If this
parameter is None, then the <code>num_results</code> parameter must be specified
which gives the number of desired samples starting from the first sample.
Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="mcmc_sample_halton_sequence_+3A_dtype">dtype</code></td>
<td>
<p>(Optional) The dtype of the sample. One of: <code>float16</code>, <code>float32</code> or
<code>float64</code>. Default value: <code>tf$float32</code>.</p>
</td></tr>
<tr><td><code id="mcmc_sample_halton_sequence_+3A_randomized">randomized</code></td>
<td>
<p>(Optional) bool indicating whether to produce a randomized
Halton sequence. If TRUE, applies the randomization described in
Owen (2017). Default value: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="mcmc_sample_halton_sequence_+3A_seed">seed</code></td>
<td>
<p>(Optional) integer to seed the random number generator. Only
used if <code>randomized</code> is TRUE. If not supplied and <code>randomized</code> is TRUE,
no seed is set. Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="mcmc_sample_halton_sequence_+3A_name">name</code></td>
<td>
<p>(Optional) string describing ops managed by this function. If
not supplied the name of this function is used. Default value: &quot;sample_halton_sequence&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the members of the low discrepancy Halton sequence in dimension
<code>dim</code>. The <code>dim</code>-dimensional sequence takes values in the unit hypercube in
<code>dim</code> dimensions. Currently, only dimensions up to 1000 are supported. The
prime base for the k-th axes is the k-th prime starting from 2. For example,
if <code>dim</code> = 3, then the bases will be <code style="white-space: pre;">&#8288;[2, 3, 5]&#8288;</code> respectively and the first
element of the non-randomized sequence will be: <code style="white-space: pre;">&#8288;[0.5, 0.333, 0.2]&#8288;</code>. For a more
complete description of the Halton sequences see
<a href="https://en.wikipedia.org/wiki/Halton_sequence">here</a>. For low discrepancy
sequences and their applications see
<a href="https://en.wikipedia.org/wiki/Low-discrepancy_sequence">here</a>.
</p>
<p>If <code>randomized</code> is true, this function produces a scrambled version of the
Halton sequence introduced by Owen (2017). For the advantages of
randomization of low discrepancy sequences see
<a href="https://en.wikipedia.org/wiki/Quasi-Monte_Carlo_method#Randomization_of_quasi-Monte_Carlo">here</a>.
</p>
<p>The number of samples produced is controlled by the <code>num_results</code> and
<code>sequence_indices</code> parameters. The user must supply either <code>num_results</code> or
<code>sequence_indices</code> but not both.
The former is the number of samples to produce starting from the first
element. If <code>sequence_indices</code> is given instead, the specified elements of
the sequence are generated. For example, sequence_indices=tf$range(10) is
equivalent to specifying n=10.
</p>


<h3>Value</h3>

<p>halton_elements Elements of the Halton sequence. <code>Tensor</code> of supplied dtype
and <code>shape</code> <code style="white-space: pre;">&#8288;[num_results, dim]&#8288;</code> if <code>num_results</code> was specified or shape
<code style="white-space: pre;">&#8288;[s, dim]&#8288;</code> where s is the size of <code>sequence_indices</code> if <code>sequence_indices</code>
were specified.
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1706.02808">Art B. Owen. A randomized Halton algorithm in R. <em>arXiv preprint arXiv:1706.02808</em>, 2017.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>For an example how to use see <code><a href="#topic+mcmc_sample_chain">mcmc_sample_chain()</a></code>.
</p>
<p>Other mcmc_functions: 
<code><a href="#topic+mcmc_effective_sample_size">mcmc_effective_sample_size</a>()</code>,
<code><a href="#topic+mcmc_potential_scale_reduction">mcmc_potential_scale_reduction</a>()</code>,
<code><a href="#topic+mcmc_sample_annealed_importance_chain">mcmc_sample_annealed_importance_chain</a>()</code>,
<code><a href="#topic+mcmc_sample_chain">mcmc_sample_chain</a>()</code>
</p>

<hr>
<h2 id='mcmc_simple_step_size_adaptation'>Adapts the inner kernel's <code>step_size</code> based on <code>log_accept_prob</code>.</h2><span id='topic+mcmc_simple_step_size_adaptation'></span>

<h3>Description</h3>

<p>The simple policy multiplicatively increases or decreases the <code>step_size</code> of
the inner kernel based on the value of <code>log_accept_prob</code>. It is based on
equation 19 of Andrieu and Thoms (2008). Given enough steps and small
enough <code>adaptation_rate</code> the median of the distribution of the acceptance
probability will converge to the <code>target_accept_prob</code>. A good target
acceptance probability depends on the inner kernel. If this kernel is
<code>HamiltonianMonteCarlo</code>, then 0.6-0.9 is a good range to aim for. For
<code>RandomWalkMetropolis</code> this should be closer to 0.25. See the individual
kernels' docstrings for guidance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc_simple_step_size_adaptation(
  inner_kernel,
  num_adaptation_steps,
  target_accept_prob = 0.75,
  adaptation_rate = 0.01,
  step_size_setter_fn = NULL,
  step_size_getter_fn = NULL,
  log_accept_prob_getter_fn = NULL,
  validate_args = FALSE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc_simple_step_size_adaptation_+3A_inner_kernel">inner_kernel</code></td>
<td>
<p><code>TransitionKernel</code>-like object.</p>
</td></tr>
<tr><td><code id="mcmc_simple_step_size_adaptation_+3A_num_adaptation_steps">num_adaptation_steps</code></td>
<td>
<p>Scalar <code>integer</code> <code>Tensor</code> number of initial steps to
during which to adjust the step size. This may be greater, less than, or
equal to the number of burnin steps.</p>
</td></tr>
<tr><td><code id="mcmc_simple_step_size_adaptation_+3A_target_accept_prob">target_accept_prob</code></td>
<td>
<p>A floating point <code>Tensor</code> representing desired
acceptance probability. Must be a positive number less than 1. This can
either be a scalar, or have shape <code>list(num_chains)</code>. Default value: <code>0.75</code>
(the center of asymptotically optimal rate for HMC).</p>
</td></tr>
<tr><td><code id="mcmc_simple_step_size_adaptation_+3A_adaptation_rate">adaptation_rate</code></td>
<td>
<p><code>Tensor</code> representing amount to scale the current
<code>step_size</code>.</p>
</td></tr>
<tr><td><code id="mcmc_simple_step_size_adaptation_+3A_step_size_setter_fn">step_size_setter_fn</code></td>
<td>
<p>A function with the signature
<code style="white-space: pre;">&#8288;(kernel_results, new_step_size) -&gt; new_kernel_results&#8288;</code> where
<code>kernel_results</code> are the results of the <code>inner_kernel</code>, <code>new_step_size</code>
is a <code>Tensor</code> or a nested collection of <code>Tensor</code>s with the same
structure as returned by the <code>step_size_getter_fn</code>, and
<code>new_kernel_results</code> are a copy of <code>kernel_results</code> with the step
size(s) set.</p>
</td></tr>
<tr><td><code id="mcmc_simple_step_size_adaptation_+3A_step_size_getter_fn">step_size_getter_fn</code></td>
<td>
<p>A function with the signature
<code>(kernel_results) -&gt; step_size</code> where <code>kernel_results</code> are the results
of the <code>inner_kernel</code>, and <code>step_size</code> is a floating point <code>Tensor</code> or a
nested collection of such <code>Tensor</code>s.</p>
</td></tr>
<tr><td><code id="mcmc_simple_step_size_adaptation_+3A_log_accept_prob_getter_fn">log_accept_prob_getter_fn</code></td>
<td>
<p>A function with the signature
<code>(kernel_results) -&gt; log_accept_prob</code> where <code>kernel_results</code> are the
results of the <code>inner_kernel</code>, and <code>log_accept_prob</code> is a floating point
<code>Tensor</code>. <code>log_accept_prob</code> can either be a scalar, or have shape
<code>list(num_chains)</code>. If it's the latter, <code>step_size</code> should also have the same
leading dimension.</p>
</td></tr>
<tr><td><code id="mcmc_simple_step_size_adaptation_+3A_validate_args">validate_args</code></td>
<td>
<p><code>Logical</code>. When <code>True</code> kernel parameters are checked
for validity. When <code>False</code> invalid inputs may silently render incorrect
outputs.</p>
</td></tr>
<tr><td><code id="mcmc_simple_step_size_adaptation_+3A_name">name</code></td>
<td>
<p>string prefixed to Ops created by this class. Default: &quot;simple_step_size_adaptation&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In general, adaptation prevents the chain from reaching a stationary
distribution, so obtaining consistent samples requires <code>num_adaptation_steps</code>
be set to a value somewhat smaller than the number of burnin steps.
However, it may sometimes be helpful to set <code>num_adaptation_steps</code> to a larger
value during development in order to inspect the behavior of the chain during
adaptation.
</p>
<p>The step size is assumed to broadcast with the chain state, potentially having
leading dimensions corresponding to multiple chains. When there are fewer of
those leading dimensions than there are chain dimensions, the corresponding
dimensions in the <code>log_accept_prob</code> are averaged (in the direct space, rather
than the log space) before being used to adjust the step size. This means that
this kernel can do both cross-chain adaptation, or per-chain step size
adaptation, depending on the shape of the step size.
</p>
<p>For example, if your problem has a state with shape <code style="white-space: pre;">&#8288;[S]&#8288;</code>, your chain state
has shape <code style="white-space: pre;">&#8288;[C0, C1, Y]&#8288;</code> (meaning that there are <code>C0 * C1</code> total chains) and
<code>log_accept_prob</code> has shape <code style="white-space: pre;">&#8288;[C0, C1]&#8288;</code> (one acceptance probability per chain),
then depending on the shape of the step size, the following will happen:
</p>

<ul>
<li><p> Step size has shape <code style="white-space: pre;">&#8288;[]&#8288;</code>, <code style="white-space: pre;">&#8288;[S]&#8288;</code> or <code style="white-space: pre;">&#8288;[1]&#8288;</code>, the <code>log_accept_prob</code> will be averaged
across its <code>C0</code> and <code>C1</code> dimensions. This means that you will learn a shared
step size based on the mean acceptance probability across all chains. This
can be useful if you don't have a lot of steps to adapt and want to average
away the noise.
</p>
</li>
<li><p> Step size has shape <code style="white-space: pre;">&#8288;[C1, 1]&#8288;</code> or <code style="white-space: pre;">&#8288;[C1, S]&#8288;</code>, the <code>log_accept_prob</code> will be
averaged across its <code>C0</code> dimension. This means that you will learn a shared
step size based on the mean acceptance probability across chains that share
the coordinate across the <code>C1</code> dimension. This can be useful when the <code>C1</code>
dimension indexes different distributions, while <code>C0</code> indexes replicas of a
single distribution, all sampled in parallel.
</p>
</li>
<li><p> Step size has shape <code style="white-space: pre;">&#8288;[C0, C1, 1]&#8288;</code> or <code style="white-space: pre;">&#8288;[C0, C1, S]&#8288;</code>, then no averaging will
happen. This means that each chain will learn its own step size. This can be
useful when all chains are sampling from different distributions. Even when
all chains are for the same distribution, this can help during the initial
warmup period.
</p>
</li>
<li><p> Step size has shape <code style="white-space: pre;">&#8288;[C0, 1, 1]&#8288;</code> or <code style="white-space: pre;">&#8288;[C0, 1, S]&#8288;</code>, the <code>log_accept_prob</code> will be
averaged across its <code>C1</code> dimension. This means that you will learn a shared
step size based on the mean acceptance probability across chains that share
the coordinate across the <code>C0</code> dimension. This can be useful when the <code>C0</code>
dimension indexes different distributions, while <code>C1</code> indexes replicas of a
single distribution, all sampled in parallel.
</p>
</li></ul>



<h3>Value</h3>

<p>a Monte Carlo sampling kernel
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://people.eecs.berkeley.edu/~jordan/sail/readings/andrieu-thoms.pdf">Andrieu, Christophe, Thoms, Johannes. A tutorial on adaptive MCMC. <em>Statistics and Computing</em>, 2008.</a>
</p>
</li>
<li> <p><a href="https://arxiv.org/abs/1411.6669">Betancourt, M. J., Byrne, S., &amp; Girolami, M. (2014). <em>Optimizing The Integrator Step Size for Hamiltonian Monte Carlo</em>.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other mcmc_kernels: 
<code><a href="#topic+mcmc_dual_averaging_step_size_adaptation">mcmc_dual_averaging_step_size_adaptation</a>()</code>,
<code><a href="#topic+mcmc_hamiltonian_monte_carlo">mcmc_hamiltonian_monte_carlo</a>()</code>,
<code><a href="#topic+mcmc_metropolis_adjusted_langevin_algorithm">mcmc_metropolis_adjusted_langevin_algorithm</a>()</code>,
<code><a href="#topic+mcmc_metropolis_hastings">mcmc_metropolis_hastings</a>()</code>,
<code><a href="#topic+mcmc_no_u_turn_sampler">mcmc_no_u_turn_sampler</a>()</code>,
<code><a href="#topic+mcmc_random_walk_metropolis">mcmc_random_walk_metropolis</a>()</code>,
<code><a href="#topic+mcmc_replica_exchange_mc">mcmc_replica_exchange_mc</a>()</code>,
<code><a href="#topic+mcmc_slice_sampler">mcmc_slice_sampler</a>()</code>,
<code><a href="#topic+mcmc_transformed_transition_kernel">mcmc_transformed_transition_kernel</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_hamiltonian_monte_carlo">mcmc_uncalibrated_hamiltonian_monte_carlo</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_langevin">mcmc_uncalibrated_langevin</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_random_walk">mcmc_uncalibrated_random_walk</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  target_log_prob_fn &lt;- tfd_normal(loc = 0, scale = 1)$log_prob
  num_burnin_steps &lt;- 500
  num_results &lt;- 500
  num_chains &lt;- 64L
  step_size &lt;- tf$fill(list(num_chains), 0.1)

  kernel &lt;- mcmc_hamiltonian_monte_carlo(
    target_log_prob_fn = target_log_prob_fn,
    num_leapfrog_steps = 2,
    step_size = step_size
  ) %&gt;%
    mcmc_simple_step_size_adaptation(num_adaptation_steps = round(num_burnin_steps * 0.8))

  res &lt;- kernel %&gt;% mcmc_sample_chain(
    num_results = num_results,
    num_burnin_steps = num_burnin_steps,
    current_state = rep(0, num_chains),
    trace_fn = function(x, pkr) {
      list (
        pkr$inner_results$accepted_results$step_size,
        pkr$inner_results$log_accept_ratio
      )
    }
  )

  samples &lt;- res$all_states
  step_size &lt;- res$trace[[1]]
  log_accept_ratio &lt;- res$trace[[2]]


</code></pre>

<hr>
<h2 id='mcmc_slice_sampler'>Runs one step of the slice sampler using a hit and run approach</h2><span id='topic+mcmc_slice_sampler'></span>

<h3>Description</h3>

<p>Slice Sampling is a Markov Chain Monte Carlo (MCMC) algorithm based, as stated
by Neal (2003), on the observation that &quot;...one can sample from a
distribution by sampling uniformly from the region under the plot of its
density function. A Markov chain that converges to this uniform distribution
can be constructed by alternately uniform sampling in the vertical direction
with uniform sampling from the horizontal <code>slice</code> defined by the current
vertical position, or more generally, with some update that leaves the uniform
distribution over this slice invariant&quot;. Mathematical details and derivations
can be found in Neal (2003). The one dimensional slice sampler is
extended to n-dimensions through use of a hit-and-run approach: choose a
random direction in n-dimensional space and take a step, as determined by the
one-dimensional slice sampling algorithm, along that direction
(Belisle at al. 1993).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc_slice_sampler(
  target_log_prob_fn,
  step_size,
  max_doublings,
  seed = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc_slice_sampler_+3A_target_log_prob_fn">target_log_prob_fn</code></td>
<td>
<p>Function which takes an argument like
<code>current_state</code> (if it's a list <code>current_state</code> will be unpacked) and returns its
(possibly unnormalized) log-density under the target distribution.</p>
</td></tr>
<tr><td><code id="mcmc_slice_sampler_+3A_step_size">step_size</code></td>
<td>
<p><code>Tensor</code> or <code>list</code> of <code>Tensor</code>s representing the step
size for the leapfrog integrator. Must broadcast with the shape of
<code>current_state</code>. Larger step sizes lead to faster progress, but
too-large step sizes make rejection exponentially more likely. When
possible, it's often helpful to match per-variable step sizes to the
standard deviations of the target distribution in each variable.</p>
</td></tr>
<tr><td><code id="mcmc_slice_sampler_+3A_max_doublings">max_doublings</code></td>
<td>
<p>Scalar positive int32 <code>tf$Tensor</code>. The maximum number of
doublings to consider.</p>
</td></tr>
<tr><td><code id="mcmc_slice_sampler_+3A_seed">seed</code></td>
<td>
<p>integer to seed the random number generator.</p>
</td></tr>
<tr><td><code id="mcmc_slice_sampler_+3A_name">name</code></td>
<td>
<p>string prefixed to Ops created by this function.
Default value: <code>NULL</code> (i.e., 'slice_sampler_kernel').</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>one_step</code> function can update multiple chains in parallel. It assumes
that all leftmost dimensions of <code>current_state</code> index independent chain states
(and are therefore updated independently). The output of
<code style="white-space: pre;">&#8288;target_log_prob_fn(*current_state)&#8288;</code> should sum log-probabilities across all
event dimensions. Slices along the rightmost dimensions may have different
target distributions; for example, <code style="white-space: pre;">&#8288;current_state[0, :]&#8288;</code> could have a
different target distribution from <code style="white-space: pre;">&#8288;current_state[1, :]&#8288;</code>. These semantics are
governed by <code style="white-space: pre;">&#8288;target_log_prob_fn(*current_state)&#8288;</code>. (The number of independent
chains is <code style="white-space: pre;">&#8288;tf$size(target_log_prob_fn(*current_state))&#8288;</code>.)
</p>
<p>Note that the sampler only supports states where all components have a common
dtype.
</p>


<h3>Value</h3>

<p>list of
<code>next_state</code> (Tensor or Python list of <code>Tensor</code>s representing the state(s)
of the Markov chain(s) at each result step. Has same shape as
and <code>current_state</code>.) and
<code>kernel_results</code> (<code>collections$namedtuple</code> of internal calculations used to
'advance the chain).
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://projecteuclid.org/download/pdf_1/euclid.aos/1056562461">Radford M. Neal. Slice Sampling. The Annals of Statistics. 2003, Vol 31, No. 3 , 705-767.</a>
</p>
</li>
<li><p> C.J.P. Belisle, H.E. Romeijn, R.L. Smith. <cite>Hit-and-run algorithms for generating multivariate distributions. Math. Oper. Res., 18(1993), 225-266.</cite>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other mcmc_kernels: 
<code><a href="#topic+mcmc_dual_averaging_step_size_adaptation">mcmc_dual_averaging_step_size_adaptation</a>()</code>,
<code><a href="#topic+mcmc_hamiltonian_monte_carlo">mcmc_hamiltonian_monte_carlo</a>()</code>,
<code><a href="#topic+mcmc_metropolis_adjusted_langevin_algorithm">mcmc_metropolis_adjusted_langevin_algorithm</a>()</code>,
<code><a href="#topic+mcmc_metropolis_hastings">mcmc_metropolis_hastings</a>()</code>,
<code><a href="#topic+mcmc_no_u_turn_sampler">mcmc_no_u_turn_sampler</a>()</code>,
<code><a href="#topic+mcmc_random_walk_metropolis">mcmc_random_walk_metropolis</a>()</code>,
<code><a href="#topic+mcmc_replica_exchange_mc">mcmc_replica_exchange_mc</a>()</code>,
<code><a href="#topic+mcmc_simple_step_size_adaptation">mcmc_simple_step_size_adaptation</a>()</code>,
<code><a href="#topic+mcmc_transformed_transition_kernel">mcmc_transformed_transition_kernel</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_hamiltonian_monte_carlo">mcmc_uncalibrated_hamiltonian_monte_carlo</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_langevin">mcmc_uncalibrated_langevin</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_random_walk">mcmc_uncalibrated_random_walk</a>()</code>
</p>

<hr>
<h2 id='mcmc_transformed_transition_kernel'>Applies a bijector to the MCMC's state space</h2><span id='topic+mcmc_transformed_transition_kernel'></span>

<h3>Description</h3>

<p>The transformed transition kernel enables fitting
a bijector which serves to decorrelate the Markov chain Monte Carlo (MCMC)
event dimensions thus making the chain mix faster. This is
particularly useful when the geometry of the target distribution is
unfavorable. In such cases it may take many evaluations of the
<code>target_log_prob_fn</code> for the chain to mix between faraway states.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc_transformed_transition_kernel(inner_kernel, bijector, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc_transformed_transition_kernel_+3A_inner_kernel">inner_kernel</code></td>
<td>
<p><code>TransitionKernel</code>-like object which has a <code>target_log_prob_fn</code> argument.</p>
</td></tr>
<tr><td><code id="mcmc_transformed_transition_kernel_+3A_bijector">bijector</code></td>
<td>
<p>bijector or list of bijectors. These bijectors use <code>forward</code> to map the
<code>inner_kernel</code> state space to the state expected by <code>inner_kernel$target_log_prob_fn</code>.</p>
</td></tr>
<tr><td><code id="mcmc_transformed_transition_kernel_+3A_name">name</code></td>
<td>
<p>string prefixed to Ops created by this function.
Default value: <code>NULL</code> (i.e., &quot;transformed_kernel&quot;).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The idea of training an affine function to decorrelate chain event dims was
presented in Parno and Marzouk (2014). Used in conjunction with the
Hamiltonian Monte Carlo transition kernel, the Parno and Marzouk (2014)
idea is an instance of Riemannian manifold HMC (Girolami and Calderhead, 2011).
</p>
<p>The transformed transition kernel enables arbitrary bijective transformations
of arbitrary transition kernels, e.g., one could use bijectors
<code>tfb_affine</code>, <code>tfb_real_nvp</code>, etc.
with transition kernels <code>mcmc_hamiltonian_monte_carlo</code>, <code>mcmc_random_walk_metropolis</code>, etc.
</p>


<h3>Value</h3>

<p>a Monte Carlo sampling kernel
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1412.5492">Matthew Parno and Youssef Marzouk. Transport map accelerated Markov chain Monte Carlo. <em>arXiv preprint arXiv:1412.5492</em>, 2014.</a>
</p>
</li>
<li> <p><a href="http://people.ee.duke.edu/~lcarin/Girolami2011.pdf">Mark Girolami and Ben Calderhead. Riemann manifold langevin and hamiltonian monte carlo methods. In <em>Journal of the Royal Statistical Society</em>, 2011.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other mcmc_kernels: 
<code><a href="#topic+mcmc_dual_averaging_step_size_adaptation">mcmc_dual_averaging_step_size_adaptation</a>()</code>,
<code><a href="#topic+mcmc_hamiltonian_monte_carlo">mcmc_hamiltonian_monte_carlo</a>()</code>,
<code><a href="#topic+mcmc_metropolis_adjusted_langevin_algorithm">mcmc_metropolis_adjusted_langevin_algorithm</a>()</code>,
<code><a href="#topic+mcmc_metropolis_hastings">mcmc_metropolis_hastings</a>()</code>,
<code><a href="#topic+mcmc_no_u_turn_sampler">mcmc_no_u_turn_sampler</a>()</code>,
<code><a href="#topic+mcmc_random_walk_metropolis">mcmc_random_walk_metropolis</a>()</code>,
<code><a href="#topic+mcmc_replica_exchange_mc">mcmc_replica_exchange_mc</a>()</code>,
<code><a href="#topic+mcmc_simple_step_size_adaptation">mcmc_simple_step_size_adaptation</a>()</code>,
<code><a href="#topic+mcmc_slice_sampler">mcmc_slice_sampler</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_hamiltonian_monte_carlo">mcmc_uncalibrated_hamiltonian_monte_carlo</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_langevin">mcmc_uncalibrated_langevin</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_random_walk">mcmc_uncalibrated_random_walk</a>()</code>
</p>

<hr>
<h2 id='mcmc_uncalibrated_hamiltonian_monte_carlo'>Runs one step of Uncalibrated Hamiltonian Monte Carlo</h2><span id='topic+mcmc_uncalibrated_hamiltonian_monte_carlo'></span>

<h3>Description</h3>

<p>Warning: this kernel will not result in a chain which converges to the
<code>target_log_prob</code>. To get a convergent MCMC, use <code>mcmc_hamiltonian_monte_carlo(...)</code>
or <code>mcmc_metropolis_hastings(mcmc_uncalibrated_hamiltonian_monte_carlo(...))</code>.
For more details on <code>UncalibratedHamiltonianMonteCarlo</code>, see <code>HamiltonianMonteCarlo</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc_uncalibrated_hamiltonian_monte_carlo(
  target_log_prob_fn,
  step_size,
  num_leapfrog_steps,
  state_gradients_are_stopped = FALSE,
  seed = NULL,
  store_parameters_in_results = FALSE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc_uncalibrated_hamiltonian_monte_carlo_+3A_target_log_prob_fn">target_log_prob_fn</code></td>
<td>
<p>Function which takes an argument like
<code>current_state</code> (if it's a list <code>current_state</code> will be unpacked) and returns its
(possibly unnormalized) log-density under the target distribution.</p>
</td></tr>
<tr><td><code id="mcmc_uncalibrated_hamiltonian_monte_carlo_+3A_step_size">step_size</code></td>
<td>
<p><code>Tensor</code> or <code>list</code> of <code>Tensor</code>s representing the step
size for the leapfrog integrator. Must broadcast with the shape of
<code>current_state</code>. Larger step sizes lead to faster progress, but
too-large step sizes make rejection exponentially more likely. When
possible, it's often helpful to match per-variable step sizes to the
standard deviations of the target distribution in each variable.</p>
</td></tr>
<tr><td><code id="mcmc_uncalibrated_hamiltonian_monte_carlo_+3A_num_leapfrog_steps">num_leapfrog_steps</code></td>
<td>
<p>Integer number of steps to run the leapfrog integrator
for. Total progress per HMC step is roughly proportional to
<code>step_size * num_leapfrog_steps</code>.</p>
</td></tr>
<tr><td><code id="mcmc_uncalibrated_hamiltonian_monte_carlo_+3A_state_gradients_are_stopped">state_gradients_are_stopped</code></td>
<td>
<p><code>logical</code> indicating that the proposed
new state be run through <code>tf$stop_gradient</code>. This is particularly useful
when combining optimization over samples from the HMC chain.
Default value: <code>FALSE</code> (i.e., do not apply <code>stop_gradient</code>).</p>
</td></tr>
<tr><td><code id="mcmc_uncalibrated_hamiltonian_monte_carlo_+3A_seed">seed</code></td>
<td>
<p>integer to seed the random number generator.</p>
</td></tr>
<tr><td><code id="mcmc_uncalibrated_hamiltonian_monte_carlo_+3A_store_parameters_in_results">store_parameters_in_results</code></td>
<td>
<p>If <code>TRUE</code>, then <code>step_size</code> and
<code>num_leapfrog_steps</code> are written to and read from eponymous fields in
the kernel results objects returned from <code>one_step</code> and
<code>bootstrap_results</code>. This allows wrapper kernels to adjust those
parameters on the fly. This is incompatible with <code>step_size_update_fn</code>,
which must be set to <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="mcmc_uncalibrated_hamiltonian_monte_carlo_+3A_name">name</code></td>
<td>
<p>string prefixed to Ops created by this function.
Default value: <code>NULL</code> (i.e., 'hmc_kernel').</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Monte Carlo sampling kernel
</p>


<h3>See Also</h3>

<p>Other mcmc_kernels: 
<code><a href="#topic+mcmc_dual_averaging_step_size_adaptation">mcmc_dual_averaging_step_size_adaptation</a>()</code>,
<code><a href="#topic+mcmc_hamiltonian_monte_carlo">mcmc_hamiltonian_monte_carlo</a>()</code>,
<code><a href="#topic+mcmc_metropolis_adjusted_langevin_algorithm">mcmc_metropolis_adjusted_langevin_algorithm</a>()</code>,
<code><a href="#topic+mcmc_metropolis_hastings">mcmc_metropolis_hastings</a>()</code>,
<code><a href="#topic+mcmc_no_u_turn_sampler">mcmc_no_u_turn_sampler</a>()</code>,
<code><a href="#topic+mcmc_random_walk_metropolis">mcmc_random_walk_metropolis</a>()</code>,
<code><a href="#topic+mcmc_replica_exchange_mc">mcmc_replica_exchange_mc</a>()</code>,
<code><a href="#topic+mcmc_simple_step_size_adaptation">mcmc_simple_step_size_adaptation</a>()</code>,
<code><a href="#topic+mcmc_slice_sampler">mcmc_slice_sampler</a>()</code>,
<code><a href="#topic+mcmc_transformed_transition_kernel">mcmc_transformed_transition_kernel</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_langevin">mcmc_uncalibrated_langevin</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_random_walk">mcmc_uncalibrated_random_walk</a>()</code>
</p>

<hr>
<h2 id='mcmc_uncalibrated_langevin'>Runs one step of Uncalibrated Langevin discretized diffusion.</h2><span id='topic+mcmc_uncalibrated_langevin'></span>

<h3>Description</h3>

<p>The class generates a Langevin proposal using <code style="white-space: pre;">&#8288;_euler_method&#8288;</code> function and
also computes helper <code>UncalibratedLangevinKernelResults</code> for the next
iteration.
Warning: this kernel will not result in a chain which converges to the
<code>target_log_prob</code>. To get a convergent MCMC, use
<code>MetropolisAdjustedLangevinAlgorithm(...)</code> or <code>MetropolisHastings(UncalibratedLangevin(...))</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc_uncalibrated_langevin(
  target_log_prob_fn,
  step_size,
  volatility_fn = NULL,
  parallel_iterations = 10,
  compute_acceptance = TRUE,
  seed = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc_uncalibrated_langevin_+3A_target_log_prob_fn">target_log_prob_fn</code></td>
<td>
<p>Function which takes an argument like
<code>current_state</code> (if it's a list <code>current_state</code> will be unpacked) and returns its
(possibly unnormalized) log-density under the target distribution.</p>
</td></tr>
<tr><td><code id="mcmc_uncalibrated_langevin_+3A_step_size">step_size</code></td>
<td>
<p><code>Tensor</code> or <code>list</code> of <code>Tensor</code>s representing the step
size for the leapfrog integrator. Must broadcast with the shape of
<code>current_state</code>. Larger step sizes lead to faster progress, but
too-large step sizes make rejection exponentially more likely. When
possible, it's often helpful to match per-variable step sizes to the
standard deviations of the target distribution in each variable.</p>
</td></tr>
<tr><td><code id="mcmc_uncalibrated_langevin_+3A_volatility_fn">volatility_fn</code></td>
<td>
<p>function which takes an argument like
<code>current_state</code> (or <code style="white-space: pre;">&#8288;*current_state&#8288;</code> if it's a list) and returns
volatility value at <code>current_state</code>. Should return a <code>Tensor</code> or
<code>list</code> of <code>Tensor</code>s that must broadcast with the shape of
<code>current_state</code>. Defaults to the identity function.</p>
</td></tr>
<tr><td><code id="mcmc_uncalibrated_langevin_+3A_parallel_iterations">parallel_iterations</code></td>
<td>
<p>the number of coordinates for which the gradients of
the volatility matrix <code>volatility_fn</code> can be computed in parallel.</p>
</td></tr>
<tr><td><code id="mcmc_uncalibrated_langevin_+3A_compute_acceptance">compute_acceptance</code></td>
<td>
<p>logical indicating whether to compute the
Metropolis log-acceptance ratio used to construct <code>MetropolisAdjustedLangevinAlgorithm</code> kernel.</p>
</td></tr>
<tr><td><code id="mcmc_uncalibrated_langevin_+3A_seed">seed</code></td>
<td>
<p>integer to seed the random number generator.</p>
</td></tr>
<tr><td><code id="mcmc_uncalibrated_langevin_+3A_name">name</code></td>
<td>
<p>String prefixed to Ops created by this function.
Default value: <code>NULL</code> (i.e., 'mala_kernel').</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of
<code>next_state</code> (Tensor or Python list of <code>Tensor</code>s representing the state(s)
of the Markov chain(s) at each result step. Has same shape as
and <code>current_state</code>.) and
<code>kernel_results</code> (<code>collections$namedtuple</code> of internal calculations used to
'advance the chain).
</p>


<h3>See Also</h3>

<p>Other mcmc_kernels: 
<code><a href="#topic+mcmc_dual_averaging_step_size_adaptation">mcmc_dual_averaging_step_size_adaptation</a>()</code>,
<code><a href="#topic+mcmc_hamiltonian_monte_carlo">mcmc_hamiltonian_monte_carlo</a>()</code>,
<code><a href="#topic+mcmc_metropolis_adjusted_langevin_algorithm">mcmc_metropolis_adjusted_langevin_algorithm</a>()</code>,
<code><a href="#topic+mcmc_metropolis_hastings">mcmc_metropolis_hastings</a>()</code>,
<code><a href="#topic+mcmc_no_u_turn_sampler">mcmc_no_u_turn_sampler</a>()</code>,
<code><a href="#topic+mcmc_random_walk_metropolis">mcmc_random_walk_metropolis</a>()</code>,
<code><a href="#topic+mcmc_replica_exchange_mc">mcmc_replica_exchange_mc</a>()</code>,
<code><a href="#topic+mcmc_simple_step_size_adaptation">mcmc_simple_step_size_adaptation</a>()</code>,
<code><a href="#topic+mcmc_slice_sampler">mcmc_slice_sampler</a>()</code>,
<code><a href="#topic+mcmc_transformed_transition_kernel">mcmc_transformed_transition_kernel</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_hamiltonian_monte_carlo">mcmc_uncalibrated_hamiltonian_monte_carlo</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_random_walk">mcmc_uncalibrated_random_walk</a>()</code>
</p>

<hr>
<h2 id='mcmc_uncalibrated_random_walk'>Generate proposal for the Random Walk Metropolis algorithm.</h2><span id='topic+mcmc_uncalibrated_random_walk'></span>

<h3>Description</h3>

<p>Warning: this kernel will not result in a chain which converges to the
<code>target_log_prob</code>. To get a convergent MCMC, use
<code>mcmc_random_walk_metropolis(...)</code> or
<code>mcmc_metropolis_hastings(mcmc_uncalibrated_random_walk(...))</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcmc_uncalibrated_random_walk(
  target_log_prob_fn,
  new_state_fn = NULL,
  seed = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcmc_uncalibrated_random_walk_+3A_target_log_prob_fn">target_log_prob_fn</code></td>
<td>
<p>Function which takes an argument like
<code>current_state</code> ((if it's a list <code>current_state</code> will be unpacked) and returns its
(possibly unnormalized) log-density under the target distribution.</p>
</td></tr>
<tr><td><code id="mcmc_uncalibrated_random_walk_+3A_new_state_fn">new_state_fn</code></td>
<td>
<p>Function which takes a list of state parts and a
seed; returns a same-type <code>list</code> of <code>Tensor</code>s, each being a perturbation
of the input state parts. The perturbation distribution is assumed to be
a symmetric distribution centered at the input state part.
Default value: <code>NULL</code> which is mapped to <code>tfp$mcmc$random_walk_normal_fn()</code>.</p>
</td></tr>
<tr><td><code id="mcmc_uncalibrated_random_walk_+3A_seed">seed</code></td>
<td>
<p>integer to seed the random number generator.</p>
</td></tr>
<tr><td><code id="mcmc_uncalibrated_random_walk_+3A_name">name</code></td>
<td>
<p>String name prefixed to Ops created by this function.
Default value: <code>NULL</code> (i.e., 'rwm_kernel').</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Monte Carlo sampling kernel
</p>


<h3>See Also</h3>

<p>Other mcmc_kernels: 
<code><a href="#topic+mcmc_dual_averaging_step_size_adaptation">mcmc_dual_averaging_step_size_adaptation</a>()</code>,
<code><a href="#topic+mcmc_hamiltonian_monte_carlo">mcmc_hamiltonian_monte_carlo</a>()</code>,
<code><a href="#topic+mcmc_metropolis_adjusted_langevin_algorithm">mcmc_metropolis_adjusted_langevin_algorithm</a>()</code>,
<code><a href="#topic+mcmc_metropolis_hastings">mcmc_metropolis_hastings</a>()</code>,
<code><a href="#topic+mcmc_no_u_turn_sampler">mcmc_no_u_turn_sampler</a>()</code>,
<code><a href="#topic+mcmc_random_walk_metropolis">mcmc_random_walk_metropolis</a>()</code>,
<code><a href="#topic+mcmc_replica_exchange_mc">mcmc_replica_exchange_mc</a>()</code>,
<code><a href="#topic+mcmc_simple_step_size_adaptation">mcmc_simple_step_size_adaptation</a>()</code>,
<code><a href="#topic+mcmc_slice_sampler">mcmc_slice_sampler</a>()</code>,
<code><a href="#topic+mcmc_transformed_transition_kernel">mcmc_transformed_transition_kernel</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_hamiltonian_monte_carlo">mcmc_uncalibrated_hamiltonian_monte_carlo</a>()</code>,
<code><a href="#topic+mcmc_uncalibrated_langevin">mcmc_uncalibrated_langevin</a>()</code>
</p>

<hr>
<h2 id='params_size_categorical_mixture_of_one_hot_categorical'>number of <code>params</code> needed to create a CategoricalMixtureOfOneHotCategorical distribution</h2><span id='topic+params_size_categorical_mixture_of_one_hot_categorical'></span>

<h3>Description</h3>

<p>number of <code>params</code> needed to create a CategoricalMixtureOfOneHotCategorical distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>params_size_categorical_mixture_of_one_hot_categorical(
  event_size,
  num_components
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="params_size_categorical_mixture_of_one_hot_categorical_+3A_event_size">event_size</code></td>
<td>
<p>event size of this distribution</p>
</td></tr>
<tr><td><code id="params_size_categorical_mixture_of_one_hot_categorical_+3A_num_components">num_components</code></td>
<td>
<p>number of components in the mixture</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a scalar
</p>

<hr>
<h2 id='params_size_independent_bernoulli'>number of <code>params</code> needed to create an IndependentBernoulli distribution</h2><span id='topic+params_size_independent_bernoulli'></span>

<h3>Description</h3>

<p>number of <code>params</code> needed to create an IndependentBernoulli distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>params_size_independent_bernoulli(event_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="params_size_independent_bernoulli_+3A_event_size">event_size</code></td>
<td>
<p>event size of this distribution</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a scalar
</p>

<hr>
<h2 id='params_size_independent_logistic'>number of <code>params</code> needed to create an IndependentLogistic distribution</h2><span id='topic+params_size_independent_logistic'></span>

<h3>Description</h3>

<p>number of <code>params</code> needed to create an IndependentLogistic distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>params_size_independent_logistic(event_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="params_size_independent_logistic_+3A_event_size">event_size</code></td>
<td>
<p>event size of this distribution</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a scalar
</p>

<hr>
<h2 id='params_size_independent_normal'>number of <code>params</code> needed to create an IndependentNormal distribution</h2><span id='topic+params_size_independent_normal'></span>

<h3>Description</h3>

<p>number of <code>params</code> needed to create an IndependentNormal distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>params_size_independent_normal(event_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="params_size_independent_normal_+3A_event_size">event_size</code></td>
<td>
<p>event size of this distribution</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a scalar
</p>

<hr>
<h2 id='params_size_independent_poisson'>number of <code>params</code> needed to create an IndependentPoisson distribution</h2><span id='topic+params_size_independent_poisson'></span>

<h3>Description</h3>

<p>number of <code>params</code> needed to create an IndependentPoisson distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>params_size_independent_poisson(event_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="params_size_independent_poisson_+3A_event_size">event_size</code></td>
<td>
<p>event size of this distribution</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a scalar
</p>

<hr>
<h2 id='params_size_mixture_logistic'>number of <code>params</code> needed to create a MixtureLogistic distribution</h2><span id='topic+params_size_mixture_logistic'></span>

<h3>Description</h3>

<p>number of <code>params</code> needed to create a MixtureLogistic distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>params_size_mixture_logistic(num_components, event_shape)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="params_size_mixture_logistic_+3A_num_components">num_components</code></td>
<td>
<p>Number of component distributions in the mixture distribution.</p>
</td></tr>
<tr><td><code id="params_size_mixture_logistic_+3A_event_shape">event_shape</code></td>
<td>
<p>Number of parameters needed to create a single component distribution.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a scalar
</p>

<hr>
<h2 id='params_size_mixture_normal'>number of <code>params</code> needed to create a MixtureNormal distribution</h2><span id='topic+params_size_mixture_normal'></span>

<h3>Description</h3>

<p>number of <code>params</code> needed to create a MixtureNormal distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>params_size_mixture_normal(num_components, event_shape)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="params_size_mixture_normal_+3A_num_components">num_components</code></td>
<td>
<p>Number of component distributions in the mixture distribution.</p>
</td></tr>
<tr><td><code id="params_size_mixture_normal_+3A_event_shape">event_shape</code></td>
<td>
<p>Number of parameters needed to create a single component distribution.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a scalar
</p>

<hr>
<h2 id='params_size_mixture_same_family'>number of <code>params</code> needed to create a MixtureSameFamily distribution</h2><span id='topic+params_size_mixture_same_family'></span>

<h3>Description</h3>

<p>number of <code>params</code> needed to create a MixtureSameFamily distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>params_size_mixture_same_family(num_components, component_params_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="params_size_mixture_same_family_+3A_num_components">num_components</code></td>
<td>
<p>Number of component distributions in the mixture distribution.</p>
</td></tr>
<tr><td><code id="params_size_mixture_same_family_+3A_component_params_size">component_params_size</code></td>
<td>
<p>Number of parameters needed to create a single component distribution.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a scalar
</p>

<hr>
<h2 id='params_size_multivariate_normal_tri_l'>number of <code>params</code> needed to create a MultivariateNormalTriL distribution</h2><span id='topic+params_size_multivariate_normal_tri_l'></span>

<h3>Description</h3>

<p>number of <code>params</code> needed to create a MultivariateNormalTriL distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>params_size_multivariate_normal_tri_l(event_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="params_size_multivariate_normal_tri_l_+3A_event_size">event_size</code></td>
<td>
<p>event size of this distribution</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a scalar
</p>

<hr>
<h2 id='params_size_one_hot_categorical'>number of <code>params</code> needed to create a OneHotCategorical distribution</h2><span id='topic+params_size_one_hot_categorical'></span>

<h3>Description</h3>

<p>number of <code>params</code> needed to create a OneHotCategorical distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>params_size_one_hot_categorical(event_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="params_size_one_hot_categorical_+3A_event_size">event_size</code></td>
<td>
<p>event size of this distribution</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a scalar
</p>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+tf'></span><span id='topic+shape'></span><span id='topic+tf_config'></span><span id='topic++25+3E+25'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>magrittr</dt><dd><p><code><a href="magrittr.html#topic+pipe">%&gt;%</a></code></p>
</dd>
<dt>tensorflow</dt><dd><p><code><a href="tensorflow.html#topic+shape">shape</a></code>, <code><a href="tensorflow.html#topic+tf">tf</a></code>, <code><a href="tensorflow.html#topic+tf_config">tf_config</a></code></p>
</dd>
</dl>


<h3>Value</h3>

<p>a alias for tensorflow::tf
</p>
<p>a alias for tensorflow::shape
</p>
<p>a alias for tensorflow::tf_config
</p>
<p>a alias for magrittr::<code style="white-space: pre;">&#8288;%&gt;%&#8288;</code>
</p>

<hr>
<h2 id='sts_additive_state_space_model'>A state space model representing a sum of component state space models.</h2><span id='topic+sts_additive_state_space_model'></span>

<h3>Description</h3>

<p>A state space model (SSM) posits a set of latent (unobserved) variables that
evolve over time with dynamics specified by a probabilistic transition model
<code>p(z[t+1] | z[t])</code>. At each timestep, we observe a value sampled from an
observation model conditioned on the current state, <code>p(x[t] | z[t])</code>. The
special case where both the transition and observation models are Gaussians
with mean specified as a linear function of the inputs, is known as a linear
Gaussian state space model and supports tractable exact probabilistic
calculations; see <code>tfd_linear_gaussian_state_space_model</code> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sts_additive_state_space_model(
  component_ssms,
  constant_offset = 0,
  observation_noise_scale = NULL,
  initial_state_prior = NULL,
  initial_step = 0,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_additive_state_space_model_+3A_component_ssms">component_ssms</code></td>
<td>
<p><code>list</code> containing one or more
<code>tfd_linear_gaussian_state_space_model</code> instances. The components
will in general implement different time-series models, with possibly
different <code>latent_size</code>, but they must have the same <code>dtype</code>, event
shape (<code>num_timesteps</code> and <code>observation_size</code>), and their batch shapes
must broadcast to a compatible batch shape.#'</p>
</td></tr>
<tr><td><code id="sts_additive_state_space_model_+3A_constant_offset">constant_offset</code></td>
<td>
<p>scalar <code>float</code> <code>tensor</code>, or batch of scalars,
specifying a constant value added to the sum of outputs from the
component models. This allows the components to model the shifted series
<code>observed_time_series - constant_offset</code>. Default value: <code>0</code>.#'</p>
</td></tr>
<tr><td><code id="sts_additive_state_space_model_+3A_observation_noise_scale">observation_noise_scale</code></td>
<td>
<p>Optional scalar <code>float</code> <code>tensor</code> indicating the
standard deviation of the observation noise. May contain additional
batch dimensions, which must broadcast with the batch shape of elements
in <code>component_ssms</code>. If <code>observation_noise_scale</code> is specified for the
<code>sts_additive_state_space_model</code>, the observation noise scales of component
models are ignored. If <code>NULL</code>, the observation noise scale is derived
by summing the noise variances of the component models, i.e.,
<code style="white-space: pre;">&#8288;observation_noise_scale = sqrt(sum([ssm.observation_noise_scale**2 for ssm in component_ssms]))&#8288;</code>.</p>
</td></tr>
<tr><td><code id="sts_additive_state_space_model_+3A_initial_state_prior">initial_state_prior</code></td>
<td>
<p>instance of <code>tfd_multivariate_normal</code>
representing the prior distribution on latent states.  Must have
event shape <code style="white-space: pre;">&#8288;[1]&#8288;</code> (as <code>tfd_linear_gaussian_state_space_model</code> requires a
rank-1 event shape).</p>
</td></tr>
<tr><td><code id="sts_additive_state_space_model_+3A_initial_step">initial_step</code></td>
<td>
<p>Optional scalar <code>integer</code> <code>tensor</code> specifying the starting
timestep. Default value: 0.</p>
</td></tr>
<tr><td><code id="sts_additive_state_space_model_+3A_validate_args">validate_args</code></td>
<td>
<p><code>logical</code>. Whether to validate input
with asserts. If <code>validate_args</code> is <code>FALSE</code>, and the inputs are
invalid, correct behavior is not guaranteed. Default value: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="sts_additive_state_space_model_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p><code>logical</code>. If <code>FALSE</code>, raise an
exception if a statistic (e.g. mean/mode/etc...) is undefined for any
batch member. If <code>TRUE</code>, batch members with valid parameters leading to
undefined statistics will return NaN for this statistic. Default value: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="sts_additive_state_space_model_+3A_name">name</code></td>
<td>
<p>string prefixed to ops created by this class.
Default value: &quot;AdditiveStateSpaceModel&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>sts_additive_state_space_model</code> represents a sum of component state space
models. Each of the <code>N</code> components describes a random process
generating a distribution on observed time series <code style="white-space: pre;">&#8288;x1[t], x2[t], ..., xN[t]&#8288;</code>.
The additive model represents the sum of these
processes, <code>y[t] = x1[t] + x2[t] + ... + xN[t] + eps[t]</code>, where
<code>eps[t] ~ N(0, observation_noise_scale)</code> is an observation noise term.
</p>
<p>Mathematical Details
</p>
<p>The additive model concatenates the latent states of its component models.
The generative process runs each component's dynamics in its own subspace of
latent space, and then observes the sum of the observation models from the
components.
</p>
<p>Formally, the transition model is linear Gaussian:
</p>
<div class="sourceCode"><pre>p(z[t+1] | z[t]) ~ Normal(loc = transition_matrix.matmul(z[t]), cov = transition_cov)
</pre></div>
<p>where each <code>z[t]</code> is a latent state vector concatenating the component
state vectors, <code style="white-space: pre;">&#8288;z[t] = [z1[t], z2[t], ..., zN[t]]&#8288;</code>, so it has size
<code style="white-space: pre;">&#8288;latent_size = sum([c.latent_size for c in components])&#8288;</code>.
</p>
<p>The transition matrix is the block-diagonal composition of transition
matrices from the component processes:
</p>
<div class="sourceCode"><pre>transition_matrix =
 [[ c0.transition_matrix,  0.,                   ..., 0.                   ],
  [ 0.,                    c1.transition_matrix, ..., 0.                   ],
  [ ...                    ...                   ...                       ],
  [ 0.,                    0.,                   ..., cN.transition_matrix ]]
</pre></div>
<p>and the noise covariance is similarly the block-diagonal composition of
component noise covariances:
</p>
<div class="sourceCode"><pre>transition_cov =
 [[ c0.transition_cov, 0.,                ..., 0.                ],
  [ 0.,                c1.transition_cov, ..., 0.                ],
  [ ...                ...                     ...               ],
  [ 0.,                0.,                ..., cN.transition_cov ]]
</pre></div>
<p>The observation model is also linear Gaussian,
</p>
<div class="sourceCode"><pre>p(y[t] | z[t]) ~ Normal(loc = observation_matrix.matmul(z[t]), stddev = observation_noise_scale)
</pre></div>
<p>This implementation assumes scalar observations, so <code>observation_matrix</code> has shape <code style="white-space: pre;">&#8288;[1, latent_size]&#8288;</code>.
The additive observation matrix simply concatenates the observation matrices from each component:
</p>
<div class="sourceCode"><pre>observation_matrix = concat([c0.obs_matrix, c1.obs_matrix, ..., cN.obs_matrix], axis=-1)
</pre></div>
<p>The effect is that each component observation matrix acts on the dimensions
of latent state corresponding to that component, and the overall expected
observation is the sum of the expected observations from each component.
</p>
<p>If <code>observation_noise_scale</code> is not explicitly specified, it is also computed
by summing the noise variances of the component processes:
</p>
<div class="sourceCode"><pre>observation_noise_scale = sqrt(sum([c.observation_noise_scale**2 for c in components]))
</pre></div>


<h3>Value</h3>

<p>an instance of <code>LinearGaussianStateSpaceModel</code>.
</p>


<h3>See Also</h3>

<p>Other sts: 
<code><a href="#topic+sts_autoregressive_state_space_model">sts_autoregressive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive">sts_autoregressive</a>()</code>,
<code><a href="#topic+sts_constrained_seasonal_state_space_model">sts_constrained_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression_state_space_model">sts_dynamic_linear_regression_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression">sts_dynamic_linear_regression</a>()</code>,
<code><a href="#topic+sts_linear_regression">sts_linear_regression</a>()</code>,
<code><a href="#topic+sts_local_level_state_space_model">sts_local_level_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_level">sts_local_level</a>()</code>,
<code><a href="#topic+sts_local_linear_trend_state_space_model">sts_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_linear_trend">sts_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_seasonal_state_space_model">sts_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_seasonal">sts_seasonal</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend_state_space_model">sts_semi_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend">sts_semi_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal_state_space_model">sts_smooth_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal">sts_smooth_seasonal</a>()</code>,
<code><a href="#topic+sts_sparse_linear_regression">sts_sparse_linear_regression</a>()</code>,
<code><a href="#topic+sts_sum">sts_sum</a>()</code>
</p>

<hr>
<h2 id='sts_autoregressive'>Formal representation of an autoregressive model.</h2><span id='topic+sts_autoregressive'></span>

<h3>Description</h3>

<p>An autoregressive (AR) model posits a latent <code>level</code> whose value at each step
is a noisy linear combination of previous steps:
</p>
<div class="sourceCode"><pre>level[t+1] = (sum(coefficients * levels[t:t-order:-1]) + Normal(0., level_scale))
</pre></div>


<h3>Usage</h3>

<pre><code class='language-R'>sts_autoregressive(
  observed_time_series = NULL,
  order,
  coefficients_prior = NULL,
  level_scale_prior = NULL,
  initial_state_prior = NULL,
  coefficient_constraining_bijector = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_autoregressive_+3A_observed_time_series">observed_time_series</code></td>
<td>
<p>optional <code>float</code> <code>tensor</code> of shape
<code style="white-space: pre;">&#8288;batch_shape + [T, 1]&#8288;</code> (omitting the trailing unit dimension is also
supported when <code>T &gt; 1</code>), specifying an observed time series.
Any priors not explicitly set will be given default values according to
the scale of the observed time series (or batch of time series). May
optionally be an instance of <code>sts_masked_time_series</code>, which includes
a mask <code>tensor</code> to specify timesteps with missing observations.
Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_autoregressive_+3A_order">order</code></td>
<td>
<p>scalar positive <code>integer</code> specifying the number of past
timesteps to regress on.</p>
</td></tr>
<tr><td><code id="sts_autoregressive_+3A_coefficients_prior">coefficients_prior</code></td>
<td>
<p>optional <code>Distribution</code> instance specifying a
prior on the <code>coefficients</code> parameter. If <code>NULL</code>, a default standard
normal (<code>tfd_multivariate_normal_diag(scale_diag = tf$ones(list(order)))</code>) prior
is used. Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_autoregressive_+3A_level_scale_prior">level_scale_prior</code></td>
<td>
<p>optional <code>Distribution</code> instance specifying a prior
on the <code>level_scale</code> parameter. If <code>NULL</code>, a heuristic default prior is
constructed based on the provided <code>observed_time_series</code>. Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_autoregressive_+3A_initial_state_prior">initial_state_prior</code></td>
<td>
<p>optional <code>Distribution</code> instance specifying a
prior on the initial state, corresponding to the values of the process
at a set of size <code>order</code> of imagined timesteps before the initial step.
If <code>NULL</code>, a heuristic default prior is constructed based on the
provided <code>observed_time_series</code>. Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_autoregressive_+3A_coefficient_constraining_bijector">coefficient_constraining_bijector</code></td>
<td>
<p>optional <code>Bijector</code> instance
representing a constraining mapping for the autoregressive coefficients.
For example, <code>tfb_tanh()</code> constrains the coefficients to lie in
<code style="white-space: pre;">&#8288;(-1, 1)&#8288;</code>, while <code>tfb_softplus()</code> constrains them to be positive, and
<code>tfb_identity()</code> implies no constraint. If <code>NULL</code>, the default behavior
constrains the coefficients to lie in <code style="white-space: pre;">&#8288;(-1, 1)&#8288;</code> using a <code>tanh</code> bijector.
Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_autoregressive_+3A_name">name</code></td>
<td>
<p>the name of this model component. Default value: 'Autoregressive'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The latent state is <code>levels[t:t-order:-1]</code>. We observe a noisy realization of
the current level: <code>f[t] = level[t] + Normal(0., observation_noise_scale)</code> at
each timestep.
</p>
<p>If <code style="white-space: pre;">&#8288;coefficients=[1.]&#8288;</code>, the AR process is a simple random walk, equivalent to
a <code>LocalLevel</code> model. However, a random walk's variance increases with time,
while many AR processes (in particular, any first-order process with
<code>abs(coefficient) &lt; 1</code>) are <em>stationary</em>, i.e., they maintain a constant
variance over time. This makes AR processes useful models of uncertainty.
</p>


<h3>Value</h3>

<p>an instance of <code>StructuralTimeSeries</code>.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+sts_fit_with_hmc">sts_fit_with_hmc()</a></code>, <code><a href="#topic+sts_forecast">sts_forecast()</a></code>, <code><a href="#topic+sts_decompose_by_component">sts_decompose_by_component()</a></code>.
</p>
<p>Other sts: 
<code><a href="#topic+sts_additive_state_space_model">sts_additive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive_state_space_model">sts_autoregressive_state_space_model</a>()</code>,
<code><a href="#topic+sts_constrained_seasonal_state_space_model">sts_constrained_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression_state_space_model">sts_dynamic_linear_regression_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression">sts_dynamic_linear_regression</a>()</code>,
<code><a href="#topic+sts_linear_regression">sts_linear_regression</a>()</code>,
<code><a href="#topic+sts_local_level_state_space_model">sts_local_level_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_level">sts_local_level</a>()</code>,
<code><a href="#topic+sts_local_linear_trend_state_space_model">sts_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_linear_trend">sts_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_seasonal_state_space_model">sts_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_seasonal">sts_seasonal</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend_state_space_model">sts_semi_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend">sts_semi_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal_state_space_model">sts_smooth_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal">sts_smooth_seasonal</a>()</code>,
<code><a href="#topic+sts_sparse_linear_regression">sts_sparse_linear_regression</a>()</code>,
<code><a href="#topic+sts_sum">sts_sum</a>()</code>
</p>

<hr>
<h2 id='sts_autoregressive_state_space_model'>State space model for an autoregressive process.</h2><span id='topic+sts_autoregressive_state_space_model'></span>

<h3>Description</h3>

<p>A state space model (SSM) posits a set of latent (unobserved) variables that
evolve over time with dynamics specified by a probabilistic transition model
<code>p(z[t+1] | z[t])</code>. At each timestep, we observe a value sampled from an
observation model conditioned on the current state, <code>p(x[t] | z[t])</code>. The
special case where both the transition and observation models are Gaussians
with mean specified as a linear function of the inputs, is known as a linear
Gaussian state space model and supports tractable exact probabilistic
calculations; see <code>tfd_linear_gaussian_state_space_model</code> for
details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sts_autoregressive_state_space_model(
  num_timesteps,
  coefficients,
  level_scale,
  initial_state_prior,
  observation_noise_scale = 0,
  initial_step = 0,
  validate_args = FALSE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_autoregressive_state_space_model_+3A_num_timesteps">num_timesteps</code></td>
<td>
<p>Scalar <code>integer</code> <code>tensor</code> number of timesteps to model
with this distribution.</p>
</td></tr>
<tr><td><code id="sts_autoregressive_state_space_model_+3A_coefficients">coefficients</code></td>
<td>
<p><code>float</code> <code>tensor</code> of shape <code>tf$concat(batch_shape, list(order))</code>
defining  the autoregressive coefficients. The coefficients are defined
backwards in time:
<code>coefficients[0] * level[t] + coefficients[1] * level[t-1] + ... + coefficients[order-1] * level[t-order+1]</code>.</p>
</td></tr>
<tr><td><code id="sts_autoregressive_state_space_model_+3A_level_scale">level_scale</code></td>
<td>
<p>Scalar (any additional dimensions are treated as batch
dimensions) <code>float</code> <code>tensor</code> indicating the standard deviation of the
transition noise at each step.</p>
</td></tr>
<tr><td><code id="sts_autoregressive_state_space_model_+3A_initial_state_prior">initial_state_prior</code></td>
<td>
<p>instance of <code>tfd_multivariate_normal</code> representing
the prior distribution on latent states.  Must have event shape <code>list(order)</code>.</p>
</td></tr>
<tr><td><code id="sts_autoregressive_state_space_model_+3A_observation_noise_scale">observation_noise_scale</code></td>
<td>
<p>Scalar (any additional dimensions are
treated as batch dimensions) <code>float</code> <code>tensor</code> indicating the standard
deviation of the observation noise. Default value: 0.</p>
</td></tr>
<tr><td><code id="sts_autoregressive_state_space_model_+3A_initial_step">initial_step</code></td>
<td>
<p>Optional scalar <code>int</code> <code>tensor</code> specifying the starting
timestep. Default value: 0.</p>
</td></tr>
<tr><td><code id="sts_autoregressive_state_space_model_+3A_validate_args">validate_args</code></td>
<td>
<p><code>logical</code>. Whether to validate input
with asserts. If <code>validate_args</code> is <code>FALSE</code>, and the inputs are
invalid, correct behavior is not guaranteed. Default value: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="sts_autoregressive_state_space_model_+3A_name">name</code></td>
<td>
<p>name prefixed to ops created by this class. Default value: &quot;AutoregressiveStateSpaceModel&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In an autoregressive process, the expected level at each timestep is a linear
function of previous levels, with added Gaussian noise:
</p>
<div class="sourceCode"><pre>level[t+1] = (sum(coefficients * levels[t:t-order:-1]) + Normal(0., level_scale))
</pre></div>
<p>The process is characterized by a vector <code>coefficients</code> whose size determines
the order of the process (how many previous values it looks at), and by
<code>level_scale</code>, the standard deviation of the noise added at each step.
This is formulated as a state space model by letting the latent state encode
the most recent values; see 'Mathematical Details' below.
</p>
<p>The parameters <code>level_scale</code> and <code>observation_noise_scale</code> are each (a batch
of) scalars, and <code>coefficients</code> is a (batch) vector of size <code>list(order)</code>. The
batch shape of this <code>Distribution</code> is the broadcast batch
shape of these parameters and of the <code>initial_state_prior</code>.
</p>
<p>Mathematical Details
</p>
<p>The autoregressive model implements a
<code>tfd_linear_gaussian_state_space_model</code> with <code>latent_size = order</code>
and <code>observation_size = 1</code>. The latent state vector encodes the recent history
of the process, with the current value in the topmost dimension. At each
timestep, the transition sums the previous values to produce the new expected
value, shifts all other values down by a dimension, and adds noise to the
current value. This is formally encoded by the transition model:
</p>
<div class="sourceCode"><pre>transition_matrix = [ coefs[0], coefs[1], ..., coefs[order]
                      1.,       0 ,       ..., 0.
                      0.,       1.,       ..., 0.
                      ...
                      0.,       0.,  ...,  1., 0.         ]
</pre></div>
<div class="sourceCode"><pre>transition_noise ~ N(loc=0., scale=diag([level_scale, 0., 0., ..., 0.]))
</pre></div>
<p>The observation model simply extracts the current (topmost) value, and
optionally adds independent noise at each step:
</p>
<div class="sourceCode"><pre>observation_matrix = [[1., 0., ..., 0.]]
observation_noise ~ N(loc=0, scale=observation_noise_scale)
</pre></div>
<p>Models with <code>observation_noise_scale = 0</code> are AR processes in the formal
sense. Setting <code>observation_noise_scale</code> to a nonzero value corresponds to a
latent AR process observed under an iid noise model.
</p>


<h3>Value</h3>

<p>an instance of <code>LinearGaussianStateSpaceModel</code>.
</p>


<h3>See Also</h3>

<p>Other sts: 
<code><a href="#topic+sts_additive_state_space_model">sts_additive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive">sts_autoregressive</a>()</code>,
<code><a href="#topic+sts_constrained_seasonal_state_space_model">sts_constrained_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression_state_space_model">sts_dynamic_linear_regression_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression">sts_dynamic_linear_regression</a>()</code>,
<code><a href="#topic+sts_linear_regression">sts_linear_regression</a>()</code>,
<code><a href="#topic+sts_local_level_state_space_model">sts_local_level_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_level">sts_local_level</a>()</code>,
<code><a href="#topic+sts_local_linear_trend_state_space_model">sts_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_linear_trend">sts_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_seasonal_state_space_model">sts_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_seasonal">sts_seasonal</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend_state_space_model">sts_semi_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend">sts_semi_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal_state_space_model">sts_smooth_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal">sts_smooth_seasonal</a>()</code>,
<code><a href="#topic+sts_sparse_linear_regression">sts_sparse_linear_regression</a>()</code>,
<code><a href="#topic+sts_sum">sts_sum</a>()</code>
</p>

<hr>
<h2 id='sts_build_factored_surrogate_posterior'>Build a variational posterior that factors over model parameters.</h2><span id='topic+sts_build_factored_surrogate_posterior'></span>

<h3>Description</h3>

<p>The surrogate posterior consists of independent Normal distributions for
each parameter with trainable <code>loc</code> and <code>scale</code>, transformed using the
parameter's <code>bijector</code> to the appropriate support space for that parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sts_build_factored_surrogate_posterior(
  model,
  batch_shape = list(),
  seed = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_build_factored_surrogate_posterior_+3A_model">model</code></td>
<td>
<p>An instance of <code>StructuralTimeSeries</code> representing a
time-series model. This represents a joint distribution over
time-series and their parameters with batch shape <code style="white-space: pre;">&#8288;[b1, ..., bN]&#8288;</code>.#'</p>
</td></tr>
<tr><td><code id="sts_build_factored_surrogate_posterior_+3A_batch_shape">batch_shape</code></td>
<td>
<p>Batch shape (<code>list</code>, or <code>integer</code>) of initial
states to optimize in parallel.
Default value: <code>list()</code>. (i.e., just run a single optimization).</p>
</td></tr>
<tr><td><code id="sts_build_factored_surrogate_posterior_+3A_seed">seed</code></td>
<td>
<p>integer to seed the random number generator.</p>
</td></tr>
<tr><td><code id="sts_build_factored_surrogate_posterior_+3A_name">name</code></td>
<td>
<p>string prefixed to ops created by this function.
Default value: <code>NULL</code> (i.e., 'build_factored_surrogate_posterior').</p>
</td></tr>
</table>


<h3>Value</h3>

<p>variational_posterior <code>tfd_joint_distribution_named</code> defining a trainable
surrogate posterior over model parameters. Samples from this
distribution are named lists with  <code>character</code> parameter names as keys.
</p>


<h3>See Also</h3>

<p>Other sts-functions: 
<code><a href="#topic+sts_build_factored_variational_loss">sts_build_factored_variational_loss</a>()</code>,
<code><a href="#topic+sts_decompose_by_component">sts_decompose_by_component</a>()</code>,
<code><a href="#topic+sts_decompose_forecast_by_component">sts_decompose_forecast_by_component</a>()</code>,
<code><a href="#topic+sts_fit_with_hmc">sts_fit_with_hmc</a>()</code>,
<code><a href="#topic+sts_forecast">sts_forecast</a>()</code>,
<code><a href="#topic+sts_one_step_predictive">sts_one_step_predictive</a>()</code>,
<code><a href="#topic+sts_sample_uniform_initial_state">sts_sample_uniform_initial_state</a>()</code>
</p>

<hr>
<h2 id='sts_build_factored_variational_loss'>Build a loss function for variational inference in STS models.</h2><span id='topic+sts_build_factored_variational_loss'></span>

<h3>Description</h3>

<p>Variational inference searches for the distribution within some family of
approximate posteriors that minimizes a divergence between the approximate
posterior <code>q(z)</code> and true posterior <code>p(z|observed_time_series)</code>. By converting
inference to optimization, it's generally much faster than sampling-based
inference algorithms such as HMC. The tradeoff is that the approximating
family rarely contains the true posterior, so it may miss important aspects of
posterior structure (in particular, dependence between variables) and should
not be blindly trusted. Results may vary; it's generally wise to compare to
HMC to evaluate whether inference quality is sufficient for your task at hand.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sts_build_factored_variational_loss(
  observed_time_series,
  model,
  init_batch_shape = list(),
  seed = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_build_factored_variational_loss_+3A_observed_time_series">observed_time_series</code></td>
<td>
<p><code>float</code> <code>tensor</code> of shape
<code style="white-space: pre;">&#8288;concat([sample_shape, model.batch_shape, [num_timesteps, 1]])&#8288;</code> where
<code>sample_shape</code> corresponds to i.i.d. observations, and the trailing <code style="white-space: pre;">&#8288;[1]&#8288;</code>
dimension may (optionally) be omitted if <code>num_timesteps &gt; 1</code>. May
optionally be an instance of <code>sts_masked_time_series</code>, which includes
a mask <code>tensor</code> to specify timesteps with missing observations.</p>
</td></tr>
<tr><td><code id="sts_build_factored_variational_loss_+3A_model">model</code></td>
<td>
<p>An instance of <code>StructuralTimeSeries</code> representing a
time-series model. This represents a joint distribution over
time-series and their parameters with batch shape <code style="white-space: pre;">&#8288;[b1, ..., bN]&#8288;</code>.</p>
</td></tr>
<tr><td><code id="sts_build_factored_variational_loss_+3A_init_batch_shape">init_batch_shape</code></td>
<td>
<p>Batch shape (<code>list</code>) of initial
states to optimize in parallel. Default value: <code>list()</code>. (i.e., just run a single optimization).</p>
</td></tr>
<tr><td><code id="sts_build_factored_variational_loss_+3A_seed">seed</code></td>
<td>
<p>integer to seed the random number generator.</p>
</td></tr>
<tr><td><code id="sts_build_factored_variational_loss_+3A_name">name</code></td>
<td>
<p>name prefixed to ops created by this function. Default value: <code>NULL</code>
(i.e., 'build_factored_variational_loss').</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method constructs a loss function for variational inference using the
Kullback-Liebler divergence <code>KL[q(z) || p(z|observed_time_series)]</code>, with an
approximating family given by independent Normal distributions transformed to
the appropriate parameter space for each parameter. Minimizing this loss (the
negative ELBO) maximizes a lower bound on the log model evidence
<code style="white-space: pre;">&#8288;-log p(observed_time_series)&#8288;</code>. This is equivalent to the 'mean-field' method
implemented in Kucukelbir et al. (2017) and is a standard approach.
The resulting posterior approximations are unimodal; they will tend to underestimate posterior
uncertainty when the true posterior contains multiple modes
(the <code>KL[q||p]</code> divergence encourages choosing a single mode) or dependence between variables.
</p>


<h3>Value</h3>

<p>list of:
</p>

<ul>
<li><p> variational_loss: <code>float</code> <code>Tensor</code> of shape
<code style="white-space: pre;">&#8288;tf$concat([init_batch_shape, model$batch_shape])&#8288;</code>, encoding a stochastic
estimate of an upper bound on the negative model evidence <code style="white-space: pre;">&#8288;-log p(y)&#8288;</code>.
Minimizing this loss performs variational inference; the gap between the
variational bound and the true (generally unknown) model evidence
corresponds to the divergence <code>KL[q||p]</code> between the approximate and true
posterior.
</p>
</li>
<li><p> variational_distributions: a named list giving
the approximate posterior for each model parameter. The keys are
<code>character</code> parameter names in order, corresponding to
<code style="white-space: pre;">&#8288;[param.name for param in model.parameters]&#8288;</code>. The values are
<code>tfd$Distribution</code> instances with batch shape
<code style="white-space: pre;">&#8288;tf$concat([init_batch_shape, model$batch_shape])&#8288;</code>; these will typically be
of the form <code>tfd$TransformedDistribution(tfd.Normal(...), bijector=param.bijector)</code>.
</p>
</li></ul>



<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1603.00788">Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David M. Blei. Automatic Differentiation Variational Inference. In <em>Journal of Machine Learning Research</em>, 2017.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other sts-functions: 
<code><a href="#topic+sts_build_factored_surrogate_posterior">sts_build_factored_surrogate_posterior</a>()</code>,
<code><a href="#topic+sts_decompose_by_component">sts_decompose_by_component</a>()</code>,
<code><a href="#topic+sts_decompose_forecast_by_component">sts_decompose_forecast_by_component</a>()</code>,
<code><a href="#topic+sts_fit_with_hmc">sts_fit_with_hmc</a>()</code>,
<code><a href="#topic+sts_forecast">sts_forecast</a>()</code>,
<code><a href="#topic+sts_one_step_predictive">sts_one_step_predictive</a>()</code>,
<code><a href="#topic+sts_sample_uniform_initial_state">sts_sample_uniform_initial_state</a>()</code>
</p>

<hr>
<h2 id='sts_constrained_seasonal_state_space_model'>Seasonal state space model with effects constrained to sum to zero.</h2><span id='topic+sts_constrained_seasonal_state_space_model'></span>

<h3>Description</h3>

<p>Seasonal state space model with effects constrained to sum to zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sts_constrained_seasonal_state_space_model(
  num_timesteps,
  num_seasons,
  drift_scale,
  initial_state_prior,
  observation_noise_scale = 1e-04,
  num_steps_per_season = 1,
  initial_step = 0,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_constrained_seasonal_state_space_model_+3A_num_timesteps">num_timesteps</code></td>
<td>
<p>Scalar <code>integer</code> <code>tensor</code> number of timesteps to model
with this distribution.</p>
</td></tr>
<tr><td><code id="sts_constrained_seasonal_state_space_model_+3A_num_seasons">num_seasons</code></td>
<td>
<p>Scalar <code>integer</code> number of seasons.</p>
</td></tr>
<tr><td><code id="sts_constrained_seasonal_state_space_model_+3A_drift_scale">drift_scale</code></td>
<td>
<p>Scalar (any additional dimensions are treated as batch
dimensions) <code>float</code> <code>tensor</code> indicating the standard deviation of the
change in effect between consecutive occurrences of a given season.
This is assumed to be the same for all seasons.</p>
</td></tr>
<tr><td><code id="sts_constrained_seasonal_state_space_model_+3A_initial_state_prior">initial_state_prior</code></td>
<td>
<p>instance of <code>tfd_multivariate_normal</code>
representing the prior distribution on latent states; must
have event shape <code style="white-space: pre;">&#8288;[num_seasons]&#8288;</code>.</p>
</td></tr>
<tr><td><code id="sts_constrained_seasonal_state_space_model_+3A_observation_noise_scale">observation_noise_scale</code></td>
<td>
<p>Scalar (any additional dimensions are
treated as batch dimensions) <code>float</code> <code>tensor</code> indicating the standard
deviation of the observation noise.</p>
</td></tr>
<tr><td><code id="sts_constrained_seasonal_state_space_model_+3A_num_steps_per_season">num_steps_per_season</code></td>
<td>
<p><code>integer</code> number of steps in each
season. This may be either a scalar (shape <code style="white-space: pre;">&#8288;[]&#8288;</code>), in which case all
seasons have the same length, or an array of shape <code style="white-space: pre;">&#8288;[num_seasons]&#8288;</code>,
in which seasons have different length, but remain constant around
different cycles, or an array of shape <code style="white-space: pre;">&#8288;[num_cycles, num_seasons]&#8288;</code>,
in which num_steps_per_season for each season also varies in different
cycle (e.g., a 4 years cycle with leap day). Default value: 1.</p>
</td></tr>
<tr><td><code id="sts_constrained_seasonal_state_space_model_+3A_initial_step">initial_step</code></td>
<td>
<p>Optional scalar <code>integer</code> <code>tensor</code> specifying the starting
timestep. Default value: 0.</p>
</td></tr>
<tr><td><code id="sts_constrained_seasonal_state_space_model_+3A_validate_args">validate_args</code></td>
<td>
<p><code>logical</code>. Whether to validate input
with asserts. If <code>validate_args</code> is <code>FALSE</code>, and the inputs are
invalid, correct behavior is not guaranteed. Default value: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="sts_constrained_seasonal_state_space_model_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p><code>logical</code>. If <code>FALSE</code>, raise an
exception if a statistic (e.g. mean/mode/etc...) is undefined for any
batch member. If <code>TRUE</code>, batch members with valid parameters leading to
undefined statistics will return NaN for this statistic. Default value: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="sts_constrained_seasonal_state_space_model_+3A_name">name</code></td>
<td>
<p>string prefixed to ops created by this class.
Default value: &quot;SeasonalStateSpaceModel&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an instance of <code>LinearGaussianStateSpaceModel</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sts_seasonal_state_space_model">sts_seasonal_state_space_model()</a></code>.
</p>
<p>Mathematical details
</p>
<p>The constrained model implements a reparameterization of the
naive <code>SeasonalStateSpaceModel</code>. Instead of directly representing the
seasonal effects in the latent space, the latent space of the constrained
model represents the difference between each effect and the mean effect.
The following discussion assumes familiarity with the mathematical details
of <code>SeasonalStateSpaceModel</code>.
</p>
<p><em>Reparameterization and constraints</em>: let the seasonal effects at a given
timestep be <code style="white-space: pre;">&#8288;E = [e_1, ..., e_N]&#8288;</code>. The difference between each effect <code>e_i</code>
and the mean effect is <code>z_i = e_i - sum_i(e_i)/N</code>. By itself, this
transformation is not invertible because recovering the absolute effects
requires that we know the mean as well. To fix this, we'll define
<code>z_N = sum_i(e_i)/N</code> as the mean effect. It's easy to see that this is
invertible: given the mean effect and the differences of the first <code>N - 1</code>
effects from the mean, it's easy to solve for all <code>N</code> effects. Formally,
we've defined the invertible linear reparameterization <code style="white-space: pre;">&#8288;Z = R E&#8288;</code>, where
</p>
<div class="sourceCode"><pre>R = [1 - 1/N, -1/N,    ..., -1/N
     -1/N,    1 - 1/N, ..., -1/N,
     ...
     1/N,     1/N,     ...,  1/N]
</pre></div>
<p>represents the change of basis from 'effect coordinates' E to
'residual coordinates' Z. The <code>Z</code>s form the latent space of the
<code>ConstrainedSeasonalStateSpaceModel</code>.
To constrain the mean effect <code>z_N</code> to zero, we fix the prior to zero,
<code>p(z_N) ~ N(0., 0)</code>, and after the transition at each timestep we project
<code>z_N</code> back to zero. Note that this projection is linear: to set the Nth
dimension to zero, we simply multiply by the identity matrix with a missing
element in the bottom right, i.e., <code style="white-space: pre;">&#8288;Z_constrained = P Z&#8288;</code>,
where <code style="white-space: pre;">&#8288;P = eye(N) - scatter((N-1, N-1), 1)&#8288;</code>.
</p>
<p><em>Model</em>: concretely, suppose a naive seasonal effect model has initial state
prior <code>N(m, S)</code>, transition matrix <code>F</code> and noise covariance
<code>Q</code>, and observation matrix <code>H</code>. Then the corresponding constrained seasonal
effect model has initial state prior <code style="white-space: pre;">&#8288;N(P R m, P R S R' P')&#8288;</code>,
transition matrix <code style="white-space: pre;">&#8288;P R F R^-1&#8288;</code> and noise covariance <code style="white-space: pre;">&#8288;F R Q R' F'&#8288;</code>, and
observation matrix <code style="white-space: pre;">&#8288;H R^-1&#8288;</code>, where the change-of-basis matrix <code>R</code> and
constraint projection matrix <code>P</code> are as defined above. This follows
directly from applying the reparameterization <code style="white-space: pre;">&#8288;Z = R E&#8288;</code>, and then enforcing
the zero-sum constraint on the prior and transition noise covariances.
In practice, because the sum of effects <code>z_N</code> is constrained to be zero, it
will never contribute a term to any linear operation on the latent space,
so we can drop that dimension from the model entirely.
<code>ConstrainedSeasonalStateSpaceModel</code> does this, so that it implements the
<code>N - 1</code> dimension latent space <code style="white-space: pre;">&#8288;z_1, ..., z_[N-1]&#8288;</code>.
Note that since we constrained the mean effect to be zero, the latent
<code>z_i</code>'s now recover their interpretation as the <em>actual</em> effects,
<code>z_i = e_i</code> for <code style="white-space: pre;">&#8288;i = &#8288;</code>1, ..., N - 1<code style="white-space: pre;">&#8288;, even though they were originally defined as residuals. The &#8288;</code>N<code style="white-space: pre;">&#8288;th effect is represented only implicitly, as the nonzero mean of the first &#8288;</code>N - 1<code style="white-space: pre;">&#8288;effects. Although the computational represention is not symmetric across all&#8288;</code>N<code style="white-space: pre;">&#8288;effects, we derived the&#8288;</code>ConstrainedSeasonalStateSpaceModel<code style="white-space: pre;">&#8288;by starting with a symmetric representation and imposing only a symmetric constraint (the zero-sum constraint), so the probability model remains symmetric over all&#8288;</code>N'
seasonal effects.
</p>
<p>Other sts: 
<code><a href="#topic+sts_additive_state_space_model">sts_additive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive_state_space_model">sts_autoregressive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive">sts_autoregressive</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression_state_space_model">sts_dynamic_linear_regression_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression">sts_dynamic_linear_regression</a>()</code>,
<code><a href="#topic+sts_linear_regression">sts_linear_regression</a>()</code>,
<code><a href="#topic+sts_local_level_state_space_model">sts_local_level_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_level">sts_local_level</a>()</code>,
<code><a href="#topic+sts_local_linear_trend_state_space_model">sts_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_linear_trend">sts_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_seasonal_state_space_model">sts_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_seasonal">sts_seasonal</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend_state_space_model">sts_semi_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend">sts_semi_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal_state_space_model">sts_smooth_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal">sts_smooth_seasonal</a>()</code>,
<code><a href="#topic+sts_sparse_linear_regression">sts_sparse_linear_regression</a>()</code>,
<code><a href="#topic+sts_sum">sts_sum</a>()</code>
</p>

<hr>
<h2 id='sts_decompose_by_component'>Decompose an observed time series into contributions from each component.</h2><span id='topic+sts_decompose_by_component'></span>

<h3>Description</h3>

<p>This method decomposes a time series according to the posterior represention
of a structural time series model. In particular, it:
</p>

<ul>
<li><p> Computes the posterior marginal mean and covariances over the additive
model's latent space.
</p>
</li>
<li><p> Decomposes the latent posterior into the marginal blocks for each
model component.
</p>
</li>
<li><p> Maps the per-component latent posteriors back through each component's
observation model, to generate the time series modeled by that component.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>sts_decompose_by_component(observed_time_series, model, parameter_samples)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_decompose_by_component_+3A_observed_time_series">observed_time_series</code></td>
<td>
<p><code>float</code> <code>tensor</code> of shape
<code style="white-space: pre;">&#8288;concat([sample_shape, model.batch_shape, [num_timesteps, 1]])&#8288;</code> where
<code>sample_shape</code> corresponds to i.i.d. observations, and the trailing <code style="white-space: pre;">&#8288;[1]&#8288;</code>
dimension may (optionally) be omitted if <code>num_timesteps &gt; 1</code>. May
optionally be an instance of <code>sts_masked_time_series</code>, which includes
a mask <code>tensor</code> to specify timesteps with missing observations.</p>
</td></tr>
<tr><td><code id="sts_decompose_by_component_+3A_model">model</code></td>
<td>
<p>An instance of <code>sts_sum</code> representing a structural time series model.</p>
</td></tr>
<tr><td><code id="sts_decompose_by_component_+3A_parameter_samples">parameter_samples</code></td>
<td>
<p><code>list</code> of <code>tensors</code> representing posterior samples
of model parameters, with shapes
<code style="white-space: pre;">&#8288;list(tf$concat(list(list(num_posterior_draws), param&lt;1&gt;$prior$batch_shape, param&lt;1&gt;$prior$event_shape), list(list(num_posterior_draws), param&lt;2&gt;$prior$batch_shape, param&lt;2&gt;$prior$event_shape), ... ) )&#8288;</code>
for all model parameters.
This may optionally also be a named list mapping parameter names to <code>tensor</code> values.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>component_dists A named list mapping
component StructuralTimeSeries instances (elements of <code>model$components</code>)
to <code>Distribution</code> instances representing the posterior marginal
distributions on the process modeled by each component. Each distribution
has batch shape matching that of <code>posterior_means</code>/<code>posterior_covs</code>, and
event shape of <code>list(num_timesteps)</code>.
</p>


<h3>See Also</h3>

<p>Other sts-functions: 
<code><a href="#topic+sts_build_factored_surrogate_posterior">sts_build_factored_surrogate_posterior</a>()</code>,
<code><a href="#topic+sts_build_factored_variational_loss">sts_build_factored_variational_loss</a>()</code>,
<code><a href="#topic+sts_decompose_forecast_by_component">sts_decompose_forecast_by_component</a>()</code>,
<code><a href="#topic+sts_fit_with_hmc">sts_fit_with_hmc</a>()</code>,
<code><a href="#topic+sts_forecast">sts_forecast</a>()</code>,
<code><a href="#topic+sts_one_step_predictive">sts_one_step_predictive</a>()</code>,
<code><a href="#topic+sts_sample_uniform_initial_state">sts_sample_uniform_initial_state</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
observed_time_series &lt;- array(rnorm(2 * 1 * 12), dim = c(2, 1, 12))
day_of_week &lt;- observed_time_series %&gt;% sts_seasonal(num_seasons = 7, name = "seasonal")
local_linear_trend &lt;- observed_time_series %&gt;% sts_local_linear_trend(name = "local_linear")
model &lt;- observed_time_series %&gt;%
  sts_sum(components = list(day_of_week, local_linear_trend))
states_and_results &lt;- observed_time_series %&gt;%
  sts_fit_with_hmc(
    model,
    num_results = 10,
    num_warmup_steps = 5,
    num_variational_steps = 15
    )
samples &lt;- states_and_results[[1]]

component_dists &lt;- observed_time_series %&gt;%
 sts_decompose_by_component(model = model, parameter_samples = samples)

</code></pre>

<hr>
<h2 id='sts_decompose_forecast_by_component'>Decompose a forecast distribution into contributions from each component.</h2><span id='topic+sts_decompose_forecast_by_component'></span>

<h3>Description</h3>

<p>Decompose a forecast distribution into contributions from each component.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sts_decompose_forecast_by_component(model, forecast_dist, parameter_samples)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_decompose_forecast_by_component_+3A_model">model</code></td>
<td>
<p>An instance of <code>sts_sum</code> representing a structural time series model.</p>
</td></tr>
<tr><td><code id="sts_decompose_forecast_by_component_+3A_forecast_dist">forecast_dist</code></td>
<td>
<p>A <code>Distribution</code> instance returned by <code>sts_forecast()</code>.
(specifically, must be a <code>tfd.MixtureSameFamily</code> over a
<code>tfd_linear_gaussian_state_space_model</code> parameterized by posterior samples).</p>
</td></tr>
<tr><td><code id="sts_decompose_forecast_by_component_+3A_parameter_samples">parameter_samples</code></td>
<td>
<p><code>list</code> of <code>tensors</code> representing posterior samples
of model parameters, with shapes
<code style="white-space: pre;">&#8288;list(tf$concat(list(list(num_posterior_draws), param&lt;1&gt;$prior$batch_shape, param&lt;1&gt;$prior$event_shape), list(list(num_posterior_draws), param&lt;2&gt;$prior$batch_shape, param&lt;2&gt;$prior$event_shape), ... ) )&#8288;</code>
for all model parameters.
This may optionally also be a named list mapping parameter names to <code>tensor</code> values.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>component_dists A named list mapping
component StructuralTimeSeries instances (elements of <code>model$components</code>)
to <code>Distribution</code> instances representing the marginal forecast for each component.
Each distribution has batch shape matching <code>forecast_dist</code> (specifically,
the event shape is <code style="white-space: pre;">&#8288;[num_steps_forecast]&#8288;</code>).
</p>


<h3>See Also</h3>

<p>Other sts-functions: 
<code><a href="#topic+sts_build_factored_surrogate_posterior">sts_build_factored_surrogate_posterior</a>()</code>,
<code><a href="#topic+sts_build_factored_variational_loss">sts_build_factored_variational_loss</a>()</code>,
<code><a href="#topic+sts_decompose_by_component">sts_decompose_by_component</a>()</code>,
<code><a href="#topic+sts_fit_with_hmc">sts_fit_with_hmc</a>()</code>,
<code><a href="#topic+sts_forecast">sts_forecast</a>()</code>,
<code><a href="#topic+sts_one_step_predictive">sts_one_step_predictive</a>()</code>,
<code><a href="#topic+sts_sample_uniform_initial_state">sts_sample_uniform_initial_state</a>()</code>
</p>

<hr>
<h2 id='sts_dynamic_linear_regression'>Formal representation of a dynamic linear regression model.</h2><span id='topic+sts_dynamic_linear_regression'></span>

<h3>Description</h3>

<p>The dynamic linear regression model is a special case of a linear Gaussian SSM
and a generalization of typical (static) linear regression. The model
represents regression <code>weights</code> with a latent state which evolves via a
Gaussian random walk:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sts_dynamic_linear_regression(
  observed_time_series = NULL,
  design_matrix,
  drift_scale_prior = NULL,
  initial_weights_prior = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_dynamic_linear_regression_+3A_observed_time_series">observed_time_series</code></td>
<td>
<p>optional <code>float</code> <code>tensor</code> of shape
<code style="white-space: pre;">&#8288;batch_shape + [T, 1]&#8288;</code> (omitting the trailing unit dimension is also
supported when <code>T &gt; 1</code>), specifying an observed time series.
Any priors not explicitly set will be given default values according to
the scale of the observed time series (or batch of time series). May
optionally be an instance of <code>sts_masked_time_series</code>, which includes
a mask <code>tensor</code> to specify timesteps with missing observations.
Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_dynamic_linear_regression_+3A_design_matrix">design_matrix</code></td>
<td>
<p>float <code>tensor</code> of shape <code>tf$concat(list(batch_shape, list(num_timesteps, num_features)))</code>.
This may also optionally be an instance of <code>tf$linalg$LinearOperator</code>.</p>
</td></tr>
<tr><td><code id="sts_dynamic_linear_regression_+3A_drift_scale_prior">drift_scale_prior</code></td>
<td>
<p>instance of <code>Distribution</code> specifying a prior on
the <code>drift_scale</code> parameter. If <code>NULL</code>, a heuristic default prior is
constructed based on the provided <code>observed_time_series</code>. Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_dynamic_linear_regression_+3A_initial_weights_prior">initial_weights_prior</code></td>
<td>
<p>instance of <code>tfd_multivariate_normal</code> representing
the prior distribution on the latent states (the regression weights).
Must have event shape <code>list(num_features)</code>. If <code>NULL</code>, a weakly-informative
Normal(0, 10) prior is used. Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_dynamic_linear_regression_+3A_name">name</code></td>
<td>
<p>the name of this component. Default value: 'DynamicLinearRegression'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code> weights[t] ~ Normal(weights[t-1], drift_scale)</code>
</p>
<p>The latent state has dimension <code>num_features</code>, while the parameters
<code>drift_scale</code> and <code>observation_noise_scale</code> are each (a batch of) scalars. The
batch shape of this distribution is the broadcast batch shape of these
parameters, the <code>initial_state_prior</code>, and the <code>design_matrix</code>.
<code>num_features</code> is determined from the last dimension of <code>design_matrix</code> (equivalent to the
number of columns in the design matrix in linear regression).
</p>


<h3>Value</h3>

<p>an instance of <code>StructuralTimeSeries</code>.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+sts_fit_with_hmc">sts_fit_with_hmc()</a></code>, <code><a href="#topic+sts_forecast">sts_forecast()</a></code>, <code><a href="#topic+sts_decompose_by_component">sts_decompose_by_component()</a></code>.
</p>
<p>Other sts: 
<code><a href="#topic+sts_additive_state_space_model">sts_additive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive_state_space_model">sts_autoregressive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive">sts_autoregressive</a>()</code>,
<code><a href="#topic+sts_constrained_seasonal_state_space_model">sts_constrained_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression_state_space_model">sts_dynamic_linear_regression_state_space_model</a>()</code>,
<code><a href="#topic+sts_linear_regression">sts_linear_regression</a>()</code>,
<code><a href="#topic+sts_local_level_state_space_model">sts_local_level_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_level">sts_local_level</a>()</code>,
<code><a href="#topic+sts_local_linear_trend_state_space_model">sts_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_linear_trend">sts_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_seasonal_state_space_model">sts_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_seasonal">sts_seasonal</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend_state_space_model">sts_semi_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend">sts_semi_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal_state_space_model">sts_smooth_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal">sts_smooth_seasonal</a>()</code>,
<code><a href="#topic+sts_sparse_linear_regression">sts_sparse_linear_regression</a>()</code>,
<code><a href="#topic+sts_sum">sts_sum</a>()</code>
</p>

<hr>
<h2 id='sts_dynamic_linear_regression_state_space_model'>State space model for a dynamic linear regression from provided covariates.</h2><span id='topic+sts_dynamic_linear_regression_state_space_model'></span>

<h3>Description</h3>

<p>A state space model (SSM) posits a set of latent (unobserved) variables that
evolve over time with dynamics specified by a probabilistic transition model
<code>p(z[t+1] | z[t])</code>. At each timestep, we observe a value sampled from an
observation model conditioned on the current state, <code>p(x[t] | z[t])</code>. The
special case where both the transition and observation models are Gaussians
with mean specified as a linear function of the inputs, is known as a linear
Gaussian state space model and supports tractable exact probabilistic
calculations; see <code>tfd_linear_gaussian_state_space_model</code> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sts_dynamic_linear_regression_state_space_model(
  num_timesteps,
  design_matrix,
  drift_scale,
  initial_state_prior,
  observation_noise_scale = 0,
  initial_step = 0,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_dynamic_linear_regression_state_space_model_+3A_num_timesteps">num_timesteps</code></td>
<td>
<p>Scalar <code>integer</code> <code>tensor</code>, number of timesteps to model
with this distribution.</p>
</td></tr>
<tr><td><code id="sts_dynamic_linear_regression_state_space_model_+3A_design_matrix">design_matrix</code></td>
<td>
<p>float <code>tensor</code> of shape <code>tf$concat(list(batch_shape, list(num_timesteps, num_features)))</code>.
This may also optionally be an instance of <code>tf$linalg$LinearOperator</code>.</p>
</td></tr>
<tr><td><code id="sts_dynamic_linear_regression_state_space_model_+3A_drift_scale">drift_scale</code></td>
<td>
<p>Scalar (any additional dimensions are treated as batch
dimensions) <code>float</code> <code>tensor</code> indicating the standard deviation of the
latent state transitions.</p>
</td></tr>
<tr><td><code id="sts_dynamic_linear_regression_state_space_model_+3A_initial_state_prior">initial_state_prior</code></td>
<td>
<p>instance of <code>tfd_multivariate_normal</code> representing
the prior distribution on latent states.  Must have
event shape <code>list(num_features)</code>.</p>
</td></tr>
<tr><td><code id="sts_dynamic_linear_regression_state_space_model_+3A_observation_noise_scale">observation_noise_scale</code></td>
<td>
<p>Scalar (any additional dimensions are
treated as batch dimensions) <code>float</code> <code>tensor</code> indicating the standard
deviation of the observation noise. Default value: <code>0</code>.</p>
</td></tr>
<tr><td><code id="sts_dynamic_linear_regression_state_space_model_+3A_initial_step">initial_step</code></td>
<td>
<p>scalar <code>integer</code> <code>tensor</code> specifying the starting timestep.
Default value: <code>0</code>.</p>
</td></tr>
<tr><td><code id="sts_dynamic_linear_regression_state_space_model_+3A_validate_args">validate_args</code></td>
<td>
<p><code>logical</code>. Whether to validate input
with asserts. If <code>validate_args</code> is <code>FALSE</code>, and the inputs are
invalid, correct behavior is not guaranteed. Default value: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="sts_dynamic_linear_regression_state_space_model_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p><code>logical</code>. If <code>FALSE</code>, raise an
exception if a statistic (e.g. mean/mode/etc...) is undefined for any
batch member. If <code>TRUE</code>, batch members with valid parameters leading to
undefined statistics will return NaN for this statistic. Default value: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="sts_dynamic_linear_regression_state_space_model_+3A_name">name</code></td>
<td>
<p>name prefixed to ops created by this class. Default value: 'DynamicLinearRegressionStateSpaceModel'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The dynamic linear regression model is a special case of a linear Gaussian SSM
and a generalization of typical (static) linear regression. The model
represents regression <code>weights</code> with a latent state which evolves via a
Gaussian random walk:
<code>weights[t] ~ Normal(weights[t-1], drift_scale)</code>
</p>
<p>The latent state (the weights) has dimension <code>num_features</code>, while the
parameters <code>drift_scale</code> and <code>observation_noise_scale</code> are each (a batch of)
scalars. The batch shape of this <code>Distribution</code> is the broadcast batch shape
of these parameters, the <code>initial_state_prior</code>, and the
<code>design_matrix</code>. <code>num_features</code> is determined from the last dimension of
<code>design_matrix</code> (equivalent to the number of columns in the design matrix in
linear regression).
</p>
<p>Mathematical Details
</p>
<p>The dynamic linear regression model implements a
<code>tfd_linear_gaussian_state_space_model</code> with <code>latent_size = num_features</code> and
<code>observation_size = 1</code> following the transition model:
</p>
<div class="sourceCode"><pre>transition_matrix = eye(num_features)
transition_noise ~ Normal(0, diag([drift_scale]))
</pre></div>
<p>which implements the evolution of <code>weights</code> described above. The observation
model is:
</p>
<div class="sourceCode"><pre>observation_matrix[t] = design_matrix[t]
observation_noise ~ Normal(0, observation_noise_scale)
</pre></div>


<h3>Value</h3>

<p>an instance of <code>LinearGaussianStateSpaceModel</code>.
</p>


<h3>See Also</h3>

<p>Other sts: 
<code><a href="#topic+sts_additive_state_space_model">sts_additive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive_state_space_model">sts_autoregressive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive">sts_autoregressive</a>()</code>,
<code><a href="#topic+sts_constrained_seasonal_state_space_model">sts_constrained_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression">sts_dynamic_linear_regression</a>()</code>,
<code><a href="#topic+sts_linear_regression">sts_linear_regression</a>()</code>,
<code><a href="#topic+sts_local_level_state_space_model">sts_local_level_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_level">sts_local_level</a>()</code>,
<code><a href="#topic+sts_local_linear_trend_state_space_model">sts_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_linear_trend">sts_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_seasonal_state_space_model">sts_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_seasonal">sts_seasonal</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend_state_space_model">sts_semi_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend">sts_semi_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal_state_space_model">sts_smooth_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal">sts_smooth_seasonal</a>()</code>,
<code><a href="#topic+sts_sparse_linear_regression">sts_sparse_linear_regression</a>()</code>,
<code><a href="#topic+sts_sum">sts_sum</a>()</code>
</p>

<hr>
<h2 id='sts_fit_with_hmc'>Draw posterior samples using Hamiltonian Monte Carlo (HMC)</h2><span id='topic+sts_fit_with_hmc'></span>

<h3>Description</h3>

<p>Markov chain Monte Carlo (MCMC) methods are considered the gold standard of
Bayesian inference; under suitable conditions and in the limit of infinitely
many draws they generate samples from the true posterior distribution. HMC (Neal, 2011)
uses gradients of the model's log-density function to propose samples,
allowing it to exploit posterior geometry. However, it is computationally more
expensive than variational inference and relatively sensitive to tuning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sts_fit_with_hmc(
  observed_time_series,
  model,
  num_results = 100,
  num_warmup_steps = 50,
  num_leapfrog_steps = 15,
  initial_state = NULL,
  initial_step_size = NULL,
  chain_batch_shape = list(),
  num_variational_steps = 150,
  variational_optimizer = NULL,
  variational_sample_size = 5,
  seed = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_fit_with_hmc_+3A_observed_time_series">observed_time_series</code></td>
<td>
<p><code>float</code> <code>tensor</code> of shape
<code style="white-space: pre;">&#8288;concat([sample_shape, model.batch_shape, [num_timesteps, 1]])&#8288;</code> where
<code>sample_shape</code> corresponds to i.i.d. observations, and the trailing <code style="white-space: pre;">&#8288;[1]&#8288;</code>
dimension may (optionally) be omitted if <code>num_timesteps &gt; 1</code>. May
optionally be an instance of <code>sts_masked_time_series</code>, which includes
a mask <code>tensor</code> to specify timesteps with missing observations.</p>
</td></tr>
<tr><td><code id="sts_fit_with_hmc_+3A_model">model</code></td>
<td>
<p>An instance of <code>StructuralTimeSeries</code> representing a
time-series model. This represents a joint distribution over
time-series and their parameters with batch shape <code style="white-space: pre;">&#8288;[b1, ..., bN]&#8288;</code>.</p>
</td></tr>
<tr><td><code id="sts_fit_with_hmc_+3A_num_results">num_results</code></td>
<td>
<p>Integer number of Markov chain draws. Default value: <code>100</code>.</p>
</td></tr>
<tr><td><code id="sts_fit_with_hmc_+3A_num_warmup_steps">num_warmup_steps</code></td>
<td>
<p>Integer number of steps to take before starting to
collect results. The warmup steps are also used to adapt the step size
towards a target acceptance rate of 0.75. Default value: <code>50</code>.</p>
</td></tr>
<tr><td><code id="sts_fit_with_hmc_+3A_num_leapfrog_steps">num_leapfrog_steps</code></td>
<td>
<p>Integer number of steps to run the leapfrog integrator
for. Total progress per HMC step is roughly proportional to <code>step_size * num_leapfrog_steps</code>.
Default value: <code>15</code>.</p>
</td></tr>
<tr><td><code id="sts_fit_with_hmc_+3A_initial_state">initial_state</code></td>
<td>
<p>Optional Python <code>list</code> of <code>Tensor</code>s, one for each model
parameter, representing the initial state(s) of the Markov chain(s). These
should have shape <code>tf$concat(list(chain_batch_shape, param$prior$batch_shape, param$prior$event_shape))</code>.
If <code>NULL</code>, the initial state is set automatically using a sample from a variational posterior.
Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_fit_with_hmc_+3A_initial_step_size">initial_step_size</code></td>
<td>
<p><code>list</code> of <code>tensor</code>s, one for each model parameter,
representing the step size for the leapfrog integrator. Must
broadcast with the shape of <code>initial_state</code>. Larger step sizes lead to
faster progress, but too-large step sizes make rejection exponentially
more likely. If <code>NULL</code>, the step size is set automatically using the
standard deviation of a variational posterior. Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_fit_with_hmc_+3A_chain_batch_shape">chain_batch_shape</code></td>
<td>
<p>Batch shape (<code>list</code> or <code>int</code>) of chains to run in parallel.
Default value: <code>list()</code> (i.e., a single chain).</p>
</td></tr>
<tr><td><code id="sts_fit_with_hmc_+3A_num_variational_steps">num_variational_steps</code></td>
<td>
<p><code>int</code> number of steps to run the variational
optimization to determine the initial state and step sizes. Default value: <code>150</code>.</p>
</td></tr>
<tr><td><code id="sts_fit_with_hmc_+3A_variational_optimizer">variational_optimizer</code></td>
<td>
<p>Optional <code>tf$train$Optimizer</code> instance to use in
the variational optimization. If <code>NULL</code>, defaults to <code>tf$train$AdamOptimizer(0.1)</code>.
Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_fit_with_hmc_+3A_variational_sample_size">variational_sample_size</code></td>
<td>
<p>integer number of Monte Carlo samples to use
in estimating the variational divergence. Larger values may stabilize
the optimization, but at higher cost per step in time and memory.
Default value: <code>1</code>.</p>
</td></tr>
<tr><td><code id="sts_fit_with_hmc_+3A_seed">seed</code></td>
<td>
<p>integer to seed the random number generator.</p>
</td></tr>
<tr><td><code id="sts_fit_with_hmc_+3A_name">name</code></td>
<td>
<p>name prefixed to ops created by this function. Default value: <code>NULL</code> (i.e., 'fit_with_hmc').</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method attempts to provide a sensible default approach for fitting
StructuralTimeSeries models using HMC. It first runs variational inference as
a fast posterior approximation, and initializes the HMC sampler from the
variational posterior, using the posterior standard deviations to set
per-variable step sizes (equivalently, a diagonal mass matrix). During the
warmup phase, it adapts the step size to target an acceptance rate of 0.75,
which is thought to be in the desirable range for optimal mixing (Betancourt et al., 2014).
</p>


<h3>Value</h3>

<p>list of:
</p>

<ul>
<li><p> samples: <code>list</code> of <code>Tensors</code> representing posterior samples of model
parameters, with shapes <code style="white-space: pre;">&#8288;[concat([[num_results], chain_batch_shape, param.prior.batch_shape, param.prior.event_shape]) for param in model.parameters]&#8288;</code>.
</p>
</li>
<li><p> kernel_results: A (possibly nested) <code>list</code> of <code>Tensor</code>s representing
internal calculations made within the HMC sampler.
</p>
</li></ul>



<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1206.1901">Radford Neal. MCMC Using Hamiltonian Dynamics. <em>Handbook of Markov Chain Monte Carlo</em>, 2011.</a>
</p>
</li>
<li> <p><a href="https://arxiv.org/abs/1411.6669">M.J. Betancourt, Simon Byrne, and Mark Girolami. Optimizing The Integrator Step Size for Hamiltonian Monte Carlo.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other sts-functions: 
<code><a href="#topic+sts_build_factored_surrogate_posterior">sts_build_factored_surrogate_posterior</a>()</code>,
<code><a href="#topic+sts_build_factored_variational_loss">sts_build_factored_variational_loss</a>()</code>,
<code><a href="#topic+sts_decompose_by_component">sts_decompose_by_component</a>()</code>,
<code><a href="#topic+sts_decompose_forecast_by_component">sts_decompose_forecast_by_component</a>()</code>,
<code><a href="#topic+sts_forecast">sts_forecast</a>()</code>,
<code><a href="#topic+sts_one_step_predictive">sts_one_step_predictive</a>()</code>,
<code><a href="#topic+sts_sample_uniform_initial_state">sts_sample_uniform_initial_state</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
observed_time_series &lt;-
  rep(c(3.5, 4.1, 4.5, 3.9, 2.4, 2.1, 1.2), 5) +
  rep(c(1.1, 1.5, 2.4, 3.1, 4.0), each = 7) %&gt;%
  tensorflow::tf$convert_to_tensor(dtype = tensorflow::tf$float64)
day_of_week &lt;- observed_time_series %&gt;% sts_seasonal(num_seasons = 7)
local_linear_trend &lt;- observed_time_series %&gt;% sts_local_linear_trend()
model &lt;- observed_time_series %&gt;%
  sts_sum(components = list(day_of_week, local_linear_trend))
states_and_results &lt;- observed_time_series %&gt;%
  sts_fit_with_hmc(
    model,
    num_results = 10,
    num_warmup_steps = 5,
    num_variational_steps = 15)


</code></pre>

<hr>
<h2 id='sts_forecast'>Construct predictive distribution over future observations</h2><span id='topic+sts_forecast'></span>

<h3>Description</h3>

<p>Given samples from the posterior over parameters, return the predictive
distribution over future observations for num_steps_forecast timesteps.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sts_forecast(
  observed_time_series,
  model,
  parameter_samples,
  num_steps_forecast
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_forecast_+3A_observed_time_series">observed_time_series</code></td>
<td>
<p><code>float</code> <code>tensor</code> of shape
<code style="white-space: pre;">&#8288;concat([sample_shape, model.batch_shape, [num_timesteps, 1]])&#8288;</code> where
<code>sample_shape</code> corresponds to i.i.d. observations, and the trailing <code style="white-space: pre;">&#8288;[1]&#8288;</code>
dimension may (optionally) be omitted if <code>num_timesteps &gt; 1</code>. May
optionally be an instance of <code>sts_masked_time_series</code>, which includes
a mask <code>tensor</code> to specify timesteps with missing observations.</p>
</td></tr>
<tr><td><code id="sts_forecast_+3A_model">model</code></td>
<td>
<p>An instance of <code>StructuralTimeSeries</code> representing a
time-series model. This represents a joint distribution over
time-series and their parameters with batch shape <code style="white-space: pre;">&#8288;[b1, ..., bN]&#8288;</code>.</p>
</td></tr>
<tr><td><code id="sts_forecast_+3A_parameter_samples">parameter_samples</code></td>
<td>
<p><code>list</code> of <code>tensors</code> representing posterior samples
of model parameters, with shapes
<code style="white-space: pre;">&#8288;list(tf$concat(list(list(num_posterior_draws), param&lt;1&gt;$prior$batch_shape, param&lt;1&gt;$prior$event_shape), list(list(num_posterior_draws), param&lt;2&gt;$prior$batch_shape, param&lt;2&gt;$prior$event_shape), ... ) )&#8288;</code>
for all model parameters.
This may optionally also be a named list mapping parameter names to <code>tensor</code> values.</p>
</td></tr>
<tr><td><code id="sts_forecast_+3A_num_steps_forecast">num_steps_forecast</code></td>
<td>
<p>scalar <code>integer</code> <code>tensor</code> number of steps to forecast</p>
</td></tr>
</table>


<h3>Value</h3>

<p>forecast_dist a <code>tfd_mixture_same_family</code> instance with event shape
<code>list(num_steps_forecast, 1)</code> and batch shape <code>tf$concat(list(sample_shape, model$batch_shape))</code>, with
<code>num_posterior_draws</code> mixture components.
</p>


<h3>See Also</h3>

<p>Other sts-functions: 
<code><a href="#topic+sts_build_factored_surrogate_posterior">sts_build_factored_surrogate_posterior</a>()</code>,
<code><a href="#topic+sts_build_factored_variational_loss">sts_build_factored_variational_loss</a>()</code>,
<code><a href="#topic+sts_decompose_by_component">sts_decompose_by_component</a>()</code>,
<code><a href="#topic+sts_decompose_forecast_by_component">sts_decompose_forecast_by_component</a>()</code>,
<code><a href="#topic+sts_fit_with_hmc">sts_fit_with_hmc</a>()</code>,
<code><a href="#topic+sts_one_step_predictive">sts_one_step_predictive</a>()</code>,
<code><a href="#topic+sts_sample_uniform_initial_state">sts_sample_uniform_initial_state</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
observed_time_series &lt;-
  rep(c(3.5, 4.1, 4.5, 3.9, 2.4, 2.1, 1.2), 5) +
  rep(c(1.1, 1.5, 2.4, 3.1, 4.0), each = 7) %&gt;%
  tensorflow::tf$convert_to_tensor(dtype = tensorflow::tf$float64)
day_of_week &lt;- observed_time_series %&gt;% sts_seasonal(num_seasons = 7)
local_linear_trend &lt;- observed_time_series %&gt;% sts_local_linear_trend()
model &lt;- observed_time_series %&gt;%
  sts_sum(components = list(day_of_week, local_linear_trend))
states_and_results &lt;- observed_time_series %&gt;%
  sts_fit_with_hmc(
    model,
    num_results = 10,
    num_warmup_steps = 5,
    num_variational_steps = 15)
samples &lt;- states_and_results[[1]]
preds &lt;- observed_time_series %&gt;%
  sts_forecast(model,
               parameter_samples = samples,
               num_steps_forecast = 50)
predictions &lt;- preds %&gt;% tfd_sample(10)


</code></pre>

<hr>
<h2 id='sts_linear_regression'>Formal representation of a linear regression from provided covariates.</h2><span id='topic+sts_linear_regression'></span>

<h3>Description</h3>

<p>This model defines a time series given by a linear combination of
covariate time series provided in a design matrix:
</p>
<div class="sourceCode"><pre>observed_time_series &lt;- tf$matmul(design_matrix, weights)
</pre></div>


<h3>Usage</h3>

<pre><code class='language-R'>sts_linear_regression(design_matrix, weights_prior = NULL, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_linear_regression_+3A_design_matrix">design_matrix</code></td>
<td>
<p>float <code>tensor</code> of shape <code>tf$concat(list(batch_shape, list(num_timesteps, num_features)))</code>.
This may also optionally be an instance of <code>tf$linalg$LinearOperator</code>.</p>
</td></tr>
<tr><td><code id="sts_linear_regression_+3A_weights_prior">weights_prior</code></td>
<td>
<p><code>Distribution</code> representing a prior over the regression
weights. Must have event shape <code>list(num_features)</code> and batch shape
broadcastable to the design matrix's <code>batch_shape</code>. Alternately,
<code>event_shape</code> may be scalar (<code>list()</code>), in which case the prior is
internally broadcast as
<code>tfd_transformed_distribution(weights_prior, tfb_identity(), event_shape = list(num_features), batch_shape = design_matrix$batch_shape)</code>.
If <code>NULL</code>, defaults to <code>tfd_student_t(df = 5, loc = 0, scale = 10)</code>,
a weakly-informative prior loosely inspired by the
<a href="https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations">Stan prior choice recommendations</a>.
Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_linear_regression_+3A_name">name</code></td>
<td>
<p>the name of this model component. Default value: 'LinearRegression'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The design matrix has shape <code>list(num_timesteps, num_features)</code>.
The weights are treated as an unknown random variable of size <code>list(num_features)</code>
(both components also support batch shape), and are integrated over using the same
approximate inference tools as other model parameters, i.e., generally HMC or
variational inference.
</p>
<p>This component does not itself include observation noise; it defines a
deterministic distribution with mass at the point
<code>tf$matmul(design_matrix, weights)</code>. In practice, it should be combined with
observation noise from another component such as <code>sts_sum</code>, as demonstrated below.
</p>


<h3>Value</h3>

<p>an instance of <code>StructuralTimeSeries</code>.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+sts_fit_with_hmc">sts_fit_with_hmc()</a></code>, <code><a href="#topic+sts_forecast">sts_forecast()</a></code>, <code><a href="#topic+sts_decompose_by_component">sts_decompose_by_component()</a></code>.
</p>
<p>Other sts: 
<code><a href="#topic+sts_additive_state_space_model">sts_additive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive_state_space_model">sts_autoregressive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive">sts_autoregressive</a>()</code>,
<code><a href="#topic+sts_constrained_seasonal_state_space_model">sts_constrained_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression_state_space_model">sts_dynamic_linear_regression_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression">sts_dynamic_linear_regression</a>()</code>,
<code><a href="#topic+sts_local_level_state_space_model">sts_local_level_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_level">sts_local_level</a>()</code>,
<code><a href="#topic+sts_local_linear_trend_state_space_model">sts_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_linear_trend">sts_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_seasonal_state_space_model">sts_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_seasonal">sts_seasonal</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend_state_space_model">sts_semi_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend">sts_semi_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal_state_space_model">sts_smooth_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal">sts_smooth_seasonal</a>()</code>,
<code><a href="#topic+sts_sparse_linear_regression">sts_sparse_linear_regression</a>()</code>,
<code><a href="#topic+sts_sum">sts_sum</a>()</code>
</p>

<hr>
<h2 id='sts_local_level'>Formal representation of a local level model</h2><span id='topic+sts_local_level'></span>

<h3>Description</h3>

<p>The local level model posits a <code>level</code> evolving via a Gaussian random walk:
</p>
<div class="sourceCode"><pre>level[t] = level[t-1] + Normal(0., level_scale)
</pre></div>


<h3>Usage</h3>

<pre><code class='language-R'>sts_local_level(
  observed_time_series = NULL,
  level_scale_prior = NULL,
  initial_level_prior = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_local_level_+3A_observed_time_series">observed_time_series</code></td>
<td>
<p>optional <code>float</code> <code>tensor</code> of shape
<code style="white-space: pre;">&#8288;batch_shape + [T, 1]&#8288;</code> (omitting the trailing unit dimension is also
supported when <code>T &gt; 1</code>), specifying an observed time series.
Any priors not explicitly set will be given default values according to
the scale of the observed time series (or batch of time series). May
optionally be an instance of <code>sts_masked_time_series</code>, which includes
a mask <code>tensor</code> to specify timesteps with missing observations.
Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_local_level_+3A_level_scale_prior">level_scale_prior</code></td>
<td>
<p>optional <code>tfp$distribution</code> instance specifying a prior
on the <code>level_scale</code> parameter. If <code>NULL</code>, a heuristic default prior is
constructed based on the provided <code>observed_time_series</code>.
Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_local_level_+3A_initial_level_prior">initial_level_prior</code></td>
<td>
<p>optional <code>tfp$distribution</code> instance specifying a
prior on the initial level. If <code>NULL</code>, a heuristic default prior is
constructed based on the provided <code>observed_time_series</code>.
Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_local_level_+3A_name">name</code></td>
<td>
<p>the name of this model component. Default value: 'LocalLevel'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The latent state is <code style="white-space: pre;">&#8288;[level]&#8288;</code>. We observe a noisy realization of the current
level: <code>f[t] = level[t] + Normal(0., observation_noise_scale)</code> at each timestep.
</p>


<h3>Value</h3>

<p>an instance of <code>StructuralTimeSeries</code>.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+sts_fit_with_hmc">sts_fit_with_hmc()</a></code>, <code><a href="#topic+sts_forecast">sts_forecast()</a></code>, <code><a href="#topic+sts_decompose_by_component">sts_decompose_by_component()</a></code>.
</p>
<p>Other sts: 
<code><a href="#topic+sts_additive_state_space_model">sts_additive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive_state_space_model">sts_autoregressive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive">sts_autoregressive</a>()</code>,
<code><a href="#topic+sts_constrained_seasonal_state_space_model">sts_constrained_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression_state_space_model">sts_dynamic_linear_regression_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression">sts_dynamic_linear_regression</a>()</code>,
<code><a href="#topic+sts_linear_regression">sts_linear_regression</a>()</code>,
<code><a href="#topic+sts_local_level_state_space_model">sts_local_level_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_linear_trend_state_space_model">sts_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_linear_trend">sts_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_seasonal_state_space_model">sts_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_seasonal">sts_seasonal</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend_state_space_model">sts_semi_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend">sts_semi_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal_state_space_model">sts_smooth_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal">sts_smooth_seasonal</a>()</code>,
<code><a href="#topic+sts_sparse_linear_regression">sts_sparse_linear_regression</a>()</code>,
<code><a href="#topic+sts_sum">sts_sum</a>()</code>
</p>

<hr>
<h2 id='sts_local_level_state_space_model'>State space model for a local level</h2><span id='topic+sts_local_level_state_space_model'></span>

<h3>Description</h3>

<p>A state space model (SSM) posits a set of latent (unobserved) variables that
evolve over time with dynamics specified by a probabilistic transition model
<code>p(z[t+1] | z[t])</code>. At each timestep, we observe a value sampled from an
observation model conditioned on the current state, <code>p(x[t] | z[t])</code>. The
special case where both the transition and observation models are Gaussians
with mean specified as a linear function of the inputs, is known as a linear
Gaussian state space model and supports tractable exact probabilistic
calculations; see <code>tfd_linear_gaussian_state_space_model</code> for
details.
The local level model is a special case of a linear Gaussian SSM, in which the
latent state posits a <code>level</code> evolving via a Gaussian random walk:
</p>
<div class="sourceCode"><pre>level[t] = level[t-1] + Normal(0., level_scale)
</pre></div>


<h3>Usage</h3>

<pre><code class='language-R'>sts_local_level_state_space_model(
  num_timesteps,
  level_scale,
  initial_state_prior,
  observation_noise_scale = 0,
  initial_step = 0,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_local_level_state_space_model_+3A_num_timesteps">num_timesteps</code></td>
<td>
<p>Scalar <code>integer</code> <code>tensor</code> number of timesteps to model
with this distribution.</p>
</td></tr>
<tr><td><code id="sts_local_level_state_space_model_+3A_level_scale">level_scale</code></td>
<td>
<p>Scalar (any additional dimensions are treated as batch
dimensions) <code>float</code> <code>tensor</code> indicating the standard deviation of the
level transitions.</p>
</td></tr>
<tr><td><code id="sts_local_level_state_space_model_+3A_initial_state_prior">initial_state_prior</code></td>
<td>
<p>instance of <code>tfd_multivariate_normal</code>
representing the prior distribution on latent states.  Must have
event shape <code style="white-space: pre;">&#8288;[1]&#8288;</code> (as <code>tfd_linear_gaussian_state_space_model</code> requires a
rank-1 event shape).</p>
</td></tr>
<tr><td><code id="sts_local_level_state_space_model_+3A_observation_noise_scale">observation_noise_scale</code></td>
<td>
<p>Scalar (any additional dimensions are
treated as batch dimensions) <code>float</code> <code>tensor</code> indicating the standard
deviation of the observation noise.</p>
</td></tr>
<tr><td><code id="sts_local_level_state_space_model_+3A_initial_step">initial_step</code></td>
<td>
<p>Optional scalar <code>integer</code> <code>tensor</code> specifying the starting
timestep. Default value: 0.</p>
</td></tr>
<tr><td><code id="sts_local_level_state_space_model_+3A_validate_args">validate_args</code></td>
<td>
<p><code>logical</code>. Whether to validate input
with asserts. If <code>validate_args</code> is <code>FALSE</code>, and the inputs are
invalid, correct behavior is not guaranteed. Default value: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="sts_local_level_state_space_model_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p><code>logical</code>. If <code>FALSE</code>, raise an
exception if a statistic (e.g. mean/mode/etc...) is undefined for any
batch member. If <code>TRUE</code>, batch members with valid parameters leading to
undefined statistics will return NaN for this statistic. Default value: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="sts_local_level_state_space_model_+3A_name">name</code></td>
<td>
<p>string name prefixed to ops created by this class.
Default value: &quot;LocalLevelStateSpaceModel&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The latent state is <code style="white-space: pre;">&#8288;[level]&#8288;</code> and <code style="white-space: pre;">&#8288;[level]&#8288;</code> is observed (with noise) at each timestep.
</p>
<p>The parameters <code>level_scale</code> and <code>observation_noise_scale</code> are each (a batch
of) scalars. The batch shape of this <code>Distribution</code> is the broadcast batch
shape of these parameters and of the <code>initial_state_prior</code>.
</p>
<p>Mathematical Details
</p>
<p>The local level model implements a <code>tfp$distributions$LinearGaussianStateSpaceModel</code> with
<code>latent_size = 1</code> and <code>observation_size = 1</code>, following the transition model:
</p>
<div class="sourceCode"><pre>transition_matrix = [[1]]
transition_noise ~ N(loc = 0, scale = diag([level_scale]))
</pre></div>
<p>which implements the evolution of <code>level</code> described above, and the observation model:
</p>
<div class="sourceCode"><pre>observation_matrix = [[1]]
observation_noise ~ N(loc = 0, scale = observation_noise_scale)
</pre></div>


<h3>Value</h3>

<p>an instance of <code>LinearGaussianStateSpaceModel</code>.
</p>


<h3>See Also</h3>

<p>Other sts: 
<code><a href="#topic+sts_additive_state_space_model">sts_additive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive_state_space_model">sts_autoregressive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive">sts_autoregressive</a>()</code>,
<code><a href="#topic+sts_constrained_seasonal_state_space_model">sts_constrained_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression_state_space_model">sts_dynamic_linear_regression_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression">sts_dynamic_linear_regression</a>()</code>,
<code><a href="#topic+sts_linear_regression">sts_linear_regression</a>()</code>,
<code><a href="#topic+sts_local_level">sts_local_level</a>()</code>,
<code><a href="#topic+sts_local_linear_trend_state_space_model">sts_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_linear_trend">sts_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_seasonal_state_space_model">sts_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_seasonal">sts_seasonal</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend_state_space_model">sts_semi_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend">sts_semi_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal_state_space_model">sts_smooth_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal">sts_smooth_seasonal</a>()</code>,
<code><a href="#topic+sts_sparse_linear_regression">sts_sparse_linear_regression</a>()</code>,
<code><a href="#topic+sts_sum">sts_sum</a>()</code>
</p>

<hr>
<h2 id='sts_local_linear_trend'>Formal representation of a local linear trend model</h2><span id='topic+sts_local_linear_trend'></span>

<h3>Description</h3>

<p>The local linear trend model posits a <code>level</code> and <code>slope</code>, each
evolving via a Gaussian random walk:
</p>
<div class="sourceCode"><pre>level[t] = level[t-1] + slope[t-1] + Normal(0., level_scale)
slope[t] = slope[t-1] + Normal(0., slope_scale)
</pre></div>


<h3>Usage</h3>

<pre><code class='language-R'>sts_local_linear_trend(
  observed_time_series = NULL,
  level_scale_prior = NULL,
  slope_scale_prior = NULL,
  initial_level_prior = NULL,
  initial_slope_prior = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_local_linear_trend_+3A_observed_time_series">observed_time_series</code></td>
<td>
<p>optional <code>float</code> <code>tensor</code> of shape
<code style="white-space: pre;">&#8288;batch_shape + [T, 1]&#8288;</code> (omitting the trailing unit dimension is also
supported when <code>T &gt; 1</code>), specifying an observed time series.
Any priors not explicitly set will be given default values according to
the scale of the observed time series (or batch of time series). May
optionally be an instance of <code>sts_masked_time_series</code>, which includes
a mask <code>tensor</code> to specify timesteps with missing observations.
Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_local_linear_trend_+3A_level_scale_prior">level_scale_prior</code></td>
<td>
<p>optional <code>tfp$distribution</code> instance specifying a prior
on the <code>level_scale</code> parameter. If <code>NULL</code>, a heuristic default prior is
constructed based on the provided <code>observed_time_series</code>.
Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_local_linear_trend_+3A_slope_scale_prior">slope_scale_prior</code></td>
<td>
<p>optional <code>tfd$Distribution</code> instance specifying a prior
on the <code>slope_scale</code> parameter. If <code>NULL</code>, a heuristic default prior is
constructed based on the provided <code>observed_time_series</code>. Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_local_linear_trend_+3A_initial_level_prior">initial_level_prior</code></td>
<td>
<p>optional <code>tfp$distribution</code> instance specifying a
prior on the initial level. If <code>NULL</code>, a heuristic default prior is
constructed based on the provided <code>observed_time_series</code>.
Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_local_linear_trend_+3A_initial_slope_prior">initial_slope_prior</code></td>
<td>
<p>optional <code>tfd$Distribution</code> instance specifying a
prior on the initial slope. If <code>NULL</code>, a heuristic default prior is
constructed based on the provided <code>observed_time_series</code>. Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_local_linear_trend_+3A_name">name</code></td>
<td>
<p>the name of this model component. Default value: 'LocalLinearTrend'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The latent state is the two-dimensional tuple <code style="white-space: pre;">&#8288;[level, slope]&#8288;</code>. At each
timestep we observe a noisy realization of the current level:
<code>f[t] = level[t] + Normal(0., observation_noise_scale)</code>.
This model is appropriate for data where the trend direction and magnitude (latent
<code>slope</code>) is consistent within short periods but may evolve over time.
</p>
<p>Note that this model can produce very high uncertainty forecasts, as
uncertainty over the slope compounds quickly. If you expect your data to
have nonzero long-term trend, i.e. that slopes tend to revert to some mean,
then the <code>SemiLocalLinearTrend</code> model may produce sharper forecasts.
</p>


<h3>Value</h3>

<p>an instance of <code>StructuralTimeSeries</code>.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+sts_fit_with_hmc">sts_fit_with_hmc()</a></code>, <code><a href="#topic+sts_forecast">sts_forecast()</a></code>, <code><a href="#topic+sts_decompose_by_component">sts_decompose_by_component()</a></code>.
</p>
<p>Other sts: 
<code><a href="#topic+sts_additive_state_space_model">sts_additive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive_state_space_model">sts_autoregressive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive">sts_autoregressive</a>()</code>,
<code><a href="#topic+sts_constrained_seasonal_state_space_model">sts_constrained_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression_state_space_model">sts_dynamic_linear_regression_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression">sts_dynamic_linear_regression</a>()</code>,
<code><a href="#topic+sts_linear_regression">sts_linear_regression</a>()</code>,
<code><a href="#topic+sts_local_level_state_space_model">sts_local_level_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_level">sts_local_level</a>()</code>,
<code><a href="#topic+sts_local_linear_trend_state_space_model">sts_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_seasonal_state_space_model">sts_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_seasonal">sts_seasonal</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend_state_space_model">sts_semi_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend">sts_semi_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal_state_space_model">sts_smooth_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal">sts_smooth_seasonal</a>()</code>,
<code><a href="#topic+sts_sparse_linear_regression">sts_sparse_linear_regression</a>()</code>,
<code><a href="#topic+sts_sum">sts_sum</a>()</code>
</p>

<hr>
<h2 id='sts_local_linear_trend_state_space_model'>State space model for a local linear trend</h2><span id='topic+sts_local_linear_trend_state_space_model'></span>

<h3>Description</h3>

<p>A state space model (SSM) posits a set of latent (unobserved) variables that
evolve over time with dynamics specified by a probabilistic transition model
<code>p(z[t+1] | z[t])</code>. At each timestep, we observe a value sampled from an
observation model conditioned on the current state, <code>p(x[t] | z[t])</code>. The
special case where both the transition and observation models are Gaussians
with mean specified as a linear function of the inputs, is known as a linear
Gaussian state space model and supports tractable exact probabilistic
calculations; see <code>tfd_linear_gaussian_state_space_model</code> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sts_local_linear_trend_state_space_model(
  num_timesteps,
  level_scale,
  slope_scale,
  initial_state_prior,
  observation_noise_scale = 0,
  initial_step = 0,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_local_linear_trend_state_space_model_+3A_num_timesteps">num_timesteps</code></td>
<td>
<p>Scalar <code>integer</code> <code>tensor</code> number of timesteps to model
with this distribution.</p>
</td></tr>
<tr><td><code id="sts_local_linear_trend_state_space_model_+3A_level_scale">level_scale</code></td>
<td>
<p>Scalar (any additional dimensions are treated as batch
dimensions) <code>float</code> <code>tensor</code> indicating the standard deviation of the
level transitions.</p>
</td></tr>
<tr><td><code id="sts_local_linear_trend_state_space_model_+3A_slope_scale">slope_scale</code></td>
<td>
<p>Scalar (any additional dimensions are treated as batch
dimensions) <code>float</code> <code>tensor</code> indicating the standard deviation of the
slope transitions.</p>
</td></tr>
<tr><td><code id="sts_local_linear_trend_state_space_model_+3A_initial_state_prior">initial_state_prior</code></td>
<td>
<p>instance of <code>tfd_multivariate_normal</code>
representing the prior distribution on latent states.  Must have
event shape <code style="white-space: pre;">&#8288;[1]&#8288;</code> (as <code>tfd_linear_gaussian_state_space_model</code> requires a
rank-1 event shape).</p>
</td></tr>
<tr><td><code id="sts_local_linear_trend_state_space_model_+3A_observation_noise_scale">observation_noise_scale</code></td>
<td>
<p>Scalar (any additional dimensions are
treated as batch dimensions) <code>float</code> <code>tensor</code> indicating the standard
deviation of the observation noise.</p>
</td></tr>
<tr><td><code id="sts_local_linear_trend_state_space_model_+3A_initial_step">initial_step</code></td>
<td>
<p>Optional scalar <code>integer</code> <code>tensor</code> specifying the starting
timestep. Default value: 0.</p>
</td></tr>
<tr><td><code id="sts_local_linear_trend_state_space_model_+3A_validate_args">validate_args</code></td>
<td>
<p><code>logical</code>. Whether to validate input
with asserts. If <code>validate_args</code> is <code>FALSE</code>, and the inputs are
invalid, correct behavior is not guaranteed. Default value: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="sts_local_linear_trend_state_space_model_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p><code>logical</code>. If <code>FALSE</code>, raise an
exception if a statistic (e.g. mean/mode/etc...) is undefined for any
batch member. If <code>TRUE</code>, batch members with valid parameters leading to
undefined statistics will return NaN for this statistic. Default value: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="sts_local_linear_trend_state_space_model_+3A_name">name</code></td>
<td>
<p>string prefixed to ops created by this class.
Default value: &quot;LocalLinearTrendStateSpaceModel&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The local linear trend model is a special case of a linear Gaussian SSM, in
which the latent state posits a <code>level</code> and <code>slope</code>, each evolving via a
Gaussian random walk:
</p>
<div class="sourceCode"><pre>level[t] = level[t-1] + slope[t-1] + Normal(0., level_scale)
slope[t] = slope[t-1] + Normal(0., slope_scale)
</pre></div>
<p>The latent state is the two-dimensional tuple <code style="white-space: pre;">&#8288;[level, slope]&#8288;</code>. The
<code>level</code> is observed at each timestep.
</p>
<p>The parameters <code>level_scale</code>, <code>slope_scale</code>, and <code>observation_noise_scale</code>
are each (a batch of) scalars. The batch shape of this <code>Distribution</code> is the
broadcast batch shape of these parameters and of the <code>initial_state_prior</code>.
</p>
<p>Mathematical Details
</p>
<p>The linear trend model implements a <code>tfd_linear_gaussian_state_space_model</code>
with <code>latent_size = 2</code> and <code>observation_size = 1</code>, following the transition model:
</p>
<div class="sourceCode"><pre>transition_matrix = [[1., 1.]
                     [0., 1.]]
transition_noise ~ N(loc = 0, scale = diag([level_scale, slope_scale]))
</pre></div>
<p>which implements the evolution of <code style="white-space: pre;">&#8288;[level, slope]&#8288;</code> described above, and the observation model:
</p>
<div class="sourceCode"><pre>observation_matrix = [[1., 0.]]
observation_noise ~ N(loc= 0 , scale = observation_noise_scale)
</pre></div>
<p>which picks out the first latent component, i.e., the <code>level</code>, as the
observation at each timestep.
</p>


<h3>Value</h3>

<p>an instance of <code>LinearGaussianStateSpaceModel</code>.
</p>


<h3>See Also</h3>

<p>Other sts: 
<code><a href="#topic+sts_additive_state_space_model">sts_additive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive_state_space_model">sts_autoregressive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive">sts_autoregressive</a>()</code>,
<code><a href="#topic+sts_constrained_seasonal_state_space_model">sts_constrained_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression_state_space_model">sts_dynamic_linear_regression_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression">sts_dynamic_linear_regression</a>()</code>,
<code><a href="#topic+sts_linear_regression">sts_linear_regression</a>()</code>,
<code><a href="#topic+sts_local_level_state_space_model">sts_local_level_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_level">sts_local_level</a>()</code>,
<code><a href="#topic+sts_local_linear_trend">sts_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_seasonal_state_space_model">sts_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_seasonal">sts_seasonal</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend_state_space_model">sts_semi_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend">sts_semi_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal_state_space_model">sts_smooth_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal">sts_smooth_seasonal</a>()</code>,
<code><a href="#topic+sts_sparse_linear_regression">sts_sparse_linear_regression</a>()</code>,
<code><a href="#topic+sts_sum">sts_sum</a>()</code>
</p>

<hr>
<h2 id='sts_one_step_predictive'>Compute one-step-ahead predictive distributions for all timesteps</h2><span id='topic+sts_one_step_predictive'></span>

<h3>Description</h3>

<p>Given samples from the posterior over parameters, return the predictive
distribution over observations at each time <code>T</code>, given observations up
through time <code>T-1</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sts_one_step_predictive(
  observed_time_series,
  model,
  parameter_samples,
  timesteps_are_event_shape = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_one_step_predictive_+3A_observed_time_series">observed_time_series</code></td>
<td>
<p><code>float</code> <code>tensor</code> of shape
<code style="white-space: pre;">&#8288;concat([sample_shape, model.batch_shape, [num_timesteps, 1]])&#8288;</code> where
<code>sample_shape</code> corresponds to i.i.d. observations, and the trailing <code style="white-space: pre;">&#8288;[1]&#8288;</code>
dimension may (optionally) be omitted if <code>num_timesteps &gt; 1</code>. May
optionally be an instance of <code>sts_masked_time_series</code>, which includes
a mask <code>tensor</code> to specify timesteps with missing observations.</p>
</td></tr>
<tr><td><code id="sts_one_step_predictive_+3A_model">model</code></td>
<td>
<p>An instance of <code>StructuralTimeSeries</code> representing a
time-series model. This represents a joint distribution over
time-series and their parameters with batch shape <code style="white-space: pre;">&#8288;[b1, ..., bN]&#8288;</code>.</p>
</td></tr>
<tr><td><code id="sts_one_step_predictive_+3A_parameter_samples">parameter_samples</code></td>
<td>
<p><code>list</code> of <code>tensors</code> representing posterior samples
of model parameters, with shapes
<code style="white-space: pre;">&#8288;list(tf$concat(list(list(num_posterior_draws), param&lt;1&gt;$prior$batch_shape, param&lt;1&gt;$prior$event_shape), list(list(num_posterior_draws), param&lt;2&gt;$prior$batch_shape, param&lt;2&gt;$prior$event_shape), ... ) )&#8288;</code>
for all model parameters.
This may optionally also be a named list mapping parameter names to <code>tensor</code> values.</p>
</td></tr>
<tr><td><code id="sts_one_step_predictive_+3A_timesteps_are_event_shape">timesteps_are_event_shape</code></td>
<td>
<p>Deprecated, for backwards compatibility only. If False, the predictive distribution will return per-timestep probabilities Default value: TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>forecast_dist a <code>tfd_mixture_same_family</code> instance with event shape
<code>list(num_timesteps)</code> and batch shape <code>tf$concat(list(sample_shape, model$batch_shape))</code>, with
<code>num_posterior_draws</code> mixture components. The <code>t</code>th step represents the
forecast distribution <code>p(observed_time_series[t] | observed_time_series[0:t-1], parameter_samples)</code>.
</p>


<h3>See Also</h3>

<p>Other sts-functions: 
<code><a href="#topic+sts_build_factored_surrogate_posterior">sts_build_factored_surrogate_posterior</a>()</code>,
<code><a href="#topic+sts_build_factored_variational_loss">sts_build_factored_variational_loss</a>()</code>,
<code><a href="#topic+sts_decompose_by_component">sts_decompose_by_component</a>()</code>,
<code><a href="#topic+sts_decompose_forecast_by_component">sts_decompose_forecast_by_component</a>()</code>,
<code><a href="#topic+sts_fit_with_hmc">sts_fit_with_hmc</a>()</code>,
<code><a href="#topic+sts_forecast">sts_forecast</a>()</code>,
<code><a href="#topic+sts_sample_uniform_initial_state">sts_sample_uniform_initial_state</a>()</code>
</p>

<hr>
<h2 id='sts_sample_uniform_initial_state'>Initialize from a uniform <code style="white-space: pre;">&#8288;[-2, 2]&#8288;</code> distribution in unconstrained space.</h2><span id='topic+sts_sample_uniform_initial_state'></span>

<h3>Description</h3>

<p>Initialize from a uniform <code style="white-space: pre;">&#8288;[-2, 2]&#8288;</code> distribution in unconstrained space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sts_sample_uniform_initial_state(
  parameter,
  return_constrained = TRUE,
  init_sample_shape = list(),
  seed = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_sample_uniform_initial_state_+3A_parameter">parameter</code></td>
<td>
<p><code>sts$Parameter</code> named tuple instance.</p>
</td></tr>
<tr><td><code id="sts_sample_uniform_initial_state_+3A_return_constrained">return_constrained</code></td>
<td>
<p>if <code>TRUE</code>, re-applies the constraining bijector
to return initializations in the original domain. Otherwise, returns
initializations in the unconstrained space.
Default value: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="sts_sample_uniform_initial_state_+3A_init_sample_shape">init_sample_shape</code></td>
<td>
<p><code>sample_shape</code> of the sampled initializations.
Default value: <code>list()</code>.</p>
</td></tr>
<tr><td><code id="sts_sample_uniform_initial_state_+3A_seed">seed</code></td>
<td>
<p>integer to seed the random number generator.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>uniform_initializer <code>Tensor</code> of shape
<code style="white-space: pre;">&#8288;concat([init_sample_shape, parameter.prior.batch_shape, transformed_event_shape])&#8288;</code>, where
<code>transformed_event_shape</code> is <code>parameter.prior.event_shape</code>, if
<code>return_constrained=TRUE</code>, and otherwise it is
<code>parameter$bijector$inverse_event_shape(parameter$prior$event_shape)</code>.
</p>


<h3>See Also</h3>

<p>Other sts-functions: 
<code><a href="#topic+sts_build_factored_surrogate_posterior">sts_build_factored_surrogate_posterior</a>()</code>,
<code><a href="#topic+sts_build_factored_variational_loss">sts_build_factored_variational_loss</a>()</code>,
<code><a href="#topic+sts_decompose_by_component">sts_decompose_by_component</a>()</code>,
<code><a href="#topic+sts_decompose_forecast_by_component">sts_decompose_forecast_by_component</a>()</code>,
<code><a href="#topic+sts_fit_with_hmc">sts_fit_with_hmc</a>()</code>,
<code><a href="#topic+sts_forecast">sts_forecast</a>()</code>,
<code><a href="#topic+sts_one_step_predictive">sts_one_step_predictive</a>()</code>
</p>

<hr>
<h2 id='sts_seasonal'>Formal representation of a seasonal effect model.</h2><span id='topic+sts_seasonal'></span>

<h3>Description</h3>

<p>A seasonal effect model posits a fixed set of recurring, discrete 'seasons',
each of which is active for a fixed number of timesteps and, while active,
contributes a different effect to the time series. These are generally not
meteorological seasons, but represent regular recurring patterns such as
hour-of-day or day-of-week effects. Each season lasts for a fixed number of
timesteps. The effect of each season drifts from one occurrence to the next
following a Gaussian random walk:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sts_seasonal(
  observed_time_series = NULL,
  num_seasons,
  num_steps_per_season = 1,
  drift_scale_prior = NULL,
  initial_effect_prior = NULL,
  constrain_mean_effect_to_zero = TRUE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_seasonal_+3A_observed_time_series">observed_time_series</code></td>
<td>
<p>optional <code>float</code> <code>tensor</code> of shape
<code style="white-space: pre;">&#8288;batch_shape + [T, 1]&#8288;</code> (omitting the trailing unit dimension is also
supported when <code>T &gt; 1</code>), specifying an observed time series.
Any priors not explicitly set will be given default values according to
the scale of the observed time series (or batch of time series). May
optionally be an instance of <code>sts_masked_time_series</code>, which includes
a mask <code>tensor</code> to specify timesteps with missing observations.
Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_seasonal_+3A_num_seasons">num_seasons</code></td>
<td>
<p>Scalar <code>integer</code> number of seasons.</p>
</td></tr>
<tr><td><code id="sts_seasonal_+3A_num_steps_per_season">num_steps_per_season</code></td>
<td>
<p><code>integer</code> number of steps in each
season. This may be either a scalar (shape <code style="white-space: pre;">&#8288;[]&#8288;</code>), in which case all
seasons have the same length, or an array of shape <code style="white-space: pre;">&#8288;[num_seasons]&#8288;</code>,
in which seasons have different length, but remain constant around
different cycles, or an array of shape <code style="white-space: pre;">&#8288;[num_cycles, num_seasons]&#8288;</code>,
in which num_steps_per_season for each season also varies in different
cycle (e.g., a 4 years cycle with leap day). Default value: 1.</p>
</td></tr>
<tr><td><code id="sts_seasonal_+3A_drift_scale_prior">drift_scale_prior</code></td>
<td>
<p>optional <code>tfd$Distribution</code> instance specifying a prior
on the <code>drift_scale</code> parameter. If <code>NULL</code>, a heuristic default prior is
constructed based on the provided <code>observed_time_series</code>. Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_seasonal_+3A_initial_effect_prior">initial_effect_prior</code></td>
<td>
<p>optional <code>tfd$Distribution</code> instance specifying a
normal prior on the initial effect of each season. This may be either
a scalar <code>tfd_normal</code> prior, in which case it applies independently to
every season, or it may be multivariate normal (e.g.,
<code>tfd_multivariate_normal_diag</code>) with event shape <code style="white-space: pre;">&#8288;[num_seasons]&#8288;</code>, in
which case it specifies a joint prior across all seasons. If <code>NULL</code>, a
heuristic default prior is constructed based on the provided
<code>observed_time_series</code>. Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_seasonal_+3A_constrain_mean_effect_to_zero">constrain_mean_effect_to_zero</code></td>
<td>
<p>if <code>TRUE</code>, use a model parameterization
that constrains the mean effect across all seasons to be zero. This
constraint is generally helpful in identifying the contributions of
different model components and can lead to more interpretable
posterior decompositions. It may be undesirable if you plan to directly
examine the latent space of the underlying state space model. Default value: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="sts_seasonal_+3A_name">name</code></td>
<td>
<p>the name of this model component. Default value: 'Seasonal'.</p>
</td></tr>
</table>


<h3>Details</h3>

<div class="sourceCode"><pre>effects[season, occurrence[i]] = (
  effects[season, occurrence[i-1]] + Normal(loc=0., scale=drift_scale))
</pre></div>
<p>The <code>drift_scale</code> parameter governs the standard deviation of the random walk;
for example, in a day-of-week model it governs the change in effect from this
Monday to next Monday.
</p>


<h3>Value</h3>

<p>an instance of <code>StructuralTimeSeries</code>.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+sts_fit_with_hmc">sts_fit_with_hmc()</a></code>, <code><a href="#topic+sts_forecast">sts_forecast()</a></code>, <code><a href="#topic+sts_decompose_by_component">sts_decompose_by_component()</a></code>.
</p>
<p>Other sts: 
<code><a href="#topic+sts_additive_state_space_model">sts_additive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive_state_space_model">sts_autoregressive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive">sts_autoregressive</a>()</code>,
<code><a href="#topic+sts_constrained_seasonal_state_space_model">sts_constrained_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression_state_space_model">sts_dynamic_linear_regression_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression">sts_dynamic_linear_regression</a>()</code>,
<code><a href="#topic+sts_linear_regression">sts_linear_regression</a>()</code>,
<code><a href="#topic+sts_local_level_state_space_model">sts_local_level_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_level">sts_local_level</a>()</code>,
<code><a href="#topic+sts_local_linear_trend_state_space_model">sts_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_linear_trend">sts_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_seasonal_state_space_model">sts_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend_state_space_model">sts_semi_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend">sts_semi_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal_state_space_model">sts_smooth_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal">sts_smooth_seasonal</a>()</code>,
<code><a href="#topic+sts_sparse_linear_regression">sts_sparse_linear_regression</a>()</code>,
<code><a href="#topic+sts_sum">sts_sum</a>()</code>
</p>

<hr>
<h2 id='sts_seasonal_state_space_model'>State space model for a seasonal effect.</h2><span id='topic+sts_seasonal_state_space_model'></span>

<h3>Description</h3>

<p>A state space model (SSM) posits a set of latent (unobserved) variables that
evolve over time with dynamics specified by a probabilistic transition model
<code>p(z[t+1] | z[t])</code>. At each timestep, we observe a value sampled from an
observation model conditioned on the current state, <code>p(x[t] | z[t])</code>. The
special case where both the transition and observation models are Gaussians
with mean specified as a linear function of the inputs, is known as a linear
Gaussian state space model and supports tractable exact probabilistic
calculations; see <code>tfd_linear_gaussian_state_space_model</code> for
details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sts_seasonal_state_space_model(
  num_timesteps,
  num_seasons,
  drift_scale,
  initial_state_prior,
  observation_noise_scale = 0,
  num_steps_per_season = 1,
  initial_step = 0,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_seasonal_state_space_model_+3A_num_timesteps">num_timesteps</code></td>
<td>
<p>Scalar <code>integer</code> <code>tensor</code> number of timesteps to model
with this distribution.</p>
</td></tr>
<tr><td><code id="sts_seasonal_state_space_model_+3A_num_seasons">num_seasons</code></td>
<td>
<p>Scalar <code>integer</code> number of seasons.</p>
</td></tr>
<tr><td><code id="sts_seasonal_state_space_model_+3A_drift_scale">drift_scale</code></td>
<td>
<p>Scalar (any additional dimensions are treated as batch
dimensions) <code>float</code> <code>tensor</code> indicating the standard deviation of the
change in effect between consecutive occurrences of a given season.
This is assumed to be the same for all seasons.</p>
</td></tr>
<tr><td><code id="sts_seasonal_state_space_model_+3A_initial_state_prior">initial_state_prior</code></td>
<td>
<p>instance of <code>tfd_multivariate_normal</code>
representing the prior distribution on latent states; must
have event shape <code style="white-space: pre;">&#8288;[num_seasons]&#8288;</code>.</p>
</td></tr>
<tr><td><code id="sts_seasonal_state_space_model_+3A_observation_noise_scale">observation_noise_scale</code></td>
<td>
<p>Scalar (any additional dimensions are
treated as batch dimensions) <code>float</code> <code>tensor</code> indicating the standard
deviation of the observation noise.</p>
</td></tr>
<tr><td><code id="sts_seasonal_state_space_model_+3A_num_steps_per_season">num_steps_per_season</code></td>
<td>
<p><code>integer</code> number of steps in each
season. This may be either a scalar (shape <code style="white-space: pre;">&#8288;[]&#8288;</code>), in which case all
seasons have the same length, or an array of shape <code style="white-space: pre;">&#8288;[num_seasons]&#8288;</code>,
in which seasons have different length, but remain constant around
different cycles, or an array of shape <code style="white-space: pre;">&#8288;[num_cycles, num_seasons]&#8288;</code>,
in which num_steps_per_season for each season also varies in different
cycle (e.g., a 4 years cycle with leap day). Default value: 1.</p>
</td></tr>
<tr><td><code id="sts_seasonal_state_space_model_+3A_initial_step">initial_step</code></td>
<td>
<p>Optional scalar <code>integer</code> <code>tensor</code> specifying the starting
timestep. Default value: 0.</p>
</td></tr>
<tr><td><code id="sts_seasonal_state_space_model_+3A_validate_args">validate_args</code></td>
<td>
<p><code>logical</code>. Whether to validate input
with asserts. If <code>validate_args</code> is <code>FALSE</code>, and the inputs are
invalid, correct behavior is not guaranteed. Default value: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="sts_seasonal_state_space_model_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p><code>logical</code>. If <code>FALSE</code>, raise an
exception if a statistic (e.g. mean/mode/etc...) is undefined for any
batch member. If <code>TRUE</code>, batch members with valid parameters leading to
undefined statistics will return NaN for this statistic. Default value: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="sts_seasonal_state_space_model_+3A_name">name</code></td>
<td>
<p>string prefixed to ops created by this class.
Default value: &quot;SeasonalStateSpaceModel&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A seasonal effect model is a special case of a linear Gaussian SSM. The
latent states represent an unknown effect from each of several 'seasons';
these are generally not meteorological seasons, but represent regular
recurring patterns such as hour-of-day or day-of-week effects. The effect of
each season drifts from one occurrence to the next, following a Gaussian random walk:
</p>
<div class="sourceCode"><pre>effects[season, occurrence[i]] = (effects[season, occurrence[i-1]] + Normal(loc=0., scale=drift_scale))
</pre></div>
<p>The latent state has dimension <code>num_seasons</code>, containing one effect for each
seasonal component. The parameters <code>drift_scale</code> and
<code>observation_noise_scale</code> are each (a batch of) scalars. The batch shape of
this <code>Distribution</code> is the broadcast batch shape of these parameters and of
the <code>initial_state_prior</code>.
Note: there is no requirement that the effects sum to zero.
</p>
<p>Mathematical Details
</p>
<p>The seasonal effect model implements a <code>tfd_linear_gaussian_state_space_model</code> with
<code>latent_size = num_seasons</code> and <code>observation_size = 1</code>. The latent state
is organized so that the <em>current</em> seasonal effect is always in the first
(zeroth) dimension. The transition model rotates the latent state to shift
to a new effect at the end of each season:
</p>
<div class="sourceCode"><pre>transition_matrix[t] = (permutation_matrix([1, 2, ..., num_seasons-1, 0])
                       if season_is_changing(t)
                       else eye(num_seasons)
transition_noise[t] ~ Normal(loc=0., scale_diag=(
                      [drift_scale, 0, ..., 0]
                      if season_is_changing(t)
                      else [0, 0, ..., 0]))
</pre></div>
<p>where <code>season_is_changing(t)</code> is <code>True</code> if <code style="white-space: pre;">&#8288;t `mod` sum(num_steps_per_season)&#8288;</code> is in
the set of final days for each season, given by <code>cumsum(num_steps_per_season) - 1</code>.
The observation model always picks out the effect for the current season, i.e.,
the first element of the latent state:
</p>
<div class="sourceCode"><pre>observation_matrix = [[1., 0., ..., 0.]]
observation_noise ~ Normal(loc=0, scale=observation_noise_scale)
</pre></div>


<h3>Value</h3>

<p>an instance of <code>LinearGaussianStateSpaceModel</code>.
</p>


<h3>See Also</h3>

<p>Other sts: 
<code><a href="#topic+sts_additive_state_space_model">sts_additive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive_state_space_model">sts_autoregressive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive">sts_autoregressive</a>()</code>,
<code><a href="#topic+sts_constrained_seasonal_state_space_model">sts_constrained_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression_state_space_model">sts_dynamic_linear_regression_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression">sts_dynamic_linear_regression</a>()</code>,
<code><a href="#topic+sts_linear_regression">sts_linear_regression</a>()</code>,
<code><a href="#topic+sts_local_level_state_space_model">sts_local_level_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_level">sts_local_level</a>()</code>,
<code><a href="#topic+sts_local_linear_trend_state_space_model">sts_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_linear_trend">sts_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_seasonal">sts_seasonal</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend_state_space_model">sts_semi_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend">sts_semi_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal_state_space_model">sts_smooth_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal">sts_smooth_seasonal</a>()</code>,
<code><a href="#topic+sts_sparse_linear_regression">sts_sparse_linear_regression</a>()</code>,
<code><a href="#topic+sts_sum">sts_sum</a>()</code>
</p>

<hr>
<h2 id='sts_semi_local_linear_trend'>Formal representation of a semi-local linear trend model.</h2><span id='topic+sts_semi_local_linear_trend'></span>

<h3>Description</h3>

<p>Like the <code>sts_local_linear_trend</code> model, a semi-local linear trend posits a
latent <code>level</code> and <code>slope</code>, with the level component updated according to
the current slope plus a random walk:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sts_semi_local_linear_trend(
  observed_time_series = NULL,
  level_scale_prior = NULL,
  slope_mean_prior = NULL,
  slope_scale_prior = NULL,
  autoregressive_coef_prior = NULL,
  initial_level_prior = NULL,
  initial_slope_prior = NULL,
  constrain_ar_coef_stationary = TRUE,
  constrain_ar_coef_positive = FALSE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_semi_local_linear_trend_+3A_observed_time_series">observed_time_series</code></td>
<td>
<p>optional <code>float</code> <code>tensor</code> of shape
<code style="white-space: pre;">&#8288;batch_shape + [T, 1]&#8288;</code> (omitting the trailing unit dimension is also
supported when <code>T &gt; 1</code>), specifying an observed time series.
Any priors not explicitly set will be given default values according to
the scale of the observed time series (or batch of time series). May
optionally be an instance of <code>sts_masked_time_series</code>, which includes
a mask <code>tensor</code> to specify timesteps with missing observations.
Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_semi_local_linear_trend_+3A_level_scale_prior">level_scale_prior</code></td>
<td>
<p>optional <code>tfp$distribution</code> instance specifying a prior
on the <code>level_scale</code> parameter. If <code>NULL</code>, a heuristic default prior is
constructed based on the provided <code>observed_time_series</code>.
Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_semi_local_linear_trend_+3A_slope_mean_prior">slope_mean_prior</code></td>
<td>
<p>optional <code>tfd$Distribution</code> instance specifying a prior
on the <code>slope_mean</code> parameter. If <code>NULL</code>, a heuristic default prior is
constructed based on the provided <code>observed_time_series</code>. Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_semi_local_linear_trend_+3A_slope_scale_prior">slope_scale_prior</code></td>
<td>
<p>optional <code>tfd$Distribution</code> instance specifying a prior
on the <code>slope_scale</code> parameter. If <code>NULL</code>, a heuristic default prior is
constructed based on the provided <code>observed_time_series</code>. Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_semi_local_linear_trend_+3A_autoregressive_coef_prior">autoregressive_coef_prior</code></td>
<td>
<p>optional <code>tfd$Distribution</code> instance specifying
a prior on the <code>autoregressive_coef</code> parameter. If <code>NULL</code>, the default
prior is a standard <code>Normal(0, 1)</code>. Note that the prior may be
implicitly truncated by <code>constrain_ar_coef_stationary</code> and/or <code>constrain_ar_coef_positive</code>.
Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_semi_local_linear_trend_+3A_initial_level_prior">initial_level_prior</code></td>
<td>
<p>optional <code>tfp$distribution</code> instance specifying a
prior on the initial level. If <code>NULL</code>, a heuristic default prior is
constructed based on the provided <code>observed_time_series</code>.
Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_semi_local_linear_trend_+3A_initial_slope_prior">initial_slope_prior</code></td>
<td>
<p>optional <code>tfd$Distribution</code> instance specifying a
prior on the initial slope. If <code>NULL</code>, a heuristic default prior is
constructed based on the provided <code>observed_time_series</code>. Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_semi_local_linear_trend_+3A_constrain_ar_coef_stationary">constrain_ar_coef_stationary</code></td>
<td>
<p>if <code>TRUE</code>, perform inference using a
parameterization that restricts <code>autoregressive_coef</code> to the interval
<code style="white-space: pre;">&#8288;(-1, 1)&#8288;</code>, or <code style="white-space: pre;">&#8288;(0, 1)&#8288;</code> if <code>force_positive_ar_coef</code> is also <code>TRUE</code>,
corresponding to stationary processes. This will implicitly truncate
the support of <code>autoregressive_coef_prior</code>. Default value: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="sts_semi_local_linear_trend_+3A_constrain_ar_coef_positive">constrain_ar_coef_positive</code></td>
<td>
<p>if <code>TRUE</code>, perform inference using a
parameterization that restricts <code>autoregressive_coef</code> to be positive,
or in <code style="white-space: pre;">&#8288;(0, 1)&#8288;</code> if <code>constrain_ar_coef_stationary</code> is also <code>TRUE</code>. This
will implicitly truncate the support of <code>autoregressive_coef_prior</code>.
Default value: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="sts_semi_local_linear_trend_+3A_name">name</code></td>
<td>
<p>the name of this model component. Default value: 'SemiLocalLinearTrend'.</p>
</td></tr>
</table>


<h3>Details</h3>

<div class="sourceCode"><pre>level[t] = level[t-1] + slope[t-1] + Normal(0., level_scale)
</pre></div>
<p>The slope component in a <code>sts_semi_local_linear_trend</code> model evolves according to
a first-order autoregressive (AR1) process with potentially nonzero mean:
</p>
<div class="sourceCode"><pre>slope[t] = (slope_mean + autoregressive_coef * (slope[t-1] - slope_mean) + Normal(0., slope_scale))
</pre></div>
<p>Unlike the random walk used in <code>LocalLinearTrend</code>, a stationary
AR1 process (coefficient in <code style="white-space: pre;">&#8288;(-1, 1)&#8288;</code>) maintains bounded variance over time,
so a <code>SemiLocalLinearTrend</code> model will often produce more reasonable
uncertainties when forecasting over long timescales.
</p>


<h3>Value</h3>

<p>an instance of <code>StructuralTimeSeries</code>.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+sts_fit_with_hmc">sts_fit_with_hmc()</a></code>, <code><a href="#topic+sts_forecast">sts_forecast()</a></code>, <code><a href="#topic+sts_decompose_by_component">sts_decompose_by_component()</a></code>.
</p>
<p>Other sts: 
<code><a href="#topic+sts_additive_state_space_model">sts_additive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive_state_space_model">sts_autoregressive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive">sts_autoregressive</a>()</code>,
<code><a href="#topic+sts_constrained_seasonal_state_space_model">sts_constrained_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression_state_space_model">sts_dynamic_linear_regression_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression">sts_dynamic_linear_regression</a>()</code>,
<code><a href="#topic+sts_linear_regression">sts_linear_regression</a>()</code>,
<code><a href="#topic+sts_local_level_state_space_model">sts_local_level_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_level">sts_local_level</a>()</code>,
<code><a href="#topic+sts_local_linear_trend_state_space_model">sts_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_linear_trend">sts_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_seasonal_state_space_model">sts_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_seasonal">sts_seasonal</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend_state_space_model">sts_semi_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal_state_space_model">sts_smooth_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal">sts_smooth_seasonal</a>()</code>,
<code><a href="#topic+sts_sparse_linear_regression">sts_sparse_linear_regression</a>()</code>,
<code><a href="#topic+sts_sum">sts_sum</a>()</code>
</p>

<hr>
<h2 id='sts_semi_local_linear_trend_state_space_model'>State space model for a semi-local linear trend.</h2><span id='topic+sts_semi_local_linear_trend_state_space_model'></span>

<h3>Description</h3>

<p>A state space model (SSM) posits a set of latent (unobserved) variables that
evolve over time with dynamics specified by a probabilistic transition model
<code>p(z[t+1] | z[t])</code>. At each timestep, we observe a value sampled from an
observation model conditioned on the current state, <code>p(x[t] | z[t])</code>. The
special case where both the transition and observation models are Gaussians
with mean specified as a linear function of the inputs, is known as a linear
Gaussian state space model and supports tractable exact probabilistic
calculations; see <code>tfd_linear_gaussian_state_space_model</code> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sts_semi_local_linear_trend_state_space_model(
  num_timesteps,
  level_scale,
  slope_mean,
  slope_scale,
  autoregressive_coef,
  initial_state_prior,
  observation_noise_scale = 0,
  initial_step = 0,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_semi_local_linear_trend_state_space_model_+3A_num_timesteps">num_timesteps</code></td>
<td>
<p>Scalar <code>integer</code> <code>tensor</code> number of timesteps to model
with this distribution.</p>
</td></tr>
<tr><td><code id="sts_semi_local_linear_trend_state_space_model_+3A_level_scale">level_scale</code></td>
<td>
<p>Scalar (any additional dimensions are treated as batch
dimensions) <code>float</code> <code>tensor</code> indicating the standard deviation of the
level transitions.</p>
</td></tr>
<tr><td><code id="sts_semi_local_linear_trend_state_space_model_+3A_slope_mean">slope_mean</code></td>
<td>
<p>Scalar (any additional dimensions are treated as batch
dimensions) <code>float</code> <code>tensor</code> indicating the expected long-term mean of
the latent slope.</p>
</td></tr>
<tr><td><code id="sts_semi_local_linear_trend_state_space_model_+3A_slope_scale">slope_scale</code></td>
<td>
<p>Scalar (any additional dimensions are treated as batch
dimensions) <code>float</code> <code>tensor</code> indicating the standard deviation of the
slope transitions.</p>
</td></tr>
<tr><td><code id="sts_semi_local_linear_trend_state_space_model_+3A_autoregressive_coef">autoregressive_coef</code></td>
<td>
<p>Scalar (any additional dimensions are treated as
batch dimensions) <code>float</code> <code>tensor</code> defining the AR1 process on the latent slope.</p>
</td></tr>
<tr><td><code id="sts_semi_local_linear_trend_state_space_model_+3A_initial_state_prior">initial_state_prior</code></td>
<td>
<p>instance of <code>tfd_multivariate_normal</code>
representing the prior distribution on latent states.  Must have
event shape <code style="white-space: pre;">&#8288;[1]&#8288;</code> (as <code>tfd_linear_gaussian_state_space_model</code> requires a
rank-1 event shape).</p>
</td></tr>
<tr><td><code id="sts_semi_local_linear_trend_state_space_model_+3A_observation_noise_scale">observation_noise_scale</code></td>
<td>
<p>Scalar (any additional dimensions are
treated as batch dimensions) <code>float</code> <code>tensor</code> indicating the standard
deviation of the observation noise.</p>
</td></tr>
<tr><td><code id="sts_semi_local_linear_trend_state_space_model_+3A_initial_step">initial_step</code></td>
<td>
<p>Optional scalar <code>integer</code> <code>tensor</code> specifying the starting
timestep. Default value: 0.</p>
</td></tr>
<tr><td><code id="sts_semi_local_linear_trend_state_space_model_+3A_validate_args">validate_args</code></td>
<td>
<p><code>logical</code>. Whether to validate input
with asserts. If <code>validate_args</code> is <code>FALSE</code>, and the inputs are
invalid, correct behavior is not guaranteed. Default value: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="sts_semi_local_linear_trend_state_space_model_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p><code>logical</code>. If <code>FALSE</code>, raise an
exception if a statistic (e.g. mean/mode/etc...) is undefined for any
batch member. If <code>TRUE</code>, batch members with valid parameters leading to
undefined statistics will return NaN for this statistic. Default value: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="sts_semi_local_linear_trend_state_space_model_+3A_name">name</code></td>
<td>
<p>string' prefixed to ops created by this class.
Default value: &quot;SemiLocalLinearTrendStateSpaceModel&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The semi-local linear trend model is a special case of a linear Gaussian
SSM, in which the latent state posits a <code>level</code> and <code>slope</code>. The <code>level</code>
evolves via a Gaussian random walk centered at the current <code>slope</code>, while
the <code>slope</code> follows a first-order autoregressive (AR1) process with
mean <code>slope_mean</code>:
</p>
<div class="sourceCode"><pre>level[t] = level[t-1] + slope[t-1] + Normal(0, level_scale)
slope[t] = (slope_mean + autoregressive_coef * (slope[t-1] - slope_mean) +
           Normal(0., slope_scale))
</pre></div>
<p>The latent state is the two-dimensional tuple <code style="white-space: pre;">&#8288;[level, slope]&#8288;</code>. The
<code>level</code> is observed at each timestep.
The parameters <code>level_scale</code>, <code>slope_mean</code>, <code>slope_scale</code>,
<code>autoregressive_coef</code>, and <code>observation_noise_scale</code> are each (a batch of)
scalars. The batch shape of this <code>Distribution</code> is the broadcast batch shape
of these parameters and of the <code>initial_state_prior</code>.
</p>
<p>Mathematical Details
</p>
<p>The semi-local linear trend model implements a
<code>tfp.distributions.LinearGaussianStateSpaceModel</code> with <code>latent_size = 2</code>
and <code>observation_size = 1</code>, following the transition model:
</p>
<div class="sourceCode"><pre>transition_matrix = [[1., 1.]
                     [0., autoregressive_coef]]
transition_noise ~ N(loc=slope_mean - autoregressive_coef * slope_mean,
                     scale=diag([level_scale, slope_scale]))
</pre></div>
<p>which implements the evolution of <code style="white-space: pre;">&#8288;[level, slope]&#8288;</code> described above, and
the observation model:
</p>
<div class="sourceCode"><pre>observation_matrix = [[1., 0.]]
observation_noise ~ N(loc=0, scale=observation_noise_scale)
</pre></div>
<p>which picks out the first latent component, i.e., the <code>level</code>, as the
observation at each timestep.
</p>


<h3>Value</h3>

<p>an instance of <code>LinearGaussianStateSpaceModel</code>.
</p>


<h3>See Also</h3>

<p>Other sts: 
<code><a href="#topic+sts_additive_state_space_model">sts_additive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive_state_space_model">sts_autoregressive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive">sts_autoregressive</a>()</code>,
<code><a href="#topic+sts_constrained_seasonal_state_space_model">sts_constrained_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression_state_space_model">sts_dynamic_linear_regression_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression">sts_dynamic_linear_regression</a>()</code>,
<code><a href="#topic+sts_linear_regression">sts_linear_regression</a>()</code>,
<code><a href="#topic+sts_local_level_state_space_model">sts_local_level_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_level">sts_local_level</a>()</code>,
<code><a href="#topic+sts_local_linear_trend_state_space_model">sts_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_linear_trend">sts_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_seasonal_state_space_model">sts_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_seasonal">sts_seasonal</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend">sts_semi_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal_state_space_model">sts_smooth_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal">sts_smooth_seasonal</a>()</code>,
<code><a href="#topic+sts_sparse_linear_regression">sts_sparse_linear_regression</a>()</code>,
<code><a href="#topic+sts_sum">sts_sum</a>()</code>
</p>

<hr>
<h2 id='sts_smooth_seasonal'>Formal representation of a smooth seasonal effect model</h2><span id='topic+sts_smooth_seasonal'></span>

<h3>Description</h3>

<p>The smooth seasonal model uses a set of trigonometric terms in order to
capture a recurring pattern whereby adjacent (in time) effects are
similar. The model uses <code>frequencies</code> calculated via:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sts_smooth_seasonal(
  period,
  frequency_multipliers,
  allow_drift = TRUE,
  drift_scale_prior = NULL,
  initial_state_prior = NULL,
  observed_time_series = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_smooth_seasonal_+3A_period">period</code></td>
<td>
<p>positive scalar <code>float</code> <code>Tensor</code> giving the number of timesteps
required for the longest cyclic effect to repeat.</p>
</td></tr>
<tr><td><code id="sts_smooth_seasonal_+3A_frequency_multipliers">frequency_multipliers</code></td>
<td>
<p>One-dimensional <code>float</code> <code>Tensor</code> listing the
frequencies (cyclic components) included in the model, as multipliers of
the base/fundamental frequency <code>2. * pi / period</code>. Each component is
specified by the number of times it repeats per period, and adds two
latent dimensions to the model. A smooth seasonal model that can
represent any periodic function is given by
<code style="white-space: pre;">&#8288;frequency_multipliers = [1,2, ..., floor(period / 2)]&#8288;</code>.
However, it is often desirable to enforce a
smoothness assumption (and reduce the computational burden) by dropping
some of the higher frequencies.</p>
</td></tr>
<tr><td><code id="sts_smooth_seasonal_+3A_allow_drift">allow_drift</code></td>
<td>
<p>optional <code>logical</code> specifying whether the seasonal
effects can drift over time.  Setting this to <code>FALSE</code>
removes the <code>drift_scale</code> parameter from the model. This is
mathematically equivalent to
<code>drift_scale_prior = tfd.Deterministic(0.)</code>, but removing drift
directly is preferred because it avoids the use of a degenerate prior.
Default value: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="sts_smooth_seasonal_+3A_drift_scale_prior">drift_scale_prior</code></td>
<td>
<p>optional <code>tfd$Distribution</code> instance specifying a prior
on the <code>drift_scale</code> parameter. If <code>NULL</code>, a heuristic default prior is
constructed based on the provided <code>observed_time_series</code>.
Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_smooth_seasonal_+3A_initial_state_prior">initial_state_prior</code></td>
<td>
<p>instance of <code>tfd$MultivariateNormal</code> representing
the prior distribution on the latent states. Must have event shape
<code style="white-space: pre;">&#8288;[2 * len(frequency_multipliers)]&#8288;</code>. If <code>NULL</code>, a heuristic default prior
is constructed based on the provided <code>observed_time_series</code>.</p>
</td></tr>
<tr><td><code id="sts_smooth_seasonal_+3A_observed_time_series">observed_time_series</code></td>
<td>
<p>optional <code>float</code> <code>Tensor</code> of shape
<code style="white-space: pre;">&#8288;batch_shape + [T, 1]&#8288;</code> (omitting the trailing unit dimension is also
supported when <code>T &gt; 1</code>), specifying an observed time series.
Any priors not explicitly set will be given default values according to
the scale of the observed time series (or batch of time series). May
optionally be an instance of <code>tfp$sts$MaskedTimeSeries</code>, which includes
a mask <code>Tensor</code> to specify timesteps with missing observations.
Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_smooth_seasonal_+3A_name">name</code></td>
<td>
<p>the name of this model component. Default value: 'LocalLinearTrend'.</p>
</td></tr>
</table>


<h3>Details</h3>

<div class="sourceCode"><pre>frequencies[j] = 2. * pi * frequency_multipliers[j] / period
</pre></div>
<p>and then posits two latent states for each <code>frequency</code>. The two latent states
associated with frequency <code>j</code> drift over time via:
</p>
<div class="sourceCode"><pre>effect[t] = (effect[t-1] * cos(frequencies[j]) +
             auxiliary[t-] * sin(frequencies[j]) +
             Normal(0., drift_scale))
auxiliary[t] = (-effect[t-1] * sin(frequencies[j]) +
                auxiliary[t-] * cos(frequencies[j]) +
                Normal(0., drift_scale))
</pre></div>
<p>where <code>effect</code> is the smooth seasonal effect and <code>auxiliary</code> only appears as a
matter of construction. The interpretation of <code>auxiliary</code> is thus not
particularly important.
</p>


<h3>Value</h3>

<p>an instance of <code>StructuralTimeSeries</code>.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+sts_fit_with_hmc">sts_fit_with_hmc()</a></code>, <code><a href="#topic+sts_forecast">sts_forecast()</a></code>, <code><a href="#topic+sts_decompose_by_component">sts_decompose_by_component()</a></code>.
</p>
<p>Other sts: 
<code><a href="#topic+sts_additive_state_space_model">sts_additive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive_state_space_model">sts_autoregressive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive">sts_autoregressive</a>()</code>,
<code><a href="#topic+sts_constrained_seasonal_state_space_model">sts_constrained_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression_state_space_model">sts_dynamic_linear_regression_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression">sts_dynamic_linear_regression</a>()</code>,
<code><a href="#topic+sts_linear_regression">sts_linear_regression</a>()</code>,
<code><a href="#topic+sts_local_level_state_space_model">sts_local_level_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_level">sts_local_level</a>()</code>,
<code><a href="#topic+sts_local_linear_trend_state_space_model">sts_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_linear_trend">sts_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_seasonal_state_space_model">sts_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_seasonal">sts_seasonal</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend_state_space_model">sts_semi_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend">sts_semi_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal_state_space_model">sts_smooth_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_sparse_linear_regression">sts_sparse_linear_regression</a>()</code>,
<code><a href="#topic+sts_sum">sts_sum</a>()</code>
</p>

<hr>
<h2 id='sts_smooth_seasonal_state_space_model'>State space model for a smooth seasonal effect</h2><span id='topic+sts_smooth_seasonal_state_space_model'></span>

<h3>Description</h3>

<p>A state space model (SSM) posits a set of latent (unobserved) variables that
evolve over time with dynamics specified by a probabilistic transition model
<code>p(z[t+1] | z[t])</code>. At each timestep, we observe a value sampled from an
observation model conditioned on the current state, <code>p(x[t] | z[t])</code>. The
special case where both the transition and observation models are Gaussians
with mean specified as a linear function of the inputs, is known as a linear
Gaussian state space model and supports tractable exact probabilistic
calculations; see <code>tfp$distributions$LinearGaussianStateSpaceModel</code> for
details.
A smooth seasonal effect model is a special case of a linear Gaussian SSM. It
is the sum of a set of &quot;cyclic&quot; components, with one component for each
frequency:
</p>
<div class="sourceCode"><pre>frequencies[j] = 2. * pi * frequency_multipliers[j] / period
</pre></div>
<p>Each cyclic component contains two latent states which we denote <code>effect</code> and
<code>auxiliary</code>. The two latent states for component <code>j</code> drift over time via:
</p>
<div class="sourceCode"><pre>effect[t] = (effect[t-1] * cos(frequencies[j]) +
             auxiliary[t-] * sin(frequencies[j]) +
             Normal(0., drift_scale))
auxiliary[t] = (-effect[t-1] * sin(frequencies[j]) +
                auxiliary[t-] * cos(frequencies[j]) +
                Normal(0., drift_scale))
</pre></div>


<h3>Usage</h3>

<pre><code class='language-R'>sts_smooth_seasonal_state_space_model(
  num_timesteps,
  period,
  frequency_multipliers,
  drift_scale,
  initial_state_prior,
  observation_noise_scale = 0,
  initial_step = 0,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_smooth_seasonal_state_space_model_+3A_num_timesteps">num_timesteps</code></td>
<td>
<p>Scalar <code>integer</code> <code>Tensor</code> number of timesteps to model
with this distribution.</p>
</td></tr>
<tr><td><code id="sts_smooth_seasonal_state_space_model_+3A_period">period</code></td>
<td>
<p>positive scalar <code>float</code> <code>Tensor</code> giving the number of timesteps
required for the longest cyclic effect to repeat.</p>
</td></tr>
<tr><td><code id="sts_smooth_seasonal_state_space_model_+3A_frequency_multipliers">frequency_multipliers</code></td>
<td>
<p>One-dimensional <code>float</code> <code>Tensor</code> listing the
frequencies (cyclic components) included in the model, as multipliers of
the base/fundamental frequency <code>2. * pi / period</code>. Each component is
specified by the number of times it repeats per period, and adds two
latent dimensions to the model. A smooth seasonal model that can
represent any periodic function is given by
<code style="white-space: pre;">&#8288;frequency_multipliers = [1,2, ..., floor(period / 2)]&#8288;</code>.
However, it is often desirable to enforce a
smoothness assumption (and reduce the computational burden) by dropping
some of the higher frequencies.</p>
</td></tr>
<tr><td><code id="sts_smooth_seasonal_state_space_model_+3A_drift_scale">drift_scale</code></td>
<td>
<p>Scalar (any additional dimensions are treated as batch
dimensions) <code>float</code> <code>Tensor</code> indicating the standard deviation of the
latent state transitions.</p>
</td></tr>
<tr><td><code id="sts_smooth_seasonal_state_space_model_+3A_initial_state_prior">initial_state_prior</code></td>
<td>
<p>instance of <code>tfd$MultivariateNormal</code>
representing the prior distribution on latent states.  Must have
event shape <code style="white-space: pre;">&#8288;[num_features]&#8288;</code>.</p>
</td></tr>
<tr><td><code id="sts_smooth_seasonal_state_space_model_+3A_observation_noise_scale">observation_noise_scale</code></td>
<td>
<p>Scalar (any additional dimensions are
treated as batch dimensions) <code>float</code> <code>Tensor</code> indicating the standard
deviation of the observation noise. Default value: <code>0.</code>.</p>
</td></tr>
<tr><td><code id="sts_smooth_seasonal_state_space_model_+3A_initial_step">initial_step</code></td>
<td>
<p>scalar <code>integer</code> <code>Tensor</code> specifying the starting timestep.
Default value: <code>0</code>.</p>
</td></tr>
<tr><td><code id="sts_smooth_seasonal_state_space_model_+3A_validate_args">validate_args</code></td>
<td>
<p><code>logical</code>. Whether to validate input
with asserts. If <code>validate_args</code> is <code>FALSE</code>, and the inputs are
invalid, correct behavior is not guaranteed. Default value: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="sts_smooth_seasonal_state_space_model_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p><code>logical</code>. If <code>FALSE</code>, raise an
exception if a statistic (e.g. mean/mode/etc...) is undefined for any
batch member. If <code>TRUE</code>, batch members with valid parameters leading to
undefined statistics will return NaN for this statistic. Default value: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="sts_smooth_seasonal_state_space_model_+3A_name">name</code></td>
<td>
<p>string prefixed to ops created by this class.
Default value: &quot;LocalLinearTrendStateSpaceModel&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>auxiliary</code> latent state only appears as a matter of construction and thus
its interpretation is not particularly important. The total smooth seasonal
effect is the sum of the <code>effect</code> values from each of the cyclic components.
The parameters <code>drift_scale</code> and <code>observation_noise_scale</code> are each (a batch
of) scalars. The batch shape of this <code>Distribution</code> is the broadcast batch
shape of these parameters and of the <code>initial_state_prior</code>.
</p>
<p>Mathematical Details
</p>
<p>The smooth seasonal effect model implements a
<code>tfp$distributions$LinearGaussianStateSpaceModel</code> with
<code>latent_size = 2 * len(frequency_multipliers)</code> and <code>observation_size = 1</code>.
The latent state is the concatenation of the cyclic latent states which themselves
comprise an <code>effect</code> and an <code>auxiliary</code> state. The transition matrix is a block diagonal
matrix where block <code>j</code> is:
</p>
<div class="sourceCode"><pre>transition_matrix[j] =  [[cos(frequencies[j]), sin(frequencies[j])],
                         [-sin(frequencies[j]), cos(frequencies[j])]]
</pre></div>
<p>The observation model picks out the cyclic <code>effect</code> values from the latent state:
</p>
<div class="sourceCode"><pre>observation_matrix = [[1., 0., 1., 0., ..., 1., 0.]]
observation_noise ~ Normal(loc=0, scale=observation_noise_scale)
</pre></div>
<p>For further mathematical details please see Harvey (1990).
</p>


<h3>Value</h3>

<p>an instance of <code>LinearGaussianStateSpaceModel</code>.
</p>


<h3>references</h3>


<ul>
<li><p> Harvey, A. Forecasting, Structural Time Series Models and the Kalman Filter. Cambridge: Cambridge University Press, 1990.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other sts: 
<code><a href="#topic+sts_additive_state_space_model">sts_additive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive_state_space_model">sts_autoregressive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive">sts_autoregressive</a>()</code>,
<code><a href="#topic+sts_constrained_seasonal_state_space_model">sts_constrained_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression_state_space_model">sts_dynamic_linear_regression_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression">sts_dynamic_linear_regression</a>()</code>,
<code><a href="#topic+sts_linear_regression">sts_linear_regression</a>()</code>,
<code><a href="#topic+sts_local_level_state_space_model">sts_local_level_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_level">sts_local_level</a>()</code>,
<code><a href="#topic+sts_local_linear_trend_state_space_model">sts_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_linear_trend">sts_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_seasonal_state_space_model">sts_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_seasonal">sts_seasonal</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend_state_space_model">sts_semi_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend">sts_semi_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal">sts_smooth_seasonal</a>()</code>,
<code><a href="#topic+sts_sparse_linear_regression">sts_sparse_linear_regression</a>()</code>,
<code><a href="#topic+sts_sum">sts_sum</a>()</code>
</p>

<hr>
<h2 id='sts_sparse_linear_regression'>Formal representation of a sparse linear regression.</h2><span id='topic+sts_sparse_linear_regression'></span>

<h3>Description</h3>

<p>This model defines a time series given by a sparse linear combination of
covariate time series provided in a design matrix:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sts_sparse_linear_regression(
  design_matrix,
  weights_prior_scale = 0.1,
  weights_batch_shape = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_sparse_linear_regression_+3A_design_matrix">design_matrix</code></td>
<td>
<p>float <code>tensor</code> of shape <code>tf$concat(list(batch_shape, list(num_timesteps, num_features)))</code>.
This may also optionally be an instance of <code>tf$linalg$LinearOperator</code>.</p>
</td></tr>
<tr><td><code id="sts_sparse_linear_regression_+3A_weights_prior_scale">weights_prior_scale</code></td>
<td>
<p>float <code>Tensor</code> defining the scale of the Horseshoe
prior on regression weights. Small values encourage the weights to be
sparse. The shape must broadcast with <code>weights_batch_shape</code>.
Default value: <code>0.1</code>.</p>
</td></tr>
<tr><td><code id="sts_sparse_linear_regression_+3A_weights_batch_shape">weights_batch_shape</code></td>
<td>
<p>if <code>NULL</code>, defaults to
<code>design_matrix.batch_shape_tensor()</code>. Must broadcast with the batch
shape of <code>design_matrix</code>. Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_sparse_linear_regression_+3A_name">name</code></td>
<td>
<p>the name of this model component. Default value: 'LinearRegression'.</p>
</td></tr>
</table>


<h3>Details</h3>

<div class="sourceCode"><pre>observed_time_series &lt;- tf$matmul(design_matrix, weights)
</pre></div>
<p>This is identical to <code>sts_linear_regression</code>, except that
<code>sts_sparse_linear_regression</code> uses a parameterization of a Horseshoe
prior to encode the assumption that many of the <code>weights</code> are zero,
i.e., many of the covariate time series are irrelevant. See the mathematical
details section below for further discussion. The prior parameterization used
by <code>sts_sparse_linear_regression</code> is more suitable for inference than that
obtained by simply passing the equivalent <code>tfd_horseshoe</code> prior to
<code>sts_linear_regression</code>; when sparsity is desired, <code>sts_sparse_linear_regression</code> will
likely yield better results.
</p>
<p>This component does not itself include observation noise; it defines a
deterministic distribution with mass at the point
<code>tf$matmul(design_matrix, weights)</code>. In practice, it should be combined with
observation noise from another component such as <code>sts_sum</code>.
</p>
<p>Mathematical Details
</p>
<p>The basic horseshoe prior Carvalho et al. (2009) is defined as a Cauchy-normal scale mixture:
</p>
<div class="sourceCode"><pre>scales[i] ~ HalfCauchy(loc=0, scale=1)
weights[i] ~ Normal(loc=0., scale=scales[i] * global_scale)`
</pre></div>
<p>The Cauchy scale parameters puts substantial mass near zero, encouraging
weights to be sparse, but their heavy tails allow weights far from zero to be
estimated without excessive shrinkage. The horseshoe can be thought of as a
continuous relaxation of a traditional 'spike-and-slab' discrete sparsity
prior, in which the latent Cauchy scale mixes between 'spike'
(<code style="white-space: pre;">&#8288;scales[i] ~= 0&#8288;</code>) and 'slab' (<code style="white-space: pre;">&#8288;scales[i] &gt;&gt; 0&#8288;</code>) regimes.
</p>
<p>Following the recommendations in Piironen et al. (2017), <code>SparseLinearRegression</code> implements
a horseshoe with the following adaptations:
</p>

<ul>
<li><p> The Cauchy prior on <code>scales[i]</code> is represented as an InverseGamma-Normal
compound.
</p>
</li>
<li><p> The <code>global_scale</code> parameter is integrated out following a <code>Cauchy(0., scale=weights_prior_scale)</code> hyperprior, which is also represented as an
InverseGamma-Normal compound.
</p>
</li>
<li><p> All compound distributions are implemented using a non-centered
parameterization.
The compound, non-centered representation defines the same marginal prior as
the original horseshoe (up to integrating out the global scale),
but allows samplers to mix more efficiently through the heavy tails; for
variational inference, the compound representation implicity expands the
representational power of the variational model.
</p>
</li></ul>

<p>Note that we do not yet implement the regularized ('Finnish') horseshoe,
proposed in Piironen et al. (2017) for models with weak likelihoods, because the likelihood
in STS models is typically Gaussian, where it's not clear that additional
regularization is appropriate. If you need this functionality, please
email tfprobability@tensorflow.org.
</p>
<p>The full prior parameterization implemented in <code>SparseLinearRegression</code> is
as follows:
</p>
<div class="sourceCode"><pre>Sample global_scale from Cauchy(0, scale=weights_prior_scale).
global_scale_variance ~ InverseGamma(alpha=0.5, beta=0.5)
global_scale_noncentered ~ HalfNormal(loc=0, scale=1)
global_scale = (global_scale_noncentered *
sqrt(global_scale_variance) *
weights_prior_scale)
Sample local_scales from Cauchy(0, 1).
local_scale_variances[i] ~ InverseGamma(alpha=0.5, beta=0.5)
local_scales_noncentered[i] ~ HalfNormal(loc=0, scale=1)
local_scales[i] = local_scales_noncentered[i] * sqrt(local_scale_variances[i])
weights[i] ~ Normal(loc=0., scale=local_scales[i] * global_scale)
</pre></div>


<h3>Value</h3>

<p>an instance of <code>StructuralTimeSeries</code>.
</p>


<h3>References</h3>


<ul>
<li> <p><a href="http://proceedings.mlr.press/v5/carvalho09a/carvalho09a.pdf">Carvalho, C., Polson, N. and Scott, J. Handling Sparsity via the Horseshoe. AISTATS (2009).</a>
</p>
</li>
<li> <p><a href="https://arxiv.org/abs/1707.01694">Juho Piironen, Aki Vehtari. Sparsity information and regularization in the horseshoe and other shrinkage priors (2017).</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+sts_fit_with_hmc">sts_fit_with_hmc()</a></code>, <code><a href="#topic+sts_forecast">sts_forecast()</a></code>, <code><a href="#topic+sts_decompose_by_component">sts_decompose_by_component()</a></code>.
</p>
<p>Other sts: 
<code><a href="#topic+sts_additive_state_space_model">sts_additive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive_state_space_model">sts_autoregressive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive">sts_autoregressive</a>()</code>,
<code><a href="#topic+sts_constrained_seasonal_state_space_model">sts_constrained_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression_state_space_model">sts_dynamic_linear_regression_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression">sts_dynamic_linear_regression</a>()</code>,
<code><a href="#topic+sts_linear_regression">sts_linear_regression</a>()</code>,
<code><a href="#topic+sts_local_level_state_space_model">sts_local_level_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_level">sts_local_level</a>()</code>,
<code><a href="#topic+sts_local_linear_trend_state_space_model">sts_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_linear_trend">sts_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_seasonal_state_space_model">sts_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_seasonal">sts_seasonal</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend_state_space_model">sts_semi_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend">sts_semi_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal_state_space_model">sts_smooth_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal">sts_smooth_seasonal</a>()</code>,
<code><a href="#topic+sts_sum">sts_sum</a>()</code>
</p>

<hr>
<h2 id='sts_sum'>Sum of structural time series components.</h2><span id='topic+sts_sum'></span>

<h3>Description</h3>

<p>This class enables compositional specification of a structural time series
model from basic components. Given a list of component models, it represents
an additive model, i.e., a model of time series that may be decomposed into a
sum of terms corresponding to the component models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sts_sum(
  observed_time_series = NULL,
  components,
  constant_offset = NULL,
  observation_noise_scale_prior = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sts_sum_+3A_observed_time_series">observed_time_series</code></td>
<td>
<p>optional <code>float</code> <code>tensor</code> of shape
<code style="white-space: pre;">&#8288;batch_shape + [T, 1]&#8288;</code> (omitting the trailing unit dimension is also
supported when <code>T &gt; 1</code>), specifying an observed time series.
Any priors not explicitly set will be given default values according to
the scale of the observed time series (or batch of time series). May
optionally be an instance of <code>sts_masked_time_series</code>, which includes
a mask <code>tensor</code> to specify timesteps with missing observations.
Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_sum_+3A_components">components</code></td>
<td>
<p><code>list</code> of one or more StructuralTimeSeries instances.
These must have unique names.</p>
</td></tr>
<tr><td><code id="sts_sum_+3A_constant_offset">constant_offset</code></td>
<td>
<p>optional scalar <code>float</code> <code>tensor</code>, or batch of scalars,
specifying a constant value added to the sum of outputs from the
component models. This allows the components to model the shifted series
<code>observed_time_series - constant_offset</code>. If <code>NULL</code>, this is set to the
mean of the provided <code>observed_time_series</code>. Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_sum_+3A_observation_noise_scale_prior">observation_noise_scale_prior</code></td>
<td>
<p>optional <code>tfd$Distribution</code> instance
specifying a prior on <code>observation_noise_scale</code>. If <code>NULL</code>, a heuristic
default prior is constructed based on the provided
<code>observed_time_series</code>. Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="sts_sum_+3A_name">name</code></td>
<td>
<p>string name of this model component; used as <code>name_scope</code>
for ops created by this class. Default value: 'Sum'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Formally, the additive model represents a random process
<code>g[t] = f1[t] + f2[t] + ... + fN[t] + eps[t]</code>, where the <code>f</code>'s are the
random processes represented by the components, and
<code>eps[t] ~ Normal(loc=0, scale=observation_noise_scale)</code> is an observation
noise term. See the <code>AdditiveStateSpaceModel</code> documentation for mathematical details.
</p>
<p>This model inherits the parameters (with priors) of its components, and
adds an <code>observation_noise_scale</code> parameter governing the level of noise in
the observed time series.
</p>


<h3>Value</h3>

<p>an instance of <code>StructuralTimeSeries</code>.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+sts_fit_with_hmc">sts_fit_with_hmc()</a></code>, <code><a href="#topic+sts_forecast">sts_forecast()</a></code>, <code><a href="#topic+sts_decompose_by_component">sts_decompose_by_component()</a></code>.
</p>
<p>Other sts: 
<code><a href="#topic+sts_additive_state_space_model">sts_additive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive_state_space_model">sts_autoregressive_state_space_model</a>()</code>,
<code><a href="#topic+sts_autoregressive">sts_autoregressive</a>()</code>,
<code><a href="#topic+sts_constrained_seasonal_state_space_model">sts_constrained_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression_state_space_model">sts_dynamic_linear_regression_state_space_model</a>()</code>,
<code><a href="#topic+sts_dynamic_linear_regression">sts_dynamic_linear_regression</a>()</code>,
<code><a href="#topic+sts_linear_regression">sts_linear_regression</a>()</code>,
<code><a href="#topic+sts_local_level_state_space_model">sts_local_level_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_level">sts_local_level</a>()</code>,
<code><a href="#topic+sts_local_linear_trend_state_space_model">sts_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_local_linear_trend">sts_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_seasonal_state_space_model">sts_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_seasonal">sts_seasonal</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend_state_space_model">sts_semi_local_linear_trend_state_space_model</a>()</code>,
<code><a href="#topic+sts_semi_local_linear_trend">sts_semi_local_linear_trend</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal_state_space_model">sts_smooth_seasonal_state_space_model</a>()</code>,
<code><a href="#topic+sts_smooth_seasonal">sts_smooth_seasonal</a>()</code>,
<code><a href="#topic+sts_sparse_linear_regression">sts_sparse_linear_regression</a>()</code>
</p>

<hr>
<h2 id='tfb_absolute_value'>Computes<code>Y = g(X) = Abs(X)</code>, element-wise</h2><span id='topic+tfb_absolute_value'></span>

<h3>Description</h3>

<p>This non-injective bijector allows for transformations of scalar distributions
with the absolute value function, which maps <code style="white-space: pre;">&#8288;(-inf, inf)&#8288;</code> to <code style="white-space: pre;">&#8288;[0, inf)&#8288;</code>.
</p>

<ul>
<li><p> For <code>y</code> in <code style="white-space: pre;">&#8288;(0, inf)&#8288;</code>, <code>tfb_absolute_value$inverse(y)</code> returns the set inverse
<code style="white-space: pre;">&#8288;{x in (-inf, inf) : |x| = y}&#8288;</code> as a tuple, <code style="white-space: pre;">&#8288;-y, y&#8288;</code>.
<code>tfb_absolute_value$inverse(0)</code> returns <code style="white-space: pre;">&#8288;0, 0&#8288;</code>, which is not the set inverse
(the set inverse is the singleton <code>{0}</code>), but &quot;works&quot; in conjunction with
<code>TransformedDistribution</code> to produce a left semi-continuous pdf.
For <code>y &lt; 0</code>, <code>tfb_absolute_value$inverse(y)</code> happily returns the wrong thing, <code style="white-space: pre;">&#8288;-y, y&#8288;</code>
This is done for efficiency.  If <code>validate_args == TRUE</code>, <code>y &lt; 0</code> will raise an exception.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>tfb_absolute_value(validate_args = FALSE, name = "absolute_value")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_absolute_value_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_absolute_value_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_affine'>Affine bijector</h2><span id='topic+tfb_affine'></span>

<h3>Description</h3>

<p>This Bijector is initialized with shift Tensor and scale arguments,
giving the forward operation: <code>Y = g(X) = scale @ X + shift</code>
where the scale term is logically equivalent to:
<code style="white-space: pre;">&#8288;scale = scale_identity_multiplier * tf.diag(tf.ones(d)) + tf.diag(scale_diag) + scale_tril + scale_perturb_factor @ diag(scale_perturb_diag) @ tf.transpose([scale_perturb_factor]))&#8288;</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_affine(
  shift = NULL,
  scale_identity_multiplier = NULL,
  scale_diag = NULL,
  scale_tril = NULL,
  scale_perturb_factor = NULL,
  scale_perturb_diag = NULL,
  adjoint = FALSE,
  validate_args = FALSE,
  name = "affine",
  dtype = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_affine_+3A_shift">shift</code></td>
<td>
<p>Floating-point Tensor. If this is set to NULL, no shift is applied.</p>
</td></tr>
<tr><td><code id="tfb_affine_+3A_scale_identity_multiplier">scale_identity_multiplier</code></td>
<td>
<p>floating point rank 0 Tensor representing a scaling done
to the identity matrix. When <code>scale_identity_multiplier = scale_diag = scale_tril = NULL</code> then
<code style="white-space: pre;">&#8288;scale += IdentityMatrix&#8288;</code>. Otherwise no scaled-identity-matrix is added to <code>scale</code>.</p>
</td></tr>
<tr><td><code id="tfb_affine_+3A_scale_diag">scale_diag</code></td>
<td>
<p>Floating-point Tensor representing the diagonal matrix.
<code>scale_diag</code> has shape <code style="white-space: pre;">&#8288;[N1, N2, ...  k]&#8288;</code>, which represents a k x k diagonal matrix.
When NULL no diagonal term is added to <code>scale</code>.</p>
</td></tr>
<tr><td><code id="tfb_affine_+3A_scale_tril">scale_tril</code></td>
<td>
<p>Floating-point Tensor representing the lower triangular matrix.
<code>scale_tril</code> has shape <code style="white-space: pre;">&#8288;[N1, N2, ...  k, k]&#8288;</code>, which represents a k x k lower triangular matrix.
When NULL no <code>scale_tril</code> term is added to <code>scale</code>. The upper triangular elements above the diagonal are ignored.</p>
</td></tr>
<tr><td><code id="tfb_affine_+3A_scale_perturb_factor">scale_perturb_factor</code></td>
<td>
<p>Floating-point Tensor representing factor matrix with last
two dimensions of shape <code style="white-space: pre;">&#8288;(k, r)&#8288;</code> When NULL, no rank-r update is added to scale.</p>
</td></tr>
<tr><td><code id="tfb_affine_+3A_scale_perturb_diag">scale_perturb_diag</code></td>
<td>
<p>Floating-point Tensor representing the diagonal matrix.
<code>scale_perturb_diag</code> has shape <code style="white-space: pre;">&#8288;[N1, N2, ...  r]&#8288;</code>, which represents an r x r diagonal matrix.
When NULL low rank updates will take the form <code>scale_perturb_factor * scale_perturb_factor.T</code>.</p>
</td></tr>
<tr><td><code id="tfb_affine_+3A_adjoint">adjoint</code></td>
<td>
<p>Logical indicating whether to use the scale matrix as specified or its adjoint.
Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfb_affine_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_affine_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
<tr><td><code id="tfb_affine_+3A_dtype">dtype</code></td>
<td>
<p><code>tf$DType</code> to prefer when converting args to Tensors. Else, we fall back to a
common dtype inferred from the args, finally falling back to float32.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If NULL of <code>scale_identity_multiplier</code>, <code>scale_diag</code>, or <code>scale_tril</code> are specified then
<code style="white-space: pre;">&#8288;scale += IdentityMatrix&#8288;</code> Otherwise specifying a scale argument has the semantics of
<code style="white-space: pre;">&#8288;scale += Expand(arg)&#8288;</code>, i.e., <code>scale_diag != NULL</code> means <code style="white-space: pre;">&#8288;scale += tf$diag(scale_diag)&#8288;</code>.
</p>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_affine_linear_operator'>Computes<code style="white-space: pre;">&#8288;Y = g(X; shift, scale) = scale @ X + shift&#8288;</code></h2><span id='topic+tfb_affine_linear_operator'></span>

<h3>Description</h3>

<p><code>shift</code> is a numeric Tensor and scale is a LinearOperator.
If <code>X</code> is a scalar then the forward transformation is: <code>scale * X + shift</code>
where <code>*</code> denotes broadcasted elementwise product.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_affine_linear_operator(
  shift = NULL,
  scale = NULL,
  adjoint = FALSE,
  validate_args = FALSE,
  name = "affine_linear_operator"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_affine_linear_operator_+3A_shift">shift</code></td>
<td>
<p>Floating-point Tensor.</p>
</td></tr>
<tr><td><code id="tfb_affine_linear_operator_+3A_scale">scale</code></td>
<td>
<p>Subclass of LinearOperator. Represents the (batch) positive definite matrix <code>M</code> in <code style="white-space: pre;">&#8288;R^{k x k}&#8288;</code>.</p>
</td></tr>
<tr><td><code id="tfb_affine_linear_operator_+3A_adjoint">adjoint</code></td>
<td>
<p>Logical indicating whether to use the scale matrix as specified or its adjoint.
Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfb_affine_linear_operator_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_affine_linear_operator_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_affine_scalar'>AffineScalar bijector (Deprecated)</h2><span id='topic+tfb_affine_scalar'></span>

<h3>Description</h3>

<p>This Bijector is initialized with shift Tensor and scale arguments, giving the forward operation:
<code>Y = g(X) = scale * X + shift</code>
If <code>scale</code> is not specified, then the bijector has the semantics of scale = 1..
Similarly, if <code>shift</code> is not specified, then the bijector has the semantics of shift = 0..
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_affine_scalar(
  shift = NULL,
  scale = NULL,
  validate_args = FALSE,
  name = "affine_scalar"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_affine_scalar_+3A_shift">shift</code></td>
<td>
<p>Floating-point Tensor. If this is set to NULL, no shift is applied.</p>
</td></tr>
<tr><td><code id="tfb_affine_scalar_+3A_scale">scale</code></td>
<td>
<p>Floating-point Tensor. If this is set to NULL, no scale is applied.</p>
</td></tr>
<tr><td><code id="tfb_affine_scalar_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_affine_scalar_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_ascending'>Maps unconstrained R^n to R^n in ascending order.</h2><span id='topic+tfb_ascending'></span>

<h3>Description</h3>

<p>Both the domain and the codomain of the mapping is <code style="white-space: pre;">&#8288;[-inf, inf]^n&#8288;</code>, however,
the input of the inverse mapping must be strictly increasing.
On the last dimension of the tensor, the Ascending bijector performs:
<code style="white-space: pre;">&#8288;y = tf$cumsum([x[0], tf$exp(x[1]), tf$exp(x[2]), ..., tf$exp(x[-1])])&#8288;</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_ascending(validate_args = FALSE, name = "ascending")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_ascending_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_ascending_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_batch_normalization'>Computes<code>Y = g(X)</code> s.t. <code>X = g^-1(Y) = (Y - mean(Y)) / std(Y)</code></h2><span id='topic+tfb_batch_normalization'></span>

<h3>Description</h3>

<p>Applies Batch Normalization (Ioffe and Szegedy, 2015) to samples from a
data distribution. This can be used to stabilize training of normalizing
flows (Papamakarios et al., 2016; Dinh et al., 2017)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_batch_normalization(
  batchnorm_layer = NULL,
  training = TRUE,
  validate_args = FALSE,
  name = "batch_normalization"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_batch_normalization_+3A_batchnorm_layer">batchnorm_layer</code></td>
<td>
<p><code>tf$layers$BatchNormalization</code> layer object. If NULL, defaults to
<code>tf$layers$BatchNormalization(gamma_constraint=tf$nn$relu(x) + 1e-6)</code>.
This ensures positivity of the scale variable.</p>
</td></tr>
<tr><td><code id="tfb_batch_normalization_+3A_training">training</code></td>
<td>
<p>If TRUE, updates running-average statistics during call to inverse().</p>
</td></tr>
<tr><td><code id="tfb_batch_normalization_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_batch_normalization_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When training Deep Neural Networks (DNNs), it is common practice to
normalize or whiten features by shifting them to have zero mean and
scaling them to have unit variance.
</p>
<p>The <code>inverse()</code> method of the BatchNormalization bijector, which is used in
the log-likelihood computation of data samples, implements the normalization
procedure (shift-and-scale) using the mean and standard deviation of the
current minibatch.
</p>
<p>Conversely, the <code>forward()</code> method of the bijector de-normalizes samples (e.g.
<code>X*std(Y) + mean(Y)</code> with the running-average mean and standard deviation
computed at training-time. De-normalization is useful for sampling.
</p>
<p>During training time, BatchNormalization.inverse and BatchNormalization.forward are not
guaranteed to be inverses of each other because <code>inverse(y)</code> uses statistics of the current minibatch,
while <code>forward(x)</code> uses running-average statistics accumulated from training.
In other words, <code>tfb_batch_normalization()$inverse(tfb_batch_normalization()$forward(...))</code> and
<code>tfb_batch_normalization()$forward(tfb_batch_normalization()$inverse(...))</code> will be identical when
training=FALSE but may be different when training=TRUE.
</p>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1502.03167">Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In <em>International Conference on Machine Learning</em>, 2015.</a>
</p>
</li>
<li> <p><a href="https://arxiv.org/abs/1605.08803">Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density Estimation using Real NVP. In <em>International Conference on Learning Representations</em>, 2017.</a>
</p>
</li>
<li> <p><a href="https://arxiv.org/abs/1705.07057">George Papamakarios, Theo Pavlakou, and Iain Murray. Masked Autoregressive Flow for Density Estimation. In <em>Neural Information Processing Systems</em>, 2017.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_blockwise'>Bijector which applies a list of bijectors to blocks of a Tensor</h2><span id='topic+tfb_blockwise'></span>

<h3>Description</h3>

<p>More specifically, given <code style="white-space: pre;">&#8288;[F_0, F_1, ... F_n]&#8288;</code> which are scalar or vector
bijectors this bijector creates a transformation which operates on the vector
<code style="white-space: pre;">&#8288;[x_0, ... x_n]&#8288;</code> with the transformation <code style="white-space: pre;">&#8288;[F_0(x_0), F_1(x_1) ..., F_n(x_n)]&#8288;</code>
where <code style="white-space: pre;">&#8288;x_0, ..., x_n&#8288;</code> are blocks (partitions) of the vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_blockwise(
  bijectors,
  block_sizes = NULL,
  validate_args = FALSE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_blockwise_+3A_bijectors">bijectors</code></td>
<td>
<p>A non-empty list of bijectors.</p>
</td></tr>
<tr><td><code id="tfb_blockwise_+3A_block_sizes">block_sizes</code></td>
<td>
<p>A 1-D integer Tensor with each element signifying the
length of the block of the input vector to pass to the corresponding
bijector. The length of block_sizes must be be equal to the length of
bijectors. If left as NULL, a vector of 1's is used.</p>
</td></tr>
<tr><td><code id="tfb_blockwise_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical indicating whether arguments should be checked for correctness.</p>
</td></tr>
<tr><td><code id="tfb_blockwise_+3A_name">name</code></td>
<td>
<p>String, name given to ops managed by this object. Default:
E.g., <code>tfb_blockwise(list(tfb_exp(), tfb_softplus()))$name == 'blockwise_of_exp_and_softplus'</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_chain'>Bijector which applies a sequence of bijectors</h2><span id='topic+tfb_chain'></span>

<h3>Description</h3>

<p>Bijector which applies a sequence of bijectors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_chain(
  bijectors = NULL,
  validate_args = FALSE,
  validate_event_size = TRUE,
  parameters = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_chain_+3A_bijectors">bijectors</code></td>
<td>
<p>list of bijector instances. An empty list makes this
bijector equivalent to the Identity bijector.</p>
</td></tr>
<tr><td><code id="tfb_chain_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical indicating whether arguments should be checked for correctness.</p>
</td></tr>
<tr><td><code id="tfb_chain_+3A_validate_event_size">validate_event_size</code></td>
<td>
<p>Checks that bijectors are not applied to inputs with
incomplete support (that is, inputs where one or more elements are a
deterministic transformation of the others). For example, the following
LDJ would be incorrect:
<code>tfb_chain(list(tfb_scale(), tfb_softmax_centered()))$forward_log_det_jacobian(matrix(1:2, ncol = 2))</code>
The jacobian contribution from <code>tfb_scale()</code> applies to a 2-dimensional input,
but the output from <code>tfb_softmax_centered()</code> is a 1-dimensional input embedded
in a 2-dimensional space. Setting <code>validate_event_size=TRUE</code> (default)
prints warnings in these cases. When <code>validate_args</code> is also <code>TRUE</code>, the
warning is promoted to an exception.</p>
</td></tr>
<tr><td><code id="tfb_chain_+3A_parameters">parameters</code></td>
<td>
<p>Locals dict captured by subclass constructor, to be used for
copy/slice re-instantiation operators.</p>
</td></tr>
<tr><td><code id="tfb_chain_+3A_name">name</code></td>
<td>
<p>String, name given to ops managed by this object. Default:
E.g., <code>tfb_chain(list(tfb_exp(), tfb_softplus()))$name == "chain_of_exp_of_softplus"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_cholesky_outer_product'>Computes<code>g(X) = X @ X.T</code> where <code>X</code> is lower-triangular, positive-diagonal matrix</h2><span id='topic+tfb_cholesky_outer_product'></span>

<h3>Description</h3>

<p>Note: the upper-triangular part of X is ignored (whether or not its zero).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_cholesky_outer_product(
  validate_args = FALSE,
  name = "cholesky_outer_product"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_cholesky_outer_product_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_cholesky_outer_product_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The surjectivity of g as a map from  the set of n x n positive-diagonal
lower-triangular matrices to the set of SPD matrices follows immediately from
executing the Cholesky factorization algorithm on an SPD matrix <code>A</code> to produce a
positive-diagonal lower-triangular matrix <code>L</code> such that <code>A = L @ L.T</code>.
</p>
<p>To prove the injectivity of g, suppose that <code>L_1</code> and <code>L_2</code> are lower-triangular
with positive diagonals and satisfy <code>A = L_1 @ L_1.T = L_2 @ L_2.T</code>. Then
<code style="white-space: pre;">&#8288;inv(L_1) @ A @ inv(L_1).T = [inv(L_1) @ L_2] @ [inv(L_1) @ L_2].T = I&#8288;</code>.
Setting <code>L_3 := inv(L_1) @ L_2</code>, that <code>L_3</code> is a positive-diagonal
lower-triangular matrix follows from <code>inv(L_1)</code> being positive-diagonal
lower-triangular (which follows from the diagonal of a triangular matrix being
its spectrum), and that the product of two positive-diagonal lower-triangular
matrices is another positive-diagonal lower-triangular matrix.
A simple inductive argument (proceeding one column of <code>L_3</code> at a time) shows
that, if <code>I = L_3 @ L_3.T</code>, with <code>L_3</code> being lower-triangular with positive-
diagonal, then <code>L_3 = I</code>. Thus, <code>L_1 = L_2</code>, proving injectivity of g.
</p>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_cholesky_to_inv_cholesky'>Maps the Cholesky factor of M to the Cholesky factor of <code>M^{-1}</code></h2><span id='topic+tfb_cholesky_to_inv_cholesky'></span>

<h3>Description</h3>

<p>The forward and inverse calculations are conceptually identical to:
<code>forward &lt;- function(x) tf$cholesky(tf$linalg$inv(tf$matmul(x, x, adjoint_b=TRUE)))</code>
<code>inverse = forward</code>
However, the actual calculations exploit the triangular structure of the matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_cholesky_to_inv_cholesky(
  validate_args = FALSE,
  name = "cholesky_to_inv_cholesky"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_cholesky_to_inv_cholesky_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_cholesky_to_inv_cholesky_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_correlation_cholesky'>Maps unconstrained reals to Cholesky-space correlation matrices.</h2><span id='topic+tfb_correlation_cholesky'></span>

<h3>Description</h3>

<p>This bijector is a mapping between <code>R^{n}</code> and the <code>n</code>-dimensional manifold of
Cholesky-space correlation matrices embedded in <code>R^{m^2}</code>, where <code>n</code> is the
<code>(m - 1)</code>th triangular number; i.e. <code>n = 1 + 2 + ... + (m - 1)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_correlation_cholesky(validate_args = FALSE, name = "correlation_cholesky")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_correlation_cholesky_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_correlation_cholesky_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The image of unconstrained reals under the <code>CorrelationCholesky</code> bijector is
the set of correlation matrices which are positive definite.
A <a href="https://en.wikipedia.org/wiki/Correlation_and_dependence#Correlation_matrices">correlation matrix</a>
can be characterized as a symmetric positive semidefinite matrix with 1s on
the main diagonal. However, the correlation matrix is positive definite if no
component can be expressed as a linear combination of the other components.
For a lower triangular matrix <code>L</code> to be a valid Cholesky-factor of a positive
definite correlation matrix, it is necessary and sufficient that each row of
<code>L</code> have unit Euclidean norm. To see this, observe that if <code>L_i</code> is the
<code>i</code>th row of the Cholesky factor corresponding to the correlation matrix <code>R</code>,
then the <code>i</code>th diagonal entry of <code>R</code> satisfies:
</p>
<div class="sourceCode"><pre>1 = R_i,i = L_i . L_i = ||L_i||^2
</pre></div>
<p>where '.' is the dot product of vectors and <code style="white-space: pre;">&#8288;||...||&#8288;</code> denotes the Euclidean
norm. Furthermore, observe that <code style="white-space: pre;">&#8288;R_i,j&#8288;</code> lies in the interval <code style="white-space: pre;">&#8288;[-1, 1]&#8288;</code>. By the
Cauchy-Schwarz inequality:
</p>
<div class="sourceCode"><pre>|R_i,j| = |L_i . L_j| &lt;= ||L_i|| ||L_j|| = 1
</pre></div>
<p>This is a consequence of the fact that <code>R</code> is symmetric positive definite with
1s on the main diagonal.
The LKJ distribution with <code>input_output_cholesky=TRUE</code> generates samples from
(and computes log-densities on) the set of Cholesky factors of positive
definite correlation matrices. The <code>CorrelationCholesky</code> bijector provides
a bijective mapping from unconstrained reals to the support of the LKJ
distribution.
</p>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://mc-stan.org/docs/2_18/functions-reference/cholesky-lkj-correlation-distribution.html">Stan Manual. Section 24.2. Cholesky LKJ Correlation Distribution.</a>
</p>
</li>
<li><p> Daniel Lewandowski, Dorota Kurowicka, and Harry Joe, &quot;Generating random correlation matrices based on vines and extended onion method,&quot; Journal of Multivariate Analysis 100 (2009), pp 1989-2001.
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_cumsum'>Computes the cumulative sum of a tensor along a specified axis.</h2><span id='topic+tfb_cumsum'></span>

<h3>Description</h3>

<p>Computes the cumulative sum of a tensor along a specified axis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_cumsum(axis = -1, validate_args = FALSE, name = "cumsum")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_cumsum_+3A_axis">axis</code></td>
<td>
<p><code>int</code> indicating the axis along which to compute the cumulative sum.
Note that positive (and zero) values are not supported</p>
</td></tr>
<tr><td><code id="tfb_cumsum_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_cumsum_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_discrete_cosine_transform'>Computes<code>Y = g(X) = DCT(X)</code>, where DCT type is indicated by the type arg</h2><span id='topic+tfb_discrete_cosine_transform'></span>

<h3>Description</h3>

<p>The <a href="https://en.wikipedia.org/wiki/Discrete_cosine_transform">discrete cosine transform</a>
efficiently applies a unitary DCT operator. This can be useful for mixing and decorrelating across
the innermost event dimension.
The inverse <code>X = g^{-1}(Y) = IDCT(Y)</code>, where IDCT is DCT-III for type==2.
This bijector can be interleaved with Affine bijectors to build a cascade of
structured efficient linear layers as in Moczulski et al., 2016.
Note that the operator applied is orthonormal (i.e. norm='ortho').
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_discrete_cosine_transform(
  validate_args = FALSE,
  dct_type = 2,
  name = "dct"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_discrete_cosine_transform_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_discrete_cosine_transform_+3A_dct_type">dct_type</code></td>
<td>
<p>integer, the DCT type performed by the forward transformation.
Currently, only 2 and 3 are supported.</p>
</td></tr>
<tr><td><code id="tfb_discrete_cosine_transform_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1511.05946">Moczulski M, Denil M, Appleyard J, de Freitas N. ACDC: A structured efficient linear layer. In <em>International Conference on Learning Representations</em>, 2016.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_exp'>Computes<code>Y=g(X)=exp(X)</code></h2><span id='topic+tfb_exp'></span>

<h3>Description</h3>

<p>Computes<code>Y=g(X)=exp(X)</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_exp(validate_args = FALSE, name = "exp")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_exp_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_exp_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_expm1'>Computes<code>Y = g(X) = exp(X) - 1</code></h2><span id='topic+tfb_expm1'></span>

<h3>Description</h3>

<p>This Bijector is no different from <code>tfb_chain(list(tfb_affine_scalar(shift=-1), tfb_exp()))</code>.
However, this makes use of the more numerically stable routines
<code>tf$math$expm1</code> and <code>tf$log1p</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_expm1(validate_args = FALSE, name = "expm1")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_expm1_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_expm1_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note: the expm1(.) is applied element-wise but the Jacobian is a reduction
over the event space.
</p>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_ffjord'>Implements a continuous normalizing flow X-&gt;Y defined via an ODE.</h2><span id='topic+tfb_ffjord'></span>

<h3>Description</h3>

<p>This bijector implements a continuous dynamics transformation
parameterized by a differential equation, where initial and terminal
conditions correspond to domain (X) and image (Y) i.e.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_ffjord(
  state_time_derivative_fn,
  ode_solve_fn = NULL,
  trace_augmentation_fn = tfp$bijectors$ffjord$trace_jacobian_hutchinson,
  initial_time = 0,
  final_time = 1,
  validate_args = FALSE,
  dtype = tf$float32,
  name = "ffjord"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_ffjord_+3A_state_time_derivative_fn">state_time_derivative_fn</code></td>
<td>
<p><code>function</code> taking arguments <code>time</code>
(a scalar representing time) and <code>state</code> (a Tensor representing the
state at given <code>time</code>) returning the time derivative of the <code>state</code> at
given <code>time</code>.</p>
</td></tr>
<tr><td><code id="tfb_ffjord_+3A_ode_solve_fn">ode_solve_fn</code></td>
<td>
<p><code>function</code> taking arguments <code>ode_fn</code> (same as
<code>state_time_derivative_fn</code> above), <code>initial_time</code> (a scalar representing
the initial time of integration), <code>initial_state</code> (a Tensor of floating
dtype represents the initial state) and <code>solution_times</code> (1D Tensor of
floating dtype representing time at which to obtain the solution)
returning a Tensor of shape <code style="white-space: pre;">&#8288;[time_axis, initial_state$shape]&#8288;</code>. Will take
<code style="white-space: pre;">&#8288;[final_time]&#8288;</code> as the <code>solution_times</code> argument and
<code>state_time_derivative_fn</code> as <code>ode_fn</code> argument.
If <code>NULL</code> a DormandPrince solver from <code>tfp$math$ode</code> is used.
Default value: NULL</p>
</td></tr>
<tr><td><code id="tfb_ffjord_+3A_trace_augmentation_fn">trace_augmentation_fn</code></td>
<td>
<p><code>function</code> taking arguments <code>ode_fn</code> (
<code>function</code> same as <code>state_time_derivative_fn</code> above),
<code>state_shape</code> (TensorShape of a the state), <code>dtype</code> (same as dtype of
the state) and returning a <code>function</code> taking arguments <code>time</code>
(a scalar representing the time at which the function is evaluted),
<code>state</code> (a Tensor representing the state at given <code>time</code>) that computes
a tuple (<code>ode_fn(time, state)</code>, <code>jacobian_trace_estimation</code>).
<code>jacobian_trace_estimation</code> should represent trace of the jacobian of
<code>ode_fn</code> with respect to <code>state</code>. <code>state_time_derivative_fn</code> will be
passed as <code>ode_fn</code> argument.
Default value: tfp$bijectors$ffjord$trace_jacobian_hutchinson</p>
</td></tr>
<tr><td><code id="tfb_ffjord_+3A_initial_time">initial_time</code></td>
<td>
<p>Scalar float representing time to which the <code>x</code> value of the
bijector corresponds to. Passed as <code>initial_time</code> to <code>ode_solve_fn</code>.
For default solver can be <code>float</code> or floating scalar <code>Tensor</code>.
Default value: 0.</p>
</td></tr>
<tr><td><code id="tfb_ffjord_+3A_final_time">final_time</code></td>
<td>
<p>Scalar float representing time to which the <code>y</code> value of the
bijector corresponds to. Passed as <code>solution_times</code> to <code>ode_solve_fn</code>.
For default solver can be <code>float</code> or floating scalar <code>Tensor</code>.
Default value: 1.</p>
</td></tr>
<tr><td><code id="tfb_ffjord_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_ffjord_+3A_dtype">dtype</code></td>
<td>
<p><code>tf$DType</code> to prefer when converting args to <code>Tensor</code>s. Else, we
fall back to a common dtype inferred from the args, finally falling
back to float32.</p>
</td></tr>
<tr><td><code id="tfb_ffjord_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<div class="sourceCode"><pre>d/dt[state(t)] = state_time_derivative_fn(t, state(t))
state(initial_time) = X
state(final_time) = Y
</pre></div>
<p>For this transformation the value of <code>log_det_jacobian</code> follows another
differential equation, reducing it to computation of the trace of the jacobian
along the trajectory
</p>
<div class="sourceCode"><pre>state_time_derivative = state_time_derivative_fn(t, state(t))
d/dt[log_det_jac(t)] = Tr(jacobian(state_time_derivative, state(t)))
</pre></div>
<p>FFJORD constructor takes two functions <code>ode_solve_fn</code> and
<code>trace_augmentation_fn</code> arguments that customize integration of the
differential equation and trace estimation.
</p>
<p>Differential equation integration is performed by a call to <code>ode_solve_fn</code>.
</p>
<p>Custom <code>ode_solve_fn</code> must accept the following arguments:
</p>

<ul>
<li><p> ode_fn(time, state): Differential equation to be solved.
</p>
</li>
<li><p> initial_time: Scalar float or floating Tensor representing the initial time.
</p>
</li>
<li><p> initial_state: Floating Tensor representing the initial state.
</p>
</li>
<li><p> solution_times: 1D floating Tensor of solution times.
</p>
</li></ul>

<p>And return a Tensor of shape <code style="white-space: pre;">&#8288;[solution_times$shape, initial_state$shape]&#8288;</code>
representing state values evaluated at <code>solution_times</code>. In addition
<code>ode_solve_fn</code> must support nested structures. For more details see the
interface of <code>tfp$math$ode$Solver$solve()</code>.
</p>
<p>Trace estimation is computed simultaneously with <code>state_time_derivative</code>
using <code>augmented_state_time_derivative_fn</code> that is generated by
<code>trace_augmentation_fn</code>. <code>trace_augmentation_fn</code> takes
<code>state_time_derivative_fn</code>, <code>state.shape</code> and <code>state.dtype</code> arguments and
returns a <code>augmented_state_time_derivative_fn</code> callable that computes both
<code>state_time_derivative</code> and unreduced <code>trace_estimation</code>.
</p>
<p>Custom <code>ode_solve_fn</code> and <code>trace_augmentation_fn</code> examples:
</p>
<div class="sourceCode"><pre># custom_solver_fn: `function(f, t_initial, t_solutions, y_initial, ...)`
# ... : Additional arguments to pass to custom_solver_fn.
ode_solve_fn &lt;- function(ode_fn, initial_time, initial_state, solution_times) {
  custom_solver_fn(ode_fn, initial_time, solution_times, initial_state, ...)
}
ffjord &lt;- tfb_ffjord(state_time_derivative_fn, ode_solve_fn = ode_solve_fn)
</pre></div>
<div class="sourceCode"><pre># state_time_derivative_fn: `function(time, state)`
# trace_jac_fn: `function(time, state)` unreduced jacobian trace function
trace_augmentation_fn &lt;- function(ode_fn, state_shape, state_dtype) {
  augmented_ode_fn &lt;- function(time, state) {
    list(ode_fn(time, state), trace_jac_fn(time, state))
  }
augmented_ode_fn
}
ffjord &lt;- tfb_ffjord(state_time_derivative_fn, trace_augmentation_fn = trace_augmentation_fn)
</pre></div>
<p>For more details on FFJORD and continous normalizing flows see Chen et al. (2018), Grathwol et al. (2018).
</p>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>References</h3>


<ul>
<li><p> Chen, T. Q., Rubanova, Y., Bettencourt, J., &amp; Duvenaud, D. K. (2018). Neural ordinary differential equations. In Advances in neural information processing systems (pp. 6571-6583)
</p>
</li>
<li> <p><a href="https://arxiv.org/abs/1810.01367">Grathwohl, W., Chen, R. T., Betterncourt, J., Sutskever, I., &amp; Duvenaud, D. (2018). Ffjord: Free-form continuous dynamics for scalable reversible generative models. arXiv preprint arXiv:1810.01367.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_fill_scale_tri_l'>Transforms unconstrained vectors to TriL matrices with positive diagonal</h2><span id='topic+tfb_fill_scale_tri_l'></span>

<h3>Description</h3>

<p>This is implemented as a simple tfb_chain of tfb_fill_triangular followed by
tfb_transform_diagonal, and provided mostly as a convenience.
The default setup is somewhat opinionated, using a Softplus transformation followed by a
small shift (1e-5) which attempts to avoid numerical issues from zeros on the diagonal.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_fill_scale_tri_l(
  diag_bijector = NULL,
  diag_shift = 1e-05,
  validate_args = FALSE,
  name = "fill_scale_tril"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_fill_scale_tri_l_+3A_diag_bijector">diag_bijector</code></td>
<td>
<p>Bijector instance, used to transform the output diagonal to be positive.
Default value: NULL (i.e., <code>tfb_softplus()</code>).</p>
</td></tr>
<tr><td><code id="tfb_fill_scale_tri_l_+3A_diag_shift">diag_shift</code></td>
<td>
<p>Float value broadcastable and added to all diagonal entries after applying the
diag_bijector. Setting a positive value forces the output diagonal entries to be positive, but
prevents inverting the transformation for matrices with diagonal entries less than this value.
Default value: 1e-5.</p>
</td></tr>
<tr><td><code id="tfb_fill_scale_tri_l_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_fill_scale_tri_l_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_fill_triangular'>Transforms vectors to triangular</h2><span id='topic+tfb_fill_triangular'></span>

<h3>Description</h3>

<p>Triangular matrix elements are filled in a clockwise spiral.
Given input with shape <code style="white-space: pre;">&#8288;batch_shape + [d]&#8288;</code>, produces output with
shape <code style="white-space: pre;">&#8288;batch_shape + [n, n]&#8288;</code>, where <code>n = (-1 + sqrt(1 + 8 * d))/2</code>.
This follows by solving the quadratic equation <code>d = 1 + 2 + ... + n = n * (n + 1)/2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_fill_triangular(
  upper = FALSE,
  validate_args = FALSE,
  name = "fill_triangular"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_fill_triangular_+3A_upper">upper</code></td>
<td>
<p>Logical representing whether output matrix should be upper triangular (TRUE)
or lower triangular (FALSE, default).</p>
</td></tr>
<tr><td><code id="tfb_fill_triangular_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_fill_triangular_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_forward'>Returns the forward Bijector evaluation, i.e., <code>X = g(Y)</code>.</h2><span id='topic+tfb_forward'></span>

<h3>Description</h3>

<p>Returns the forward Bijector evaluation, i.e., <code>X = g(Y)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_forward(bijector, x, name = "forward")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_forward_+3A_bijector">bijector</code></td>
<td>
<p>The bijector to apply</p>
</td></tr>
<tr><td><code id="tfb_forward_+3A_x">x</code></td>
<td>
<p>Tensor. The input to the &quot;forward&quot; evaluation.</p>
</td></tr>
<tr><td><code id="tfb_forward_+3A_name">name</code></td>
<td>
<p>name of the operation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tensor
</p>


<h3>See Also</h3>

<p>Other bijector_methods: 
<code><a href="#topic+tfb_forward_log_det_jacobian">tfb_forward_log_det_jacobian</a>()</code>,
<code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian</a>()</code>,
<code><a href="#topic+tfb_inverse">tfb_inverse</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  b &lt;- tfb_affine_scalar(shift = 1, scale = 2)
  x &lt;- 10
  b %&gt;% tfb_forward(x)

</code></pre>

<hr>
<h2 id='tfb_forward_log_det_jacobian'>Returns the result of the forward evaluation of the log determinant of the Jacobian</h2><span id='topic+tfb_forward_log_det_jacobian'></span>

<h3>Description</h3>

<p>Returns the result of the forward evaluation of the log determinant of the Jacobian
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_forward_log_det_jacobian(
  bijector,
  x,
  event_ndims,
  name = "forward_log_det_jacobian"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_forward_log_det_jacobian_+3A_bijector">bijector</code></td>
<td>
<p>The bijector to apply</p>
</td></tr>
<tr><td><code id="tfb_forward_log_det_jacobian_+3A_x">x</code></td>
<td>
<p>Tensor. The input to the &quot;forward&quot; Jacobian determinant evaluation.</p>
</td></tr>
<tr><td><code id="tfb_forward_log_det_jacobian_+3A_event_ndims">event_ndims</code></td>
<td>
<p>Number of dimensions in the probabilistic events being transformed.
Must be greater than or equal to bijector$forward_min_event_ndims. The result is summed over the final
dimensions to produce a scalar Jacobian determinant for each event, i.e. it has shape
x$shape$ndims - event_ndims dimensions.</p>
</td></tr>
<tr><td><code id="tfb_forward_log_det_jacobian_+3A_name">name</code></td>
<td>
<p>name of the operation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tensor
</p>


<h3>See Also</h3>

<p>Other bijector_methods: 
<code><a href="#topic+tfb_forward">tfb_forward</a>()</code>,
<code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian</a>()</code>,
<code><a href="#topic+tfb_inverse">tfb_inverse</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  b &lt;- tfb_affine_scalar(shift = 1, scale = 2)
  x &lt;- 10
  b %&gt;% tfb_forward_log_det_jacobian(x, event_ndims = 0)

</code></pre>

<hr>
<h2 id='tfb_glow'>Implements the Glow Bijector from Kingma &amp; Dhariwal (2018).</h2><span id='topic+tfb_glow'></span>

<h3>Description</h3>

<p>Overview: <code>Glow</code> is a chain of bijectors which transforms a rank-1 tensor
(vector) into a rank-3 tensor (e.g. an RGB image). <code>Glow</code> does this by
chaining together an alternating series of &quot;Blocks,&quot; &quot;Squeezes,&quot; and &quot;Exits&quot;
which are each themselves special chains of other bijectors. The intended use
of <code>Glow</code> is as part of a <code>tfd_transformed_distribution</code>, in
which the base distribution over the vector space is used to generate samples
in the image space. In the paper, an Independent Normal distribution is used
as the base distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_glow(
  output_shape = c(32, 32, 3),
  num_glow_blocks = 3,
  num_steps_per_block = 32,
  coupling_bijector_fn = NULL,
  exit_bijector_fn = NULL,
  grab_after_block = NULL,
  use_actnorm = TRUE,
  seed = NULL,
  validate_args = FALSE,
  name = "glow"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_glow_+3A_output_shape">output_shape</code></td>
<td>
<p>A list of integers, specifying the event shape of the
output, of the bijectors forward pass (the image).  Specified as
<code style="white-space: pre;">&#8288;[H, W, C]&#8288;</code>. Default Value: (32, 32, 3)</p>
</td></tr>
<tr><td><code id="tfb_glow_+3A_num_glow_blocks">num_glow_blocks</code></td>
<td>
<p>An integer, specifying how many downsampling levels to
include in the model. This must divide equally into both H and W,
otherwise the bijector would not be invertible. Default Value: 3</p>
</td></tr>
<tr><td><code id="tfb_glow_+3A_num_steps_per_block">num_steps_per_block</code></td>
<td>
<p>An integer specifying how many Affine Coupling and
1x1 convolution layers to include at each level of the spatial
hierarchy. Default Value: 32 (i.e. the value used in the original glow paper).</p>
</td></tr>
<tr><td><code id="tfb_glow_+3A_coupling_bijector_fn">coupling_bijector_fn</code></td>
<td>
<p>A function which takes the argument <code>input_shape</code>
and returns a callable neural network (e.g. a <code>keras_model_sequential()</code>). The
network should either return a tensor with the same event shape as
<code>input_shape</code> (this will employ additive coupling), a tensor with the
same height and width as <code>input_shape</code> but twice the number of channels
(this will employ affine coupling), or a bijector which takes in a
tensor with event shape <code>input_shape</code>, and returns a tensor with shape
<code>input_shape</code>.</p>
</td></tr>
<tr><td><code id="tfb_glow_+3A_exit_bijector_fn">exit_bijector_fn</code></td>
<td>
<p>Similar to coupling_bijector_fn, exit_bijector_fn is
a function which takes the argument <code>input_shape</code> and <code>output_chan</code>
and returns a callable neural network. The neural network it returns
should take a tensor of shape <code>input_shape</code> as the input, and return
one of three options: A tensor with <code>output_chan</code> channels, a tensor
with <code>2 * output_chan</code> channels, or a bijector. Additional details can
be found in the documentation for ExitBijector.</p>
</td></tr>
<tr><td><code id="tfb_glow_+3A_grab_after_block">grab_after_block</code></td>
<td>
<p>A tuple of floats, specifying what fraction of the
remaining channels to remove following each glow block. Glow will take
the integer floor of this number multiplied by the remaining number of
channels. The default is half at each spatial hierarchy.
Default value: None (this will take out half of the channels after each block.</p>
</td></tr>
<tr><td><code id="tfb_glow_+3A_use_actnorm">use_actnorm</code></td>
<td>
<p>A boolean deciding whether or not to use actnorm. Data-dependent
initialization is used to initialize this layer. Default value: <code>FALSE</code></p>
</td></tr>
<tr><td><code id="tfb_glow_+3A_seed">seed</code></td>
<td>
<p>A seed to control randomness in the 1x1 convolution initialization.
Default value: <code>NULL</code> (i.e., non-reproducible sampling).</p>
</td></tr>
<tr><td><code id="tfb_glow_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_glow_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A &quot;Block&quot; (implemented as the <code>GlowBlock</code> Bijector) performs much of the
transformations which allow glow to produce sophisticated and complex mappings
between the image space and the latent space and therefore achieve rich image
generation performance. A Block is composed of <code>num_steps_per_block</code> steps,
which are each implemented as a <code>Chain</code> containing an
<code>ActivationNormalization</code> (ActNorm) bijector, followed by an (invertible)
<code>OneByOneConv</code> bijector, and finally a coupling bijector. The coupling
bijector is an instance of a <code>RealNVP</code> bijector, and uses the
<code>coupling_bijector_fn</code> function to instantiate the coupling bijector function
which is given to the <code>RealNVP</code>. This function returns a bijector which
defines the coupling (e.g. <code>Shift(Scale)</code> for affine coupling or <code>Shift</code> for
additive coupling).
</p>
<p>A &quot;Squeeze&quot; converts spatial features into channel features. It is
implemented using the <code>Expand</code> bijector. The difference in names is
due to the fact that the <code>forward</code> function from glow is meant to ultimately
correspond to sampling from a <code>tfp$util$TransformedDistribution</code> object,
which would use <code>Expand</code> (Squeeze is just Invert(Expand)). The <code>Expand</code>
bijector takes a tensor with shape <code style="white-space: pre;">&#8288;[H, W, C]&#8288;</code> and returns a tensor with shape
<code style="white-space: pre;">&#8288;[2H, 2W, C / 4]&#8288;</code>, such that each 2x2x1 spatial tile in the output is composed
from a single 1x1x4 tile in the input tensor, as depicted in the figure below.
</p>
<p>Forward pass (Expand)
</p>
<div class="sourceCode"><pre>\     \       \    \    \
\\     \ ----&gt; \  1 \  2 \
\\\__1__\       \____\____\
\\\__2__\        \    \    \
\\__3__\  &lt;----  \  3 \  4 \
\__4__\          \____\____\
</pre></div>
<p>Inverse pass (Squeeze)
This is implemented using a chain of <code>Reshape</code> -&gt; <code>Transpose</code> -&gt; <code>Reshape</code>
bijectors. Note that on an inverse pass through the bijector, each Squeeze
will cause the width/height of the image to decrease by a factor of 2.
Therefore, the input image must be evenly divisible by 2 at least
<code>num_glow_blocks</code> times, since it will pass through a Squeeze step that many
times.
</p>
<p>An &quot;Exit&quot; is simply a junction at which some of the tensor &quot;exits&quot; from the
glow bijector and therefore avoids any further alteration. Each exit is
implemented as a <code>Blockwise</code> bijector, where some channels are given to the
rest of the glow model, and the rest are given to a bypass implemented using
the <code>Identity</code> bijector. The fraction of channels to be removed at each exit
is determined by the <code>grab_after_block</code> arg, indicates the fraction of
remaining channels which join the identity bypass. The fraction is
converted to an integer number of channels by multiplying by the remaining
number of channels and rounding.
Additionally, at each exit, glow couples the tensor exiting the highway to
the tensor continuing onward. This makes small scale features in the image
dependent on larger scale features, since the larger scale features dictate
the mean and scale of the distribution over the smaller scale features.
This coupling is done similarly to the Coupling bijector in each step of the
flow (i.e. using a RealNVP bijector). However for the exit bijector, the
coupling is instantiated using <code>exit_bijector_fn</code> rather than coupling
bijector fn, allowing for different behaviors between standard coupling and
exit coupling. Also note that because the exit utilizes a coupling bijector,
there are two special cases (all channels exiting and no channels exiting).
The full Glow bijector consists of <code>num_glow_blocks</code> Blocks each of which
contains <code>num_steps_per_block</code> steps. Each step implements a coupling using
<code>bijector_coupling_fn</code>. Between blocks, glow converts between spatial pixels
and channels using the Expand Bijector, and splits channels out of the
bijector using the Exit Bijector. The channels which have exited continue
onward through Identity bijectors and those which have not exited are given
to the next block. After passing through all Blocks, the tensor is reshaped
to a rank-1 tensor with the same number of elements. This is where the
distribution will be defined.
A schematic diagram of Glow is shown below. The <code>forward</code> function of the
bijector starts from the bottom and goes upward, while the <code>inverse</code> function
starts from the top and proceeds downward.
</p>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>#' &ldquo;'</h3>

<p>Glow Schematic Diagram
Input Image     ########################   shape = [H, W, C]
\                      /&lt;- Expand Bijector turns spatial
\                    /    dimensions into channels.
|  XXXXXXXXXXXXXXXXXXXX
|  XXXXXXXXXXXXXXXXXXXX
|  XXXXXXXXXXXXXXXXXXXX     A single step of the flow consists
Glow Block  - |  XXXXXXXXXXXXXXXXXXXX  &lt;- of ActNorm -&gt; 1x1Conv -&gt; Coupling.
|  XXXXXXXXXXXXXXXXXXXX     there are num_steps_per_block
|  XXXXXXXXXXXXXXXXXXXX     steps of the flow in each block.
|_ XXXXXXXXXXXXXXXXXXXX
\                  / &lt;&ndash; Expand bijectors follow each glow
\                /      block
XXXXXXXX\\\\   &lt;&ndash; Exit Bijector removes channels
_                    _     from additional alteration.
|    XXXXXXXX !  |  !
|    XXXXXXXX !  |  !
|    XXXXXXXX !  |  !       After exiting, channels are passed
Glow Block  - |    XXXXXXXX !  |  !  &lt;&mdash; downward using the Blockwise and
|    XXXXXXXX !  |  !       Identify bijectors.
|    XXXXXXXX !  |  !
|_   XXXXXXXX !  |  !
\              / &lt;&mdash;- Expand Bijector
\            /
XXX\\    | !  &lt;&mdash;- Exit Bijector
_
|      XXX ! |   | !
|      XXX ! |   | !
|      XXX ! |   | !
low Block  - |      XXX ! |   | !
|      XXX ! |   | !
|      XXX ! |   | !
|_     XXX ! |   | !
XX\ ! |   | ! &lt;&mdash;&ndash; (Optional) Exit Bijector
|    |   |
v    v   v
Output Distribution    ##########          shape = [H * W * C]
</p>
<div class="sourceCode"><pre>    Legend
</pre></div>


<h3>| XX  = Step of flow      |
| X\  = Exit bijector     |
| \/  = Expand bijector   |
| !|! = Identity bijector |
|                         |
| up  = Forward pass      |
| dn  = Inverse pass      |
|_________________________|</h3>

<div class="sourceCode"><pre>
[H, W, C]: R:H,%20W,%20C
[2H, 2W, C / 4]: R:2H,%202W,%20C%20/%204
[H, W, C]: R:H,%20W,%20C
[H * W * C]: R:H%20*%20W%20*%20C
</pre></div>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1807.03039">Diederik P Kingma, Prafulla Dhariwal, Glow: Generative Flow with Invertible 1x1 Convolutions. In <em>Neural Information Processing Systems</em>, 2018.</a>
</p>
</li>
<li> <p><a href="https://arxiv.org/abs/1605.08803">Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density Estimation using Real NVP. In <em>International Conference on Learning Representations</em>, 2017.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_gompertz_cdf'>Compute <code style="white-space: pre;">&#8288;Y = g(X) = 1 - exp(-c * (exp(rate * X) - 1)&#8288;</code>, the Gompertz CDF.</h2><span id='topic+tfb_gompertz_cdf'></span>

<h3>Description</h3>

<p>This bijector maps inputs from <code style="white-space: pre;">&#8288;[-inf, inf]&#8288;</code> to <code style="white-space: pre;">&#8288;[0, inf]&#8288;</code>. The inverse of the
bijector applied to a uniform random variable <code>X ~ U(0, 1)</code> gives back a
random variable with the
<a href="https://en.wikipedia.org/wiki/Gompertz_distribution">Gompertz distribution</a>:
</p>
<div class="sourceCode"><pre>Y ~ GompertzCDF(concentration, rate)
pdf(y; c, r) = r * c * exp(r * y + c - c * exp(-c * exp(r * y)))
</pre></div>
<p>Note: Because the Gompertz distribution concentrates its mass close to zero,
for larger rates or larger concentrations, <code>bijector.forward</code> will quickly
saturate to 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_gompertz_cdf(
  concentration,
  rate,
  validate_args = FALSE,
  name = "gompertz_cdf"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_gompertz_cdf_+3A_concentration">concentration</code></td>
<td>
<p>Positive Float-like <code>Tensor</code> that is the same dtype and is
broadcastable with <code>concentration</code>.
This is <code>c</code> in <code style="white-space: pre;">&#8288;Y = g(X) = 1 - exp(-c * (exp(rate * X) - 1)&#8288;</code>.</p>
</td></tr>
<tr><td><code id="tfb_gompertz_cdf_+3A_rate">rate</code></td>
<td>
<p>Positive Float-like <code>Tensor</code> that is the same dtype and is
broadcastable with <code>concentration</code>.
This is <code>rate</code> in <code style="white-space: pre;">&#8288;Y = g(X) = 1 - exp(-c * (exp(rate * X) - 1)&#8288;</code>.</p>
</td></tr>
<tr><td><code id="tfb_gompertz_cdf_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_gompertz_cdf_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_gumbel'>Computes<code>Y = g(X) = exp(-exp(-(X - loc) / scale))</code></h2><span id='topic+tfb_gumbel'></span>

<h3>Description</h3>

<p>This bijector maps inputs from <code style="white-space: pre;">&#8288;[-inf, inf]&#8288;</code> to <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code>. The inverse of the
bijector applied to a uniform random variable <code>X ~ U(0, 1)</code> gives back a
random variable with the <a href="https://en.wikipedia.org/wiki/Gumbel_distribution">Gumbel distribution</a>:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_gumbel(loc = 0, scale = 1, validate_args = FALSE, name = "gumbel")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_gumbel_+3A_loc">loc</code></td>
<td>
<p>Float-like Tensor that is the same dtype and is broadcastable with scale.
This is loc in <code>Y = g(X) = exp(-exp(-(X - loc) / scale))</code>.</p>
</td></tr>
<tr><td><code id="tfb_gumbel_+3A_scale">scale</code></td>
<td>
<p>Positive Float-like Tensor that is the same dtype and is broadcastable with loc.
This is scale in <code>Y = g(X) = exp(-exp(-(X - loc) / scale))</code>.</p>
</td></tr>
<tr><td><code id="tfb_gumbel_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_gumbel_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>Y ~ Gumbel(loc, scale)</code>
<code style="white-space: pre;">&#8288;pdf(y; loc, scale) = exp(-( (y - loc) / scale + exp(- (y - loc) / scale) ) ) / scale&#8288;</code>
</p>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_gumbel_cdf'>Compute <code>Y = g(X) = exp(-exp(-(X - loc) / scale))</code>, the Gumbel CDF.</h2><span id='topic+tfb_gumbel_cdf'></span>

<h3>Description</h3>

<p>This bijector maps inputs from <code style="white-space: pre;">&#8288;[-inf, inf]&#8288;</code> to <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code>. The inverse of the
bijector applied to a uniform random variable <code>X ~ U(0, 1)</code> gives back a
random variable with the <a href="https://en.wikipedia.org/wiki/Gumbel_distribution">Gumbel distribution</a>:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_gumbel_cdf(loc = 0, scale = 1, validate_args = FALSE, name = "gumbel_cdf")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_gumbel_cdf_+3A_loc">loc</code></td>
<td>
<p>Float-like <code>Tensor</code> that is the same dtype and is
broadcastable with <code>scale</code>.
This is <code>loc</code> in <code>Y = g(X) = exp(-exp(-(X - loc) / scale))</code>.</p>
</td></tr>
<tr><td><code id="tfb_gumbel_cdf_+3A_scale">scale</code></td>
<td>
<p>Positive Float-like <code>Tensor</code> that is the same dtype and is
broadcastable with <code>loc</code>.
This is <code>scale</code> in <code>Y = g(X) = exp(-exp(-(X - loc) / scale))</code>.</p>
</td></tr>
<tr><td><code id="tfb_gumbel_cdf_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_gumbel_cdf_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<div class="sourceCode"><pre>Y ~ GumbelCDF(loc, scale)
pdf(y; loc, scale) = exp(-( (y - loc) / scale + exp(- (y - loc) / scale) ) ) / scale
</pre></div>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_identity'>Computes<code>Y = g(X) = X</code></h2><span id='topic+tfb_identity'></span>

<h3>Description</h3>

<p>Computes<code>Y = g(X) = X</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_identity(validate_args = FALSE, name = "identity")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_identity_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_identity_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_inline'>Bijector constructed from custom functions</h2><span id='topic+tfb_inline'></span>

<h3>Description</h3>

<p>Bijector constructed from custom functions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_inline(
  forward_fn = NULL,
  inverse_fn = NULL,
  inverse_log_det_jacobian_fn = NULL,
  forward_log_det_jacobian_fn = NULL,
  forward_event_shape_fn = NULL,
  forward_event_shape_tensor_fn = NULL,
  inverse_event_shape_fn = NULL,
  inverse_event_shape_tensor_fn = NULL,
  is_constant_jacobian = NULL,
  validate_args = FALSE,
  forward_min_event_ndims = NULL,
  inverse_min_event_ndims = NULL,
  name = "inline"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_inline_+3A_forward_fn">forward_fn</code></td>
<td>
<p>Function implementing the forward transformation.</p>
</td></tr>
<tr><td><code id="tfb_inline_+3A_inverse_fn">inverse_fn</code></td>
<td>
<p>Function implementing the inverse transformation.</p>
</td></tr>
<tr><td><code id="tfb_inline_+3A_inverse_log_det_jacobian_fn">inverse_log_det_jacobian_fn</code></td>
<td>
<p>Function implementing the log_det_jacobian of the forward transformation.</p>
</td></tr>
<tr><td><code id="tfb_inline_+3A_forward_log_det_jacobian_fn">forward_log_det_jacobian_fn</code></td>
<td>
<p>Function implementing the log_det_jacobian of the inverse transformation.</p>
</td></tr>
<tr><td><code id="tfb_inline_+3A_forward_event_shape_fn">forward_event_shape_fn</code></td>
<td>
<p>Function implementing non-identical static event shape changes. Default: shape is assumed unchanged.</p>
</td></tr>
<tr><td><code id="tfb_inline_+3A_forward_event_shape_tensor_fn">forward_event_shape_tensor_fn</code></td>
<td>
<p>Function implementing non-identical event shape changes. Default: shape is assumed unchanged.</p>
</td></tr>
<tr><td><code id="tfb_inline_+3A_inverse_event_shape_fn">inverse_event_shape_fn</code></td>
<td>
<p>Function implementing non-identical static event shape changes. Default: shape is assumed unchanged.</p>
</td></tr>
<tr><td><code id="tfb_inline_+3A_inverse_event_shape_tensor_fn">inverse_event_shape_tensor_fn</code></td>
<td>
<p>Function implementing non-identical event shape changes. Default: shape is assumed unchanged.</p>
</td></tr>
<tr><td><code id="tfb_inline_+3A_is_constant_jacobian">is_constant_jacobian</code></td>
<td>
<p>Logical indicating that the Jacobian is constant for all input arguments.</p>
</td></tr>
<tr><td><code id="tfb_inline_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_inline_+3A_forward_min_event_ndims">forward_min_event_ndims</code></td>
<td>
<p>Integer indicating the minimal dimensionality this bijector acts on.</p>
</td></tr>
<tr><td><code id="tfb_inline_+3A_inverse_min_event_ndims">inverse_min_event_ndims</code></td>
<td>
<p>Integer indicating the minimal dimensionality this bijector acts on.</p>
</td></tr>
<tr><td><code id="tfb_inline_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_inverse'>Returns the inverse Bijector evaluation, i.e., <code>X = g^{-1}(Y)</code>.</h2><span id='topic+tfb_inverse'></span>

<h3>Description</h3>

<p>Returns the inverse Bijector evaluation, i.e., <code>X = g^{-1}(Y)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_inverse(bijector, y, name = "inverse")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_inverse_+3A_bijector">bijector</code></td>
<td>
<p>The bijector to apply</p>
</td></tr>
<tr><td><code id="tfb_inverse_+3A_y">y</code></td>
<td>
<p>Tensor. The input to the &quot;inverse&quot; evaluation.</p>
</td></tr>
<tr><td><code id="tfb_inverse_+3A_name">name</code></td>
<td>
<p>name of the operation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tensor
</p>


<h3>See Also</h3>

<p>Other bijector_methods: 
<code><a href="#topic+tfb_forward_log_det_jacobian">tfb_forward_log_det_jacobian</a>()</code>,
<code><a href="#topic+tfb_forward">tfb_forward</a>()</code>,
<code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  b &lt;- tfb_affine_scalar(shift = 1, scale = 2)
  x &lt;- 10
  y &lt;- b %&gt;% tfb_forward(x)
  b %&gt;% tfb_inverse(y)

</code></pre>

<hr>
<h2 id='tfb_inverse_log_det_jacobian'>Returns the result of the inverse evaluation of the log determinant of the Jacobian</h2><span id='topic+tfb_inverse_log_det_jacobian'></span>

<h3>Description</h3>

<p>Returns the result of the inverse evaluation of the log determinant of the Jacobian
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_inverse_log_det_jacobian(
  bijector,
  y,
  event_ndims,
  name = "inverse_log_det_jacobian"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_inverse_log_det_jacobian_+3A_bijector">bijector</code></td>
<td>
<p>The bijector to apply</p>
</td></tr>
<tr><td><code id="tfb_inverse_log_det_jacobian_+3A_y">y</code></td>
<td>
<p>Tensor. The input to the &quot;inverse&quot; Jacobian determinant evaluation.</p>
</td></tr>
<tr><td><code id="tfb_inverse_log_det_jacobian_+3A_event_ndims">event_ndims</code></td>
<td>
<p>Number of dimensions in the probabilistic events being transformed.
Must be greater than or equal to bijector$inverse_min_event_ndims. The result is summed over the final
dimensions to produce a scalar Jacobian determinant for each event, i.e. it has shape
x$shape$ndims - event_ndims dimensions.</p>
</td></tr>
<tr><td><code id="tfb_inverse_log_det_jacobian_+3A_name">name</code></td>
<td>
<p>name of the operation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a tensor
</p>


<h3>See Also</h3>

<p>Other bijector_methods: 
<code><a href="#topic+tfb_forward_log_det_jacobian">tfb_forward_log_det_jacobian</a>()</code>,
<code><a href="#topic+tfb_forward">tfb_forward</a>()</code>,
<code><a href="#topic+tfb_inverse">tfb_inverse</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  b &lt;- tfb_affine_scalar(shift = 1, scale = 2)
  x &lt;- 10
  y &lt;- b %&gt;% tfb_forward(x)
  b %&gt;% tfb_inverse_log_det_jacobian(y, event_ndims = 0)

</code></pre>

<hr>
<h2 id='tfb_invert'>Bijector which inverts another Bijector</h2><span id='topic+tfb_invert'></span>

<h3>Description</h3>

<p>Creates a Bijector which swaps the meaning of inverse and forward.
Note: An inverted bijector's inverse_log_det_jacobian is often more
efficient if the base bijector implements _forward_log_det_jacobian. If
_forward_log_det_jacobian is not implemented then the following code is
used:
<code>y = b$inverse(x)</code>
<code> -b$inverse_log_det_jacobian(y)</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_invert(bijector, validate_args = FALSE, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_invert_+3A_bijector">bijector</code></td>
<td>
<p>Bijector instance.</p>
</td></tr>
<tr><td><code id="tfb_invert_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_invert_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_iterated_sigmoid_centered'>Bijector which applies a Stick Breaking procedure.</h2><span id='topic+tfb_iterated_sigmoid_centered'></span>

<h3>Description</h3>

<p>Bijector which applies a Stick Breaking procedure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_iterated_sigmoid_centered(validate_args = FALSE, name = "iterated_sigmoid")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_iterated_sigmoid_centered_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_iterated_sigmoid_centered_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_kumaraswamy'>Computes<code>Y = g(X) = (1 - (1 - X)**(1 / b))**(1 / a)</code>, with X in <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code></h2><span id='topic+tfb_kumaraswamy'></span>

<h3>Description</h3>

<p>This bijector maps inputs from <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code> to <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code>. The inverse of the
bijector applied to a uniform random variable X ~ U(0, 1) gives back a
random variable with the <a href="https://en.wikipedia.org/wiki/Kumaraswamy_distribution">Kumaraswamy distribution</a>:
<code>Y ~ Kumaraswamy(a, b)</code>
<code style="white-space: pre;">&#8288;pdf(y; a, b, 0 &lt;= y &lt;= 1) = a * b * y ** (a - 1) * (1 - y**a) ** (b - 1)&#8288;</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_kumaraswamy(
  concentration1 = NULL,
  concentration0 = NULL,
  validate_args = FALSE,
  name = "kumaraswamy"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_kumaraswamy_+3A_concentration1">concentration1</code></td>
<td>
<p>float scalar indicating the transform power, i.e.,
<code style="white-space: pre;">&#8288;Y = g(X) = (1 - (1 - X)**(1 / b))**(1 / a) where a is concentration1.&#8288;</code></p>
</td></tr>
<tr><td><code id="tfb_kumaraswamy_+3A_concentration0">concentration0</code></td>
<td>
<p>float scalar indicating the transform power,
i.e., <code>Y = g(X) = (1 - (1 - X)**(1 / b))**(1 / a)</code> where b is concentration0.</p>
</td></tr>
<tr><td><code id="tfb_kumaraswamy_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_kumaraswamy_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_kumaraswamy_cdf'>Computes<code>Y = g(X) = (1 - (1 - X)**(1 / b))**(1 / a)</code>, with X in <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code></h2><span id='topic+tfb_kumaraswamy_cdf'></span>

<h3>Description</h3>

<p>This bijector maps inputs from <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code> to <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code>. The inverse of the
bijector applied to a uniform random variable X ~ U(0, 1) gives back a
random variable with the <a href="https://en.wikipedia.org/wiki/Kumaraswamy_distribution">Kumaraswamy distribution</a>:
<code>Y ~ Kumaraswamy(a, b)</code>
<code style="white-space: pre;">&#8288;pdf(y; a, b, 0 &lt;= y &lt;= 1) = a * b * y ** (a - 1) * (1 - y**a) ** (b - 1)&#8288;</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_kumaraswamy_cdf(
  concentration1 = 1,
  concentration0 = 1,
  validate_args = FALSE,
  name = "kumaraswamy_cdf"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_kumaraswamy_cdf_+3A_concentration1">concentration1</code></td>
<td>
<p>float scalar indicating the transform power, i.e.,
<code style="white-space: pre;">&#8288;Y = g(X) = (1 - (1 - X)**(1 / b))**(1 / a) where a is concentration1.&#8288;</code></p>
</td></tr>
<tr><td><code id="tfb_kumaraswamy_cdf_+3A_concentration0">concentration0</code></td>
<td>
<p>float scalar indicating the transform power,
i.e., <code>Y = g(X) = (1 - (1 - X)**(1 / b))**(1 / a)</code> where b is concentration0.</p>
</td></tr>
<tr><td><code id="tfb_kumaraswamy_cdf_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_kumaraswamy_cdf_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_lambert_w_tail'>LambertWTail transformation for heavy-tail Lambert W x F random variables.</h2><span id='topic+tfb_lambert_w_tail'></span>

<h3>Description</h3>

<p>A random variable Y has a Lambert W x F distribution if W_tau(Y) = X has
distribution F, where tau = (shift, scale, tail) parameterizes the inverse
transformation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_lambert_w_tail(
  shift = NULL,
  scale = NULL,
  tailweight = NULL,
  validate_args = FALSE,
  name = "lambertw_tail"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_lambert_w_tail_+3A_shift">shift</code></td>
<td>
<p>Floating point tensor; the shift for centering (uncentering) the
input (output) random variable(s).</p>
</td></tr>
<tr><td><code id="tfb_lambert_w_tail_+3A_scale">scale</code></td>
<td>
<p>Floating point tensor; the scaling (unscaling) of the input
(output) random variable(s). Must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfb_lambert_w_tail_+3A_tailweight">tailweight</code></td>
<td>
<p>Floating point tensor; the tail behaviors of the output random
variable(s).  Must contain only non-negative values.</p>
</td></tr>
<tr><td><code id="tfb_lambert_w_tail_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_lambert_w_tail_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This bijector defines the transformation underlying Lambert W x F
distributions that transform an input random variable to an output
random variable with heavier tails. It is defined as
Y = (U * exp(0.5 * tail * U^2)) * scale + shift,  tail &gt;= 0
where U = (X - shift) / scale is a shifted/scaled input random variable, and
tail &gt;= 0 is the tail parameter.
</p>
<p>Attributes:
shift: shift to center (uncenter) the input data.
scale: scale to normalize (de-normalize) the input data.
tailweight: Tail parameter <code>delta</code> of heavy-tail transformation; must be &gt;= 0.
</p>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_masked_autoregressive_default_template'>Masked Autoregressive Density Estimator</h2><span id='topic+tfb_masked_autoregressive_default_template'></span>

<h3>Description</h3>

<p>This will be wrapped in a make_template to ensure the variables are only
created once. It takes the input and returns the loc (&quot;mu&quot; in
Germain et al. (2015)) and log_scale (&quot;alpha&quot; in Germain et al. (2015)) from
the MADE network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_masked_autoregressive_default_template(
  hidden_layers,
  shift_only = FALSE,
  activation = tf$nn$relu,
  log_scale_min_clip = -5,
  log_scale_max_clip = 3,
  log_scale_clip_gradient = FALSE,
  name = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_masked_autoregressive_default_template_+3A_hidden_layers">hidden_layers</code></td>
<td>
<p>list-like of non-negative integer, scalars indicating the number
of units in each hidden layer. Default: <code>list(512, 512)</code>.</p>
</td></tr>
<tr><td><code id="tfb_masked_autoregressive_default_template_+3A_shift_only">shift_only</code></td>
<td>
<p>logical indicating if only the shift term shall be
computed. Default: FALSE.</p>
</td></tr>
<tr><td><code id="tfb_masked_autoregressive_default_template_+3A_activation">activation</code></td>
<td>
<p>Activation function (callable). Explicitly setting to NULL implies a linear activation.</p>
</td></tr>
<tr><td><code id="tfb_masked_autoregressive_default_template_+3A_log_scale_min_clip">log_scale_min_clip</code></td>
<td>
<p>float-like scalar Tensor, or a Tensor with the same shape as log_scale. The minimum value to clip by. Default: -5.</p>
</td></tr>
<tr><td><code id="tfb_masked_autoregressive_default_template_+3A_log_scale_max_clip">log_scale_max_clip</code></td>
<td>
<p>float-like scalar Tensor, or a Tensor with the same shape as log_scale. The maximum value to clip by. Default: 3.</p>
</td></tr>
<tr><td><code id="tfb_masked_autoregressive_default_template_+3A_log_scale_clip_gradient">log_scale_clip_gradient</code></td>
<td>
<p>logical indicating that the gradient of tf$clip_by_value should be preserved. Default: FALSE.</p>
</td></tr>
<tr><td><code id="tfb_masked_autoregressive_default_template_+3A_name">name</code></td>
<td>
<p>A name for ops managed by this function. Default: &quot;tfb_masked_autoregressive_default_template&quot;.</p>
</td></tr>
<tr><td><code id="tfb_masked_autoregressive_default_template_+3A_...">...</code></td>
<td>
<p><code>tf$layers$dense</code> arguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Warning: This function uses masked_dense to create randomly initialized
<code>tf$Variables</code>. It is presumed that these will be fit, just as you would any
other neural architecture which uses <code>tf$layers$dense</code>.
</p>
<p>About Hidden Layers
Each element of hidden_layers should be greater than the input_depth
(i.e., <code>input_depth = tf$shape(input)[-1]</code> where input is the input to the
neural network). This is necessary to ensure the autoregressivity property.
</p>
<p>About Clipping
This function also optionally clips the log_scale (but possibly not its
gradient). This is useful because if log_scale is too small/large it might
underflow/overflow making it impossible for the MaskedAutoregressiveFlow
bijector to implement a bijection. Additionally, the log_scale_clip_gradient
bool indicates whether the gradient should also be clipped. The default does
not clip the gradient; this is useful because it still provides gradient
information (for fitting) yet solves the numerical stability problem. I.e.,
log_scale_clip_gradient = FALSE means <code style="white-space: pre;">&#8288;grad[exp(clip(x))] = grad[x] exp(clip(x))&#8288;</code>
rather than the usual <code style="white-space: pre;">&#8288;grad[clip(x)] exp(clip(x))&#8288;</code>.
</p>


<h3>Value</h3>

<p>list of:
</p>

<ul>
<li><p> shift: <code>Float</code>-like <code>Tensor</code> of shift terms
</p>
</li>
<li><p> log_scale: <code>Float</code>-like <code>Tensor</code> of log(scale) terms
</p>
</li></ul>



<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1502.03509">Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. MADE: Masked Autoencoder for Distribution Estimation. In <em>International Conference on Machine Learning</em>, 2015.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_masked_autoregressive_flow'>Affine MaskedAutoregressiveFlow bijector</h2><span id='topic+tfb_masked_autoregressive_flow'></span>

<h3>Description</h3>

<p>The affine autoregressive flow (Papamakarios et al., 2016) provides a
relatively simple framework for user-specified (deep) architectures to learn a
distribution over continuous events. Regarding terminology,
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_masked_autoregressive_flow(
  shift_and_log_scale_fn,
  is_constant_jacobian = FALSE,
  unroll_loop = FALSE,
  event_ndims = 1L,
  validate_args = FALSE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_masked_autoregressive_flow_+3A_shift_and_log_scale_fn">shift_and_log_scale_fn</code></td>
<td>
<p>Function which computes shift and log_scale from both the
forward domain (x) and the inverse domain (y).
Calculation must respect the &quot;autoregressive property&quot;. Suggested default:
tfb_masked_autoregressive_default_template(hidden_layers=...).
Typically the function contains <code>tf$Variables</code> and is wrapped using <code>tf$make_template</code>.
Returning NULL for either (both) shift, log_scale is equivalent to (but more efficient than) returning zero.</p>
</td></tr>
<tr><td><code id="tfb_masked_autoregressive_flow_+3A_is_constant_jacobian">is_constant_jacobian</code></td>
<td>
<p>Logical, default: FALSE. When TRUE the implementation assumes log_scale
does not depend on the forward domain (x) or inverse domain (y) values.
(No validation is made; is_constant_jacobian=FALSE is always safe but possibly computationally inefficient.)</p>
</td></tr>
<tr><td><code id="tfb_masked_autoregressive_flow_+3A_unroll_loop">unroll_loop</code></td>
<td>
<p>Logical indicating whether the <code>tf$while_loop</code> in _forward should be replaced with a
static for loop. Requires that the final dimension of x be known at graph construction time. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="tfb_masked_autoregressive_flow_+3A_event_ndims">event_ndims</code></td>
<td>
<p>integer, the intrinsic dimensionality of this bijector.
1 corresponds to a simple vector autoregressive bijector as implemented by the
<code>tfb_masked_autoregressive_default_template</code>, 2 might be useful for a 2D convolutional shift_and_log_scale_fn and so on.</p>
</td></tr>
<tr><td><code id="tfb_masked_autoregressive_flow_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_masked_autoregressive_flow_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>&quot;Autoregressive models decompose the joint density as a product of
conditionals, and model each conditional in turn. Normalizing flows
transform a base density (e.g. a standard Gaussian) into the target density
by an invertible transformation with tractable Jacobian.&quot; (Papamakarios et al., 2016)
</p>
<p>In other words, the &quot;autoregressive property&quot; is equivalent to the
decomposition, <code style="white-space: pre;">&#8288;p(x) = prod{ p(x[perm[i]] | x[perm[0:i]]) : i=0, ..., d }&#8288;</code>
where perm is some permutation of <code style="white-space: pre;">&#8288;{0, ..., d}&#8288;</code>. In the simple case where
the permutation is identity this reduces to:
</p>
<p><code style="white-space: pre;">&#8288;p(x) = prod{ p(x[i] | x[0:i]) : i=0, ..., d }&#8288;</code>. The provided
shift_and_log_scale_fn, tfb_masked_autoregressive_default_template, achieves
this property by zeroing out weights in its masked_dense layers.
In TensorFlow Probability, &quot;normalizing flows&quot; are implemented as
tfp.bijectors.Bijectors. The forward &quot;autoregression&quot; is implemented
using a tf.while_loop and a deep neural network (DNN) with masked weights
such that the autoregressive property is automatically met in the inverse.
A TransformedDistribution using MaskedAutoregressiveFlow(...) uses the
(expensive) forward-mode calculation to draw samples and the (cheap)
reverse-mode calculation to compute log-probabilities. Conversely, a
TransformedDistribution using Invert(MaskedAutoregressiveFlow(...)) uses
the (expensive) forward-mode calculation to compute log-probabilities and the
(cheap) reverse-mode calculation to compute samples.
</p>
<p>Given a shift_and_log_scale_fn, the forward and inverse transformations are
(a sequence of) affine transformations. A &quot;valid&quot; shift_and_log_scale_fn
must compute each shift (aka loc or &quot;mu&quot; in Germain et al. (2015)])
and log(scale) (aka &quot;alpha&quot; in Germain et al. (2015)) such that ech
are broadcastable with the arguments to forward and inverse, i.e., such
that the calculations in forward, inverse below are possible.
</p>
<p>For convenience, tfb_masked_autoregressive_default_template is offered as a
possible shift_and_log_scale_fn function. It implements the MADE
architecture (Germain et al., 2015). MADE is a feed-forward network that
computes a shift and log(scale) using masked_dense layers in a deep
neural network. Weights are masked to ensure the autoregressive property. It
is possible that this architecture is suboptimal for your task. To build
alternative networks, either change the arguments to
tfb_masked_autoregressive_default_template, use the masked_dense function to
roll-out your own, or use some other architecture, e.g., using tf.layers.
Warning: no attempt is made to validate that the shift_and_log_scale_fn
enforces the &quot;autoregressive property&quot;.
</p>
<p>Assuming shift_and_log_scale_fn has valid shape and autoregressive semantics,
the forward transformation is
</p>
<div class="sourceCode"><pre>def forward(x):
   y = zeros_like(x)
   event_size = x.shape[-event_dims:].num_elements()
   for _ in range(event_size):
     shift, log_scale = shift_and_log_scale_fn(y)
     y = x * tf.exp(log_scale) + shift
   return y
</pre></div>
<p>and the inverse transformation is
</p>
<div class="sourceCode"><pre>def inverse(y):
  shift, log_scale = shift_and_log_scale_fn(y)
  return (y - shift) / tf.exp(log_scale)
</pre></div>
<p>Notice that the inverse does not need a for-loop. This is because in the
forward pass each calculation of shift and log_scale is based on the y
calculated so far (not x). In the inverse, the y is fully known, thus is
equivalent to the scaling used in forward after event_size passes, i.e.,
the &quot;last&quot; y used to compute shift, log_scale.
(Roughly speaking, this also proves the transform is bijective.)
</p>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1502.03509">Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. MADE: Masked Autoencoder for Distribution Estimation. In <em>International Conference on Machine Learning</em>, 2015.</a>
</p>
</li>
<li> <p><a href="https://arxiv.org/abs/1606.04934">Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improving Variational Inference with Inverse Autoregressive Flow. In <em>Neural Information Processing Systems</em>, 2016.</a>
</p>
</li>
<li> <p><a href="https://arxiv.org/abs/1705.07057">George Papamakarios, Theo Pavlakou, and Iain Murray. Masked Autoregressive Flow for Density Estimation. In <em>Neural Information Processing Systems</em>, 2017.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_masked_dense'>Autoregressively masked dense layer</h2><span id='topic+tfb_masked_dense'></span>

<h3>Description</h3>

<p>Analogous to <code>tf$layers$dense</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_masked_dense(
  inputs,
  units,
  num_blocks = NULL,
  exclusive = FALSE,
  kernel_initializer = NULL,
  reuse = NULL,
  name = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_masked_dense_+3A_inputs">inputs</code></td>
<td>
<p>Tensor input.</p>
</td></tr>
<tr><td><code id="tfb_masked_dense_+3A_units">units</code></td>
<td>
<p>integer scalar representing the dimensionality of the output space.</p>
</td></tr>
<tr><td><code id="tfb_masked_dense_+3A_num_blocks">num_blocks</code></td>
<td>
<p>integer scalar representing the number of blocks for the MADE masks.</p>
</td></tr>
<tr><td><code id="tfb_masked_dense_+3A_exclusive">exclusive</code></td>
<td>
<p>logical scalar representing whether to zero the diagonal of
the mask, used for the first layer of a MADE.</p>
</td></tr>
<tr><td><code id="tfb_masked_dense_+3A_kernel_initializer">kernel_initializer</code></td>
<td>
<p>Initializer function for the weight matrix.
If NULL (default), weights are initialized using the <code>tf$glorot_random_initializer</code></p>
</td></tr>
<tr><td><code id="tfb_masked_dense_+3A_reuse">reuse</code></td>
<td>
<p>logical scalar representing whether to reuse the weights of a previous layer by the same name.</p>
</td></tr>
<tr><td><code id="tfb_masked_dense_+3A_name">name</code></td>
<td>
<p>string used to describe ops managed by this function.</p>
</td></tr>
<tr><td><code id="tfb_masked_dense_+3A_...">...</code></td>
<td>
<p><code>tf$layers$dense</code> arguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See Germain et al. (2015)for detailed explanation.
</p>


<h3>Value</h3>

<p>tensor
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1502.03509">Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. MADE: Masked Autoencoder for Distribution Estimation. In <em>International Conference on Machine Learning</em>, 2015.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_matrix_inverse_tri_l'>Computes <code>g(L) = inv(L)</code>, where L is a lower-triangular matrix</h2><span id='topic+tfb_matrix_inverse_tri_l'></span>

<h3>Description</h3>

<p>L must be nonsingular; equivalently, all diagonal entries of L must be nonzero.
The input must have rank &gt;= 2.  The input is treated as a batch of matrices
with batch shape <code style="white-space: pre;">&#8288;input.shape[:-2]&#8288;</code>, where each matrix has dimensions
<code>input.shape[-2]</code> by <code>input.shape[-1]</code> (hence <code>input.shape[-2]</code> must equal <code>input.shape[-1]</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_matrix_inverse_tri_l(validate_args = FALSE, name = "matrix_inverse_tril")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_matrix_inverse_tri_l_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_matrix_inverse_tri_l_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_matvec_lu'>Matrix-vector multiply using LU decomposition</h2><span id='topic+tfb_matvec_lu'></span>

<h3>Description</h3>

<p>This bijector is identical to the &quot;Convolution1x1&quot; used in Glow (Kingma and Dhariwal, 2018).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_matvec_lu(lower_upper, permutation, validate_args = FALSE, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_matvec_lu_+3A_lower_upper">lower_upper</code></td>
<td>
<p>The LU factorization as returned by <code>tf$linalg$lu</code>.</p>
</td></tr>
<tr><td><code id="tfb_matvec_lu_+3A_permutation">permutation</code></td>
<td>
<p>The LU factorization permutation as returned by <code>tf$linalg$lu</code>.</p>
</td></tr>
<tr><td><code id="tfb_matvec_lu_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_matvec_lu_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Warning: this bijector never verifies the scale matrix (as parameterized by LU
ecomposition) is invertible. Ensuring this is the case is the caller's responsibility.
</p>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1807.03039">Diederik P. Kingma, Prafulla Dhariwal. Glow: Generative Flow with Invertible 1x1 Convolutions. <em>arXiv preprint arXiv:1807.03039</em>, 2018.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_normal_cdf'>Computes<code>Y = g(X) = NormalCDF(x)</code></h2><span id='topic+tfb_normal_cdf'></span>

<h3>Description</h3>

<p>This bijector maps inputs from <code style="white-space: pre;">&#8288;[-inf, inf]&#8288;</code> to <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code>. The inverse of the
bijector applied to a uniform random variable <code>X ~ U(0, 1)</code> gives back a
random variable with the <a href="https://en.wikipedia.org/wiki/Normal_distribution">Normal distribution</a>:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_normal_cdf(validate_args = FALSE, name = "normal")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_normal_cdf_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_normal_cdf_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>Y ~ Normal(0, 1)</code>
<code style="white-space: pre;">&#8288;pdf(y; 0., 1.) = 1 / sqrt(2 * pi) * exp(-y ** 2 / 2)&#8288;</code>
</p>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_ordered'>Bijector which maps a tensor x_k that has increasing elements in the last dimension to an unconstrained tensor y_k</h2><span id='topic+tfb_ordered'></span>

<h3>Description</h3>

<p>Both the domain and the codomain of the mapping is <code style="white-space: pre;">&#8288;[-inf, inf]&#8288;</code>, however,
the input of the forward mapping must be strictly increasing.
The inverse of the bijector applied to a normal random vector <code>y ~ N(0, 1)</code>
gives back a sorted random vector with the same distribution <code>x ~ N(0, 1)</code>
where x = sort(y)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_ordered(validate_args = FALSE, name = "ordered")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_ordered_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_ordered_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>On the last dimension of the tensor, Ordered bijector performs:
<code>y[0] = x[0]</code>
<code style="white-space: pre;">&#8288;y[1:] = tf$log(x[1:] - x[:-1])&#8288;</code>
</p>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_pad'>Pads a value to the <code>event_shape</code> of a <code>Tensor</code>.</h2><span id='topic+tfb_pad'></span>

<h3>Description</h3>

<p>The semantics of <code>bijector_pad</code> generally follow that of <code>tf$pad()</code>
except that <code>bijector_pad</code>'s <code>paddings</code> argument applies to the rightmost
dimensions. Additionally, the new argument <code>axis</code> enables overriding the
dimensions to which <code>paddings</code> is applied. Like <code>paddings</code>, the <code>axis</code>
argument is also relative to the rightmost dimension and must therefore be
negative.
The argument <code>paddings</code> is a vector of <code>integer</code> pairs each representing the
number of left and/or right <code>constant_values</code> to pad to the corresponding
righmost dimensions. That is, unless <code>axis</code> is specified<code style="white-space: pre;">&#8288;, specifiying &#8288;</code>k<code>different</code>paddings<code style="white-space: pre;">&#8288;means the rightmost&#8288;</code>k<code style="white-space: pre;">&#8288;dimensions will be "grown" by the sum of the respective&#8288;</code>paddings<code style="white-space: pre;">&#8288;row. When&#8288;</code>axis<code style="white-space: pre;">&#8288;is specified, it indicates the dimension to which the corresponding&#8288;</code>paddings<code style="white-space: pre;">&#8288;element is applied. By default&#8288;</code>axis<code>is</code>NULL<code style="white-space: pre;">&#8288;which means it is logically equivalent to&#8288;</code>range(start=-len(paddings), limit=0)', i.e., the rightmost dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_pad(
  paddings = list(c(0, 1)),
  mode = "CONSTANT",
  constant_values = 0,
  axis = NULL,
  validate_args = FALSE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_pad_+3A_paddings">paddings</code></td>
<td>
<p>A vector-shaped <code>Tensor</code> of <code>integer</code> pairs representing the number
of elements to pad on the left and right, respectively.
Default value: <code>list(reticulate::tuple(0L, 1L))</code>.</p>
</td></tr>
<tr><td><code id="tfb_pad_+3A_mode">mode</code></td>
<td>
<p>One of <code>'CONSTANT'</code>, <code>'REFLECT'</code>, or <code>'SYMMETRIC'</code>
(case-insensitive). For more details, see <code>tf$pad</code>.</p>
</td></tr>
<tr><td><code id="tfb_pad_+3A_constant_values">constant_values</code></td>
<td>
<p>In &quot;CONSTANT&quot; mode, the scalar pad value to use. Must be
same type as <code>tensor</code>. For more details, see <code>tf$pad</code>.</p>
</td></tr>
<tr><td><code id="tfb_pad_+3A_axis">axis</code></td>
<td>
<p>The dimensions for which <code>paddings</code> are applied. Must be 1:1 with
<code>paddings</code> or <code>NULL</code>.
Default value: <code>NULL</code> (i.e., <code>tf$range(start = -length(paddings), limit = 0)</code>).</p>
</td></tr>
<tr><td><code id="tfb_pad_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_pad_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_permute'>Permutes the rightmost dimension of a Tensor</h2><span id='topic+tfb_permute'></span>

<h3>Description</h3>

<p>Permutes the rightmost dimension of a Tensor
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_permute(permutation, axis = -1L, validate_args = FALSE, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_permute_+3A_permutation">permutation</code></td>
<td>
<p>An integer-like vector-shaped Tensor representing the
permutation to apply to the axis dimension of the transformed Tensor.</p>
</td></tr>
<tr><td><code id="tfb_permute_+3A_axis">axis</code></td>
<td>
<p>Scalar integer Tensor representing the dimension over which to tf$gather.
axis must be relative to the end (reading left to right) thus must be negative.
Default value: -1 (i.e., right-most).</p>
</td></tr>
<tr><td><code id="tfb_permute_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_permute_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_power_transform'>Computes<code>Y = g(X) = (1 + X * c)**(1 / c)</code>, where <code>X &gt;= -1 / c</code></h2><span id='topic+tfb_power_transform'></span>

<h3>Description</h3>

<p>The <a href="https://en.wikipedia.org/wiki/Power_transform">power transform</a> maps
inputs from <code style="white-space: pre;">&#8288;[0, inf]&#8288;</code> to <code style="white-space: pre;">&#8288;[-1/c, inf]&#8288;</code>; this is equivalent to the inverse of this bijector.
This bijector is equivalent to the Exp bijector when c=0.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_power_transform(power, validate_args = FALSE, name = "power_transform")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_power_transform_+3A_power">power</code></td>
<td>
<p>float scalar indicating the transform power, i.e.,
<code>Y = g(X) = (1 + X * c)**(1 / c)</code> where c is the power.</p>
</td></tr>
<tr><td><code id="tfb_power_transform_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_power_transform_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_rational_quadratic_spline'>A piecewise rational quadratic spline, as developed in Conor et al.(2019).</h2><span id='topic+tfb_rational_quadratic_spline'></span>

<h3>Description</h3>

<p>This transformation represents a monotonically increasing piecewise rational
quadratic function. Outside of the bounds of <code>knot_x</code>/<code>knot_y</code>, the transform
behaves as an identity function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_rational_quadratic_spline(
  bin_widths,
  bin_heights,
  knot_slopes,
  range_min = -1,
  validate_args = FALSE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_rational_quadratic_spline_+3A_bin_widths">bin_widths</code></td>
<td>
<p>The widths of the spans between subsequent knot <code>x</code> positions,
a floating point <code>Tensor</code>. Must be positive, and at least 1-D. Innermost
axis must sum to the same value as <code>bin_heights</code>. The knot <code>x</code> positions
will be a first at <code>range_min</code>, followed by knots at <code>range_min + cumsum(bin_widths, axis=-1)</code>.</p>
</td></tr>
<tr><td><code id="tfb_rational_quadratic_spline_+3A_bin_heights">bin_heights</code></td>
<td>
<p>The heights of the spans between subsequent knot <code>y</code>
positions, a floating point <code>Tensor</code>. Must be positive, and at least
1-D. Innermost axis must sum to the same value as <code>bin_widths</code>. The knot
<code>y</code> positions will be a first at <code>range_min</code>, followed by knots at
<code>range_min + cumsum(bin_heights, axis=-1)</code>.</p>
</td></tr>
<tr><td><code id="tfb_rational_quadratic_spline_+3A_knot_slopes">knot_slopes</code></td>
<td>
<p>The slope of the spline at each knot, a floating point
<code>Tensor</code>. Must be positive. <code>1</code>s are implicitly padded for the first and
last implicit knots corresponding to <code>range_min</code> and <code>range_min + sum(bin_widths, axis=-1)</code>. Innermost axis size should be 1 less than
that of <code>bin_widths</code>/<code>bin_heights</code>, or 1 for broadcasting.</p>
</td></tr>
<tr><td><code id="tfb_rational_quadratic_spline_+3A_range_min">range_min</code></td>
<td>
<p>The <code>x</code>/<code>y</code> position of the first knot, which has implicit
slope <code>1</code>. <code>range_max</code> is implicit, and can be computed as <code>range_min + sum(bin_widths, axis=-1)</code>. Scalar floating point <code>Tensor</code>.</p>
</td></tr>
<tr><td><code id="tfb_rational_quadratic_spline_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_rational_quadratic_spline_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Typically this bijector will be used as part of a chain, with splines for
trailing <code>x</code> dimensions conditioned on some of the earlier <code>x</code> dimensions, and
with the inverse then solved first for unconditioned dimensions, then using
conditioning derived from those inverses, and so forth.
</p>
<p>For each argument, the innermost axis indexes bins/knots and batch axes
index axes of <code>x</code>/<code>y</code> spaces. A <code>RationalQuadraticSpline</code> with a separate
transform for each of three dimensions might have <code>bin_widths</code> shaped
<code style="white-space: pre;">&#8288;[3, 32]&#8288;</code>. To use the same spline for each of <code>x</code>'s three dimensions we may
broadcast against <code>x</code> and use a <code>bin_widths</code> parameter shaped <code style="white-space: pre;">&#8288;[32]&#8288;</code>.
</p>
<p>Parameters will be broadcast against each other and against the input
<code>x</code>/<code>y</code>s, so if we want fixed slopes, we can use kwarg <code>knot_slopes=1</code>.
A typical recipe for acquiring compatible bin widths and heights would be:
</p>
<div class="sourceCode"><pre>nbins &lt;- unconstrained_vector$shape[-1]
range_min &lt;- 1
range_max &lt;- 1
min_bin_size = 1e-2
scale &lt;- range_max - range_min - nbins * min_bin_size
bin_widths = tf$math$softmax(unconstrained_vector) * scale + min_bin_size
</pre></div>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1906.04032">Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios. Neural Spline Flows. <em>arXiv preprint arXiv:1906.04032</em>, 2019.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_rayleigh_cdf'>Compute <code style="white-space: pre;">&#8288;Y = g(X) = 1 - exp( -(X/scale)**2 / 2 ), X &gt;= 0&#8288;</code>.</h2><span id='topic+tfb_rayleigh_cdf'></span>

<h3>Description</h3>

<p>This bijector maps inputs from <code style="white-space: pre;">&#8288;[0, inf]&#8288;</code> to <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code>. The inverse of the
bijector applied to a uniform random variable <code>X ~ U(0, 1)</code> gives back a
random variable with the
<a href="https://en.wikipedia.org/wiki/Rayleigh_distribution">Rayleigh distribution</a>:
</p>
<div class="sourceCode"><pre>Y ~ Rayleigh(scale)
pdf(y; scale, y &gt;= 0) = (1 / scale) * (y / scale) * exp(-(y / scale)**2 / 2)
</pre></div>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_rayleigh_cdf(scale, validate_args = FALSE, name = "rayleigh_cdf")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_rayleigh_cdf_+3A_scale">scale</code></td>
<td>
<p>Positive floating-point tensor.
This is <code>l</code> in <code style="white-space: pre;">&#8288;Y = g(X) = 1 - exp( -(X/l)**2 / 2 ), X &gt;= 0&#8288;</code>.</p>
</td></tr>
<tr><td><code id="tfb_rayleigh_cdf_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_rayleigh_cdf_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Likewise, the forward of this bijector is the Rayleigh distribution CDF.
</p>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_real_nvp'>RealNVP affine coupling layer for vector-valued events</h2><span id='topic+tfb_real_nvp'></span>

<h3>Description</h3>

<p>Real NVP models a normalizing flow on a D-dimensional distribution via a
single D-d-dimensional conditional distribution (Dinh et al., 2017):
<code>y[d:D] = x[d:D] * tf.exp(log_scale_fn(x[0:d])) + shift_fn(x[0:d])</code>
<code>y[0:d] = x[0:d]</code>
The last D-d units are scaled and shifted based on the first d units only,
while the first d units are 'masked' and left unchanged. Real NVP's
shift_and_log_scale_fn computes vector-valued quantities.
For scale-and-shift transforms that do not depend on any masked units, i.e.
d=0, use the tfb_affine bijector with learned parameters instead.
Masking is currently only supported for base distributions with
event_ndims=1. For more sophisticated masking schemes like checkerboard or
channel-wise masking (Papamakarios et al., 2016), use the tfb_permute
bijector to re-order desired masked units into the first d units. For base
distributions with event_ndims &gt; 1, use the tfb_reshape bijector to
flatten the event shape.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_real_nvp(
  num_masked,
  shift_and_log_scale_fn,
  is_constant_jacobian = FALSE,
  validate_args = FALSE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_real_nvp_+3A_num_masked">num_masked</code></td>
<td>
<p>integer indicating that the first d units of the event
should be masked. Must be in the closed interval <code style="white-space: pre;">&#8288;[1, D-1]&#8288;</code>, where D
is the event size of the base distribution.</p>
</td></tr>
<tr><td><code id="tfb_real_nvp_+3A_shift_and_log_scale_fn">shift_and_log_scale_fn</code></td>
<td>
<p>Function which computes shift and log_scale from both the
forward domain (x) and the inverse domain (y).
Calculation must respect the &quot;autoregressive property&quot;. Suggested default:
<code>tfb_real_nvp_default_template(hidden_layers=...)</code>.
Typically the function contains <code>tf$Variables</code> and is wrapped using <code>tf$make_template</code>.
Returning NULL for either (both) shift, log_scale is equivalent to (but more efficient than) returning zero.</p>
</td></tr>
<tr><td><code id="tfb_real_nvp_+3A_is_constant_jacobian">is_constant_jacobian</code></td>
<td>
<p>Logical, default: FALSE. When TRUE the implementation assumes log_scale
does not depend on the forward domain (x) or inverse domain (y) values.
(No validation is made; is_constant_jacobian=FALSE is always safe but possibly computationally inefficient.)</p>
</td></tr>
<tr><td><code id="tfb_real_nvp_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_real_nvp_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Recall that the MAF bijector (Papamakarios et al., 2016) implements a
normalizing flow via an autoregressive transformation. MAF and IAF have
opposite computational tradeoffs - MAF can train all units in parallel but
must sample units sequentially, while IAF must train units sequentially but
can sample in parallel. In contrast, Real NVP can compute both forward and
inverse computations in parallel. However, the lack of an autoregressive
transformations makes it less expressive on a per-bijector basis.
</p>
<p>A &quot;valid&quot; shift_and_log_scale_fn must compute each shift (aka loc or
&quot;mu&quot; in Papamakarios et al. (2016) and log(scale) (aka &quot;alpha&quot; in
Papamakarios et al. (2016)) such that each are broadcastable with the
arguments to forward and inverse, i.e., such that the calculations in
forward, inverse below are possible. For convenience,
real_nvp_default_nvp is offered as a possible shift_and_log_scale_fn function.
</p>
<p>NICE (Dinh et al., 2014) is a special case of the Real NVP bijector
which discards the scale transformation, resulting in a constant-time
inverse-log-determinant-Jacobian. To use a NICE bijector instead of Real
NVP, shift_and_log_scale_fn should return (shift, NULL), and
is_constant_jacobian should be set to TRUE in the RealNVP constructor.
Calling tfb_real_nvp_default_template with shift_only=TRUE returns one such
NICE-compatible shift_and_log_scale_fn.
</p>
<p>Caching: the scalar input depth D of the base distribution is not known at
construction time. The first call to any of forward(x), inverse(x),
inverse_log_det_jacobian(x), or forward_log_det_jacobian(x) memoizes
D, which is re-used in subsequent calls. This shape must be known prior to
graph execution (which is the case if using <code>tf$layers</code>).
</p>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1705.07057">George Papamakarios, Theo Pavlakou, and Iain Murray. Masked Autoregressive Flow for Density Estimation. In <em>Neural Information Processing Systems</em>, 2017.</a>
</p>
</li>
<li> <p><a href="https://arxiv.org/abs/1605.08803">Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density Estimation using Real NVP. In <em>International Conference on Learning Representations</em>, 2017.</a>
</p>
</li>
<li> <p><a href="https://arxiv.org/abs/1410.8516">Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear Independent Components Estimation.<em>arXiv preprint arXiv:1410.8516</em>,2014.</a>
</p>
</li>
<li> <p><a href="https://blog.evjang.com/2018/01/nf2.html">Eric Jang. Normalizing Flows Tutorial, Part 2: Modern Normalizing Flows. Technical Report_, 2018.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_real_nvp_default_template'>Build a scale-and-shift function using a multi-layer neural network</h2><span id='topic+tfb_real_nvp_default_template'></span>

<h3>Description</h3>

<p>This will be wrapped in a make_template to ensure the variables are only
created once. It takes the d-dimensional input <code>x[0:d]</code> and returns the <code>D-d</code>
dimensional outputs loc (&quot;mu&quot;) and log_scale (&quot;alpha&quot;).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_real_nvp_default_template(
  hidden_layers,
  shift_only = FALSE,
  activation = tf$nn$relu,
  name = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_real_nvp_default_template_+3A_hidden_layers">hidden_layers</code></td>
<td>
<p>list-like of non-negative integer, scalars indicating the number
of units in each hidden layer. Default: <code>list(512, 512)</code>.</p>
</td></tr>
<tr><td><code id="tfb_real_nvp_default_template_+3A_shift_only">shift_only</code></td>
<td>
<p>logical indicating if only the shift term shall be
computed (i.e. NICE bijector). Default: FALSE.</p>
</td></tr>
<tr><td><code id="tfb_real_nvp_default_template_+3A_activation">activation</code></td>
<td>
<p>Activation function (callable). Explicitly setting to NULL implies a linear activation.</p>
</td></tr>
<tr><td><code id="tfb_real_nvp_default_template_+3A_name">name</code></td>
<td>
<p>A name for ops managed by this function. Default: &quot;tfb_real_nvp_default_template&quot;.</p>
</td></tr>
<tr><td><code id="tfb_real_nvp_default_template_+3A_...">...</code></td>
<td>
<p>tf$layers$dense arguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The default template does not support conditioning and will raise an
exception if condition_kwargs are passed to it. To use conditioning in
real nvp bijector, implement a conditioned shift/scale template that
handles the condition_kwargs.
</p>


<h3>Value</h3>

<p>list of:
</p>

<ul>
<li><p> shift: <code>Float</code>-like <code>Tensor</code> of shift terms
</p>
</li>
<li><p> log_scale: <code>Float</code>-like <code>Tensor</code> of log(scale) terms
</p>
</li></ul>



<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1705.07057">George Papamakarios, Theo Pavlakou, and Iain Murray. Masked Autoregressive Flow for Density Estimation. In <em>Neural Information Processing Systems</em>, 2017.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_reciprocal'>A Bijector that computes <code>b(x) = 1. / x</code></h2><span id='topic+tfb_reciprocal'></span>

<h3>Description</h3>

<p>A Bijector that computes <code>b(x) = 1. / x</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_reciprocal(validate_args = FALSE, name = "reciprocal")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_reciprocal_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_reciprocal_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_reshape'>Reshapes the event_shape of a Tensor</h2><span id='topic+tfb_reshape'></span>

<h3>Description</h3>

<p>The semantics generally follow that of <code>tf$reshape()</code>, with a few differences:
</p>

<ul>
<li><p> The user must provide both the input and output shape, so that
the transformation can be inverted. If an input shape is not
specified, the default assumes a vector-shaped input, i.e.,
<code>event_shape_in = list(-1)</code>.
</p>
</li>
<li><p> The Reshape bijector automatically broadcasts over the leftmost
dimensions of its input (sample_shape and batch_shape); only
the rightmost event_ndims_in dimensions are reshaped. The
number of dimensions to reshape is inferred from the provided
event_shape_in (<code style="white-space: pre;">&#8288;event_ndims_in = length(event_shape_in))&#8288;</code>.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>tfb_reshape(
  event_shape_out,
  event_shape_in = c(-1),
  validate_args = FALSE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_reshape_+3A_event_shape_out">event_shape_out</code></td>
<td>
<p>An integer-like vector-shaped Tensor
representing the event shape of the transformed output.</p>
</td></tr>
<tr><td><code id="tfb_reshape_+3A_event_shape_in">event_shape_in</code></td>
<td>
<p>An optional integer-like vector-shape Tensor
representing the event shape of the input. This is required in
order to define inverse operations; the default of list(-1) assumes a vector-shaped input.</p>
</td></tr>
<tr><td><code id="tfb_reshape_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_reshape_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_scale'>Compute <code style="white-space: pre;">&#8288;Y = g(X; scale) = scale * X&#8288;</code>.</h2><span id='topic+tfb_scale'></span>

<h3>Description</h3>

<p>Examples:
</p>
<div class="sourceCode"><pre>Y &lt;- 2 * X
b &lt;- tfb_scale(scale = 2)
</pre></div>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_scale(
  scale = NULL,
  log_scale = NULL,
  validate_args = FALSE,
  name = "scale"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_scale_+3A_scale">scale</code></td>
<td>
<p>Floating-point <code>Tensor</code>.</p>
</td></tr>
<tr><td><code id="tfb_scale_+3A_log_scale">log_scale</code></td>
<td>
<p>Floating-point <code>Tensor</code>. Logarithm of the scale. If this is set
to <code>NULL</code>, no scale is applied. This should not be set if <code>scale</code> is set.</p>
</td></tr>
<tr><td><code id="tfb_scale_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_scale_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_scale_matvec_diag'>Compute <code style="white-space: pre;">&#8288;Y = g(X; scale) = scale @ X&#8288;</code></h2><span id='topic+tfb_scale_matvec_diag'></span>

<h3>Description</h3>

<p>In TF parlance, the <code>scale</code> term is logically equivalent to:
</p>
<div class="sourceCode"><pre>scale = tf$diag(scale_diag)
</pre></div>
<p>The <code>scale</code> term is applied without materializing a full dense matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_scale_matvec_diag(
  scale_diag,
  adjoint = FALSE,
  validate_args = FALSE,
  name = "scale_matvec_diag",
  dtype = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_scale_matvec_diag_+3A_scale_diag">scale_diag</code></td>
<td>
<p>Floating-point <code>Tensor</code> representing the diagonal matrix.
<code>scale_diag</code> has shape <code style="white-space: pre;">&#8288;[N1, N2, ...  k]&#8288;</code>, which represents a k x k
diagonal matrix.</p>
</td></tr>
<tr><td><code id="tfb_scale_matvec_diag_+3A_adjoint">adjoint</code></td>
<td>
<p><code>logical</code> indicating whether to use the <code>scale</code> matrix as
specified or its adjoint. Default value: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="tfb_scale_matvec_diag_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_scale_matvec_diag_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
<tr><td><code id="tfb_scale_matvec_diag_+3A_dtype">dtype</code></td>
<td>
<p><code>tf$DType</code> to prefer when converting args to <code>Tensor</code>s. Else, we
fall back to a common dtype inferred from the args, finally falling back
to <code>float32</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_scale_matvec_linear_operator'>Compute <code style="white-space: pre;">&#8288;Y = g(X; scale) = scale @ X&#8288;</code>.</h2><span id='topic+tfb_scale_matvec_linear_operator'></span>

<h3>Description</h3>

<p><code>scale</code> is a <code>LinearOperator</code>.
If <code>X</code> is a scalar then the forward transformation is: <code>scale * X</code>
where <code>*</code> denotes broadcasted elementwise product.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_scale_matvec_linear_operator(
  scale,
  adjoint = FALSE,
  validate_args = FALSE,
  name = "scale_matvec_linear_operator"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_scale_matvec_linear_operator_+3A_scale">scale</code></td>
<td>
<p>Subclass of <code>LinearOperator</code>. Represents the (batch, non-singular)
linear transformation by which the <code>Bijector</code> transforms inputs.</p>
</td></tr>
<tr><td><code id="tfb_scale_matvec_linear_operator_+3A_adjoint">adjoint</code></td>
<td>
<p><code>logical</code> indicating whether to use the <code>scale</code> matrix as
specified or its adjoint. Default value: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="tfb_scale_matvec_linear_operator_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_scale_matvec_linear_operator_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_scale_matvec_lu'>Matrix-vector multiply using LU decomposition.</h2><span id='topic+tfb_scale_matvec_lu'></span>

<h3>Description</h3>

<p>This bijector is identical to the &quot;Convolution1x1&quot; used in Glow (Kingma and Dhariwal, 2018).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_scale_matvec_lu(
  lower_upper,
  permutation,
  validate_args = FALSE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_scale_matvec_lu_+3A_lower_upper">lower_upper</code></td>
<td>
<p>The LU factorization as returned by <code>tf$linalg$lu</code>.</p>
</td></tr>
<tr><td><code id="tfb_scale_matvec_lu_+3A_permutation">permutation</code></td>
<td>
<p>The LU factorization permutation as returned by <code>tf$linalg$lu</code>.</p>
</td></tr>
<tr><td><code id="tfb_scale_matvec_lu_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_scale_matvec_lu_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1807.03039">Diederik P. Kingma, Prafulla Dhariwal. Glow: Generative Flow with Invertible 1x1 Convolutions. <em>arXiv preprint arXiv:1807.03039</em>, 2018.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_scale_matvec_tri_l'>Compute <code style="white-space: pre;">&#8288;Y = g(X; scale) = scale @ X&#8288;</code>.</h2><span id='topic+tfb_scale_matvec_tri_l'></span>

<h3>Description</h3>

<p>The <code>scale</code> term is presumed lower-triangular and non-singular (ie, no zeros
on the diagonal), which permits efficient determinant calculation (linear in
matrix dimension, instead of cubic).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_scale_matvec_tri_l(
  scale_tril,
  adjoint = FALSE,
  validate_args = FALSE,
  name = "scale_matvec_tril",
  dtype = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_scale_matvec_tri_l_+3A_scale_tril">scale_tril</code></td>
<td>
<p>Floating-point <code>Tensor</code> representing the lower triangular
matrix. <code>scale_tril</code> has shape <code style="white-space: pre;">&#8288;[N1, N2, ...  k, k]&#8288;</code>, which represents a
k x k lower triangular matrix.
When <code>NULL</code> no <code>scale_tril</code> term is added to <code>scale</code>.
The upper triangular elements above the diagonal are ignored.</p>
</td></tr>
<tr><td><code id="tfb_scale_matvec_tri_l_+3A_adjoint">adjoint</code></td>
<td>
<p><code>logical</code> indicating whether to use the <code>scale</code> matrix as
specified or its adjoint. Note that lower-triangularity is taken into
account first: the region above the diagonal of <code>scale_tril</code> is treated
as zero (irrespective of the <code>adjoint</code> setting). A lower-triangular
input with <code>adjoint=TRUE</code> will behave like an upper triangular
transform. Default value: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="tfb_scale_matvec_tri_l_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_scale_matvec_tri_l_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
<tr><td><code id="tfb_scale_matvec_tri_l_+3A_dtype">dtype</code></td>
<td>
<p><code>tf$DType</code> to prefer when converting args to <code>Tensor</code>s. Else, we
fall back to a common dtype inferred from the args, finally falling back
to float32.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_scale_tri_l'>Transforms unconstrained vectors to TriL matrices with positive diagonal</h2><span id='topic+tfb_scale_tri_l'></span>

<h3>Description</h3>

<p>This is implemented as a simple tfb_chain of tfb_fill_triangular followed by
tfb_transform_diagonal, and provided mostly as a convenience.
The default setup is somewhat opinionated, using a Softplus transformation followed by a
small shift (1e-5) which attempts to avoid numerical issues from zeros on the diagonal.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_scale_tri_l(
  diag_bijector = NULL,
  diag_shift = 1e-05,
  validate_args = FALSE,
  name = "scale_tril"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_scale_tri_l_+3A_diag_bijector">diag_bijector</code></td>
<td>
<p>Bijector instance, used to transform the output diagonal to be positive.
Default value: NULL (i.e., <code>tfb_softplus()</code>).</p>
</td></tr>
<tr><td><code id="tfb_scale_tri_l_+3A_diag_shift">diag_shift</code></td>
<td>
<p>Float value broadcastable and added to all diagonal entries after applying the
diag_bijector. Setting a positive value forces the output diagonal entries to be positive, but
prevents inverting the transformation for matrices with diagonal entries less than this value.
Default value: 1e-5.</p>
</td></tr>
<tr><td><code id="tfb_scale_tri_l_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_scale_tri_l_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_shift'>Compute <code style="white-space: pre;">&#8288;Y = g(X; shift) = X + shift&#8288;</code>.</h2><span id='topic+tfb_shift'></span>

<h3>Description</h3>

<p>where <code>shift</code> is a numeric <code>Tensor</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_shift(shift, validate_args = FALSE, name = "shift")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_shift_+3A_shift">shift</code></td>
<td>
<p>floating-point tensor</p>
</td></tr>
<tr><td><code id="tfb_shift_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_shift_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_shifted_gompertz_cdf'>Compute <code>Y = g(X) = (1 - exp(-rate * X)) * exp(-c * exp(-rate * X))</code></h2><span id='topic+tfb_shifted_gompertz_cdf'></span>

<h3>Description</h3>

<p>This bijector maps inputs from <code style="white-space: pre;">&#8288;[-inf, inf]&#8288;</code> to <code style="white-space: pre;">&#8288;[0, inf]&#8288;</code>. The inverse of the
bijector applied to a uniform random variable <code>X ~ U(0, 1)</code> gives back a
random variable with the
<a href="https://en.wikipedia.org/wiki/Shifted_Gompertz_distribution">Shifted Gompertz distribution</a>:
</p>
<div class="sourceCode"><pre>Y ~ ShiftedGompertzCDF(concentration, rate)
pdf(y; c, r) = r * exp(-r * y - exp(-r * y) / c) * (1 + (1 - exp(-r * y)) / c)
</pre></div>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_shifted_gompertz_cdf(
  concentration,
  rate,
  validate_args = FALSE,
  name = "shifted_gompertz_cdf"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_shifted_gompertz_cdf_+3A_concentration">concentration</code></td>
<td>
<p>Positive Float-like <code>Tensor</code> that is the same dtype and is
broadcastable with <code>concentration</code>.
This is <code>c</code> in <code>Y = g(X) = (1 - exp(-rate * X)) * exp(-c * exp(-rate * X))</code>.</p>
</td></tr>
<tr><td><code id="tfb_shifted_gompertz_cdf_+3A_rate">rate</code></td>
<td>
<p>Positive Float-like <code>Tensor</code> that is the same dtype and is
broadcastable with <code>concentration</code>.
This is <code>rate</code> in <code>Y = g(X) = (1 - exp(-rate * X)) * exp(-c * exp(-rate * X))</code>.</p>
</td></tr>
<tr><td><code id="tfb_shifted_gompertz_cdf_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_shifted_gompertz_cdf_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note: Even though this is called <code>ShiftedGompertzCDF</code>, when applied to the
<code>Uniform</code> distribution, this is not the same as applying a <code>GompertzCDF</code> with
a <code>Shift</code> bijector (i.e. the Shifted Gompertz distribution is not the same as
a Gompertz distribution with a location parameter).
</p>
<p>Note: Because the Shifted Gompertz distribution concentrates its mass close
to zero, for larger rates or larger concentrations, <code>bijector$forward</code> will
quickly saturate to 1.
</p>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_sigmoid'>Computes<code>Y = g(X) = 1 / (1 + exp(-X))</code></h2><span id='topic+tfb_sigmoid'></span>

<h3>Description</h3>

<p>Computes<code>Y = g(X) = 1 / (1 + exp(-X))</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_sigmoid(validate_args = FALSE, name = "sigmoid")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_sigmoid_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_sigmoid_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_sinh'>Bijector that computes <code>Y = sinh(X)</code>.</h2><span id='topic+tfb_sinh'></span>

<h3>Description</h3>

<p>Bijector that computes <code>Y = sinh(X)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_sinh(validate_args = FALSE, name = "sinh")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_sinh_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_sinh_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_sinh_arcsinh'>Computes<code>Y = g(X) = Sinh( (Arcsinh(X) + skewness) * tailweight )</code></h2><span id='topic+tfb_sinh_arcsinh'></span>

<h3>Description</h3>

<p>For skewness in <code style="white-space: pre;">&#8288;(-inf, inf)&#8288;</code> and tailweight in <code style="white-space: pre;">&#8288;(0, inf)&#8288;</code>, this
transformation is a diffeomorphism of the real line <code style="white-space: pre;">&#8288;(-inf, inf)&#8288;</code>.
The inverse transform is <code>X = g^{-1}(Y) = Sinh( ArcSinh(Y) / tailweight - skewness )</code>.
The SinhArcsinh transformation of the Normal is described in
<a href="https://oro.open.ac.uk/22510/">Sinh-arcsinh distributions</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_sinh_arcsinh(
  skewness = NULL,
  tailweight = NULL,
  validate_args = FALSE,
  name = "SinhArcsinh"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_sinh_arcsinh_+3A_skewness">skewness</code></td>
<td>
<p>Skewness parameter.  Float-type Tensor.  Default is 0 of type float32.</p>
</td></tr>
<tr><td><code id="tfb_sinh_arcsinh_+3A_tailweight">tailweight</code></td>
<td>
<p>Tailweight parameter.  Positive Tensor of same dtype as skewness and broadcastable shape.  Default is 1 of type float32.</p>
</td></tr>
<tr><td><code id="tfb_sinh_arcsinh_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_sinh_arcsinh_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This Bijector allows a similar transformation of any distribution supported on <code style="white-space: pre;">&#8288;(-inf, inf)&#8288;</code>.
</p>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>Meaning of the parameters</h3>


<ul>
<li><p> If skewness = 0 and tailweight = 1, this transform is the identity.
</p>
</li>
<li><p> Positive (negative) skewness leads to positive (negative) skew.
</p>
</li>
<li><p> positive skew means, for unimodal X centered at zero, the mode of Y is &quot;tilted&quot; to the right.
</p>
</li>
<li><p> positive skew means positive values of Y become more likely, and negative values become less likely.
</p>
</li>
<li><p> Larger (smaller) tailweight leads to fatter (thinner) tails.
</p>
</li>
<li><p> Fatter tails mean larger values of |Y| become more likely.
</p>
</li>
<li><p> If X is a unit Normal, tailweight &lt; 1 leads to a distribution that is &quot;flat&quot; around Y = 0, and a very steep drop-off in the tails.
</p>
</li>
<li><p> If X is a unit Normal, tailweight &gt; 1 leads to a distribution more peaked at the mode with heavier tails.
To see the argument about the tails, note that for |X| &gt;&gt; 1 and |X| &gt;&gt; (|skewness| * tailweight)<strong>tailweight, we have
Y approx 0.5 X</strong>tailweight e**(sign(X) skewness * tailweight).
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_softmax_centered'>Computes <code style="white-space: pre;">&#8288;Y = g(X) = exp([X 0]) / sum(exp([X 0]))&#8288;</code></h2><span id='topic+tfb_softmax_centered'></span>

<h3>Description</h3>

<p>To implement <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a> as a
bijection, the forward transformation appends a value to the input and the
inverse removes this coordinate. The appended coordinate represents a pivot,
e.g., softmax(x) = exp(x-c) / sum(exp(x-c)) where c is the implicit last
coordinate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_softmax_centered(validate_args = FALSE, name = "softmax_centered")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_softmax_centered_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_softmax_centered_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>At first blush it may seem like the <a href="https://en.wikipedia.org/wiki/Invariance_of_domain">Invariance of domain</a>
theorem implies this implementation is not a bijection. However, the appended dimension
makes the (forward) image non-open and the theorem does not directly apply.
</p>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_softplus'>Computes <code>Y = g(X) = Log[1 + exp(X)]</code></h2><span id='topic+tfb_softplus'></span>

<h3>Description</h3>

<p>The softplus Bijector has the following two useful properties:
</p>

<ul>
<li><p> The domain is the positive real numbers
</p>
</li>
<li><p> softplus(x) approx x, for large x, so it does not overflow as easily as the Exp Bijector.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>tfb_softplus(
  hinge_softness = NULL,
  low = NULL,
  validate_args = FALSE,
  name = "softplus"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_softplus_+3A_hinge_softness">hinge_softness</code></td>
<td>
<p>Nonzero floating point Tensor.  Controls the softness of what
would otherwise be a kink at the origin.  Default is 1.0.</p>
</td></tr>
<tr><td><code id="tfb_softplus_+3A_low">low</code></td>
<td>
<p>Nonzero floating point tensor, lower bound on output values.
Implicitly zero if <code>NULL</code>. Otherwise, the
transformation <code>y = softplus(x) + low</code> is implemented. This
is equivalent to a <code>tfb_chain(list(tfb_shift(low), tfb_softplus()))</code> bijector
and is provided for convenience.</p>
</td></tr>
<tr><td><code id="tfb_softplus_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_softplus_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The optional nonzero hinge_softness parameter changes the transition at zero.
With hinge_softness = c, the bijector is:
</p>
<div class="sourceCode"><pre>f_c(x) := c * g(x / c) = c * Log[1 + exp(x / c)].
```

For large x &gt;&gt; 1,

```
c * Log[1 + exp(x / c)] approx c * Log[exp(x / c)] = x
```

so the behavior for large x is the same as the standard softplus.
As c &gt; 0 approaches 0 from the right, f_c(x) becomes less and less soft,
approaching max(0, x).
* c = 1 is the default.
* c &gt; 0 but small means f(x) approx ReLu(x) = max(0, x).
* c &lt; 0 flips sign and reflects around the y-axis: f_{-c}(x) = -f_c(-x).
* c = 0 results in a non-bijective transformation and triggers an exception.
Note: log(.) and exp(.) are applied element-wise but the Jacobian is a reduction over the event space.

[1 + exp(x / c)]: R:1%20+%20exp(x%20/%20c)
[1 + exp(x / c)]: R:1%20+%20exp(x%20/%20c)
[exp(x / c)]: R:exp(x%20/%20c)
</pre></div>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_softsign'>Computes <code style="white-space: pre;">&#8288;Y = g(X) = X / (1 + |X|)&#8288;</code></h2><span id='topic+tfb_softsign'></span>

<h3>Description</h3>

<p>The softsign Bijector has the following two useful properties:
</p>

<ul>
<li><p> The domain is all real numbers
</p>
</li>
<li><p> softsign(x) approx sgn(x), for large |x|.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>tfb_softsign(validate_args = FALSE, name = "softsign")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_softsign_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_softsign_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_split'>Split a <code>Tensor</code> event along an axis into a list of <code>Tensor</code>s.</h2><span id='topic+tfb_split'></span>

<h3>Description</h3>

<p>The inverse of <code>split</code> concatenates a list of <code>Tensor</code>s along <code>axis</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_split(num_or_size_splits, axis = -1, validate_args = FALSE, name = "split")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_split_+3A_num_or_size_splits">num_or_size_splits</code></td>
<td>
<p>Either an integer indicating the number of
splits along <code>axis</code> or a 1-D integer <code>Tensor</code> or Python list containing
the sizes of each output tensor along <code>axis</code>. If a list/<code>Tensor</code>, it may
contain at most one value of <code>-1</code>, which indicates a split size that is
unknown and determined from input.</p>
</td></tr>
<tr><td><code id="tfb_split_+3A_axis">axis</code></td>
<td>
<p>A negative integer or scalar <code>int32</code> <code>Tensor</code>. The dimension along
which to split. Must be negative to enable the bijector to support
arbitrary batch dimensions. Defaults to -1 (note that this is different from the <code>tf$Split</code> default of <code>0</code>).
Must be statically known.</p>
</td></tr>
<tr><td><code id="tfb_split_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_split_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_square'>Computes<code>g(X) = X^2</code>; X is a positive real number.</h2><span id='topic+tfb_square'></span>

<h3>Description</h3>

<p>g is a bijection between the non-negative real numbers (R_+) and the non-negative real numbers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_square(validate_args = FALSE, name = "square")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_square_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_square_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_tanh'>Computes <code>Y = tanh(X)</code></h2><span id='topic+tfb_tanh'></span>

<h3>Description</h3>

<p><code>Y = tanh(X)</code>, therefore Y in <code style="white-space: pre;">&#8288;(-1, 1)&#8288;</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_tanh(validate_args = FALSE, name = "tanh")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_tanh_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_tanh_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This can be achieved by an affine transform of the Sigmoid bijector, i.e., it is equivalent to
</p>
<p><code>tfb_chain(list(tfb_affine(shift = -1, scale = 2),
               tfb_sigmoid(),
               tfb_affine(scale = 2)))</code>
</p>
<p>However, using the Tanh bijector directly is slightly faster and more numerically stable.
</p>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_transform_diagonal'>Applies a Bijector to the diagonal of a matrix</h2><span id='topic+tfb_transform_diagonal'></span>

<h3>Description</h3>

<p>Applies a Bijector to the diagonal of a matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_transform_diagonal(
  diag_bijector,
  validate_args = FALSE,
  name = "transform_diagonal"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_transform_diagonal_+3A_diag_bijector">diag_bijector</code></td>
<td>
<p>Bijector instance used to transform the diagonal.</p>
</td></tr>
<tr><td><code id="tfb_transform_diagonal_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_transform_diagonal_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_transpose'>Computes<code>Y = g(X) = transpose_rightmost_dims(X, rightmost_perm)</code></h2><span id='topic+tfb_transpose'></span>

<h3>Description</h3>

<p>This bijector is semantically similar to tf.transpose except that it
transposes only the rightmost &quot;event&quot; dimensions. That is, unlike
<code>tf$transpose</code> the perm argument is itself a permutation of
<code>tf$range(rightmost_transposed_ndims)</code> rather than <code>tf$range(tf$rank(x))</code>,
i.e., users specify the (rightmost) dimensions to permute, not all dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_transpose(
  perm = NULL,
  rightmost_transposed_ndims = NULL,
  validate_args = FALSE,
  name = "transpose"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_transpose_+3A_perm">perm</code></td>
<td>
<p>Positive integer vector-shaped Tensor representing permutation of
rightmost dims (for forward transformation).  Note that the 0th index
represents the first of the rightmost dims and the largest value must be
rightmost_transposed_ndims - 1 and corresponds to <code>tf$rank(x) - 1</code>.
Only one of perm and rightmost_transposed_ndims can (and must) be specified.
Default value: <code>tf$range(start=rightmost_transposed_ndims, limit=-1, delta=-1)</code>.</p>
</td></tr>
<tr><td><code id="tfb_transpose_+3A_rightmost_transposed_ndims">rightmost_transposed_ndims</code></td>
<td>
<p>Positive integer scalar-shaped Tensor
representing the number of rightmost dimensions to permute.
Only one of perm and rightmost_transposed_ndims can (and must) be
specified. Default value: <code>tf$size(perm)</code>.</p>
</td></tr>
<tr><td><code id="tfb_transpose_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_transpose_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The actual (forward) transformation is:
</p>
<p><code>sample_batch_ndims &lt;- tf$rank(x) - tf$size(perm)
perm = tf$concat(list(tf$range(sample_batch_ndims), sample_batch_ndims + perm),axis=0)
tf$transpose(x, perm)</code>
</p>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfb_weibull'>Computes<code>Y = g(X) = 1 - exp((-X / scale) ** concentration)</code> where X &gt;= 0</h2><span id='topic+tfb_weibull'></span>

<h3>Description</h3>

<p>This bijector maps inputs from <code style="white-space: pre;">&#8288;[0, inf]&#8288;</code> to <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code>. The inverse of the
bijector applied to a uniform random variable X ~ U(0, 1) gives back a
random variable with the <a href="https://en.wikipedia.org/wiki/Weibull_distribution">Weibull distribution</a>:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_weibull(
  scale = 1,
  concentration = 1,
  validate_args = FALSE,
  name = "weibull"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_weibull_+3A_scale">scale</code></td>
<td>
<p>Positive Float-type Tensor that is the same dtype and is
broadcastable with concentration.
This is l in <code>Y = g(X) = 1 - exp((-x / l) ** k)</code>.</p>
</td></tr>
<tr><td><code id="tfb_weibull_+3A_concentration">concentration</code></td>
<td>
<p>Positive Float-type Tensor that is the same dtype and is
broadcastable with scale.
This is k in <code>Y = g(X) = 1 - exp((-x / l) ** k)</code>.</p>
</td></tr>
<tr><td><code id="tfb_weibull_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_weibull_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>Y ~ Weibull(scale, concentration)</code>
<code style="white-space: pre;">&#8288;pdf(y; scale, concentration, y &gt;= 0) = (concentration / scale) * (y / scale)**(concentration - 1) * exp(-(y / scale)**concentration)&#8288;</code>
</p>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull_cdf">tfb_weibull_cdf</a>()</code>
</p>

<hr>
<h2 id='tfb_weibull_cdf'>Compute <code style="white-space: pre;">&#8288;Y = g(X) = 1 - exp((-X / scale) ** concentration), X &gt;= 0&#8288;</code>.</h2><span id='topic+tfb_weibull_cdf'></span>

<h3>Description</h3>

<p>This bijector maps inputs from <code style="white-space: pre;">&#8288;[0, inf]&#8288;</code> to <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code>. The inverse of the
bijector applied to a uniform random variable <code>X ~ U(0, 1)</code> gives back a
random variable with the
<a href="https://en.wikipedia.org/wiki/Weibull_distribution">Weibull distribution</a>:
</p>
<div class="sourceCode"><pre>Y ~ Weibull(scale, concentration)
pdf(y; scale, concentration, y &gt;= 0) =
  (concentration / scale) * (y / scale)**(concentration - 1) *
    exp(-(y / scale)**concentration)
</pre></div>


<h3>Usage</h3>

<pre><code class='language-R'>tfb_weibull_cdf(
  scale = 1,
  concentration = 1,
  validate_args = FALSE,
  name = "weibull_cdf"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfb_weibull_cdf_+3A_scale">scale</code></td>
<td>
<p>Positive Float-type <code>Tensor</code> that is the same dtype and is
broadcastable with <code>concentration</code>.
This is <code>l</code> in <code>Y = g(X) = 1 - exp((-x / l) ** k)</code>.</p>
</td></tr>
<tr><td><code id="tfb_weibull_cdf_+3A_concentration">concentration</code></td>
<td>
<p>Positive Float-type <code>Tensor</code> that is the same dtype and is
broadcastable with <code>scale</code>.
This is <code>k</code> in <code>Y = g(X) = 1 - exp((-x / l) ** k)</code>.</p>
</td></tr>
<tr><td><code id="tfb_weibull_cdf_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td></tr>
<tr><td><code id="tfb_weibull_cdf_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Likwewise, the forward of this bijector is the Weibull distribution CDF.
</p>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>See Also</h3>

<p>For usage examples see <code><a href="#topic+tfb_forward">tfb_forward()</a></code>, <code><a href="#topic+tfb_inverse">tfb_inverse()</a></code>, <code><a href="#topic+tfb_inverse_log_det_jacobian">tfb_inverse_log_det_jacobian()</a></code>.
</p>
<p>Other bijectors: 
<code><a href="#topic+tfb_absolute_value">tfb_absolute_value</a>()</code>,
<code><a href="#topic+tfb_affine_linear_operator">tfb_affine_linear_operator</a>()</code>,
<code><a href="#topic+tfb_affine_scalar">tfb_affine_scalar</a>()</code>,
<code><a href="#topic+tfb_affine">tfb_affine</a>()</code>,
<code><a href="#topic+tfb_ascending">tfb_ascending</a>()</code>,
<code><a href="#topic+tfb_batch_normalization">tfb_batch_normalization</a>()</code>,
<code><a href="#topic+tfb_blockwise">tfb_blockwise</a>()</code>,
<code><a href="#topic+tfb_chain">tfb_chain</a>()</code>,
<code><a href="#topic+tfb_cholesky_outer_product">tfb_cholesky_outer_product</a>()</code>,
<code><a href="#topic+tfb_cholesky_to_inv_cholesky">tfb_cholesky_to_inv_cholesky</a>()</code>,
<code><a href="#topic+tfb_correlation_cholesky">tfb_correlation_cholesky</a>()</code>,
<code><a href="#topic+tfb_cumsum">tfb_cumsum</a>()</code>,
<code><a href="#topic+tfb_discrete_cosine_transform">tfb_discrete_cosine_transform</a>()</code>,
<code><a href="#topic+tfb_expm1">tfb_expm1</a>()</code>,
<code><a href="#topic+tfb_exp">tfb_exp</a>()</code>,
<code><a href="#topic+tfb_ffjord">tfb_ffjord</a>()</code>,
<code><a href="#topic+tfb_fill_scale_tri_l">tfb_fill_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_fill_triangular">tfb_fill_triangular</a>()</code>,
<code><a href="#topic+tfb_glow">tfb_glow</a>()</code>,
<code><a href="#topic+tfb_gompertz_cdf">tfb_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel_cdf">tfb_gumbel_cdf</a>()</code>,
<code><a href="#topic+tfb_gumbel">tfb_gumbel</a>()</code>,
<code><a href="#topic+tfb_identity">tfb_identity</a>()</code>,
<code><a href="#topic+tfb_inline">tfb_inline</a>()</code>,
<code><a href="#topic+tfb_invert">tfb_invert</a>()</code>,
<code><a href="#topic+tfb_iterated_sigmoid_centered">tfb_iterated_sigmoid_centered</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy_cdf">tfb_kumaraswamy_cdf</a>()</code>,
<code><a href="#topic+tfb_kumaraswamy">tfb_kumaraswamy</a>()</code>,
<code><a href="#topic+tfb_lambert_w_tail">tfb_lambert_w_tail</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_default_template">tfb_masked_autoregressive_default_template</a>()</code>,
<code><a href="#topic+tfb_masked_autoregressive_flow">tfb_masked_autoregressive_flow</a>()</code>,
<code><a href="#topic+tfb_masked_dense">tfb_masked_dense</a>()</code>,
<code><a href="#topic+tfb_matrix_inverse_tri_l">tfb_matrix_inverse_tri_l</a>()</code>,
<code><a href="#topic+tfb_matvec_lu">tfb_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_normal_cdf">tfb_normal_cdf</a>()</code>,
<code><a href="#topic+tfb_ordered">tfb_ordered</a>()</code>,
<code><a href="#topic+tfb_pad">tfb_pad</a>()</code>,
<code><a href="#topic+tfb_permute">tfb_permute</a>()</code>,
<code><a href="#topic+tfb_power_transform">tfb_power_transform</a>()</code>,
<code><a href="#topic+tfb_rational_quadratic_spline">tfb_rational_quadratic_spline</a>()</code>,
<code><a href="#topic+tfb_rayleigh_cdf">tfb_rayleigh_cdf</a>()</code>,
<code><a href="#topic+tfb_real_nvp_default_template">tfb_real_nvp_default_template</a>()</code>,
<code><a href="#topic+tfb_real_nvp">tfb_real_nvp</a>()</code>,
<code><a href="#topic+tfb_reciprocal">tfb_reciprocal</a>()</code>,
<code><a href="#topic+tfb_reshape">tfb_reshape</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_diag">tfb_scale_matvec_diag</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_linear_operator">tfb_scale_matvec_linear_operator</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_lu">tfb_scale_matvec_lu</a>()</code>,
<code><a href="#topic+tfb_scale_matvec_tri_l">tfb_scale_matvec_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale_tri_l">tfb_scale_tri_l</a>()</code>,
<code><a href="#topic+tfb_scale">tfb_scale</a>()</code>,
<code><a href="#topic+tfb_shifted_gompertz_cdf">tfb_shifted_gompertz_cdf</a>()</code>,
<code><a href="#topic+tfb_shift">tfb_shift</a>()</code>,
<code><a href="#topic+tfb_sigmoid">tfb_sigmoid</a>()</code>,
<code><a href="#topic+tfb_sinh_arcsinh">tfb_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfb_sinh">tfb_sinh</a>()</code>,
<code><a href="#topic+tfb_softmax_centered">tfb_softmax_centered</a>()</code>,
<code><a href="#topic+tfb_softplus">tfb_softplus</a>()</code>,
<code><a href="#topic+tfb_softsign">tfb_softsign</a>()</code>,
<code><a href="#topic+tfb_split">tfb_split</a>()</code>,
<code><a href="#topic+tfb_square">tfb_square</a>()</code>,
<code><a href="#topic+tfb_tanh">tfb_tanh</a>()</code>,
<code><a href="#topic+tfb_transform_diagonal">tfb_transform_diagonal</a>()</code>,
<code><a href="#topic+tfb_transpose">tfb_transpose</a>()</code>,
<code><a href="#topic+tfb_weibull">tfb_weibull</a>()</code>
</p>

<hr>
<h2 id='tfd_autoregressive'>Autoregressive distribution</h2><span id='topic+tfd_autoregressive'></span>

<h3>Description</h3>

<p>The Autoregressive distribution enables learning (often) richer multivariate
distributions by repeatedly applying a <a href="https://en.wikipedia.org/wiki/Diffeomorphism">diffeomorphic</a>
transformation (such as implemented by <code>Bijector</code>s).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_autoregressive(
  distribution_fn,
  sample0 = NULL,
  num_steps = NULL,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Autoregressive"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_autoregressive_+3A_distribution_fn">distribution_fn</code></td>
<td>
<p>Function which constructs a <code>tfd$Distribution</code>-like instance from a <code>Tensor</code>
(e.g., <code>sample0</code>). The function must respect the &quot;autoregressive property&quot;,
i.e., there exists a permutation of event such that each coordinate is a
diffeomorphic function of on preceding coordinates.</p>
</td></tr>
<tr><td><code id="tfd_autoregressive_+3A_sample0">sample0</code></td>
<td>
<p>Initial input to <code>distribution_fn</code>; used to
build the distribution in <code style="white-space: pre;">&#8288;__init__&#8288;</code> which in turn specifies this
distribution's properties, e.g., <code>event_shape</code>, <code>batch_shape</code>, <code>dtype</code>.
If unspecified, then <code>distribution_fn</code> should be default constructable.</p>
</td></tr>
<tr><td><code id="tfd_autoregressive_+3A_num_steps">num_steps</code></td>
<td>
<p>Number of times <code>distribution_fn</code> is composed from samples,
e.g., <code>num_steps=2</code> implies <code>distribution_fn(distribution_fn(sample0)$sample(n))$sample()</code>.</p>
</td></tr>
<tr><td><code id="tfd_autoregressive_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_autoregressive_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_autoregressive_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Regarding terminology,
&quot;Autoregressive models decompose the joint density as a product of
conditionals, and model each conditional in turn. Normalizing flows
transform a base density (e.g. a standard Gaussian) into the target density
by an invertible transformation with tractable Jacobian.&quot; (Papamakarios et al., 2016)
</p>
<p>In other words, the &quot;autoregressive property&quot; is equivalent to the
decomposition, <code style="white-space: pre;">&#8288;p(x) = prod{ p(x[i] | x[0:i]) : i=0, ..., d }&#8288;</code>. The provided
<code>shift_and_log_scale_fn</code>, <code>tfb_masked_autoregressive_default_template</code>, achieves
this property by zeroing out weights in its <code>masked_dense</code> layers.
Practically speaking the autoregressive property means that there exists a
permutation of the event coordinates such that each coordinate is a
diffeomorphic function of only preceding coordinates
(van den Oord et al., 2016).
</p>
<p>Mathematical Details
</p>
<p>The probability function is
</p>
<div class="sourceCode"><pre>prob(x; fn, n) = fn(x).prob(x)
</pre></div>
<p>And a sample is generated by
</p>
<div class="sourceCode"><pre>x = fn(...fn(fn(x0).sample()).sample()).sample()
</pre></div>
<p>where the ellipses (<code>...</code>) represent <code>n-2</code> composed calls to <code>fn</code>, <code>fn</code>
constructs a <code>tfd$Distribution</code>-like instance, and <code>x0</code> is a fixed initializing <code>Tensor</code>.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1705.07057">George Papamakarios, Theo Pavlakou, and Iain Murray. Masked Autoregressive Flow for Density Estimation. In <em>Neural Information Processing Systems</em>, 2017.</a>
</p>
</li>
<li> <p><a href="https://arxiv.org/abs/1606.05328">Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. Conditional Image Generation with PixelCNN Decoders. In <em>Neural Information Processing Systems</em>, 2016.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_batch_reshape'>Batch-Reshaping distribution</h2><span id='topic+tfd_batch_reshape'></span>

<h3>Description</h3>

<p>This &quot;meta-distribution&quot; reshapes the batch dimensions of another distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_batch_reshape(
  distribution,
  batch_shape,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_batch_reshape_+3A_distribution">distribution</code></td>
<td>
<p>The base distribution instance to reshape. Typically an
instance of <code>Distribution</code>.</p>
</td></tr>
<tr><td><code id="tfd_batch_reshape_+3A_batch_shape">batch_shape</code></td>
<td>
<p>Positive <code>integer</code>-like vector-shaped <code>Tensor</code> representing
the new shape of the batch dimensions. Up to one dimension may contain
<code>-1</code>, meaning the remainder of the batch size.</p>
</td></tr>
<tr><td><code id="tfd_batch_reshape_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_batch_reshape_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_batch_reshape_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_bates'>Bates distribution.</h2><span id='topic+tfd_bates'></span>

<h3>Description</h3>

<p>The Bates distribution is the distribution of the average of <code>total_count</code>
independent samples from <code>Uniform(low, high)</code>. It is parameterized by the
interval bounds <code>low</code> and <code>high</code>, and <code>total_count</code>, the number of samples.
Although some care has been taken to avoid numerical issues, the <code>pdf</code>, <code>cdf</code>,
and log versions thereof may still exhibit numerical instability. They are
relatively stable near the tails; however near the mode they are unstable if
<code>total_count</code> is greater than about <code>75</code> for <code>tf$float64</code>, <code>25</code> for
<code>tf$float32</code>, and <code>7</code> for <code>tf$float16</code>. Beyond these limits a warning will be
shown if <code>validate_args=FALSE</code>; otherwise an exception is thrown. For high
<code>total_count</code>, consider using a <code>Normal</code> approximation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_bates(
  total_count,
  low = 0,
  high = 1,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Bates"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_bates_+3A_total_count">total_count</code></td>
<td>
<p>Non-negative integer-valued <code>Tensor</code> with shape broadcastable
to the batch shape <code style="white-space: pre;">&#8288;[N1,..., Nm]&#8288;</code>, <code>m &gt;= 0</code>. This controls the number of
samples of <code>Uniform(low, high)</code> to take the mean of.</p>
</td></tr>
<tr><td><code id="tfd_bates_+3A_low">low</code></td>
<td>
<p>Floating point <code>Tensor</code> representing the lower bounds of the support.
Should be broadcastable to <code style="white-space: pre;">&#8288;[N1,..., Nm]&#8288;</code> with <code>m &gt;= 0</code>, the same dtype
as <code>total_count</code>, and <code>low &lt; high</code> component-wise, after broadcasting.
Defaults to <code>0</code>.</p>
</td></tr>
<tr><td><code id="tfd_bates_+3A_high">high</code></td>
<td>
<p>Floating point <code>Tensor</code> representing the upper bounds of the
support.  Should be broadcastable to <code style="white-space: pre;">&#8288;[N1,..., Nm]&#8288;</code> with <code>m &gt;= 0</code>, the
same dtype as <code>total_count</code>, and <code>low &lt; high</code> component-wise, after
broadcasting.  Defaults to <code>1</code>.</p>
</td></tr>
<tr><td><code id="tfd_bates_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_bates_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_bates_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The probability density function (pdf) is supported in the interval
<code style="white-space: pre;">&#8288;[low, high]&#8288;</code>. If <code style="white-space: pre;">&#8288;[low, high]&#8288;</code> is the unit interval <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code>, the pdf
is,
</p>
<div class="sourceCode"><pre>pdf(x; n, 0, 1) = ((n / (n-1)!) sum_{k=0}^j (-1)^k (n choose k) (nx - k)^{n-1}
</pre></div>
<p>where
</p>

<ul>
<li> <p><code>total_count = n</code>,
</p>
</li>
<li> <p><code>j = floor(nx)</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;n!&#8288;</code> is the factorial of <code>n</code>,
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;(n choose k)&#8288;</code> is the binomial coefficient <code style="white-space: pre;">&#8288;n! / (k!(n - k)!)&#8288;</code>
For arbitrary intervals <code style="white-space: pre;">&#8288;[low, high]&#8288;</code>, the pdf is,
</p>
</li></ul>

<div class="sourceCode"><pre>pdf(x; n, low, high) = pdf((x - low) / (high - low); n, 0, 1) / (high - low)
</pre></div>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_bernoulli'>Bernoulli distribution</h2><span id='topic+tfd_bernoulli'></span>

<h3>Description</h3>

<p>The Bernoulli distribution with <code>probs</code> parameter, i.e., the probability of a
<code>1</code> outcome (vs a <code>0</code> outcome).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_bernoulli(
  logits = NULL,
  probs = NULL,
  dtype = tf$int32,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Bernoulli"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_bernoulli_+3A_logits">logits</code></td>
<td>
<p>An N-D Tensor representing the log-odds of a 1 event. Each entry in the Tensor
parametrizes an independent Bernoulli distribution where the probability of an event
is sigmoid(logits). Only one of logits or probs should be passed in.</p>
</td></tr>
<tr><td><code id="tfd_bernoulli_+3A_probs">probs</code></td>
<td>
<p>An N-D Tensor representing the probability of a 1 event. Each entry in the Tensor
parameterizes an independent Bernoulli distribution. Only one of logits or probs
should be passed in.</p>
</td></tr>
<tr><td><code id="tfd_bernoulli_+3A_dtype">dtype</code></td>
<td>
<p>The type of the event samples. Default: int32.</p>
</td></tr>
<tr><td><code id="tfd_bernoulli_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_bernoulli_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_bernoulli_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_beta'>Beta distribution</h2><span id='topic+tfd_beta'></span>

<h3>Description</h3>

<p>The Beta distribution is defined over the <code style="white-space: pre;">&#8288;(0, 1)&#8288;</code> interval using parameters
<code>concentration1</code> (aka &quot;alpha&quot;) and <code>concentration0</code> (aka &quot;beta&quot;).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_beta(
  concentration1 = NULL,
  concentration0 = NULL,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Beta"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_beta_+3A_concentration1">concentration1</code></td>
<td>
<p>Positive floating-point <code>Tensor</code> indicating mean
number of successes; aka &quot;alpha&quot;. Implies <code>self$dtype</code> and <code>self$batch_shape</code>, i.e.,
<code style="white-space: pre;">&#8288;concentration1$shape = [N1, N2, ..., Nm] = self$batch_shape&#8288;</code>.</p>
</td></tr>
<tr><td><code id="tfd_beta_+3A_concentration0">concentration0</code></td>
<td>
<p>Positive floating-point <code>Tensor</code> indicating mean
number of failures; aka &quot;beta&quot;. Otherwise has same semantics as <code>concentration1</code>.</p>
</td></tr>
<tr><td><code id="tfd_beta_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_beta_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_beta_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; alpha, beta) = x**(alpha - 1) (1 - x)**(beta - 1) / Z
Z = Gamma(alpha) Gamma(beta) / Gamma(alpha + beta)
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>concentration1 = alpha</code>,
</p>
</li>
<li> <p><code>concentration0 = beta</code>,
</p>
</li>
<li> <p><code>Z</code> is the normalization constant, and,
</p>
</li>
<li> <p><code>Gamma</code> is the <a href="https://en.wikipedia.org/wiki/Gamma_function">gamma function</a>.
The concentration parameters represent mean total counts of a <code>1</code> or a <code>0</code>,
i.e.,
</p>
</li></ul>

<div class="sourceCode"><pre>concentration1 = alpha = mean * total_concentration
concentration0 = beta  = (1. - mean) * total_concentration
</pre></div>
<p>where <code>mean</code> in <code style="white-space: pre;">&#8288;(0, 1)&#8288;</code> and <code>total_concentration</code> is a positive real number
representing a mean <code>total_count = concentration1 + concentration0</code>.
Distribution parameters are automatically broadcast in all functions; see
examples for details.
Warning: The samples can be zero due to finite precision.
This happens more often when some of the concentrations are very small.
Make sure to round the samples to <code>np$finfo(dtype)$tiny</code> before computing the density.
Samples of this distribution are reparameterized (pathwise differentiable).
The derivatives are computed using the approach described in the paper
<a href="https://arxiv.org/abs/1805.08498">Michael Figurnov, Shakir Mohamed, Andriy Mnih. Implicit Reparameterization Gradients, 2018</a>
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_beta_binomial'>Beta-Binomial compound distribution</h2><span id='topic+tfd_beta_binomial'></span>

<h3>Description</h3>

<p>The Beta-Binomial distribution is parameterized by (a batch of) <code>total_count</code>
parameters, the number of trials per draw from Binomial distributions where
the probabilities of success per trial are drawn from underlying Beta
distributions; the Beta distributions are parameterized by <code>concentration1</code>
(aka 'alpha') and <code>concentration0</code> (aka 'beta').
Mathematically, it is (equivalent to) a special case of the
Dirichlet-Multinomial over two classes, although the computational
representation is slightly different: while the Beta-Binomial is a
distribution over the number of successes in <code>total_count</code> trials, the
two-class Dirichlet-Multinomial is a distribution over the number of successes
and failures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_beta_binomial(
  total_count,
  concentration1,
  concentration0,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "BetaBinomial"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_beta_binomial_+3A_total_count">total_count</code></td>
<td>
<p>Non-negative integer-valued tensor, whose dtype is the same
as <code>concentration1</code> and <code>concentration0</code>. The shape is broadcastable to
<code style="white-space: pre;">&#8288;[N1,..., Nm]&#8288;</code> with <code>m &gt;= 0</code>. When <code>total_count</code> is broadcast with
<code>concentration1</code> and <code>concentration0</code>, it defines the distribution as a
batch of <code style="white-space: pre;">&#8288;N1 x ... x Nm&#8288;</code> different Beta-Binomial distributions. Its
components should be equal to integer values.</p>
</td></tr>
<tr><td><code id="tfd_beta_binomial_+3A_concentration1">concentration1</code></td>
<td>
<p>Positive floating-point <code>Tensor</code> indicating mean number of
successes. Specifically, the expected number of successes is
<code>total_count * concentration1 / (concentration1 + concentration0)</code>.</p>
</td></tr>
<tr><td><code id="tfd_beta_binomial_+3A_concentration0">concentration0</code></td>
<td>
<p>Positive floating-point <code>Tensor</code> indicating mean number of
failures; see description of <code>concentration1</code> for details.</p>
</td></tr>
<tr><td><code id="tfd_beta_binomial_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_beta_binomial_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_beta_binomial_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The Beta-Binomial is a distribution over the number of successes in
<code>total_count</code> independent Binomial trials, with each trial having the same
probability of success, the underlying probability being unknown but drawn
from a Beta distribution with known parameters.
The probability mass function (pmf) is,
</p>
<div class="sourceCode"><pre>pmf(k; n, a, b) = Beta(k + a, n - k + b) / Z
Z = (k! (n - k)! / n!) * Beta(a, b)
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>concentration1 = a &gt; 0</code>,
</p>
</li>
<li> <p><code>concentration0 = b &gt; 0</code>,
</p>
</li>
<li> <p><code>total_count = n</code>, <code>n</code> a positive integer,
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;n!&#8288;</code> is <code>n</code> factorial,
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;Beta(x, y) = Gamma(x) Gamma(y) / Gamma(x + y)&#8288;</code> is the
<a href="https://en.wikipedia.org/wiki/Beta_function">beta function</a>, and
</p>
</li>
<li> <p><code>Gamma</code> is the <a href="https://en.wikipedia.org/wiki/Gamma_function">gamma function</a>.
</p>
</li></ul>

<p>Dirichlet-Multinomial is a <a href="https://en.wikipedia.org/wiki/Compound_probability_distribution">compound distribution</a>,
i.e., its samples are generated as follows.
</p>

<ol>
<li><p> Choose success probabilities:
<code>probs ~ Beta(concentration1, concentration0)</code>
</p>
</li>
<li><p> Draw integers representing the number of successes:
<code>counts ~ Binomial(total_count, probs)</code>
Distribution parameters are automatically broadcast in all functions; see
examples for details.
</p>
</li></ol>



<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_binomial'>Binomial distribution</h2><span id='topic+tfd_binomial'></span>

<h3>Description</h3>

<p>This distribution is parameterized by <code>probs</code>, a (batch of) probabilities for
drawing a <code>1</code> and <code>total_count</code>, the number of trials per draw from the
Binomial.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_binomial(
  total_count,
  logits = NULL,
  probs = NULL,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Beta"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_binomial_+3A_total_count">total_count</code></td>
<td>
<p>Non-negative floating point tensor with shape broadcastable
to <code style="white-space: pre;">&#8288;[N1,..., Nm]&#8288;</code> with <code>m &gt;= 0</code> and the same dtype as <code>probs</code> or
<code>logits</code>. Defines this as a batch of <code style="white-space: pre;">&#8288;N1 x ...  x Nm&#8288;</code> different Binomial
distributions. Its components should be equal to integer values.</p>
</td></tr>
<tr><td><code id="tfd_binomial_+3A_logits">logits</code></td>
<td>
<p>Floating point tensor representing the log-odds of a
positive event with shape broadcastable to <code style="white-space: pre;">&#8288;[N1,..., Nm]&#8288;</code> <code>m &gt;= 0</code>, and
the same dtype as <code>total_count</code>. Each entry represents logits for the
probability of success for independent Binomial distributions. Only one
of <code>logits</code> or <code>probs</code> should be passed in.</p>
</td></tr>
<tr><td><code id="tfd_binomial_+3A_probs">probs</code></td>
<td>
<p>Positive floating point tensor with shape broadcastable to
<code style="white-space: pre;">&#8288;[N1,..., Nm]&#8288;</code> <code>m &gt;= 0</code>, <code style="white-space: pre;">&#8288;probs in [0, 1]&#8288;</code>. Each entry represents the
probability of success for independent Binomial distributions. Only one
of <code>logits</code> or <code>probs</code> should be passed in.</p>
</td></tr>
<tr><td><code id="tfd_binomial_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_binomial_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_binomial_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The Binomial is a distribution over the number of <code>1</code>'s in <code>total_count</code>
independent trials, with each trial having the same probability of <code>1</code>, i.e.,
<code>probs</code>.
</p>
<p>The probability mass function (pmf) is,
</p>
<div class="sourceCode"><pre>pmf(k; n, p) = p**k (1 - p)**(n - k) / Z
Z = k! (n - k)! / n!
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>total_count = n</code>,
</p>
</li>
<li> <p><code>probs = p</code>,
</p>
</li>
<li> <p><code>Z</code> is the normalizing constant, and,
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;n!&#8288;</code> is the factorial of <code>n</code>.
</p>
</li></ul>



<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_blockwise'>Blockwise distribution</h2><span id='topic+tfd_blockwise'></span>

<h3>Description</h3>

<p>Blockwise distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_blockwise(
  distributions,
  dtype_override = NULL,
  validate_args = FALSE,
  allow_nan_stats = FALSE,
  name = "Blockwise"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_blockwise_+3A_distributions">distributions</code></td>
<td>
<p>list of Distribution instances. All distribution instances
must have the same batch_shape and all must have 'event_ndims==1&ldquo;, i.e., be
vector-variate distributions.</p>
</td></tr>
<tr><td><code id="tfd_blockwise_+3A_dtype_override">dtype_override</code></td>
<td>
<p>samples of distributions will be cast to this dtype. If
unspecified, all distributions must have the same dtype. Default value:
<code>NULL</code> (i.e., do not cast).</p>
</td></tr>
<tr><td><code id="tfd_blockwise_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_blockwise_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_blockwise_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>

<hr>
<h2 id='tfd_categorical'>Categorical distribution over integers</h2><span id='topic+tfd_categorical'></span>

<h3>Description</h3>

<p>The Categorical distribution is parameterized by either probabilities or
log-probabilities of a set of <code>K</code> classes. It is defined over the integers
<code style="white-space: pre;">&#8288;{0, 1, ..., K-1}&#8288;</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_categorical(
  logits = NULL,
  probs = NULL,
  dtype = tf$int32,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Categorical"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_categorical_+3A_logits">logits</code></td>
<td>
<p>An N-D <code>Tensor</code>, <code>N &gt;= 1</code>, representing the log probabilities
of a set of Categorical distributions. The first <code>N - 1</code> dimensions
index into a batch of independent distributions and the last dimension
represents a vector of logits for each class. Only one of <code>logits</code> or
<code>probs</code> should be passed in.</p>
</td></tr>
<tr><td><code id="tfd_categorical_+3A_probs">probs</code></td>
<td>
<p>An N-D <code>Tensor</code>, <code>N &gt;= 1</code>, representing the probabilities
of a set of Categorical distributions. The first <code>N - 1</code> dimensions
index into a batch of independent distributions and the last dimension
represents a vector of probabilities for each class. Only one of
<code>logits</code> or <code>probs</code> should be passed in.</p>
</td></tr>
<tr><td><code id="tfd_categorical_+3A_dtype">dtype</code></td>
<td>
<p>The type of the event samples (default: int32).</p>
</td></tr>
<tr><td><code id="tfd_categorical_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_categorical_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_categorical_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Categorical distribution is closely related to the <code>OneHotCategorical</code> and
<code>Multinomial</code> distributions.  The Categorical distribution can be intuited as
generating samples according to <code style="white-space: pre;">&#8288;argmax{ OneHotCategorical(probs) }&#8288;</code> itself
being identical to <code style="white-space: pre;">&#8288;argmax{ Multinomial(probs, total_count=1) }&#8288;</code>.
</p>
<p>Mathematical Details
</p>
<p>The probability mass function (pmf) is,
</p>
<div class="sourceCode"><pre>pmf(k; pi) = prod_j pi_j**[k == j]
</pre></div>
<p>Pitfalls
</p>
<p>The number of classes, <code>K</code>, must not exceed:
</p>

<ul>
<li><p> the largest integer representable by <code>self$dtype</code>, i.e.,
<code>2**(mantissa_bits+1)</code> (IEEE 754),
</p>
</li>
<li><p> the maximum <code>Tensor</code> index, i.e., <code>2**31-1</code>.
</p>
</li></ul>

<p>Note: This condition is validated only when <code>validate_args = TRUE</code>.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_cauchy'>Cauchy distribution with location <code>loc</code> and scale <code>scale</code></h2><span id='topic+tfd_cauchy'></span>

<h3>Description</h3>

<p>Mathematical details
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_cauchy(
  loc,
  scale,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Cauchy"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_cauchy_+3A_loc">loc</code></td>
<td>
<p>Floating point tensor; the modes of the distribution(s).</p>
</td></tr>
<tr><td><code id="tfd_cauchy_+3A_scale">scale</code></td>
<td>
<p>Floating point tensor; the locations of the distribution(s).
Must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_cauchy_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_cauchy_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_cauchy_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; loc, scale) = 1 / (pi scale (1 + z**2))
z = (x - loc) / scale
</pre></div>
<p>where <code>loc</code> is the location, and <code>scale</code> is the scale.
The Cauchy distribution is a member of the <a href="https://en.wikipedia.org/wiki/Location-scale_family">location-scale family</a>, i.e.
<code>Y ~ Cauchy(loc, scale)</code> is equivalent to,
</p>
<div class="sourceCode"><pre>X ~ Cauchy(loc=0, scale=1)
Y = loc + scale * X
</pre></div>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_cdf'>Cumulative distribution function.
Given random variable X, the cumulative distribution function cdf is:
<code>cdf(x) := P[X &lt;= x]</code></h2><span id='topic+tfd_cdf'></span>

<h3>Description</h3>

<p>Cumulative distribution function.
Given random variable X, the cumulative distribution function cdf is:
<code>cdf(x) := P[X &lt;= x]</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_cdf(distribution, value, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_cdf_+3A_distribution">distribution</code></td>
<td>
<p>The distribution being used.</p>
</td></tr>
<tr><td><code id="tfd_cdf_+3A_value">value</code></td>
<td>
<p>float or double Tensor.</p>
</td></tr>
<tr><td><code id="tfd_cdf_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to Python.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Tensor of shape <code>sample_shape(x) + self$batch_shape</code> with values of type <code>self$dtype</code>.
</p>


<h3>See Also</h3>

<p>Other distribution_methods: 
<code><a href="#topic+tfd_covariance">tfd_covariance</a>()</code>,
<code><a href="#topic+tfd_cross_entropy">tfd_cross_entropy</a>()</code>,
<code><a href="#topic+tfd_entropy">tfd_entropy</a>()</code>,
<code><a href="#topic+tfd_kl_divergence">tfd_kl_divergence</a>()</code>,
<code><a href="#topic+tfd_log_cdf">tfd_log_cdf</a>()</code>,
<code><a href="#topic+tfd_log_prob">tfd_log_prob</a>()</code>,
<code><a href="#topic+tfd_log_survival_function">tfd_log_survival_function</a>()</code>,
<code><a href="#topic+tfd_mean">tfd_mean</a>()</code>,
<code><a href="#topic+tfd_mode">tfd_mode</a>()</code>,
<code><a href="#topic+tfd_prob">tfd_prob</a>()</code>,
<code><a href="#topic+tfd_quantile">tfd_quantile</a>()</code>,
<code><a href="#topic+tfd_sample">tfd_sample</a>()</code>,
<code><a href="#topic+tfd_stddev">tfd_stddev</a>()</code>,
<code><a href="#topic+tfd_survival_function">tfd_survival_function</a>()</code>,
<code><a href="#topic+tfd_variance">tfd_variance</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  d &lt;- tfd_normal(loc = c(1, 2), scale = c(1, 0.5))
  x &lt;- d %&gt;% tfd_sample()
  d %&gt;% tfd_cdf(x)

</code></pre>

<hr>
<h2 id='tfd_chi'>Chi distribution</h2><span id='topic+tfd_chi'></span>

<h3>Description</h3>

<p>The Chi distribution is defined over nonnegative real numbers and uses a
degrees of freedom (&quot;df&quot;) parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_chi(df, validate_args = FALSE, allow_nan_stats = TRUE, name = "Chi")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_chi_+3A_df">df</code></td>
<td>
<p>Floating point tensor, the degrees of freedom of the distribution(s).
<code>df</code> must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_chi_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_chi_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_chi_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; df, x &gt;= 0) = x**(df - 1) exp(-0.5 x**2) / Z
Z = 2**(0.5 df - 1) Gamma(0.5 df)
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>df</code> denotes the degrees of freedom,
</p>
</li>
<li> <p><code>Z</code> is the normalization constant, and,
</p>
</li>
<li> <p><code>Gamma</code> is the <a href="https://en.wikipedia.org/wiki/Gamma_function">gamma function</a>.
</p>
</li></ul>

<p>The Chi distribution is a transformation of the Chi2 distribution; it is the
distribution of the positive square root of a variable obeying a Chi
distribution.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_chi2'>Chi Square distribution</h2><span id='topic+tfd_chi2'></span>

<h3>Description</h3>

<p>The Chi2 distribution is defined over positive real numbers using a degrees of
freedom (&quot;df&quot;) parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_chi2(df, validate_args = FALSE, allow_nan_stats = TRUE, name = "Chi2")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_chi2_+3A_df">df</code></td>
<td>
<p>Floating point tensor, the degrees of freedom of the
distribution(s). <code>df</code> must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_chi2_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_chi2_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_chi2_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; df, x &gt; 0) = x**(0.5 df - 1) exp(-0.5 x) / Z
Z = 2**(0.5 df) Gamma(0.5 df)
</pre></div>
<p>where
</p>

<ul>
<li> <p><code>df</code> denotes the degrees of freedom,
</p>
</li>
<li> <p><code>Z</code> is the normalization constant, and,
</p>
</li>
<li> <p><code>Gamma</code> is the <a href="https://en.wikipedia.org/wiki/Gamma_function">gamma function</a>.
The Chi2 distribution is a special case of the Gamma distribution, i.e.,
</p>
</li></ul>

<div class="sourceCode"><pre>Chi2(df) = Gamma(concentration=0.5 * df, rate=0.5)
</pre></div>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_cholesky_lkj'>The CholeskyLKJ distribution on cholesky factors of correlation matrices</h2><span id='topic+tfd_cholesky_lkj'></span>

<h3>Description</h3>

<p>This is a one-parameter family of distributions on cholesky factors of
correlation matrices.
In other words, if If <code>X ~ CholeskyLKJ(c)</code>, then <code>X @ X^T ~ LKJ(c)</code>.
For more details on the LKJ distribution, see <code>tfd_lkj</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_cholesky_lkj(
  dimension,
  concentration,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "CholeskyLKJ"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_cholesky_lkj_+3A_dimension">dimension</code></td>
<td>
<p><code>integer</code>. The dimension of the correlation matrices
to sample.</p>
</td></tr>
<tr><td><code id="tfd_cholesky_lkj_+3A_concentration">concentration</code></td>
<td>
<p><code>float</code> or <code>double</code> <code>Tensor</code>. The positive concentration
parameter of the CholeskyLKJ distributions.</p>
</td></tr>
<tr><td><code id="tfd_cholesky_lkj_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_cholesky_lkj_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_cholesky_lkj_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_continuous_bernoulli'>Continuous Bernoulli distribution.</h2><span id='topic+tfd_continuous_bernoulli'></span>

<h3>Description</h3>

<p>This distribution is parameterized by <code>probs</code>, a (batch of) parameters
taking values in <code style="white-space: pre;">&#8288;(0, 1)&#8288;</code>. Note that, unlike in the Bernoulli case, <code>probs</code>
does not correspond to a probability, but the same name is used due to the
similarity with the Bernoulli.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_continuous_bernoulli(
  logits = NULL,
  probs = NULL,
  dtype = tf$float32,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "ContinuousBernoulli"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_continuous_bernoulli_+3A_logits">logits</code></td>
<td>
<p>An N-D <code>Tensor</code>. Each entry in the <code>Tensor</code> parameterizes
an independent continuous Bernoulli distribution with parameter
sigmoid(logits). Only one of <code>logits</code> or <code>probs</code> should be passed
in. Note that this does not correspond to the log-odds as in the
Bernoulli case.</p>
</td></tr>
<tr><td><code id="tfd_continuous_bernoulli_+3A_probs">probs</code></td>
<td>
<p>An N-D <code>Tensor</code> representing the parameter of a continuous
Bernoulli. Each entry in the <code>Tensor</code> parameterizes an independent
continuous Bernoulli distribution. Only one of <code>logits</code> or <code>probs</code>
should be passed in. Note that this also does not correspond to a
probability as in the Bernoulli case.</p>
</td></tr>
<tr><td><code id="tfd_continuous_bernoulli_+3A_dtype">dtype</code></td>
<td>
<p>The type of the event samples. Default: <code>float32</code>.</p>
</td></tr>
<tr><td><code id="tfd_continuous_bernoulli_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_continuous_bernoulli_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_continuous_bernoulli_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The continuous Bernoulli is a distribution over the interval <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code>,
parameterized by <code>probs</code> in <code style="white-space: pre;">&#8288;(0, 1)&#8288;</code>.
The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; probs) = probs**x * (1 - probs)**(1 - x) * C(probs)
C(probs) = (2 * atanh(1 - 2 * probs) / (1 - 2 * probs) if probs != 0.5 else 2.)
</pre></div>
<p>While the normalizing constant <code>C(probs)</code> is a continuous function of <code>probs</code>
(even at <code>probs = 0.5</code>), computing it at values close to 0.5 can result in
numerical instabilities due to 0/0 errors. A Taylor approximation of
<code>C(probs)</code> is thus used for values of <code>probs</code>
in a small interval <code style="white-space: pre;">&#8288;[lims[0], lims[1]]&#8288;</code> around 0.5. For more details,
see Loaiza-Ganem and Cunningham (2019).
NOTE: Unlike the Bernoulli, numerical instabilities can happen for <code>probs</code>
very close to 0 or 1. Current implementation allows any value in <code style="white-space: pre;">&#8288;(0, 1)&#8288;</code>,
but this could be changed to <code style="white-space: pre;">&#8288;(1e-6, 1-1e-6)&#8288;</code> to avoid these issues.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>References</h3>


<ul>
<li><p> Loaiza-Ganem G and Cunningham JP. The continuous Bernoulli: fixing a
pervasive error in variational autoencoders. NeurIPS2019.
https://arxiv.org/abs/1907.06845
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_covariance'>Covariance.</h2><span id='topic+tfd_covariance'></span>

<h3>Description</h3>

<p>Covariance is (possibly) defined only for non-scalar-event distributions.
For example, for a length-k, vector-valued distribution, it is calculated as,
<code>Cov[i, j] = Covariance(X_i, X_j) = E[(X_i - E[X_i]) (X_j - E[X_j])]</code>
where Cov is a (batch of) k x k matrix, 0 &lt;= (i, j) &lt; k, and E denotes expectation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_covariance(distribution, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_covariance_+3A_distribution">distribution</code></td>
<td>
<p>The distribution being used.</p>
</td></tr>
<tr><td><code id="tfd_covariance_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to Python.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Alternatively, for non-vector, multivariate distributions (e.g., matrix-valued, Wishart),
Covariance shall return a (batch of) matrices under some vectorization of the events, i.e.,
<code style="white-space: pre;">&#8288;Cov[i, j] = Covariance(Vec(X)_i, Vec(X)_j) = [as above]&#8288;</code>
where Cov is a (batch of) k x k matrices, 0 &lt;= (i, j) &lt; k = reduce_prod(event_shape),
and Vec is some function mapping indices of this distribution's event dimensions to indices of a
length-k vector.
</p>


<h3>Value</h3>

<p>Floating-point Tensor with shape <code style="white-space: pre;">&#8288;[B1, ..., Bn, k, k]&#8288;</code> where the first n dimensions
are batch coordinates and <code>k = reduce_prod(self.event_shape)</code>.
</p>


<h3>See Also</h3>

<p>Other distribution_methods: 
<code><a href="#topic+tfd_cdf">tfd_cdf</a>()</code>,
<code><a href="#topic+tfd_cross_entropy">tfd_cross_entropy</a>()</code>,
<code><a href="#topic+tfd_entropy">tfd_entropy</a>()</code>,
<code><a href="#topic+tfd_kl_divergence">tfd_kl_divergence</a>()</code>,
<code><a href="#topic+tfd_log_cdf">tfd_log_cdf</a>()</code>,
<code><a href="#topic+tfd_log_prob">tfd_log_prob</a>()</code>,
<code><a href="#topic+tfd_log_survival_function">tfd_log_survival_function</a>()</code>,
<code><a href="#topic+tfd_mean">tfd_mean</a>()</code>,
<code><a href="#topic+tfd_mode">tfd_mode</a>()</code>,
<code><a href="#topic+tfd_prob">tfd_prob</a>()</code>,
<code><a href="#topic+tfd_quantile">tfd_quantile</a>()</code>,
<code><a href="#topic+tfd_sample">tfd_sample</a>()</code>,
<code><a href="#topic+tfd_stddev">tfd_stddev</a>()</code>,
<code><a href="#topic+tfd_survival_function">tfd_survival_function</a>()</code>,
<code><a href="#topic+tfd_variance">tfd_variance</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
d &lt;- tfd_normal(loc = c(1, 2), scale = c(1, 0.5))
d %&gt;% tfd_variance()

</code></pre>

<hr>
<h2 id='tfd_cross_entropy'>Computes the (Shannon) cross entropy.</h2><span id='topic+tfd_cross_entropy'></span>

<h3>Description</h3>

<p>Denote this distribution (self) by P and the other distribution by Q.
Assuming P, Q are absolutely continuous with respect to one another and permit densities
p(x) dr(x) and q(x) dr(x), (Shannon) cross entropy is defined as:
<code style="white-space: pre;">&#8288;H[P, Q] = E_p[-log q(X)] = -int_F p(x) log q(x) dr(x)&#8288;</code>
where F denotes the support of the random variable <code>X ~ P</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_cross_entropy(distribution, other, name = "cross_entropy")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_cross_entropy_+3A_distribution">distribution</code></td>
<td>
<p>The distribution being used.</p>
</td></tr>
<tr><td><code id="tfd_cross_entropy_+3A_other">other</code></td>
<td>
<p><code>tfp$distributions$Distribution</code> instance.</p>
</td></tr>
<tr><td><code id="tfd_cross_entropy_+3A_name">name</code></td>
<td>
<p>String prepended to names of ops created by this function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>cross_entropy: self.dtype Tensor with shape <code style="white-space: pre;">&#8288;[B1, ..., Bn]&#8288;</code> representing n different calculations of (Shannon) cross entropy.
</p>


<h3>See Also</h3>

<p>Other distribution_methods: 
<code><a href="#topic+tfd_cdf">tfd_cdf</a>()</code>,
<code><a href="#topic+tfd_covariance">tfd_covariance</a>()</code>,
<code><a href="#topic+tfd_entropy">tfd_entropy</a>()</code>,
<code><a href="#topic+tfd_kl_divergence">tfd_kl_divergence</a>()</code>,
<code><a href="#topic+tfd_log_cdf">tfd_log_cdf</a>()</code>,
<code><a href="#topic+tfd_log_prob">tfd_log_prob</a>()</code>,
<code><a href="#topic+tfd_log_survival_function">tfd_log_survival_function</a>()</code>,
<code><a href="#topic+tfd_mean">tfd_mean</a>()</code>,
<code><a href="#topic+tfd_mode">tfd_mode</a>()</code>,
<code><a href="#topic+tfd_prob">tfd_prob</a>()</code>,
<code><a href="#topic+tfd_quantile">tfd_quantile</a>()</code>,
<code><a href="#topic+tfd_sample">tfd_sample</a>()</code>,
<code><a href="#topic+tfd_stddev">tfd_stddev</a>()</code>,
<code><a href="#topic+tfd_survival_function">tfd_survival_function</a>()</code>,
<code><a href="#topic+tfd_variance">tfd_variance</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  d1 &lt;- tfd_normal(loc = 1, scale = 1)
  d2 &lt;- tfd_normal(loc = 2, scale = 1)
  d1 %&gt;% tfd_cross_entropy(d2)

</code></pre>

<hr>
<h2 id='tfd_deterministic'>Scalar <code>Deterministic</code> distribution on the real line</h2><span id='topic+tfd_deterministic'></span>

<h3>Description</h3>

<p>The scalar <code>Deterministic</code> distribution is parameterized by a (batch) point
<code>loc</code> on the real line.  The distribution is supported at this point only,
and corresponds to a random variable that is constant, equal to <code>loc</code>.
See <a href="https://en.wikipedia.org/wiki/Degenerate_distribution">Degenerate rv</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_deterministic(
  loc,
  atol = NULL,
  rtol = NULL,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Deterministic"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_deterministic_+3A_loc">loc</code></td>
<td>
<p>Numeric <code>Tensor</code> of shape <code style="white-space: pre;">&#8288;[B1, ..., Bb]&#8288;</code>, with <code>b &gt;= 0</code>.
The point (or batch of points) on which this distribution is supported.</p>
</td></tr>
<tr><td><code id="tfd_deterministic_+3A_atol">atol</code></td>
<td>
<p>Non-negative <code>Tensor</code> of same <code>dtype</code> as <code>loc</code> and broadcastable
shape.  The absolute tolerance for comparing closeness to <code>loc</code>.
Default is <code>0</code>.</p>
</td></tr>
<tr><td><code id="tfd_deterministic_+3A_rtol">rtol</code></td>
<td>
<p>Non-negative <code>Tensor</code> of same <code>dtype</code> as <code>loc</code> and broadcastable
shape.  The relative tolerance for comparing closeness to <code>loc</code>.
Default is <code>0</code>.</p>
</td></tr>
<tr><td><code id="tfd_deterministic_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_deterministic_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_deterministic_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The probability mass function (pmf) and cumulative distribution function (cdf) are
</p>
<div class="sourceCode"><pre>pmf(x; loc) = 1, if x == loc, else 0
cdf(x; loc) = 1, if x &gt;= loc, else 0
</pre></div>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_dirichlet'>Dirichlet distribution</h2><span id='topic+tfd_dirichlet'></span>

<h3>Description</h3>

<p>The Dirichlet distribution is defined over the
<a href="https://en.wikipedia.org/wiki/Simplex"><code>(k-1)</code>-simplex</a> using a positive,
length-<code>k</code> vector <code>concentration</code> (<code>k &gt; 1</code>). The Dirichlet is identically the
Beta distribution when <code>k = 2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_dirichlet(
  concentration,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Dirichlet"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_dirichlet_+3A_concentration">concentration</code></td>
<td>
<p>Positive floating-point <code>Tensor</code> indicating mean number
of class occurrences; aka &quot;alpha&quot;. Implies <code>self$dtype</code>, and
<code>self$batch_shape</code>, <code>self$event_shape</code>, i.e., if
<code style="white-space: pre;">&#8288;concentration$shape = [N1, N2, ..., Nm, k]&#8288;</code> then
<code style="white-space: pre;">&#8288;batch_shape = [N1, N2, ..., Nm]&#8288;</code> and <code style="white-space: pre;">&#8288;event_shape = [k]&#8288;</code>.</p>
</td></tr>
<tr><td><code id="tfd_dirichlet_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_dirichlet_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_dirichlet_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The Dirichlet is a distribution over the open <code>(k-1)</code>-simplex, i.e.,
</p>
<div class="sourceCode"><pre>S^{k-1} = { (x_0, ..., x_{k-1}) in R^k : sum_j x_j = 1 and all_j x_j &gt; 0 }.
</pre></div>
<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; alpha) = prod_j x_j**(alpha_j - 1) / Z
Z = prod_j Gamma(alpha_j) / Gamma(sum_j alpha_j)
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;x in S^{k-1}&#8288;</code>, i.e., the <code>(k-1)</code>-simplex,
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;concentration = alpha = [alpha_0, ..., alpha_{k-1}]&#8288;</code>, <code>alpha_j &gt; 0</code>,
</p>
</li>
<li> <p><code>Z</code> is the normalization constant aka the <a href="https://en.wikipedia.org/wiki/Beta_function#Multivariate_beta_function">multivariate beta function</a>,
and,
</p>
</li>
<li> <p><code>Gamma</code> is the <a href="https://en.wikipedia.org/wiki/Gamma_function">gamma function</a>.
</p>
</li></ul>

<p>The <code>concentration</code> represents mean total counts of class occurrence, i.e.,
</p>
<div class="sourceCode"><pre>concentration = alpha = mean * total_concentration
</pre></div>
<p>where <code>mean</code> in <code>S^{k-1}</code> and <code>total_concentration</code> is a positive real number
representing a mean total count.
Distribution parameters are automatically broadcast in all functions; see
examples for details.
Warning: Some components of the samples can be zero due to finite precision.
This happens more often when some of the concentrations are very small.
Make sure to round the samples to <code>np$finfo(dtype)$tiny</code> before computing the density.
Samples of this distribution are reparameterized (pathwise differentiable).
The derivatives are computed using the approach described in the paper
<a href="https://arxiv.org/abs/1805.08498">Michael Figurnov, Shakir Mohamed, Andriy Mnih. Implicit Reparameterization Gradients, 2018</a>
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_dirichlet_multinomial'>Dirichlet-Multinomial compound distribution</h2><span id='topic+tfd_dirichlet_multinomial'></span>

<h3>Description</h3>

<p>The Dirichlet-Multinomial distribution is parameterized by a (batch of)
length-<code>K</code> <code>concentration</code> vectors (<code>K &gt; 1</code>) and a <code>total_count</code> number of
trials, i.e., the number of trials per draw from the DirichletMultinomial. It
is defined over a (batch of) length-<code>K</code> vector <code>counts</code> such that
<code>tf$reduce_sum(counts, -1) = total_count</code>. The Dirichlet-Multinomial is
identically the Beta-Binomial distribution when <code>K = 2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_dirichlet_multinomial(
  total_count,
  concentration,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "DirichletMultinomial"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_dirichlet_multinomial_+3A_total_count">total_count</code></td>
<td>
<p>Non-negative floating point tensor, whose dtype is the same
as <code>concentration</code>. The shape is broadcastable to <code style="white-space: pre;">&#8288;[N1,..., Nm]&#8288;</code> with
<code>m &gt;= 0</code>. Defines this as a batch of <code style="white-space: pre;">&#8288;N1 x ... x Nm&#8288;</code> different
Dirichlet multinomial distributions. Its components should be equal to
integer values.</p>
</td></tr>
<tr><td><code id="tfd_dirichlet_multinomial_+3A_concentration">concentration</code></td>
<td>
<p>Positive floating point tensor, whose dtype is the
same as <code>n</code> with shape broadcastable to <code style="white-space: pre;">&#8288;[N1,..., Nm, K]&#8288;</code> <code>m &gt;= 0</code>.
Defines this as a batch of <code style="white-space: pre;">&#8288;N1 x ... x Nm&#8288;</code> different <code>K</code> class Dirichlet
multinomial distributions.</p>
</td></tr>
<tr><td><code id="tfd_dirichlet_multinomial_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_dirichlet_multinomial_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_dirichlet_multinomial_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The Dirichlet-Multinomial is a distribution over <code>K</code>-class counts, i.e., a
length-<code>K</code> vector of non-negative integer <code style="white-space: pre;">&#8288;counts = n = [n_0, ..., n_{K-1}]&#8288;</code>.
</p>
<p>The probability mass function (pmf) is,
</p>
<div class="sourceCode"><pre>pmf(n; alpha, N) = Beta(alpha + n) / (prod_j n_j!) / Z
Z = Beta(alpha) / N!
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;concentration = alpha = [alpha_0, ..., alpha_{K-1}]&#8288;</code>, <code>alpha_j &gt; 0</code>,
</p>
</li>
<li> <p><code>total_count = N</code>, <code>N</code> a positive integer,
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;N!&#8288;</code> is <code>N</code> factorial, and,
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;Beta(x) = prod_j Gamma(x_j) / Gamma(sum_j x_j)&#8288;</code> is the
<a href="https://en.wikipedia.org/wiki/Beta_function#Multivariate_beta_function">multivariate beta function</a>,
and,
</p>
</li>
<li> <p><code>Gamma</code> is the <a href="https://en.wikipedia.org/wiki/Gamma_function">gamma function</a>.
</p>
</li></ul>

<p>Dirichlet-Multinomial is a <a href="https://en.wikipedia.org/wiki/Compound_probability_distribution">compound distribution</a>, i.e., its
samples are generated as follows.
</p>

<ol>
<li><p> Choose class probabilities:
<code style="white-space: pre;">&#8288;probs = [p_0,...,p_{K-1}] ~ Dir(concentration)&#8288;</code>
</p>
</li>
<li><p> Draw integers:
<code style="white-space: pre;">&#8288;counts = [n_0,...,n_{K-1}] ~ Multinomial(total_count, probs)&#8288;</code>
</p>
</li></ol>

<p>The last <code>concentration</code> dimension parametrizes a single Dirichlet-Multinomial
distribution. When calling distribution functions (e.g., <code>dist$prob(counts)</code>),
<code>concentration</code>, <code>total_count</code> and <code>counts</code> are broadcast to the same shape.
The last dimension of <code>counts</code> corresponds single Dirichlet-Multinomial distributions.
Distribution parameters are automatically broadcast in all functions; see examples for details.
</p>
<p>Pitfalls
The number of classes, <code>K</code>, must not exceed:
</p>

<ul>
<li><p> the largest integer representable by <code>self$dtype</code>, i.e.,
<code>2**(mantissa_bits+1)</code> (IEE754),
</p>
</li>
<li><p> the maximum <code>Tensor</code> index, i.e., <code>2**31-1</code>.
</p>
</li></ul>

<p>Note: This condition is validated only when <code>validate_args = TRUE</code>.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_doublesided_maxwell'>Double-sided Maxwell distribution.</h2><span id='topic+tfd_doublesided_maxwell'></span>

<h3>Description</h3>

<p>This distribution is useful to compute measure valued derivatives for Gaussian
distributions. See Mohamed et al. (2019) for more details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_doublesided_maxwell(
  loc,
  scale,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "doublesided_maxwell"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_doublesided_maxwell_+3A_loc">loc</code></td>
<td>
<p>Floating point tensor; location of the distribution</p>
</td></tr>
<tr><td><code id="tfd_doublesided_maxwell_+3A_scale">scale</code></td>
<td>
<p>Floating point tensor; the scales of the distribution.
Must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_doublesided_maxwell_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_doublesided_maxwell_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_doublesided_maxwell_+3A_name">name</code></td>
<td>
<p>string prefixed to Ops created by this class. Default value: 'doublesided_maxwell'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical details
</p>
<p>The double-sided Maxwell distribution generalizes the Maxwell distribution to
the entire real line.
</p>
<div class="sourceCode"><pre>pdf(x; mu, sigma) = 1/(sigma*sqrt(2*pi)) * ((x-mu)/sigma)^2 * exp(-0.5 ((x-mu)/sigma)^2)
</pre></div>
<p>where <code>loc = mu</code> and <code>scale = sigma</code>.
The DoublesidedMaxwell distribution is a member of the
<a href="https://en.wikipedia.org/wiki/Location-scale_family">location-scale family</a>,
i.e., it can be constructed as,
</p>
<div class="sourceCode"><pre>X ~ DoublesidedMaxwell(loc=0, scale=1)
Y = loc + scale * X
</pre></div>
<p>The double-sided Maxwell is a symmetric distribution that extends the
one-sided maxwell from R+ to the entire real line. Their densities are
therefore the same up to a factor of 0.5.
</p>
<p>It has several methods for generating random variates from it. The version
here uses 3 Gaussian variates and a uniform variate to generate the samples
The sampling path is:
</p>
<p><code style="white-space: pre;">&#8288;mu + sigma* sgn(U-0.5)* sqrt(X^2 + Y^2 + Z^2) U~Unif; X,Y,Z ~N(0,1)&#8288;</code>
</p>
<p>In the sampling process above, the random variates generated by
sqrt(X^2 + Y^2 + Z^2) are samples from the one-sided Maxwell
(or Maxwell-Boltzmann) distribution.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1906.10652">Mohamed, et all, &quot;Monte Carlo Gradient Estimation in Machine Learning.&quot;,2019</a>
</p>
</li>
<li><p> B. Heidergott, et al &quot;Sensitivity estimation for Gaussian systems&quot;, 2008.  European Journal of Operational Research, vol. 187, pp193-207.
</p>
</li>
<li><p> G. Pflug. &quot;Optimization of Stochastic Models: The Interface Between Simulation and Optimization&quot;, 2002. Chp. 4.2, pg 247.
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>

<hr>
<h2 id='tfd_empirical'>Empirical distribution</h2><span id='topic+tfd_empirical'></span>

<h3>Description</h3>

<p>The Empirical distribution is parameterized by a (batch) multiset of samples.
It describes the empirical measure (observations) of a variable.
Note: some methods (log_prob, prob, cdf, mode, entropy) are not differentiable
with regard to samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_empirical(
  samples,
  event_ndims = 0,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Empirical"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_empirical_+3A_samples">samples</code></td>
<td>
<p>Numeric <code>Tensor</code> of shape <code style="white-space: pre;">&#8288;[B1, ..., Bk, S, E1, ..., En]&#8288;</code>,
<code style="white-space: pre;">&#8288;k, n &gt;= 0&#8288;</code>. Samples or batches of samples on which the distribution
is based. The first <code>k</code> dimensions index into a batch of independent
distributions. Length of <code>S</code> dimension determines number of samples
in each multiset. The last <code>n</code> dimension represents samples for each
distribution. n is specified by argument event_ndims.</p>
</td></tr>
<tr><td><code id="tfd_empirical_+3A_event_ndims">event_ndims</code></td>
<td>
<p><code>int32</code>, default <code>0</code>. number of dimensions for each
event. When <code>0</code> this distribution has scalar samples. When <code>1</code> this
distribution has vector-like samples.</p>
</td></tr>
<tr><td><code id="tfd_empirical_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_empirical_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_empirical_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The probability mass function (pmf) and cumulative distribution function (cdf) are
</p>
<div class="sourceCode"><pre>pmf(k; s1, ..., sn) = sum_i I(k)^{k == si} / n
I(k)^{k == si} == 1, if k == si, else 0.
cdf(k; s1, ..., sn) = sum_i I(k)^{k &gt;= si} / n
I(k)^{k &gt;= si} == 1, if k &gt;= si, else 0.
</pre></div>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_entropy'>Shannon entropy in nats.</h2><span id='topic+tfd_entropy'></span>

<h3>Description</h3>

<p>Shannon entropy in nats.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_entropy(distribution, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_entropy_+3A_distribution">distribution</code></td>
<td>
<p>The distribution being used.</p>
</td></tr>
<tr><td><code id="tfd_entropy_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to Python.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Tensor of shape <code>sample_shape(x) + self$batch_shape</code> with values of type <code>self$dtype</code>.
</p>


<h3>See Also</h3>

<p>Other distribution_methods: 
<code><a href="#topic+tfd_cdf">tfd_cdf</a>()</code>,
<code><a href="#topic+tfd_covariance">tfd_covariance</a>()</code>,
<code><a href="#topic+tfd_cross_entropy">tfd_cross_entropy</a>()</code>,
<code><a href="#topic+tfd_kl_divergence">tfd_kl_divergence</a>()</code>,
<code><a href="#topic+tfd_log_cdf">tfd_log_cdf</a>()</code>,
<code><a href="#topic+tfd_log_prob">tfd_log_prob</a>()</code>,
<code><a href="#topic+tfd_log_survival_function">tfd_log_survival_function</a>()</code>,
<code><a href="#topic+tfd_mean">tfd_mean</a>()</code>,
<code><a href="#topic+tfd_mode">tfd_mode</a>()</code>,
<code><a href="#topic+tfd_prob">tfd_prob</a>()</code>,
<code><a href="#topic+tfd_quantile">tfd_quantile</a>()</code>,
<code><a href="#topic+tfd_sample">tfd_sample</a>()</code>,
<code><a href="#topic+tfd_stddev">tfd_stddev</a>()</code>,
<code><a href="#topic+tfd_survival_function">tfd_survival_function</a>()</code>,
<code><a href="#topic+tfd_variance">tfd_variance</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  d &lt;- tfd_normal(loc = c(1, 2), scale = c(1, 0.5))
  d %&gt;% tfd_entropy()

</code></pre>

<hr>
<h2 id='tfd_exp_gamma'>ExpGamma distribution.</h2><span id='topic+tfd_exp_gamma'></span>

<h3>Description</h3>

<p>The ExpGamma distribution is defined over the real line using
parameters <code>concentration</code> (aka &quot;alpha&quot;) and <code>rate</code> (aka &quot;beta&quot;).
This distribution is a transformation of the Gamma distribution such that
X ~ ExpGamma(..) =&gt; exp(X) ~ Gamma(..).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_exp_gamma(
  concentration,
  rate = NULL,
  log_rate = NULL,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "ExpGamma"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_exp_gamma_+3A_concentration">concentration</code></td>
<td>
<p>Floating point tensor, the concentration params of the
distribution(s). Must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_exp_gamma_+3A_rate">rate</code></td>
<td>
<p>Floating point tensor, the inverse scale params of the
distribution(s). Must contain only positive values. Mutually exclusive
with <code>log_rate</code>.</p>
</td></tr>
<tr><td><code id="tfd_exp_gamma_+3A_log_rate">log_rate</code></td>
<td>
<p>Floating point tensor, natural logarithm of the inverse scale
params of the distribution(s). Mutually exclusive with <code>rate</code>.</p>
</td></tr>
<tr><td><code id="tfd_exp_gamma_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_exp_gamma_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_exp_gamma_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The probability density function (pdf) can be derived from the change of
variables rule (since the distribution is logically equivalent to
<code>tfb_log()(tfd_gamma(..))</code>):
</p>
<div class="sourceCode"><pre>pdf(x; alpha, beta &gt; 0) = exp(x)**(alpha - 1) exp(-exp(x) beta) / Z + x
Z = Gamma(alpha) beta**(-alpha)
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>concentration = alpha</code>, <code>alpha &gt; 0</code>,
</p>
</li>
<li> <p><code>rate = beta</code>, <code>beta &gt; 0</code>,
</p>
</li>
<li> <p><code>Z</code> is the normalizing constant of the corresponding Gamma distribution, and
</p>
</li>
<li> <p><code>Gamma</code> is the <a href="https://en.wikipedia.org/wiki/Gamma_function">gamma function</a>.
</p>
</li></ul>

<p>The cumulative density function (cdf) is,
</p>
<div class="sourceCode"><pre>cdf(x; alpha, beta, x) = GammaInc(alpha, beta exp(x)) / Gamma(alpha)
</pre></div>
<p>where <code>GammaInc</code> is the <a href="https://en.wikipedia.org/wiki/Incomplete_gamma_function">lower incomplete Gamma function</a>.
</p>
<p>Distribution parameters are automatically broadcast in all functions.
Samples of this distribution are reparameterized (pathwise differentiable).
The derivatives are computed using the approach described in Figurnov et al., 2018.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1805.08498">Michael Figurnov, Shakir Mohamed, Andriy Mnih. Implicit Reparameterization Gradients. <em>arXiv preprint arXiv:1805.08498</em>, 2018.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_exp_inverse_gamma'>ExpInverseGamma distribution.</h2><span id='topic+tfd_exp_inverse_gamma'></span>

<h3>Description</h3>

<p>The <code>ExpInverseGamma</code> distribution is defined over the real numbers such that
X ~ ExpInverseGamma(..) =&gt; exp(X) ~ InverseGamma(..).
The distribution is logically equivalent to <code>tfb_log()(tfd_inverse_gamma(..))</code>,
but can be sampled with much better precision.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_exp_inverse_gamma(
  concentration,
  scale = NULL,
  log_scale = NULL,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "ExpGamma"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_exp_inverse_gamma_+3A_concentration">concentration</code></td>
<td>
<p>Floating point tensor, the concentration params of the
distribution(s). Must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_exp_inverse_gamma_+3A_scale">scale</code></td>
<td>
<p>Floating point tensor, the scale params of the distribution(s).
Must contain only positive values. Mutually exclusive with <code>log_scale</code>.</p>
</td></tr>
<tr><td><code id="tfd_exp_inverse_gamma_+3A_log_scale">log_scale</code></td>
<td>
<p>Floating point tensor, the natural logarithm of the scale
params of the distribution(s). Mutually exclusive with <code>scale</code>.</p>
</td></tr>
<tr><td><code id="tfd_exp_inverse_gamma_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_exp_inverse_gamma_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_exp_inverse_gamma_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The probability density function (pdf) is very similar to ExpGamma,
</p>
<div class="sourceCode"><pre>pdf(x; alpha, beta &gt; 0) = exp(-x)**(alpha - 1) exp(-exp(-x) beta) / Z - x
Z = Gamma(alpha) beta**(-alpha)
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>concentration = alpha</code>,
</p>
</li>
<li> <p><code>scale = beta</code>,
</p>
</li>
<li> <p><code>Z</code> is the normalizing constant, and,
</p>
</li>
<li> <p><code>Gamma</code> is the <a href="https://en.wikipedia.org/wiki/Gamma_function">gamma function</a>.
</p>
</li></ul>

<p>The cumulative density function (cdf) is,
</p>
<div class="sourceCode"><pre>cdf(x; alpha, beta, x) = 1 - GammaInc(alpha, beta exp(-x)) / Gamma(alpha)
</pre></div>
<p>where <code>GammaInc</code> is the <a href="https://en.wikipedia.org/wiki/Incomplete_gamma_function">upper incomplete Gamma function</a>.
</p>
<p>Distribution parameters are automatically broadcast in all functions.
Samples of this distribution are reparameterized (pathwise differentiable).
The derivatives are computed using the approach described in Figurnov et al, 2018.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1805.08498">Michael Figurnov, Shakir Mohamed, Andriy Mnih. Implicit Reparameterization Gradients. <em>arXiv preprint arXiv:1805.08498</em>, 2018.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_exp_relaxed_one_hot_categorical'>ExpRelaxedOneHotCategorical distribution with temperature and logits.</h2><span id='topic+tfd_exp_relaxed_one_hot_categorical'></span>

<h3>Description</h3>

<p>ExpRelaxedOneHotCategorical distribution with temperature and logits.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_exp_relaxed_one_hot_categorical(
  temperature,
  logits = NULL,
  probs = NULL,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "ExpRelaxedOneHotCategorical"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_exp_relaxed_one_hot_categorical_+3A_temperature">temperature</code></td>
<td>
<p>An 0-D Tensor, representing the temperature of a set of
ExpRelaxedCategorical distributions. The temperature should be positive.</p>
</td></tr>
<tr><td><code id="tfd_exp_relaxed_one_hot_categorical_+3A_logits">logits</code></td>
<td>
<p>An N-D Tensor, N &gt;= 1, representing the log probabilities of a
set of ExpRelaxedCategorical distributions. The first N - 1 dimensions index
into a batch of independent distributions and the last dimension represents a
vector of logits for each class. Only one of logits or probs should be passed
in.</p>
</td></tr>
<tr><td><code id="tfd_exp_relaxed_one_hot_categorical_+3A_probs">probs</code></td>
<td>
<p>An N-D Tensor, N &gt;= 1, representing the probabilities of a set of
ExpRelaxedCategorical distributions. The first N - 1 dimensions index into a
batch of independent distributions and the last dimension represents a vector
of probabilities for each class. Only one of logits or probs should be passed
in.</p>
</td></tr>
<tr><td><code id="tfd_exp_relaxed_one_hot_categorical_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_exp_relaxed_one_hot_categorical_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_exp_relaxed_one_hot_categorical_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>

<hr>
<h2 id='tfd_exponential'>Exponential distribution</h2><span id='topic+tfd_exponential'></span>

<h3>Description</h3>

<p>The Exponential distribution is parameterized by an event <code>rate</code> parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_exponential(
  rate,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Exponential"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_exponential_+3A_rate">rate</code></td>
<td>
<p>Floating point tensor, equivalent to <code>1 / mean</code>. Must contain only
positive values.</p>
</td></tr>
<tr><td><code id="tfd_exponential_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_exponential_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_exponential_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; lambda, x &gt; 0) = exp(-lambda x) / Z
Z = 1 / lambda
</pre></div>
<p>where <code>rate = lambda</code> and <code>Z</code> is the normalizing constant.
</p>
<p>The Exponential distribution is a special case of the Gamma distribution,
i.e.,
</p>
<div class="sourceCode"><pre>Exponential(rate) = Gamma(concentration=1., rate)
</pre></div>
<p>The Exponential distribution uses a <code>rate</code> parameter, or &quot;inverse scale&quot;,
which can be intuited as,
</p>
<div class="sourceCode"><pre>X ~ Exponential(rate=1)
Y = X / rate
</pre></div>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_finite_discrete'>The finite discrete distribution.</h2><span id='topic+tfd_finite_discrete'></span>

<h3>Description</h3>

<p>The FiniteDiscrete distribution is parameterized by either probabilities or
log-probabilities of a set of <code>K</code> possible outcomes, which is defined by
a strictly ascending list of <code>K</code> values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_finite_discrete(
  outcomes,
  logits = NULL,
  probs = NULL,
  rtol = NULL,
  atol = NULL,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "FiniteDiscrete"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_finite_discrete_+3A_outcomes">outcomes</code></td>
<td>
<p>A 1-D floating or integer <code>Tensor</code>, representing a list of
possible outcomes in strictly ascending order.</p>
</td></tr>
<tr><td><code id="tfd_finite_discrete_+3A_logits">logits</code></td>
<td>
<p>A floating N-D <code>Tensor</code>, <code>N &gt;= 1</code>, representing the log
probabilities of a set of FiniteDiscrete distributions. The first <code>N - 1</code>
dimensions index into a batch of independent distributions and the
last dimension represents a vector of logits for each discrete value.
Only one of <code>logits</code> or <code>probs</code> should be passed in.</p>
</td></tr>
<tr><td><code id="tfd_finite_discrete_+3A_probs">probs</code></td>
<td>
<p>A floating  N-D <code>Tensor</code>, <code>N &gt;= 1</code>, representing the probabilities
of a set of FiniteDiscrete distributions. The first <code>N - 1</code> dimensions
index into a batch of independent distributions and the last dimension
represents a vector of probabilities for each discrete value. Only one
of <code>logits</code> or <code>probs</code> should be passed in.</p>
</td></tr>
<tr><td><code id="tfd_finite_discrete_+3A_rtol">rtol</code></td>
<td>
<p><code>Tensor</code> with same <code>dtype</code> as <code>outcomes</code>. The relative tolerance for
floating number comparison. Only effective when <code>outcomes</code> is a floating
<code>Tensor</code>. Default is <code>10 * eps</code>.</p>
</td></tr>
<tr><td><code id="tfd_finite_discrete_+3A_atol">atol</code></td>
<td>
<p><code>Tensor</code> with same <code>dtype</code> as <code>outcomes</code>. The absolute tolerance for
floating number comparison. Only effective when <code>outcomes</code> is a floating
<code>Tensor</code>. Default is <code>10 * eps</code>.</p>
</td></tr>
<tr><td><code id="tfd_finite_discrete_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_finite_discrete_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_finite_discrete_+3A_name">name</code></td>
<td>
<p>string prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note: log_prob, prob, cdf, mode, and entropy are differentiable with respect
to <code>logits</code> or <code>probs</code> but not with respect to <code>outcomes</code>.
</p>
<p>Mathematical Details
</p>
<p>The probability mass function (pmf) is,
</p>
<p><code style="white-space: pre;">&#8288;pmf(x; pi, qi) = prod_j pi_j**[x == qi_j]&#8288;</code>
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>

<hr>
<h2 id='tfd_gamma'>Gamma distribution</h2><span id='topic+tfd_gamma'></span>

<h3>Description</h3>

<p>The Gamma distribution is defined over positive real numbers using
parameters <code>concentration</code> (aka &quot;alpha&quot;) and <code>rate</code> (aka &quot;beta&quot;).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_gamma(
  concentration,
  rate,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Gamma"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_gamma_+3A_concentration">concentration</code></td>
<td>
<p>Floating point tensor, the concentration params of the
distribution(s). Must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_gamma_+3A_rate">rate</code></td>
<td>
<p>Floating point tensor, the inverse scale params of the
distribution(s). Must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_gamma_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_gamma_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_gamma_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; alpha, beta, x &gt; 0) = x**(alpha - 1) exp(-x beta) / Z
Z = Gamma(alpha) beta**(-alpha)
</pre></div>
<p>where
</p>

<ul>
<li> <p><code>concentration = alpha</code>, <code>alpha &gt; 0</code>,
</p>
</li>
<li> <p><code>rate = beta</code>, <code>beta &gt; 0</code>,
</p>
</li>
<li> <p><code>Z</code> is the normalizing constant, and,
</p>
</li>
<li> <p><code>Gamma</code> is the <a href="https://en.wikipedia.org/wiki/Gamma_function">gamma function</a>.
</p>
</li></ul>

<p>The cumulative density function (cdf) is,
</p>
<div class="sourceCode"><pre>cdf(x; alpha, beta, x &gt; 0) = GammaInc(alpha, beta x) / Gamma(alpha)
</pre></div>
<p>where <code>GammaInc</code> is the <a href="https://en.wikipedia.org/wiki/Incomplete_gamma_function">lower incomplete Gamma function</a>.
The parameters can be intuited via their relationship to mean and stddev,
</p>
<div class="sourceCode"><pre>concentration = alpha = (mean / stddev)**2
rate = beta = mean / stddev**2 = concentration / mean
</pre></div>
<p>Distribution parameters are automatically broadcast in all functions; see
examples for details.
</p>
<p>Warning: The samples of this distribution are always non-negative. However,
the samples that are smaller than <code>np$finfo(dtype)$tiny</code> are rounded
to this value, so it appears more often than it should.
This should only be noticeable when the <code>concentration</code> is very small, or the
<code>rate</code> is very large. See note in <code>tf$random_gamma</code> docstring.
Samples of this distribution are reparameterized (pathwise differentiable).
The derivatives are computed using the approach described in the paper
<a href="https://arxiv.org/abs/1805.08498">Michael Figurnov, Shakir Mohamed, Andriy Mnih. Implicit Reparameterization Gradients, 2018</a>
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_gamma_gamma'>Gamma-Gamma distribution</h2><span id='topic+tfd_gamma_gamma'></span>

<h3>Description</h3>

<p>Gamma-Gamma is a <a href="https://en.wikipedia.org/wiki/Compound_probability_distribution">compound distribution</a>
defined over positive real numbers using parameters <code>concentration</code>,
<code>mixing_concentration</code> and <code>mixing_rate</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_gamma_gamma(
  concentration,
  mixing_concentration,
  mixing_rate,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "GammaGamma"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_gamma_gamma_+3A_concentration">concentration</code></td>
<td>
<p>Floating point tensor, the concentration params of the
distribution(s). Must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_gamma_gamma_+3A_mixing_concentration">mixing_concentration</code></td>
<td>
<p>Floating point tensor, the concentration params of
the mixing Gamma distribution(s). Must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_gamma_gamma_+3A_mixing_rate">mixing_rate</code></td>
<td>
<p>Floating point tensor, the rate params of the mixing Gamma
distribution(s). Must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_gamma_gamma_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_gamma_gamma_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_gamma_gamma_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This distribution is also referred to as the beta of the second kind (B2), and
can be useful for transaction value modeling, as in Fader and Hardi, 2013.
</p>
<p>Mathematical Details
</p>
<p>It is derived from the following Gamma-Gamma hierarchical model by integrating
out the random variable <code>beta</code>.
</p>
<div class="sourceCode"><pre>beta ~ Gamma(alpha0, beta0)
X | beta ~ Gamma(alpha, beta)
</pre></div>
<p>where
</p>

<ul>
<li> <p><code>concentration = alpha</code>
</p>
</li>
<li> <p><code>mixing_concentration = alpha0</code>
</p>
</li>
<li> <p><code>mixing_rate = beta0</code>
</p>
</li></ul>

<p>The probability density function (pdf) is
</p>
<div class="sourceCode"><pre>x**(alpha - 1)
pdf(x; alpha, alpha0, beta0) =  Z * (x + beta0)**(alpha + alpha0)
</pre></div>
<p>where the normalizing constant <code>Z = Beta(alpha, alpha0) * beta0**(-alpha0)</code>.
Samples of this distribution are reparameterized as samples of the Gamma
distribution are reparameterized using the technique described in
(Figurnov et al., 2018).
</p>
<p>@section References:
</p>

<ul>
<li> <p><a href="http://www.brucehardie.com/notes/025/gamma_gamma.pdf">Peter S. Fader, Bruce G. S. Hardi. The Gamma-Gamma Model of Monetary Value. <em>Technical Report</em>, 2013.</a>
</p>
</li>
<li> <p><a href="https://arxiv.org/abs/1805.08498">Michael Figurnov, Shakir Mohamed, Andriy Mnih. Implicit Reparameterization Gradients. <em>arXiv preprint arXiv:1805.08498</em>, 2018</a>
</p>
</li></ul>



<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_gaussian_process'>Marginal distribution of a Gaussian process at finitely many points.</h2><span id='topic+tfd_gaussian_process'></span>

<h3>Description</h3>

<p>A Gaussian process (GP) is an indexed collection of random variables, any
finite collection of which are jointly Gaussian. While this definition applies
to finite index sets, it is typically implicit that the index set is infinite;
in applications, it is often some finite dimensional real or complex vector
space. In such cases, the GP may be thought of as a distribution over
(real- or complex-valued) functions defined over the index set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_gaussian_process(
  kernel,
  index_points,
  mean_fn = NULL,
  observation_noise_variance = 0,
  jitter = 1e-06,
  validate_args = FALSE,
  allow_nan_stats = FALSE,
  name = "GaussianProcess"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_gaussian_process_+3A_kernel">kernel</code></td>
<td>
<p><code>PositiveSemidefiniteKernel</code>-like instance representing the
GP's covariance function.</p>
</td></tr>
<tr><td><code id="tfd_gaussian_process_+3A_index_points">index_points</code></td>
<td>
<p><code>float</code> <code>Tensor</code> representing finite (batch of) vector(s) of
points in the index set over which the GP is defined. Shape has the
form <code style="white-space: pre;">&#8288;[b1, ..., bB, e1, f1, ..., fF]&#8288;</code> where <code>F</code> is the number of feature
dimensions and must equal <code>kernel$feature_ndims</code> and <code>e1</code> is the number
(size) of index points in each batch (we denote it <code>e1</code> to distinguish
it from the numer of inducing index points, denoted <code>e2</code> below).
Ultimately the GaussianProcess distribution corresponds to an
<code>e1</code>-dimensional multivariate normal. The batch shape must be
broadcastable with <code>kernel$batch_shape</code>, the batch shape of
<code>inducing_index_points</code>, and any batch dims yielded by <code>mean_fn</code>.</p>
</td></tr>
<tr><td><code id="tfd_gaussian_process_+3A_mean_fn">mean_fn</code></td>
<td>
<p>function that acts on index points to produce a (batch
of) vector(s) of mean values at those index points. Takes a <code>Tensor</code> of
shape <code style="white-space: pre;">&#8288;[b1, ..., bB, f1, ..., fF]&#8288;</code> and returns a <code>Tensor</code> whose shape is
(broadcastable with) <code style="white-space: pre;">&#8288;[b1, ..., bB]&#8288;</code>. Default value: <code>NULL</code> implies constant zero function.</p>
</td></tr>
<tr><td><code id="tfd_gaussian_process_+3A_observation_noise_variance">observation_noise_variance</code></td>
<td>
<p><code>float</code> <code>Tensor</code> representing the variance
of the noise in the Normal likelihood distribution of the model. May be
batched, in which case the batch shape must be broadcastable with the
shapes of all other batched parameters (<code>kernel$batch_shape</code>, <code>index_points</code>, etc.).
Default value: <code>0.</code></p>
</td></tr>
<tr><td><code id="tfd_gaussian_process_+3A_jitter">jitter</code></td>
<td>
<p><code>float</code> scalar <code>Tensor</code> added to the diagonal of the covariance
matrix to ensure positive definiteness of the covariance matrix. Default value: <code>1e-6</code>.</p>
</td></tr>
<tr><td><code id="tfd_gaussian_process_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_gaussian_process_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_gaussian_process_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Just as Gaussian distributions are fully specified by their first and second
moments, a Gaussian process can be completely specified by a mean and
covariance function.
Let <code>S</code> denote the index set and <code>K</code> the space in which
each indexed random variable takes its values (again, often R or C). The mean
function is then a map <code>m: S -&gt; K</code>, and the covariance function, or kernel, is
a positive-definite function <code style="white-space: pre;">&#8288;k: (S x S) -&gt; K&#8288;</code>. The properties of functions
drawn from a GP are entirely dictated (up to translation) by the form of the
kernel function.
</p>
<p>This <code>Distribution</code> represents the marginal joint distribution over function
values at a given finite collection of points <code style="white-space: pre;">&#8288;[x[1], ..., x[N]]&#8288;</code> from the
index set <code>S</code>. By definition, this marginal distribution is just a
multivariate normal distribution, whose mean is given by the vector
<code style="white-space: pre;">&#8288;[ m(x[1]), ..., m(x[N]) ]&#8288;</code> and whose covariance matrix is constructed from
pairwise applications of the kernel function to the given inputs:
</p>
<div class="sourceCode"><pre>| k(x[1], x[1])    k(x[1], x[2])  ...  k(x[1], x[N]) |
| k(x[2], x[1])    k(x[2], x[2])  ...  k(x[2], x[N]) |
|      ...              ...                 ...      |
| k(x[N], x[1])    k(x[N], x[2])  ...  k(x[N], x[N]) |
</pre></div>
<p>For this to be a valid covariance matrix, it must be symmetric and positive
definite; hence the requirement that <code>k</code> be a positive definite function
(which, by definition, says that the above procedure will yield PD matrices).
</p>
<p>We also support the inclusion of zero-mean Gaussian noise in the model, via
the <code>observation_noise_variance</code> parameter. This augments the generative model
to
</p>
<div class="sourceCode"><pre>f ~ GP(m, k)
(y[i] | f, x[i]) ~ Normal(f(x[i]), s)
</pre></div>
<p>where
</p>

<ul>
<li> <p><code>m</code> is the mean function
</p>
</li>
<li> <p><code>k</code> is the covariance kernel function
</p>
</li>
<li> <p><code>f</code> is the function drawn from the GP
</p>
</li>
<li> <p><code>x[i]</code> are the index points at which the function is observed
</p>
</li>
<li> <p><code>y[i]</code> are the observed values at the index points
</p>
</li>
<li> <p><code>s</code> is the scale of the observation noise.
</p>
</li></ul>

<p>Note that this class represents an <em>unconditional</em> Gaussian process; it does
not implement posterior inference conditional on observed function
evaluations. This class is useful, for example, if one wishes to combine a GP
prior with a non-conjugate likelihood using MCMC to sample from the posterior.
</p>
<p>Mathematical Details
</p>
<p>The probability density function (pdf) is a multivariate normal whose
parameters are derived from the GP's properties:
</p>
<div class="sourceCode"><pre>pdf(x; index_points, mean_fn, kernel) = exp(-0.5 * y) / Z
K = (kernel.matrix(index_points, index_points) +
    (observation_noise_variance + jitter) * eye(N))
y = (x - mean_fn(index_points))^T @ K @ (x - mean_fn(index_points))
Z = (2 * pi)**(.5 * N) |det(K)|**(.5)
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>index_points</code> are points in the index set over which the GP is defined,
</p>
</li>
<li> <p><code>mean_fn</code> is a callable mapping the index set to the GP's mean values,
</p>
</li>
<li> <p><code>kernel</code> is <code>PositiveSemidefiniteKernel</code>-like and represents the covariance
function of the GP,
</p>
</li>
<li> <p><code>observation_noise_variance</code> represents (optional) observation noise.
</p>
</li>
<li> <p><code>jitter</code> is added to the diagonal to ensure positive definiteness up to
machine precision (otherwise Cholesky-decomposition is prone to failure),
</p>
</li>
<li> <p><code>eye(N)</code> is an N-by-N identity matrix.
</p>
</li></ul>



<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_gaussian_process_regression_model'>Posterior predictive distribution in a conjugate GP regression model.</h2><span id='topic+tfd_gaussian_process_regression_model'></span>

<h3>Description</h3>

<p>Posterior predictive distribution in a conjugate GP regression model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_gaussian_process_regression_model(
  kernel,
  index_points = NULL,
  observation_index_points = NULL,
  observations = NULL,
  observation_noise_variance = 0,
  predictive_noise_variance = NULL,
  mean_fn = NULL,
  jitter = 1e-06,
  validate_args = FALSE,
  allow_nan_stats = FALSE,
  name = "GaussianProcessRegressionModel"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_gaussian_process_regression_model_+3A_kernel">kernel</code></td>
<td>
<p><code>PositiveSemidefiniteKernel</code>-like instance representing the
GP's covariance function.</p>
</td></tr>
<tr><td><code id="tfd_gaussian_process_regression_model_+3A_index_points">index_points</code></td>
<td>
<p><code>float</code> <code>Tensor</code> representing finite (batch of) vector(s) of
points in the index set over which the GP is defined. Shape has the
form <code style="white-space: pre;">&#8288;[b1, ..., bB, e1, f1, ..., fF]&#8288;</code> where <code>F</code> is the number of feature
dimensions and must equal <code>kernel$feature_ndims</code> and <code>e1</code> is the number
(size) of index points in each batch (we denote it <code>e1</code> to distinguish
it from the numer of inducing index points, denoted <code>e2</code> below).
Ultimately the GaussianProcess distribution corresponds to an
<code>e1</code>-dimensional multivariate normal. The batch shape must be
broadcastable with <code>kernel$batch_shape</code>, the batch shape of
<code>inducing_index_points</code>, and any batch dims yielded by <code>mean_fn</code>.</p>
</td></tr>
<tr><td><code id="tfd_gaussian_process_regression_model_+3A_observation_index_points">observation_index_points</code></td>
<td>
<p>Tensor representing finite collection, or batch
of collections, of points in the index set for which some data has been observed.
Shape has the form [b1, ..., bB, e, f1, ..., fF] where F is the number of
feature dimensions and must equal <code>kernel$feature_ndims</code>, and e is the number
(size) of index points in each batch. [b1, ..., bB, e] must be broadcastable
with the shape of observations, and [b1, ..., bB] must be broadcastable with
the shapes of all other batched parameters (kernel.batch_shape, index_points, etc).
The default value is None, which corresponds to the empty set of observations,
and simply results in the prior predictive model (a GP with noise of variance
<code>predictive_noise_variance</code>).</p>
</td></tr>
<tr><td><code id="tfd_gaussian_process_regression_model_+3A_observations">observations</code></td>
<td>
<p>Tensor representing collection, or batch of collections,
of observations corresponding to observation_index_points. Shape has the
form [b1, ..., bB, e], which must be brodcastable with the batch and example
shapes of observation_index_points. The batch shape [b1, ..., bB\ ] must be
broadcastable with the shapes of all other batched parameters (kernel.batch_shape,
index_points, etc.). The default value is None, which corresponds to the empty
set of observations, and simply results in the prior predictive model (a GP
with noise of variance <code>predictive_noise_variance</code>).</p>
</td></tr>
<tr><td><code id="tfd_gaussian_process_regression_model_+3A_observation_noise_variance">observation_noise_variance</code></td>
<td>
<p><code>float</code> <code>Tensor</code> representing the variance
of the noise in the Normal likelihood distribution of the model. May be
batched, in which case the batch shape must be broadcastable with the
shapes of all other batched parameters (<code>kernel$batch_shape</code>, <code>index_points</code>, etc.).
Default value: <code>0.</code></p>
</td></tr>
<tr><td><code id="tfd_gaussian_process_regression_model_+3A_predictive_noise_variance">predictive_noise_variance</code></td>
<td>
<p>Tensor representing the variance in the posterior
predictive model. If None, we simply re-use observation_noise_variance for the
posterior predictive noise. If set explicitly, however, we use this value. This
allows us, for example, to omit predictive noise variance (by setting this to zero)
to obtain noiseless posterior predictions of function values, conditioned on noisy
observations.</p>
</td></tr>
<tr><td><code id="tfd_gaussian_process_regression_model_+3A_mean_fn">mean_fn</code></td>
<td>
<p>callable that acts on <code>index_points</code> to produce a collection, or
batch of collections, of mean values at index_points. Takes a Tensor of shape
[b1, ..., bB, f1, ..., fF] and returns a Tensor whose shape is broadcastable
with [b1, ..., bB]. Default value: None implies the constant zero function.</p>
</td></tr>
<tr><td><code id="tfd_gaussian_process_regression_model_+3A_jitter">jitter</code></td>
<td>
<p><code>float</code> scalar <code>Tensor</code> added to the diagonal of the covariance
matrix to ensure positive definiteness of the covariance matrix. Default value: <code>1e-6</code>.</p>
</td></tr>
<tr><td><code id="tfd_gaussian_process_regression_model_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_gaussian_process_regression_model_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_gaussian_process_regression_model_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_generalized_normal'>The Generalized Normal distribution.</h2><span id='topic+tfd_generalized_normal'></span>

<h3>Description</h3>

<p>The Generalized Normal (or Generalized Gaussian) generalizes the Normal
distribution with an additional shape parameter. It is parameterized by
location <code>loc</code>, scale <code>scale</code> and shape <code>power</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_generalized_normal(
  loc,
  scale,
  power,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "GeneralizedNormal"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_generalized_normal_+3A_loc">loc</code></td>
<td>
<p>Floating point tensor; the means of the distribution(s).</p>
</td></tr>
<tr><td><code id="tfd_generalized_normal_+3A_scale">scale</code></td>
<td>
<p>Floating point tensor; the scale of the
distribution(s). Must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_generalized_normal_+3A_power">power</code></td>
<td>
<p>Floating point tensor; the shape parameter of the distribution(s).
Must contain only positive values. <code>loc</code>, <code>scale</code> and <code>power</code> must have
compatible shapes for broadcasting.</p>
</td></tr>
<tr><td><code id="tfd_generalized_normal_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_generalized_normal_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_generalized_normal_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical details
The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; loc, scale, power) = 1 / (2 * scale * Gamma(1 + 1 / power)) *
  exp(-(|x - loc| / scale) ^ power)
</pre></div>
<p>where <code>loc</code> is the mean, <code>scale</code> is the scale, and,  <code>power</code> is the shape
parameter. If the power is above two, the distribution becomes platykurtic.
A power equal to two results in a Normal distribution. A power smaller than
two produces a leptokurtic (heavy-tailed) distribution. Mean and scale behave
the same way as in the equivalent Normal distribution.
</p>
<p>See https://en.wikipedia.org/w/index.php?title=Generalized_normal_distribution&amp;oldid=954254464
for the definitions used here, including CDF, variance and entropy. See
https://sccn.ucsd.edu/wiki/Generalized_Gaussian_Probability_Density_Function
for the sampling method used here.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_generalized_pareto'>The Generalized Pareto distribution.</h2><span id='topic+tfd_generalized_pareto'></span>

<h3>Description</h3>

<p>The Generalized Pareto distributions are a family of continuous distributions
on the reals. Special cases include <code>Exponential</code> (when <code>loc = 0</code>,
<code>concentration = 0</code>), <code>Pareto</code> (when <code>concentration &gt; 0</code>,
<code>loc = scale / concentration</code>), and <code>Uniform</code> (when <code>concentration = -1</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_generalized_pareto(
  loc,
  scale,
  concentration,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_generalized_pareto_+3A_loc">loc</code></td>
<td>
<p>The location / shift of the distribution. GeneralizedPareto is a
location-scale distribution. This parameter lower bounds the
distribution's support. Must broadcast with <code>scale</code>, <code>concentration</code>.
Floating point <code>Tensor</code>.</p>
</td></tr>
<tr><td><code id="tfd_generalized_pareto_+3A_scale">scale</code></td>
<td>
<p>The scale of the distribution. GeneralizedPareto is a
location-scale distribution, so doubling the <code>scale</code> doubles a sample
and halves the density. Strictly positive floating point <code>Tensor</code>. Must
broadcast with <code>loc</code>, <code>concentration</code>.</p>
</td></tr>
<tr><td><code id="tfd_generalized_pareto_+3A_concentration">concentration</code></td>
<td>
<p>The shape parameter of the distribution. The larger the
magnitude, the more the distribution concentrates near <code>loc</code> (for
<code>concentration &gt;= 0</code>) or near <code>loc - (scale/concentration)</code> (for
<code>concentration &lt; 0</code>). Floating point <code>Tensor</code>.</p>
</td></tr>
<tr><td><code id="tfd_generalized_pareto_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_generalized_pareto_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_generalized_pareto_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This distribution is often used to model the tails of other distributions.
As a member of the location-scale family,
<code>X ~ GeneralizedPareto(loc=loc, scale=scale, concentration=conc)</code> maps to
<code>Y ~ GeneralizedPareto(loc=0, scale=1, concentration=conc)</code> via
<code>Y = (X - loc) / scale</code>.
</p>
<p>For positive concentrations, the distribution is equivalent to a hierarchical
Exponential-Gamma model with <code>X|rate ~ Exponential(rate)</code> and
<code>rate ~ Gamma(concentration=1 / concentration, scale=scale / concentration)</code>.
In the following, <code>samps1</code> and <code>samps2</code> are identically distributed:
</p>
<div class="sourceCode"><pre>genp &lt;- tfd_generalized_pareto(loc = 0, scale = scale, concentration = conc)
samps1 &lt;- genp %&gt;% tfd_sample(1000)
jd &lt;- tfd_joint_distribution_named(
  list(
    rate =  tfd_gamma(1 / genp$concentration, genp$scale / genp$concentration),
    x = function(rate) tfd_exponential(rate)))
samps2 &lt;- jd %&gt;% tfd_sample(1000) %&gt;% .$x
</pre></div>
<p>The support of the distribution is always lower bounded by <code>loc</code>. When
<code>concentration &lt; 0</code>, the support is also upper bounded by
<code>loc + scale / abs(concentration)</code>.
</p>
<p>Mathematical Details
</p>
<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; mu, sigma, shp, x &gt; mu) =   (1 + shp * (x - mu) / sigma)**(-1 / shp - 1) / sigma
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>concentration = shp</code>, any real value,
</p>
</li>
<li> <p><code>scale = sigma</code>, <code>sigma &gt; 0</code>,
</p>
</li>
<li> <p><code>loc = mu</code>.
</p>
</li></ul>

<p>The cumulative density function (cdf) is,
</p>
<div class="sourceCode"><pre>cdf(x; mu, sigma, shp, x &gt; mu) = 1 - (1 + shp * (x - mu) / sigma)**(-1 / shp)
</pre></div>
<p>Distribution parameters are automatically broadcast in all functions; see
examples for details.
Samples of this distribution are reparameterized (pathwise differentiable).
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>

<hr>
<h2 id='tfd_geometric'>Geometric distribution</h2><span id='topic+tfd_geometric'></span>

<h3>Description</h3>

<p>The Geometric distribution is parameterized by p, the probability of a
positive event. It represents the probability that in k + 1 Bernoulli trials,
the first k trials failed, before seeing a success.
The pmf of this distribution is:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_geometric(
  logits = NULL,
  probs = NULL,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Geometric"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_geometric_+3A_logits">logits</code></td>
<td>
<p>Floating-point <code>Tensor</code> with shape <code style="white-space: pre;">&#8288;[B1, ..., Bb]&#8288;</code> where <code>b &gt;= 0</code>
indicates the number of batch dimensions. Each entry represents logits
for the probability of success for independent Geometric distributions
and must be in the range <code style="white-space: pre;">&#8288;(-inf, inf]&#8288;</code>. Only one of <code>logits</code> or <code>probs</code>
should be specified.</p>
</td></tr>
<tr><td><code id="tfd_geometric_+3A_probs">probs</code></td>
<td>
<p>Positive floating-point <code>Tensor</code> with shape <code style="white-space: pre;">&#8288;[B1, ..., Bb]&#8288;</code>
where <code>b &gt;= 0</code> indicates the number of batch dimensions. Each entry
represents the probability of success for independent Geometric
distributions and must be in the range <code style="white-space: pre;">&#8288;(0, 1]&#8288;</code>. Only one of <code>logits</code>
or <code>probs</code> should be specified.</p>
</td></tr>
<tr><td><code id="tfd_geometric_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_geometric_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_geometric_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<div class="sourceCode"><pre>pmf(k; p) = (1 - p)**k * p
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>p</code> is the success probability, <code style="white-space: pre;">&#8288;0 &lt; p &lt;= 1&#8288;</code>, and,
</p>
</li>
<li> <p><code>k</code> is a non-negative integer.
</p>
</li></ul>



<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_gumbel'>Scalar Gumbel distribution with location <code>loc</code> and <code>scale</code> parameters</h2><span id='topic+tfd_gumbel'></span>

<h3>Description</h3>

<p>Mathematical details
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_gumbel(
  loc,
  scale,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Gumbel"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_gumbel_+3A_loc">loc</code></td>
<td>
<p>Floating point tensor, the means of the distribution(s).</p>
</td></tr>
<tr><td><code id="tfd_gumbel_+3A_scale">scale</code></td>
<td>
<p>Floating point tensor, the scales of the distribution(s). 'scale&ldquo; must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_gumbel_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_gumbel_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_gumbel_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The probability density function (pdf) of this distribution is,
</p>
<div class="sourceCode"><pre>pdf(x; mu, sigma) = exp(-(x - mu) / sigma - exp(-(x - mu) / sigma)) / sigma
</pre></div>
<p>where <code>loc = mu</code> and <code>scale = sigma</code>.
</p>
<p>The cumulative density function of this distribution is,
<code style="white-space: pre;">&#8288;cdf(x; mu, sigma) = exp(-exp(-(x - mu) / sigma))&#8288;</code>
</p>
<p>The Gumbel distribution is a member of the <a href="https://en.wikipedia.org/wiki/Location-scale_family">location-scale family</a>, i.e., it can be
constructed as,
</p>
<div class="sourceCode"><pre>X ~ Gumbel(loc=0, scale=1)
Y = loc + scale * X
</pre></div>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_half_cauchy'>Half-Cauchy distribution</h2><span id='topic+tfd_half_cauchy'></span>

<h3>Description</h3>

<p>The half-Cauchy distribution is parameterized by a <code>loc</code> and a
<code>scale</code> parameter. It represents the right half of the two symmetric halves in
a <a href="https://en.wikipedia.org/wiki/Cauchy_distribution">Cauchy distribution</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_half_cauchy(
  loc,
  scale,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "HalfCauchy"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_half_cauchy_+3A_loc">loc</code></td>
<td>
<p>Floating-point <code>Tensor</code>; the location(s) of the distribution(s).</p>
</td></tr>
<tr><td><code id="tfd_half_cauchy_+3A_scale">scale</code></td>
<td>
<p>Floating-point <code>Tensor</code>; the scale(s) of the distribution(s).
Must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_half_cauchy_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_half_cauchy_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_half_cauchy_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The probability density function (pdf) for the half-Cauchy distribution
is given by
</p>
<div class="sourceCode"><pre>pdf(x; loc, scale) = 2 / (pi scale (1 + z**2))
z = (x - loc) / scale
</pre></div>
<p>where <code>loc</code> is a scalar in <code>R</code> and <code>scale</code> is a positive scalar in <code>R</code>.
The support of the distribution is given by the interval <code style="white-space: pre;">&#8288;[loc, infinity)&#8288;</code>.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_half_normal'>Half-Normal distribution with scale <code>scale</code></h2><span id='topic+tfd_half_normal'></span>

<h3>Description</h3>

<p>Mathematical details
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_half_normal(
  scale,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "HalfNormal"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_half_normal_+3A_scale">scale</code></td>
<td>
<p>Floating point tensor; the scales of the distribution(s).
Must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_half_normal_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_half_normal_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_half_normal_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The half normal is a transformation of a centered normal distribution.
If some random variable <code>X</code> has normal distribution,
</p>
<div class="sourceCode"><pre>X ~ Normal(0.0, scale)
Y = |X|
</pre></div>
<p>Then <code>Y</code> will have half normal distribution. The probability density
function (pdf) is:
</p>
<div class="sourceCode"><pre>pdf(x; scale, x &gt; 0) = sqrt(2) / (scale * sqrt(pi)) * exp(- 1/2 * (x / scale) ** 2))
</pre></div>
<p>Where <code>scale = sigma</code> is the standard deviation of the underlying normal
distribution.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_hidden_markov_model'>Hidden Markov model distribution</h2><span id='topic+tfd_hidden_markov_model'></span>

<h3>Description</h3>

<p>The <code>HiddenMarkovModel</code> distribution implements a (batch of) hidden
Markov models where the initial states, transition probabilities
and observed states are all given by user-provided distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_hidden_markov_model(
  initial_distribution,
  transition_distribution,
  observation_distribution,
  num_steps,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "HiddenMarkovModel"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_hidden_markov_model_+3A_initial_distribution">initial_distribution</code></td>
<td>
<p>A <code>Categorical</code>-like instance.
Determines probability of first hidden state in Markov chain.
The number of categories must match the number of categories of
<code>transition_distribution</code> as well as both the rightmost batch
dimension of <code>transition_distribution</code> and the rightmost batch
dimension of <code>observation_distribution</code>.</p>
</td></tr>
<tr><td><code id="tfd_hidden_markov_model_+3A_transition_distribution">transition_distribution</code></td>
<td>
<p>A <code>Categorical</code>-like instance.
The rightmost batch dimension indexes the probability distribution
of each hidden state conditioned on the previous hidden state.</p>
</td></tr>
<tr><td><code id="tfd_hidden_markov_model_+3A_observation_distribution">observation_distribution</code></td>
<td>
<p>A <code>tfp$distributions$Distribution</code>-like
instance.  The rightmost batch dimension indexes the distribution
of each observation conditioned on the corresponding hidden state.</p>
</td></tr>
<tr><td><code id="tfd_hidden_markov_model_+3A_num_steps">num_steps</code></td>
<td>
<p>The number of steps taken in Markov chain. An <code>integer</code>.</p>
</td></tr>
<tr><td><code id="tfd_hidden_markov_model_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_hidden_markov_model_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_hidden_markov_model_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This model assumes that the transition matrices are fixed over time.
In this model, there is a sequence of integer-valued hidden states:
<code style="white-space: pre;">&#8288;z[0], z[1], ..., z[num_steps - 1]&#8288;</code> and a sequence of observed states:
<code style="white-space: pre;">&#8288;x[0], ..., x[num_steps - 1]&#8288;</code>.
</p>
<p>The distribution of <code>z[0]</code> is given by <code>initial_distribution</code>.
The conditional probability of <code>z[i  +  1]</code> given <code>z[i]</code> is described by
the batch of distributions in <code>transition_distribution</code>.
For a batch of hidden Markov models, the coordinates before the rightmost one
of the <code>transition_distribution</code> batch correspond to indices into the hidden
Markov model batch. The rightmost coordinate of the batch is used to select
which distribution <code>z[i + 1]</code> is drawn from.  The distributions corresponding
to the probability of <code>z[i + 1]</code> conditional on <code>z[i] == k</code> is given by the
elements of the batch whose rightmost coordinate is <code>k</code>.
</p>
<p>Similarly, the conditional distribution of <code>z[i]</code> given <code>x[i]</code> is given by
the batch of <code>observation_distribution</code>.
When the rightmost coordinate of <code>observation_distribution</code> is <code>k</code> it
gives the conditional probabilities of <code>x[i]</code> given <code>z[i] == k</code>.
The probability distribution associated with the <code>HiddenMarkovModel</code>
distribution is the marginal distribution of <code style="white-space: pre;">&#8288;x[0],...,x[num_steps - 1]&#8288;</code>.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_horseshoe'>Horseshoe distribution</h2><span id='topic+tfd_horseshoe'></span>

<h3>Description</h3>

<p>The so-called 'horseshoe' distribution is a Cauchy-Normal scale mixture,
proposed as a sparsity-inducing prior for Bayesian regression. It is
symmetric around zero, has heavy (Cauchy-like) tails, so that large
coefficients face relatively little shrinkage, but an infinitely tall spike at
0, which pushes small coefficients towards zero. It is parameterized by a
positive scalar <code>scale</code> parameter: higher values yield a weaker
sparsity-inducing effect.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_horseshoe(
  scale,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Horseshoe"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_horseshoe_+3A_scale">scale</code></td>
<td>
<p>Floating point tensor; the scales of the distribution(s). Must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_horseshoe_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_horseshoe_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_horseshoe_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical details
</p>
<p>The Horseshoe distribution is centered at zero, with scale parameter $lambda$.
It is defined by:
</p>
<div class="sourceCode"><pre> horseshoe(scale = lambda) ~ Normal(0, lamda * sigma)
</pre></div>
<p>where <code>sigma ~ half_cauchy(0, 1)</code>
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://faculty.chicagobooth.edu/nicholas.polson/research/papers/Horse.pdf">Carvalho, Polson, Scott. Handling Sparsity via the Horseshoe (2008)</a>.
</p>
</li>
<li> <p><a href="https://ui.adsabs.harvard.edu/abs/2000JHyd..227..287B/abstract">Barry, Parlange, Li. Approximation for the exponential integral (2000)</a>.
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_independent'>Independent distribution from batch of distributions</h2><span id='topic+tfd_independent'></span>

<h3>Description</h3>

<p>This distribution is useful for regarding a collection of independent,
non-identical distributions as a single random variable. For example, the
<code>Independent</code> distribution composed of a collection of <code>Bernoulli</code>
distributions might define a distribution over an image (where each
<code>Bernoulli</code> is a distribution over each pixel).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_independent(
  distribution,
  reinterpreted_batch_ndims = NULL,
  validate_args = FALSE,
  name = paste0("Independent", distribution$name)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_independent_+3A_distribution">distribution</code></td>
<td>
<p>The base distribution instance to transform. Typically an  instance of Distribution</p>
</td></tr>
<tr><td><code id="tfd_independent_+3A_reinterpreted_batch_ndims">reinterpreted_batch_ndims</code></td>
<td>
<p>Scalar, integer number of rightmost batch dims  which
will be regarded as event dims. When NULL all but the first batch axis (batch axis 0)
will be transferred to event dimensions (analogous to tf$layers$flatten).</p>
</td></tr>
<tr><td><code id="tfd_independent_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_independent_+3A_name">name</code></td>
<td>
<p>The name for ops managed by the distribution.  Default value: Independent + distribution.name.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>More precisely, a collection of <code>B</code> (independent) <code>E</code>-variate random variables
(rv) <code style="white-space: pre;">&#8288;{X_1, ..., X_B}&#8288;</code>, can be regarded as a <code style="white-space: pre;">&#8288;[B, E]&#8288;</code>-variate random variable
<code style="white-space: pre;">&#8288;(X_1, ..., X_B)&#8288;</code> with probability
<code>p(x_1, ..., x_B) = p_1(x_1) * ... * p_B(x_B)</code> where <code>p_b(X_b)</code> is the
probability of the <code>b</code>-th rv. More generally <code style="white-space: pre;">&#8288;B, E&#8288;</code> can be arbitrary shapes.
Similarly, the <code>Independent</code> distribution specifies a distribution over
<code style="white-space: pre;">&#8288;[B, E]&#8288;</code>-shaped events. It operates by reinterpreting the rightmost batch dims as
part of the event dimensions. The <code>reinterpreted_batch_ndims</code> parameter
controls the number of batch dims which are absorbed as event dims;
<code>reinterpreted_batch_ndims &lt;= len(batch_shape)</code>.  For example, the <code>log_prob</code>
function entails a <code>reduce_sum</code> over the rightmost <code>reinterpreted_batch_ndims</code>
after calling the base distribution's <code>log_prob</code>.  In other words, since the
batch dimension(s) index independent distributions, the resultant multivariate
will have independent components.
</p>
<p>Mathematical Details
</p>
<p>The probability function is,
</p>
<div class="sourceCode"><pre>prob(x; reinterpreted_batch_ndims) =
 tf.reduce_prod(dist.prob(x), axis=-1-range(reinterpreted_batch_ndims))
</pre></div>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_inverse_gamma'>InverseGamma distribution</h2><span id='topic+tfd_inverse_gamma'></span>

<h3>Description</h3>

<p>The <code>InverseGamma</code> distribution is defined over positive real numbers using
parameters <code>concentration</code> (aka &quot;alpha&quot;) and <code>scale</code> (aka &quot;beta&quot;).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_inverse_gamma(
  concentration,
  scale,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "InverseGamma"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_inverse_gamma_+3A_concentration">concentration</code></td>
<td>
<p>Floating point tensor, the concentration params of the
distribution(s). Must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_inverse_gamma_+3A_scale">scale</code></td>
<td>
<p>Floating point tensor, the scale params of the distribution(s).
Must contain only positive values. This parameter was called <code>rate</code> before release 0.8.</p>
</td></tr>
<tr><td><code id="tfd_inverse_gamma_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_inverse_gamma_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_inverse_gamma_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; alpha, beta, x &gt; 0) = x**(-alpha - 1) exp(-beta / x) / Z
Z = Gamma(alpha) beta**-alpha
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>concentration = alpha</code>,
</p>
</li>
<li> <p><code>scale = beta</code>,
</p>
</li>
<li> <p><code>Z</code> is the normalizing constant, and,
</p>
</li>
<li> <p><code>Gamma</code> is the <a href="https://en.wikipedia.org/wiki/Gamma_function">gamma function</a>.
</p>
</li></ul>

<p>The cumulative density function (cdf) is,
</p>
<div class="sourceCode"><pre>cdf(x; alpha, beta, x &gt; 0) = GammaInc(alpha, beta / x) / Gamma(alpha)#' ```

where `GammaInc` is the [upper incomplete Gamma function](https://en.wikipedia.org/wiki/Incomplete_gamma_function).
The parameters can be intuited via their relationship to mean and variance
when these moments exist,
</pre></div>
<p>mean = beta / (alpha - 1) when alpha &gt; 1
variance = beta**2 / (alpha - 1)**2 / (alpha - 2)   when alpha &gt; 2
</p>
<div class="sourceCode"><pre>i.e., under the same conditions:
</pre></div>
<p>alpha = mean<strong>2 / variance + 2
beta = mean * (mean</strong>2 / variance + 1)
</p>
<div class="sourceCode"><pre>
Distribution parameters are automatically broadcast in all functions; see
examples for details.
Samples of this distribution are reparameterized (pathwise differentiable).
The derivatives are computed using the approach described in the paper
[Michael Figurnov, Shakir Mohamed, Andriy Mnih. Implicit Reparameterization Gradients, 2018](https://arxiv.org/abs/1805.08498)

[gamma function]: R:gamma%20function
[upper incomplete Gamma function]: R:upper%20incomplete%20Gamma%20function
[Michael Figurnov, Shakir Mohamed, Andriy Mnih. Implicit Reparameterization Gradients, 2018]: R:Michael%20Figurnov,%20Shakir%20Mohamed,%20Andriy%20Mnih.%20Implicit%20Reparameterization%20Gradients,%202018
</pre></div>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_inverse_gaussian'>Inverse Gaussian distribution</h2><span id='topic+tfd_inverse_gaussian'></span>

<h3>Description</h3>

<p>The <a href="https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution">inverse Gaussian distribution</a>
is parameterized by a <code>loc</code> and a <code>concentration</code> parameter. It's also known
as the Wald distribution. Some, e.g., the Python scipy package, refer to the
special case when <code>loc</code> is 1 as the Wald distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_inverse_gaussian(
  loc,
  concentration,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "InverseGaussian"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_inverse_gaussian_+3A_loc">loc</code></td>
<td>
<p>Floating-point <code>Tensor</code>, the loc params. Must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_inverse_gaussian_+3A_concentration">concentration</code></td>
<td>
<p>Floating-point <code>Tensor</code>, the concentration params. Must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_inverse_gaussian_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_inverse_gaussian_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_inverse_gaussian_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The &quot;inverse&quot; in the name does not refer to the distribution associated to
the multiplicative inverse of a random variable. Rather, the cumulant
generating function of this distribution is the inverse to that of a Gaussian
random variable.
</p>
<p>Mathematical Details
</p>
<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; mu, lambda) = [lambda / (2 pi x ** 3)] ** 0.5
exp{-lambda(x - mu) ** 2 / (2 mu ** 2 x)}
</pre></div>
<p>where
</p>

<ul>
<li> <p><code>loc = mu</code>
</p>
</li>
<li> <p><code>concentration = lambda</code>.
</p>
</li></ul>

<p>The support of the distribution is defined on <code style="white-space: pre;">&#8288;(0, infinity)&#8288;</code>.
Mapping to R and Python scipy's parameterization:
</p>

<ul>
<li><p> R: statmod::invgauss
</p>
</li></ul>


<ul>
<li><p> mean = loc
</p>
</li>
<li><p> shape = concentration
</p>
</li>
<li><p> dispersion = 1 / concentration. Used only if shape is NULL.
</p>
</li></ul>


<ul>
<li><p> Python: scipy.stats.invgauss
</p>
</li></ul>


<ul>
<li><p> mu = loc / concentration
</p>
</li>
<li><p> scale = concentration
</p>
</li></ul>



<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_johnson_s_u'>Johnson's SU-distribution.</h2><span id='topic+tfd_johnson_s_u'></span>

<h3>Description</h3>

<p>This distribution has parameters: shape parameters <code>skewness</code> and
<code>tailweight</code>, location <code>loc</code>, and <code>scale</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_johnson_s_u(
  skewness,
  tailweight,
  loc,
  scale,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_johnson_s_u_+3A_skewness">skewness</code></td>
<td>
<p>Floating-point <code>Tensor</code>. Skewness of the distribution(s).</p>
</td></tr>
<tr><td><code id="tfd_johnson_s_u_+3A_tailweight">tailweight</code></td>
<td>
<p>Floating-point <code>Tensor</code>. Tail weight of the
distribution(s). <code>tailweight</code> must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_johnson_s_u_+3A_loc">loc</code></td>
<td>
<p>Floating-point <code>Tensor</code>. The mean(s) of the distribution(s).</p>
</td></tr>
<tr><td><code id="tfd_johnson_s_u_+3A_scale">scale</code></td>
<td>
<p>Floating-point <code>Tensor</code>. The scaling factor(s) for the
distribution(s). Note that <code>scale</code> is not technically the standard
deviation of this distribution but has semantics more similar to
standard deviation than variance.</p>
</td></tr>
<tr><td><code id="tfd_johnson_s_u_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_johnson_s_u_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_johnson_s_u_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical details
</p>
<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; s, t, xi, sigma) = exp(-0.5 (s + t arcsinh(y))**2) / Z
  where,
    s = skewness
    t = tailweight
    y = (x - xi) / sigma
    Z = sigma sqrt(2 pi) sqrt(1 + y**2) / t
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>loc = xi</code>,
</p>
</li>
<li> <p><code>scale = sigma</code>, and,
</p>
</li>
<li> <p><code>Z</code> is the normalization constant.
The JohnsonSU distribution is a member of the
<a href="https://en.wikipedia.org/wiki/Location-scale_family">location-scale family</a>, i.e., it can be
constructed as,
</p>
</li></ul>

<div class="sourceCode"><pre>X ~ JohnsonSU(skewness, tailweight, loc=0, scale=1)
Y = loc + scale * X
</pre></div>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_joint_distribution_named'>Joint distribution parameterized by named distribution-making functions.</h2><span id='topic+tfd_joint_distribution_named'></span>

<h3>Description</h3>

<p>This distribution enables both sampling and joint probability computation from
a single model specification.
A joint distribution is a collection of possibly interdependent distributions.
Like <code>JointDistributionSequential</code>, <code>JointDistributionNamed</code> is parameterized
by several distribution-making functions. Unlike <code>JointDistributionNamed</code>,
each distribution-making function must have its own key. Additionally every
distribution-making function's arguments must refer to only specified keys.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_joint_distribution_named(model, validate_args = FALSE, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_joint_distribution_named_+3A_model">model</code></td>
<td>
<p>named list of distribution-making functions each
with required args corresponding only to other keys in the named list.</p>
</td></tr>
<tr><td><code id="tfd_joint_distribution_named_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_joint_distribution_named_+3A_name">name</code></td>
<td>
<p>The name for ops managed by the distribution. Default value: <code>"JointDistributionNamed"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>Internally <code>JointDistributionNamed</code> implements the chain rule of probability.
That is, the probability function of a length-<code>d</code> vector <code>x</code> is,
</p>
<div class="sourceCode"><pre>p(x) = prod{ p(x[i] | x[:i]) : i = 0, ..., (d - 1) }
</pre></div>
<p>The <code>JointDistributionNamed</code> is parameterized by a <code>dict</code> (or <code>namedtuple</code>)
composed of either:
</p>

<ol>
<li> <p><code>tfp$distributions$Distribution</code>-like instances or,
</p>
</li>
<li><p> functions which return a <code>tfp$distributions$Distribution</code>-like instance.
The &quot;conditioned on&quot; elements are represented by the function's required
arguments; every argument must correspond to a key in the named
distribution-making functions. Distribution-makers which are directly a
<code>Distribution</code>-like instance are allowed for convenience and semantically
identical a zero argument function. When the maker takes no arguments it is
preferable to directly provide the distribution instance.
</p>
</li></ol>



<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_joint_distribution_named_auto_batched'>Joint distribution parameterized by named distribution-making functions.</h2><span id='topic+tfd_joint_distribution_named_auto_batched'></span>

<h3>Description</h3>

<p>This class provides automatic vectorization and alternative semantics for
<code>tfd_joint_distribution_named()</code>, which in many cases allows for
simplifications in the model specification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_joint_distribution_named_auto_batched(
  model,
  batch_ndims = 0,
  use_vectorized_map = TRUE,
  validate_args = FALSE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_joint_distribution_named_auto_batched_+3A_model">model</code></td>
<td>
<p>A generator that yields a sequence of <code>tfd$Distribution</code>-like
instances.</p>
</td></tr>
<tr><td><code id="tfd_joint_distribution_named_auto_batched_+3A_batch_ndims">batch_ndims</code></td>
<td>
<p><code>integer</code> <code>Tensor</code> number of batch dimensions. The <code>batch_shape</code>s
of all component distributions must be such that the prefixes of
length <code>batch_ndims</code> broadcast to a consistent joint batch shape.
Default value: <code>0</code>.</p>
</td></tr>
<tr><td><code id="tfd_joint_distribution_named_auto_batched_+3A_use_vectorized_map">use_vectorized_map</code></td>
<td>
<p><code>logical</code>. Whether to use <code>tf$vectorized_map</code>
to automatically vectorize evaluation of the model. This allows the
model specification to focus on drawing a single sample, which is often
simpler, but some ops may not be supported. Default value: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="tfd_joint_distribution_named_auto_batched_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_joint_distribution_named_auto_batched_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Automatic vectorization
</p>
<p>Auto-vectorized variants of JointDistribution allow the user to avoid
explicitly annotating a model's vectorization semantics.
When using manually-vectorized joint distributions, each operation in the
model must account for the possibility of batch dimensions in Distributions
and their samples. By contrast, auto-vectorized models need only describe
a <em>single</em> sample from the joint distribution; any batch evaluation is
automated using <code>tf$vectorized_map</code> as required. In many cases this
allows for significant simplications. For example, the following
manually-vectorized <code>tfd_joint_distribution_named()</code> model:
</p>
<div class="sourceCode"><pre>model &lt;- tfd_joint_distribution_sequential(
    list(
      x = tfd_normal(loc = 0, scale = tf$ones(3L)),
      y = tfd_normal(loc = 0, scale = 1),
      z = function(y, x) {
        tfd_normal(loc = x[reticulate::py_ellipsis(), 1:2] + y[reticulate::py_ellipsis(), tf$newaxis], scale = 1)
      }
    )
)
</pre></div>
<p>can be written in auto-vectorized form as
</p>
<div class="sourceCode"><pre>model &lt;- tfd_joint_distribution_sequential_auto_batched(
  list(
    x = tfd_normal(loc = 0, scale = tf$ones(3L)),
    y = tfd_normal(loc = 0, scale = 1),
    z = function(y, x) {tfd_normal(loc = x[1:2] + y, scale = 1)}
  )
)
</pre></div>
<p>in which we were able to avoid explicitly accounting for batch dimensions
when indexing and slicing computed quantities in the third line.
Note: auto-vectorization is still experimental and some TensorFlow ops may
be unsupported. It can be disabled by setting <code>use_vectorized_map=FALSE</code>.
</p>
<p>Alternative batch semantics
This class also provides alternative semantics for specifying a batch of
independent (non-identical) joint distributions.
Instead of simply summing the <code>log_prob</code>s of component distributions
(which may have different shapes), it first reduces the component <code>log_prob</code>s
to ensure that <code>jd$log_prob(jd$sample())</code> always returns a scalar, unless
<code>batch_ndims</code> is explicitly set to a nonzero value (in which case the result
will have the corresponding tensor rank).
</p>
<p>The essential changes are:
</p>

<ul>
<li><p> An <code>event</code> of <code>JointDistributionNamedAutoBatched</code> is the list of
tensors produced by <code style="white-space: pre;">&#8288;$sample()&#8288;</code>; thus, the <code>event_shape</code> is the
list containing the shapes of sampled tensors. These combine both
the event and batch dimensions of the component distributions. By contrast,
the event shape of a base <code>JointDistribution</code>s does not include batch
dimensions of component distributions.
</p>
</li>
<li><p> The <code>batch_shape</code> is a global property of the entire model, rather
than a per-component property as in base <code>JointDistribution</code>s.
The global batch shape must be a prefix of the batch shapes of
each component; the length of this prefix is specified by an optional
argument <code>batch_ndims</code>. If <code>batch_ndims</code> is not specified, the model has
batch shape <code style="white-space: pre;">&#8288;()&#8288;</code>.#'
</p>
</li></ul>



<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_joint_distribution_sequential'>Joint distribution parameterized by distribution-making functions</h2><span id='topic+tfd_joint_distribution_sequential'></span>

<h3>Description</h3>

<p>This distribution enables both sampling and joint probability computation from
a single model specification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_joint_distribution_sequential(model, validate_args = FALSE, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_joint_distribution_sequential_+3A_model">model</code></td>
<td>
<p>list of either <code>tfp$distributions$Distribution</code> instances and/or
functions which take the <code>k</code> previous distributions and returns a
new <code>tfp$distributions$Distribution</code> instance.</p>
</td></tr>
<tr><td><code id="tfd_joint_distribution_sequential_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_joint_distribution_sequential_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A joint distribution is a collection of possibly interdependent distributions.
Like <code>tf$keras$Sequential</code>, the <code>JointDistributionSequential</code> can be specified
via a <code>list</code> of functions (each responsible for making a
<code>tfp$distributions$Distribution</code>-like instance).  Unlike
<code>tf$keras$Sequential</code>, each function can depend on the output of all previous
elements rather than only the immediately previous.
</p>
<p>Mathematical Details
</p>
<p>The <code>JointDistributionSequential</code> implements the chain rule of probability.
</p>
<p>That is, the probability function of a length-<code>d</code> vector <code>x</code> is,
</p>
<div class="sourceCode"><pre>p(x) = prod{ p(x[i] | x[:i]) : i = 0, ..., (d - 1) }
</pre></div>
<p>The <code>JointDistributionSequential</code> is parameterized by a <code>list</code> comprised of
either:
</p>

<ol>
<li> <p><code>tfp$distributions$Distribution</code>-like instances or,
</p>
</li>
<li> <p><code>callable</code>s which return a <code>tfp$distributions$Distribution</code>-like instance.
Each <code>list</code> element implements the <code>i</code>-th <em>full conditional distribution</em>,
<code style="white-space: pre;">&#8288;p(x[i] | x[:i])&#8288;</code>. The &quot;conditioned on&quot; elements are represented by the
<code>callable</code>'s required arguments. Directly providing a <code>Distribution</code>-like
nstance is a convenience and is semantically identical a zero argument
<code>callable</code>.
Denote the <code>i</code>-th <code>callable</code>s non-default arguments as <code>args[i]</code>. Since the
<code>callable</code> is the conditional manifest, <code style="white-space: pre;">&#8288;0 &lt;= len(args[i]) &lt;= i - 1&#8288;</code>. When
<code>len(args[i]) &lt; i - 1</code>, the <code>callable</code> only depends on a subset of the
previous distributions, specifically those at indexes:
<code>range(i - 1, i - 1 - num_args[i], -1)</code>.
</p>
</li></ol>

<p><strong>Name resolution</strong>: <code style="white-space: pre;">&#8288;The names of &#8288;</code>JointDistributionSequential<code style="white-space: pre;">&#8288;components are defined by explicit&#8288;</code>name<code style="white-space: pre;">&#8288; arguments passed to distributions (&#8288;</code>tfd.Normal(0., 1., name='x')<code style="white-space: pre;">&#8288;) and/or by the argument names in distribution-making functions (&#8288;</code>lambda x: tfd.Normal(x., 1.)<code style="white-space: pre;">&#8288;). Both approaches may be used in the same distribution, as long as they are consistent; referring to a single component by multiple names will raise a &#8288;</code>ValueError'. Unnamed components will be assigned a dummy name.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_joint_distribution_sequential_auto_batched'>Joint distribution parameterized by distribution-making functions.</h2><span id='topic+tfd_joint_distribution_sequential_auto_batched'></span>

<h3>Description</h3>

<p>This class provides automatic vectorization and alternative semantics for
<code>tfd_joint_distribution_sequential()</code>, which in many cases allows for
simplifications in the model specification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_joint_distribution_sequential_auto_batched(
  model,
  batch_ndims = 0,
  use_vectorized_map = TRUE,
  validate_args = FALSE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_joint_distribution_sequential_auto_batched_+3A_model">model</code></td>
<td>
<p>A generator that yields a sequence of <code>tfd$Distribution</code>-like
instances.</p>
</td></tr>
<tr><td><code id="tfd_joint_distribution_sequential_auto_batched_+3A_batch_ndims">batch_ndims</code></td>
<td>
<p><code>integer</code> <code>Tensor</code> number of batch dimensions. The <code>batch_shape</code>s
of all component distributions must be such that the prefixes of
length <code>batch_ndims</code> broadcast to a consistent joint batch shape.
Default value: <code>0</code>.</p>
</td></tr>
<tr><td><code id="tfd_joint_distribution_sequential_auto_batched_+3A_use_vectorized_map">use_vectorized_map</code></td>
<td>
<p><code>logical</code>. Whether to use <code>tf$vectorized_map</code>
to automatically vectorize evaluation of the model. This allows the
model specification to focus on drawing a single sample, which is often
simpler, but some ops may not be supported. Default value: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="tfd_joint_distribution_sequential_auto_batched_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_joint_distribution_sequential_auto_batched_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Automatic vectorization
</p>
<p>Auto-vectorized variants of JointDistribution allow the user to avoid
explicitly annotating a model's vectorization semantics.
When using manually-vectorized joint distributions, each operation in the
model must account for the possibility of batch dimensions in Distributions
and their samples. By contrast, auto-vectorized models need only describe
a <em>single</em> sample from the joint distribution; any batch evaluation is
automated using <code>tf$vectorized_map</code> as required. In many cases this
allows for significant simplications. For example, the following
manually-vectorized <code>tfd_joint_distribution_sequential()</code> model:
</p>
<div class="sourceCode"><pre>model &lt;- tfd_joint_distribution_sequential(
    list(
      tfd_normal(loc = 0, scale = tf$ones(3L)),
      tfd_normal(loc = 0, scale = 1),
      function(y, x) {
        tfd_normal(loc = x[reticulate::py_ellipsis(), 1:2] + y[reticulate::py_ellipsis(), tf$newaxis], scale = 1)
      }
    )
)
</pre></div>
<p>can be written in auto-vectorized form as
</p>
<div class="sourceCode"><pre>model &lt;- tfd_joint_distribution_sequential_auto_batched(
  list(
    tfd_normal(loc = 0, scale = tf$ones(3L)),
    tfd_normal(loc = 0, scale = 1),
    function(y, x) {tfd_normal(loc = x[1:2] + y, scale = 1)}
  )
)
</pre></div>
<p>in which we were able to avoid explicitly accounting for batch dimensions
when indexing and slicing computed quantities in the third line.
Note: auto-vectorization is still experimental and some TensorFlow ops may
be unsupported. It can be disabled by setting <code>use_vectorized_map=FALSE</code>.
</p>
<p>Alternative batch semantics
This class also provides alternative semantics for specifying a batch of
independent (non-identical) joint distributions.
Instead of simply summing the <code>log_prob</code>s of component distributions
(which may have different shapes), it first reduces the component <code>log_prob</code>s
to ensure that <code>jd$log_prob(jd$sample())</code> always returns a scalar, unless
<code>batch_ndims</code> is explicitly set to a nonzero value (in which case the result
will have the corresponding tensor rank).
</p>
<p>The essential changes are:
</p>

<ul>
<li><p> An <code>event</code> of <code>JointDistributionSequentialAutoBatched</code> is the list of
tensors produced by <code style="white-space: pre;">&#8288;$sample()&#8288;</code>; thus, the <code>event_shape</code> is the
list containing the shapes of sampled tensors. These combine both
the event and batch dimensions of the component distributions. By contrast,
the event shape of a base <code>JointDistribution</code>s does not include batch
dimensions of component distributions.
</p>
</li>
<li><p> The <code>batch_shape</code> is a global property of the entire model, rather
than a per-component property as in base <code>JointDistribution</code>s.
The global batch shape must be a prefix of the batch shapes of
each component; the length of this prefix is specified by an optional
argument <code>batch_ndims</code>. If <code>batch_ndims</code> is not specified, the model has
batch shape <code style="white-space: pre;">&#8288;()&#8288;</code>.#'
</p>
</li></ul>



<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_kl_divergence'>Computes the Kullback&ndash;Leibler divergence.</h2><span id='topic+tfd_kl_divergence'></span>

<h3>Description</h3>

<p>Denote this distribution by p and the other distribution by q.
Assuming p, q are absolutely continuous with respect to reference measure r,
the KL divergence is defined as:
<code style="white-space: pre;">&#8288;KL[p, q] = E_p[log(p(X)/q(X))] = -int_F p(x) log q(x) dr(x) + int_F p(x) log p(x) dr(x) = H[p, q] - H[p]&#8288;</code>
where F denotes the support of the random variable <code>X ~ p</code>, <code>H[., .]</code>
denotes (Shannon) cross entropy, and <code>H[.]</code> denotes (Shannon) entropy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_kl_divergence(distribution, other, name = "kl_divergence")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_kl_divergence_+3A_distribution">distribution</code></td>
<td>
<p>The distribution being used.</p>
</td></tr>
<tr><td><code id="tfd_kl_divergence_+3A_other">other</code></td>
<td>
<p><code>tfp$distributions$Distribution</code> instance.</p>
</td></tr>
<tr><td><code id="tfd_kl_divergence_+3A_name">name</code></td>
<td>
<p>String prepended to names of ops created by this function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>self$dtype Tensor with shape <code style="white-space: pre;">&#8288;[B1, ..., Bn]&#8288;</code> representing n different calculations
of the Kullback-Leibler divergence.
</p>


<h3>See Also</h3>

<p>Other distribution_methods: 
<code><a href="#topic+tfd_cdf">tfd_cdf</a>()</code>,
<code><a href="#topic+tfd_covariance">tfd_covariance</a>()</code>,
<code><a href="#topic+tfd_cross_entropy">tfd_cross_entropy</a>()</code>,
<code><a href="#topic+tfd_entropy">tfd_entropy</a>()</code>,
<code><a href="#topic+tfd_log_cdf">tfd_log_cdf</a>()</code>,
<code><a href="#topic+tfd_log_prob">tfd_log_prob</a>()</code>,
<code><a href="#topic+tfd_log_survival_function">tfd_log_survival_function</a>()</code>,
<code><a href="#topic+tfd_mean">tfd_mean</a>()</code>,
<code><a href="#topic+tfd_mode">tfd_mode</a>()</code>,
<code><a href="#topic+tfd_prob">tfd_prob</a>()</code>,
<code><a href="#topic+tfd_quantile">tfd_quantile</a>()</code>,
<code><a href="#topic+tfd_sample">tfd_sample</a>()</code>,
<code><a href="#topic+tfd_stddev">tfd_stddev</a>()</code>,
<code><a href="#topic+tfd_survival_function">tfd_survival_function</a>()</code>,
<code><a href="#topic+tfd_variance">tfd_variance</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  d1 &lt;- tfd_normal(loc = c(1, 2), scale = c(1, 0.5))
  d2 &lt;- tfd_normal(loc = c(1.5, 2), scale = c(1, 0.5))
  d1 %&gt;% tfd_kl_divergence(d2)

</code></pre>

<hr>
<h2 id='tfd_kumaraswamy'>Kumaraswamy distribution</h2><span id='topic+tfd_kumaraswamy'></span>

<h3>Description</h3>

<p>The Kumaraswamy distribution is defined over the <code style="white-space: pre;">&#8288;(0, 1)&#8288;</code> interval using
parameters <code>concentration1</code> (aka &quot;alpha&quot;) and <code>concentration0</code> (aka &quot;beta&quot;).  It has a
shape similar to the Beta distribution, but is easier to reparameterize.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_kumaraswamy(
  concentration1 = 1,
  concentration0 = 1,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Kumaraswamy"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_kumaraswamy_+3A_concentration1">concentration1</code></td>
<td>
<p>Positive floating-point <code>Tensor</code> indicating mean
number of successes; aka &quot;alpha&quot;. Implies <code>self$dtype</code> and
<code>self$batch_shape</code>, i.e.,
<code style="white-space: pre;">&#8288;concentration1$shape = [N1, N2, ..., Nm] = self$batch_shape&#8288;</code>.</p>
</td></tr>
<tr><td><code id="tfd_kumaraswamy_+3A_concentration0">concentration0</code></td>
<td>
<p>Positive floating-point <code>Tensor</code> indicating mean
number of failures; aka &quot;beta&quot;. Otherwise has same semantics as
<code>concentration1</code>.</p>
</td></tr>
<tr><td><code id="tfd_kumaraswamy_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_kumaraswamy_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_kumaraswamy_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; alpha, beta) = alpha * beta * x**(alpha - 1) * (1 - x**alpha)**(beta - 1)
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>concentration1 = alpha</code>,
</p>
</li>
<li> <p><code>concentration0 = beta</code>,
Distribution parameters are automatically broadcast in all functions.
</p>
</li></ul>



<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_laplace'>Laplace distribution with location <code>loc</code> and <code>scale</code> parameters</h2><span id='topic+tfd_laplace'></span>

<h3>Description</h3>

<p>Mathematical details
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_laplace(
  loc,
  scale,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Laplace"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_laplace_+3A_loc">loc</code></td>
<td>
<p>Floating point tensor which characterizes the location (center)
of the distribution.</p>
</td></tr>
<tr><td><code id="tfd_laplace_+3A_scale">scale</code></td>
<td>
<p>Positive floating point tensor which characterizes the spread of
the distribution.</p>
</td></tr>
<tr><td><code id="tfd_laplace_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_laplace_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_laplace_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The probability density function (pdf) of this distribution is,
</p>
<div class="sourceCode"><pre>pdf(x; mu, sigma) = exp(-|x - mu| / sigma) / Z
Z = 2 sigma
</pre></div>
<p>where <code>loc = mu</code>, <code>scale = sigma</code>, and <code>Z</code> is the normalization constant.
</p>
<p>Note that the Laplace distribution can be thought of two exponential
distributions spliced together &quot;back-to-back.&quot;
The Laplace distribution is a member of the <a href="https://en.wikipedia.org/wiki/Location-scale_family">location-scale family</a>, i.e., it can be
constructed as,
</p>
<div class="sourceCode"><pre>X ~ Laplace(loc=0, scale=1)
Y = loc + scale * X
</pre></div>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_linear_gaussian_state_space_model'>Observation distribution from a linear Gaussian state space model</h2><span id='topic+tfd_linear_gaussian_state_space_model'></span>

<h3>Description</h3>

<p>The state space model, sometimes called a Kalman filter, posits a
latent state vector <code>z_t</code> of dimension <code>latent_size</code> that evolves
over time following linear Gaussian transitions,
<code style="white-space: pre;">&#8288;z_{t+1} = F * z_t + N(b; Q)&#8288;</code>
for transition matrix <code>F</code>, bias <code>b</code> and covariance matrix
<code>Q</code>. At each timestep, we observe a noisy projection of the
latent state <code style="white-space: pre;">&#8288;x_t = H * z_t + N(c; R)&#8288;</code>. The transition and
observation models may be fixed or may vary between timesteps.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_linear_gaussian_state_space_model(
  num_timesteps,
  transition_matrix,
  transition_noise,
  observation_matrix,
  observation_noise,
  initial_state_prior,
  initial_step = 0L,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "LinearGaussianStateSpaceModel"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_linear_gaussian_state_space_model_+3A_num_timesteps">num_timesteps</code></td>
<td>
<p>Integer <code>Tensor</code> total number of timesteps.</p>
</td></tr>
<tr><td><code id="tfd_linear_gaussian_state_space_model_+3A_transition_matrix">transition_matrix</code></td>
<td>
<p>A transition operator, represented by a Tensor or
LinearOperator of shape <code style="white-space: pre;">&#8288;[latent_size, latent_size]&#8288;</code>, or by a
callable taking as argument a scalar integer Tensor <code>t</code> and
returning a Tensor or LinearOperator representing the transition
operator from latent state at time <code>t</code> to time <code>t + 1</code>.</p>
</td></tr>
<tr><td><code id="tfd_linear_gaussian_state_space_model_+3A_transition_noise">transition_noise</code></td>
<td>
<p>An instance of
<code>tfd$MultivariateNormalLinearOperator</code> with event shape
<code style="white-space: pre;">&#8288;[latent_size]&#8288;</code>, representing the mean and covariance of the
transition noise model, or a callable taking as argument a
scalar integer Tensor <code>t</code> and returning such a distribution
representing the noise in the transition from time <code>t</code> to time <code>t + 1</code>.</p>
</td></tr>
<tr><td><code id="tfd_linear_gaussian_state_space_model_+3A_observation_matrix">observation_matrix</code></td>
<td>
<p>An observation operator, represented by a Tensor
or LinearOperator of shape <code style="white-space: pre;">&#8288;[observation_size, latent_size]&#8288;</code>,
or by a callable taking as argument a scalar integer Tensor
<code>t</code> and returning a timestep-specific Tensor or LinearOperator.</p>
</td></tr>
<tr><td><code id="tfd_linear_gaussian_state_space_model_+3A_observation_noise">observation_noise</code></td>
<td>
<p>An instance of <code>tfd.MultivariateNormalLinearOperator</code>
with event shape <code style="white-space: pre;">&#8288;[observation_size]&#8288;</code>, representing the mean and covariance of
the observation noise model, or a callable taking as argument
a scalar integer Tensor <code>t</code> and returning a timestep-specific
noise model.</p>
</td></tr>
<tr><td><code id="tfd_linear_gaussian_state_space_model_+3A_initial_state_prior">initial_state_prior</code></td>
<td>
<p>An instance of <code>MultivariateNormalLinearOperator</code>
representing the prior distribution on latent states; must
have event shape <code style="white-space: pre;">&#8288;[latent_size]&#8288;</code>.</p>
</td></tr>
<tr><td><code id="tfd_linear_gaussian_state_space_model_+3A_initial_step">initial_step</code></td>
<td>
<p>optional <code>integer</code> specifying the time of the first
modeled timestep.  This is added as an offset when passing
timesteps <code>t</code> to (optional) callables specifying
timestep-specific transition and observation models.</p>
</td></tr>
<tr><td><code id="tfd_linear_gaussian_state_space_model_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_linear_gaussian_state_space_model_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_linear_gaussian_state_space_model_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This Distribution represents the marginal distribution on
observations, <code>p(x)</code>. The marginal <code>log_prob</code> is computed by
Kalman filtering, and <code>sample</code> by an efficient forward
recursion. Both operations require time linear in <code>T</code>, the total
number of timesteps.
</p>
<p>Shapes
</p>
<p>The event shape is <code style="white-space: pre;">&#8288;[num_timesteps, observation_size]&#8288;</code>, where
<code>observation_size</code> is the dimension of each observation <code>x_t</code>.
The observation and transition models must return consistent
shapes.
This implementation supports vectorized computation over a batch of
models. All of the parameters (prior distribution, transition and
observation operators and noise models) must have a consistent
batch shape.
</p>
<p>Time-varying processes
</p>
<p>Any of the model-defining parameters (prior distribution, transition
and observation operators and noise models) may be specified as a
callable taking an integer timestep <code>t</code> and returning a
time-dependent value. The dimensionality (<code>latent_size</code> and
<code>observation_size</code>) must be the same at all timesteps.
</p>
<p>Importantly, the timestep is passed as a <code>Tensor</code>, not a Python
integer, so any conditional behavior must occur <em>inside</em> the
TensorFlow graph. For example, suppose we want to use a different
transition model on even days than odd days. It does <em>not</em> work to
write
</p>
<div class="sourceCode"><pre>transition_matrix &lt;- function(t) {
if(t %% 2 == 0) even_day_matrix else odd_day_matrix
}
</pre></div>
<p>since the value of <code>t</code> is not fixed at graph-construction
time. Instead we need to write
</p>
<div class="sourceCode"><pre>transition_matrix &lt;- function(t) {
tf$cond(tf$equal(tf$mod(t, 2), 0), function() even_day_matrix, function() odd_day_matrix)
}
</pre></div>
<p>so that TensorFlow can switch between operators appropriately at runtime.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_lkj'>LKJ distribution on correlation matrices</h2><span id='topic+tfd_lkj'></span>

<h3>Description</h3>

<p>This is a one-parameter  of distributions on correlation matrices.  The
probability density is proportional to the determinant raised to the power of
the parameter: <code style="white-space: pre;">&#8288;pdf(X; eta) = Z(eta) * det(X) ** (eta - 1)&#8288;</code>, where <code>Z(eta)</code> is
a normalization constant.  The uniform distribution on correlation matrices is
the special case <code>eta = 1</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_lkj(
  dimension,
  concentration,
  input_output_cholesky = FALSE,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "LKJ"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_lkj_+3A_dimension">dimension</code></td>
<td>
<p><code>integer</code>. The dimension of the correlation matrices
to sample.</p>
</td></tr>
<tr><td><code id="tfd_lkj_+3A_concentration">concentration</code></td>
<td>
<p><code>float</code> or <code>double</code> <code>Tensor</code>. The positive concentration
parameter of the LKJ distributions. The pdf of a sample matrix <code>X</code> is
proportional to <code>det(X) ** (concentration - 1)</code>.</p>
</td></tr>
<tr><td><code id="tfd_lkj_+3A_input_output_cholesky">input_output_cholesky</code></td>
<td>
<p><code>Logical</code>. If <code>TRUE</code>, functions whose input or
output have the semantics of samples assume inputs are in Cholesky form
and return outputs in Cholesky form. In particular, if this flag is
<code>TRUE</code>, input to <code>log_prob</code> is presumed of Cholesky form and output from
<code>sample</code> is of Cholesky form.  Setting this argument to <code>TRUE</code> is purely
a computational optimization and does not change the underlying
distribution. Additionally, validation checks which are only defined on
the multiplied-out form are omitted, even if <code>validate_args</code> is <code>TRUE</code>.
Default value: <code>FALSE</code> (i.e., input/output does not have Cholesky semantics).</p>
</td></tr>
<tr><td><code id="tfd_lkj_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_lkj_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_lkj_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The distribution is named after Lewandowski, Kurowicka, and Joe, who gave a
sampler for the distribution in Lewandowski, Kurowicka, Joe, 2009.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_log_cdf'>Log cumulative distribution function.</h2><span id='topic+tfd_log_cdf'></span>

<h3>Description</h3>

<p>Given random variable X, the cumulative distribution function cdf is:
<code>tfd_log_cdf(x) := Log[ P[X &lt;= x] ]</code>
Often, a numerical approximation can be used for <code>tfd_log_cdf(x)</code> that yields
a more accurate answer than simply taking the logarithm of the cdf when x &lt;&lt; -1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_log_cdf(distribution, value, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_log_cdf_+3A_distribution">distribution</code></td>
<td>
<p>The distribution being used.</p>
</td></tr>
<tr><td><code id="tfd_log_cdf_+3A_value">value</code></td>
<td>
<p>float or double Tensor.</p>
</td></tr>
<tr><td><code id="tfd_log_cdf_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to Python.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Tensor of shape <code>sample_shape(x) + self$batch_shape</code> with values of type <code>self$dtype</code>.
</p>


<h3>See Also</h3>

<p>Other distribution_methods: 
<code><a href="#topic+tfd_cdf">tfd_cdf</a>()</code>,
<code><a href="#topic+tfd_covariance">tfd_covariance</a>()</code>,
<code><a href="#topic+tfd_cross_entropy">tfd_cross_entropy</a>()</code>,
<code><a href="#topic+tfd_entropy">tfd_entropy</a>()</code>,
<code><a href="#topic+tfd_kl_divergence">tfd_kl_divergence</a>()</code>,
<code><a href="#topic+tfd_log_prob">tfd_log_prob</a>()</code>,
<code><a href="#topic+tfd_log_survival_function">tfd_log_survival_function</a>()</code>,
<code><a href="#topic+tfd_mean">tfd_mean</a>()</code>,
<code><a href="#topic+tfd_mode">tfd_mode</a>()</code>,
<code><a href="#topic+tfd_prob">tfd_prob</a>()</code>,
<code><a href="#topic+tfd_quantile">tfd_quantile</a>()</code>,
<code><a href="#topic+tfd_sample">tfd_sample</a>()</code>,
<code><a href="#topic+tfd_stddev">tfd_stddev</a>()</code>,
<code><a href="#topic+tfd_survival_function">tfd_survival_function</a>()</code>,
<code><a href="#topic+tfd_variance">tfd_variance</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  d &lt;- tfd_normal(loc = c(1, 2), scale = c(1, 0.5))
  x &lt;- d %&gt;% tfd_sample()
  d %&gt;% tfd_log_cdf(x)

</code></pre>

<hr>
<h2 id='tfd_log_logistic'>The log-logistic distribution.</h2><span id='topic+tfd_log_logistic'></span>

<h3>Description</h3>

<p>The LogLogistic distribution models positive-valued random variables
whose logarithm is a logistic distribution with loc <code>loc</code> and
scale <code>scale</code>. It is constructed as the exponential
transformation of a Logistic distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_log_logistic(
  loc,
  scale,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "LogLogistic"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_log_logistic_+3A_loc">loc</code></td>
<td>
<p>Floating-point <code>Tensor</code>; the loc of the underlying logistic
distribution(s).</p>
</td></tr>
<tr><td><code id="tfd_log_logistic_+3A_scale">scale</code></td>
<td>
<p>Floating-point <code>Tensor</code>; the scale of the underlying logistic
distribution(s).</p>
</td></tr>
<tr><td><code id="tfd_log_logistic_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_log_logistic_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_log_logistic_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_log_normal'>Log-normal distribution</h2><span id='topic+tfd_log_normal'></span>

<h3>Description</h3>

<p>The LogNormal distribution models positive-valued random variables
whose logarithm is normally distributed with mean <code>loc</code> and
standard deviation <code>scale</code>. It is constructed as the exponential
transformation of a Normal distribution.
</p>
<p>The LogNormal distribution models positive-valued random variables
whose logarithm is normally distributed with mean <code>loc</code> and
standard deviation <code>scale</code>. It is constructed as the exponential
transformation of a Normal distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_log_normal(
  loc,
  scale,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "LogNormal"
)

tfd_log_normal(
  loc,
  scale,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "LogNormal"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_log_normal_+3A_loc">loc</code></td>
<td>
<p>Floating-point <code>Tensor</code>; the means of the underlying
Normal distribution(s).</p>
</td></tr>
<tr><td><code id="tfd_log_normal_+3A_scale">scale</code></td>
<td>
<p>Floating-point <code>Tensor</code>; the stddevs of the underlying
Normal distribution(s).</p>
</td></tr>
<tr><td><code id="tfd_log_normal_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_log_normal_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_log_normal_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_log_prob'>Log probability density/mass function.</h2><span id='topic+tfd_log_prob'></span>

<h3>Description</h3>

<p>Log probability density/mass function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_log_prob(distribution, value, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_log_prob_+3A_distribution">distribution</code></td>
<td>
<p>The distribution being used.</p>
</td></tr>
<tr><td><code id="tfd_log_prob_+3A_value">value</code></td>
<td>
<p>float or double Tensor.</p>
</td></tr>
<tr><td><code id="tfd_log_prob_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to Python.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Tensor of shape <code>sample_shape(x) + self$batch_shape</code> with values of type <code>self$dtype</code>.
</p>


<h3>See Also</h3>

<p>Other distribution_methods: 
<code><a href="#topic+tfd_cdf">tfd_cdf</a>()</code>,
<code><a href="#topic+tfd_covariance">tfd_covariance</a>()</code>,
<code><a href="#topic+tfd_cross_entropy">tfd_cross_entropy</a>()</code>,
<code><a href="#topic+tfd_entropy">tfd_entropy</a>()</code>,
<code><a href="#topic+tfd_kl_divergence">tfd_kl_divergence</a>()</code>,
<code><a href="#topic+tfd_log_cdf">tfd_log_cdf</a>()</code>,
<code><a href="#topic+tfd_log_survival_function">tfd_log_survival_function</a>()</code>,
<code><a href="#topic+tfd_mean">tfd_mean</a>()</code>,
<code><a href="#topic+tfd_mode">tfd_mode</a>()</code>,
<code><a href="#topic+tfd_prob">tfd_prob</a>()</code>,
<code><a href="#topic+tfd_quantile">tfd_quantile</a>()</code>,
<code><a href="#topic+tfd_sample">tfd_sample</a>()</code>,
<code><a href="#topic+tfd_stddev">tfd_stddev</a>()</code>,
<code><a href="#topic+tfd_survival_function">tfd_survival_function</a>()</code>,
<code><a href="#topic+tfd_variance">tfd_variance</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  d &lt;- tfd_normal(loc = c(1, 2), scale = c(1, 0.5))
  x &lt;- d %&gt;% tfd_sample()
  d %&gt;% tfd_log_prob(x)

</code></pre>

<hr>
<h2 id='tfd_log_survival_function'>Log survival function.</h2><span id='topic+tfd_log_survival_function'></span>

<h3>Description</h3>

<p>Given random variable X, the survival function is defined:
<code>tfd_log_survival_function(x) = Log[ P[X &gt; x] ] = Log[ 1 - P[X &lt;= x] ] = Log[ 1 - cdf(x) ]</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_log_survival_function(distribution, value, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_log_survival_function_+3A_distribution">distribution</code></td>
<td>
<p>The distribution being used.</p>
</td></tr>
<tr><td><code id="tfd_log_survival_function_+3A_value">value</code></td>
<td>
<p>float or double Tensor.</p>
</td></tr>
<tr><td><code id="tfd_log_survival_function_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to Python.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Typically, different numerical approximations can be used for the log survival function,
which are more accurate than 1 - cdf(x) when x &gt;&gt; 1.
</p>


<h3>Value</h3>

<p>a Tensor of shape <code>sample_shape(x) + self$batch_shape</code> with values of type <code>self$dtype</code>.
</p>


<h3>See Also</h3>

<p>Other distribution_methods: 
<code><a href="#topic+tfd_cdf">tfd_cdf</a>()</code>,
<code><a href="#topic+tfd_covariance">tfd_covariance</a>()</code>,
<code><a href="#topic+tfd_cross_entropy">tfd_cross_entropy</a>()</code>,
<code><a href="#topic+tfd_entropy">tfd_entropy</a>()</code>,
<code><a href="#topic+tfd_kl_divergence">tfd_kl_divergence</a>()</code>,
<code><a href="#topic+tfd_log_cdf">tfd_log_cdf</a>()</code>,
<code><a href="#topic+tfd_log_prob">tfd_log_prob</a>()</code>,
<code><a href="#topic+tfd_mean">tfd_mean</a>()</code>,
<code><a href="#topic+tfd_mode">tfd_mode</a>()</code>,
<code><a href="#topic+tfd_prob">tfd_prob</a>()</code>,
<code><a href="#topic+tfd_quantile">tfd_quantile</a>()</code>,
<code><a href="#topic+tfd_sample">tfd_sample</a>()</code>,
<code><a href="#topic+tfd_stddev">tfd_stddev</a>()</code>,
<code><a href="#topic+tfd_survival_function">tfd_survival_function</a>()</code>,
<code><a href="#topic+tfd_variance">tfd_variance</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  d &lt;- tfd_normal(loc = c(1, 2), scale = c(1, 0.5))
  x &lt;- d %&gt;% tfd_sample()
  d %&gt;% tfd_log_survival_function(x)

</code></pre>

<hr>
<h2 id='tfd_logistic'>Logistic distribution with location <code>loc</code> and <code>scale</code> parameters</h2><span id='topic+tfd_logistic'></span>

<h3>Description</h3>

<p>Mathematical details
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_logistic(
  loc,
  scale,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Logistic"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_logistic_+3A_loc">loc</code></td>
<td>
<p>Floating point tensor, the means of the distribution(s).</p>
</td></tr>
<tr><td><code id="tfd_logistic_+3A_scale">scale</code></td>
<td>
<p>Floating point tensor, the scales of the distribution(s). Must
contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_logistic_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_logistic_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_logistic_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The cumulative density function of this distribution is:
</p>
<div class="sourceCode"><pre>cdf(x; mu, sigma) = 1 / (1 + exp(-(x - mu) / sigma))
</pre></div>
<p>where <code>loc = mu</code> and <code>scale = sigma</code>.
</p>
<p>The Logistic distribution is a member of the <a href="https://en.wikipedia.org/wiki/Location-scale_family">location-scale family</a>, i.e., it can be
constructed as,
</p>
<div class="sourceCode"><pre>X ~ Logistic(loc=0, scale=1)
Y = loc + scale * X
</pre></div>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_logit_normal'>The Logit-Normal distribution</h2><span id='topic+tfd_logit_normal'></span>

<h3>Description</h3>

<p>The Logit-Normal distribution models positive-valued random variables whose
logit (i.e., sigmoid_inverse, i.e., <code>log(p) - log1p(-p)</code>) is normally
distributed with mean <code>loc</code> and standard deviation <code>scale</code>. It is
constructed as the sigmoid transformation, (i.e., <code>1 / (1 + exp(-x))</code>) of a
Normal distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_logit_normal(
  loc,
  scale,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "LogitNormal"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_logit_normal_+3A_loc">loc</code></td>
<td>
<p>Floating point tensor; the means of the distribution(s).</p>
</td></tr>
<tr><td><code id="tfd_logit_normal_+3A_scale">scale</code></td>
<td>
<p>loating point tensor; the stddevs of the distribution(s).
Must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_logit_normal_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_logit_normal_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_logit_normal_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>

<hr>
<h2 id='tfd_mean'>Mean.</h2><span id='topic+tfd_mean'></span>

<h3>Description</h3>

<p>Mean.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_mean(distribution, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_mean_+3A_distribution">distribution</code></td>
<td>
<p>The distribution being used.</p>
</td></tr>
<tr><td><code id="tfd_mean_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to Python.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Tensor of shape <code>sample_shape(x) + self$batch_shape</code> with values of type <code>self$dtype</code>.
</p>


<h3>See Also</h3>

<p>Other distribution_methods: 
<code><a href="#topic+tfd_cdf">tfd_cdf</a>()</code>,
<code><a href="#topic+tfd_covariance">tfd_covariance</a>()</code>,
<code><a href="#topic+tfd_cross_entropy">tfd_cross_entropy</a>()</code>,
<code><a href="#topic+tfd_entropy">tfd_entropy</a>()</code>,
<code><a href="#topic+tfd_kl_divergence">tfd_kl_divergence</a>()</code>,
<code><a href="#topic+tfd_log_cdf">tfd_log_cdf</a>()</code>,
<code><a href="#topic+tfd_log_prob">tfd_log_prob</a>()</code>,
<code><a href="#topic+tfd_log_survival_function">tfd_log_survival_function</a>()</code>,
<code><a href="#topic+tfd_mode">tfd_mode</a>()</code>,
<code><a href="#topic+tfd_prob">tfd_prob</a>()</code>,
<code><a href="#topic+tfd_quantile">tfd_quantile</a>()</code>,
<code><a href="#topic+tfd_sample">tfd_sample</a>()</code>,
<code><a href="#topic+tfd_stddev">tfd_stddev</a>()</code>,
<code><a href="#topic+tfd_survival_function">tfd_survival_function</a>()</code>,
<code><a href="#topic+tfd_variance">tfd_variance</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  d &lt;- tfd_normal(loc = c(1, 2), scale = c(1, 0.5))
  d %&gt;% tfd_mean()

</code></pre>

<hr>
<h2 id='tfd_mixture'>Mixture distribution</h2><span id='topic+tfd_mixture'></span>

<h3>Description</h3>

<p>The <code>Mixture</code> object implements batched mixture distributions.
The mixture model is defined by a <code>Categorical</code> distribution (the mixture)
and a list of <code>Distribution</code> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_mixture(
  cat,
  components,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Mixture"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_mixture_+3A_cat">cat</code></td>
<td>
<p>A <code>Categorical</code> distribution instance, representing the probabilities
of <code>distributions</code>.</p>
</td></tr>
<tr><td><code id="tfd_mixture_+3A_components">components</code></td>
<td>
<p>A list or tuple of <code>Distribution</code> instances.
Each instance must have the same type, be defined on the same domain,
and have matching <code>event_shape</code> and <code>batch_shape</code>.</p>
</td></tr>
<tr><td><code id="tfd_mixture_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_mixture_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_mixture_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Methods supported include <code>tfd_log_prob</code>, <code>tfd_prob</code>, <code>tfd_mean</code>, <code>tfd_sample</code>,
and <code>entropy_lower_bound</code>.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_mixture_same_family'>Mixture (same-family) distribution</h2><span id='topic+tfd_mixture_same_family'></span>

<h3>Description</h3>

<p>The <code>MixtureSameFamily</code> distribution implements a (batch of) mixture
distribution where all components are from different parameterizations of the
same distribution type. It is parameterized by a <code>Categorical</code> &quot;selecting
distribution&quot; (over <code>k</code> components) and a components distribution, i.e., a
<code>Distribution</code> with a rightmost batch shape (equal to <code style="white-space: pre;">&#8288;[k]&#8288;</code>) which indexes
each (batch of) component.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_mixture_same_family(
  mixture_distribution,
  components_distribution,
  reparameterize = FALSE,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "MixtureSameFamily"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_mixture_same_family_+3A_mixture_distribution">mixture_distribution</code></td>
<td>
<p><code>tfp$distributions$Categorical</code>-like instance.
Manages the probability of selecting components. The number of
categories must match the rightmost batch dimension of the
<code>components_distribution</code>. Must have either scalar <code>batch_shape</code> or
<code>batch_shape</code> matching <code style="white-space: pre;">&#8288;components_distribution$batch_shape[:-1]&#8288;</code>.</p>
</td></tr>
<tr><td><code id="tfd_mixture_same_family_+3A_components_distribution">components_distribution</code></td>
<td>
<p><code>tfp$distributions$Distribution</code>-like instance.
Right-most batch dimension indexes components.</p>
</td></tr>
<tr><td><code id="tfd_mixture_same_family_+3A_reparameterize">reparameterize</code></td>
<td>
<p>Logical, default <code>FALSE</code>. Whether to reparameterize
samples of the distribution using implicit reparameterization gradients
(Figurnov et al., 2018). The gradients for the mixture logits are
equivalent to the ones described by (Graves, 2016). The gradients
for the components parameters are also computed using implicit
reparameterization (as opposed to ancestral sampling), meaning that
all components are updated every step.
Only works when:
(1) components_distribution is fully reparameterized;
(2) components_distribution is either a scalar distribution or
fully factorized (tfd.Independent applied to a scalar distribution);
(3) batch shape has a known rank.
Experimental, may be slow and produce infs/NaNs.</p>
</td></tr>
<tr><td><code id="tfd_mixture_same_family_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_mixture_same_family_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_mixture_same_family_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1805.08498">Michael Figurnov, Shakir Mohamed and Andriy Mnih. Implicit reparameterization gradients. In <em>Neural Information Processing Systems</em>, 2018. </a>
</p>
</li>
<li> <p><a href="https://arxiv.org/abs/1607.05690">Alex Graves. Stochastic Backpropagation through Mixture Density Distributions. <em>arXiv</em>, 2016.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_mode'>Mode.</h2><span id='topic+tfd_mode'></span>

<h3>Description</h3>

<p>Mode.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_mode(distribution, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_mode_+3A_distribution">distribution</code></td>
<td>
<p>The distribution being used.</p>
</td></tr>
<tr><td><code id="tfd_mode_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to Python.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Tensor of shape <code>sample_shape(x) + self$batch_shape</code> with values of type <code>self$dtype</code>.
</p>


<h3>See Also</h3>

<p>Other distribution_methods: 
<code><a href="#topic+tfd_cdf">tfd_cdf</a>()</code>,
<code><a href="#topic+tfd_covariance">tfd_covariance</a>()</code>,
<code><a href="#topic+tfd_cross_entropy">tfd_cross_entropy</a>()</code>,
<code><a href="#topic+tfd_entropy">tfd_entropy</a>()</code>,
<code><a href="#topic+tfd_kl_divergence">tfd_kl_divergence</a>()</code>,
<code><a href="#topic+tfd_log_cdf">tfd_log_cdf</a>()</code>,
<code><a href="#topic+tfd_log_prob">tfd_log_prob</a>()</code>,
<code><a href="#topic+tfd_log_survival_function">tfd_log_survival_function</a>()</code>,
<code><a href="#topic+tfd_mean">tfd_mean</a>()</code>,
<code><a href="#topic+tfd_prob">tfd_prob</a>()</code>,
<code><a href="#topic+tfd_quantile">tfd_quantile</a>()</code>,
<code><a href="#topic+tfd_sample">tfd_sample</a>()</code>,
<code><a href="#topic+tfd_stddev">tfd_stddev</a>()</code>,
<code><a href="#topic+tfd_survival_function">tfd_survival_function</a>()</code>,
<code><a href="#topic+tfd_variance">tfd_variance</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  d &lt;- tfd_normal(loc = c(1, 2), scale = c(1, 0.5))
  d %&gt;% tfd_mode()

</code></pre>

<hr>
<h2 id='tfd_multinomial'>Multinomial distribution</h2><span id='topic+tfd_multinomial'></span>

<h3>Description</h3>

<p>This Multinomial distribution is parameterized by <code>probs</code>, a (batch of)
length-<code>K</code> <code>prob</code> (probability) vectors (<code>K &gt; 1</code>) such that
<code>tf.reduce_sum(probs, -1) = 1</code>, and a <code>total_count</code> number of trials, i.e.,
the number of trials per draw from the Multinomial. It is defined over a
(batch of) length-<code>K</code> vector <code>counts</code> such that
<code>tf$reduce_sum(counts, -1) = total_count</code>. The Multinomial is identically the
Binomial distribution when <code>K = 2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_multinomial(
  total_count,
  logits = NULL,
  probs = NULL,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Multinomial"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_multinomial_+3A_total_count">total_count</code></td>
<td>
<p>Non-negative floating point tensor with shape broadcastable
to <code style="white-space: pre;">&#8288;[N1,..., Nm]&#8288;</code> with <code>m &gt;= 0</code>. Defines this as a batch of
<code style="white-space: pre;">&#8288;N1 x ... x Nm&#8288;</code> different Multinomial distributions. Its components
should be equal to integer values.</p>
</td></tr>
<tr><td><code id="tfd_multinomial_+3A_logits">logits</code></td>
<td>
<p>Floating point tensor representing unnormalized log-probabilities
of a positive event with shape broadcastable to
<code style="white-space: pre;">&#8288;[N1,..., Nm, K]&#8288;</code> <code>m &gt;= 0</code>, and the same dtype as <code>total_count</code>. Defines
this as a batch of <code style="white-space: pre;">&#8288;N1 x ... x Nm&#8288;</code> different <code>K</code> class Multinomial
distributions. Only one of <code>logits</code> or <code>probs</code> should be passed in.</p>
</td></tr>
<tr><td><code id="tfd_multinomial_+3A_probs">probs</code></td>
<td>
<p>Positive floating point tensor with shape broadcastable to
<code style="white-space: pre;">&#8288;[N1,..., Nm, K]&#8288;</code> <code>m &gt;= 0</code> and same dtype as <code>total_count</code>. Defines
this as a batch of <code style="white-space: pre;">&#8288;N1 x ... x Nm&#8288;</code> different <code>K</code> class Multinomial
distributions. <code>probs</code>'s components in the last portion of its shape
should sum to <code>1</code>. Only one of <code>logits</code> or <code>probs</code> should be passed in.</p>
</td></tr>
<tr><td><code id="tfd_multinomial_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_multinomial_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_multinomial_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The Multinomial is a distribution over <code>K</code>-class counts, i.e., a length-<code>K</code>
vector of non-negative integer <code style="white-space: pre;">&#8288;counts = n = [n_0, ..., n_{K-1}]&#8288;</code>.
The probability mass function (pmf) is,
</p>
<div class="sourceCode"><pre>pmf(n; pi, N) = prod_j (pi_j)**n_j / Z
Z = (prod_j n_j!) / N!
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;probs = pi = [pi_0, ..., pi_{K-1}]&#8288;</code>, <code>pi_j &gt; 0</code>, <code style="white-space: pre;">&#8288;sum_j pi_j = 1&#8288;</code>,
</p>
</li>
<li> <p><code>total_count = N</code>, <code>N</code> a positive integer,
</p>
</li>
<li> <p><code>Z</code> is the normalization constant, and,
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;N!&#8288;</code> denotes <code>N</code> factorial.
</p>
</li></ul>

<p>Distribution parameters are automatically broadcast in all functions; see
examples for details.
</p>
<p>Pitfalls
</p>
<p>The number of classes, <code>K</code>, must not exceed:
</p>

<ul>
<li><p> the largest integer representable by <code>self$dtype</code>, i.e.,
<code>2**(mantissa_bits+1)</code> (IEE754),
</p>
</li>
<li><p> the maximum <code>Tensor</code> index, i.e., <code>2**31-1</code>.
</p>
</li></ul>

<p>Note: This condition is validated only when <code>validate_args = TRUE</code>.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_multivariate_normal_diag'>Multivariate normal distribution on <code>R^k</code></h2><span id='topic+tfd_multivariate_normal_diag'></span>

<h3>Description</h3>

<p>The Multivariate Normal distribution is defined over <code style="white-space: pre;">&#8288;R^k`` and parameterized by a (batch of) length-k loc vector (aka "mu") and a (batch of) &#8288;</code>k x k<code style="white-space: pre;">&#8288;scale matrix;&#8288;</code>covariance = scale @ scale.T<code>where</code>@' denotes
matrix-multiplication.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_multivariate_normal_diag(
  loc = NULL,
  scale_diag = NULL,
  scale_identity_multiplier = NULL,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "MultivariateNormalDiag"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_multivariate_normal_diag_+3A_loc">loc</code></td>
<td>
<p>Floating-point Tensor. If this is set to NULL, loc is implicitly 0.
When specified, may have shape <code style="white-space: pre;">&#8288;[B1, ..., Bb, k]&#8288;</code> where b &gt;= 0 and k is the event size.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_normal_diag_+3A_scale_diag">scale_diag</code></td>
<td>
<p>Non-zero, floating-point Tensor representing a diagonal matrix added to scale.
May have shape <code style="white-space: pre;">&#8288;[B1, ..., Bb, k]&#8288;</code>, b &gt;= 0, and characterizes b-batches of <code style="white-space: pre;">&#8288;k x k&#8288;</code> diagonal matrices
added to scale. When both scale_identity_multiplier and scale_diag are NULL then scale
is the Identity.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_normal_diag_+3A_scale_identity_multiplier">scale_identity_multiplier</code></td>
<td>
<p>Non-zero, floating-point Tensor representing a scaled-identity-matrix
added to scale. May have shape <code style="white-space: pre;">&#8288;[B1, ..., Bb]&#8288;</code>, b &gt;= 0, and characterizes b-batches of scaled
<code style="white-space: pre;">&#8288;k x k&#8288;</code> identity matrices added to scale. When both scale_identity_multiplier and scale_diag
are NULL then scale is the Identity.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_normal_diag_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_normal_diag_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_normal_diag_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; loc, scale) = exp(-0.5 ||y||**2) / Z
y = inv(scale) @ (x - loc)
Z = (2 pi)**(0.5 k) |det(scale)|
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>loc</code> is a vector in <code>R^k</code>,
</p>
</li>
<li> <p><code>scale</code> is a linear operator in <code style="white-space: pre;">&#8288;R^{k x k}&#8288;</code>, <code>cov = scale @ scale.T</code>,
</p>
</li>
<li> <p><code>Z</code> denotes the normalization constant, and,
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;||y||**2&#8288;</code> denotes the squared Euclidean norm of <code>y</code>.
</p>
</li></ul>

<p>A (non-batch) <code>scale</code> matrix is:
</p>
<div class="sourceCode"><pre>scale = diag(scale_diag + scale_identity_multiplier * ones(k))
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;scale_diag.shape = [k]&#8288;</code>, and,
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;scale_identity_multiplier.shape = []&#8288;</code>.#'
</p>
</li></ul>

<p>Additional leading dimensions (if any) will index batches.
</p>
<p>If both <code>scale_diag</code> and <code>scale_identity_multiplier</code> are <code>NULL</code>, then
<code>scale</code> is the Identity matrix.
The MultivariateNormal distribution is a member of the
<a href="https://en.wikipedia.org/wiki/Location-scale_family">location-scale family</a>, i.e., it can be
constructed as,
</p>
<div class="sourceCode"><pre>X ~ MultivariateNormal(loc=0, scale=1)   # Identity scale, zero shift.
Y = scale @ X + loc
</pre></div>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_multivariate_normal_diag_plus_low_rank'>Multivariate normal distribution on <code>R^k</code></h2><span id='topic+tfd_multivariate_normal_diag_plus_low_rank'></span>

<h3>Description</h3>

<p>The Multivariate Normal distribution is defined over <code style="white-space: pre;">&#8288;R^k`` and parameterized by a (batch of) length-k loc vector (aka "mu") and a (batch of) &#8288;</code>k x k<code style="white-space: pre;">&#8288;scale matrix;&#8288;</code>covariance = scale @ scale.T<code>where</code>@' denotes
matrix-multiplication.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_multivariate_normal_diag_plus_low_rank(
  loc = NULL,
  scale_diag = NULL,
  scale_identity_multiplier = NULL,
  scale_perturb_factor = NULL,
  scale_perturb_diag = NULL,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "MultivariateNormalDiagPlusLowRank"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_multivariate_normal_diag_plus_low_rank_+3A_loc">loc</code></td>
<td>
<p>Floating-point Tensor. If this is set to NULL, loc is implicitly 0.
When specified, may have shape <code style="white-space: pre;">&#8288;[B1, ..., Bb, k]&#8288;</code> where b &gt;= 0 and k is the event size.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_normal_diag_plus_low_rank_+3A_scale_diag">scale_diag</code></td>
<td>
<p>Non-zero, floating-point Tensor representing a diagonal matrix added to scale.
May have shape <code style="white-space: pre;">&#8288;[B1, ..., Bb, k]&#8288;</code>, b &gt;= 0, and characterizes b-batches of <code style="white-space: pre;">&#8288;k x k&#8288;</code> diagonal matrices
added to scale. When both scale_identity_multiplier and scale_diag are NULL then scale
is the Identity.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_normal_diag_plus_low_rank_+3A_scale_identity_multiplier">scale_identity_multiplier</code></td>
<td>
<p>Non-zero, floating-point Tensor representing a scaled-identity-matrix
added to scale. May have shape <code style="white-space: pre;">&#8288;[B1, ..., Bb]&#8288;</code>, b &gt;= 0, and characterizes b-batches of scaled
<code style="white-space: pre;">&#8288;k x k&#8288;</code> identity matrices added to scale. When both scale_identity_multiplier and scale_diag
are NULL then scale is the Identity.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_normal_diag_plus_low_rank_+3A_scale_perturb_factor">scale_perturb_factor</code></td>
<td>
<p>Floating-point <code>Tensor</code> representing a rank-<code>r</code>
perturbation added to <code>scale</code>. May have shape <code style="white-space: pre;">&#8288;[B1, ..., Bb, k, r]&#8288;</code>,
<code>b &gt;= 0</code>, and characterizes <code>b</code>-batches of rank-<code>r</code> updates to <code>scale</code>.
When <code>NULL</code>, no rank-<code>r</code> update is added to <code>scale</code>.#'</p>
</td></tr>
<tr><td><code id="tfd_multivariate_normal_diag_plus_low_rank_+3A_scale_perturb_diag">scale_perturb_diag</code></td>
<td>
<p>Floating-point <code>Tensor</code> representing a diagonal matrix
inside the rank-<code>r</code> perturbation added to <code>scale</code>. May have shape
<code style="white-space: pre;">&#8288;[B1, ..., Bb, r]&#8288;</code>, <code>b &gt;= 0</code>, and characterizes <code>b</code>-batches of <code>r</code> x <code>r</code>
diagonal matrices inside the perturbation added to <code>scale</code>. When
<code>NULL</code>, an identity matrix is used inside the perturbation. Can only be
specified if <code>scale_perturb_factor</code> is also specified.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_normal_diag_plus_low_rank_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_normal_diag_plus_low_rank_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_normal_diag_plus_low_rank_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; loc, scale) = exp(-0.5 ||y||**2) / Z
y = inv(scale) @ (x - loc)
Z = (2 pi)**(0.5 k) |det(scale)|
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>loc</code> is a vector in <code>R^k</code>,
</p>
</li>
<li> <p><code>scale</code> is a linear operator in <code style="white-space: pre;">&#8288;R^{k x k}&#8288;</code>, <code>cov = scale @ scale.T</code>,
</p>
</li>
<li> <p><code>Z</code> denotes the normalization constant, and,
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;||y||**2&#8288;</code> denotes the squared Euclidean norm of <code>y</code>.
</p>
</li></ul>

<p>A (non-batch) <code>scale</code> matrix is:
</p>
<div class="sourceCode"><pre>scale = diag(scale_diag + scale_identity_multiplier ones(k)) +
scale_perturb_factor @ diag(scale_perturb_diag) @ scale_perturb_factor.T
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;scale_diag.shape = [k]&#8288;</code>,
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;scale_identity_multiplier.shape = []&#8288;</code>,
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;scale_perturb_factor.shape = [k, r]&#8288;</code>, typically <code style="white-space: pre;">&#8288;k &gt;&gt; r&#8288;</code>, and,
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;scale_perturb_diag.shape = [r]&#8288;</code>.
</p>
</li></ul>

<p>Additional leading dimensions (if any) will index batches.
If both <code>scale_diag</code> and <code>scale_identity_multiplier</code> are <code>NULL</code>, then
<code>scale</code> is the Identity matrix.
The MultivariateNormal distribution is a member of the
<a href="https://en.wikipedia.org/wiki/Location-scale_family">location-scale family</a>, i.e., it can be
constructed as,
</p>
<div class="sourceCode"><pre>X ~ MultivariateNormal(loc=0, scale=1)   # Identity scale, zero shift.
Y = scale @ X + loc
</pre></div>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_multivariate_normal_full_covariance'>Multivariate normal distribution on <code>R^k</code></h2><span id='topic+tfd_multivariate_normal_full_covariance'></span>

<h3>Description</h3>

<p>The Multivariate Normal distribution is defined over <code style="white-space: pre;">&#8288;R^k`` and parameterized by a (batch of) length-k loc vector (aka "mu") and a (batch of) &#8288;</code>k x k<code style="white-space: pre;">&#8288;scale matrix;&#8288;</code>covariance = scale @ scale.T<code>where</code>@' denotes
matrix-multiplication.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_multivariate_normal_full_covariance(
  loc = NULL,
  covariance_matrix = NULL,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "MultivariateNormalFullCovariance"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_multivariate_normal_full_covariance_+3A_loc">loc</code></td>
<td>
<p>Floating-point <code>Tensor</code>. If this is set to <code>NULL</code>, <code>loc</code> is
implicitly <code>0</code>. When specified, may have shape <code style="white-space: pre;">&#8288;[B1, ..., Bb, k]&#8288;</code> where
<code>b &gt;= 0</code> and <code>k</code> is the event size.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_normal_full_covariance_+3A_covariance_matrix">covariance_matrix</code></td>
<td>
<p>Floating-point, symmetric positive definite <code>Tensor</code> of
same <code>dtype</code> as <code>loc</code>.  The strict upper triangle of <code>covariance_matrix</code>
is ignored, so if <code>covariance_matrix</code> is not symmetric no error will be
raised (unless <code style="white-space: pre;">&#8288;validate_args is TRUE&#8288;</code>).  <code>covariance_matrix</code> has shape
<code style="white-space: pre;">&#8288;[B1, ..., Bb, k, k]&#8288;</code> where <code>b &gt;= 0</code> and <code>k</code> is the event size.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_normal_full_covariance_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_normal_full_covariance_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_normal_full_covariance_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; loc, scale) = exp(-0.5 ||y||**2) / Z
y = inv(scale) @ (x - loc)
Z = (2 pi)**(0.5 k) |det(scale)|
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>loc</code> is a vector in <code>R^k</code>,
</p>
</li>
<li> <p><code>scale</code> is a linear operator in <code style="white-space: pre;">&#8288;R^{k x k}&#8288;</code>, <code>cov = scale @ scale.T</code>,
</p>
</li>
<li> <p><code>Z</code> denotes the normalization constant, and,
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;||y||**2&#8288;</code> denotes the squared Euclidean norm of <code>y</code>.
</p>
</li></ul>

<p>The MultivariateNormal distribution is a member of the
<a href="https://en.wikipedia.org/wiki/Location-scale_family">location-scale family</a>, i.e., it can be
constructed as,
</p>
<div class="sourceCode"><pre>X ~ MultivariateNormal(loc=0, scale=1)   # Identity scale, zero shift.
Y = scale @ X + loc
</pre></div>
<p>The <code>batch_shape</code> is the broadcast shape between <code>loc</code> and
<code>covariance_matrix</code> arguments.
The <code>event_shape</code> is given by last dimension of the matrix implied by
<code>covariance_matrix</code>. The last dimension of <code>loc</code> (if provided) must
broadcast with this.
A non-batch <code>covariance_matrix</code> matrix is a <code style="white-space: pre;">&#8288;k x k&#8288;</code> symmetric positive
definite matrix.  In other words it is (real) symmetric with all eigenvalues
strictly positive.
Additional leading dimensions (if any) will index batches.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_multivariate_normal_linear_operator'>The multivariate normal distribution on <code>R^k</code></h2><span id='topic+tfd_multivariate_normal_linear_operator'></span>

<h3>Description</h3>

<p>The Multivariate Normal distribution is defined over <code style="white-space: pre;">&#8288;R^k`` and parameterized by a (batch of) length-k loc vector (aka "mu") and a (batch of) &#8288;</code>k x k<code style="white-space: pre;">&#8288;scale matrix;&#8288;</code>covariance = scale @ scale.T<code>where</code>@' denotes
matrix-multiplication.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_multivariate_normal_linear_operator(
  loc = NULL,
  scale = NULL,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "MultivariateNormalLinearOperator"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_multivariate_normal_linear_operator_+3A_loc">loc</code></td>
<td>
<p>Floating-point <code>Tensor</code>. If this is set to <code>NULL</code>, <code>loc</code> is
implicitly <code>0</code>. When specified, may have shape <code style="white-space: pre;">&#8288;[B1, ..., Bb, k]&#8288;</code> where
<code>b &gt;= 0</code> and <code>k</code> is the event size.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_normal_linear_operator_+3A_scale">scale</code></td>
<td>
<p>Instance of <code>LinearOperator</code> with same <code>dtype</code> as <code>loc</code> and shape
<code style="white-space: pre;">&#8288;[B1, ..., Bb, k, k]&#8288;</code>.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_normal_linear_operator_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_normal_linear_operator_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_normal_linear_operator_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; loc, scale) = exp(-0.5 ||y||**2) / Z
y = inv(scale) @ (x - loc)
Z = (2 pi)**(0.5 k) |det(scale)|
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>loc</code> is a vector in <code>R^k</code>,
</p>
</li>
<li> <p><code>scale</code> is a linear operator in <code style="white-space: pre;">&#8288;R^{k x k}&#8288;</code>, <code>cov = scale @ scale.T</code>,
</p>
</li>
<li> <p><code>Z</code> denotes the normalization constant, and,
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;||y||**2&#8288;</code> denotes the squared Euclidean norm of <code>y</code>.
</p>
</li></ul>

<p>The MultivariateNormal distribution is a member of the
<a href="https://en.wikipedia.org/wiki/Location-scale_family">location-scale family</a>, i.e., it can be
constructed as,
</p>
<div class="sourceCode"><pre>X ~ MultivariateNormal(loc=0, scale=1)   # Identity scale, zero shift.
Y = scale @ X + loc
</pre></div>
<p>The <code>batch_shape</code> is the broadcast shape between <code>loc</code> and <code>scale</code>
arguments.
The <code>event_shape</code> is given by last dimension of the matrix implied by
<code>scale</code>. The last dimension of <code>loc</code> (if provided) must broadcast with this.
Recall that <code>covariance = scale @ scale.T</code>.
Additional leading dimensions (if any) will index batches.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_multivariate_normal_tri_l'>The multivariate normal distribution on <code>R^k</code></h2><span id='topic+tfd_multivariate_normal_tri_l'></span>

<h3>Description</h3>

<p>The Multivariate Normal distribution is defined over <code style="white-space: pre;">&#8288;R^k`` and parameterized by a (batch of) length-k loc vector (aka "mu") and a (batch of) &#8288;</code>k x k<code style="white-space: pre;">&#8288;scale matrix;&#8288;</code>covariance = scale @ scale.T<code>where</code>@' denotes
matrix-multiplication.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_multivariate_normal_tri_l(
  loc = NULL,
  scale_tril = NULL,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "MultivariateNormalTriL"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_multivariate_normal_tri_l_+3A_loc">loc</code></td>
<td>
<p>Floating-point <code>Tensor</code>. If this is set to <code>NULL</code>, <code>loc</code> is
implicitly <code>0</code>. When specified, may have shape <code style="white-space: pre;">&#8288;[B1, ..., Bb, k]&#8288;</code> where
<code>b &gt;= 0</code> and <code>k</code> is the event size.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_normal_tri_l_+3A_scale_tril">scale_tril</code></td>
<td>
<p>Floating-point, lower-triangular <code>Tensor</code> with non-zero
diagonal elements. <code>scale_tril</code> has shape <code style="white-space: pre;">&#8288;[B1, ..., Bb, k, k]&#8288;</code> where
<code>b &gt;= 0</code> and <code>k</code> is the event size.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_normal_tri_l_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_normal_tri_l_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_normal_tri_l_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; loc, scale) = exp(-0.5 ||y||**2) / Z
y = inv(scale) @ (x - loc)
Z = (2 pi)**(0.5 k) |det(scale)|
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>loc</code> is a vector in <code>R^k</code>,
</p>
</li>
<li> <p><code>scale</code> is a linear operator in <code style="white-space: pre;">&#8288;R^{k x k}&#8288;</code>, <code>cov = scale @ scale.T</code>,
</p>
</li>
<li> <p><code>Z</code> denotes the normalization constant, and,
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;||y||**2&#8288;</code> denotes the squared Euclidean norm of <code>y</code>.
</p>
</li></ul>

<p>A (non-batch) <code>scale</code> matrix is:
</p>
<div class="sourceCode"><pre>scale = scale_tril
</pre></div>
<p>where <code>scale_tril</code> is lower-triangular <code style="white-space: pre;">&#8288;k x k&#8288;</code> matrix with non-zero diagonal,
i.e., <code>tf$diag_part(scale_tril) != 0</code>.
Additional leading dimensions (if any) will index batches.
</p>
<p>The MultivariateNormal distribution is a member of the
<a href="https://en.wikipedia.org/wiki/Location-scale_family">location-scale family</a>, i.e., it can be
constructed as,
</p>
<div class="sourceCode"><pre>X ~ MultivariateNormal(loc=0, scale=1)   # Identity scale, zero shift.
Y = scale @ X + loc
</pre></div>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_multivariate_student_t_linear_operator'>Multivariate Student's t-distribution on <code>R^k</code></h2><span id='topic+tfd_multivariate_student_t_linear_operator'></span>

<h3>Description</h3>

<p>Mathematical Details
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_multivariate_student_t_linear_operator(
  df,
  loc,
  scale,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "MultivariateStudentTLinearOperator"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_multivariate_student_t_linear_operator_+3A_df">df</code></td>
<td>
<p>A positive floating-point <code>Tensor</code>. Has shape <code style="white-space: pre;">&#8288;[B1, ..., Bb]&#8288;</code> where <code>b &gt;= 0</code>.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_student_t_linear_operator_+3A_loc">loc</code></td>
<td>
<p>Floating-point <code>Tensor</code>. Has shape <code style="white-space: pre;">&#8288;[B1, ..., Bb, k]&#8288;</code> where <code>k</code> is
the event size.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_student_t_linear_operator_+3A_scale">scale</code></td>
<td>
<p>Instance of <code>LinearOperator</code> with a floating <code>dtype</code> and shape
<code style="white-space: pre;">&#8288;[B1, ..., Bb, k, k]&#8288;</code>.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_student_t_linear_operator_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_student_t_linear_operator_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_multivariate_student_t_linear_operator_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; df, loc, Sigma) = (1 + ||y||**2 / df)**(-0.5 (df + k)) / Z
where,
y = inv(Sigma) (x - loc)
Z = abs(det(Sigma)) sqrt(df pi)**k Gamma(0.5 df) / Gamma(0.5 (df + k))
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>df</code> is a positive scalar.
</p>
</li>
<li> <p><code>loc</code> is a vector in <code>R^k</code>,
</p>
</li>
<li> <p><code>Sigma</code> is a positive definite <code>shape</code> matrix in <code style="white-space: pre;">&#8288;R^{k x k}&#8288;</code>, parameterized
as <code>scale @ scale.T</code> in this class,
</p>
</li>
<li> <p><code>Z</code> denotes the normalization constant, and,
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;||y||**2&#8288;</code> denotes the squared Euclidean norm of <code>y</code>.
</p>
</li></ul>

<p>The Multivariate Student's t-distribution distribution is a member of the
<a href="https://en.wikipedia.org/wiki/Location-scale_family">location-scale family</a>, i.e., it can be
constructed as,
</p>
<div class="sourceCode"><pre>X ~ MultivariateT(loc=0, scale=1)   # Identity scale, zero shift.
Y = scale @ X + loc
</pre></div>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_negative_binomial'>NegativeBinomial distribution</h2><span id='topic+tfd_negative_binomial'></span>

<h3>Description</h3>

<p>The NegativeBinomial distribution is related to the experiment of performing
Bernoulli trials in sequence. Given a Bernoulli trial with probability <code>p</code> of
success, the NegativeBinomial distribution represents the distribution over
the number of successes <code>s</code> that occur until we observe <code>f</code> failures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_negative_binomial(
  total_count,
  logits = NULL,
  probs = NULL,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "NegativeBinomial"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_negative_binomial_+3A_total_count">total_count</code></td>
<td>
<p>Non-negative floating-point <code>Tensor</code> with shape
broadcastable to <code style="white-space: pre;">&#8288;[B1,..., Bb]&#8288;</code> with <code>b &gt;= 0</code> and the same dtype as
<code>probs</code> or <code>logits</code>. Defines this as a batch of <code style="white-space: pre;">&#8288;N1 x ... x Nm&#8288;</code>
different Negative Binomial distributions. In practice, this represents
the number of negative Bernoulli trials to stop at (the <code>total_count</code>
of failures), but this is still a valid distribution when
<code>total_count</code> is a non-integer.</p>
</td></tr>
<tr><td><code id="tfd_negative_binomial_+3A_logits">logits</code></td>
<td>
<p>Floating-point <code>Tensor</code> with shape broadcastable to
<code style="white-space: pre;">&#8288;[B1, ..., Bb]&#8288;</code> where <code>b &gt;= 0</code> indicates the number of batch dimensions.
Each entry represents logits for the probability of success for
independent Negative Binomial distributions and must be in the open
interval <code style="white-space: pre;">&#8288;(-inf, inf)&#8288;</code>. Only one of <code>logits</code> or <code>probs</code> should be
specified.</p>
</td></tr>
<tr><td><code id="tfd_negative_binomial_+3A_probs">probs</code></td>
<td>
<p>Positive floating-point <code>Tensor</code> with shape broadcastable to
<code style="white-space: pre;">&#8288;[B1, ..., Bb]&#8288;</code> where <code>b &gt;= 0</code> indicates the number of batch dimensions.
Each entry represents the probability of success for independent
Negative Binomial distributions and must be in the open interval
<code style="white-space: pre;">&#8288;(0, 1)&#8288;</code>. Only one of <code>logits</code> or <code>probs</code> should be specified.</p>
</td></tr>
<tr><td><code id="tfd_negative_binomial_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_negative_binomial_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_negative_binomial_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The probability mass function (pmf) is,
</p>
<div class="sourceCode"><pre>pmf(s; f, p) = p**s (1 - p)**f / Z
Z = s! (f - 1)! / (s + f - 1)!
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>total_count = f</code>,
</p>
</li>
<li> <p><code>probs = p</code>,
</p>
</li>
<li> <p><code>Z</code> is the normalizaing constant, and,
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;n!&#8288;</code> is the factorial of <code>n</code>.
</p>
</li></ul>



<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_normal'>Normal distribution with loc and scale parameters</h2><span id='topic+tfd_normal'></span>

<h3>Description</h3>

<p>Mathematical details
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_normal(
  loc,
  scale,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Normal"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_normal_+3A_loc">loc</code></td>
<td>
<p>Floating point tensor; the means of the distribution(s).</p>
</td></tr>
<tr><td><code id="tfd_normal_+3A_scale">scale</code></td>
<td>
<p>loating point tensor; the stddevs of the distribution(s).
Must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_normal_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_normal_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_normal_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; mu, sigma) = exp(-0.5 (x - mu)**2 / sigma**2) / Z
Z = (2 pi sigma**2)**0.5
</pre></div>
<p>where <code>loc = mu</code> is the mean, <code>scale = sigma</code> is the std. deviation, and, <code>Z</code>
is the normalization constant.
The Normal distribution is a member of the <a href="https://en.wikipedia.org/wiki/Location-scale_family">location-scale family</a>, i.e., it can be
constructed as,
</p>
<div class="sourceCode"><pre>X ~ Normal(loc=0, scale=1)
Y = loc + scale * X
</pre></div>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_one_hot_categorical'>OneHotCategorical distribution</h2><span id='topic+tfd_one_hot_categorical'></span>

<h3>Description</h3>

<p>The categorical distribution is parameterized by the log-probabilities of a set of classes.
The difference between OneHotCategorical and Categorical distributions is that OneHotCategorical
is a discrete distribution over one-hot bit vectors whereas Categorical is a discrete distribution
over positive integers. OneHotCategorical is equivalent to Categorical except Categorical has
event_dim=() while OneHotCategorical has event_dim=K, where K is the number of classes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_one_hot_categorical(
  logits = NULL,
  probs = NULL,
  dtype = tf$int32,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "OneHotCategorical"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_one_hot_categorical_+3A_logits">logits</code></td>
<td>
<p>An N-D Tensor, N &gt;= 1, representing the log probabilities of a set of Categorical distributions.
The first N - 1 dimensions index into a batch of independent distributions and the last dimension represents
a vector of logits for each class. Only one of logits or probs should be passed in.</p>
</td></tr>
<tr><td><code id="tfd_one_hot_categorical_+3A_probs">probs</code></td>
<td>
<p>An N-D Tensor, N &gt;= 1, representing the probabilities of a set of Categorical distributions. The
first N - 1 dimensions index into a batch of independent distributions and the last dimension represents a
vector of probabilities for each class. Only one of logits or probs should be passed in.</p>
</td></tr>
<tr><td><code id="tfd_one_hot_categorical_+3A_dtype">dtype</code></td>
<td>
<p>The type of the event samples (default: int32).</p>
</td></tr>
<tr><td><code id="tfd_one_hot_categorical_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_one_hot_categorical_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_one_hot_categorical_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This class provides methods to create indexed batches of OneHotCategorical distributions.
If the provided logits or probs is rank 2 or higher, for every fixed set of leading dimensions,
the last dimension represents one single OneHotCategorical distribution. When calling distribution
functions (e.g. dist.prob(x)), logits and x are broadcast to the same shape (if possible).
In all cases, the last dimension of logits, x represents single OneHotCategorical distributions.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_pareto'>Pareto distribution</h2><span id='topic+tfd_pareto'></span>

<h3>Description</h3>

<p>The Pareto distribution is parameterized by a <code>scale</code> and a
<code>concentration</code> parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_pareto(
  concentration,
  scale = 1,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Pareto"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_pareto_+3A_concentration">concentration</code></td>
<td>
<p>Floating point tensor. Must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_pareto_+3A_scale">scale</code></td>
<td>
<p>Floating point tensor, equivalent to <code>mode</code>. <code>scale</code> also
restricts the domain of this distribution to be in <code style="white-space: pre;">&#8288;[scale, inf)&#8288;</code>.
Must contain only positive values. Default value: <code>1</code>.</p>
</td></tr>
<tr><td><code id="tfd_pareto_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_pareto_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_pareto_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; alpha, scale, x &gt;= scale) = alpha * scale ** alpha / x ** (alpha + 1)
```#'
where `concentration = alpha`.

Note that `scale` acts as a scaling parameter, since
`Pareto(c, scale).pdf(x) == Pareto(c, 1.).pdf(x / scale)`.
The support of the distribution is defined on `[scale, infinity)`.
</pre></div>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_pert'>Modified PERT distribution for modeling expert predictions.</h2><span id='topic+tfd_pert'></span>

<h3>Description</h3>

<p>The PERT distribution is a loc-scale family of Beta distributions
fit onto a real interval between <code>low</code> and <code>high</code> values set by the user,
along with a <code>peak</code> to indicate the expert's most frequent prediction,
and <code>temperature</code> to control how sharp the peak is.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_pert(
  low,
  peak,
  high,
  temperature = 4,
  validate_args = FALSE,
  allow_nan_stats = FALSE,
  name = "Pert"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_pert_+3A_low">low</code></td>
<td>
<p>lower bound</p>
</td></tr>
<tr><td><code id="tfd_pert_+3A_peak">peak</code></td>
<td>
<p>most frequent value</p>
</td></tr>
<tr><td><code id="tfd_pert_+3A_high">high</code></td>
<td>
<p>upper bound</p>
</td></tr>
<tr><td><code id="tfd_pert_+3A_temperature">temperature</code></td>
<td>
<p>controls the shape of the distribution</p>
</td></tr>
<tr><td><code id="tfd_pert_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_pert_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_pert_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The distribution is similar to a <a href="https://en.wikipedia.org/wiki/Triangular_distribution">Triangular distribution</a>
(i.e. <code>tfd.Triangular</code>) but with a smooth peak.
</p>
<p>Mathematical Details
</p>
<p>In terms of a Beta distribution, PERT can be expressed as
</p>
<div class="sourceCode"><pre>PERT ~ loc + scale * Beta(concentration1, concentration0)
</pre></div>
<p>where
</p>
<div class="sourceCode"><pre>loc = low
scale = high - low
concentration1 = 1 + temperature * (peak - low)/(high - low)
concentration0 = 1 + temperature * (high - peak)/(high - low)
temperature &gt; 0
</pre></div>
<p>The support is <code style="white-space: pre;">&#8288;[low, high]&#8288;</code>.  The <code>peak</code> must fit in that interval:
<code style="white-space: pre;">&#8288;low &lt; peak &lt; high&#8288;</code>.  The <code>temperature</code> is a positive parameter that
controls the shape of the distribution. Higher values yield a sharper peak.
The standard PERT distribution is obtained when <code>temperature = 4</code>.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>

<hr>
<h2 id='tfd_pixel_cnn'>The Pixel CNN++ distribution</h2><span id='topic+tfd_pixel_cnn'></span>

<h3>Description</h3>

<p>Pixel CNN++ (Salimans et al., 2017) models a distribution over image
data, parameterized by a neural network. It builds on Pixel CNN and
Conditional Pixel CNN, as originally proposed by
(van den Oord et al., 2016).
The model expresses the joint distribution over pixels as
the product of conditional distributions:
<code style="white-space: pre;">&#8288;p(x|h) = prod{ p(x[i] | x[0:i], h) : i=0, ..., d }&#8288;</code>, in which
<code style="white-space: pre;">&#8288;p(x[i] | x[0:i], h) : i=0, ..., d&#8288;</code> is the
probability of the <code>i</code>-th pixel conditional on the pixels that preceded it in
raster order (color channels in RGB order, then left to right, then top to
bottom). <code>h</code> is optional additional data on which to condition the image
distribution, such as class labels or VAE embeddings. The Pixel CNN++
network enforces the dependency structure among pixels by applying a mask to
the kernels of the convolutional layers that ensures that the values for each
pixel depend only on other pixels up and to the left.
Pixel values are modeled with a mixture of quantized logistic distributions,
which can take on a set of distinct integer values (e.g. between 0 and 255
for an 8-bit image).
Color intensity <code>v</code> of each pixel is modeled as:
<code style="white-space: pre;">&#8288;v ~ sum{q[i] * quantized_logistic(loc[i], scale[i]) : i = 0, ..., k }&#8288;</code>,
in which <code>k</code> is the number of mixture components and the <code>q[i]</code> are the
Categorical probabilities over the components.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_pixel_cnn(
  image_shape,
  conditional_shape = NULL,
  num_resnet = 5,
  num_hierarchies = 3,
  num_filters = 160,
  num_logistic_mix = 10,
  receptive_field_dims = c(3, 3),
  dropout_p = 0.5,
  resnet_activation = "concat_elu",
  use_weight_norm = TRUE,
  use_data_init = TRUE,
  high = 255,
  low = 0,
  dtype = tf$float32,
  name = "PixelCNN"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_pixel_cnn_+3A_image_shape">image_shape</code></td>
<td>
<p>3D <code>TensorShape</code> or tuple for the <code style="white-space: pre;">&#8288;[height, width, channels]&#8288;</code>
dimensions of the image.</p>
</td></tr>
<tr><td><code id="tfd_pixel_cnn_+3A_conditional_shape">conditional_shape</code></td>
<td>
<p><code>TensorShape</code> or tuple for the shape of the
conditional input, or <code>NULL</code> if there is no conditional input.</p>
</td></tr>
<tr><td><code id="tfd_pixel_cnn_+3A_num_resnet">num_resnet</code></td>
<td>
<p><code>integer</code>, the number of layers (shown in Figure 2 of https://arxiv.org/abs/1606.05328)
within each highest-level block of Figure 2 of https://pdfs.semanticscholar.org/9e90/6792f67cbdda7b7777b69284a81044857656.pdf.</p>
</td></tr>
<tr><td><code id="tfd_pixel_cnn_+3A_num_hierarchies">num_hierarchies</code></td>
<td>
<p><code>integer</code>, the number of hightest-level blocks (separated by
expansions/contractions of dimensions in Figure 2 of https://pdfs.semanticscholar.org/9e90/6792f67cbdda7b7777b69284a81044857656.pdf.)</p>
</td></tr>
<tr><td><code id="tfd_pixel_cnn_+3A_num_filters">num_filters</code></td>
<td>
<p><code>integer</code>, the number of convolutional filters.</p>
</td></tr>
<tr><td><code id="tfd_pixel_cnn_+3A_num_logistic_mix">num_logistic_mix</code></td>
<td>
<p><code>integer</code>, number of components in the logistic mixture
distribution.</p>
</td></tr>
<tr><td><code id="tfd_pixel_cnn_+3A_receptive_field_dims">receptive_field_dims</code></td>
<td>
<p><code>tuple</code>, height and width in pixels of the receptive
field of the convolutional layers above and to the left of a given
pixel. The width (second element of the tuple) should be odd. Figure 1
(middle) of https://arxiv.org/abs/1606.05328 shows a receptive field of (3, 5)
(the row containing the current pixel is included in the height).
The default of (3, 3) was used to produce the results in https://pdfs.semanticscholar.org/9e90/6792f67cbdda7b7777b69284a81044857656.pdf.</p>
</td></tr>
<tr><td><code id="tfd_pixel_cnn_+3A_dropout_p">dropout_p</code></td>
<td>
<p><code>float</code>, the dropout probability. Should be between 0 and 1.</p>
</td></tr>
<tr><td><code id="tfd_pixel_cnn_+3A_resnet_activation">resnet_activation</code></td>
<td>
<p><code>string</code>, the type of activation to use in the resnet blocks.
May be 'concat_elu', 'elu', or 'relu'.</p>
</td></tr>
<tr><td><code id="tfd_pixel_cnn_+3A_use_weight_norm">use_weight_norm</code></td>
<td>
<p><code>logical</code>, if <code>TRUE</code> then use weight normalization (works
only in Eager mode).</p>
</td></tr>
<tr><td><code id="tfd_pixel_cnn_+3A_use_data_init">use_data_init</code></td>
<td>
<p><code>logical</code>, if <code>TRUE</code> then use data-dependent initialization
(has no effect if <code>use_weight_norm</code> is <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="tfd_pixel_cnn_+3A_high">high</code></td>
<td>
<p><code>integer</code>, the maximum value of the input data (255 for an 8-bit image).</p>
</td></tr>
<tr><td><code id="tfd_pixel_cnn_+3A_low">low</code></td>
<td>
<p><code>integer</code>, the minimum value of the input data.</p>
</td></tr>
<tr><td><code id="tfd_pixel_cnn_+3A_dtype">dtype</code></td>
<td>
<p>Data type of the <code>Distribution</code>.</p>
</td></tr>
<tr><td><code id="tfd_pixel_cnn_+3A_name">name</code></td>
<td>
<p><code>string</code>, the name of the <code>Distribution</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://www.semanticscholar.org/paper/OTHER-MODIFICATIONS-Karpathy/2e77b99e8bd10b9e4551a780c0bde9dd10fdbe9b?p2df">Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications. In <em>International Conference on Learning Representations</em>, 2017.</a>
</p>
</li>
<li> <p><a href="https://arxiv.org/abs/1606.05328">Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. Conditional Image Generation with PixelCNN Decoders. In <em>Neural Information Processing Systems</em>, 2016.</a>
</p>
</li>
<li> <p><a href="https://arxiv.org/pdf/1601.06759.pdf">Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel Recurrent Neural Networks. In <em>International Conference on Machine Learning</em>, 2016.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_plackett_luce'>Plackett-Luce distribution over permutations.</h2><span id='topic+tfd_plackett_luce'></span>

<h3>Description</h3>

<p>The Plackett-Luce distribution is defined over permutations of
fixed length. It is parameterized by a positive score vector of same length.
This class provides methods to create indexed batches of PlackettLuce
distributions. If the provided <code>scores</code> is rank 2 or higher, for
every fixed set of leading dimensions, the last dimension represents one
single PlackettLuce distribution. When calling distribution
functions (e.g. <code>dist.log_prob(x)</code>), <code>scores</code> and <code>x</code> are broadcast to the
same shape (if possible). In all cases, the last dimension of <code style="white-space: pre;">&#8288;scores, x&#8288;</code>
represents single PlackettLuce distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_plackett_luce(
  scores,
  dtype = tf$int32,
  validate_args = FALSE,
  allow_nan_stats = FALSE,
  name = "PlackettLuce"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_plackett_luce_+3A_scores">scores</code></td>
<td>
<p>An N-D <code>Tensor</code>, <code>N &gt;= 1</code>, representing the scores of a set of
elements to be permuted. The first <code>N - 1</code> dimensions index into a
batch of independent distributions and the last dimension represents a
vector of scores for the elements.</p>
</td></tr>
<tr><td><code id="tfd_plackett_luce_+3A_dtype">dtype</code></td>
<td>
<p>The type of the event samples (default: int32).</p>
</td></tr>
<tr><td><code id="tfd_plackett_luce_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_plackett_luce_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_plackett_luce_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The Plackett-Luce is a distribution over permutation vectors <code>p</code> of length <code>k</code>
where the permutation <code>p</code> is an arbitrary ordering of <code>k</code> indices
<code style="white-space: pre;">&#8288;{0, 1, ..., k-1}&#8288;</code>.
</p>
<p>The probability mass function (pmf) is,
</p>
<div class="sourceCode"><pre>pmf(p; s) = prod_i s_{p_i} / (Z - Z_i)
Z = sum_{j=0}^{k-1} s_j
Z_i = sum_{j=0}^{i-1} s_{p_j} for i&gt;0 and 0 for i=0
</pre></div>
<p>where <code style="white-space: pre;">&#8288;scores = s = [s_0, ..., s_{k-1}]&#8288;</code>, <code>s_i &gt;= 0</code>.
</p>
<p>Samples from Plackett-Luce distribution are generated sequentially as follows.
</p>
<div class="sourceCode"><pre>Initialize normalization `N_0 = Z`
For `i` in `{0, 1, ..., k-1}`
  1. Sample i-th element of permutation
     `p_i ~ Categorical(probs=[s_0/N_i, ..., s_{k-1}/N_i])`
  2. Update normalization
     `N_{i+1} = N_i-s_{p_i}`
  3. Mask out sampled index for subsequent rounds
     `s_{p_i} = 0`
Return p
</pre></div>
<p>Alternately, an equivalent way to sample from this distribution is to sort
Gumbel perturbed log-scores (Aditya et al. 2019)
</p>
<div class="sourceCode"><pre>p = argsort(log s + g) ~ PlackettLuce(s)
g = [g_0, ..., g_{k-1}], g_i~ Gumbel(0, 1)
</pre></div>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>References</h3>


<ul>
<li><p> Aditya Grover, Eric Wang, Aaron Zweig, Stefano Ermon. Stochastic Optimization of Sorting Networks via Continuous Relaxations. ICLR 2019.
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>

<hr>
<h2 id='tfd_poisson'>Poisson distribution</h2><span id='topic+tfd_poisson'></span>

<h3>Description</h3>

<p>The Poisson distribution is parameterized by an event <code>rate</code> parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_poisson(
  rate = NULL,
  log_rate = NULL,
  interpolate_nondiscrete = TRUE,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Poisson"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_poisson_+3A_rate">rate</code></td>
<td>
<p>Floating point tensor, the rate parameter. <code>rate</code> must be positive.
Must specify exactly one of <code>rate</code> and <code>log_rate</code>.</p>
</td></tr>
<tr><td><code id="tfd_poisson_+3A_log_rate">log_rate</code></td>
<td>
<p>Floating point tensor, the log of the rate parameter.
Must specify exactly one of <code>rate</code> and <code>log_rate</code>.</p>
</td></tr>
<tr><td><code id="tfd_poisson_+3A_interpolate_nondiscrete">interpolate_nondiscrete</code></td>
<td>
<p>Logical. When <code>FALSE</code>,
<code>log_prob</code> returns <code>-inf</code> (and <code>prob</code> returns <code>0</code>) for non-integer
inputs. When <code>TRUE</code>, <code>log_prob</code> evaluates the continuous function
<code>k * log_rate - lgamma(k+1) - rate</code>, which matches the Poisson pmf
at integer arguments <code>k</code> (note that this function is not itself
a normalized probability log-density). Default value: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="tfd_poisson_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_poisson_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_poisson_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The probability mass function (pmf) is,
</p>
<div class="sourceCode"><pre>pmf(k; lambda, k &gt;= 0) = (lambda^k / k!) / Z
Z = exp(lambda).
</pre></div>
<p>where <code>rate = lambda</code> and <code>Z</code> is the normalizing constant.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_poisson_log_normal_quadrature_compound'><code>PoissonLogNormalQuadratureCompound</code> distribution</h2><span id='topic+tfd_poisson_log_normal_quadrature_compound'></span>

<h3>Description</h3>

<p>The <code>PoissonLogNormalQuadratureCompound</code> is an approximation to a
Poisson-LogNormal <a href="https://en.wikipedia.org/wiki/Compound_probability_distribution">compound distribution</a>, i.e.,
</p>
<div class="sourceCode"><pre>p(k|loc, scale) = int_{R_+} dl LogNormal(l | loc, scale) Poisson(k | l)
approx= sum{ prob[d] Poisson(k | lambda(grid[d])) : d=0, ..., deg-1 }
</pre></div>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_poisson_log_normal_quadrature_compound(
  loc,
  scale,
  quadrature_size = 8,
  quadrature_fn = tfp$distributions$quadrature_scheme_lognormal_quantiles,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "PoissonLogNormalQuadratureCompound"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_poisson_log_normal_quadrature_compound_+3A_loc">loc</code></td>
<td>
<p><code>float</code>-like (batch of) scalar <code>Tensor</code>; the location parameter of
the LogNormal prior.</p>
</td></tr>
<tr><td><code id="tfd_poisson_log_normal_quadrature_compound_+3A_scale">scale</code></td>
<td>
<p><code>float</code>-like (batch of) scalar <code>Tensor</code>; the scale parameter of
the LogNormal prior.</p>
</td></tr>
<tr><td><code id="tfd_poisson_log_normal_quadrature_compound_+3A_quadrature_size">quadrature_size</code></td>
<td>
<p><code>integer</code> scalar representing the number of quadrature
points.</p>
</td></tr>
<tr><td><code id="tfd_poisson_log_normal_quadrature_compound_+3A_quadrature_fn">quadrature_fn</code></td>
<td>
<p>Function taking <code>loc</code>, <code>scale</code>,
<code>quadrature_size</code>, <code>validate_args</code> and returning <code>tuple(grid, probs)</code>
representing the LogNormal grid and corresponding normalized weight.
Default value: <code>quadrature_scheme_lognormal_quantiles</code>.</p>
</td></tr>
<tr><td><code id="tfd_poisson_log_normal_quadrature_compound_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_poisson_log_normal_quadrature_compound_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_poisson_log_normal_quadrature_compound_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default, the <code>grid</code> is chosen as quantiles of the <code>LogNormal</code> distribution
parameterized by <code>loc</code>, <code>scale</code> and the <code>prob</code> vector is
<code style="white-space: pre;">&#8288;[1. / quadrature_size]*quadrature_size&#8288;</code>.
</p>
<p>In the non-approximation case, a draw from the LogNormal prior represents the
Poisson rate parameter. Unfortunately, the non-approximate distribution lacks
an analytical probability density function (pdf). Therefore the
<code>PoissonLogNormalQuadratureCompound</code> class implements an approximation based
on <a href="https://en.wikipedia.org/wiki/Numerical_integration">quadrature</a>.
Note: although the <code>PoissonLogNormalQuadratureCompound</code> is approximately the
Poisson-LogNormal compound distribution, it is itself a valid distribution.
Viz., it possesses a <code>sample</code>, <code>log_prob</code>, <code>mean</code>, <code>variance</code>, etc. which are
all mutually consistent.
</p>
<p>Mathematical Details
</p>
<p>The <code>PoissonLogNormalQuadratureCompound</code> approximates a Poisson-LogNormal
<a href="https://en.wikipedia.org/wiki/Compound_probability_distribution">compound distribution</a>.
Using variable-substitution and <a href="https://en.wikipedia.org/wiki/Numerical_integration">numerical quadrature</a> (default:
based on <code>LogNormal</code> quantiles) we can redefine the distribution to be a
parameter-less convex combination of <code>deg</code> different Poisson samples.
That is, defined over positive integers, this distribution is parameterized
by a (batch of) <code>loc</code> and <code>scale</code> scalars.
</p>
<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(k | loc, scale, deg) = sum{ prob[d] Poisson(k | lambda=exp(grid[d])) : d=0, ..., deg-1 }
</pre></div>
<p>Note: <code>probs</code> returned by (optional) <code>quadrature_fn</code> are presumed to be
either a length-<code>quadrature_size</code> vector or a batch of vectors in 1-to-1
correspondence with the returned <code>grid</code>. (I.e., broadcasting is only partially supported.)
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_power_spherical'>The Power Spherical distribution over unit vectors on <code>S^{n-1}</code>.</h2><span id='topic+tfd_power_spherical'></span>

<h3>Description</h3>

<p>The Power Spherical distribution is a distribution over vectors
on the unit hypersphere <code>S^{n-1}</code> embedded in <code>n</code> dimensions (<code>R^n</code>).
It serves as an alternative to the von Mises-Fisher distribution with a
simpler (faster) <code>log_prob</code> calculation, as well as a reparameterizable
sampler. In contrast, the Power Spherical distribution does have
-<code>mean_direction</code> as a point with zero density (and hence a neighborhood
around that having arbitrarily small density), in contrast with the
von Mises-Fisher distribution which has non-zero density everywhere.
NOTE: <code>mean_direction</code> is not in general the mean of the distribution. For
spherical distributions, the mean is generally not in the support of the
distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_power_spherical(
  mean_direction,
  concentration,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "PowerSpherical"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_power_spherical_+3A_mean_direction">mean_direction</code></td>
<td>
<p>Floating-point <code>Tensor</code> with shape <code style="white-space: pre;">&#8288;[B1, ... Bn, N]&#8288;</code>.
A unit vector indicating the mode of the distribution, or the
unit-normalized direction of the mean.</p>
</td></tr>
<tr><td><code id="tfd_power_spherical_+3A_concentration">concentration</code></td>
<td>
<p>Floating-point <code>Tensor</code> having batch shape <code style="white-space: pre;">&#8288;[B1, ... Bn]&#8288;</code>
broadcastable with <code>mean_direction</code>. The level of concentration of
samples around the <code>mean_direction</code>. <code>concentration=0</code> indicates a
uniform distribution over the unit hypersphere, and <code>concentration=+inf</code>
indicates a <code>Deterministic</code> distribution (delta function) at
<code>mean_direction</code>.</p>
</td></tr>
<tr><td><code id="tfd_power_spherical_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_power_spherical_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_power_spherical_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical details
</p>
<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; mu, kappa) = C(kappa) (1 + mu^T x) ** k
  where,
     C(kappa) = 2**(a + b) pi**b Gamma(a) / Gamma(a + b)
     a = (n - 1) / 2. + k
     b = (n - 1) / 2.
</pre></div>
<p>where
</p>

<ul>
<li> <p><code>mean_direction = mu</code>; a unit vector in <code>R^k</code>,
</p>
</li>
<li> <p><code>concentration = kappa</code>; scalar real &gt;= 0, concentration of samples around
<code>mean_direction</code>, where 0 pertains to the uniform distribution on the
hypersphere, and <code style="white-space: pre;">&#8288;\inf&#8288;</code> indicates a delta function at <code>mean_direction</code>.
</p>
</li></ul>



<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_prob'>Probability density/mass function.</h2><span id='topic+tfd_prob'></span>

<h3>Description</h3>

<p>Probability density/mass function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_prob(distribution, value, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_prob_+3A_distribution">distribution</code></td>
<td>
<p>The distribution being used.</p>
</td></tr>
<tr><td><code id="tfd_prob_+3A_value">value</code></td>
<td>
<p>float or double Tensor.</p>
</td></tr>
<tr><td><code id="tfd_prob_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to Python.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Tensor of shape <code>sample_shape(x) + self$batch_shape</code> with values of type <code>self$dtype</code>.
</p>


<h3>See Also</h3>

<p>Other distribution_methods: 
<code><a href="#topic+tfd_cdf">tfd_cdf</a>()</code>,
<code><a href="#topic+tfd_covariance">tfd_covariance</a>()</code>,
<code><a href="#topic+tfd_cross_entropy">tfd_cross_entropy</a>()</code>,
<code><a href="#topic+tfd_entropy">tfd_entropy</a>()</code>,
<code><a href="#topic+tfd_kl_divergence">tfd_kl_divergence</a>()</code>,
<code><a href="#topic+tfd_log_cdf">tfd_log_cdf</a>()</code>,
<code><a href="#topic+tfd_log_prob">tfd_log_prob</a>()</code>,
<code><a href="#topic+tfd_log_survival_function">tfd_log_survival_function</a>()</code>,
<code><a href="#topic+tfd_mean">tfd_mean</a>()</code>,
<code><a href="#topic+tfd_mode">tfd_mode</a>()</code>,
<code><a href="#topic+tfd_quantile">tfd_quantile</a>()</code>,
<code><a href="#topic+tfd_sample">tfd_sample</a>()</code>,
<code><a href="#topic+tfd_stddev">tfd_stddev</a>()</code>,
<code><a href="#topic+tfd_survival_function">tfd_survival_function</a>()</code>,
<code><a href="#topic+tfd_variance">tfd_variance</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  d &lt;- tfd_normal(loc = c(1, 2), scale = c(1, 0.5))
  x &lt;- d %&gt;% tfd_sample()
  d %&gt;% tfd_prob(x)

</code></pre>

<hr>
<h2 id='tfd_probit_bernoulli'>ProbitBernoulli distribution.</h2><span id='topic+tfd_probit_bernoulli'></span>

<h3>Description</h3>

<p>The ProbitBernoulli distribution with <code>probs</code> parameter, i.e., the probability
of a <code>1</code> outcome (vs a <code>0</code> outcome). Unlike a regular Bernoulli distribution,
which uses the logistic (aka 'sigmoid') function to go from the un-constrained
parameters to probabilities, this distribution uses the CDF of the
<a href="https://en.wikipedia.org/wiki/Normal_distribution">standard normal distribution</a>:
</p>
<div class="sourceCode"><pre>p(x=1; probits) = 0.5 * (1 + erf(probits / sqrt(2)))
p(x=0; probits) = 1 - p(x=1; probits)
</pre></div>
<p>Where <code>erf</code> is the <a href="https://en.wikipedia.org/wiki/Error_function">error function</a>.
A typical application of this distribution is in
<a href="https://en.wikipedia.org/wiki/Probit_model">probit  regression</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_probit_bernoulli(
  probits = NULL,
  probs = NULL,
  dtype = tf$int32,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "ProbitBernoulli"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_probit_bernoulli_+3A_probits">probits</code></td>
<td>
<p>An N-D <code>Tensor</code> representing the probit-odds of a <code>1</code> event. Each
entry in the <code>Tensor</code> parameterizes an independent ProbitBernoulli
distribution where the probability of an event is normal_cdf(probits).
Only one of <code>probits</code> or <code>probs</code> should be passed in.</p>
</td></tr>
<tr><td><code id="tfd_probit_bernoulli_+3A_probs">probs</code></td>
<td>
<p>An N-D <code>Tensor</code> representing the probability of a <code>1</code>
event. Each entry in the <code>Tensor</code> parameterizes an independent
ProbitBernoulli distribution. Only one of <code>probits</code> or <code>probs</code> should be
passed in.</p>
</td></tr>
<tr><td><code id="tfd_probit_bernoulli_+3A_dtype">dtype</code></td>
<td>
<p>The type of the event samples. Default: <code>int32</code>.</p>
</td></tr>
<tr><td><code id="tfd_probit_bernoulli_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_probit_bernoulli_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_probit_bernoulli_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_quantile'>Quantile function. Aka &quot;inverse cdf&quot; or &quot;percent point function&quot;.</h2><span id='topic+tfd_quantile'></span>

<h3>Description</h3>

<p>Given random variable X and p in <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code>, the quantile is:
<code>tfd_quantile(p) := x</code> such that <code>P[X &lt;= x] == p</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_quantile(distribution, value, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_quantile_+3A_distribution">distribution</code></td>
<td>
<p>The distribution being used.</p>
</td></tr>
<tr><td><code id="tfd_quantile_+3A_value">value</code></td>
<td>
<p>float or double Tensor.</p>
</td></tr>
<tr><td><code id="tfd_quantile_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to Python.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Tensor of shape <code>sample_shape(x) + self$batch_shape</code> with values of type <code>self$dtype</code>.
</p>


<h3>See Also</h3>

<p>Other distribution_methods: 
<code><a href="#topic+tfd_cdf">tfd_cdf</a>()</code>,
<code><a href="#topic+tfd_covariance">tfd_covariance</a>()</code>,
<code><a href="#topic+tfd_cross_entropy">tfd_cross_entropy</a>()</code>,
<code><a href="#topic+tfd_entropy">tfd_entropy</a>()</code>,
<code><a href="#topic+tfd_kl_divergence">tfd_kl_divergence</a>()</code>,
<code><a href="#topic+tfd_log_cdf">tfd_log_cdf</a>()</code>,
<code><a href="#topic+tfd_log_prob">tfd_log_prob</a>()</code>,
<code><a href="#topic+tfd_log_survival_function">tfd_log_survival_function</a>()</code>,
<code><a href="#topic+tfd_mean">tfd_mean</a>()</code>,
<code><a href="#topic+tfd_mode">tfd_mode</a>()</code>,
<code><a href="#topic+tfd_prob">tfd_prob</a>()</code>,
<code><a href="#topic+tfd_sample">tfd_sample</a>()</code>,
<code><a href="#topic+tfd_stddev">tfd_stddev</a>()</code>,
<code><a href="#topic+tfd_survival_function">tfd_survival_function</a>()</code>,
<code><a href="#topic+tfd_variance">tfd_variance</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  d &lt;- tfd_normal(loc = c(1, 2), scale = c(1, 0.5))
  d %&gt;% tfd_quantile(0.5)

</code></pre>

<hr>
<h2 id='tfd_quantized'>Distribution representing the quantization <code>Y = ceiling(X)</code></h2><span id='topic+tfd_quantized'></span>

<h3>Description</h3>

<p>Definition in Terms of Sampling
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_quantized(
  distribution,
  low = NULL,
  high = NULL,
  validate_args = FALSE,
  name = "QuantizedDistribution"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_quantized_+3A_distribution">distribution</code></td>
<td>
<p>The base distribution class to transform. Typically an
instance of <code>Distribution</code>.</p>
</td></tr>
<tr><td><code id="tfd_quantized_+3A_low">low</code></td>
<td>
<p><code>Tensor</code> with same <code>dtype</code> as this distribution and shape
able to be added to samples. Should be a whole number. Default <code>NULL</code>.
If provided, base distribution's <code>prob</code> should be defined at <code>low</code>.</p>
</td></tr>
<tr><td><code id="tfd_quantized_+3A_high">high</code></td>
<td>
<p><code>Tensor</code> with same <code>dtype</code> as this distribution and shape
able to be added to samples. Should be a whole number. Default <code>NULL</code>.
If provided, base distribution's <code>prob</code> should be defined at <code>high - 1</code>.
<code>high</code> must be strictly greater than <code>low</code>.</p>
</td></tr>
<tr><td><code id="tfd_quantized_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_quantized_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<div class="sourceCode"><pre>1. Draw X
2. Set Y &lt;-- ceiling(X)
3. If Y &lt; low, reset Y &lt;-- low
4. If Y &gt; high, reset Y &lt;-- high
5. Return Y
</pre></div>
<p>Definition in Terms of the Probability Mass Function
</p>
<p>Given scalar random variable <code>X</code>, we define a discrete random variable <code>Y</code>
supported on the integers as follows:
</p>
<div class="sourceCode"><pre>P[Y = j] := P[X &lt;= low],  if j == low,
         := P[X &gt; high - 1],  j == high,
         := 0, if j &lt; low or j &gt; high,
         := P[j - 1 &lt; X &lt;= j],  all other j.
</pre></div>
<p>Conceptually, without cutoffs, the quantization process partitions the real
line <code>R</code> into half open intervals, and identifies an integer <code>j</code> with the
right endpoints:
</p>
<div class="sourceCode"><pre>R = ... (-2, -1](-1, 0](0, 1](1, 2](2, 3](3, 4] ...
j = ...      -1      0     1     2     3     4  ...
</pre></div>
<p><code>P[Y = j]</code> is the mass of <code>X</code> within the <code>jth</code> interval.
If <code>low = 0</code>, and <code>high = 2</code>, then the intervals are redrawn
and <code>j</code> is re-assigned:
</p>
<div class="sourceCode"><pre>R = (-infty, 0](0, 1](1, infty)
j =          0     1     2
</pre></div>
<p><code>P[Y = j]</code> is still the mass of <code>X</code> within the <code>jth</code> interval.
</p>
<p>@section References:
</p>

<ul>
<li> <p><a href="https://arxiv.org/abs/1701.05517">Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications. International Conference on Learning Representations_, 2017.</a>
</p>
</li>
<li> <p><a href="https://arxiv.org/abs/1711.10433">Aaron van den Oord et al. Parallel WaveNet: Fast High-Fidelity Speech Synthesis. <em>arXiv preprint arXiv:1711.10433</em>, 2017.</a>
</p>
</li></ul>



<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_relaxed_bernoulli'>RelaxedBernoulli distribution with temperature and logits parameters</h2><span id='topic+tfd_relaxed_bernoulli'></span>

<h3>Description</h3>

<p>The RelaxedBernoulli is a distribution over the unit interval (0,1), which continuously approximates a Bernoulli.
The degree of approximation is controlled by a temperature: as the temperature goes to 0 the RelaxedBernoulli
becomes discrete with a distribution described by the logits or probs parameters, as the temperature goes to
infinity the RelaxedBernoulli becomes the constant distribution that is identically 0.5.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_relaxed_bernoulli(
  temperature,
  logits = NULL,
  probs = NULL,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "RelaxedBernoulli"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_relaxed_bernoulli_+3A_temperature">temperature</code></td>
<td>
<p>An 0-D Tensor, representing the temperature of a set of RelaxedBernoulli distributions.
The temperature should be positive.</p>
</td></tr>
<tr><td><code id="tfd_relaxed_bernoulli_+3A_logits">logits</code></td>
<td>
<p>An N-D Tensor representing the log-odds of a positive event. Each entry in the Tensor
parametrizes an independent RelaxedBernoulli distribution where the probability of an event is sigmoid(logits).
Only one of logits or probs should be passed in.</p>
</td></tr>
<tr><td><code id="tfd_relaxed_bernoulli_+3A_probs">probs</code></td>
<td>
<p>AAn N-D Tensor representing the probability of a positive event. Each entry in the Tensor parameterizes
an independent Bernoulli distribution. Only one of logits or probs should be passed in.</p>
</td></tr>
<tr><td><code id="tfd_relaxed_bernoulli_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_relaxed_bernoulli_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_relaxed_bernoulli_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The RelaxedBernoulli distribution is a reparameterized continuous
distribution that is the binary special case of the RelaxedOneHotCategorical
distribution (Maddison et al., 2016; Jang et al., 2016). For details on the
binary special case see the appendix of Maddison et al. (2016) where it is
referred to as BinConcrete. If you use this distribution, please cite both papers.
</p>
<p>Some care needs to be taken for loss functions that depend on the
log-probability of RelaxedBernoullis, because computing log-probabilities of
the RelaxedBernoulli can suffer from underflow issues. In many case loss
functions such as these are invariant under invertible transformations of
the random variables. The KL divergence, found in the variational autoencoder
loss, is an example. Because RelaxedBernoullis are sampled by a Logistic
random variable followed by a <code>tf$sigmoid</code> op, one solution is to treat
the Logistic as the random variable and <code>tf$sigmoid</code> as downstream. The
KL divergences of two Logistics, which are always followed by a <code>tf.sigmoid</code>
op, is equivalent to evaluating KL divergences of RelaxedBernoulli samples.
See Maddison et al., 2016 for more details where this distribution is called
the BinConcrete.
An alternative approach is to evaluate Bernoulli log probability or KL
directly on relaxed samples, as done in Jang et al., 2016. In this case,
guarantees on the loss are usually violated. For instance, using a Bernoulli
KL in a relaxed ELBO is no longer a lower bound on the log marginal
probability of the observation. Thus care and early stopping are important.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_relaxed_one_hot_categorical'>RelaxedOneHotCategorical distribution with temperature and logits</h2><span id='topic+tfd_relaxed_one_hot_categorical'></span>

<h3>Description</h3>

<p>The RelaxedOneHotCategorical is a distribution over random probability
vectors, vectors of positive real values that sum to one, which continuously
approximates a OneHotCategorical. The degree of approximation is controlled by
a temperature: as the temperature goes to 0 the RelaxedOneHotCategorical
becomes discrete with a distribution described by the <code>logits</code> or <code>probs</code>
parameters, as the temperature goes to infinity the RelaxedOneHotCategorical
becomes the constant distribution that is identically the constant vector of
(1/event_size, ..., 1/event_size).
The RelaxedOneHotCategorical distribution was concurrently introduced as the
Gumbel-Softmax (Jang et al., 2016) and Concrete (Maddison et al., 2016)
distributions for use as a reparameterized continuous approximation to the
<code>Categorical</code> one-hot distribution. If you use this distribution, please cite
both papers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_relaxed_one_hot_categorical(
  temperature,
  logits = NULL,
  probs = NULL,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "RelaxedOneHotCategorical"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_relaxed_one_hot_categorical_+3A_temperature">temperature</code></td>
<td>
<p>An 0-D Tensor, representing the temperature of a set of RelaxedOneHotCategorical distributions.
The temperature should be positive.</p>
</td></tr>
<tr><td><code id="tfd_relaxed_one_hot_categorical_+3A_logits">logits</code></td>
<td>
<p>An N-D Tensor, N &gt;= 1, representing the log probabilities of a set of RelaxedOneHotCategorical
distributions. The first N - 1 dimensions index into a batch of independent distributions and the last dimension
represents a vector of logits for each class. Only one of logits or probs should be passed in.</p>
</td></tr>
<tr><td><code id="tfd_relaxed_one_hot_categorical_+3A_probs">probs</code></td>
<td>
<p>An N-D Tensor, N &gt;= 1, representing the probabilities of a set of RelaxedOneHotCategorical distributions.
The first N - 1 dimensions index into a batch of independent distributions and the last dimension represents a vector
of probabilities for each class. Only one of logits or probs should be passed in.</p>
</td></tr>
<tr><td><code id="tfd_relaxed_one_hot_categorical_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_relaxed_one_hot_categorical_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_relaxed_one_hot_categorical_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>References</h3>


<ul>
<li><p> Eric Jang, Shixiang Gu, and Ben Poole. Categorical Reparameterization with
Gumbel-Softmax. 2016.
</p>
</li>
<li><p> Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The Concrete Distribution:
A Continuous Relaxation of Discrete Random Variables. 2016.
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_sample'>Generate samples of the specified shape.</h2><span id='topic+tfd_sample'></span>

<h3>Description</h3>

<p>Note that a call to <code>tfd_sample()</code> without arguments will generate a single sample.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_sample(distribution, sample_shape = list(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_sample_+3A_distribution">distribution</code></td>
<td>
<p>The distribution being used.</p>
</td></tr>
<tr><td><code id="tfd_sample_+3A_sample_shape">sample_shape</code></td>
<td>
<p>0D or 1D int32 Tensor. Shape of the generated samples.</p>
</td></tr>
<tr><td><code id="tfd_sample_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to Python.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Tensor with prepended dimensions sample_shape.
</p>


<h3>See Also</h3>

<p>Other distribution_methods: 
<code><a href="#topic+tfd_cdf">tfd_cdf</a>()</code>,
<code><a href="#topic+tfd_covariance">tfd_covariance</a>()</code>,
<code><a href="#topic+tfd_cross_entropy">tfd_cross_entropy</a>()</code>,
<code><a href="#topic+tfd_entropy">tfd_entropy</a>()</code>,
<code><a href="#topic+tfd_kl_divergence">tfd_kl_divergence</a>()</code>,
<code><a href="#topic+tfd_log_cdf">tfd_log_cdf</a>()</code>,
<code><a href="#topic+tfd_log_prob">tfd_log_prob</a>()</code>,
<code><a href="#topic+tfd_log_survival_function">tfd_log_survival_function</a>()</code>,
<code><a href="#topic+tfd_mean">tfd_mean</a>()</code>,
<code><a href="#topic+tfd_mode">tfd_mode</a>()</code>,
<code><a href="#topic+tfd_prob">tfd_prob</a>()</code>,
<code><a href="#topic+tfd_quantile">tfd_quantile</a>()</code>,
<code><a href="#topic+tfd_stddev">tfd_stddev</a>()</code>,
<code><a href="#topic+tfd_survival_function">tfd_survival_function</a>()</code>,
<code><a href="#topic+tfd_variance">tfd_variance</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  d &lt;- tfd_normal(loc = c(1, 2), scale = c(1, 0.5))
  d %&gt;% tfd_sample()

</code></pre>

<hr>
<h2 id='tfd_sample_distribution'>Sample distribution via independent draws.</h2><span id='topic+tfd_sample_distribution'></span>

<h3>Description</h3>

<p>This distribution is useful for reducing over a collection of independent,
identical draws. It is otherwise identical to the input distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_sample_distribution(
  distribution,
  sample_shape = list(),
  validate_args = FALSE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_sample_distribution_+3A_distribution">distribution</code></td>
<td>
<p>The base distribution instance to transform. Typically an
instance of <code>Distribution</code>.</p>
</td></tr>
<tr><td><code id="tfd_sample_distribution_+3A_sample_shape">sample_shape</code></td>
<td>
<p><code>integer</code> scalar or vector <code>Tensor</code> representing the shape of a
single sample.</p>
</td></tr>
<tr><td><code id="tfd_sample_distribution_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_sample_distribution_+3A_name">name</code></td>
<td>
<p>The name for ops managed by the distribution.
Default value: <code>NULL</code> (i.e., <code>'Sample' + distribution$name</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
The probability function is,
</p>
<div class="sourceCode"><pre>p(x) = prod{ p(x[i]) : i = 0, ..., (n - 1) }
</pre></div>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_sinh_arcsinh'>The SinhArcsinh transformation of a distribution on <code style="white-space: pre;">&#8288;(-inf, inf)&#8288;</code></h2><span id='topic+tfd_sinh_arcsinh'></span>

<h3>Description</h3>

<p>This distribution models a random variable, making use of
a <code>SinhArcsinh</code> transformation (which has adjustable tailweight and skew),
a rescaling, and a shift.
The <code>SinhArcsinh</code> transformation of the Normal is described in great depth in
<a href="https://oro.open.ac.uk/22510/">Sinh-arcsinh distributions</a>.
Here we use a slightly different parameterization, in terms of <code>tailweight</code>
and <code>skewness</code>.  Additionally we allow for distributions other than Normal,
and control over <code>scale</code> as well as a &quot;shift&quot; parameter <code>loc</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_sinh_arcsinh(
  loc,
  scale,
  skewness = NULL,
  tailweight = NULL,
  distribution = NULL,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "SinhArcsinh"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_sinh_arcsinh_+3A_loc">loc</code></td>
<td>
<p>Floating-point <code>Tensor</code>.</p>
</td></tr>
<tr><td><code id="tfd_sinh_arcsinh_+3A_scale">scale</code></td>
<td>
<p><code>Tensor</code> of same <code>dtype</code> as <code>loc</code>.</p>
</td></tr>
<tr><td><code id="tfd_sinh_arcsinh_+3A_skewness">skewness</code></td>
<td>
<p>Skewness parameter.  Default is <code>0.0</code> (no skew).</p>
</td></tr>
<tr><td><code id="tfd_sinh_arcsinh_+3A_tailweight">tailweight</code></td>
<td>
<p>Tailweight parameter. Default is <code>1.0</code> (unchanged tailweight)</p>
</td></tr>
<tr><td><code id="tfd_sinh_arcsinh_+3A_distribution">distribution</code></td>
<td>
<p><code>tf$distributions$Distribution</code>-like instance. Distribution that is
transformed to produce this distribution. Default is <code>tfd_normal(0, 1)</code>.
Must be a scalar-batch, scalar-event distribution.  Typically
<code>distribution$reparameterization_type = FULLY_REPARAMETERIZED</code> or it is
a function of non-trainable parameters. WARNING: If you backprop through
a <code>SinhArcsinh</code> sample and <code>distribution</code> is not
<code>FULLY_REPARAMETERIZED</code> yet is a function of trainable variables, then
the gradient will be incorrect!</p>
</td></tr>
<tr><td><code id="tfd_sinh_arcsinh_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_sinh_arcsinh_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_sinh_arcsinh_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>Given random variable <code>Z</code>, we define the SinhArcsinh
transformation of <code>Z</code>, <code>Y</code>, parameterized by
<code style="white-space: pre;">&#8288;(loc, scale, skewness, tailweight)&#8288;</code>, via the relation:
</p>
<div class="sourceCode"><pre>Y := loc + scale * F(Z) * (2 / F_0(2))
F(Z) := Sinh( (Arcsinh(Z) + skewness) * tailweight )
F_0(Z) := Sinh( Arcsinh(Z) * tailweight )
</pre></div>
<p>This distribution is similar to the location-scale transformation
<code>L(Z) := loc + scale * Z</code> in the following ways:
</p>

<ul>
<li><p> If <code>skewness = 0</code> and <code>tailweight = 1</code> (the defaults), <code>F(Z) = Z</code>, and then
<code>Y = L(Z)</code> exactly.
</p>
</li>
<li> <p><code>loc</code> is used in both to shift the result by a constant factor.
</p>
</li>
<li><p> The multiplication of <code>scale</code> by <code>2 / F_0(2)</code> ensures that if <code>skewness = 0</code>
<code>P[Y - loc &lt;= 2 * scale] = P[L(Z) - loc &lt;= 2 * scale]</code>.
Thus it can be said that the weights in the tails of <code>Y</code> and <code>L(Z)</code> beyond
<code>loc + 2 * scale</code> are the same.
</p>
</li></ul>

<p>This distribution is different than <code>loc + scale * Z</code> due to the
reshaping done by <code>F</code>:
</p>

<ul>
<li><p> Positive (negative) <code>skewness</code> leads to positive (negative) skew.
</p>
</li>
<li><p> positive skew means, the mode of <code>F(Z)</code> is &quot;tilted&quot; to the right.
</p>
</li>
<li><p> positive skew means positive values of <code>F(Z)</code> become more likely, and
negative values become less likely.
</p>
</li>
<li><p> Larger (smaller) <code>tailweight</code> leads to fatter (thinner) tails.
</p>
</li>
<li><p> Fatter tails mean larger values of <code style="white-space: pre;">&#8288;|F(Z)|&#8288;</code> become more likely.
</p>
</li>
<li> <p><code>tailweight &lt; 1</code> leads to a distribution that is &quot;flat&quot; around <code>Y = loc</code>,
and a very steep drop-off in the tails.
</p>
</li>
<li> <p><code>tailweight &gt; 1</code> leads to a distribution more peaked at the mode with
heavier tails.
</p>
</li></ul>

<p>To see the argument about the tails, note that for <code style="white-space: pre;">&#8288;|Z| &gt;&gt; 1&#8288;</code> and
<code style="white-space: pre;">&#8288;|Z| &gt;&gt; (|skewness| * tailweight)**tailweight&#8288;</code>, we have
<code style="white-space: pre;">&#8288;Y approx 0.5 Z**tailweight e**(sign(Z) skewness * tailweight)&#8288;</code>.
</p>
<p>To see the argument regarding multiplying <code>scale</code> by <code>2 / F_0(2)</code>,
</p>
<div class="sourceCode"><pre>P[(Y - loc) / scale &lt;= 2] = P[F(Z) * (2 / F_0(2)) &lt;= 2]
                          = P[F(Z) &lt;= F_0(2)]
                          = P[Z &lt;= 2]  (if F = F_0).
</pre></div>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_skellam'>Skellam distribution.</h2><span id='topic+tfd_skellam'></span>

<h3>Description</h3>

<p>The Skellam distribution is parameterized by two rate parameters,
<code>rate1</code> and <code>rate2</code>. Its samples are defined as:
</p>
<div class="sourceCode"><pre>x ~ Poisson(rate1)
y ~ Poisson(rate2)
z = x - y
z ~ Skellam(rate1, rate2)
</pre></div>
<p>where the samples <code>x</code> and <code>y</code> are assumed to be independent.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_skellam(
  rate1 = NULL,
  rate2 = NULL,
  log_rate1 = NULL,
  log_rate2 = NULL,
  force_probs_to_zero_outside_support = FALSE,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Skellam"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_skellam_+3A_rate1">rate1</code></td>
<td>
<p>Floating point tensor, the first rate parameter. <code>rate1</code> must be
positive. Must specify exactly one of <code>rate1</code> and <code>log_rate1</code></p>
</td></tr>
<tr><td><code id="tfd_skellam_+3A_rate2">rate2</code></td>
<td>
<p>Floating point tensor, the second rate parameter. <code>rate</code> must be
positive.  Must specify exactly one of <code>rate2</code> and <code>log_rate2</code>.</p>
</td></tr>
<tr><td><code id="tfd_skellam_+3A_log_rate1">log_rate1</code></td>
<td>
<p>Floating point tensor, the log of the first rate parameter.
Must specify exactly one of <code>rate1</code> and <code>log_rate1</code>.</p>
</td></tr>
<tr><td><code id="tfd_skellam_+3A_log_rate2">log_rate2</code></td>
<td>
<p>Floating point tensor, the log of the second rate parameter.
Must specify exactly one of <code>rate2</code> and <code>log_rate2</code>.</p>
</td></tr>
<tr><td><code id="tfd_skellam_+3A_force_probs_to_zero_outside_support">force_probs_to_zero_outside_support</code></td>
<td>
<p>logical. When <code>TRUE</code>,
<code>log_prob</code> returns <code>-inf</code> (and <code>prob</code> returns <code>0</code>) for non-integer
inputs. When <code>FALSE</code>, <code>log_prob</code> evaluates the Skellam pmf as a
continuous function (note that this function is not itself
a normalized probability log-density). Default value: <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="tfd_skellam_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_skellam_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_skellam_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
The probability mass function (pmf) is,
</p>
<div class="sourceCode"><pre>pmf(k; l1, l2) = (l1 / l2) ** (k / 2) * I_k(2 * sqrt(l1 * l2)) / Z
Z = exp(l1 + l2).
</pre></div>
<p>where <code>rate1 = l1</code>, <code>rate2 = l2</code>,  <code>Z</code> is the normalizing constant
and <code>I_k</code> is the modified bessel function of the first kind.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_spherical_uniform'>The uniform distribution over unit vectors on <code>S^{n-1}</code>.</h2><span id='topic+tfd_spherical_uniform'></span>

<h3>Description</h3>

<p>The uniform distribution on the unit hypersphere <code>S^{n-1}</code> embedded in
<code>n</code> dimensions (<code>R^n</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_spherical_uniform(
  dimension,
  batch_shape = list(),
  dtype = tf$float32,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "SphericalUniform"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_spherical_uniform_+3A_dimension">dimension</code></td>
<td>
<p><code>integer</code>. The dimension of the embedded space where the
sphere resides.</p>
</td></tr>
<tr><td><code id="tfd_spherical_uniform_+3A_batch_shape">batch_shape</code></td>
<td>
<p>Positive <code>integer</code>-like vector-shaped <code>Tensor</code> representing
the new shape of the batch dimensions. Default value: [].</p>
</td></tr>
<tr><td><code id="tfd_spherical_uniform_+3A_dtype">dtype</code></td>
<td>
<p>dtype of the generated samples.</p>
</td></tr>
<tr><td><code id="tfd_spherical_uniform_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_spherical_uniform_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_spherical_uniform_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical details
</p>
<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; n) = 1. / A(n)
where,
   A(n) = 2 * pi^{n / 2} / Gamma(n / 2),
   Gamma being the Gamma function.
</pre></div>
<p>where <code>n = dimension</code>; corresponds to <code>S^{n-1}</code> embedded in <code>R^n</code>.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_stddev'>Standard deviation.</h2><span id='topic+tfd_stddev'></span>

<h3>Description</h3>

<p>Standard deviation is defined as, stddev = <code>E[(X - E[X])**2]**0.5</code>
#' where X is the random variable associated with this distribution, E denotes expectation,
and <code>Var$shape = batch_shape + event_shape</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_stddev(distribution, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_stddev_+3A_distribution">distribution</code></td>
<td>
<p>The distribution being used.</p>
</td></tr>
<tr><td><code id="tfd_stddev_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to Python.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Tensor of shape <code>sample_shape(x) + self$batch_shape</code> with values of type <code>self$dtype</code>.
</p>


<h3>See Also</h3>

<p>Other distribution_methods: 
<code><a href="#topic+tfd_cdf">tfd_cdf</a>()</code>,
<code><a href="#topic+tfd_covariance">tfd_covariance</a>()</code>,
<code><a href="#topic+tfd_cross_entropy">tfd_cross_entropy</a>()</code>,
<code><a href="#topic+tfd_entropy">tfd_entropy</a>()</code>,
<code><a href="#topic+tfd_kl_divergence">tfd_kl_divergence</a>()</code>,
<code><a href="#topic+tfd_log_cdf">tfd_log_cdf</a>()</code>,
<code><a href="#topic+tfd_log_prob">tfd_log_prob</a>()</code>,
<code><a href="#topic+tfd_log_survival_function">tfd_log_survival_function</a>()</code>,
<code><a href="#topic+tfd_mean">tfd_mean</a>()</code>,
<code><a href="#topic+tfd_mode">tfd_mode</a>()</code>,
<code><a href="#topic+tfd_prob">tfd_prob</a>()</code>,
<code><a href="#topic+tfd_quantile">tfd_quantile</a>()</code>,
<code><a href="#topic+tfd_sample">tfd_sample</a>()</code>,
<code><a href="#topic+tfd_survival_function">tfd_survival_function</a>()</code>,
<code><a href="#topic+tfd_variance">tfd_variance</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  d &lt;- tfd_normal(loc = c(1, 2), scale = c(1, 0.5))
  d %&gt;% tfd_stddev()

</code></pre>

<hr>
<h2 id='tfd_student_t'>Student's t-distribution</h2><span id='topic+tfd_student_t'></span>

<h3>Description</h3>

<p>This distribution has parameters: degree of freedom <code>df</code>, location <code>loc</code>, and <code>scale</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_student_t(
  df,
  loc,
  scale,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "StudentT"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_student_t_+3A_df">df</code></td>
<td>
<p>Floating-point <code>Tensor</code>. The degrees of freedom of the
distribution(s). <code>df</code> must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_student_t_+3A_loc">loc</code></td>
<td>
<p>Floating-point <code>Tensor</code>. The mean(s) of the distribution(s).</p>
</td></tr>
<tr><td><code id="tfd_student_t_+3A_scale">scale</code></td>
<td>
<p>Floating-point <code>Tensor</code>. The scaling factor(s) for the
distribution(s). Note that <code>scale</code> is not technically the standard
deviation of this distribution but has semantics more similar to
standard deviation than variance.</p>
</td></tr>
<tr><td><code id="tfd_student_t_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_student_t_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_student_t_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical details
</p>
<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; df, mu, sigma) = (1 + y**2 / df)**(-0.5 (df + 1)) / Z
where,
y = (x - mu) / sigma
Z = abs(sigma) sqrt(df pi) Gamma(0.5 df) / Gamma(0.5 (df + 1))
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>loc = mu</code>,
</p>
</li>
<li> <p><code>scale = sigma</code>, and,
</p>
</li>
<li> <p><code>Z</code> is the normalization constant, and,
</p>
</li>
<li> <p><code>Gamma</code> is the <a href="https://en.wikipedia.org/wiki/Gamma_function">gamma function</a>.
The StudentT distribution is a member of the <a href="https://en.wikipedia.org/wiki/Location-scale_family">location-scale family</a>, i.e., it can be
constructed as,
</p>
</li></ul>

<div class="sourceCode"><pre>X ~ StudentT(df, loc=0, scale=1)
Y = loc + scale * X
</pre></div>
<p>Notice that <code>scale</code> has semantics more similar to standard deviation than
variance. However it is not actually the std. deviation; the Student's
t-distribution std. dev. is <code style="white-space: pre;">&#8288;scale sqrt(df / (df - 2))&#8288;</code> when <code>df &gt; 2</code>.
</p>
<p>Samples of this distribution are reparameterized (pathwise differentiable).
The derivatives are computed using the approach described in the paper
<a href="https://arxiv.org/abs/1805.08498">Michael Figurnov, Shakir Mohamed, Andriy Mnih. Implicit Reparameterization Gradients, 2018</a>
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_student_t_process'>Marginal distribution of a Student's T process at finitely many points</h2><span id='topic+tfd_student_t_process'></span>

<h3>Description</h3>

<p>A Student's T process (TP) is an indexed collection of random variables, any
finite collection of which are jointly Multivariate Student's T. While this
definition applies to finite index sets, it is typically implicit that the
index set is infinite; in applications, it is often some finite dimensional
real or complex vector space. In such cases, the TP may be thought of as a
distribution over (real- or complex-valued) functions defined over the index set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_student_t_process(
  df,
  kernel,
  index_points,
  mean_fn = NULL,
  jitter = 1e-06,
  validate_args = FALSE,
  allow_nan_stats = FALSE,
  name = "StudentTProcess"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_student_t_process_+3A_df">df</code></td>
<td>
<p>Positive Floating-point <code>Tensor</code> representing the degrees of freedom.
Must be greater than 2.</p>
</td></tr>
<tr><td><code id="tfd_student_t_process_+3A_kernel">kernel</code></td>
<td>
<p><code>PositiveSemidefiniteKernel</code>-like instance representing the
TP's covariance function.</p>
</td></tr>
<tr><td><code id="tfd_student_t_process_+3A_index_points">index_points</code></td>
<td>
<p><code>float</code> <code>Tensor</code> representing finite (batch of) vector(s) of
points in the index set over which the TP is defined. Shape has the form
<code style="white-space: pre;">&#8288;[b1, ..., bB, e, f1, ..., fF]&#8288;</code> where <code>F</code> is the number of feature
dimensions and must equal <code>kernel.feature_ndims</code> and <code>e</code> is the number
(size) of index points in each batch. Ultimately this distribution
corresponds to a <code>e</code>-dimensional multivariate Student's T. The batch
shape must be broadcastable with <code>kernel.batch_shape</code> and any batch dims
yielded by <code>mean_fn</code>.</p>
</td></tr>
<tr><td><code id="tfd_student_t_process_+3A_mean_fn">mean_fn</code></td>
<td>
<p>Function that acts on <code>index_points</code> to produce a (batch
of) vector(s) of mean values at <code>index_points</code>. Takes a <code>Tensor</code> of
shape <code style="white-space: pre;">&#8288;[b1, ..., bB, f1, ..., fF]&#8288;</code> and returns a <code>Tensor</code> whose shape is
broadcastable with <code style="white-space: pre;">&#8288;[b1, ..., bB]&#8288;</code>. Default value: <code>NULL</code> implies
constant zero function.</p>
</td></tr>
<tr><td><code id="tfd_student_t_process_+3A_jitter">jitter</code></td>
<td>
<p><code>float</code> scalar <code>Tensor</code> added to the diagonal of the covariance
matrix to ensure positive definiteness of the covariance matrix. Default value: <code>1e-6</code>.</p>
</td></tr>
<tr><td><code id="tfd_student_t_process_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_student_t_process_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_student_t_process_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Just as Student's T distributions are fully specified by their degrees of
freedom, location and scale, a Student's T process can be completely specified
by a degrees of freedom parameter, mean function and covariance function.
</p>
<p>Let <code>S</code> denote the index set and <code>K</code> the space in which each indexed random variable
takes its values (again, often R or C).
The mean function is then a map <code>m: S -&gt; K</code>, and the covariance function,
or kernel, is a positive-definite function <code style="white-space: pre;">&#8288;k: (S x S) -&gt; K&#8288;</code>. The properties
of functions drawn from a TP are entirely dictated (up to translation) by
the form of the kernel function.
</p>
<p>This <code>Distribution</code> represents the marginal joint distribution over function
values at a given finite collection of points <code style="white-space: pre;">&#8288;[x[1], ..., x[N]]&#8288;</code> from the
index set <code>S</code>. By definition, this marginal distribution is just a
multivariate Student's T distribution, whose mean is given by the vector
<code style="white-space: pre;">&#8288;[ m(x[1]), ..., m(x[N]) ]&#8288;</code> and whose covariance matrix is constructed from
pairwise applications of the kernel function to the given inputs:
</p>
<div class="sourceCode"><pre>| k(x[1], x[1])    k(x[1], x[2])  ...  k(x[1], x[N]) |
| k(x[2], x[1])    k(x[2], x[2])  ...  k(x[2], x[N]) |
|      ...              ...                 ...      |
| k(x[N], x[1])    k(x[N], x[2])  ...  k(x[N], x[N]) |

</pre></div>
<p>For this to be a valid covariance matrix, it must be symmetric and positive
definite; hence the requirement that <code>k</code> be a positive definite function
(which, by definition, says that the above procedure will yield PD matrices).
Note also we use a parameterization as suggested in Shat et al. (2014), which requires <code>df</code>
to be greater than 2. This allows for the covariance for any finite
dimensional marginal of the TP (a multivariate Student's T distribution) to
just be the PD matrix generated by the kernel.
</p>
<p>Mathematical Details
</p>
<p>The probability density function (pdf) is a multivariate Student's T whose
parameters are derived from the TP's properties:
</p>
<div class="sourceCode"><pre>pdf(x; df, index_points, mean_fn, kernel) = MultivariateStudentT(df, loc, K)
K = (df - 2) / df  * (kernel.matrix(index_points, index_points) + jitter * eye(N))
loc = (x - mean_fn(index_points))^T @ K @ (x - mean_fn(index_points))
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>df</code> is the degrees of freedom parameter for the TP.
</p>
</li>
<li> <p><code>index_points</code> are points in the index set over which the TP is defined,
</p>
</li>
<li> <p><code>mean_fn</code> is a callable mapping the index set to the TP's mean values,
</p>
</li>
<li> <p><code>kernel</code> is <code>PositiveSemidefiniteKernel</code>-like and represents the covariance
function of the TP,
</p>
</li>
<li> <p><code>jitter</code> is added to the diagonal to ensure positive definiteness up to
machine precision (otherwise Cholesky-decomposition is prone to failure),
</p>
</li>
<li> <p><code>eye(N)</code> is an N-by-N identity matrix.
</p>
</li></ul>



<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://www.cs.cmu.edu/~andrewgw/tprocess.pdf">Amar Shah, Andrew Gordon Wilson, and Zoubin Ghahramani. Student-t Processes as Alternatives to Gaussian Processes. In <em>Artificial Intelligence and Statistics</em>, 2014.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_survival_function'>Survival function.</h2><span id='topic+tfd_survival_function'></span>

<h3>Description</h3>

<p>Given random variable X, the survival function is defined:
<code>tfd_survival_function(x) = P[X &gt; x] = 1 - P[X &lt;= x] = 1 - cdf(x)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_survival_function(distribution, value, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_survival_function_+3A_distribution">distribution</code></td>
<td>
<p>The distribution being used.</p>
</td></tr>
<tr><td><code id="tfd_survival_function_+3A_value">value</code></td>
<td>
<p>float or double Tensor.</p>
</td></tr>
<tr><td><code id="tfd_survival_function_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to Python.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Tensor of shape <code>sample_shape(x) + self$batch_shape</code> with values of type <code>self$dtype</code>.
</p>


<h3>See Also</h3>

<p>Other distribution_methods: 
<code><a href="#topic+tfd_cdf">tfd_cdf</a>()</code>,
<code><a href="#topic+tfd_covariance">tfd_covariance</a>()</code>,
<code><a href="#topic+tfd_cross_entropy">tfd_cross_entropy</a>()</code>,
<code><a href="#topic+tfd_entropy">tfd_entropy</a>()</code>,
<code><a href="#topic+tfd_kl_divergence">tfd_kl_divergence</a>()</code>,
<code><a href="#topic+tfd_log_cdf">tfd_log_cdf</a>()</code>,
<code><a href="#topic+tfd_log_prob">tfd_log_prob</a>()</code>,
<code><a href="#topic+tfd_log_survival_function">tfd_log_survival_function</a>()</code>,
<code><a href="#topic+tfd_mean">tfd_mean</a>()</code>,
<code><a href="#topic+tfd_mode">tfd_mode</a>()</code>,
<code><a href="#topic+tfd_prob">tfd_prob</a>()</code>,
<code><a href="#topic+tfd_quantile">tfd_quantile</a>()</code>,
<code><a href="#topic+tfd_sample">tfd_sample</a>()</code>,
<code><a href="#topic+tfd_stddev">tfd_stddev</a>()</code>,
<code><a href="#topic+tfd_variance">tfd_variance</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  d &lt;- tfd_normal(loc = c(1, 2), scale = c(1, 0.5))
  x &lt;- d %&gt;% tfd_sample()
  d %&gt;% tfd_survival_function(x)

</code></pre>

<hr>
<h2 id='tfd_transformed_distribution'>A Transformed Distribution</h2><span id='topic+tfd_transformed_distribution'></span>

<h3>Description</h3>

<p>A TransformedDistribution models <code>p(y)</code> given a base distribution <code>p(x)</code>,
and a deterministic, invertible, differentiable transform,<code>Y = g(X)</code>. The
transform is typically an instance of the Bijector class and the base
distribution is typically an instance of the Distribution class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_transformed_distribution(
  distribution,
  bijector,
  batch_shape = NULL,
  event_shape = NULL,
  kwargs_split_fn = NULL,
  validate_args = FALSE,
  parameters = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_transformed_distribution_+3A_distribution">distribution</code></td>
<td>
<p>The base distribution instance to transform. Typically an instance of Distribution.</p>
</td></tr>
<tr><td><code id="tfd_transformed_distribution_+3A_bijector">bijector</code></td>
<td>
<p>The object responsible for calculating the transformation. Typically an instance of Bijector.</p>
</td></tr>
<tr><td><code id="tfd_transformed_distribution_+3A_batch_shape">batch_shape</code></td>
<td>
<p>integer vector Tensor which overrides distribution batch_shape;
valid only if distribution.is_scalar_batch().</p>
</td></tr>
<tr><td><code id="tfd_transformed_distribution_+3A_event_shape">event_shape</code></td>
<td>
<p>integer vector Tensor which overrides distribution event_shape;
valid only if distribution.is_scalar_event().</p>
</td></tr>
<tr><td><code id="tfd_transformed_distribution_+3A_kwargs_split_fn">kwargs_split_fn</code></td>
<td>
<p>Python <code>callable</code> which takes a kwargs <code>dict</code> and returns
a tuple of kwargs <code>dict</code>s for each of the <code>distribution</code> and <code>bijector</code>
parameters respectively. Default value: <code style="white-space: pre;">&#8288;_default_kwargs_split_fn&#8288;</code> (i.e.,
<code style="white-space: pre;">&#8288;lambda kwargs: (kwargs.get('distribution_kwargs', {}), kwargs.get('bijector_kwargs', {}))&#8288;</code>)</p>
</td></tr>
<tr><td><code id="tfd_transformed_distribution_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_transformed_distribution_+3A_parameters">parameters</code></td>
<td>
<p>Locals dict captured by subclass constructor, to be used for
copy/slice re-instantiation operations.</p>
</td></tr>
<tr><td><code id="tfd_transformed_distribution_+3A_name">name</code></td>
<td>
<p>The name for ops managed by the distribution.  Default value: bijector.name + distribution.name.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A <code>Bijector</code> is expected to implement the following functions:
</p>

<ul>
<li> <p><code>forward</code>,
</p>
</li>
<li> <p><code>inverse</code>,
</p>
</li>
<li> <p><code>inverse_log_det_jacobian</code>.
</p>
</li></ul>

<p>The semantics of these functions are outlined in the <code>Bijector</code> documentation.
</p>
<p>We now describe how a <code>TransformedDistribution</code> alters the input/outputs of a
<code>Distribution</code> associated with a random variable (rv) <code>X</code>.
Write <code>cdf(Y=y)</code> for an absolutely continuous cumulative distribution function
of random variable <code>Y</code>; write the probability density function
<code style="white-space: pre;">&#8288;pdf(Y=y) := d^k / (dy_1,...,dy_k) cdf(Y=y)&#8288;</code> for its derivative wrt to <code>Y</code> evaluated at
<code>y</code>. Assume that <code>Y = g(X)</code> where <code>g</code> is a deterministic diffeomorphism,
i.e., a non-random, continuous, differentiable, and invertible function.
Write the inverse of <code>g</code> as <code>X = g^{-1}(Y)</code> and <code style="white-space: pre;">&#8288;(J o g)(x)&#8288;</code> for the Jacobian
of <code>g</code> evaluated at <code>x</code>.
</p>
<p>A <code>TransformedDistribution</code> implements the following operations:
</p>

<ul>
<li> <p><code>sample</code>
Mathematically:   <code>Y = g(X)</code>
Programmatically: <code>bijector.forward(distribution.sample(...))</code>
</p>
</li>
<li> <p><code>log_prob</code>
Mathematically:   <code style="white-space: pre;">&#8288;(log o pdf)(Y=y) = (log o pdf o g^{-1})(y) + (log o abs o det o J o g^{-1})(y)&#8288;</code>
Programmatically: <code>(distribution.log_prob(bijector.inverse(y)) + bijector.inverse_log_det_jacobian(y))</code>
</p>
</li>
<li> <p><code>log_cdf</code>
Mathematically:   <code style="white-space: pre;">&#8288;(log o cdf)(Y=y) = (log o cdf o g^{-1})(y)&#8288;</code>
Programmatically: <code>distribution.log_cdf(bijector.inverse(x))</code>
</p>
</li>
<li><p> and similarly for: <code>cdf</code>, <code>prob</code>, <code>log_survival_function</code>, <code>survival_function</code>.
</p>
</li></ul>



<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_triangular'>Triangular distribution with <code>low</code>, <code>high</code> and <code>peak</code> parameters</h2><span id='topic+tfd_triangular'></span>

<h3>Description</h3>

<p>The parameters <code>low</code>, <code>high</code> and <code>peak</code> must be shaped in a way that supports
broadcasting (e.g., <code>high - low</code> is a valid operation).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_triangular(
  low = 0,
  high = 1,
  peak = 0.5,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Triangular"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_triangular_+3A_low">low</code></td>
<td>
<p>Floating point tensor, lower boundary of the output interval. Must
have <code>low &lt; high</code>. Default value: <code>0</code>.</p>
</td></tr>
<tr><td><code id="tfd_triangular_+3A_high">high</code></td>
<td>
<p>Floating point tensor, upper boundary of the output interval. Must
have <code>low &lt; high</code>. Default value: <code>1</code>.</p>
</td></tr>
<tr><td><code id="tfd_triangular_+3A_peak">peak</code></td>
<td>
<p>Floating point tensor, mode of the output interval. Must have
<code>low &lt;= peak</code> and <code>peak &lt;= high</code>. Default value: <code>0.5</code>.</p>
</td></tr>
<tr><td><code id="tfd_triangular_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_triangular_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_triangular_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_truncated_cauchy'>The Truncated Cauchy distribution.</h2><span id='topic+tfd_truncated_cauchy'></span>

<h3>Description</h3>

<p>The truncated Cauchy is a Cauchy distribution bounded between <code>low</code>
and <code>high</code> (the pdf is 0 outside these bounds and renormalized).
Samples from this distribution are differentiable with respect to <code>loc</code>
and <code>scale</code>, but not with respect to the bounds <code>low</code> and <code>high</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_truncated_cauchy(
  loc,
  scale,
  low,
  high,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "TruncatedCauchy"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_truncated_cauchy_+3A_loc">loc</code></td>
<td>
<p>Floating point tensor; the modes of the corresponding non-truncated
Cauchy distribution(s).</p>
</td></tr>
<tr><td><code id="tfd_truncated_cauchy_+3A_scale">scale</code></td>
<td>
<p>Floating point tensor; the scales of the distribution(s).
Must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_truncated_cauchy_+3A_low">low</code></td>
<td>
<p><code>float</code> <code>Tensor</code> representing lower bound of the distribution's
support. Must be such that <code>low &lt; high</code>.</p>
</td></tr>
<tr><td><code id="tfd_truncated_cauchy_+3A_high">high</code></td>
<td>
<p><code>float</code> <code>Tensor</code> representing upper bound of the distribution's
support. Must be such that <code>low &lt; high</code>.</p>
</td></tr>
<tr><td><code id="tfd_truncated_cauchy_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_truncated_cauchy_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_truncated_cauchy_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The probability density function (pdf) of this distribution is:
</p>
<div class="sourceCode"><pre>pdf(x; loc, scale, low, high) =
    { 1 / (pi * scale * (1 + z**2) * A) for low &lt;= x &lt;= high
    { 0                                 otherwise
    where
        z = (x - loc) / scale
        A = CauchyCDF((high - loc) / scale) - CauchyCDF((low - loc) / scale)
</pre></div>
<p>where <code>CauchyCDF</code> is the cumulative density function of the Cauchy distribution
with 0 mean and unit variance.
This is a scalar distribution so the event shape is always scalar and the
dimensions of the parameters define the batch_shape.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_truncated_normal'>Truncated Normal distribution</h2><span id='topic+tfd_truncated_normal'></span>

<h3>Description</h3>

<p>The truncated normal is a normal distribution bounded between <code>low</code>
and <code>high</code> (the pdf is 0 outside these bounds and renormalized).
Samples from this distribution are differentiable with respect to <code>loc</code>,
<code>scale</code> as well as the bounds, <code>low</code> and <code>high</code>, i.e., this
implementation is fully reparameterizeable.
For more details, see <a href="https://en.wikipedia.org/wiki/Truncated_normal_distribution">here</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_truncated_normal(
  loc,
  scale,
  low,
  high,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "TruncatedNormal"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_truncated_normal_+3A_loc">loc</code></td>
<td>
<p>Floating point tensor; the means of the distribution(s).</p>
</td></tr>
<tr><td><code id="tfd_truncated_normal_+3A_scale">scale</code></td>
<td>
<p>loating point tensor; the stddevs of the distribution(s).
Must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_truncated_normal_+3A_low">low</code></td>
<td>
<p><code>float</code> <code>Tensor</code> representing lower bound of the distribution's
support. Must be such that <code>low &lt; high</code>.</p>
</td></tr>
<tr><td><code id="tfd_truncated_normal_+3A_high">high</code></td>
<td>
<p><code>float</code> <code>Tensor</code> representing upper bound of the distribution's
support. Must be such that <code>low &lt; high</code>.</p>
</td></tr>
<tr><td><code id="tfd_truncated_normal_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_truncated_normal_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_truncated_normal_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The probability density function (pdf) of this distribution is:
</p>
<div class="sourceCode"><pre>pdf(x; loc, scale, low, high) =
  { (2 pi)**(-0.5) exp(-0.5 y**2) / (scale * z)} for low &lt;= x &lt;= high
  { 0 }                                  otherwise
y = (x - loc)/scale
z = NormalCDF((high - loc) / scale) - NormalCDF((lower - loc) / scale)
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>NormalCDF</code> is the cumulative density function of the Normal distribution
with 0 mean and unit variance.
</p>
</li></ul>

<p>This is a scalar distribution so the event shape is always scalar and the
dimensions of the parameters defined the batch_shape.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_uniform'>Uniform distribution with <code>low</code> and <code>high</code> parameters</h2><span id='topic+tfd_uniform'></span>

<h3>Description</h3>

<p>Mathematical Details
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_uniform(
  low = 0,
  high = 1,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Uniform"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_uniform_+3A_low">low</code></td>
<td>
<p>Floating point tensor, lower boundary of the output interval. Must
have <code>low &lt; high</code>.</p>
</td></tr>
<tr><td><code id="tfd_uniform_+3A_high">high</code></td>
<td>
<p>Floating point tensor, upper boundary of the output interval. Must
have <code>low &lt; high</code>.</p>
</td></tr>
<tr><td><code id="tfd_uniform_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_uniform_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_uniform_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; a, b) = I[a &lt;= x &lt; b] / Z
Z = b - a
</pre></div>
<p>where
</p>

<ul>
<li> <p><code>low = a</code>,
</p>
</li>
<li> <p><code>high = b</code>,
</p>
</li>
<li> <p><code>Z</code> is the normalizing constant, and
</p>
</li>
<li> <p><code>I[predicate]</code> is the <a href="https://en.wikipedia.org/wiki/Indicator_function">indicator function</a> for <code>predicate</code>.
</p>
</li></ul>

<p>The parameters <code>low</code> and <code>high</code> must be shaped in a way that supports
broadcasting (e.g., <code>high - low</code> is a valid operation).
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_variance'>Variance.</h2><span id='topic+tfd_variance'></span>

<h3>Description</h3>

<p>Variance is defined as, <code>Var = E[(X - E[X])**2]</code>
where X is the random variable associated with this distribution, E denotes expectation,
and <code>Var$shape = batch_shape + event_shape</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_variance(distribution, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_variance_+3A_distribution">distribution</code></td>
<td>
<p>The distribution being used.</p>
</td></tr>
<tr><td><code id="tfd_variance_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to Python.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a Tensor of shape <code>sample_shape(x) + self$batch_shape</code> with values of type <code>self$dtype</code>.
</p>


<h3>See Also</h3>

<p>Other distribution_methods: 
<code><a href="#topic+tfd_cdf">tfd_cdf</a>()</code>,
<code><a href="#topic+tfd_covariance">tfd_covariance</a>()</code>,
<code><a href="#topic+tfd_cross_entropy">tfd_cross_entropy</a>()</code>,
<code><a href="#topic+tfd_entropy">tfd_entropy</a>()</code>,
<code><a href="#topic+tfd_kl_divergence">tfd_kl_divergence</a>()</code>,
<code><a href="#topic+tfd_log_cdf">tfd_log_cdf</a>()</code>,
<code><a href="#topic+tfd_log_prob">tfd_log_prob</a>()</code>,
<code><a href="#topic+tfd_log_survival_function">tfd_log_survival_function</a>()</code>,
<code><a href="#topic+tfd_mean">tfd_mean</a>()</code>,
<code><a href="#topic+tfd_mode">tfd_mode</a>()</code>,
<code><a href="#topic+tfd_prob">tfd_prob</a>()</code>,
<code><a href="#topic+tfd_quantile">tfd_quantile</a>()</code>,
<code><a href="#topic+tfd_sample">tfd_sample</a>()</code>,
<code><a href="#topic+tfd_stddev">tfd_stddev</a>()</code>,
<code><a href="#topic+tfd_survival_function">tfd_survival_function</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  d &lt;- tfd_normal(loc = c(1, 2), scale = c(1, 0.5))
  d %&gt;% tfd_variance()

</code></pre>

<hr>
<h2 id='tfd_variational_gaussian_process'>Posterior predictive of a variational Gaussian process</h2><span id='topic+tfd_variational_gaussian_process'></span>

<h3>Description</h3>

<p>This distribution implements the variational Gaussian process (VGP), as
described in Titsias (2009) and Hensman (2013). The VGP is an
inducing point-based approximation of an exact GP posterior.
Ultimately, this Distribution class represents a marginal distribution over function values at a
collection of <code>index_points</code>. It is parameterized by
</p>

<ul>
<li><p> a kernel function,
</p>
</li>
<li><p> a mean function,
</p>
</li>
<li><p> the (scalar) observation noise variance of the normal likelihood,
</p>
</li>
<li><p> a set of index points,
</p>
</li>
<li><p> a set of inducing index points, and
</p>
</li>
<li><p> the parameters of the (full-rank, Gaussian) variational posterior
distribution over function values at the inducing points, conditional on some observations.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>tfd_variational_gaussian_process(
  kernel,
  index_points,
  inducing_index_points,
  variational_inducing_observations_loc,
  variational_inducing_observations_scale,
  mean_fn = NULL,
  observation_noise_variance = 0,
  predictive_noise_variance = 0,
  jitter = 1e-06,
  validate_args = FALSE,
  allow_nan_stats = FALSE,
  name = "VariationalGaussianProcess"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_variational_gaussian_process_+3A_kernel">kernel</code></td>
<td>
<p><code>PositiveSemidefiniteKernel</code>-like instance representing the
GP's covariance function.</p>
</td></tr>
<tr><td><code id="tfd_variational_gaussian_process_+3A_index_points">index_points</code></td>
<td>
<p><code>float</code> <code>Tensor</code> representing finite (batch of) vector(s) of
points in the index set over which the VGP is defined. Shape has the
form <code style="white-space: pre;">&#8288;[b1, ..., bB, e1, f1, ..., fF]&#8288;</code> where <code>F</code> is the number of feature
dimensions and must equal <code>kernel$feature_ndims</code> and <code>e1</code> is the number
(size) of index points in each batch (we denote it <code>e1</code> to distinguish
it from the numer of inducing index points, denoted <code>e2</code> below).
Ultimately the VariationalGaussianProcess distribution corresponds to an
<code>e1</code>-dimensional multivariate normal. The batch shape must be
broadcastable with <code>kernel$batch_shape</code>, the batch shape of
<code>inducing_index_points</code>, and any batch dims yielded by <code>mean_fn</code>.</p>
</td></tr>
<tr><td><code id="tfd_variational_gaussian_process_+3A_inducing_index_points">inducing_index_points</code></td>
<td>
<p><code>float</code> <code>Tensor</code> of locations of inducing points in
the index set. Shape has the form <code style="white-space: pre;">&#8288;[b1, ..., bB, e2, f1, ..., fF]&#8288;</code>, just
like <code>index_points</code>. The batch shape components needn't be identical to
those of <code>index_points</code>, but must be broadcast compatible with them.</p>
</td></tr>
<tr><td><code id="tfd_variational_gaussian_process_+3A_variational_inducing_observations_loc">variational_inducing_observations_loc</code></td>
<td>
<p><code>float</code> <code>Tensor</code>; the mean of the
(full-rank Gaussian) variational posterior over function values at the
inducing points, conditional on observed data. Shape has the form <code style="white-space: pre;">&#8288;[b1, ..., bB, e2]&#8288;</code>,
where <code style="white-space: pre;">&#8288;b1, ..., bB&#8288;</code> is broadcast compatible with other
parameters' batch shapes, and <code>e2</code> is the number of inducing points.</p>
</td></tr>
<tr><td><code id="tfd_variational_gaussian_process_+3A_variational_inducing_observations_scale">variational_inducing_observations_scale</code></td>
<td>
<p><code>float</code> <code>Tensor</code>; the scale
matrix of the (full-rank Gaussian) variational posterior over function
values at the inducing points, conditional on observed data. Shape has
the form <code style="white-space: pre;">&#8288;[b1, ..., bB, e2, e2]&#8288;</code>, where <code style="white-space: pre;">&#8288;b1, ..., bB&#8288;</code> is broadcast
compatible with other parameters and <code>e2</code> is the number of inducing points.</p>
</td></tr>
<tr><td><code id="tfd_variational_gaussian_process_+3A_mean_fn">mean_fn</code></td>
<td>
<p>function that acts on index points to produce a (batch
of) vector(s) of mean values at those index points. Takes a <code>Tensor</code> of
shape <code style="white-space: pre;">&#8288;[b1, ..., bB, f1, ..., fF]&#8288;</code> and returns a <code>Tensor</code> whose shape is
(broadcastable with) <code style="white-space: pre;">&#8288;[b1, ..., bB]&#8288;</code>. Default value: <code>NULL</code> implies constant zero function.</p>
</td></tr>
<tr><td><code id="tfd_variational_gaussian_process_+3A_observation_noise_variance">observation_noise_variance</code></td>
<td>
<p><code>float</code> <code>Tensor</code> representing the variance
of the noise in the Normal likelihood distribution of the model. May be
batched, in which case the batch shape must be broadcastable with the
shapes of all other batched parameters (<code>kernel$batch_shape</code>, <code>index_points</code>, etc.).
Default value: <code>0.</code></p>
</td></tr>
<tr><td><code id="tfd_variational_gaussian_process_+3A_predictive_noise_variance">predictive_noise_variance</code></td>
<td>
<p><code>float</code> <code>Tensor</code> representing additional
variance in the posterior predictive model. If <code>NULL</code>, we simply re-use
<code>observation_noise_variance</code> for the posterior predictive noise. If set
explicitly, however, we use the given value. This allows us, for
example, to omit predictive noise variance (by setting this to zero) to
obtain noiseless posterior predictions of function values, conditioned
on noisy observations.</p>
</td></tr>
<tr><td><code id="tfd_variational_gaussian_process_+3A_jitter">jitter</code></td>
<td>
<p><code>float</code> scalar <code>Tensor</code> added to the diagonal of the covariance
matrix to ensure positive definiteness of the covariance matrix. Default value: <code>1e-6</code>.</p>
</td></tr>
<tr><td><code id="tfd_variational_gaussian_process_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_variational_gaussian_process_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_variational_gaussian_process_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A VGP is &quot;trained&quot; by selecting any kernel parameters, the locations of the
inducing index points, and the variational parameters. Titsias (2009) and
Hensman (2013) describe a variational lower bound on the marginal log
likelihood of observed data, which this class offers through the
<code>variational_loss</code> method (this is the negative lower bound, for convenience
when plugging into a TF Optimizer's <code>minimize</code> function).
Training may be done in minibatches.
</p>
<p>Titsias (2009) describes a closed form for the optimal variational
parameters, in the case of sufficiently small observational data (ie,
small enough to fit in memory but big enough to warrant approximating the GP
posterior). A method to compute these optimal parameters in terms of the full
observational data set is provided as a staticmethod,
<code>optimal_variational_posterior</code>. It returns a
<code>MultivariateNormalLinearOperator</code> instance with optimal location and scale parameters.
</p>
<p>Mathematical Details
</p>
<p>Notation
We will in general be concerned about three collections of index points, and
it'll be good to give them names:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;x[1], ..., x[N]&#8288;</code>: observation index points &ndash; locations of our observed data.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;z[1], ..., z[M]&#8288;</code>: inducing index points  &ndash; locations of the
&quot;summarizing&quot; inducing points
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;t[1], ..., t[P]&#8288;</code>: predictive index points &ndash; locations where we are
making posterior predictions based on observations and the variational
parameters.
</p>
</li></ul>

<p>To lighten notation, we'll use <code style="white-space: pre;">&#8288;X, Z, T&#8288;</code> to denote the above collections.
Similarly, we'll denote by <code>f(X)</code> the collection of function values at each of
the <code>x[i]</code>, and by <code>Y</code>, the collection of (noisy) observed data at each <code>x[i]</code>.
We'll denote kernel matrices generated from pairs of index points as <code>K_tt</code>,
<code>K_xt</code>, <code>K_tz</code>, etc, e.g.,
</p>
<div class="sourceCode"><pre>K_tz =
| k(t[1], z[1])    k(t[1], z[2])  ...  k(t[1], z[M]) |
| k(t[2], z[1])    k(t[2], z[2])  ...  k(t[2], z[M]) |
|      ...              ...                 ...      |
| k(t[P], z[1])    k(t[P], z[2])  ...  k(t[P], z[M]) |

</pre></div>
<p>Preliminaries
A Gaussian process is an indexed collection of random variables, any finite
collection of which are jointly Gaussian. Typically, the index set is some
finite-dimensional, real vector space, and indeed we make this assumption in
what follows. The GP may then be thought of as a distribution over functions
on the index set. Samples from the GP are functions <em>on the whole index set</em>;
these can't be represented in finite compute memory, so one typically works
with the marginals at a finite collection of index points. The properties of
the GP are entirely determined by its mean function <code>m</code> and covariance
function <code>k</code>. The generative process, assuming a mean-zero normal likelihood
with stddev <code>sigma</code>, is
</p>
<div class="sourceCode"><pre>f ~ GP(m, k)
Y | f(X) ~ Normal(f(X), sigma),   i = 1, ... , N
</pre></div>
<p>In finite terms (ie, marginalizing out all but a finite number of f(X), sigma),
we can write
</p>
<div class="sourceCode"><pre>f(X) ~ MVN(loc=m(X), cov=K_xx)
Y | f(X) ~ Normal(f(X), sigma),   i = 1, ... , N
</pre></div>
<p>Posterior inference is possible in analytical closed form but becomes
intractible as data sizes get large. See Rasmussen (2006) for details.
</p>
<p>The VGP
</p>
<p>The VGP is an inducing point-based approximation of an exact GP posterior,
where two approximating assumptions have been made:
</p>

<ol>
<li><p> function values at non-inducing points are mutually independent
conditioned on function values at the inducing points,
</p>
</li>
<li><p> the (expensive) posterior over function values at inducing points
conditional on obseravtions is replaced with an arbitrary (learnable)
full-rank Gaussian distribution,
</p>
</li></ol>

<div class="sourceCode"><pre>q(f(Z)) = MVN(loc=m, scale=S),
</pre></div>
<p>where <code>m</code> and <code>S</code> are parameters to be chosen by optimizing an evidence
lower bound (ELBO).
The posterior predictive distribution becomes
</p>
<div class="sourceCode"><pre>q(f(T)) = integral df(Z) p(f(T) | f(Z)) q(f(Z)) = MVN(loc = A @ m, scale = B^(1/2))
</pre></div>
<p>where
</p>
<div class="sourceCode"><pre>A = K_tz @ K_zz^-1
B = K_tt - A @ (K_zz - S S^T) A^T
</pre></div>
<p>The approximate posterior predictive distribution <code>q(f(T))</code> is what the
<code>VariationalGaussianProcess</code> class represents.
</p>
<p>Model selection in this framework entails choosing the kernel parameters,
inducing point locations, and variational parameters. We do this by optimizing
a variational lower bound on the marginal log likelihood of observed data. The
lower bound takes the following form (see Titsias (2009) and
Hensman (2013) for details on the derivation):
</p>
<div class="sourceCode"><pre>L(Z, m, S, Y) = MVN(loc=
(K_zx @ K_zz^-1) @ m, scale_diag=sigma).log_prob(Y) -
(Tr(K_xx - K_zx @ K_zz^-1 @ K_xz) +
Tr(S @ S^T @ K_zz^1 @ K_zx @ K_xz @ K_zz^-1)) / (2 * sigma^2) -
KL(q(f(Z)) || p(f(Z))))
</pre></div>
<p>where in the final KL term, <code>p(f(Z))</code> is the GP prior on inducing point
function values. This variational lower bound can be computed on minibatches
of the full data set <code style="white-space: pre;">&#8288;(X, Y)&#8288;</code>. A method to compute the <em>negative</em> variational
lower bound is implemented as <code>VariationalGaussianProcess$variational_loss</code>.
</p>
<p>Optimal variational parameters
</p>
<p>As described in Titsias (2009), a closed form optimum for the variational
location and scale parameters, <code>m</code> and <code>S</code>, can be computed when the
observational data are not prohibitively voluminous. The
<code>optimal_variational_posterior</code> function to computes the optimal variational
posterior distribution over inducing point function values in terms of the GP
parameters (mean and kernel functions), inducing point locations, observation
index points, and observations. Note that the inducing index point locations
must still be optimized even when these parameters are known functions of the
inducing index points. The optimal parameters are computed as follows:
</p>
<div class="sourceCode"><pre>C = sigma^-2 (K_zz + K_zx @ K_xz)^-1
optimal Gaussian covariance: K_zz @ C @ K_zz
optimal Gaussian location: sigma^-2 K_zz @ C @ K_zx @ Y
</pre></div>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>References</h3>


<ul>
<li> <p><a href="http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf">Titsias, M. &quot;Variational Model Selection for Sparse Gaussian Process Regression&quot;, 2009.</a>
</p>
</li>
<li> <p><a href="https://arxiv.org/abs/1309.6835">Hensman, J., Lawrence, N. &quot;Gaussian Processes for Big Data&quot;, 2013.</a>
</p>
</li>
<li> <p><a href="http://gaussianprocess.org/gpml/">Carl Rasmussen, Chris Williams. Gaussian Processes For Machine Learning, 2006.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_vector_deterministic'>Vector Deterministic Distribution</h2><span id='topic+tfd_vector_deterministic'></span>

<h3>Description</h3>

<p>The VectorDeterministic distribution is parameterized by a batch point loc in R^k.
The distribution is supported at this point only, and corresponds to a random
variable that is constant, equal to loc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_vector_deterministic(
  loc,
  atol = NULL,
  rtol = NULL,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "VectorDeterministic"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_vector_deterministic_+3A_loc">loc</code></td>
<td>
<p>Numeric Tensor of shape [B1, ..., Bb, k], with b &gt;= 0, k &gt;= 0 The
point (or batch of points) on which this distribution is supported.</p>
</td></tr>
<tr><td><code id="tfd_vector_deterministic_+3A_atol">atol</code></td>
<td>
<p>Non-negative Tensor of same dtype as loc and broadcastable shape.
The absolute tolerance for comparing closeness to loc. Default is 0.</p>
</td></tr>
<tr><td><code id="tfd_vector_deterministic_+3A_rtol">rtol</code></td>
<td>
<p>Non-negative Tensor of same dtype as loc and broadcastable shape.
The relative tolerance for comparing closeness to loc. Default is 0.</p>
</td></tr>
<tr><td><code id="tfd_vector_deterministic_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_vector_deterministic_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_vector_deterministic_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <a href="https://en.wikipedia.org/wiki/Degenerate_distribution">Degenerate rv</a>.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>

<hr>
<h2 id='tfd_vector_diffeomixture'>VectorDiffeomixture distribution</h2><span id='topic+tfd_vector_diffeomixture'></span>

<h3>Description</h3>

<p>A vector diffeomixture (VDM) is a distribution parameterized by a convex
combination of <code>K</code> component <code>loc</code> vectors, <code style="white-space: pre;">&#8288;loc[k], k = 0,...,K-1&#8288;</code>, and <code>K</code>
<code>scale</code> matrices <code style="white-space: pre;">&#8288;scale[k], k = 0,..., K-1&#8288;</code>.  It approximates the following
<a href="https://en.wikipedia.org/wiki/Compound_probability_distribution">compound distribution</a>
<code style="white-space: pre;">&#8288;p(x) = int p(x | z) p(z) dz&#8288;</code>, where z is in the K-simplex, and
<code style="white-space: pre;">&#8288;p(x | z) := p(x | loc=sum_k z[k] loc[k], scale=sum_k z[k] scale[k])&#8288;</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_vector_diffeomixture(
  mix_loc,
  temperature,
  distribution,
  loc = NULL,
  scale = NULL,
  quadrature_size = 8,
  quadrature_fn = tfp$distributions$quadrature_scheme_softmaxnormal_quantiles,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "VectorDiffeomixture"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_vector_diffeomixture_+3A_mix_loc">mix_loc</code></td>
<td>
<p><code>float</code>-like <code>Tensor</code> with shape <code style="white-space: pre;">&#8288;[b1, ..., bB, K-1]&#8288;</code>.
In terms of samples, larger <code>mix_loc[..., k]</code> ==&gt;
<code>Z</code> is more likely to put more weight on its <code>kth</code> component.</p>
</td></tr>
<tr><td><code id="tfd_vector_diffeomixture_+3A_temperature">temperature</code></td>
<td>
<p><code>float</code>-like <code>Tensor</code>. Broadcastable with <code>mix_loc</code>.
In terms of samples, smaller <code>temperature</code> means one component is more
likely to dominate.  I.e., smaller <code>temperature</code> makes the VDM look more
like a standard mixture of <code>K</code> components.</p>
</td></tr>
<tr><td><code id="tfd_vector_diffeomixture_+3A_distribution">distribution</code></td>
<td>
<p><code>tfp$distributions$Distribution</code>-like instance. Distribution
from which <code>d</code> iid samples are used as input to the selected affine
transformation. Must be a scalar-batch, scalar-event distribution.
Typically <code>distribution$reparameterization_type = FULLY_REPARAMETERIZED</code>
or it is a function of non-trainable parameters. WARNING: If you
backprop through a VectorDiffeomixture sample and the <code>distribution</code>
is not <code>FULLY_REPARAMETERIZED</code> yet is a function of trainable variables,
then the gradient will be incorrect!</p>
</td></tr>
<tr><td><code id="tfd_vector_diffeomixture_+3A_loc">loc</code></td>
<td>
<p>Length-<code>K</code> list of <code>float</code>-type <code>Tensor</code>s. The <code>k</code>-th element
represents the <code>shift</code> used for the <code>k</code>-th affine transformation.  If
the <code>k</code>-th item is <code>NULL</code>, <code>loc</code> is implicitly <code>0</code>.  When specified,
must have shape <code style="white-space: pre;">&#8288;[B1, ..., Bb, d]&#8288;</code> where <code>b &gt;= 0</code> and <code>d</code> is the event
size.</p>
</td></tr>
<tr><td><code id="tfd_vector_diffeomixture_+3A_scale">scale</code></td>
<td>
<p>Length-<code>K</code> list of <code>LinearOperator</code>s. Each should be
positive-definite and operate on a <code>d</code>-dimensional vector space. The
<code>k</code>-th element represents the <code>scale</code> used for the <code>k</code>-th affine
transformation. <code>LinearOperator</code>s must have shape <code style="white-space: pre;">&#8288;[B1, ..., Bb, d, d]&#8288;</code>,
<code>b &gt;= 0</code>, i.e., characterizes <code>b</code>-batches of <code style="white-space: pre;">&#8288;d x d&#8288;</code> matrices</p>
</td></tr>
<tr><td><code id="tfd_vector_diffeomixture_+3A_quadrature_size">quadrature_size</code></td>
<td>
<p><code>integer</code> scalar representing number of
quadrature points.  Larger <code>quadrature_size</code> means <code>q_N(x)</code> better
approximates <code>p(x)</code>.</p>
</td></tr>
<tr><td><code id="tfd_vector_diffeomixture_+3A_quadrature_fn">quadrature_fn</code></td>
<td>
<p>Function taking <code>normal_loc</code>, <code>normal_scale</code>,
<code>quadrature_size</code>, <code>validate_args</code> and returning <code>tuple(grid, probs)</code>
representing the SoftmaxNormal grid and corresponding normalized weight.
normalized) weight.
Default value: <code>quadrature_scheme_softmaxnormal_quantiles</code>.</p>
</td></tr>
<tr><td><code id="tfd_vector_diffeomixture_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_vector_diffeomixture_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_vector_diffeomixture_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The integral <code style="white-space: pre;">&#8288;int p(x | z) p(z) dz&#8288;</code> is approximated with a quadrature scheme
adapted to the mixture density <code>p(z)</code>.  The <code>N</code> quadrature points <code style="white-space: pre;">&#8288;z_{N, n}&#8288;</code>
and weights <code style="white-space: pre;">&#8288;w_{N, n}&#8288;</code> (which are non-negative and sum to 1) are chosen such that
<code style="white-space: pre;">&#8288;q_N(x) := sum_{n=1}^N w_{n, N} p(x | z_{N, n}) --&gt; p(x)&#8288;</code> as <code style="white-space: pre;">&#8288;N --&gt; infinity&#8288;</code>.
</p>
<p>Since <code>q_N(x)</code> is in fact a mixture (of <code>N</code> points), we may sample from
<code>q_N</code> exactly.  It is important to note that the VDM is <em>defined</em> as <code>q_N</code>
above, and <em>not</em> <code>p(x)</code>.  Therefore, sampling and pdf may be implemented as
exact (up to floating point error) methods.
</p>
<p>A common choice for the conditional <code>p(x | z)</code> is a multivariate Normal.
The implemented marginal <code>p(z)</code> is the <code>SoftmaxNormal</code>, which is a
<code>K-1</code> dimensional Normal transformed by a <code>SoftmaxCentered</code> bijector, making
it a density on the <code>K</code>-simplex.  That is,
<code>Z = SoftmaxCentered(X)</code>, <code>X = Normal(mix_loc / temperature, 1 / temperature)</code>
</p>
<p>The default quadrature scheme chooses <code style="white-space: pre;">&#8288;z_{N, n}&#8288;</code> as <code>N</code> midpoints of
the quantiles of <code>p(z)</code> (generalized quantiles if <code>K &gt; 2</code>).
See Dillon and Langmore (2018) for more details.
</p>
<p>About <code>Vector</code> distributions in TensorFlow.
</p>
<p>The <code>VectorDiffeomixture</code> is a non-standard distribution that has properties
particularly useful in <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">variational Bayesian methods</a>.
Conditioned on a draw from the SoftmaxNormal, <code>X|z</code> is a vector whose
components are linear combinations of affine transformations, thus is itself
an affine transformation.
</p>
<p>Note: The marginals <code style="white-space: pre;">&#8288;X_1|v, ..., X_d|v&#8288;</code> are <em>not</em> generally identical to some
parameterization of <code>distribution</code>.  This is due to the fact that the sum of
draws from <code>distribution</code> are not generally itself the same <code>distribution</code>.
</p>
<p>About <code>Diffeomixture</code>s and reparameterization.
</p>
<p>The <code>VectorDiffeomixture</code> is designed to be reparameterized, i.e., its
parameters are only used to transform samples from a distribution which has no
trainable parameters. This property is important because backprop stops at
sources of stochasticity. That is, as long as the parameters are used <em>after</em>
the underlying source of stochasticity, the computed gradient is accurate.
Reparametrization means that we can use gradient-descent (via backprop) to
optimize Monte-Carlo objectives. Such objectives are a finite-sample
approximation of an expectation and arise throughout scientific computing.
</p>
<p>WARNING: If you backprop through a VectorDiffeomixture sample and the &quot;base&quot;
distribution is both: not <code>FULLY_REPARAMETERIZED</code> and a function of trainable
variables, then the gradient is not guaranteed correct!
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1801.03080">Joshua Dillon and Ian Langmore. Quadrature Compound: An approximating family of distributions. <em>arXiv preprint arXiv:1801.03080</em>, 2018.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_vector_exponential_diag'>The vectorization of the Exponential distribution on <code>R^k</code></h2><span id='topic+tfd_vector_exponential_diag'></span>

<h3>Description</h3>

<p>The vector exponential distribution is defined over a subset of <code>R^k</code>, and
parameterized by a (batch of) length-<code>k</code> <code>loc</code> vector and a (batch of) <code style="white-space: pre;">&#8288;k x k&#8288;</code>
<code>scale</code> matrix:  <code>covariance = scale @ scale.T</code>, where <code>@</code> denotes
matrix-multiplication.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_vector_exponential_diag(
  loc = NULL,
  scale_diag = NULL,
  scale_identity_multiplier = NULL,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "VectorExponentialDiag"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_vector_exponential_diag_+3A_loc">loc</code></td>
<td>
<p>Floating-point Tensor. If this is set to NULL, loc is
implicitly 0. When specified, may have shape <code style="white-space: pre;">&#8288;[B1, ..., Bb, k]&#8288;</code> where
b &gt;= 0 and k is the event size.</p>
</td></tr>
<tr><td><code id="tfd_vector_exponential_diag_+3A_scale_diag">scale_diag</code></td>
<td>
<p>Non-zero, floating-point Tensor representing a diagonal
matrix added to scale. May have shape <code style="white-space: pre;">&#8288;[B1, ..., Bb, k]&#8288;</code>, b &gt;= 0,
and characterizes b-batches of k x k diagonal matrices added to
scale. When both scale_identity_multiplier and scale_diag are
NULL then scale is the Identity.</p>
</td></tr>
<tr><td><code id="tfd_vector_exponential_diag_+3A_scale_identity_multiplier">scale_identity_multiplier</code></td>
<td>
<p>Non-zero, floating-point Tensor representing
a scaled-identity-matrix added to scale. May have shape
<code style="white-space: pre;">&#8288;[B1, ..., Bb]&#8288;</code>, b &gt;= 0, and characterizes b-batches of scaled
k x k identity matrices added to scale. When both
scale_identity_multiplier and scale_diag are NULL then scale is
the Identity.</p>
</td></tr>
<tr><td><code id="tfd_vector_exponential_diag_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_vector_exponential_diag_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_vector_exponential_diag_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
The probability density function (pdf) is defined over the image of the
<code>scale</code> matrix + <code>loc</code>, applied to the positive half-space:
<code style="white-space: pre;">&#8288;Supp = {loc + scale @ x : x in R^k, x_1 &gt; 0, ..., x_k &gt; 0}&#8288;</code>.  On this set,
</p>
<div class="sourceCode"><pre>pdf(y; loc, scale) = exp(-||x||_1) / Z,  for y in Supp
x = inv(scale) @ (y - loc),
Z = |det(scale)|,
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>loc</code> is a vector in <code>R^k</code>,
</p>
</li>
<li> <p><code>scale</code> is a linear operator in <code style="white-space: pre;">&#8288;R^{k x k}&#8288;</code>, <code>cov = scale @ scale.T</code>,
</p>
</li>
<li> <p><code>Z</code> denotes the normalization constant, and,
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;||x||_1&#8288;</code> denotes the <code>l1</code> norm of <code>x</code>, <code style="white-space: pre;">&#8288;sum_i |x_i|&#8288;</code>.
The VectorExponential distribution is a member of the <a href="https://en.wikipedia.org/wiki/Location-scale_family">location-scale family</a>, i.e., it can be
constructed as,
</p>
</li></ul>

<div class="sourceCode"><pre>X = (X_1, ..., X_k), each X_i ~ Exponential(rate=1)
Y = (Y_1, ...,Y_k) = scale @ X + loc
</pre></div>
<p>About <code>VectorExponential</code> and <code>Vector</code> distributions in TensorFlow.
</p>
<p>The <code>VectorExponential</code> is a non-standard distribution that has useful
properties.
The marginals <code style="white-space: pre;">&#8288;Y_1, ..., Y_k&#8288;</code> are <em>not</em> Exponential random variables, due to
the fact that the sum of Exponential random variables is not Exponential.
Instead, <code>Y</code> is a vector whose components are linear combinations of
Exponential random variables.  Thus, <code>Y</code> lives in the vector space generated
by <code>vectors</code> of Exponential distributions.  This allows the user to decide the
mean and covariance (by setting <code>loc</code> and <code>scale</code>), while preserving some
properties of the Exponential distribution.  In particular, the tails of <code>Y_i</code>
will be (up to polynomial factors) exponentially decaying.
To see this last statement, note that the pdf of <code>Y_i</code> is the convolution of
the pdf of <code>k</code> independent Exponential random variables.  One can then show by
induction that distributions with exponential (up to polynomial factors) tails
are closed under convolution.
</p>
<p>The batch_shape is the broadcast shape between loc and scale
arguments.
The event_shape is given by last dimension of the matrix implied by
scale. The last dimension of loc (if provided) must broadcast with this.
Recall that <code>covariance = 2 * scale @ scale.T</code>.
Additional leading dimensions (if any) will index batches.
If both <code>scale_diag</code> and <code>scale_identity_multiplier</code> are <code>NULL</code>, then
<code>scale</code> is the Identity matrix.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_vector_exponential_linear_operator'>The vectorization of the Exponential distribution on <code>R^k</code></h2><span id='topic+tfd_vector_exponential_linear_operator'></span>

<h3>Description</h3>

<p>The vector exponential distribution is defined over a subset of <code>R^k</code>, and
parameterized by a (batch of) length-<code>k</code> <code>loc</code> vector and a (batch of) <code style="white-space: pre;">&#8288;k x k&#8288;</code>
<code>scale</code> matrix:  <code>covariance = scale @ scale.T</code>, where <code>@</code> denotes
matrix-multiplication.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_vector_exponential_linear_operator(
  loc = NULL,
  scale = NULL,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "VectorExponentialLinearOperator"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_vector_exponential_linear_operator_+3A_loc">loc</code></td>
<td>
<p>Floating point tensor; the means of the distribution(s).</p>
</td></tr>
<tr><td><code id="tfd_vector_exponential_linear_operator_+3A_scale">scale</code></td>
<td>
<p>Instance of LinearOperator with same dtype as loc and shape
<code style="white-space: pre;">&#8288;[B1, ..., Bb, k, k]&#8288;</code>.</p>
</td></tr>
<tr><td><code id="tfd_vector_exponential_linear_operator_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_vector_exponential_linear_operator_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_vector_exponential_linear_operator_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
The probability density function (pdf) is
</p>
<div class="sourceCode"><pre>pdf(y; loc, scale) = exp(-||x||_1) / Z,  for y in S(loc, scale),
x = inv(scale) @ (y - loc),
Z = |det(scale)|,
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>loc</code> is a vector in <code>R^k</code>,
</p>
</li>
<li> <p><code>scale</code> is a linear operator in <code style="white-space: pre;">&#8288;R^{k x k}&#8288;</code>, <code>cov = scale @ scale.T</code>,
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;S = {loc + scale @ x : x in R^k, x_1 &gt; 0, ..., x_k &gt; 0}&#8288;</code>, is an image of
the positive half-space,
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;||x||_1&#8288;</code> denotes the <code>l1</code> norm of <code>x</code>, <code style="white-space: pre;">&#8288;sum_i |x_i|&#8288;</code>,
</p>
</li>
<li> <p><code>Z</code> denotes the normalization constant.
</p>
</li></ul>

<p>The VectorExponential distribution is a member of the <a href="https://en.wikipedia.org/wiki/Location-scale_family">location-scale family</a>, i.e., it can be
constructed as,
</p>
<div class="sourceCode"><pre>X = (X_1, ..., X_k), each X_i ~ Exponential(rate=1)
Y = (Y_1, ...,Y_k) = scale @ X + loc
</pre></div>
<p>About <code>VectorExponential</code> and <code>Vector</code> distributions in TensorFlow.
</p>
<p>The <code>VectorExponential</code> is a non-standard distribution that has useful
properties.
The marginals <code style="white-space: pre;">&#8288;Y_1, ..., Y_k&#8288;</code> are <em>not</em> Exponential random variables, due to
the fact that the sum of Exponential random variables is not Exponential.
Instead, <code>Y</code> is a vector whose components are linear combinations of
Exponential random variables.  Thus, <code>Y</code> lives in the vector space generated
by <code>vectors</code> of Exponential distributions.  This allows the user to decide the
mean and covariance (by setting <code>loc</code> and <code>scale</code>), while preserving some
properties of the Exponential distribution.  In particular, the tails of <code>Y_i</code>
will be (up to polynomial factors) exponentially decaying.
To see this last statement, note that the pdf of <code>Y_i</code> is the convolution of
the pdf of <code>k</code> independent Exponential random variables.  One can then show by
induction that distributions with exponential (up to polynomial factors) tails
are closed under convolution.
</p>
<p>The batch_shape is the broadcast shape between loc and scale
arguments.
The event_shape is given by last dimension of the matrix implied by
scale. The last dimension of loc (if provided) must broadcast with this.
Recall that <code>covariance = 2 * scale @ scale.T</code>.
Additional leading dimensions (if any) will index batches.
</p>
<p>#' @param  loc Floating-point Tensor. If this is set to NULL, loc is
implicitly 0. When specified, may have shape <code style="white-space: pre;">&#8288;[B1, ..., Bb, k]&#8288;</code> where
b &gt;= 0 and k is the event size.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_vector_laplace_diag'>The vectorization of the Laplace distribution on <code>R^k</code></h2><span id='topic+tfd_vector_laplace_diag'></span>

<h3>Description</h3>

<p>The vector laplace distribution is defined over <code>R^k</code>, and parameterized by
a (batch of) length-k loc vector (the means) and a (batch of) k x k
scale matrix:  <code>covariance = 2 * scale @ scale.T</code>, where @ denotes
matrix-multiplication.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_vector_laplace_diag(
  loc = NULL,
  scale_diag = NULL,
  scale_identity_multiplier = NULL,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "VectorLaplaceDiag"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_vector_laplace_diag_+3A_loc">loc</code></td>
<td>
<p>Floating-point Tensor. If this is set to NULL, loc is
implicitly 0. When specified, may have shape <code style="white-space: pre;">&#8288;[B1, ..., Bb, k]&#8288;</code> where
b &gt;= 0 and k is the event size.</p>
</td></tr>
<tr><td><code id="tfd_vector_laplace_diag_+3A_scale_diag">scale_diag</code></td>
<td>
<p>Non-zero, floating-point Tensor representing a diagonal
matrix added to scale. May have shape <code style="white-space: pre;">&#8288;[B1, ..., Bb, k]&#8288;</code>, b &gt;= 0,
and characterizes b-batches of k x k diagonal matrices added to
scale. When both scale_identity_multiplier and scale_diag are
NULL then scale is the Identity.</p>
</td></tr>
<tr><td><code id="tfd_vector_laplace_diag_+3A_scale_identity_multiplier">scale_identity_multiplier</code></td>
<td>
<p>Non-zero, floating-point Tensor representing
a scaled-identity-matrix added to scale. May have shape
<code style="white-space: pre;">&#8288;[B1, ..., Bb]&#8288;</code>, b &gt;= 0, and characterizes b-batches of scaled
k x k identity matrices added to scale. When both
scale_identity_multiplier and scale_diag are NULL then scale is
the Identity.</p>
</td></tr>
<tr><td><code id="tfd_vector_laplace_diag_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_vector_laplace_diag_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_vector_laplace_diag_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; loc, scale) = exp(-||y||_1) / Z
y = inv(scale) @ (x - loc)
Z = 2**k |det(scale)|
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>loc</code> is a vector in <code>R^k</code>,
</p>
</li>
<li> <p><code>scale</code> is a linear operator in <code style="white-space: pre;">&#8288;R^{k x k}&#8288;</code>, <code>cov = scale @ scale.T</code>,
</p>
</li>
<li> <p><code>Z</code> denotes the normalization constant, and,
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;||y||_1&#8288;</code> denotes the <code>l1</code> norm of <code>y</code>, 'sum_i |y_i|.
</p>
</li></ul>

<p>A (non-batch) <code>scale</code> matrix is:
</p>
<div class="sourceCode"><pre>scale = diag(scale_diag + scale_identity_multiplier * ones(k))
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;scale_diag.shape = [k]&#8288;</code>, and,
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;scale_identity_multiplier.shape = []&#8288;</code>.
Additional leading dimensions (if any) will index batches.
If both <code>scale_diag</code> and <code>scale_identity_multiplier</code> are <code>NULL</code>, then
<code>scale</code> is the Identity matrix.
</p>
</li></ul>

<p>About VectorLaplace and Vector distributions in TensorFlow
</p>
<p>The VectorLaplace is a non-standard distribution that has useful properties.
The marginals Y_1, ..., Y_k are <em>not</em> Laplace random variables, due to
the fact that the sum of Laplace random variables is not Laplace.
Instead, Y is a vector whose components are linear combinations of Laplace
random variables.  Thus, Y lives in the vector space generated by vectors
of Laplace distributions.  This allows the user to decide the mean and
covariance (by setting loc and scale), while preserving some properties of
the Laplace distribution.  In particular, the tails of Y_i will be (up to
polynomial factors) exponentially decaying.
To see this last statement, note that the pdf of Y_i is the convolution of
the pdf of k independent Laplace random variables.  One can then show by
induction that distributions with exponential (up to polynomial factors) tails
are closed under convolution.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_vector_laplace_linear_operator'>The vectorization of the Laplace distribution on <code>R^k</code></h2><span id='topic+tfd_vector_laplace_linear_operator'></span>

<h3>Description</h3>

<p>The vector laplace distribution is defined over <code>R^k</code>, and parameterized by
a (batch of) length-k loc vector (the means) and a (batch of) k x k
scale matrix:  <code>covariance = 2 * scale @ scale.T</code>, where <code>@</code> denotes
matrix-multiplication.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_vector_laplace_linear_operator(
  loc = NULL,
  scale = NULL,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "VectorLaplaceLinearOperator"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_vector_laplace_linear_operator_+3A_loc">loc</code></td>
<td>
<p>Floating-point Tensor. If this is set to NULL, loc is
implicitly 0. When specified, may have shape <code style="white-space: pre;">&#8288;[B1, ..., Bb, k]&#8288;</code> where
b &gt;= 0 and k is the event size.</p>
</td></tr>
<tr><td><code id="tfd_vector_laplace_linear_operator_+3A_scale">scale</code></td>
<td>
<p>Instance of LinearOperator with same dtype as loc and shape
<code style="white-space: pre;">&#8288;[B1, ..., Bb, k, k]&#8288;</code>.</p>
</td></tr>
<tr><td><code id="tfd_vector_laplace_linear_operator_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_vector_laplace_linear_operator_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_vector_laplace_linear_operator_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; loc, scale) = exp(-||y||_1) / Z,
y = inv(scale) @ (x - loc),
Z = 2**k |det(scale)|,
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>loc</code> is a vector in <code>R^k</code>,
</p>
</li>
<li> <p><code>scale</code> is a linear operator in <code style="white-space: pre;">&#8288;R^{k x k}&#8288;</code>, <code>cov = scale @ scale.T</code>,
</p>
</li>
<li> <p><code>Z</code> denotes the normalization constant, and,
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;||y||_1&#8288;</code> denotes the <code>l1</code> norm of <code>y</code>, 'sum_i |y_i|.
</p>
</li></ul>

<p>The VectorLaplace distribution is a member of the <a href="https://en.wikipedia.org/wiki/Location-scale_family">location-scale family</a>, i.e., it can be
constructed as,
</p>
<div class="sourceCode"><pre>X = (X_1, ..., X_k), each X_i ~ Laplace(loc=0, scale=1)
Y = (Y_1, ...,Y_k) = scale @ X + loc
</pre></div>
<p>About VectorLaplace and Vector distributions in TensorFlow
</p>
<p>The VectorLaplace is a non-standard distribution that has useful properties.
The marginals Y_1, ..., Y_k are <em>not</em> Laplace random variables, due to
the fact that the sum of Laplace random variables is not Laplace.
Instead, Y is a vector whose components are linear combinations of Laplace
random variables.  Thus, Y lives in the vector space generated by vectors
of Laplace distributions.  This allows the user to decide the mean and
covariance (by setting loc and scale), while preserving some properties of
the Laplace distribution.  In particular, the tails of Y_i will be (up to
polynomial factors) exponentially decaying.
To see this last statement, note that the pdf of Y_i is the convolution of
the pdf of k independent Laplace random variables.  One can then show by
induction that distributions with exponential (up to polynomial factors) tails
are closed under convolution.
</p>
<p>The batch_shape is the broadcast shape between loc and scale
arguments.
The event_shape is given by last dimension of the matrix implied by
scale. The last dimension of loc (if provided) must broadcast with this.
Recall that <code>covariance = 2 * scale @ scale.T</code>.
Additional leading dimensions (if any) will index batches.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_vector_sinh_arcsinh_diag'>The (diagonal) SinhArcsinh transformation of a distribution on <code>R^k</code></h2><span id='topic+tfd_vector_sinh_arcsinh_diag'></span>

<h3>Description</h3>

<p>This distribution models a random vector <code style="white-space: pre;">&#8288;Y = (Y1,...,Yk)&#8288;</code>, making use of
a SinhArcsinh transformation (which has adjustable tailweight and skew),
a rescaling, and a shift.
The SinhArcsinh transformation of the Normal is described in great depth in
<a href="https://oro.open.ac.uk/22510/">Sinh-arcsinh distributions</a>.
Here we use a slightly different parameterization, in terms of tailweight
and skewness.  Additionally we allow for distributions other than Normal,
and control over scale as well as a &quot;shift&quot; parameter loc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_vector_sinh_arcsinh_diag(
  loc = NULL,
  scale_diag = NULL,
  scale_identity_multiplier = NULL,
  skewness = NULL,
  tailweight = NULL,
  distribution = NULL,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "VectorSinhArcsinhDiag"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_vector_sinh_arcsinh_diag_+3A_loc">loc</code></td>
<td>
<p>Floating-point Tensor. If this is set to NULL, loc is
implicitly 0. When specified, may have shape <code style="white-space: pre;">&#8288;[B1, ..., Bb, k]&#8288;</code> where
b &gt;= 0 and k is the event size.</p>
</td></tr>
<tr><td><code id="tfd_vector_sinh_arcsinh_diag_+3A_scale_diag">scale_diag</code></td>
<td>
<p>Non-zero, floating-point Tensor representing a diagonal
matrix added to scale. May have shape <code style="white-space: pre;">&#8288;[B1, ..., Bb, k]&#8288;</code>, b &gt;= 0,
and characterizes b-batches of k x k diagonal matrices added to
scale. When both scale_identity_multiplier and scale_diag are
NULL then scale is the Identity.</p>
</td></tr>
<tr><td><code id="tfd_vector_sinh_arcsinh_diag_+3A_scale_identity_multiplier">scale_identity_multiplier</code></td>
<td>
<p>Non-zero, floating-point Tensor representing
a scale-identity-matrix added to scale. May have shape
<code style="white-space: pre;">&#8288;[B1, ..., Bb]&#8288;</code>, b &gt;= 0, and characterizes b-batches of scale
k x k identity matrices added to scale. When both
scale_identity_multiplier and scale_diag are NULL then scale
is the Identity.</p>
</td></tr>
<tr><td><code id="tfd_vector_sinh_arcsinh_diag_+3A_skewness">skewness</code></td>
<td>
<p>Skewness parameter.  floating-point Tensor with shape
broadcastable with event_shape.</p>
</td></tr>
<tr><td><code id="tfd_vector_sinh_arcsinh_diag_+3A_tailweight">tailweight</code></td>
<td>
<p>Tailweight parameter.  floating-point Tensor with shape
broadcastable with event_shape.</p>
</td></tr>
<tr><td><code id="tfd_vector_sinh_arcsinh_diag_+3A_distribution">distribution</code></td>
<td>
<p><code>tf$distributions$Distribution</code>-like instance. Distribution from which k
iid samples are used as input to transformation F.  Default is
tfd_normal(loc = 0, scale = 1).
Must be a scalar-batch, scalar-event distribution.  Typically
distribution$reparameterization_type = FULLY_REPARAMETERIZED or it is
a function of non-trainable parameters. WARNING: If you backprop through
a VectorSinhArcsinhDiag sample and distribution is not
FULLY_REPARAMETERIZED yet is a function of trainable variables, then
the gradient will be incorrect!</p>
</td></tr>
<tr><td><code id="tfd_vector_sinh_arcsinh_diag_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_vector_sinh_arcsinh_diag_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_vector_sinh_arcsinh_diag_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>Given iid random vector <code style="white-space: pre;">&#8288;Z = (Z1,...,Zk)&#8288;</code>, we define the VectorSinhArcsinhDiag
transformation of <code>Z</code>, <code>Y</code>, parameterized by
<code style="white-space: pre;">&#8288;(loc, scale, skewness, tailweight)&#8288;</code>, via the relation (with <code>@</code> denoting matrix multiplication):
</p>
<div class="sourceCode"><pre>Y := loc + scale @ F(Z) * (2 / F_0(2))
F(Z) := Sinh( (Arcsinh(Z) + skewness) * tailweight )
F_0(Z) := Sinh( Arcsinh(Z) * tailweight )
</pre></div>
<p>This distribution is similar to the location-scale transformation
<code>L(Z) := loc + scale @ Z</code> in the following ways:
</p>

<ul>
<li><p> If <code>skewness = 0</code> and <code>tailweight = 1</code> (the defaults), <code>F(Z) = Z</code>, and then
<code>Y = L(Z)</code> exactly.
</p>
</li>
<li> <p><code>loc</code> is used in both to shift the result by a constant factor.
</p>
</li>
<li><p> The multiplication of <code>scale</code> by <code>2 / F_0(2)</code> ensures that if <code>skewness = 0</code>
<code>P[Y - loc &lt;= 2 * scale] = P[L(Z) - loc &lt;= 2 * scale]</code>.
Thus it can be said that the weights in the tails of <code>Y</code> and <code>L(Z)</code> beyond
<code>loc + 2 * scale</code> are the same.
This distribution is different than <code>loc + scale @ Z</code> due to the
reshaping done by <code>F</code>:
</p>

<ul>
<li><p> Positive (negative) <code>skewness</code> leads to positive (negative) skew.
</p>
</li>
<li><p> positive skew means, the mode of <code>F(Z)</code> is &quot;tilted&quot; to the right.
</p>
</li>
<li><p> positive skew means positive values of <code>F(Z)</code> become more likely, and
negative values become less likely.
</p>
</li>
<li><p> Larger (smaller) <code>tailweight</code> leads to fatter (thinner) tails.
</p>
</li>
<li><p> Fatter tails mean larger values of <code style="white-space: pre;">&#8288;|F(Z)|&#8288;</code> become more likely.
</p>
</li>
<li> <p><code>tailweight &lt; 1</code> leads to a distribution that is &quot;flat&quot; around <code>Y = loc</code>,
and a very steep drop-off in the tails.
</p>
</li>
<li> <p><code>tailweight &gt; 1</code> leads to a distribution more peaked at the mode with
heavier tails.
To see the argument about the tails, note that for <code style="white-space: pre;">&#8288;|Z| &gt;&gt; 1&#8288;</code> and
<code style="white-space: pre;">&#8288;|Z| &gt;&gt; (|skewness| * tailweight)**tailweight&#8288;</code>, we have
<code style="white-space: pre;">&#8288;Y approx 0.5 Z**tailweight e**(sign(Z) skewness * tailweight)&#8288;</code>.
To see the argument regarding multiplying <code>scale</code> by <code>2 / F_0(2)</code>,
</p>
</li></ul>

<div class="sourceCode"><pre>P[(Y - loc) / scale &lt;= 2] = P[F(Z) * (2 / F_0(2)) &lt;= 2]
= P[F(Z) &lt;= F_0(2)]
= P[Z &lt;= 2]  (if F = F_0).
</pre></div>
</li></ul>



<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_von_mises'>The von Mises distribution over angles</h2><span id='topic+tfd_von_mises'></span>

<h3>Description</h3>

<p>The von Mises distribution is a univariate directional distribution.
Similarly to Normal distribution, it is a maximum entropy distribution.
The samples of this distribution are angles, measured in radians.
They are 2 pi-periodic: x = 0 and x = 2pi are equivalent.
This means that the density is also 2 pi-periodic.
The generated samples, however, are guaranteed to be in <code style="white-space: pre;">&#8288;[-pi, pi)&#8288;</code> range.
When concentration = 0, this distribution becomes a Uniform distribuion on
the <code style="white-space: pre;">&#8288;[-pi, pi)&#8288;</code> domain.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_von_mises(
  loc,
  concentration,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "VonMises"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_von_mises_+3A_loc">loc</code></td>
<td>
<p>Floating point tensor, the circular means of the distribution(s).</p>
</td></tr>
<tr><td><code id="tfd_von_mises_+3A_concentration">concentration</code></td>
<td>
<p>Floating point tensor, the level of concentration of the
distribution(s) around loc. Must take non-negative values.
concentration = 0 defines a Uniform distribution, while
concentration = +inf indicates a Deterministic distribution at loc.</p>
</td></tr>
<tr><td><code id="tfd_von_mises_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_von_mises_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_von_mises_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The von Mises distribution is a special case of von Mises-Fisher distribution
for n=2. However, the TFP's VonMisesFisher implementation represents the
samples and location as (x, y) points on a circle, while VonMises represents
them as scalar angles.
</p>
<p>Mathematical details
The probability density function (pdf) of this distribution is,
</p>
<div class="sourceCode"><pre>pdf(x; loc, concentration) = exp(concentration cos(x - loc)) / Z
Z = 2 * pi * I_0 (concentration)
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>I_0 (concentration)</code> is the modified Bessel function of order zero;
</p>
</li>
<li> <p><code>loc</code> the circular mean of the distribution, a scalar. It can take arbitrary
values, but it is 2pi-periodic: loc and loc + 2pi result in the same
distribution.
</p>
</li>
<li> <p><code>concentration &gt;= 0</code> parameter is the concentration parameter. When
<code>concentration = 0</code>,
this distribution becomes a Uniform distribution on [-pi, pi).
</p>
</li></ul>

<p>The parameters loc and concentration must be shaped in a way that
supports broadcasting (e.g. loc + concentration is a valid operation).
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_von_mises_fisher'>The von Mises-Fisher distribution over unit vectors on <code>S^{n-1}</code></h2><span id='topic+tfd_von_mises_fisher'></span>

<h3>Description</h3>

<p>The von Mises-Fisher distribution is a directional distribution over vectors
on the unit hypersphere <code>S^{n-1}</code> embedded in n dimensions <code>(R^n)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_von_mises_fisher(
  mean_direction,
  concentration,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "VonMisesFisher"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_von_mises_fisher_+3A_mean_direction">mean_direction</code></td>
<td>
<p>Floating-point Tensor with shape <code style="white-space: pre;">&#8288;[B1, ... Bn, D]&#8288;</code>.
A unit vector indicating the mode of the distribution, or the
unit-normalized direction of the mean. (This is <em>not</em> in general the
mean of the distribution; the mean is not generally in the support of
the distribution.) NOTE: D is currently restricted to &lt;= 5.</p>
</td></tr>
<tr><td><code id="tfd_von_mises_fisher_+3A_concentration">concentration</code></td>
<td>
<p>Floating-point Tensor having batch shape <code style="white-space: pre;">&#8288;[B1, ... Bn]&#8288;</code>
broadcastable with mean_direction. The level of concentration of
samples around the mean_direction. concentration=0 indicates a
uniform distribution over the unit hypersphere, and concentration=+inf
indicates a Deterministic distribution (delta function) at mean_direction.</p>
</td></tr>
<tr><td><code id="tfd_von_mises_fisher_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_von_mises_fisher_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_von_mises_fisher_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical details
The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(x; mu, kappa) = C(kappa) exp(kappa * mu^T x)
where,
C(kappa) = (2 pi)^{-n/2} kappa^{n/2-1} / I_{n/2-1}(kappa),
I_v(z) being the modified Bessel function of the first kind of order v
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>mean_direction = mu</code>; a unit vector in <code>R^k</code>,
</p>
</li>
<li> <p><code>concentration = kappa</code>; scalar real &gt;= 0, concentration of samples around
<code>mean_direction</code>, where 0 pertains to the uniform distribution on the
hypersphere, and <code>inf</code> indicates a delta function at <code>mean_direction</code>.
</p>
</li></ul>

<p>NOTE: Currently only n in 2, 3, 4, 5 are supported. For n=5 some numerical
instability can occur for low concentrations (&lt;.01).
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_weibull'>The Weibull distribution with 'concentration' and <code>scale</code> parameters.</h2><span id='topic+tfd_weibull'></span>

<h3>Description</h3>

<p>The probability density function (pdf) of this distribution is,
</p>
<div class="sourceCode"><pre>pdf(x; lambda, k) = k / lambda * (x / lambda) ** (k - 1) * exp(-(x / lambda) ** k)
</pre></div>
<p>where <code>concentration = k</code> and <code>scale = lambda</code>.
The cumulative density function of this distribution is,
</p>
<div class="sourceCode"><pre>cdf(x; lambda, k) = 1 - exp(-(x / lambda) ** k)
</pre></div>
<p>The Weibull distribution includes the Exponential and Rayleigh distributions
as special cases:
</p>
<div class="sourceCode"><pre>Exponential(rate) = Weibull(concentration=1., 1. / rate)
</pre></div>
<div class="sourceCode"><pre>Rayleigh(scale) = Weibull(concentration=2., sqrt(2.) * scale)
</pre></div>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_weibull(
  concentration,
  scale,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Weibull"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_weibull_+3A_concentration">concentration</code></td>
<td>
<p>Positive Float-type <code>Tensor</code>, the concentration param of the
distribution. Must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_weibull_+3A_scale">scale</code></td>
<td>
<p>Positive Float-type <code>Tensor</code>, the scale param of the distribution.
Must contain only positive values.</p>
</td></tr>
<tr><td><code id="tfd_weibull_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_weibull_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_weibull_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_wishart'>The matrix Wishart distribution on positive definite matrices</h2><span id='topic+tfd_wishart'></span>

<h3>Description</h3>

<p>This distribution is defined by a scalar number of degrees of freedom df and
an instance of LinearOperator, which provides matrix-free access to a
symmetric positive definite operator, which defines the scale matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_wishart(
  df,
  scale = NULL,
  scale_tril = NULL,
  input_output_cholesky = FALSE,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "Wishart"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_wishart_+3A_df">df</code></td>
<td>
<p>float or double tensor, the degrees of freedom of the
distribution(s). df must be greater than or equal to k.</p>
</td></tr>
<tr><td><code id="tfd_wishart_+3A_scale">scale</code></td>
<td>
<p>float or double Tensor. The symmetric positive definite
scale matrix of the distribution. Exactly one of scale and 'scale_tril must be passed.</p>
</td></tr>
<tr><td><code id="tfd_wishart_+3A_scale_tril">scale_tril</code></td>
<td>
<p>float or double Tensor. The Cholesky factorization
of the symmetric positive definite scale matrix of the distribution.
Exactly one of scale and 'scale_tril must be passed.</p>
</td></tr>
<tr><td><code id="tfd_wishart_+3A_input_output_cholesky">input_output_cholesky</code></td>
<td>
<p>Logical. If TRUE, functions whose input or
output have the semantics of samples assume inputs are in Cholesky form
and return outputs in Cholesky form. In particular, if this flag is
TRUE, input to log_prob is presumed of Cholesky form and output from
sample, mean, and mode are of Cholesky form.  Setting this
argument to TRUE is purely a computational optimization and does not
change the underlying distribution; for instance, mean returns the
Cholesky of the mean, not the mean of Cholesky factors. The variance
and stddev methods are unaffected by this flag.
Default value: FALSE (i.e., input/output does not have Cholesky semantics).</p>
</td></tr>
<tr><td><code id="tfd_wishart_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_wishart_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_wishart_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(X; df, scale) = det(X)**(0.5 (df-k-1)) exp(-0.5 tr[inv(scale) X]) / Z
Z = 2**(0.5 df k) |det(scale)|**(0.5 df) Gamma_k(0.5 df)
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>df &gt;= k</code> denotes the degrees of freedom,
</p>
</li>
<li> <p><code>scale</code> is a symmetric, positive definite, <code style="white-space: pre;">&#8288;k x k&#8288;</code> matrix,
</p>
</li>
<li> <p><code>Z</code> is the normalizing constant, and,
</p>
</li>
<li> <p><code>Gamma_k</code> is the <a href="https://en.wikipedia.org/wiki/Multivariate_gamma_function">multivariate Gamma function</a>.
</p>
</li></ul>



<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_wishart_linear_operator'>The matrix Wishart distribution on positive definite matrices</h2><span id='topic+tfd_wishart_linear_operator'></span>

<h3>Description</h3>

<p>This distribution is defined by a scalar number of degrees of freedom df and
an instance of LinearOperator, which provides matrix-free access to a
symmetric positive definite operator, which defines the scale matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_wishart_linear_operator(
  df,
  scale,
  input_output_cholesky = FALSE,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "WishartLinearOperator"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_wishart_linear_operator_+3A_df">df</code></td>
<td>
<p>float or double tensor, the degrees of freedom of the
distribution(s). df must be greater than or equal to k.</p>
</td></tr>
<tr><td><code id="tfd_wishart_linear_operator_+3A_scale">scale</code></td>
<td>
<p><code>float</code> or <code>double</code> instance of <code>LinearOperator</code>.</p>
</td></tr>
<tr><td><code id="tfd_wishart_linear_operator_+3A_input_output_cholesky">input_output_cholesky</code></td>
<td>
<p>Logical. If TRUE, functions whose input or
output have the semantics of samples assume inputs are in Cholesky form
and return outputs in Cholesky form. In particular, if this flag is
TRUE, input to log_prob is presumed of Cholesky form and output from
sample, mean, and mode are of Cholesky form.  Setting this
argument to TRUE is purely a computational optimization and does not
change the underlying distribution; for instance, mean returns the
Cholesky of the mean, not the mean of Cholesky factors. The variance
and stddev methods are unaffected by this flag.
Default value: FALSE (i.e., input/output does not have Cholesky semantics).</p>
</td></tr>
<tr><td><code id="tfd_wishart_linear_operator_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_wishart_linear_operator_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_wishart_linear_operator_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(X; df, scale) = det(X)**(0.5 (df-k-1)) exp(-0.5 tr[inv(scale) X]) / Z
Z = 2**(0.5 df k) |det(scale)|**(0.5 df) Gamma_k(0.5 df)
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>df &gt;= k</code> denotes the degrees of freedom,
</p>
</li>
<li> <p><code>scale</code> is a symmetric, positive definite, <code style="white-space: pre;">&#8288;k x k&#8288;</code> matrix,
</p>
</li>
<li> <p><code>Z</code> is the normalizing constant, and,
</p>
</li>
<li> <p><code>Gamma_k</code> is the <a href="https://en.wikipedia.org/wiki/Multivariate_gamma_function">multivariate Gamma function</a>.
</p>
</li></ul>



<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_wishart_tri_l'>The matrix Wishart distribution parameterized with Cholesky factors.</h2><span id='topic+tfd_wishart_tri_l'></span>

<h3>Description</h3>

<p>This distribution is defined by a scalar degrees of freedom <code>df</code> and a scale
matrix, expressed as a lower triangular Cholesky factor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_wishart_tri_l(
  df,
  scale_tril,
  input_output_cholesky = FALSE,
  validate_args = FALSE,
  allow_nan_stats = TRUE,
  name = "WishartTriL"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_wishart_tri_l_+3A_df">df</code></td>
<td>
<p>float or double tensor, the degrees of freedom of the
distribution(s). df must be greater than or equal to k.</p>
</td></tr>
<tr><td><code id="tfd_wishart_tri_l_+3A_scale_tril">scale_tril</code></td>
<td>
<p><code>float</code> or <code>double</code> <code>Tensor</code>. The Cholesky factorization
of the symmetric positive definite scale matrix of the distribution.</p>
</td></tr>
<tr><td><code id="tfd_wishart_tri_l_+3A_input_output_cholesky">input_output_cholesky</code></td>
<td>
<p>Logical. If TRUE, functions whose input or
output have the semantics of samples assume inputs are in Cholesky form
and return outputs in Cholesky form. In particular, if this flag is
TRUE, input to log_prob is presumed of Cholesky form and output from
sample, mean, and mode are of Cholesky form.  Setting this
argument to TRUE is purely a computational optimization and does not
change the underlying distribution; for instance, mean returns the
Cholesky of the mean, not the mean of Cholesky factors. The variance
and stddev methods are unaffected by this flag.
Default value: FALSE (i.e., input/output does not have Cholesky semantics).</p>
</td></tr>
<tr><td><code id="tfd_wishart_tri_l_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_wishart_tri_l_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p>
</td></tr>
<tr><td><code id="tfd_wishart_tri_l_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
</p>
<p>The probability density function (pdf) is,
</p>
<div class="sourceCode"><pre>pdf(X; df, scale) = det(X)**(0.5 (df-k-1)) exp(-0.5 tr[inv(scale) X]) / Z
Z = 2**(0.5 df k) |det(scale)|**(0.5 df) Gamma_k(0.5 df)
</pre></div>
<p>where:
</p>

<ul>
<li> <p><code>df &gt;= k</code> denotes the degrees of freedom,
</p>
</li>
<li> <p><code>scale</code> is a symmetric, positive definite, <code style="white-space: pre;">&#8288;k x k&#8288;</code> matrix,
</p>
</li>
<li> <p><code>Z</code> is the normalizing constant, and,
</p>
</li>
<li> <p><code>Gamma_k</code> is the <a href="https://en.wikipedia.org/wiki/Multivariate_gamma_function">multivariate Gamma function</a>.
</p>
</li></ul>



<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>,
<code><a href="#topic+tfd_zipf">tfd_zipf</a>()</code>
</p>

<hr>
<h2 id='tfd_zipf'>Zipf distribution</h2><span id='topic+tfd_zipf'></span>

<h3>Description</h3>

<p>The Zipf distribution is parameterized by a power parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfd_zipf(
  power,
  dtype = tf$int32,
  interpolate_nondiscrete = TRUE,
  sample_maximum_iterations = 100,
  validate_args = FALSE,
  allow_nan_stats = FALSE,
  name = "Zipf"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfd_zipf_+3A_power">power</code></td>
<td>
<p>Float like Tensor representing the power parameter. Must be
strictly greater than 1.</p>
</td></tr>
<tr><td><code id="tfd_zipf_+3A_dtype">dtype</code></td>
<td>
<p>The dtype of Tensor returned by sample. Default value: tf$int32.</p>
</td></tr>
<tr><td><code id="tfd_zipf_+3A_interpolate_nondiscrete">interpolate_nondiscrete</code></td>
<td>
<p>Logical. When FALSE, log_prob returns
-inf (and prob returns 0) for non-integer inputs. When TRUE,
log_prob evaluates the continuous function <code style="white-space: pre;">&#8288;-power log(k) -   log(zeta(power))&#8288;</code> ,
which matches the Zipf pmf at integer arguments k
(note that this function is not itself a normalized probability  log-density).
Default value: TRUE.</p>
</td></tr>
<tr><td><code id="tfd_zipf_+3A_sample_maximum_iterations">sample_maximum_iterations</code></td>
<td>
<p>Maximum number of iterations of allowable
iterations in sample. When validate_args=TRUE, samples which fail to
reach convergence (subject to this cap) are masked out with
<code>self$dtype$min</code> or nan depending on <code>self$dtype$is_integer</code>.
Default value: 100.</p>
</td></tr>
<tr><td><code id="tfd_zipf_+3A_validate_args">validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_zipf_+3A_allow_nan_stats">allow_nan_stats</code></td>
<td>
<p>Default value: FALSE.</p>
</td></tr>
<tr><td><code id="tfd_zipf_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Mathematical Details
The probability mass function (pmf) is,
</p>
<div class="sourceCode"><pre>pmf(k; alpha, k &gt;= 0) = (k^(-alpha)) / Z
Z = zeta(alpha).
</pre></div>
<p>where <code>power = alpha</code> and Z is the normalization constant.
<code>zeta</code> is the <a href="https://en.wikipedia.org/wiki/Riemann_zeta_function">Riemann zeta function</a>.
Note that gradients with respect to the <code>power</code> parameter are not
supported in the current implementation.
</p>


<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code><a href="#topic+tfd_sample">tfd_sample()</a></code>, <code><a href="#topic+tfd_log_prob">tfd_log_prob()</a></code>, <code><a href="#topic+tfd_mean">tfd_mean()</a></code>.
</p>
<p>Other distributions: 
<code><a href="#topic+tfd_autoregressive">tfd_autoregressive</a>()</code>,
<code><a href="#topic+tfd_batch_reshape">tfd_batch_reshape</a>()</code>,
<code><a href="#topic+tfd_bates">tfd_bates</a>()</code>,
<code><a href="#topic+tfd_bernoulli">tfd_bernoulli</a>()</code>,
<code><a href="#topic+tfd_beta_binomial">tfd_beta_binomial</a>()</code>,
<code><a href="#topic+tfd_beta">tfd_beta</a>()</code>,
<code><a href="#topic+tfd_binomial">tfd_binomial</a>()</code>,
<code><a href="#topic+tfd_categorical">tfd_categorical</a>()</code>,
<code><a href="#topic+tfd_cauchy">tfd_cauchy</a>()</code>,
<code><a href="#topic+tfd_chi2">tfd_chi2</a>()</code>,
<code><a href="#topic+tfd_chi">tfd_chi</a>()</code>,
<code><a href="#topic+tfd_cholesky_lkj">tfd_cholesky_lkj</a>()</code>,
<code><a href="#topic+tfd_continuous_bernoulli">tfd_continuous_bernoulli</a>()</code>,
<code><a href="#topic+tfd_deterministic">tfd_deterministic</a>()</code>,
<code><a href="#topic+tfd_dirichlet_multinomial">tfd_dirichlet_multinomial</a>()</code>,
<code><a href="#topic+tfd_dirichlet">tfd_dirichlet</a>()</code>,
<code><a href="#topic+tfd_empirical">tfd_empirical</a>()</code>,
<code><a href="#topic+tfd_exp_gamma">tfd_exp_gamma</a>()</code>,
<code><a href="#topic+tfd_exp_inverse_gamma">tfd_exp_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_exponential">tfd_exponential</a>()</code>,
<code><a href="#topic+tfd_gamma_gamma">tfd_gamma_gamma</a>()</code>,
<code><a href="#topic+tfd_gamma">tfd_gamma</a>()</code>,
<code><a href="#topic+tfd_gaussian_process_regression_model">tfd_gaussian_process_regression_model</a>()</code>,
<code><a href="#topic+tfd_gaussian_process">tfd_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_generalized_normal">tfd_generalized_normal</a>()</code>,
<code><a href="#topic+tfd_geometric">tfd_geometric</a>()</code>,
<code><a href="#topic+tfd_gumbel">tfd_gumbel</a>()</code>,
<code><a href="#topic+tfd_half_cauchy">tfd_half_cauchy</a>()</code>,
<code><a href="#topic+tfd_half_normal">tfd_half_normal</a>()</code>,
<code><a href="#topic+tfd_hidden_markov_model">tfd_hidden_markov_model</a>()</code>,
<code><a href="#topic+tfd_horseshoe">tfd_horseshoe</a>()</code>,
<code><a href="#topic+tfd_independent">tfd_independent</a>()</code>,
<code><a href="#topic+tfd_inverse_gamma">tfd_inverse_gamma</a>()</code>,
<code><a href="#topic+tfd_inverse_gaussian">tfd_inverse_gaussian</a>()</code>,
<code><a href="#topic+tfd_johnson_s_u">tfd_johnson_s_u</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named_auto_batched">tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_named">tfd_joint_distribution_named</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential_auto_batched">tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href="#topic+tfd_joint_distribution_sequential">tfd_joint_distribution_sequential</a>()</code>,
<code><a href="#topic+tfd_kumaraswamy">tfd_kumaraswamy</a>()</code>,
<code><a href="#topic+tfd_laplace">tfd_laplace</a>()</code>,
<code><a href="#topic+tfd_linear_gaussian_state_space_model">tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href="#topic+tfd_lkj">tfd_lkj</a>()</code>,
<code><a href="#topic+tfd_log_logistic">tfd_log_logistic</a>()</code>,
<code><a href="#topic+tfd_log_normal">tfd_log_normal</a>()</code>,
<code><a href="#topic+tfd_logistic">tfd_logistic</a>()</code>,
<code><a href="#topic+tfd_mixture_same_family">tfd_mixture_same_family</a>()</code>,
<code><a href="#topic+tfd_mixture">tfd_mixture</a>()</code>,
<code><a href="#topic+tfd_multinomial">tfd_multinomial</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag_plus_low_rank">tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_diag">tfd_multivariate_normal_diag</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_full_covariance">tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_linear_operator">tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href="#topic+tfd_multivariate_normal_tri_l">tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href="#topic+tfd_multivariate_student_t_linear_operator">tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href="#topic+tfd_negative_binomial">tfd_negative_binomial</a>()</code>,
<code><a href="#topic+tfd_normal">tfd_normal</a>()</code>,
<code><a href="#topic+tfd_one_hot_categorical">tfd_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_pareto">tfd_pareto</a>()</code>,
<code><a href="#topic+tfd_pixel_cnn">tfd_pixel_cnn</a>()</code>,
<code><a href="#topic+tfd_poisson_log_normal_quadrature_compound">tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href="#topic+tfd_poisson">tfd_poisson</a>()</code>,
<code><a href="#topic+tfd_power_spherical">tfd_power_spherical</a>()</code>,
<code><a href="#topic+tfd_probit_bernoulli">tfd_probit_bernoulli</a>()</code>,
<code><a href="#topic+tfd_quantized">tfd_quantized</a>()</code>,
<code><a href="#topic+tfd_relaxed_bernoulli">tfd_relaxed_bernoulli</a>()</code>,
<code><a href="#topic+tfd_relaxed_one_hot_categorical">tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href="#topic+tfd_sample_distribution">tfd_sample_distribution</a>()</code>,
<code><a href="#topic+tfd_sinh_arcsinh">tfd_sinh_arcsinh</a>()</code>,
<code><a href="#topic+tfd_skellam">tfd_skellam</a>()</code>,
<code><a href="#topic+tfd_spherical_uniform">tfd_spherical_uniform</a>()</code>,
<code><a href="#topic+tfd_student_t_process">tfd_student_t_process</a>()</code>,
<code><a href="#topic+tfd_student_t">tfd_student_t</a>()</code>,
<code><a href="#topic+tfd_transformed_distribution">tfd_transformed_distribution</a>()</code>,
<code><a href="#topic+tfd_triangular">tfd_triangular</a>()</code>,
<code><a href="#topic+tfd_truncated_cauchy">tfd_truncated_cauchy</a>()</code>,
<code><a href="#topic+tfd_truncated_normal">tfd_truncated_normal</a>()</code>,
<code><a href="#topic+tfd_uniform">tfd_uniform</a>()</code>,
<code><a href="#topic+tfd_variational_gaussian_process">tfd_variational_gaussian_process</a>()</code>,
<code><a href="#topic+tfd_vector_diffeomixture">tfd_vector_diffeomixture</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_diag">tfd_vector_exponential_diag</a>()</code>,
<code><a href="#topic+tfd_vector_exponential_linear_operator">tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_diag">tfd_vector_laplace_diag</a>()</code>,
<code><a href="#topic+tfd_vector_laplace_linear_operator">tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href="#topic+tfd_vector_sinh_arcsinh_diag">tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href="#topic+tfd_von_mises_fisher">tfd_von_mises_fisher</a>()</code>,
<code><a href="#topic+tfd_von_mises">tfd_von_mises</a>()</code>,
<code><a href="#topic+tfd_weibull">tfd_weibull</a>()</code>,
<code><a href="#topic+tfd_wishart_linear_operator">tfd_wishart_linear_operator</a>()</code>,
<code><a href="#topic+tfd_wishart_tri_l">tfd_wishart_tri_l</a>()</code>,
<code><a href="#topic+tfd_wishart">tfd_wishart</a>()</code>
</p>

<hr>
<h2 id='tfp'>Handle to the <code>tensorflow_probability</code> module</h2><span id='topic+tfp'></span>

<h3>Description</h3>

<p>Handle to the <code>tensorflow_probability</code> module
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfp
</code></pre>


<h3>Format</h3>

<p>An object of class <code>python.builtin.module</code> (inherits from <code>python.builtin.object</code>) of length 0.
</p>


<h3>Value</h3>

<p>Module(tensorflow_probability)
</p>

<hr>
<h2 id='tfp_version'>TensorFlow Probability Version</h2><span id='topic+tfp_version'></span>

<h3>Description</h3>

<p>TensorFlow Probability Version
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfp_version()
</code></pre>


<h3>Value</h3>

<p>the Python TFP version
</p>

<hr>
<h2 id='vi_amari_alpha'>The Amari-alpha Csiszar-function in log-space</h2><span id='topic+vi_amari_alpha'></span>

<h3>Description</h3>

<p>A Csiszar-function is a member of <code style="white-space: pre;">&#8288;F = { f:R_+ to R : f convex }&#8288;</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi_amari_alpha(logu, alpha = 1, self_normalized = FALSE, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vi_amari_alpha_+3A_logu">logu</code></td>
<td>
<p><code>float</code>-like <code>Tensor</code> representing <code>log(u)</code> from above.</p>
</td></tr>
<tr><td><code id="vi_amari_alpha_+3A_alpha">alpha</code></td>
<td>
<p><code>float</code>-like scalar.</p>
</td></tr>
<tr><td><code id="vi_amari_alpha_+3A_self_normalized">self_normalized</code></td>
<td>
<p><code>logical</code> indicating whether <code style="white-space: pre;">&#8288;f'(u=1)=0&#8288;</code>. When
<code style="white-space: pre;">&#8288;f'(u=1)=0&#8288;</code> the implied Csiszar f-Divergence remains non-negative even
when <code style="white-space: pre;">&#8288;p, q&#8288;</code> are unnormalized measures.</p>
</td></tr>
<tr><td><code id="vi_amari_alpha_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>self_normalized = TRUE</code>, the Amari-alpha Csiszar-function is:
</p>
<div class="sourceCode"><pre>f(u) = { -log(u) + (u - 1)},     alpha = 0
       { u log(u) - (u - 1)},    alpha = 1
       { ((u^alpha - 1) - alpha (u - 1) / (alpha (alpha - 1))},    otherwise
</pre></div>
<p>When <code>self_normalized = FALSE</code> the <code>(u - 1)</code> terms are omitted.
</p>
<p>Warning: when <code>alpha != 0</code> and/or <code>self_normalized = True</code> this function makes
non-log-space calculations and may therefore be numerically unstable for
<code style="white-space: pre;">&#8288;|logu| &gt;&gt; 0&#8288;</code>.
</p>


<h3>Value</h3>

<p>amari_alpha_of_u <code>float</code>-like <code>Tensor</code> of the Csiszar-function evaluated
at <code>u = exp(logu)</code>.
</p>


<h3>References</h3>


<ul>
<li><p> A. Cichocki and S. Amari. &quot;Families of Alpha-Beta-and GammaDivergences: Flexible and Robust Measures of Similarities.&quot; Entropy, vol. 12, no. 6, pp. 1532-1568, 2010.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other vi-functions: 
<code><a href="#topic+vi_arithmetic_geometric">vi_arithmetic_geometric</a>()</code>,
<code><a href="#topic+vi_chi_square">vi_chi_square</a>()</code>,
<code><a href="#topic+vi_csiszar_vimco">vi_csiszar_vimco</a>()</code>,
<code><a href="#topic+vi_dual_csiszar_function">vi_dual_csiszar_function</a>()</code>,
<code><a href="#topic+vi_fit_surrogate_posterior">vi_fit_surrogate_posterior</a>()</code>,
<code><a href="#topic+vi_jeffreys">vi_jeffreys</a>()</code>,
<code><a href="#topic+vi_jensen_shannon">vi_jensen_shannon</a>()</code>,
<code><a href="#topic+vi_kl_forward">vi_kl_forward</a>()</code>,
<code><a href="#topic+vi_kl_reverse">vi_kl_reverse</a>()</code>,
<code><a href="#topic+vi_log1p_abs">vi_log1p_abs</a>()</code>,
<code><a href="#topic+vi_modified_gan">vi_modified_gan</a>()</code>,
<code><a href="#topic+vi_monte_carlo_variational_loss">vi_monte_carlo_variational_loss</a>()</code>,
<code><a href="#topic+vi_pearson">vi_pearson</a>()</code>,
<code><a href="#topic+vi_squared_hellinger">vi_squared_hellinger</a>()</code>,
<code><a href="#topic+vi_symmetrized_csiszar_function">vi_symmetrized_csiszar_function</a>()</code>
</p>

<hr>
<h2 id='vi_arithmetic_geometric'>The Arithmetic-Geometric Csiszar-function in log-space</h2><span id='topic+vi_arithmetic_geometric'></span>

<h3>Description</h3>

<p>A Csiszar-function is a member of <code style="white-space: pre;">&#8288;F = { f:R_+ to R : f convex }&#8288;</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi_arithmetic_geometric(logu, self_normalized = FALSE, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vi_arithmetic_geometric_+3A_logu">logu</code></td>
<td>
<p><code>float</code>-like <code>Tensor</code> representing <code>log(u)</code> from above.</p>
</td></tr>
<tr><td><code id="vi_arithmetic_geometric_+3A_self_normalized">self_normalized</code></td>
<td>
<p><code>logical</code> indicating whether <code style="white-space: pre;">&#8288;f'(u=1)=0&#8288;</code>. When
<code style="white-space: pre;">&#8288;f'(u=1)=0&#8288;</code> the implied Csiszar f-Divergence remains non-negative even
when <code style="white-space: pre;">&#8288;p, q&#8288;</code> are unnormalized measures.</p>
</td></tr>
<tr><td><code id="vi_arithmetic_geometric_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>self_normalized = True</code> the Arithmetic-Geometric Csiszar-function is:
</p>
<div class="sourceCode"><pre>f(u) = (1 + u) log( (1 + u) / sqrt(u) ) - (1 + u) log(2)
</pre></div>
<p>When <code>self_normalized = False</code> the <code style="white-space: pre;">&#8288;(1 + u) log(2)&#8288;</code> term is omitted.
</p>
<p>Observe that as an f-Divergence, this Csiszar-function implies:
</p>
<div class="sourceCode"><pre>D_f[p, q] = KL[m, p] + KL[m, q]
m(x) = 0.5 p(x) + 0.5 q(x)
</pre></div>
<p>In a sense, this divergence is the &quot;reverse&quot; of the Jensen-Shannon
f-Divergence.
This Csiszar-function induces a symmetric f-Divergence, i.e.,
<code>D_f[p, q] = D_f[q, p]</code>.
</p>
<p>Warning: when self_normalized = True<code style="white-space: pre;">&#8288;this function makes non-log-space calculations and may therefore be numerically unstable for&#8288;</code>|logu| &gt;&gt; 0'.
</p>


<h3>Value</h3>

<p>arithmetic_geometric_of_u: <code>float</code>-like <code>Tensor</code> of the
Csiszar-function evaluated at <code>u = exp(logu)</code>.
</p>


<h3>See Also</h3>

<p>Other vi-functions: 
<code><a href="#topic+vi_amari_alpha">vi_amari_alpha</a>()</code>,
<code><a href="#topic+vi_chi_square">vi_chi_square</a>()</code>,
<code><a href="#topic+vi_csiszar_vimco">vi_csiszar_vimco</a>()</code>,
<code><a href="#topic+vi_dual_csiszar_function">vi_dual_csiszar_function</a>()</code>,
<code><a href="#topic+vi_fit_surrogate_posterior">vi_fit_surrogate_posterior</a>()</code>,
<code><a href="#topic+vi_jeffreys">vi_jeffreys</a>()</code>,
<code><a href="#topic+vi_jensen_shannon">vi_jensen_shannon</a>()</code>,
<code><a href="#topic+vi_kl_forward">vi_kl_forward</a>()</code>,
<code><a href="#topic+vi_kl_reverse">vi_kl_reverse</a>()</code>,
<code><a href="#topic+vi_log1p_abs">vi_log1p_abs</a>()</code>,
<code><a href="#topic+vi_modified_gan">vi_modified_gan</a>()</code>,
<code><a href="#topic+vi_monte_carlo_variational_loss">vi_monte_carlo_variational_loss</a>()</code>,
<code><a href="#topic+vi_pearson">vi_pearson</a>()</code>,
<code><a href="#topic+vi_squared_hellinger">vi_squared_hellinger</a>()</code>,
<code><a href="#topic+vi_symmetrized_csiszar_function">vi_symmetrized_csiszar_function</a>()</code>
</p>

<hr>
<h2 id='vi_chi_square'>The chi-square Csiszar-function in log-space</h2><span id='topic+vi_chi_square'></span>

<h3>Description</h3>

<p>A Csiszar-function is a member of <code style="white-space: pre;">&#8288;F = { f:R_+ to R : f convex }&#8288;</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi_chi_square(logu, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vi_chi_square_+3A_logu">logu</code></td>
<td>
<p><code>float</code>-like <code>Tensor</code> representing <code>log(u)</code> from above.</p>
</td></tr>
<tr><td><code id="vi_chi_square_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Chi-square Csiszar-function is:
</p>
<div class="sourceCode"><pre>f(u) = u**2 - 1
</pre></div>
<p>Warning: this function makes non-log-space calculations and may
therefore be numerically unstable for <code style="white-space: pre;">&#8288;|logu| &gt;&gt; 0&#8288;</code>.
</p>


<h3>Value</h3>

<p>chi_square_of_u: <code>float</code>-like <code>Tensor</code> of the Csiszar-function
evaluated at <code>u = exp(logu)</code>.
</p>


<h3>See Also</h3>

<p>Other vi-functions: 
<code><a href="#topic+vi_amari_alpha">vi_amari_alpha</a>()</code>,
<code><a href="#topic+vi_arithmetic_geometric">vi_arithmetic_geometric</a>()</code>,
<code><a href="#topic+vi_csiszar_vimco">vi_csiszar_vimco</a>()</code>,
<code><a href="#topic+vi_dual_csiszar_function">vi_dual_csiszar_function</a>()</code>,
<code><a href="#topic+vi_fit_surrogate_posterior">vi_fit_surrogate_posterior</a>()</code>,
<code><a href="#topic+vi_jeffreys">vi_jeffreys</a>()</code>,
<code><a href="#topic+vi_jensen_shannon">vi_jensen_shannon</a>()</code>,
<code><a href="#topic+vi_kl_forward">vi_kl_forward</a>()</code>,
<code><a href="#topic+vi_kl_reverse">vi_kl_reverse</a>()</code>,
<code><a href="#topic+vi_log1p_abs">vi_log1p_abs</a>()</code>,
<code><a href="#topic+vi_modified_gan">vi_modified_gan</a>()</code>,
<code><a href="#topic+vi_monte_carlo_variational_loss">vi_monte_carlo_variational_loss</a>()</code>,
<code><a href="#topic+vi_pearson">vi_pearson</a>()</code>,
<code><a href="#topic+vi_squared_hellinger">vi_squared_hellinger</a>()</code>,
<code><a href="#topic+vi_symmetrized_csiszar_function">vi_symmetrized_csiszar_function</a>()</code>
</p>

<hr>
<h2 id='vi_csiszar_vimco'>Use VIMCO to lower the variance of the gradient of csiszar_function(Avg(logu))</h2><span id='topic+vi_csiszar_vimco'></span>

<h3>Description</h3>

<p>This function generalizes VIMCO (Mnih and Rezende, 2016) to Csiszar
f-Divergences.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi_csiszar_vimco(
  f,
  p_log_prob,
  q,
  num_draws,
  num_batch_draws = 1,
  seed = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vi_csiszar_vimco_+3A_f">f</code></td>
<td>
<p>function representing a Csiszar-function in log-space.</p>
</td></tr>
<tr><td><code id="vi_csiszar_vimco_+3A_p_log_prob">p_log_prob</code></td>
<td>
<p>function representing the natural-log of the
probability under distribution <code>p</code>. (In variational inference <code>p</code> is the
joint distribution.)</p>
</td></tr>
<tr><td><code id="vi_csiszar_vimco_+3A_q">q</code></td>
<td>
<p><code>tfd$Distribution</code>-like instance; must implement: <code>sample(n, seed)</code>, and
<code>log_prob(x)</code>. (In variational inference <code>q</code> is the approximate posterior
distribution.)</p>
</td></tr>
<tr><td><code id="vi_csiszar_vimco_+3A_num_draws">num_draws</code></td>
<td>
<p>Integer scalar number of draws used to approximate the
f-Divergence expectation.</p>
</td></tr>
<tr><td><code id="vi_csiszar_vimco_+3A_num_batch_draws">num_batch_draws</code></td>
<td>
<p>Integer scalar number of draws used to approximate the
f-Divergence expectation.</p>
</td></tr>
<tr><td><code id="vi_csiszar_vimco_+3A_seed">seed</code></td>
<td>
<p><code>integer</code> seed for <code>q$sample</code>.</p>
</td></tr>
<tr><td><code id="vi_csiszar_vimco_+3A_name">name</code></td>
<td>
<p>String prefixed to Ops created by this function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note: if <code>q.reparameterization_type = tfd.FULLY_REPARAMETERIZED</code>,
consider using <code>monte_carlo_csiszar_f_divergence</code>.
</p>
<p>The VIMCO loss is:
</p>
<div class="sourceCode"><pre>vimco = f(Avg{logu[i] : i=0,...,m-1})
where,
logu[i] = log( p(x, h[i]) / q(h[i] | x) )
h[i] iid~ q(H | x)
</pre></div>
<p>Interestingly, the VIMCO gradient is not the naive gradient of <code>vimco</code>.
Rather, it is characterized by:
</p>
<div class="sourceCode"><pre>grad[vimco] - variance_reducing_term
</pre></div>
<p>where,
</p>
<div class="sourceCode"><pre>variance_reducing_term = Sum{ grad[log q(h[i] | x)] * (vimco - f(log Avg{h[j;i] : j=0,...,m-1})) #' : i=0, ..., m-1 }
h[j;i] =  u[j]  for j!=i,  GeometricAverage{ u[k] : k!=i} for j==i
</pre></div>
<p>(We omitted <code>stop_gradient</code> for brevity. See implementation for more details.)
The <code style="white-space: pre;">&#8288;Avg{h[j;i] : j}&#8288;</code> term is a kind of &quot;swap-out average&quot; where the <code>i</code>-th
element has been replaced by the leave-<code>i</code>-out Geometric-average.
</p>
<p>This implementation prefers numerical precision over efficiency, i.e.,
<code>O(num_draws * num_batch_draws * prod(batch_shape) * prod(event_shape))</code>.
(The constant may be fairly large, perhaps around 12.)
</p>


<h3>Value</h3>

<p>vimco The Csiszar f-Divergence generalized VIMCO objective
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1602.06725">Andriy Mnih and Danilo Rezende. Variational Inference for Monte Carlo objectives. In <em>International Conference on Machine Learning</em>, 2016.</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other vi-functions: 
<code><a href="#topic+vi_amari_alpha">vi_amari_alpha</a>()</code>,
<code><a href="#topic+vi_arithmetic_geometric">vi_arithmetic_geometric</a>()</code>,
<code><a href="#topic+vi_chi_square">vi_chi_square</a>()</code>,
<code><a href="#topic+vi_dual_csiszar_function">vi_dual_csiszar_function</a>()</code>,
<code><a href="#topic+vi_fit_surrogate_posterior">vi_fit_surrogate_posterior</a>()</code>,
<code><a href="#topic+vi_jeffreys">vi_jeffreys</a>()</code>,
<code><a href="#topic+vi_jensen_shannon">vi_jensen_shannon</a>()</code>,
<code><a href="#topic+vi_kl_forward">vi_kl_forward</a>()</code>,
<code><a href="#topic+vi_kl_reverse">vi_kl_reverse</a>()</code>,
<code><a href="#topic+vi_log1p_abs">vi_log1p_abs</a>()</code>,
<code><a href="#topic+vi_modified_gan">vi_modified_gan</a>()</code>,
<code><a href="#topic+vi_monte_carlo_variational_loss">vi_monte_carlo_variational_loss</a>()</code>,
<code><a href="#topic+vi_pearson">vi_pearson</a>()</code>,
<code><a href="#topic+vi_squared_hellinger">vi_squared_hellinger</a>()</code>,
<code><a href="#topic+vi_symmetrized_csiszar_function">vi_symmetrized_csiszar_function</a>()</code>
</p>

<hr>
<h2 id='vi_dual_csiszar_function'>Calculates the dual Csiszar-function in log-space</h2><span id='topic+vi_dual_csiszar_function'></span>

<h3>Description</h3>

<p>A Csiszar-function is a member of <code style="white-space: pre;">&#8288;F = { f:R_+ to R : f convex }&#8288;</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi_dual_csiszar_function(logu, csiszar_function, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vi_dual_csiszar_function_+3A_logu">logu</code></td>
<td>
<p><code>float</code>-like <code>Tensor</code> representing <code>log(u)</code> from above.</p>
</td></tr>
<tr><td><code id="vi_dual_csiszar_function_+3A_csiszar_function">csiszar_function</code></td>
<td>
<p>function representing a Csiszar-function over log-domain.</p>
</td></tr>
<tr><td><code id="vi_dual_csiszar_function_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Csiszar-dual is defined as:
</p>
<div class="sourceCode"><pre>f^*(u) = u f(1 / u)
</pre></div>
<p>where <code>f</code> is some other Csiszar-function.
For example, the dual of <code>kl_reverse</code> is <code>kl_forward</code>, i.e.,
</p>
<div class="sourceCode"><pre>f(u) = -log(u)
f^*(u) = u f(1 / u) = -u log(1 / u) = u log(u)
</pre></div>
<p>The dual of the dual is the original function:
</p>
<div class="sourceCode"><pre>f^**(u) = {u f(1/u)}^*(u) = u (1/u) f(1/(1/u)) = f(u)
</pre></div>
<p>Warning: this function makes non-log-space calculations and may therefore be
numerically unstable for <code style="white-space: pre;">&#8288;|logu| &gt;&gt; 0&#8288;</code>.
</p>


<h3>Value</h3>

<p>dual_f_of_u <code>float</code>-like <code>Tensor</code> of the result of calculating the dual of
<code>f</code> at <code>u = exp(logu)</code>.
</p>


<h3>See Also</h3>

<p>Other vi-functions: 
<code><a href="#topic+vi_amari_alpha">vi_amari_alpha</a>()</code>,
<code><a href="#topic+vi_arithmetic_geometric">vi_arithmetic_geometric</a>()</code>,
<code><a href="#topic+vi_chi_square">vi_chi_square</a>()</code>,
<code><a href="#topic+vi_csiszar_vimco">vi_csiszar_vimco</a>()</code>,
<code><a href="#topic+vi_fit_surrogate_posterior">vi_fit_surrogate_posterior</a>()</code>,
<code><a href="#topic+vi_jeffreys">vi_jeffreys</a>()</code>,
<code><a href="#topic+vi_jensen_shannon">vi_jensen_shannon</a>()</code>,
<code><a href="#topic+vi_kl_forward">vi_kl_forward</a>()</code>,
<code><a href="#topic+vi_kl_reverse">vi_kl_reverse</a>()</code>,
<code><a href="#topic+vi_log1p_abs">vi_log1p_abs</a>()</code>,
<code><a href="#topic+vi_modified_gan">vi_modified_gan</a>()</code>,
<code><a href="#topic+vi_monte_carlo_variational_loss">vi_monte_carlo_variational_loss</a>()</code>,
<code><a href="#topic+vi_pearson">vi_pearson</a>()</code>,
<code><a href="#topic+vi_squared_hellinger">vi_squared_hellinger</a>()</code>,
<code><a href="#topic+vi_symmetrized_csiszar_function">vi_symmetrized_csiszar_function</a>()</code>
</p>

<hr>
<h2 id='vi_fit_surrogate_posterior'>Fit a surrogate posterior to a target (unnormalized) log density</h2><span id='topic+vi_fit_surrogate_posterior'></span>

<h3>Description</h3>

<p>The default behavior constructs and minimizes the negative variational
evidence lower bound (ELBO), given by <code style="white-space: pre;">&#8288;q_samples &lt;- surrogate_posterior$sample(num_draws) elbo_loss &lt;- -tf$reduce_mean(target_log_prob_fn(q_samples) - surrogate_posterior$log_prob(q_samples))&#8288;</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi_fit_surrogate_posterior(
  target_log_prob_fn,
  surrogate_posterior,
  optimizer,
  num_steps,
  convergence_criterion = NULL,
  trace_fn = tfp$vi$optimization$`_trace_loss`,
  variational_loss_fn = NULL,
  discrepancy_fn = tfp$vi$kl_reverse,
  sample_size = 1,
  importance_sample_size = 1,
  trainable_variables = NULL,
  jit_compile = NULL,
  seed = NULL,
  name = "fit_surrogate_posterior"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vi_fit_surrogate_posterior_+3A_target_log_prob_fn">target_log_prob_fn</code></td>
<td>
<p>function that takes a set of <code>Tensor</code> arguments
and returns a <code>Tensor</code> log-density. Given <code>q_sample &lt;- surrogate_posterior$sample(sample_size)</code>, this will be (in Python)
called as <code>target_log_prob_fn(q_sample)</code> if <code>q_sample</code> is a list or a
tuple, <code style="white-space: pre;">&#8288;target_log_prob_fn(**q_sample)&#8288;</code> if <code>q_sample</code> is a dictionary,
or <code>target_log_prob_fn(q_sample)</code> if <code>q_sample</code> is a <code>Tensor</code>. It
should support batched evaluation, i.e., should return a result of
shape <code style="white-space: pre;">&#8288;[sample_size]&#8288;</code>.</p>
</td></tr>
<tr><td><code id="vi_fit_surrogate_posterior_+3A_surrogate_posterior">surrogate_posterior</code></td>
<td>
<p>A <code>tfp$distributions$Distribution</code> instance
defining a variational posterior (could be a
<code>tfp$distributions$JointDistribution</code>). Crucially, the distribution's
<code>log_prob</code> and (if reparameterized) <code>sample</code> methods must directly
invoke all ops that generate gradients to the underlying variables. One
way to ensure this is to use <code>tfp$util$DeferredTensor</code> to represent any
parameters defined as transformations of unconstrained variables, so
that the transformations execute at runtime instead of at distribution
creation.</p>
</td></tr>
<tr><td><code id="vi_fit_surrogate_posterior_+3A_optimizer">optimizer</code></td>
<td>
<p>Optimizer instance to use. This may be a TF1-style
<code>tf$train$Optimizer</code>, TF2-style <code>tf$optimizers$Optimizer</code>, or any
Python-compatible object that implements
<code>optimizer$apply_gradients(grads_and_vars)</code>.</p>
</td></tr>
<tr><td><code id="vi_fit_surrogate_posterior_+3A_num_steps">num_steps</code></td>
<td>
<p><code>integer</code> number of steps to run the optimizer.</p>
</td></tr>
<tr><td><code id="vi_fit_surrogate_posterior_+3A_convergence_criterion">convergence_criterion</code></td>
<td>
<p>Optional instance of
<code>tfp$optimizer$convergence_criteria$ConvergenceCriterion</code> representing
a criterion for detecting convergence. If <code>NULL</code>, the optimization will
run for <code>num_steps</code> steps, otherwise, it will run for at <em>most</em>
<code>num_steps</code> steps, as determined by the provided criterion. Default
value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="vi_fit_surrogate_posterior_+3A_trace_fn">trace_fn</code></td>
<td>
<p>function with signature <code>state = trace_fn(loss, grads, variables)</code>, where <code>state</code> may be a <code>Tensor</code> or nested structure of
<code>Tensor</code>s. The state values are accumulated (by <code>tf$scan</code>) and
returned. The default <code>trace_fn</code> simply returns the loss, but in
general can depend on the gradients and variables (if
<code>trainable_variables</code> is not <code>NULL</code> then
<code>variables==trainable_variables</code>; otherwise it is the list of all
variables accessed during execution of <code>loss_fn()</code>), as well as any
other quantities captured in the closure of <code>trace_fn</code>, for example,
statistics of a variational distribution. Default value:
<code>function(loss, grads, variables) loss</code>.</p>
</td></tr>
<tr><td><code id="vi_fit_surrogate_posterior_+3A_variational_loss_fn">variational_loss_fn</code></td>
<td>
<p>function with signature <code>loss &lt;- variational_loss_fn(target_log_prob_fn, surrogate_posterior, sample_size, seed)</code> defining a variational loss function. The default
is a Monte Carlo approximation to the standard evidence lower bound
(ELBO), equivalent to minimizing the 'reverse' <code>KL[q||p]</code> divergence
between the surrogate <code>q</code> and true posterior <code>p</code>. Default value:
<code>functools.partial(tfp.vi.monte_carlo_variational_loss, discrepancy_fn=tfp.vi.kl_reverse, use_reparameterization=True)</code>.</p>
</td></tr>
<tr><td><code id="vi_fit_surrogate_posterior_+3A_discrepancy_fn">discrepancy_fn</code></td>
<td>
<p>A function of Python <code>callable</code> representing a
Csiszar <code>f</code> function in log-space. See the docs for
<code>tfp.vi.monte_carlo_variational_loss</code> for examples. This argument is
ignored if a <code>variational_loss_fn</code> is explicitly specified. Default
value: <code>tfp$vi$kl_reverse</code>.</p>
</td></tr>
<tr><td><code id="vi_fit_surrogate_posterior_+3A_sample_size">sample_size</code></td>
<td>
<p><code>integer</code> number of Monte Carlo samples to use in
estimating the variational divergence. Larger values may stabilize the
optimization, but at higher cost per step in time and memory. Default
value: <code>1</code>.</p>
</td></tr>
<tr><td><code id="vi_fit_surrogate_posterior_+3A_importance_sample_size">importance_sample_size</code></td>
<td>
<p>An integer number of terms used to define
an importance-weighted divergence. If <code>importance_sample_size &gt; 1</code>,
then the <code>surrogate_posterior</code> is optimized to function as an
importance-sampling proposal distribution. In this case, posterior
expectations should be approximated by importance sampling, as
demonstrated in the example below. This argument is ignored if a
<code>variational_loss_fn</code> is explicitly specified. Default value: <code>1</code>.</p>
</td></tr>
<tr><td><code id="vi_fit_surrogate_posterior_+3A_trainable_variables">trainable_variables</code></td>
<td>
<p>Optional list of <code>tf$Variable</code> instances to
optimize with respect to. If <code>NULL</code>, defaults to the set of all
variables accessed during the computation of the variational bound,
i.e., those defining <code>surrogate_posterior</code> and the model
<code>target_log_prob_fn</code>. Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="vi_fit_surrogate_posterior_+3A_jit_compile">jit_compile</code></td>
<td>
<p>If <code>TRUE</code>, compiles the loss function and gradient
update using XLA. XLA performs compiler optimizations, such as fusion,
and attempts to emit more efficient code. This may drastically improve
the performance. See the docs for <code>tf.function</code>. Default value: <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="vi_fit_surrogate_posterior_+3A_seed">seed</code></td>
<td>
<p>integer to seed the random number generator.</p>
</td></tr>
<tr><td><code id="vi_fit_surrogate_posterior_+3A_name">name</code></td>
<td>
<p>name prefixed to ops created by this function. Default value:
'fit_surrogate_posterior'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This corresponds to minimizing the 'reverse' Kullback-Liebler divergence
(<code>KL[q||p]</code>) between the variational distribution and the unnormalized
<code>target_log_prob_fn</code>, and  defines a lower bound on the marginal log
likelihood, <code style="white-space: pre;">&#8288;log p(x) &gt;= -elbo_loss&#8288;</code>.
</p>
<p>More generally, this function supports fitting variational distributions
that minimize any <a href="https://en.wikipedia.org/wiki/F-divergence">Csiszar f-divergence</a>.
</p>


<h3>Value</h3>

<p>results <code>Tensor</code> or nested structure of <code>Tensor</code>s, according to
the return type of <code>result_fn</code>. Each <code>Tensor</code> has an added leading
dimension of size <code>num_steps</code>, packing the trajectory of the result
over the course of the optimization.
</p>


<h3>See Also</h3>

<p>Other vi-functions: 
<code><a href="#topic+vi_amari_alpha">vi_amari_alpha</a>()</code>,
<code><a href="#topic+vi_arithmetic_geometric">vi_arithmetic_geometric</a>()</code>,
<code><a href="#topic+vi_chi_square">vi_chi_square</a>()</code>,
<code><a href="#topic+vi_csiszar_vimco">vi_csiszar_vimco</a>()</code>,
<code><a href="#topic+vi_dual_csiszar_function">vi_dual_csiszar_function</a>()</code>,
<code><a href="#topic+vi_jeffreys">vi_jeffreys</a>()</code>,
<code><a href="#topic+vi_jensen_shannon">vi_jensen_shannon</a>()</code>,
<code><a href="#topic+vi_kl_forward">vi_kl_forward</a>()</code>,
<code><a href="#topic+vi_kl_reverse">vi_kl_reverse</a>()</code>,
<code><a href="#topic+vi_log1p_abs">vi_log1p_abs</a>()</code>,
<code><a href="#topic+vi_modified_gan">vi_modified_gan</a>()</code>,
<code><a href="#topic+vi_monte_carlo_variational_loss">vi_monte_carlo_variational_loss</a>()</code>,
<code><a href="#topic+vi_pearson">vi_pearson</a>()</code>,
<code><a href="#topic+vi_squared_hellinger">vi_squared_hellinger</a>()</code>,
<code><a href="#topic+vi_symmetrized_csiszar_function">vi_symmetrized_csiszar_function</a>()</code>
</p>

<hr>
<h2 id='vi_jeffreys'>The Jeffreys Csiszar-function in log-space</h2><span id='topic+vi_jeffreys'></span>

<h3>Description</h3>

<p>A Csiszar-function is a member of <code style="white-space: pre;">&#8288;F = { f:R_+ to R : f convex }&#8288;</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi_jeffreys(logu, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vi_jeffreys_+3A_logu">logu</code></td>
<td>
<p><code>float</code>-like <code>Tensor</code> representing <code>log(u)</code> from above.</p>
</td></tr>
<tr><td><code id="vi_jeffreys_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Jeffreys Csiszar-function is:
</p>
<div class="sourceCode"><pre>f(u) = 0.5 ( u log(u) - log(u))
= 0.5 kl_forward + 0.5 kl_reverse
= symmetrized_csiszar_function(kl_reverse)
= symmetrized_csiszar_function(kl_forward)
</pre></div>
<p>This Csiszar-function induces a symmetric f-Divergence, i.e.,
<code>D_f[p, q] = D_f[q, p]</code>.
</p>
<p>Warning: this function makes non-log-space calculations and may
therefore be numerically unstable for <code style="white-space: pre;">&#8288;|logu| &gt;&gt; 0&#8288;</code>.
</p>


<h3>Value</h3>

<p>jeffreys_of_u: <code>float</code>-like <code>Tensor</code> of the Csiszar-function
evaluated at <code>u = exp(logu)</code>.
</p>


<h3>See Also</h3>

<p>Other vi-functions: 
<code><a href="#topic+vi_amari_alpha">vi_amari_alpha</a>()</code>,
<code><a href="#topic+vi_arithmetic_geometric">vi_arithmetic_geometric</a>()</code>,
<code><a href="#topic+vi_chi_square">vi_chi_square</a>()</code>,
<code><a href="#topic+vi_csiszar_vimco">vi_csiszar_vimco</a>()</code>,
<code><a href="#topic+vi_dual_csiszar_function">vi_dual_csiszar_function</a>()</code>,
<code><a href="#topic+vi_fit_surrogate_posterior">vi_fit_surrogate_posterior</a>()</code>,
<code><a href="#topic+vi_jensen_shannon">vi_jensen_shannon</a>()</code>,
<code><a href="#topic+vi_kl_forward">vi_kl_forward</a>()</code>,
<code><a href="#topic+vi_kl_reverse">vi_kl_reverse</a>()</code>,
<code><a href="#topic+vi_log1p_abs">vi_log1p_abs</a>()</code>,
<code><a href="#topic+vi_modified_gan">vi_modified_gan</a>()</code>,
<code><a href="#topic+vi_monte_carlo_variational_loss">vi_monte_carlo_variational_loss</a>()</code>,
<code><a href="#topic+vi_pearson">vi_pearson</a>()</code>,
<code><a href="#topic+vi_squared_hellinger">vi_squared_hellinger</a>()</code>,
<code><a href="#topic+vi_symmetrized_csiszar_function">vi_symmetrized_csiszar_function</a>()</code>
</p>

<hr>
<h2 id='vi_jensen_shannon'>The Jensen-Shannon Csiszar-function in log-space</h2><span id='topic+vi_jensen_shannon'></span>

<h3>Description</h3>

<p>A Csiszar-function is a member of <code style="white-space: pre;">&#8288;F = { f:R_+ to R : f convex }&#8288;</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi_jensen_shannon(logu, self_normalized = FALSE, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vi_jensen_shannon_+3A_logu">logu</code></td>
<td>
<p><code>float</code>-like <code>Tensor</code> representing <code>log(u)</code> from above.</p>
</td></tr>
<tr><td><code id="vi_jensen_shannon_+3A_self_normalized">self_normalized</code></td>
<td>
<p><code>logical</code> indicating whether <code style="white-space: pre;">&#8288;f'(u=1)=0&#8288;</code>. When
<code style="white-space: pre;">&#8288;f'(u=1)=0&#8288;</code> the implied Csiszar f-Divergence remains non-negative even
when <code style="white-space: pre;">&#8288;p, q&#8288;</code> are unnormalized measures.</p>
</td></tr>
<tr><td><code id="vi_jensen_shannon_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>self_normalized = True</code>, the Jensen-Shannon Csiszar-function is:
</p>
<div class="sourceCode"><pre>f(u) = u log(u) - (1 + u) log(1 + u) + (u + 1) log(2)
</pre></div>
<p>When <code>self_normalized = False</code> the <code style="white-space: pre;">&#8288;(u + 1) log(2)&#8288;</code> term is omitted.
</p>
<p>Observe that as an f-Divergence, this Csiszar-function implies:
</p>
<div class="sourceCode"><pre>D_f[p, q] = KL[p, m] + KL[q, m]
m(x) = 0.5 p(x) + 0.5 q(x)
</pre></div>
<p>In a sense, this divergence is the &quot;reverse&quot; of the Arithmetic-Geometric
f-Divergence.
</p>
<p>This Csiszar-function induces a symmetric f-Divergence, i.e.,
<code>D_f[p, q] = D_f[q, p]</code>.
</p>
<p>Warning: this function makes non-log-space calculations and may therefore be
numerically unstable for <code style="white-space: pre;">&#8288;|logu| &gt;&gt; 0&#8288;</code>.
</p>


<h3>Value</h3>

<p>jensen_shannon_of_u, <code>float</code>-like <code>Tensor</code> of the Csiszar-function
evaluated at <code>u = exp(logu)</code>.
</p>


<h3>References</h3>


<ul>
<li><p> Lin, J. &quot;Divergence measures based on the Shannon entropy.&quot; IEEE Trans.
Inf. Th., 37, 145-151, 1991.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other vi-functions: 
<code><a href="#topic+vi_amari_alpha">vi_amari_alpha</a>()</code>,
<code><a href="#topic+vi_arithmetic_geometric">vi_arithmetic_geometric</a>()</code>,
<code><a href="#topic+vi_chi_square">vi_chi_square</a>()</code>,
<code><a href="#topic+vi_csiszar_vimco">vi_csiszar_vimco</a>()</code>,
<code><a href="#topic+vi_dual_csiszar_function">vi_dual_csiszar_function</a>()</code>,
<code><a href="#topic+vi_fit_surrogate_posterior">vi_fit_surrogate_posterior</a>()</code>,
<code><a href="#topic+vi_jeffreys">vi_jeffreys</a>()</code>,
<code><a href="#topic+vi_kl_forward">vi_kl_forward</a>()</code>,
<code><a href="#topic+vi_kl_reverse">vi_kl_reverse</a>()</code>,
<code><a href="#topic+vi_log1p_abs">vi_log1p_abs</a>()</code>,
<code><a href="#topic+vi_modified_gan">vi_modified_gan</a>()</code>,
<code><a href="#topic+vi_monte_carlo_variational_loss">vi_monte_carlo_variational_loss</a>()</code>,
<code><a href="#topic+vi_pearson">vi_pearson</a>()</code>,
<code><a href="#topic+vi_squared_hellinger">vi_squared_hellinger</a>()</code>,
<code><a href="#topic+vi_symmetrized_csiszar_function">vi_symmetrized_csiszar_function</a>()</code>
</p>

<hr>
<h2 id='vi_kl_forward'>The forward Kullback-Leibler Csiszar-function in log-space</h2><span id='topic+vi_kl_forward'></span>

<h3>Description</h3>

<p>A Csiszar-function is a member of <code style="white-space: pre;">&#8288;F = { f:R_+ to R : f convex }&#8288;</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi_kl_forward(logu, self_normalized = FALSE, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vi_kl_forward_+3A_logu">logu</code></td>
<td>
<p><code>float</code>-like <code>Tensor</code> representing <code>log(u)</code> from above.</p>
</td></tr>
<tr><td><code id="vi_kl_forward_+3A_self_normalized">self_normalized</code></td>
<td>
<p><code>logical</code> indicating whether <code style="white-space: pre;">&#8288;f'(u=1)=0&#8288;</code>. When
<code style="white-space: pre;">&#8288;f'(u=1)=0&#8288;</code> the implied Csiszar f-Divergence remains non-negative even
when <code style="white-space: pre;">&#8288;p, q&#8288;</code> are unnormalized measures.</p>
</td></tr>
<tr><td><code id="vi_kl_forward_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>self_normalized = TRUE</code>, the KL-reverse Csiszar-function is <code style="white-space: pre;">&#8288;f(u) = u log(u) - (u - 1)&#8288;</code>.
When <code>self_normalized = FALSE</code> the <code>(u - 1)</code> term is omitted.
Observe that as an f-Divergence, this Csiszar-function implies: <code>D_f[p, q] = KL[q, p]</code>
</p>
<p>The KL is &quot;forward&quot; because in maximum likelihood we think of minimizing <code>q</code> as in <code>KL[p, q]</code>.
</p>
<p>Warning: when self_normalized = True<code style="white-space: pre;">&#8288;this function makes non-log-space calculations and may therefore be numerically unstable for&#8288;</code>|logu| &gt;&gt; 0'.
</p>


<h3>Value</h3>

<p>kl_forward_of_u: <code>float</code>-like <code>Tensor</code> of the Csiszar-function evaluated at
<code>u = exp(logu)</code>.
</p>


<h3>See Also</h3>

<p>Other vi-functions: 
<code><a href="#topic+vi_amari_alpha">vi_amari_alpha</a>()</code>,
<code><a href="#topic+vi_arithmetic_geometric">vi_arithmetic_geometric</a>()</code>,
<code><a href="#topic+vi_chi_square">vi_chi_square</a>()</code>,
<code><a href="#topic+vi_csiszar_vimco">vi_csiszar_vimco</a>()</code>,
<code><a href="#topic+vi_dual_csiszar_function">vi_dual_csiszar_function</a>()</code>,
<code><a href="#topic+vi_fit_surrogate_posterior">vi_fit_surrogate_posterior</a>()</code>,
<code><a href="#topic+vi_jeffreys">vi_jeffreys</a>()</code>,
<code><a href="#topic+vi_jensen_shannon">vi_jensen_shannon</a>()</code>,
<code><a href="#topic+vi_kl_reverse">vi_kl_reverse</a>()</code>,
<code><a href="#topic+vi_log1p_abs">vi_log1p_abs</a>()</code>,
<code><a href="#topic+vi_modified_gan">vi_modified_gan</a>()</code>,
<code><a href="#topic+vi_monte_carlo_variational_loss">vi_monte_carlo_variational_loss</a>()</code>,
<code><a href="#topic+vi_pearson">vi_pearson</a>()</code>,
<code><a href="#topic+vi_squared_hellinger">vi_squared_hellinger</a>()</code>,
<code><a href="#topic+vi_symmetrized_csiszar_function">vi_symmetrized_csiszar_function</a>()</code>
</p>

<hr>
<h2 id='vi_kl_reverse'>The reverse Kullback-Leibler Csiszar-function in log-space</h2><span id='topic+vi_kl_reverse'></span>

<h3>Description</h3>

<p>A Csiszar-function is a member of <code style="white-space: pre;">&#8288;F = { f:R_+ to R : f convex }&#8288;</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi_kl_reverse(logu, self_normalized = FALSE, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vi_kl_reverse_+3A_logu">logu</code></td>
<td>
<p><code>float</code>-like <code>Tensor</code> representing <code>log(u)</code> from above.</p>
</td></tr>
<tr><td><code id="vi_kl_reverse_+3A_self_normalized">self_normalized</code></td>
<td>
<p><code>logical</code> indicating whether <code style="white-space: pre;">&#8288;f'(u=1)=0&#8288;</code>. When
<code style="white-space: pre;">&#8288;f'(u=1)=0&#8288;</code> the implied Csiszar f-Divergence remains non-negative even
when <code style="white-space: pre;">&#8288;p, q&#8288;</code> are unnormalized measures.</p>
</td></tr>
<tr><td><code id="vi_kl_reverse_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>self_normalized = TRUE</code>, the KL-reverse Csiszar-function is <code>f(u) = -log(u) + (u - 1)</code>.
When <code>self_normalized = FALSE</code> the <code>(u - 1)</code> term is omitted.
Observe that as an f-Divergence, this Csiszar-function implies: <code>D_f[p, q] = KL[q, p]</code>
</p>
<p>The KL is &quot;reverse&quot; because in maximum likelihood we think of minimizing <code>q</code> as in <code>KL[p, q]</code>.
</p>
<p>Warning: when self_normalized = True<code style="white-space: pre;">&#8288;this function makes non-log-space calculations and may therefore be numerically unstable for&#8288;</code>|logu| &gt;&gt; 0'.
</p>


<h3>Value</h3>

<p>kl_reverse_of_u <code>float</code>-like <code>Tensor</code> of the Csiszar-function evaluated at
<code>u = exp(logu)</code>.
</p>


<h3>See Also</h3>

<p>Other vi-functions: 
<code><a href="#topic+vi_amari_alpha">vi_amari_alpha</a>()</code>,
<code><a href="#topic+vi_arithmetic_geometric">vi_arithmetic_geometric</a>()</code>,
<code><a href="#topic+vi_chi_square">vi_chi_square</a>()</code>,
<code><a href="#topic+vi_csiszar_vimco">vi_csiszar_vimco</a>()</code>,
<code><a href="#topic+vi_dual_csiszar_function">vi_dual_csiszar_function</a>()</code>,
<code><a href="#topic+vi_fit_surrogate_posterior">vi_fit_surrogate_posterior</a>()</code>,
<code><a href="#topic+vi_jeffreys">vi_jeffreys</a>()</code>,
<code><a href="#topic+vi_jensen_shannon">vi_jensen_shannon</a>()</code>,
<code><a href="#topic+vi_kl_forward">vi_kl_forward</a>()</code>,
<code><a href="#topic+vi_log1p_abs">vi_log1p_abs</a>()</code>,
<code><a href="#topic+vi_modified_gan">vi_modified_gan</a>()</code>,
<code><a href="#topic+vi_monte_carlo_variational_loss">vi_monte_carlo_variational_loss</a>()</code>,
<code><a href="#topic+vi_pearson">vi_pearson</a>()</code>,
<code><a href="#topic+vi_squared_hellinger">vi_squared_hellinger</a>()</code>,
<code><a href="#topic+vi_symmetrized_csiszar_function">vi_symmetrized_csiszar_function</a>()</code>
</p>

<hr>
<h2 id='vi_log1p_abs'>The log1p-abs Csiszar-function in log-space</h2><span id='topic+vi_log1p_abs'></span>

<h3>Description</h3>

<p>A Csiszar-function is a member of <code style="white-space: pre;">&#8288;F = { f:R_+ to R : f convex }&#8288;</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi_log1p_abs(logu, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vi_log1p_abs_+3A_logu">logu</code></td>
<td>
<p><code>float</code>-like <code>Tensor</code> representing <code>log(u)</code> from above.</p>
</td></tr>
<tr><td><code id="vi_log1p_abs_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Log1p-Abs Csiszar-function is:
</p>
<div class="sourceCode"><pre>f(u) = u**(sign(u-1)) - 1
</pre></div>
<p>This function is so-named because it was invented from the following recipe.
Choose a convex function g such that g(0)=0 and solve for f:
</p>
<div class="sourceCode"><pre>log(1 + f(u)) = g(log(u)).
&lt;=&gt;
f(u) = exp(g(log(u))) - 1
</pre></div>
<p>That is, the graph is identically <code>g</code> when y-axis is <code>log1p</code>-domain and x-axis
is <code>log</code>-domain.
</p>
<p>Warning: this function makes non-log-space calculations and may
therefore be numerically unstable for <code style="white-space: pre;">&#8288;|logu| &gt;&gt; 0&#8288;</code>.
</p>


<h3>Value</h3>

<p>log1p_abs_of_u: <code>float</code>-like <code>Tensor</code> of the Csiszar-function
evaluated at <code>u = exp(logu)</code>.
</p>


<h3>See Also</h3>

<p>Other vi-functions: 
<code><a href="#topic+vi_amari_alpha">vi_amari_alpha</a>()</code>,
<code><a href="#topic+vi_arithmetic_geometric">vi_arithmetic_geometric</a>()</code>,
<code><a href="#topic+vi_chi_square">vi_chi_square</a>()</code>,
<code><a href="#topic+vi_csiszar_vimco">vi_csiszar_vimco</a>()</code>,
<code><a href="#topic+vi_dual_csiszar_function">vi_dual_csiszar_function</a>()</code>,
<code><a href="#topic+vi_fit_surrogate_posterior">vi_fit_surrogate_posterior</a>()</code>,
<code><a href="#topic+vi_jeffreys">vi_jeffreys</a>()</code>,
<code><a href="#topic+vi_jensen_shannon">vi_jensen_shannon</a>()</code>,
<code><a href="#topic+vi_kl_forward">vi_kl_forward</a>()</code>,
<code><a href="#topic+vi_kl_reverse">vi_kl_reverse</a>()</code>,
<code><a href="#topic+vi_modified_gan">vi_modified_gan</a>()</code>,
<code><a href="#topic+vi_monte_carlo_variational_loss">vi_monte_carlo_variational_loss</a>()</code>,
<code><a href="#topic+vi_pearson">vi_pearson</a>()</code>,
<code><a href="#topic+vi_squared_hellinger">vi_squared_hellinger</a>()</code>,
<code><a href="#topic+vi_symmetrized_csiszar_function">vi_symmetrized_csiszar_function</a>()</code>
</p>

<hr>
<h2 id='vi_modified_gan'>The Modified-GAN Csiszar-function in log-space</h2><span id='topic+vi_modified_gan'></span>

<h3>Description</h3>

<p>A Csiszar-function is a member of <code style="white-space: pre;">&#8288;F = { f:R_+ to R : f convex }&#8288;</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi_modified_gan(logu, self_normalized = FALSE, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vi_modified_gan_+3A_logu">logu</code></td>
<td>
<p><code>float</code>-like <code>Tensor</code> representing <code>log(u)</code> from above.</p>
</td></tr>
<tr><td><code id="vi_modified_gan_+3A_self_normalized">self_normalized</code></td>
<td>
<p><code>logical</code> indicating whether <code style="white-space: pre;">&#8288;f'(u=1)=0&#8288;</code>. When
<code style="white-space: pre;">&#8288;f'(u=1)=0&#8288;</code> the implied Csiszar f-Divergence remains non-negative even
when <code style="white-space: pre;">&#8288;p, q&#8288;</code> are unnormalized measures.</p>
</td></tr>
<tr><td><code id="vi_modified_gan_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>self_normalized = True</code> the modified-GAN (Generative/Adversarial
Network) Csiszar-function is:
</p>
<div class="sourceCode"><pre>f(u) = log(1 + u) - log(u) + 0.5 (u - 1)
</pre></div>
<p>When <code>self_normalized = False</code> the <code>0.5 (u - 1)</code> is omitted.
</p>
<p>The unmodified GAN Csiszar-function is identical to Jensen-Shannon (with
<code>self_normalized = False</code>).
</p>
<p>Warning: this function makes non-log-space calculations and may therefore be
numerically unstable for <code style="white-space: pre;">&#8288;|logu| &gt;&gt; 0&#8288;</code>.
</p>


<h3>Value</h3>

<p>jensen_shannon_of_u, <code>float</code>-like <code>Tensor</code> of the Csiszar-function
evaluated at <code>u = exp(logu)</code>.
</p>


<h3>See Also</h3>

<p>Other vi-functions: 
<code><a href="#topic+vi_amari_alpha">vi_amari_alpha</a>()</code>,
<code><a href="#topic+vi_arithmetic_geometric">vi_arithmetic_geometric</a>()</code>,
<code><a href="#topic+vi_chi_square">vi_chi_square</a>()</code>,
<code><a href="#topic+vi_csiszar_vimco">vi_csiszar_vimco</a>()</code>,
<code><a href="#topic+vi_dual_csiszar_function">vi_dual_csiszar_function</a>()</code>,
<code><a href="#topic+vi_fit_surrogate_posterior">vi_fit_surrogate_posterior</a>()</code>,
<code><a href="#topic+vi_jeffreys">vi_jeffreys</a>()</code>,
<code><a href="#topic+vi_jensen_shannon">vi_jensen_shannon</a>()</code>,
<code><a href="#topic+vi_kl_forward">vi_kl_forward</a>()</code>,
<code><a href="#topic+vi_kl_reverse">vi_kl_reverse</a>()</code>,
<code><a href="#topic+vi_log1p_abs">vi_log1p_abs</a>()</code>,
<code><a href="#topic+vi_monte_carlo_variational_loss">vi_monte_carlo_variational_loss</a>()</code>,
<code><a href="#topic+vi_pearson">vi_pearson</a>()</code>,
<code><a href="#topic+vi_squared_hellinger">vi_squared_hellinger</a>()</code>,
<code><a href="#topic+vi_symmetrized_csiszar_function">vi_symmetrized_csiszar_function</a>()</code>
</p>

<hr>
<h2 id='vi_monte_carlo_variational_loss'>Monte-Carlo approximation of an f-Divergence variational loss</h2><span id='topic+vi_monte_carlo_variational_loss'></span>

<h3>Description</h3>

<p>Variational losses measure the divergence between an unnormalized target
distribution <code>p</code> (provided via <code>target_log_prob_fn</code>) and a surrogate
distribution <code>q</code> (provided as <code>surrogate_posterior</code>). When the
target distribution is an unnormalized posterior from conditioning a model on
data, minimizing the loss with respect to the parameters of
<code>surrogate_posterior</code> performs approximate posterior inference.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi_monte_carlo_variational_loss(
  target_log_prob_fn,
  surrogate_posterior,
  sample_size = 1L,
  importance_sample_size = 1L,
  discrepancy_fn = vi_kl_reverse,
  use_reparametrization = NULL,
  seed = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vi_monte_carlo_variational_loss_+3A_target_log_prob_fn">target_log_prob_fn</code></td>
<td>
<p>function that takes a set of <code>Tensor</code> arguments
and returns a <code>Tensor</code> log-density. Given
<code>q_sample &lt;- surrogate_posterior$sample(sample_size)</code>, this
will be (in Python) called as <code>target_log_prob_fn(q_sample)</code> if <code>q_sample</code> is a list
or a tuple, <code style="white-space: pre;">&#8288;target_log_prob_fn(**q_sample)&#8288;</code> if <code>q_sample</code> is a
dictionary, or <code>target_log_prob_fn(q_sample)</code> if <code>q_sample</code> is a <code>Tensor</code>.
It should support batched evaluation, i.e., should return a result of
shape <code style="white-space: pre;">&#8288;[sample_size]&#8288;</code>.</p>
</td></tr>
<tr><td><code id="vi_monte_carlo_variational_loss_+3A_surrogate_posterior">surrogate_posterior</code></td>
<td>
<p>A <code>tfp$distributions$Distribution</code>
instance defining a variational posterior (could be a
<code>tfp$distributions$JointDistribution</code>). Crucially, the distribution's <code>log_prob</code> and
(if reparameterized) <code>sample</code> methods must directly invoke all ops
that generate gradients to the underlying variables. One way to ensure
this is to use <code>tfp$util$DeferredTensor</code> to represent any parameters
defined as transformations of unconstrained variables, so that the
transformations execute at runtime instead of at distribution creation.</p>
</td></tr>
<tr><td><code id="vi_monte_carlo_variational_loss_+3A_sample_size">sample_size</code></td>
<td>
<p><code>integer</code> number of Monte Carlo samples to use
in estimating the variational divergence. Larger values may stabilize
the optimization, but at higher cost per step in time and memory.
Default value: <code>1</code>.</p>
</td></tr>
<tr><td><code id="vi_monte_carlo_variational_loss_+3A_importance_sample_size">importance_sample_size</code></td>
<td>
<p>integer number of terms used to define an
importance-weighted divergence. If importance_sample_size &gt; 1, then the
surrogate_posterior is optimized to function as an importance-sampling
proposal distribution. In this case it often makes sense to use importance
sampling to approximate posterior expectations (see
tfp.vi.fit_surrogate_posterior for an example). Default value: 1.</p>
</td></tr>
<tr><td><code id="vi_monte_carlo_variational_loss_+3A_discrepancy_fn">discrepancy_fn</code></td>
<td>
<p>function representing a Csiszar <code>f</code> function in
in log-space. That is, <code>discrepancy_fn(log(u)) = f(u)</code>, where <code>f</code> is
convex in <code>u</code>.  Default value: <code>vi_kl_reverse</code>.</p>
</td></tr>
<tr><td><code id="vi_monte_carlo_variational_loss_+3A_use_reparametrization">use_reparametrization</code></td>
<td>
<p><code>logical</code>. When <code>NULL</code> (the default),
automatically set to: <code>surrogate_posterior.reparameterization_type == tfp$distributions$FULLY_REPARAMETERIZED</code>.
When <code>TRUE</code> uses the standard Monte-Carlo average. When <code>FALSE</code> uses the score-gradient trick. (See above for
details.)  When <code>FALSE</code>, consider using <code>csiszar_vimco</code>.</p>
</td></tr>
<tr><td><code id="vi_monte_carlo_variational_loss_+3A_seed">seed</code></td>
<td>
<p><code>integer</code> seed for <code>surrogate_posterior$sample</code>.</p>
</td></tr>
<tr><td><code id="vi_monte_carlo_variational_loss_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function defines divergences of the form
<code style="white-space: pre;">&#8288;E_q[discrepancy_fn(log p(z) - log q(z))]&#8288;</code>, sometimes known as
<a href="https://en.wikipedia.org/wiki/F-divergence">f-divergences</a>.
</p>
<p>In the special case <code>discrepancy_fn(logu) == -logu</code> (the default
<code>vi_kl_reverse</code>), this is the reverse Kullback-Liebler divergence
<code>KL[q||p]</code>, whose negation applied to an unnormalized <code>p</code> is the widely-used
evidence lower bound (ELBO). Other cases of interest available under
<code>tfp$vi</code> include the forward <code>KL[p||q]</code> (given by <code>vi_kl_forward(logu) == exp(logu) * logu</code>),
total variation distance, Amari alpha-divergences, and more.
</p>
<p>Csiszar f-divergences
</p>
<p>A Csiszar function <code>f</code> is a convex function from <code style="white-space: pre;">&#8288;R^+&#8288;</code> (the positive reals)
to <code>R</code>. The Csiszar f-Divergence is given by:
</p>
<div class="sourceCode"><pre>D_f[p(X), q(X)] := E_{q(X)}[ f( p(X) / q(X) ) ]
~= m**-1 sum_j^m f( p(x_j) / q(x_j) ),
where x_j ~iid q(X)
</pre></div>
<p>For example, <code style="white-space: pre;">&#8288;f = lambda u: -log(u)&#8288;</code> recovers <code>KL[q||p]</code>, while <code style="white-space: pre;">&#8288;f = lambda u: u * log(u)&#8288;</code>
recovers the forward <code>KL[p||q]</code>. These and other functions are available in <code>tfp$vi</code>.
</p>
<p>Tricks: Reparameterization and Score-Gradient
</p>
<p>When q is &quot;reparameterized&quot;, i.e., a diffeomorphic transformation of a
parameterless distribution (e.g., <code style="white-space: pre;">&#8288;Normal(Y; m, s) &lt;=&gt; Y = sX + m, X ~ Normal(0,1)&#8288;</code>),
we can swap gradient and expectation, i.e.,
<code style="white-space: pre;">&#8288;grad[Avg{ s_i : i=1...n }] = Avg{ grad[s_i] : i=1...n }&#8288;</code> where <code style="white-space: pre;">&#8288;S_n=Avg{s_i}&#8288;</code>
and <code style="white-space: pre;">&#8288;s_i = f(x_i), x_i ~iid q(X)&#8288;</code>.
</p>
<p>However, if q is not reparameterized, TensorFlow's gradient will be incorrect
since the chain-rule stops at samples of unreparameterized distributions. In
this circumstance using the Score-Gradient trick results in an unbiased
gradient, i.e.,
</p>
<div class="sourceCode"><pre>grad[ E_q[f(X)] ]
  = grad[ int dx q(x) f(x) ]
  = int dx grad[ q(x) f(x) ]
  = int dx [ q'(x) f(x) + q(x) f'(x) ]
  = int dx q(x) [q'(x) / q(x) f(x) + f'(x) ]
  = int dx q(x) grad[ f(x) q(x) / stop_grad[q(x)] ]
  = E_q[ grad[ f(x) q(x) / stop_grad[q(x)] ] ]
</pre></div>
<p>Unless <code>q.reparameterization_type != tfd.FULLY_REPARAMETERIZED</code> it is
usually preferable to set <code>use_reparametrization = True</code>.
</p>
<p>Example Application:
The Csiszar f-Divergence is a useful framework for variational inference.
I.e., observe that,
</p>
<div class="sourceCode"><pre>f(p(x)) =  f( E_{q(Z | x)}[ p(x, Z) / q(Z | x) ] )
        &lt;= E_{q(Z | x)}[ f( p(x, Z) / q(Z | x) ) ]
        := D_f[p(x, Z), q(Z | x)]
</pre></div>
<p>The inequality follows from the fact that the &quot;perspective&quot; of <code>f</code>, i.e.,
<code style="white-space: pre;">&#8288;(s, t) |-&gt; t f(s / t))&#8288;</code>, is convex in <code style="white-space: pre;">&#8288;(s, t)&#8288;</code> when <code style="white-space: pre;">&#8288;s/t in domain(f)&#8288;</code> and
<code>t</code> is a real. Since the above framework includes the popular Evidence Lower
BOund (ELBO) as a special case, i.e., <code>f(u) = -log(u)</code>, we call this framework
&quot;Evidence Divergence Bound Optimization&quot; (EDBO).
</p>


<h3>Value</h3>

<p>monte_carlo_variational_loss <code>float</code>-like <code>Tensor</code> Monte Carlo
approximation of the Csiszar f-Divergence.
</p>


<h3>References</h3>


<ul>
<li><p> Ali, Syed Mumtaz, and Samuel D. Silvey. &quot;A general class of coefficients of divergence of one distribution from another.&quot;
Journal of the Royal Statistical Society: Series B (Methodological) 28.1 (1966): 131-142.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other vi-functions: 
<code><a href="#topic+vi_amari_alpha">vi_amari_alpha</a>()</code>,
<code><a href="#topic+vi_arithmetic_geometric">vi_arithmetic_geometric</a>()</code>,
<code><a href="#topic+vi_chi_square">vi_chi_square</a>()</code>,
<code><a href="#topic+vi_csiszar_vimco">vi_csiszar_vimco</a>()</code>,
<code><a href="#topic+vi_dual_csiszar_function">vi_dual_csiszar_function</a>()</code>,
<code><a href="#topic+vi_fit_surrogate_posterior">vi_fit_surrogate_posterior</a>()</code>,
<code><a href="#topic+vi_jeffreys">vi_jeffreys</a>()</code>,
<code><a href="#topic+vi_jensen_shannon">vi_jensen_shannon</a>()</code>,
<code><a href="#topic+vi_kl_forward">vi_kl_forward</a>()</code>,
<code><a href="#topic+vi_kl_reverse">vi_kl_reverse</a>()</code>,
<code><a href="#topic+vi_log1p_abs">vi_log1p_abs</a>()</code>,
<code><a href="#topic+vi_modified_gan">vi_modified_gan</a>()</code>,
<code><a href="#topic+vi_pearson">vi_pearson</a>()</code>,
<code><a href="#topic+vi_squared_hellinger">vi_squared_hellinger</a>()</code>,
<code><a href="#topic+vi_symmetrized_csiszar_function">vi_symmetrized_csiszar_function</a>()</code>
</p>

<hr>
<h2 id='vi_pearson'>The Pearson Csiszar-function in log-space</h2><span id='topic+vi_pearson'></span>

<h3>Description</h3>

<p>A Csiszar-function is a member of <code style="white-space: pre;">&#8288;F = { f:R_+ to R : f convex }&#8288;</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi_pearson(logu, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vi_pearson_+3A_logu">logu</code></td>
<td>
<p><code>float</code>-like <code>Tensor</code> representing <code>log(u)</code> from above.</p>
</td></tr>
<tr><td><code id="vi_pearson_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Pearson Csiszar-function is:
</p>
<div class="sourceCode"><pre>f(u) = (u - 1)**2
</pre></div>
<p>Warning: this function makes non-log-space calculations and may therefore be
numerically unstable for <code style="white-space: pre;">&#8288;|logu| &gt;&gt; 0&#8288;</code>.
</p>


<h3>Value</h3>

<p>pearson_of_u: <code>float</code>-like <code>Tensor</code> of the Csiszar-function
evaluated at <code>u = exp(logu)</code>.
</p>


<h3>See Also</h3>

<p>Other vi-functions: 
<code><a href="#topic+vi_amari_alpha">vi_amari_alpha</a>()</code>,
<code><a href="#topic+vi_arithmetic_geometric">vi_arithmetic_geometric</a>()</code>,
<code><a href="#topic+vi_chi_square">vi_chi_square</a>()</code>,
<code><a href="#topic+vi_csiszar_vimco">vi_csiszar_vimco</a>()</code>,
<code><a href="#topic+vi_dual_csiszar_function">vi_dual_csiszar_function</a>()</code>,
<code><a href="#topic+vi_fit_surrogate_posterior">vi_fit_surrogate_posterior</a>()</code>,
<code><a href="#topic+vi_jeffreys">vi_jeffreys</a>()</code>,
<code><a href="#topic+vi_jensen_shannon">vi_jensen_shannon</a>()</code>,
<code><a href="#topic+vi_kl_forward">vi_kl_forward</a>()</code>,
<code><a href="#topic+vi_kl_reverse">vi_kl_reverse</a>()</code>,
<code><a href="#topic+vi_log1p_abs">vi_log1p_abs</a>()</code>,
<code><a href="#topic+vi_modified_gan">vi_modified_gan</a>()</code>,
<code><a href="#topic+vi_monte_carlo_variational_loss">vi_monte_carlo_variational_loss</a>()</code>,
<code><a href="#topic+vi_squared_hellinger">vi_squared_hellinger</a>()</code>,
<code><a href="#topic+vi_symmetrized_csiszar_function">vi_symmetrized_csiszar_function</a>()</code>
</p>

<hr>
<h2 id='vi_squared_hellinger'>The Squared-Hellinger Csiszar-function in log-space</h2><span id='topic+vi_squared_hellinger'></span>

<h3>Description</h3>

<p>A Csiszar-function is a member of <code style="white-space: pre;">&#8288;F = { f:R_+ to R : f convex }&#8288;</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi_squared_hellinger(logu, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vi_squared_hellinger_+3A_logu">logu</code></td>
<td>
<p><code>float</code>-like <code>Tensor</code> representing <code>log(u)</code> from above.</p>
</td></tr>
<tr><td><code id="vi_squared_hellinger_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Squared-Hellinger Csiszar-function is:
</p>
<div class="sourceCode"><pre>f(u) = (sqrt(u) - 1)**2
</pre></div>
<p>This Csiszar-function induces a symmetric f-Divergence, i.e.,
<code>D_f[p, q] = D_f[q, p]</code>.
</p>
<p>Warning: this function makes non-log-space calculations and may
therefore be numerically unstable for <code style="white-space: pre;">&#8288;|logu| &gt;&gt; 0&#8288;</code>.
</p>


<h3>Value</h3>

<p>Squared-Hellinger_of_u: <code>float</code>-like <code>Tensor</code> of the Csiszar-function
evaluated at <code>u = exp(logu)</code>.
</p>


<h3>See Also</h3>

<p>Other vi-functions: 
<code><a href="#topic+vi_amari_alpha">vi_amari_alpha</a>()</code>,
<code><a href="#topic+vi_arithmetic_geometric">vi_arithmetic_geometric</a>()</code>,
<code><a href="#topic+vi_chi_square">vi_chi_square</a>()</code>,
<code><a href="#topic+vi_csiszar_vimco">vi_csiszar_vimco</a>()</code>,
<code><a href="#topic+vi_dual_csiszar_function">vi_dual_csiszar_function</a>()</code>,
<code><a href="#topic+vi_fit_surrogate_posterior">vi_fit_surrogate_posterior</a>()</code>,
<code><a href="#topic+vi_jeffreys">vi_jeffreys</a>()</code>,
<code><a href="#topic+vi_jensen_shannon">vi_jensen_shannon</a>()</code>,
<code><a href="#topic+vi_kl_forward">vi_kl_forward</a>()</code>,
<code><a href="#topic+vi_kl_reverse">vi_kl_reverse</a>()</code>,
<code><a href="#topic+vi_log1p_abs">vi_log1p_abs</a>()</code>,
<code><a href="#topic+vi_modified_gan">vi_modified_gan</a>()</code>,
<code><a href="#topic+vi_monte_carlo_variational_loss">vi_monte_carlo_variational_loss</a>()</code>,
<code><a href="#topic+vi_pearson">vi_pearson</a>()</code>,
<code><a href="#topic+vi_symmetrized_csiszar_function">vi_symmetrized_csiszar_function</a>()</code>
</p>

<hr>
<h2 id='vi_symmetrized_csiszar_function'>Symmetrizes a Csiszar-function in log-space</h2><span id='topic+vi_symmetrized_csiszar_function'></span>

<h3>Description</h3>

<p>A Csiszar-function is a member of <code style="white-space: pre;">&#8288;F = { f:R_+ to R : f convex }&#8288;</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi_symmetrized_csiszar_function(logu, csiszar_function, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vi_symmetrized_csiszar_function_+3A_logu">logu</code></td>
<td>
<p><code>float</code>-like <code>Tensor</code> representing <code>log(u)</code> from above.</p>
</td></tr>
<tr><td><code id="vi_symmetrized_csiszar_function_+3A_csiszar_function">csiszar_function</code></td>
<td>
<p>function representing a Csiszar-function over log-domain.</p>
</td></tr>
<tr><td><code id="vi_symmetrized_csiszar_function_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The symmetrized Csiszar-function is defined as:
</p>
<div class="sourceCode"><pre>f_g(u) = 0.5 g(u) + 0.5 u g (1 / u)
</pre></div>
<p>where <code>g</code> is some other Csiszar-function.
We say the function is &quot;symmetrized&quot; because:
</p>
<div class="sourceCode"><pre>D_{f_g}[p, q] = D_{f_g}[q, p]
</pre></div>
<p>for all <code style="white-space: pre;">&#8288;p &lt;&lt; &gt;&gt; q&#8288;</code> (i.e., <code>support(p) = support(q)</code>).
</p>
<p>There exists alternatives for symmetrizing a Csiszar-function. For example,
</p>
<div class="sourceCode"><pre>f_g(u) = max(f(u), f^*(u)),
</pre></div>
<p>where <code style="white-space: pre;">&#8288;f^*&#8288;</code> is the dual Csiszar-function, also implies a symmetric
f-Divergence.
</p>
<p>Example:
When either of the following functions are symmetrized, we obtain the
Jensen-Shannon Csiszar-function, i.e.,
</p>
<div class="sourceCode"><pre>g(u) = -log(u) - (1 + u) log((1 + u) / 2) + u - 1
h(u) = log(4) + 2 u log(u / (1 + u))
</pre></div>
<p>implies,
</p>
<div class="sourceCode"><pre>f_g(u) = f_h(u) = u log(u) - (1 + u) log((1 + u) / 2)
= jensen_shannon(log(u)).
</pre></div>
<p>Warning: this function makes non-log-space calculations and may therefore be
numerically unstable for <code style="white-space: pre;">&#8288;|logu| &gt;&gt; 0&#8288;</code>.
</p>


<h3>Value</h3>

<p>symmetrized_g_of_u: <code>float</code>-like <code>Tensor</code> of the result of applying the
symmetrization of <code>g</code> evaluated at <code>u = exp(logu)</code>.
</p>


<h3>See Also</h3>

<p>Other vi-functions: 
<code><a href="#topic+vi_amari_alpha">vi_amari_alpha</a>()</code>,
<code><a href="#topic+vi_arithmetic_geometric">vi_arithmetic_geometric</a>()</code>,
<code><a href="#topic+vi_chi_square">vi_chi_square</a>()</code>,
<code><a href="#topic+vi_csiszar_vimco">vi_csiszar_vimco</a>()</code>,
<code><a href="#topic+vi_dual_csiszar_function">vi_dual_csiszar_function</a>()</code>,
<code><a href="#topic+vi_fit_surrogate_posterior">vi_fit_surrogate_posterior</a>()</code>,
<code><a href="#topic+vi_jeffreys">vi_jeffreys</a>()</code>,
<code><a href="#topic+vi_jensen_shannon">vi_jensen_shannon</a>()</code>,
<code><a href="#topic+vi_kl_forward">vi_kl_forward</a>()</code>,
<code><a href="#topic+vi_kl_reverse">vi_kl_reverse</a>()</code>,
<code><a href="#topic+vi_log1p_abs">vi_log1p_abs</a>()</code>,
<code><a href="#topic+vi_modified_gan">vi_modified_gan</a>()</code>,
<code><a href="#topic+vi_monte_carlo_variational_loss">vi_monte_carlo_variational_loss</a>()</code>,
<code><a href="#topic+vi_pearson">vi_pearson</a>()</code>,
<code><a href="#topic+vi_squared_hellinger">vi_squared_hellinger</a>()</code>
</p>

<hr>
<h2 id='vi_t_power'>The T-Power Csiszar-function in log-space</h2><span id='topic+vi_t_power'></span>

<h3>Description</h3>

<p>A Csiszar-function is a member of <code style="white-space: pre;">&#8288;F = { f:R_+ to R : f convex }&#8288;</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi_t_power(logu, t, self_normalized = FALSE, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vi_t_power_+3A_logu">logu</code></td>
<td>
<p><code>float</code>-like <code>Tensor</code> representing <code>log(u)</code> from above.</p>
</td></tr>
<tr><td><code id="vi_t_power_+3A_t">t</code></td>
<td>
<p><code>Tensor</code> of same <code>dtype</code> as <code>logu</code> and broadcastable shape.</p>
</td></tr>
<tr><td><code id="vi_t_power_+3A_self_normalized">self_normalized</code></td>
<td>
<p><code>logical</code> indicating whether <code style="white-space: pre;">&#8288;f'(u=1)=0&#8288;</code>. When
<code style="white-space: pre;">&#8288;f'(u=1)=0&#8288;</code> the implied Csiszar f-Divergence remains non-negative even
when <code style="white-space: pre;">&#8288;p, q&#8288;</code> are unnormalized measures.</p>
</td></tr>
<tr><td><code id="vi_t_power_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>self_normalized = True</code> the T-Power Csiszar-function is:
</p>
<div class="sourceCode"><pre>f(u) = s [ u**t - 1 - t(u - 1) ]
s = { -1   0 &lt; t &lt; 1 }
    { +1   otherwise }
</pre></div>
<p>When <code>self_normalized = False</code> the <code>- t(u - 1)</code> term is omitted.
</p>
<p>This is similar to the <code>amari_alpha</code> Csiszar-function, with the associated
divergence being the same up to factors depending only on <code>t</code>.
</p>
<p>Warning: when self_normalized = True<code style="white-space: pre;">&#8288;this function makes non-log-space calculations and may therefore be numerically unstable for&#8288;</code>|logu| &gt;&gt; 0'.
</p>


<h3>Value</h3>

<p>t_power_of_u: <code>float</code>-like <code>Tensor</code> of the Csiszar-function
evaluated at <code>u = exp(logu)</code>.
</p>


<h3>See Also</h3>

<p>Other vi-functions#': 
<code><a href="#topic+vi_total_variation">vi_total_variation</a>()</code>,
<code><a href="#topic+vi_triangular">vi_triangular</a>()</code>
</p>

<hr>
<h2 id='vi_total_variation'>The Total Variation Csiszar-function in log-space</h2><span id='topic+vi_total_variation'></span>

<h3>Description</h3>

<p>A Csiszar-function is a member of <code style="white-space: pre;">&#8288;F = { f:R_+ to R : f convex }&#8288;</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi_total_variation(logu, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vi_total_variation_+3A_logu">logu</code></td>
<td>
<p><code>float</code>-like <code>Tensor</code> representing <code>log(u)</code> from above.</p>
</td></tr>
<tr><td><code id="vi_total_variation_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Total-Variation Csiszar-function is:
</p>
<div class="sourceCode"><pre>f(u) = 0.5 |u - 1|
</pre></div>
<p>Warning: this function makes non-log-space calculations and may therefore be
numerically unstable for <code style="white-space: pre;">&#8288;|logu| &gt;&gt; 0&#8288;</code>.
</p>


<h3>Value</h3>

<p>total_variation_of_u: <code>float</code>-like <code>Tensor</code> of the Csiszar-function
evaluated at <code>u = exp(logu)</code>.
</p>


<h3>See Also</h3>

<p>Other vi-functions#': 
<code><a href="#topic+vi_t_power">vi_t_power</a>()</code>,
<code><a href="#topic+vi_triangular">vi_triangular</a>()</code>
</p>

<hr>
<h2 id='vi_triangular'>The Triangular Csiszar-function in log-space</h2><span id='topic+vi_triangular'></span>

<h3>Description</h3>

<p>The Triangular Csiszar-function is:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi_triangular(logu, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vi_triangular_+3A_logu">logu</code></td>
<td>
<p><code>float</code>-like <code>Tensor</code> representing <code>log(u)</code> from above.</p>
</td></tr>
<tr><td><code id="vi_triangular_+3A_name">name</code></td>
<td>
<p>name prefixed to Ops created by this function.</p>
</td></tr>
</table>


<h3>Details</h3>

<div class="sourceCode"><pre>f(u) = (u - 1)**2 / (1 + u)
</pre></div>
<p>Warning: this function makes non-log-space calculations and may
therefore be numerically unstable for <code style="white-space: pre;">&#8288;|logu| &gt;&gt; 0&#8288;</code>.
</p>


<h3>Value</h3>

<p>triangular_of_u: <code>float</code>-like <code>Tensor</code> of the Csiszar-function
evaluated at <code>u = exp(logu)</code>.
</p>


<h3>See Also</h3>

<p>Other vi-functions#': 
<code><a href="#topic+vi_t_power">vi_t_power</a>()</code>,
<code><a href="#topic+vi_total_variation">vi_total_variation</a>()</code>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
