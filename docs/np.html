<!DOCTYPE html><html><head><title>Help for package np</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {np}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#b.star'><p>Compute Optimal Block Length for Stationary and Circular Bootstrap</p></a></li>
<li><a href='#cps71'><p> Canadian High School Graduate Earnings</p></a></li>
<li><a href='#Engel95'><p> 1995 British Family Expenditure Survey</p></a></li>
<li><a href='#gradients'><p>Extract Gradients</p></a></li>
<li><a href='#Italy'><p> Italian GDP Panel</p></a></li>
<li><a href='#np'><p>Nonparametric Kernel Smoothing Methods for Mixed Data Types</p></a></li>
<li><a href='#npcdens'><p>Kernel Conditional Density Estimation with Mixed Data Types</p></a></li>
<li><a href='#npcdensbw'><p>Kernel Conditional Density Bandwidth Selection with Mixed Data Types</p></a></li>
<li><a href='#npcdist'><p>Kernel Conditional Distribution Estimation with Mixed Data Types</p></a></li>
<li><a href='#npcdistbw'><p>Kernel Conditional Distribution Bandwidth Selection with Mixed</p>
Data Types</a></li>
<li><a href='#npcmstest'><p> Kernel Consistent Model Specification Test with Mixed Data Types</p></a></li>
<li><a href='#npconmode'><p> Kernel Modal Regression with Mixed Data Types</p></a></li>
<li><a href='#npcopula'><p> Kernel Copula Estimation with Mixed Data Types</p></a></li>
<li><a href='#npdeneqtest'><p> Kernel Consistent Density Equality Test with Mixed Data Types</p></a></li>
<li><a href='#npdeptest'><p> Kernel Consistent Pairwise Nonlinear Dependence Test for Univariate Processes</p></a></li>
<li><a href='#npindex'><p>Semiparametric Single Index Model</p></a></li>
<li><a href='#npindexbw'><p>Semiparametric Single Index Model Parameter and Bandwidth Selection</p></a></li>
<li><a href='#npksum'><p> Kernel Sums with Mixed Data Types</p></a></li>
<li><a href='#npplot'><p>General Purpose Plotting of Nonparametric Objects</p></a></li>
<li><a href='#npplreg'><p>Partially Linear Kernel Regression with Mixed Data Types</p></a></li>
<li><a href='#npplregbw'><p>Partially Linear Kernel Regression Bandwidth Selection with Mixed Data Types</p></a></li>
<li><a href='#npqcmstest'><p> Kernel Consistent Quantile Regression Model Specification Test</p>
with Mixed Data Types</a></li>
<li><a href='#npqreg'><p>Kernel Quantile Regression with Mixed Data Types</p></a></li>
<li><a href='#npquantile'><p> Kernel Univariate Quantile Estimation</p></a></li>
<li><a href='#npreg'><p>Kernel Regression with Mixed Data Types</p></a></li>
<li><a href='#npregbw'><p>Kernel Regression Bandwidth Selection with Mixed Data Types</p></a></li>
<li><a href='#npregiv'>
<p>Nonparametric Instrumental Regression</p></a></li>
<li><a href='#npregivderiv'>
<p>Nonparametric Instrumental Derivatives</p></a></li>
<li><a href='#npscoef'><p>Smooth Coefficient Kernel Regression</p></a></li>
<li><a href='#npscoefbw'><p>Smooth Coefficient Kernel Regression Bandwidth Selection</p></a></li>
<li><a href='#npsdeptest'><p> Kernel Consistent Serial Dependence Test for Univariate Nonlinear Processes</p></a></li>
<li><a href='#npseed'><p>Set Random Seed</p></a></li>
<li><a href='#npsigtest'><p>Kernel Regression Significance Test with Mixed Data Types</p></a></li>
<li><a href='#npsymtest'><p> Kernel Consistent Density Asymmetry Test with Mixed Data Types</p></a></li>
<li><a href='#nptgauss'><p>Truncated Second-order Gaussian Kernels</p></a></li>
<li><a href='#npudens'><p> Kernel Density Estimation with Mixed Data Types</p></a></li>
<li><a href='#npudensbw'><p>Kernel Density Bandwidth Selection with Mixed Data Types</p></a></li>
<li><a href='#npudist'><p> Kernel Distribution Estimation with Mixed Data Types</p></a></li>
<li><a href='#npudistbw'><p>Kernel Distribution Bandwidth Selection with Mixed Data Types</p></a></li>
<li><a href='#npuniden.boundary'><p> Kernel Bounded Univariate Density Estimation Via Boundary Kernel</p>
Functions</a></li>
<li><a href='#npuniden.reflect'><p> Kernel Bounded Univariate Density Estimation Via Data-Reflection</p></a></li>
<li><a href='#npuniden.sc'>
<p>Kernel Shape Constrained Bounded Univariate Density Estimation</p></a></li>
<li><a href='#npunitest'><p> Kernel Consistent Univariate Density Equality Test with Mixed Data Types</p></a></li>
<li><a href='#oecdpanel'><p> Cross Country Growth Panel</p></a></li>
<li><a href='#se'><p>Extract Standard Errors</p></a></li>
<li><a href='#uocquantile'><p>Compute Quantiles</p></a></li>
<li><a href='#wage1'><p> Cross-Sectional Data on Wages</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>0.60-17</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-03-12</td>
</tr>
<tr>
<td>Imports:</td>
<td>boot, cubature, methods, quadprog, quantreg, stats</td>
</tr>
<tr>
<td>Suggests:</td>
<td>MASS, logspline, ks</td>
</tr>
<tr>
<td>Title:</td>
<td>Nonparametric Kernel Smoothing Methods for Mixed Data Types</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jeffrey S. Racine &lt;racinej@mcmaster.ca&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Nonparametric (and semiparametric) kernel methods that seamlessly handle a mix of continuous, unordered, and ordered factor data types. We would like to gratefully acknowledge support from the Natural Sciences and Engineering Research Council of Canada (NSERC, <a href="https://www.nserc-crsng.gc.ca/">https://www.nserc-crsng.gc.ca/</a>), the Social Sciences and Humanities Research Council of Canada (SSHRC, <a href="https://www.sshrc-crsh.gc.ca/">https://www.sshrc-crsh.gc.ca/</a>), and the Shared Hierarchical Academic Research Computing Network (SHARCNET, <a href="https://sharcnet.ca/">https://sharcnet.ca/</a>). We would also like to acknowledge the contributions of the GNU GSL authors. In particular, we adapt the GNU GSL B-spline routine gsl_bspline.c adding automated support for quantile knots (in addition to uniform knots), providing missing functionality for derivatives, and for extending the splines beyond their endpoints.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/JeffreyRacine/R-Package-np">https://github.com/JeffreyRacine/R-Package-np</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/JeffreyRacine/R-Package-np/issues">https://github.com/JeffreyRacine/R-Package-np/issues</a></td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-03-12 22:19:04 UTC; jracine</td>
</tr>
<tr>
<td>Author:</td>
<td>Jeffrey S. Racine [aut, cre],
  Tristen Hayfield [aut]</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-03-13 09:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='b.star'>Compute Optimal Block Length for Stationary and Circular Bootstrap</h2><span id='topic+b.star'></span>

<h3>Description</h3>

<p><code>b.star</code> is a function which computes the optimal block length
for the continuous variable <code>data</code> using the method described in
Patton, Politis and White (2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>b.star(data,
       Kn = NULL,
       mmax= NULL,
       Bmax = NULL,
       c = NULL,
       round = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="b.star_+3A_data">data</code></td>
<td>
<p>data, an n x k matrix, each column being a data series.</p>
</td></tr>
<tr><td><code id="b.star_+3A_kn">Kn</code></td>
<td>
<p> See footnote c, page 59, Politis and White (2004). Defaults
to <code>ceiling(log10(n))</code>.</p>
</td></tr>
<tr><td><code id="b.star_+3A_mmax">mmax</code></td>
<td>
<p>See Politis and White (2004). Defaults to
<code>ceiling(sqrt(n))+Kn</code>.</p>
</td></tr>
<tr><td><code id="b.star_+3A_bmax">Bmax</code></td>
<td>
<p>See Politis and White (2004). Defaults to
<code>ceiling(min(3*sqrt(n),n/3))</code>.</p>
</td></tr>
<tr><td><code id="b.star_+3A_c">c</code></td>
<td>
<p>See Politis and White (2004). Defaults to
<code>qnorm(0.975)</code>.</p>
</td></tr>
<tr><td><code id="b.star_+3A_round">round</code></td>
<td>
<p>whether to round the result or not. Defaults to FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>b.star</code> is a function which computes optimal block lengths for
the stationary and circular bootstraps. This allows the use of
<code>tsboot</code> from the <code><a href="boot.html#topic+boot">boot</a></code> package to be fully
automatic by using the output from <code>b.star</code> as an input to the
argument <code>l = </code> in <code>tsboot</code>. See below for an example.
</p>


<h3>Value</h3>

<p>A kx2 matrix of optimal bootstrap block lengths computed from
<code>data</code> for the stationary bootstrap and circular bootstrap (column
1 is for the stationary bootstrap, column 2 the circular).
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Patton, A. and D.N. Politis and H. White (2009), &ldquo;CORRECTION TO
&quot;Automatic block-length selection for the dependent bootstrap&quot; by
D. Politis and H. White&rdquo;, Econometric Reviews 28(4), 372-375.
</p>
<p>Politis, D.N. and J.P. Romano (1994), &ldquo;Limit theorems for
weakly dependent Hilbert space valued random variables with
applications to the stationary bootstrap&rdquo;, Statistica Sinica 4,
461-476.
</p>
<p>Politis, D.N. and H. White (2004), &ldquo;Automatic block-length
selection for the dependent bootstrap&rdquo;, Econometric Reviews 23(1),
53-70.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(12345)

# Function to generate an AR(1) series

ar.series &lt;- function(phi,epsilon) {
  n &lt;- length(epsilon)
  series &lt;- numeric(n)
  series[1] &lt;- epsilon[1]/(1-phi)
  for(i in 2:n) {
    series[i] &lt;- phi*series[i-1] + epsilon[i]
  }
  return(series)
}

yt &lt;- ar.series(0.1,rnorm(10000))
b.star(yt,round=TRUE)

yt &lt;- ar.series(0.9,rnorm(10000))
b.star(yt,round=TRUE)
</code></pre>

<hr>
<h2 id='cps71'> Canadian High School Graduate Earnings  </h2><span id='topic+cps71'></span>

<h3>Description</h3>

<p>Canadian cross-section wage data consisting of a random sample taken
from the 1971 Canadian Census Public Use Tapes for male individuals
having common education (grade 13). There are 205 observations in total.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("cps71")</code></pre>


<h3>Format</h3>

<p> A data frame with 2 columns, and 205 rows.
</p>

<dl>
<dt>logwage</dt><dd><p> the first column, of type <code>numeric</code></p>
</dd>
<dt>age</dt><dd><p> the second column, of type <code>integer</code></p>
</dd>
</dl>



<h3>Source</h3>

<p> Aman Ullah </p>


<h3>References</h3>

<p>Pagan, A. and A. Ullah (1999), <em>Nonparametric Econometrics,</em>
Cambridge	University Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("cps71")
attach(cps71)

plot(age, logwage, xlab="Age", ylab="log(wage)")

detach(cps71)
</code></pre>

<hr>
<h2 id='Engel95'> 1995 British Family Expenditure Survey  </h2><span id='topic+Engel95'></span>

<h3>Description</h3>

<p>British cross-section data consisting of a random sample taken from
the British Family Expenditure Survey for 1995. The households consist
of married couples with an employed head-of-household between the ages
of 25 and 55 years. There are 1655 household-level observations in
total.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("Engel95")</code></pre>


<h3>Format</h3>

<p> A data frame with 10 columns, and 1655 rows.
</p>

<dl>
<dt>food</dt><dd><p> expenditure share on food, of type <code>numeric</code></p>
</dd>
<dt>catering</dt><dd><p> expenditure share on catering, of type <code>numeric</code></p>
</dd>
<dt>alcohol</dt><dd><p> expenditure share on alcohol, of type <code>numeric</code></p>
</dd>
<dt>fuel</dt><dd><p> expenditure share on fuel, of type <code>numeric</code></p>
</dd>
<dt>motor</dt><dd><p> expenditure share on motor, of type <code>numeric</code></p>
</dd>
<dt>fares</dt><dd><p> expenditure share on fares, of type <code>numeric</code></p>
</dd>
<dt>leisure</dt><dd><p> expenditure share on leisure, of type <code>numeric</code></p>
</dd>
<dt>logexp</dt><dd><p> logarithm of total expenditure, of type <code>numeric</code></p>
</dd>
<dt>logwages</dt><dd><p> logarithm of total earnings, of type <code>numeric</code></p>
</dd>
<dt>nkids</dt><dd><p> number of children, of type <code>numeric</code></p>
</dd>  
</dl>



<h3>Source</h3>

<p> Richard Blundell and Dennis Kristensen </p>


<h3>References</h3>

<p>Blundell, R. and X. Chen and D. Kristensen (2007),
&ldquo;Semi-Nonparametric IV Estimation of Shape-Invariant Engel
Curves,&rdquo; Econometrica, 75, 1613-1669.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics:
Theory and Practice,</em> Princeton University Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Example - compute nonparametric instrumental regression using
## Landweber-Fridman iteration of Fredholm integral equations of the
## first kind.

## We consider an equation with an endogenous regressor (`z') and an
## instrument (`w'). Let y = phi(z) + u where phi(z) is the function of
## interest. Here E(u|z) is not zero hence the conditional mean E(y|z)
## does not coincide with the function of interest, but if there exists
## an instrument w such that E(u|w) = 0, then we can recover the
## function of interest by solving an ill-posed inverse problem.

data(Engel95)

## Sort on logexp (the endogenous regressor) for plotting purposes

Engel95 &lt;- Engel95[order(Engel95$logexp),] 

attach(Engel95)

model.iv &lt;- npregiv(y=food,z=logexp,w=logwages,method="Landweber-Fridman")
phihat &lt;- model.iv$phi

## Compute the non-IV regression (i.e. regress y on z)

ghat &lt;- npreg(food~logexp,regtype="ll")

## For the plots, restrict focal attention to the bulk of the data
## (i.e. for the plotting area trim out 1/4 of one percent from each
## tail of y and z)

trim &lt;- 0.0025

plot(logexp,food,
     ylab="Food Budget Share",
     xlab="log(Total Expenditure)",
     xlim=quantile(logexp,c(trim,1-trim)),
     ylim=quantile(food,c(trim,1-trim)),
     main="Nonparametric Instrumental Kernel Regression",
     type="p",
     cex=.5,
     col="lightgrey")

lines(logexp,phihat,col="blue",lwd=2,lty=2)

lines(logexp,fitted(ghat),col="red",lwd=2,lty=4)

legend(quantile(logexp,trim),quantile(food,1-trim),
       c(expression(paste("Nonparametric IV: ",hat(varphi)(logexp))),
         "Nonparametric Regression: E(food | logexp)"),
       lty=c(2,4),
       col=c("blue","red"),
       lwd=c(2,2))

## End(Not run) 
</code></pre>

<hr>
<h2 id='gradients'>Extract Gradients</h2><span id='topic+gradients'></span><span id='topic+gradients.condensity'></span><span id='topic+gradients.condistribution'></span><span id='topic+gradients.npregression'></span><span id='topic+gradients.qregression'></span><span id='topic+gradients.singleindex'></span>

<h3>Description</h3>

<p><code>gradients</code> is a generic function which extracts gradients 
from objects. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gradients(x, ...)

## S3 method for class 'condensity'
gradients(x, errors = FALSE, ...)

## S3 method for class 'condistribution'
gradients(x, errors = FALSE, ...)

## S3 method for class 'npregression'
gradients(x, errors = FALSE, ...)

## S3 method for class 'qregression'
gradients(x, errors = FALSE, ...)

## S3 method for class 'singleindex'
gradients(x, errors = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gradients_+3A_x">x</code></td>
<td>
<p>an object for which the extraction of gradients is
meaningful.</p>
</td></tr>
<tr><td><code id="gradients_+3A_...">...</code></td>
<td>
<p>other arguments.</p>
</td></tr>
<tr><td><code id="gradients_+3A_errors">errors</code></td>
<td>
<p> a logical value specifying whether or not standard
errors of gradients are desired. Defaults to <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function provides a generic interface for extraction of gradients
from objects.
</p>


<h3>Value</h3>

<p>Gradients extracted from the model object <code>x</code>.
</p>


<h3>Note</h3>

<p>This method currently only supports objects from the <code><a href="#topic+np">np</a></code> library.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>See the references for the method being interrogated via
<code><a href="#topic+gradients">gradients</a></code> in the appropriate help file. For example, for
the particulars of the gradients for nonparametric regression see the
references in <code><a href="#topic+npreg">npreg</a></code>
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+fitted">fitted</a></code>, <code><a href="stats.html#topic+residuals">residuals</a></code>, <code><a href="stats.html#topic+coef">coef</a></code>,
and <code><a href="#topic+se">se</a></code>, for related methods;
<code><a href="#topic+np">np</a></code> for supported objects. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- runif(10)
y &lt;- x + rnorm(10, sd = 0.1)
gradients(npreg(y~x, gradients=TRUE))
</code></pre>

<hr>
<h2 id='Italy'> Italian GDP Panel </h2><span id='topic+Italy'></span>

<h3>Description</h3>

<p>Italian GDP growth panel for 21 regions covering the period 1951-1998
(millions of Lire, 1990=base). There are 1008 observations in total.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("Italy")</code></pre>


<h3>Format</h3>

<p> A data frame with 2 columns, and 1008 rows.
</p>

<dl>
<dt>year</dt><dd><p> the first column, of type <code>ordered</code></p>
</dd>
<dt>gdp</dt><dd><p> the second column, of type <code>numeric</code>: millions of
Lire, 1990=base</p>
</dd>
</dl>



<h3>Source</h3>

<p>Giovanni Baiocchi</p>


<h3>References</h3>

<p>Baiocchi, G. (2006), &ldquo;Economic Applications of Nonparametric
Methods,&rdquo; Ph.D. Thesis, University of York.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Italy")
attach(Italy)

plot(ordered(year), gdp, xlab="Year (ordered factor)",
     ylab="GDP (millions of Lire, 1990=base)")

detach(Italy)
</code></pre>

<hr>
<h2 id='np'>Nonparametric Kernel Smoothing Methods for Mixed Data Types</h2><span id='topic+np'></span>

<h3>Description</h3>

<p>This package provides a variety of nonparametric and semiparametric
kernel methods that seamlessly handle a mix of continuous, unordered,
and ordered factor data types (unordered and ordered factors are often
referred to as &lsquo;nominal&rsquo; and &lsquo;ordinal&rsquo; categorical
variables respectively). A vignette containing many of the examples
found in the help files accompanying the <span class="pkg">np</span> package that is
intended to serve as a gentle introduction to this package can be
accessed via <code>vignette("np", package="np")</code>.
</p>
<p>For a listing of all routines in the <span class="pkg">np</span> package type:
&lsquo;library(help=&quot;np&quot;)&rsquo;.  
</p>
<p>Bandwidth selection is a key aspect of sound nonparametric and
semiparametric kernel estimation. <code>np</code> is designed from the
ground up to make bandwidth selection the focus of attention. To this
end, one typically begins by creating a &lsquo;bandwidth object&rsquo;
which embodies all aspects of the method, including specific kernel
functions, data names, data types, and the like. One then passes these
bandwidth objects to other functions, and those functions can grab the
specifics from the bandwidth object thereby removing potential
inconsistencies and unnecessary repetition. Furthermore, many
functions such as <code><a href="base.html#topic+plot">plot</a></code> (which automatically calls
<code>npplot</code>) can work with the bandwidth object directly without
having to do the subsequent companion function evaluation.
</p>
<p>As of <code>np</code> version 0.20-0, we allow the user to combine these
steps. When using <code>np</code> versions 0.20-0 and higher, if the first
step (bandwidth selection) is not performed explicitly then the second
step will automatically call the omitted first step bandwidth selector
using defaults unless otherwise specified, and the bandwidth object
could then be retrieved retroactively if so desired via
<code>objectname$bws</code>. Furthermore, options for bandwidth selection
will be passed directly to the bandwidth selector function. Note that
the combined approach would not be a wise choice for certain
applications such as when bootstrapping (as it would involve
unnecessary computation since the bandwidths would properly be those
for the original sample and not the bootstrap resamples) or when
conducting quantile regression (as it would involve unnecessary
computation when different quantiles are computed from the same
conditional cumulative distribution estimate).
</p>
<p>There are two ways in which you can interact with functions in
<code>np</code>, either i) using data frames, or ii) using a formula
interface, where appropriate.
</p>
<p>To some, it may be natural to use the data frame interface.  The R
<code><a href="base.html#topic+data.frame">data.frame</a></code> function preserves a variable's type once it
has been cast (unlike <code><a href="base.html#topic+cbind">cbind</a></code>, which we avoid for this
reason).  If you find this most natural for your project, you first
create a data frame casting data according to their type (i.e., one of
continuous (default, <code><a href="base.html#topic+numeric">numeric</a></code>), <code><a href="base.html#topic+factor">factor</a></code>,
<code><a href="base.html#topic+ordered">ordered</a></code>). Then you would simply pass this data frame to
the appropriate <code>np</code> function, for example
<code>npudensbw(dat=data)</code>.
</p>
<p>To others, however, it may be natural to use the formula interface
that is used for the regression examples, among others. For
nonparametric regression functions such as <code><a href="#topic+npreg">npreg</a></code>, you
would proceed as you would using <code><a href="stats.html#topic+lm">lm</a></code> (e.g., <code>bw &lt;-
  npregbw(y~factor(x1)+x2))</code> except that you would of course not need to
specify, e.g., polynomials in variables, interaction terms, or create
a number of dummy variables for a factor.  Every function in np
supports both interfaces, where appropriate.
</p>
<p>Note that if your factor is in fact a character string such as, say,
<code>X</code> being either <code>"MALE"</code> or <code>"FEMALE"</code>, np will handle
this directly, i.e., there is no need to map the string values into
unique integers such as (0,1). Once the user casts a variable as a
particular data type (i.e., <code><a href="base.html#topic+factor">factor</a></code>,
<code><a href="base.html#topic+ordered">ordered</a></code>, or continuous (default,
<code><a href="base.html#topic+numeric">numeric</a></code>)), all subsequent methods automatically detect
the type and use the appropriate kernel function and method where
appropriate.
</p>
<p>All estimation methods are fully multivariate, i.e., there are no
limitations on the number of variables one can model (or number of
observations for that matter). Execution time for most routines is,
however, exponentially increasing in the number of observations and
increases with the number of variables involved.
</p>
<p>Nonparametric methods include unconditional density (distribution),
conditional density (distribution), regression, mode, and quantile
estimators along with gradients where appropriate, while
semiparametric methods include single index, partially linear, and
smooth (i.e., varying) coefficient models.
</p>
<p>A number of tests are included such as consistent specification tests
for parametric regression and quantile regression models along with
tests of significance for nonparametric regression.
</p>
<p>A variety of bootstrap methods for computing standard errors,
nonparametric confidence bounds, and bias-corrected bounds are
implemented.
</p>
<p>A variety of bandwidth methods are implemented including fixed,
nearest-neighbor, and adaptive nearest-neighbor.
</p>
<p>A variety of data-driven methods of bandwidth selection are
implemented, while the user can specify their own bandwidths should
they so choose (either a raw bandwidth or scaling factor).
</p>
<p>A flexible plotting utility, <code><a href="#topic+npplot">npplot</a></code> (which is
automatically invoked by <code><a href="base.html#topic+plot">plot</a></code>) , facilitates graphing of
multivariate objects. An example for creating postscript graphs using
the <code><a href="#topic+npplot">npplot</a></code> utility and pulling this into a LaTeX
document is provided.
</p>
<p>The function <code><a href="#topic+npksum">npksum</a></code> allows users to create or implement
their own kernel estimators or tests should they so desire.
</p>
<p>The underlying functions are written in C for computational
efficiency. Despite this, due to their nature, data-driven bandwidth
selection methods involving multivariate numerical search can be
time-consuming, particularly for large datasets. A version of this
package using the <code>Rmpi</code> wrapper is under development that allows
one to deploy this software in a clustered computing environment to
facilitate computation involving large datasets.
</p>
<p>To cite the <code>np</code> package, type <code>citation("np")</code> from within
<code>R</code> for details.
</p>


<h3>Details</h3>

<p> The kernel methods in <code>np</code> employ the so-called
&lsquo;generalized product kernels&rsquo; found in Hall, Racine,
and Li (2004), Li, Lin, and Racine (2013), Li, Ouyang, and
Racine (2013), Li and Racine (2003), Li and Racine (2004), Li
and Racine (2007), Li and Racine (2010), Ouyang, Li, and
Racine (2006), and Racine and Li (2004), among others. For
details on a particular method, kindly refer to the original
references listed above.
</p>
<p>We briefly describe the particulars of various univariate kernels used
to generate the generalized product kernels that underlie the kernel
estimators implemented in the <code>np</code> package. In a nutshell, the
generalized kernel functions that underlie the kernel estimators in
<code>np</code> are formed by taking the product of univariate kernels such
as those listed below. When you cast your data as a particular type
(continuous, factor, or ordered factor) in a data frame or formula,
the routines will automatically recognize the type of variable being
modelled and use the appropriate kernel type for each variable in the
resulting estimator.
</p>

<dl>
<dt>Second Order Gaussian (<code class="reqn">x</code> is continuous)</dt><dd>
<p><code class="reqn">k(z) = \exp(-z^2/2)/\sqrt{2\pi}</code> where <code class="reqn">z=(x_i-x)/h</code>,
and <code class="reqn">h&gt;0</code>.
</p>
</dd>
<dt>Second Order Truncated Gaussian (<code class="reqn">x</code> is continuous)</dt><dd>
<p><code class="reqn">k(z) = (\exp(-z^2/2)-\exp(-b^2/2))/(\textrm{erf}(b/\sqrt{2})\sqrt{2\pi}-2b\exp(-b^2/2))</code> where <code class="reqn">z=(x_i-x)/h</code>, <code class="reqn">b&gt;0</code>, <code class="reqn">|z|\le b</code>
and <code class="reqn">h&gt;0</code>.
</p>
<p>See <code><a href="#topic+nptgauss">nptgauss</a></code> for details on modifying <code class="reqn">b</code>.
</p>
</dd>
<dt>Second Order Epanechnikov (<code class="reqn">x</code> is continuous)</dt><dd>
<p><code class="reqn">k(z) =  3\left(1 - z^2/5\right)/(4\sqrt{5})</code>
if <code class="reqn">z^2&lt;5</code>, <code class="reqn">0</code> otherwise, where
<code class="reqn">z=(x_i-x)/h</code>, and <code class="reqn">h&gt;0</code>.
</p>
</dd>
<dt>Uniform (<code class="reqn">x</code> is continuous)</dt><dd>
<p><code class="reqn">k(z) = 1/2</code> if <code class="reqn">|z|&lt;1</code>, <code class="reqn">0</code> otherwise, where
<code class="reqn">z=(x_i-x)/h</code>, and <code class="reqn">h&gt;0</code>.
</p>
</dd>
<dt>Aitchison and Aitken (<code class="reqn">x</code> is a (discrete) factor)</dt><dd>
<p><code class="reqn">l(x_i,x,\lambda) = 1 - \lambda</code> if <code class="reqn">x_i=x</code>, and
<code class="reqn">\lambda/(c-1)</code> if <code class="reqn">x_i \neq x</code>,
where <code class="reqn">c</code> is the number of (discrete) outcomes assumed by the
factor <code class="reqn">x</code>.
</p>
<p>Note that <code class="reqn">\lambda</code> must lie between <code class="reqn">0</code> and
<code class="reqn">(c-1)/c</code>.
</p>
</dd>
<dt>Wang and van Ryzin (<code class="reqn">x</code> is a (discrete) ordered factor)</dt><dd>
<p><code class="reqn">l(x_i,x,\lambda) = 1 - \lambda</code> if <code class="reqn">|x_i-x|=0</code>, and
<code class="reqn">((1-\lambda)/2)\lambda^{|x_i-x|}</code>
if <code class="reqn">|x_i - x|\ge1</code>.
</p>
<p>Note that <code class="reqn">\lambda</code> must lie between <code class="reqn">0</code> and
<code class="reqn">1</code>.
</p>
</dd>
<dt>Li and Racine (<code class="reqn">x</code> is a (discrete) factor)</dt><dd>
<p><code class="reqn">l(x_i,x,\lambda) = 1 </code> if <code class="reqn">x_i=x</code>, and
<code class="reqn">\lambda</code> if <code class="reqn">x_i \neq x</code>.
</p>
<p>Note that <code class="reqn">\lambda</code> must lie between <code class="reqn">0</code> and
<code class="reqn">1</code>.
</p>
</dd>
<dt>Li and Racine Normalised for Unconditional Objects (<code class="reqn">x</code> is a (discrete) factor)</dt><dd>
<p><code class="reqn">l(x_i,x,\lambda) = 1/(1+(c-1)\lambda) </code> if <code class="reqn">x_i=x</code>, and
<code class="reqn">\lambda/(1+(c-1)\lambda)</code> if <code class="reqn">x_i \neq x</code>.
</p>
<p>Note that <code class="reqn">\lambda</code> must lie between <code class="reqn">0</code> and
<code class="reqn">1</code>.
</p>
</dd>
<dt>Li and Racine (<code class="reqn">x</code> is a (discrete) ordered factor)</dt><dd>
<p><code class="reqn">l(x_i,x,\lambda) = 1</code> if
<code class="reqn">|x_i-x|=0</code>, and
<code class="reqn">\lambda^{|x_i-x|}</code> if <code class="reqn">|x_i -
        x|\ge1</code>.
</p>
<p>Note that <code class="reqn">\lambda</code> must lie between <code class="reqn">0</code> and
<code class="reqn">1</code>.
</p>
</dd>
<dt>Li and Racine Normalised for Unconditional Objects (<code class="reqn">x</code> is a (discrete) ordered factor)</dt><dd>
<p><code class="reqn">l(x_i,x,\lambda) = (1-\lambda)/(1+\lambda)</code> if
<code class="reqn">|x_i-x|=0</code>, and
<code class="reqn">(1-\lambda)/(1+\lambda)\lambda^{|x_i-x|}</code> if <code class="reqn">|x_i -
        x|\ge1</code>.
</p>
<p>Note that <code class="reqn">\lambda</code> must lie between <code class="reqn">0</code> and
<code class="reqn">1</code>.
</p>
</dd>
</dl>

<p>So, if you had two variables, <code class="reqn">x_{i1}</code> and
<code class="reqn">x_{i2}</code>, and <code class="reqn">x_{i1}</code> was continuous while
<code class="reqn">x_{i2}</code> was, say, binary (0/1), and you created a data
frame of the form <code>X &lt;- data.frame(x1,x2=factor(x2))</code>, then the
kernel function used by <code>np</code> would be
<code class="reqn">K(\cdot)=k(\cdot)\times l(\cdot)</code> where the
particular kernel functions <code class="reqn">k(\cdot)</code> and
<code class="reqn">l(\cdot)</code> would be, say, the second order Gaussian
(<code>ckertype="gaussian"</code>) and Aitchison and Aitken
(<code>ukertype="aitchisonaitken"</code>) kernels by default, respectively
(note that for conditional density and distribution objects we can
specify kernels for the left-hand side and right-hand side variables
in this manner using <code>cykertype="gaussian"</code>,
<code>cxkertype="gaussian"</code> and <code>uykertype="aitchisonaitken"</code>,
<code>uxkertype="aitchisonaitken"</code>).
</p>
<p>Note that higher order continuous kernels (i.e., fourth, sixth, and
eighth order) are derived from the second order kernels given above
(see Li and Racine (2007) for details).
</p>
<p>For particulars on any given method, kindly see the references listed
for the method in question.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield &lt;tristen.hayfield@gmail.com&gt;, Jeffrey S. Racine
&lt;racinej@mcmaster.ca&gt;
</p>
<p>Maintainer: Jeffrey S. Racine &lt;racinej@mcmaster.ca&gt;
</p>
<p>We are grateful to John Fox and Achim Zeleis for their valuable input
and encouragement. We would like to gratefully acknowledge support
from the Natural Sciences and Engineering Research Council of Canada
(NSERC:www.nserc.ca), the Social Sciences and Humanities Research
Council of Canada (SSHRC:www.sshrc.ca), and the Shared Hierarchical
Academic Research Computing Network (SHARCNET:www.sharcnet.ca)
</p>


<h3>References</h3>

<p>Aitchison, J. and C.G.G. Aitken (1976), &ldquo;Multivariate binary
discrimination by the kernel method,&rdquo; Biometrika, 63, 413-420.
</p>
<p>Hall, P. and J.S. Racine and Q. Li (2004), &ldquo;Cross-validation
and the estimation of conditional probability densities,&rdquo; Journal of
the American Statistical Association, 99, 1015-1026.
</p>
<p>Li, Q. and J. Lin and J.S. Racine (2013), &ldquo;Optimal bandwidth
selection for nonparametric conditional distribution and quantile
functions&rdquo;, Journal of Business and Economic Statistics, 31, 57-65.
</p>
<p>Li, Q. and D. Ouyang and J.S. Racine (2013), &ldquo;Categorical
Semiparametric Varying-Coefficient Models,&rdquo; Journal of Applied
Econometrics, 28, 551-589.
</p>
<p>Li, Q. and J.S. Racine (2003), &ldquo;Nonparametric estimation of
distributions with categorical and continuous data,&rdquo; Journal of
Multivariate Analysis, 86, 266-292.
</p>
<p>Li, Q. and J.S. Racine (2004), &ldquo;Cross-validated local linear
nonparametric regression,&rdquo; Statistica Sinica, 14, 485-512.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics:
Theory and Practice,</em> Princeton University Press.
</p>
<p>Li, Q. and J.S. Racine (2010), &ldquo;Smooth varying-coefficient
estimation and inference for qualitative and quantitative data,&rdquo;
Econometric Theory, 26, 1-31.
</p>
<p>Ouyang, D. and Q. Li and J.S. Racine (2006), &ldquo;Cross-validation
and the estimation of probability distributions with categorical
data,&rdquo; Journal of Nonparametric Statistics, 18, 69-100.
</p>
<p>Racine, J.S. and Q. Li (2004), &ldquo;Nonparametric estimation of
regression functions with both categorical and continuous data,&rdquo;
Journal of Econometrics, 119, 99-130.
</p>
<p>Pagan, A. and A. Ullah (1999), <em>Nonparametric Econometrics,</em>
Cambridge University Press.
</p>
<p>Scott, D.W. (1992), <em>Multivariate Density Estimation: Theory,
Practice and Visualization,</em> New York: Wiley.
</p>
<p>Silverman, B.W. (1986), <em>Density Estimation,</em> London: Chapman and
Hall.
</p>
<p>Wang, M.C. and J. van Ryzin (1981), &ldquo;A class of smooth
estimators for discrete distributions,&rdquo; Biometrika, 68, 301-309.
</p>

<hr>
<h2 id='npcdens'>Kernel Conditional Density Estimation with Mixed Data Types</h2><span id='topic+npcdens'></span><span id='topic+npcdens.call'></span><span id='topic+npcdens.conbandwidth'></span><span id='topic+npcdens.default'></span><span id='topic+npcdens.formula'></span>

<h3>Description</h3>

<p><code>npcdens</code> computes kernel conditional density estimates on
<code class="reqn">p+q</code>-variate evaluation data, given a set of training data (both
explanatory and dependent) and a bandwidth specification (a
<code>conbandwidth</code> object or a bandwidth vector, bandwidth type, and
kernel type) using the method of Hall, Racine, and Li (2004).
The data may be continuous, discrete (unordered and ordered
factors), or some combination thereof. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npcdens(bws, ...)

## S3 method for class 'formula'
npcdens(bws, data = NULL, newdata = NULL, ...)

## S3 method for class 'call'
npcdens(bws, ...)

## S3 method for class 'conbandwidth'
npcdens(bws,
        txdat = stop("invoked without training data 'txdat'"),
        tydat = stop("invoked without training data 'tydat'"),
        exdat,
        eydat,
        gradients = FALSE,
        ...)

## Default S3 method:
npcdens(bws, txdat, tydat, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npcdens_+3A_bws">bws</code></td>
<td>

<p>a bandwidth specification. This can be set as a <code>conbandwidth</code>
object returned from a previous invocation of
<code><a href="#topic+npcdensbw">npcdensbw</a></code>, or as a <code class="reqn">p+q</code>-vector of bandwidths,
with each element <code class="reqn">i</code> up to <code class="reqn">i=q</code> corresponding to the
bandwidth for column <code class="reqn">i</code> in <code>tydat</code>, and each element
<code class="reqn">i</code> from <code class="reqn">i=q+1</code> to <code class="reqn">i=p+q</code> corresponding to the
bandwidth for column <code class="reqn">i-q</code> in <code>txdat</code>. If specified as a
vector, then additional arguments will need to be supplied as
necessary to specify the bandwidth type, kernel types, training
data, and so on.
</p>
</td></tr>
<tr><td><code id="npcdens_+3A_gradients">gradients</code></td>
<td>

<p>a logical value specifying whether to return estimates of the
gradients at the evaluation points. Defaults to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="npcdens_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to specify the bandwidth type,
kernel types, and so on.  This is necessary if you
specify bws as a <code class="reqn">p+q</code>-vector and not a <code>conbandwidth</code>
object, and you do not desire the default behaviours. To do this,
you may specify any of <code>bwmethod</code>, <code>bwscaling</code>,
<code>bwtype</code>, <code>cxkertype</code>, <code>cxkerorder</code>,
<code>cykertype</code>, <code>cykerorder</code>, <code>uxkertype</code>,
<code>uykertype</code>, <code>oxkertype</code>, <code>oykertype</code>, as described
in <code><a href="#topic+npcdensbw">npcdensbw</a></code>. 
</p>
</td></tr>
<tr><td><code id="npcdens_+3A_data">data</code></td>
<td>

<p>an optional data frame, list or environment (or object
coercible to a data frame by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code>) containing the variables
in the model. If not found in data, the variables are taken from
<code>environment(bws)</code>, typically the environment from which
<code><a href="#topic+npcdensbw">npcdensbw</a></code> was called.
</p>
</td></tr>
<tr><td><code id="npcdens_+3A_newdata">newdata</code></td>
<td>

<p>An optional data frame in which to look for evaluation data. If
omitted, the training data are used.  
</p>
</td></tr>
<tr><td><code id="npcdens_+3A_txdat">txdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of sample realizations of explanatory
data (training data). Defaults to the training data used to
compute the bandwidth object.
</p>
</td></tr>
<tr><td><code id="npcdens_+3A_tydat">tydat</code></td>
<td>

<p>a <code class="reqn">q</code>-variate data frame of sample realizations of dependent
data (training data). Defaults to the training data used to
compute the bandwidth object.
</p>
</td></tr>
<tr><td><code id="npcdens_+3A_exdat">exdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of explanatory data on which
conditional densities will be evaluated. By default,
evaluation takes place on the data provided by <code>txdat</code>.
</p>
</td></tr>
<tr><td><code id="npcdens_+3A_eydat">eydat</code></td>
<td>

<p>a <code class="reqn">q</code>-variate data frame of dependent data on which conditional
densities will be evaluated. By default,
evaluation takes place on the data provided by <code>tydat</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>npcdens</code> implements a variety of methods for estimating
multivariate conditional distributions (<code class="reqn">p+q</code>-variate) defined
over a set of possibly continuous and/or discrete (unordered, ordered)
data. The approach is based on Li and Racine (2004) who employ
&lsquo;generalized product kernels&rsquo; that admit a mix of continuous
and discrete data types.
</p>
<p>Three classes of kernel estimators for the continuous data types are
available: fixed, adaptive nearest-neighbor, and generalized
nearest-neighbor. Adaptive nearest-neighbor bandwidths change with
each sample realization in the set, <code class="reqn">x_i</code>, when estimating
the density at the point <code class="reqn">x</code>. Generalized nearest-neighbor
bandwidths change with the point at which the density is estimated,
<code class="reqn">x</code>. Fixed bandwidths are constant over the support of <code class="reqn">x</code>.
</p>
<p>Training and evaluation input data  may be a
mix of continuous (default), unordered discrete (to be specified in
the data frames using <code><a href="base.html#topic+factor">factor</a></code>), and ordered discrete (to be
specified in the data frames using <code><a href="base.html#topic+ordered">ordered</a></code>). Data can be
entered in an arbitrary order and data types will be detected
automatically by the routine (see <code><a href="#topic+np">np</a></code> for details).
</p>
<p>A variety of kernels may be specified by the user. Kernels implemented
for continuous data types include the second, fourth, sixth, and eighth
order Gaussian and Epanechnikov kernels, and the uniform
kernel. Unordered discrete data types use a variation on Aitchison and
Aitken's (1976) kernel, while ordered data types use a variation of the
Wang and van Ryzin (1981) kernel.
</p>


<h3>Value</h3>

<p><code>npcdens</code> returns a <code>condensity</code> object. The generic
accessor functions <code><a href="stats.html#topic+fitted">fitted</a></code>, <code><a href="#topic+se">se</a></code>, and
<code><a href="#topic+gradients">gradients</a></code>, extract estimated values, asymptotic standard
errors on estimates, and gradients, respectively, from the returned
object. Furthermore, the functions <code><a href="stats.html#topic+predict">predict</a></code>,
<code><a href="base.html#topic+summary">summary</a></code> and <code><a href="base.html#topic+plot">plot</a></code> support objects of both
classes. The returned objects have the following components:
</p>
<table>
<tr><td><code>xbw</code></td>
<td>
<p> bandwidth(s), scale factor(s) or nearest neighbours for the
explanatory data, <code>txdat</code> </p>
</td></tr>
<tr><td><code>ybw</code></td>
<td>
<p> bandwidth(s), scale factor(s) or nearest neighbours for the
dependent data, <code>tydat</code> </p>
</td></tr>
<tr><td><code>xeval</code></td>
<td>
<p> the evaluation points of the explanatory data </p>
</td></tr>
<tr><td><code>yeval</code></td>
<td>
<p> the evaluation points of the dependent data </p>
</td></tr>
<tr><td><code>condens</code></td>
<td>
<p> estimates of the conditional density at the evaluation
points </p>
</td></tr>
<tr><td><code>conderr</code></td>
<td>
<p> standard errors of the conditional density
estimates </p>
</td></tr>
<tr><td><code>congrad</code></td>
<td>
<p> if invoked with <code>gradients = TRUE</code>, estimates of
the gradients at the evaluation points </p>
</td></tr>
<tr><td><code>congerr</code></td>
<td>
<p> if invoked with <code>gradients = TRUE</code>, standard
errors of the gradients at the evaluation points </p>
</td></tr>
<tr><td><code>log_likelihood</code></td>
<td>
<p> log likelihood of the conditional density estimate </p>
</td></tr>
</table>


<h3>Usage Issues</h3>

<p>If you are using data of mixed types, then it is advisable to use the
<code><a href="base.html#topic+data.frame">data.frame</a></code> function to construct your input data and not
<code><a href="base.html#topic+cbind">cbind</a></code>, since <code><a href="base.html#topic+cbind">cbind</a></code> will typically not work as
intended on mixed data types and will coerce the data to the same
type.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine
<a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>    
</p>


<h3>References</h3>

<p>Aitchison, J. and C.G.G. Aitken (1976), &ldquo;Multivariate binary
discrimination by the kernel method,&rdquo; Biometrika, 63, 413-420.
</p>
<p>Hall, P. and J.S. Racine and Q. Li (2004), &ldquo;Cross-validation and the
estimation of conditional probability densities,&rdquo; Journal of the
American Statistical Association, 99, 1015-1026.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics: Theory
and Practice,</em> Princeton University Press.
</p>
<p>Pagan, A. and A. Ullah (1999), <em>Nonparametric Econometrics,</em> Cambridge
University Press. 
</p>
<p>Scott, D.W. (1992), <em>Multivariate Density Estimation. Theory,
Practice and Visualization,</em> New York: Wiley.
</p>
<p>Silverman, B.W. (1986), <em>Density Estimation,</em> London: Chapman and
Hall.
</p>
<p>Wang, M.C. and J. van Ryzin (1981), &ldquo;A class of smooth estimators
for discrete distributions,&rdquo;  Biometrika, 68, 301-309.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npudens">npudens</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# EXAMPLE 1 (INTERFACE=FORMULA): For this example, we load Giovanni
# Baiocchi's Italian GDP panel (see Italy for details), and compute the
# likelihood cross-validated bandwidths (default) using a second-order
# Gaussian kernel (default). Note - this may take a minute or two
# depending on the speed of your computer.

data("Italy")
attach(Italy)

# First, compute the bandwidths... note that this may take a minute or
# two depending on the speed of your computer. 

bw &lt;- npcdensbw(formula=gdp~ordered(year))

# Next, compute the condensity object...

fhat &lt;- npcdens(bws=bw)

# The object fhat now contains results such as the estimated conditional
# density function (fhat$condens) and so on...

summary(fhat)

# Call the plot() function to visualize the results (&lt;ctrl&gt;-C will
# interrupt on *NIX systems, &lt;esc&gt; will interrupt on MS Windows
# systems).

plot(bw)

detach(Italy)

# EXAMPLE 1 (INTERFACE=DATA FRAME): For this example, we load Giovanni
# Baiocchi's Italian GDP panel (see Italy for details), and compute the
# likelihood cross-validated bandwidths (default) using a second-order
# Gaussian kernel (default). Note - this may take a minute or two
# depending on the speed of your computer.

data("Italy")
attach(Italy)

# First, compute the bandwidths... note that this may take a minute or
# two depending on the speed of your computer. 

# Note - we cast `X' and `y' as data frames so that plot() can
# automatically grab names (this looks like overkill, but in
# multivariate settings you would do this anyway, so may as well get in
# the habit).

X &lt;- data.frame(year=ordered(year))
y &lt;- data.frame(gdp)

bw &lt;- npcdensbw(xdat=X, ydat=y)

# Next, compute the condensity object...

fhat &lt;- npcdens(bws=bw)

# The object fhat now contains results such as the estimated conditional
# density function (fhat$condens) and so on...

summary(fhat)

# Call the plot() function to visualize the results (&lt;ctrl&gt;-C will
# interrupt on *NIX systems, &lt;esc&gt; will interrupt on MS Windows systems).

plot(bw)

detach(Italy)

# EXAMPLE 2 (INTERFACE=FORMULA): For this example, we load the old
# faithful geyser data from the R `datasets' library and compute the
# conditional density function.

library("datasets")
data("faithful")
attach(faithful)

# Note - this may take a few minutes depending on the speed of your
# computer...

bw &lt;- npcdensbw(formula=eruptions~waiting)

summary(bw)

# Plot the density function (&lt;ctrl&gt;-C will interrupt on *NIX systems,
# &lt;esc&gt; will interrupt on MS Windows systems).

plot(bw)

detach(faithful)

# EXAMPLE 2 (INTERFACE=DATA FRAME): For this example, we load the old
# faithful geyser data from the R `datasets' library and compute the
# conditional density function.

library("datasets")
data("faithful")
attach(faithful)

# Note - this may take a few minutes depending on the speed of your
# computer...

# Note - we cast `X' and `y' as data frames so that plot() can
# automatically grab names (this looks like overkill, but in
# multivariate settings you would do this anyway, so may as well get in
# the habit).

X &lt;- data.frame(waiting)
y &lt;- data.frame(eruptions)

bw &lt;- npcdensbw(xdat=X, ydat=y)

summary(bw)

# Plot the density function (&lt;ctrl&gt;-C will interrupt on *NIX systems, 
# &lt;esc&gt; will interrupt on MS Windows systems)

plot(bw)

detach(faithful)

# EXAMPLE 3 (INTERFACE=FORMULA): Replicate the DGP of Klein &amp; Spady
# (1993) (see their description on page 405, pay careful attention to
# footnote 6 on page 405).

set.seed(123)

n &lt;- 1000

# x1 is chi-squared having 3 df truncated at 6 standardized by
# subtracting 2.348 and dividing by 1.511

x &lt;- rchisq(n, df=3)
x1 &lt;- (ifelse(x &lt; 6, x, 6) - 2.348)/1.511

# x2 is normal (0, 1) truncated at +- 2 divided by 0.8796

x &lt;- rnorm(n)
x2 &lt;- ifelse(abs(x) &lt; 2 , x, 2) / 0.8796

# y is 1 if y* &gt; 0, 0 otherwise.

y &lt;- ifelse(x1 + x2 + rnorm(n) &gt; 0, 1, 0)

# Generate data-driven bandwidths (likelihood cross-validation). Note -
# this may take a few minutes depending on the speed of your computer...

bw &lt;- npcdensbw(formula=factor(y)~x1+x2)

# Next, create the evaluation data in order to generate a perspective
# plot

x1.seq &lt;- seq(min(x1), max(x1), length=50)
x2.seq &lt;- seq(min(x2), max(x2), length=50)
X.eval &lt;- expand.grid(x1=x1.seq,x2=x2.seq)

data.eval &lt;- data.frame(y=factor(rep(1, nrow(X.eval))),x1=X.eval[,1],x2=X.eval[,2])

# Now evaluate the conditional probability for y=1 and for the
# evaluation Xs

fit &lt;- fitted(npcdens(bws=bw,newdata=data.eval))

# Finally, coerce the data into a matrix for plotting with persp()

fit.mat &lt;- matrix(fit, 50, 50)

# Generate a perspective plot similar to Figure 2 b of Klein and Spady
# (1993)

persp(x1.seq, 
      x2.seq, 
      fit.mat, 
      col="white", 
      ticktype="detailed", 
      expand=0.5, 
      axes=FALSE, 
      box=FALSE, 
      main="Estimated Nonparametric Probability Perspective", 
      theta=310, 
      phi=25)

# EXAMPLE 3 (INTERFACE=DATA FRAME): Replicate the DGP of Klein &amp; Spady
# (1993) (see their description on page 405, pay careful attention to
# footnote 6 on page 405).

set.seed(123)

n &lt;- 1000

# x1 is chi-squared having 3 df truncated at 6 standardized by
# subtracting 2.348 and dividing by 1.511

x &lt;- rchisq(n, df=3)
x1 &lt;- (ifelse(x &lt; 6, x, 6) - 2.348)/1.511

# x2 is normal (0, 1) truncated at +- 2 divided by 0.8796

x &lt;- rnorm(n)
x2 &lt;- ifelse(abs(x) &lt; 2 , x, 2) / 0.8796

# y is 1 if y* &gt; 0, 0 otherwise.

y &lt;- ifelse(x1 + x2 + rnorm(n) &gt; 0, 1, 0)

# Create the X matrix

X &lt;- cbind(x1, x2)

# Generate data-driven bandwidths (likelihood cross-validation). Note -
# this may take a few minutes depending on the speed of your computer...

bw &lt;- npcdensbw(xdat=X, ydat=factor(y))

# Next, create the evaluation data in order to generate a perspective
# plot

x1.seq &lt;- seq(min(x1), max(x1), length=50)
x2.seq &lt;- seq(min(x2), max(x2), length=50)
X.eval &lt;- expand.grid(x1=x1.seq,x2=x2.seq)

# Now evaluate the conditional probability for y=1 and for the
# evaluation Xs

fit &lt;- fitted(npcdens(exdat=X.eval, 
               eydat=factor(rep(1, nrow(X.eval))), 
               bws=bw))

# Finally, coerce the data into a matrix for plotting with persp()

fit.mat &lt;- matrix(fit, 50, 50)

# Generate a perspective plot similar to Figure 2 b of Klein and Spady
# (1993)

persp(x1.seq, 
      x2.seq, 
      fit.mat, 
      col="white", 
      ticktype="detailed", 
      expand=0.5, 
      axes=FALSE, 
      box=FALSE, 
      main="Estimated Nonparametric Probability Perspective", 
      theta=310, 
      phi=25)

## End(Not run) 
</code></pre>

<hr>
<h2 id='npcdensbw'>Kernel Conditional Density Bandwidth Selection with Mixed Data Types</h2><span id='topic+npcdensbw'></span><span id='topic+npcdensbw.NULL'></span><span id='topic+npcdensbw.conbandwidth'></span><span id='topic+npcdensbw.default'></span><span id='topic+npcdensbw.formula'></span>

<h3>Description</h3>

<p><code>npcdensbw</code> computes a <code>conbandwidth</code> object for
estimating the conditional density of a <code class="reqn">p+q</code>-variate kernel
density estimator defined over mixed continuous and discrete
(unordered, ordered) data using either the normal-reference
rule-of-thumb, likelihood cross-validation, or least-squares cross
validation using the method of Hall, Racine, and Li (2004). </p>


<h3>Usage</h3>

<pre><code class='language-R'>npcdensbw(...)

## S3 method for class 'formula'
npcdensbw(formula, data, subset, na.action, call, ...)

## S3 method for class 'NULL'
npcdensbw(xdat = stop("data 'xdat' missing"),
          ydat = stop("data 'ydat' missing"),
          bws, ...)

## S3 method for class 'conbandwidth'
npcdensbw(xdat = stop("data 'xdat' missing"),
          ydat = stop("data 'ydat' missing"),
          bws,
          bandwidth.compute = TRUE,
          nmulti,
          remin = TRUE,
          itmax = 10000,
          ftol = 1.490116e-07,
          tol = 1.490116e-04,
          small = 1.490116e-05,
          memfac = 500,
          lbc.dir = 0.5,
          dfc.dir = 3,
          cfac.dir = 2.5*(3.0-sqrt(5)),
          initc.dir = 1.0,
          lbd.dir = 0.1,
          hbd.dir = 1,
          dfac.dir = 0.25*(3.0-sqrt(5)),
          initd.dir = 1.0,
          lbc.init = 0.1,
          hbc.init = 2.0,
          cfac.init = 0.5,
          lbd.init = 0.1,
          hbd.init = 0.9,
          dfac.init = 0.375, 
          scale.init.categorical.sample = FALSE,
          ...)

## Default S3 method:
npcdensbw(xdat = stop("data 'xdat' missing"),
          ydat = stop("data 'ydat' missing"),
          bws,
          bandwidth.compute = TRUE,
          nmulti,
          remin,
          itmax,
          ftol,
          tol,
          small,
          memfac,
          lbc.dir,
          dfc.dir,
          cfac.dir,
          initc.dir,
          lbd.dir,
          hbd.dir,
          dfac.dir,
          initd.dir,
          lbc.init,
          hbc.init,
          cfac.init,
          lbd.init,
          hbd.init,
          dfac.init,
          scale.init.categorical.sample,
          bwmethod,
          bwscaling,
          bwtype,
          cxkertype,
          cxkerorder,
          cykertype,
          cykerorder,
          uxkertype,
          uykertype,
          oxkertype,
          oykertype,
          ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npcdensbw_+3A_formula">formula</code></td>
<td>

<p>a symbolic description of variables on which bandwidth selection is
to be performed. The details of constructing a formula are
described below.
</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_data">data</code></td>
<td>

<p>an optional data frame, list or environment (or object
coercible to a data frame by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code>) containing the variables
in the model. If not found in data, the variables are taken from
<code>environment(formula)</code>, typically the environment from which the
function is called.
</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations to be used in
the fitting process. 
</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_na.action">na.action</code></td>
<td>

<p>a function which indicates what should happen when the data contain
<code>NA</code>s. The default is set by the <code><a href="stats.html#topic+na.action">na.action</a></code> setting of options, and is
<code><a href="stats.html#topic+na.fail">na.fail</a></code> if that is unset. The (recommended) default is
<code><a href="stats.html#topic+na.omit">na.omit</a></code>.  
</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_call">call</code></td>
<td>

<p>the original function call. This is passed internally by
<code><a href="#topic+np">np</a></code> when a bandwidth search has been implied by a call to
another function. It is not recommended that the user set this.
</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_xdat">xdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of explanatory data on which bandwidth selection will
be performed. The data types may be continuous, discrete (unordered
and ordered factors), or some combination thereof.
</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_ydat">ydat</code></td>
<td>

<p>a <code class="reqn">q</code>-variate data frame of dependent data on which bandwidth selection will
be performed. The data types may be continuous, discrete (unordered
and ordered factors), or some combination thereof.
</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_bws">bws</code></td>
<td>

<p>a bandwidth specification. This can be set as a <code>conbandwidth</code>
object returned from a previous invocation, or as a <code class="reqn">p+q</code>-vector
of bandwidths, with each element <code class="reqn">i</code> up to <code class="reqn">i=q</code>
corresponding to the bandwidth for column <code class="reqn">i</code> in <code>ydat</code>,
and each element <code class="reqn">i</code> from <code class="reqn">i=q+1</code> to <code class="reqn">i=p+q</code>
corresponding to the bandwidth for column <code class="reqn">i-q</code> in
<code>xdat</code>. In either case, the bandwidth supplied will serve as a
starting point in the numerical search for optimal bandwidths. If
specified as a vector, then additional arguments will need to be
supplied as necessary to specify the bandwidth type, kernel types,
selection methods, and so on. This can be left unset.
</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to specify the bandwidth type,
kernel types, selection methods, and so on, detailed below.
</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_bwmethod">bwmethod</code></td>
<td>
<p> which method to use to select
bandwidths. <code>cv.ml</code> specifies likelihood cross-validation,
<code>cv.ls</code> specifies least-squares cross-validation, and
<code>normal-reference</code> just computes the &lsquo;rule-of-thumb&rsquo;
bandwidth <code class="reqn">h_j</code> using the standard formula <code class="reqn">h_j = 1.06
    \sigma_j n^{-1/(2P+l)}</code>,
where <code class="reqn">\sigma_j</code> is an adaptive measure of spread of
the <code class="reqn">j</code>th continuous variable defined as min(standard deviation,
mean absolute deviation/1.4826, interquartile range/1.349), <code class="reqn">n</code>
the number of observations, <code class="reqn">P</code> the order of the kernel, and
<code class="reqn">l</code> the number of continuous variables. Note that when there
exist factors and the normal-reference rule is used, there is zero
smoothing of the factors. Defaults to <code>cv.ml</code>.  </p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_bwscaling">bwscaling</code></td>
<td>
<p> a logical value that when set to <code>TRUE</code> the
supplied bandwidths are interpreted as &lsquo;scale factors&rsquo;
(<code class="reqn">c_j</code>), otherwise when the value is <code>FALSE</code> they are
interpreted as &lsquo;raw bandwidths&rsquo; (<code class="reqn">h_j</code> for continuous data
types, <code class="reqn">\lambda_j</code> for discrete data types). For
continuous data types, <code class="reqn">c_j</code> and <code class="reqn">h_j</code> are
related by the formula <code class="reqn">h_j = c_j \sigma_j n^{-1/(2P+l)}</code>, where <code class="reqn">\sigma_j</code> is an
adaptive measure of spread of continuous variable <code class="reqn">j</code> defined as
min(standard deviation, mean absolute deviation/1.4826,
interquartile range/1.349), <code class="reqn">n</code> the number of observations,
<code class="reqn">P</code> the order of the kernel, and <code class="reqn">l</code> the number of
continuous variables. For discrete data types, <code class="reqn">c_j</code> and
<code class="reqn">h_j</code> are related by the formula <code class="reqn">h_j =
    c_jn^{-2/(2P+l)}</code>, where here
<code class="reqn">j</code> denotes discrete variable <code class="reqn">j</code>.  Defaults to
<code>FALSE</code>.  </p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_bwtype">bwtype</code></td>
<td>

<p>character string used for the continuous variable bandwidth type,
specifying the type of bandwidth to compute and return in the
<code>conbandwidth</code> object. Defaults to <code>fixed</code>. Option
summary:<br />
<code>fixed</code>: compute fixed bandwidths <br />
<code>generalized_nn</code>: compute generalized nearest neighbors <br />
<code>adaptive_nn</code>: compute adaptive nearest neighbors
</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_bandwidth.compute">bandwidth.compute</code></td>
<td>

<p>a logical value which specifies whether to do a numerical search for
bandwidths or not. If set to <code>FALSE</code>, a <code>conbandwidth</code> object
will be returned with bandwidths set to those specified
in <code>bws</code>. Defaults to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_cxkertype">cxkertype</code></td>
<td>

<p>character string used to specify the continuous kernel type for
<code>xdat</code>.  Can be set as <code>gaussian</code>,
<code>epanechnikov</code>, or <code>uniform</code>. Defaults to <code>gaussian</code>.
</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_cxkerorder">cxkerorder</code></td>
<td>

<p>numeric value specifying kernel order for
<code>xdat</code> (one of
<code>(2,4,6,8)</code>). Kernel order specified along with a
<code>uniform</code> continuous kernel type will be ignored. Defaults to
<code>2</code>.
</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_cykertype">cykertype</code></td>
<td>

<p>character string used to specify the continuous kernel type for
<code>ydat</code>.
Can be set as <code>gaussian</code>, <code>epanechnikov</code>, or
<code>uniform</code>. Defaults to <code>gaussian</code>.
</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_cykerorder">cykerorder</code></td>
<td>

<p>numeric value specifying kernel order for
<code>ydat</code> (one of
<code>(2,4,6,8)</code>). Kernel order specified along with a
<code>uniform</code> continuous kernel type will be ignored. Defaults to
<code>2</code>.
</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_uxkertype">uxkertype</code></td>
<td>

<p>character string used to specify the unordered categorical
kernel type. Can be set as <code>aitchisonaitken</code> or
<code>liracine</code>. Defaults to <code>aitchisonaitken</code>.
</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_uykertype">uykertype</code></td>
<td>

<p>character string used to specify the unordered categorical
kernel type. Can be set as <code>aitchisonaitken</code> or <code>liracine</code>.
</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_oxkertype">oxkertype</code></td>
<td>

<p>character string used to specify the ordered categorical
kernel type. Can be set as <code>wangvanryzin</code> or
<code>liracine</code>. Defaults to <code>liracine</code>.
</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_oykertype">oykertype</code></td>
<td>

<p>character string used to specify the ordered categorical
kernel type. Can be set as <code>wangvanryzin</code> or <code>liracine</code>.
</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_nmulti">nmulti</code></td>
<td>

<p>integer number of times to restart the process of finding extrema of
the cross-validation function from different (random) initial points
</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_remin">remin</code></td>
<td>

<p>a logical value which when set as <code>TRUE</code> the search routine
restarts from located minima for a minor gain in accuracy. Defaults
to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_itmax">itmax</code></td>
<td>

<p>integer number of iterations before failure in the numerical
optimization routine. Defaults to <code>10000</code>.
</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_ftol">ftol</code></td>
<td>

<p>fractional tolerance on the value of the cross-validation function
evaluated at located minima (of order the machine precision or
perhaps slightly larger so as not to be diddled by
roundoff). Defaults to <code>1.490116e-07</code>
(1.0e+01*sqrt(.Machine$double.eps)).
</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_tol">tol</code></td>
<td>

<p>tolerance on the position of located minima of the cross-validation
function (tol should generally be no smaller than the square root of
your machine's floating point precision). Defaults to <code>
      1.490116e-04 (1.0e+04*sqrt(.Machine$double.eps))</code>.
</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_small">small</code></td>
<td>

<p>a small number used to bracket a minimum (it is hopeless to ask for
a bracketing interval of width less than sqrt(epsilon) times its
central value, a fractional width of only about 10-04 (single
precision) or 3x10-8 (double precision)). Defaults to <code>small
      = 1.490116e-05 (1.0e+03*sqrt(.Machine$double.eps))</code>.
</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_lbc.dir">lbc.dir</code>, <code id="npcdensbw_+3A_dfc.dir">dfc.dir</code>, <code id="npcdensbw_+3A_cfac.dir">cfac.dir</code>, <code id="npcdensbw_+3A_initc.dir">initc.dir</code></td>
<td>
<p> lower bound, chi-square
degrees of freedom, stretch factor, and initial non-random values
for direction set search for Powell's algorithm for <code>numeric</code>
variables. See Details</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_lbd.dir">lbd.dir</code>, <code id="npcdensbw_+3A_hbd.dir">hbd.dir</code>, <code id="npcdensbw_+3A_dfac.dir">dfac.dir</code>, <code id="npcdensbw_+3A_initd.dir">initd.dir</code></td>
<td>
<p> lower bound, upper bound,
stretch factor, and initial non-random values for direction set
search for Powell's algorithm for categorical variables. See
Details</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_lbc.init">lbc.init</code>, <code id="npcdensbw_+3A_hbc.init">hbc.init</code>, <code id="npcdensbw_+3A_cfac.init">cfac.init</code></td>
<td>
<p> lower bound, upper bound, and
non-random initial values for scale factors for <code>numeric</code>
variables for Powell's algorithm. See Details</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_lbd.init">lbd.init</code>, <code id="npcdensbw_+3A_hbd.init">hbd.init</code>, <code id="npcdensbw_+3A_dfac.init">dfac.init</code></td>
<td>
<p> lower bound, upper bound, and
non-random initial values for scale factors for categorical
variables for Powell's algorithm. See Details</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_scale.init.categorical.sample">scale.init.categorical.sample</code></td>
<td>
<p> a logical value that when set
to <code>TRUE</code> scales <code>lbd.dir</code>, <code>hbd.dir</code>,
<code>dfac.dir</code>, and <code>initd.dir</code> by <code class="reqn">n^{-2/(2P+l)}</code>,
<code class="reqn">n</code> the number of observations, <code class="reqn">P</code> the order of the
kernel, and <code class="reqn">l</code> the number of <code>numeric</code> variables. See
Details</p>
</td></tr>
<tr><td><code id="npcdensbw_+3A_memfac">memfac</code></td>
<td>

<p>The algorithm to compute the least-squares objective function uses
a block-based algorithm to eliminate or minimize redundant kernel
evaluations. Due to memory, hardware and software constraints, a
maximum block size must be imposed by the algorithm. This block size
is roughly equal to memfac*10^5 elements. Empirical tests on
modern hardware find that a memfac of 500 performs well. If
you experience out of memory errors, or strange behaviour for
large data sets (&gt;100k elements) setting memfac to a lower value may
fix the problem.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>npcdensbw</code> implements a variety of methods for choosing
bandwidths for multivariate distributions (<code class="reqn">p+q</code>-variate) defined
over a set of possibly continuous and/or discrete (unordered, ordered)
data. The approach is based on Li and Racine (2004) who employ
&lsquo;generalized product kernels&rsquo; that admit a mix of continuous
and discrete data types.
</p>
<p>The cross-validation methods employ multivariate numerical search
algorithms (direction set (Powell's) methods in multidimensions).
</p>
<p>Bandwidths can (and will) differ for each variable which is, of
course, desirable.
</p>
<p>Three classes of kernel estimators for the continuous data types are
available: fixed, adaptive nearest-neighbor, and generalized
nearest-neighbor. Adaptive nearest-neighbor bandwidths change with
each sample realization in the set, <code class="reqn">x_i</code>, when estimating
the density at the point <code class="reqn">x</code>. Generalized nearest-neighbor
bandwidths change with the point at which the density is estimated,
<code class="reqn">x</code>. Fixed bandwidths are constant over the support of <code class="reqn">x</code>.
</p>
<p><code>npcdensbw</code> may be invoked <em>either</em> with a formula-like
symbolic 
description of variables on which bandwidth selection is to be
performed <em>or</em> through a simpler interface whereby data is passed
directly to the function via the <code>xdat</code> and <code>ydat</code>
parameters. Use of these two interfaces is <b>mutually exclusive</b>.
</p>
<p>Data contained in the data frames <code>xdat</code> and <code>ydat</code> may be a
mix of continuous (default), unordered discrete (to be specified in
the data frames using <code><a href="base.html#topic+factor">factor</a></code>), and ordered discrete (to be
specified in the data frames using <code><a href="base.html#topic+ordered">ordered</a></code>). Data can be
entered in an arbitrary order and data types will be detected
automatically by the routine (see <code><a href="#topic+np">np</a></code> for details).
</p>
<p>Data for which bandwidths are to be estimated may be specified
symbolically. A typical description has the form <code>dependent data
  ~ explanatory data</code>,
where <code>dependent data</code> and <code>explanatory data</code> are both
series of variables specified by name, separated by 
the separation character '+'. For example, <code> y1 + y2 ~ x1 + x2 </code>
specifies that the bandwidths for the joint distribution of variables
<code>y1</code> and <code>y2</code> conditioned on <code>x1</code> and <code>x2</code> are to
be estimated. See below for further examples. 
</p>
<p>A variety of kernels may be specified by the user. Kernels implemented
for continuous data types include the second, fourth, sixth, and eighth
order Gaussian and Epanechnikov kernels, and the uniform
kernel. Unordered discrete data types use a variation on Aitchison and
Aitken's (1976) kernel, while ordered data types use a variation of the
Wang and van Ryzin (1981) kernel.
</p>
<p>The optimizer invoked for search is Powell's conjugate direction
method which requires the setting of (non-random) initial values and
search directions for bandwidths, and, when restarting, random values
for successive invocations. Bandwidths for <code>numeric</code> variables
are scaled by robust measures of spread, the sample size, and the
number of <code>numeric</code> variables where appropriate. Two sets of
parameters for bandwidths for <code>numeric</code> can be modified, those
for initial values for the parameters themselves, and those for the
directions taken (Powell's algorithm does not involve explicit
computation of the function's gradient). The default values are set by
considering search performance for a variety of difficult test cases
and simulated cases. We highly recommend restarting search a large
number of times to avoid the presence of local minima (achieved by
modifying <code>nmulti</code>). Further refinement for difficult cases can
be achieved by modifying these sets of parameters. However, these
parameters are intended more for the authors of the package to enable
&lsquo;tuning&rsquo; for various methods rather than for the user themselves.
</p>


<h3>Value</h3>

<p><code>npcdensbw</code> returns a <code>conbandwidth</code> object, with the
following components:
</p>
<table>
<tr><td><code>xbw</code></td>
<td>
<p> bandwidth(s), scale factor(s) or nearest neighbours for the
explanatory data, <code>xdat</code> </p>
</td></tr>
<tr><td><code>ybw</code></td>
<td>
<p> bandwidth(s), scale factor(s) or nearest neighbours for the
dependent data, <code>ydat</code> </p>
</td></tr>
<tr><td><code>fval</code></td>
<td>
<p> objective function value at minimum </p>
</td></tr>
</table>
<p>if <code>bwtype</code> is set to <code>fixed</code>, an object containing
bandwidths (or scale factors if <code>bwscaling = TRUE</code>) is
returned. If it is set to <code>generalized_nn</code> or <code>adaptive_nn</code>,
then instead the <code class="reqn">k</code>th nearest neighbors are returned for the
continuous variables while the discrete kernel bandwidths are returned
for the discrete variables.
</p>
<p>The functions <code><a href="stats.html#topic+predict">predict</a></code>, <code><a href="base.html#topic+summary">summary</a></code> and <code><a href="base.html#topic+plot">plot</a></code> support
objects of type <code>conbandwidth</code>.
</p>


<h3>Usage Issues</h3>

<p>If you are using data of mixed types, then it is advisable to use the
<code><a href="base.html#topic+data.frame">data.frame</a></code> function to construct your input data and not
<code><a href="base.html#topic+cbind">cbind</a></code>, since <code><a href="base.html#topic+cbind">cbind</a></code> will typically not work as
intended on mixed data types and will coerce the data to the same
type.
</p>
<p>Caution: multivariate data-driven bandwidth selection methods are, by
their nature, <em>computationally intensive</em>. Virtually all methods
require dropping the <code class="reqn">i</code>th observation from the data set, computing an
object, repeating this for all observations in the sample, then
averaging each of these leave-one-out estimates for a <em>given</em>
value of the bandwidth vector, and only then repeating this a large
number of times in order to conduct multivariate numerical
minimization/maximization. Furthermore, due to the potential for local
minima/maxima, <em>restarting this procedure a large number of times may
often be necessary</em>. This can be frustrating for users possessing
large datasets. For exploratory purposes, you may wish to override the
default search tolerances, say, setting ftol=.01 and tol=.01 and
conduct multistarting (the default is to restart min(5, ncol(xdat,ydat))
times) as is done for a number of examples. Once the procedure
terminates, you can restart search with default tolerances using those
bandwidths obtained from the less rigorous search (i.e., set
<code>bws=bw</code> on subsequent calls to this routine where <code>bw</code> is
the initial bandwidth object).  A version of this package using the
<code>Rmpi</code> wrapper is under development that allows one to deploy
this software in a clustered computing environment to facilitate
computation involving large datasets.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine
<a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Aitchison, J. and C.G.G. Aitken (1976), &ldquo;Multivariate binary
discrimination by the kernel method,&rdquo; Biometrika, 63, 413-420.
</p>
<p>Hall, P. and J.S. Racine and Q. Li (2004), &ldquo;Cross-validation and
the estimation of conditional probability densities,&rdquo; Journal of the
American Statistical Association, 99, 1015-1026.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics: Theory
and Practice,</em> Princeton University Press.
</p>
<p>Pagan, A. and A. Ullah (1999), <em>Nonparametric Econometrics,</em> Cambridge
University Press. 
</p>
<p>Scott, D.W. (1992), <em>Multivariate Density Estimation. Theory,
Practice and Visualization,</em> New York: Wiley.
</p>
<p>Silverman, B.W. (1986), <em>Density Estimation,</em> London: Chapman and
Hall.
</p>
<p>Wang, M.C. and J. van Ryzin (1981), &ldquo;A class of smooth estimators
for discrete distributions,&rdquo;  Biometrika, 68, 301-309.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+bw.nrd">bw.nrd</a></code>, <code><a href="stats.html#topic+bw.SJ">bw.SJ</a></code>, <code><a href="graphics.html#topic+hist">hist</a></code>,
<code><a href="#topic+npudens">npudens</a></code>, <code><a href="#topic+npudist">npudist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# EXAMPLE 1 (INTERFACE=FORMULA): For this example, we compute the
# likelihood cross-validated bandwidths (default) using a second-order
# Gaussian kernel (default). Note - this may take a minute or two
# depending on the speed of your computer.

data("Italy")
attach(Italy)

bw &lt;- npcdensbw(formula=gdp~ordered(year))

# The object bw can be used for further estimation using
# npcdens(), plotting using plot() etc. Entering the name of
# the object provides useful summary information, and names() will also
# provide useful information.

summary(bw)

# Note - see the example for npudensbw() for multiple illustrations
# of how to change the kernel function, kernel order, and so forth.

detach(Italy)

# EXAMPLE 1 (INTERFACE=DATA FRAME): For this example, we compute the
# likelihood cross-validated bandwidths (default) using a second-order
# Gaussian kernel (default). Note - this may take a minute or two
# depending on the speed of your computer.

data("Italy")
attach(Italy)

bw &lt;- npcdensbw(xdat=ordered(year), ydat=gdp)

# The object bw can be used for further estimation using
# npcdens(), plotting using plot() etc. Entering the name of
# the object provides useful summary information, and names() will also
# provide useful information.

summary(bw)

# Note - see the example for npudensbw() for multiple illustrations
# of how to change the kernel function, kernel order, and so forth.

detach(Italy)

## End(Not run) 
</code></pre>

<hr>
<h2 id='npcdist'>Kernel Conditional Distribution Estimation with Mixed Data Types</h2><span id='topic+npcdist'></span><span id='topic+npcdist.call'></span><span id='topic+npcdist.condbandwidth'></span><span id='topic+npcdist.default'></span><span id='topic+npcdist.formula'></span>

<h3>Description</h3>

<p><code>npcdist</code> computes kernel cumulative conditional distribution
estimates on <code class="reqn">p+q</code>-variate evaluation data, given a set of
training data (both explanatory and dependent) and a bandwidth
specification (a <code>condbandwidth</code> object or a bandwidth vector,
bandwidth type, and kernel type) using the method of Li and Racine
(2008) and Li, Lin, and Racine (2013).  The data may be continuous,
discrete (unordered and ordered factors), or some combination thereof.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npcdist(bws, ...)

## S3 method for class 'formula'
npcdist(bws, data = NULL, newdata = NULL, ...)

## S3 method for class 'call'
npcdist(bws, ...)

## S3 method for class 'condbandwidth'
npcdist(bws,
        txdat = stop("invoked without training data 'txdat'"),
        tydat = stop("invoked without training data 'tydat'"),
        exdat,
        eydat,
        gradients = FALSE,
        ...)

## Default S3 method:
npcdist(bws, txdat, tydat, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npcdist_+3A_bws">bws</code></td>
<td>

<p>a bandwidth specification. This can be set as a <code>condbandwidth</code>
object returned from a previous invocation of
<code><a href="#topic+npcdistbw">npcdistbw</a></code>, or as a <code class="reqn">p+q</code>-vector of bandwidths,
with each element <code class="reqn">i</code> up to <code class="reqn">i=q</code> corresponding to the
bandwidth for column <code class="reqn">i</code> in <code>tydat</code>, and each element
<code class="reqn">i</code> from <code class="reqn">i=q+1</code> to <code class="reqn">i=p+q</code> corresponding to the
bandwidth for column <code class="reqn">i-q</code> in <code>txdat</code>. If specified as a
vector, then additional arguments will need to be supplied as
necessary to specify the bandwidth type, kernel types, training
data, and so on.
</p>
</td></tr>
<tr><td><code id="npcdist_+3A_gradients">gradients</code></td>
<td>

<p>a logical value specifying whether to return estimates of the
gradients at the evaluation points. Defaults to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="npcdist_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to specify the bandwidth type, kernel
types, and so on.  This is necessary if you specify bws as a
<code class="reqn">p+q</code>-vector and not a <code>condbandwidth</code> object, and you do
not desire the default behaviours. To do this, you may specify any
of <code>bwmethod</code>, <code>bwscaling</code>, <code>bwtype</code>,
<code>cxkertype</code>, <code>cxkerorder</code>, <code>cykertype</code>,
<code>cykerorder</code>, <code>uxkertype</code>, <code>oxkertype</code>,
<code>oykertype</code>, as described in <code><a href="#topic+npcdistbw">npcdistbw</a></code>.
</p>
</td></tr>
<tr><td><code id="npcdist_+3A_data">data</code></td>
<td>

<p>an optional data frame, list or environment (or object coercible to
a data frame by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code>) containing the
variables in the model. If not found in data, the variables are
taken from <code>environment(bws)</code>, typically the environment from
which <code><a href="#topic+npcdistbw">npcdistbw</a></code> was called.
</p>
</td></tr>
<tr><td><code id="npcdist_+3A_newdata">newdata</code></td>
<td>

<p>An optional data frame in which to look for evaluation data. If
omitted, the training data are used.  
</p>
</td></tr>
<tr><td><code id="npcdist_+3A_txdat">txdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of sample realizations of explanatory
data (training data). Defaults to the training data used to
compute the bandwidth object.
</p>
</td></tr>
<tr><td><code id="npcdist_+3A_tydat">tydat</code></td>
<td>

<p>a <code class="reqn">q</code>-variate data frame of sample realizations of dependent
data (training data). Defaults to the training data used to
compute the bandwidth object.
</p>
</td></tr>
<tr><td><code id="npcdist_+3A_exdat">exdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of explanatory data on
which cumulative conditional distributions will be evaluated. By
default, evaluation takes place on the data provided by
<code>txdat</code>.
</p>
</td></tr>
<tr><td><code id="npcdist_+3A_eydat">eydat</code></td>
<td>

<p>a <code class="reqn">q</code>-variate data frame of dependent data on which
cumulative conditional distributions will be evaluated. By default,
evaluation takes place on the data provided by <code>tydat</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>npcdist</code> implements a variety of methods for estimating
multivariate conditional cumulative distributions (<code class="reqn">p+q</code>-variate)
defined over a set of possibly continuous and/or discrete (unordered,
ordered) data. The approach is based on Li and Racine (2004) who
employ &lsquo;generalized product kernels&rsquo; that admit a mix of
continuous and discrete data types.
</p>
<p>Three classes of kernel estimators for the continuous data types are
available: fixed, adaptive nearest-neighbor, and generalized
nearest-neighbor. Adaptive nearest-neighbor bandwidths change with
each sample realization in the set, <code class="reqn">x_i</code>, when estimating
the cumulative conditional distribution at the point
<code class="reqn">x</code>. Generalized nearest-neighbor bandwidths change with the point
at which the cumulative conditional distribution is estimated,
<code class="reqn">x</code>. Fixed bandwidths are constant over the support of <code class="reqn">x</code>.
</p>
<p>Training and evaluation input data  may be a
mix of continuous (default), unordered discrete (to be specified in
the data frames using <code><a href="base.html#topic+factor">factor</a></code>), and ordered discrete (to be
specified in the data frames using <code><a href="base.html#topic+ordered">ordered</a></code>). Data can be
entered in an arbitrary order and data types will be detected
automatically by the routine (see <code><a href="#topic+np">np</a></code> for details).
</p>
<p>A variety of kernels may be specified by the user. Kernels implemented
for continuous data types include the second, fourth, sixth, and eighth
order Gaussian and Epanechnikov kernels, and the uniform
kernel. Unordered discrete data types use a variation on Aitchison and
Aitken's (1976) kernel, while ordered data types use a variation of the
Wang and van Ryzin (1981) kernel.
</p>


<h3>Value</h3>

<p><code>npcdist</code> returns a <code>condistribution</code> object. The generic
accessor functions <code><a href="stats.html#topic+fitted">fitted</a></code>, <code><a href="#topic+se">se</a></code>, and
<code><a href="#topic+gradients">gradients</a></code>, extract estimated values, asymptotic standard
errors on estimates, and gradients, respectively, from
the returned object. Furthermore, the functions <code><a href="stats.html#topic+predict">predict</a></code>,
<code><a href="base.html#topic+summary">summary</a></code>
and <code><a href="base.html#topic+plot">plot</a></code> support objects of both classes. The returned objects
have the following components:
</p>
<table>
<tr><td><code>xbw</code></td>
<td>
<p> bandwidth(s), scale factor(s) or nearest neighbours for the
explanatory data, <code>txdat</code> </p>
</td></tr>
<tr><td><code>ybw</code></td>
<td>
<p> bandwidth(s), scale factor(s) or nearest neighbours for the
dependent data, <code>tydat</code> </p>
</td></tr>
<tr><td><code>xeval</code></td>
<td>
<p> the evaluation points of the explanatory data </p>
</td></tr>
<tr><td><code>yeval</code></td>
<td>
<p> the evaluation points of the dependent data </p>
</td></tr>
<tr><td><code>condist</code></td>
<td>
<p> estimates of the conditional cumulative
distribution at the evaluation points </p>
</td></tr>
<tr><td><code>conderr</code></td>
<td>
<p> standard errors of the cumulative conditional distribution
estimates </p>
</td></tr>
<tr><td><code>congrad</code></td>
<td>
<p> if invoked with <code>gradients = TRUE</code>, estimates of
the gradients at the evaluation points </p>
</td></tr>
<tr><td><code>congerr</code></td>
<td>
<p> if invoked with <code>gradients = TRUE</code>, standard
errors of the gradients at the evaluation points </p>
</td></tr>
<tr><td><code>log_likelihood</code></td>
<td>
<p> log likelihood of the cumulative conditional distribution estimate </p>
</td></tr>
</table>


<h3>Usage Issues</h3>

<p>If you are using data of mixed types, then it is advisable to use the
<code><a href="base.html#topic+data.frame">data.frame</a></code> function to construct your input data and not
<code><a href="base.html#topic+cbind">cbind</a></code>, since <code><a href="base.html#topic+cbind">cbind</a></code> will typically not work as
intended on mixed data types and will coerce the data to the same
type.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine
<a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>    
</p>


<h3>References</h3>

<p>Aitchison, J. and C.G.G. Aitken (1976), &ldquo;Multivariate binary
discrimination by the kernel method,&rdquo; Biometrika, 63, 413-420.
</p>
<p>Hall, P. and J.S. Racine and Q. Li (2004), &ldquo;Cross-validation and the
estimation of conditional probability densities,&rdquo; Journal of the
American Statistical Association, 99, 1015-1026.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics: Theory
and Practice,</em> Princeton University Press.
</p>
<p>Li, Q. and J.S. Racine (2008), &ldquo;Nonparametric estimation of
conditional CDF and quantile functions with mixed categorical and
continuous data,&rdquo; Journal of Business and Economic Statistics, 26,
423-434.
</p>
<p>Li, Q. and J. Lin and J.S. Racine (2013), &ldquo;Optimal bandwidth
selection for nonparametric conditional distribution and quantile
functions&rdquo;, Journal of Business and Economic Statistics, 31, 57-65.
</p>
<p>Pagan, A. and A. Ullah (1999), <em>Nonparametric Econometrics,</em> Cambridge
University Press. 
</p>
<p>Scott, D.W. (1992), <em>Multivariate Density Estimation. Theory,
Practice and Visualization,</em> New York: Wiley.
</p>
<p>Silverman, B.W. (1986), <em>Density Estimation,</em> London: Chapman and
Hall.
</p>
<p>Wang, M.C. and J. van Ryzin (1981), &ldquo;A class of smooth estimators
for discrete distributions,&rdquo;  Biometrika, 68, 301-309.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npudens">npudens</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# EXAMPLE 1 (INTERFACE=FORMULA): For this example, we load Giovanni
# Baiocchi's Italian GDP panel (see Italy for details), and compute the
# cross-validated bandwidths (default) using a second-order Gaussian
# kernel (default). Note - this may take a minute or two depending on
# the speed of your computer.

data("Italy")
attach(Italy)

# First, compute the bandwidths.

bw &lt;- npcdistbw(formula=gdp~ordered(year))

# Next, compute the condistribution object...

Fhat &lt;- npcdist(bws=bw)

# The object Fhat now contains results such as the estimated cumulative
# conditional distribution function (Fhat$condist) and so on...

summary(Fhat)

# Call the plot() function to visualize the results (&lt;ctrl&gt;-C will
# interrupt on *NIX systems, &lt;esc&gt; will interrupt on MS Windows
# systems).

plot(bw)

detach(Italy)

# EXAMPLE 1 (INTERFACE=DATA FRAME): For this example, we load Giovanni
# Baiocchi's Italian GDP panel (see Italy for details), and compute the
# cross-validated bandwidths (default) using a second-order Gaussian
# kernel (default). Note - this may take a minute or two depending on
# the speed of your computer.

data("Italy")
attach(Italy)

# First, compute the bandwidths.

# Note - we cast `X' and `y' as data frames so that plot() can
# automatically grab names (this looks like overkill, but in
# multivariate settings you would do this anyway, so may as well get in
# the habit).

X &lt;- data.frame(year=ordered(year))
y &lt;- data.frame(gdp)

bw &lt;- npcdistbw(xdat=X, ydat=y)

# Next, compute the condistribution object...

Fhat &lt;- npcdist(bws=bw)

# The object Fhat now contains results such as the estimated cumulative
# conditional distribution function (Fhat$condist) and so on...

summary(Fhat)

# Call the plot() function to visualize the results (&lt;ctrl&gt;-C will
# interrupt on *NIX systems, &lt;esc&gt; will interrupt on MS Windows systems).

plot(bw)

detach(Italy)

# EXAMPLE 2 (INTERFACE=FORMULA): For this example, we load the old
# faithful geyser data from the R `datasets' library and compute the
# conditional distribution function.

library("datasets")
data("faithful")
attach(faithful)

# Note - this may take a few minutes depending on the speed of your
# computer...

bw &lt;- npcdistbw(formula=eruptions~waiting)

summary(bw)

# Plot the conditional cumulative distribution function (&lt;ctrl&gt;-C will
# interrupt on *NIX systems, &lt;esc&gt; will interrupt on MS Windows
# systems).

plot(bw)

detach(faithful)

# EXAMPLE 2 (INTERFACE=DATA FRAME): For this example, we load the old
# faithful geyser data from the R `datasets' library and compute the
# cumulative conditional distribution function.

library("datasets")
data("faithful")
attach(faithful)

# Note - this may take a few minutes depending on the speed of your
# computer...

# Note - we cast `X' and `y' as data frames so that plot() can
# automatically grab names (this looks like overkill, but in
# multivariate settings you would do this anyway, so may as well get in
# the habit).

X &lt;- data.frame(waiting)
y &lt;- data.frame(eruptions)

bw &lt;- npcdistbw(xdat=X, ydat=y)

summary(bw)

# Plot the conditional cumulative distribution function (&lt;ctrl&gt;-C will
# interrupt on *NIX systems, &lt;esc&gt; will interrupt on MS Windows systems)

plot(bw)

detach(faithful)


## End(Not run) 
</code></pre>

<hr>
<h2 id='npcdistbw'>Kernel Conditional Distribution Bandwidth Selection with Mixed
Data Types</h2><span id='topic+npcdistbw'></span><span id='topic+npcdistbw.NULL'></span><span id='topic+npcdistbw.condbandwidth'></span><span id='topic+npcdistbw.default'></span><span id='topic+npcdistbw.formula'></span>

<h3>Description</h3>

 
<p><code>npcdistbw</code> computes a <code>condbandwidth</code> object for estimating
a <code class="reqn">p+q</code>-variate kernel conditional cumulative distribution
estimator defined over mixed continuous and discrete (unordered
<code>xdat</code>, ordered <code>xdat</code> and <code>ydat</code>) data using either
the normal-reference rule-of-thumb or least-squares cross validation
method of Li and Racine (2008) and Li, Lin and Racine
(2013).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npcdistbw(...)

## S3 method for class 'formula'
npcdistbw(formula, data, subset, na.action, call, gdata = NULL,...)

## S3 method for class 'NULL'
npcdistbw(xdat = stop("data 'xdat' missing"),
          ydat = stop("data 'ydat' missing"),
          bws, ...)

## S3 method for class 'condbandwidth'
npcdistbw(xdat = stop("data 'xdat' missing"),
          ydat = stop("data 'ydat' missing"),
          gydat = NULL,
          bws,
          bandwidth.compute = TRUE,
          nmulti,
          remin = TRUE,
          itmax = 10000,
          do.full.integral = FALSE,
          ngrid = 100,
          ftol = 1.490116e-07,
          tol = 1.490116e-04,
          small = 1.490116e-05,
          memfac = 500.0,
          lbc.dir = 0.5,
          dfc.dir = 3,
          cfac.dir = 2.5*(3.0-sqrt(5)),
          initc.dir = 1.0,
          lbd.dir = 0.1,
          hbd.dir = 1,
          dfac.dir = 0.25*(3.0-sqrt(5)),
          initd.dir = 1.0,
          lbc.init = 0.1,
          hbc.init = 2.0,
          cfac.init = 0.5,
          lbd.init = 0.1,
          hbd.init = 0.9,
          dfac.init = 0.375, 
          scale.init.categorical.sample = FALSE,
          ...)

## Default S3 method:
npcdistbw(xdat = stop("data 'xdat' missing"),
          ydat = stop("data 'ydat' missing"),
          gydat,
          bws,
          bandwidth.compute = TRUE,
          nmulti,
          remin,
          itmax,
          do.full.integral,
          ngrid,
          ftol,
          tol,
          small,
          memfac,
          lbc.dir,
          dfc.dir,
          cfac.dir,
          initc.dir,
          lbd.dir,
          hbd.dir,
          dfac.dir,
          initd.dir,
          lbc.init,
          hbc.init,
          cfac.init,
          lbd.init,
          hbd.init,
          dfac.init,
          scale.init.categorical.sample,
          bwmethod,
          bwscaling,
          bwtype,
          cxkertype,
          cxkerorder,
          cykertype,
          cykerorder,
          uxkertype,
          oxkertype,
          oykertype,
          ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npcdistbw_+3A_formula">formula</code></td>
<td>

<p>a symbolic description of variables on which bandwidth selection is
to be performed. The details of constructing a formula are
described below.
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_data">data</code></td>
<td>

<p>an optional data frame, list or environment (or object
coercible to a data frame by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code>) containing the variables
in the model. If not found in data, the variables are taken from
<code>environment(formula)</code>, typically the environment from which the
function is called.
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations to be used in
the fitting process. 
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_na.action">na.action</code></td>
<td>

<p>a function which indicates what should happen when the data contain
<code>NA</code>s. The default is set by the <code><a href="stats.html#topic+na.action">na.action</a></code> setting of options, and is
<code><a href="stats.html#topic+na.fail">na.fail</a></code> if that is unset. The (recommended) default is
<code><a href="stats.html#topic+na.omit">na.omit</a></code>.  
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_call">call</code></td>
<td>

<p>the original function call. This is passed internally by
<code><a href="#topic+np">np</a></code> when a bandwidth search has been implied by a call to
another function. It is not recommended that the user set this.
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_gdata">gdata</code></td>
<td>

<p>a grid of data on which the indicator function for
least-squares cross-validation is to be computed (can be the sample
or a grid of quantiles).
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_xdat">xdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of explanatory data on which bandwidth selection will
be performed. The data types may be continuous, discrete (unordered
and ordered factors), or some combination thereof.
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_ydat">ydat</code></td>
<td>

<p>a <code class="reqn">q</code>-variate data frame of dependent data on which bandwidth
selection will be performed. The data types may be continuous,
discrete (ordered factors), or some combination thereof.
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_gydat">gydat</code></td>
<td>

<p>a grid of data on which the indicator function for
least-squares cross-validation is to be computed (can be the sample
or a grid of quantiles for <code>ydat</code>).
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_bws">bws</code></td>
<td>

<p>a bandwidth specification. This can be set as a <code>condbandwidth</code>
object returned from a previous invocation, or as a <code class="reqn">p+q</code>-vector
of bandwidths, with each element <code class="reqn">i</code> up to <code class="reqn">i=q</code>
corresponding to the bandwidth for column <code class="reqn">i</code> in <code>ydat</code>,
and each element <code class="reqn">i</code> from <code class="reqn">i=q+1</code> to <code class="reqn">i=p+q</code>
corresponding to the bandwidth for column <code class="reqn">i-q</code> in
<code>xdat</code>. In either case, the bandwidth supplied will serve as a
starting point in the numerical search for optimal bandwidths. If
specified as a vector, then additional arguments will need to be
supplied as necessary to specify the bandwidth type, kernel types,
selection methods, and so on. This can be left unset.
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to specify the bandwidth type,
kernel types, selection methods, and so on, detailed below.
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_bwmethod">bwmethod</code></td>
<td>
<p> which method to use to select bandwidths.
<code>cv.ls</code> specifies least-squares cross-validation (Li, Lin and
Racine (2013), and <code>normal-reference</code> just computes the
&lsquo;rule-of-thumb&rsquo; bandwidth <code class="reqn">h_j</code> using the standard
formula <code class="reqn">h_j = 1.06 \sigma_j n^{-1/(2P+l)}</code>, where <code class="reqn">\sigma_j</code> is
an adaptive measure of spread of the <code class="reqn">j</code>th continuous variable
defined as min(standard deviation, mean absolute deviation/1.4826,
interquartile range/1.349), <code class="reqn">n</code> the number of observations,
<code class="reqn">P</code> the order of the kernel, and <code class="reqn">l</code> the number of
continuous variables. Note that when there exist factors and the
normal-reference rule is used, there is zero smoothing of the
factors. Defaults to <code>cv.ls</code>.  </p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_bwscaling">bwscaling</code></td>
<td>
<p> a logical value that when set to <code>TRUE</code> the
supplied bandwidths are interpreted as &lsquo;scale factors&rsquo;
(<code class="reqn">c_j</code>), otherwise when the value is <code>FALSE</code> they are
interpreted as &lsquo;raw bandwidths&rsquo; (<code class="reqn">h_j</code> for continuous data
types, <code class="reqn">\lambda_j</code> for discrete data types). For
continuous data types, <code class="reqn">c_j</code> and <code class="reqn">h_j</code> are
related by the formula <code class="reqn">h_j = c_j \sigma_j n^{-1/(2P+l)}</code>, where <code class="reqn">\sigma_j</code> is an
adaptive measure of spread of continuous variable <code class="reqn">j</code> defined as
min(standard deviation, mean absolute deviation/1.4826, interquartile
range/1.349), <code class="reqn">n</code> the number of observations, <code class="reqn">P</code> the
order of the kernel, and <code class="reqn">l</code> the number of continuous
variables. For discrete data types, <code class="reqn">c_j</code> and
<code class="reqn">h_j</code> are related by the formula <code class="reqn">h_j =
    c_jn^{-2/(2P+l)}</code>, where here
<code class="reqn">j</code> denotes discrete variable <code class="reqn">j</code>.  Defaults to
<code>FALSE</code>.  </p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_bwtype">bwtype</code></td>
<td>

<p>character string used for the continuous variable bandwidth type,
specifying the type of bandwidth to compute and return in the
<code>condbandwidth</code> object. Defaults to <code>fixed</code>. Option
summary:<br />
<code>fixed</code>: compute fixed bandwidths <br />
<code>generalized_nn</code>: compute generalized nearest neighbors <br />
<code>adaptive_nn</code>: compute adaptive nearest neighbors
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_bandwidth.compute">bandwidth.compute</code></td>
<td>

<p>a logical value which specifies whether to do a numerical search for
bandwidths or not. If set to <code>FALSE</code>, a <code>condbandwidth</code> object
will be returned with bandwidths set to those specified
in <code>bws</code>. Defaults to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_cxkertype">cxkertype</code></td>
<td>

<p>character string used to specify the continuous kernel type for
<code>xdat</code>.  Can be set as <code>gaussian</code>,
<code>epanechnikov</code>, or <code>uniform</code>. Defaults to <code>gaussian</code>.
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_cxkerorder">cxkerorder</code></td>
<td>

<p>numeric value specifying kernel order for
<code>xdat</code> (one of
<code>(2,4,6,8)</code>). Kernel order specified along with a
<code>uniform</code> continuous kernel type will be ignored. Defaults to
<code>2</code>.
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_cykertype">cykertype</code></td>
<td>

<p>character string used to specify the continuous kernel type for
<code>ydat</code>.
Can be set as <code>gaussian</code>, <code>epanechnikov</code>, or
<code>uniform</code>. Defaults to <code>gaussian</code>.
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_cykerorder">cykerorder</code></td>
<td>

<p>numeric value specifying kernel order for
<code>ydat</code> (one of
<code>(2,4,6,8)</code>). Kernel order specified along with a
<code>uniform</code> continuous kernel type will be ignored. Defaults to
<code>2</code>.
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_uxkertype">uxkertype</code></td>
<td>

<p>character string used to specify the unordered categorical
kernel type. Can be set as <code>aitchisonaitken</code> or
<code>liracine</code>. Defaults to <code>aitchisonaitken</code>.
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_oxkertype">oxkertype</code></td>
<td>

<p>character string used to specify the ordered categorical
kernel type. Can be set as <code>wangvanryzin</code> or
<code>liracine</code>. Defaults to <code>liracine</code>.
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_oykertype">oykertype</code></td>
<td>

<p>character string used to specify the ordered categorical
kernel type. Can be set as <code>wangvanryzin</code> or <code>liracine</code>.
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_nmulti">nmulti</code></td>
<td>

<p>integer number of times to restart the process of finding extrema of
the cross-validation function from different (random) initial points
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_remin">remin</code></td>
<td>

<p>a logical value which when set as <code>TRUE</code> the search routine
restarts from located minima for a minor gain in accuracy. Defaults
to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_itmax">itmax</code></td>
<td>

<p>integer number of iterations before failure in the numerical
optimization routine. Defaults to <code>10000</code>.
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_do.full.integral">do.full.integral</code></td>
<td>

<p>a logical value which when set as <code>TRUE</code> evaluates the
moment-based integral on the entire sample.
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_ngrid">ngrid</code></td>
<td>

<p>integer number of grid points to use when computing the moment-based
integral. Defaults to <code>100</code>.
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_ftol">ftol</code></td>
<td>

<p>fractional tolerance on the value of the cross-validation function
evaluated at located minima (of order the machine precision or
perhaps slightly larger so as not to be diddled by
roundoff). Defaults to <code>1.490116e-07</code>
(1.0e+01*sqrt(.Machine$double.eps)).
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_tol">tol</code></td>
<td>

<p>tolerance on the position of located minima of the cross-validation
function (tol should generally be no smaller than the square root of
your machine's floating point precision). Defaults to <code>
      1.490116e-04 (1.0e+04*sqrt(.Machine$double.eps))</code>.
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_small">small</code></td>
<td>

<p>a small number used to bracket a minimum (it is hopeless to ask for
a bracketing interval of width less than sqrt(epsilon) times its
central value, a fractional width of only about 10-04 (single
precision) or 3x10-8 (double precision)). Defaults to <code>small
      = 1.490116e-05 (1.0e+03*sqrt(.Machine$double.eps))</code>.
</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_lbc.dir">lbc.dir</code>, <code id="npcdistbw_+3A_dfc.dir">dfc.dir</code>, <code id="npcdistbw_+3A_cfac.dir">cfac.dir</code>, <code id="npcdistbw_+3A_initc.dir">initc.dir</code></td>
<td>
<p> lower bound, chi-square
degrees of freedom, stretch factor, and initial non-random values
for direction set search for Powell's algorithm for <code>numeric</code>
variables. See Details</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_lbd.dir">lbd.dir</code>, <code id="npcdistbw_+3A_hbd.dir">hbd.dir</code>, <code id="npcdistbw_+3A_dfac.dir">dfac.dir</code>, <code id="npcdistbw_+3A_initd.dir">initd.dir</code></td>
<td>
<p> lower bound, upper bound,
stretch factor, and initial non-random values for direction set
search for Powell's algorithm for categorical variables. See
Details</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_lbc.init">lbc.init</code>, <code id="npcdistbw_+3A_hbc.init">hbc.init</code>, <code id="npcdistbw_+3A_cfac.init">cfac.init</code></td>
<td>
<p> lower bound, upper bound, and
non-random initial values for scale factors for <code>numeric</code>
variables for Powell's algorithm. See Details</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_lbd.init">lbd.init</code>, <code id="npcdistbw_+3A_hbd.init">hbd.init</code>, <code id="npcdistbw_+3A_dfac.init">dfac.init</code></td>
<td>
<p> lower bound, upper bound, and
non-random initial values for scale factors for categorical
variables for Powell's algorithm. See Details</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_scale.init.categorical.sample">scale.init.categorical.sample</code></td>
<td>
<p> a logical value that when set
to <code>TRUE</code> scales <code>lbd.dir</code>, <code>hbd.dir</code>,
<code>dfac.dir</code>, and <code>initd.dir</code> by <code class="reqn">n^{-2/(2P+l)}</code>,
<code class="reqn">n</code> the number of observations, <code class="reqn">P</code> the order of the
kernel, and <code class="reqn">l</code> the number of <code>numeric</code> variables. See
Details</p>
</td></tr>
<tr><td><code id="npcdistbw_+3A_memfac">memfac</code></td>
<td>

<p>The algorithm to compute the least-squares objective function uses
a block-based algorithm to eliminate or minimize redundant kernel
evaluations. Due to memory, hardware and software constraints, a
maximum block size must be imposed by the algorithm. This block size
is roughly equal to memfac*10^5 elements. Empirical tests on
modern hardware find that a memfac of around 500 performs well. If
you experience out of memory errors, or strange behaviour for
large data sets (&gt;100k elements) setting memfac to a lower value may
fix the problem.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>npcdistbw</code> implements a variety of methods for choosing
bandwidths for multivariate distributions (<code class="reqn">p+q</code>-variate) defined
over a set of possibly continuous and/or discrete (unordered
<code>xdat</code>, ordered <code>xdat</code> and <code>ydat</code>) data. The approach
is based on Li and Racine (2004) who employ &lsquo;generalized
product kernels&rsquo; that admit a mix of continuous and discrete data
types.
</p>
<p>The cross-validation methods employ multivariate numerical search
algorithms (direction set (Powell's) methods in multidimensions).
</p>
<p>Bandwidths can (and will) differ for each variable which is, of
course, desirable.
</p>
<p>Three classes of kernel estimators for the continuous data types are
available: fixed, adaptive nearest-neighbor, and generalized
nearest-neighbor. Adaptive nearest-neighbor bandwidths change with
each sample realization in the set, <code class="reqn">x_i</code>, when estimating
the cumulative distribution at the point <code class="reqn">x</code>. Generalized nearest-neighbor
bandwidths change with the point at which the cumulative distribution is estimated,
<code class="reqn">x</code>. Fixed bandwidths are constant over the support of <code class="reqn">x</code>.
</p>
<p><code>npcdistbw</code> may be invoked <em>either</em> with a formula-like
symbolic 
description of variables on which bandwidth selection is to be
performed <em>or</em> through a simpler interface whereby data is passed
directly to the function via the <code>xdat</code> and <code>ydat</code>
parameters. Use of these two interfaces is <b>mutually exclusive</b>.
</p>
<p>Data contained in the data frame <code>xdat</code> may be a mix of
continuous (default), unordered discrete (to be specified in the data
frames using <code><a href="base.html#topic+factor">factor</a></code>), and ordered discrete (to be
specified in the data frames using <code><a href="base.html#topic+ordered">ordered</a></code>).  Data
contained in the data frame <code>ydat</code> may be a mix of continuous
(default) and ordered discrete (to be specified in the data frames
using <code><a href="base.html#topic+ordered">ordered</a></code>). Data can be entered in an arbitrary
order and data types will be detected automatically by the routine
(see <code><a href="#topic+np">np</a></code> for details).
</p>
<p>Data for which bandwidths are to be estimated may be specified
symbolically. A typical description has the form <code>dependent data
  ~ explanatory data</code>,
where <code>dependent data</code> and <code>explanatory data</code> are both
series of variables specified by name, separated by 
the separation character '+'. For example, <code> y1 + y2 ~ x1 + x2 </code>
specifies that the bandwidths for the joint distribution of variables
<code>y1</code> and <code>y2</code> conditioned on <code>x1</code> and <code>x2</code> are to
be estimated. See below for further examples. 
</p>
<p>A variety of kernels may be specified by the user. Kernels implemented
for continuous data types include the second, fourth, sixth, and eighth
order Gaussian and Epanechnikov kernels, and the uniform
kernel. Unordered discrete data types use a variation on Aitchison and
Aitken's (1976) kernel, while ordered data types use a variation of the
Wang and van Ryzin (1981) kernel.
</p>
<p>The optimizer invoked for search is Powell's conjugate direction
method which requires the setting of (non-random) initial values and
search directions for bandwidths, and, when restarting, random values
for successive invocations. Bandwidths for <code>numeric</code> variables
are scaled by robust measures of spread, the sample size, and the
number of <code>numeric</code> variables where appropriate. Two sets of
parameters for bandwidths for <code>numeric</code> can be modified, those
for initial values for the parameters themselves, and those for the
directions taken (Powell's algorithm does not involve explicit
computation of the function's gradient). The default values are set by
considering search performance for a variety of difficult test cases
and simulated cases. We highly recommend restarting search a large
number of times to avoid the presence of local minima (achieved by
modifying <code>nmulti</code>). Further refinement for difficult cases can
be achieved by modifying these sets of parameters. However, these
parameters are intended more for the authors of the package to enable
&lsquo;tuning&rsquo; for various methods rather than for the user themselves.
</p>


<h3>Value</h3>

<p><code>npcdistbw</code> returns a <code>condbandwidth</code> object, with the
following components:
</p>
<table>
<tr><td><code>xbw</code></td>
<td>
<p> bandwidth(s), scale factor(s) or nearest neighbours for the
explanatory data, <code>xdat</code> </p>
</td></tr>
<tr><td><code>ybw</code></td>
<td>
<p> bandwidth(s), scale factor(s) or nearest neighbours for the
dependent data, <code>ydat</code> </p>
</td></tr>
<tr><td><code>fval</code></td>
<td>
<p> objective function value at minimum </p>
</td></tr>
</table>
<p>if <code>bwtype</code> is set to <code>fixed</code>, an object containing
bandwidths (or scale factors if <code>bwscaling = TRUE</code>) is
returned. If it is set to <code>generalized_nn</code> or <code>adaptive_nn</code>,
then instead the <code class="reqn">k</code>th nearest neighbors are returned for the
continuous variables while the discrete kernel bandwidths are returned
for the discrete variables.
</p>
<p>The functions <code><a href="stats.html#topic+predict">predict</a></code>, <code><a href="base.html#topic+summary">summary</a></code> and <code><a href="base.html#topic+plot">plot</a></code> support
objects of type <code>condbandwidth</code>.
</p>


<h3>Usage Issues</h3>

<p>If you are using data of mixed types, then it is advisable to use the
<code><a href="base.html#topic+data.frame">data.frame</a></code> function to construct your input data and not
<code><a href="base.html#topic+cbind">cbind</a></code>, since <code><a href="base.html#topic+cbind">cbind</a></code> will typically not work as
intended on mixed data types and will coerce the data to the same
type.
</p>
<p>Caution: multivariate data-driven bandwidth selection methods are, by
their nature, <em>computationally intensive</em>. Virtually all methods
require dropping the <code class="reqn">i</code>th observation from the data set, computing an
object, repeating this for all observations in the sample, then
averaging each of these leave-one-out estimates for a <em>given</em>
value of the bandwidth vector, and only then repeating this a large
number of times in order to conduct multivariate numerical
minimization/maximization. Furthermore, due to the potential for local
minima/maxima, <em>restarting this procedure a large number of times may
often be necessary</em>. This can be frustrating for users possessing
large datasets. For exploratory purposes, you may wish to override the
default search tolerances, say, setting ftol=.01 and tol=.01 and
conduct multistarting (the default is to restart min(5, ncol(xdat,ydat))
times) as is done for a number of examples. Once the procedure
terminates, you can restart search with default tolerances using those
bandwidths obtained from the less rigorous search (i.e., set
<code>bws=bw</code> on subsequent calls to this routine where <code>bw</code> is
the initial bandwidth object).  A version of this package using the
<code>Rmpi</code> wrapper is under development that allows one to deploy
this software in a clustered computing environment to facilitate
computation involving large datasets.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine
<a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Aitchison, J. and C.G.G. Aitken (1976), &ldquo;Multivariate binary
discrimination by the kernel method,&rdquo; Biometrika, 63, 413-420.
</p>
<p>Hall, P. and J.S. Racine and Q. Li (2004), &ldquo;Cross-validation and
the estimation of conditional probability densities,&rdquo; Journal of the
American Statistical Association, 99, 1015-1026.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics: Theory
and Practice,</em> Princeton University Press.
</p>
<p>Li, Q. and J.S. Racine (2008), &ldquo;Nonparametric estimation of
conditional CDF and quantile functions with mixed categorical and
continuous data,&rdquo; Journal of Business and Economic Statistics, 26,
423-434.
</p>
<p>Li, Q. and J. Lin and J.S. Racine (2013), &ldquo;Optimal bandwidth
selection for nonparametric conditional distribution and quantile
functions&rdquo;, Journal of Business and Economic Statistics, 31, 57-65.
</p>
<p>Pagan, A. and A. Ullah (1999), <em>Nonparametric Econometrics,</em>
Cambridge University Press.
</p>
<p>Scott, D.W. (1992), <em>Multivariate Density Estimation. Theory,
Practice and Visualization,</em> New York: Wiley.
</p>
<p>Silverman, B.W. (1986), <em>Density Estimation,</em> London: Chapman and
Hall.
</p>
<p>Wang, M.C. and J. van Ryzin (1981), &ldquo;A class of smooth estimators
for discrete distributions,&rdquo;  Biometrika, 68, 301-309.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+bw.nrd">bw.nrd</a></code>, <code><a href="stats.html#topic+bw.SJ">bw.SJ</a></code>, <code><a href="graphics.html#topic+hist">hist</a></code>,
<code><a href="#topic+npudens">npudens</a></code>, <code><a href="#topic+npudist">npudist</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# EXAMPLE 1 (INTERFACE=FORMULA): For this example, we compute the
# cross-validated bandwidths (default) using a second-order Gaussian
# kernel (default). Note - this may take a minute or two depending on
# the speed of your computer.

data("Italy")
attach(Italy)

bw &lt;- npcdistbw(formula=gdp~ordered(year))

# The object bw can be used for further estimation using
# npcdist(), plotting using plot() etc. Entering the name of
# the object provides useful summary information, and names() will also
# provide useful information.

summary(bw)

# Note - see the example for npudensbw() for multiple illustrations
# of how to change the kernel function, kernel order, and so forth.

detach(Italy)

# EXAMPLE 1 (INTERFACE=DATA FRAME): For this example, we compute the
# cross-validated bandwidths (default) using a second-order Gaussian
# kernel (default). Note - this may take a minute or two depending on
# the speed of your computer.

data("Italy")
attach(Italy)

bw &lt;- npcdistbw(xdat=ordered(year), ydat=gdp)

# The object bw can be used for further estimation using npcdist(),
# plotting using plot() etc. Entering the name of the object provides
# useful summary information, and names() will also provide useful
# information.

summary(bw)

# Note - see the example for npudensbw() for multiple illustrations
# of how to change the kernel function, kernel order, and so forth.

detach(Italy)

## End(Not run) 
</code></pre>

<hr>
<h2 id='npcmstest'> Kernel Consistent Model Specification Test with Mixed Data Types </h2><span id='topic+npcmstest'></span>

<h3>Description</h3>

<p><code>npcmstest</code> implements a consistent test for correct
specification of parametric regression models (linear or nonlinear) as
described in Hsiao, Li, and Racine (2007).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npcmstest(formula,
          data = NULL,
          subset,
          xdat,
          ydat,
          model = stop(paste(sQuote("model")," has not been provided")),
          distribution = c("bootstrap", "asymptotic"),
          boot.method = c("iid","wild","wild-rademacher"),
          boot.num = 399,
          pivot = TRUE,
          density.weighted = TRUE,
          random.seed = 42,
          ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npcmstest_+3A_formula">formula</code></td>
<td>

<p>a symbolic description of variables on which the test is
to be performed. The details of constructing a formula are
described below.
</p>
</td></tr>
<tr><td><code id="npcmstest_+3A_data">data</code></td>
<td>

<p>an optional data frame, list or environment (or object
coercible to a data frame by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code>) containing the variables
in the model. If not found in data, the variables are taken from
<code>environment(formula)</code>, typically the environment from which the
function is called.
</p>
</td></tr>
<tr><td><code id="npcmstest_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations to be used. 
</p>
</td></tr>
<tr><td><code id="npcmstest_+3A_model">model</code></td>
<td>

<p>a model object obtained from a call to <code><a href="stats.html#topic+lm">lm</a></code> (or
<code><a href="stats.html#topic+glm">glm</a></code>). Important: the 
call to either <code><a href="stats.html#topic+glm">glm</a></code> or <code><a href="stats.html#topic+lm">lm</a></code> must have the arguments
<code>x=TRUE</code> and 
<code>y=TRUE</code> or <code>npcmstest</code> will not work. Also, the test is
based on residual bootstrapping hence the outcome must be continuous
(which rules out Logit, Probit, and Count models).
</p>
</td></tr>
<tr><td><code id="npcmstest_+3A_xdat">xdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of explanatory data (training data) used to
calculate the regression estimators.
</p>
</td></tr>
<tr><td><code id="npcmstest_+3A_ydat">ydat</code></td>
<td>

<p>a one (1) dimensional numeric or integer vector of dependent data, each
element <code class="reqn">i</code> corresponding to each observation (row) <code class="reqn">i</code> of
<code>xdat</code>. 
</p>
</td></tr>
<tr><td><code id="npcmstest_+3A_distribution">distribution</code></td>
<td>

<p>a character string used to specify the method of estimating the
distribution of the statistic to be calculated. <code>bootstrap</code>
will conduct bootstrapping. <code>asymptotic</code> will use the normal
distribution. Defaults to <code>bootstrap</code>.
</p>
</td></tr>
<tr><td><code id="npcmstest_+3A_boot.method">boot.method</code></td>
<td>

<p>a character string used to specify the bootstrap method.
<code>iid</code> will generate independent identically distributed
draws. <code>wild</code> will use a wild bootstrap. <code>wild-rademacher</code>
will use a wild bootstrap with Rademacher variables. Defaults to
<code>iid</code>.
</p>
</td></tr>
<tr><td><code id="npcmstest_+3A_boot.num">boot.num</code></td>
<td>

<p>an integer value specifying the number of bootstrap replications to
use. Defaults to <code>399</code>.
</p>
</td></tr>
<tr><td><code id="npcmstest_+3A_pivot">pivot</code></td>
<td>

<p>a logical value specifying whether the statistic should be
normalised such that it approaches <code class="reqn">N(0,1)</code> in
distribution. Defaults to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npcmstest_+3A_density.weighted">density.weighted</code></td>
<td>

<p>a logical value specifying whether the statistic should be
weighted by the density of <code>xdat</code>. Defaults to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npcmstest_+3A_random.seed">random.seed</code></td>
<td>

<p>an integer used to seed R's random number generator. This is to
ensure replicability. Defaults to 42.
</p>
</td></tr>
<tr><td><code id="npcmstest_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to control bandwidth selection on the
residuals. One can specify the bandwidth type,
kernel types, and so on. To do this, you may specify any of <code>bwscaling</code>,
<code>bwtype</code>, <code>ckertype</code>, <code>ckerorder</code>, <code>ukertype</code>,
<code>okertype</code>, as described in <code><a href="#topic+npregbw">npregbw</a></code>.
This is necessary if you specify <code>bws</code> as a <code class="reqn">p</code>-vector and not
a <code>bandwidth</code> object, and you do not desire the default behaviours.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>npcmstest</code> returns an object of type <code>cmstest</code> with the
following components, components will contain information
related to <code>Jn</code> or <code>In</code> depending on the value of <code>pivot</code>:
</p>
<table>
<tr><td><code>Jn</code></td>
<td>
<p> the statistic <code>Jn</code> </p>
</td></tr>
<tr><td><code>In</code></td>
<td>
<p> the statistic <code>In</code> </p>
</td></tr>
<tr><td><code>Omega.hat</code></td>
<td>
<p> as described in Hsiao, C. and Q. Li and J.S. Racine. </p>
</td></tr>
<tr><td><code>q.*</code></td>
<td>
<p> the various quantiles of the statistic <code>Jn</code> (or
<code>In</code> if 
<code>pivot=FALSE</code>)  are in
components <code>q.90</code>,
<code>q.95</code>, <code>q.99</code> (one-sided 1%, 5%, 10% critical values) </p>
</td></tr>
<tr><td><code>P</code></td>
<td>
<p> the P-value of the statistic </p>
</td></tr>
<tr><td><code>Jn.bootstrap</code></td>
<td>
<p> if <code>pivot=TRUE</code> contains the bootstrap
replications of <code>Jn</code> </p>
</td></tr>
<tr><td><code>In.bootstrap</code></td>
<td>
<p> if <code>pivot=FALSE</code> contains the bootstrap
replications of <code>In</code> </p>
</td></tr>
</table>
<p><code><a href="base.html#topic+summary">summary</a></code> supports object of type <code>cmstest</code>.
</p>


<h3>Usage Issues</h3>

<p><code>npcmstest</code> supports regression objects generated by
<code><a href="stats.html#topic+lm">lm</a></code> and uses features specific to objects of type
<code><a href="stats.html#topic+lm">lm</a></code> hence if you attempt to pass objects of a different
type the function cannot be expected to work.
</p>
<p>If you are using data of mixed types, then it is advisable to use the
<code><a href="base.html#topic+data.frame">data.frame</a></code> function to construct your input data and not
<code><a href="base.html#topic+cbind">cbind</a></code>, since <code><a href="base.html#topic+cbind">cbind</a></code> will typically not work as
intended on mixed data types and will coerce the data to the same
type.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine
<a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Aitchison, J. and C.G.G. Aitken (1976), &ldquo;Multivariate binary
discrimination by the kernel method,&rdquo; Biometrika, 63, 413-420.
</p>
<p>Hsiao, C. and Q. Li and J.S. Racine (2007), &ldquo;A consistent
model specification test with mixed categorical and continuous
data,&rdquo; Journal of Econometrics, 140, 802-826.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics: Theory
and Practice,</em> Princeton University Press.
</p>
<p>Maasoumi, E. and J.S. Racine and T. Stengos (2007),
&ldquo;Growth and convergence: a profile of distribution dynamics and
mobility,&rdquo; Journal of Econometrics, 136, 483-508.
</p>
<p>Murphy, K. M. and F. Welch (1990), &ldquo;Empirical age-earnings
profiles,&rdquo; Journal of Labor Economics, 8, 202-229.
</p>
<p>Pagan, A. and A. Ullah (1999), <em>Nonparametric Econometrics,</em>
Cambridge University Press.
</p>
<p>Wang, M.C. and J. van Ryzin (1981), &ldquo;A class of smooth estimators
for discrete distributions,&rdquo;  Biometrika, 68, 301-309.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# EXAMPLE 1: For this example, we conduct a consistent model
# specification test for a parametric wage regression model that is
# quadratic in age. The work of Murphy and Welch (1990) would suggest
# that this parametric regression model is misspecified.

data("cps71")
attach(cps71)

model &lt;- lm(logwage~age+I(age^2), x=TRUE, y=TRUE)

plot(age, logwage)
lines(age, fitted(model))

# Note - this may take a few minutes depending on the speed of your
# computer...

npcmstest(model = model, xdat = age, ydat = logwage)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Next try Murphy &amp; Welch's (1990) suggested quintic specification.

model &lt;- lm(logwage~age+I(age^2)+I(age^3)+I(age^4)+I(age^5), x=TRUE, y=TRUE)

plot(age, logwage)
lines(age, fitted(model))

X &lt;- data.frame(age)

# Note - this may take a few minutes depending on the speed of your
# computer...

npcmstest(model = model, xdat = age, ydat = logwage)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Note - you can pass in multiple arguments to this function. For
# instance, to use local linear rather than local constant regression, 
# you would use npcmstest(model, X, regtype="ll"), while you could also
# change the kernel type (default is second order Gaussian), numerical
# search tolerance, or feed in your own vector of bandwidths and so
# forth.

detach(cps71)

# EXAMPLE 2: For this example, we replicate the application in Maasoumi,
# Racine, and Stengos (2007) (see oecdpanel for details). We
# estimate a parametric model that is used in the literature, then
# subject it to the model specification test.

data("oecdpanel")
attach(oecdpanel)

model &lt;- lm(growth ~ oecd +
            factor(year) +
            initgdp +
            I(initgdp^2) +
            I(initgdp^3) +
            I(initgdp^4) +
            popgro +
            inv +
            humancap +
            I(humancap^2) +
            I(humancap^3) - 1, 
            x=TRUE, 
            y=TRUE)

X &lt;- data.frame(factor(oecd), factor(year), initgdp, popgro, inv, humancap)

npcmstest(model = model, xdat = X, ydat = growth)

detach(oecdpanel)

## End(Not run) 
</code></pre>

<hr>
<h2 id='npconmode'> Kernel Modal Regression with Mixed Data Types </h2><span id='topic+npconmode'></span><span id='topic+npconmode.call'></span><span id='topic+npconmode.default'></span><span id='topic+npconmode.formula'></span><span id='topic+npconmode.conbandwidth'></span>

<h3>Description</h3>

<p><code>npconmode</code> performs kernel modal regression on mixed data,
and finds the
conditional mode given a set of training data, consisting of
explanatory data and dependent data, and possibly evaluation data.
Automatically computes various in sample and out of sample measures of
accuracy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npconmode(bws, ...)

## S3 method for class 'formula'
npconmode(bws, data = NULL, newdata = NULL, ...)

## S3 method for class 'call'
npconmode(bws, ...)

## Default S3 method:
npconmode(bws, txdat, tydat, ...)

## S3 method for class 'conbandwidth'
npconmode(bws,
          txdat = stop("invoked without training data 'txdat'"),
          tydat = stop("invoked without training data 'tydat'"),
          exdat,
          eydat,
          ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npconmode_+3A_bws">bws</code></td>
<td>

<p>a bandwidth specification. This can be set as a <code>conbandwidth</code>
object returned from an invocation of <code><a href="#topic+npcdensbw">npcdensbw</a></code>
</p>
</td></tr>
<tr><td><code id="npconmode_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to specify the bandwidth type,
kernel types, and so on, detailed below.
This is necessary if you specify bws as a <code class="reqn">p+q</code>-vector and not
a <code>conbandwidth</code> object, and you do not desire the default behaviours.
</p>
</td></tr>
<tr><td><code id="npconmode_+3A_data">data</code></td>
<td>

<p>an optional data frame, list or environment (or object
coercible to a data frame by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code>) containing the variables
in the model. If not found in data, the variables are taken from
<code>environment(bws)</code>, typically the environment from which
<code><a href="#topic+npcdensbw">npcdensbw</a></code> was called.
</p>
</td></tr>
<tr><td><code id="npconmode_+3A_newdata">newdata</code></td>
<td>

<p>An optional data frame in which to look for evaluation data. If
omitted, the training data are used.  
</p>
</td></tr>
<tr><td><code id="npconmode_+3A_txdat">txdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of explanatory data (conditioning data) used to
calculate the regression estimators. Defaults to the training data used to
compute the bandwidth object.
</p>
</td></tr>
<tr><td><code id="npconmode_+3A_tydat">tydat</code></td>
<td>

<p>a one (1) dimensional vector of unordered or ordered factors,
containing the dependent data. Defaults to the training data used to
compute the bandwidth object.
</p>
</td></tr>
<tr><td><code id="npconmode_+3A_exdat">exdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of points on which the regression will be
estimated (evaluation data). By default,
evaluation takes place on the data provided by <code>txdat</code>.
</p>
</td></tr>
<tr><td><code id="npconmode_+3A_eydat">eydat</code></td>
<td>

<p>a one (1) dimensional numeric or integer vector of the true values
(outcomes) of the dependent variable. By default,
evaluation takes place on the data provided by <code>tydat</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>npconmode</code> returns a <code>conmode</code> object with the following
components:
</p>
<table>
<tr><td><code>conmode</code></td>
<td>
<p> a vector of type <code>factor</code> (or <code>ordered
      factor</code>) containing the conditional mode at each evaluation
point</p>
</td></tr>
<tr><td><code>condens</code></td>
<td>
<p> a vector of numeric type containing the modal density
estimates at each evaluation point</p>
</td></tr>
<tr><td><code>xeval</code></td>
<td>
<p> a data frame of evaluation points </p>
</td></tr>
<tr><td><code>yeval</code></td>
<td>
<p> a vector of type <code>factor</code> (or <code>ordered
      factor</code>) containing the actual outcomes, or <code>NA</code> if not
provided </p>
</td></tr>
<tr><td><code>confusion.matrix</code></td>
<td>
<p> the confusion matrix or <code>NA</code> if outcomes
are not available </p>
</td></tr>
<tr><td><code>CCR.overall</code></td>
<td>
<p> the overall correct
classification ratio, or <code>NA</code> if outcomes are not available </p>
</td></tr>
<tr><td><code>CCR.byoutcome</code></td>
<td>
<p> a numeric vector containing the correct
classification ratio by outcome, or <code>NA</code> if outcomes are not
available </p>
</td></tr>
<tr><td><code>fit.mcfadden</code></td>
<td>
<p> the McFadden-Puig-Kerschner performance measure
or <code>NA</code> if outcomes are not available </p>
</td></tr>
</table>
<p>The functions <code><a href="base.html#topic+mode">mode</a></code>, and <code><a href="stats.html#topic+fitted">fitted</a></code> may be used to
extract the conditional mode estimates, and the conditional density
estimates at the conditional mode, respectively,
from the resulting object. Also, <code><a href="base.html#topic+summary">summary</a></code> supports
<code>conmode</code> objects.
</p>


<h3>Usage Issues</h3>

<p>If you are using data of mixed types, then it is advisable to use the
<code><a href="base.html#topic+data.frame">data.frame</a></code> function to construct your input data and not
<code><a href="base.html#topic+cbind">cbind</a></code>, since <code><a href="base.html#topic+cbind">cbind</a></code> will typically not work as
intended on mixed data types and will coerce the data to the same
type.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine
<a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Aitchison, J. and  C.G.G. Aitken (1976), &ldquo;Multivariate binary
discrimination by the kernel method,&rdquo; Biometrika, 63, 413-420.
</p>
<p>Hall, P. and J.S. Racine and Q. Li (2004), &ldquo;Cross-validation and the
estimation of conditional probability densities,&rdquo; Journal of the
American Statistical Association, 99, 1015-1026.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics: Theory
and Practice,</em> Princeton University Press.
</p>
<p>McFadden, D. and C. Puig and D. Kerschner (1977), &ldquo;Determinants
of the long-run demand for electricity,&rdquo; Proceedings of the
American Statistical Association (Business and Economics Section),
109-117. 
</p>
<p>Pagan, A. and A. Ullah (1999), <em>Nonparametric Econometrics,</em>
Cambridge University Press.
</p>
<p>Scott, D.W. (1992), <em>Multivariate Density Estimation. Theory,
Practice and Visualization,</em> New York: Wiley.
</p>
<p>Silverman, B.W. (1986), <em>Density Estimation,</em> London: Chapman and
Hall.
</p>
<p>Wang, M.C. and J. van Ryzin (1981), &ldquo;A class of smooth estimators
for discrete distributions,&rdquo;  Biometrika, 68, 301-309.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# EXAMPLE 1 (INTERFACE=FORMULA): For this example, we use the
# birthweight data taken from the MASS library, and compute a parametric
# logit model and a nonparametric conditional mode model. We then
# compare their confusion matrices and summary measures of
# classification ability.

library("MASS")
data("birthwt")
attach(birthwt)

# Fit a parametric logit model with low (0/1) as the dependent
# variable and age, lwt, and smoke (0/1) as the covariates

# From ?birthwt
# 'low' indicator of birth weight less than 2.5kg
# 'smoke' smoking status during pregnancy
# 'race' mother's race ('1' = white, '2' = black, '3' = other)
# 'ht' history of hypertension
# 'ui' presence of uterine irritability
# 'ftv' number of physician visits during the first trimester
# 'age' mother's age in years
# 'lwt' mother's weight in pounds at last menstrual period

model.logit &lt;- glm(low~factor(smoke)+
                   factor(race)+
                   factor(ht)+
                   factor(ui)+
                   ordered(ftv)+
                   age+
                   lwt, 
                   family=binomial(link=logit))

# Generate the confusion matrix and correct classification ratio

cm &lt;- table(low, ifelse(fitted(model.logit)&gt;0.5, 1, 0))
ccr &lt;- sum(diag(cm))/sum(cm)

# Now do the same with a nonparametric model.  Note - this may take a
# few minutes depending on the speed of your computer... 

bw &lt;- npcdensbw(formula=factor(low)~factor(smoke)+
                factor(race)+
                factor(ht)+
                factor(ui)+
                ordered(ftv)+
                age+
                lwt)

model.np &lt;- npconmode(bws=bw)

# Compare confusion matrices from the logit and nonparametric model

# Logit

cm
ccr

# Nonparametric
summary(model.np)

detach(birthwt)

# EXAMPLE 1 (INTERFACE=DATA FRAME): For this example, we use the
# birthweight data taken from the MASS library, and compute a parametric
# logit model and a nonparametric conditional mode model. We then
# compare their confusion matrices and summary measures of
# classification ability.

library("MASS")
data("birthwt")
attach(birthwt)

# Fit a parametric logit model with low (0/1) as the dependent
# variable and age, lwt, and smoke (0/1) as the covariates

# From ?birthwt
# 'low' indicator of birth weight less than 2.5kg
# 'smoke' smoking status during pregnancy
# 'race' mother's race ('1' = white, '2' = black, '3' = other)
# 'ht' history of hypertension
# 'ui' presence of uterine irritability
# 'ftv' number of physician visits during the first trimester
# 'age' mother's age in years
# 'lwt' mother's weight in pounds at last menstrual period

model.logit &lt;- glm(low~factor(smoke)+
                   factor(race)+
                   factor(ht)+
                   factor(ui)+
                   ordered(ftv)+
                   age+
                   lwt, 
                   family=binomial(link=logit))

# Generate the confusion matrix and correct classification ratio

cm &lt;- table(low, ifelse(fitted(model.logit)&gt;0.5, 1, 0))
ccr &lt;- sum(diag(cm))/sum(cm)

# Now do the same with a nonparametric model...

X &lt;- data.frame(factor(smoke), 
                factor(race), 
                factor(ht), 
                factor(ui), 
                ordered(ftv), 
                age, 
                lwt)

y &lt;- factor(low)

# Note - this may take a few minutes depending on the speed of your
# computer... 

bw &lt;- npcdensbw(xdat=X, ydat=y)

model.np &lt;- npconmode(bws=bw)

# Compare confusion matrices from the logit and nonparametric model

# Logit

cm
ccr

# Nonparametric
summary(model.np)

detach(birthwt)

## End(Not run) 
</code></pre>

<hr>
<h2 id='npcopula'> Kernel Copula Estimation with Mixed Data Types </h2><span id='topic+npcopula'></span>

<h3>Description</h3>

<p><code>npcopula</code> implements the nonparametric mixed data kernel copula
approach of Racine (2015) for an arbitrary number of dimensions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npcopula(bws,
         data,
         u = NULL,
         n.quasi.inv = 1000,
         er.quasi.inv = 1) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npcopula_+3A_bws">bws</code></td>
<td>

<p>an unconditional joint distribution (<code>npudistbw</code>) or joint
density (<code>npudensbw</code>) bandwidth object (if <code>bws</code> is
delivered via <code>npudistbw</code> the copula distribution is estimated,
while if <code>bws</code> is delivered via <code>npudensbw</code> the copula
density is estimated)
</p>
</td></tr>
<tr><td><code id="npcopula_+3A_data">data</code></td>
<td>

<p>a data frame containing variables used to construct <code>bws</code>
</p>
</td></tr>
<tr><td><code id="npcopula_+3A_u">u</code></td>
<td>

<p>an optional matrix of real numbers lying in [0,1], each column of
which corresponds to the vector of uth quantile values desired for
each variable in the copula (otherwise the u values returned are
those corresponding to the sample realizations)
</p>
</td></tr>
<tr><td><code id="npcopula_+3A_n.quasi.inv">n.quasi.inv</code></td>
<td>

<p>number of grid points generated when <code>u</code> is provided in order to
compute the quasi-inverse of each marginal distribution (see
details)
</p>
</td></tr>
<tr><td><code id="npcopula_+3A_er.quasi.inv">er.quasi.inv</code></td>
<td>

<p>number passed to <code><a href="grDevices.html#topic+extendrange">extendrange</a></code> when <code>u</code> is provided
specifying the fraction by which the data range should be extended when
constructing the grid used to compute the quasi-inverse (see details)
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>npcopula</code> computes the nonparametric copula or copula density
using inversion (Nelsen (2006), page 51). For the inversion approach,
we exploit Sklar's theorem (Corollary 2.3.7, Nelsen (2006)) to produce
copulas directly from the joint distribution function using
<code class="reqn">C(u,v) = H(F^{-1}(u),G^{-1}(v))</code> rather than the typical approach
that instead uses <code class="reqn">H(x,y) = C(F(x),G(y))</code>. Whereas the latter
requires kernel density estimation on a d-dimensional unit hypercube
which necessitates the use of boundary correction methods, the former
does not.
</p>
<p>Note that if <code>u</code> is provided then <code><a href="base.html#topic+expand.grid">expand.grid</a></code> is
called on <code>u</code>. As the dimension increases this can become
unwieldy and potentially consume an enormous amount of memory unless
the number of grid points is kept very small. Given that computing the
copula on a grid is typically done for graphical purposes, providing
<code>u</code> is typically done for two-dimensional problems only. Even
here, however, providing a grid of length 100 will expand into a
matrix of dimension 10000 by 2 which, though not memory intensive, may
be computationally burdensome.
</p>
<p>The &lsquo;quasi-inverse&rsquo; is computed via Definition 2.3.6 from
Nelsen (2006). We compute an equi-quantile grid on the range of the
data of length <code>n.quasi.inv/2</code>. We then extend the range of the
data by the factor <code>er.quasi.inv</code> and compute an equi-spaced grid
of points of length <code>n.quasi.inv/2</code> (e.g. using the default
<code>er.quasi.inv=1</code> we go from the minimum data value minus
<code class="reqn">1\times</code> the range to the maximum data value plus
<code class="reqn">1\times</code> the range for each marginal). We then take these two
grids, concatenate and sort, and these form the final grid of length
<code>n.quasi.inv</code> for computing the quasi-inverse.
</p>
<p>Note that if <code>u</code> is provided and any elements of (the columns of)
<code>u</code> are such that they lie beyond the respective values of
<code>F</code> for the evaluation data for the respective marginal, such
values are reset to the minimum/maximum values of <code>F</code> for the
respective marginal. It is therefore prudent to inspect the values of
<code>u</code> returned by <code><a href="#topic+npcopula">npcopula</a></code> when <code>u</code> is provided.
</p>
<p>Note that copula are only defined for data of type
<code><a href="base.html#topic+numeric">numeric</a></code> or <code><a href="base.html#topic+ordered">ordered</a></code>.
</p>


<h3>Value</h3>

<p><code>npcopula</code> returns an object of type <code><a href="base.html#topic+data.frame">data.frame</a></code>
with the following components
</p>
<table>
<tr><td><code>copula</code></td>
<td>

<p>the copula (bandwidth object obtained from <code>npudistbw</code>) or
copula density (bandwidth object obtained from <code>npudensbw</code>)
</p>
</td></tr>
<tr><td><code>u</code></td>
<td>
<p> the matrix of marginal u values associated with the sample
realizations (<code>u=NULL</code>) or those created via
<code><a href="base.html#topic+expand.grid">expand.grid</a></code> when <code>u</code> is provided</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p> the matrix of marginal quantiles constructed when
<code>u</code> is provided (<code>data</code> returned has the same names as
<code>data</code> inputted)</p>
</td></tr>
</table>


<h3>Usage Issues</h3>

<p>See the example below for proper usage.
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Nelsen, R. B. (2006), <em>An Introduction to Copulas,</em> Second
Edition, Springer-Verlag.
</p>
<p>Racine, J.S. (2015), &ldquo;Mixed Data Kernel Copulas,&rdquo; Empirical
Economics, 48, 37-59.
</p>


<h3>See Also</h3>

<p><a href="#topic+npudensbw">npudensbw</a>,<a href="#topic+npudens">npudens</a>,<a href="#topic+npudist">npudist</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Example 1: Bivariate Mixed Data

require(MASS)

set.seed(42)

## Simulate correlated Gaussian data (rho(x,y)=0.99)

n &lt;- 1000
n.eval &lt;- 100
rho &lt;- 0.99
mu &lt;- c(0,0)
Sigma &lt;- matrix(c(1,rho,rho,1),2,2)
mydat &lt;- mvrnorm(n=n, mu, Sigma)
mydat &lt;- data.frame(x=mydat[,1],
                    y=ordered(as.integer(cut(mydat[,2],
                      quantile(mydat[,2],seq(0,1,by=.1)),
                      include.lowest=TRUE))-1))
q.min &lt;- 0.0
q.max &lt;- 1.0
grid.seq &lt;- seq(q.min,q.max,length=n.eval)
grid.dat &lt;- cbind(grid.seq,grid.seq)

## Estimate the copula (bw object obtained from npudistbw())

bw.cdf &lt;- npudistbw(~x+y,data=mydat)
copula &lt;- npcopula(bws=bw.cdf,data=mydat,u=grid.dat)

## Plot the copula


contour(grid.seq,grid.seq,matrix(copula$copula,n.eval,n.eval),
        xlab="u1",
        ylab="u2",
        main="Copula Contour")

persp(grid.seq,grid.seq,matrix(copula$copula,n.eval,n.eval),
      ticktype="detailed",
      xlab="u1",
      ylab="u2",
      zlab="Copula",zlim=c(0,1))

## Plot the empirical copula

copula.emp &lt;- npcopula(bws=bw.cdf,data=mydat)

plot(copula.emp$u1,copula.emp$u2,
     xlab="u1",
     ylab="u2",
     cex=.25,
     main="Empirical Copula")

## Estimate the copula density (bw object obtained from npudensbw())

bw.pdf &lt;- npudensbw(~x+y,data=mydat)
copula &lt;- npcopula(bws=bw.pdf,data=mydat,u=grid.dat)

## Plot the copula density

persp(grid.seq,grid.seq,matrix(copula$copula,n.eval,n.eval),
      ticktype="detailed",
      xlab="u1",
      ylab="u2",
      zlab="Copula Density")

## Example 2: Bivariate Continuous Data

require(MASS)

set.seed(42)

## Simulate correlated Gaussian data (rho(x,y)=0.99)

n &lt;- 1000
n.eval &lt;- 100
rho &lt;- 0.99
mu &lt;- c(0,0)
Sigma &lt;- matrix(c(1,rho,rho,1),2,2)
mydat &lt;- mvrnorm(n=n, mu, Sigma)
mydat &lt;- data.frame(x=mydat[,1],y=mydat[,2])

q.min &lt;- 0.0
q.max &lt;- 1.0
grid.seq &lt;- seq(q.min,q.max,length=n.eval)
grid.dat &lt;- cbind(grid.seq,grid.seq)

## Estimate the copula (bw object obtained from npudistbw())

bw.cdf &lt;- npudistbw(~x+y,data=mydat)
copula &lt;- npcopula(bws=bw.cdf,data=mydat,u=grid.dat)

## Plot the copula

contour(grid.seq,grid.seq,matrix(copula$copula,n.eval,n.eval),
        xlab="u1",
        ylab="u2",
        main="Copula Contour")

persp(grid.seq,grid.seq,matrix(copula$copula,n.eval,n.eval),
      ticktype="detailed",
      xlab="u1",
      ylab="u2",
      zlab="Copula",
      zlim=c(0,1))

## Plot the empirical copula

copula.emp &lt;- npcopula(bws=bw.cdf,data=mydat)

plot(copula.emp$u1,copula.emp$u2,
     xlab="u1",
     ylab="u2",
     cex=.25,
     main="Empirical Copula")

## Estimate the copula density (bw object obtained from npudensbw())

bw.pdf &lt;- npudensbw(~x+y,data=mydat)
copula &lt;- npcopula(bws=bw.pdf,data=mydat,u=grid.dat)

## Plot the copula density

persp(grid.seq,grid.seq,matrix(copula$copula,n.eval,n.eval),
      ticktype="detailed",
      xlab="u1",
      ylab="u2",
      zlab="Copula Density")

## End(Not run) 
</code></pre>

<hr>
<h2 id='npdeneqtest'> Kernel Consistent Density Equality Test with Mixed Data Types </h2><span id='topic+npdeneqtest'></span>

<h3>Description</h3>

<p><code>npdeneqtest</code> implements a consistent integrated squared
difference test for equality of densities as described in Li, Maasoumi,
and Racine (2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npdeneqtest(x = NULL,
            y = NULL,
            bw.x = NULL,
            bw.y = NULL,
            boot.num = 399,
            random.seed = 42,
            ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npdeneqtest_+3A_x">x</code>, <code id="npdeneqtest_+3A_y">y</code></td>
<td>

<p>data frames for the two samples for which one wishes to
test equality of densities. The variables in each data
frame must be the same (i.e. have identical names).
</p>
</td></tr>
<tr><td><code id="npdeneqtest_+3A_bw.x">bw.x</code>, <code id="npdeneqtest_+3A_bw.y">bw.y</code></td>
<td>

<p>optional bandwidth objects for <code>x,y</code>
</p>
</td></tr>
<tr><td><code id="npdeneqtest_+3A_boot.num">boot.num</code></td>
<td>

<p>an integer value specifying the number of bootstrap
replications to use. Defaults to <code>399</code>.
</p>
</td></tr>
<tr><td><code id="npdeneqtest_+3A_random.seed">random.seed</code></td>
<td>

<p>an integer used to seed R's random number generator. This is to
ensure replicability. Defaults to 42.
</p>
</td></tr>
<tr><td><code id="npdeneqtest_+3A_...">...</code></td>
<td>
<p> additional arguments supplied to specify the bandwidth
type, kernel types, and so on.  This is used if you do not pass in
bandwidth objects and you do not desire the default behaviours. To
do this, you may specify any of <code>bwscaling</code>, <code>bwtype</code>,
<code>ckertype</code>, <code>ckerorder</code>, <code>ukertype</code>,
<code>okertype</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>npdeneqtest</code> computes the integrated squared density difference
between the estimated densities/probabilities of two samples having
identical variables/datatypes. See Li, Maasoumi, and Racine (2009) for
details.
</p>


<h3>Value</h3>

<p><code>npdeneqtest</code> returns an object of type <code>deneqtest</code> with the
following components
</p>
<table>
<tr><td><code>Tn</code></td>
<td>
<p> the (standardized) statistic <code>Tn</code> </p>
</td></tr>
<tr><td><code>In</code></td>
<td>
<p> the (unstandardized) statistic <code>In</code> </p>
</td></tr>
<tr><td><code>Tn.bootstrap</code></td>
<td>
<p> contains the bootstrap replications of <code>Tn</code> </p>
</td></tr>
<tr><td><code>In.bootstrap</code></td>
<td>
<p> contains the bootstrap replications of <code>In</code> </p>
</td></tr>
<tr><td><code>Tn.P</code></td>
<td>
<p> the P-value of the <code>Tn</code> statistic </p>
</td></tr>
<tr><td><code>In.P</code></td>
<td>
<p> the P-value of the <code>In</code> statistic </p>
</td></tr>
<tr><td><code>boot.num</code></td>
<td>
<p> number of bootstrap replications </p>
</td></tr>
</table>
<p><code><a href="base.html#topic+summary">summary</a></code> supports object of type <code>deneqtest</code>.
</p>


<h3>Usage Issues</h3>

<p>If you are using data of mixed types, then it is advisable to use the
<code><a href="base.html#topic+data.frame">data.frame</a></code> function to construct your input data and not
<code><a href="base.html#topic+cbind">cbind</a></code>, since <code><a href="base.html#topic+cbind">cbind</a></code> will typically not work as
intended on mixed data types and will coerce the data to the same
type.
</p>
<p>It is crucial that both data frames have the same variable names.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine
<a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Li, Q. and E. Maasoumi and J.S. Racine (2009), &ldquo;A Nonparametric
Test for Equality of Distributions with Mixed Categorical and
Continuous Data,&rdquo; Journal of Econometrics, 148, pp 186-200.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npdeptest">npdeptest</a>,<a href="#topic+npsdeptest">npsdeptest</a>,<a href="#topic+npsymtest">npsymtest</a>,<a href="#topic+npunitest">npunitest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
set.seed(1234)

## Distributions are equal

n &lt;- 250

sample.A &lt;- data.frame(x=rnorm(n))
sample.B &lt;- data.frame(x=rnorm(n))

npdeneqtest(sample.A,sample.B,boot.num=99)

Sys.sleep(5)

## Distributions are unequal

sample.A &lt;- data.frame(x=rnorm(n))
sample.B &lt;- data.frame(x=rchisq(n,df=5))

npdeneqtest(sample.A,sample.B,boot.num=99)

## Mixed datatypes, distributions are equal

sample.A &lt;- data.frame(a=rnorm(n),b=factor(rbinom(n,2,.5)))
sample.B &lt;- data.frame(a=rnorm(n),b=factor(rbinom(n,2,.5)))

npdeneqtest(sample.A,sample.B,boot.num=99)

Sys.sleep(5)

## Mixed datatypes, distributions are unequal

sample.A &lt;- data.frame(a=rnorm(n),b=factor(rbinom(n,2,.5)))
sample.B &lt;- data.frame(a=rnorm(n,sd=10),b=factor(rbinom(n,2,.25)))

npdeneqtest(sample.A,sample.B,boot.num=99)

## End(Not run) 
</code></pre>

<hr>
<h2 id='npdeptest'> Kernel Consistent Pairwise Nonlinear Dependence Test for Univariate Processes </h2><span id='topic+npdeptest'></span>

<h3>Description</h3>

<p><code>npdeptest</code> implements the consistent metric entropy test of
pairwise independence as described in Maasoumi and Racine (2002).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
npdeptest(data.x = NULL,
          data.y = NULL,
          method = c("integration","summation"),
          bootstrap = TRUE,
          boot.num = 399,
          random.seed = 42)
  
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npdeptest_+3A_data.x">data.x</code>, <code id="npdeptest_+3A_data.y">data.y</code></td>
<td>

<p>two univariate vectors containing two variables that are of type
<code><a href="base.html#topic+numeric">numeric</a></code>.
</p>
</td></tr>
<tr><td><code id="npdeptest_+3A_method">method</code></td>
<td>

<p>a character string used to specify whether to compute the integral
version or the summation version of the statistic. Can be set as
<code>integration</code> or <code>summation</code> (see below for
details). Defaults to <code>integration</code>.
</p>
</td></tr>
<tr><td><code id="npdeptest_+3A_bootstrap">bootstrap</code></td>
<td>

<p>a logical value which specifies whether to conduct
the bootstrap test or not. If set to <code>FALSE</code>, only the
statistic will be computed. Defaults to <code>TRUE</code>.    
</p>
</td></tr>
<tr><td><code id="npdeptest_+3A_boot.num">boot.num</code></td>
<td>

<p>an integer value specifying the number of bootstrap
replications to use. Defaults to <code>399</code>.
</p>
</td></tr>
<tr><td><code id="npdeptest_+3A_random.seed">random.seed</code></td>
<td>

<p>an integer used to seed R's random number generator. This is to
ensure replicability. Defaults to 42.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>npsdeptest</code> computes the nonparametric metric entropy
(normalized Hellinger of Granger, Maasoumi and Racine (2004)) for
testing pairwise nonlinear dependence between the densities of two
data series. See Maasoumi and Racine (2002) for details. Default
bandwidths are of the Kullback-Leibler variety obtained via
likelihood cross-validation. The null distribution is obtained via
bootstrap resampling under the null of pairwise independence.
</p>
<p><code>npdeptest</code> computes the distance between the joint distribution
and the product of marginals (i.e. the joint distribution under the
null), <code class="reqn">D[f(y, \hat y), f(y)\times f(\hat y)]</code>. Examples include, (a) a measure/test of &ldquo;fit&rdquo;,
for in-sample values of a variable <code class="reqn">y</code> and its fitted values,
<code class="reqn">\hat y</code>, and (b) a measure of &ldquo;predictability&rdquo; for
a variable <code class="reqn">y</code> and its predicted values <code class="reqn">\hat y</code> (from
a user implemented model).
</p>
<p>The summation version of this statistic will be numerically unstable
when <code>data.x</code> and <code>data.y</code> lack common support or are sparse
(the summation version involves division of densities while the
integration version involves differences). Warning messages are
produced should this occur (&lsquo;integration recommended&rsquo;) and should be
heeded.
</p>


<h3>Value</h3>

<p><code>npdeptest</code> returns an object of type <code>deptest</code> with the
following components
</p>
<table>
<tr><td><code>Srho</code></td>
<td>
<p> the statistic <code>Srho</code> </p>
</td></tr>
<tr><td><code>Srho.bootstrap.vec</code></td>
<td>
<p> contains the bootstrap replications of
<code>Srho</code> </p>
</td></tr>
<tr><td><code>P</code></td>
<td>
<p> the P-value of the Srho statistic </p>
</td></tr>
<tr><td><code>bootstrap</code></td>
<td>
<p> a logical value indicating whether bootstrapping was
performed </p>
</td></tr>
<tr><td><code>boot.num</code></td>
<td>
<p> number of bootstrap replications </p>
</td></tr>
<tr><td><code>bw.data.x</code></td>
<td>
<p> the numeric bandwidth for <code>data.x</code> marginal
density</p>
</td></tr>
<tr><td><code>bw.data.y</code></td>
<td>
<p> the numeric bandwidth for
<code>data.y</code> marginal density</p>
</td></tr>  
<tr><td><code>bw.joint</code></td>
<td>
<p> the numeric matrix of bandwidths for <code>data</code>
and lagged <code>data</code> joint density  at lag <code>num.lag</code></p>
</td></tr>      
</table>
<p><code><a href="base.html#topic+summary">summary</a></code> supports object of type <code>deptest</code>.
</p>


<h3>Usage Issues</h3>

<p>The <code>integration</code> version of the statistic uses multidimensional
numerical methods from the <code><a href="cubature.html#topic+cubature">cubature</a></code> package. See
<code><a href="cubature.html#topic+adaptIntegrate">adaptIntegrate</a></code> for details. The <code>integration</code>
version of the statistic will be substantially slower than the
<code>summation</code> version, however, it will likely be both more
accurate and powerful.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine
<a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Granger, C.W. and E. Maasoumi and J.S. Racine (2004), &ldquo;A
dependence metric for possibly nonlinear processes&rdquo;, Journal of Time
Series Analysis, 25, 649-669.
</p>
<p>Maasoumi, E. and J.S. Racine (2002), &ldquo;Entropy and
Predictability of Stock Market Returns,&rdquo; Journal of Econometrics,
107, 2, pp 291-312.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npdeneqtest">npdeneqtest</a>,<a href="#topic+npsdeptest">npsdeptest</a>,<a href="#topic+npsymtest">npsymtest</a>,<a href="#topic+npunitest">npunitest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
set.seed(1234)

## Test/measure lack of fit between y and its fitted value from a
## regression model when x is relevant using the `summation' version.

n &lt;- 100
x &lt;- rnorm(n)
y &lt;- 1 + x + rnorm(n)
model &lt;- lm(y~x)
y.fit &lt;- fitted(model)

npdeptest(y,y.fit,boot.num=99,method="summation")

Sys.sleep(5)

## Test/measure lack of fit between y and its fitted value from a
## regression model when x is irrelevant using the `summation' version.

n &lt;- 100
x &lt;- runif(n,-2,2)
y &lt;- 1 + rnorm(n)
model &lt;- lm(y~x)
y.fit &lt;- fitted(model)

npdeptest(y,y.fit,boot.num=99,method="summation")

## Test/measure lack of fit between y and its fitted value from a
## regression model when x is relevant using the `integration'
## version (default, slower than summation version).

n &lt;- 100
x &lt;- rnorm(n)
y &lt;- 1 + x + rnorm(n)
model &lt;- lm(y~x)
y.fit &lt;- fitted(model)

npdeptest(y,y.fit,boot.num=99)

Sys.sleep(5)

## Test/measure lack of fit between y and its fitted value from a
## regression model when x is irrelevant using the `integration'
## version (default, slower than summation version).

n &lt;- 100
x &lt;- runif(n,-2,2)
y &lt;- 1 + rnorm(n)
model &lt;- lm(y~x)
y.fit &lt;- fitted(model)

npdeptest(y,y.fit,boot.num=99)

## End(Not run) 
</code></pre>

<hr>
<h2 id='npindex'>Semiparametric Single Index Model</h2><span id='topic+npindex'></span><span id='topic+npindex.call'></span><span id='topic+npindex.default'></span><span id='topic+npindex.formula'></span><span id='topic+npindex.sibandwidth'></span>

<h3>Description</h3>

<p><code>npindex</code> computes a semiparametric single index model
for a dependent variable and <code class="reqn">p</code>-variate explanatory data using
the model <code class="reqn">Y = G(X\beta) + \epsilon</code>, given a
set of evaluation points, training points (consisting of explanatory
data and dependent data), and a <code>npindexbw</code> bandwidth
specification. Note that for this semiparametric estimator, the
bandwidth object contains parameters for the single index model and
the (scalar) bandwidth for the index function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npindex(bws, ...)

## S3 method for class 'formula'
npindex(bws,
        data = NULL,
        newdata = NULL,
        y.eval = FALSE,
        ...)

## S3 method for class 'call'
npindex(bws,
        ...)

## Default S3 method:
npindex(bws,
        txdat,
        tydat,
        ...) 

## S3 method for class 'sibandwidth'
npindex(bws,
        txdat = stop("training data 'txdat' missing"),
        tydat = stop("training data 'tydat' missing"),
        exdat,
        eydat,
        gradients = FALSE,
        residuals = FALSE,
        errors = FALSE,
        boot.num = 399,
        ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npindex_+3A_bws">bws</code></td>
<td>

<p>a bandwidth specification. This can be set as a
<code>sibandwidth</code>
object returned from an invocation of <code>npindexbw</code>, or
as a vector of parameters (beta) with each element <code class="reqn">i</code>
corresponding to the coefficient for column <code class="reqn">i</code> in <code>txdat</code>
where the first element is normalized to 1, and a scalar bandwidth
(h). 
</p>
</td></tr>
<tr><td><code id="npindex_+3A_gradients">gradients</code></td>
<td>

<p>a logical value indicating that you want gradients and the
asymptotic covariance matrix for beta computed and returned in the
resulting <code>singleindex</code> object. Defaults to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="npindex_+3A_residuals">residuals</code></td>
<td>

<p>a logical value indicating that you want residuals computed and
returned in the resulting <code>singleindex</code> object. Defaults to
<code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="npindex_+3A_errors">errors</code></td>
<td>

<p>a logical value indicating that you want (bootstrapped)
standard errors for the conditional mean, gradients (when
<code>gradients=TRUE</code> is set), and average gradients (when
<code>gradients=TRUE</code> is set), computed and returned in the
resulting <code>singleindex</code> object. Defaults to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="npindex_+3A_boot.num">boot.num</code></td>
<td>

<p>an integer specifying the number of bootstrap replications to use
when performing standard error calculations. Defaults to
<code>399</code>. 
</p>
</td></tr>
<tr><td><code id="npindex_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to specify the parameters to the
<code>sibandwidth</code> S3 method, which is called during estimation. 
</p>
</td></tr>
<tr><td><code id="npindex_+3A_data">data</code></td>
<td>

<p>an optional data frame, list or environment (or object
coercible to a data frame by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code>) containing the variables
in the model. If not found in data, the variables are taken from
<code>environment(bws)</code>, typically the environment from which
<code><a href="#topic+npindexbw">npindexbw</a></code> was called.
</p>
</td></tr>
<tr><td><code id="npindex_+3A_newdata">newdata</code></td>
<td>

<p>An optional data frame in which to look for evaluation data. If
omitted, the training data are used.  
</p>
</td></tr>
<tr><td><code id="npindex_+3A_y.eval">y.eval</code></td>
<td>

<p>If <code>newdata</code> contains dependent data and <code>y.eval = TRUE</code>,
<code><a href="#topic+np">np</a></code> will compute goodness of fit statistics on these
data and return them. Defaults to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="npindex_+3A_txdat">txdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of explanatory data (training data) used to
calculate the regression estimators. Defaults to the training data used to
compute the bandwidth object.
</p>
</td></tr>
<tr><td><code id="npindex_+3A_tydat">tydat</code></td>
<td>

<p>a one (1) dimensional numeric or integer vector of dependent data, each
element <code class="reqn">i</code> corresponding to each observation (row) <code class="reqn">i</code> of
<code>txdat</code>. Defaults to the training data used to
compute the bandwidth object.
</p>
</td></tr>
<tr><td><code id="npindex_+3A_exdat">exdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of points on which the regression will be
estimated (evaluation data). By default,
evaluation takes place on the data provided by <code>txdat</code>.
</p>
</td></tr>
<tr><td><code id="npindex_+3A_eydat">eydat</code></td>
<td>

<p>a one (1) dimensional numeric or integer vector of the true values
of the dependent variable. Optional, and used only to calculate the
true errors. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A matrix of gradients along with average derivatives are computed and
returned if <code>gradients=TRUE</code> is used.
</p>


<h3>Value</h3>

<p><code>npindex</code> returns a <code>npsingleindex</code> object.  The generic
functions <code><a href="stats.html#topic+fitted">fitted</a></code>, <code><a href="stats.html#topic+residuals">residuals</a></code>,
<code><a href="stats.html#topic+coef">coef</a></code>, <code><a href="stats.html#topic+vcov">vcov</a></code>, <code><a href="#topic+se">se</a></code>,
<code><a href="stats.html#topic+predict">predict</a></code>, and <code><a href="#topic+gradients">gradients</a></code>, extract (or
generate) estimated values, residuals, coefficients,
variance-covariance matrix, bootstrapped standard errors on estimates,
predictions, and gradients, respectively, from the returned
object. Furthermore, the functions <code><a href="base.html#topic+summary">summary</a></code> and
<code><a href="base.html#topic+plot">plot</a></code> support objects of this type. The returned object
has the following components:
</p>
<table>
<tr><td><code>eval</code></td>
<td>
<p> evaluation points </p>
</td></tr>
<tr><td><code>mean</code></td>
<td>
<p> estimates of the regression function (conditional mean) at the
evaluation points </p>
</td></tr> 
<tr><td><code>beta</code></td>
<td>
<p> the model coefficients </p>
</td></tr>
<tr><td><code>betavcov</code></td>
<td>
<p> the asymptotic covariance matrix for the model coefficients</p>
</td></tr>
<tr><td><code>merr</code></td>
<td>
<p> standard errors of the regression function estimates </p>
</td></tr> 
<tr><td><code>grad</code></td>
<td>
<p> estimates of the gradients at each evaluation point </p>
</td></tr>
<tr><td><code>gerr</code></td>
<td>
<p> standard errors of the gradient estimates </p>
</td></tr>
<tr><td><code>mean.grad</code></td>
<td>
<p> mean (average) gradient over the evaluation points</p>
</td></tr>
<tr><td><code>mean.gerr</code></td>
<td>
<p> bootstrapped standard error of the mean gradient estimates </p>
</td></tr>
<tr><td><code>R2</code></td>
<td>
<p> if <code>method="ichimura"</code>, coefficient of determination
(Doksum and Samarov (1995))</p>
</td></tr>
<tr><td><code>MSE</code></td>
<td>
<p>if <code>method="ichimura"</code>, mean squared error </p>
</td></tr>
<tr><td><code>MAE</code></td>
<td>
<p>if <code>method="ichimura"</code>, mean absolute error </p>
</td></tr>
<tr><td><code>MAPE</code></td>
<td>
<p>if <code>method="ichimura"</code>, mean absolute percentage error </p>
</td></tr>
<tr><td><code>CORR</code></td>
<td>
<p>if <code>method="ichimura"</code>, absolute value of Pearson's correlation coefficient </p>
</td></tr>
<tr><td><code>SIGN</code></td>
<td>
<p>if <code>method="ichimura"</code>, fraction of observations where fitted and observed values
agree in sign </p>
</td></tr>
<tr><td><code>confusion.matrix</code></td>
<td>
<p>if <code>method="kleinspady"</code>, the confusion matrix or <code>NA</code> if outcomes
are not available </p>
</td></tr>
<tr><td><code>CCR.overall</code></td>
<td>
<p>if <code>method="kleinspady"</code>,  the overall correct
classification ratio, or <code>NA</code> if outcomes are not available </p>
</td></tr>
<tr><td><code>CCR.byoutcome</code></td>
<td>
<p>if <code>method="kleinspady"</code>,  a numeric vector containing the correct
classification ratio by outcome, or <code>NA</code> if outcomes are not
available </p>
</td></tr>
<tr><td><code>fit.mcfadden</code></td>
<td>
<p>if <code>method="kleinspady"</code>,  the McFadden-Puig-Kerschner performance measure
or <code>NA</code> if outcomes are not available </p>
</td></tr>
</table>


<h3>Usage Issues</h3>

<p>If you are using data of mixed types, then it is advisable to use the
<code><a href="base.html#topic+data.frame">data.frame</a></code> function to construct your input data and not
<code><a href="base.html#topic+cbind">cbind</a></code>, since <code><a href="base.html#topic+cbind">cbind</a></code> will typically not work as
intended on mixed data types and will coerce the data to the same
type.
</p>
<p><code><a href="stats.html#topic+vcov">vcov</a></code> requires that <code>gradients=TRUE</code> be set.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield  <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Aitchison, J. and C.G.G. Aitken (1976), &ldquo;Multivariate binary
discrimination by the kernel method,&rdquo; Biometrika, 63, 413-420.
</p>
<p>Doksum, K. and A. Samarov (1995), &ldquo;Nonparametric estimation of
global functionals and a measure of the explanatory power of
covariates  regression,&rdquo; The Annals of Statistics, 23 1443-1473.
</p>
<p>Ichimura, H., (1993), &ldquo;Semiparametric least squares (SLS) and
weighted SLS estimation of single-index models,&rdquo; Journal of
Econometrics, 58, 71-120.
</p>
<p>Klein, R. W. and R. H. Spady (1993), &ldquo;An efficient semiparametric
estimator for binary response models,&rdquo; Econometrica, 61, 387-421.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics:
Theory and Practice,</em> Princeton University Press.
</p>
<p>McFadden, D. and C. Puig and D. Kerschner (1977), &ldquo;Determinants
of the long-run demand for electricity,&rdquo; Proceedings of the
American Statistical Association (Business and Economics Section),
109-117. 
</p>
<p>Wang, M.C. and J. van Ryzin (1981), &ldquo;A class of smooth
estimators for discrete distributions,&rdquo; Biometrika, 68, 301-309.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# EXAMPLE 1 (INTERFACE=FORMULA): Generate a simple linear model then
# estimate it using a semiparametric single index specification and
# Ichimura's nonlinear least squares coefficients and bandwidth
# (default). Also compute the matrix of gradients and average derivative
# estimates.

set.seed(12345)

n &lt;- 100

x1 &lt;- runif(n, min=-1, max=1)
x2 &lt;- runif(n, min=-1, max=1)

y &lt;- x1 - x2 + rnorm(n)

# Note - this may take a minute or two depending on the speed of your
# computer. Note also that the first element of the vector beta is
# normalized to one for identification purposes, and that X must contain
# at least one continuous variable.

bw &lt;- npindexbw(formula=y~x1+x2)

summary(bw)

model &lt;- npindex(bws=bw, gradients=TRUE)

summary(model)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Or you can visualize the input with plot.

plot(bw)

Sys.sleep(5)

# EXAMPLE 1 (INTERFACE=DATA FRAME): Generate a simple linear model then
# estimate it using a semiparametric single index specification and
# Ichimura's nonlinear least squares coefficients and bandwidth
# (default). Also compute the matrix of gradients and average derivative
# estimates.

set.seed(12345)

n &lt;- 100

x1 &lt;- runif(n, min=-1, max=1)
x2 &lt;- runif(n, min=-1, max=1)

y &lt;- x1 - x2 + rnorm(n)

X &lt;- cbind(x1, x2)

# Note - this may take a minute or two depending on the speed of your
# computer. Note also that the first element of the vector beta is
# normalized to one for identification purposes, and that X must contain
# at least one continuous variable.

bw &lt;- npindexbw(xdat=X, ydat=y)

summary(bw)

model &lt;- npindex(bws=bw, gradients=TRUE)

summary(model)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Or you can visualize the input with plot.

plot(bw)

Sys.sleep(5)

# EXAMPLE 2 (INTERFACE=FORMULA): Generate a simple binary outcome linear
# model then estimate it using a semiparametric single index
# specification and Klein and Spady's likelihood-based coefficients and
# bandwidth (default). Also compute the matrix of gradients and average
# derivative estimates.

n &lt;- 100

x1 &lt;- runif(n, min=-1, max=1)
x2 &lt;- runif(n, min=-1, max=1)

y &lt;- ifelse(x1 + x2 + rnorm(n) &gt; 0, 1, 0)

# Note that the first element of the vector beta is normalized to one
# for identification purposes, and that X must contain at least one
# continuous variable.

bw &lt;- npindexbw(formula=y~x1+x2, method="kleinspady")

summary(bw)

model &lt;- npindex(bws=bw, gradients=TRUE)

# Note that, since the outcome is binary, we can assess model
# performance using methods appropriate for binary outcomes. We look at
# the confusion matrix, various classification ratios, and McFadden et
# al's measure of predictive performance.

summary(model)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# EXAMPLE 2 (INTERFACE=DATA FRAME): Generate a simple binary outcome
# linear model then estimate it using a semiparametric single index
# specification and Klein and Spady's likelihood-based coefficients and
# bandwidth (default). Also compute the matrix of gradients and average
# derivative estimates.

n &lt;- 100

x1 &lt;- runif(n, min=-1, max=1)
x2 &lt;- runif(n, min=-1, max=1)

y &lt;- ifelse(x1 + x2 + rnorm(n) &gt; 0, 1, 0)

X &lt;- cbind(x1, x2)

# Note that the first element of the vector beta is normalized to one
# for identification purposes, and that X must contain at least one
# continuous variable.

bw &lt;- npindexbw(xdat=X, ydat=y, method="kleinspady")

summary(bw)

model &lt;- npindex(bws=bw, gradients=TRUE)

# Note that, since the outcome is binary, we can assess model
# performance using methods appropriate for binary outcomes. We look at
# the confusion matrix, various classification ratios, and McFadden et
# al's measure of predictive performance.

summary(model)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# EXAMPLE 3 (INTERFACE=FORMULA): Replicate the DGP of Klein &amp; Spady
# (1993) (see their description on page 405, pay careful attention to
# footnote 6 on page 405).

set.seed(123)

n &lt;- 1000

# x1 is chi-squared having 3 df truncated at 6 standardized by
# subtracting 2.348 and dividing by 1.511

x &lt;- rchisq(n, df=3)
x1 &lt;- (ifelse(x &lt; 6, x, 6) - 2.348)/1.511

# x2 is normal (0, 1) truncated at +- 2 divided by 0.8796

x &lt;- rnorm(n)
x2 &lt;- ifelse(abs(x) &lt; 2 , x, 2) / 0.8796

# y is 1 if y* &gt; 0, 0 otherwise.

y &lt;- ifelse(x1 + x2 + rnorm(n) &gt; 0, 1, 0)

# Compute the parameter vector and bandwidth. Note that the first
# element of the vector beta is normalized to one for identification
# purposes, and that X must contain at least one continuous variable.


bw &lt;- npindexbw(formula=y~x1+x2, method="kleinspady")

# Next, create the evaluation data in order to generate a perspective
# plot

# Create an evaluation data matrix

x1.seq &lt;- seq(min(x1), max(x1), length=50)
x2.seq &lt;- seq(min(x2), max(x2), length=50)
X.eval &lt;- expand.grid(x1=x1.seq, x2=x2.seq)

# Now evaluate the single index model on the evaluation data

fit &lt;- fitted(npindex(exdat=X.eval, 
               eydat=rep(1, nrow(X.eval)), 
               bws=bw))

# Finally, coerce the fitted model into a matrix suitable for 3D
# plotting via persp()

fit.mat &lt;- matrix(fit, 50, 50)

# Generate a perspective plot similar to Figure 2 b of Klein and Spady
# (1993)

persp(x1.seq, 
      x2.seq, 
      fit.mat, 
      col="white", 
      ticktype="detailed", 
      expand=0.5, 
      axes=FALSE, 
      box=FALSE, 
      main="Estimated Semiparametric Probability Perspective", 
      theta=310, 
      phi=25)

# EXAMPLE 3 (INTERFACE=DATA FRAME): Replicate the DGP of Klein &amp; Spady
# (1993) (see their description on page 405, pay careful attention to
# footnote 6 on page 405).

set.seed(123)

n &lt;- 1000

# x1 is chi-squared having 3 df truncated at 6 standardized by
# subtracting 2.348 and dividing by 1.511

x &lt;- rchisq(n, df=3)
x1 &lt;- (ifelse(x &lt; 6, x, 6) - 2.348)/1.511

# x2 is normal (0, 1) truncated at +- 2 divided by 0.8796

x &lt;- rnorm(n)
x2 &lt;- ifelse(abs(x) &lt; 2 , x, 2) / 0.8796

# y is 1 if y* &gt; 0, 0 otherwise.

y &lt;- ifelse(x1 + x2 + rnorm(n) &gt; 0, 1, 0)

# Create the X matrix

X &lt;- cbind(x1, x2)

# Compute the parameter vector and bandwidth. Note that the first
# element of the vector beta is normalized to one for identification
# purposes, and that X must contain at least one continuous variable.


bw &lt;- npindexbw(xdat=X, ydat=y, method="kleinspady")

# Next, create the evaluation data in order to generate a perspective
# plot

# Create an evaluation data matrix

x1.seq &lt;- seq(min(x1), max(x1), length=50)
x2.seq &lt;- seq(min(x2), max(x2), length=50)
X.eval &lt;- expand.grid(x1=x1.seq, x2=x2.seq)

# Now evaluate the single index model on the evaluation data

fit &lt;- fitted(npindex(exdat=X.eval, 
               eydat=rep(1, nrow(X.eval)), 
               bws=bw))

# Finally, coerce the fitted model into a matrix suitable for 3D
# plotting via persp()

fit.mat &lt;- matrix(fit, 50, 50)

# Generate a perspective plot similar to Figure 2 b of Klein and Spady
# (1993)

persp(x1.seq, 
      x2.seq, 
      fit.mat, 
      col="white", 
      ticktype="detailed", 
      expand=0.5, 
      axes=FALSE, 
      box=FALSE, 
      main="Estimated Semiparametric Probability Perspective", 
      theta=310, 
      phi=25)

## End(Not run) 
</code></pre>

<hr>
<h2 id='npindexbw'>Semiparametric Single Index Model Parameter and Bandwidth Selection</h2><span id='topic+npindexbw'></span><span id='topic+npindexbw.NULL'></span><span id='topic+npindexbw.default'></span><span id='topic+npindexbw.formula'></span><span id='topic+npindexbw.sibandwidth'></span>

<h3>Description</h3>

<p><code>npindexbw</code> computes a <code>npindexbw</code> bandwidth specification
using the model <code class="reqn">Y = G(X\beta) + \epsilon</code>. For continuous <code class="reqn">Y</code>, the approach is that of Hardle, Hall
and Ichimura (1993) which jointly minimizes a least-squares
cross-validation function with respect to the parameters and
bandwidth. For binary <code class="reqn">Y</code>, a likelihood-based cross-validation
approach is employed which jointly maximizes a likelihood
cross-validation function with respect to the parameters and
bandwidth. The bandwidth object contains parameters for the single
index model and the (scalar) bandwidth for the index function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npindexbw(...)

## S3 method for class 'formula'
npindexbw(formula, data, subset, na.action, call, ...)

## S3 method for class 'NULL'
npindexbw(xdat = stop("training data xdat missing"),
          ydat = stop("training data ydat missing"),
          bws,
          ...)

## Default S3 method:
npindexbw(xdat = stop("training data xdat missing"),
          ydat = stop("training data ydat missing"),
          bws,
          bandwidth.compute = TRUE,
          nmulti,
          random.seed,
          optim.method,
          optim.maxattempts,
          optim.reltol,
          optim.abstol,
          optim.maxit,
          only.optimize.beta,
          ...)

## S3 method for class 'sibandwidth'
npindexbw(xdat = stop("training data xdat missing"),
          ydat = stop("training data ydat missing"),
          bws,
          bandwidth.compute = TRUE,
          nmulti,
          random.seed = 42,
          optim.method = c("Nelder-Mead", "BFGS", "CG"),
          optim.maxattempts = 10,
          optim.reltol = sqrt(.Machine$double.eps),
          optim.abstol = .Machine$double.eps,
          optim.maxit = 500,
          only.optimize.beta = FALSE,
          ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npindexbw_+3A_formula">formula</code></td>
<td>

<p>a symbolic description of variables on which bandwidth selection is
to be performed. The details of constructing a formula are
described below.
</p>
</td></tr>
<tr><td><code id="npindexbw_+3A_data">data</code></td>
<td>

<p>an optional data frame, list or environment (or object
coercible to a data frame by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code>) containing
the variables 
in the model. If not found in data, the variables are taken from
<code>environment(formula)</code>, typically the environment from which the
function is called.
</p>
</td></tr>
<tr><td><code id="npindexbw_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations to be used in
the fitting process. 
</p>
</td></tr>
<tr><td><code id="npindexbw_+3A_na.action">na.action</code></td>
<td>

<p>a function which indicates what should happen when the data contain
<code>NA</code>s. The default is set by the
<code><a href="stats.html#topic+na.action">na.action</a></code> setting of options, and is
<code><a href="stats.html#topic+na.fail">na.fail</a></code> if that is unset. The (recommended) default is
<code><a href="stats.html#topic+na.omit">na.omit</a></code>.  
</p>
</td></tr>
<tr><td><code id="npindexbw_+3A_call">call</code></td>
<td>

<p>the original function call. This is passed internally by
<code><a href="#topic+np">np</a></code> when a bandwidth search has been implied by a call to
another function. It is not recommended that the user set this.
</p>
</td></tr>
<tr><td><code id="npindexbw_+3A_xdat">xdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of explanatory data (training data) used to
calculate the regression estimators.
</p>
</td></tr>
<tr><td><code id="npindexbw_+3A_ydat">ydat</code></td>
<td>

<p>a one (1) dimensional numeric or integer vector of dependent data, each
element <code class="reqn">i</code> corresponding to each observation (row) <code class="reqn">i</code> of
<code>xdat</code>. 
</p>
</td></tr>
<tr><td><code id="npindexbw_+3A_bws">bws</code></td>
<td>

<p>a bandwidth specification. This can be set as a
<code>singleindexbandwidth</code>
object returned from an invocation of <code>npindexbw</code>, or
as a vector of parameters (beta) with each element <code class="reqn">i</code>
corresponding to the coefficient for column <code class="reqn">i</code> in <code>xdat</code>
where the first element is normalized to 1, and a scalar bandwidth
(h). If specified as a vector, then additional arguments will need
to be supplied as necessary to specify the bandwidth type, kernel
types, and so on.
</p>
</td></tr>
<tr><td><code id="npindexbw_+3A_method">method</code></td>
<td>

<p>the single index model method, one of either &ldquo;ichimura&rdquo;
(Ichimura (1993)) or &ldquo;kleinspady&rdquo; (Klein and Spady
(1993)). Defaults to
<code>ichimura</code>.
</p>
</td></tr>
<tr><td><code id="npindexbw_+3A_nmulti">nmulti</code></td>
<td>

<p>integer number of times to restart the process of finding extrema of
the cross-validation function from different (random) initial
points. Defaults to <code>min(5,ncol(xdat))</code>.
</p>
</td></tr>
<tr><td><code id="npindexbw_+3A_random.seed">random.seed</code></td>
<td>

<p>an integer used to seed R's random number generator. This ensures
replicability of the numerical search. Defaults to 42.
</p>
</td></tr>
<tr><td><code id="npindexbw_+3A_bandwidth.compute">bandwidth.compute</code></td>
<td>

<p>a logical value which specifies whether to do a numerical search for
bandwidths or not. If set to <code>FALSE</code>, a <code>bandwidth</code> object
will be returned with bandwidths set to those specified
in <code>bws</code>. Defaults to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npindexbw_+3A_optim.method">optim.method</code></td>
<td>
<p> method used by <code><a href="stats.html#topic+optim">optim</a></code> for minimization of
the objective function. See <code>?optim</code> for references. Defaults
to <code>"Nelder-Mead"</code>.
</p>
<p>the default method is an implementation of that of Nelder and Mead
(1965), that uses only function values and is robust but relatively
slow.  It will work reasonably well for non-differentiable
functions.
</p>
<p>method <code>"BFGS"</code> is a quasi-Newton method (also known as a
variable metric algorithm), specifically that published
simultaneously in 1970 by Broyden, Fletcher, Goldfarb and Shanno.
This uses function values and gradients to build up a picture of the
surface to be optimized.
</p>
<p>method <code>"CG"</code> is a conjugate gradients method based
on that by Fletcher and Reeves (1964) (but with the option of
Polak-Ribiere or Beale-Sorenson updates).  Conjugate gradient
methods will generally be more fragile than the BFGS method, but as
they do not store a matrix they may be successful in much larger
optimization problems.
</p>
</td></tr>
<tr><td><code id="npindexbw_+3A_optim.maxattempts">optim.maxattempts</code></td>
<td>

<p>maximum number of attempts taken trying to achieve successful
convergence in <code><a href="stats.html#topic+optim">optim</a></code>. Defaults to <code>100</code>.
</p>
</td></tr>
<tr><td><code id="npindexbw_+3A_optim.abstol">optim.abstol</code></td>
<td>

<p>the absolute convergence tolerance used by <code><a href="stats.html#topic+optim">optim</a></code>. Only useful
for non-negative functions, as a tolerance for reaching
zero. Defaults to <code>.Machine$double.eps</code>.
</p>
</td></tr>
<tr><td><code id="npindexbw_+3A_optim.reltol">optim.reltol</code></td>
<td>

<p>relative convergence tolerance used by <code><a href="stats.html#topic+optim">optim</a></code>.  The algorithm
stops if it is unable to reduce the value by a factor of 'reltol *
(abs(val) + reltol)' at a step.  Defaults to
<code>sqrt(.Machine$double.eps)</code>, typically about <code>1e-8</code>.
</p>
</td></tr>
<tr><td><code id="npindexbw_+3A_optim.maxit">optim.maxit</code></td>
<td>

<p>maximum number of iterations used by <code><a href="stats.html#topic+optim">optim</a></code>. Defaults
to <code>500</code>.
</p>
</td></tr>
<tr><td><code id="npindexbw_+3A_only.optimize.beta">only.optimize.beta</code></td>
<td>

<p>signals the routine to only minimize the objective function with
respect to beta
</p>
</td></tr>
<tr><td><code id="npindexbw_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to specify the parameters to the
<code>sibandwidth</code> S3 method, which is called during the numerical
search. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>We implement Ichimura's (1993) method via
joint estimation of the bandwidth and coefficient vector using
leave-one-out nonlinear least squares. We implement Klein and Spady's
(1993) method maximizing the leave-one-out log likelihood function
jointly with respect to the bandwidth and coefficient vector. Note
that Klein and Spady's (1993) method is for <em>binary outcomes
only</em>, while Ichimura's (1993) method can be applied for any outcome
data type (i.e., continuous or discrete).
</p>
<p>We impose the identification condition that the first element of the
coefficient vector beta is equal to one, while identification also
requires that the explanatory variables contain <em>at least one</em>
continuous variable.
</p>
<p><code>npindexbw</code> may be invoked <em>either</em> with a formula-like
symbolic description of variables on which bandwidth selection is to
be performed <em>or</em> through a simpler interface whereby data is
passed directly to the function via the <code>xdat</code> and <code>ydat</code>
parameters. Use of these two interfaces is <b>mutually exclusive</b>.
</p>
<p>Note that, unlike most other bandwidth methods in the <code>np</code>
package, this implementation uses the R <code><a href="stats.html#topic+optim">optim</a></code> nonlinear
minimization routines and <code><a href="#topic+npksum">npksum</a></code>. We have implemented
multistarting and strongly encourage its use in practice. For
exploratory purposes, you may wish to override the default search
tolerances, say, setting <code>optim.reltol=.1</code> and conduct
multistarting (the default is to restart min(5, ncol(xdat)) times) as is done
for a number of examples.
</p>
<p>Data for which bandwidths are to be estimated may be specified
symbolically. A typical description has the form <code>dependent data
  ~ explanatory data</code>, where <code>dependent data</code> is a univariate
response, and <code>explanatory data</code> is a series of variables
specified by name, separated by the separation character '+'. For
example <code> y1 ~ x1 + x2 </code> specifies that the bandwidth object for
the regression of response <code>y1</code> and semiparametric regressors
<code>x1</code> and <code>x2</code> are to be estimated.  See below for further
examples.
</p>


<h3>Value</h3>

<p><code>npindexbw</code> returns a <code>sibandwidth</code> object, with the
following components:
</p>
<table>
<tr><td><code>bw</code></td>
<td>
<p> bandwidth(s), scale factor(s) or nearest neighbours for the
data, <code>xdat</code> </p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p> coefficients of the model </p>
</td></tr>
<tr><td><code>fval</code></td>
<td>
<p> objective function value at minimum </p>
</td></tr>
</table>
<p>If <code>bwtype</code> is set to <code>fixed</code>, an object containing a scalar
bandwidth for the function <code class="reqn">G(X\beta)</code> and an estimate of
the parameter vector <code class="reqn">\beta</code> is returned.
</p>
<p>If <code>bwtype</code> is set to <code>generalized_nn</code> or
<code>adaptive_nn</code>, then instead the scalar <code class="reqn">k</code>th nearest neighbor
is returned.
</p>
<p>The functions <code><a href="stats.html#topic+coef">coef</a></code>, <code><a href="stats.html#topic+predict">predict</a></code>,
<code><a href="base.html#topic+summary">summary</a></code>, and  <code><a href="base.html#topic+plot">plot</a></code> support
objects of this class.
</p>


<h3>Usage Issues</h3>

<p>If you are using data of mixed types, then it is advisable to use the
<code><a href="base.html#topic+data.frame">data.frame</a></code> function to construct your input data and not
<code><a href="base.html#topic+cbind">cbind</a></code>, since <code><a href="base.html#topic+cbind">cbind</a></code> will typically not work as
intended on mixed data types and will coerce the data to the same
type.
</p>
<p>Caution: multivariate data-driven bandwidth selection methods are, by
their nature, <em>computationally intensive</em>. Virtually all methods
require dropping the <code class="reqn">i</code>th observation from the data set,
computing an object, repeating this for all observations in the
sample, then averaging each of these leave-one-out estimates for a
<em>given</em> value of the bandwidth vector, and only then repeating
this a large number of times in order to conduct multivariate
numerical minimization/maximization. Furthermore, due to the potential
for local minima/maxima, <em>restarting this procedure a large
number of times may often be necessary</em>. This can be frustrating for
users possessing large datasets. For exploratory purposes, you may
wish to override the default search tolerances, say, setting
<code>optim.reltol=.1</code> and conduct multistarting (the default is to
restart min(5, ncol(xdat)) times). Once the procedure terminates, you can
restart search with default tolerances using those bandwidths obtained
from the less rigorous search (i.e., set <code>bws=bw</code> on subsequent
calls to this routine where <code>bw</code> is the initial bandwidth
object).  A version of this package using the <code>Rmpi</code> wrapper is
under development that allows one to deploy this software in a
clustered computing environment to facilitate computation involving
large datasets.  </p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Aitchison, J. and C.G.G. Aitken (1976), &ldquo;Multivariate binary
discrimination by the kernel method,&rdquo; Biometrika, 63, 413-420.
</p>
<p>Hardle, W. and P. Hall and H. Ichimura (1993), &ldquo;Optimal
Smoothing in Single-Index Models,&rdquo; The Annals of Statistics, 21,
157-178.
</p>
<p>Ichimura, H., (1993), &ldquo;Semiparametric least squares (SLS) and
weighted SLS estimation of single-index models,&rdquo; Journal of
Econometrics, 58, 71-120.
</p>
<p>Klein, R. W. and R. H. Spady (1993), &ldquo;An efficient semiparametric
estimator for binary response models,&rdquo; Econometrica, 61, 387-421.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics:
Theory and Practice,</em> Princeton University Press.
</p>
<p>Wang, M.C. and J. van Ryzin (1981), &ldquo;A class of smooth
estimators for discrete distributions,&rdquo; Biometrika, 68, 301-309.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# EXAMPLE 1 (INTERFACE=FORMULA): Generate a simple linear model then
# compute coefficients and the bandwidth using Ichimura's nonlinear
# least squares approach.

set.seed(12345)

n &lt;- 100

x1 &lt;- runif(n, min=-1, max=1)
x2 &lt;- runif(n, min=-1, max=1)

y &lt;- x1 - x2 + rnorm(n)

# Note - this may take a minute or two depending on the speed of your
# computer. Note also that the first element of the vector beta is
# normalized to one for identification purposes, and that X must contain
# at least one continuous variable.

bw &lt;- npindexbw(formula=y~x1+x2, method="ichimura")

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# EXAMPLE 1 (INTERFACE=DATA FRAME): Generate a simple linear model then
# compute coefficients and the bandwidth using Ichimura's nonlinear
# least squares approach.

set.seed(12345)

n &lt;- 100

x1 &lt;- runif(n, min=-1, max=1)
x2 &lt;- runif(n, min=-1, max=1)

y &lt;- x1 - x2 + rnorm(n)

X &lt;- cbind(x1, x2)

# Note - this may take a minute or two depending on the speed of your
# computer. Note also that the first element of the vector beta is
# normalized to one for identification purposes, and that X must contain
# at least one continuous variable.

bw &lt;- npindexbw(xdat=X, ydat=y, method="ichimura")

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# EXAMPLE 2 (INTERFACE=DATA FRAME): Generate a simple binary outcome
# model then compute coefficients and the bandwidth using Klein and
# Spady's likelihood-based approach.

n &lt;- 100

x1 &lt;- runif(n, min=-1, max=1)
x2 &lt;- runif(n, min=-1, max=1)

y &lt;- ifelse(x1 + x2 + rnorm(n) &gt; 0, 1, 0)

# Note that the first element of the vector beta is normalized to one
# for identification purposes, and that X must contain at least one
# continuous variable.

bw &lt;- npindexbw(formula=y~x1+x2, method="kleinspady")

summary(bw)

# EXAMPLE 2 (INTERFACE=DATA FRAME): Generate a simple binary outcome
# model then compute coefficients and the bandwidth using Klein and
# Spady's likelihood-based approach.

n &lt;- 100

x1 &lt;- runif(n, min=-1, max=1)
x2 &lt;- runif(n, min=-1, max=1)

y &lt;- ifelse(x1 + x2 + rnorm(n) &gt; 0, 1, 0)

X &lt;- cbind(x1, x2)

# Note that the first element of the vector beta is normalized to one
# for identification purposes, and that X must contain at least one
# continuous variable.

bw &lt;- npindexbw(xdat=X, ydat=y, method="kleinspady")

summary(bw)

## End(Not run) 
</code></pre>

<hr>
<h2 id='npksum'> Kernel Sums with Mixed Data Types </h2><span id='topic+npksum'></span><span id='topic+npksum.default'></span><span id='topic+npksum.formula'></span><span id='topic+npksum.numeric'></span>

<h3>Description</h3>

<p><code>npksum</code> computes kernel sums on evaluation
data, given a set of training data, data to be weighted (optional), and a
bandwidth specification (any bandwidth object).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npksum(...)

## S3 method for class 'formula'
npksum(formula, data, newdata, subset, na.action, ...)

## Default S3 method:
npksum(bws,
       txdat = stop("training data 'txdat' missing"),
       tydat = NULL,
       exdat = NULL,
       weights = NULL,
       leave.one.out = FALSE,
       kernel.pow = 1.0,
       bandwidth.divide = FALSE,
       operator = names(ALL_OPERATORS),
       permutation.operator = names(PERMUTATION_OPERATORS),
       compute.score = FALSE,
       compute.ocg = FALSE,
       return.kernel.weights = FALSE,
       ...)

## S3 method for class 'numeric'
npksum(bws,
       txdat = stop("training data 'txdat' missing"),
       tydat,
       exdat,
       weights,
       leave.one.out,
       kernel.pow,
       bandwidth.divide,
       operator,
       permutation.operator,
       compute.score,
       compute.ocg,
       return.kernel.weights,
       ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npksum_+3A_formula">formula</code></td>
<td>

<p>a symbolic description of variables on which the sum is
to be performed. The details of constructing a formula are
described below.
</p>
</td></tr>
<tr><td><code id="npksum_+3A_data">data</code></td>
<td>

<p>an optional data frame, list or environment (or object
coercible to a data frame by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code>) containing the variables
in the model. If not found in data, the variables are taken from
<code>environment(formula)</code>, typically the environment from which the
function is called.
</p>
</td></tr>
<tr><td><code id="npksum_+3A_newdata">newdata</code></td>
<td>

<p>An optional data frame in which to look for evaluation data. If
omitted, <code>data</code> is used.
</p>
</td></tr>
<tr><td><code id="npksum_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations to be used. 
</p>
</td></tr>
<tr><td><code id="npksum_+3A_na.action">na.action</code></td>
<td>

<p>a function which indicates what should happen when the data contain
<code>NA</code>s. The default is set by the <code><a href="stats.html#topic+na.action">na.action</a></code> setting of options, and is
<code><a href="stats.html#topic+na.fail">na.fail</a></code> if that is unset. The (recommended) default is
<code><a href="stats.html#topic+na.omit">na.omit</a></code>.  
</p>
</td></tr>
<tr><td><code id="npksum_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to specify the parameters to the
<code>default</code> S3 method, which is called during estimation. 
</p>
</td></tr>
<tr><td><code id="npksum_+3A_txdat">txdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of sample realizations (training data) used to
compute the sum.
</p>
</td></tr>
<tr><td><code id="npksum_+3A_tydat">tydat</code></td>
<td>

<p>a numeric vector of data to be weighted. The <code class="reqn">i</code>th kernel weight
is applied to the <code class="reqn">i</code>th element. Defaults to  <code>1</code>.
</p>
</td></tr>
<tr><td><code id="npksum_+3A_exdat">exdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of sum evaluation points (if omitted,
defaults to the training data itself).
</p>
</td></tr>
<tr><td><code id="npksum_+3A_bws">bws</code></td>
<td>

<p>a bandwidth specification. This can be set as any suitable bandwidth
object returned from a bandwidth-generating function, or a numeric vector.
</p>
</td></tr>
<tr><td><code id="npksum_+3A_weights">weights</code></td>
<td>

<p>a <code class="reqn">n</code> by <code class="reqn">q</code>  matrix of weights which can optionally be
applied to <code>tydat</code> in the sum. See details.
</p>
</td></tr>
<tr><td><code id="npksum_+3A_leave.one.out">leave.one.out</code></td>
<td>

<p>a logical value to specify whether or not to compute the leave one
out sums. Will not work if <code>exdat</code> is specified. Defaults to
<code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="npksum_+3A_kernel.pow">kernel.pow</code></td>
<td>

<p>an integer specifying the power to which the kernels will be raised
in the sum. Defaults to <code>1</code>.
</p>
</td></tr>
<tr><td><code id="npksum_+3A_bandwidth.divide">bandwidth.divide</code></td>
<td>

<p>a logical specifying whether or not to divide continuous kernel
weights by their bandwidths. Use this with nearest-neighbor
methods. Defaults to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="npksum_+3A_operator">operator</code></td>
<td>

<p>a string specifying whether the <code>normal</code>, <code>convolution</code>,
<code>derivative</code>, or <code>integral</code> kernels are to be
used. Operators scale results by factors of <code class="reqn">h</code> or <code class="reqn">1/h</code> where
appropriate. Defaults to <code>normal</code> and applies to all elements in a
multivariate object. See details.
</p>
</td></tr>
<tr><td><code id="npksum_+3A_permutation.operator">permutation.operator</code></td>
<td>

<p>a string which can have a value of <code>none</code>, <code>normal</code>,
<code>derivative</code>, or <code>integral</code>. If set to something other
than <code>none</code> (the default), then a separate result will be returned for each
term in the product kernel, with the operator applied to that term.
Permutation operators scale results by factors of <code class="reqn">h</code> or <code class="reqn">1/h</code> where
appropriate. This is useful for computing gradients, for example.
</p>
</td></tr>
<tr><td><code id="npksum_+3A_compute.score">compute.score</code></td>
<td>

<p>a logical specifying whether or not to return the score
(the &lsquo;grad h&rsquo; terms) for each dimension in addition to the kernel
sum. Cannot be <code>TRUE</code> if a permutation operator other than
<code>"none"</code> is selected. 
</p>
</td></tr>
<tr><td><code id="npksum_+3A_compute.ocg">compute.ocg</code></td>
<td>

<p>a logical specifying whether or not to return a separate result for
each unordered and ordered dimension, where the product kernel term
for that dimension is evaluated at an appropriate reference
category. This is used primarily in <code>np</code> to compute ordered and
unordered categorical gradients. See details.
</p>
</td></tr>
<tr><td><code id="npksum_+3A_return.kernel.weights">return.kernel.weights</code></td>
<td>

<p>a logical specifying whether or not to return the matrix of
generalized product kernel weights. Defaults to <code>FALSE</code>. See
details.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>npksum</code>
exists so that you can create your own kernel objects with
or without a variable to be weighted (default <code class="reqn">Y=1</code>). With the options
available, you could create new nonparametric tests or even new kernel
estimators. The convolution kernel option would allow you to create,
say, the least squares cross-validation function for kernel density
estimation.
</p>
<p><code>npksum</code> uses highly-optimized C code that strives to minimize
its &lsquo;memory footprint&rsquo;, while there is low overhead involved
when using repeated calls to this function (see, by way of
illustration, the example below that conducts leave-one-out
cross-validation for a local constant regression estimator via calls
to the <code>R</code> function <code><a href="stats.html#topic+nlm">nlm</a></code>, and compares this to the
<code><a href="#topic+npregbw">npregbw</a></code> function).
</p>
<p><code>npksum</code> implements a variety of methods for computing
multivariate kernel sums (<code class="reqn">p</code>-variate) defined over a set of
possibly continuous and/or discrete (unordered, ordered) data. The
approach is based on Li and Racine (2003) who employ
&lsquo;generalized product kernels&rsquo; that admit a mix of continuous
and discrete data types.
</p>
<p>Three classes of kernel estimators for the continuous data types are
available: fixed, adaptive nearest-neighbor, and generalized
nearest-neighbor. Adaptive nearest-neighbor bandwidths change with
each sample realization in the set, <code class="reqn">x_i</code>, when estimating
the kernel sum at the point <code class="reqn">x</code>. Generalized nearest-neighbor
bandwidths change with the point at which the sum is computed,
<code class="reqn">x</code>. Fixed bandwidths are constant over the support of <code class="reqn">x</code>.
</p>
<p><code>npksum</code> computes <code class="reqn">\sum_{j=1}^{n}{W_j^\prime Y_j
  K(X_j)}</code>, where <code class="reqn">W_j</code>
represents a row vector extracted from <code class="reqn">W</code>.  That is, it computes
the kernel weighted sum of the outer product of the rows of <code class="reqn">W</code>
and <code class="reqn">Y</code>. In the examples, the uses of such sums are illustrated.
</p>
<p><code>npksum</code> may be invoked <em>either</em> with a formula-like
symbolic 
description of variables on which the sum is to be
performed <em>or</em> through a simpler interface whereby data is passed
directly to the function via the <code>txdat</code> and <code>tydat</code>
parameters. Use of these two interfaces is <b>mutually exclusive</b>.
</p>
<p>Data contained in the data frame <code>txdat</code> (and also <code>exdat</code>)
may be a mix of continuous (default), unordered discrete (to be
specified in the data frame <code>txdat</code> using the
<code><a href="base.html#topic+factor">factor</a></code> command), and ordered discrete (to be specified
in the data frame <code>txdat</code> using the <code><a href="base.html#topic+ordered">ordered</a></code>
command). Data can be entered in an arbitrary order and data types
will be detected automatically by the routine (see <code><a href="#topic+np">np</a></code>
for details).
</p>
<p>Data for which bandwidths are to be estimated may be specified
symbolically. A typical description has the form <code>dependent data
  ~ explanatory data</code>, where <code>dependent data</code> and <code>explanatory
  data</code> are both series of variables specified by name, separated by the
separation character '+'. For example, <code> y1 ~ x1 + x2 </code> specifies
that <code>y1</code> is to be kernel-weighted by <code>x1</code> and <code>x2</code>
throughout the sum. See below for further examples.
</p>
<p>A variety of kernels may be specified by the user. Kernels implemented
for continuous data types include the second, fourth, sixth, and
eighth order Gaussian and Epanechnikov kernels, and the uniform
kernel. Unordered discrete data types use a variation on Aitchison and
Aitken's (1976) kernel, while ordered data types use a variation of
the Wang and van Ryzin (1981) kernel (see <code><a href="#topic+np">np</a></code> for
details).
</p>
<p>The option <code>operator=</code> can be used to &lsquo;mix and match&rsquo;
operator strings to create a &lsquo;hybrid&rsquo; kernel provided they
match the dimension of the data. For example, for a two-dimensional
data frame of <a href="base.html#topic+numeric">numeric</a> datatypes,
<code>operator=c("normal","derivative")</code> will use the normal
(i.e. PDF) kernel for variable one and the derivative of the PDF
kernel for variable two. Please note that applying operators will scale the
results by factors of <code class="reqn">h</code> or <code class="reqn">1/h</code> where appropriate.
</p>
<p>The option <code>permutation.operator=</code> can be used to &lsquo;mix and match&rsquo;
operator strings to create a &lsquo;hybrid&rsquo; kernel, in addition to
the kernel sum with no operators applied, one for each continuous
dimension in the data. For example, for a two-dimensional
data frame of <a href="base.html#topic+numeric">numeric</a> datatypes,
<code>permutation.operator=c("derivative")</code> will return the usual
kernel sum as if <code>operator = c("normal","normal")</code> in the
<code>ksum</code> member, and in the <code>p.ksum</code> member, it will return
kernel sums for <code>operator = c("derivative","normal")</code>, and
<code>operator = c("normal","derivative")</code>. This makes the computation
of gradients much easier.
</p>
<p>The option <code>compute.score=</code> can be used to compute the gradients
with respect to <code class="reqn">h</code> in addition to the normal kernel sum. Like
permutations, the additional results are returned in the
<code>p.ksum</code>. This option does not work in conjunction with
<code>permutation.operator</code>.
</p>
<p>The option <code>compute.ocg=</code> works much like <code>permutation.operator</code>,
but for discrete variables. The kernel is evaluated at a reference
category in each dimension: for ordered data, the next lowest category
is selected, except in the case of the lowest category, where the
second lowest category is selected; for unordered data, the first
category is selected. These additional data are returned in the
<code>p.ksum</code> member. This option can be set simultaneously with
<code>permutation.operator</code>.
</p>
<p>The option <code>return.kernel.weights=TRUE</code> returns a matrix of
dimension &lsquo;number of training observations&rsquo; by &lsquo;number
of evaluation observations&rsquo; and contains only the generalized product
kernel weights ignoring all other objects and options that may be
provided to <code>npksum</code> (e.g. <code>bandwidth.divide=TRUE</code> will be
ignored, etc.). Summing the columns of the weight matrix and dividing
by &lsquo;number of training observations&rsquo; times the product of the
bandwidths (i.e. <code><a href="base.html#topic+colMeans">colMeans</a>(foo$kw)/prod(h)</code>) would produce
the kernel estimator of a (multivariate) density
(<code>operator="normal"</code>) or multivariate cumulative distribution
(<code>operator="integral"</code>).
</p>


<h3>Value</h3>

<p><code>npksum</code> returns a <code>npkernelsum</code> object
with the following components:
</p>
<table>
<tr><td><code>eval</code></td>
<td>
<p> the evaluation points </p>
</td></tr>
<tr><td><code>ksum</code></td>
<td>
<p> the sum at the evaluation points </p>
</td></tr>
<tr><td><code>kw</code></td>
<td>
<p> the kernel weights (when <code>return.kernel.weights=TRUE</code>
is specified)</p>
</td></tr>
</table>


<h3>Usage Issues</h3>

<p>If you are using data of mixed types, then it is advisable to use the
<code><a href="base.html#topic+data.frame">data.frame</a></code> function to construct your input data and not
<code><a href="base.html#topic+cbind">cbind</a></code>, since <code><a href="base.html#topic+cbind">cbind</a></code> will typically not work as
intended on mixed data types and will coerce the data to the same
type.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine
<a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Aitchison, J. and C.G.G. Aitken (1976), &ldquo; Multivariate binary
discrimination by the kernel method,&rdquo; Biometrika, 63, 413-420.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics: Theory
and Practice,</em> Princeton University Press.
</p>
<p>Li, Q. and J.S. Racine (2003), &ldquo;Nonparametric estimation of
distributions with categorical and continuous data,&rdquo; Journal
of Multivariate Analysis, 86, 266-292.
</p>
<p>Pagan, A. and A. Ullah (1999), <em>Nonparametric Econometrics,</em> Cambridge
University Press. 
</p>
<p>Scott, D.W. (1992), <em>Multivariate Density Estimation. Theory,
Practice and Visualization,</em> New York: Wiley.
</p>
<p>Silverman, B.W. (1986), <em>Density Estimation,</em> London: Chapman and
Hall.
</p>
<p>Wang, M.C. and J. van Ryzin (1981), &ldquo;A class of smooth
estimators for discrete distributions,&rdquo; Biometrika, 68, 301-309.  </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# EXAMPLE 1: For this example, we generate 100,000 observations from a
# N(0, 1) distribution, then evaluate the kernel density on a grid of 50
# equally spaced points using the npksum() function, then compare
# results with the (identical) npudens() function output

n &lt;- 100000
x &lt;- rnorm(n)
x.eval &lt;- seq(-4, 4, length=50)

# Compute the bandwidth with the normal-reference rule-of-thumb

bw &lt;- npudensbw(dat=x, bwmethod="normal-reference")

# Compute the univariate kernel density estimate using the 100,000
# training data points evaluated on the 50 evaluation data points, 
# i.e., 1/nh times the sum of the kernel function evaluated at each of
# the 50 points

den.ksum &lt;- npksum(txdat=x, exdat=x.eval, bws=bw$bw)$ksum/(n*bw$bw[1])

# Note that, alternatively (easier perhaps), you could use the
# bandwidth.divide=TRUE argument and drop the *bw$bw[1] term in the
# denominator, as in
# npksum(txdat=x, exdat=x.eval, bws=bw$bw, bandwidth.divide=TRUE)$ksum/n

# Compute the density directly with the npudens() function...

den &lt;- fitted(npudens(tdat=x, edat=x.eval, bws=bw$bw))

# Plot the true DGP, the npksum()/(nh) estimate and (identical)
# npudens() estimate

plot(x.eval, dnorm(x.eval), xlab="X", ylab="Density", type="l")
points(x.eval, den.ksum, col="blue")
points(x.eval, den, col="red", cex=0.2)
legend(1, .4, 
       c("DGP", "npksum()", 
       "npudens()"), 
       col=c("black", "blue", "red"), 
       lty=c(1, 1, 1))

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# EXAMPLE 2: For this example, we first obtain residuals from a
# parametric regression model, then compute a vector of leave-one-out
# kernel weighted sums of squared residuals where the kernel function is
# raised to the power 2. Note that this returns a vector of kernel
# weighted sums, one for each element of the error vector. Note also
# that you can specify the bandwidth type, kernel function, kernel order
# etc.

data("cps71")
attach(cps71)

error &lt;- residuals(lm(logwage~age+I(age^2)))

bw &lt;- npreg(error~age)

ksum &lt;- npksum(txdat=age, 
               tydat=error^2, 
               bws=bw$bw,
               leave.one.out=TRUE, 
               kernel.pow=2)

ksum

# Obviously, if we wanted the sum of these weighted kernel sums then, 
# trivially, 

sum(ksum$ksum)

detach(cps71)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Note that weighted leave-one-out sums of squared residuals are used to
# construct consistent model specification tests. In fact, the
# npcmstest() routine in this package is constructed purely from calls
# to npksum(). You can type npcmstest to see the npcmstest()
# code and also examine some examples of the many uses of
# npksum().

# EXAMPLE 3: For this example, we conduct local-constant (i.e., 
# Nadaraya-Watson) kernel regression. We shall use cross-validated
# bandwidths using npregbw() by way of example. Note we extract
# the kernel sum from npksum() via the `$ksum' argument in both
# the numerator and denominator.

data("cps71")
attach(cps71)

bw &lt;- npregbw(xdat=age, ydat=logwage)

fit.lc &lt;- npksum(txdat=age, tydat=logwage, bws=bw$bw)$ksum/
          npksum(txdat=age, bws=bw$bw)$ksum

plot(age, logwage, xlab="Age", ylab="log(wage)")
lines(age, fit.lc)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# If you wished to compute the kernel sum for a set of evaluation points, 
# you first generate the set of points then feed this to npksum, 
# e.g., for the set (20, 30, ..., 60) we would use

age.seq &lt;- seq(20, 60, 10)

fit.lc &lt;- npksum(txdat=age, exdat=age.seq, tydat=logwage, bws=bw$bw)$ksum/
          npksum(txdat=age, exdat=age.seq, bws=bw$bw)$ksum

# Note that now fit.lc contains the 5 values of the local constant
# estimator corresponding to age.seq...

fit.lc

detach(cps71)

# EXAMPLE 4: For this example, we conduct least-squares cross-validation
# for the local-constant regression estimator. We first write an R
# function `ss' that computes the leave-one-out sum of squares using the
# npksum() function, and then feed this function, along with
# random starting values for the bandwidth vector, to the nlm() routine
# in R (nlm = Non-Linear Minimization). Finally, we compare results with
# the function npregbw() that is written solely in C and calls
# a tightly coupled C-level search routine.  Note that one could make
# repeated calls to nlm() using different starting values for h (highly
# recommended in general).

# Increase the number of digits printed out by default in R and avoid
# using scientific notation for this example (we wish to compare
# objective function minima)

options(scipen=100, digits=12)

# Generate 100 observations from a simple DGP where one explanatory
# variable is irrelevant.

n &lt;- 100

x1 &lt;- runif(n)
x2 &lt;- rnorm(n)
x3 &lt;- runif(n)

txdat &lt;- data.frame(x1, x2, x3)

# Note - x3 is irrelevant

tydat &lt;- x1 + sin(x2) + rnorm(n)

# Write an R function that returns the average leave-one-out sum of
# squared residuals for the local constant estimator based upon
# npksum(). This function accepts one argument and presumes that
# txdat and tydat have been defined already.

ss &lt;- function(h) {

# Test for valid (non-negative) bandwidths - return infinite penalty
# when this occurs

  if(min(h)&lt;=0) {

    return(.Machine$double.xmax)

  } else {

    mean &lt;-  npksum(txdat, 
                    tydat, 
                    leave.one.out=TRUE, 
                    bandwidth.divide=TRUE, 
                    bws=h)$ksum/
             npksum(txdat, 
                    leave.one.out=TRUE, 
                    bandwidth.divide=TRUE, 
                    bws=h)$ksum
  
    return(sum((tydat-mean)^2)/length(tydat))

  }

}

# Now pass this function to R's nlm() routine along with random starting
# values and place results in `nlm.return'.

nlm.return &lt;- nlm(ss, runif(length(txdat)))

bw &lt;- npregbw(xdat=txdat, ydat=tydat)

# Bandwidths from nlm()

nlm.return$estimate

# Bandwidths from npregbw()

bw$bw

# Function value (minimum) from nlm()

nlm.return$minimum

# Function value (minimum) from npregbw()

bw$fval

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# EXAMPLE 5: For this example, we use npksum() to plot the kernel
# function itself. Our `training data' is the singleton, 0, and our
# evaluation data a grid in [-4,4], while we use a bandwidth of 1. By
# way of example we plot a sixth order Gaussian kernel (note that this
# kernel function can assume negative values)

x &lt;- 0
x.eval &lt;- seq(-4,4,length=500)

kernel &lt;- npksum(txdat=x,exdat=x.eval,bws=1,
                 bandwidth.divide=TRUE,
                 ckertype="gaussian",
                 ckerorder=6)$ksum

plot(x.eval,kernel,type="l",xlab="X",ylab="Kernel Function") 
abline(0,0)

## End(Not run) 
</code></pre>

<hr>
<h2 id='npplot'>General Purpose Plotting of Nonparametric Objects</h2><span id='topic+npplot'></span><span id='topic+npplot.bandwidth'></span><span id='topic+npplot.conbandwidth'></span><span id='topic+npplot.plbandwidth'></span><span id='topic+npplot.rbandwidth'></span><span id='topic+npplot.scbandwidth'></span><span id='topic+npplot.sibandwidth'></span>

<h3>Description</h3>

<p><code>npplot</code> is invoked by <code><a href="base.html#topic+plot">plot</a></code> and generates plots of
nonparametric statistical objects such as regressions, quantile
regressions, partially linear regressions, single-index models,
densities and distributions, given training data and a bandwidth
object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npplot(bws = stop("'bws' has not been set"), ..., random.seed = 42)

## S3 method for class 'bandwidth'
npplot(bws,
       xdat,
       data = NULL,
       xq = 0.5,
       xtrim = 0.0,
       neval = 50,
       common.scale = TRUE,
       perspective = TRUE,
       main = NULL,
       type = NULL,
       border = NULL,
       cex.axis = NULL,
       cex.lab = NULL,
       cex.main = NULL,
       cex.sub = NULL,
       col = NULL,
       ylab = NULL,
       xlab = NULL,
       zlab = NULL,
       sub = NULL,
       ylim = NULL,
       xlim = NULL,
       zlim = NULL,
       lty = NULL,
       lwd = NULL,
       theta = 0.0,
       phi = 10.0,
       view = c("rotate","fixed"),
       plot.behavior = c("plot","plot-data","data"),
       plot.errors.method = c("none","bootstrap","asymptotic"),
       plot.errors.boot.method = c("inid", "fixed", "geom"),
       plot.errors.boot.blocklen = NULL,
       plot.errors.boot.num = 399,
       plot.errors.center = c("estimate","bias-corrected"),
       plot.errors.type = c("standard","quantiles"),
       plot.errors.quantiles = c(0.025,0.975),
       plot.errors.style = c("band","bar"),
       plot.errors.bar = c("|","I"),
       plot.errors.bar.num = min(neval,25),
       plot.bxp = FALSE,
       plot.bxp.out = TRUE,
       plot.par.mfrow = TRUE,
       ...,
       random.seed)

## S3 method for class 'conbandwidth'
npplot(bws,
       xdat,
       ydat,
       data = NULL,
       xq = 0.5,
       yq = 0.5,
       xtrim = 0.0,
       ytrim = 0.0,
       neval = 50,
       gradients = FALSE,
       common.scale = TRUE,
       perspective = TRUE,
       main = NULL,
       type = NULL,
       border = NULL,
       cex.axis = NULL,
       cex.lab = NULL,
       cex.main = NULL,
       cex.sub = NULL,
       col = NULL,
       ylab = NULL,
       xlab = NULL,
       zlab = NULL,
       sub = NULL,
       ylim = NULL,
       xlim = NULL,
       zlim = NULL,
       lty = NULL,
       lwd = NULL,
       theta = 0.0,
       phi = 10.0,
       tau = 0.5,
       view = c("rotate","fixed"),
       plot.behavior = c("plot","plot-data","data"),
       plot.errors.method = c("none","bootstrap","asymptotic"),
       plot.errors.boot.method = c("inid", "fixed", "geom"),
       plot.errors.boot.blocklen = NULL,
       plot.errors.boot.num = 399,
       plot.errors.center = c("estimate","bias-corrected"),
       plot.errors.type = c("standard","quantiles"),
       plot.errors.quantiles = c(0.025,0.975),
       plot.errors.style = c("band","bar"),
       plot.errors.bar = c("|","I"),
       plot.errors.bar.num = min(neval,25),
       plot.bxp = FALSE,
       plot.bxp.out = TRUE,
       plot.par.mfrow = TRUE,
       ...,
       random.seed)

## S3 method for class 'plbandwidth'
npplot(bws,
       xdat,
       ydat,
       zdat,
       data = NULL,
       xq = 0.5,
       zq = 0.5,
       xtrim = 0.0,
       ztrim = 0.0,
       neval = 50,
       common.scale = TRUE,
       perspective = TRUE,
       gradients = FALSE,
       main = NULL,
       type = NULL,
       border = NULL,
       cex.axis = NULL,
       cex.lab = NULL,
       cex.main = NULL,
       cex.sub = NULL,
       col = NULL,
       ylab = NULL,
       xlab = NULL,
       zlab = NULL,
       sub = NULL,
       ylim = NULL,
       xlim = NULL,
       zlim = NULL,
       lty = NULL,
       lwd = NULL,
       theta = 0.0,
       phi = 10.0,
       view = c("rotate","fixed"),
       plot.behavior = c("plot","plot-data","data"),
       plot.errors.method = c("none","bootstrap","asymptotic"),
       plot.errors.boot.method = c("inid", "fixed", "geom"),
       plot.errors.boot.blocklen = NULL,
       plot.errors.boot.num = 399,
       plot.errors.center = c("estimate","bias-corrected"),
       plot.errors.type = c("standard","quantiles"),
       plot.errors.quantiles = c(0.025,0.975),
       plot.errors.style = c("band","bar"),
       plot.errors.bar = c("|","I"),
       plot.errors.bar.num = min(neval,25),
       plot.bxp = FALSE,
       plot.bxp.out = TRUE,
       plot.par.mfrow = TRUE,
       ...,
       random.seed)

## S3 method for class 'rbandwidth'
npplot(bws,
       xdat,
       ydat,
       data = NULL,
       xq = 0.5,
       xtrim = 0.0,
       neval = 50,
       common.scale = TRUE,
       perspective = TRUE,
       gradients = FALSE,
       main = NULL,
       type = NULL,
       border = NULL,
       cex.axis = NULL,
       cex.lab = NULL,
       cex.main = NULL,
       cex.sub = NULL,
       col = NULL,
       ylab = NULL,
       xlab = NULL,
       zlab = NULL,
       sub = NULL,
       ylim = NULL,
       xlim = NULL,
       zlim = NULL,
       lty = NULL,
       lwd = NULL,
       theta = 0.0,
       phi = 10.0,
       view = c("rotate","fixed"),
       plot.behavior = c("plot","plot-data","data"),
       plot.errors.method = c("none","bootstrap","asymptotic"),
       plot.errors.boot.num = 399,
       plot.errors.boot.method = c("inid", "fixed", "geom"),
       plot.errors.boot.blocklen = NULL,
       plot.errors.center = c("estimate","bias-corrected"),
       plot.errors.type = c("standard","quantiles"),
       plot.errors.quantiles = c(0.025,0.975),
       plot.errors.style = c("band","bar"),
       plot.errors.bar = c("|","I"),
       plot.errors.bar.num = min(neval,25),
       plot.bxp = FALSE,
       plot.bxp.out = TRUE,
       plot.par.mfrow = TRUE,
       ...,
       random.seed)

## S3 method for class 'scbandwidth'
npplot(bws,
       xdat,
       ydat,
       zdat = NULL,
       data = NULL,
       xq = 0.5,
       zq = 0.5,
       xtrim = 0.0,
       ztrim = 0.0,
       neval = 50,
       common.scale = TRUE,
       perspective = TRUE,
       gradients = FALSE,
       main = NULL,
       type = NULL,
       border = NULL,
       cex.axis = NULL,
       cex.lab = NULL,
       cex.main = NULL,
       cex.sub = NULL,
       col = NULL,
       ylab = NULL,
       xlab = NULL,
       zlab = NULL,
       sub = NULL,
       ylim = NULL,
       xlim = NULL,
       zlim = NULL,
       lty = NULL,
       lwd = NULL,
       theta = 0.0,
       phi = 10.0,
       view = c("rotate","fixed"),
       plot.behavior = c("plot","plot-data","data"),
       plot.errors.method = c("none","bootstrap","asymptotic"),
       plot.errors.boot.num = 399,
       plot.errors.boot.method = c("inid", "fixed", "geom"),
       plot.errors.boot.blocklen = NULL,
       plot.errors.center = c("estimate","bias-corrected"),
       plot.errors.type = c("standard","quantiles"),
       plot.errors.quantiles = c(0.025,0.975),
       plot.errors.style = c("band","bar"),
       plot.errors.bar = c("|","I"),
       plot.errors.bar.num = min(neval,25),
       plot.bxp = FALSE,
       plot.bxp.out = TRUE,
       plot.par.mfrow = TRUE,
       ...,
       random.seed)

## S3 method for class 'sibandwidth'
npplot(bws,
       xdat,
       ydat,
       data = NULL,
       common.scale = TRUE,
       gradients = FALSE,
       main = NULL,
       type = NULL,
       cex.axis = NULL,
       cex.lab = NULL,
       cex.main = NULL,
       cex.sub = NULL,
       col = NULL,
       ylab = NULL,
       xlab = NULL,
       sub = NULL,
       ylim = NULL,
       xlim = NULL,
       lty = NULL,
       lwd = NULL,
       plot.behavior = c("plot","plot-data","data"),
       plot.errors.method = c("none","bootstrap","asymptotic"),
       plot.errors.boot.num = 399,
       plot.errors.boot.method = c("inid", "fixed", "geom"),
       plot.errors.boot.blocklen = NULL,
       plot.errors.center = c("estimate","bias-corrected"),
       plot.errors.type = c("standard","quantiles"),
       plot.errors.quantiles = c(0.025,0.975),
       plot.errors.style = c("band","bar"),
       plot.errors.bar = c("|","I"),
       plot.errors.bar.num = NULL,
       plot.par.mfrow = TRUE,
       ...,
       random.seed)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npplot_+3A_bws">bws</code></td>
<td>

<p>a bandwidth specification. This should be a bandwidth object
returned from an invocation of <code><a href="#topic+npudensbw">npudensbw</a></code>, <a href="#topic+npcdensbw">npcdensbw</a>,
<code><a href="#topic+npregbw">npregbw</a></code>, <a href="#topic+npplregbw">npplregbw</a>, <a href="#topic+npindexbw">npindexbw</a>, or
<code><a href="#topic+npscoefbw">npscoefbw</a></code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to control various aspects of
plotting, depending on the type of object to be plotted, detailed
below. 
</p>
</td></tr>
<tr><td><code id="npplot_+3A_data">data</code></td>
<td>

<p>an optional data frame, list or environment (or object
coercible to a data frame by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code>) containing the variables
in the model. If not found in data, the variables are taken from
<code>environment(bws)</code>, typically the environment where the
bandwidth object was generated.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_xdat">xdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of sample realizations (training
data). 
</p>
</td></tr>
<tr><td><code id="npplot_+3A_ydat">ydat</code></td>
<td>

<p>a <code class="reqn">q</code>-variate data frame of sample realizations (training
data). In a regression or conditional density context, this is the
dependent data.  
</p>
</td></tr>
<tr><td><code id="npplot_+3A_zdat">zdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of sample realizations (training
data).
</p>
</td></tr>
<tr><td><code id="npplot_+3A_xq">xq</code></td>
<td>

<p>a numeric <code class="reqn">p</code>-vector of quantiles. Each element <code class="reqn">i</code> of
<code>xq</code> corresponds to the <code class="reqn">i</code>th column of
<code>txdat</code>. Defaults to the median (0.5). See
details. 
</p>
</td></tr>
<tr><td><code id="npplot_+3A_yq">yq</code></td>
<td>

<p>a numeric <code class="reqn">q</code>-vector of quantiles. Each element <code class="reqn">i</code> of
<code>yq</code> corresponds to the <code class="reqn">i</code>th column of <code>tydat</code>. Only
to be specified in a conditional density context. Defaults to the
median (0.5). See details. 
</p>
</td></tr>
<tr><td><code id="npplot_+3A_zq">zq</code></td>
<td>

<p>a numeric <code class="reqn">q</code>-vector of quantiles. Each element <code class="reqn">i</code> of
<code>zq</code> corresponds to the <code class="reqn">i</code>th column of <code>tzdat</code>. Only
to be specified in a semiparametric model context. Defaults to the
median (0.5). See details. 
</p>
</td></tr>
<tr><td><code id="npplot_+3A_xtrim">xtrim</code></td>
<td>

<p>a numeric <code class="reqn">p</code>-vector of quantiles. Each element <code class="reqn">i</code> of
<code>xtrim</code> corresponds to the <code class="reqn">i</code>th column of
<code>txdat</code>. Defaults to <code>0.0</code>. See details.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_ytrim">ytrim</code></td>
<td>

<p>a numeric <code class="reqn">q</code>-vector of quantiles. Each element <code class="reqn">i</code> of
<code>ytrim</code> corresponds to the <code class="reqn">i</code>th column of
<code>tydat</code>. Defaults to <code>0.0</code>. See details.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_ztrim">ztrim</code></td>
<td>

<p>a numeric <code class="reqn">q</code>-vector of quantiles. Each element <code class="reqn">i</code> of
<code>ztrim</code> corresponds to the <code class="reqn">i</code>th column of
<code>tzdat</code>. Defaults to <code>0.0</code>. See details.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_neval">neval</code></td>
<td>

<p>an integer specifying the number of evaluation points. Only applies
to continuous variables however, as discrete variables will be
evaluated once at each category. Defaults to <code>50</code>. 
</p>
</td></tr>
<tr><td><code id="npplot_+3A_common.scale">common.scale</code></td>
<td>

<p>a logical value specifying whether or not all graphs are to be
plotted on a common scale. Defaults to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_perspective">perspective</code></td>
<td>

<p>a logical value specifying whether a perspective plot should be
displayed (if possible). Defaults to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_gradients">gradients</code></td>
<td>

<p>a logical value specifying whether gradients should be plotted
(if possible). Defaults to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_main">main</code></td>
<td>

<p>optional title, see <code><a href="graphics.html#topic+title">title</a></code>. Defaults to <code>NULL</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_sub">sub</code></td>
<td>

<p>optional subtitle, see <code><a href="base.html#topic+sub">sub</a></code>. Defaults to <code>NULL</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_type">type</code></td>
<td>

<p>optional character indicating the type of plotting; actually any of
the types as in <code><a href="graphics.html#topic+plot.default">plot.default</a></code>. Defaults to <code>NULL</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_border">border</code></td>
<td>

<p>optional character indicating the border of plotting; actually any of
the borders as in <code><a href="graphics.html#topic+plot.default">plot.default</a></code>. Defaults to <code>NULL</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_cex.axis">cex.axis</code></td>
<td>

<p>The magnification to be used for axis annotation relative to the
current setting of cex.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_cex.lab">cex.lab</code></td>
<td>

<p>The magnification to be used for x and y labels relative to the
current setting of cex.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_cex.main">cex.main</code></td>
<td>

<p>The magnification to be used for main titles relative to the
current setting of cex.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_cex.sub">cex.sub</code></td>
<td>

<p>The magnification to be used for sub-titles relative to the
current setting of cex.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_col">col</code></td>
<td>

<p>optional character indicating the color of plotting; actually any of
the colours as in <code><a href="graphics.html#topic+plot.default">plot.default</a></code>. Defaults to <code>NULL</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_ylab">ylab</code></td>
<td>

<p>optional character indicating the y axis label of plotting; actually any of
the ylabs as in <code><a href="graphics.html#topic+plot.default">plot.default</a></code>. Defaults to <code>NULL</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_xlab">xlab</code></td>
<td>

<p>optional character indicating the x axis label of plotting; actually any of
the xlabs as in <code><a href="graphics.html#topic+plot.default">plot.default</a></code>. Defaults to <code>NULL</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_zlab">zlab</code></td>
<td>

<p>optional character indicating the z axis label of plotting; actually any of
the zlabs as in <code><a href="graphics.html#topic+plot.default">plot.default</a></code>. Defaults to <code>NULL</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_ylim">ylim</code></td>
<td>

<p>optional a two-element numeric vector of the minimum and maximum y plotting
limits. Defaults to <code>NULL</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_xlim">xlim</code></td>
<td>

<p>a two-element numeric vector of the minimum and maximum x plotting
limits. Defaults to <code>NULL</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_zlim">zlim</code></td>
<td>

<p>a two-element numeric vector of the minimum and maximum z plotting
limits. Defaults to <code>NULL</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_lty">lty</code></td>
<td>

<p>a numeric value indicating the line type of plotting; actually any of
the ltys as in <code><a href="graphics.html#topic+plot.default">plot.default</a></code>. Defaults to <code>1</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_lwd">lwd</code></td>
<td>

<p>a numeric value indicating the width of the line of plotting;
actually any of the lwds as in <code><a href="graphics.html#topic+plot.default">plot.default</a></code>. Defaults
to <code>1</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_theta">theta</code></td>
<td>

<p>a numeric value specifying the starting azimuthal angle of the
perspective plot. Defaults to <code>0.0</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_phi">phi</code></td>
<td>

<p>a numeric value specifying the starting zenith angle of the
perspective plot. Defaults to <code>10.0</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_tau">tau</code></td>
<td>

<p>a numeric value specifying the <code class="reqn">\tau</code>th quantile is
desired when plotting quantile regressions. Defaults to <code>0.5</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_view">view</code></td>
<td>

<p>a character string used to specify the viewing mode of the
perspective plot. Can be set as <code>rotate</code> or
<code>fixed</code>. Defaults to <code>rotate</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_plot.behavior">plot.behavior</code></td>
<td>

<p>a character string used to specify the net behavior of <code>npplot</code>. Can
be set as <code>plot</code>, <code>plot-data</code> or <code>data</code>. Defaults to
<code>plot</code>. See value.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_plot.errors.method">plot.errors.method</code></td>
<td>

<p>a character string used to specify the method to calculate
errors. Can be set as <code>none</code>, <code>bootstrap</code>, or
<code>asymptotic</code>. Defaults to <code>none</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_plot.errors.boot.method">plot.errors.boot.method</code></td>
<td>

<p>a character string used to specify the bootstrap method. Can be set
as <code>inid</code>, <code>fixed</code>, or <code>geom</code> (see
below for details). Defaults to <code>inid</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_plot.errors.boot.blocklen">plot.errors.boot.blocklen</code></td>
<td>

<p>an integer used to specify the block length <code class="reqn">b</code> for the
<code>fixed</code> or <code>geom</code> bootstrap (see below for
details).
</p>
</td></tr>
<tr><td><code id="npplot_+3A_plot.errors.boot.num">plot.errors.boot.num</code></td>
<td>

<p>an integer used to specify the number of bootstrap samples to use
for the calculation of errors. Defaults to <code>399</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_plot.errors.center">plot.errors.center</code></td>
<td>

<p>a character string used to specify where to center the errors on the
plot(s). Can be set as <code>estimate</code> or
<code>bias-corrected</code>. Defaults to <code>estimate</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_plot.errors.type">plot.errors.type</code></td>
<td>

<p>a character string used to specify the type of error to
calculate. Can be set as <code>standard</code> or
<code>quantiles</code>. Defaults to <code>standard</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_plot.errors.quantiles">plot.errors.quantiles</code></td>
<td>

<p>a numeric vector specifying the quantiles of the statistic to
calculate for the purpose of error plotting. Defaults to
<code>c(0.025,0.975)</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_plot.errors.style">plot.errors.style</code></td>
<td>

<p>a character string used to specify the style of error plotting. Can
be set as <code>band</code> or <code>bar</code>. Defaults to <code>band</code>. Bands
are not drawn for discrete variables.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_plot.errors.bar">plot.errors.bar</code></td>
<td>

<p>a character string used to specify the error bar shape. Can be set
as <code>|</code> (vertical bar character) for a dashed vertical bar, or
as <code>I</code> for an &lsquo;I&rsquo; shaped error bar with horizontal
bounding bars. Defaults to <code>|</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_plot.errors.bar.num">plot.errors.bar.num</code></td>
<td>

<p>an integer specifying the number of error bars to plot. Defaults to
<code>min(neval,25)</code>. 
</p>
</td></tr>
<tr><td><code id="npplot_+3A_plot.bxp">plot.bxp</code></td>
<td>

<p>a logical value specifying whether boxplots should be produced when
appropriate. Defaults to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_plot.bxp.out">plot.bxp.out</code></td>
<td>

<p>a logical value specifying whether outliers should be plotted on
boxplots. Defaults to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_plot.par.mfrow">plot.par.mfrow</code></td>
<td>

<p>a logical value specifying whether <code>par(mfrow=c(,))</code> should be called
before plotting. Defaults to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npplot_+3A_random.seed">random.seed</code></td>
<td>

<p>an integer used to seed R's random number generator. This ensures
replicability of the bootstrapped errors. Defaults to 42.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>npplot</code> is a general purpose plotting routine for visually
exploring objects generated by the <code>np</code> library, such as
regressions, quantile regressions, partially linear regressions,
single-index models, densities and distributions. There is no need to
call <code>npplot</code> directly as it is automatically invoked when
<code><a href="base.html#topic+plot">plot</a></code> is used with an object generated by the <span class="pkg">np</span>
package.
</p>
<p>Visualizing one and two dimensional datasets is a straightforward
process. The default behavior of <code>npplot</code> is to generate a
standard 2D plot to visualize univariate data, and a perspective plot
for bivariate data. When visualizing higher dimensional data,
<code>npplot</code> resorts to plotting a series of 1D slices of the
data. For a slice along dimension <code class="reqn">i</code>, all other variables at
indices <code class="reqn">j \ne i</code> are held constant at the quantiles
specified in the <code class="reqn">j</code>th element of <code>xq</code>. The default is the
median. 
</p>
<p>The slice itself is evaluated on a uniformly spaced sequence of
<code class="reqn">neval</code> points. The interval of evaluation is determined by the
training data. The default behavior is to evaluate from
<code>min(txdat[,i])</code> to <code>max(txdat[,i])</code>. The <code>xtrim</code>
variable allows for control over this behavior. When <code>xtrim</code> is
set, data is evaluated from the <code>xtrim[i]</code>th quantile of
<code>txdat[,i]</code> to the <code>1.0-xtrim[i]</code>th quantile of
<code>txdat[,i]</code>.
</p>
<p>Furthermore, <code>xtrim</code> can be set to a negative
value in which case it will expand the limits of the evaluation
interval beyond the support of the training data, by measuring the
distance between <code>min(txdat[,i])</code> and the <code>xtrim[i]</code>th
quantile of <code>txdat[,i]</code>, and extending the support by that
distance on the lower limit of the interval. <code>npplot</code> uses an
analogous procedure to extend the upper limit of the interval.
</p>
<p>Bootstrap resampling is conducted pairwise on <code class="reqn">(y,X,Z)</code> (i.e., by
resampling from rows of the <code class="reqn">(y,X)</code> data or <code class="reqn">(y,X,Z)</code> data
where appropriate). <code>inid</code> admits general
heteroskedasticity of unknown form, though it does not allow for
dependence. <code>fixed</code> conducts Kunsch's (1988) block bootstrap
for dependent data, while <code>geom</code> conducts Politis and
Romano's (1994) stationary bootstrap.
</p>
<p>For consistency of the block and stationary bootstrap, the (mean)
block length <code class="reqn">b</code> should grow with the sample size <code class="reqn">n</code> at an
appropriate rate. If <code class="reqn">b</code> is not given, then a default growth rate
of <code class="reqn">const \times n^{1/3}</code> is used. This rate is
&ldquo;optimal&rdquo; under certain conditions (see Politis and Romano
(1994) for more details). However, in general the growth rate depends on
the specific properties of the DGP. A default value for <code class="reqn">const</code>
(<code class="reqn">3.15</code>) has been determined by a Monte Carlo simulation using a
Gaussian AR(1) process (AR(1)-parameter of 0.5, 500
observations). <code class="reqn">const</code> has been chosen such that the mean square
error for the bootstrap estimate of the variance of the empirical mean
is minimized.
</p>


<h3>Value</h3>

<p>Setting <code>plot.behavior</code> will instruct <code>npplot</code> what data
to return. Option summary:<br />
<code>plot</code>: instruct <code>npplot</code> to just plot the data and
return <code>NULL</code> <br />
<code>plot-data</code>: instruct <code>npplot</code> to plot the data and return
the data used to generate the plots. The data will be a <code>list</code> of
objects of the appropriate type, with one object per plot. For
example, invoking <code>npplot</code> on 3D density data will have it
return a list of three npdensity objects. If biases were calculated,
they are stored in a component named <code>bias</code><br />
<code>data</code>: instruct <code>npplot</code> to generate data only and no plots
</p>


<h3>Usage Issues</h3>

<p>If you are using data of mixed types, then it is advisable to use the
<code><a href="base.html#topic+data.frame">data.frame</a></code> function to construct your input data and not
<code><a href="base.html#topic+cbind">cbind</a></code>, since <code><a href="base.html#topic+cbind">cbind</a></code> will typically not work as
intended on mixed data types and will coerce the data to the same
type.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine
<a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Aitchison, J. and C.G.G. Aitken (1976), &ldquo;Multivariate binary
discrimination by the kernel method,&rdquo; Biometrika, 63, 413-420.
</p>
<p>Hall, P. and J.S. Racine and Q. Li (2004), &ldquo;Cross-validation and the
estimation of conditional probability densities,&rdquo; Journal of the
American Statistical Association, 99, 1015-1026.
</p>
<p>Kunsch, H.R. (1989), &ldquo;The jackknife and the bootstrap for
general stationary observations,&rdquo; The Annals of Statistics, 17,
1217-1241.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics: Theory
and Practice,</em> Princeton University Press.
</p>
<p>Pagan, A. and A. Ullah (1999), <em>Nonparametric Econometrics,</em> Cambridge
University Press. 
</p>
<p>Politis, D.N. and J.P. Romano (1994), &ldquo;The stationary
bootstrap,&rdquo; Journal of the American Statistical Association, 89,
1303-1313.
</p>
<p>Scott, D.W. (1992), <em>Multivariate Density Estimation. Theory,
Practice and Visualization,</em> New York: Wiley.
</p>
<p>Silverman, B.W. (1986), <em>Density Estimation,</em> London: Chapman and
Hall.
</p>
<p>Wang, M.C. and J. van Ryzin (1981), &ldquo;A class of smooth
estimators for discrete distributions,&rdquo; Biometrika, 68, 301-309.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# EXAMPLE 1: For this example, we load Giovanni Baiocchi's Italian GDP
# panel (see Italy for details), then create a data frame in which year
# is an ordered factor, GDP is continuous, compute bandwidths using
# likelihood cross-validation, then create a grid of data on which the
# density will be evaluated for plotting purposes

data("Italy")
attach(Italy)

data &lt;- data.frame(ordered(year), gdp)

# Compute bandwidths using likelihood cross-validation (default). Note
# that this may take a minute or two depending on the speed of your
# computer...

bw &lt;- npudensbw(dat=data)

# You can always do things manually, as the following example demonstrates

# Create an evaluation data matrix

year.seq &lt;- sort(unique(year))
gdp.seq &lt;- seq(1,36,length=50)
data.eval &lt;- expand.grid(year=year.seq,gdp=gdp.seq)

# Generate the estimated density computed for the evaluation data

fhat &lt;- fitted(npudens(tdat = data, edat = data.eval, bws=bw))

# Coerce the data into a matrix for plotting with persp()

f &lt;- matrix(fhat, length(unique(year)), 50)

# Next, create a 3D perspective plot of the PDF f

persp(as.integer(levels(year.seq)), gdp.seq, f, col="lightblue",
      ticktype="detailed", ylab="GDP", xlab="Year", zlab="Density",
      theta=300, phi=50)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# However, npplot simply streamlines this process and aids in the
# visualization process (&lt;ctrl&gt;-C will interrupt on *NIX systems, &lt;esc&gt;
# will interrupt on MS Windows systems).

plot(bw)

# npplot also streamlines construction of variability bounds (&lt;ctrl&gt;-C
# will interrupt on *NIX systems, &lt;esc&gt; will interrupt on MS Windows
# systems)

plot(bw, plot.errors.method = "asymptotic")

# EXAMPLE 2: For this example, we simulate multivariate data, and plot the
# partial regression surfaces for a locally linear estimator and its
# derivatives.

set.seed(123)

n &lt;- 100

x1 &lt;- runif(n)
x2 &lt;- runif(n)
x3 &lt;- runif(n)
x4 &lt;- rbinom(n, 2, .3)

y &lt;- 1 + x1 + x2 + x3 + x4 + rnorm(n)

X &lt;- data.frame(x1, x2, x3, ordered(x4))

bw &lt;- npregbw(xdat=X, ydat=y, regtype="ll", bwmethod="cv.aic")

plot(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Now plot the gradients...

plot(bw, gradients=TRUE)

# Plot the partial regression surfaces with bias-corrected bootstrapped
# nonparametric confidence intervals... this may take a minute or two
# depending on the speed of your computer as the bootstrapping must be
# completed prior to results being displayed...

plot(bw,
     plot.errors.method="bootstrap", 
     plot.errors.center="bias-corrected",
     plot.errors.type="quantiles")

# Note - if you wished to create, say, a postscript graph for inclusion
# in, say, a latex document, use R's `postscript' command to switch to
# the postscript device (turn off the device once completed). The
# following will create a disk file `graph.ps' that can be pulled into,
# say, a latex document via \includegraphics[width=5in, height=5in,
# angle=270]{graph.ps}

# Note - make sure to include the graphicx package in your latex
# document via adding \usepackage{graphicx} in your latex file. Also, 
# you might want to use larger fonts, which can be achieved by adding the
# pointsize= argument, e.g., postscript(file="graph.ps", pointsize=20)

postscript(file="graph.ps")
plot(bw)
dev.off()

# The following latex file compiled in the same directory as graph.ps
# ought to work (remove the #s and place in a file named, e.g., 
# test.tex).
# \documentclass[]{article}
# \usepackage{graphicx}
# \begin{document}
# \begin{figure}[!ht]
# \includegraphics[width=5in, height=5in, angle=270]{graph.ps}
# \caption{Local linear partial regression surfaces.}
# \end{figure}
# \end{document}


# EXAMPLE 3: This example demonstrates how to retrieve plotting data from
# npplot(). When npplot() is called with the arguments
# `plot.behavior="plot-data"' (or "data"), it returns plotting objects
# named r1, r2, and so on (rg1, rg2, and so on when `gradients=TRUE' is
# set).  Each plotting object's index (1,2,...) corresponds to the index
# of the explanatory data data frame xdat (and zdat if appropriate). 

# Take the cps71 data by way of example. In this case, there is only one
# object returned by default, `r1', since xdat is univariate.

data("cps71")
attach(cps71)

# Compute bandwidths for local linear regression using cv.aic...

bw &lt;- npregbw(xdat=age,ydat=logwage,regtype="ll",bwmethod="cv.aic")

# Generate the plot and return plotting data, and store output in
# `plot.out' (NOTE: the call to `plot.behavior' is necessary).

plot.out &lt;- plot(bw,
                 perspective=FALSE,
                 plot.errors.method="bootstrap",
                 plot.errors.boot.num=25,
                 plot.behavior="plot-data")

# Now grab the r1 object that npplot plotted on the screen, and take
# what you need. First, take the output, lower error bound and upper
# error bound...

logwage.eval &lt;- fitted(plot.out$r1)
logwage.se &lt;- se(plot.out$r1)
logwage.lower.ci &lt;- logwage.eval + logwage.se[,1]
logwage.upper.ci &lt;- logwage.eval + logwage.se[,2]

# Next grab the x data evaluation data. xdat is a data.frame(), so we
# need to coerce it into a vector (take the `first column' of data frame
# even though there is only one column)

age.eval &lt;- plot.out$r1$eval[,1]

# Now we could plot this if we wished, or direct it to whatever end use
# we envisioned. We plot the results using R's plot() routines...

plot(age,logwage,cex=0.2,xlab="Age",ylab="log(Wage)")
lines(age.eval,logwage.eval)
lines(age.eval,logwage.lower.ci,lty=3)
lines(age.eval,logwage.upper.ci,lty=3)

# If you wanted npplot() data for gradients, you would use the argument
# `gradients=TRUE' in the call to npplot() as the following
# demonstrates...

plot.out &lt;- plot(bw,
                 perspective=FALSE,
                 plot.errors.method="bootstrap",
                 plot.errors.boot.num=25,
                 plot.behavior="plot-data",
                 gradients=TRUE)

# Now grab object that npplot() plotted on the screen. First, take the
# output, lower error bound and upper error bound... note that gradients
# are stored in objects rg1, rg2 etc.

grad.eval &lt;- gradients(plot.out$rg1)
grad.se &lt;- gradients(plot.out$rg1, errors = TRUE)
grad.lower.ci &lt;- grad.eval + grad.se[,1]
grad.upper.ci &lt;- grad.eval + grad.se[,2]

# Next grab the x evaluation data. xdat is a data.frame(), so we need to
# coerce it into a vector (take `first column' of data frame even though
# there is only one column)

age.eval &lt;- plot.out$rg1$eval[,1]

# We plot the results using R's plot() routines...

plot(age.eval,grad.eval,cex=0.2,
     ylim=c(min(grad.lower.ci),max(grad.upper.ci)),
     xlab="Age",ylab="d log(Wage)/d Age",type="l")
lines(age.eval,grad.lower.ci,lty=3)
lines(age.eval,grad.upper.ci,lty=3)

detach(cps71)

## End(Not run) 
</code></pre>

<hr>
<h2 id='npplreg'>Partially Linear Kernel Regression with Mixed Data Types</h2><span id='topic+npplreg'></span><span id='topic+npplreg.call'></span><span id='topic+npplreg.formula'></span><span id='topic+npplreg.plbandwidth'></span>

<h3>Description</h3>

<p><code>npplreg</code> computes a partially linear kernel regression estimate
of a one (1) dimensional dependent variable on <code class="reqn">p+q</code>-variate
explanatory data, using the model <code class="reqn">Y = X\beta + \Theta (Z) +
  \epsilon</code> given a set of estimation
points, training points (consisting of explanatory data and dependent
data), and a bandwidth specification, which can be a <code>rbandwidth</code>
object, or a bandwidth vector, bandwidth type and kernel type.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npplreg(bws, ...)

## S3 method for class 'formula'
npplreg(bws, data = NULL, newdata = NULL, y.eval =
FALSE, ...)

## S3 method for class 'call'
npplreg(bws, ...)

## S3 method for class 'plbandwidth'
npplreg(bws,
        txdat = stop("training data txdat missing"),
        tydat = stop("training data tydat missing"),
        tzdat = stop("training data tzdat missing"),
        exdat,
        eydat,
        ezdat,
        residuals = FALSE,
        ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npplreg_+3A_bws">bws</code></td>
<td>

<p>a bandwidth specification. This can be set as a <code>plbandwidth</code>
object returned from an invocation of <code><a href="#topic+npplregbw">npplregbw</a></code>, or as
a matrix of bandwidths, where each row is a set of bandwidths for <code class="reqn">Z</code>,
with a column for each variable <code class="reqn">Z_i</code>. In the first row are
the bandwidths for the regression of <code class="reqn">Y</code> on <code class="reqn">Z</code>, the following
rows contain the bandwidths for the regressions of the columns of
<code class="reqn">X</code> on <code class="reqn">Z</code>.  If specified as a matrix
additional arguments will need to be supplied as necessary to
specify the bandwidth type, kernel types, training data, and so on.
</p>
</td></tr>
<tr><td><code id="npplreg_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to specify the regression type,
bandwidth type, kernel types, selection methods, and so on. To do
this, you may specify any of <code>regtype</code>, 
<code>bwmethod</code>, <code>bwscaling</code>, <code>bwtype</code>, <code>ckertype</code>,
<code>ckerorder</code>, <code>ukertype</code>, <code>okertype</code>, as described in
<code><a href="#topic+npregbw">npregbw</a></code>.
</p>
</td></tr>
<tr><td><code id="npplreg_+3A_data">data</code></td>
<td>

<p>an optional data frame, list or environment (or object
coercible to a data frame by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code>) containing the variables
in the model. If not found in data, the variables are taken from
<code>environment(bws)</code>, typically the environment from which
<code><a href="#topic+npplregbw">npplregbw</a></code> was called.
</p>
</td></tr>
<tr><td><code id="npplreg_+3A_newdata">newdata</code></td>
<td>

<p>An optional data frame in which to look for evaluation data. If
omitted, the training data are used.  
</p>
</td></tr>
<tr><td><code id="npplreg_+3A_y.eval">y.eval</code></td>
<td>

<p>If <code>newdata</code> contains dependent data and <code>y.eval = TRUE</code>,
<code><a href="#topic+np">np</a></code> will compute goodness of fit statistics on these
data and return them. Defaults to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="npplreg_+3A_txdat">txdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of explanatory data (training data),
corresponding to <code class="reqn">X</code> in the model equation, whose linear
relationship with the dependent data <code class="reqn">Y</code> is posited. Defaults to
the training data used to 
compute the bandwidth object.
</p>
</td></tr>
<tr><td><code id="npplreg_+3A_tydat">tydat</code></td>
<td>

<p>a one (1) dimensional numeric or integer vector of dependent data, each
element <code class="reqn">i</code> corresponding to each observation (row) <code class="reqn">i</code> of
<code>txdat</code>. Defaults to the training data used to
compute the bandwidth object.
</p>
</td></tr>
<tr><td><code id="npplreg_+3A_tzdat">tzdat</code></td>
<td>

<p>a <code class="reqn">q</code>-variate data frame of explanatory data (training data),
corresponding to <code class="reqn">Z</code> in the model equation, whose relationship
to the dependent variable is unspecified (nonparametric). Defaults to
the training data used to 
compute the bandwidth object.
</p>
</td></tr>
<tr><td><code id="npplreg_+3A_exdat">exdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of points on which the regression will be
estimated (evaluation data).  By default,
evaluation takes place on the data provided by <code>txdat</code>.
</p>
</td></tr>
<tr><td><code id="npplreg_+3A_eydat">eydat</code></td>
<td>

<p>a one (1) dimensional numeric or integer vector of the true values
of the dependent variable. Optional, and used only to calculate the
true errors.  By default,
evaluation takes place on the data provided by <code>tydat</code>.
</p>
</td></tr>
<tr><td><code id="npplreg_+3A_ezdat">ezdat</code></td>
<td>

<p>a <code class="reqn">q</code>-variate data frame of points on which the regression will
be estimated (evaluation data).  By default,
evaluation takes place on the data provided by <code>tzdat</code>.
</p>
</td></tr>
<tr><td><code id="npplreg_+3A_residuals">residuals</code></td>
<td>

<p>a logical value indicating that you want residuals computed and
returned in the resulting <code>plregression</code> object. Defaults to
<code>FALSE</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>npplreg</code> uses a combination of OLS and nonparametric
regression to estimate the parameter <code class="reqn">\beta</code> in the model
<code class="reqn">Y = X\beta + \Theta (Z) + \epsilon</code>.
</p>
<p><code>npplreg</code> implements a variety of methods for
nonparametric regression on multivariate (<code class="reqn">q</code>-variate) explanatory
data defined over a set of possibly continuous and/or discrete
(unordered, ordered) data. The approach is based on Li and Racine
(2003) who employ &lsquo;generalized product kernels&rsquo; that admit a mix
of continuous and discrete data types.
</p>
<p>Three classes of kernel estimators for the continuous data types are
available: fixed, adaptive nearest-neighbor, and generalized
nearest-neighbor. Adaptive nearest-neighbor bandwidths change with
each sample realization in the set, <code class="reqn">x_i</code>, when estimating the
density at the point <code class="reqn">x</code>. Generalized nearest-neighbor bandwidths change
with the point at which the density is estimated, <code class="reqn">x</code>. Fixed bandwidths
are constant over the support of <code class="reqn">x</code>.
</p>
<p>Data contained in the data frame <code>tzdat</code> may be a mix of
continuous (default), unordered discrete (to be specified in the data
frame <code>tzdat</code> using <code><a href="base.html#topic+factor">factor</a></code>), and ordered discrete
(to be specified in the data frame <code>tzdat</code> using
<code><a href="base.html#topic+ordered">ordered</a></code>). Data can be entered in an arbitrary order and
data types will be detected automatically by the routine (see
<code><a href="#topic+np">np</a></code> for details).
</p>
<p>A variety of kernels may be specified by the user. Kernels implemented
for continuous data types include the second, fourth, sixth, and eighth
order Gaussian and Epanechnikov kernels, and the uniform
kernel. Unordered discrete data types use a variation on Aitchison and
Aitken's (1976) kernel, while ordered data types use a variation of the
Wang and van Ryzin (1981) kernel.
</p>


<h3>Value</h3>

<p><code>npplreg</code> returns a <code>plregression</code> object. The generic
accessor functions <code><a href="stats.html#topic+coef">coef</a></code>, <code><a href="stats.html#topic+fitted">fitted</a></code>,
<code><a href="stats.html#topic+residuals">residuals</a></code>, <code><a href="stats.html#topic+predict">predict</a></code>, and
<code><a href="stats.html#topic+vcov">vcov</a></code>, extract (or
estimate) coefficients, estimated values, residuals,
predictions, and variance-covariance matrices,
respectively, from
the returned object. Furthermore, the functions <code>summary</code>
and <code>plot</code> support objects of this type. The returned object
has the following components:
</p>
<table>
<tr><td><code>evalx</code></td>
<td>
<p> evaluation points </p>
</td></tr>
<tr><td><code>evalz</code></td>
<td>
<p> evaluation points </p>
</td></tr>
<tr><td><code>mean</code></td>
<td>
<p> estimation of the regression, or conditional mean, at the
evaluation points </p>
</td></tr> 
<tr><td><code>xcoef</code></td>
<td>
<p> coefficient(s) corresponding to the components
<code class="reqn">\beta_i</code> in the model </p>
</td></tr>
<tr><td><code>xcoeferr</code></td>
<td>
<p> standard errors of the coefficients </p>
</td></tr>
<tr><td><code>xcoefvcov</code></td>
<td>
<p> covariance matrix of the coefficients </p>
</td></tr>
<tr><td><code>bw</code></td>
<td>
<p> the bandwidths, stored as a <code>plbandwidth</code> object </p>
</td></tr>
<tr><td><code>resid</code></td>
<td>
<p> if <code>residuals = TRUE</code>, in-sample or out-of-sample
residuals where appropriate (or possible) </p>
</td></tr>
<tr><td><code>R2</code></td>
<td>
<p> coefficient of determination (Doksum and Samarov (1995))</p>
</td></tr>
<tr><td><code>MSE</code></td>
<td>
<p> mean squared error </p>
</td></tr>
<tr><td><code>MAE</code></td>
<td>
<p> mean absolute error </p>
</td></tr>
<tr><td><code>MAPE</code></td>
<td>
<p> mean absolute percentage error </p>
</td></tr>
<tr><td><code>CORR</code></td>
<td>
<p> absolute value of Pearson's correlation coefficient </p>
</td></tr>
<tr><td><code>SIGN</code></td>
<td>
<p> fraction of observations where fitted and observed values
agree in sign </p>
</td></tr>
</table>


<h3>Usage Issues</h3>

<p>If you are using data of mixed types, then it is advisable to use the
<code><a href="base.html#topic+data.frame">data.frame</a></code> function to construct your input data and not
<code><a href="base.html#topic+cbind">cbind</a></code>, since <code><a href="base.html#topic+cbind">cbind</a></code> will typically not work as
intended on mixed data types and will coerce the data to the same
type.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine
<a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Aitchison, J. and C.G.G. Aitken (1976), &ldquo;Multivariate binary
discrimination by the kernel method,&rdquo; Biometrika, 63, 413-420.
</p>
<p>Doksum, K. and A. Samarov (1995), &ldquo;Nonparametric estimation of
global functionals and a measure of the explanatory power of covariates
in regression,&rdquo; The Annals of Statistics, 23 1443-1473.
</p>
<p>Gao, Q. and L. Liu and J.S. Racine (2015), &ldquo;A partially linear
kernel estimator for categorical data,&rdquo; Econometric Reviews, 34 (6-10),
958-977.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics: Theory
and Practice,</em> Princeton University Press.
</p>
<p>Li, Q. and J.S. Racine (2004), &ldquo;Cross-validated local linear
nonparametric regression,&rdquo; Statistica Sinica, 14, 485-512.
</p>
<p>Pagan, A. and A. Ullah (1999), <em>Nonparametric Econometrics,</em>
Cambridge University Press.
</p>
<p>Racine, J.S. and Q. Li (2004), &ldquo;Nonparametric estimation of regression
functions with both categorical and continuous data,&rdquo; Journal of
Econometrics, 119, 99-130.
</p>
<p>Robinson, P.M. (1988), &ldquo;Root-n-consistent semiparametric regression,&rdquo;
Econometrica, 56, 931-954. 
</p>
<p>Wang, M.C. and J. van Ryzin (1981), &ldquo;A class of smooth estimators
for discrete distributions,&rdquo;  Biometrika, 68, 301-309.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npregbw">npregbw</a></code>, <code><a href="#topic+npreg">npreg</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# EXAMPLE 1 (INTERFACE=FORMULA): For this example, we simulate an
# example for a partially linear model and compare the coefficient
# estimates from the partially linear model with those from a correctly
# specified parametric model...

set.seed(42)

n &lt;- 250
x1 &lt;- rnorm(n)
x2 &lt;- rbinom(n, 1, .5)

z1 &lt;- rbinom(n, 1, .5)
z2 &lt;- rnorm(n)

y &lt;- 1 + x1 + x2 + z1 + sin(z2) + rnorm(n)

# First, compute data-driven bandwidths. This may take a few minutes
# depending on the speed of your computer...

bw &lt;- npplregbw(formula=y~x1+factor(x2)|factor(z1)+z2)

# Next, compute the partially linear fit

pl &lt;- npplreg(bws=bw)

# Print a summary of the model...

summary(pl)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Retrieve the coefficient estimates and their standard errors...

coef(pl)
coef(pl, errors = TRUE)

# Compare the partially linear results to those from a correctly
# specified model's coefficients for x1 and x2

ols &lt;- lm(y~x1+factor(x2)+factor(z1)+I(sin(z2)))

# The intercept is coef()[1], and those for x1 and x2 are coef()[2] and
# coef()[3]. The standard errors are the square root of the diagonal of
# the variance-covariance matrix (elements 2 and 3)

coef(ols)[2:3]
sqrt(diag(vcov(ols)))[2:3]

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Plot the regression surfaces via plot() (i.e., plot the `partial
# regression surface plots').

plot(bw)

# Note - to plot regression surfaces with variability bounds constructed
# from bootstrapped standard errors, use the following (note that this
# may take a minute or two depending on the speed of your computer as
# the bootstrapping is done in real time, and note also that we override
# the default number of bootstrap replications (399) reducing them to 25
# in order to quickly compute standard errors in this instance - don't
# of course do this in general)

plot(bw,
     plot.errors.boot.num=25, 
     plot.errors.method="bootstrap")


# EXAMPLE 1 (INTERFACE=DATA FRAME): For this example, we simulate an
# example for a partially linear model and compare the coefficient
# estimates from the partially linear model with those from a correctly
# specified parametric model...

set.seed(42)

n &lt;- 250
x1 &lt;- rnorm(n)
x2 &lt;- rbinom(n, 1, .5)

z1 &lt;- rbinom(n, 1, .5)
z2 &lt;- rnorm(n)

y &lt;- 1 + x1 + x2 + z1 + sin(z2) + rnorm(n)

X &lt;- data.frame(x1, factor(x2))
Z &lt;- data.frame(factor(z1), z2)

# First, compute data-driven bandwidths. This may take a few minutes
# depending on the speed of your computer...

bw &lt;- npplregbw(xdat=X, zdat=Z, ydat=y)

# Next, compute the partially linear fit

pl &lt;- npplreg(bws=bw)

# Print a summary of the model...

summary(pl)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Retrieve the coefficient estimates and their standard errors...

coef(pl)
coef(pl, errors = TRUE)

# Compare the partially linear results to those from a correctly
# specified model's coefficients for x1 and x2

ols &lt;- lm(y~x1+factor(x2)+factor(z1)+I(sin(z2)))

# The intercept is coef()[1], and those for x1 and x2 are coef()[2] and
# coef()[3]. The standard errors are the square root of the diagonal of
# the variance-covariance matrix (elements 2 and 3)

coef(ols)[2:3]
sqrt(diag(vcov(ols)))[2:3]

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Plot the regression surfaces via plot() (i.e., plot the `partial
# regression surface plots').

plot(bw)

# Note - to plot regression surfaces with variability bounds constructed
# from bootstrapped standard errors, use the following (note that this
# may take a minute or two depending on the speed of your computer as
# the bootstrapping is done in real time, and note also that we override
# the default number of bootstrap replications (399) reducing them to 25
# in order to quickly compute standard errors in this instance - don't
# of course do this in general)

plot(bw,
     plot.errors.boot.num=25, 
     plot.errors.method="bootstrap")

## End(Not run) 
</code></pre>

<hr>
<h2 id='npplregbw'>Partially Linear Kernel Regression Bandwidth Selection with Mixed Data Types</h2><span id='topic+npplregbw'></span><span id='topic+npplregbw.NULL'></span><span id='topic+npplregbw.default'></span><span id='topic+npplregbw.formula'></span><span id='topic+npplregbw.plbandwidth'></span>

<h3>Description</h3>

<p><code>npplregbw</code> computes a bandwidth object for a partially linear
kernel regression estimate of a one (1) dimensional dependent variable
on <code class="reqn">p+q</code>-variate explanatory data, using the model <code class="reqn">Y = X\beta
  + \Theta (Z) + \epsilon</code> given a set of
estimation points, training points (consisting of explanatory data and
dependent data), and a bandwidth specification, which can be a
<code>rbandwidth</code> object, or a bandwidth vector, bandwidth type and
kernel type.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npplregbw(...)

## S3 method for class 'formula'
npplregbw(formula, data, subset, na.action, call, ...)

## S3 method for class 'NULL'
npplregbw(xdat = stop("invoked without data `xdat'"),
          ydat = stop("invoked without data `ydat'"),
          zdat = stop("invoked without data `zdat'"),
          bws,
          ...)

## Default S3 method:
npplregbw(xdat = stop("invoked without data `xdat'"),
          ydat = stop("invoked without data `ydat'"),
          zdat = stop("invoked without data `zdat'"),
          bws,
          ...,
          bandwidth.compute = TRUE,
          nmulti,
          remin,
          itmax,
          ftol,
          tol,
          small)

## S3 method for class 'plbandwidth'
npplregbw(xdat = stop("invoked without data `xdat'"),
          ydat = stop("invoked without data `ydat'"),
          zdat = stop("invoked without data `zdat'"),
          bws,
          nmulti,
          ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npplregbw_+3A_formula">formula</code></td>
<td>

<p>a symbolic description of variables on which bandwidth selection is
to be performed. The details of constructing a formula are
described below.
</p>
</td></tr>
<tr><td><code id="npplregbw_+3A_data">data</code></td>
<td>

<p>an optional data frame, list or environment (or object
coercible to a data frame by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code>) containing the variables
in the model. If not found in data, the variables are taken from
<code>environment(formula)</code>, typically the environment from which the
function is called.
</p>
</td></tr>
<tr><td><code id="npplregbw_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations to be used in
the fitting process. 
</p>
</td></tr>
<tr><td><code id="npplregbw_+3A_na.action">na.action</code></td>
<td>

<p>a function which indicates what should happen when the data contain
<code>NA</code>s. The default is set by the <code><a href="stats.html#topic+na.action">na.action</a></code> setting of options, and is
<code><a href="stats.html#topic+na.fail">na.fail</a></code> if that is unset. The (recommended) default is
<code><a href="stats.html#topic+na.omit">na.omit</a></code>.  
</p>
</td></tr>
<tr><td><code id="npplregbw_+3A_call">call</code></td>
<td>

<p>the original function call. This is passed internally by
<code><a href="#topic+np">np</a></code> when a bandwidth search has been implied by a call to
another function. It is not recommended that the user set this.
</p>
</td></tr>
<tr><td><code id="npplregbw_+3A_xdat">xdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of explanatory data (training data),
corresponding to <code class="reqn">X</code> in the model equation, whose linear
relationship with the dependent data <code class="reqn">Y</code> is posited.
</p>
</td></tr>
<tr><td><code id="npplregbw_+3A_ydat">ydat</code></td>
<td>

<p>a one (1) dimensional numeric or integer vector of dependent data, each
element <code class="reqn">i</code> corresponding to each observation (row) <code class="reqn">i</code> of
<code>xdat</code>. 
</p>
</td></tr>
<tr><td><code id="npplregbw_+3A_zdat">zdat</code></td>
<td>

<p>a <code class="reqn">q</code>-variate data frame of explanatory data (training data),
corresponding to <code class="reqn">Z</code> in the model equation, whose relationship
to the dependent variable is unspecified (nonparametric)
</p>
</td></tr>
<tr><td><code id="npplregbw_+3A_bws">bws</code></td>
<td>

<p>a bandwidth specification. This can be set as a <code>plbandwidth</code>
object returned from an invocation of <code>npplregbw</code>, or as
a matrix of bandwidths, where each row is a set of bandwidths for <code class="reqn">Z</code>,
with a column for each variable <code class="reqn">Z_i</code>. In the first row
are the bandwidths for the regression of <code class="reqn">Y</code> on <code class="reqn">Z</code>. The
following rows contain the bandwidths for the regressions of the
columns of <code class="reqn">X</code> on <code class="reqn">Z</code>.  If specified as a matrix,
additional arguments will need to be supplied as necessary to
specify the bandwidth type, kernel types, and so on.
</p>
<p>If left unspecified, <code>npplregbw</code> will search for optimal
bandwidths using <code><a href="#topic+npregbw">npregbw</a></code> in the course of
calculations. If specified, <code>npplregbw</code> will use the given
bandwidths as the starting point for the numerical search for optimal
bandwidths, unless you specify bandwidth.compute = FALSE. 
</p>
</td></tr>
<tr><td><code id="npplregbw_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to specify the regression type,
bandwidth type, kernel types, selection methods, and so on. To do
this, you may specify any of <code>regtype</code>, 
<code>bwmethod</code>, <code>bwscaling</code>, <code>bwtype</code>, <code>ckertype</code>,
<code>ckerorder</code>, <code>ukertype</code>, <code>okertype</code>, as described in
<code><a href="#topic+npregbw">npregbw</a></code>.
</p>
</td></tr>
<tr><td><code id="npplregbw_+3A_bandwidth.compute">bandwidth.compute</code></td>
<td>

<p>a logical value which specifies whether to do a numerical search for
bandwidths or not. If set to <code>FALSE</code>, a <code>plbandwidth</code> object
will be returned with bandwidths set to those specified
in <code>bws</code>. Defaults to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npplregbw_+3A_nmulti">nmulti</code></td>
<td>

<p>integer number of times to restart the process of finding extrema of
the cross-validation function from different (random) initial
points. Defaults to <code>min(5,ncol(zdat))</code>.
</p>
</td></tr>
<tr><td><code id="npplregbw_+3A_remin">remin</code></td>
<td>

<p>a logical value which when set as <code>TRUE</code> the search routine
restarts from located minima for a minor gain in accuracy. Defaults
to <code>TRUE</code>
</p>
</td></tr>
<tr><td><code id="npplregbw_+3A_itmax">itmax</code></td>
<td>

<p>integer number of iterations before failure in the numerical
optimization routine. Defaults to <code>10000</code>
</p>
</td></tr>
<tr><td><code id="npplregbw_+3A_ftol">ftol</code></td>
<td>

<p>tolerance on the value of the cross-validation function 
evaluated at located minima. Defaults to <code>1.19e-07
      (FLT_EPSILON)</code>
</p>
</td></tr>
<tr><td><code id="npplregbw_+3A_tol">tol</code></td>
<td>

<p>tolerance on the position of located minima of the 
cross-validation function. Defaults to <code>1.49e-08
      (sqrt(DBL_EPSILON))</code>
</p>
</td></tr>
<tr><td><code id="npplregbw_+3A_small">small</code></td>
<td>
<p> a small number, at about the precision of the data type
used. Defaults to <code>2.22e-16 (DBL_EPSILON)</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>npplregbw</code> implements a variety of methods for nonparametric
regression on multivariate (<code class="reqn">q</code>-variate) explanatory data defined
over a set of possibly continuous and/or discrete (unordered, ordered)
data. The approach is based on Li and Racine (2003), who employ
&lsquo;generalized product kernels&rsquo; that admit a mix of continuous and
discrete data types.
</p>
<p>Three classes of kernel estimators for the continuous data types are
available: fixed, adaptive nearest-neighbor, and generalized
nearest-neighbor. Adaptive nearest-neighbor bandwidths change with
each sample realization in the set, <code class="reqn">x_i</code>, when estimating the
density at the point <code class="reqn">x</code>. Generalized nearest-neighbor bandwidths change
with the point at which the density is estimated, <code class="reqn">x</code>. Fixed bandwidths
are constant over the support of <code class="reqn">x</code>.
</p>
<p><code>npplregbw</code> may be invoked <em>either</em> with a formula-like
symbolic 
description of variables on which bandwidth selection is to be
performed <em>or</em> through a simpler interface whereby data is passed
directly to the function via the <code>xdat</code>, <code>ydat</code>, and
<code>zdat</code> 
parameters. Use of these two interfaces is <b>mutually exclusive</b>.
</p>
<p>Data contained in the data frame <code>zdat</code> may be a mix of continuous
(default), unordered discrete (to be specified in the data frame
<code>zdat</code> using <code><a href="base.html#topic+factor">factor</a></code>), and ordered discrete (to be
specified in the data frame <code>zdat</code> using
<code><a href="base.html#topic+ordered">ordered</a></code>). Data can be entered in an arbitrary order and
data types will be detected automatically by the routine (see
<code><a href="#topic+np">np</a></code> for details).
</p>
<p>Data for which bandwidths are to be estimated may be specified
symbolically. A typical description has the form <code>dependent</code>
<code>data</code> <code>~</code> <code>parametric</code> <code>explanatory</code> <code>data</code>
<code>|</code> <code>nonparametric</code> <code>explanatory</code> <code>data</code>,
where <code>dependent</code> <code>data</code> is a univariate response, and
<code>parametric</code> <code>explanatory</code> <code>data</code> and
<code>nonparametric</code> <code>explanatory</code>
<code>data</code> are both series of variables specified by name, separated by
the separation character '+'. For example, <code> y1 ~ x1 + x2 | z1 </code>
specifies that the bandwidth object for the partially linear model with
response <code>y1</code>, linear parametric regressors <code>x1</code> and
<code>x2</code>, and 
nonparametric regressor <code>z1</code> is to be estimated. See below for
further examples.  
</p>
<p>A variety of kernels may be specified by the user. Kernels implemented
for continuous data types include the second, fourth, sixth, and eighth
order Gaussian and Epanechnikov kernels, and the uniform
kernel. Unordered discrete data types use a variation on Aitchison and
Aitken's (1976) kernel, while ordered data types use a variation of the
Wang and van Ryzin (1981) kernel.
</p>


<h3>Value</h3>

<p>if <code>bwtype</code> is set to <code>fixed</code>, an object containing bandwidths
(or scale factors if <code>bwscaling = TRUE</code>) is returned. If it is set to
<code>generalized_nn</code> or <code>adaptive_nn</code>, then instead the <code class="reqn">k</code>th nearest
neighbors are returned for the continuous variables while the discrete
kernel bandwidths are returned for the discrete variables. Bandwidths
are stored in a list under the component name <code>bw</code>. Each element
is an <code>rbandwidth</code> object. The first
element of the list corresponds to the regression of <code class="reqn">Y</code> on <code class="reqn">Z</code>.  
Each subsequent element is the bandwidth object corresponding to the
regression of the <code class="reqn">i</code>th column of <code class="reqn">X</code> on <code class="reqn">Z</code>. See examples
for more information.
</p>


<h3>Usage Issues</h3>

<p>If you are using data of mixed types, then it is advisable to use the
<code><a href="base.html#topic+data.frame">data.frame</a></code> function to construct your input data and not
<code><a href="base.html#topic+cbind">cbind</a></code>, since <code><a href="base.html#topic+cbind">cbind</a></code> will typically not work as
intended on mixed data types and will coerce the data to the same
type.
</p>
<p>Caution: multivariate data-driven bandwidth selection methods are, by
their nature, <em>computationally intensive</em>. Virtually all methods
require dropping the <code class="reqn">i</code>th observation from the data set, computing an
object, repeating this for all observations in the sample, then
averaging each of these leave-one-out estimates for a <em>given</em>
value of the bandwidth vector, and only then repeating this a large
number of times in order to conduct multivariate numerical
minimization/maximization. Furthermore, due to the potential for local
minima/maxima, <em>restarting this procedure a large number of times may
often be necessary</em>. This can be frustrating for users possessing
large datasets. For exploratory purposes, you may wish to override the
default search tolerances, say, setting ftol=.01 and tol=.01 and
conduct multistarting (the default is to restart min(5, ncol(zdat))
times) as is done for a number of examples. Once the procedure
terminates, you can restart search with default tolerances using those
bandwidths obtained from the less rigorous search (i.e., set
<code>bws=bw</code> on subsequent calls to this routine where <code>bw</code> is
the initial bandwidth object).  A version of this package using the
<code>Rmpi</code> wrapper is under development that allows one to deploy
this software in a clustered computing environment to facilitate
computation involving large datasets.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine
<a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Aitchison, J. and C.G.G. Aitken (1976), &ldquo;Multivariate binary
discrimination by the kernel method,&rdquo; Biometrika, 63, 413-420.
</p>
<p>Gao, Q. and L. Liu and J.S. Racine (2015), &ldquo;A partially linear
kernel estimator for categorical data,&rdquo; Econometric Reviews, 34 (6-10),
958-977.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics: Theory
and Practice,</em> Princeton University Press.
</p>
<p>Li, Q. and J.S. Racine (2004), &ldquo;Cross-validated local linear
nonparametric regression,&rdquo; Statistica Sinica, 14, 485-512.
</p>
<p>Pagan, A. and A. Ullah (1999), <em>Nonparametric Econometrics,</em>
Cambridge University Press.
</p>
<p>Racine, J.S. and Q. Li (2004), &ldquo;Nonparametric estimation of regression
functions with both categorical and continuous data,&rdquo; Journal of
Econometrics, 119, 99-130.
</p>
<p>Robinson, P.M. (1988), &ldquo;Root-n-consistent semiparametric regression,&rdquo;
Econometrica, 56, 931-954. 
</p>
<p>Wang, M.C. and J. van Ryzin (1981), &ldquo;A class of smooth estimators
for discrete distributions,&rdquo;  Biometrika, 68, 301-309.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npregbw">npregbw</a></code>, <code><a href="#topic+npreg">npreg</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# EXAMPLE 1 (INTERFACE=FORMULA): For this example, we simulate an
# example for a partially linear model and perform bandwidth selection

set.seed(42)

n &lt;- 250
x1 &lt;- rnorm(n)
x2 &lt;- rbinom(n, 1, .5)

z1 &lt;- rbinom(n, 1, .5)
z2 &lt;- rnorm(n)

y &lt;- 1 + x1 + x2 + z1 + sin(z2) + rnorm(n)

X &lt;- data.frame(x1, factor(x2))
Z &lt;- data.frame(factor(z1), z2)

# Compute data-driven bandwidths... this may take a minute or two
# depending on the speed of your computer...

bw &lt;- npplregbw(formula=y~x1+factor(x2)|factor(z1)+z2)

summary(bw)

# Note - the default is to use the local constant estimator. If you wish
# to use instead a local linear estimator, this is accomplished via
# npplregbw(xdat=X, zdat=Z, ydat=y, regtype="ll")

# Note - see the example for npudensbw() for multiple illustrations
# of how to change the kernel function, kernel order, and so forth.

# You may want to manually specify your bandwidths
bw.mat &lt;- matrix(data =  c(0.19, 0.34,  # y on Z
                           0.00, 0.74,  # X[,1] on Z
                           0.29, 0.23), # X[,2] on Z
                ncol = ncol(Z), byrow=TRUE)

bw &lt;- npplregbw(formula=y~x1+factor(x2)|factor(z1)+z2, 
                          bws=bw.mat, bandwidth.compute=FALSE)
summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# You may want to tweak some of the bandwidths
bw$bw[[1]] # y on Z, alternatively bw$bw$yzbw
bw$bw[[1]]$bw &lt;- c(0.17, 0.30)

bw$bw[[2]] # X[,1] on Z
bw$bw[[2]]$bw[1] &lt;- 0.00054

summary(bw)

# EXAMPLE 1 (INTERFACE=DATA FRAME): For this example, we simulate an
# example for a partially linear model and perform bandwidth selection

set.seed(42)

n &lt;- 250
x1 &lt;- rnorm(n)
x2 &lt;- rbinom(n, 1, .5)

z1 &lt;- rbinom(n, 1, .5)
z2 &lt;- rnorm(n)

y &lt;- 1 + x1 + x2 + z1 + sin(z2) + rnorm(n)

X &lt;- data.frame(x1, factor(x2))
Z &lt;- data.frame(factor(z1), z2)

# Compute data-driven bandwidths... this may take a minute or two
# depending on the speed of your computer...

bw &lt;- npplregbw(xdat=X, zdat=Z, ydat=y)

summary(bw)

# Note - the default is to use the local constant estimator. If you wish
# to use instead a local linear estimator, this is accomplished via
# npplregbw(xdat=X, zdat=Z, ydat=y, regtype="ll")

# Note - see the example for npudensbw() for multiple illustrations
# of how to change the kernel function, kernel order, and so forth.

# You may want to manually specify your bandwidths
bw.mat &lt;- matrix(data =  c(0.19, 0.34,  # y on Z
                           0.00, 0.74,  # X[,1] on Z
                           0.29, 0.23), # X[,2] on Z
                ncol = ncol(Z), byrow=TRUE)

bw &lt;- npplregbw(xdat=X, zdat=Z, ydat=y, 
                          bws=bw.mat, bandwidth.compute=FALSE)
summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# You may want to tweak some of the bandwidths
bw$bw[[1]] # y on Z, alternatively bw$bw$yzbw
bw$bw[[1]]$bw &lt;- c(0.17, 0.30)

bw$bw[[2]] # X[,1] on Z
bw$bw[[2]]$bw[1] &lt;- 0.00054

summary(bw)

## End(Not run) 
</code></pre>

<hr>
<h2 id='npqcmstest'> Kernel Consistent Quantile Regression Model Specification Test
with Mixed Data Types </h2><span id='topic+npqcmstest'></span>

<h3>Description</h3>

<p><code>npqcmstest</code> implements a consistent test for correct
specification of parametric quantile regression models (linear or
nonlinear) as described in Racine (2006) which extends the work of
Zheng (1998).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npqcmstest(formula,
           data = NULL,
           subset,
           xdat,
           ydat,
           model = stop(paste(sQuote("model")," has not been provided")),
           tau = 0.5,
           distribution = c("bootstrap", "asymptotic"),
           bwydat = c("y","varepsilon"),
           boot.method = c("iid","wild","wild-rademacher"),
           boot.num = 399,
           pivot = TRUE,
           density.weighted = TRUE,
           random.seed = 42,
           ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npqcmstest_+3A_formula">formula</code></td>
<td>

<p>a symbolic description of variables on which the test is
to be performed. The details of constructing a formula are
described below.
</p>
</td></tr>
<tr><td><code id="npqcmstest_+3A_data">data</code></td>
<td>

<p>an optional data frame, list or environment (or object
coercible to a data frame by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code>) containing the variables
in the model. If not found in data, the variables are taken from
<code>environment(formula)</code>, typically the environment from which the
function is called.
</p>
</td></tr>
<tr><td><code id="npqcmstest_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations to be used. 
</p>
</td></tr>
<tr><td><code id="npqcmstest_+3A_model">model</code></td>
<td>

<p>a model object obtained from a call to <code><a href="quantreg.html#topic+rq">rq</a></code>. Important: the
call to <code><a href="quantreg.html#topic+rq">rq</a></code> must have the argument <code>model=TRUE</code> or
<code>npqcmstest</code> will not work.
</p>
</td></tr>
<tr><td><code id="npqcmstest_+3A_xdat">xdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of explanatory data (training data) used to
calculate the quantile regression estimators.
</p>
</td></tr>
<tr><td><code id="npqcmstest_+3A_ydat">ydat</code></td>
<td>

<p>a one (1) dimensional numeric or integer vector of dependent data, each
element <code class="reqn">i</code> corresponding to each observation (row) <code class="reqn">i</code> of
<code>xdat</code>. 
</p>
</td></tr>
<tr><td><code id="npqcmstest_+3A_tau">tau</code></td>
<td>

<p>a numeric value specifying the <code class="reqn">\tau</code>th quantile is desired
</p>
</td></tr>
<tr><td><code id="npqcmstest_+3A_distribution">distribution</code></td>
<td>

<p>a character string used to specify the method of estimating the
distribution of the statistic to be calculated. <code>bootstrap</code>
will conduct bootstrapping. <code>asymptotic</code> will use the normal
distribution. Defaults to <code>bootstrap</code>.
</p>
</td></tr>
<tr><td><code id="npqcmstest_+3A_bwydat">bwydat</code></td>
<td>

<p>a character string used to specify the left hand side variable used
in bandwidth selection. <code>"varepsilon"</code> uses
<code class="reqn">1-\tau,-\tau</code> for <code>ydat</code> while <code>"y"</code> will
use <code class="reqn">y</code>. Defaults to <code>"y"</code>.
</p>
</td></tr>
<tr><td><code id="npqcmstest_+3A_boot.method">boot.method</code></td>
<td>

<p>a character string used to specify the bootstrap method.
<code>iid</code> will generate independent identically distributed
draws. <code>wild</code> will use a wild bootstrap. <code>wild-rademacher</code>
will use a wild bootstrap with Rademacher variables. Defaults to
<code>iid</code>.
</p>
</td></tr>
<tr><td><code id="npqcmstest_+3A_boot.num">boot.num</code></td>
<td>

<p>an integer value specifying the number of bootstrap replications to
use. Defaults to <code>399</code>.
</p>
</td></tr>
<tr><td><code id="npqcmstest_+3A_pivot">pivot</code></td>
<td>

<p>a logical value specifying whether the statistic should be
normalised such that it approaches <code class="reqn">N(0,1)</code> in
distribution. Defaults to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npqcmstest_+3A_density.weighted">density.weighted</code></td>
<td>

<p>a logical value specifying whether the statistic should be
weighted by the density of <code>xdat</code>. Defaults to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npqcmstest_+3A_random.seed">random.seed</code></td>
<td>

<p>an integer used to seed R's random number generator. This is to
ensure replicability. Defaults to 42.
</p>
</td></tr>
<tr><td><code id="npqcmstest_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to control bandwidth selection on the
residuals. One can specify the bandwidth type,
kernel types, and so on. To do this, you may specify any of <code>bwscaling</code>,
<code>bwtype</code>, <code>ckertype</code>, <code>ckerorder</code>, <code>ukertype</code>,
<code>okertype</code>, as described in <code><a href="#topic+npregbw">npregbw</a></code>.
This is necessary if you specify <code>bws</code> as a <code class="reqn">p</code>-vector and not
a <code>bandwidth</code> object, and you do not desire the default behaviours.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>npqcmstest</code> returns an object of type <code>cmstest</code> with the
following components. Components will contain information
related to <code>Jn</code> or <code>In</code> depending on the value of <code>pivot</code>:
</p>
<table>
<tr><td><code>Jn</code></td>
<td>
<p> the statistic <code>Jn</code> </p>
</td></tr>
<tr><td><code>In</code></td>
<td>
<p> the statistic <code>In</code> </p>
</td></tr>
<tr><td><code>Omega.hat</code></td>
<td>
<p> as described in Racine, J.S. (2006). </p>
</td></tr>
<tr><td><code>q.*</code></td>
<td>
<p> the various quantiles of the statistic <code>Jn</code> (or
<code>In</code> if 
<code>pivot=FALSE</code>)  are in
components <code>q.90</code>,
<code>q.95</code>, <code>q.99</code> (one-sided 1%, 5%, 10% critical values) </p>
</td></tr>
<tr><td><code>P</code></td>
<td>
<p> the P-value of the statistic </p>
</td></tr>
<tr><td><code>Jn.bootstrap</code></td>
<td>
<p> if <code>pivot=TRUE</code> contains the bootstrap
replications of <code>Jn</code> </p>
</td></tr>
<tr><td><code>In.bootstrap</code></td>
<td>
<p> if <code>pivot=FALSE</code> contains the bootstrap
replications of <code>In</code> </p>
</td></tr>
</table>
<p><code><a href="base.html#topic+summary">summary</a></code> supports object of type <code>cmstest</code>.
</p>


<h3>Usage Issues</h3>

<p>If you are using data of mixed types, then it is advisable to use the
<code><a href="base.html#topic+data.frame">data.frame</a></code> function to construct your input data and not
<code><a href="base.html#topic+cbind">cbind</a></code>, since <code><a href="base.html#topic+cbind">cbind</a></code> will typically not work as
intended on mixed data types and will coerce the data to the same
type.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine
<a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Aitchison, J. and C.G.G. Aitken (1976), &ldquo;Multivariate binary
discrimination by the kernel method,&rdquo; Biometrika, 63, 413-420.
</p>
<p>Koenker, R.W. and G.W. Bassett (1978), &ldquo;Regression quantiles,&rdquo;
Econometrica, 46, 33-50.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics: Theory
and Practice,</em> Princeton University Press.
</p>
<p>Murphy, K. M. and F. Welch (1990), &ldquo;Empirical age-earnings
profiles,&rdquo; Journal of Labor Economics, 8, 202-229.
</p>
<p>Pagan, A. and A. Ullah (1999), <em>Nonparametric Econometrics,</em>
Cambridge University Press.
</p>
<p>Racine, J.S. (2006), &ldquo;Consistent specification testing of
heteroskedastic parametric regression quantile models with mixed
data,&rdquo; manuscript.
</p>
<p>Wang, M.C. and J. van Ryzin (1981), &ldquo;A class of smooth estimators
for discrete distributions,&rdquo;  Biometrika, 68, 301-309.
</p>
<p>Zheng, J. (1998), &ldquo;A consistent nonparametric test of
parametric regression models under conditional quantile
restrictions,&rdquo; Econometric Theory, 14, 123-138.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# EXAMPLE 1: For this example, we conduct a consistent quantile regression
# model specification test for a parametric wage quantile regression
# model that is quadratic in age. The work of Murphy and Welch (1990)
# would suggest that this parametric quantile regression model is
# misspecified.

library("quantreg")

data("cps71")
attach(cps71)

model &lt;- rq(logwage~age+I(age^2), tau=0.5, model=TRUE)

plot(age, logwage)
lines(age, fitted(model))

X &lt;- data.frame(age)

# Note - this may take a few minutes depending on the speed of your
# computer...

npqcmstest(model = model, xdat = X, ydat = logwage, tau=0.5)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Next try Murphy &amp; Welch's (1990) suggested quintic specification.

model &lt;- rq(logwage~age+I(age^2)+I(age^3)+I(age^4)+I(age^5), model=TRUE)

plot(age, logwage)
lines(age, fitted(model))

X &lt;- data.frame(age)

# Note - this may take a few minutes depending on the speed of your
# computer...

npqcmstest(model = model, xdat = X, ydat = logwage, tau=0.5)

detach(cps71)

## End(Not run) 
</code></pre>

<hr>
<h2 id='npqreg'>Kernel Quantile Regression with Mixed Data Types</h2><span id='topic+npqreg'></span><span id='topic+npqreg.call'></span><span id='topic+npqreg.condbandwidth'></span><span id='topic+npqreg.default'></span><span id='topic+npqreg.formula'></span>

<h3>Description</h3>

<p><code>npqreg</code> computes a kernel quantile regression estimate of a one
(1) dimensional dependent variable on <code class="reqn">p</code>-variate explanatory
data, given a set of evaluation points, training points (consisting of
explanatory data and dependent data), and a bandwidth specification
using the methods of Li and Racine (2008) and Li, Lin and Racine
(2013). A bandwidth specification can be a <code>condbandwidth</code> object,
or a bandwidth vector, bandwidth type and kernel type.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npqreg(bws, ...)

## S3 method for class 'formula'
npqreg(bws, data = NULL, newdata = NULL, ...)

## S3 method for class 'call'
npqreg(bws, ...)

## S3 method for class 'condbandwidth'
npqreg(bws,
       txdat = stop("training data 'txdat' missing"),
       tydat = stop("training data 'tydat' missing"),
       exdat,
       tau = 0.5,
       gradients = FALSE,
       ftol = 1.490116e-07,
       tol = 1.490116e-04,
       small = 1.490116e-05,
       itmax = 10000,
       lbc.dir = 0.5,
       dfc.dir = 3,
       cfac.dir = 2.5*(3.0-sqrt(5)),
       initc.dir = 1.0,
       lbd.dir = 0.1,
       hbd.dir = 1,
       dfac.dir = 0.25*(3.0-sqrt(5)),
       initd.dir = 1.0,
       ...)

## Default S3 method:
npqreg(bws, txdat, tydat, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npqreg_+3A_bws">bws</code></td>
<td>

<p>a bandwidth specification. This can be set as a <code>condbandwidth</code>
object returned from an invocation of <code><a href="#topic+npcdistbw">npcdistbw</a></code>, or
as a vector of bandwidths, with each element <code class="reqn">i</code> corresponding
to the bandwidth for column <code class="reqn">i</code> in <code>txdat</code>. If specified as
a vector, then additional arguments will need to be supplied as
necessary to specify the bandwidth type, kernel types, and so on.
</p>
</td></tr>
<tr><td><code id="npqreg_+3A_tau">tau</code></td>
<td>

<p>a numeric value specifying the <code class="reqn">\tau</code>th quantile is
desired. Defaults to <code>0.5</code>.
</p>
</td></tr>
<tr><td><code id="npqreg_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to specify the regression type,
bandwidth type, kernel types, training data, and so on.
To do this,
you may specify any of <code>bwmethod</code>, <code>bwscaling</code>,
<code>bwtype</code>, <code>cxkertype</code>, <code>cxkerorder</code>,
<code>cykertype</code>, <code>cykerorder</code>, <code>uxkertype</code>,
<code>uykertype</code>, <code>oxkertype</code>, <code>oykertype</code>, as described
in <code><a href="#topic+npcdistbw">npcdistbw</a></code>. 
</p>
</td></tr>
<tr><td><code id="npqreg_+3A_data">data</code></td>
<td>

<p>an optional data frame, list or environment (or object
coercible to a data frame by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code>) containing the variables
in the model. If not found in data, the variables are taken from
<code>environment(bws)</code>, typically the environment from which
<code><a href="#topic+npcdistbw">npcdistbw</a></code> was called.
</p>
</td></tr>
<tr><td><code id="npqreg_+3A_newdata">newdata</code></td>
<td>

<p>An optional data frame in which to look for evaluation data. If
omitted, the training data are used.  
</p>
</td></tr>
<tr><td><code id="npqreg_+3A_txdat">txdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of explanatory data (training data) used to
calculate the regression estimators. Defaults to the training data used to
compute the bandwidth object.
</p>
</td></tr>
<tr><td><code id="npqreg_+3A_tydat">tydat</code></td>
<td>

<p>a one (1) dimensional numeric or integer vector of dependent data, each
element <code class="reqn">i</code> corresponding to each observation (row) <code class="reqn">i</code> of
<code>txdat</code>. Defaults to the training data used to
compute the bandwidth object.
</p>
</td></tr>
<tr><td><code id="npqreg_+3A_exdat">exdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of points on which the regression will be
estimated (evaluation data). By default,
evaluation takes place on the data provided by <code>txdat</code>.
</p>
</td></tr>
<tr><td><code id="npqreg_+3A_gradients">gradients</code></td>
<td>

<p>[currently not supported] a logical value indicating that you want
gradients computed and returned in the resulting <code>npregression</code>
object. Defaults to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="npqreg_+3A_itmax">itmax</code></td>
<td>

<p>integer number of iterations before failure in the numerical
optimization routine. Defaults to <code>10000</code>.
</p>
</td></tr>
<tr><td><code id="npqreg_+3A_ftol">ftol</code></td>
<td>

<p>fractional tolerance on the value of the cross-validation function
evaluated at located minima (of order the machine precision or
perhaps slightly larger so as not to be diddled by
roundoff). Defaults to <code>1.490116e-07</code>
(1.0e+01*sqrt(.Machine$double.eps)).
</p>
</td></tr>
<tr><td><code id="npqreg_+3A_tol">tol</code></td>
<td>

<p>tolerance on the position of located minima of the cross-validation
function (tol should generally be no smaller than the square root of
your machine's floating point precision). Defaults to <code>
      1.490116e-04 (1.0e+04*sqrt(.Machine$double.eps))</code>.
</p>
</td></tr>
<tr><td><code id="npqreg_+3A_small">small</code></td>
<td>

<p>a small number used to bracket a minimum (it is hopeless to ask for
a bracketing interval of width less than sqrt(epsilon) times its
central value, a fractional width of only about 10-04 (single
precision) or 3x10-8 (double precision)). Defaults to <code>small
      = 1.490116e-05 (1.0e+03*sqrt(.Machine$double.eps))</code>.
</p>
</td></tr>
<tr><td><code id="npqreg_+3A_lbc.dir">lbc.dir</code>, <code id="npqreg_+3A_dfc.dir">dfc.dir</code>, <code id="npqreg_+3A_cfac.dir">cfac.dir</code>, <code id="npqreg_+3A_initc.dir">initc.dir</code></td>
<td>
<p> lower bound, chi-square
degrees of freedom, stretch factor, and initial non-random values
for direction set search for Powell's algorithm for <code>numeric</code>
variables. See Details</p>
</td></tr>
<tr><td><code id="npqreg_+3A_lbd.dir">lbd.dir</code>, <code id="npqreg_+3A_hbd.dir">hbd.dir</code>, <code id="npqreg_+3A_dfac.dir">dfac.dir</code>, <code id="npqreg_+3A_initd.dir">initd.dir</code></td>
<td>
<p> lower bound, upper bound,
stretch factor, and initial non-random values for direction set
search for Powell's algorithm for categorical variables. See
Details</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The optimizer invoked for search is Powell's conjugate direction
method which requires the setting of (non-random) initial values and
search directions for bandwidths, and, when restarting, random values
for successive invocations. Bandwidths for <code>numeric</code> variables
are scaled by robust measures of spread, the sample size, and the
number of <code>numeric</code> variables where appropriate. Two sets of
parameters for bandwidths for <code>numeric</code> can be modified, those
for initial values for the parameters themselves, and those for the
directions taken (Powell's algorithm does not involve explicit
computation of the function's gradient). The default values are set by
considering search performance for a variety of difficult test cases
and simulated cases. We highly recommend restarting search a large
number of times to avoid the presence of local minima (achieved by
modifying <code>nmulti</code>). Further refinement for difficult cases can
be achieved by modifying these sets of parameters. However, these
parameters are intended more for the authors of the package to enable
&lsquo;tuning&rsquo; for various methods rather than for the user themselves.
</p>


<h3>Value</h3>

<p><code>npqreg</code> returns a <code>npqregression</code> object.  The generic
functions <code><a href="stats.html#topic+fitted">fitted</a></code> (or <code><a href="stats.html#topic+quantile">quantile</a></code>),
<code><a href="#topic+se">se</a></code>, <code><a href="stats.html#topic+predict">predict</a></code> (when using
<code><a href="stats.html#topic+predict">predict</a></code> you must add the argument <code>tau=</code> to
generate predictions other than the median), and
<code><a href="#topic+gradients">gradients</a></code>, extract (or generate) estimated values,
asymptotic standard errors on estimates, predictions, and gradients,
respectively, from the returned object. Furthermore, the functions
<code><a href="base.html#topic+summary">summary</a></code> and <code><a href="base.html#topic+plot">plot</a></code> support objects of this
type. The returned object has the following components:
</p>
<table>
<tr><td><code>eval</code></td>
<td>
<p> evaluation points </p>
</td></tr>
<tr><td><code>quantile</code></td>
<td>
<p> estimation of the quantile regression function
(conditional quantile) at the  evaluation points </p>
</td></tr>
<tr><td><code>quanterr</code></td>
<td>
<p> standard errors of the quantile regression estimates </p>
</td></tr> 
<tr><td><code>quantgrad</code></td>
<td>
<p> gradients at each evaluation point </p>
</td></tr>
<tr><td><code>tau</code></td>
<td>
<p> the <code class="reqn">\tau</code>th quantile computed </p>
</td></tr>
</table>


<h3>Usage Issues</h3>

<p>If you are using data of mixed types, then it is advisable to use the
<code><a href="base.html#topic+data.frame">data.frame</a></code> function to construct your input data and not
<code><a href="base.html#topic+cbind">cbind</a></code>, since <code><a href="base.html#topic+cbind">cbind</a></code> will typically not work as
intended on mixed data types and will coerce the data to the same
type.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Aitchison, J. and C.G.G. Aitken (1976), &ldquo;Multivariate binary
discrimination by the kernel method,&rdquo; Biometrika, 63, 413-420.
</p>
<p>Hall, P. and J.S. Racine and Q. Li (2004), &ldquo;Cross-validation
and the estimation of conditional probability densities,&rdquo; Journal of
the American Statistical Association, 99, 1015-1026.
</p>
<p>Koenker, R. W. and G.W. Bassett (1978), &ldquo;Regression
quantiles,&rdquo; Econometrica, 46, 33-50.
</p>
<p>Koenker, R. (2005), <em>Quantile Regression,</em> Econometric Society
Monograph Series, Cambridge University Press.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics:
Theory and Practice,</em> Princeton University Press.
</p>
<p>Li, Q. and J.S. Racine (2008), &ldquo;Nonparametric estimation of
conditional CDF and quantile functions with mixed categorical and
continuous data,&rdquo; Journal of Business and Economic Statistics, 26,
423-434.
</p>
<p>Li, Q. and J. Lin and J.S. Racine (2013), &ldquo;Optimal Bandwidth
Selection for Nonparametric Conditional Distribution and Quantile
Functions&rdquo;, Journal of Business and Economic Statistics, 31, 57-65.
</p>
<p>Wang, M.C. and J. van Ryzin (1981), &ldquo;A class of smooth
estimators for discrete distributions,&rdquo; Biometrika, 68, 301-309.
</p>


<h3>See Also</h3>

 <p><span class="pkg">quantreg</span> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# EXAMPLE 1 (INTERFACE=FORMULA): For this example, we compute a
# bivariate nonparametric quantile regression estimate for Giovanni
# Baiocchi's Italian income panel (see Italy for details)

data("Italy")
attach(Italy)

# First, compute the cross-validated bandwidths.  Note - this may take a
# few minutes depending on the speed of your computer...

bw &lt;- npcdistbw(formula=gdp~ordered(year))

# Note - numerical search for computing the quantiles may take a minute
# or so...

model.q0.25 &lt;- npqreg(bws=bw, tau=0.25)
model.q0.50 &lt;- npqreg(bws=bw, tau=0.50)
model.q0.75 &lt;- npqreg(bws=bw, tau=0.75)

# Plot the resulting quantiles manually...

plot(ordered(year), gdp, 
     main="CDF Quantile Estimates for the Italian Income Panel", 
     xlab="Year", 
     ylab="GDP Quantiles")

lines(ordered(year), model.q0.25$quantile, col="red", lty=2)
lines(ordered(year), model.q0.50$quantile, col="blue", lty=3)
lines(ordered(year), model.q0.75$quantile, col="red", lty=2)

legend(ordered(1951), 32, c("tau = 0.25", "tau = 0.50", "tau = 0.75"), 
       lty=c(2, 3, 2), col=c("red", "blue", "red"))

detach(Italy)

# EXAMPLE 1 (INTERFACE=DATA FRAME): For this example, we compute a
# bivariate nonparametric quantile regression estimate for Giovanni
# Baiocchi's Italian income panel (see Italy for details)

data("Italy")
attach(Italy)
data &lt;- data.frame(ordered(year), gdp)

# First, compute the likelihood cross-validation bandwidths (default).
# Note - this may take a few minutes depending on the speed of your
# computer...

bw &lt;- npcdistbw(xdat=ordered(year), ydat=gdp)

# Note - numerical search for computing the quantiles will take a
# minute or so...

model.q0.25 &lt;- npqreg(bws=bw, tau=0.25)
model.q0.50 &lt;- npqreg(bws=bw, tau=0.50)
model.q0.75 &lt;- npqreg(bws=bw, tau=0.75)

# Plot the resulting quantiles manually...

plot(ordered(year), gdp, 
     main="CDF Quantile Estimates for the Italian Income Panel", 
     xlab="Year", 
     ylab="GDP Quantiles")

lines(ordered(year), model.q0.25$quantile, col="red", lty=2)
lines(ordered(year), model.q0.50$quantile, col="blue", lty=3)
lines(ordered(year), model.q0.75$quantile, col="red", lty=2)

legend(ordered(1951), 32, c("tau = 0.25", "tau = 0.50", "tau = 0.75"), 
       lty=c(2, 3, 2), col=c("red", "blue", "red"))

detach(Italy)

## End(Not run) 
</code></pre>

<hr>
<h2 id='npquantile'> Kernel Univariate Quantile Estimation </h2><span id='topic+npquantile'></span>

<h3>Description</h3>

<p><code>npquantile</code> computes smooth quantiles from a univariate
unconditional kernel cumulative distribution estimate given data and,
optionally, a bandwidth specification i.e. a <code>dbandwidth</code> object
using the bandwidth selection method of Li, Li and Racine (2017).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npquantile(x = NULL,
           tau = c(0.01,0.05,0.25,0.50,0.75,0.95,0.99),
           num.eval = 10000,
           bws = NULL,
           f = 1,
           ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npquantile_+3A_x">x</code></td>
<td>

<p>a univariate vector of type <code><a href="base.html#topic+numeric">numeric</a></code> containing sample
realizations (training data) used to estimate the cumulative
distribution (must be the same training data used to compute the
bandwidth object <code>bws</code> passed in).
</p>
</td></tr>
<tr><td><code id="npquantile_+3A_tau">tau</code></td>
<td>

<p>an optional vector containing the probabilities for quantile(s) to
be estimated (must contain numbers in <code class="reqn">[0,1]</code>). Defaults to
<code>c(0.01,0.05,0.25,0.50,0.75,0.95,0.99)</code>.
</p>
</td></tr>
<tr><td><code id="npquantile_+3A_num.eval">num.eval</code></td>
<td>

<p>an optional integer specifying the length of the grid on which the
quasi-inverse is computed. Defaults to <code>10000</code>.
</p>
</td></tr>
<tr><td><code id="npquantile_+3A_bws">bws</code></td>
<td>

<p>an optional <code>dbandwidth</code> specification (if already computed
avoid unnecessary computation inside <code>npquantile</code>). This must
be set as a <code>dbandwidth</code> object returned from an invocation of
<code><a href="#topic+npudistbw">npudistbw</a></code>. If not provided <code><a href="#topic+npudistbw">npudistbw</a></code> is
invoked with optional arguments passed via <code>...</code>.
</p>
</td></tr>
<tr><td><code id="npquantile_+3A_f">f</code></td>
<td>

<p>an optional argument fed to <code><a href="grDevices.html#topic+extendrange">extendrange</a></code>. Defaults to
<code>1</code>. See <code>?<a href="grDevices.html#topic+extendrange">extendrange</a></code> for details.
</p>
</td></tr>
<tr><td><code id="npquantile_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to specify the bandwidth type, kernel
types, bandwidth selection methods, and so on. See
<code>?<a href="#topic+npudistbw">npudistbw</a></code> for details.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Typical usage is
</p>
<pre>
    x &lt;- rchisq(100,df=10)
    npquantile(x)
  </pre>
<p>The quantile function <code class="reqn">q_\tau</code> is defined to be the
left-continuous inverse of the distribution function <code class="reqn">F(x)</code>,
i.e. <code class="reqn">q_\tau = \inf\{x: F(x) \ge \tau\}</code>.
</p>
<p>A traditional estimator of <code class="reqn">q_\tau</code> is the <code class="reqn">\tau</code>th sample
quantile. However, these estimates suffer from lack of efficiency
arising from variability of individual order statistics; see Sheather
and Marron (1990) and Hyndman and Fan (1996) for methods that
interpolate/smooth the order statistics, each of which discussed in
the latter can be invoked through <code><a href="stats.html#topic+quantile">quantile</a></code> via
<code>type=j</code>, <code>j=1,...,9</code>.
</p>
<p>The function <code>npquantile</code> implements a method for estimating
smooth quantiles based on the quasi-inverse of a <code><a href="#topic+npudist">npudist</a></code>
object where <code class="reqn">F(x)</code> is replaced with its kernel estimator and
bandwidth selection is that appropriate for such objects; see
Definition 2.3.6, page 21, Nelsen 2006 for a definition of the
quasi-inverse of <code class="reqn">F(x)</code>.
</p>
<p>For construction of the quasi-inverse we create a grid of evaluation
points based on the function <code><a href="grDevices.html#topic+extendrange">extendrange</a></code> along with the
sample quantiles themselves computed from invocation of
<code><a href="stats.html#topic+quantile">quantile</a></code>. The coarseness of the grid defined by
<code><a href="grDevices.html#topic+extendrange">extendrange</a></code> (which has been passed the option
<code>f=1</code>) is controlled by <code>num.eval</code>.
</p>
<p>Note that for any value of <code class="reqn">\tau</code> less/greater than the
smallest/largest value of <code class="reqn">F(x)</code> computed for the evaluation data
(i.e. that outlined in the paragraph above), the quantile returned for
such values is that associated with the smallest/largest value of
<code class="reqn">F(x)</code>, respectively.
</p>


<h3>Value</h3>

<p><code><a href="#topic+npquantile">npquantile</a></code> returns a vector of quantiles corresponding
to <code>tau</code>.
</p>


<h3>Usage Issues</h3>

<p>Cross-validated bandwidth selection is used by default
(<code><a href="#topic+npudistbw">npudistbw</a></code>). For large datasets this can be
computationally demanding. In such cases one might instead consider a
rule-of-thumb bandwidth (<code>bwmethod="normal-reference"</code>) or,
alternatively, use kd-trees (<code>options(np.tree=TRUE)</code> along with a
bounded kernel (<code>ckertype="epanechnikov"</code>)), both of which will
reduce the computational burden appreciably.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine
<a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Cheng, M.-Y. and Sun, S. (2006), &ldquo;Bandwidth selection for
kernel quantile estimation,&rdquo; Journal of the Chinese Statistical
Association, <b>44</b>, 271-295.
</p>
<p>Hyndman, R.J. and Fan, Y. (1996), &ldquo;Sample quantiles in
statistical packages,&rdquo; American Statistician, <b>50</b>, 361-365.
</p>
<p>Li, Q. and J.S. Racine (2017), &ldquo;Smooth Unconditional Quantile
Estimation,&rdquo; Manuscript.
</p>
<p>Li, C. and H. Li and J.S. Racine (2017), &ldquo;Cross-Validated Mixed
Datatype Bandwidth Selection for Nonparametric Cumulative
Distribution/Survivor Functions,&rdquo; Econometric Reviews, <b>36</b>,
970-987.
</p>
<p>Nelsen, R.B. (2006), <em>An Introduction to Copulas,</em> Second
Edition, Springer-Verlag.
</p>
<p>Sheather, S. and J.S. Marron (1990), &ldquo;Kernel quantile
estimators,&rdquo; Journal of the American Statistical Association, Vol. 85,
No. 410, 410-416.
</p>
<p>Yang, S.-S. (1985), &ldquo;A Smooth Nonparametric Estimator of a
Quantile Function,&rdquo; Journal of the American Statistical Association,
<b>80</b>, 1004-1011.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+quantile">quantile</a></code> for various types of sample quantiles;
<code><a href="stats.html#topic+ecdf">ecdf</a></code> for empirical distributions of which
<code><a href="stats.html#topic+quantile">quantile</a></code> is an inverse; <code><a href="grDevices.html#topic+boxplot.stats">boxplot.stats</a></code> and
<code><a href="stats.html#topic+fivenum">fivenum</a></code> for computing other versions of quartiles;
<code><a href="logspline.html#topic+qlogspline">qlogspline</a></code> for logspline density quantiles;
<code><a href="ks.html#topic+qkde">qkde</a></code> for alternative kernel quantiles, etc.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Simulate data from a chi-square distribution
df &lt;- 50
x &lt;- rchisq(100,df=df)

## Vector of quantiles desired
tau &lt;- c(0.01,0.05,0.25,0.50,0.75,0.95,0.99)

## Compute kernel smoothed sample quantiles
npquantile(x,tau)

## Compute sample quantiles using the default method in R (Type 7)
quantile(x,tau)

## True quantiles based on known distribution
qchisq(tau,df=df)

## End(Not run) 
</code></pre>

<hr>
<h2 id='npreg'>Kernel Regression with Mixed Data Types</h2><span id='topic+npreg'></span><span id='topic+npreg.call'></span><span id='topic+npreg.default'></span><span id='topic+npreg.formula'></span><span id='topic+npreg.rbandwidth'></span>

<h3>Description</h3>

<p><code>npreg</code> computes a kernel regression estimate of a one
(1) dimensional dependent variable on <code class="reqn">p</code>-variate explanatory
data, given a set of evaluation points, training points (consisting of
explanatory data and dependent data), and a bandwidth specification
using the method of Racine and Li (2004) and Li and Racine (2004). A
bandwidth specification can be a <code>rbandwidth</code> object, or a
bandwidth vector, bandwidth type and kernel type.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npreg(bws, ...)

## S3 method for class 'formula'
npreg(bws, data = NULL, newdata = NULL, y.eval =
FALSE, ...)

## S3 method for class 'call'
npreg(bws, ...)

## Default S3 method:
npreg(bws, txdat, tydat, ...)

## S3 method for class 'rbandwidth'
npreg(bws,
      txdat = stop("training data 'txdat' missing"),
      tydat = stop("training data 'tydat' missing"),
      exdat,
      eydat,
      gradients = FALSE,
      residuals = FALSE,
      ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npreg_+3A_bws">bws</code></td>
<td>

<p>a bandwidth specification. This can be set as a <code>rbandwidth</code>
object returned from an invocation of <code><a href="#topic+npregbw">npregbw</a></code>, or
as a vector of bandwidths, with each element <code class="reqn">i</code> corresponding
to the bandwidth for column <code class="reqn">i</code> in <code>txdat</code>. If specified as
a vector, then additional arguments will need to be supplied as
necessary to specify the bandwidth type, kernel types, and so on.
</p>
</td></tr>
<tr><td><code id="npreg_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to specify the regression type,
bandwidth type, kernel types, training data, and so on, detailed
below. 
</p>
</td></tr>
<tr><td><code id="npreg_+3A_data">data</code></td>
<td>

<p>an optional data frame, list or environment (or object
coercible to a data frame by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code>) containing the variables
in the model. If not found in data, the variables are taken from
<code>environment(bws)</code>, typically the environment from which
<code><a href="#topic+npregbw">npregbw</a></code> was called.
</p>
</td></tr>
<tr><td><code id="npreg_+3A_newdata">newdata</code></td>
<td>

<p>An optional data frame in which to look for evaluation data. If
omitted, the training data are used.  
</p>
</td></tr>
<tr><td><code id="npreg_+3A_y.eval">y.eval</code></td>
<td>

<p>If <code>newdata</code> contains dependent data and <code>y.eval = TRUE</code>,
<code><a href="#topic+np">np</a></code> will compute goodness of fit statistics on these
data and return them. Defaults to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="npreg_+3A_txdat">txdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of explanatory data (training data) used to
calculate the regression estimators. Defaults to the training data used to
compute the bandwidth object.
</p>
</td></tr>
<tr><td><code id="npreg_+3A_tydat">tydat</code></td>
<td>

<p>a one (1) dimensional numeric or integer vector of dependent data, each
element <code class="reqn">i</code> corresponding to each observation (row) <code class="reqn">i</code> of
<code>txdat</code>. Defaults to the training data used to
compute the bandwidth object.
</p>
</td></tr>
<tr><td><code id="npreg_+3A_exdat">exdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of points on which the regression will be
estimated (evaluation data). By default,
evaluation takes place on the data provided by <code>txdat</code>.
</p>
</td></tr>
<tr><td><code id="npreg_+3A_eydat">eydat</code></td>
<td>

<p>a one (1) dimensional numeric or integer vector of the true values
of the dependent variable. Optional, and used only to calculate the
true errors. 
</p>
</td></tr>
<tr><td><code id="npreg_+3A_gradients">gradients</code></td>
<td>

<p>a logical value indicating that you want gradients computed and
returned in the resulting <code>npregression</code> object. Defaults to
<code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="npreg_+3A_residuals">residuals</code></td>
<td>

<p>a logical value indicating that you want residuals computed and
returned in the resulting <code>npregression</code> object. Defaults to
<code>FALSE</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Typical usages are (see below for a complete list of options and also
the examples at the end of this help file)
</p>
<pre>
    Usage 1: first compute the bandwidth object via npregbw and then
    compute the conditional mean:
    
    bw &lt;- npregbw(y~x)
    ghat &lt;- npreg(bw)
    
    Usage 2: alternatively, compute the bandwidth object indirectly:
    
    ghat &lt;- npreg(y~x)
    
    Usage 3: modify the default kernel and order:
    
    ghat &lt;- npreg(y~x, ckertype="epanechnikov", ckerorder=4)

    Usage 4: use the data frame interface rather than the formula
    interface:

    ghat &lt;- npreg(tydat=y, txdat=x, ckertype="epanechnikov", ckerorder=4)
  </pre>
<p><code>npreg</code> implements a variety of methods for regression on
multivariate (<code class="reqn">p</code>-variate) data, the types of which are possibly
continuous and/or discrete (unordered, ordered). The approach is
based on Li and Racine (2003) who employ &lsquo;generalized product kernels&rsquo;
that admit a mix of continuous and discrete data types.
</p>
<p>Three classes of kernel estimators for the continuous data types are
available: fixed, adaptive nearest-neighbor, and generalized
nearest-neighbor. Adaptive nearest-neighbor bandwidths change with
each sample realization in the set, <code class="reqn">x_i</code>, when estimating the
density at the point <code class="reqn">x</code>. Generalized nearest-neighbor bandwidths change
with the point at which the density is estimated, <code class="reqn">x</code>. Fixed bandwidths
are constant over the support of <code class="reqn">x</code>.
</p>
<p>Data contained in the data frame <code>txdat</code> may be a mix of
continuous (default), unordered discrete (to be specified in the data
frame <code>txdat</code> using <code><a href="base.html#topic+factor">factor</a></code>), and ordered discrete
(to be specified in the data frame <code>txdat</code> using
<code><a href="base.html#topic+ordered">ordered</a></code>). Data can be entered in an arbitrary order and
data types will be detected automatically by the routine (see
<code><a href="#topic+np">np</a></code> for details).
</p>
<p>A variety of kernels may be specified by the user. Kernels implemented
for continuous data types include the second, fourth, sixth, and eighth
order Gaussian and Epanechnikov kernels, and the uniform
kernel. Unordered discrete data types use a variation on Aitchison and
Aitken's (1976) kernel, while ordered data types use a variation of the
Wang and van Ryzin (1981) kernel.
</p>
<p>The use of compactly supported kernels or the occurrence of small
bandwidths can lead to numerical problems for the local linear
estimator when computing the locally weighted least squares
solution. To overcome this problem we rely on a form or
&lsquo;ridging&rsquo; proposed by Cheng, Hall, and Titterington (1997),
modified so that we solve the problem pointwise rather than globally
(i.e. only when it is needed).
</p>


<h3>Value</h3>

<p><code>npreg</code> returns a <code>npregression</code> object.
The generic
functions <code><a href="stats.html#topic+fitted">fitted</a></code>, <code><a href="stats.html#topic+residuals">residuals</a></code>,
<code><a href="#topic+se">se</a></code>, <code><a href="stats.html#topic+predict">predict</a></code>, and
<code><a href="#topic+gradients">gradients</a></code>, extract (or generate) estimated values,
residuals, asymptotic standard
errors on estimates, predictions, and gradients, respectively, from
the returned object. Furthermore, the functions <code><a href="base.html#topic+summary">summary</a></code>
and <code><a href="base.html#topic+plot">plot</a></code> support objects of this type. The returned object
has the following components:
</p>
<table>
<tr><td><code>eval</code></td>
<td>
<p> evaluation points </p>
</td></tr>
<tr><td><code>mean</code></td>
<td>
<p> estimates of the regression function (conditional mean) at the
evaluation points </p>
</td></tr> 
<tr><td><code>merr</code></td>
<td>
<p> standard errors of the regression function estimates </p>
</td></tr> 
<tr><td><code>grad</code></td>
<td>
<p> estimates of the gradients at each evaluation point </p>
</td></tr>
<tr><td><code>gerr</code></td>
<td>
<p> standard errors of the gradient estimates </p>
</td></tr>
<tr><td><code>resid</code></td>
<td>
<p> if <code>residuals = TRUE</code>, in-sample or out-of-sample
residuals where appropriate (or possible) </p>
</td></tr>
<tr><td><code>R2</code></td>
<td>
<p> coefficient of determination (Doksum and Samarov (1995))</p>
</td></tr>
<tr><td><code>MSE</code></td>
<td>
<p> mean squared error </p>
</td></tr>
<tr><td><code>MAE</code></td>
<td>
<p> mean absolute error </p>
</td></tr>
<tr><td><code>MAPE</code></td>
<td>
<p> mean absolute percentage error </p>
</td></tr>
<tr><td><code>CORR</code></td>
<td>
<p> absolute value of Pearson's correlation coefficient </p>
</td></tr>
<tr><td><code>SIGN</code></td>
<td>
<p> fraction of observations where fitted and observed values
agree in sign </p>
</td></tr>
</table>


<h3>Usage Issues</h3>

<p>If you are using data of mixed types, then it is advisable to use the
<code><a href="base.html#topic+data.frame">data.frame</a></code> function to construct your input data and not
<code><a href="base.html#topic+cbind">cbind</a></code>, since <code><a href="base.html#topic+cbind">cbind</a></code> will typically not work as
intended on mixed data types and will coerce the data to the same
type.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Aitchison, J. and C.G.G. Aitken (1976), &ldquo;Multivariate binary
discrimination by the kernel method,&rdquo; Biometrika, 63, 413-420.
</p>
<p>Cheng, M.-Y. and P. Hall and D.M. Titterington (1997), &ldquo;On the
shrinkage of local linear curve estimators,&rdquo; Statistics and Computing,
7, 11-17.
</p>
<p>Doksum, K. and A. Samarov (1995), &ldquo;Nonparametric estimation of
global functionals and a measure of the explanatory power of
covariates in regression,&rdquo; The Annals of Statistics, 23 1443-1473.  
</p>
<p>Hall, P. and Q. Li and J.S. Racine (2007), &ldquo;Nonparametric
estimation of regression functions in the presence of irrelevant
regressors,&rdquo; The Review of Economics and Statistics, 89, 784-789.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics: Theory
and Practice,</em> Princeton University Press.
</p>
<p>Li, Q. and J.S. Racine (2004), &ldquo;Cross-validated local linear
nonparametric regression,&rdquo; Statistica Sinica, 14, 485-512.
</p>
<p>Pagan, A. and A. Ullah (1999), <em>Nonparametric Econometrics,</em>
Cambridge University Press.
</p>
<p>Racine, J.S. and Q. Li (2004), &ldquo;Nonparametric estimation of
regression functions with both categorical and continuous data,&rdquo;
Journal of Econometrics, 119, 99-130.
</p>
<p>Wang, M.C. and J. van Ryzin (1981), &ldquo;A class of smooth
estimators for discrete distributions,&rdquo; Biometrika, 68, 301-309.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+loess">loess</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# EXAMPLE 1 (INTERFACE=FORMULA): For this example, we compute a
# bivariate nonparametric regression estimate for Giovanni Baiocchi's
# Italian income panel (see Italy for details)

data("Italy")
attach(Italy)

# First, compute the least-squares cross-validated bandwidths for the
# local constant estimator (default).

bw &lt;- npregbw(formula=gdp~ordered(year))

# Now take these bandwidths and fit the model and gradients

model &lt;- npreg(bws = bw, gradients = TRUE)

summary(model)

# Use plot() to visualize the regression function, add bootstrap
# error bars, and overlay the data on the same plot.

# Note - this may take a minute or two depending on the speed of your
# computer due to bootstrapping being conducted (&lt;ctrl&gt;-C will
# interrupt). Note - nothing will appear in the graphics window until
# all computations are completed (if you use
# plot.errors.method="asymptotic" the figure will instantly appear).

plot(bw, plot.errors.method="bootstrap")
points(ordered(year), gdp, cex=.2, col="red")

detach(Italy)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# EXAMPLE 1 (INTERFACE=DATA FRAME): For this example, we compute a
# bivariate nonparametric regression estimate for Giovanni Baiocchi's
# Italian income panel (see Italy for details)

data("Italy")
attach(Italy)

# First, compute the least-squares cross-validated bandwidths for the
# local constant estimator (default).

bw &lt;- npregbw(xdat=ordered(year), ydat=gdp)

# Now take these bandwidths and fit the model and gradients

model &lt;- npreg(bws = bw, gradients = TRUE)

summary(model)

# Use plot() to visualize the regression function, add bootstrap
# error bars, and overlay the data on the same plot.

# Note - this may take a minute or two depending on the speed of your
# computer due to bootstrapping being conducted (&lt;ctrl&gt;-C will
# interrupt). Note - nothing will appear in the graphics window until
# all computations are completed (if you use
# plot.errors.method="asymptotic" the figure will instantly appear).

plot(bw, plot.errors.method="bootstrap")
points(ordered(year), gdp, cex=.2, col="red")

detach(Italy)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# EXAMPLE 2 (INTERFACE=FORMULA): For this example, we compute a local
# linear fit using the AIC_c bandwidth selection criterion. We then plot
# the estimator and its gradient using asymptotic standard errors.

data("cps71")
attach(cps71)

bw &lt;- npregbw(logwage~age, regtype="ll", bwmethod="cv.aic")

# Next, plot the regression function...

plot(bw, plot.errors.method="asymptotic")
points(age, logwage, cex=.2, col="red")

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Next, plot the derivative...

plot(bw, plot.errors.method="asymptotic", gradient=TRUE)

detach(cps71)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# EXAMPLE 2 (INTERFACE=DATA FRAME): For this example, we compute a local
# linear fit using the AIC_c bandwidth selection criterion. We then plot
# the estimator and its gradient using asymptotic standard errors.

data("cps71")
attach(cps71)

bw &lt;- npregbw(xdat=age, ydat=logwage, regtype="ll", bwmethod="cv.aic")

# Next, plot the regression function...

plot(bw, plot.errors.method="asymptotic")
points(age, logwage, cex=.2, col="red")

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Next, plot the derivative...

plot(bw, plot.errors.method="asymptotic", gradient=TRUE)

detach(cps71)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# EXAMPLE 3 (INTERFACE=FORMULA): For this example, we replicate the
# nonparametric regression in Maasoumi, Racine, and Stengos
# (2007) (see oecdpanel for details). Note that X is multivariate
# containing a mix of unordered, ordered, and continuous data types. Note
# - this may take a few minutes depending on the speed of your computer.

data("oecdpanel")
attach(oecdpanel)

bw &lt;- npregbw(formula=growth~
              factor(oecd)+
              factor(year)+
              initgdp+
              popgro+
              inv+
              humancap)

plot(bw, plot.errors.method="asymptotic")

detach(oecdpanel)

# EXAMPLE 3 (INTERFACE=DATA FRAME): For this example, we replicate the
# nonparametric regression in Maasoumi, Racine, and Stengos
# (2007) (see oecdpanel for details). Note that X is multivariate
# containing a mix of unordered, ordered, and continuous data types. Note
# - this may take a few minutes depending on the speed of your computer.

data("oecdpanel")
attach(oecdpanel)

y &lt;- growth
X &lt;- data.frame(factor(oecd), factor(year), initgdp, popgro, inv, humancap)

bw &lt;- npregbw(xdat=X, ydat=y)

plot(bw, plot.errors.method="asymptotic")

detach(oecdpanel)


# EXAMPLE 4 (INTERFACE=FORMULA): Experimental data - the effect of
# vitamin C on tooth growth in guinea pigs
#
# Description:
#
#     The response is the length of odontoblasts (teeth) in each of 10
#     guinea pigs at each of three dose levels of Vitamin C (0.5, 1, and
#     2 mg) with each of two delivery methods (orange juice or ascorbic
#     acid).
#
# Usage:
#
#     ToothGrowth
#
# Format:
#
#     A data frame with 60 observations on 3 variables.
#
#       [,1]  len   numeric  Tooth length
#       [,2]  supp  factor   Supplement type (VC or OJ).
#       [,3]  dose  numeric  Dose in milligrams.

library("datasets")
attach(ToothGrowth)

# Note - in this example, there are six cells. 

bw &lt;- npregbw(formula=len~factor(supp)+ordered(dose))

# Now plot the partial regression surfaces with bootstrapped
# nonparametric confidence bounds

plot(bw, plot.errors.method="bootstrap", plot.errors.type="quantile")

detach(ToothGrowth)

# EXAMPLE 4 (INTERFACE=DATA FRAME): Experimental data - the effect of
# vitamin C on tooth growth in guinea pigs
#
# Description:
#
#     The response is the length of odontoblasts (teeth) in each of 10
#     guinea pigs at each of three dose levels of Vitamin C (0.5, 1, and
#     2 mg) with each of two delivery methods (orange juice or ascorbic
#     acid).
#
# Usage:
#
#     ToothGrowth
#
# Format:
#
#     A data frame with 60 observations on 3 variables.
#
#       [,1]  len   numeric  Tooth length
#       [,2]  supp  factor   Supplement type (VC or OJ).
#       [,3]  dose  numeric  Dose in milligrams.

library("datasets")
attach(ToothGrowth)

# Note - in this example, there are six cells. 

y &lt;- len
X &lt;- data.frame(supp=factor(supp), dose=ordered(dose))

bw &lt;- npregbw(X, y)

# Now plot the partial regression surfaces with bootstrapped
# nonparametric confidence bounds

plot(bw, plot.errors.method="bootstrap", plot.errors.type="quantile")

detach(ToothGrowth)

# EXAMPLE 5 (INTERFACE=FORMULA): a pretty 2-d smoothing example adapted
# from the R mgcv library which was written by Simon N. Wood.

set.seed(12345)

# This function generates a smooth nonlinear DGP

dgp.func &lt;- function(x, z, sx=0.3, sz=0.4)
  { (pi**sx*sz)*(1.2*exp(-(x-0.2)^2/sx^2-(z-0.3)^2/sz^2)+
                 0.8*exp(-(x-0.7)^2/sx^2-(z-0.8)^2/sz^2))
  }

# Generate 500 observations, compute the true DGP (i.e., no noise), 
# then a noisy sample

n &lt;- 500

x &lt;- runif(n)
z &lt;- runif(n)

xs &lt;- seq(0, 1, length=30)
zs &lt;- seq(0, 1, length=30)

X.eval &lt;- data.frame(x=rep(xs, 30), z=rep(zs, rep(30, 30)))

dgp &lt;- matrix(dgp.func(X.eval$x, X.eval$z), 30, 30)

y &lt;- dgp.func(x, z)+rnorm(n)*0.1

# Prepare the screen for output... first, plot the true DGP

split.screen(c(2, 1))

screen(1)

persp(xs, zs, dgp, xlab="x1", ylab="x2", zlab="y", main="True DGP")

# Next, compute a local linear fit and plot that

bw &lt;- npregbw(formula=y~x+z, regtype="ll", bwmethod="cv.aic")
fit &lt;- fitted(npreg(bws=bw, newdata=X.eval))
fit.mat &lt;- matrix(fit, 30, 30)

screen(2)

persp(xs, zs, fit.mat, xlab="x1", ylab="x2", zlab="y",
      main="Local linear estimate")

# EXAMPLE 5 (INTERFACE=DATA FRAME): a pretty 2-d smoothing example
# adapted from the R mgcv library which was written by Simon N. Wood.

set.seed(12345)

# This function generates a smooth nonlinear DGP

dgp.func &lt;- function(x, z, sx=0.3, sz=0.4)
  { (pi**sx*sz)*(1.2*exp(-(x-0.2)^2/sx^2-(z-0.3)^2/sz^2)+
                 0.8*exp(-(x-0.7)^2/sx^2-(z-0.8)^2/sz^2))
  }

# Generate 500 observations, compute the true DGP (i.e., no noise), 
# then a noisy sample

n &lt;- 500

x &lt;- runif(n)
z &lt;- runif(n)

xs &lt;- seq(0, 1, length=30)
zs &lt;- seq(0, 1, length=30)

X &lt;- data.frame(x, z)
X.eval &lt;- data.frame(x=rep(xs, 30), z=rep(zs, rep(30, 30)))

dgp &lt;- matrix(dgp.func(X.eval$x, X.eval$z), 30, 30)

y &lt;- dgp.func(x, z)+rnorm(n)*0.1

# Prepare the screen for output... first, plot the true DGP

split.screen(c(2, 1))

screen(1)

persp(xs, zs, dgp, xlab="x1", ylab="x2", zlab="y", main="True DGP")

# Next, compute a local linear fit and plot that

bw &lt;- npregbw(xdat=X, ydat=y, regtype="ll", bwmethod="cv.aic")
fit &lt;- fitted(npreg(exdat=X.eval, bws=bw))
fit.mat &lt;- matrix(fit, 30, 30)

screen(2)

persp(xs, zs, fit.mat, xlab="x1", ylab="x2", zlab="y",
      main="Local linear estimate")

## End(Not run) 
</code></pre>

<hr>
<h2 id='npregbw'>Kernel Regression Bandwidth Selection with Mixed Data Types</h2><span id='topic+npregbw'></span><span id='topic+npregbw.NULL'></span><span id='topic+npregbw.default'></span><span id='topic+npregbw.formula'></span><span id='topic+npregbw.rbandwidth'></span>

<h3>Description</h3>

<p><code>npregbw</code> computes a bandwidth object for a
<code class="reqn">p</code>-variate kernel regression estimator defined over mixed
continuous and discrete (unordered, ordered) data using expected
Kullback-Leibler cross-validation, or least-squares cross validation
using the method of Racine and Li (2004) and Li and Racine (2004).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npregbw(...)

## S3 method for class 'formula'
npregbw(formula, data, subset, na.action, call, ...)

## S3 method for class 'NULL'
npregbw(xdat = stop("invoked without data 'xdat'"),
        ydat = stop("invoked without data 'ydat'"),
        bws,
        ...)

## Default S3 method:
npregbw(xdat = stop("invoked without data 'xdat'"),
        ydat = stop("invoked without data 'ydat'"),
        bws,
        bandwidth.compute = TRUE,
        nmulti,
        remin,
        itmax,
        ftol,
        tol,
        small,
        lbc.dir,
        dfc.dir,
        cfac.dir,
        initc.dir,
        lbd.dir,
        hbd.dir,
        dfac.dir,
        initd.dir,
        lbc.init,
        hbc.init,
        cfac.init,
        lbd.init,
        hbd.init,
        dfac.init,
        scale.init.categorical.sample,
        regtype,
        bwmethod,
        bwscaling,
        bwtype,
        ckertype,
        ckerorder,
        ukertype,
        okertype,
        ...)

## S3 method for class 'rbandwidth'
npregbw(xdat = stop("invoked without data 'xdat'"),
        ydat = stop("invoked without data 'ydat'"),
        bws,
        bandwidth.compute = TRUE,
        nmulti,
        remin = TRUE,
        itmax = 10000,
        ftol = 1.490116e-07,
        tol = 1.490116e-04,
        small = 1.490116e-05,
        lbc.dir = 0.5,
        dfc.dir = 3,
        cfac.dir = 2.5*(3.0-sqrt(5)),
        initc.dir = 1.0,
        lbd.dir = 0.1,
        hbd.dir = 1,
        dfac.dir = 0.25*(3.0-sqrt(5)),
        initd.dir = 1.0,
        lbc.init = 0.1,
        hbc.init = 2.0,
        cfac.init = 0.5,
        lbd.init = 0.1,
        hbd.init = 0.9,
        dfac.init = 0.375, 
        scale.init.categorical.sample = FALSE,
        ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npregbw_+3A_formula">formula</code></td>
<td>

<p>a symbolic description of variables on which bandwidth selection is
to be performed. The details of constructing a formula are
described below.
</p>
</td></tr>
<tr><td><code id="npregbw_+3A_data">data</code></td>
<td>

<p>an optional data frame, list or environment (or object
coercible to a data frame by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code>) containing the variables
in the model. If not found in data, the variables are taken from
<code>environment(formula)</code>, typically the environment from which the
function is called.
</p>
</td></tr>
<tr><td><code id="npregbw_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations to be used in
the fitting process. 
</p>
</td></tr>
<tr><td><code id="npregbw_+3A_na.action">na.action</code></td>
<td>

<p>a function which indicates what should happen when the data contain
<code>NA</code>s. The default is set by the <code><a href="stats.html#topic+na.action">na.action</a></code> setting of options, and is
<code><a href="stats.html#topic+na.fail">na.fail</a></code> if that is unset. The (recommended) default is
<code><a href="stats.html#topic+na.omit">na.omit</a></code>.  
</p>
</td></tr>
<tr><td><code id="npregbw_+3A_call">call</code></td>
<td>

<p>the original function call. This is passed internally by
<code><a href="#topic+np">np</a></code> when a bandwidth search has been implied by a call to
another function. It is not recommended that the user set this.
</p>
</td></tr>
<tr><td><code id="npregbw_+3A_xdat">xdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of regressors on which bandwidth selection will
be performed. The data types may be continuous, discrete (unordered
and ordered factors), or some combination thereof.
</p>
</td></tr>
<tr><td><code id="npregbw_+3A_ydat">ydat</code></td>
<td>

<p>a one (1) dimensional numeric or integer vector of dependent data, each
element <code class="reqn">i</code> corresponding to each observation (row) <code class="reqn">i</code> of
<code>xdat</code>. 
</p>
</td></tr>
<tr><td><code id="npregbw_+3A_bws">bws</code></td>
<td>

<p>a bandwidth specification. This can be set as a <code>rbandwidth</code>
object returned from a previous invocation, or as a vector of
bandwidths, with each element <code class="reqn">i</code> corresponding to the bandwidth
for column <code class="reqn">i</code> in <code>xdat</code>. In either case, the bandwidth
supplied will serve as a starting point in the numerical search for
optimal bandwidths. If specified as a vector, then additional
arguments will need to be supplied as necessary to specify the
bandwidth type, kernel types, selection methods, and so on. This can
be left unset.
</p>
</td></tr>
<tr><td><code id="npregbw_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to specify the bandwidth type,
kernel types, selection methods, and so on, detailed below.
</p>
</td></tr>
<tr><td><code id="npregbw_+3A_regtype">regtype</code></td>
<td>

<p>a character string specifying which type of kernel regression
estimator to use. <code>lc</code> specifies a local-constant estimator
(Nadaraya-Watson) and <code>ll</code> specifies a local-linear
estimator. Defaults to <code>lc</code>.
</p>
</td></tr>
<tr><td><code id="npregbw_+3A_bwmethod">bwmethod</code></td>
<td>

<p>which method to use to select bandwidths. <code>cv.aic</code> specifies
expected Kullback-Leibler cross-validation (Hurvich, Simonoff, and
Tsai (1998)), and <code>cv.ls</code> specifies least-squares
cross-validation. Defaults to <code>cv.ls</code>.
</p>
</td></tr>
<tr><td><code id="npregbw_+3A_bwscaling">bwscaling</code></td>
<td>
<p> a logical value that when set to <code>TRUE</code> the
supplied bandwidths are interpreted as &lsquo;scale factors&rsquo;
(<code class="reqn">c_j</code>), otherwise when the value is <code>FALSE</code> they are
interpreted as &lsquo;raw bandwidths&rsquo; (<code class="reqn">h_j</code> for
continuous data types, <code class="reqn">\lambda_j</code> for discrete data
types). For continuous data types, <code class="reqn">c_j</code> and
<code class="reqn">h_j</code> are related by the formula <code class="reqn">h_j = c_j \sigma_j
    n^{-1/(2P+l)}</code>, where
<code class="reqn">\sigma_j</code> is an adaptive measure of spread of
continuous variable <code class="reqn">j</code> defined as min(standard deviation, mean
absolute deviation/1.4826, interquartile range/1.349), <code class="reqn">n</code> the
number of observations, <code class="reqn">P</code> the order of the kernel, and
<code class="reqn">l</code> the number of continuous variables. For discrete data
types, <code class="reqn">c_j</code> and <code class="reqn">h_j</code> are related by the
formula <code class="reqn">h_j = c_jn^{-2/(2P+l)}</code>,
where here <code class="reqn">j</code> denotes discrete variable <code class="reqn">j</code>.
Defaults to <code>FALSE</code>.  </p>
</td></tr>
<tr><td><code id="npregbw_+3A_bwtype">bwtype</code></td>
<td>

<p>character string used for the continuous variable bandwidth type,
specifying the type of bandwidth to compute and return in the
<code>bandwidth</code> object. Defaults to <code>fixed</code>. Option
summary:<br />
<code>fixed</code>: compute fixed bandwidths <br />
<code>generalized_nn</code>: compute generalized nearest neighbors <br />
<code>adaptive_nn</code>: compute adaptive nearest neighbors
</p>
</td></tr>
<tr><td><code id="npregbw_+3A_bandwidth.compute">bandwidth.compute</code></td>
<td>

<p>a logical value which specifies whether to do a numerical search for
bandwidths or not. If set to <code>FALSE</code>, a <code>rbandwidth</code> object
will be returned with bandwidths set to those specified
in <code>bws</code>. Defaults to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npregbw_+3A_ckertype">ckertype</code></td>
<td>

<p>character string used to specify the continuous kernel type.
Can be set as <code>gaussian</code>, <code>epanechnikov</code>, or
<code>uniform</code>. Defaults to <code>gaussian</code>.
</p>
</td></tr>
<tr><td><code id="npregbw_+3A_ckerorder">ckerorder</code></td>
<td>

<p>numeric value specifying kernel order (one of
<code>(2,4,6,8)</code>). Kernel order specified along with a
<code>uniform</code> continuous kernel type will be ignored. Defaults to
<code>2</code>.
</p>
</td></tr>
<tr><td><code id="npregbw_+3A_ukertype">ukertype</code></td>
<td>

<p>character string used to specify the unordered categorical kernel type.
Can be set as <code>aitchisonaitken</code> or <code>liracine</code>. Defaults to
<code>aitchisonaitken</code>.
</p>
</td></tr>
<tr><td><code id="npregbw_+3A_okertype">okertype</code></td>
<td>

<p>character string used to specify the ordered categorical kernel type.
Can be set as <code>wangvanryzin</code> or <code>liracine</code>. Defaults to
<code>liracine</code>.
</p>
</td></tr>
<tr><td><code id="npregbw_+3A_nmulti">nmulti</code></td>
<td>

<p>integer number of times to restart the process of finding extrema of
the cross-validation function from different (random) initial
points. Defaults to <code>min(5,ncol(xdat))</code>.
</p>
</td></tr>
<tr><td><code id="npregbw_+3A_remin">remin</code></td>
<td>

<p>a logical value which when set as <code>TRUE</code> the search routine
restarts from located minima for a minor gain in accuracy. Defaults
to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npregbw_+3A_itmax">itmax</code></td>
<td>

<p>integer number of iterations before failure in the numerical
optimization routine. Defaults to <code>10000</code>.
</p>
</td></tr>
<tr><td><code id="npregbw_+3A_ftol">ftol</code></td>
<td>

<p>fractional tolerance on the value of the cross-validation function
evaluated at located minima (of order the machine precision or
perhaps slightly larger so as not to be diddled by
roundoff). Defaults to <code>1.490116e-07</code>
(1.0e+01*sqrt(.Machine$double.eps)).
</p>
</td></tr>
<tr><td><code id="npregbw_+3A_tol">tol</code></td>
<td>

<p>tolerance on the position of located minima of the cross-validation
function (tol should generally be no smaller than the square root of
your machine's floating point precision). Defaults to <code>
      1.490116e-04 (1.0e+04*sqrt(.Machine$double.eps))</code>.
</p>
</td></tr>
<tr><td><code id="npregbw_+3A_small">small</code></td>
<td>

<p>a small number used to bracket a minimum (it is hopeless to ask for
a bracketing interval of width less than sqrt(epsilon) times its
central value, a fractional width of only about 10-04 (single
precision) or 3x10-8 (double precision)). Defaults to <code>small
      = 1.490116e-05 (1.0e+03*sqrt(.Machine$double.eps))</code>.
</p>
</td></tr>
<tr><td><code id="npregbw_+3A_lbc.dir">lbc.dir</code>, <code id="npregbw_+3A_dfc.dir">dfc.dir</code>, <code id="npregbw_+3A_cfac.dir">cfac.dir</code>, <code id="npregbw_+3A_initc.dir">initc.dir</code></td>
<td>
<p> lower bound, chi-square
degrees of freedom, stretch factor, and initial non-random values
for direction set search for Powell's algorithm for <code>numeric</code>
variables. See Details</p>
</td></tr>
<tr><td><code id="npregbw_+3A_lbd.dir">lbd.dir</code>, <code id="npregbw_+3A_hbd.dir">hbd.dir</code>, <code id="npregbw_+3A_dfac.dir">dfac.dir</code>, <code id="npregbw_+3A_initd.dir">initd.dir</code></td>
<td>
<p> lower bound, upper bound,
stretch factor, and initial non-random values for direction set
search for Powell's algorithm for categorical variables. See
Details</p>
</td></tr>
<tr><td><code id="npregbw_+3A_lbc.init">lbc.init</code>, <code id="npregbw_+3A_hbc.init">hbc.init</code>, <code id="npregbw_+3A_cfac.init">cfac.init</code></td>
<td>
<p> lower bound, upper bound, and
non-random initial values for scale factors for <code>numeric</code>
variables for Powell's algorithm. See Details</p>
</td></tr>
<tr><td><code id="npregbw_+3A_lbd.init">lbd.init</code>, <code id="npregbw_+3A_hbd.init">hbd.init</code>, <code id="npregbw_+3A_dfac.init">dfac.init</code></td>
<td>
<p> lower bound, upper bound, and
non-random initial values for scale factors for categorical
variables for Powell's algorithm. See Details</p>
</td></tr>
<tr><td><code id="npregbw_+3A_scale.init.categorical.sample">scale.init.categorical.sample</code></td>
<td>
<p> a logical value that when set
to <code>TRUE</code> scales <code>lbd.dir</code>, <code>hbd.dir</code>,
<code>dfac.dir</code>, and <code>initd.dir</code> by <code class="reqn">n^{-2/(2P+l)}</code>,
<code class="reqn">n</code> the number of observations, <code class="reqn">P</code> the order of the
kernel, and <code class="reqn">l</code> the number of <code>numeric</code> variables. See
Details</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>npregbw</code> implements a variety of methods for choosing
bandwidths for multivariate (<code class="reqn">p</code>-variate) regression data defined
over a set of possibly continuous and/or discrete (unordered, ordered)
data. The approach is based on Li and Racine (2003) who employ
&lsquo;generalized product kernels&rsquo; that admit a mix of continuous
and discrete data types.
</p>
<p>The cross-validation methods employ multivariate numerical search
algorithms (direction set (Powell's) methods in multidimensions).
</p>
<p>Bandwidths can (and will) differ for each variable which is, of
course, desirable.
</p>
<p>Three classes of kernel estimators for the continuous data types are
available: fixed, adaptive nearest-neighbor, and generalized
nearest-neighbor. Adaptive nearest-neighbor bandwidths change with
each sample realization in the set, <code class="reqn">x_i</code>, when estimating the
density at the point <code class="reqn">x</code>. Generalized nearest-neighbor bandwidths change
with the point at which the density is estimated, <code class="reqn">x</code>. Fixed bandwidths
are constant over the support of <code class="reqn">x</code>.
</p>
<p><code>npregbw</code> may be invoked <em>either</em> with a formula-like
symbolic 
description of variables on which bandwidth selection is to be
performed <em>or</em> through a simpler interface whereby data is passed
directly to the function via the <code>xdat</code> and <code>ydat</code>
parameters. Use of these two interfaces is <b>mutually exclusive</b>.
</p>
<p>Data contained in the data frame <code>xdat</code> may be a mix of
continuous (default), unordered discrete (to be specified in the data
frame <code>xdat</code> using <code><a href="base.html#topic+factor">factor</a></code>), and ordered discrete
(to be specified in the data frame <code>xdat</code> using
<code><a href="base.html#topic+ordered">ordered</a></code>). Data can be entered in an arbitrary order and
data types will be detected automatically by the routine (see
<code><a href="#topic+np">np</a></code> for details).
</p>
<p>Data for which bandwidths are to be estimated may be specified
symbolically. A typical description has the form <code>dependent data
    ~ explanatory data</code>,
where <code>dependent data</code> is a univariate response, and
<code>explanatory data</code> is a
series of variables specified by name, separated by 
the separation character '+'. For example, <code> y1 ~ x1 + x2 </code>
specifies that the bandwidths for the regression of response <code>y1</code>
and 
nonparametric regressors <code>x1</code> and <code>x2</code> are to be estimated.
See below for further examples. 
</p>
<p>A variety of kernels may be specified by the user. Kernels implemented
for continuous data types include the second, fourth, sixth, and eighth
order Gaussian and Epanechnikov kernels, and the uniform
kernel. Unordered discrete data types use a variation on Aitchison and
Aitken's (1976) kernel, while ordered data types use a variation of the
Wang and van Ryzin (1981) kernel.
</p>
<p>The use of compactly supported kernels or the occurrence of small
bandwidths during cross-validation can lead to numerical problems for
the local linear estimator when computing the locally weighted least
squares solution. To overcome this problem we rely on a form or
&lsquo;ridging&rsquo; proposed by Cheng, Hall, and Titterington (1997),
modified so that we solve the problem pointwise rather than globally
(i.e. only when it is needed).
</p>
<p>The optimizer invoked for search is Powell's conjugate direction
method which requires the setting of (non-random) initial values and
search directions for bandwidths, and, when restarting, random values
for successive invocations. Bandwidths for <code>numeric</code> variables
are scaled by robust measures of spread, the sample size, and the
number of <code>numeric</code> variables where appropriate. Two sets of
parameters for bandwidths for <code>numeric</code> can be modified, those
for initial values for the parameters themselves, and those for the
directions taken (Powell's algorithm does not involve explicit
computation of the function's gradient). The default values are set by
considering search performance for a variety of difficult test cases
and simulated cases. We highly recommend restarting search a large
number of times to avoid the presence of local minima (achieved by
modifying <code>nmulti</code>). Further refinement for difficult cases can
be achieved by modifying these sets of parameters. However, these
parameters are intended more for the authors of the package to enable
&lsquo;tuning&rsquo; for various methods rather than for the user
themselves.
</p>


<h3>Value</h3>

<p><code>npregbw</code> returns a <code>rbandwidth</code> object, with the
following components:
</p>
<table>
<tr><td><code>bw</code></td>
<td>
<p> bandwidth(s), scale factor(s) or nearest neighbours for the
data, <code>xdat</code> </p>
</td></tr>
<tr><td><code>fval</code></td>
<td>
<p> objective function value at minimum </p>
</td></tr>
</table>
<p>if <code>bwtype</code> is set to <code>fixed</code>, an object containing bandwidths
(or scale factors if <code>bwscaling = TRUE</code>) is returned. If it is set to
<code>generalized_nn</code> or <code>adaptive_nn</code>, then instead the <code class="reqn">k</code>th nearest
neighbors are returned for the continuous variables while the discrete
kernel bandwidths are returned for the discrete variables. Bandwidths
are stored under the component name <code>bw</code>, with each
element <code class="reqn">i</code> corresponding to column <code class="reqn">i</code> of input data
<code>xdat</code>.
</p>
<p>The functions <code><a href="stats.html#topic+predict">predict</a></code>, <code><a href="base.html#topic+summary">summary</a></code>, and <code><a href="base.html#topic+plot">plot</a></code> support
objects of this class.
</p>


<h3>Usage Issues</h3>

<p>If you are using data of mixed types, then it is advisable to use the
<code><a href="base.html#topic+data.frame">data.frame</a></code> function to construct your input data and not
<code><a href="base.html#topic+cbind">cbind</a></code>, since <code><a href="base.html#topic+cbind">cbind</a></code> will typically not work as
intended on mixed data types and will coerce the data to the same
type.
</p>
<p>Caution: multivariate data-driven bandwidth selection methods are, by
their nature, <em>computationally intensive</em>. Virtually all methods
require dropping the <code class="reqn">i</code>th observation from the data set, computing an
object, repeating this for all observations in the sample, then
averaging each of these leave-one-out estimates for a <em>given</em>
value of the bandwidth vector, and only then repeating this a large
number of times in order to conduct multivariate numerical
minimization/maximization. Furthermore, due to the potential for local
minima/maxima, <em>restarting this procedure a large number of times may
often be necessary</em>. This can be frustrating for users possessing
large datasets. For exploratory purposes, you may wish to override the
default search tolerances, say, setting ftol=.01 and tol=.01 and
conduct multistarting (the default is to restart min(5, ncol(xdat))
times) as is done for a number of examples. Once the procedure
terminates, you can restart search with default tolerances using those
bandwidths obtained from the less rigorous search (i.e., set
<code>bws=bw</code> on subsequent calls to this routine where <code>bw</code> is
the initial bandwidth object).  A version of this package using the
<code>Rmpi</code> wrapper is under development that allows one to deploy
this software in a clustered computing environment to facilitate
computation involving large datasets.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine
<a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Aitchison, J. and C.G.G. Aitken (1976), &ldquo;Multivariate binary
discrimination by the kernel method,&rdquo; Biometrika, 63, 413-420.
</p>
<p>Cheng, M.-Y. and P. Hall and D.M. Titterington (1997), &ldquo;On the
shrinkage of local linear curve estimators,&rdquo; Statistics and Computing,
7, 11-17.
</p>
<p>Hall, P. and Q. Li and J.S. Racine (2007), &ldquo;Nonparametric
estimation of regression functions in the presence of irrelevant
regressors,&rdquo; The Review of Economics and Statistics, 89, 784-789.
</p>
<p>Hurvich, C.M. and J.S. Simonoff and C.L. Tsai (1998),
&ldquo;Smoothing parameter selection in nonparametric regression
using an improved Akaike information criterion,&rdquo; Journal of the
Royal Statistical Society B, 60, 271-293.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics: Theory
and Practice,</em> Princeton University Press.
</p>
<p>Li, Q. and J.S. Racine (2004), &ldquo;Cross-validated local linear
nonparametric regression,&rdquo; Statistica Sinica, 14, 485-512.
</p>
<p>Pagan, A. and A. Ullah (1999), <em>Nonparametric Econometrics,</em>
Cambridge University Press.
</p>
<p>Racine, J.S. and Q. Li (2004), &ldquo;Nonparametric estimation of regression
functions with both categorical and continuous data,&rdquo; Journal of
Econometrics, 119, 99-130.
</p>
<p>Wang, M.C. and J. van Ryzin (1981), &ldquo;A class of smooth estimators
for discrete distributions,&rdquo;  Biometrika, 68, 301-309.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npreg">npreg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# EXAMPLE 1 (INTERFACE=FORMULA): For this example, we compute a
# Bivariate nonparametric regression estimate for Giovanni Baiocchi's
# Italian income panel (see Italy for details)

data("Italy")
attach(Italy)

# Compute the least-squares cross-validated bandwidths for the local
# constant estimator (default)

bw &lt;- npregbw(formula=gdp~ordered(year))

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Supply your own bandwidth...

bw &lt;- npregbw(formula=gdp~ordered(year), bws=c(0.75),
              bandwidth.compute=FALSE)

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Treat year as continuous and supply your own scaling factor c in
# c sigma n^{-1/(2p+q)}

bw &lt;- npregbw(formula=gdp~year, bws=c(1.06),
              bandwidth.compute=FALSE, 
              bwscaling=TRUE)

summary(bw)

# Note - see also the example for npudensbw() for more extensive
# multiple illustrations of how to change the kernel function, kernel
# order, bandwidth type and so forth.

detach(Italy)

# EXAMPLE 1 (INTERFACE=DATA FRAME): For this example, we compute a
# Bivariate nonparametric regression estimate for Giovanni Baiocchi's
# Italian income panel (see Italy for details)

data("Italy")
attach(Italy)

# Compute the least-squares cross-validated bandwidths for the local
# constant estimator (default)

bw &lt;- npregbw(xdat=ordered(year), ydat=gdp)

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Supply your own bandwidth...

bw &lt;- npregbw(xdat=ordered(year), ydat=gdp, bws=c(0.75),
              bandwidth.compute=FALSE)

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Treat year as continuous and supply your own scaling factor c in
# c sigma n^{-1/(2p+q)}

bw &lt;- npregbw(xdat=year, ydat=gdp, bws=c(1.06),
              bandwidth.compute=FALSE, 
              bwscaling=TRUE)

summary(bw)

# Note - see also the example for npudensbw() for more extensive
# multiple illustrations of how to change the kernel function, kernel
# order, bandwidth type and so forth.

detach(Italy)

## End(Not run) 
</code></pre>

<hr>
<h2 id='npregiv'>
Nonparametric Instrumental Regression
</h2><span id='topic+npregiv'></span>

<h3>Description</h3>

<p><code>npregiv</code> computes nonparametric estimation of an instrumental
regression function <code class="reqn">\varphi</code> defined by conditional moment
restrictions stemming from a structural econometric model: <code class="reqn">E [Y -
\varphi (Z,X) | W ] = 0</code>, and involving
endogenous variables <code class="reqn">Y</code> and <code class="reqn">Z</code> and exogenous variables
<code class="reqn">X</code> and instruments <code class="reqn">W</code>. The function <code class="reqn">\varphi</code> is the
solution of an ill-posed inverse problem.
</p>
<p>When <code>method="Tikhonov"</code>, <code>npregiv</code> uses the approach of
Darolles, Fan, Florens and Renault (2011) modified for local
polynomial kernel regression of any order (Darolles et al use local
constant kernel weighting which corresponds to setting <code>p=0</code>; see
below for details). When <code>method="Landweber-Fridman"</code>,
<code>npregiv</code> uses the approach of Horowitz (2011) again using local
polynomial kernel regression (Horowitz uses B-spline weighting).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npregiv(y,
        z,
        w,
        x = NULL,
        zeval = NULL,
        xeval = NULL,
        p = 1,
        nmulti = 1,
        random.seed = 42,
        optim.maxattempts = 10,
        optim.method = c("Nelder-Mead", "BFGS", "CG"),
        optim.reltol = sqrt(.Machine$double.eps),
        optim.abstol = .Machine$double.eps,
        optim.maxit = 500,
        alpha = NULL,
        alpha.iter = NULL,
        alpha.min = 1e-10,
        alpha.max = 1e-01,
        alpha.tol = .Machine$double.eps^0.25,
        iterate.Tikhonov = TRUE,
        iterate.Tikhonov.num = 1,
        iterate.max = 1000,
        iterate.diff.tol = 1.0e-08,
        constant = 0.5,
        method = c("Landweber-Fridman","Tikhonov"),
        penalize.iteration = TRUE,
        smooth.residuals = TRUE,        
        start.from = c("Eyz","EEywz"),
        starting.values  = NULL,
        stop.on.increase = TRUE,
        return.weights.phi = FALSE,
        return.weights.phi.deriv.1 = FALSE,
        return.weights.phi.deriv.2 = FALSE,
        bw = NULL,
        ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npregiv_+3A_y">y</code></td>
<td>

<p>a one (1) dimensional numeric or integer vector of dependent data, each
element <code class="reqn">i</code> corresponding to each observation (row) <code class="reqn">i</code> of
<code>z</code>.
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_z">z</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of endogenous regressors. The data
types may be continuous, discrete (unordered and ordered factors),
or some combination thereof.
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_w">w</code></td>
<td>

<p>a <code class="reqn">q</code>-variate data frame of instruments. The data types may be
continuous, discrete (unordered and ordered factors), or some
combination thereof.
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_x">x</code></td>
<td>

<p>an <code class="reqn">r</code>-variate data frame of exogenous regressors. The data
types may be continuous, discrete (unordered and ordered factors),
or some combination thereof.
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_zeval">zeval</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of endogenous regressors on which the
regression will be estimated (evaluation data). By default, evaluation
takes place on the data provided by <code>z</code>.
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_xeval">xeval</code></td>
<td>

<p>an <code class="reqn">r</code>-variate data frame of exogenous regressors on which the
regression will be estimated (evaluation data). By default,
evaluation takes place on the data provided by <code>x</code>.
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_p">p</code></td>
<td>

<p>the order of the local polynomial regression (defaults to
<code>p=1</code>, i.e. local linear).
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_nmulti">nmulti</code></td>
<td>

<p>integer number of times to restart the process of finding extrema of
the cross-validation function from different (random) initial
points.
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_random.seed">random.seed</code></td>
<td>

<p>an integer used to seed R's random number generator. This ensures
replicability of the numerical search. Defaults to 42.
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_optim.method">optim.method</code></td>
<td>
<p> method used by <code><a href="stats.html#topic+optim">optim</a></code> for minimization of
the objective function. See <code>?optim</code> for references. Defaults
to <code>"Nelder-Mead"</code>.
</p>
<p>the default method is an implementation of that of Nelder and Mead
(1965), that uses only function values and is robust but relatively
slow.  It will work reasonably well for non-differentiable
functions.
</p>
<p>method <code>"BFGS"</code> is a quasi-Newton method (also known as a
variable metric algorithm), specifically that published
simultaneously in 1970 by Broyden, Fletcher, Goldfarb and Shanno.
This uses function values and gradients to build up a picture of the
surface to be optimized.
</p>
<p>method <code>"CG"</code> is a conjugate gradients method based
on that by Fletcher and Reeves (1964) (but with the option of
Polak-Ribiere or Beale-Sorenson updates).  Conjugate gradient
methods will generally be more fragile than the BFGS method, but as
they do not store a matrix they may be successful in much larger
optimization problems.
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_optim.maxattempts">optim.maxattempts</code></td>
<td>

<p>maximum number of attempts taken trying to achieve successful
convergence in <code><a href="stats.html#topic+optim">optim</a></code>. Defaults to <code>100</code>.
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_optim.abstol">optim.abstol</code></td>
<td>

<p>the absolute convergence tolerance used by <code><a href="stats.html#topic+optim">optim</a></code>. Only useful
for non-negative functions, as a tolerance for reaching
zero. Defaults to <code>.Machine$double.eps</code>.
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_optim.reltol">optim.reltol</code></td>
<td>

<p>relative convergence tolerance used by <code><a href="stats.html#topic+optim">optim</a></code>.  The algorithm
stops if it is unable to reduce the value by a factor of 'reltol *
(abs(val) + reltol)' at a step.  Defaults to
<code>sqrt(.Machine$double.eps)</code>, typically about <code>1e-8</code>.
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_optim.maxit">optim.maxit</code></td>
<td>

<p>maximum number of iterations used by <code><a href="stats.html#topic+optim">optim</a></code>. Defaults
to <code>500</code>.
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_alpha">alpha</code></td>
<td>

<p>a numeric scalar that, if supplied, is used rather than numerically
solving for <code>alpha</code>, when using <code>method="Tikhonov"</code>.
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_alpha.iter">alpha.iter</code></td>
<td>

<p>a numeric scalar that, if supplied, is used for iterated Tikhonov
rather than numerically solving for <code>alpha</code>, when using
<code>method="Tikhonov"</code>.
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_alpha.min">alpha.min</code></td>
<td>

<p>minimum of search range for <code class="reqn">\alpha</code>, the Tikhonov
regularization parameter, when using <code>method="Tikhonov"</code>.
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_alpha.max">alpha.max</code></td>
<td>

<p>maximum of search range for <code class="reqn">\alpha</code>, the Tikhonov
regularization parameter, when using  <code>method="Tikhonov"</code>.
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_alpha.tol">alpha.tol</code></td>
<td>

<p>the search tolerance for <code>optimize</code> when solving for
<code class="reqn">\alpha</code>, the Tikhonov regularization parameter,
when using <code>method="Tikhonov"</code>.
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_iterate.tikhonov">iterate.Tikhonov</code></td>
<td>

<p>a logical value indicating whether to use iterated Tikhonov (one
iteration) or not when using <code>method="Tikhonov"</code>.
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_iterate.tikhonov.num">iterate.Tikhonov.num</code></td>
<td>

<p>an integer indicating the number of iterations to conduct when using
<code>method="Tikhonov"</code>.
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_iterate.max">iterate.max</code></td>
<td>

<p>an integer indicating the maximum number of iterations permitted
before termination occurs when using <code>method="Landweber-Fridman"</code>.
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_iterate.diff.tol">iterate.diff.tol</code></td>
<td>

<p>the search tolerance for the difference in the stopping rule from
iteration to iteration when using <code>method="Landweber-Fridman"</code>
(disable by setting to zero).
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_constant">constant</code></td>
<td>

<p>the constant to use when using  <code>method="Landweber-Fridman"</code>.
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_method">method</code></td>
<td>

<p>the regularization method employed (defaults to
<code>"Landweber-Fridman"</code>, see Horowitz (2011); see Darolles,
Fan, Florens and Renault (2011) for details for
<code>"Tikhonov"</code>).
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_penalize.iteration">penalize.iteration</code></td>
<td>
<p> a logical value indicating whether to
penalize the norm by the number of iterations or not (default
<code>TRUE</code>)
</p>
</td></tr>    
<tr><td><code id="npregiv_+3A_smooth.residuals">smooth.residuals</code></td>
<td>
<p> a logical value indicating whether to
optimize bandwidths for the regression of
<code class="reqn">(y-\varphi(z))</code> on <code class="reqn">w</code> (defaults to
<code>TRUE</code>) or for the regression of <code class="reqn">\varphi(z)</code> on
<code class="reqn">w</code> during iteration
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_start.from">start.from</code></td>
<td>
<p> a character string indicating whether to start from
<code class="reqn">E(Y|z)</code> (default, <code>"Eyz"</code>) or from <code class="reqn">E(E(Y|z)|z)</code> (this can
be overridden by providing <code>starting.values</code> below)
</p>
</td></tr>  
<tr><td><code id="npregiv_+3A_starting.values">starting.values</code></td>
<td>
<p> a value indicating whether to commence
Landweber-Fridman assuming
<code class="reqn">\varphi_{-1}=starting.values</code> (proper
Landweber-Fridman) or instead begin from <code class="reqn">E(y|z)</code> (defaults to
<code>NULL</code>, see details below)
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_stop.on.increase">stop.on.increase</code></td>
<td>

<p>a logical value (defaults to <code>TRUE</code>) indicating whether to halt
iteration if the stopping criterion (see below) increases over the
course of one iteration (i.e. it may be above the iteration tolerance
but increased)
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_return.weights.phi">return.weights.phi</code></td>
<td>

<p>a logical value (defaults to <code>FALSE</code>) indicating whether to
return the weight matrix which when postmultiplied by the response
<code class="reqn">y</code> delivers the instrumental regression
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_return.weights.phi.deriv.1">return.weights.phi.deriv.1</code></td>
<td>

<p>a logical value (defaults to <code>FALSE</code>) indicating whether to
return the weight matrix which when postmultiplied by the response
<code class="reqn">y</code> delivers the first partial derivative of the instrumental
regression with respect to <code class="reqn">z</code>
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_return.weights.phi.deriv.2">return.weights.phi.deriv.2</code></td>
<td>

<p>a logical value (defaults to <code>FALSE</code>) indicating whether to
return the weight matrix which when postmultiplied by the response
<code class="reqn">y</code> delivers the second partial derivative of the instrumental
regression with respect to <code class="reqn">z</code>
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_bw">bw</code></td>
<td>

<p>an object which, if provided, contains bandwidths and parameters
(obtained from a previous invocation of <code>npregiv</code>) required to
re-compute the estimator without having to re-run cross-validation
and/or numerical optimization which is particularly costly in this
setting (see details below for an illustration of its use)
</p>
</td></tr>
<tr><td><code id="npregiv_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to <code><a href="#topic+npksum">npksum</a></code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Tikhonov regularization requires computation of weight matrices of
dimension <code class="reqn">n\times n</code> which can be computationally costly
in terms of memory requirements and may be unsuitable for large
datasets. Landweber-Fridman will be preferred in such settings as it
does not require construction and storage of these weight matrices
while it also avoids the need for numerical optimization methods to
determine <code class="reqn">\alpha</code>.
</p>
<p><code>method="Landweber-Fridman"</code> uses an optimal stopping rule based
upon <code class="reqn">||E(y|w)-E(\varphi_k(z,x)|w)||^2
  </code>. However, if insufficient training is
conducted the estimates can be overly noisy. To best guard against
this eventuality set <code>nmulti</code> to a larger number than the default
<code>nmulti=1</code> for <code><a href="#topic+npreg">npreg</a></code>.
</p>
<p>When using <code>method="Landweber-Fridman"</code>, iteration will terminate
when either the change in the value of
<code class="reqn">||(E(y|w)-E(\varphi_k(z,x)|w))/E(y|w)||^2
  </code> from iteration to iteration is
less than <code>iterate.diff.tol</code> or we hit <code>iterate.max</code> or
<code class="reqn">||(E(y|w)-E(\varphi_k(z,x)|w))/E(y|w)||^2
  </code> stops falling in value and
starts rising.
</p>
<p>The option <code>bw=</code> would be useful, say, when bootstrapping is
necessary. Note that when passing <code>bw</code>, it must be obtained from
a previous invocation of <code>npregiv</code>. For instance, if
<code>model.iv</code> was obtained from an invocation of <code>npregiv</code> with
<code>method="Landweber-Fridman"</code>, then the following needs to be fed
to the subsequent invocation of <code>npregiv</code>:
</p>
<pre>

    model.iv &lt;- npregiv(\dots)

    bw &lt;- NULL
    bw$bw.E.y.w &lt;- model.iv$bw.E.y.w
    bw$bw.E.y.z &lt;- model.iv$bw.E.y.z
    bw$bw.resid.w &lt;- model.iv$bw.resid.w
    bw$bw.resid.fitted.w.z &lt;- model.iv$bw.resid.fitted.w.z
    bw$norm.index &lt;- model.iv$norm.index

    foo &lt;- npregiv(\dots,bw=bw)
  </pre>
<p>If, on the other hand <code>model.iv</code> was obtained from an invocation
of <code>npregiv</code> with <code>method="Tikhonov"</code>, then the following
needs to be fed to the subsequent invocation of <code>npregiv</code>:
</p>
<pre>

    model.iv &lt;- npregiv(\dots)

    bw &lt;- NULL
    bw$alpha &lt;- model.iv$alpha
    bw$alpha.iter &lt;- model.iv$alpha.iter
    bw$bw.E.y.w &lt;- model.iv$bw.E.y.w
    bw$bw.E.E.y.w.z &lt;- model.iv$bw.E.E.y.w.z
    bw$bw.E.phi.w &lt;- model.iv$bw.E.phi.w
    bw$bw.E.E.phi.w.z &lt;- model.iv$bw.E.E.phi.w.z

    foo &lt;- npregiv(\dots,bw=bw)    

  </pre>
<p>Or, if <code>model.iv</code> was obtained from an invocation of
<code>npregiv</code> with either <code>method="Landweber-Fridman"</code> or
<code>method="Tikhonov"</code>, then the following would also work:
</p>
<pre>

    model.iv &lt;- npregiv(\dots)

    foo &lt;- npregiv(\dots,bw=model.iv)    

  </pre>
<p>When exogenous predictors <code>x</code> (<code>xeval</code>) are passed, they are
appended to both the endogenous predictors <code>z</code> and the
instruments <code>w</code> as additional columns. If this is not desired,
one can manually append the exogenous variables to <code>z</code> (or
<code>w</code>) prior to passing <code>z</code> (or <code>w</code>), and then they will
only appear among the <code>z</code> or <code>w</code> as desired.
</p>


<h3>Value</h3>

<p><code>npregiv</code> returns a list with components <code>phi</code>,
<code>phi.mat</code> and either <code>alpha</code> when <code>method="Tikhonov"</code>
or <code>norm.index</code>, <code>norm.stop</code> and <code>convergence</code> when
<code>method="Landweber-Fridman"</code>, among others.
</p>
<p>In addition, if any of <code>return.weights.*</code> are invoked
(<code>*=1,2</code>), then <code>phi.weights</code> and <code>phi.deriv.*.weights</code>
return weight matrices for computing the instrumental regression and
its partial derivatives. Note that these weights, post multiplied by
the response vector <code class="reqn">y</code>, will deliver the estimates returned in
<code>phi</code>, <code>phi.deriv.1</code>, and <code>phi.deriv.2</code> (the latter
only being produced when <code>p</code> is 2 or greater). When invoked with
evaluation data, similar matrices are returned but named
<code>phi.eval.weights</code> and <code>phi.deriv.eval.*.weights</code>. These
weights can be used for constrained estimation, among others.
</p>
<p>When <code>method="Landweber-Fridman"</code> is invoked, bandwidth objects
are returned in <code>bw.E.y.w</code> (scalar/vector), <code>bw.E.y.z</code>
(scalar/vector), and <code>bw.resid.w</code> (matrix) and
<code>bw.resid.fitted.w.z</code>, the latter matrices containing bandwidths
for each iteration stored as rows. When <code>method="Tikhonov"</code> is
invoked, bandwidth objects are returned in <code>bw.E.y.w</code>,
<code>bw.E.E.y.w.z</code>, and <code>bw.E.phi.w</code> and <code>bw.E.E.phi.w.z</code>.
</p>


<h3>Note</h3>

<p>This function should be considered to be in &lsquo;beta test&rsquo; status until further notice.
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>, Samuele Centorrino
<a href="mailto:samuele.centorrino@univ-tlse1.fr">samuele.centorrino@univ-tlse1.fr</a>
</p>


<h3>References</h3>

<p>Carrasco, M. and J.P. Florens and E. Renault (2007), &ldquo;Linear
Inverse Problems in Structural Econometrics Estimation Based on
Spectral Decomposition and Regularization,&rdquo; In: James J. Heckman and
Edward E. Leamer, Editor(s), Handbook of Econometrics, Elsevier, 2007,
Volume 6, Part 2, Chapter 77, Pages 5633-5751
</p>
<p>Darolles, S. and Y. Fan and J.P. Florens and E. Renault (2011),
&ldquo;Nonparametric instrumental regression,&rdquo; Econometrica, 79,
1541-1565.
</p>
<p>Feve, F. and J.P. Florens (2010), &ldquo;The practice of
non-parametric estimation by solving inverse problems: the example of
transformation models,&rdquo; Econometrics Journal, 13, S1-S27.
</p>
<p>Florens, J.P. and J.S. Racine and S. Centorrino (forthcoming),
&ldquo;Nonparametric instrumental derivatives,&rdquo; Journal of
Nonparametric Statistics.
</p>
<p>Fridman, V. M. (1956), &ldquo;A method of successive approximations
for Fredholm integral equations of the first kind,&rdquo; Uspeskhi,
Math. Nauk., 11, 233-334, in Russian.
</p>
<p>Horowitz, J.L. (2011), &ldquo;Applied nonparametric instrumental
variables estimation,&rdquo; Econometrica, 79, 347-394.
</p>
<p>Landweber, L. (1951), &ldquo;An iterative formula for Fredholm
integral equations of the first kind,&rdquo; American Journal of
Mathematics, 73, 615-24.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics:
Theory and Practice,</em> Princeton University Press.
</p>
<p>Li, Q. and J.S. Racine (2004), &ldquo;Cross-validated Local Linear
Nonparametric Regression,&rdquo; Statistica Sinica, 14, 485-512.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npregivderiv">npregivderiv</a>,<a href="#topic+npreg">npreg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## This illustration was made possible by Samuele Centorrino
## &lt;samuele.centorrino@univ-tlse1.fr&gt;

set.seed(42)
n &lt;- 500

## The DGP is as follows:

## 1) y = phi(z) + u

## 2) E(u|z) != 0 (endogeneity present)

## 3) Suppose there exists an instrument w such that z = f(w) + v and
## E(u|w) = 0

## 4) We generate v, w, and generate u such that u and z are
## correlated. To achieve this we express u as a function of v (i.e. u =
## gamma v + eps)

v &lt;- rnorm(n,mean=0,sd=0.27)
eps &lt;- rnorm(n,mean=0,sd=0.05)
u &lt;- -0.5*v + eps
w &lt;- rnorm(n,mean=0,sd=1)

## In Darolles et al (2011) there exist two DGPs. The first is
## phi(z)=z^2 and the second is phi(z)=exp(-abs(z)) (which is
## discontinuous and has a kink at zero).

fun1 &lt;- function(z) { z^2 }
fun2 &lt;- function(z) { exp(-abs(z)) }

z &lt;- 0.2*w + v

## Generate two y vectors for each function.

y1 &lt;- fun1(z) + u
y2 &lt;- fun2(z) + u

## You set y to be either y1 or y2 (ditto for phi) depending on which
## DGP you are considering:

y &lt;- y1
phi &lt;- fun1

## Sort on z (for plotting)

ivdata &lt;- data.frame(y,z,w)
ivdata &lt;- ivdata[order(ivdata$z),]
rm(y,z,w)
attach(ivdata)

model.iv &lt;- npregiv(y=y,z=z,w=w)
phi.iv &lt;- model.iv$phi

## Now the non-iv local linear estimator of E(y|z)

ll.mean &lt;- fitted(npreg(y~z,regtype="ll"))

## For the plots, restrict focal attention to the bulk of the data
## (i.e. for the plotting area trim out 1/4 of one percent from each
## tail of y and z)

trim &lt;- 0.0025

curve(phi,min(z),max(z),
      xlim=quantile(z,c(trim,1-trim)),
      ylim=quantile(y,c(trim,1-trim)),
      ylab="Y",
      xlab="Z",
      main="Nonparametric Instrumental Kernel Regression",
      lwd=2,lty=1)

points(z,y,type="p",cex=.25,col="grey")

lines(z,phi.iv,col="blue",lwd=2,lty=2)

lines(z,ll.mean,col="red",lwd=2,lty=4)

legend("topright",
       c(expression(paste(varphi(z))),
         expression(paste("Nonparametric ",hat(varphi)(z))),
         "Nonparametric E(y|z)"),
       lty=c(1,2,4),
       col=c("black","blue","red"),
       lwd=c(2,2,2),
       bty="n")
       

## End(Not run) 
</code></pre>

<hr>
<h2 id='npregivderiv'>
Nonparametric Instrumental Derivatives
</h2><span id='topic+npregivderiv'></span>

<h3>Description</h3>

<p><code>npregivderiv</code> uses the approach of Florens, Racine and Centorrino
(forthcoming) to compute the partial derivative of a nonparametric
estimation of an instrumental regression function <code class="reqn">\varphi</code>
defined by conditional moment restrictions stemming from a structural
econometric model: <code class="reqn">E [Y - \varphi (Z,X) | W ] = 0</code>, and involving endogenous variables <code class="reqn">Y</code> and <code class="reqn">Z</code> and
exogenous variables <code class="reqn">X</code> and instruments <code class="reqn">W</code>. The derivative
function <code class="reqn">\varphi'</code> is the solution of an ill-posed inverse
problem, and is computed using Landweber-Fridman regularization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npregivderiv(y,
             z,
             w,
             x = NULL,
             zeval = NULL,
             weval = NULL,
             xeval = NULL,
             random.seed = 42,
             iterate.max = 1000,
             iterate.break = TRUE,
             constant = 0.5,
             start.from = c("Eyz","EEywz"),
             starting.values = NULL,
             stop.on.increase = TRUE,
             smooth.residuals = TRUE,
             ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npregivderiv_+3A_y">y</code></td>
<td>

<p>a one (1) dimensional numeric or integer vector of dependent data, each
element <code class="reqn">i</code> corresponding to each observation (row) <code class="reqn">i</code> of
<code>z</code>.
</p>
</td></tr>
<tr><td><code id="npregivderiv_+3A_z">z</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of endogenous regressors. The data
types may be continuous, discrete (unordered and ordered factors),
or some combination thereof.
</p>
</td></tr>
<tr><td><code id="npregivderiv_+3A_w">w</code></td>
<td>

<p>a <code class="reqn">q</code>-variate data frame of instruments. The data types may be
continuous, discrete (unordered and ordered factors), or some
combination thereof.
</p>
</td></tr>
<tr><td><code id="npregivderiv_+3A_x">x</code></td>
<td>

<p>an <code class="reqn">r</code>-variate data frame of exogenous regressors. The data
types may be continuous, discrete (unordered and ordered factors),
or some combination thereof.
</p>
</td></tr>
<tr><td><code id="npregivderiv_+3A_zeval">zeval</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of endogenous regressors on which the
regression will be estimated (evaluation data). By default, evaluation
takes place on the data provided by <code>z</code>.
</p>
</td></tr>
<tr><td><code id="npregivderiv_+3A_weval">weval</code></td>
<td>

<p>a <code class="reqn">q</code>-variate data frame of instruments on which the regression
will be estimated (evaluation data). By default, evaluation
takes place on the data provided by <code>w</code>.
</p>
</td></tr>
<tr><td><code id="npregivderiv_+3A_xeval">xeval</code></td>
<td>

<p>an <code class="reqn">r</code>-variate data frame of exogenous regressors on which the
regression will be estimated (evaluation data). By default,
evaluation takes place on the data provided by <code>x</code>.
</p>
</td></tr>
<tr><td><code id="npregivderiv_+3A_random.seed">random.seed</code></td>
<td>

<p>an integer used to seed R's random number generator. This ensures
replicability of the numerical search. Defaults to 42.
</p>
</td></tr>
<tr><td><code id="npregivderiv_+3A_iterate.max">iterate.max</code></td>
<td>

<p>an integer indicating the maximum number of iterations permitted
before termination occurs for Landweber-Fridman iteration.
</p>
</td></tr>
<tr><td><code id="npregivderiv_+3A_iterate.break">iterate.break</code></td>
<td>

<p>a logical value indicating whether to compute all objects up to
<code>iterate.max</code> or to break when a potential optimum arises
(useful for inspecting full stopping rule profile up to
<code>iterate.max</code>)
</p>
</td></tr>
<tr><td><code id="npregivderiv_+3A_constant">constant</code></td>
<td>

<p>the constant to use for Landweber-Fridman iteration.
</p>
</td></tr>
<tr><td><code id="npregivderiv_+3A_start.from">start.from</code></td>
<td>
<p> a character string indicating whether to start from
<code class="reqn">E(Y|z)</code> (default, <code>"Eyz"</code>) or from <code class="reqn">E(E(Y|z)|z)</code> (this can
be overridden by providing <code>starting.values</code> below)
</p>
</td></tr>  
<tr><td><code id="npregivderiv_+3A_starting.values">starting.values</code></td>
<td>
<p> a value indicating whether to commence
Landweber-Fridman assuming
<code class="reqn">\varphi'_{-1}=starting.values</code> (proper
Landweber-Fridman) or instead begin from <code class="reqn">E(y|z)</code> (defaults to
<code>NULL</code>, see details below)
</p>
</td></tr>
<tr><td><code id="npregivderiv_+3A_stop.on.increase">stop.on.increase</code></td>
<td>

<p>a logical value (defaults to <code>TRUE</code>) indicating whether to halt
iteration if the stopping criterion (see below) increases over the
course of one iteration (i.e. it may be above the iteration tolerance
but increased).
</p>
</td></tr>
<tr><td><code id="npregivderiv_+3A_smooth.residuals">smooth.residuals</code></td>
<td>

<p>a logical value (defaults to <code>TRUE</code>) indicating whether to
optimize bandwidths for the regression of <code class="reqn">y-\varphi(z)</code>
on <code class="reqn">w</code> or for the regression of <code class="reqn">\varphi(z)</code> on
<code class="reqn">w</code> during Landweber-Fridman iteration.
</p>
</td></tr>
<tr><td><code id="npregivderiv_+3A_...">...</code></td>
<td>
 
<p>additional arguments supplied to <code><a href="#topic+npreg">npreg</a></code> and <code><a href="#topic+npksum">npksum</a></code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that Landweber-Fridman iteration presumes that
<code class="reqn">\varphi_{-1}=0</code>, and so for derivative estimation we
commence iterating from a model having derivatives all equal to
zero. Given this starting point it may require a fairly large number
of iterations in order to converge. Other perhaps more reasonable
starting values might present themselves. When <code>start.phi.zero</code>
is set to <code>FALSE</code> iteration will commence instead using
derivatives from the conditional mean model <code class="reqn">E(y|z)</code>. Should the
default iteration terminate quickly or you are concerned about your
results, it would be prudent to verify that this alternative starting
value produces the same result. Also, check the norm.stop vector for
any anomalies (such as the error criterion increasing immediately).
</p>
<p>Landweber-Fridman iteration uses an optimal stopping rule based upon
<code class="reqn">||E(y|w)-E(\varphi_k(z,x)|w)||^2 </code>. However, if insufficient training is conducted the estimates can be
overly noisy. To best guard against this eventuality set <code>nmulti</code>
to a larger number than the default <code>nmulti=1</code> for
<code><a href="#topic+npreg">npreg</a></code>.
</p>
<p>Iteration will terminate when either the change in the value of
<code class="reqn">||(E(y|w)-E(\varphi_k(z,x)|w))/E(y|w)||^2
  </code> from iteration to iteration is
less than <code>iterate.diff.tol</code> or we hit <code>iterate.max</code> or
<code class="reqn">||(E(y|w)-E(\varphi_k(z,x)|w))/E(y|w)||^2
  </code> stops falling in value and
starts rising.  </p>


<h3>Value</h3>

<p><code>npregivderiv</code> returns a list with components <code>phi.prime</code>,
<code>phi</code>, <code>num.iterations</code>, <code>norm.stop</code> and
<code>convergence</code>.
</p>


<h3>Note</h3>

<p>This function currently supports univariate <code>z</code> only.  This
function should be considered to be in &lsquo;beta test&rsquo; status until
further notice.
</p>


<h3>Author(s)</h3>

<p>Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Carrasco, M. and J.P. Florens and E. Renault (2007), &ldquo;Linear
Inverse Problems in Structural Econometrics Estimation Based on
Spectral Decomposition and Regularization,&rdquo; In: James J. Heckman and
Edward E. Leamer, Editor(s), Handbook of Econometrics, Elsevier, 2007,
Volume 6, Part 2, Chapter 77, Pages 5633-5751
</p>
<p>Darolles, S. and Y. Fan and J.P. Florens and E. Renault (2011),
&ldquo;Nonparametric instrumental regression,&rdquo; Econometrica, 79,
1541-1565.
</p>
<p>Feve, F. and J.P. Florens (2010), &ldquo;The practice of
non-parametric estimation by solving inverse problems: the example of
transformation models,&rdquo; Econometrics Journal, 13, S1-S27.
</p>
<p>Florens, J.P. and J.S. Racine and S. Centorrino (forthcoming),
&ldquo;Nonparametric instrumental derivatives,&rdquo; Journal of
Nonparametric Statistics.
</p>
<p>Fridman, V. M. (1956), &ldquo;A method of successive approximations
for Fredholm integral equations of the first kind,&rdquo; Uspeskhi,
Math. Nauk., 11, 233-334, in Russian.
</p>
<p>Horowitz, J.L. (2011), &ldquo;Applied nonparametric instrumental
variables estimation,&rdquo; Econometrica, 79, 347-394.
</p>
<p>Landweber, L. (1951), &ldquo;An iterative formula for Fredholm
integral equations of the first kind,&rdquo; American Journal of
Mathematics, 73, 615-24.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics:
Theory and Practice,</em> Princeton University Press.
</p>
<p>Li, Q. and J.S. Racine (2004), &ldquo;Cross-validated Local Linear
Nonparametric Regression,&rdquo; Statistica Sinica, 14, 485-512.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npregiv">npregiv</a>,<a href="#topic+npreg">npreg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## This illustration was made possible by Samuele Centorrino
## &lt;samuele.centorrino@univ-tlse1.fr&gt;

set.seed(42)
n &lt;- 1500

## For trimming the plot (trim .5% from each tail)

trim &lt;- 0.005

## The DGP is as follows:

## 1) y = phi(z) + u

## 2) E(u|z) != 0 (endogeneity present)

## 3) Suppose there exists an instrument w such that z = f(w) + v and
## E(u|w) = 0

## 4) We generate v, w, and generate u such that u and z are
## correlated. To achieve this we express u as a function of v (i.e. u =
## gamma v + eps)

v &lt;- rnorm(n,mean=0,sd=0.27)
eps &lt;- rnorm(n,mean=0,sd=0.05)
u &lt;- -0.5*v + eps
w &lt;- rnorm(n,mean=0,sd=1)

## In Darolles et al (2011) there exist two DGPs. The first is
## phi(z)=z^2 and the second is phi(z)=exp(-abs(z)) (which is
## discontinuous and has a kink at zero).

fun1 &lt;- function(z) { z^2 }
fun2 &lt;- function(z) { exp(-abs(z)) }

z &lt;- 0.2*w + v

## Generate two y vectors for each function.

y1 &lt;- fun1(z) + u
y2 &lt;- fun2(z) + u

## You set y to be either y1 or y2 (ditto for phi) depending on which
## DGP you are considering:

y &lt;- y1
phi &lt;- fun1

## Sort on z (for plotting)

ivdata &lt;- data.frame(y,z,w,u,v)
ivdata &lt;- ivdata[order(ivdata$z),]
rm(y,z,w,u,v)
attach(ivdata)

model.ivderiv &lt;- npregivderiv(y=y,z=z,w=w)

ylim &lt;-c(quantile(model.ivderiv$phi.prime,trim),
         quantile(model.ivderiv$phi.prime,1-trim))

plot(z,model.ivderiv$phi.prime,
     xlim=quantile(z,c(trim,1-trim)),
     main="",
     ylim=ylim,
     xlab="Z",
     ylab="Derivative",
     type="l",
     lwd=2)
rug(z)     

## End(Not run) 
</code></pre>

<hr>
<h2 id='npscoef'>Smooth Coefficient Kernel Regression</h2><span id='topic+npscoef'></span><span id='topic+npscoef.call'></span><span id='topic+npscoef.default'></span><span id='topic+npscoef.formula'></span><span id='topic+npscoef.scbandwidth'></span>

<h3>Description</h3>

<p><code>npscoef</code> computes a kernel regression estimate of a one (1)
dimensional dependent variable on <code class="reqn">p</code>-variate explanatory data,
using the model <code class="reqn">Y_i = W_{i}^{\prime} \gamma (Z_i) + u_i</code> where
<code class="reqn">W_i'=(1,X_i')</code>, given a set of evaluation
points, training points (consisting of explanatory data and dependent
data), and a bandwidth specification. A bandwidth specification can be
a <code>scbandwidth</code> object, or a bandwidth vector, bandwidth type and
kernel type.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npscoef(bws, ...)

## S3 method for class 'formula'
npscoef(bws, data = NULL, newdata = NULL, y.eval =
FALSE, ...)

## S3 method for class 'call'
npscoef(bws, ...)

## Default S3 method:
npscoef(bws, txdat, tydat, tzdat, ...)

## S3 method for class 'scbandwidth'
npscoef(bws,
        txdat = stop("training data 'txdat' missing"),
        tydat = stop("training data 'tydat' missing"),
        tzdat = NULL,
        exdat,
        eydat,
        ezdat,
        residuals = FALSE,
        errors = TRUE,
        iterate = TRUE,
        maxiter = 100,
        tol = .Machine$double.eps,
        leave.one.out = FALSE,
        betas = FALSE,
        ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npscoef_+3A_bws">bws</code></td>
<td>

<p>a bandwidth specification. This can be set as a <code>scbandwidth</code>
object returned from an invocation of <code><a href="#topic+npscoefbw">npscoefbw</a></code>, or
as a vector of bandwidths, with each element <code class="reqn">i</code> corresponding
to the bandwidth for column <code class="reqn">i</code> in <code>tzdat</code>. If specified as
a vector
additional arguments will need to be supplied as necessary to
specify the bandwidth type, kernel types, training data, and so on.
</p>
</td></tr>
<tr><td><code id="npscoef_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to specify the regression type,
bandwidth type, kernel types, selection methods, and so on.
To do this, you may specify any of <code>bwscaling</code>, <code>bwtype</code>,
<code>ckertype</code>, <code>ckerorder</code>, as described in
<code><a href="#topic+npscoefbw">npscoefbw</a></code>. 
</p>
</td></tr>
<tr><td><code id="npscoef_+3A_data">data</code></td>
<td>

<p>an optional data frame, list or environment (or object
coercible to a data frame by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code>) containing
the variables 
in the model. If not found in data, the variables are taken from
<code>environment(bws)</code>, typically the environment from which
<code><a href="#topic+npscoefbw">npscoefbw</a></code> was called.
</p>
</td></tr>
<tr><td><code id="npscoef_+3A_newdata">newdata</code></td>
<td>

<p>An optional data frame in which to look for evaluation data. If
omitted, the training data are used.  
</p>
</td></tr>
<tr><td><code id="npscoef_+3A_y.eval">y.eval</code></td>
<td>

<p>If <code>newdata</code> contains dependent data and <code>y.eval = TRUE</code>,
<code><a href="#topic+np">np</a></code> will compute goodness of fit statistics on these
data and return them. Defaults to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="npscoef_+3A_txdat">txdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of explanatory data (training data),
which, by default, populates the columns <code class="reqn">2</code> through <code class="reqn">p+1</code>
of <code class="reqn">W</code> in the model equation, and in the 
absence of <code>zdat</code>, will also correspond to 
<code class="reqn">Z</code> from the model equation. Defaults to
the training data used to 
compute the bandwidth object.
</p>
</td></tr>
<tr><td><code id="npscoef_+3A_tydat">tydat</code></td>
<td>

<p>a one (1) dimensional numeric or integer vector of dependent data, each
element <code class="reqn">i</code> corresponding to each observation (row) <code class="reqn">i</code> of
<code>txdat</code>. Defaults to
the training data used to 
compute the bandwidth object.
</p>
</td></tr>
<tr><td><code id="npscoef_+3A_tzdat">tzdat</code></td>
<td>

<p>an optionally specified <code class="reqn">q</code>-variate data frame of explanatory data
(training data), which corresponds to <code class="reqn">Z</code> in the model
equation. Defaults to 
the training data used to 
compute the bandwidth object.
</p>
</td></tr>
<tr><td><code id="npscoef_+3A_exdat">exdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of points on which the regression will be
estimated (evaluation data).By default,
evaluation takes place on the data provided by <code>txdat</code>.
</p>
</td></tr>
<tr><td><code id="npscoef_+3A_eydat">eydat</code></td>
<td>

<p>a one (1) dimensional numeric or integer vector of the true values
of the dependent variable. Optional, and used only to calculate the
true errors. 
</p>
</td></tr>
<tr><td><code id="npscoef_+3A_ezdat">ezdat</code></td>
<td>

<p>an optionally specified <code class="reqn">q</code>-variate data frame of points on
which
the regression will be estimated
(evaluation data), which corresponds to <code class="reqn">Z</code>
in the model equation. Defaults to be the same as <code>txdat</code>.
</p>
</td></tr>
<tr><td><code id="npscoef_+3A_errors">errors</code></td>
<td>

<p>a logical value indicating whether or not asymptotic standard errors
should be computed and returned in the resulting
<code>smoothcoefficient</code> object. Defaults to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npscoef_+3A_residuals">residuals</code></td>
<td>

<p>a logical value indicating that you want residuals computed and
returned in the resulting <code>smoothcoefficient</code> object. Defaults to
<code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="npscoef_+3A_iterate">iterate</code></td>
<td>

<p>a logical value indicating whether or not backfitted estimates
should be iterated for self-consistency. Defaults to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npscoef_+3A_maxiter">maxiter</code></td>
<td>

<p>integer specifying the maximum number of times to iterate the
backfitted estimates while attempting to make the backfitted estimates
converge to the desired tolerance. Defaults to <code>100</code>.
</p>
</td></tr>
<tr><td><code id="npscoef_+3A_tol">tol</code></td>
<td>

<p>desired tolerance on the relative convergence of backfit
estimates. Defaults to <code>.Machine$double.eps</code>.
</p>
</td></tr>
<tr><td><code id="npscoef_+3A_leave.one.out">leave.one.out</code></td>
<td>

<p>a logical value to specify whether or not to compute the leave one
out estimates. Will not work if <code>e[xyz]dat</code> is specified. Defaults to
<code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="npscoef_+3A_betas">betas</code></td>
<td>

<p>a logical value indicating whether or not estimates of the
components of <code class="reqn">\gamma</code> should be returned in the
<code>smoothcoefficient</code> object along with the regression estimates.
Defaults to <code>FALSE</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>npscoef</code> returns a <code>smoothcoefficient</code> object.   The generic
functions <code><a href="stats.html#topic+fitted">fitted</a></code>, <code><a href="stats.html#topic+residuals">residuals</a></code>, <code><a href="stats.html#topic+coef">coef</a></code>,
<code><a href="#topic+se">se</a></code>, and <code><a href="stats.html#topic+predict">predict</a></code>,
extract (or generate) estimated values,
residuals, coefficients, bootstrapped standard
errors on estimates, and predictions, respectively, from
the returned object. Furthermore, the functions <code><a href="base.html#topic+summary">summary</a></code>
and <code><a href="base.html#topic+plot">plot</a></code> support objects of this type. The returned object
has the following components:
</p>
<table>
<tr><td><code>eval</code></td>
<td>
<p> evaluation points </p>
</td></tr>
<tr><td><code>mean</code></td>
<td>
<p> estimation of the regression function (conditional mean) at the
evaluation points </p>
</td></tr> 
<tr><td><code>merr</code></td>
<td>
<p> if <code>errors = TRUE</code>, standard errors of the
regression estimates </p>
</td></tr> 
<tr><td><code>beta</code></td>
<td>
<p> if <code>betas = TRUE</code>, estimates of the coefficients
<code class="reqn">\gamma</code> at the 
evaluation points </p>
</td></tr>
<tr><td><code>resid</code></td>
<td>
<p> if <code>residuals = TRUE</code>, in-sample or out-of-sample
residuals where appropriate (or possible) </p>
</td></tr>
<tr><td><code>R2</code></td>
<td>
<p> coefficient of determination (Doksum and Samarov (1995))</p>
</td></tr>
<tr><td><code>MSE</code></td>
<td>
<p> mean squared error </p>
</td></tr>
<tr><td><code>MAE</code></td>
<td>
<p> mean absolute error </p>
</td></tr>
<tr><td><code>MAPE</code></td>
<td>
<p> mean absolute percentage error </p>
</td></tr>
<tr><td><code>CORR</code></td>
<td>
<p> absolute value of Pearson's correlation coefficient </p>
</td></tr>
<tr><td><code>SIGN</code></td>
<td>
<p> fraction of observations where fitted and observed values
agree in sign </p>
</td></tr>
</table>


<h3>Usage Issues</h3>

<p>If you are using data of mixed types, then it is advisable to use the
<code><a href="base.html#topic+data.frame">data.frame</a></code> function to construct your input data and not
<code><a href="base.html#topic+cbind">cbind</a></code>, since <code><a href="base.html#topic+cbind">cbind</a></code> will typically not work as
intended on mixed data types and will coerce the data to the same
type.
</p>
<p>Support for backfitted bandwidths is experimental and is limited in
functionality. The code does not support asymptotic standard errors
or out of sample estimates with backfitting.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Aitchison, J. and C.G.G. Aitken (1976), &ldquo;Multivariate binary
discrimination by the kernel method,&rdquo; Biometrika, 63, 413-420.
</p>
<p>Cai Z. (2007), &ldquo;Trending time-varying coefficient time series
models with serially correlated errors,&rdquo; Journal of Econometrics,
136, 163-188.
</p>
<p>Doksum, K. and A. Samarov (1995), &ldquo;Nonparametric estimation of
global functionals and a measure of the explanatory power of
covariates in regression,&rdquo; The Annals of Statistics, 23 1443-1473.
</p>
<p>Hastie, T. and R. Tibshirani (1993), &ldquo;Varying-coefficient
models,&rdquo; Journal of the Royal Statistical Society, B 55, 757-796.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics:
Theory and Practice,</em> Princeton University Press.
</p>
<p>Li, Q. and J.S. Racine (2010), &ldquo;Smooth varying-coefficient
estimation and inference for qualitative and quantitative data,&rdquo;
Econometric Theory, 26, 1-31.
</p>
<p>Pagan, A. and A. Ullah (1999), <em>Nonparametric Econometrics,</em>
Cambridge University Press.
</p>
<p>Li, Q. and D. Ouyang and J.S. Racine (2013), &ldquo;Categorical
semiparametric varying-coefficient models,&rdquo; Journal of Applied
Econometrics, 28, 551-589.
</p>
<p>Wang, M.C. and J. van Ryzin (1981), &ldquo;A class of smooth
estimators for discrete distributions,&rdquo;  Biometrika, 68, 301-309.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+bw.nrd">bw.nrd</a></code>, <code><a href="stats.html#topic+bw.SJ">bw.SJ</a></code>, <code><a href="graphics.html#topic+hist">hist</a></code>,
<code><a href="#topic+npudens">npudens</a></code>, <code><a href="#topic+npudist">npudist</a></code>,
<code><a href="#topic+npudensbw">npudensbw</a></code>, <code><a href="#topic+npscoefbw">npscoefbw</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# EXAMPLE 1 (INTERFACE=FORMULA): 

n &lt;- 250
x &lt;- runif(n)
z &lt;- runif(n, min=-2, max=2)
y &lt;- x*exp(z)*(1.0+rnorm(n,sd = 0.2))
bw &lt;- npscoefbw(y~x|z)
model &lt;- npscoef(bw)
plot(model)

# EXAMPLE 1 (INTERFACE=DATA FRAME): 

n &lt;- 250
x &lt;- runif(n)
z &lt;- runif(n, min=-2, max=2)
y &lt;- x*exp(z)*(1.0+rnorm(n,sd = 0.2))
bw &lt;- npscoefbw(xdat=x, ydat=y, zdat=z)
model &lt;- npscoef(bw)
plot(model)

## End(Not run) 
</code></pre>

<hr>
<h2 id='npscoefbw'>Smooth Coefficient Kernel Regression Bandwidth Selection</h2><span id='topic+npscoefbw'></span><span id='topic+npscoefbw.NULL'></span><span id='topic+npscoefbw.default'></span><span id='topic+npscoefbw.formula'></span><span id='topic+npscoefbw.scbandwidth'></span>

<h3>Description</h3>

<p><code>npscoefbw</code> computes a bandwidth object for a smooth
coefficient kernel regression estimate of a one (1) dimensional
dependent variable on 
<code class="reqn">p+q</code>-variate explanatory data, using the model
<code class="reqn">Y_i = W_{i}^{\prime} \gamma (Z_i) + u_i</code> where <code class="reqn">W_i'=(1,X_i')</code>
given training points (consisting of explanatory data and dependent
data), and a bandwidth specification, which can be a <code>rbandwidth</code>
object, or a bandwidth vector, bandwidth type and kernel type. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npscoefbw(...)

## S3 method for class 'formula'
npscoefbw(formula, data, subset, na.action, call, ...)

## S3 method for class 'NULL'
npscoefbw(xdat = stop("invoked without data 'xdat'"),
          ydat = stop("invoked without data 'ydat'"),
          zdat = NULL,
          bws,
          ...)

## Default S3 method:
npscoefbw(xdat = stop("invoked without data 'xdat'"),
          ydat = stop("invoked without data 'ydat'"),
          zdat = NULL,
          bws,
          nmulti,
          random.seed,
          cv.iterate,
          cv.num.iterations,
          backfit.iterate,
          backfit.maxiter,
          backfit.tol,
          bandwidth.compute = TRUE,
          bwmethod,
          bwscaling,
          bwtype,
          ckertype,
          ckerorder,
          ukertype,
          okertype,
          optim.method,
          optim.maxattempts,
          optim.reltol,
          optim.abstol,
          optim.maxit,
          ...)

## S3 method for class 'scbandwidth'
npscoefbw(xdat = stop("invoked without data 'xdat'"),
          ydat = stop("invoked without data 'ydat'"),
          zdat = NULL,
          bws,
          nmulti,
          random.seed = 42,
          cv.iterate = FALSE,
          cv.num.iterations = 1,
          backfit.iterate = FALSE,
          backfit.maxiter = 100,
          backfit.tol = .Machine$double.eps,
          bandwidth.compute = TRUE,
          optim.method = c("Nelder-Mead", "BFGS", "CG"),
          optim.maxattempts = 10,
          optim.reltol = sqrt(.Machine$double.eps),
          optim.abstol = .Machine$double.eps,
          optim.maxit = 500,
          ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npscoefbw_+3A_formula">formula</code></td>
<td>

<p>a symbolic description of variables on which bandwidth selection is
to be performed. The details of constructing a formula are
described below.
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_data">data</code></td>
<td>

<p>an optional data frame, list or environment (or object
coercible to a data frame by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code>) containing the variables
in the model. If not found in data, the variables are taken from
<code>environment(formula)</code>, typically the environment from which the
function is called.
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations to be used in
the fitting process. 
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_na.action">na.action</code></td>
<td>

<p>a function which indicates what should happen when the data contain
<code>NA</code>s. The default is set by the <code><a href="stats.html#topic+na.action">na.action</a></code> setting of options, and is
<code><a href="stats.html#topic+na.fail">na.fail</a></code> if that is unset. The (recommended) default is
<code><a href="stats.html#topic+na.omit">na.omit</a></code>.  
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_call">call</code></td>
<td>

<p>the original function call. This is passed internally by
<code><a href="#topic+np">np</a></code> when a bandwidth search has been implied by a call to
another function. It is not recommended that the user set this.
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_xdat">xdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of explanatory data (training data),
which, by default, populates the columns <code class="reqn">2</code> through <code class="reqn">p+1</code>
of <code class="reqn">W</code> in the model equation, and in the 
absence of <code>zdat</code>, will also correspond to 
<code class="reqn">Z</code> from the model equation.
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_ydat">ydat</code></td>
<td>

<p>a one (1) dimensional numeric or integer vector of dependent data, each
element <code class="reqn">i</code> corresponding to each observation (row) <code class="reqn">i</code> of
<code>xdat</code>. 
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_zdat">zdat</code></td>
<td>

<p>an optionally specified <code class="reqn">q</code>-variate data frame of explanatory
data (training data), which corresponds to <code class="reqn">Z</code>
in the model equation. Defaults to be the same as <code>xdat</code>.
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_bws">bws</code></td>
<td>

<p>a bandwidth specification. This can be set as a <code>scbandwidth</code>
object returned from a previous invocation, or as a vector of
bandwidths, with each element <code class="reqn">i</code> corresponding to the bandwidth
for column <code class="reqn">i</code> in <code>xdat</code>. In either case, the bandwidth
supplied will serve as a starting point in the numerical search for
optimal bandwidths. If specified as a vector, then additional
arguments will need to be supplied as necessary to specify the
bandwidth type, kernel types, selection methods, and so on. This can
be left unset.
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to specify the regression type,
bandwidth type, kernel types, selection methods, and so on, detailed
below. 
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_bandwidth.compute">bandwidth.compute</code></td>
<td>

<p>a logical value which specifies whether to do a numerical search for
bandwidths or not. If set to <code>FALSE</code>, a <code>scbandwidth</code> object
will be returned with bandwidths set to those specified
in <code>bws</code>. Defaults to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_bwmethod">bwmethod</code></td>
<td>

<p>which method was used to select bandwidths. <code>cv.ls</code>
specifies least-squares cross-validation, which is all that is
currently supported. Defaults to <code>cv.ls</code>.
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_bwscaling">bwscaling</code></td>
<td>
<p> a logical value that when set to <code>TRUE</code> the
supplied bandwidths are interpreted as &lsquo;scale factors&rsquo;
(<code class="reqn">c_j</code>), otherwise when the value is <code>FALSE</code> they are
interpreted as &lsquo;raw bandwidths&rsquo; (<code class="reqn">h_j</code> for continuous data
types, <code class="reqn">\lambda_j</code> for discrete data types). For
continuous data types, <code class="reqn">c_j</code> and <code class="reqn">h_j</code> are
related by the formula <code class="reqn">h_j = c_j \sigma_j n^{-1/(2P+l)}</code>, where <code class="reqn">\sigma_j</code> is an
adaptive measure of spread of continuous variable <code class="reqn">j</code> defined as
min(standard deviation, mean absolute deviation, interquartile
range/1.349), <code class="reqn">n</code> the number of observations, <code class="reqn">P</code> the
order of the kernel, and <code class="reqn">l</code> the number of continuous
variables. For discrete data types, <code class="reqn">c_j</code> and
<code class="reqn">h_j</code> are related by the formula <code class="reqn">h_j =
    c_jn^{-2/(2P+l)}</code>, where here
<code class="reqn">j</code> denotes discrete variable <code class="reqn">j</code>.  Defaults to
<code>FALSE</code>.  </p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_bwtype">bwtype</code></td>
<td>

<p>character string used for the continuous variable bandwidth type,
specifying the type of bandwidth provided. Defaults to
<code>fixed</code>. Option summary:<br />
<code>fixed</code>: fixed bandwidths or scale factors <br />
<code>generalized_nn</code>: generalized nearest neighbors <br />
<code>adaptive_nn</code>:  adaptive nearest neighbors
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_ckertype">ckertype</code></td>
<td>

<p>character string used to specify the continuous kernel type.
Can be set as <code>gaussian</code>, <code>epanechnikov</code>, or
<code>uniform</code>. Defaults to <code>gaussian</code>.
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_ckerorder">ckerorder</code></td>
<td>

<p>numeric value specifying kernel order (one of
<code>(2,4,6,8)</code>). Kernel order specified along with a
<code>uniform</code> continuous kernel type will be ignored. Defaults to
<code>2</code>.
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_ukertype">ukertype</code></td>
<td>

<p>character string used to specify the unordered categorical kernel type.
Can be set as <code>aitchisonaitken</code> or <code>liracine</code>. Defaults to
<code>aitchisonaitken</code>.
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_okertype">okertype</code></td>
<td>

<p>character string used to specify the ordered categorical kernel type.
Can be set as <code>wangvanryzin</code> or <code>liracine</code>. Defaults to
<code>liracine</code>.
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_nmulti">nmulti</code></td>
<td>

<p>integer number of times to restart the process of finding extrema of
the cross-validation function from different (random) initial
points. Defaults to <code>min(5,ncol(xdat))</code>.
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_random.seed">random.seed</code></td>
<td>

<p>an integer used to seed R's random number generator. This ensures
replicability of the numerical search. Defaults to 42.
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_optim.method">optim.method</code></td>
<td>
<p> method used by <code><a href="stats.html#topic+optim">optim</a></code> for minimization of
the objective function. See <code>?optim</code> for references. Defaults
to <code>"Nelder-Mead"</code>.
</p>
<p>the default method is an implementation of that of Nelder and Mead
(1965), that uses only function values and is robust but relatively
slow.  It will work reasonably well for non-differentiable
functions.
</p>
<p>method <code>"BFGS"</code> is a quasi-Newton method (also known as a
variable metric algorithm), specifically that published
simultaneously in 1970 by Broyden, Fletcher, Goldfarb and Shanno.
This uses function values and gradients to build up a picture of the
surface to be optimized.
</p>
<p>method <code>"CG"</code> is a conjugate gradients method based
on that by Fletcher and Reeves (1964) (but with the option of
Polak-Ribiere or Beale-Sorenson updates).  Conjugate gradient
methods will generally be more fragile than the BFGS method, but as
they do not store a matrix they may be successful in much larger
optimization problems.
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_optim.maxattempts">optim.maxattempts</code></td>
<td>

<p>maximum number of attempts taken trying to achieve successful
convergence in <code><a href="stats.html#topic+optim">optim</a></code>. Defaults to <code>100</code>.
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_optim.abstol">optim.abstol</code></td>
<td>

<p>the absolute convergence tolerance used by <code><a href="stats.html#topic+optim">optim</a></code>. Only useful
for non-negative functions, as a tolerance for reaching
zero. Defaults to <code>.Machine$double.eps</code>.
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_optim.reltol">optim.reltol</code></td>
<td>

<p>relative convergence tolerance used by <code><a href="stats.html#topic+optim">optim</a></code>.  The algorithm
stops if it is unable to reduce the value by a factor of 'reltol *
(abs(val) + reltol)' at a step.  Defaults to
<code>sqrt(.Machine$double.eps)</code>, typically about <code>1e-8</code>.
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_optim.maxit">optim.maxit</code></td>
<td>

<p>maximum number of iterations used by <code><a href="stats.html#topic+optim">optim</a></code>. Defaults
to <code>500</code>.
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_cv.iterate">cv.iterate</code></td>
<td>

<p>boolean value specifying whether or not to perform iterative,
cross-validated backfitting on the data. See details for limitations
of the backfitting procedure. Defaults to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_cv.num.iterations">cv.num.iterations</code></td>
<td>

<p>integer specifying the number of times to iterate the backfitting
process over all covariates. Defaults to <code>1</code>.
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_backfit.iterate">backfit.iterate</code></td>
<td>

<p>boolean value specifying whether or not to iterate evaluations of
the smooth coefficient estimator, for extra accuracy, during the
cross-validated backfitting procedure. Defaults to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_backfit.maxiter">backfit.maxiter</code></td>
<td>

<p>integer specifying the maximum number of times to iterate the
evaluation of the smooth coefficient estimator in the attempt to
obtain the desired accuracy. Defaults to <code>100</code>.
</p>
</td></tr>
<tr><td><code id="npscoefbw_+3A_backfit.tol">backfit.tol</code></td>
<td>

<p>tolerance to determine convergence of iterated evaluations of the
smooth coefficient estimator. Defaults to <code>.Machine$double.eps</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>npscoefbw</code> implements a variety of methods for semiparametric
regression on multivariate (<code class="reqn">p+q</code>-variate) explanatory data defined
over a set of possibly continuous data. The approach is based on Li and
Racine (2003) who employ &lsquo;generalized product kernels&rsquo; that
admit a mix of continuous and discrete data types.
</p>
<p>Three classes of kernel estimators for the continuous data types are
available: fixed, adaptive nearest-neighbor, and generalized
nearest-neighbor. Adaptive nearest-neighbor bandwidths change with
each sample realization in the set, <code class="reqn">x_i</code>, when estimating the
density at the point <code class="reqn">x</code>. Generalized nearest-neighbor bandwidths change
with the point at which the density is estimated, <code class="reqn">x</code>. Fixed bandwidths
are constant over the support of <code class="reqn">x</code>.
</p>
<p><code>npscoefbw</code> may be invoked <em>either</em> with a formula-like
symbolic description of variables on which bandwidth selection is to be
performed <em>or</em> through a simpler interface whereby data is passed
directly to the function via the <code>xdat</code>, <code>ydat</code>, and
<code>zdat</code> parameters. Use of these two interfaces is <b>mutually
exclusive</b>.
</p>
<p>Data contained in the data frame <code>xdat</code> may be continuous and in
<code>zdat</code> may be of mixed type. Data can be entered in an arbitrary
order and data types will be detected automatically by the routine (see
<code><a href="#topic+np">np</a></code> for details).
</p>
<p>Data for which bandwidths are to be estimated may be specified
symbolically. A typical description has the form <code>dependent</code>
<code>data</code> <code>~</code> <code>parametric</code> <code>explanatory</code> <code>data</code>
<code>|</code> <code>nonparametric</code> <code>explanatory</code> <code>data</code>, where
<code>dependent</code> <code>data</code> is a univariate response, and
<code>parametric</code> <code>explanatory</code> <code>data</code> and
<code>nonparametric</code> <code>explanatory</code> <code>data</code> are both series of
variables specified by name, separated by the separation character
'+'. For example, <code> y1 ~ x1 + x2 | z1 </code> specifies that the
bandwidth object for the smooth coefficient model with response
<code>y1</code>, linear parametric regressors <code>x1</code> and <code>x2</code>, and
nonparametric regressor (that is, the slope-changing variable)
<code>z1</code> is to be estimated. See below for further examples.  In the
case where the nonparametric (slope-changing) variable is not
specified, it is assumed to be the same as the parametric variable.
</p>
<p>A variety of kernels may be specified by the user. Kernels implemented
for continuous data types include the second, fourth, sixth, and eighth
order Gaussian and Epanechnikov kernels, and the uniform
kernel. Unordered discrete data types use a variation on Aitchison and
Aitken's (1976) kernel, while ordered data types use a variation of the
Wang and van Ryzin (1981) kernel.
</p>


<h3>Value</h3>

<p>if <code>bwtype</code> is set to <code>fixed</code>, an object containing
bandwidths (or scale factors if <code>bwscaling = TRUE</code>) is
returned. If it is set to <code>generalized_nn</code> or <code>adaptive_nn</code>,
then instead the <code class="reqn">k</code>th nearest neighbors are returned for the
continuous variables while the discrete kernel bandwidths are returned
for the discrete variables. Bandwidths are stored in a vector under the
component name <code>bw</code>. Backfitted bandwidths are stored under the
component name <code>bw.fitted</code>.
</p>
<p>The functions <code><a href="stats.html#topic+predict">predict</a></code>, <code><a href="base.html#topic+summary">summary</a></code>, and
<code><a href="base.html#topic+plot">plot</a></code> support 
objects of this class.
</p>


<h3>Usage Issues</h3>

<p>If you are using data of mixed types, then it is advisable to use the
<code><a href="base.html#topic+data.frame">data.frame</a></code> function to construct your input data and not
<code><a href="base.html#topic+cbind">cbind</a></code>, since <code><a href="base.html#topic+cbind">cbind</a></code> will typically not work as
intended on mixed data types and will coerce the data to the same
type.
</p>
<p>Caution: multivariate data-driven bandwidth selection methods are, by
their nature, <em>computationally intensive</em>. Virtually all methods
require dropping the <code class="reqn">i</code>th observation from the data set,
computing an object, repeating this for all observations in the
sample, then averaging each of these leave-one-out estimates for a
<em>given</em> value of the bandwidth vector, and only then repeating
this a large number of times in order to conduct multivariate
numerical minimization/maximization. Furthermore, due to the potential
for local minima/maxima, <em>restarting this procedure a large
number of times may often be necessary</em>. This can be frustrating for
users possessing large datasets. For exploratory purposes, you may
wish to override the default search tolerances, say, setting
optim.reltol=.1 and conduct multistarting (the default is to restart
min(5,ncol(zdat)) times). Once the procedure terminates, you can restart
search with default tolerances using those bandwidths obtained from
the less rigorous search (i.e., set <code>bws=bw</code> on subsequent calls
to this routine where <code>bw</code> is the initial bandwidth object).  A
version of this package using the <code>Rmpi</code> wrapper is under
development that allows one to deploy this software in a clustered
computing environment to facilitate computation involving large
datasets.
</p>
<p>Support for backfitted bandwidths is experimental and is limited in
functionality. The code does not support asymptotic standard errors
or out of sample estimates with backfitting.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine
<a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Aitchison, J. and C.G.G. Aitken (1976), &ldquo;Multivariate binary
discrimination by the kernel method,&rdquo; Biometrika, 63, 413-420.
</p>
<p>Cai Z. (2007), &ldquo;Trending time-varying coefficient time series
models with serially correlated errors,&rdquo; Journal of Econometrics,
136, 163-188.
</p>
<p>Hastie, T. and R. Tibshirani (1993), &ldquo;Varying-coefficient
models,&rdquo; Journal of the Royal Statistical Society, B 55, 757-796.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics: Theory
and Practice,</em> Princeton University Press.
</p>
<p>Li, Q. and J.S. Racine (2010), &ldquo;Smooth varying-coefficient
estimation and inference for qualitative and quantitative data,&rdquo;
Econometric Theory, 26, 1-31.
</p>
<p>Pagan, A. and A. Ullah (1999), <em>Nonparametric Econometrics,</em>
Cambridge University Press.
</p>
<p>Li, Q. and D. Ouyang and J.S. Racine (2013), &ldquo;Categorical
semiparametric varying-coefficient models,&rdquo; Journal of Applied
Econometrics, 28, 551-589.
</p>
<p>Wang, M.C. and J. van Ryzin (1981), &ldquo;A class of smooth estimators
for discrete distributions,&rdquo;  Biometrika, 68, 301-309.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npregbw">npregbw</a></code>, <code><a href="#topic+npreg">npreg</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# EXAMPLE 1 (INTERFACE=FORMULA):
set.seed(42)

n &lt;- 100
x &lt;- runif(n)
z &lt;- runif(n, min=-2, max=2)
y &lt;- x*exp(z)*(1.0+rnorm(n,sd = 0.2))
bw &lt;- npscoefbw(formula=y~x|z)
summary(bw)

# EXAMPLE 1 (INTERFACE=DATA FRAME): 

n &lt;- 100
x &lt;- runif(n)
z &lt;- runif(n, min=-2, max=2)
y &lt;- x*exp(z)*(1.0+rnorm(n,sd = 0.2))
bw &lt;- npscoefbw(xdat=x, ydat=y, zdat=z)
summary(bw)

## End(Not run) 
</code></pre>

<hr>
<h2 id='npsdeptest'> Kernel Consistent Serial Dependence Test for Univariate Nonlinear Processes </h2><span id='topic+npsdeptest'></span>

<h3>Description</h3>

<p><code>npsdeptest</code> implements the consistent metric entropy test of
nonlinear serial dependence as described in Granger, Maasoumi and
Racine (2004).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
npsdeptest(data = NULL,
           lag.num = 1,
           method = c("integration","summation"),
           bootstrap = TRUE,
           boot.num = 399,
           random.seed = 42)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npsdeptest_+3A_data">data</code></td>
<td>

<p>a vector containing the variable that can be of type
<code><a href="base.html#topic+numeric">numeric</a></code> or <code><a href="stats.html#topic+ts">ts</a></code>.
</p>
</td></tr>
<tr><td><code id="npsdeptest_+3A_lag.num">lag.num</code></td>
<td>

<p>an integer value specifying the maximum number of lags to
use. Defaults to <code>1</code>.
</p>
</td></tr>
<tr><td><code id="npsdeptest_+3A_method">method</code></td>
<td>

<p>a character string used to specify whether to compute the integral
version or the summation version of the statistic. Can be set as
<code>integration</code> or <code>summation</code> (see below for
details). Defaults to <code>integration</code>.
</p>
</td></tr>
<tr><td><code id="npsdeptest_+3A_bootstrap">bootstrap</code></td>
<td>

<p>a logical value which specifies whether to conduct
the bootstrap test or not. If set to <code>FALSE</code>, only the
statistic will be computed. Defaults to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npsdeptest_+3A_boot.num">boot.num</code></td>
<td>

<p>an integer value specifying the number of bootstrap
replications to use. Defaults to <code>399</code>.
</p>
</td></tr>
<tr><td><code id="npsdeptest_+3A_random.seed">random.seed</code></td>
<td>

<p>an integer used to seed R's random number generator. This is to
ensure replicability. Defaults to 42.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>npsdeptest</code> computes the nonparametric metric entropy
(normalized Hellinger of Granger, Maasoumi and Racine (2004)) for
testing for nonlinear serial dependence, <code class="reqn">D[f(y_t, \hat y_{t-k}),
   f(y_t)\times f(\hat y_{t-k})]</code>.  Default bandwidths are of the Kullback-Leibler
variety obtained via likelihood cross-validation.
</p>
<p>The test may be applied to a raw data series or to residuals of user
estimated models.
</p>
<p>The summation version of this statistic may be numerically unstable
when <code>data</code> is sparse (the summation version involves division of
densities while the integration version involves differences). Warning
messages are produced should this occur (&lsquo;integration recommended&rsquo;)
and should be heeded.
</p>


<h3>Value</h3>

<p><code>npsdeptest</code> returns an object of type <code>deptest</code> with the
following components
</p>
<table>
<tr><td><code>Srho</code></td>
<td>
<p> the statistic vector <code>Srho</code> </p>
</td></tr>
<tr><td><code>Srho.cumulant</code></td>
<td>
<p> the cumulant statistic vector <code>Srho.cumulant</code> </p>
</td></tr>            
<tr><td><code>Srho.bootstrap.mat</code></td>
<td>
<p> contains the bootstrap replications of
<code>Srho</code> </p>
</td></tr>
<tr><td><code>Srho.cumulant.bootstrap.mat</code></td>
<td>
<p> contains the bootstrap
replications of <code>Srho.cumulant</code> </p>
</td></tr>
<tr><td><code>P</code></td>
<td>
<p> the P-value vector of the Srho statistic vector </p>
</td></tr>
<tr><td><code>P.cumulant</code></td>
<td>
<p> the P-value vector of the cumulant Srho statistic vector </p>
</td></tr>  
<tr><td><code>bootstrap</code></td>
<td>
<p> a logical value indicating whether bootstrapping was
performed </p>
</td></tr>
<tr><td><code>boot.num</code></td>
<td>
<p> number of bootstrap replications </p>
</td></tr>
<tr><td><code>lag.num</code></td>
<td>
<p> the number of lags </p>
</td></tr>
<tr><td><code>bw.y</code></td>
<td>
<p> the numeric vector of bandwidths for <code>data</code>
marginal density at lag <code>num.lag</code></p>
</td></tr>
<tr><td><code>bw.y.lag</code></td>
<td>
<p> the numeric vector of bandwidths for lagged
<code>data</code> marginal density  at lag <code>num.lag</code></p>
</td></tr>  
<tr><td><code>bw.joint</code></td>
<td>
<p> the numeric matrix of bandwidths for <code>data</code>
and lagged <code>data</code> joint density  at lag <code>num.lag</code></p>
</td></tr>      
</table>
<p><code><a href="base.html#topic+summary">summary</a></code> supports object of type <code>deptest</code>.
</p>


<h3>Usage Issues</h3>

<p>The <code>integration</code> version of the statistic uses multidimensional
numerical methods from the <code><a href="cubature.html#topic+cubature">cubature</a></code> package. See
<code><a href="cubature.html#topic+adaptIntegrate">adaptIntegrate</a></code> for details. The <code>integration</code>
version of the statistic will be substantially slower than the
<code>summation</code> version, however, it will likely be both more
accurate and powerful.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine
<a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Granger, C.W. and E. Maasoumi and J.S. Racine (2004), &ldquo;A
dependence metric for possibly nonlinear processes&rdquo;, Journal of Time
Series Analysis, 25, 649-669.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npdeptest">npdeptest</a>,<a href="#topic+npdeneqtest">npdeneqtest</a>,<a href="#topic+npsymtest">npsymtest</a>,<a href="#topic+npunitest">npunitest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
set.seed(1234)

## A function to create a time series

ar.series &lt;- function(phi,epsilon) {
  n &lt;- length(epsilon)
  series &lt;- numeric(n)
  series[1] &lt;- epsilon[1]/(1-phi)
  for(i in 2:n) {
    series[i] &lt;- phi*series[i-1] + epsilon[i]
  }
  return(series)
}

n &lt;- 100

## Stationary persistent time-series

yt &lt;- ar.series(0.95,rnorm(n))
npsdeptest(yt,lag.num=2,boot.num=99,method="summation")

Sys.sleep(5)

## Stationary independent time-series

yt &lt;- ar.series(0.0,rnorm(n))
npsdeptest(yt,lag.num=2,boot.num=99,method="summation")

## Stationary persistent time-series

yt &lt;- ar.series(0.95,rnorm(n))
npsdeptest(yt,lag.num=2,boot.num=99,method="integration")

Sys.sleep(5)

## Stationary independent time-series

yt &lt;- ar.series(0.0,rnorm(n))
npsdeptest(yt,lag.num=2,boot.num=99,method="integration")


## End(Not run) 

</code></pre>

<hr>
<h2 id='npseed'>Set Random Seed</h2><span id='topic+npseed'></span>

<h3>Description</h3>

<p><code>npseed</code> is a function which sets the random seed in the
<code><a href="#topic+np">np</a></code> C backend, resetting the random number generator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npseed(seed)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npseed_+3A_seed">seed</code></td>
<td>
<p>an integer seed for the random number generator.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>npseed</code> provides an interface for setting the random seed (and
resetting the random number generator) used
by <code><a href="#topic+np">np</a></code>. The random number generator is used during the
bandwidth search procedure to set the search starting point, and in
subsequent searches when using multistarting, to avoid being trapped
in local minima if the objective function is not globally concave.
</p>
<p>Calling <code>npseed</code> will only affect the numerical search if it is
performed by the C backend. The affected functions include:
<code><a href="#topic+npudensbw">npudensbw</a></code>, <code><a href="#topic+npcdensbw">npcdensbw</a></code>,
<code><a href="#topic+npregbw">npregbw</a></code>, <code><a href="#topic+npplregbw">npplregbw</a></code>, <code><a href="#topic+npqreg">npqreg</a></code>,
<code><a href="#topic+npcmstest">npcmstest</a></code> (via <code><a href="#topic+npregbw">npregbw</a></code>),
<code><a href="#topic+npqcmstest">npqcmstest</a></code> (via <code><a href="#topic+npregbw">npregbw</a></code>),
<code><a href="#topic+npsigtest">npsigtest</a></code> (via <code><a href="#topic+npregbw">npregbw</a></code>).
</p>


<h3>Value</h3>

<p>None.
</p>


<h3>Note</h3>

<p>This method currently only supports objects from the <code><a href="#topic+np">np</a></code> library.
</p>


<h3>Author(s)</h3>

<p> Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey
S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics:
Theory and Practice,</em> Princeton University Press.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+set.seed">set.seed</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>npseed(712)
x &lt;- runif(10)
y &lt;- x + rnorm(10, sd = 0.1)
bw &lt;- npregbw(y~x)
</code></pre>

<hr>
<h2 id='npsigtest'>Kernel Regression Significance Test with Mixed Data Types</h2><span id='topic+npsigtest'></span><span id='topic+npsigtest.call'></span><span id='topic+npsigtest.default'></span><span id='topic+npsigtest.formula'></span><span id='topic+npsigtest.rbandwidth'></span><span id='topic+npsigtest.npregression'></span>

<h3>Description</h3>

<p><code>npsigtest</code> implements a consistent test of significance of an
explanatory variable(s) in a nonparametric regression setting that is
analogous to a simple <code class="reqn">t</code>-test (<code class="reqn">F</code>-test) in a parametric
regression setting. The test is based on Racine, Hart, and Li (2006)
and Racine (1997).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npsigtest(bws, ...)

## S3 method for class 'formula'
npsigtest(bws, data = NULL, ...)

## S3 method for class 'call'
npsigtest(bws, ...)

## S3 method for class 'npregression'
npsigtest(bws, ...)

## Default S3 method:
npsigtest(bws, xdat, ydat, ...)

## S3 method for class 'rbandwidth'
npsigtest(bws,
          xdat = stop("data xdat missing"),
          ydat = stop("data ydat missing"),
          boot.num = 399,
          boot.method = c("iid","wild","wild-rademacher","pairwise"),
          boot.type = c("I","II"),
          pivot=TRUE,
          joint=FALSE,
          index = seq(1,ncol(xdat)),
          random.seed = 42,
          ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npsigtest_+3A_bws">bws</code></td>
<td>

<p>a bandwidth specification. This can be set as a <code>rbandwidth</code>
object returned from a previous invocation, or as a vector of
bandwidths, with each element <code class="reqn">i</code> corresponding to the bandwidth
for column <code class="reqn">i</code> in <code>xdat</code>. In either case, the bandwidth
supplied will serve as a starting point in the numerical search for
optimal bandwidths when using <code>boot.type="II"</code>. If specified
as a vector, then additional arguments will need to be supplied as
necessary to specify the bandwidth type, kernel types, selection
methods, and so on.
</p>
</td></tr>
<tr><td><code id="npsigtest_+3A_data">data</code></td>
<td>

<p>an optional data frame, list or environment (or object coercible to
a data frame by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code>) containing the
variables in the model. If not found in data, the variables are
taken from <code>environment(bws)</code>, typically the environment from
which <code><a href="#topic+npregbw">npregbw</a></code> was called.
</p>
</td></tr>
<tr><td><code id="npsigtest_+3A_xdat">xdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of explanatory data (training data)
used to calculate the regression estimators.
</p>
</td></tr>
<tr><td><code id="npsigtest_+3A_ydat">ydat</code></td>
<td>

<p>a one (1) dimensional numeric or integer vector of dependent data,
each element <code class="reqn">i</code> corresponding to each observation (row) <code class="reqn">i</code>
of <code>xdat</code>.
</p>
</td></tr>
<tr><td><code id="npsigtest_+3A_boot.method">boot.method</code></td>
<td>

<p>a character string used to specify the bootstrap method for
determining the null distribution. <code>pairwise</code> resamples
pairwise, while the remaining methods use a residual bootstrap
procedure.  <code>iid</code> will generate independent identically
distributed draws. <code>wild</code> will use a wild
bootstrap. <code>wild-rademacher</code> will use a wild bootstrap with
Rademacher variables. Defaults to <code>iid</code>.
</p>
</td></tr>
<tr><td><code id="npsigtest_+3A_boot.num">boot.num</code></td>
<td>

<p>an integer value specifying the number of bootstrap replications to
use. Defaults to <code>399</code>.
</p>
</td></tr>
<tr><td><code id="npsigtest_+3A_boot.type">boot.type</code></td>
<td>

<p>a character string specifying whether to use a &lsquo;Bootstrap I&rsquo; or
&lsquo;Bootstrap II&rsquo; method (see Racine, Hart, and Li (2006) for
details). The &lsquo;Bootstrap II&rsquo; method re-runs cross-validation for
each bootstrap replication and uses the new cross-validated
bandwidth for variable <code class="reqn">i</code> and the original ones for the
remaining variables. Defaults to <code>boot.type="I"</code>.
</p>
</td></tr>
<tr><td><code id="npsigtest_+3A_pivot">pivot</code></td>
<td>

<p>a logical value which specifies whether to bootstrap a pivotal
statistic or not (pivoting is achieved by dividing gradient
estimates by their asymptotic standard errors). Defaults to
<code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npsigtest_+3A_joint">joint</code></td>
<td>

<p>a logical value which specifies whether to conduct a joint test or
individual test. This is to be used in conjunction with <code>index</code>
where <code>index</code> contains two or more integers corresponding to
the variables being tested, where the integers correspond to the
variables in the order in which they appear among the set of
explanatory variables in the function call to
<code>npreg</code>/<code>npregbw</code>. Defaults to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="npsigtest_+3A_index">index</code></td>
<td>

<p>a vector of indices for the columns of <code>xdat</code> for which the
test of significance is to be conducted. Defaults to
(1,2,...,<code class="reqn">p</code>) where <code class="reqn">p</code> is the number of columns in
<code>xdat</code>.
</p>
</td></tr>
<tr><td><code id="npsigtest_+3A_random.seed">random.seed</code></td>
<td>

<p>an integer used to seed R's random number generator. This is to
ensure replicability. Defaults to 42.
</p>
</td></tr>
<tr><td><code id="npsigtest_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to specify the bandwidth type,
kernel types, selection methods, and so on, detailed below.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>npsigtest</code> implements a variety of methods for computing the
null distribution of the test statistic and allows the user to
investigate the impact of a variety of default settings including
whether or not to pivot the statistic (<code>pivot</code>), whether pairwise
or residual resampling is to be used (<code>boot.method</code>), and whether
or not to recompute the bandwidths for the variables being tested
(<code>boot.type</code>), among others.
</p>
<p>Defaults are chosen so as to provide reasonable behaviour in a broad
range of settings and this involves a trade-off between computational
expense and finite-sample performance. However, the default
<code>boot.type="I"</code>, though computationally expedient, can deliver a
test that can be slightly over-sized in small sample settings (e.g.
at the 5% level the test might reject 8% of the time for samples of
size <code class="reqn">n=100</code> for some data generating processes).  If the default
setting (<code>boot.type="I"</code>) delivers a P-value that is in the
neighborhood (i.e. slightly smaller) of any classical level
(e.g. 0.05) and you only have a modest amount of data, it might be
prudent to re-run the test using the more computationally intensive
<code>boot.type="II"</code> setting to confirm the original result. Note
also that <code>boot.method="pairwise"</code> is not recommended for the
multivariate local linear estimator due to substantial size
distortions that may arise in certain cases.
</p>


<h3>Value</h3>

<p><code>npsigtest</code> returns an object of type
<code>sigtest</code>. <code><a href="base.html#topic+summary">summary</a></code> supports <code>sigtest</code> objects. It
has the
following components:
</p>
<table>
<tr><td><code>In</code></td>
<td>
<p> the vector of statistics <code>In</code> </p>
</td></tr>
<tr><td><code>P</code></td>
<td>
<p> the vector of P-values for each statistic in <code>In</code></p>
</td></tr>
<tr><td><code>In.bootstrap</code></td>
<td>
<p> contains a matrix of the bootstrap
replications of the vector <code>In</code>, each column corresponding to
replications associated with explanatory variables in <code>xdat</code>
indexed by <code>index</code> (e.g., if you selected <code>index = c(1,4)</code>
then In.bootstrap will have two columns, the first being the
bootstrap replications of <code>In</code> associated with variable
<code>1</code>, the second with variable <code>4</code>).</p>
</td></tr>
</table>


<h3>Usage Issues</h3>

<p>If you are using data of mixed types, then it is advisable to use the
<code><a href="base.html#topic+data.frame">data.frame</a></code> function to construct your input data and not
<code><a href="base.html#topic+cbind">cbind</a></code>, since <code><a href="base.html#topic+cbind">cbind</a></code> will typically not work
as intended on mixed data types and will coerce the data to the same
type.
</p>
<p>Caution: bootstrap methods are, by their nature, <em>computationally
intensive</em>. This can be frustrating for users possessing large
datasets. For exploratory purposes, you may wish to override the
default number of bootstrap replications, say, setting them to
<code>boot.num=99</code>. A version of this package using the <code>Rmpi</code>
wrapper is under development that allows one to deploy this software
in a clustered computing environment to facilitate computation
involving large datasets.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine
<a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Aitchison, J. and C.G.G. Aitken (1976), &ldquo;Multivariate binary
discrimination by the kernel method,&rdquo; Biometrika, 63, 413-420.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics:
Theory and Practice,</em> Princeton University Press.
</p>
<p>Racine, J.S., J. Hart, and Q. Li (2006), &ldquo;Testing the
significance of categorical predictor variables in nonparametric
regression models,&rdquo; Econometric Reviews, 25, 523-544.
</p>
<p>Racine, J.S. (1997), &ldquo;Consistent significance testing for
nonparametric regression,&rdquo; Journal of Business and Economic
Statistics 15, 369-379.
</p>
<p>Wang, M.C. and J. van Ryzin (1981), &ldquo;A class of smooth estimators
for discrete distributions,&rdquo;  Biometrika, 68, 301-309.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# EXAMPLE 1 (INTERFACE=FORMULA): For this example, we simulate 100 draws
# from a DGP in which z, the first column of X, is an irrelevant
# discrete variable

set.seed(12345)

n &lt;- 100

z &lt;- rbinom(n,1,.5)
x1 &lt;- rnorm(n)
x2 &lt;- runif(n,-2,2)

y &lt;- x1 + x2 + rnorm(n)

# Next, we must compute bandwidths for our regression model. In this
# case we conduct local linear regression. Note - this may take a few
# minutes depending on the speed of your computer...

bw &lt;- npregbw(formula=y~factor(z)+x1+x2,regtype="ll",bwmethod="cv.aic")

# We then compute a vector of tests corresponding to the columns of
# X. Note - this may take a few minutes depending on the speed of your
# computer... we have to generate the null distribution of the statistic
# for each variable whose significance is being tested using 399
# bootstrap replications for each...

npsigtest(bws=bw)

# If you wished, you could conduct the test for, say, variables 1 and 3
# only, as in

npsigtest(bws=bw,index=c(1,3))

# EXAMPLE 1 (INTERFACE=DATA FRAME): For this example, we simulate 100
# draws from a DGP in which z, the first column of X, is an irrelevant
# discrete variable

set.seed(12345)

n &lt;- 100

z &lt;- rbinom(n,1,.5)
x1 &lt;- rnorm(n)
x2 &lt;- runif(n,-2,2)

X &lt;- data.frame(factor(z),x1,x2)

y &lt;- x1 + x2 + rnorm(n)

# Next, we must compute bandwidths for our regression model. In this
# case we conduct local linear regression. Note - this may take a few
# minutes depending on the speed of your computer...

bw &lt;- npregbw(xdat=X,ydat=y,regtype="ll",bwmethod="cv.aic")

# We then compute a vector of tests corresponding to the columns of
# X. Note - this may take a few minutes depending on the speed of your
# computer... we have to generate the null distribution of the statistic
# for each variable whose significance is being tested using 399
# bootstrap replications for each...

npsigtest(bws=bw)

# If you wished, you could conduct the test for, say, variables 1 and 3
# only, as in

npsigtest(bws=bw,index=c(1,3))

## End(Not run) 
</code></pre>

<hr>
<h2 id='npsymtest'> Kernel Consistent Density Asymmetry Test with Mixed Data Types </h2><span id='topic+npsymtest'></span>

<h3>Description</h3>

<p><code>npsymtest</code> implements the consistent metric entropy test of
asymmetry as described in Maasoumi and Racine (2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npsymtest(data = NULL,
          method = c("integration","summation"),
          boot.num = 399,
          bw = NULL,
          boot.method = c("iid", "geom"),
          random.seed = 42,
          ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npsymtest_+3A_data">data</code></td>
<td>

<p>a vector containing the variable.
</p>
</td></tr>
<tr><td><code id="npsymtest_+3A_method">method</code></td>
<td>

<p>a character string used to specify whether to compute the integral
version or the summation version of the statistic. Can be set as
<code>integration</code> or <code>summation</code> (see below for
details). Defaults to <code>integration</code>.
</p>
</td></tr>
<tr><td><code id="npsymtest_+3A_boot.num">boot.num</code></td>
<td>

<p>an integer value specifying the number of bootstrap
replications to use. Defaults to <code>399</code>.
</p>
</td></tr>
<tr><td><code id="npsymtest_+3A_bw">bw</code></td>
<td>

<p>a numeric (scalar) bandwidth. Defaults to plug-in (see details below).
</p>
</td></tr>
<tr><td><code id="npsymtest_+3A_boot.method">boot.method</code></td>
<td>
<p> a character string used to specify the
bootstrap method. Can be set as <code>iid</code> or <code>geom</code> (see below
for details). Defaults to <code>iid</code>.
</p>
</td></tr>
<tr><td><code id="npsymtest_+3A_random.seed">random.seed</code></td>
<td>

<p>an integer used to seed R's random number generator. This is to
ensure replicability. Defaults to 42.
</p>
</td></tr>
<tr><td><code id="npsymtest_+3A_...">...</code></td>
<td>
<p> additional arguments supplied to specify the bandwidth
type, kernel types, and so on.  This is used since we specify bw as
a numeric scalar and not a <code>bandwidth</code> object, and is of
interest if you do not desire the default behaviours. To change the
defaults, you may specify any of <code>bwscaling</code>, <code>bwtype</code>,
<code>ckertype</code>, <code>ckerorder</code>, <code>ukertype</code>,
<code>okertype</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>npsymtest</code> computes the nonparametric metric entropy (normalized
Hellinger of Granger, Maasoumi and Racine (2004)) for testing
symmetry using the densities/probabilities of the data and the
rotated data, <code class="reqn">D[f(y), f(\tilde y)]</code>. See
Maasoumi and Racine (2009) for details. Default bandwidths are of the
plug-in variety (<code><a href="stats.html#topic+bw.SJ">bw.SJ</a></code> for continuous variables and
direct plug-in for discrete variables).
</p>
<p>For bootstrapping the null distribution of the statistic, <code>iid</code>
conducts simple random resampling, while <code>geom</code> conducts Politis
and Romano's (1994) stationary bootstrap using automatic block length
selection via the <code><a href="#topic+b.star">b.star</a></code> function in the
<code><a href="#topic+np">np</a></code> package. See the <code><a href="boot.html#topic+boot">boot</a></code> package for
details.
</p>
<p>The summation version of this statistic may be numerically unstable
when <code>y</code> is sparse (the summation version involves division of
densities while the integration version involves differences). Warning
messages are produced should this occur (&lsquo;integration recommended&rsquo;)
and should be heeded.
</p>


<h3>Value</h3>

<p><code>npsymtest</code> returns an object of type <code>symtest</code> with the
following components
</p>
<table>
<tr><td><code>Srho</code></td>
<td>
<p> the statistic <code>Srho</code> </p>
</td></tr>
<tr><td><code>Srho.bootstrap</code></td>
<td>
<p> contains the bootstrap replications of <code>Srho</code> </p>
</td></tr>
<tr><td><code>P</code></td>
<td>
<p> the P-value of the statistic </p>
</td></tr>
<tr><td><code>boot.num</code></td>
<td>
<p> number of bootstrap replications </p>
</td></tr>
<tr><td><code>data.rotate</code></td>
<td>
<p> the rotated data series </p>
</td></tr>
<tr><td><code>bw</code></td>
<td>
<p> the numeric (scalar) bandwidth </p>
</td></tr>
</table>
<p><code><a href="base.html#topic+summary">summary</a></code> supports object of type <code>symtest</code>.
</p>


<h3>Usage Issues</h3>

<p>When using data of type <code><a href="base.html#topic+factor">factor</a></code> it is crucial that the
variable not be an alphabetic character string (i.e. the factor must
be integer-valued). The rotation is conducted about the median after
conversion to type <code><a href="base.html#topic+numeric">numeric</a></code> which is then converted back
to type <code><a href="base.html#topic+factor">factor</a></code>. Failure to do so will have unpredictable
results. See the example below for proper usage.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine
<a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Granger, C.W. and E. Maasoumi and J.S. Racine (2004), &ldquo;A
dependence metric for possibly nonlinear processes&rdquo;, Journal of Time
Series Analysis, 25, 649-669.
</p>
<p>Maasoumi, E. and J.S. Racine (2009), &ldquo;A robust entropy-based
test of asymmetry for discrete and continuous processes,&rdquo; Econometric
Reviews, 28, 246-261.
</p>
<p>Politis, D.N. and J.P. Romano (1994), &ldquo;The stationary
bootstrap,&rdquo; Journal of the American Statistical Association, 89,
1303-1313.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npdeneqtest">npdeneqtest</a>,<a href="#topic+npdeptest">npdeptest</a>,<a href="#topic+npsdeptest">npsdeptest</a>,<a href="#topic+npunitest">npunitest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
set.seed(1234)

n &lt;- 100

## Asymmetric discrete probability distribution function

x &lt;- factor(rbinom(n,2,.8))
npsymtest(x,boot.num=99)

Sys.sleep(5)

## Symmetric discrete probability distribution function

x &lt;- factor(rbinom(n,2,.5))
npsymtest(x,boot.num=99)

Sys.sleep(5)

## Asymmetric continuous distribution function

y &lt;- rchisq(n,df=2)
npsymtest(y,boot.num=99)

Sys.sleep(5)

## Symmetric continuous distribution function

y &lt;- rnorm(n)
npsymtest(y,boot.num=99)

## Time-series bootstrap

ar.series &lt;- function(phi,epsilon) {
  n &lt;- length(epsilon)
  series &lt;- numeric(n)
  series[1] &lt;- epsilon[1]/(1-phi)
  for(i in 2:n) {
    series[i] &lt;- phi*series[i-1] + epsilon[i]
  }
  return(series)
}

## Asymmetric time-series

yt &lt;- ar.series(0.5,rchisq(n,df=3))
npsymtest(yt,boot.num=99,boot.method="geom")

## Symmetric time-series

yt &lt;- ar.series(0.5,rnorm(n))
npsymtest(yt,boot.num=99,boot.method="geom")


## End(Not run) 

</code></pre>

<hr>
<h2 id='nptgauss'>Truncated Second-order Gaussian Kernels</h2><span id='topic+nptgauss'></span>

<h3>Description</h3>

<p><code>nptgauss</code> provides an interface for setting the truncation
radius of the truncated second-order Gaussian kernel used
by <span class="pkg">np</span>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nptgauss(b)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nptgauss_+3A_b">b</code></td>
<td>

<p>Truncation radius of the kernel.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>nptgauss</code> allows one to set the truncation radius of the truncated Gaussian kernel used by <span class="pkg">np</span>, which defaults to 3. It automatically computes the constants describing the truncated gaussian kernel for the user.
</p>
<p>We define the truncated gaussion kernel on the interval <code class="reqn">[-b,b]</code> as:
</p>
<p style="text-align: center;"><code class="reqn">K = \frac{\alpha}{\sqrt{2\pi}}\left(e^{-z^2/2} - e^{-b^2/2}\right)</code>
</p>

<p>The constant <code class="reqn">\alpha</code> is computed as:
</p>
<p style="text-align: center;"><code class="reqn">\alpha = \left[\int_{-b}^{b} \frac{1}{\sqrt{2\pi}}\left(e^{-z^2/2} - e^{-b^2/2}\right)\right]^{-1}</code>
</p>

<p>Given these definitions, the derivative kernel is simply:
</p>
<p style="text-align: center;"><code class="reqn">K' = (-z)\frac{\alpha}{\sqrt{2\pi}}e^{-z^2/2}</code>
</p>

<p>The CDF kernel is:
</p>
<p style="text-align: center;"><code class="reqn">G = \frac{\alpha}{2}\mathrm{erf}(z/\sqrt{2}) + \frac{1}{2} - c_0z</code>
</p>

<p>The convolution kernel on <code class="reqn">[-2b,0]</code> has the general form:
</p>
<p style="text-align: center;"><code class="reqn">H_- = a_0\,\mathrm{erf}(z/2 + b) e^{-z^2/4} + a_1z + a_2\,\mathrm{erf}((z+b)/\sqrt{2}) - c_0</code>
</p>

<p>and on <code class="reqn">[0,2b]</code> it is:
</p>
<p style="text-align: center;"><code class="reqn">H_+ = -a_0\,\mathrm{erf}(z/2 - b) e^{-z^2/4} - a_1z - a_2\,\mathrm{erf}((z-b)/\sqrt{2}) - c_0</code>
</p>

<p>where <code class="reqn">a_0</code> is determined by the normalisation condition on H,
<code class="reqn">a_2</code> is determined by considering the value of the kernel at
<code class="reqn">z = 0</code> and <code class="reqn">a_1</code> is determined by the requirement that <code class="reqn">H = 0</code> at <code class="reqn">[-2b,2b]</code>.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine
<a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## The default kernel, a gaussian truncated at +- 3
nptgauss(b = 3.0)

</code></pre>

<hr>
<h2 id='npudens'> Kernel Density Estimation with Mixed Data Types </h2><span id='topic+npudens'></span><span id='topic+npudens.bandwidth'></span><span id='topic+npudens.call'></span><span id='topic+npudens.default'></span><span id='topic+npudens.formula'></span>

<h3>Description</h3>

<p><code>npudens</code> computes  kernel unconditional density estimates on
evaluation data, given a set of training data and a bandwidth
specification (a <code>bandwidth</code> object or a bandwidth vector,
bandwidth type, and kernel type) using the method of Li and Racine
(2003).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npudens(bws, ...)

## S3 method for class 'formula'
npudens(bws, data = NULL, newdata = NULL, ...)

## S3 method for class 'bandwidth'
npudens(bws,
        tdat = stop("invoked without training data 'tdat'"),
        edat,
        ...)

## S3 method for class 'call'
npudens(bws, ...)

## Default S3 method:
npudens(bws, tdat, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npudens_+3A_bws">bws</code></td>
<td>

<p>a bandwidth specification. This can be set as a <code>bandwidth</code>
object returned from an invocation of <code><a href="#topic+npudensbw">npudensbw</a></code>, or as a
<code class="reqn">p</code>-vector of bandwidths, with an element for each variable in the
training data. If specified as a vector, then additional arguments
will need to be supplied as necessary to change them from the
defaults to specify the bandwidth type, kernel types, training data,
and so on.
</p>
</td></tr>
<tr><td><code id="npudens_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to specify, the training
data, the
bandwidth type, kernel types, and so on.
This is necessary if you specify bws as a <code class="reqn">p</code>-vector and not
a <code>bandwidth</code> object, and you do not desire the default
behaviours. To do this, you may specify any of <code>bwscaling</code>,
<code>bwtype</code>, <code>ckertype</code>, <code>ckerorder</code>, <code>ukertype</code>,
<code>okertype</code>, as described in <code><a href="#topic+npudensbw">npudensbw</a></code>.
</p>
</td></tr>
<tr><td><code id="npudens_+3A_tdat">tdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of sample realizations (training data)
used to estimate the density. Defaults to the training data used to
compute the bandwidth object.
</p>
</td></tr>
<tr><td><code id="npudens_+3A_edat">edat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of density evaluation points. By default,
evaluation takes place on the data provided by <code>tdat</code>.
</p>
</td></tr>
<tr><td><code id="npudens_+3A_data">data</code></td>
<td>

<p>an optional data frame, list or environment (or object
coercible to a data frame by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code>) containing the variables
in the model. If not found in data, the variables are taken from
<code>environment(bws)</code>, typically the environment from which
<code><a href="#topic+npudensbw">npudensbw</a></code> was called.
</p>
</td></tr>
<tr><td><code id="npudens_+3A_newdata">newdata</code></td>
<td>

<p>An optional data frame in which to look for evaluation data. If
omitted, the training data are used.  
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Typical usages are (see below for a complete list of options and also
the examples at the end of this help file)
</p>
<pre>
    Usage 1: first compute the bandwidth object via npudensbw and then
    compute the density:
    
    bw &lt;- npudensbw(~y)
    fhat &lt;- npudens(bw)
    
    Usage 2: alternatively, compute the bandwidth object indirectly:
    
    fhat &lt;- npudens(~y)
    
    Usage 3: modify the default kernel and order:
    
    fhat &lt;- npudens(~y, ckertype="epanechnikov", ckerorder=4)

    Usage 4: use the data frame interface rather than the formula
    interface:

    fhat &lt;- npudens(tdat = y, ckertype="epanechnikov", ckerorder=4)
  </pre>
<p><code>npudens</code> implements a variety of methods for estimating
multivariate density functions (<code class="reqn">p</code>-variate) defined over a set of
possibly continuous and/or discrete (unordered, ordered) data. The
approach is based on Li and Racine (2003) who employ
&lsquo;generalized product kernels&rsquo; that admit a mix of continuous
and discrete data types.
</p>
<p>Three classes of kernel estimators for the continuous data types are
available: fixed, adaptive nearest-neighbor, and generalized
nearest-neighbor. Adaptive nearest-neighbor bandwidths change with
each sample realization in the set, <code class="reqn">x_i</code>, when estimating
the density at the point <code class="reqn">x</code>. Generalized nearest-neighbor
bandwidths change with the point at which the density is estimated,
<code class="reqn">x</code>. Fixed bandwidths are constant over the support of <code class="reqn">x</code>.
</p>
<p>Data contained in the data frame <code>tdat</code> (and also <code>edat</code>)
may be a mix of continuous (default), unordered discrete (to be
specified in the data frame <code>tdat</code> using the <code><a href="base.html#topic+factor">factor</a></code>
command), and ordered discrete (to be specified in the data frame
<code>tdat</code> using the <code><a href="base.html#topic+ordered">ordered</a></code> command). Data can be
entered in an arbitrary order and data types will be detected
automatically by the routine (see <code><a href="#topic+np">np</a></code> for details).
</p>
<p>A variety of kernels may be specified by the user. Kernels implemented
for continuous data types include the second, fourth, sixth, and
eighth order Gaussian and Epanechnikov kernels, and the uniform
kernel. Unordered discrete data types use a variation on Aitchison and
Aitken's (1976) kernel, while ordered data types use a variation of
the Wang and van Ryzin (1981) kernel.
</p>


<h3>Value</h3>

<p><code>npudens</code> returns a <code>npdensity</code> object. The generic accessor
functions <code><a href="stats.html#topic+fitted">fitted</a></code>, and <code><a href="#topic+se">se</a></code>, extract
estimated values and asymptotic standard errors on estimates,
respectively, from the returned object. Furthermore, the functions
<code><a href="stats.html#topic+predict">predict</a></code>, <code><a href="base.html#topic+summary">summary</a></code> and <code><a href="base.html#topic+plot">plot</a></code>
support objects of both classes. The returned objects have the
following components:
</p>
<table>
<tr><td><code>eval</code></td>
<td>
<p> the evaluation points. </p>
</td></tr>
<tr><td><code>dens</code></td>
<td>
<p> estimation of the density at the evaluation points </p>
</td></tr>
<tr><td><code>derr</code></td>
<td>
<p> standard errors of the density estimates </p>
</td></tr>
<tr><td><code>log_likelihood</code></td>
<td>
<p> log likelihood of the density estimates </p>
</td></tr>
</table>


<h3>Usage Issues</h3>

<p>If you are using data of mixed types, then it is advisable to use the
<code><a href="base.html#topic+data.frame">data.frame</a></code> function to construct your input data and not
<code><a href="base.html#topic+cbind">cbind</a></code>, since <code><a href="base.html#topic+cbind">cbind</a></code> will typically not work as
intended on mixed data types and will coerce the data to the same
type.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine
<a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Aitchison, J. and C.G.G. Aitken (1976), &ldquo; Multivariate binary
discrimination by the kernel method,&rdquo; Biometrika, 63, 413-420.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics: Theory
and Practice,</em> Princeton University Press.
</p>
<p>Li, Q. and J.S. Racine (2003), &ldquo;Nonparametric estimation of
distributions with categorical and continuous data,&rdquo; Journal
of Multivariate Analysis, 86, 266-292.
</p>
<p>Ouyang, D. and Q. Li and J.S. Racine (2006), &ldquo;Cross-validation
and the estimation of probability distributions with categorical
data,&rdquo; Journal of Nonparametric Statistics, 18, 69-100.
</p>
<p>Pagan, A. and A. Ullah (1999), <em>Nonparametric Econometrics,</em> Cambridge
University Press. 
</p>
<p>Scott, D.W. (1992), <em>Multivariate Density Estimation: Theory,
Practice and Visualization,</em> New York: Wiley.
</p>
<p>Silverman, B.W. (1986), <em>Density Estimation,</em> London: Chapman and
Hall.
</p>
<p>Wang, M.C. and J. van Ryzin (1981), &ldquo;A class of smooth estimators
for discrete distributions,&rdquo;  Biometrika, 68, 301-309.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+npudensbw">npudensbw</a></code> , <code><a href="stats.html#topic+density">density</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# EXAMPLE 1 (INTERFACE=FORMULA): For this example, we load Giovanni
# Baiocchi's Italian GDP panel (see Italy for details), then create a
# data frame in which year is an ordered factor, GDP is continuous,
# compute bandwidths using likelihood cross-validation, then create a
# grid of data on which the density will be evaluated for plotting
# purposes.

data("Italy")
attach(Italy)

# Compute bandwidths using likelihood cross-validation (default).

bw &lt;- npudensbw(formula=~ordered(year)+gdp)

# At this stage you could use npudens() to do a variety of
# things. Here we compute the npudens() object and place it in fhat.

fhat &lt;- npudens(bws=bw)

# Note that simply typing the name of the object returns some useful
# information. For more info, one can call summary: 

summary(fhat)

# Next, we illustrate how to create a grid of `evaluation data' and feed
# it to the perspective plotting routines in R, among others.

# Create an evaluation data matrix

year.seq &lt;- sort(unique(year))
gdp.seq &lt;- seq(1,36,length=50)
data.eval &lt;- expand.grid(year=year.seq,gdp=gdp.seq)

# Generate the estimated density computed for the evaluation data

fhat &lt;- fitted(npudens(bws=bw, newdata=data.eval))

# Coerce the data into a matrix for plotting with persp()

f &lt;- matrix(fhat, length(unique(year)), 50)

# Next, create a 3D perspective plot of the PDF f, and a 2D
# contour plot.

persp(as.integer(levels(year.seq)), gdp.seq, f, col="lightblue",
      ticktype="detailed", ylab="GDP", xlab="Year", zlab="Density",
      theta=300, phi=50)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

contour(as.integer(levels(year.seq)), 
        gdp.seq, 
        f, 
        xlab="Year", 
        ylab="GDP", 
        main = "Density Contour Plot", 
        col=topo.colors(100))

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Alternatively, you could use the plot() command (&lt;ctrl&gt;-C will
# interrupt on *NIX systems, &lt;esc&gt; will interrupt on MS Windows
# systems).

plot(bw)

detach(Italy)

# EXAMPLE 1 (INTERFACE=DATA FRAME): For this example, we load Giovanni
# Baiocchi's Italian GDP panel (see Italy for details), then create a
# data frame in which year is an ordered factor, GDP is continuous,
# compute bandwidths using likelihood cross-validation, then create a
# grid of data on which the density will be evaluated for plotting
# purposes.

data("Italy")
attach(Italy)

data &lt;- data.frame(year=ordered(year), gdp)

# Compute bandwidths using likelihood cross-validation (default).

bw &lt;- npudensbw(dat=data)

# At this stage you could use npudens() to do a variety of
# things. Here we compute the npudens() object and place it in fhat.

fhat &lt;- npudens(bws=bw)

# Note that simply typing the name of the object returns some useful
# information. For more info, one can call summary: 

summary(fhat)

# Next, we illustrate how to create a grid of `evaluation data' and feed
# it to the perspective plotting routines in R, among others.

# Create an evaluation data matrix

year.seq &lt;- sort(unique(year))
gdp.seq &lt;- seq(1,36,length=50)
data.eval &lt;- expand.grid(year=year.seq,gdp=gdp.seq)

# Generate the estimated density computed for the evaluation data

fhat &lt;- fitted(npudens(edat = data.eval, bws=bw))

# Coerce the data into a matrix for plotting with persp()

f &lt;- matrix(fhat, length(unique(year)), 50)

# Next, create a 3D perspective plot of the PDF f, and a 2D
# contour plot.

persp(as.integer(levels(year.seq)), gdp.seq, f, col="lightblue",
      ticktype="detailed", ylab="GDP", xlab="Year", zlab="Density",
      theta=300, phi=50)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

contour(as.integer(levels(year.seq)),
        gdp.seq, 
        f, 
        xlab="Year", 
        ylab="GDP", 
        main = "Density Contour Plot", 
        col=topo.colors(100))

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Alternatively, you could use the plot() command (&lt;ctrl&gt;-C will
# interrupt on *NIX systems, &lt;esc&gt; will interrupt on MS Windows
# systems).

plot(bw)

detach(Italy)

# EXAMPLE 2 (INTERFACE=FORMULA): For this example, we load the old
# faithful geyser data and compute the density and distribution
# functions.

library("datasets")
data("faithful")
attach(faithful)

# Note - this may take a few minutes depending on the speed of your
# computer...

bw &lt;- npudensbw(formula=~eruptions+waiting)

summary(bw)

# Plot the density function (&lt;ctrl&gt;-C will interrupt on *NIX systems, 
# &lt;esc&gt; will interrupt on MS Windows systems). Note that we use xtrim =
# -0.2 to extend the plot outside the support of the data (i.e., extend
# the tails of the estimate to meet the horizontal axis).

plot(bw, xtrim=-0.2)

detach(faithful)

# EXAMPLE 2 (INTERFACE=DATA FRAME): For this example, we load the old
# faithful geyser data and compute the density and distribution
# functions.

library("datasets")
data("faithful")
attach(faithful)

# Note - this may take a few minutes depending on the speed of your
# computer...

bw &lt;- npudensbw(dat=faithful)

summary(bw)

# Plot the density function (&lt;ctrl&gt;-C will interrupt on *NIX systems, 
# &lt;esc&gt; will interrupt on MS Windows systems). Note that we use xtrim =
# -0.2 to extend the plot outside the support of the data (i.e., extend
# the tails of the estimate to meet the horizontal axis).

plot(bw, xtrim=-0.2)

detach(faithful)

## End(Not run) 
</code></pre>

<hr>
<h2 id='npudensbw'>Kernel Density Bandwidth Selection with Mixed Data Types</h2><span id='topic+npudensbw'></span><span id='topic+npudensbw.formula'></span><span id='topic+npudensbw.NULL'></span><span id='topic+npudensbw.default'></span><span id='topic+npudensbw.bandwidth'></span>

<h3>Description</h3>

<p><code>npudensbw</code> computes a bandwidth object for a <code class="reqn">p</code>-variate
kernel unconditional density estimator defined over mixed continuous
and discrete (unordered, ordered) data using either the normal
reference rule-of-thumb, likelihood cross-validation, or least-squares
cross validation using the method of Li and Racine (2003).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
npudensbw(...)

## S3 method for class 'formula'
npudensbw(formula, data, subset, na.action, call, ...)

## S3 method for class 'NULL'
npudensbw(dat = stop("invoked without input data 'dat'"),
          bws,
          ...)

## S3 method for class 'bandwidth'
npudensbw(dat = stop("invoked without input data 'dat'"),
          bws,
          bandwidth.compute = TRUE,
          nmulti,
          remin = TRUE,
          itmax = 10000,
          ftol = 1.490116e-07,
          tol = 1.490116e-04,
          small = 1.490116e-05,
          lbc.dir = 0.5,
          dfc.dir = 3,
          cfac.dir = 2.5*(3.0-sqrt(5)),
          initc.dir = 1.0,
          lbd.dir = 0.1,
          hbd.dir = 1,
          dfac.dir = 0.25*(3.0-sqrt(5)),
          initd.dir = 1.0,
          lbc.init = 0.1,
          hbc.init = 2.0,
          cfac.init = 0.5,
          lbd.init = 0.1,
          hbd.init = 0.9,
          dfac.init = 0.375, 
          scale.init.categorical.sample = FALSE,
          ...)

## Default S3 method:
npudensbw(dat = stop("invoked without input data 'dat'"),
          bws,
          bandwidth.compute = TRUE,
          nmulti,
          remin,
          itmax,
          ftol,
          tol,
          small,
          lbc.dir,
          dfc.dir,
          cfac.dir,
          initc.dir,
          lbd.dir,
          hbd.dir,
          dfac.dir,
          initd.dir,
          lbc.init,
          hbc.init,
          cfac.init,
          lbd.init,
          hbd.init,
          dfac.init,
          scale.init.categorical.sample,
          bwmethod,
          bwscaling,
          bwtype,
          ckertype,
          ckerorder,
          ukertype,
          okertype,
          ...) 

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npudensbw_+3A_formula">formula</code></td>
<td>

<p>a symbolic description of variables on which bandwidth selection is
to be performed. The details of constructing a formula are
described below.
</p>
</td></tr>
<tr><td><code id="npudensbw_+3A_data">data</code></td>
<td>

<p>an optional data frame, list or environment (or object
coercible to a data frame by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code>) containing the variables
in the model. If not found in data, the variables are taken from
<code>environment(formula)</code>, typically the environment from which the
function is called.
</p>
</td></tr>
<tr><td><code id="npudensbw_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations to be used in
the fitting process. 
</p>
</td></tr>
<tr><td><code id="npudensbw_+3A_na.action">na.action</code></td>
<td>

<p>a function which indicates what should happen when the data contain
<code>NA</code>s. The default is set by the <code><a href="stats.html#topic+na.action">na.action</a></code> setting
of options, and is <code><a href="stats.html#topic+na.fail">na.fail</a></code> if that is unset. The
(recommended) default is <code><a href="stats.html#topic+na.omit">na.omit</a></code>.
</p>
</td></tr>
<tr><td><code id="npudensbw_+3A_call">call</code></td>
<td>

<p>the original function call. This is passed internally by
<code><a href="#topic+np">np</a></code> when a bandwidth search has been implied by a call to
another function. It is not recommended that the user set this.
</p>
</td></tr>
<tr><td><code id="npudensbw_+3A_dat">dat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame on which bandwidth selection will
be performed. The data types may be continuous, discrete (unordered
and ordered factors), or some combination thereof.
</p>
</td></tr>
<tr><td><code id="npudensbw_+3A_bws">bws</code></td>
<td>

<p>a bandwidth specification. This can be set as a bandwidth object
returned from a previous invocation, or as a vector of bandwidths,
with each element <code class="reqn">i</code> corresponding to the bandwidth for column
<code class="reqn">i</code> in <code>dat</code>. In either case, the bandwidth supplied will
serve as a starting point in the numerical search for optimal
bandwidths. If specified as a vector, then additional arguments will
need to be supplied as necessary to specify the bandwidth type,
kernel types, selection methods, and so on. This can be left unset.
</p>
</td></tr>
<tr><td><code id="npudensbw_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to specify the bandwidth type,
kernel types, selection methods, and so on, detailed below.
</p>
</td></tr>
<tr><td><code id="npudensbw_+3A_bwmethod">bwmethod</code></td>
<td>
<p> a character string specifying the bandwidth selection
method. <code>cv.ml</code> specifies likelihood cross-validation,
<code>cv.ls</code> specifies least-squares cross-validation, and
<code>normal-reference</code> just computes the &lsquo;rule-of-thumb&rsquo;
bandwidth <code class="reqn">h_j</code> using the standard formula <code class="reqn">h_j = 1.06
    \sigma_j n^{-1/(2P+l)}</code>,
where <code class="reqn">\sigma_j</code> is an adaptive measure of spread of
the <code class="reqn">j</code>th continuous variable defined as min(standard deviation,
mean absolute deviation/1.4826, interquartile range/1.349), <code class="reqn">n</code>
the number of observations, <code class="reqn">P</code> the order of the kernel, and
<code class="reqn">l</code> the number of continuous variables. Note that when there
exist factors and the normal-reference rule is used, there is zero
smoothing of the factors. Defaults to <code>cv.ml</code>.  </p>
</td></tr>
<tr><td><code id="npudensbw_+3A_bwscaling">bwscaling</code></td>
<td>
<p> a logical value that when set to <code>TRUE</code> the
supplied bandwidths are interpreted as &lsquo;scale factors&rsquo;
(<code class="reqn">c_j</code>), otherwise when the value is <code>FALSE</code> they are
interpreted as &lsquo;raw bandwidths&rsquo; (<code class="reqn">h_j</code> for continuous data
types, <code class="reqn">\lambda_j</code> for discrete data types). For
continuous data types, <code class="reqn">c_j</code> and <code class="reqn">h_j</code> are
related by the formula <code class="reqn">h_j = c_j \sigma_j n^{-1/(2P+l)}</code>, where <code class="reqn">\sigma_j</code> is an
adaptive measure of spread of the <code class="reqn">j</code>th continuous variable
defined as min(standard deviation, mean absolute deviation/1.4826,
interquartile range/1.349), <code class="reqn">n</code> the number of observations,
<code class="reqn">P</code> the order of the kernel, and <code class="reqn">l</code> the number of
continuous variables. For discrete data types, <code class="reqn">c_j</code> and
<code class="reqn">h_j</code> are related by the formula <code class="reqn">h_j =
    c_jn^{-2/(2P+l)}</code>, where here
<code class="reqn">j</code> denotes discrete variable <code class="reqn">j</code>.  Defaults to
<code>FALSE</code>.  </p>
</td></tr>
<tr><td><code id="npudensbw_+3A_bwtype">bwtype</code></td>
<td>

<p>character string used for the continuous variable bandwidth type,
specifying the type of bandwidth to compute and return in the
<code>bandwidth</code> object. Defaults to <code>fixed</code>. Option
summary:<br />
<code>fixed</code>: compute fixed bandwidths <br />
<code>generalized_nn</code>: compute generalized nearest neighbors <br />
<code>adaptive_nn</code>: compute adaptive nearest neighbors
</p>
</td></tr>
<tr><td><code id="npudensbw_+3A_bandwidth.compute">bandwidth.compute</code></td>
<td>

<p>a logical value which specifies whether to do a numerical search for
bandwidths or not. If set to <code>FALSE</code>, a <code>bandwidth</code> object
will be returned with bandwidths set to those specified
in <code>bws</code>. Defaults to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npudensbw_+3A_ckertype">ckertype</code></td>
<td>

<p>character string used to specify the continuous kernel type.
Can be set as <code>gaussian</code>, <code>epanechnikov</code>, or
<code>uniform</code>. Defaults to <code>gaussian</code>.
</p>
</td></tr>
<tr><td><code id="npudensbw_+3A_ckerorder">ckerorder</code></td>
<td>

<p>numeric value specifying kernel order (one of
<code>(2,4,6,8)</code>). Kernel order specified along with a
<code>uniform</code> continuous kernel type will be ignored. Defaults to
<code>2</code>.
</p>
</td></tr>
<tr><td><code id="npudensbw_+3A_ukertype">ukertype</code></td>
<td>

<p>character string used to specify the unordered categorical kernel type.
Can be set as <code>aitchisonaitken</code> or <code>liracine</code>.
</p>
</td></tr>
<tr><td><code id="npudensbw_+3A_okertype">okertype</code></td>
<td>

<p>character string used to specify the ordered categorical kernel type.
Can be set as <code>wangvanryzin</code> or <code>liracine</code>.
</p>
</td></tr>
<tr><td><code id="npudensbw_+3A_nmulti">nmulti</code></td>
<td>

<p>integer number of times to restart the process of finding extrema of
the cross-validation function from different (random) initial points.
</p>
</td></tr>
<tr><td><code id="npudensbw_+3A_remin">remin</code></td>
<td>

<p>a logical value which when set as <code>TRUE</code> the search routine
restarts from located minima for a minor gain in accuracy. Defaults
to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npudensbw_+3A_itmax">itmax</code></td>
<td>

<p>integer number of iterations before failure in the numerical
optimization routine. Defaults to <code>10000</code>.
</p>
</td></tr>
<tr><td><code id="npudensbw_+3A_ftol">ftol</code></td>
<td>

<p>fractional tolerance on the value of the cross-validation function
evaluated at located minima (of order the machine precision or
perhaps slightly larger so as not to be diddled by
roundoff). Defaults to <code>1.490116e-07</code>
(1.0e+01*sqrt(.Machine$double.eps)).
</p>
</td></tr>
<tr><td><code id="npudensbw_+3A_tol">tol</code></td>
<td>

<p>tolerance on the position of located minima of the cross-validation
function (tol should generally be no smaller than the square root of
your machine's floating point precision). Defaults to <code>
      1.490116e-04 (1.0e+04*sqrt(.Machine$double.eps))</code>.
</p>
</td></tr>
<tr><td><code id="npudensbw_+3A_small">small</code></td>
<td>

<p>a small number used to bracket a minimum (it is hopeless to ask for
a bracketing interval of width less than sqrt(epsilon) times its
central value, a fractional width of only about 10-04 (single
precision) or 3x10-8 (double precision)). Defaults to <code>small
      = 1.490116e-05 (1.0e+03*sqrt(.Machine$double.eps))</code>.
</p>
</td></tr>
<tr><td><code id="npudensbw_+3A_lbc.dir">lbc.dir</code>, <code id="npudensbw_+3A_dfc.dir">dfc.dir</code>, <code id="npudensbw_+3A_cfac.dir">cfac.dir</code>, <code id="npudensbw_+3A_initc.dir">initc.dir</code></td>
<td>
<p> lower bound, chi-square
degrees of freedom, stretch factor, and initial non-random values
for direction set search for Powell's algorithm for <code>numeric</code>
variables. See Details</p>
</td></tr>
<tr><td><code id="npudensbw_+3A_lbd.dir">lbd.dir</code>, <code id="npudensbw_+3A_hbd.dir">hbd.dir</code>, <code id="npudensbw_+3A_dfac.dir">dfac.dir</code>, <code id="npudensbw_+3A_initd.dir">initd.dir</code></td>
<td>
<p> lower bound, upper bound,
stretch factor, and initial non-random values for direction set
search for Powell's algorithm for categorical variables. See
Details</p>
</td></tr>
<tr><td><code id="npudensbw_+3A_lbc.init">lbc.init</code>, <code id="npudensbw_+3A_hbc.init">hbc.init</code>, <code id="npudensbw_+3A_cfac.init">cfac.init</code></td>
<td>
<p> lower bound, upper bound, and
non-random initial values for scale factors for <code>numeric</code>
variables for Powell's algorithm. See Details</p>
</td></tr>
<tr><td><code id="npudensbw_+3A_lbd.init">lbd.init</code>, <code id="npudensbw_+3A_hbd.init">hbd.init</code>, <code id="npudensbw_+3A_dfac.init">dfac.init</code></td>
<td>
<p> lower bound, upper bound, and
non-random initial values for scale factors for categorical
variables for Powell's algorithm. See Details</p>
</td></tr>
<tr><td><code id="npudensbw_+3A_scale.init.categorical.sample">scale.init.categorical.sample</code></td>
<td>
<p> a logical value that when set
to <code>TRUE</code> scales <code>lbd.dir</code>, <code>hbd.dir</code>,
<code>dfac.dir</code>, and <code>initd.dir</code> by <code class="reqn">n^{-2/(2P+l)}</code>,
<code class="reqn">n</code> the number of observations, <code class="reqn">P</code> the order of the
kernel, and <code class="reqn">l</code> the number of <code>numeric</code> variables. See
Details</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Typical usages are (see below for a complete list of options and also
the examples at the end of this help file)
</p>
<pre>
    
    Usage 1: compute a bandwidth object using the formula interface:
    
    bw &lt;- npudensbw(~y)
    
    Usage 2: compute a bandwidth object using the data frame interface
    and change the default kernel and order:

    fhat &lt;- npudensbw(tdat = y, ckertype="epanechnikov", ckerorder=4)
    
  </pre>
<p><code>npudensbw</code> implements a variety of methods for choosing
bandwidths for multivariate (<code class="reqn">p</code>-variate) distributions defined over
a set of possibly continuous and/or discrete (unordered, ordered)
data. The approach is based on Li and Racine (2003) who employ
&lsquo;generalized product kernels&rsquo; that admit a mix of continuous
and discrete data types.
</p>
<p>The cross-validation methods employ multivariate numerical search
algorithms (direction set (Powell's) methods in multidimensions).
</p>
<p>Bandwidths can (and will) differ for each variable which is, of
course, desirable.
</p>
<p>Three classes of kernel estimators for the continuous data types are
available: fixed, adaptive nearest-neighbor, and generalized
nearest-neighbor. Adaptive nearest-neighbor bandwidths change with
each sample realization in the set, <code class="reqn">x_i</code>, when estimating the
density at the point <code class="reqn">x</code>. Generalized nearest-neighbor bandwidths change
with the point at which the density is estimated, <code class="reqn">x</code>. Fixed bandwidths
are constant over the support of <code class="reqn">x</code>.
</p>
<p><code>npudensbw</code> may be invoked <em>either</em> with a formula-like
symbolic description of variables on which bandwidth selection is to
be performed <em>or</em> through a simpler interface whereby data is
passed directly to the function via the <code>dat</code> parameter. Use of
these two interfaces is <b>mutually exclusive</b>.
</p>
<p>Data contained in the data frame <code>dat</code> may be a mix of continuous
(default), unordered discrete (to be specified in the data frame
<code>dat</code> using <code><a href="base.html#topic+factor">factor</a></code>), and ordered discrete (to be
specified in the data frame <code>dat</code> using
<code><a href="base.html#topic+ordered">ordered</a></code>). Data can be entered in an arbitrary order and
data types will be detected automatically by the routine (see
<code><a href="#topic+np">np</a></code> for details).
</p>
<p>Data for which bandwidths are to be estimated may be specified
symbolically. A typical description has the form <code>~ data</code>, where
<code>data</code> is a series of variables specified by name, separated by
the separation character '+'. For example, <code> ~ x + y </code> specifies
that the bandwidths for the joint distribution of variables <code>x</code>
and <code>y</code> are to be estimated. See below for further examples.
</p>
<p>A variety of kernels may be specified by the user. Kernels implemented
for continuous data types include the second, fourth, sixth, and
eighth order Gaussian and Epanechnikov kernels, and the uniform
kernel. Unordered discrete data types use a variation on Aitchison and
Aitken's (1976) kernel, while ordered data types use a variation of
the Wang and van Ryzin (1981) kernel.
</p>
<p>The optimizer invoked for search is Powell's conjugate direction
method which requires the setting of (non-random) initial values and
search directions for bandwidths, and, when restarting, random values
for successive invocations. Bandwidths for <code>numeric</code> variables
are scaled by robust measures of spread, the sample size, and the
number of <code>numeric</code> variables where appropriate. Two sets of
parameters for bandwidths for <code>numeric</code> can be modified, those
for initial values for the parameters themselves, and those for the
directions taken (Powell's algorithm does not involve explicit
computation of the function's gradient). The default values are set by
considering search performance for a variety of difficult test cases
and simulated cases. We highly recommend restarting search a large
number of times to avoid the presence of local minima (achieved by
modifying <code>nmulti</code>). Further refinement for difficult cases can
be achieved by modifying these sets of parameters. However, these
parameters are intended more for the authors of the package to enable
&lsquo;tuning&rsquo; for various methods rather than for the user themselves.
</p>


<h3>Value</h3>

<p><code>npudensbw</code> returns a <code>bandwidth</code> object, with the
following components:
</p>
<table>
<tr><td><code>bw</code></td>
<td>
<p> bandwidth(s), scale factor(s) or nearest neighbours for the
data, <code>dat</code> </p>
</td></tr>
<tr><td><code>fval</code></td>
<td>
<p> objective function value at minimum </p>
</td></tr>
</table>
<p>if <code>bwtype</code> is set to <code>fixed</code>, an object containing
bandwidths, of class <code>bandwidth</code>
(or scale factors if <code>bwscaling = TRUE</code>) is returned. If it is set to
<code>generalized_nn</code> or <code>adaptive_nn</code>, then instead the
<code class="reqn">k</code>th nearest 
neighbors are returned for the continuous variables while the discrete
kernel bandwidths are returned for the discrete variables. Bandwidths
are stored under the component name <code>bw</code>, with each
element <code class="reqn">i</code> corresponding to column <code class="reqn">i</code> of input data
<code>dat</code>.
</p>
<p>The functions  <code><a href="stats.html#topic+predict">predict</a></code>, <code><a href="base.html#topic+summary">summary</a></code> and <code><a href="base.html#topic+plot">plot</a></code> support
objects of type <code>bandwidth</code>.
</p>


<h3>Usage Issues</h3>

<p>If you are using data of mixed types, then it is advisable to use the
<code><a href="base.html#topic+data.frame">data.frame</a></code> function to construct your input data and not
<code><a href="base.html#topic+cbind">cbind</a></code>, since <code><a href="base.html#topic+cbind">cbind</a></code> will typically not work as
intended on mixed data types and will coerce the data to the same
type.
</p>
<p>Caution: multivariate data-driven bandwidth selection methods are, by
their nature, <em>computationally intensive</em>. Virtually all methods
require dropping the <code class="reqn">i</code>th observation from the data set, computing an
object, repeating this for all observations in the sample, then
averaging each of these leave-one-out estimates for a <em>given</em>
value of the bandwidth vector, and only then repeating this a large
number of times in order to conduct multivariate numerical
minimization/maximization. Furthermore, due to the potential for local
minima/maxima, <em>restarting this procedure a large number of times may
often be necessary</em>. This can be frustrating for users possessing
large datasets. For exploratory purposes, you may wish to override the
default search tolerances, say, setting ftol=.01 and tol=.01 and
conduct multistarting (the default is to restart min(5, ncol(dat))
times) as is done for a number of examples. Once the procedure
terminates, you can restart search with default tolerances using those
bandwidths obtained from the less rigorous search (i.e., set
<code>bws=bw</code> on subsequent calls to this routine where <code>bw</code> is
the initial bandwidth object).  A version of this package using the
<code>Rmpi</code> wrapper is under development that allows one to deploy
this software in a clustered computing environment to facilitate
computation involving large datasets.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine
<a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Aitchison, J. and , C.G.G. Aitken (1976), &ldquo;Multivariate binary
discrimination by the kernel method,&rdquo; Biometrika, 63, 413-420.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics: Theory
and Practice,</em> Princeton University Press.
</p>
<p>Li, Q. and J.S. Racine (2003), &ldquo;Nonparametric estimation of
distributions with categorical and continuous data,&rdquo; Journal
of Multivariate Analysis, 86, 266-292.
</p>
<p>Ouyang, D. and Q. Li and J.S. Racine (2006), &ldquo;Cross-validation
and the estimation of probability distributions with categorical
data,&rdquo; Journal of Nonparametric Statistics, 18, 69-100.
</p>
<p>Pagan, A. and A. Ullah (1999), <em>Nonparametric Econometrics,</em>
Cambridge University Press.
</p>
<p>Scott, D.W. (1992), <em>Multivariate Density Estimation. Theory,
Practice and Visualization,</em> New York: Wiley.
</p>
<p>Silverman, B.W. (1986), <em>Density Estimation,</em> London: Chapman and
Hall.
</p>
<p>Wang, M.C. and J. van Ryzin (1981), &ldquo;A class of smooth estimators
for discrete distributions,&rdquo;  Biometrika, 68, 301-309.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+bw.nrd">bw.nrd</a></code>, <code><a href="stats.html#topic+bw.SJ">bw.SJ</a></code>,  <code><a href="graphics.html#topic+hist">hist</a></code>,
<code><a href="#topic+npudens">npudens</a></code>, <code><a href="#topic+npudist">npudist</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# EXAMPLE 1 (INTERFACE=FORMULA): For this example, we load Giovanni
# Baiocchi's Italian GDP panel (see Italy for details), then create a
# data frame in which year is an ordered factor, GDP is continuous.

data("Italy")
attach(Italy)

data &lt;- data.frame(ordered(year), gdp)

# We compute bandwidths for the kernel density estimator using the
# normal-reference rule-of-thumb. Otherwise, we use the defaults (second
# order Gaussian kernel, fixed bandwidths). Note that the bandwidth
# object you compute inherits all properties of the estimator (kernel
# type, kernel order, estimation method) and can be fed directly into
# the plotting utility plot() or into the npudens() function.

bw &lt;- npudensbw(formula=~ordered(year)+gdp, bwmethod="normal-reference")

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Next, specify a value for the bandwidths manually (0.5 for the first
# variable, 1.0 for the second)...

bw &lt;- npudensbw(formula=~ordered(year)+gdp, bws=c(0.5, 1.0),
                bandwidth.compute=FALSE)

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Next, if you wanted to use the 1.06 sigma n^{-1/(2p+q)} rule-of-thumb
# for the bandwidth for the continuous variable and, say, no smoothing
# for the discrete variable, you would use the bwscaling=TRUE argument
# and feed in the values 0 for the first variable (year) and 1.06 for
# the second (gdp). Note that in the printout it reports the `scale
# factors' rather than the `bandwidth' as reported in some of the
# previous examples.

bw &lt;- npudensbw(formula=~ordered(year)+gdp, bws=c(0, 1.06),
                bwscaling=TRUE, 
                bandwidth.compute=FALSE)

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# If you wished to use, say, an eighth order Epanechnikov kernel for the
# continuous variables and specify your own bandwidths, you could do
# that as follows.

bw &lt;- npudensbw(formula=~ordered(year)+gdp, bws=c(0.5, 1.0),
                bandwidth.compute=FALSE, 
                ckertype="epanechnikov",
                ckerorder=8)

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# If you preferred, say, nearest-neighbor bandwidths and a generalized
# kernel estimator for the continuous variable, you would use the
# bwtype="generalized_nn" argument.

bw &lt;- npudensbw(formula=~ordered(year)+gdp, bwtype = "generalized_nn")

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Next, compute bandwidths using likelihood cross-validation, fixed
# bandwidths, and a second order Gaussian kernel for the continuous
# variable (default).  Note - this may take a few minutes depending on
# the speed of your computer.

bw &lt;- npudensbw(formula=~ordered(year)+gdp)

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Finally, if you wish to use initial values for numerical search, you
# can either provide a vector of bandwidths as in bws=c(...) or a
# bandwidth object from a previous run, as in

bw &lt;- npudensbw(formula=~ordered(year)+gdp, bws=c(1, 1))

summary(bw)

detach(Italy)

# EXAMPLE 1 (INTERFACE=DATA FRAME): For this example, we load Giovanni
# Baiocchi's Italian GDP panel (see Italy for details), then create a
# data frame in which year is an ordered factor, GDP is continuous.

data("Italy")
attach(Italy)

data &lt;- data.frame(ordered(year), gdp)

# We compute bandwidths for the kernel density estimator using the
# normal-reference rule-of-thumb. Otherwise, we use the defaults (second
# order Gaussian kernel, fixed bandwidths). Note that the bandwidth
# object you compute inherits all properties of the estimator (kernel
# type, kernel order, estimation method) and can be fed directly into
# the plotting utility plot() or into the npudens() function.

bw &lt;- npudensbw(dat=data, bwmethod="normal-reference")

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Next, specify a value for the bandwidths manually (0.5 for the first
# variable, 1.0 for the second)...

bw &lt;- npudensbw(dat=data, bws=c(0.5, 1.0), bandwidth.compute=FALSE)

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Next, if you wanted to use the 1.06 sigma n^{-1/(2p+q)} rule-of-thumb
# for the bandwidth for the continuous variable and, say, no smoothing
# for the discrete variable, you would use the bwscaling=TRUE argument
# and feed in the values 0 for the first variable (year) and 1.06 for
# the second (gdp). Note that in the printout it reports the `scale
# factors' rather than the `bandwidth' as reported in some of the
# previous examples.

bw &lt;- npudensbw(dat=data, bws=c(0, 1.06),
                bwscaling=TRUE, 
                bandwidth.compute=FALSE)

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# If you wished to use, say, an eighth order Epanechnikov kernel for the
# continuous variables and specify your own bandwidths, you could do
# that as follows:

bw &lt;- npudensbw(dat=data, bws=c(0.5, 1.0),
                bandwidth.compute=FALSE, 
                ckertype="epanechnikov",
                ckerorder=8)

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# If you preferred, say, nearest-neighbor bandwidths and a generalized
# kernel estimator for the continuous variable, you would use the
# bwtype="generalized_nn" argument.

bw &lt;- npudensbw(dat=data, bwtype = "generalized_nn")

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Next, compute bandwidths using likelihood cross-validation, fixed
# bandwidths, and a second order Gaussian kernel for the continuous
# variable (default).  Note - this may take a few minutes depending on
# the speed of your computer.

bw &lt;- npudensbw(dat=data)

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Finally, if you wish to use initial values for numerical search, you
# can either provide a vector of bandwidths as in bws=c(...) or a
# bandwidth object from a previous run, as in

bw &lt;- npudensbw(dat=data, bws=c(1, 1))

summary(bw)

detach(Italy)

## End(Not run) 
</code></pre>

<hr>
<h2 id='npudist'> Kernel Distribution Estimation with Mixed Data Types </h2><span id='topic+npudist'></span><span id='topic+npudist.dbandwidth'></span><span id='topic+npudist.call'></span><span id='topic+npudist.default'></span><span id='topic+npudist.formula'></span>

<h3>Description</h3>

<p><code>npudist</code> computes kernel unconditional cumulative distribution
estimates on evaluation data, given a set of training data and a
bandwidth specification (a <code>dbandwidth</code> object or a bandwidth
vector, bandwidth type, and kernel type) using the method of Li, Li
and Racine (2017).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npudist(bws, ...)

## S3 method for class 'formula'
npudist(bws, data = NULL, newdata = NULL, ...)

## S3 method for class 'dbandwidth'
npudist(bws,
        tdat = stop("invoked without training data 'tdat'"),
        edat,
        ...)

## S3 method for class 'call'
npudist(bws, ...)

## Default S3 method:
npudist(bws, tdat, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npudist_+3A_bws">bws</code></td>
<td>

<p>a <code>dbandwidth</code> specification. This can be set as a <code>dbandwidth</code>
object returned from an invocation of <code><a href="#topic+npudistbw">npudistbw</a></code>, or as a
<code class="reqn">p</code>-vector of bandwidths, with an element for each variable in the
training data. If specified as a vector, then additional arguments
will need to be supplied as necessary to change them from the
defaults to specify the bandwidth type, kernel types, training data,
and so on.
</p>
</td></tr>
<tr><td><code id="npudist_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to specify the training data, the
bandwidth type, kernel types, and so on.  This is necessary if you
specify bws as a <code class="reqn">p</code>-vector and not a <code>dbandwidth</code> object,
and you do not desire the default behaviours. To do this, you may
specify any of <code>bwscaling</code>, <code>bwtype</code>, <code>ckertype</code>,
<code>ckerorder</code>, <code>okertype</code>, as described in
<code><a href="#topic+npudistbw">npudistbw</a></code>.
</p>
</td></tr>
<tr><td><code id="npudist_+3A_tdat">tdat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of sample realizations (training data)
used to estimate the cumulative distribution. Defaults to the training data used to
compute the bandwidth object.
</p>
</td></tr>
<tr><td><code id="npudist_+3A_edat">edat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame of cumulative distribution evaluation points. By default,
evaluation takes place on the data provided by <code>tdat</code>.
</p>
</td></tr>
<tr><td><code id="npudist_+3A_data">data</code></td>
<td>

<p>an optional data frame, list or environment (or object
coercible to a data frame by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code>) containing the variables
in the model. If not found in data, the variables are taken from
<code>environment(bws)</code>, typically the environment from which
<code><a href="#topic+npudistbw">npudistbw</a></code> was called.
</p>
</td></tr>
<tr><td><code id="npudist_+3A_newdata">newdata</code></td>
<td>

<p>An optional data frame in which to look for evaluation data. If
omitted, the training data are used.  
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Typical usages are (see below for a complete list of options and also
the examples at the end of this help file)
</p>
<pre>
    Usage 1: first compute the bandwidth object via npudistbw and then
    compute the cumulative distribution:
    
    bw &lt;- npudistbw(~y)
    Fhat &lt;- npudist(bw)
    
    Usage 2: alternatively, compute the bandwidth object indirectly:
    
    Fhat &lt;- npudist(~y)
    
    Usage 3: modify the default kernel and order:
    
    Fhat &lt;- npudist(~y, ckertype="epanechnikov", ckerorder=4)

    Usage 4: use the data frame interface rather than the formula
    interface:

    Fhat &lt;- npudist(tdat = y, ckertype="epanechnikov", ckerorder=4)
  </pre>
<p><code>npudist</code> implements a variety of methods for estimating
multivariate cumulative distributions (<code class="reqn">p</code>-variate) defined over a
set of possibly continuous and/or discrete (ordered) data. The
approach is based on Li and Racine (2003) who employ
&lsquo;generalized product kernels&rsquo; that admit a mix of continuous
and discrete data types.
</p>
<p>Three classes of kernel estimators for the continuous data types are
available: fixed, adaptive nearest-neighbor, and generalized
nearest-neighbor. Adaptive nearest-neighbor bandwidths change with
each sample realization in the set, <code class="reqn">x_i</code>, when estimating
the cumulative distribution at the point <code class="reqn">x</code>. Generalized nearest-neighbor
bandwidths change with the point at which the cumulative distribution is estimated,
<code class="reqn">x</code>. Fixed bandwidths are constant over the support of <code class="reqn">x</code>.
</p>
<p>Data contained in the data frame <code>tdat</code> (and also <code>edat</code>)
may be a mix of continuous (default) and ordered discrete (to be
specified in the data frame <code>tdat</code> using the
<code><a href="base.html#topic+ordered">ordered</a></code> command). Data can be entered in an arbitrary
order and data types will be detected automatically by the routine
(see <code><a href="#topic+np">np</a></code> for details).
</p>
<p>A variety of kernels may be specified by the user. Kernels implemented
for continuous data types include the second, fourth, sixth, and
eighth-order Gaussian and Epanechnikov kernels, and the uniform
kernel. Ordered data types use a variation of the Wang and van Ryzin
(1981) kernel.
</p>


<h3>Value</h3>

<p><code><a href="#topic+npudist">npudist</a></code> returns a <code>npdistribution</code> object. The
generic accessor functions <code><a href="stats.html#topic+fitted">fitted</a></code> and <code><a href="#topic+se">se</a></code>
extract estimated values and asymptotic standard errors on estimates,
respectively, from the returned object. Furthermore, the functions
<code><a href="stats.html#topic+predict">predict</a></code>, <code><a href="base.html#topic+summary">summary</a></code> and <code><a href="base.html#topic+plot">plot</a></code>
support objects of both classes. The returned objects have the
following components:
</p>
<table>
<tr><td><code>eval</code></td>
<td>
<p> the evaluation points. </p>
</td></tr>
<tr><td><code>dist</code></td>
<td>
<p> estimate of the cumulative distribution at the evaluation points </p>
</td></tr>
<tr><td><code>derr</code></td>
<td>
<p> standard errors of the cumulative distribution estimates </p>
</td></tr>
</table>


<h3>Usage Issues</h3>

<p>If you are using data of mixed types, then it is advisable to use the
<code><a href="base.html#topic+data.frame">data.frame</a></code> function to construct your input data and not
<code><a href="base.html#topic+cbind">cbind</a></code>, since <code><a href="base.html#topic+cbind">cbind</a></code> will typically not work as
intended on mixed data types and will coerce the data to the same
type.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine
<a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Aitchison, J. and C.G.G. Aitken (1976), &ldquo; Multivariate binary
discrimination by the kernel method,&rdquo; Biometrika, 63, 413-420.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics: Theory
and Practice,</em> Princeton University Press.
</p>
<p>Li, Q. and J.S. Racine (2003), &ldquo;Nonparametric estimation of
distributions with categorical and continuous data,&rdquo; Journal
of Multivariate Analysis, 86, 266-292.
</p>
<p>Li, C. and H. Li and J.S. Racine (2017), &ldquo;Cross-Validated Mixed
Datatype Bandwidth Selection for Nonparametric Cumulative
Distribution/Survivor Functions,&rdquo; Econometric Reviews, <b>36</b>,
970-987.
</p>
<p>Ouyang, D. and Q. Li and J.S. Racine (2006), &ldquo;Cross-validation
and the estimation of probability distributions with categorical
data,&rdquo; Journal of Nonparametric Statistics, 18, 69-100.
</p>
<p>Pagan, A. and A. Ullah (1999), <em>Nonparametric Econometrics,</em> Cambridge
University Press. 
</p>
<p>Scott, D.W. (1992), <em>Multivariate Density Estimation. Theory,
Practice and Visualization,</em> New York: Wiley.
</p>
<p>Silverman, B.W. (1986), <em>Density Estimation,</em> London: Chapman and
Hall.
</p>
<p>Wang, M.C. and J. van Ryzin (1981), &ldquo;A class of smooth estimators
for discrete distributions,&rdquo;  Biometrika, 68, 301-309.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+npudistbw">npudistbw</a></code> , <code><a href="stats.html#topic+density">density</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# EXAMPLE 1 (INTERFACE=FORMULA): For this example, we load Giovanni
# Baiocchi's Italian GDP panel (see Italy for details), then create a
# data frame in which year is an ordered factor, GDP is continuous,
# compute bandwidths using cross-validation, then create a grid of data
# on which the cumulative distribution will be evaluated for plotting
# purposes.

data("Italy")
attach(Italy)

# Compute bandwidths using cross-validation (default).

bw &lt;- npudistbw(formula=~ordered(year)+gdp)

# At this stage you could use npudist() to do a variety of things. Here
# we compute the npudist() object and place it in Fhat.

Fhat &lt;- npudist(bws=bw)

# Note that simply typing the name of the object returns some useful
# information. For more info, one can call summary:

summary(Fhat)

# Next, we illustrate how to create a grid of `evaluation data' and feed
# it to the perspective plotting routines in R, among others.

# Create an evaluation data matrix

year.seq &lt;- sort(unique(year))
gdp.seq &lt;- seq(1,36,length=50)
data.eval &lt;- expand.grid(year=year.seq,gdp=gdp.seq)

# Generate the estimated cumulative distribution computed for the
# evaluation data

Fhat &lt;- fitted(npudist(bws=bw, newdata=data.eval))

# Coerce the data into a matrix for plotting with persp()

F &lt;- matrix(Fhat, length(unique(year)), 50)

# Next, create a 3D perspective plot of the CDF F, and a 2D
# contour plot.

persp(as.integer(levels(year.seq)), gdp.seq, F, col="lightblue",
      ticktype="detailed", ylab="GDP", xlab="Year", zlab="Density",
      theta=300, phi=50)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

contour(as.integer(levels(year.seq)), 
        gdp.seq, 
        F, 
        xlab="Year", 
        ylab="GDP", 
        main = "Cumulative Distribution Contour Plot", 
        col=topo.colors(100))

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Alternatively, you could use the plot() command (&lt;ctrl&gt;-C will
# interrupt on *NIX systems, &lt;esc&gt; will interrupt on MS Windows
# systems).

plot(bw)

detach(Italy)

# EXAMPLE 1 (INTERFACE=DATA FRAME): For this example, we load Giovanni
# Baiocchi's Italian GDP panel (see Italy for details), then create a
# data frame in which year is an ordered factor, GDP is continuous,
# compute bandwidths using cross-validation, then create a grid of data
# on which the cumulative distribution will be evaluated for plotting
# purposes.

data("Italy")
attach(Italy)

data &lt;- data.frame(year=ordered(year), gdp)

# Compute bandwidths using cross-validation (default).

bw &lt;- npudistbw(dat=data)

# At this stage you could use npudist() to do a variety of
# things. Here we compute the npudist() object and place it in Fhat.

Fhat &lt;- npudist(bws=bw)

# Note that simply typing the name of the object returns some useful
# information. For more info, one can call summary:

summary(Fhat)

# Next, we illustrate how to create a grid of `evaluation data' and feed
# it to the perspective plotting routines in R, among others.

# Create an evaluation data matrix

year.seq &lt;- sort(unique(year))
gdp.seq &lt;- seq(1,36,length=50)
data.eval &lt;- expand.grid(year=year.seq,gdp=gdp.seq)

# Generate the estimated cumulative distribution computed for the
# evaluation data

Fhat &lt;- fitted(npudist(edat = data.eval, bws=bw))

# Coerce the data into a matrix for plotting with persp()

F &lt;- matrix(Fhat, length(unique(year)), 50)

# Next, create a 3D perspective plot of the CDF F, and a 2D
# contour plot.

persp(as.integer(levels(year.seq)), gdp.seq, F, col="lightblue",
      ticktype="detailed", ylab="GDP", xlab="Year",
      zlab="Cumulative Distribution",
      theta=300, phi=50)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

contour(as.integer(levels(year.seq)),
        gdp.seq, 
        F, 
        xlab="Year", 
        ylab="GDP", 
        main = "Cumulative Distribution Contour Plot", 
        col=topo.colors(100))

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Alternatively, you could use the plot() command (&lt;ctrl&gt;-C will
# interrupt on *NIX systems, &lt;esc&gt; will interrupt on MS Windows
# systems).

plot(bw)

detach(Italy)

# EXAMPLE 2 (INTERFACE=FORMULA): For this example, we load the old
# faithful geyser data and compute the cumulative distribution function.

library("datasets")
data("faithful")
attach(faithful)

# Note - this may take a few minutes depending on the speed of your
# computer...

bw &lt;- npudistbw(formula=~eruptions+waiting)

summary(bw)

# Plot the cumulative distribution function (&lt;ctrl&gt;-C will interrupt on
# *NIX systems, &lt;esc&gt; will interrupt on MS Windows systems). Note that
# we use xtrim = -0.2 to extend the plot outside the support of the data
# (i.e., extend the tails of the estimate to meet the horizontal axis).

plot(bw, xtrim=-0.2)

detach(faithful)

# EXAMPLE 2 (INTERFACE=DATA FRAME): For this example, we load the old
# faithful geyser data and compute the cumulative distribution function.

library("datasets")
data("faithful")
attach(faithful)

# Note - this may take a few minutes depending on the speed of your
# computer...

bw &lt;- npudistbw(dat=faithful)

summary(bw)

# Plot the cumulative distribution function (&lt;ctrl&gt;-C will interrupt on
# *NIX systems, &lt;esc&gt; will interrupt on MS Windows systems). Note that
# we use xtrim = -0.2 to extend the plot outside the support of the data
# (i.e., extend the tails of the estimate to meet the horizontal axis).

plot(bw, xtrim=-0.2)

detach(faithful)

## End(Not run) 
</code></pre>

<hr>
<h2 id='npudistbw'>Kernel Distribution Bandwidth Selection with Mixed Data Types</h2><span id='topic+npudistbw'></span><span id='topic+npudistbw.formula'></span><span id='topic+npudistbw.NULL'></span><span id='topic+npudistbw.default'></span><span id='topic+npudistbw.dbandwidth'></span>

<h3>Description</h3>

<p><code>npudistbw</code> computes a bandwidth object for a <code class="reqn">p</code>-variate
kernel cumulative distribution estimator defined over mixed continuous
and discrete (ordered) data using either the normal reference
rule-of-thumb or least-squares cross validation using the method of
Li, Li and Racine (2017).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
npudistbw(...)

## S3 method for class 'formula'
npudistbw(formula, data, subset, na.action, call, gdata = NULL,...)

## S3 method for class 'NULL'
npudistbw(dat = stop("invoked without input data 'dat'"),
          bws,
          ...)

## S3 method for class 'dbandwidth'
npudistbw(dat = stop("invoked without input data 'dat'"),
          bws,
          gdat = NULL,
          bandwidth.compute = TRUE,
          nmulti,
          remin = TRUE,
          itmax = 10000,
          do.full.integral = FALSE,
          ngrid = 100,
          ftol = 1.490116e-07,
          tol = 1.490116e-04,
          small = 1.490116e-05,
          lbc.dir = 0.5,
          dfc.dir = 3,
          cfac.dir = 2.5*(3.0-sqrt(5)),
          initc.dir = 1.0,
          lbd.dir = 0.1,
          hbd.dir = 1,
          dfac.dir = 0.25*(3.0-sqrt(5)),
          initd.dir = 1.0,
          lbc.init = 0.1,
          hbc.init = 2.0,
          cfac.init = 0.5,
          lbd.init = 0.1,
          hbd.init = 0.9,
          dfac.init = 0.375, 
          scale.init.categorical.sample = FALSE,
          memfac = 500.0,
          ...)

## Default S3 method:
npudistbw(dat = stop("invoked without input data 'dat'"),
          bws,
          gdat,
          bandwidth.compute = TRUE,
          nmulti,
          remin,
          itmax,
          do.full.integral,
          ngrid,
          ftol,
          tol,
          small,
          lbc.dir,
          dfc.dir,
          cfac.dir,
          initc.dir,
          lbd.dir,
          hbd.dir,
          dfac.dir,
          initd.dir,
          lbc.init,
          hbc.init,
          cfac.init,
          lbd.init,
          hbd.init,
          dfac.init,
          scale.init.categorical.sample,
          memfac,
          bwmethod,
          bwscaling,
          bwtype,
          ckertype,
          ckerorder,
          okertype,
          ...) 

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npudistbw_+3A_formula">formula</code></td>
<td>

<p>a symbolic description of variables on which bandwidth selection is
to be performed. The details of constructing a formula are
described below.
</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_data">data</code></td>
<td>

<p>an optional data frame, list or environment (or object coercible to
a data frame by <code><a href="base.html#topic+as.data.frame">as.data.frame</a></code>) containing the
variables in the model. If not found in data, the variables are
taken from <code>environment(formula)</code>, typically the environment
from which the function is called.
</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_subset">subset</code></td>
<td>

<p>an optional vector specifying a subset of observations to be used in
the fitting process. 
</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_na.action">na.action</code></td>
<td>

<p>a function which indicates what should happen when the data contain
<code>NA</code>s. The default is set by the <code><a href="stats.html#topic+na.action">na.action</a></code> setting
of options, and is <code><a href="stats.html#topic+na.fail">na.fail</a></code> if that is unset. The
(recommended) default is <code><a href="stats.html#topic+na.omit">na.omit</a></code>.
</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_call">call</code></td>
<td>

<p>the original function call. This is passed internally by
<code><a href="#topic+np">np</a></code> when a bandwidth search has been implied by a call to
another function. It is not recommended that the user set this.
</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_gdata">gdata</code></td>
<td>

<p>a grid of data on which the indicator function for
least-squares cross-validation is to be computed (can be the sample
or a grid of quantiles).
</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_dat">dat</code></td>
<td>

<p>a <code class="reqn">p</code>-variate data frame on which bandwidth selection will be
performed. The data types may be continuous, discrete (ordered
factors), or some combination thereof.
</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_bws">bws</code></td>
<td>

<p>a bandwidth specification. This can be set as a bandwidth object
returned from a previous invocation, or as a vector of bandwidths,
with each element <code class="reqn">i</code> corresponding to the bandwidth for column
<code class="reqn">i</code> in <code>dat</code>. In either case, the bandwidth supplied will
serve as a starting point in the numerical search for optimal
bandwidths. If specified as a vector, then additional arguments will
need to be supplied as necessary to specify the bandwidth type,
kernel types, selection methods, and so on. This can be left unset.
</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_gdat">gdat</code></td>
<td>

<p>a grid of data on which the indicator function for
least-squares cross-validation is to be computed (can be the sample
or a grid of quantiles).
</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_...">...</code></td>
<td>

<p>additional arguments supplied to specify the bandwidth type,
kernel types, selection methods, and so on, detailed below.
</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_bwmethod">bwmethod</code></td>
<td>
<p> a character string specifying the bandwidth selection
method.  <code>cv.cdf</code> specifies least-squares cross-validation for
cumulative distribution functions (Li, Li and Racine (2017)), and
<code>normal-reference</code> just computes the &lsquo;rule-of-thumb&rsquo;
bandwidth <code class="reqn">h_j</code> using the standard formula <code class="reqn">h_j =
    1.587 \sigma_j n^{-1/(P+l)}</code>,
where <code class="reqn">\sigma_j</code> is an adaptive measure of spread of
the <code class="reqn">j</code>th continuous variable defined as min(standard deviation,
mean absolute deviation/1.4826, interquartile range/1.349), <code class="reqn">n</code>
the number of observations, <code class="reqn">P</code> the order of the kernel, and
<code class="reqn">l</code> the number of continuous variables. Note that when there
exist factors and the normal-reference rule is used, there is zero
smoothing of the factors. Defaults to <code>cv.cdf</code>.  </p>
</td></tr>
<tr><td><code id="npudistbw_+3A_bwscaling">bwscaling</code></td>
<td>
<p> a logical value that when set to <code>TRUE</code> the
supplied bandwidths are interpreted as &lsquo;scale factors&rsquo;
(<code class="reqn">c_j</code>), otherwise when the value is <code>FALSE</code> they are
interpreted as &lsquo;raw bandwidths&rsquo; (<code class="reqn">h_j</code> for continuous data
types, <code class="reqn">\lambda_j</code> for discrete data types). For
continuous data types, <code class="reqn">c_j</code> and <code class="reqn">h_j</code> are
related by the formula <code class="reqn">h_j = c_j \sigma_j n^{-1/(P+l)}</code>, where <code class="reqn">\sigma_j</code> is an
adaptive measure of spread of the <code class="reqn">j</code>th continuous variable
defined as min(standard deviation, mean absolute deviation/1.4826,
interquartile range/1.349), <code class="reqn">n</code> the number of observations,
<code class="reqn">P</code> the order of the kernel, and <code class="reqn">l</code> the number of
continuous variables. For discrete data types, <code class="reqn">c_j</code> and
<code class="reqn">h_j</code> are related by the formula <code class="reqn">h_j =
    c_jn^{-2/(P+l)}</code>, where here <code class="reqn">j</code>
denotes discrete variable <code class="reqn">j</code>.  Defaults to <code>FALSE</code>.  </p>
</td></tr>
<tr><td><code id="npudistbw_+3A_bwtype">bwtype</code></td>
<td>

<p>character string used for the continuous variable bandwidth type,
specifying the type of bandwidth to compute and return in the
<code>bandwidth</code> object. Defaults to <code>fixed</code>. Option
summary:<br />
<code>fixed</code>: compute fixed bandwidths <br />
<code>generalized_nn</code>: compute generalized nearest neighbors <br />
<code>adaptive_nn</code>: compute adaptive nearest neighbors
</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_bandwidth.compute">bandwidth.compute</code></td>
<td>

<p>a logical value which specifies whether to do a numerical search for
bandwidths or not. If set to <code>FALSE</code>, a <code>bandwidth</code> object
will be returned with bandwidths set to those specified
in <code>bws</code>. Defaults to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_ckertype">ckertype</code></td>
<td>

<p>character string used to specify the continuous kernel type.
Can be set as <code>gaussian</code>, <code>epanechnikov</code>, or
<code>uniform</code>. Defaults to <code>gaussian</code>.
</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_ckerorder">ckerorder</code></td>
<td>

<p>numeric value specifying kernel order (one of
<code>(2,4,6,8)</code>). Kernel order specified along with a
<code>uniform</code> continuous kernel type will be ignored. Defaults to
<code>2</code>.
</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_okertype">okertype</code></td>
<td>

<p>character string used to specify the ordered categorical kernel type.
Can be set as <code>wangvanryzin</code>.
</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_nmulti">nmulti</code></td>
<td>

<p>integer number of times to restart the process of finding extrema of
the cross-validation function from different (random) initial points.
</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_remin">remin</code></td>
<td>

<p>a logical value which when set as <code>TRUE</code> the search routine
restarts from located minima for a minor gain in accuracy. Defaults
to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_itmax">itmax</code></td>
<td>

<p>integer number of iterations before failure in the numerical
optimization routine. Defaults to <code>10000</code>.
</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_do.full.integral">do.full.integral</code></td>
<td>

<p>a logical value which when set as <code>TRUE</code> evaluates the
moment-based integral on the entire sample. Defaults
to <code>FALSE</code>.
</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_ngrid">ngrid</code></td>
<td>

<p>integer number of grid points to use when computing the moment-based
integral. Defaults to <code>100</code>.
</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_ftol">ftol</code></td>
<td>

<p>fractional tolerance on the value of the cross-validation function
evaluated at located minima (of order the machine precision or
perhaps slightly larger so as not to be diddled by
roundoff). Defaults to <code>1.490116e-07</code>
(1.0e+01*sqrt(.Machine$double.eps)).
</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_tol">tol</code></td>
<td>

<p>tolerance on the position of located minima of the cross-validation
function (tol should generally be no smaller than the square root of
your machine's floating point precision). Defaults to <code>
      1.490116e-04 (1.0e+04*sqrt(.Machine$double.eps))</code>.
</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_small">small</code></td>
<td>

<p>a small number used to bracket a minimum (it is hopeless to ask for
a bracketing interval of width less than sqrt(epsilon) times its
central value, a fractional width of only about 10-04 (single
precision) or 3x10-8 (double precision)). Defaults to <code>small
      = 1.490116e-05 (1.0e+03*sqrt(.Machine$double.eps))</code>.
</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_lbc.dir">lbc.dir</code>, <code id="npudistbw_+3A_dfc.dir">dfc.dir</code>, <code id="npudistbw_+3A_cfac.dir">cfac.dir</code>, <code id="npudistbw_+3A_initc.dir">initc.dir</code></td>
<td>
<p> lower bound, chi-square
degrees of freedom, stretch factor, and initial non-random values
for direction set search for Powell's algorithm for <code>numeric</code>
variables. See Details</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_lbd.dir">lbd.dir</code>, <code id="npudistbw_+3A_hbd.dir">hbd.dir</code>, <code id="npudistbw_+3A_dfac.dir">dfac.dir</code>, <code id="npudistbw_+3A_initd.dir">initd.dir</code></td>
<td>
<p> lower bound, upper bound,
stretch factor, and initial non-random values for direction set
search for Powell's algorithm for categorical variables. See
Details</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_lbc.init">lbc.init</code>, <code id="npudistbw_+3A_hbc.init">hbc.init</code>, <code id="npudistbw_+3A_cfac.init">cfac.init</code></td>
<td>
<p> lower bound, upper bound, and
non-random initial values for scale factors for <code>numeric</code>
variables for Powell's algorithm. See Details</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_lbd.init">lbd.init</code>, <code id="npudistbw_+3A_hbd.init">hbd.init</code>, <code id="npudistbw_+3A_dfac.init">dfac.init</code></td>
<td>
<p> lower bound, upper bound, and
non-random initial values for scale factors for categorical
variables for Powell's algorithm. See Details</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_scale.init.categorical.sample">scale.init.categorical.sample</code></td>
<td>
<p> a logical value that when set
to <code>TRUE</code> scales <code>lbd.dir</code>, <code>hbd.dir</code>,
<code>dfac.dir</code>, and <code>initd.dir</code> by <code class="reqn">n^{-2/(2P+l)}</code>,
<code class="reqn">n</code> the number of observations, <code class="reqn">P</code> the order of the
kernel, and <code class="reqn">l</code> the number of <code>numeric</code> variables. See
Details</p>
</td></tr>
<tr><td><code id="npudistbw_+3A_memfac">memfac</code></td>
<td>

<p>The algorithm to compute the least-squares objective function uses
a block-based algorithm to eliminate or minimize redundant kernel
evaluations. Due to memory, hardware and software constraints, a
maximum block size must be imposed by the algorithm. This block size
is roughly equal to memfac*10^5 elements. Empirical tests on
modern hardware find that a memfac of 500 performs well. If
you experience out of memory errors, or strange behaviour for
large data sets (&gt;100k elements) setting memfac to a lower value may
fix the problem.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Typical usages are (see below for a complete list of options and also
the examples at the end of this help file)
</p>
<pre>
    
    Usage 1: compute a bandwidth object using the formula interface:
    
    bw &lt;- npudistbw(~y)
    
    Usage 2: compute a bandwidth object using the data frame interface
    and change the default kernel and order:

    Fhat &lt;- npudistbw(tdat = y, ckertype="epanechnikov", ckerorder=4)
    
  </pre>
<p><code>npudistbw</code> implements a variety of methods for choosing
bandwidths for multivariate (<code class="reqn">p</code>-variate) distributions defined
over a set of possibly continuous and/or discrete (ordered) data. The
approach is based on Li and Racine (2003) who employ
&lsquo;generalized product kernels&rsquo; that admit a mix of continuous
and discrete data types.
</p>
<p>The cross-validation methods employ multivariate numerical search
algorithms (direction set (Powell's) methods in multidimensions).
</p>
<p>Bandwidths can (and will) differ for each variable which is, of
course, desirable.
</p>
<p>Three classes of kernel estimators for the continuous data types are
available: fixed, adaptive nearest-neighbor, and generalized
nearest-neighbor. Adaptive nearest-neighbor bandwidths change with
each sample realization in the set, <code class="reqn">x_i</code>, when estimating the
cumulative distribution at the point <code class="reqn">x</code>. Generalized nearest-neighbor bandwidths change
with the point at which the cumulative distribution is estimated, <code class="reqn">x</code>. Fixed bandwidths
are constant over the support of <code class="reqn">x</code>.
</p>
<p><code>npudistbw</code> may be invoked <em>either</em> with a formula-like
symbolic description of variables on which bandwidth selection is to
be performed <em>or</em> through a simpler interface whereby data is
passed directly to the function via the <code>dat</code> parameter. Use of
these two interfaces is <b>mutually exclusive</b>.
</p>
<p>Data contained in the data frame <code>dat</code> may be a mix of continuous
(default) and ordered discrete (to be specified in the data frame
<code>dat</code> using <code><a href="base.html#topic+ordered">ordered</a></code>). Data can be entered in an
arbitrary order and data types will be detected automatically by the
routine (see <code><a href="#topic+np">np</a></code> for details).
</p>
<p>Data for which bandwidths are to be estimated may be specified
symbolically. A typical description has the form <code>~ data</code>, where
<code>data</code> is a series of variables specified by name, separated by
the separation character '+'. For example, <code> ~ x + y </code> specifies
that the bandwidths for the joint distribution of variables <code>x</code>
and <code>y</code> are to be estimated. See below for further examples.
</p>
<p>A variety of kernels may be specified by the user. Kernels implemented
for continuous data types include the second, fourth, sixth, and
eighth-order Gaussian and Epanechnikov kernels, and the uniform
kernel. Ordered data types use a variation of the Wang and van Ryzin
(1981) kernel.
</p>
<p>The optimizer invoked for search is Powell's conjugate direction
method which requires the setting of (non-random) initial values and
search directions for bandwidths, and when restarting, random values
for successive invocations. Bandwidths for <code>numeric</code> variables
are scaled by robust measures of spread, the sample size, and the
number of <code>numeric</code> variables where appropriate. Two sets of
parameters for bandwidths for <code>numeric</code> can be modified, those
for initial values for the parameters themselves, and those for the
directions taken (Powell's algorithm does not involve explicit
computation of the function's gradient). The default values are set by
considering search performance for a variety of difficult test cases
and simulated cases. We highly recommend restarting search a large
number of times to avoid the presence of local minima (achieved by
modifying <code>nmulti</code>). Further refinement for difficult cases can
be achieved by modifying these sets of parameters. However, these
parameters are intended more for the authors of the package to enable
&lsquo;tuning&rsquo; for various methods rather than for the user them
self.
</p>


<h3>Value</h3>

<p><code>npudistbw</code> returns a <code>bandwidth</code> object with the
following components:
</p>
<table>
<tr><td><code>bw</code></td>
<td>
<p> bandwidth(s), scale factor(s) or nearest neighbours for the
data, <code>dat</code> </p>
</td></tr>
<tr><td><code>fval</code></td>
<td>
<p> objective function value at minimum </p>
</td></tr>
</table>
<p>if <code>bwtype</code> is set to <code>fixed</code>, an object containing
bandwidths, of class <code>bandwidth</code>
(or scale factors if <code>bwscaling = TRUE</code>) is returned. If it is set to
<code>generalized_nn</code> or <code>adaptive_nn</code>, then instead the
<code class="reqn">k</code>th nearest 
neighbors are returned for the continuous variables while the discrete
kernel bandwidths are returned for the discrete variables. Bandwidths
are stored under the component name <code>bw</code>, with each
element <code class="reqn">i</code> corresponding to column <code class="reqn">i</code> of input data
<code>dat</code>.
</p>
<p>The functions  <code><a href="stats.html#topic+predict">predict</a></code>, <code><a href="base.html#topic+summary">summary</a></code> and <code><a href="base.html#topic+plot">plot</a></code> support
objects of type <code>bandwidth</code>.
</p>


<h3>Usage Issues</h3>

<p>If you are using data of mixed types, then it is advisable to use the
<code><a href="base.html#topic+data.frame">data.frame</a></code> function to construct your input data and not
<code><a href="base.html#topic+cbind">cbind</a></code>, since <code><a href="base.html#topic+cbind">cbind</a></code> will typically not work as
intended on mixed data types and will coerce the data to the same
type.
</p>
<p>Caution: multivariate data-driven bandwidth selection methods are, by
their nature, <em>computationally intensive</em>. Virtually all methods
require dropping the <code class="reqn">i</code>th observation from the data set, computing an
object, repeating this for all observations in the sample, then
averaging each of these leave-one-out estimates for a <em>given</em>
value of the bandwidth vector, and only then repeating this a large
number of times in order to conduct multivariate numerical
minimization/maximization. Furthermore, due to the potential for local
minima/maxima, <em>restarting this procedure a large number of times may
often be necessary</em>. This can be frustrating for users possessing
large datasets. For exploratory purposes, you may wish to override the
default search tolerances, say, setting ftol=.01 and tol=.01 and
conduct multistarting (the default is to restart min(5, ncol(dat))
times) as is done for a number of examples. Once the procedure
terminates, you can restart search with default tolerances using those
bandwidths obtained from the less rigorous search (i.e., set
<code>bws=bw</code> on subsequent calls to this routine where <code>bw</code> is
the initial bandwidth object).  A version of this package using the
<code>Rmpi</code> wrapper is under development that allows one to deploy
this software in a clustered computing environment to facilitate
computation involving large datasets.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine
<a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Aitchison, J. and C.G.G. Aitken (1976), &ldquo;Multivariate binary
discrimination by the kernel method,&rdquo; Biometrika, 63, 413-420.
</p>
<p>Bowman, A. and P. Hall and T. Prvan (1998), &ldquo;Bandwidth
selection for the smoothing of distribution functions,&rdquo; Biometrika,
85, 799-808.
</p>
<p>Li, Q. and J.S. Racine (2007), <em>Nonparametric Econometrics: Theory
and Practice,</em> Princeton University Press.
</p>
<p>Li, Q. and J.S. Racine (2003), &ldquo;Nonparametric estimation of
distributions with categorical and continuous data,&rdquo; Journal
of Multivariate Analysis, 86, 266-292.
</p>
<p>Li, C. and H. Li and J.S. Racine (2017), &ldquo;Cross-Validated Mixed
Datatype Bandwidth Selection for Nonparametric Cumulative
Distribution/Survivor Functions,&rdquo; Econometric Reviews, <b>36</b>,
970-987.
</p>
<p>Ouyang, D. and Q. Li and J.S. Racine (2006), &ldquo;Cross-validation
and the estimation of probability distributions with categorical
data,&rdquo; Journal of Nonparametric Statistics, 18, 69-100.
</p>
<p>Pagan, A. and A. Ullah (1999), <em>Nonparametric Econometrics,</em>
Cambridge University Press.
</p>
<p>Scott, D.W. (1992), <em>Multivariate Cumulative Distribution
Estimation: Theory, Practice and Visualization,</em> New York: Wiley.
</p>
<p>Silverman, B.W. (1986), <em>Density Estimation,</em> London: Chapman and
Hall.
</p>
<p>Wang, M.C. and J. van Ryzin (1981), &ldquo;A class of smooth estimators
for discrete distributions,&rdquo;  Biometrika, 68, 301-309.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+bw.nrd">bw.nrd</a></code>, <code><a href="stats.html#topic+bw.SJ">bw.SJ</a></code>,  <code><a href="graphics.html#topic+hist">hist</a></code>,
<code><a href="#topic+npudist">npudist</a></code>, <code><a href="#topic+npudist">npudist</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# EXAMPLE 1 (INTERFACE=FORMULA): For this example, we load Giovanni
# Baiocchi's Italian GDP panel (see Italy for details), then create a
# data frame in which year is an ordered factor, GDP is continuous.

data("Italy")
attach(Italy)

data &lt;- data.frame(ordered(year), gdp)

# We compute bandwidths for the kernel cumulative distribution estimator
# using the normal-reference rule-of-thumb. Otherwise, we use the
# defaults (second order Gaussian kernel, fixed bandwidths). Note that
# the bandwidth object you compute inherits all properties of the
# estimator (kernel type, kernel order, estimation method) and can be
# fed directly into the plotting utility plot() or into the npudist()
# function.

bw &lt;- npudistbw(formula=~ordered(year)+gdp, bwmethod="normal-reference")

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Next, specify a value for the bandwidths manually (0.5 for the first
# variable, 1.0 for the second)...

bw &lt;- npudistbw(formula=~ordered(year)+gdp, bws=c(0.5, 1.0),
                bandwidth.compute=FALSE)

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Next, if you wanted to use the 1.587 sigma n^{-1/(2p+q)} rule-of-thumb
# for the bandwidth for the continuous variable and, say, no smoothing
# for the discrete variable, you would use the bwscaling=TRUE argument
# and feed in the values 0 for the first variable (year) and 1.587 for
# the second (gdp). Note that in the printout it reports the `scale
# factors' rather than the `bandwidth' as reported in some of the
# previous examples.

bw &lt;- npudistbw(formula=~ordered(year)+gdp, bws=c(0, 1.587),
                bwscaling=TRUE, 
                bandwidth.compute=FALSE)

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# If you wished to use, say, an eighth-order Epanechnikov kernel for the
# continuous variables and specify your own bandwidths, you could do
# that as follows.

bw &lt;- npudistbw(formula=~ordered(year)+gdp, bws=c(0.5, 1.0),
                bandwidth.compute=FALSE, 
                ckertype="epanechnikov",
                ckerorder=8)

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# If you preferred, say, nearest-neighbor bandwidths and a generalized
# kernel estimator for the continuous variable, you would use the
# bwtype="generalized_nn" argument.

bw &lt;- npudistbw(formula=~ordered(year)+gdp, bwtype = "generalized_nn")

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Next, compute bandwidths using cross-validation, fixed bandwidths, and
# a second-order Gaussian kernel for the continuous variable (default).
# Note - this may take a few minutes depending on the speed of your
# computer.

bw &lt;- npudistbw(formula=~ordered(year)+gdp)

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Finally, if you wish to use initial values for numerical search, you
# can either provide a vector of bandwidths as in bws=c(...) or a
# bandwidth object from a previous run, as in

bw &lt;- npudistbw(formula=~ordered(year)+gdp, bws=c(1, 1))

summary(bw)

detach(Italy)

# EXAMPLE 1 (INTERFACE=DATA FRAME): For this example, we load Giovanni
# Baiocchi's Italian GDP panel (see Italy for details), then create a
# data frame in which year is an ordered factor, GDP is continuous.

data("Italy")
attach(Italy)

data &lt;- data.frame(ordered(year), gdp)

# We compute bandwidths for the kernel cumulative distribution estimator
# using the normal-reference rule-of-thumb. Otherwise, we use the
# defaults (second-order Gaussian kernel, fixed bandwidths). Note that
# the bandwidth object you compute inherits all properties of the
# estimator (kernel type, kernel order, estimation method) and can be
# fed directly into the plotting utility plot() or into the npudist()
# function.

bw &lt;- npudistbw(dat=data, bwmethod="normal-reference")

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Next, specify a value for the bandwidths manually (0.5 for the first
# variable, 1.0 for the second)...

bw &lt;- npudistbw(dat=data, bws=c(0.5, 1.0), bandwidth.compute=FALSE)

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Next, if you wanted to use the 1.587 sigma n^{-1/(2p+q)} rule-of-thumb
# for the bandwidth for the continuous variable and, say, no smoothing
# for the discrete variable, you would use the bwscaling=TRUE argument
# and feed in the values 0 for the first variable (year) and 1.587 for
# the second (gdp). Note that in the printout it reports the `scale
# factors' rather than the `bandwidth' as reported in some of the
# previous examples.

bw &lt;- npudistbw(dat=data, bws=c(0, 1.587),
                bwscaling=TRUE, 
                bandwidth.compute=FALSE)

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# If you wished to use, say, an eighth-order Epanechnikov kernel for the
# continuous variables and specify your own bandwidths, you could do
# that as follows:

bw &lt;- npudistbw(dat=data, bws=c(0.5, 1.0),
                bandwidth.compute=FALSE, 
                ckertype="epanechnikov",
                ckerorder=8)

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# If you preferred, say, nearest-neighbor bandwidths and a generalized
# kernel estimator for the continuous variable, you would use the
# bwtype="generalized_nn" argument.

bw &lt;- npudistbw(dat=data, bwtype = "generalized_nn")

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Next, compute bandwidths using cross-validation, fixed bandwidths, and
# a second order Gaussian kernel for the continuous variable (default).
# Note - this may take a few minutes depending on the speed of your
# computer.

bw &lt;- npudistbw(dat=data)

summary(bw)

# Sleep for 5 seconds so that we can examine the output...

Sys.sleep(5)

# Finally, if you wish to use initial values for numerical search, you
# can either provide a vector of bandwidths as in bws=c(...) or a
# bandwidth object from a previous run, as in

bw &lt;- npudistbw(dat=data, bws=c(1, 1))

summary(bw)

detach(Italy)

## End(Not run) 
</code></pre>

<hr>
<h2 id='npuniden.boundary'> Kernel Bounded Univariate Density Estimation Via Boundary Kernel
Functions </h2><span id='topic+npuniden.boundary'></span>

<h3>Description</h3>

<p><code>npuniden.boundary</code> computes kernel univariate unconditional
density estimates given a vector of continuously distributed training
data and, optionally, a bandwidth (otherwise least squares
cross-validation is used for its selection). Lower and upper bounds
[<code>a</code>,<code>b</code>] can be supplied (default is the empirical support 
<code class="reqn">[min(X),max(X)]</code>) and if <code>a</code>
is set to <code>-Inf</code> there is only one bound on the right, while if
<code>b</code> is set to <code>Inf</code> there is only one bound on the left. If
<code>a</code> is set to <code>-Inf</code> and <code>b</code> to <code>Inf</code> and the
Gaussian type 1 kernel function is used, this will deliver the
standard unadjusted kernel density estimate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npuniden.boundary(X = NULL,
                  Y = NULL,
                  h = NULL,
                  a = min(X),
                  b = max(X),
                  bwmethod = c("cv.ls","cv.ml"),
                  cv = c("grid-hybrid","numeric"),
                  grid = NULL,
                  kertype = c("gaussian1","gaussian2",
                            "beta1","beta2",
                            "fb","fbl","fbu",
                            "rigaussian","gamma"),
                  nmulti = 5,
                  proper = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npuniden.boundary_+3A_x">X</code></td>
<td>

<p>a required numeric vector of training data lying in <code class="reqn">[a,b]</code>
</p>
</td></tr>
<tr><td><code id="npuniden.boundary_+3A_y">Y</code></td>
<td>

<p>an optional numeric vector of evaluation data lying in <code class="reqn">[a,b]</code>
</p>
</td></tr>
<tr><td><code id="npuniden.boundary_+3A_h">h</code></td>
<td>

<p>an optional bandwidth (&gt;0)
</p>
</td></tr>
<tr><td><code id="npuniden.boundary_+3A_a">a</code></td>
<td>

<p>an optional lower bound (defaults to lower bound of empirical support <code class="reqn">min(X)</code>)
</p>
</td></tr>
<tr><td><code id="npuniden.boundary_+3A_b">b</code></td>
<td>

<p>an optional upper bound (defaults to upper bound of empirical support <code class="reqn">max(X)</code>)
</p>
</td></tr>
<tr><td><code id="npuniden.boundary_+3A_bwmethod">bwmethod</code></td>
<td>

<p>whether to conduct bandwidth search via least squares cross-validation
(<code>"cv.ls"</code>) or likelihood cross-validation (<code>"cv.ml"</code>)
</p>
</td></tr>
<tr><td><code id="npuniden.boundary_+3A_cv">cv</code></td>
<td>

<p>an optional argument for search (default is likely more reliable in the
presence of local maxima)
</p>
</td></tr>
<tr><td><code id="npuniden.boundary_+3A_grid">grid</code></td>
<td>

<p>an optional grid used for the initial grid search when <code>cv="grid-hybrid"</code>
</p>
</td></tr>
<tr><td><code id="npuniden.boundary_+3A_kertype">kertype</code></td>
<td>

<p>an optional kernel specification (defaults to &quot;gaussian1&quot;)
</p>
</td></tr>
<tr><td><code id="npuniden.boundary_+3A_nmulti">nmulti</code></td>
<td>

<p>number of multi-starts used when <code>cv="numeric"</code> (defaults to 5)
</p>
</td></tr>
<tr><td><code id="npuniden.boundary_+3A_proper">proper</code></td>
<td>

<p>an optional logical value indicating whether to enforce proper density
and distribution function estimates over the range <code class="reqn">[a,b]</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Typical usages are (see below for a complete list of options and also
the examples at the end of this help file)
</p>
<pre>
    model &lt;- npuniden.boundary(X,a=-2,b=3)
  </pre>
<p><code>npuniden.boundary</code> implements a variety of methods for
estimating a univariate density function defined over a continuous
random variable in the presence of bounds via the use of so-called
boundary or edge kernel functions.
</p>
<p>The kernel functions <code>"beta1"</code> and <code>"beta2"</code> are Chen's
(1999) type 1 and 2 kernel functions with biases of <code class="reqn">O(h)</code>, the
<code>"gamma"</code> kernel function is from Chen (2000) with a bias of
<code class="reqn">O(h)</code>, <code>"rigaussian"</code> is the reciprocal inverse Gaussian
kernel function (Scaillet (2004), Igarashi &amp; Kakizawa (2014)) with
bias of <code class="reqn">O(h)</code>, and <code>"gaussian1"</code> and <code>"gaussian2"</code> are
truncated Gaussian kernel functions with biases of <code class="reqn">O(h)</code> and
<code class="reqn">O(h^2)</code>, respectively. The kernel functions <code>"fb"</code>,
<code>"fbl"</code> and <code>"fbu"</code> are floating boundary polynomial
biweight kernels with biases of <code class="reqn">O(h^2)</code> (Scott (1992), Page
146). Without exception, these kernel functions are asymmetric in
general with shape that changes depending on where the density is
being estimated (i.e., how close the estimation point <code class="reqn">x</code> in
<code class="reqn">\hat f(x)</code> is to a boundary). This function is written purely in
R, so to see the exact form for each of these kernel functions, simply
enter the name of this function in R (i.e., enter
<code>npuniden.boundary</code> after loading this package) and scroll up for
their definitions.
</p>
<p>The kernel functions <code>"gamma"</code>, <code>"rigaussian"</code>, and
<code>"fbl"</code> have support <code class="reqn">[a,\infty]</code>. The kernel function
<code>"fbu"</code> has support <code class="reqn">[-\infty,b]</code>. The rest have support on
<code class="reqn">[a,b]</code>.  Note that the two sided support default values are
<code>a=0</code> and <code>b=1</code>. When estimating a variable on
<code class="reqn">[0,\infty)</code> the default lower bound can be used but when
estimating a variable on <code class="reqn">(-\infty,0]</code> you must manually set the
upper bound to <code>b=0</code>.
</p>
<p>Note that data-driven bandwidth selection is more nuanced in bounded
settings, therefore it would be prudent to manually select a bandwidth
that is, say, 1/25th of the range of the data and manually inspect the
estimate (say <code>h=0.05</code> when <code class="reqn">X\in [0,1]</code>). Also, it may be
wise to compare the density estimate with that from a histogram with
the option <code>breaks=25</code>. Note also that the kernel functions
<code>"gaussian2"</code>, <code>"fb"</code>, <code>"fbl"</code> and <code>"fbu"</code> can
assume negative values leading to potentially negative density
estimates, and must be trimmed when conducting likelihood
cross-validation which can lead to oversmoothing. Least squares
cross-validation is unaffected and appears to be more reliable in such
instances hence is the default here.
</p>
<p>Scott (1992, Page 149) writes &ldquo;While boundary kernels can be
very useful, there are potentially serious problems with real
data. There are an infinite number of boundary kernels reflecting the
spectrum of possible design constraints, and these kernels are not
interchangeable. Severe artifacts can be introduced by any one of them
in inappropriate situations. Very careful examination is required to
avoid being victimized by the particular boundary kernel
chosen. Artifacts can unfortunately be introduced by the choice of the
support interval for the boundary kernel.&rdquo;
</p>
<p>Note that since some kernel functions can assume negative values, this
can lead to improper density estimates. The estimated distribution
function is obtained via numerical integration of the estimated
density function and may itself not be proper even when evaluated on
the full range of the data <code class="reqn">[a,b]</code>. Setting the option
<code>proper=TRUE</code> will render the density and distribution estimates
proper over the full range of the data, though this may not in
general be a mean square error optimal strategy.
</p>
<p>Finally, note that this function is pretty bare-bones relative to
other functions in this package. For one, at this time there is no
automatic print support so kindly see the examples for illustrations
of its use, among other differences.
</p>


<h3>Value</h3>

<p><code>npuniden.boundary</code> returns the following components:
</p>
<table>
<tr><td><code>f</code></td>
<td>
<p> estimated density at the points X </p>
</td></tr>
<tr><td><code>F</code></td>
<td>
<p> estimated distribution at the points X (numeric integral of f) </p>
</td></tr>
<tr><td><code>sd.f</code></td>
<td>
<p> asymptotic standard error of the estimated density at the points X </p>
</td></tr>
<tr><td><code>sd.F</code></td>
<td>
<p> asymptotic standard error of the estimated distribution at the points X </p>
</td></tr>
<tr><td><code>h</code></td>
<td>
<p> bandwidth used </p>
</td></tr>
<tr><td><code>nmulti</code></td>
<td>
<p> number of multi-starts used </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Bouezmarni, T. and Rolin, J.-M. (2003). &ldquo;Consistency of the
beta kernel density function estimator,&rdquo;  The Canadian Journal of
Statistics / La Revue Canadienne de Statistique, 31(1):89-98.
</p>
<p>Chen, S. X. (1999). &ldquo;Beta kernel estimators for density
functions,&rdquo; Computational Statistics &amp; Data Analysis, 31(2):131-145.
</p>
<p>Chen, S. X. (2000). &ldquo;Probability density function estimation
using gamma kernels,&rdquo; Annals of the Institute of Statistical
Mathematics, 52(3):471-480.
</p>
<p>Diggle, P. (1985). &ldquo;A kernel method for smoothing point process
data,&rdquo; Journal of the Royal Statistical Society. Series C (Applied
Statistics), 34(2):138-147.
</p>
<p>Igarashi, G. and Y. Kakizawa (2014). &ldquo;Re-formulation of inverse
Gaussian, reciprocal inverse Gaussian, and Birnbaum-Saunders kernel
estimators,&rdquo; Statistics &amp; Probability Letters, 84:235-246.
</p>
<p>Igarashi, G. and Y. Kakizawa (2015). &ldquo;Bias corrections for some
asymmetric kernel estimators,&rdquo; Journal of Statistical Planning and
Inference, 159:37-63.
</p>
<p>Igarashi, G. (2016). &ldquo;Bias reductions for beta kernel
estimation,&rdquo; Journal of Nonparametric Statistics, 28(1):1-30.
</p>
<p>Scaillet, O. (2004). &ldquo;Density estimation using inverse and
reciprocal inverse Gaussian kernels,&rdquo; Journal of Nonparametric
Statistics, 16(1-2):217-226.
</p>
<p>Scott, D. W. (1992). &ldquo;Multivariate density estimation: Theory,
practice, and visualization,&rdquo; New York: Wiley.
</p>
<p>Zhang, S. and R. J. Karunamuni (2010). &ldquo;Boundary performance of
the beta kernel estimators,&rdquo; Journal of Nonparametric Statistics,
22(1):81-104.
</p>


<h3>See Also</h3>

<p>The <span class="pkg">Ake</span>, <span class="pkg">bde</span>, and <span class="pkg">Conake</span> packages and the function <code><a href="#topic+npuniden.reflect">npuniden.reflect</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Example 1: f(0)=0, f(1)=1, plot boundary corrected density,
## unadjusted density, and DGP
set.seed(42)
n &lt;- 100
X &lt;- sort(rbeta(n,5,1))
dgp &lt;- dbeta(X,5,1)
model.g1 &lt;- npuniden.boundary(X,kertype="gaussian1")
model.g2 &lt;- npuniden.boundary(X,kertype="gaussian2")
model.b1 &lt;- npuniden.boundary(X,kertype="beta1")
model.b2 &lt;- npuniden.boundary(X,kertype="beta2")
model.fb &lt;- npuniden.boundary(X,kertype="fb")
model.unadjusted &lt;- npuniden.boundary(X,a=-Inf,b=Inf)
ylim &lt;- c(0,max(c(dgp,model.g1$f,model.g2$f,model.b1$f,model.b2$f,model.fb$f)))
plot(X,dgp,ylab="Density",ylim=ylim,type="l")
lines(X,model.g1$f,lty=2,col=2)
lines(X,model.g2$f,lty=3,col=3)
lines(X,model.b1$f,lty=4,col=4)
lines(X,model.b2$f,lty=5,col=5)
lines(X,model.fb$f,lty=6,col=6)
lines(X,model.unadjusted$f,lty=7,col=7)
rug(X)
legend("topleft",c("DGP",
                   "Boundary Kernel (gaussian1)",
                   "Boundary Kernel (gaussian2)",
                   "Boundary Kernel (beta1)",
                   "Boundary Kernel (beta2)",
                   "Boundary Kernel (floating boundary)",
                   "Unadjusted"),col=1:7,lty=1:7,bty="n")

## Example 2: f(0)=0, f(1)=0, plot density, distribution, DGP, and
## asymptotic point-wise confidence intervals
set.seed(42)
X &lt;- sort(rbeta(100,5,3))
model &lt;- npuniden.boundary(X)
par(mfrow=c(1,2))
ylim=range(c(model$f,model$f+1.96*model$sd.f,model$f-1.96*model$sd.f,dbeta(X,5,3)))
plot(X,model$f,ylim=ylim,ylab="Density",type="l",)
lines(X,model$f+1.96*model$sd.f,lty=2)
lines(X,model$f-1.96*model$sd.f,lty=2)
lines(X,dbeta(X,5,3),col=2)
rug(X)
legend("topleft",c("Density","DGP"),lty=c(1,1),col=1:2,bty="n")

plot(X,model$F,ylab="Distribution",type="l")
lines(X,model$F+1.96*model$sd.F,lty=2)
lines(X,model$F-1.96*model$sd.F,lty=2)
lines(X,pbeta(X,5,3),col=2)
rug(X)
legend("topleft",c("Distribution","DGP"),lty=c(1,1),col=1:2,bty="n")

## Example 3: Age for working age males in the cps71 data set bounded
## below by 21 and above by 65
data(cps71)
attach(cps71)
model &lt;- npuniden.boundary(age,a=21,b=65)
par(mfrow=c(1,1))
hist(age,prob=TRUE,main="")
lines(age,model$f)
lines(density(age,bw=model$h),col=2)
legend("topright",c("Boundary Kernel","Unadjusted"),lty=c(1,1),col=1:2,bty="n")
detach(cps71)

## End(Not run) 
</code></pre>

<hr>
<h2 id='npuniden.reflect'> Kernel Bounded Univariate Density Estimation Via Data-Reflection </h2><span id='topic+npuniden.reflect'></span>

<h3>Description</h3>

<p><code>npuniden.reflect</code> computes kernel univariate unconditional
density estimates given a vector of continuously distributed training
data and, optionally, a bandwidth (otherwise likelihood
cross-validation is used for its selection). Lower and upper bounds
[<code>a</code>,<code>b</code>] can be supplied (default is [0,1]) and if <code>a</code>
is set to <code>-Inf</code> there is only one bound on the right, while if
<code>b</code> is set to <code>Inf</code> there is only one bound on the left.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npuniden.reflect(X = NULL,
                 Y = NULL,
                 h = NULL,
                 a = 0,
                 b = 1,
                 ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npuniden.reflect_+3A_x">X</code></td>
<td>

<p>a required numeric vector of training data lying in <code class="reqn">[a,b]</code>
</p>
</td></tr>
<tr><td><code id="npuniden.reflect_+3A_y">Y</code></td>
<td>

<p>an optional numeric vector of evaluation data lying in <code class="reqn">[a,b]</code>
</p>
</td></tr>
<tr><td><code id="npuniden.reflect_+3A_h">h</code></td>
<td>

<p>an optional bandwidth (&gt;0)
</p>
</td></tr>
<tr><td><code id="npuniden.reflect_+3A_a">a</code></td>
<td>

<p>an optional lower bound (defaults to 0)
</p>
</td></tr>
<tr><td><code id="npuniden.reflect_+3A_b">b</code></td>
<td>

<p>an optional upper bound (defaults to 1)
</p>
</td></tr>
<tr><td><code id="npuniden.reflect_+3A_...">...</code></td>
<td>

<p>optional arguments passed to <code>npudensbw</code> and <code>npudens</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Typical usages are (see below for a complete list of options and also
the examples at the end of this help file)
</p>
<pre>
    model &lt;- npuniden.reflect(X,a=-2,b=3)
  </pre>
<p><code>npuniden.reflect</code> implements the data-reflection method for
estimating a univariate density function defined over a continuous
random variable in the presence of bounds.
</p>
<p>Note that data-reflection imposes a zero derivative at the boundary,
i.e., <code class="reqn">f'(a)=f'(b)=0</code>.
</p>


<h3>Value</h3>

<p><code>npuniden.reflect</code> returns the following components:
</p>
<table>
<tr><td><code>f</code></td>
<td>
<p> estimated density at the points X </p>
</td></tr>
<tr><td><code>F</code></td>
<td>
<p> estimated distribution at the points X (numeric integral of f) </p>
</td></tr>
<tr><td><code>sd.f</code></td>
<td>
<p> asymptotic standard error of the estimated density at the points X </p>
</td></tr>
<tr><td><code>sd.F</code></td>
<td>
<p> asymptotic standard error of the estimated distribution at the points X </p>
</td></tr>
<tr><td><code>h</code></td>
<td>
<p> bandwidth used </p>
</td></tr>
<tr><td><code>nmulti</code></td>
<td>
<p> number of multi-starts used </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Boneva, L. I., Kendall, D., and Stefanov, I. (1971). &ldquo;Spline
transformations: Three new diagnostic aids for the statistical data-
analyst,&rdquo; Journal of the Royal Statistical Society. Series B
(Methodological), 33(1):1-71.
</p>
<p>Cline, D. B. H. and Hart, J. D. (1991). &ldquo;Kernel estimation of
densities with discontinuities or discontinuous
derivatives,&rdquo; Statistics, 22(1):69-84.
</p>
<p>Hall, P. and Wehrly, T. E. (1991). &ldquo;A geometrical method for
removing edge effects from kernel- type nonparametric regression
estimators,&rdquo; Journal of the American Statistical Association,
86(415):665-672.
</p>


<h3>See Also</h3>

<p>The <span class="pkg">Ake</span>, <span class="pkg">bde</span>, and <span class="pkg">Conake</span> packages and the function <code><a href="#topic+npuniden.boundary">npuniden.boundary</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## Example 1: f(0)=0, f(1)=1, plot boundary corrected density,
## unadjusted density, and DGP
set.seed(42)
n &lt;- 100
X &lt;- sort(rbeta(n,5,1))
dgp &lt;- dbeta(X,5,1)
model &lt;- npuniden.reflect(X)
model.unadjusted &lt;- npuniden.boundary(X,a=-Inf,b=Inf)
ylim &lt;- c(0,max(c(dgp,model$f,model.unadjusted$f)))
plot(X,model$f,ylab="Density",ylim=ylim,type="l")
lines(X,model.unadjusted$f,lty=2,col=2)
lines(X,dgp,lty=3,col=3)
rug(X)
legend("topleft",c("Data-Reflection","Unadjusted","DGP"),col=1:3,lty=1:3,bty="n")

## Example 2: f(0)=0, f(1)=0, plot density, distribution, DGP, and
## asymptotic point-wise confidence intervals
set.seed(42)
X &lt;- sort(rbeta(100,5,3))
model &lt;- npuniden.reflect(X)
par(mfrow=c(1,2))
ylim=range(c(model$f,model$f+1.96*model$sd.f,model$f-1.96*model$sd.f,dbeta(X,5,3)))
plot(X,model$f,ylim=ylim,ylab="Density",type="l",)
lines(X,model$f+1.96*model$sd.f,lty=2)
lines(X,model$f-1.96*model$sd.f,lty=2)
lines(X,dbeta(X,5,3),col=2)
rug(X)
legend("topleft",c("Density","DGP"),lty=c(1,1),col=1:2,bty="n")

plot(X,model$F,ylab="Distribution",type="l")
lines(X,model$F+1.96*model$sd.F,lty=2)
lines(X,model$F-1.96*model$sd.F,lty=2)
lines(X,pbeta(X,5,3),col=2)
rug(X)
legend("topleft",c("Distribution","DGP"),lty=c(1,1),col=1:2,bty="n")


## Example 3: Age for working age males in the cps71 data set bounded
## below by 21 and above by 65
data(cps71)
attach(cps71)
model &lt;- npuniden.reflect(age,a=21,b=65)
par(mfrow=c(1,1))
hist(age,prob=TRUE,main="",ylim=c(0,max(model$f)))
lines(age,model$f)
lines(density(age,bw=model$h),col=2)
legend("topright",c("Data-Reflection","Unadjusted"),lty=c(1,1),col=1:2,bty="n")
detach(cps71)

## End(Not run) 
</code></pre>

<hr>
<h2 id='npuniden.sc'>
Kernel Shape Constrained Bounded Univariate Density Estimation
</h2><span id='topic+npuniden.sc'></span>

<h3>Description</h3>

<p><code>npuniden.sc</code> computes shape constrained kernel univariate
unconditional density estimates given a vector of continuously
distributed training data and a bandwidth. Lower and upper bounds
[<code>a</code>,<code>b</code>] can be supplied (default is [0,1]) and if <code>a</code>
is set to <code>-Inf</code> there is only one bound on the right, while if
<code>b</code> is set to <code>Inf</code> there is only one bound on the left.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npuniden.sc(X = NULL,
            Y = NULL,
            h = NULL,
            a = 0,
            b = 1,
            lb = NULL,
            ub = NULL,
            extend.range = 0,
            num.grid = 0,
            function.distance = TRUE,
            integral.equal = FALSE,
            constraint = c("density",
                           "mono.incr",
                           "mono.decr",
                           "concave",
                           "convex",
                           "log-concave",
                           "log-convex"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npuniden.sc_+3A_x">X</code></td>
<td>

<p>a required numeric vector of training data lying in <code class="reqn">[a,b]</code>
</p>
</td></tr>
<tr><td><code id="npuniden.sc_+3A_y">Y</code></td>
<td>

<p>an optional numeric vector of evaluation data lying in <code class="reqn">[a,b]</code>
</p>
</td></tr>
<tr><td><code id="npuniden.sc_+3A_h">h</code></td>
<td>

<p>a bandwidth (<code class="reqn">&gt;0</code>)
</p>
</td></tr>
<tr><td><code id="npuniden.sc_+3A_a">a</code></td>
<td>

<p>an optional lower bound on the support of <code>X</code> or <code>Y</code> (defaults to 0)
</p>
</td></tr>
<tr><td><code id="npuniden.sc_+3A_b">b</code></td>
<td>

<p>an optional upper bound on the support of <code>X</code> or <code>Y</code> (defaults to 1)
</p>
</td></tr>
<tr><td><code id="npuniden.sc_+3A_lb">lb</code></td>
<td>

<p>a scalar lower bound (<code class="reqn">\ge 0</code>) to be used in conjunction with
<code>constraint="density"</code>
</p>
</td></tr>
<tr><td><code id="npuniden.sc_+3A_ub">ub</code></td>
<td>

<p>a scalar upper bound (<code class="reqn">\ge 0</code> and <code class="reqn">\ge</code> <code>lb</code>) to be used
in conjunction with <code>constraint="density"</code>
</p>
</td></tr>
<tr><td><code id="npuniden.sc_+3A_extend.range">extend.range</code></td>
<td>

<p>number specifying the fraction by which the range of the training data
should be extended for the additional grid points (passed to the
function <code>extendrange</code>)
</p>
</td></tr>
<tr><td><code id="npuniden.sc_+3A_num.grid">num.grid</code></td>
<td>

<p>number of additional grid points (in addition to <code>X</code> and
<code>Y</code>) placed on an equi-spaced grid spanning
<code>extendrange(c(X,Y),f=extend.range)</code> (if <code>num.grid=0</code> no
additional grid points will be used regardless of the value of
<code>extend.range</code>)
</p>
</td></tr>
<tr><td><code id="npuniden.sc_+3A_function.distance">function.distance</code></td>
<td>

<p>a logical value that, if <code>TRUE</code>, minimizes the squared deviation between
the constrained and unconstrained estimates, otherwise, minimizes the
squared deviation between the constrained and unconstrained weights
</p>
</td></tr>
<tr><td><code id="npuniden.sc_+3A_integral.equal">integral.equal</code></td>
<td>

<p>a logical value, that, if <code>TRUE</code>, adjusts the constrained estimate
to have the same probability mass over the range <code>X</code>, <code>Y</code>,
and the additional grid points
</p>
</td></tr>
<tr><td><code id="npuniden.sc_+3A_constraint">constraint</code></td>
<td>

<p>a character string indicating whether the estimate is to be
constrained to be monotonically increasing
(<code>constraint="mono.incr"</code>), decreasing
(<code>constraint="mono.incr"</code>), convex (<code>constraint="convex"</code>),
concave (<code>constraint="concave"</code>), log-convex
(<code>constraint="log-convex"</code>), or log-concave
(<code>constraint="log-concave"</code>)
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Typical usages are (see below for a complete list of options and also
the examples at the end of this help file)
</p>
<pre>
    model &lt;- npuniden.sc(X,a=-2,b=3)
  </pre>
<p><code>npuniden.sc</code> implements a methods for estimating a univariate
density function defined over a continuous random variable in the
presence of bounds subject to a variety of shape constraints.  The
bounded estimates use the truncated Gaussian kernel function.
</p>
<p>Note that for the log-constrained estimates, the derivative estimate
returned is that for the log-constrained estimate not the non-log
value of the estimate returned by the function. See Example 5 below
hat manually plots the log-density and returned derivative (no
transformation is needed when plotting the density estimate itself).
</p>
<p>If the quadratic program solver fails to find a solution, the
unconstrained estimate is returned with an immediate warning. Possible
causes to be investigated are undersmoothing, sparsity, and the
presence of non-sample grid points. To investigate the possibility of
undersmoothing try using a larger bandwidth, to investigate sparsity
try decreasing <code>extend.range</code>, and to investigate non-sample grid
points try setting <code>num.grid</code> to <code>0</code>.
</p>
<p>Mean square error performance seems to improve generally when using
additional grid points in the empirical support of <code>X</code> and
<code>Y</code> (i.e., in the observed range of the data sample) but appears
to deteriorate when imposing constraints beyond the empirical support
(i.e., when <code>extend.range</code> is positive). Increasing the number of
additional points beyond a hundred or so appears to have a limited
impact.
</p>
<p>The option <code>function.distance=TRUE</code> appears to perform better for
imposing convexity, concavity, log-convexity and log-concavity, while
<code>function.distance=FALSE</code> appears to perform better for imposing
monotonicity, whether increasing or decreasing (based on simulations
for the Beta(s1,s2) distribution with sample size <code class="reqn">n=100</code>).
</p>


<h3>Value</h3>

<p>A list with the following elements:
</p>
<table>
<tr><td><code>f</code></td>
<td>
<p>unconstrained density estimate</p>
</td></tr>
<tr><td><code>f.sc</code></td>
<td>
<p>shape constrained density estimate</p>
</td></tr>
<tr><td><code>se.f</code></td>
<td>
<p>asymptotic standard error of the unconstrained density estimate</p>
</td></tr>
<tr><td><code>se.f.sc</code></td>
<td>
<p>asymptotic standard error of the shape constrained density estimate</p>
</td></tr>
<tr><td><code>f.deriv</code></td>
<td>
<p>unconstrained derivative estimate (of order 1 or 2 or log thereof)</p>
</td></tr>
<tr><td><code>f.sc.deriv</code></td>
<td>
<p>shape constrained derivative estimate (of order 1 or 2 or log thereof)</p>
</td></tr>
<tr><td><code>F</code></td>
<td>
<p>unconstrained distribution estimate</p>
</td></tr>
<tr><td><code>F.sc</code></td>
<td>
<p>shape constrained distribution estimate</p>
</td></tr>
<tr><td><code>integral.f</code></td>
<td>
<p>the integral of the unconstrained estimate over <code>X</code>, <code>Y</code>, and the additional grid points</p>
</td></tr>
<tr><td><code>integral.f.sc</code></td>
<td>
<p>the integral of the constrained estimate over <code>X</code>, <code>Y</code>, and the additional grid points</p>
</td></tr>
<tr><td><code>solve.QP</code></td>
<td>
<p>logical, if <code>TRUE</code> <code>solve.QP</code> succeeded, otherwise failed</p>
</td></tr>
<tr><td><code>attempts</code></td>
<td>
<p>number of attempts when <code>solve.QP</code> fails (max = 9)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Du, P. and C. Parmeter and J. Racine, &ldquo;Shape Constrained Kernel Density Estimation&rdquo;, manuscript.
</p>


<h3>See Also</h3>

<p>The <span class="pkg">logcondens</span>, <span class="pkg">LogConDEAD</span>, and <span class="pkg">scdensity</span> packages,
and the function <code><a href="#topic+npuniden.boundary">npuniden.boundary</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
n &lt;- 100
set.seed(42)

## Example 1: N(0,1), constrain the density to lie within lb=.1 and ub=.2

X &lt;- sort(rnorm(n))
h &lt;- npuniden.boundary(X,a=-Inf,b=Inf)$h
foo &lt;- npuniden.sc(X,h=h,constraint="density",a=-Inf,b=Inf,lb=.1,ub=.2)
ylim &lt;- range(c(foo$f.sc,foo$f))
plot(X,foo$f.sc,type="l",ylim=ylim,xlab="X",ylab="Density")
lines(X,foo$f,col=2,lty=2)
rug(X)
legend("topleft",c("Constrained","Unconstrained"),lty=1:2,col=1:2,bty="n")

## Example 2: Beta(5,1), DGP is monotone increasing, impose valid
## restriction

X &lt;- sort(rbeta(n,5,1))
h &lt;- npuniden.boundary(X)$h

foo &lt;- npuniden.sc(X=X,h=h,constraint=c("mono.incr"))

par(mfrow=c(1,2))
ylim &lt;- range(c(foo$f.sc,foo$f))
plot(X,foo$f.sc,type="l",ylim=ylim,xlab="X",ylab="Density")
lines(X,foo$f,col=2,lty=2)
rug(X)
legend("topleft",c("Constrained","Unconstrained"),lty=1:2,col=1:2,bty="n")

ylim &lt;- range(c(foo$f.sc.deriv,foo$f.deriv))
plot(X,foo$f.sc.deriv,type="l",ylim=ylim,xlab="X",ylab="First Derivative")
lines(X,foo$f.deriv,col=2,lty=2)
abline(h=0,lty=2)
rug(X)
legend("topleft",c("Constrained","Unconstrained"),lty=1:2,col=1:2,bty="n")

## Example 3: Beta(1,5), DGP is monotone decreasing, impose valid
## restriction

X &lt;- sort(rbeta(n,1,5))
h &lt;- npuniden.boundary(X)$h

foo &lt;- npuniden.sc(X=X,h=h,constraint=c("mono.decr"))

par(mfrow=c(1,2))
ylim &lt;- range(c(foo$f.sc,foo$f))
plot(X,foo$f.sc,type="l",ylim=ylim,xlab="X",ylab="Density")
lines(X,foo$f,col=2,lty=2)
rug(X)
legend("topleft",c("Constrained","Unconstrained"),lty=1:2,col=1:2,bty="n")

ylim &lt;- range(c(foo$f.sc.deriv,foo$f.deriv))
plot(X,foo$f.sc.deriv,type="l",ylim=ylim,xlab="X",ylab="First Derivative")
lines(X,foo$f.deriv,col=2,lty=2)
abline(h=0,lty=2)
rug(X)
legend("topleft",c("Constrained","Unconstrained"),lty=1:2,col=1:2,bty="n")
 
## Example 4: N(0,1), DGP is log-concave, impose invalid concavity
## restriction

X &lt;- sort(rnorm(n))
h &lt;- npuniden.boundary(X,a=-Inf,b=Inf)$h

foo &lt;- npuniden.sc(X=X,h=h,a=-Inf,b=Inf,constraint=c("concave"))

par(mfrow=c(1,2))
ylim &lt;- range(c(foo$f.sc,foo$f))
plot(X,foo$f.sc,type="l",ylim=ylim,xlab="X",ylab="Density")
lines(X,foo$f,col=2,lty=2)
rug(X)
legend("topleft",c("Constrained","Unconstrained"),lty=1:2,col=1:2,bty="n")
ylim &lt;- range(c(foo$f.sc.deriv,foo$f.deriv))

plot(X,foo$f.sc.deriv,type="l",ylim=ylim,xlab="X",ylab="Second Derivative")
lines(X,foo$f.deriv,col=2,lty=2)
abline(h=0,lty=2)
rug(X)
legend("topleft",c("Constrained","Unconstrained"),lty=1:2,col=1:2,bty="n")

## Example 45: Beta(3/4,3/4), DGP is convex, impose valid restriction

X &lt;- sort(rbeta(n,3/4,3/4))
h &lt;- npuniden.boundary(X)$h

foo &lt;- npuniden.sc(X=X,h=h,constraint=c("convex"))

par(mfrow=c(1,2))
ylim &lt;- range(c(foo$f.sc,foo$f))
plot(X,foo$f.sc,type="l",ylim=ylim,xlab="X",ylab="Density")
lines(X,foo$f,col=2,lty=2)
rug(X)
legend("topleft",c("Constrained","Unconstrained"),lty=1:2,col=1:2,bty="n")

ylim &lt;- range(c(foo$f.sc.deriv,foo$f.deriv))
plot(X,foo$f.sc.deriv,type="l",ylim=ylim,xlab="X",ylab="Second Derivative")
lines(X,foo$f.deriv,col=2,lty=2)
abline(h=0,lty=2)
rug(X)
legend("topleft",c("Constrained","Unconstrained"),lty=1:2,col=1:2,bty="n")

## Example 6: N(0,1), DGP is log-concave, impose log-concavity
## restriction

X &lt;- sort(rnorm(n))
h &lt;- npuniden.boundary(X,a=-Inf,b=Inf)$h

foo &lt;- npuniden.sc(X=X,h=h,a=-Inf,b=Inf,constraint=c("log-concave"))

par(mfrow=c(1,2))

ylim &lt;- range(c(log(foo$f.sc),log(foo$f)))
plot(X,log(foo$f.sc),type="l",ylim=ylim,xlab="X",ylab="Log-Density")
lines(X,log(foo$f),col=2,lty=2)
rug(X)
legend("topleft",c("Constrained-log","Unconstrained-log"),lty=1:2,col=1:2,bty="n")

ylim &lt;- range(c(foo$f.sc.deriv,foo$f.deriv))
plot(X,foo$f.sc.deriv,type="l",ylim=ylim,xlab="X",ylab="Second Derivative of Log-Density")
lines(X,foo$f.deriv,col=2,lty=2)
abline(h=0,lty=2)
rug(X)
legend("topleft",c("Constrained-log","Unconstrained-log"),lty=1:2,col=1:2,bty="n")

## End(Not run) 
</code></pre>

<hr>
<h2 id='npunitest'> Kernel Consistent Univariate Density Equality Test with Mixed Data Types</h2><span id='topic+npunitest'></span>

<h3>Description</h3>

<p><code>npunitest</code> implements the consistent metric entropy test of
Maasoumi and Racine (2002) for two arbitrary, stationary
univariate nonparametric densities on common support.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npunitest(data.x = NULL,
          data.y = NULL,
          method = c("integration","summation"),
          bootstrap = TRUE,
          boot.num = 399,
          bw.x = NULL,
          bw.y = NULL,
          random.seed = 42,
          ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npunitest_+3A_data.x">data.x</code>, <code id="npunitest_+3A_data.y">data.y</code></td>
<td>

<p>common support univariate vectors containing the variables.
</p>
</td></tr>
<tr><td><code id="npunitest_+3A_method">method</code></td>
<td>

<p>a character string used to specify whether to compute
the integral version or the summation version of the statistic. Can
be set as <code>integration</code> or <code>summation</code>. Defaults to
<code>integration</code>. See &lsquo;Details&rsquo; below for important information
regarding the use of <code>summation</code> when <code>data.x</code> and
<code>data.y</code> lack common support and/or are sparse.
</p>
</td></tr>
<tr><td><code id="npunitest_+3A_bootstrap">bootstrap</code></td>
<td>

<p>a logical value which specifies whether to conduct the bootstrap
test or not. If set to <code>FALSE</code>, only the statistic will be
computed. Defaults to <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="npunitest_+3A_boot.num">boot.num</code></td>
<td>

<p>an integer value specifying the number of bootstrap
replications to use. Defaults to <code>399</code>.
</p>
</td></tr>
<tr><td><code id="npunitest_+3A_bw.x">bw.x</code>, <code id="npunitest_+3A_bw.y">bw.y</code></td>
<td>

<p>numeric (scalar) bandwidths. Defaults to plug-in (see details below).
</p>
</td></tr>
<tr><td><code id="npunitest_+3A_random.seed">random.seed</code></td>
<td>

<p>an integer used to seed R's random number generator. This is to
ensure replicability. Defaults to 42.
</p>
</td></tr>
<tr><td><code id="npunitest_+3A_...">...</code></td>
<td>
<p> additional arguments supplied to specify the bandwidth
type, kernel types, and so on.  This is used since we specify bw as
a numeric scalar and not a <code>bandwidth</code> object, and is of
interest if you do not desire the default behaviours. To change the
defaults, you may specify any of <code>bwscaling</code>, <code>bwtype</code>,
<code>ckertype</code>, <code>ckerorder</code>, <code>ukertype</code>,
<code>okertype</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>npunitest</code> computes the nonparametric metric entropy (normalized
Hellinger of Granger, Maasoumi and Racine (2004)) for testing
equality of two univariate density/probability functions,
<code class="reqn">D[f(x), f(y)]</code>. See Maasoumi and Racine (2002)
for details. Default bandwidths are of the plug-in variety
(<code><a href="stats.html#topic+bw.SJ">bw.SJ</a></code> for continuous variables and direct plug-in for
discrete variables). The bootstrap is conducted via simple resampling
with replacement from the pooled <code>data.x</code> and <code>data.y</code>
(<code>data.x</code> only for <code>summation</code>).
</p>
<p>The summation version of this statistic can be numerically unstable
when <code>data.x</code> and <code>data.y</code> lack common support or when the
overlap is sparse (the summation version involves division of
densities while the integration version involves differences, and the
statistic in such cases can be reported as exactly 0.5 or 0). Warning
messages are produced when this occurs (&lsquo;integration recommended&rsquo;)
and should be heeded.
</p>
<p>Numerical integration can occasionally fail when the <code>data.x</code>
and <code>data.y</code> distributions lack common support and/or lie an
extremely large distance from one another (the statistic in such
cases will be reported as exactly 0.5 or 0). However, in these
extreme cases, simple tests will reveal the obvious differences in
the distributions and entropy-based tests for equality will be
clearly unnecessary.
</p>


<h3>Value</h3>

<p><code>npunitest</code> returns an object of type <code>unitest</code> with the
following components
</p>
<table>
<tr><td><code>Srho</code></td>
<td>
<p> the statistic <code>Srho</code> </p>
</td></tr>
<tr><td><code>Srho.bootstrap</code></td>
<td>
<p> contains the bootstrap replications of <code>Srho</code> </p>
</td></tr>
<tr><td><code>P</code></td>
<td>
<p> the P-value of the statistic </p>
</td></tr>
<tr><td><code>boot.num</code></td>
<td>
<p> number of bootstrap replications </p>
</td></tr>
<tr><td><code>bw.x</code>, <code>bw.y</code></td>
<td>
<p> scalar bandwidths for <code>data.x, data.y</code> </p>
</td></tr>
</table>
<p><code><a href="base.html#topic+summary">summary</a></code> supports object of type <code>unitest</code>.
</p>


<h3>Usage Issues</h3>

<p>See the example below for proper usage.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine
<a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>References</h3>

<p>Granger, C.W. and E. Maasoumi and J.S. Racine (2004), &ldquo;A
dependence metric for possibly nonlinear processes&rdquo;, Journal of Time
Series Analysis, 25, 649-669.
</p>
<p>Maasoumi, E. and J.S. Racine (2002), &ldquo;Entropy and
predictability of stock market returns,&rdquo; Journal of Econometrics,
107, 2, pp 291-312.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npdeneqtest">npdeneqtest</a>,<a href="#topic+npdeptest">npdeptest</a>,<a href="#topic+npsdeptest">npsdeptest</a>,<a href="#topic+npsymtest">npsymtest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
set.seed(1234)
n &lt;- 1000

## Compute the statistic only for data drawn from same distribution

x &lt;- rnorm(n)
y &lt;- rnorm(n)

npunitest(x,y,bootstrap=FALSE)

Sys.sleep(5)

## Conduct the test for this data

npunitest(x,y,boot.num=99)

Sys.sleep(5)

## Conduct the test for data drawn from different distributions having
## the same mean and variance

x &lt;- rchisq(n,df=5)
y &lt;- rnorm(n,mean=5,sd=sqrt(10))
mean(x)
mean(y)
sd(x)
sd(y)

npunitest(x,y,boot.num=99)

Sys.sleep(5)

## Two sample t-test for equality of means
t.test(x,y)
## F test for equality of variances and asymptotic
## critical values
F &lt;- var(x)/var(y)
qf(c(0.025,0.975),df1=n-1,df2=n-1)

## Plot the nonparametric density estimates on the same axes

fx &lt;- density(x)
fy &lt;- density(y)
xlim &lt;- c(min(fx$x,fy$x),max(fx$x,fy$x))
ylim &lt;- c(min(fx$y,fy$y),max(fx$y,fy$y))
plot(fx,xlim=xlim,ylim=ylim,xlab="Data",main="f(x), f(y)")
lines(fy$x,fy$y,col="red")

Sys.sleep(5)

## Test for equality of log(wage) distributions

data(wage1)
attach(wage1)
lwage.male &lt;- lwage[female=="Male"]
lwage.female &lt;- lwage[female=="Female"]

npunitest(lwage.male,lwage.female,boot.num=99)

Sys.sleep(5)

## Plot the nonparametric density estimates on the same axes

f.m &lt;- density(lwage.male)
f.f &lt;- density(lwage.female)
xlim &lt;- c(min(f.m$x,f.f$x),max(f.m$x,f.f$x))
ylim &lt;- c(min(f.m$y,f.f$y),max(f.m$y,f.f$y))
plot(f.m,xlim=xlim,ylim=ylim,
     xlab="log(wage)",
     main="Male/Female log(wage) Distributions")
lines(f.f$x,f.f$y,col="red",lty=2)
rug(lwage.male)
legend(-1,1.2,c("Male","Female"),lty=c(1,2),col=c("black","red"))

detach(wage1)

Sys.sleep(5)

## Conduct the test for data drawn from different discrete probability
## distributions

x &lt;- factor(rbinom(n,2,.5))
y &lt;- factor(rbinom(n,2,.1))

npunitest(x,y,boot.num=99)

## End(Not run) 
</code></pre>

<hr>
<h2 id='oecdpanel'> Cross Country Growth Panel </h2><span id='topic+oecdpanel'></span><span id='topic+bw'></span>

<h3>Description</h3>

<p>Cross country GDP growth panel covering the period 1960-1995 used by
Liu and Stengos (2000) and Maasoumi, Racine, and Stengos (2007). There
are 616 observations in total. <code>data("oecdpanel")</code> makes available
the dataset <code>"oecdpanel"</code> plus an additional object <code>"bw"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("oecdpanel")</code></pre>


<h3>Format</h3>

<p>A data frame with 7 columns, and 616 rows.  This panel covers 7
5-year periods: 1960-1964, 1965-1969, 1970-1974, 1975-1979, 1980-1984,
1985-1989 and 1990-1994.
</p>
<p>A separate local-linear <code>rbandwidth</code> object (<code>bw</code>) has
been computed for the user's convenience which can be used to
visualize this dataset using <code><a href="base.html#topic+plot">plot</a>(bw)</code>.
</p>

<dl>
<dt>growth</dt><dd><p> the first column, of type <code>numeric</code>: growth rate
of real GDP per capita for each 5-year period</p>
</dd>
<dt>oecd</dt><dd><p> the second column, of type <code>factor</code>: equal to
1 for OECD members, 0 otherwise</p>
</dd>
<dt>year</dt><dd><p> the third column, of type <code>integer</code></p>
</dd>
<dt>initgdp</dt><dd><p> the fourth column, of type <code>numeric</code>: per capita
real GDP at the beginning of each 5-year period</p>
</dd>
<dt>popgro</dt><dd><p> the fifth column, of type <code>numeric</code>: average
annual population growth rate for each 5-year period</p>
</dd>
<dt>inv</dt><dd><p> the sixth column, of type <code>numeric</code>: average
investment/GDP ratio for each 5-year period</p>
</dd>
<dt>humancap</dt><dd><p> the seventh column, of type <code>numeric</code>:
average secondary school enrolment rate for each 5-year period</p>
</dd>
</dl>



<h3>Source</h3>

<p> Thanasis Stengos </p>


<h3>References</h3>

<p>Liu, Z. and T. Stengos (1999), &ldquo;Non-linearities in cross
country growth regressions: a semiparametric approach,&rdquo;
Journal of Applied Econometrics, 14, 527-538.
</p>
<p>Maasoumi, E. and J.S. Racine and T. Stengos (2007),
&ldquo;Growth and convergence: a profile of distribution dynamics and
mobility,&rdquo; Journal of Econometrics, 136, 483-508 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("oecdpanel")
attach(oecdpanel)
summary(oecdpanel)
detach(oecdpanel)
</code></pre>

<hr>
<h2 id='se'>Extract Standard Errors</h2><span id='topic+se'></span>

<h3>Description</h3>

<p><code>se</code> is a generic function which extracts standard errors 
from objects. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>se(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="se_+3A_x">x</code></td>
<td>
<p>an object for which the extraction of standard errors is
meaningful.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function provides a generic interface for extraction of standard
errors from objects.
</p>


<h3>Value</h3>

<p>Standard errors extracted from the model object <code>x</code>.
</p>


<h3>Note</h3>

<p>This method currently only supports objects from the <code><a href="#topic+np">np</a></code> library.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+fitted">fitted</a></code>, <code><a href="stats.html#topic+residuals">residuals</a></code>, <code><a href="stats.html#topic+coef">coef</a></code>,
and <code><a href="#topic+gradients">gradients</a></code>, for related methods;
<code><a href="#topic+np">np</a></code> for supported objects. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(10)
se(npudens(npudensbw(~x)))
</code></pre>

<hr>
<h2 id='uocquantile'>Compute Quantiles</h2><span id='topic+uocquantile'></span>

<h3>Description</h3>

<p><code>uocquantile</code> is a function which computes quantiles of an
unordered, ordered or continuous variable  <code>x</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>uocquantile(x, prob)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="uocquantile_+3A_x">x</code></td>
<td>
<p>an ordered, unordered or continuous variable.</p>
</td></tr>
<tr><td><code id="uocquantile_+3A_prob">prob</code></td>
<td>
<p>quantile to compute.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>uocquantile</code> is a function which computes quantiles of
an unordered, ordered or continuous variable <code>x</code>. If <code>x</code>
is unordered, the mode is returned. If <code>x</code> is ordered, the level
for which the cumulative distribution is &gt;= prob is returned. If
<code>x</code> is continuous, <code><a href="stats.html#topic+quantile">quantile</a></code> is invoked and the
result returned. 
</p>


<h3>Value</h3>

<p>A quantile computed from <code>x</code>.
</p>


<h3>Author(s)</h3>

<p>Tristen Hayfield <a href="mailto:tristen.hayfield@gmail.com">tristen.hayfield@gmail.com</a>, Jeffrey S. Racine <a href="mailto:racinej@mcmaster.ca">racinej@mcmaster.ca</a>
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+quantile">quantile</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rbinom(n = 100, size = 10, prob = 0.5)
uocquantile(x, 0.5)
</code></pre>

<hr>
<h2 id='wage1'> Cross-Sectional Data on Wages </h2><span id='topic+wage1'></span><span id='topic+bw.subset'></span><span id='topic+bw.all'></span>

<h3>Description</h3>

<p> Cross-section wage data consisting of a random sample
taken from the U.S. Current Population Survey for the year 1976. There
are 526 observations in total. <code>data("wage1")</code> makes available the
dataset <code>"wage"</code> plus additional objects <code>"bw.all"</code> and
<code>"bw.subset"</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("wage1")</code></pre>


<h3>Format</h3>

<p> A data frame with 24 columns, and 526 rows.
</p>
<p>Two local-linear <code>rbandwidth</code> objects (<code>bw.all</code> and
<code>bw.subset</code>) have been computed for the user's convenience
which can be used to visualize this dataset using
<code><a href="base.html#topic+plot">plot</a>(bw.all)</code>
</p>

<dl>
<dt>wage</dt><dd><p>column 1, of type <code>numeric</code>, average hourly earnings</p>
</dd>
<dt>educ</dt><dd><p>column 2, of type <code>numeric</code>, years of education</p>
</dd>
<dt>exper</dt><dd><p>column 3, of type <code>numeric</code>, years potential experience</p>
</dd>
<dt>tenure</dt><dd><p>column 4, of type <code>numeric</code>, years with current employer</p>
</dd>
<dt>nonwhite</dt><dd><p>column 5, of type <code>factor</code>, =&ldquo;Nonwhite&rdquo; if nonwhite, &ldquo;White&rdquo; otherwise</p>
</dd>
<dt>female</dt><dd><p>column 6, of type <code>factor</code>, =&ldquo;Female&rdquo; if female, &ldquo;Male&rdquo; otherwise</p>
</dd>
<dt>married</dt><dd><p>column 7, of type <code>factor</code>, =&ldquo;Married&rdquo; if Married, &ldquo;Nonmarried&rdquo; otherwise</p>
</dd>
<dt>numdep</dt><dd><p>column 8, of type <code>numeric</code>, number of dependants</p>
</dd>
<dt>smsa</dt><dd><p>column 9, of type <code>numeric</code>, =1 if live in SMSA</p>
</dd>
<dt>northcen</dt><dd><p>column 10, of type <code>numeric</code>, =1 if live in north central U.S</p>
</dd>
<dt>south</dt><dd><p>column 11, of type <code>numeric</code>, =1 if live in southern region</p>
</dd>
<dt>west</dt><dd><p>column 12, of type <code>numeric</code>, =1 if live in western region</p>
</dd>
<dt>construc</dt><dd><p>column 13, of type <code>numeric</code>, =1 if work in construction industry</p>
</dd>
<dt>ndurman</dt><dd><p>column 14, of type <code>numeric</code>, =1 if in non-durable manufacturing industry</p>
</dd>
<dt>trcommpu</dt><dd><p>column 15, of type <code>numeric</code>, =1 if in transportation, communications, public utility</p>
</dd>
<dt>trade</dt><dd><p>column 16, of type <code>numeric</code>, =1 if in wholesale or retail</p>
</dd>
<dt>services</dt><dd><p>column 17, of type <code>numeric</code>, =1 if in services industry</p>
</dd>
<dt>profserv</dt><dd><p>column 18, of type <code>numeric</code>, =1 if in professional services industry</p>
</dd>
<dt>profocc</dt><dd><p>column 19, of type <code>numeric</code>, =1 if in professional occupation</p>
</dd>
<dt>clerocc</dt><dd><p>column 20, of type <code>numeric</code>, =1 if in clerical occupation</p>
</dd>
<dt>servocc</dt><dd><p>column 21, of type <code>numeric</code>, =1 if in service occupation</p>
</dd>
<dt>lwage</dt><dd><p>column 22, of type <code>numeric</code>, log(wage)</p>
</dd>
<dt>expersq</dt><dd><p>column 23, of type <code>numeric</code>, exper<code class="reqn">^2</code></p>
</dd>
<dt>tenursq</dt><dd><p>column 24, of type <code>numeric</code>, tenure<code class="reqn">^2</code></p>
</dd>
</dl>



<h3>Source</h3>

<p> Jeffrey M. Wooldridge </p>


<h3>References</h3>

<p>Wooldridge, J.M. (2000), <em>Introductory Econometrics: A Modern
Approach</em>, South-Western College Publishing.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("wage1")
attach(wage1)
summary(wage1)
detach(wage1)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
