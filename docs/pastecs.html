<!DOCTYPE html><html><head><title>Help for package pastecs</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {pastecs}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#.gleissberg.table'><p>Table of probabilities according to the Gleissberg distribution</p></a></li>
<li><a href='#abund'><p> Sort variables by abundance</p></a></li>
<li><a href='#AutoD2'><p> AutoD2, CrossD2 or CenterD2 analysis of a multiple time-series</p></a></li>
<li><a href='#bnr'><p> A data frame of 163 benthic species measured across a transect</p></a></li>
<li><a href='#buysbal'><p> Buys-Ballot table</p></a></li>
<li><a href='#daystoyears'><p> Convert time units from &quot;days&quot; to &quot;years&quot; or back</p></a></li>
<li><a href='#decaverage'><p> Time series decomposition using a moving average</p></a></li>
<li><a href='#deccensus'><p> Time decomposition using the CENSUS II method</p></a></li>
<li><a href='#decdiff'><p> Time series decomposition using differences (trend elimination)</p></a></li>
<li><a href='#decevf'><p> Time series decomposition using eigenvector filtering (EVF)</p></a></li>
<li><a href='#decloess'><p> Time series decomposition by the LOESS method</p></a></li>
<li><a href='#decmedian'><p> Time series decomposition using a running median</p></a></li>
<li><a href='#decreg'><p> Time series decomposition using a regression model</p></a></li>
<li><a href='#disjoin'><p> Complete disjoined coded data (binary coding)</p></a></li>
<li><a href='#disto'><p> Compute and plot a distogram</p></a></li>
<li><a href='#escouf'><p> Choose variables using the Escoufier's equivalent vectors method</p></a></li>
<li><a href='#extract'><p> Extract a subset of the original dataset</p></a></li>
<li><a href='#first'><p> Get the first element of a vector</p></a></li>
<li><a href='#GetUnitText'><p> Format a nice time units for labels in graphs</p></a></li>
<li><a href='#is.tseries'><p> Is this object a time series?</p></a></li>
<li><a href='#last'><p> Get the last element of a vector</p></a></li>
<li><a href='#local.trend'><p> Calculate local trends using cumsum</p></a></li>
<li><a href='#marbio'><p> Several zooplankton taxa measured across a transect</p></a></li>
<li><a href='#marphy'><p> Physico-chemical records at the same stations as for marbio</p></a></li>
<li><a href='#match.tol'><p> Determine matching observation with a tolerance in time-scale</p></a></li>
<li><a href='#pennington'><p> Calculate Pennington statistics</p></a></li>
<li><a href='#pgleissberg'><p> Gleissberg distribution probability</p></a></li>
<li><a href='#regarea'><p> Regulate a series using the area method</p></a></li>
<li><a href='#regconst'><p> Regulate a series using the constant value method</p></a></li>
<li><a href='#reglin'><p> Regulation of a series using a linear interpolation</p></a></li>
<li><a href='#regspline'><p> Regulation of a time series using splines</p></a></li>
<li><a href='#regul'><p> Regulation of one or several time series using various methods</p></a></li>
<li><a href='#regul.adj'><p> Adjust regulation parameters</p></a></li>
<li><a href='#regul.screen'><p> Test various regulation parameters</p></a></li>
<li><a href='#releve'><p> A data frame of six phytoplankton taxa followed in time at one station</p></a></li>
<li><a href='#specs'><p> Collect parameters (&quot;specifications&quot;) from one object to use them in another analysis</p></a></li>
<li><a href='#stat.desc'><p> Descriptive statistics on a data frame or time series</p></a></li>
<li><a href='#stat.pen'><p> Pennington statistics on a data frame or time series</p></a></li>
<li><a href='#stat.slide'><p> Sliding statistics</p></a></li>
<li><a href='#trend.test'><p> Test if an increasing or decreasing trend exists in a time series</p></a></li>
<li><a href='#tsd'><p> Decomposition of one or several regular time series using various methods</p></a></li>
<li><a href='#tseries'><p> Convert a 'regul' or a 'tsd'  object into a time series</p></a></li>
<li><a href='#turnogram'><p> Calculate and plot a turnogram for a regular time series</p></a></li>
<li><a href='#turnpoints'><p> Analyze turning points (peaks or pits)</p></a></li>
<li><a href='#vario'><p> Compute and plot a semi-variogram</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Version:</td>
<td>1.4.2</td>
</tr>
<tr>
<td>Title:</td>
<td>Package for Analysis of Space-Time Ecological Series</td>
</tr>
<tr>
<td>Description:</td>
<td>Regularisation, decomposition and analysis of space-time series.
  The pastecs R package is a PNEC-Art4 and IFREMER (Benoit Beliaeff
  &lt;Benoit.Beliaeff@ifremer.fr&gt;) initiative to bring PASSTEC 2000 functionalities to R.</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Philippe Grosjean &lt;phgrosjean@sciviews.org&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>boot, stats, graphics, utils, grDevices</td>
</tr>
<tr>
<td>Suggests:</td>
<td>svUnit, covr, knitr, rmarkdown</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>yes</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/SciViews/pastecs">https://github.com/SciViews/pastecs</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/SciViews/pastecs/issues">https://github.com/SciViews/pastecs/issues</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-01 14:29:37 UTC; phgrosjean</td>
</tr>
<tr>
<td>Author:</td>
<td>Philippe Grosjean <a href="https://orcid.org/0000-0002-2694-9471"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Frederic Ibanez [aut],
  Michele Etienne [ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-01 14:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='.gleissberg.table'>Table of probabilities according to the Gleissberg distribution</h2><span id='topic+.gleissberg.table'></span>

<h3>Description</h3>

<p>The table of probabilities to have 0 to n-2 extrema in a series, for n=3 to 50 (for n &gt; 50, the Gleissberg distribution is approximated with a normal distribution
</p>


<h3>Note</h3>

<p>This dataset is used internally by <code>pgleissberg()</code>. You do not have to access it directly. See <code>pgleissberg()</code> for more information </p>


<h3>See Also</h3>

 <p><code><a href="#topic+pgleissberg">pgleissberg</a></code> </p>

<hr>
<h2 id='abund'> Sort variables by abundance </h2><span id='topic+abund'></span><span id='topic+extract.abund'></span><span id='topic+identify.abund'></span><span id='topic+lines.abund'></span><span id='topic+plot.abund'></span><span id='topic+print.abund'></span><span id='topic+print.summary.abund'></span><span id='topic+summary.abund'></span>

<h3>Description</h3>

<p>Sort variables (usually species in a species x stations matrix) in function of
their abundance, either in number of non-null values, or in number of
individuals (in log). The <code>f</code> coefficient allows adjusting weight given to each of these two criteria.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>abund(x, f = 0.2)

## S3 method for class 'abund'
extract(e, n, left = TRUE, ...)
## S3 method for class 'abund'
identify(x, label.pts = FALSE, lvert = TRUE, lvars = TRUE, col = 2, lty = 2, ...)
## S3 method for class 'abund'
lines(x, n = x$n, lvert = TRUE, lvars = TRUE, col = 2, lty = 2, ...)
## S3 method for class 'abund'
plot(x, n = x$n, lvert = TRUE, lvars = TRUE, lcol = 2, llty = 2, all = TRUE,
    dlab = c("cumsum", "% log(ind.)", "% non-zero"), dcol = c(1,2,4),
    dlty = c(par("lty"), par("lty"), par("lty")), dpos = c(1.5, 20), type = "l",
    xlab = "variables", ylab = "abundance",
    main = paste("Abundance sorting for:",x$data, "with f =", round(x$f, 4)), ...)
## S3 method for class 'abund'
print(x, ...)
## S3 method for class 'summary.abund'
print(x, ...)
## S3 method for class 'abund'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="abund_+3A_x">x</code></td>
<td>
<p> A data frame containing the variables to sort according to their
abundance in columns for <code>abund</code>, or an 'abund' object for the methods </p>
</td></tr>
<tr><td><code id="abund_+3A_f">f</code></td>
<td>
<p> Weight given to the number of individuals criterium (strictly
included between 0 and 1; weight for the non-null values is <code>1-f</code>. The
default value, <code>f=0.2</code>, gives enough weight to the number of non-null
values to get abundant species according to this criterium first, but
allowing to get at the other extreme rare, but locally abundant species </p>
</td></tr>
<tr><td><code id="abund_+3A_object">object</code></td>
<td>
<p> An 'abund' object returned by <code>abund</code> </p>
</td></tr>
<tr><td><code id="abund_+3A_e">e</code></td>
<td>
<p> An 'abund' object returned by <code>abund</code> </p>
</td></tr>
<tr><td><code id="abund_+3A_n">n</code></td>
<td>
<p> The number of variables selected at left </p>
</td></tr>
<tr><td><code id="abund_+3A_type">type</code></td>
<td>
<p> the type of graph to plot. By default, lines with 'l' </p>
</td></tr>
<tr><td><code id="abund_+3A_lvert">lvert</code></td>
<td>
<p> If <code>TRUE</code> then a vertical line separate the n variables at
left from the others </p>
</td></tr>
<tr><td><code id="abund_+3A_lvars">lvars</code></td>
<td>
<p> If <code>TRUE</code> then the x-axis labels of the n left variables
are printed in a different color to emphasize them </p>
</td></tr>
<tr><td><code id="abund_+3A_lcol">lcol</code></td>
<td>
<p> The color to use to draw the vertical line (<code>lvert=TRUE</code>)
and the variables labels (<code>lvars=TRUE</code>) at left af the nth variable.
By default, color 2 is used </p>
</td></tr>
<tr><td><code id="abund_+3A_llty">llty</code></td>
<td>
<p> The style used to draw the vertical line (<code>lvert=TRUE</code>).
By default, a dashed line is used </p>
</td></tr>
<tr><td><code id="abund_+3A_xlab">xlab</code></td>
<td>
<p> the label of the x-axis </p>
</td></tr>
<tr><td><code id="abund_+3A_ylab">ylab</code></td>
<td>
<p> the label of the y-axis </p>
</td></tr>
<tr><td><code id="abund_+3A_main">main</code></td>
<td>
<p> the main title of the graph</p>
</td></tr>
<tr><td><code id="abund_+3A_all">all</code></td>
<td>
<p> If <code>TRUE</code> then all lines are drawn (cumsum, %log(ind.) and
%non-null). If <code>FALSE</code>, only the cumsum line is drawn </p>
</td></tr>
<tr><td><code id="abund_+3A_dlab">dlab</code></td>
<td>
<p> The legend labels </p>
</td></tr>
<tr><td><code id="abund_+3A_dcol">dcol</code></td>
<td>
<p> Colors to use for drawing the various curves on the graph </p>
</td></tr>
<tr><td><code id="abund_+3A_dlty">dlty</code></td>
<td>
<p> The line style to use for drawing the various curves on the graph </p>
</td></tr>
<tr><td><code id="abund_+3A_dpos">dpos</code></td>
<td>
<p> The position of the legend box on the graph (coordinates of its
top-left corner). A legend box is drawn only if <code>all=TRUE</code> </p>
</td></tr>
<tr><td><code id="abund_+3A_col">col</code></td>
<td>
<p> The color to use to draw lines </p>
</td></tr>
<tr><td><code id="abund_+3A_lty">lty</code></td>
<td>
<p> The style used to draw lines </p>
</td></tr>
<tr><td><code id="abund_+3A_...">...</code></td>
<td>
<p> additional parameters </p>
</td></tr>
<tr><td><code id="abund_+3A_label.pts">label.pts</code></td>
<td>
<p> Do we have to label points on the graph or to chose an
extraction level with the <code>identify()</code> method? </p>
</td></tr>
<tr><td><code id="abund_+3A_left">left</code></td>
<td>
<p> If <code>TRUE</code>, the n variables at left are extracted. Otherwise,
the total-n variables at right are extracted </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Successive sorts can be applied. For instance, a first sort with
<code>f = 0.2</code>, followed by an extraction of rare species and another sort
with <code>f = 1</code> allows to collect only rare but locally abundant species.
</p>


<h3>Value</h3>

<p>An object of type 'abund' is returned. It has methods <code>print()</code>,
<code>summary()</code>, <code>plot()</code>, <code>lines()</code>, <code>identify()</code>, <code>extract()</code>.
</p>


<h3>Author(s)</h3>

<p> Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>),
Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>) </p>


<h3>References</h3>

<p>Ibanez, F., J.-C. Dauvin &amp; M. Etienne, 1993. <em>Comparaison des évolutions
à long terme (1977-1990) de deux peuplements macrobenthiques de la baie de
Morlaix (Manche occidentale): relations avec les facteurs hydroclimatiques.</em>
J. Exp. Mar. Biol. Ecol., 169:181-214.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+escouf">escouf</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(bnr)
bnr.abd &lt;- abund(bnr)
summary(bnr.abd)
plot(bnr.abd, dpos=c(105, 100))
bnr.abd$n &lt;- 26
# To identify a point on the graph, use: bnr.abd$n &lt;- identify(bnr.abd)
lines(bnr.abd)
bnr2 &lt;- extract(bnr.abd)
names(bnr2)
</code></pre>

<hr>
<h2 id='AutoD2'> AutoD2, CrossD2 or CenterD2 analysis of a multiple time-series </h2><span id='topic+AutoD2'></span><span id='topic+CrossD2'></span><span id='topic+CenterD2'></span>

<h3>Description</h3>

<p>Compute and plot multiple autocorrelation using Mahalanobis generalized distance D2. AutoD2 uses the same multiple time-series. CrossD2 compares two sets of multiple time-series having same size (same number of descriptors). CenterD2 compares subsamples issued from a single multivariate time-series, aiming to detect discontinuities.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AutoD2(series, lags=c(1, nrow(series)/3), step=1, plotit=TRUE,
        add=FALSE, ...)
CrossD2(series, series2, lags=c(1, nrow(series)/3), step=1,
        plotit=TRUE, add=FALSE, ...)
CenterD2(series, window=nrow(series)/5, plotit=TRUE, add=FALSE,
        type="l", level=0.05, lhorz=TRUE, lcol=2, llty=2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AutoD2_+3A_series">series</code></td>
<td>
<p> regularized multiple time-series </p>
</td></tr>
<tr><td><code id="AutoD2_+3A_series2">series2</code></td>
<td>
<p> a second set of regularized multiple time-series </p>
</td></tr>
<tr><td><code id="AutoD2_+3A_lags">lags</code></td>
<td>
<p> minimal and maximal lag to use. By default, 1 and a third of the number of observations in the series respectively </p>
</td></tr>
<tr><td><code id="AutoD2_+3A_step">step</code></td>
<td>
<p> step between successive lags. By default, 1 </p>
</td></tr>
<tr><td><code id="AutoD2_+3A_window">window</code></td>
<td>
<p> the window to use for CenterD2. By default, a fifth of the total number of observations in the series </p>
</td></tr>
<tr><td><code id="AutoD2_+3A_plotit">plotit</code></td>
<td>
<p> if <code>TRUE</code> then also plot the graph </p>
</td></tr>
<tr><td><code id="AutoD2_+3A_add">add</code></td>
<td>
<p> if <code>TRUE</code> then the graph is added to the current figure </p>
</td></tr>
<tr><td><code id="AutoD2_+3A_type">type</code></td>
<td>
<p> The type of line to draw in the CenterD2 graph. By default, a line without points </p>
</td></tr>
<tr><td><code id="AutoD2_+3A_level">level</code></td>
<td>
<p> The significance level to consider in the CenterD2 analysis. By default 5% </p>
</td></tr>
<tr><td><code id="AutoD2_+3A_lhorz">lhorz</code></td>
<td>
<p> Do we have to plot also the horizontal line representing the significance level on the graph? </p>
</td></tr>
<tr><td><code id="AutoD2_+3A_lcol">lcol</code></td>
<td>
<p> The color of the significance level line. By default, color 2 is used </p>
</td></tr>
<tr><td><code id="AutoD2_+3A_llty">llty</code></td>
<td>
<p> The style for the significance level line. By default: <code>llty=2</code>, a dashed line is drawn</p>
</td></tr>
<tr><td><code id="AutoD2_+3A_...">...</code></td>
<td>
<p> additional graph parameters </p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class 'D2' which contains:
</p>
<table>
<tr><td><code>lag</code></td>
<td>
<p>The vector of lags</p>
</td></tr>
<tr><td><code>D2</code></td>
<td>
<p>The D2 value for this lag</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The command invoked when this function was called</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>The series used</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>The type of 'D2' analysis: 'AutoD2', 'CrossD2' or 'CenterD2'</p>
</td></tr>
<tr><td><code>window</code></td>
<td>
<p>The size of the window used in the CenterD2 analysis</p>
</td></tr>
<tr><td><code>level</code></td>
<td>
<p>The significance level for CenterD2</p>
</td></tr>
<tr><td><code>chisq</code></td>
<td>
<p>The chi-square value corresponding to the significance level in the CenterD2 analysis</p>
</td></tr>
<tr><td><code>units.text</code></td>
<td>
<p>Time units of the series, nicely formatted for graphs</p>
</td></tr>
</table>


<h3>WARNING </h3>

<p>If data are too heterogeneous, results could be biased (a singularity matrix appears in the calculations).</p>


<h3>Author(s)</h3>

<p> Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>), Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>) </p>


<h3>References</h3>

<p>Cooley, W.W. &amp; P.R. Lohnes, 1962. <em>Multivariate procedures for the behavioural sciences.</em> Whiley &amp; sons.
</p>
<p>Dagnélie, P., 1975. <em>Analyse statistique à plusieurs variables.</em> Presses Agronomiques de Gembloux.
</p>
<p>Ibanez, F., 1975. <em>Contribution à l'analyse mathématique des évènements en écologie planctonique: optimisations méthodologiques; étude expérimentale en continu à petite échelle du plancton côtier.</em> Thèse d'état, Paris VI.
</p>
<p>Ibanez, F., 1976. <em>Contribution à l'analyse mathématique des évènements en écologie planctonique. Optimisations méthodologiques.</em> Bull. Inst. Océanogr. Monaco, 72:1-96.
</p>
<p>Ibanez, F., 1981. <em>Immediate detection of heterogeneities in continuous multivariate oceanographic recordings. Application to time series analysis of changes in the bay of Villefranche sur mer.</em> Limnol. Oceanogr., 26:336-349.
</p>
<p>Ibanez, F., 1991. <em>Treatment of the data deriving from the COST 647 project on coastal benthic ecology: The within-site analysis.</em> In: B. Keegan (ed), <em>Space and time series data analysis in coastal benthic ecology</em>, p 5-43.
</p>


<h3>See Also</h3>

 <p><code><a href="stats.html#topic+acf">acf</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(marphy)
marphy.ts &lt;- as.ts(as.matrix(marphy[, 1:3]))
AutoD2(marphy.ts)
marphy.ts2 &lt;- as.ts(as.matrix(marphy[, c(1, 4, 3)]))
CrossD2(marphy.ts, marphy.ts2)
# This is not identical to:
CrossD2(marphy.ts2, marphy.ts)
marphy.d2 &lt;- CenterD2(marphy.ts, window=16)
lines(c(17, 17), c(-1, 15), col=4, lty=2)
lines(c(25, 25), c(-1, 15), col=4, lty=2)
lines(c(30, 30), c(-1, 15), col=4, lty=2)
lines(c(41, 41), c(-1, 15), col=4, lty=2)
lines(c(46, 46), c(-1, 15), col=4, lty=2)
text(c(8.5, 21, 27.5, 35, 43.5, 57), 11, labels=c("Peripheral Zone", "D1",
        "C", "Front", "D2", "Central Zone")) # Labels
time(marphy.ts)[marphy.d2$D2 &gt; marphy.d2$chisq]
</code></pre>

<hr>
<h2 id='bnr'> A data frame of 163 benthic species measured across a transect </h2><span id='topic+bnr'></span>

<h3>Description</h3>

<p>The <code>bnr</code> data frame has 103 rows and 163 columns.
Each column is a separate species observed at least once in one of all stations. Several species are very abundant, other are very rare.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(bnr)</code></pre>


<h3>Source</h3>

<p>Unpublished dataset.
</p>

<hr>
<h2 id='buysbal'> Buys-Ballot table </h2><span id='topic+buysbal'></span>

<h3>Description</h3>

<p>Calculate a Buys-Ballot table from a time-series
</p>


<h3>Usage</h3>

<pre><code class='language-R'>buysbal(x, y=NULL, frequency=12, units="years", datemin=NULL,
        dateformat="m/d/Y", count=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="buysbal_+3A_x">x</code></td>
<td>
<p> Either a vector with time values (in this case, <code>y</code> must be defined), or a regular time-series </p>
</td></tr>
<tr><td><code id="buysbal_+3A_y">y</code></td>
<td>
<p> If <code>x</code> is a vector with time values, <code>y</code> must contain corresponding observations </p>
</td></tr>
<tr><td><code id="buysbal_+3A_frequency">frequency</code></td>
<td>
<p> The frequency of observations per year to use in the Buys-Ballot table. By default <code>frequency=12</code> which corresponds to monthly samples, and the resulting table has 12 column, one per month </p>
</td></tr>
<tr><td><code id="buysbal_+3A_units">units</code></td>
<td>
<p> either <code>"years"</code> (by default), and time is not rescaled, or <code>"days"</code>, and the time is rescaled to &quot;years&quot; with the function <code>daystoyears()</code> </p>
</td></tr>
<tr><td><code id="buysbal_+3A_datemin">datemin</code></td>
<td>
<p> A character string representing the first date, using a format corresponding to <code>dateformat</code>. For instance, with <code>datemin="04/23/1998"</code> and <code>dateformat="m/d/Y"</code>, the first observation is assumed to be made the 23th April 1998. In <span class="rlang"><b>R</b></span>, it can also be a POSIXt date (see <code>?DataTimeClasses</code>). In this case, <code>dateformat</code> is not required and is ignored. By default, <code>datemin=NULL</code> </p>
</td></tr>
<tr><td><code id="buysbal_+3A_dateformat">dateformat</code></td>
<td>
<p> The format used for the date in <code>datemin</code>. For instance, <code>"d/m/Y"</code> or <code>"m/d/Y"</code> (by default). The distinction between &quot;Y&quot; and &quot;y&quot; is not important in Splus, but it is vital in <span class="rlang"><b>R</b></span> to use &quot;y&quot; for two-digit years (ex: 89) and &quot;Y&quot; for four-digits years (ex: 1989), or the date will not be correctly converted! In <span class="rlang"><b>R</b></span>, you can also use a POSIXt format specification like &quot;%d-%m%Y&quot; for instance (see <code>?strptime</code> for a complete description of POSIXt format specification. In both Splus and <span class="rlang"><b>R</b></span>, you can also use &quot;mon&quot; for abbreviated months like &quot;mon d Y&quot; for &quot;Apr 20 1965&quot;, and &quot;month&quot; for fully-spelled months like &quot;d month Y&quot; for &quot;24 September 2003&quot; </p>
</td></tr>
<tr><td><code id="buysbal_+3A_count">count</code></td>
<td>
<p> If <code>FALSE</code> (by default), the Buys-Ballot table is calculated. If <code>TRUE</code>, the function returns only the number of observations that are used in each cell of the Buys-Ballot table </p>
</td></tr>
</table>


<h3>Details</h3>

<p>A Buys-Ballot table summarizes data to highlight seasonal variations. Each line is one year and each column is a period of the year (12 months, 4 quarters, 52 weeks,...). A cell ij of this table contain the mean value for all observations made during the year i at the period j.
</p>


<h3>Value</h3>

<p>A matrix containing either the Buys-Ballot table (<code>count=FALSE</code>), or the number of observations used to build the table (<code>count=TRUE</code>)
</p>


<h3>Author(s)</h3>

<p> Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>), Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>) </p>


<h3>See Also</h3>

 <p><code><a href="#topic+daystoyears">daystoyears</a></code>, <code><a href="#topic+tsd">tsd</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(releve)
buysbal(releve$Day, releve$Melosul, frequency=4, units="days",
        datemin="21/03/1989", dateformat="d/m/Y")
buysbal(releve$Day, releve$Melosul, frequency=4, units="days",
        datemin="21/03/1989", dateformat="d/m/Y", count=TRUE)
</code></pre>

<hr>
<h2 id='daystoyears'> Convert time units from &quot;days&quot; to &quot;years&quot; or back </h2><span id='topic+daystoyears'></span><span id='topic+yearstodays'></span>

<h3>Description</h3>

<p>Convert time scales. The time scale &quot;days&quot; corresponds to 1 unit per day. The time scale &quot;years&quot; uses 1 unit for 1 year. It is used in any analysis that requires seasonal decomposition and/or elimination. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>daystoyears(x, datemin=NULL, dateformat="m/d/Y")
yearstodays(x, xmin=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="daystoyears_+3A_x">x</code></td>
<td>
<p> A vector of time values </p>
</td></tr>
<tr><td><code id="daystoyears_+3A_datemin">datemin</code></td>
<td>
<p> A character string representing the first date, using a format corresponding to <code>dateformat</code>. For instance, with <code>datemin="04/23/1998"</code> and <code>dateformat="m/d/Y"</code>, the first observation is assumed to be made the 23th April 1998. In <span class="rlang"><b>R</b></span>, it can also be a POSIXt date (see <code>?DataTimeClasses</code>). In this case, <code>dateformat</code> is not required and is ignored. By default, <code>datemin=NULL</code> </p>
</td></tr>
<tr><td><code id="daystoyears_+3A_dateformat">dateformat</code></td>
<td>
<p> The format used for the date in <code>datemin</code>. For instance, <code>"d/m/Y"</code> or <code>"m/d/y"</code>. The distinction between &quot;Y&quot; and &quot;y&quot; is not important in Splus, but it is vital in <span class="rlang"><b>R</b></span> to use &quot;y&quot; for two-digit years (ex: 89) and &quot;Y&quot; for four-digits years (ex: 1989), or the date will not be correctly converted! In <span class="rlang"><b>R</b></span>, you can also use a POSIXt format specification like &quot;%d-%m%Y&quot; for instance (see <code>?strptime</code> for a complete description of POSIXt format specification. In both Splus and <span class="rlang"><b>R</b></span>, you can also use &quot;mon&quot; for abbreviated months like &quot;mon d Y&quot; for &quot;Apr 20 1965&quot;, and &quot;month&quot; for fully-spelled months like &quot;d month Y&quot; for &quot;24 September 2003&quot; </p>
</td></tr>
<tr><td><code id="daystoyears_+3A_xmin">xmin</code></td>
<td>
<p> The minimum value for the &quot;days&quot; time-scale </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The &quot;years&quot; time-scale uses one unit for each year. We deliberately &quot;linearized&quot; time in this time-scale and each year is considered to have exactly 365.25 days. There is thus no adjustment for lep years.
Indeed, a small shift (less than one day) is introduced. This could result, for some dates, especially the 31st December, or 1st January of a year to be considered as belonging to the next, or previous year, respectively!
Similarly, one month is considered to be 1/12 year, no mather if it has 28, 29, 30 or 31 days. Thus, the same warning applies: there are shifts in months introduced by this linearization of time!
This representation simplifies further calculations, especially regarding seasonal effects (a quarter is exactly 0.25 units for instance), but shifts introduced in time may or may not be a problem for your particular application
(if exact dates matters, do not use this; if shifts of up to one day is not significant, there is no problem, like when working on long-term biological series with years as units).
Notice that converting it back to &quot;days&quot;, using <code>yearstodays()</code> restablishes exact dates without errors. So, no data is lost, it just a conversion to a simplified (linearized) calendar!
</p>


<h3>Value</h3>

<p>A numerical vector of the same length as <code>x</code> with the converted time-scale
</p>


<h3>Author(s)</h3>

<p> Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>), Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>) </p>


<h3>See Also</h3>

 <p><code><a href="#topic+buysbal">buysbal</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'># A vector with a "days" time-scale (25 values every 30 days)
A &lt;- (1:25)*30
# Convert it to a "years" time-scale, using 23/05/2001 (d/m/Y) as first value
B &lt;- daystoyears(A, datemin="23/05/2001", dateformat="d/m/Y")
B
# Convert it back to "days" time-scale
yearstodays(B, xmin=30)

# Here is an example showing the shift introduced, and its consequence:
C &lt;- daystoyears(unclass(as.Date(c("1970-1-1","1971-1-1","1972-1-1","1973-1-1"),
	format = "%Y-%m-%d")))
C
</code></pre>

<hr>
<h2 id='decaverage'> Time series decomposition using a moving average </h2><span id='topic+decaverage'></span>

<h3>Description</h3>

<p>Decompose a single regular time series with a moving average filtering. Return a 'tsd' object. To decompose several time series at once, use <code>tsd()</code> with the argument <code>method="average"</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>decaverage(x, type="additive", order=1, times=1, sides=2, ends="fill",
        weights=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="decaverage_+3A_x">x</code></td>
<td>
<p> a regular time series ('rts' under S+ and 'ts' under R) </p>
</td></tr>
<tr><td><code id="decaverage_+3A_type">type</code></td>
<td>
<p> the type of model, either <code>type="additive"</code> (by default), or <code>type="multiplicative"</code> </p>
</td></tr>
<tr><td><code id="decaverage_+3A_order">order</code></td>
<td>
<p> the order of the moving average (the window of the average being 2*order+1), centered around the current observation or at left of this observation depending upon the value of the <code>sides</code> argument. Weights are the same for all observations within the window. However, if the argument <code>weights</code> is provided, it supersedes <code>order</code>. One can also use <code>order="periodic"</code>. In this case, a deseasoning filter is calculated according to the value of <code>frequency</code> </p>
</td></tr>
<tr><td><code id="decaverage_+3A_times">times</code></td>
<td>
<p> The number of times to apply the method (by default, once) </p>
</td></tr>
<tr><td><code id="decaverage_+3A_sides">sides</code></td>
<td>
<p> If 2 (by default), the window is centered around the current observation. If 1, the window is at left of the current observation (including it) </p>
</td></tr>
<tr><td><code id="decaverage_+3A_ends">ends</code></td>
<td>
<p> either &quot;NAs&quot; (fill first and last values that are not calculable with NAs), or &quot;fill&quot; (fill them with the average of observations before applying the filter, by default), or &quot;circular&quot; (use last values for estimating first ones and vice versa), or &quot;periodic&quot; (use entire periods of contiguous cycles, deseasoning) </p>
</td></tr>
<tr><td><code id="decaverage_+3A_weights">weights</code></td>
<td>
<p> a vector indicating weight to give to all observations in the window. This argument has the priority over <code>order</code> </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a wrapper around the <code>filter()</code> function and returns a 'tsd' object. However, it offers more methods to handle ends.
</p>


<h3>Value</h3>

<p>A 'tsd' object
</p>


<h3>Author(s)</h3>

<p> Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>), Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>) </p>


<h3>References</h3>

<p>Kendall, M., 1976. <em>Time-series.</em> Charles Griffin &amp; Co Ltd. 197 pp.
</p>
<p>Laloire, J.C., 1972. <em>Méthodes du traitement des chroniques.</em> Dunod, Paris, 194 pp.
</p>
<p>Malinvaud, E., 1978. <em>Méthodes statistiques de l'économétrie.</em> Dunod, Paris. 846 pp.
</p>
<p>Philips, L. &amp; R. Blomme, 1973. <em>Analyse chronologique.</em> Université Catholique de Louvain. Vander ed. 339 pp.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+tsd">tsd</a></code>, <code><a href="#topic+tseries">tseries</a></code>, <code><a href="#topic+deccensus">deccensus</a></code>, <code><a href="#topic+decdiff">decdiff</a></code>, <code><a href="#topic+decmedian">decmedian</a></code>,  <code><a href="#topic+decevf">decevf</a></code>, <code><a href="#topic+decreg">decreg</a></code>, <code><a href="#topic+decloess">decloess</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(marbio)
ClausoB.ts &lt;- ts(log(marbio$ClausocalanusB + 1))
ClausoB.dec &lt;- decaverage(ClausoB.ts, order=2, times=10, sides=2, ends="fill")
plot(ClausoB.dec, col=c(1, 3, 2), xlab="stations")
# A stacked graph is more representative in this case
plot(ClausoB.dec, col=c(1, 3), xlab="stations", stack=FALSE, resid=FALSE,
        lpos=c(53, 4.3))
</code></pre>

<hr>
<h2 id='deccensus'> Time decomposition using the CENSUS II method </h2><span id='topic+deccensus'></span>

<h3>Description</h3>

<p>The CENSUS II method allows to decompose a regular time series into a trend, a seasonal component and residuals, according to a multiplicative model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deccensus(x, type="multiplicative", trend=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deccensus_+3A_x">x</code></td>
<td>
<p> A single regular time serie (a 'rts' object under S+ and a 'ts' object under R) with a &quot;years&quot; time scale (one unit = one year) and a complete number of cycles (at least 3 complete years) </p>
</td></tr>
<tr><td><code id="deccensus_+3A_type">type</code></td>
<td>
<p> The type of model. This is for compatibility with other <code>decxxx()</code> functions, but only a multiplicative model is allowed here </p>
</td></tr>
<tr><td><code id="deccensus_+3A_trend">trend</code></td>
<td>
<p> If <code>trend=TRUE</code> a trend component is also calculated, otherwise, the decomposition gives only a seasonal component and residuals </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The trend component contains both a general trend and long-term oscillations. The seasonal trend may vary from year to year. For a seasonal decomposition using an additive model, use <code>decloess()</code> instead
</p>


<h3>Value</h3>

<p>a 'tsd' object
</p>


<h3>Author(s)</h3>

<p> Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>), Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>) </p>


<h3>References</h3>

<p>Béthoux, N., M. Etienne, F. Ibanez &amp; J.L. Rapaire, 1980. <em>Spécificités hydrologiques des zones littorales. Analyse chronologique par la méthode CENSUS II et estimation des échanges océan-atmosphère appliqués à la baie de Villefranche sur mer.</em> Ann. Inst. Océanogr. Paris, 56:81-95.
</p>
<p>Fromentin, J.M. &amp; F. Ibanez, 1994. <em>Year to year changes in meteorological features on the French coast area during the last half-century. Examples of two biological responses.</em> Oceanologica Acta, 17:285-296.
</p>
<p>Institut National de Statistique de Belgique, 1965. <em>Décomposition des séries chronologiques en leurs composantes suivant différentes méthodes. Etudes statistiques et économétriques.</em> Bull. Stat. INS, 10:1449-1524.
</p>
<p>Philips, J. &amp; R. Blomme, 1973. <em>Analyse chronologique.</em> Université Catholique de Louvain, Vander ed. 339 pp.
</p>
<p>Rosenblatt, H.M., 1968. <em>Spectral evaluation of BLS and CENSUS revised seasonal adjustment procedures.</em> J. Amer. Stat. Assoc., 68:472-501.
</p>
<p>Shiskin, J. &amp; H. Eisenpress, 1957. <em>Seasonal adjustment by electronic computer methods.</em> J. Amer. Stat. Assoc., 52:415-449.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+tsd">tsd</a></code>, <code><a href="#topic+tseries">tseries</a></code>, <code><a href="#topic+decaverage">decaverage</a></code>, <code><a href="#topic+decdiff">decdiff</a></code>, <code><a href="#topic+decmedian">decmedian</a></code>, <code><a href="#topic+decevf">decevf</a></code>, <code><a href="#topic+decreg">decreg</a></code>, <code><a href="#topic+decloess">decloess</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(releve)
# Get regulated time series with a 'years' time-scale
rel.regy &lt;- regul(releve$Day, releve[3:8], xmin=6, n=87, units="daystoyears",
    frequency=24, tol=2.2, methods="linear", datemin="21/03/1989", dateformat="d/m/Y")
rel.ts &lt;- tseries(rel.regy)
# We must have complete cycles to allow using deccensus()
start(rel.ts)
end(rel.ts)
rel.ts2 &lt;- window(rel.ts, end=c(1992,5))
rel.dec2 &lt;- deccensus(rel.ts2[, "Melosul"], trend=TRUE)
plot(rel.dec2, col=c(1,4,3,2))
</code></pre>

<hr>
<h2 id='decdiff'> Time series decomposition using differences (trend elimination) </h2><span id='topic+decdiff'></span>

<h3>Description</h3>

<p>A filtering using X(t+lag) - X(t) has the property to eliminate the general trend from the series, whatever its shape
</p>


<h3>Usage</h3>

<pre><code class='language-R'>decdiff(x, type="additive", lag=1, order=1, ends="fill")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="decdiff_+3A_x">x</code></td>
<td>
<p> a regular time series ('rts' under S+ and 'ts' under R) </p>
</td></tr>
<tr><td><code id="decdiff_+3A_type">type</code></td>
<td>
<p> the type of model, either <code>type="additive"</code> (by default), or <code>type="multiplicative"</code> </p>
</td></tr>
<tr><td><code id="decdiff_+3A_lag">lag</code></td>
<td>
<p> The lag between the two observations used to calculate differences. By default, <code>lag=1</code> </p>
</td></tr>
<tr><td><code id="decdiff_+3A_order">order</code></td>
<td>
<p> The order of the difference corresponds to the number of times it is applied, by default <code>order=1</code> </p>
</td></tr>
<tr><td><code id="decdiff_+3A_ends">ends</code></td>
<td>
<p> either &quot;NAs&quot; (fill first values that are not calculable with NAs), or &quot;fill&quot; (fill them with the average of following observations before applying the filter, by default), or &quot;drop&quot; (do not fill them). If <code>ends="drop"</code>, the filtered series will be shorter than the initial one by lag*order. In all other cases, the filtered series is as large as the initial one </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a wrapper around the <code>diff()</code> function to create a 'tsd' object. It also manages initial values through the <code>ends</code> argument.
</p>


<h3>Value</h3>

<p>a 'tsd' object
</p>


<h3>Author(s)</h3>

<p> Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>), Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>) </p>


<h3>References</h3>

 
<p>Kendall, M., 1976. <em>Time-series.</em> Charles Griffin &amp; Co Ltd. 197 pp.
</p>
<p>Laloire, J.C., 1972. <em>Méthodes du traitement des chroniques.</em> Dunod, Paris, 194 pp.
</p>
<p>Philips, L. &amp; R. Blomme, 1973. <em>Analyse chronologique.</em> Université Catholique de Louvain. Vander ed. 339 pp.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+tsd">tsd</a></code>, <code><a href="#topic+tseries">tseries</a></code>, <code><a href="#topic+decaverage">decaverage</a></code>, <code><a href="#topic+deccensus">deccensus</a></code>, <code><a href="#topic+decmedian">decmedian</a></code>,  <code><a href="#topic+decevf">decevf</a></code>, <code><a href="#topic+decreg">decreg</a></code>, <code><a href="#topic+decloess">decloess</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(marbio)
ClausoB.ts &lt;- ts(log(marbio$ClausocalanusB + 1))
ClausoB.dec &lt;- decdiff(ClausoB.ts, lag=1, order=2, ends="fill")
plot(ClausoB.dec, col=c(1, 4, 2), xlab="stations")
</code></pre>

<hr>
<h2 id='decevf'> Time series decomposition using eigenvector filtering (EVF) </h2><span id='topic+decevf'></span>

<h3>Description</h3>

<p>The eigenvector filtering decomposes the signal by applying a principal component analysis (PCA) on the original signal and a certain number of copies of it incrementally lagged, collected in a multivariate matrix. Reconstructing the signal using only the most representative eigenvectors allows filtering it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>decevf(x, type="additive", lag=5, axes=1:2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="decevf_+3A_x">x</code></td>
<td>
<p> a regular time series ('rts' under S+ and 'ts' under R) </p>
</td></tr>
<tr><td><code id="decevf_+3A_type">type</code></td>
<td>
<p> the type of model, either <code>type="additive"</code> (by default), or <code>type="multiplicative"</code> </p>
</td></tr>
<tr><td><code id="decevf_+3A_lag">lag</code></td>
<td>
<p> The maximum lag used. A PCA is run on the matrix constituted by vectors lagged from 0 to <code>lag</code>. The defaulf value is 5, but a value corresponding to no significant autocorrelation, on basis of examination of the autocorrelation plot obtained by <code>acf</code> in the library 'ts' should be used (Lag at first time the autocorrelation curve crosses significance lines multiplied by the frequency of the series). </p>
</td></tr>
<tr><td><code id="decevf_+3A_axes">axes</code></td>
<td>
<p> The principal axes to use to reconstruct the filtered signal. For instance, to use axes 2 and 4, use <code>axes=c(2,4)</code>. By default, the first two axes are considered (<code>axes=1:2</code>) </p>
</td></tr>
</table>


<h3>Value</h3>

<p>a 'tsd' object
</p>


<h3>Author(s)</h3>

<p> Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>), Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>) </p>


<h3>References</h3>

 
<p>Colebrook, J.M., 1978. <em>Continuous plankton records: zooplankton and environment, North-East Atlantic and North Sea 1948-1975.</em> Oceanologica Acta, 1:9-23.
</p>
<p>Ibanez, F. &amp; J.C. Dauvin, 1988. <em>Long-term changes (1977-1987) on a muddy fine sand Abra alba - Melinna palmate population community from the Western English Channel.</em> J. Mar. Prog. Ser., 49:65-81.
</p>
<p>Ibanez, F., 1991. <em>Treatment of data deriving from the COST 647 project on coastal benthic ecology: The within-site analysis.</em> In: B. Keegan (ed.) <em>Space and time series data analysis in coastal benthic ecology.</em> Pp. 5-43.
</p>
<p>Ibanez, F. &amp; M. Etienne, 1992. <em>Le filtrage des séries chronologiques par l'analyse en composantes principales de processus (ACPP).</em> J. Rech. Océanogr., 16:27-33.
</p>
<p>Ibanez, F., J.C. Dauvin &amp; M. Etienne, 1993. <em>Comparaison des évolutions à long-terme (1977-1990) de deux peuplements macrobenthiques de la Baie de Morlaix (Manche Occidentale): relations avec les facteurs hydroclimatiques.</em> J. Exp. Mar. Biol. Ecol., 169:181-214.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+tsd">tsd</a></code>, <code><a href="#topic+tseries">tseries</a></code>, <code><a href="#topic+decaverage">decaverage</a></code>, <code><a href="#topic+deccensus">deccensus</a></code>, <code><a href="#topic+decmedian">decmedian</a></code>,  <code><a href="#topic+decdiff">decdiff</a></code>, <code><a href="#topic+decreg">decreg</a></code>, <code><a href="#topic+decloess">decloess</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(releve)
melo.regy &lt;- regul(releve$Day, releve$Melosul, xmin=9, n=87,
        units="daystoyears", frequency=24, tol=2.2, methods="linear",
        datemin="21/03/1989", dateformat="d/m/Y")
melo.ts &lt;- tseries(melo.regy)
acf(melo.ts)
# Autocorrelation is not significant after 0.16
# That corresponds to a lag of 0.16*24=4 (frequency=24)
melo.evf &lt;- decevf(melo.ts, lag=4, axes=1)
plot(melo.evf, col=c(1, 4, 2))
# A superposed graph is better in the present case
plot(melo.evf, col=c(1, 4), xlab="stations", stack=FALSE, resid=FALSE,
        lpos=c(0, 60000))
</code></pre>

<hr>
<h2 id='decloess'> Time series decomposition by the LOESS method </h2><span id='topic+decloess'></span>

<h3>Description</h3>

<p>Compute a seasonal decomposition of a regular time series using a LOESS method (local polynomial regression)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>decloess(x, type="additive", s.window=NULL, s.degree=0, t.window=NULL,
        t.degree=2, robust=FALSE, trend=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="decloess_+3A_x">x</code></td>
<td>
<p> a regular time series ('rts' under S+ and 'ts' under R) </p>
</td></tr>
<tr><td><code id="decloess_+3A_type">type</code></td>
<td>
<p> the type of model. This is for compatibility purpose. The only model type that is accepted for this method is <code>type="additive"</code>. For a multiplicative model, use <code>deccensus()</code> instead </p>
</td></tr>
<tr><td><code id="decloess_+3A_s.window">s.window</code></td>
<td>
<p> the width of the window used to extract the seasonal component. Use an odd value equal or just larger than the number of annual values (frequency of the time series). Use another value to extract other cycles (circadian, lunar,...). Using <code>s.window="periodic"</code> ensures a correct value for extracting a seasonal component when the time scale is in years units </p>
</td></tr>
<tr><td><code id="decloess_+3A_s.degree">s.degree</code></td>
<td>
<p> the order of the polynome to use to extract the seasonal component (0 or 1). By default <code>s.degree=0</code> </p>
</td></tr>
<tr><td><code id="decloess_+3A_t.window">t.window</code></td>
<td>
<p> the width of the window to use to extract the general trend when <code>trend=TRUE</code> (indicate an odd value). If this parameter is not provided, a reasonable value is first calculated, and then used by the algorithm. </p>
</td></tr>
<tr><td><code id="decloess_+3A_t.degree">t.degree</code></td>
<td>
<p> the order of the polynome to use to extract the general trend (0, 1 or 2). By default <code>t.degree=2</code> </p>
</td></tr>
<tr><td><code id="decloess_+3A_robust">robust</code></td>
<td>
<p> if <code>TRUE</code> a robust regression method is used. Otherwise (<code>FALSE</code>), by default, a classical least-square regression is used </p>
</td></tr>
<tr><td><code id="decloess_+3A_trend">trend</code></td>
<td>
<p> If <code>TRUE</code> a trend is calculated (under R only). Otherwise, the series is decomposed into a seasonal component and residuals only </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses the <code>stl()</code> function for the decomposition. It is a wrapper that create a 'tsd' object
</p>


<h3>Value</h3>

<p>a 'tsd' object
</p>


<h3>Author(s)</h3>

<p> Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>), Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>) </p>


<h3>References</h3>

 
<p>Cleveland, W.S., E. Grosse &amp; W.M. Shyu, 1992. <em>Local regression models. Chapter 8 of Statistical Models in S.</em> J.M. Chambers &amp; T.J. Hastie (eds). Wadsworth &amp; Brook/Cole.
</p>
<p>Cleveland, R.B.,  W.S. Cleveland, J.E. McRae, &amp; I. Terpenning, 1990. <em>STL: A seasonal-trend  decomposition  procedure based on loess.</em> J. Official Stat., 6:3-73.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+tsd">tsd</a></code>, <code><a href="#topic+tseries">tseries</a></code>, <code><a href="#topic+decaverage">decaverage</a></code>, <code><a href="#topic+deccensus">deccensus</a></code>, <code><a href="#topic+decmedian">decmedian</a></code>, <code><a href="#topic+decdiff">decdiff</a></code>, <code><a href="#topic+decevf">decevf</a></code>, <code><a href="#topic+decreg">decreg</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(releve)
melo.regy &lt;- regul(releve$Day, releve$Melosul, xmin=9, n=87,
        units="daystoyears", frequency=24, tol=2.2, methods="linear",
        datemin="21/03/1989", dateformat="d/m/Y")
melo.ts &lt;- tseries(melo.regy)
melo.dec &lt;- decloess(melo.ts, s.window="periodic")
plot(melo.dec, col=1:3)
</code></pre>

<hr>
<h2 id='decmedian'> Time series decomposition using a running median </h2><span id='topic+decmedian'></span>

<h3>Description</h3>

<p>This is a nonlinear filtering method used to smooth, but also to segment a time series. The isolated peaks and pits are leveraged by this method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>decmedian(x, type="additive", order=1, times=1, ends="fill")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="decmedian_+3A_x">x</code></td>
<td>
<p> a regular time series ('rts' under S+ and 'ts' under R) </p>
</td></tr>
<tr><td><code id="decmedian_+3A_type">type</code></td>
<td>
<p> the type of model, either <code>type="additive"</code> (by default), or <code>type="multiplicative"</code> </p>
</td></tr>
<tr><td><code id="decmedian_+3A_order">order</code></td>
<td>
<p> the window used for the running median corresponds to 2*order + 1  </p>
</td></tr>
<tr><td><code id="decmedian_+3A_times">times</code></td>
<td>
<p> the number of times the running median is applied. By default, 1 </p>
</td></tr>
<tr><td><code id="decmedian_+3A_ends">ends</code></td>
<td>
<p> the method used to calculate ends. Either &quot;NAs&quot; (fill extremes, non-calculable values with NAs), or &quot;fill&quot; (fill these extremes with the closest calculable median) </p>
</td></tr>
</table>


<h3>Value</h3>

<p>a 'tsd' object
</p>


<h3>Author(s)</h3>

<p> Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>), Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>) </p>


<h3>References</h3>

<p>Gebski, V.J., 1985. <em>Some properties of splicing when applied to non-linear smoothers.</em> Comp. Stat. Data Anal., 3:151-157.
</p>
<p>Philips, L. &amp; R. Blomme, 1973. <em>Analyse chronologique. Université Catholique de Louvain.</em> Vander ed. 339 pp.
</p>
<p>Tukey, J.W., 1977. <em>Exploratory Data Analysis.</em> Reading Massachusetts: Addison-Wesley.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+tsd">tsd</a></code>, <code><a href="#topic+tseries">tseries</a></code>, <code><a href="#topic+decaverage">decaverage</a></code>, <code><a href="#topic+deccensus">deccensus</a></code>, <code><a href="#topic+decdiff">decdiff</a></code>, <code><a href="#topic+decevf">decevf</a></code>, <code><a href="#topic+decreg">decreg</a></code>, <code><a href="#topic+decloess">decloess</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(marbio)
ClausoB.ts &lt;- ts(log(marbio$ClausocalanusB + 1))
ClausoB.dec &lt;- decmedian(ClausoB.ts, order=2, times=10, ends="fill")
plot(ClausoB.dec, col=c(1, 4, 2), xlab="stations")
# This is a transect across a frontal zone:
plot(ClausoB.dec, col=c(0, 2), xlab="stations", stack=FALSE, resid=FALSE)
lines(c(17, 17), c(0, 10), col=4, lty=2)
lines(c(25, 25), c(0, 10), col=4, lty=2)
lines(c(30, 30), c(0, 10), col=4, lty=2)
lines(c(41, 41), c(0, 10), col=4, lty=2)
lines(c(46, 46), c(0, 10), col=4, lty=2)
text(c(8.5, 21, 27.5, 35, 43.5, 57), 8.7, labels=c("Peripheral Zone", "D1",
        "C", "Front", "D2", "Central Zone"))
</code></pre>

<hr>
<h2 id='decreg'> Time series decomposition using a regression model </h2><span id='topic+decreg'></span>

<h3>Description</h3>

<p>Providing values coming from a regression on the original series, a <code>tsd</code> object is created using the original series, the regression model and the residuals
</p>


<h3>Usage</h3>

<pre><code class='language-R'>decreg(x, xreg, type="additive")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="decreg_+3A_x">x</code></td>
<td>
<p> a regular time series ('rts' under S+ and 'ts' under R) </p>
</td></tr>
<tr><td><code id="decreg_+3A_xreg">xreg</code></td>
<td>
<p> a second regular time series or a vector of the same length as <code>x</code> with corresponding values from the regression model </p>
</td></tr>
<tr><td><code id="decreg_+3A_type">type</code></td>
<td>
<p> the type of model, either <code>type="additive"</code> (by default), or <code>type="multiplicative"</code> </p>
</td></tr>
</table>


<h3>Value</h3>

<p>a 'tsd' object
</p>


<h3>Author(s)</h3>

<p> Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>), Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>) </p>


<h3>References</h3>

 
<p>Frontier, S., 1981. <em>Méthodes statistiques.</em> Masson, Paris. 246 pp.
</p>
<p>Kendall, M., 1976. <em>Time-series.</em> Charles Griffin &amp; Co Ltd. 197 pp.
</p>
<p>Legendre, L. &amp; P. Legendre, 1984. <em>Ecologie numérique. Tome 2: La structure des données écologiques.</em> Masson, Paris. 335 pp.
</p>
<p>Malinvaud, E., 1978. <em>Méthodes statistiques de l'économétrie.</em> Dunod, Paris. 846 pp.
</p>
<p>Sokal, R.R. &amp; F.J. Rohlf, 1981. <em>Biometry.</em> Freeman &amp; Co, San Francisco. 860 pp.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+tsd">tsd</a></code>, <code><a href="#topic+tseries">tseries</a></code>, <code><a href="#topic+decaverage">decaverage</a></code>, <code><a href="#topic+deccensus">deccensus</a></code>, <code><a href="#topic+decdiff">decdiff</a></code>, <code><a href="#topic+decevf">decevf</a></code>, <code><a href="#topic+decmedian">decmedian</a></code>, <code><a href="#topic+decloess">decloess</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(marphy)
density &lt;- ts(marphy[, "Density"])
plot(density)
Time &lt;- time(density)

# Linear model to represent trend
density.lin &lt;- lm(density ~ Time)
summary(density.lin)
xreg &lt;- predict(density.lin)
lines(xreg, col=3)
density.dec &lt;- decreg(density, xreg)
plot(density.dec, col=c(1, 3, 2), xlab="stations")

# Order 2 polynomial to represent trend
density.poly &lt;- lm(density ~ Time + I(Time^2))
summary(density.poly)
xreg2 &lt;- predict(density.poly)
plot(density)
lines(xreg2, col=3)
density.dec2 &lt;- decreg(density, xreg2)
plot(density.dec2, col=c(1, 3, 2), xlab="stations")

# Fit a sinusoidal model on seasonal (artificial) data
tser &lt;- ts(sin((1:100)/12*pi)+rnorm(100, sd=0.3), start=c(1998, 4),
        frequency=24)
Time &lt;- time(tser)
tser.sin &lt;- lm(tser ~ I(cos(2*pi*Time)) + I(sin(2*pi*Time)))
summary(tser.sin)
tser.reg &lt;- predict(tser.sin)
tser.dec &lt;- decreg(tser, tser.reg)
plot(tser.dec, col=c(1, 4), xlab="stations", stack=FALSE, resid=FALSE,
        lpos=c(0, 4))
plot(tser.dec, col=c(1, 4, 2), xlab="stations")

# One can also use nonlinear models (see 'nls')
# or autoregressive models (see 'ar' and others in 'ts' library)
</code></pre>

<hr>
<h2 id='disjoin'> Complete disjoined coded data (binary coding) </h2><span id='topic+disjoin'></span>

<h3>Description</h3>

<p>Transform a factor in separate variables (one per level) with a binary code (0 for absent, 1 for present) in each variable
</p>


<h3>Usage</h3>

<pre><code class='language-R'>disjoin(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="disjoin_+3A_x">x</code></td>
<td>
<p> a vector containing a factor data </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Use <code>cut()</code> to transform a numerical variable into a factor variable
</p>


<h3>Value</h3>

<p>a matrix containing the data with binary coding
</p>


<h3>Author(s)</h3>

<p> Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>), Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>) </p>


<h3>References</h3>

<p>Fromentin J.-M., F. Ibanez &amp; P. Legendre, 1993. <em>A phytosociological method for interpreting plankton data.</em> Mar. Ecol. Prog. Ser., 93:285-306.
</p>
<p>Gebski, V.J., 1985. <em>Some properties of splicing when applied to non-linear smoothers.</em> Comput. Stat. Data Anal., 3:151-157.
</p>
<p>Grandjouan, G., 1982. <em>Une méthode de comparaison statistique entre les répartitions des plantes et des climats.</em> Thèse d'Etat, Université Louis Pasteur, Strasbourg.
</p>
<p>Ibanez, F., 1976. <em>Contribution à l'analyse mathématique des événements en Ecologie planctonique. Optimisations méthodologiques.</em> Bull. Inst. Océanogr. Monaco, 72:1-96.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+buysbal">buysbal</a></code>, <code><a href="base.html#topic+cut">cut</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'># Artificial data with 1/5 of zeros
Z &lt;- c(abs(rnorm(8000)), rep(0, 2000))
# Let the program chose cuts
table(cut(Z, breaks=5))
# Create one class for zeros, and 4 classes for the other observations
Z2 &lt;- Z[Z != 0]
cuts &lt;- c(-1e-10, 1e-10, quantile(Z2, 1:5/5, na.rm=TRUE))
cuts
table(cut(Z, breaks=cuts))
# Binary coding of these data
disjoin(cut(Z, breaks=cuts))[1:10, ]
</code></pre>

<hr>
<h2 id='disto'> Compute and plot a distogram </h2><span id='topic+disto'></span>

<h3>Description</h3>

<p>A distogram is an extension of the variogram to a multivariate time-series. It computes, for each observation (with a constant interval h between each observation), the euclidean distance normated to one (chord distance)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>disto(x, max.dist=nrow(x)/4, plotit=TRUE, disto.data=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="disto_+3A_x">x</code></td>
<td>
<p> a matrix, a data frame or a multiple time-series </p>
</td></tr>
<tr><td><code id="disto_+3A_max.dist">max.dist</code></td>
<td>
<p> the maximum distance to calculate. By default, it is the third of the number of observations (that is, the number of rows in the matrix) </p>
</td></tr>
<tr><td><code id="disto_+3A_plotit">plotit</code></td>
<td>
<p> If <code>plotit=TRUE</code> then the graph of the distogram is plotted </p>
</td></tr>
<tr><td><code id="disto_+3A_disto.data">disto.data</code></td>
<td>
<p> data coming from a previous call to <code>disto()</code>. Call the function again with these data to plot the corresponding graph </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame containing distance and distogram values
</p>


<h3>Author(s)</h3>

<p> Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>), Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>) </p>


<h3>References</h3>

 
<p>Dauvin, J.C. &amp; F. Ibanez, 1986. <em>Variations à long-terme (1977-1985) du peuplement des sables fins de la Pierre Noire (baie de Morlaix, Manche Occidentale): analyse statistique de l'évolution structurale.</em> Hydrobiologia, 142:171-186.
</p>
<p>Ibanez, F. &amp; J.C. Dauvin, 1988. <em>Long-term changes (1977-1987) in a muddy fine sand Abra alba - Melinna palmate community from the Western English Channel: multivariate time-series analysis.</em> Mar. Ecol. Prog. Ser., 49:65-81.
</p>
<p>Mackas, D.L., 1984. <em>Spatial autocorrelation of plankton community composition in a continental shelf ecosystem.</em> Limnol. Ecol., 20:451-471.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+vario">vario</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(bnr)
disto(bnr)
</code></pre>

<hr>
<h2 id='escouf'> Choose variables using the Escoufier's equivalent vectors method </h2><span id='topic+escouf'></span><span id='topic+extract.escouf'></span><span id='topic+identify.escouf'></span><span id='topic+lines.escouf'></span><span id='topic+plot.escouf'></span><span id='topic+print.escouf'></span><span id='topic+print.summary.escouf'></span><span id='topic+summary.escouf'></span>

<h3>Description</h3>

<p>Calculate equivalent vectors sensu Escoufier, that is, most significant variables from a multivariate data frame according to a principal component analysis (variables that are most correlated with the principal axes). This method is useful mainly for physical or chemical data where simply summarizing them with a PCA does not always gives easily interpretable principal axes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>escouf(x, level=1, verbose=TRUE)
## S3 method for class 'escouf'
print(x, ...)
## S3 method for class 'escouf'
summary(object, ...)
## S3 method for class 'summary.escouf'
print(x, ...)
## S3 method for class 'escouf'
plot(x, level=x$level, lhorz=TRUE, lvert=TRUE, lvars=TRUE,
        lcol=2, llty=2, diff=TRUE, dlab="RV' (units not shown)", dcol=4,
        dlty=par("lty"), dpos=0.8, type="s", xlab="variables", ylab="RV",
        main=paste("Escoufier's equivalent vectors for:",x$data), ...)
## S3 method for class 'escouf'
lines(x, level=x$level, lhorz=TRUE, lvert=TRUE, lvars=TRUE,
        col=2, lty=2, ...)
## S3 method for class 'escouf'
identify(x, lhorz=TRUE, lvert=TRUE, lvars=TRUE, col=2,
        lty=2, ...)
## S3 method for class 'escouf'
extract(e, n, level=e$level, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="escouf_+3A_x">x</code></td>
<td>
<p> For <code>escouf()</code>, a data frame containing the variables to sort according to the Escoufier's method. For the other functions, an 'escouf' object </p>
</td></tr>
<tr><td><code id="escouf_+3A_level">level</code></td>
<td>
<p> The level of correlation at which to stop calculation. By default <code>level=1</code>, the highest value, and all variables are sorted. Specify a value lower than one to speed up calculation. If you specify a too low values you will not be able to extract all significant variables (extraction level must be lower than calculation level). We advise you keep 0.95 &lt; level &lt; 1 </p>
</td></tr>
<tr><td><code id="escouf_+3A_verbose">verbose</code></td>
<td>
<p> Print calculation steps. This allows to control the percentage of calculation already achieved when computation takes a long time (that is, with many variables to sort) </p>
</td></tr>
<tr><td><code id="escouf_+3A_object">object</code></td>
<td>
<p> An 'escouf' object returned by <code>escouf</code></p>
</td></tr>
<tr><td><code id="escouf_+3A_e">e</code></td>
<td>
<p> An 'escouf' object returned by <code>escouf</code></p>
</td></tr>
<tr><td><code id="escouf_+3A_lhorz">lhorz</code></td>
<td>
<p> If <code>TRUE</code> then an horizontal line indicating the extraction level is drawn </p>
</td></tr>
<tr><td><code id="escouf_+3A_lvert">lvert</code></td>
<td>
<p> If <code>TRUE</code> then a vertical line separate the n extracted variables at left from the rest </p>
</td></tr>
<tr><td><code id="escouf_+3A_lvars">lvars</code></td>
<td>
<p> If <code>TRUE</code> then the x-axis labels of the n extracted variables at left are printed in a different color to emphasize them </p>
</td></tr>
<tr><td><code id="escouf_+3A_lcol">lcol</code></td>
<td>
<p> The color to use to draw the lines (<code>lhorz=TRUE</code> and <code>lvert=TRUE</code>) and the variables labels (<code>lvars=TRUE</code>) of the n extracted variables. By default, color 2 is used </p>
</td></tr>
<tr><td><code id="escouf_+3A_llty">llty</code></td>
<td>
<p> The style used to draw the lines (<code>lhorz=TRUE</code> and <code>lvert=TRUE</code>). By default, lines are dashed </p>
</td></tr>
<tr><td><code id="escouf_+3A_diff">diff</code></td>
<td>
<p> If <code>TRUE</code> then the RV' curve is also plotted (by default) </p>
</td></tr>
<tr><td><code id="escouf_+3A_dlab">dlab</code></td>
<td>
<p> The label to use for the RV' curve. By default: <code>"RV' (units not shown)"</code> </p>
</td></tr>
<tr><td><code id="escouf_+3A_dcol">dcol</code></td>
<td>
<p> The color to use for the RV' curve (by default, color 4 is used) </p>
</td></tr>
<tr><td><code id="escouf_+3A_type">type</code></td>
<td>
<p> The type of graph to plot </p>
</td></tr>
<tr><td><code id="escouf_+3A_xlab">xlab</code></td>
<td>
<p> the label of the x-axis </p>
</td></tr>
<tr><td><code id="escouf_+3A_ylab">ylab</code></td>
<td>
<p> the label of the y-axis </p>
</td></tr>
<tr><td><code id="escouf_+3A_main">main</code></td>
<td>
<p> the main title of the graph</p>
</td></tr>
<tr><td><code id="escouf_+3A_dlty">dlty</code></td>
<td>
<p> The style for the RV' curve </p>
</td></tr>
<tr><td><code id="escouf_+3A_col">col</code></td>
<td>
<p> The color to use to draw the lines (<code>lhorz=TRUE</code> and <code>lvert=TRUE</code>) and the variables labels (<code>lvars=TRUE</code>) of the n extracted variables. By default, color 2 is used </p>
</td></tr>
<tr><td><code id="escouf_+3A_lty">lty</code></td>
<td>
<p> The style used to draw the lines (<code>lhorz=TRUE</code> and <code>lvert=TRUE</code>). By default, lines are dashed </p>
</td></tr>
<tr><td><code id="escouf_+3A_dpos">dpos</code></td>
<td>
<p> The relative horizontal position of the label for the RV' curve. The default value of 0.8 means that the label is placed at 80% of the horizontal axis.Vertical position of the label is automatically determined </p>
</td></tr>
<tr><td><code id="escouf_+3A_n">n</code></td>
<td>
<p> The number of variables to extract. If a value is given, it has the priority on <code>level</code> </p>
</td></tr>
<tr><td><code id="escouf_+3A_...">...</code></td>
<td>
<p> additional parameters </p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of type 'escouf' is returned. It has methods <code>print()</code>, <code>summary()</code>, <code>plot()</code>, <code>lines()</code>, <code>identify()</code>, <code>extract()</code>.
</p>


<h3>WARNING </h3>

<p>Since a large number of iterations is done, this function is slow with a large number of variables (more than 25-30)!</p>


<h3>Author(s)</h3>

<p> Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>), Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>), Benjamin Planque (<a href="mailto:Benjamin.Planque@ifremer.fr">Benjamin.Planque@ifremer.fr</a>),  Jean-Marc Fromentin (<a href="mailto:Jean.Marc.Fromentin@ifremer.fr">Jean.Marc.Fromentin@ifremer.fr</a>) </p>


<h3>References</h3>

 
<p>Cambon, J., 1974. <em>Vecteur équivalent à un autre au sens des composantes principales.</em> Application hydrologique. DEA de Mathématiques Appliquées, Université de Montpellier.
</p>
<p>Escoufier, Y., 1970. <em>Echantillonnage dans une population de variables aléatoires réelles.</em> Pub. Inst. Stat. Univ. Paris, 19:1-47.
</p>
<p>Jabaud, A., 1996. <em>Cadre climatique et hydrobiologique du lac Léman.</em> DEA d'Océanologie Biologique Paris.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+abund">abund</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(marbio)
marbio.esc &lt;- escouf(marbio)
summary(marbio.esc)
plot(marbio.esc)
# The x-axis has short labels. For more info., enter: 
marbio.esc$vr
# Define a level at which to extract most significant variables
marbio.esc$level &lt;- 0.90
# Show it on the graph
lines(marbio.esc)
# This can also be done interactively on the plot using:
# marbio.esc$level &lt;- identify(marbio.esc)
# Finally, extract most significant variables
marbio2 &lt;- extract(marbio.esc)
names(marbio2)
</code></pre>

<hr>
<h2 id='extract'> Extract a subset of the original dataset </h2><span id='topic+extract'></span>

<h3>Description</h3>

<p>&lsquo;extract&rsquo; is a generic function for extracting a part of the original dataset according to an analysis...)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract(e, n, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_+3A_e">e</code></td>
<td>
<p> An object from where extraction is performed </p>
</td></tr>
<tr><td><code id="extract_+3A_n">n</code></td>
<td>
<p> The number of elements to extract </p>
</td></tr>
<tr><td><code id="extract_+3A_...">...</code></td>
<td>
<p> Additional arguments affecting the extraction </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A subset of the original dataset contained in the extracted object
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+abund">abund</a></code>, <code><a href="#topic+regul">regul</a></code>, <code><a href="#topic+tsd">tsd</a></code>, <code><a href="#topic+turnogram">turnogram</a></code>, <code><a href="#topic+turnpoints">turnpoints</a></code> </p>

<hr>
<h2 id='first'> Get the first element of a vector </h2><span id='topic+first'></span>

<h3>Description</h3>

<p>Extract the first element of a vector. Useful for the <code>turnogram()</code> function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>first(x, na.rm=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="first_+3A_x">x</code></td>
<td>
<p> a numerical vector </p>
</td></tr>
<tr><td><code id="first_+3A_na.rm">na.rm</code></td>
<td>
<p> if <code>na.rm=TRUE</code>, then the first non-missing value (if any) is returned. By default, it is FALSE and the first element (whatever its value) is returned </p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numerical value
</p>


<h3>Author(s)</h3>

<p> Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>), Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>) </p>


<h3>See Also</h3>

 <p><code><a href="#topic+last">last</a></code>, <code><a href="#topic+turnogram">turnogram</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>a &lt;- c(NA, 1, 2, NA, 3, 4, NA)
first(a)
first(a, na.rm=TRUE)
</code></pre>

<hr>
<h2 id='GetUnitText'> Format a nice time units for labels in graphs </h2><span id='topic+GetUnitText'></span>

<h3>Description</h3>

<p>This is an internal function called by some <code>plot()</code> methods. Considering the time series 'units' attribute and the frequency of the observations in the series, the function returns a string with a pertinent time unit. For instance, if the unit is 'years' and the frequency is 12, then data are monthly sampled and <code>GetUnitText()</code> returns the string &quot;months&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GetUnitText(series)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GetUnitText_+3A_series">series</code></td>
<td>
<p> a regular time series (a 'rts' object in Splus, or a 'ts' object in <span class="rlang"><b>R</b></span>) </p>
</td></tr>
</table>


<h3>Value</h3>

<p>a string with the best time unit for graphs
</p>


<h3>Author(s)</h3>

<p> Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>), Fr?d?ric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>) </p>


<h3>See Also</h3>

 <p><code><a href="#topic+daystoyears">daystoyears</a></code>, <code><a href="#topic+yearstodays">yearstodays</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>timeser &lt;- ts(1:24, frequency=12)           # 12 observations per year
attr(timeser, "units") &lt;- "years"           # time in years for 'ts' object
GetUnitText(timeser)                        # formats unit (1/12 year=months)
</code></pre>

<hr>
<h2 id='is.tseries'> Is this object a time series? </h2><span id='topic+is.tseries'></span>

<h3>Description</h3>

<p>This is equivalent to <code>is.rts()</code> in Splus and to <code>is.ts()</code> in <span class="rlang"><b>R</b></span>. <code>is.tseries()</code> recognizes both 'rts' and 'ts' objects whatever the environment (Splus or R)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.tseries(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.tseries_+3A_x">x</code></td>
<td>
<p> an object </p>
</td></tr>
</table>


<h3>Value</h3>

<p>a boolean value
</p>


<h3>Author(s)</h3>

<p> Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>), Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>) </p>


<h3>See Also</h3>

 <p><code><a href="#topic+tseries">tseries</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>tser &lt;- ts(sin((1:100)/6*pi)+rnorm(100, sd=0.5), start=c(1998, 4), frequency=12)
is.tseries(tser)      # TRUE
not.a.ts &lt;- c(1,2,3)
is.tseries(not.a.ts)  # FALSE
</code></pre>

<hr>
<h2 id='last'> Get the last element of a vector </h2><span id='topic+last'></span>

<h3>Description</h3>

<p>Extract the last element of a vector. Useful for the <code>turnogram()</code> function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>last(x, na.rm=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="last_+3A_x">x</code></td>
<td>
<p> a numerical vector </p>
</td></tr>
<tr><td><code id="last_+3A_na.rm">na.rm</code></td>
<td>
<p> if <code>na.rm=TRUE</code>, then the last non-missing value (if any) is returned. By default, it is FALSE and the last element (whatever its value) is returned </p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numerical value
</p>


<h3>Author(s)</h3>

<p> Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>), Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>) </p>


<h3>See Also</h3>

 <p><code><a href="#topic+first">first</a></code>, <code><a href="#topic+turnogram">turnogram</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>a &lt;- c(NA, 1, 2, NA, 3, 4, NA)
last(a)
last(a, na.rm=TRUE)
</code></pre>

<hr>
<h2 id='local.trend'> Calculate local trends using cumsum </h2><span id='topic+local.trend'></span><span id='topic+identify.local.trend'></span>

<h3>Description</h3>

<p>A simple method using cumulated sums that allows to detect changes in the
tendency in a time series
</p>


<h3>Usage</h3>

<pre><code class='language-R'>local.trend(x, k=mean(x), plotit=TRUE, type="l", cols=1:2, ltys=2:1,
    xlab="Time", ylab="cusum", ...)
## S3 method for class 'local.trend'
identify(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="local.trend_+3A_x">x</code></td>
<td>
<p> a regular time series (a 'ts' object) for <code>local.trend()</code> or a
'local.trend' object for <code>identify()</code> </p>
</td></tr>
<tr><td><code id="local.trend_+3A_k">k</code></td>
<td>
<p> the reference value to substract from cumulated sums. By default, it
is the mean of all observations in the series </p>
</td></tr>
<tr><td><code id="local.trend_+3A_plotit">plotit</code></td>
<td>
<p> if <code>plotit=TRUE</code> (by default), a graph with the cumsum
curve superposed to the original series is plotted </p>
</td></tr>
<tr><td><code id="local.trend_+3A_type">type</code></td>
<td>
<p> the type of plot (as usual notation for this argument) </p>
</td></tr>
<tr><td><code id="local.trend_+3A_cols">cols</code></td>
<td>
<p> colors to use for original data and for the trend line </p>
</td></tr>
<tr><td><code id="local.trend_+3A_ltys">ltys</code></td>
<td>
<p> line types to use for original data and the trend line </p>
</td></tr>
<tr><td><code id="local.trend_+3A_xlab">xlab</code></td>
<td>
<p> label of the x-axis </p>
</td></tr>
<tr><td><code id="local.trend_+3A_ylab">ylab</code></td>
<td>
<p> label of the y-axis </p>
</td></tr>
<tr><td><code id="local.trend_+3A_...">...</code></td>
<td>
<p> additional arguments for the graph </p>
</td></tr>
</table>


<h3>Details</h3>

<p>With <code>local.trend()</code>, you can:
</p>
<p>- detect changes in the mean value of a time series
</p>
<p>- determine the date of occurence of such changes
</p>
<p>- estimate the mean values on homogeneous intervals
</p>


<h3>Value</h3>

<p>a 'local.trend' object is returned. It has the method <code>identify()</code>
</p>


<h3>Note</h3>

<p>Once transitions are identified with this method, you can use
<code>stat.slide()</code> to get more detailed information on each phase. A
smoothing of the series using running medians (see <code>decmedian()</code>) allows
also to detect various levels in a time series, but according to the median
statistic. Under <span class="rlang"><b>R</b></span>, see also the 'strucchange' package for a more complete,
but more complex, implementation of cumsum applied to time series. </p>


<h3>Author(s)</h3>

<p> Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>), Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>) </p>


<h3>References</h3>

 
<p>Ibanez, F., J.M. Fromentin &amp; J. Castel, 1993. <em>Application de la méthode
des sommes cumulées à l'analyse des séries chronologiques océanographiques.</em>
C. R. Acad. Sci. Paris, Life Sciences, 316:745-748.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+trend.test">trend.test</a></code>, <code><a href="#topic+stat.slide">stat.slide</a></code>,
<code><a href="#topic+decmedian">decmedian</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(bnr)
# Calculate and plot cumsum for the 8th series
bnr8.lt &lt;- local.trend(bnr[,8])
# To identify local trends, use:
# identify(bnr8.lt)
# and click points between which you want to compute local linear trends...
</code></pre>

<hr>
<h2 id='marbio'> Several zooplankton taxa measured across a transect </h2><span id='topic+marbio'></span>

<h3>Description</h3>

<p>The <code>marbio</code> data frame has 68 rows (stations) and 24 columns (taxonomic zooplankton groups).
Abundances are expressed in no per cubic meter of seawater.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(marbio)</code></pre>


<h3>Format</h3>

<p>This data frame contains the following columns giving abundance of:
</p>

<dl>
<dt>Acartia</dt><dd><p>Acartia clausi - herbivorous</p>
</dd>
<dt>AdultsOfCalanus</dt><dd><p>Calanus helgolandicus (adults) - herbivorous</p>
</dd>
<dt>Copepodits1</dt><dd><p>Idem (copepodits 1) - omnivorous</p>
</dd>
<dt>Copepodits2</dt><dd><p>Idem (copepodits 2) - omnivorous</p>
</dd>
<dt>Copepodits3</dt><dd><p>Idem (copepodits 3) - omnivorous</p>
</dd>
<dt>Copepodits4</dt><dd><p>Idem (copepodits 4) - omnivorous</p>
</dd>
<dt>Copepodits5</dt><dd><p>Idem (copepodits 5) - omnivorous</p>
</dd>
<dt>ClausocalanusA</dt><dd><p>Clausocalanus size class A - herbivorous</p>
</dd>
<dt>ClausocalanusB</dt><dd><p>Clausocalanus size class B - herbivorous</p>
</dd>
<dt>ClausocalanusC</dt><dd><p>Clausocalanus size class C - herbivorous</p>
</dd>
<dt>AdultsOfCentropages</dt><dd><p>Centropages tipicus (adults) - omnivorous</p>
</dd>
<dt>JuvenilesOfCentropages</dt><dd><p>Centropages typicus (juv.) - omnivorous</p>
</dd>
<dt>Nauplii</dt><dd><p>Nauplii of copepods - filter feeders</p>
</dd>
<dt>Oithona</dt><dd><p>Oithona sp. - carnivorous</p>
</dd>
<dt>Acanthaires</dt><dd><p>Various species of acanthaires - misc</p>
</dd>
<dt>Cladocerans</dt><dd><p>Various species of cladocerans - carnivorous</p>
</dd>
<dt>EchinodermsLarvae</dt><dd><p>Larvae of echinoderms - filter feeders</p>
</dd>
<dt>DecapodsLarvae</dt><dd><p>Larvae of decapods - omnivorous</p>
</dd>
<dt>GasteropodsLarvae</dt><dd><p>Larvae of gastropods - filter feeders</p>
</dd>
<dt>EggsOfCrustaceans</dt><dd><p>Eggs of crustaceans - misc</p>
</dd>
<dt>Ostracods</dt><dd><p>Various species of ostracods - omnivorous</p>
</dd>
<dt>Pteropods</dt><dd><p>Various species of pteropods - herbivorous</p>
</dd>
<dt>Siphonophores</dt><dd><p>Various species of siphonophores - carnivorous</p>
</dd>
<dt>BellsOfCalycophores</dt><dd><p>Bells of calycophores - misc</p>
</dd>
</dl>



<h3>Details</h3>

<p>This dataset corresponds to a single transect sampled with a plankton net across the Ligurian Sea front in the Mediterranean Sea, between Nice (France) and Calvi (Corsica). The transect extends from the Cap Ferrat (close to Nice) to about 65 km offshore in the direction of Calvi (along a bearing of 123°). 68 regularly spaced samples where collected on this transect. For more information about the water masses and their physico-chemical characteristics, see the marphy dataset.
</p>


<h3>Source</h3>

<p>Boucher, J., F. Ibanez &amp; L. Prieur (1987) <em>Daily and seasonal variations in the spatial distribution of zooplankton populations in relation to the physical structure in the Ligurian Sea Front.</em> Journal of Marine Research, 45:133-173.
</p>


<h3>References</h3>

<p>Fromentin, J.-M., F. Ibanez &amp; P. Legendre (1993) <em>A phytosociological method for interpreting plankton data.</em> Marine Ecology Progress Series, 93:285-306.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+marphy">marphy</a></code> </p>

<hr>
<h2 id='marphy'> Physico-chemical records at the same stations as for marbio </h2><span id='topic+marphy'></span>

<h3>Description</h3>

<p>The <code>marphy</code> data frame has 68 rows (stations) and 4 columns.
They are seawater measurements at a deep of 3 to 7 m at the same 68 stations as <code>marbio</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(marphy)</code></pre>


<h3>Format</h3>

<p>This data frame contains the following columns:
</p>

<dl>
<dt>Temperature</dt><dd><p>Seawater temperature in °C</p>
</dd>
<dt>Salinity</dt><dd><p>Salinity in g/kg</p>
</dd>
<dt>Fluorescence</dt><dd><p>Fluorescence of chlorophyll a</p>
</dd>
<dt>Density</dt><dd><p>Excess of volumic mass of the seawater in g/l</p>
</dd>
</dl>



<h3>Details</h3>

<p>This dataset corresponds to a single transect sampled across the Ligurian Sea front in the Mediterranean Sea, between Nice (France) and Calvi (Corsica). The transect extends from the Cap Ferrat (close to Nice) to about 65 km offshore in the direction of Calvi (along a bearing of 123°). 68 regularly spaced measurements where recorded. They correspond to the stations where zooplankton was collected in the <code>marbio</code> dataset. Water masses in the transect across the front where identified as:
</p>

<dl>
<dt>Stations 1 to 17</dt><dd><p>Peripheral zone</p>
</dd>
<dt>Stations 17 to 25</dt><dd><p>D1 (divergence) zone</p>
</dd>
<dt>Stations 25 to 30</dt><dd><p>C (convergence) zone</p>
</dd>
<dt>Stations 30 to 41</dt><dd><p>Frontal zone</p>
</dd>
<dt>Stations 41 to 46</dt><dd><p>D2 (divergence) zone</p>
</dd>
<dt>Stations 46 to 68</dt><dd><p>Central zone</p>
</dd>
</dl>



<h3>Source</h3>

<p>Boucher, J., F. Ibanez &amp; L. Prieur (1987) <em>Daily and seasonal variations in the spatial distribution of zooplankton populations in relation to the physical structure in the Ligurian Sea Front.</em> Journal of Marine Research, 45:133-173.
</p>


<h3>References</h3>

<p>Fromentin, J.-M., F. Ibanez &amp; P. Legendre (1993) <em>A phytosociological method for interpreting plankton data.</em> Marine Ecology Progress Series, 93:285-306.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+marbio">marbio</a></code> </p>

<hr>
<h2 id='match.tol'> Determine matching observation with a tolerance in time-scale </h2><span id='topic+match.tol'></span>

<h3>Description</h3>

<p>Determine which observations in a regular time series match observation in an original irregular one, accepting a given tolerance window in the time-scale. This function is internally used for regulation (functions <code>regul()</code>, <code>regul.screen()</code> and <code>regul.adj()</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>match.tol(x, table, nomatch=NA, tol.type="both", tol=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="match.tol_+3A_x">x</code></td>
<td>
<p> a numerical vector containing seeked values (time-scale of the regular series) </p>
</td></tr>
<tr><td><code id="match.tol_+3A_table">table</code></td>
<td>
<p> a numerical vector containing initial times to look for match in <code>x</code> </p>
</td></tr>
<tr><td><code id="match.tol_+3A_nomatch">nomatch</code></td>
<td>
<p> the symbol tu use to flag an entry where no match is found. By default, <code>nomatch=NA</code> </p>
</td></tr>
<tr><td><code id="match.tol_+3A_tol.type">tol.type</code></td>
<td>
<p> the type of adjustment to use for the time-tolerance: <code>"left"</code>, <code>"right"</code>, <code>"both"</code> (by default) or <code>"none"</code>. If <code>tol.type="left"</code>, corresponding <code>x</code> values are seeked in a window ]table-tol, table]. If <code>tol.type="right"</code>, they are seeked in the window [table, table+tol[. If <code>tol.type="both"</code>, then they are seeked in the window ]table-tol, table+tol]. If several observations are in this window, the closest one is used. Finally, if <code>tol.type="none"</code>, then the function returns the <code>nomatch</code> symbol for all entries </p>
</td></tr>
<tr><td><code id="match.tol_+3A_tol">tol</code></td>
<td>
<p> the tolerance in the time-scale to determine if a value in <code>x</code> matches a value in <code>table</code>. If <code>tol=0</code>, observations in each respective series must match exactly, otherwise observations in the regulated series are interpolated. <code>tol</code> must be a round fraction of the interval between observations in <code>x</code> (x[i+1] - x[i], (x[i+1] - x[i])/2, (x[i+1] - x[i])/3, etc...), and cannot be larger than it, otherwise, <code>tol</code> is automatically adjusted to the closest allowed value. By default, <code>tol=NULL</code>. This is equivalent to <code>tol=0</code>. Warning! </p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of the same length of <code>x</code>, indicating the position of the matching observations in <code>table</code>
</p>


<h3>Author(s)</h3>

<p> Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>), Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>) </p>


<h3>See Also</h3>

 <p><code><a href="#topic+regul">regul</a></code>, <code><a href="#topic+regul.screen">regul.screen</a></code>, <code><a href="#topic+regul.adj">regul.adj</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>X &lt;- 1:5
Table &lt;- c(1, 3.1, 3.8, 4.4, 5.1, 6)
match.tol(X, Table)
match.tol(X, Table, tol=0.2)
match.tol(X, Table, tol=0.55)
</code></pre>

<hr>
<h2 id='pennington'> Calculate Pennington statistics </h2><span id='topic+pennington'></span>

<h3>Description</h3>

<p>Calculate the mean, the variance and the variance of the mean for a single series, according to Pennington (correction for zero/missing values for abundance of species collected with a net)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pennington(x, calc="all", na.rm=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pennington_+3A_x">x</code></td>
<td>
<p> a numerical vector </p>
</td></tr>
<tr><td><code id="pennington_+3A_calc">calc</code></td>
<td>
<p> indicate which estimator(s) should be calculated. Use: <code>"mean"</code>, <code>"var"</code>, <code>"mean.var"</code> or <code>"all"</code> (by default) for the mean, the variance, the variance of the mean or all these three statitics, respectively </p>
</td></tr>
<tr><td><code id="pennington_+3A_na.rm">na.rm</code></td>
<td>
<p> if <code>na.rm=TRUE</code>, missing data are eliminated first. If it is FALSE (by default), any missing value in the series leads to NA as the result for the statistic </p>
</td></tr>
</table>


<h3>Details</h3>

<p>A complex problem in marine ecology is the notion of zero. In fact, the random sampling of a fish population often leads to a table with many null values. Do these null values indicate that the fish was absent or do we have to consider these null values as missing data, that is, that the fish was rare but present and was not caught by the net? For instance, for 100 net trails giving 80 null values, how to estimate the mean? Do we have to divide the sum of fishes caught by 100 or by 20?
</p>
<p>Pennington (1983) applied in this case the probabilistic theory of Aitchison (1955). The later established a method to estimate mean and variance of a random variable with a non-null probability to present zero values and whose the conditional distribution corresponding to its non-null values follows a Gaussian curve. In practice, a lognormal distribution is found most of the time for non-null values in fishes population. It is also often the case for the plankton, after our own experience. <code>pennington()</code> calculates thus statistics for such conditional lognormal distributions.
</p>


<h3>Value</h3>

<p>a single value, or a vector with &quot;mean&quot;, &quot;var&quot; and &quot;mean.var&quot; if the argument <code>calc="all"</code> 
</p>


<h3>Note</h3>

<p>For multiple series, or for getting more statistics on the series, use <code>stat.pen()</code>. Use <code>stat.slide()</code> for obtaining statistics calculated separately for various intervals along the time for a time series
</p>


<h3>Author(s)</h3>

<p> Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>), Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>) </p>


<h3>References</h3>

 
<p>Aitchison, J., 1955. <em>On the distribution of a positive random variable having a discrete probability mass at the origin.</em> J. Amer. Stat. Ass., 50:901-908.
</p>
<p>Pennington, M., 1983. <em>Efficient estimations of abundance for fish and plankton surveys.</em> Biometrics, 39:281-286.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+stat.pen">stat.pen</a></code>, <code><a href="#topic+stat.slide">stat.slide</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(marbio)
pennington(marbio[, "Copepodits2"])
pennington(marbio[, "Copepodits2"], calc="mean", na.rm=TRUE)
</code></pre>

<hr>
<h2 id='pgleissberg'> Gleissberg distribution probability </h2><span id='topic+pgleissberg'></span>

<h3>Description</h3>

<p>The Gleissberg distribution gives the probability to have k extrema in a series of n observations. This distribution is used in the turnogram to determine if monotony indices are significant (see <code>turnogram()</code>)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pgleissberg(n, k, lower.tail=TRUE, two.tailed=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pgleissberg_+3A_n">n</code></td>
<td>
<p> the number of observations in the series </p>
</td></tr>
<tr><td><code id="pgleissberg_+3A_k">k</code></td>
<td>
<p> the number of extrema in the series, as calculated by <code>turnpoints()</code> </p>
</td></tr>
<tr><td><code id="pgleissberg_+3A_lower.tail">lower.tail</code></td>
<td>
<p> if <code>lower.tail=TRUE</code> (by default) and <code>two.tailed=FALSE</code>, the left-side probability is returned. If it is FALSE, the right-side probability is returned </p>
</td></tr>
<tr><td><code id="pgleissberg_+3A_two.tailed">two.tailed</code></td>
<td>
<p> if <code>two.tailed=TRUE</code>, the two-sided probability is returned. By default, it is FALSE and a one-sided probability is returned (left or right, depending on the value of <code>lower.tail</code> </p>
</td></tr>
</table>


<h3>Value</h3>

<p>a value giving the probability to have <code>k</code> extrema in a series of <code>n</code> observations
</p>


<h3>Note</h3>

<p> The Gleissberg distribution is asymptotically normal. For <code>n</code> &gt; 50, the distribution is approximated by a Gaussian curve. For lower <code>n</code> values, the exact probability is returned (using data in the variable <code>.gleissberg.table</code> </p>


<h3>Author(s)</h3>

<p> Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>), Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>) </p>


<h3>References</h3>

 
<p>Dallot, S. &amp; M. Etienne, 1990. <em>Une méthode non paramétrique d'analyse des séries en océanographie biologique: les tournogrammes.</em> Biométrie et océanographie - Société de biométrie, 6, Lille, 26-28 mai 1986. IFREMER, Actes de colloques, 10:13-31.
</p>
<p>Johnson, N.L. &amp; Kotz, S., 1969. <em>Discrete distributions.</em> J. Wiley &amp; sons, New York, 328 pp.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+.gleissberg.table">.gleissberg.table</a></code>, <code><a href="#topic+turnpoints">turnpoints</a></code>, <code><a href="#topic+turnogram">turnogram</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># Until n=50, the exact probability is returned
pgleissberg(20, 10, lower.tail=TRUE, two.tailed=FALSE)
# For higher n values, it is approximated by a normal distribution
pgleissberg(60, 33, lower.tail=TRUE, two.tailed=FALSE)
</code></pre>

<hr>
<h2 id='regarea'> Regulate a series using the area method </h2><span id='topic+regarea'></span>

<h3>Description</h3>

<p>Transform an irregular time series in a regular time series, or fill gaps in regular time series using the area method
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regarea(x, y=NULL, xmin=min(x), n=length(x),
        deltat=(max(x) - min(x))/(n - 1), rule=1,
        window=deltat, interp=FALSE, split=100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regarea_+3A_x">x</code></td>
<td>
<p> a vector with time in the irregular series. Missing values are allowed </p>
</td></tr>
<tr><td><code id="regarea_+3A_y">y</code></td>
<td>
<p> a vector of same length as <code>x</code> and holding observations at corresponding times </p>
</td></tr>
<tr><td><code id="regarea_+3A_xmin">xmin</code></td>
<td>
<p> allows to respecify the origin of time in the calculated regular time series. By default, the origin is not redefined and it is equivalent to the smallest value in <code>x</code> </p>
</td></tr>
<tr><td><code id="regarea_+3A_n">n</code></td>
<td>
<p> the number of observations in the regular time series. By default, it is the same number than in the original irregular time series (i.e., <code>length(x)</code> </p>
</td></tr>
<tr><td><code id="regarea_+3A_deltat">deltat</code></td>
<td>
<p> the time interval between two observations in the regulated time series </p>
</td></tr>
<tr><td><code id="regarea_+3A_rule">rule</code></td>
<td>
<p> the rule to use for extrapolated values (outside of the range in the initial irregular time series) in the regular time series. With <code>rule=1</code> (by default), these entries are not calculated and get <code>NA</code>; with <code>rule=2</code>, these entries are extrapolated </p>
</td></tr>
<tr><td><code id="regarea_+3A_window">window</code></td>
<td>
<p> size of the window to use for interpolation. By default, adjacent windows are used (<code>window=deltat</code>) </p>
</td></tr>
<tr><td><code id="regarea_+3A_interp">interp</code></td>
<td>
<p> indicates if matching observations in both series must be calculated (<code>interp=TRUE</code>), or if initial observations are used &quot;as is&quot; in the final regular series (<code>interp=FALSE</code>, by default) </p>
</td></tr>
<tr><td><code id="regarea_+3A_split">split</code></td>
<td>
<p> a parameter to optimise calculation time and to avoid saturation of the memory. Very long time series are splitted into smaller subunits. This is transparent for the user. The default value of <code>split=100</code> should be rarely changed. Give a lower value if the program fails and reports a memory problem during calculation </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a linear interpolation method described by Fox (1972). It takes into account all observations located in a given time window around the missing observation. On the contrary to many other interpolations (constant, linear, spline), the interpolated curve does not pass by the initial observations. Indeed, a smoothing is also operated simultaneously by this method. The importance of the smoothing is dependent on the size of the window (the largest it is, the smoothest will be the calculated regular time series) 
</p>


<h3>Value</h3>

<p>An object of type 'regul' is returned. It has methods <code>print()</code>, <code>summary()</code>, <code>plot()</code>, <code>lines()</code>, <code>identify()</code>, <code>hist()</code>, <code>extract()</code> and <code>specs()</code>.
</p>


<h3>Author(s)</h3>

<p> Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>), Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>) </p>


<h3>References</h3>

 
<p>Fox, W.T. &amp; J.A. Brown, 1965. <em>The use of time-trend analysis for environmental interpretation of limestones.</em> J. Geol., 73:510-518.
</p>
<p>Ibanez, F., 1991. <em>Treatment of the data deriving from the COST 647 project on coastal benthic ecology: The within-site  analysis.</em> In: B. Keegan (ed). Space and Time Series Data Analysis in Coastal Benthic Ecology. Pp 5-43.
</p>
<p>Ibanez, F. &amp; J.C. Dauvin, 1988. <em>Long-term changes (1977-1987) on a muddy fine sand Abra alba - Melinna palmata population community from the Western English Channel.</em> J. Mar. Ecol. Prog. Ser., 49:65-81.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+regul">regul</a></code>, <code><a href="#topic+regconst">regconst</a></code>, <code><a href="#topic+reglin">reglin</a></code>, <code><a href="#topic+regspline">regspline</a></code>, <code><a href="#topic+regul.screen">regul.screen</a></code>, <code><a href="#topic+regul.adj">regul.adj</a></code>, <code><a href="#topic+tseries">tseries</a></code>, <code><a href="#topic+is.tseries">is.tseries</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(releve)
# The 'Melosul' series is regulated with a 25-days window
reg &lt;- regarea(releve$Day, releve$Melosul, window=25)
# Then with a 50-days window
reg2 &lt;- regarea(releve$Day, releve$Melosul, window=50)
# The initial and both regulated series are shown on the graph for comparison
plot(releve$Day, releve$Melosul, type="l")
lines(reg$x, reg$y, col=2)
lines(reg2$x, reg2$y, col=4)
</code></pre>

<hr>
<h2 id='regconst'> Regulate a series using the constant value method </h2><span id='topic+regconst'></span>

<h3>Description</h3>

<p>Transform an irregular time series in a regular time series, or fill gaps in regular time series using the constant value method
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regconst(x, y=NULL, xmin=min(x), n=length(x),
        deltat=(max(x) - min(x))/(n - 1), rule=1, f=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regconst_+3A_x">x</code></td>
<td>
<p> a vector with time in the irregular series. Missing values are allowed </p>
</td></tr>
<tr><td><code id="regconst_+3A_y">y</code></td>
<td>
<p> a vector of same length as <code>x</code> and holding observations at corresponding times </p>
</td></tr>
<tr><td><code id="regconst_+3A_xmin">xmin</code></td>
<td>
<p> allows to respecify the origin of time in the calculated regular time series. By default, the origin is not redefined and it is equivalent to the smallest value in <code>x</code> </p>
</td></tr>
<tr><td><code id="regconst_+3A_n">n</code></td>
<td>
<p> the number of observations in the regular time series. By default, it is the same number than in the original irregular time series (i.e., <code>length(x)</code> </p>
</td></tr>
<tr><td><code id="regconst_+3A_deltat">deltat</code></td>
<td>
<p> the time interval between two observations in the regulated time series </p>
</td></tr>
<tr><td><code id="regconst_+3A_rule">rule</code></td>
<td>
<p> the rule to use for extrapolated values (outside of the range in the initial irregular time series) in the regular time series. With <code>rule=1</code> (by default), these entries are not calculated and get <code>NA</code>; with <code>rule=2</code>, these entries are extrapolated </p>
</td></tr>
<tr><td><code id="regconst_+3A_f">f</code></td>
<td>
<p> coefficient giving more weight to the left value (<code>f=0</code>, by default), to the right value (<code>f=</code>) or to a combination of these two observations (0 &lt; f &lt;1) </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is the simplest, but the less powerful regulation method. Interpolated values are calculated according to existing observations at left and at right as: x[reg] = x[right]*f + x[left]*(f-1), with 0 &lt; f &lt; 1.
</p>


<h3>Value</h3>

<p>An object of type 'regul' is returned. It has methods <code>print()</code>, <code>summary()</code>, <code>plot()</code>, <code>lines()</code>, <code>identify()</code>, <code>hist()</code>, <code>extract()</code> and <code>specs()</code>.  
</p>


<h3>Note</h3>

<p> This function uses <code>approx()</code> for internal calculations </p>


<h3>Author(s)</h3>

<p> Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>), Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>) </p>


<h3>See Also</h3>

 <p><code><a href="#topic+regul">regul</a></code>, <code><a href="#topic+regarea">regarea</a></code>, <code><a href="#topic+reglin">reglin</a></code>, <code><a href="#topic+regspline">regspline</a></code>, <code><a href="#topic+regul.screen">regul.screen</a></code>, <code><a href="#topic+regul.adj">regul.adj</a></code>, <code><a href="#topic+tseries">tseries</a></code>, <code><a href="#topic+is.tseries">is.tseries</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(releve)
reg &lt;- regconst(releve$Day, releve$Melosul)
plot(releve$Day, releve$Melosul, type="l")
lines(reg$x, reg$y, col=2)
</code></pre>

<hr>
<h2 id='reglin'> Regulation of a series using a linear interpolation </h2><span id='topic+reglin'></span>

<h3>Description</h3>

<p>Transform an irregular time series in a regular time series, or fill gaps in regular time series using a linear interpolation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reglin(x, y=NULL, xmin=min(x), n=length(x),
        deltat=(max(x) - min(x))/(n - 1), rule=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reglin_+3A_x">x</code></td>
<td>
<p> a vector with time in the irregular series. Missing values are allowed </p>
</td></tr>
<tr><td><code id="reglin_+3A_y">y</code></td>
<td>
<p> a vector of same length as <code>x</code> and holding observations at corresponding times </p>
</td></tr>
<tr><td><code id="reglin_+3A_xmin">xmin</code></td>
<td>
<p> allows to respecify the origin of time in the calculated regular time series. By default, the origin is not redefined and it is equivalent to the smallest value in <code>x</code> </p>
</td></tr>
<tr><td><code id="reglin_+3A_n">n</code></td>
<td>
<p> the number of observations in the regular time series. By default, it is the same number than in the original irregular time series (i.e., <code>length(x)</code> </p>
</td></tr>
<tr><td><code id="reglin_+3A_deltat">deltat</code></td>
<td>
<p> the time interval between two observations in the regulated time series </p>
</td></tr>
<tr><td><code id="reglin_+3A_rule">rule</code></td>
<td>
<p> the rule to use for extrapolated values (outside of the range in the initial irregular time series) in the regular time series. With <code>rule=1</code> (by default), these entries are not calculated and get <code>NA</code>; with <code>rule=2</code>, these entries are extrapolated </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Observed values are connected by lines and interpolated values are obtained from this &quot;polyline&quot;.
</p>


<h3>Value</h3>

<p>An object of type 'regul' is returned. It has methods <code>print()</code>, <code>summary()</code>, <code>plot()</code>, <code>lines()</code>, <code>identify()</code>, <code>hist()</code>, <code>extract()</code> and <code>specs()</code>.
</p>


<h3>Note</h3>

<p> This function uses <code>approx()</code> for internal calculations </p>


<h3>Author(s)</h3>

<p> Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>), Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>) </p>


<h3>See Also</h3>

 <p><code><a href="#topic+regul">regul</a></code>, <code><a href="#topic+regarea">regarea</a></code>, <code><a href="#topic+regconst">regconst</a></code>, <code><a href="#topic+regspline">regspline</a></code>, <code><a href="#topic+regul.screen">regul.screen</a></code>, <code><a href="#topic+regul.adj">regul.adj</a></code>, <code><a href="#topic+tseries">tseries</a></code>, <code><a href="#topic+is.tseries">is.tseries</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(releve)
reg &lt;- reglin(releve$Day, releve$Melosul)
plot(releve$Day, releve$Melosul, type="l")
lines(reg$x, reg$y, col=2)
</code></pre>

<hr>
<h2 id='regspline'> Regulation of a time series using splines </h2><span id='topic+regspline'></span>

<h3>Description</h3>

<p>Transform an irregular time series in a regular time series, or fill gaps in regular time series using splines
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regspline(x, y=NULL, xmin=min(x), n=length(x),
        deltat=(max(x) - min(x))/(n - 1), rule=1, periodic=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regspline_+3A_x">x</code></td>
<td>
<p> a vector with time in the irregular series. Missing values are allowed </p>
</td></tr>
<tr><td><code id="regspline_+3A_y">y</code></td>
<td>
<p> a vector of same length as <code>x</code> and holding observations at corresponding times </p>
</td></tr>
<tr><td><code id="regspline_+3A_xmin">xmin</code></td>
<td>
<p> allows to respecify the origin of time in the calculated regular time series. By default, the origin is not redefined and it is equivalent to the smallest value in <code>x</code> </p>
</td></tr>
<tr><td><code id="regspline_+3A_n">n</code></td>
<td>
<p> the number of observations in the regular time series. By default, it is the same number than in the original irregular time series (i.e., <code>length(x)</code> </p>
</td></tr>
<tr><td><code id="regspline_+3A_deltat">deltat</code></td>
<td>
<p> the time interval between two observations in the regulated time series </p>
</td></tr>
<tr><td><code id="regspline_+3A_rule">rule</code></td>
<td>
<p> the rule to use for extrapolated values (outside of the range in the initial irregular time series) in the regular time series. With <code>rule=1</code> (by default), these entries are not calculated and get <code>NA</code>; with <code>rule=2</code>, these entries are extrapolated </p>
</td></tr>
<tr><td><code id="regspline_+3A_periodic">periodic</code></td>
<td>
<p> indicates if the time series should be considered as periodic (<code>periodic=TRUE</code>, first value must be equal to the last one). If this is the case, first and second derivates used to calculate spline segments around first and last observations use data in the other extreme of the series. In the other case (<code>periodic=FALSE</code> (by default), derivates for extremes observations are considered to be equal to zero </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Missing values are interpolated using cubic splines between observed values.
</p>


<h3>Value</h3>

<p>An object of type 'regul' is returned. It has methods <code>print()</code>, <code>summary()</code>, <code>plot()</code>, <code>lines()</code>, <code>identify()</code>, <code>hist()</code>, <code>extract()</code> and <code>specs()</code>.
</p>


<h3>Note</h3>

<p> This function uses <code>spline()</code> for internal calculations. However, interpolated values are not allowed to be higher than the largest initial observation or lower than the smallest one. </p>


<h3>Author(s)</h3>

<p> Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>), Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>) </p>


<h3>References</h3>

 
<p>Lancaster, P. &amp; K. Salkauskas, 1986. <em>Curve and surface fitting.</em> Academic Press, England, 280 pp.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+regul">regul</a></code>, <code><a href="#topic+regarea">regarea</a></code>, <code><a href="#topic+regconst">regconst</a></code>, <code><a href="#topic+reglin">reglin</a></code>, <code><a href="#topic+regul.screen">regul.screen</a></code>, <code><a href="#topic+regul.adj">regul.adj</a></code>, <code><a href="#topic+tseries">tseries</a></code>, <code><a href="#topic+is.tseries">is.tseries</a></code>, <code><a href="stats.html#topic+splinefun">splinefun</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(releve)
reg &lt;- regspline(releve$Day, releve$Melosul)
plot(releve$Day, releve$Melosul, type="l")
lines(reg$x, reg$y, col=2)
</code></pre>

<hr>
<h2 id='regul'> Regulation of one or several time series using various methods </h2><span id='topic+regul'></span><span id='topic+extract.regul'></span><span id='topic+hist.regul'></span><span id='topic+identify.regul'></span><span id='topic+lines.regul'></span><span id='topic+plot.regul'></span><span id='topic+print.regul'></span><span id='topic+print.specs.regul'></span><span id='topic+print.summary.regul'></span><span id='topic+specs.regul'></span><span id='topic+summary.regul'></span>

<h3>Description</h3>

<p>Regulate irregular time series or regular time series with gaps. Create a <code>regul</code> object from whose one or several regular time series can be extracted using <code>extract()</code> or <code>tseries()</code>. This is the function to apply most of the time to create regular time series ('rts' objects in Splus or 'ts' objects in <span class="rlang"><b>R</b></span>) that will be further analyzed by other functions that apply to regular time series.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regul(x, y=NULL, xmin=min(x), n=length(x), units="days", frequency=NULL,
        deltat=1/frequency, datemin=NULL, dateformat="m/d/Y", tol=NULL,
        tol.type="both", methods="linear", rule=1, f=0, periodic=FALSE,
        window=(max(x) - min(x))/(n - 1), split=100, specs=NULL)
## S3 method for class 'regul'
print(x, ...)
## S3 method for class 'regul'
summary(object, ...)
## S3 method for class 'summary.regul'
print(x, ...)
## S3 method for class 'regul'
plot(x, series=1, col=c(1, 2), lty=c(par("lty"), par("lty")), plot.pts=TRUE,
        leg=FALSE, llab=c("initial", x$specs$methods[series]), lpos=c(1.5, 10),
        xlab=paste("Time (", x$units, ")", sep = ""), ylab="Series",
        main=paste("Regulation of", names(x$y)[series]), ...)
## S3 method for class 'regul'
lines(x, series=1, col=3, lty=1, plot.pts=TRUE, ...)
## S3 method for class 'regul'
identify(x, series=1, col=3, label="#", ...)
## S3 method for class 'regul'
hist(x, nclass=30, col=c(4, 5, 2),
        xlab=paste("Time distance in", x$units, "with start =", min(x$x),
        ", n = ", length(x$x), ", deltat =", x$tspar$deltat),
        ylab=paste("Frequency, tol =", x$specs$tol),
        main="Number of matching observations", plotit=TRUE, ...)
## S3 method for class 'regul'
extract(e, n, series=NULL, ...)
## S3 method for class 'regul'
specs(x, ...)
## S3 method for class 'specs.regul'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regul_+3A_x">x</code></td>
<td>
<p> for regul: a vector containing times at which observations are sampled in the initial irregular time series. It can be expressed in any unit (&quot;years&quot;, &quot;days&quot;, &quot;weeks&quot;, &quot;hours&quot;, &quot;min&quot;, &quot;sec&quot;,...) as defined by the argument <code>units</code>. It is often expressed in &quot;days&quot; and the decimal part represents the part of the day, that is the time in hour:min:sec (dates coming from Excel, or even standard dates in S+ or <span class="rlang"><b>R</b></span> are expressed like that). For the methods, a 'tsd' object </p>
</td></tr>
<tr><td><code id="regul_+3A_y">y</code></td>
<td>
<p> a vector (single series) or a matrix/data frame whose columns correspond to the various irregular time series to regulate. Rows are observations made at corresponding times in <code>x</code>. The number of rows must thus match the length of vector <code>x</code> </p>
</td></tr>
<tr><td><code id="regul_+3A_xmin">xmin</code></td>
<td>
<p> allows to respecify the origin of time in <code>x</code>. By default, the origin is not redefined and thus, the smallest value in <code>x</code> is used </p>
</td></tr>
<tr><td><code id="regul_+3A_n">n</code></td>
<td>
<p> the number of observations in the regular time series. By default, it is the same number than in the original irregular time series (i.e., <code>length(x)</code> </p>
</td></tr>
<tr><td><code id="regul_+3A_units">units</code></td>
<td>
<p> the time unit for the <code>x</code> vector. By default <code>units="days"</code>. A special value, <code>units="daystoyears"</code> indicates that <code>x</code> is expressed in &quot;days&quot; (1 unit = 1 day) but that we want to obtain the final regular time series expressed in &quot;years&quot; (1 unit = 1 year). Give a correct value for <code>datemin</code> to make sure the right fraction of the year is computed for each observation (see example hereunder) </p>
</td></tr>
<tr><td><code id="regul_+3A_frequency">frequency</code></td>
<td>
<p> the frequency of the regulated time series in the corresponding time unit. For instance, <code>frequency=12</code> with <code>units="years"</code> means montly sampled observations. Warning! When using <code>units="daystoyears"</code>, specify <code>frequency</code> (or <code>deltat</code>) in years! </p>
</td></tr>
<tr><td><code id="regul_+3A_deltat">deltat</code></td>
<td>
<p> the interval between two observations in the regulated time series. It is the inverse of <code>frequency</code>. If both <code>frequency</code> and <code>deltat</code> are provided, then <code>frequency</code> supersedes <code>deltat</code> </p>
</td></tr>
<tr><td><code id="regul_+3A_datemin">datemin</code></td>
<td>
<p> this is mostly useful for converting &quot;days&quot; in &quot;years&quot; time-scales (<code>units="daystoyears"</code>). If the <code>x</code> vector contains: 1, 3, 6,... (day 1, day 3, day 6... of the experiment), one can give here the exact date of the first observation, allowing to define a correct origin in the &quot;years&quot; time scale. Provide a string in a format compatible with <code>dateformat</code>. For instance, if day 1 is the 21th March 1998, give <code>datemin="03/21/1998"</code> with <code>dateformat="m/d/Y"</code> </p>
</td></tr>
<tr><td><code id="regul_+3A_dateformat">dateformat</code></td>
<td>
<p> indicate how <code>datemin</code> is formated. For instance: <code>"d/m/Y"</code>, or <code>"m/d/Y"</code> (by default), see <code>daystoyears()</code> for more info on date formatting </p>
</td></tr>
<tr><td><code id="regul_+3A_tol">tol</code></td>
<td>
<p> the tolerance in the time-scale to determine if a measured value is used to approximate a regulated value. If <code>tol=0</code>, observations in each respective series must match exactly, otherwise observations in the regulated series are interpolated. <code>tol</code> must be a round fraction of <code>deltat</code> (deltat, deltat/2, deltat/3, etc...), and cannot be larger than it, otherwise, <code>tol</code> is automatically adjusted to the closest allowed value. By default, <code>tol=NULL</code>. This is equivalent to <code>tol=0</code>. Warning! In the particular case of <code>units="daystoyears"</code>, <code>tol</code> must be expressed in the original time-scale, that is &quot;days&quot;, while <code>deltat</code> must be expressed in the fimal time-scale, that is &quot;years&quot;! </p>
</td></tr>
<tr><td><code id="regul_+3A_tol.type">tol.type</code></td>
<td>
<p> the type of adjustment to use for the time-tolerance: <code>"left"</code>, <code>"right"</code>, <code>"both"</code> (by default) or <code>"none"</code>. If <code>tol.type="left"</code>, corresponding <code>x</code> values are seeked in a window ]xregul-tol, xregul]. If <code>tol.type="right"</code>, they are seeked in the window [xregul, xregul+tol[. If <code>tol.type="both"</code>, then they are seeked in the window ]xregul-tol, xregul+tol]. If several observations are in this window, the closest one is used. Finally, if <code>tol.type="none"</code>, then <em>all</em> observations in the regulated time series are interpolated (even if exactly matching observations exist!) </p>
</td></tr>
<tr><td><code id="regul_+3A_methods">methods</code></td>
<td>
<p> the method(s) to use to regulate the time series. Currently, it can be: <code>"constant"</code>, <code>"linear"</code>, <code>"spline"</code> or <code>"area"</code> (or a unique abbreviation of them). If several time series are provided (<code>y</code> is a matrix or a data frame), it is possible to define methods individually for each series. For instance, <code>methods=c("l", "a", "s")</code> defines the &quot;linear&quot; method for the first series, the &quot;area&quot; method for the second one, the &quot;spline&quot; method for the third one,... and again the &quot;linear&quot; for the fourth, the &quot;area&quot; for the fifth one, etc. (recycling rule). By default, the &quot;linear&quot; method is selected for all series </p>
</td></tr>
<tr><td><code id="regul_+3A_rule">rule</code></td>
<td>
<p> the rule to use for extrapolated values (observations in the final regular time series that are outside the range of observed values in the initial time series). With <code>rule=1</code> (by default), these entries are not calculated and get NA; with <code>rule=2</code>, these entries are extrapolated (avoid using this option, or use with extreme care!!!) </p>
</td></tr>
<tr><td><code id="regul_+3A_f">f</code></td>
<td>
<p> parameter for the <code>"constant"</code> regulation method. Coefficient giving more weight to the observation at left (<code>f=0</code>, by default), to the observation at right (<code>f=1</code>), or give an intermediate weight to both of these observations (0 &lt; f &lt; 1) during the interpolation (see <code>reglin()</code> </p>
</td></tr>
<tr><td><code id="regul_+3A_periodic">periodic</code></td>
<td>
<p> parameter for the <code>"spline"</code> regulation method. Indicate if the time series should be considered as periodic (<code>periodic=TRUE</code>, first value must be equal to the last one). If this is the case, first and second derivates used to calculate spline segments around first and last observations use data in the other extreme of the series. In the other case (<code>periodic=FALSE</code>, by default), derivates for extremes observations are considered to be equal to zero </p>
</td></tr>
<tr><td><code id="regul_+3A_window">window</code></td>
<td>
<p> parameter for the <code>"area"</code> regulation method. Size of the window to consider (see <code>regarea()</code>). By default, the mean interval between observations in the initial irregular time series is used. Give the same value as for deltat for working with adjacent windows </p>
</td></tr>
<tr><td><code id="regul_+3A_split">split</code></td>
<td>
<p> other parameter for the <code>"area"</code> method. To optimise calculation time and to avoid to saturate memory, very long time series are splitted into smaller subunits (see <code>regarea()</code>). This is transparent for the user. The default value of <code>split=100</code> should be rarely changed. Give a lower value if the program fails and reports a memory problem during calculation </p>
</td></tr>
<tr><td><code id="regul_+3A_specs">specs</code></td>
<td>
<p> a <code>specs.regul</code> object returned by the function <code>specs()</code> applied to a <code>regul</code> object. Allows to collect parameterization of the <code>regul()</code> function and to apply them to another regulation </p>
</td></tr>
<tr><td><code id="regul_+3A_object">object</code></td>
<td>
<p> A <code>regul</code> object as obtained after using the <code>regul()</code> function </p>
</td></tr>
<tr><td><code id="regul_+3A_e">e</code></td>
<td>
<p> A <code>regul</code> object as obtained after using the <code>regul()</code> function </p>
</td></tr>
<tr><td><code id="regul_+3A_series">series</code></td>
<td>
<p> the series to plot. By default, <code>series=1</code>, corresponding to the first (or possibly the unique) series in the <code>regul</code> object </p>
</td></tr>
<tr><td><code id="regul_+3A_col">col</code></td>
<td>
<p> (1) for <code>plot()</code>: the two colors to use to draw respectively the initial irregular series and the final regulated series. <code>col=c(1,2)</code> by default. (2) for <code>hist()</code>: the three colors to use to represent respectively the fist bar (exact coincidence), the middle bars (coincidence in a certain tolerance window) and the last bar (values always interpolated). By default, <code>col=c(4,5,2)</code> </p>
</td></tr>
<tr><td><code id="regul_+3A_lty">lty</code></td>
<td>
<p> the style to use to draw lines for the initial series and the regulated series, respectively. The default style is used for both lines if this argument is not provided </p>
</td></tr>
<tr><td><code id="regul_+3A_plot.pts">plot.pts</code></td>
<td>
<p> if <code>plot.pts=TRUE</code> (by default) then points are also drawn for the regulated series (+). Those points that match observations in the initial irregular series, and are not interpolated, are further marked with a circle </p>
</td></tr>
<tr><td><code id="regul_+3A_leg">leg</code></td>
<td>
<p> do we add a legend to the graph? By default, <code>leg=FALSE</code>, no legend is added </p>
</td></tr>
<tr><td><code id="regul_+3A_llab">llab</code></td>
<td>
<p> the labels to use for the initial irregular and the final regulated series, respectively. By default, it is <code>"initial"</code> for the first one and the name of the regulation method used for the second one (see <code>methods</code> argument) </p>
</td></tr>
<tr><td><code id="regul_+3A_lpos">lpos</code></td>
<td>
<p> the position of the top-left corner of the legend box (x,y), in the graph coordinates </p>
</td></tr>
<tr><td><code id="regul_+3A_xlab">xlab</code></td>
<td>
<p> the label of the x-axis </p>
</td></tr>
<tr><td><code id="regul_+3A_ylab">ylab</code></td>
<td>
<p> the label of the y-axis </p>
</td></tr>
<tr><td><code id="regul_+3A_main">main</code></td>
<td>
<p> the main title of the graph</p>
</td></tr>
<tr><td><code id="regul_+3A_label">label</code></td>
<td>
<p> the character to use to mark points interactively selected on the graph. By default, <code>label="#"</code> </p>
</td></tr>
<tr><td><code id="regul_+3A_nclass">nclass</code></td>
<td>
<p> the number of classes to calculate in the histogram. This is indicative and this value is automatically adjusted to obtain a nicely-formatted histogram. By default, <code>nclass=30</code> </p>
</td></tr>
<tr><td><code id="regul_+3A_plotit">plotit</code></td>
<td>
<p> If <code>plotit=TRUE</code> then the histogram is plotted. Otherwise, it is only calculated </p>
</td></tr>
<tr><td><code id="regul_+3A_...">...</code></td>
<td>
<p> additional parameters </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Several irregular time series (for instance, contained in a data frame) can be treated at once. Specify a vector with <code>"constant"</code>, <code>"linear"</code>, <code>"spline"</code> or <code>"area"</code> for the argument <code>methods</code> to use a different regulation method for each series. See corresponding fonctions (<code>regconst()</code>, <code>reglin()</code>, <code>regspline()</code> and <code>regarea()</code>), respectively, for more details on these methods. Arguments can be saved in a <code>specs</code> object and reused for other similar regulation processes. Functions <code>regul.screen()</code> and <code>regul.adj()</code> are useful to chose best time interval in the computed regular time series. If you want to work on seasonal effects in the time series, you will better use a &quot;years&quot; time-scale (1 unit = 1 year), or convert into such a scale. If initial time unit is &quot;days&quot; (1 unit = 1 day), a conversion can be operated at the same time as the regulation by specifying <code>units="daystoyears"</code>.
</p>


<h3>Value</h3>

<p>An object of type 'regul' is returned. It has methods <code>print()</code>, <code>summary()</code>, <code>plot()</code>, <code>lines()</code>, <code>identify()</code>, <code>hist()</code>, <code>extract()</code> and <code>specs()</code>.
</p>


<h3>Author(s)</h3>

<p> Fr?d?ric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>), Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>) </p>


<h3>References</h3>

<p>Lancaster, P. &amp; K. Salkauskas, 1986. <em>Curve and surface fitting.</em> Academic Press, England, 280 pp.
</p>
<p>Fox, W.T. &amp; J.A. Brown, 1965. <em>The use of time-trend analysis for environmental interpretation of limestones.</em> J. Geol., 73:510-518.
</p>
<p>Ibanez, F., 1991. <em>Treatment of the data deriving from the COST 647 project on coastal benthic ecology: The within-site  analysis.</em> In: B. Keegan (ed). Space and Time Series Data Analysis in Coastal Benthic Ecology. Pp 5-43.
</p>
<p>Ibanez, F. &amp; J.C. Dauvin, 1988. <em>Long-term changes (1977-1987) on a muddy fine sand Abra alba - Melinna palmata population community from the Western English Channel.</em> J. Mar. Ecol. Prog. Ser., 49:65-81.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+regul.screen">regul.screen</a></code>, <code><a href="#topic+regul.adj">regul.adj</a></code>, <code><a href="#topic+tseries">tseries</a></code>, <code><a href="#topic+is.tseries">is.tseries</a></code>, <code><a href="#topic+regconst">regconst</a></code>, <code><a href="#topic+reglin">reglin</a></code>, <code><a href="#topic+regspline">regspline</a></code>, <code><a href="#topic+regarea">regarea</a></code>, <code><a href="#topic+daystoyears">daystoyears</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(releve)
# The series in this data frame are very irregularly sampled in time:
releve$Day
length(releve$Day)
intervals &lt;- releve$Day[2:61]-releve$Day[1:60]
intervals
range(intervals)
mean(intervals)
# The series must be regulated to be converted in a 'rts' or 'ts object
rel.reg &lt;- regul(releve$Day, releve[3:8], xmin=9, n=63, deltat=21,
        tol=1.05, methods=c("s","c","l","a","s","a"), window=21)
rel.reg
plot(rel.reg, 5)
specs(rel.reg)
# Now we can extract one or several regular time series
melo.ts &lt;- extract(rel.reg, series="Melosul")
is.tseries(melo.ts)

# One can convert time-scale from "days" to "years" during regulation
# This is most useful for analyzing seasonal cycles in a second step
melo.regy &lt;- regul(releve$Day, releve$Melosul, xmin=6, n=87,
        units="daystoyears", frequency=24, tol=2.2, methods="linear",
        datemin="21/03/1989", dateformat="d/m/Y")
melo.regy
plot(melo.regy, main="Regulation of Melosul")
# In this case, we have only one series in 'melo.regy'
# We can use also 'tseries()' instead of 'extract()'
melo.tsy &lt;- tseries(melo.regy)
is.tseries(melo.tsy)
</code></pre>

<hr>
<h2 id='regul.adj'> Adjust regulation parameters </h2><span id='topic+regul.adj'></span>

<h3>Description</h3>

<p>Calculate and plot an histogram of the distances between interpolated observations in a regulated time series and closest observations in the initial irregular time series. This allows to optimise the <code>tol</code> parameter
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regul.adj(x, xmin=min(x), frequency=NULL,
     deltat=(max(x, na.rm = TRUE) - min(x, na.rm = TRUE))/(length(x) - 1),
     tol=deltat, tol.type="both", nclass=50, col=c(4, 5, 2),
     xlab=paste("Time distance"), ylab=paste("Frequency"),
     main="Number of matching observations", plotit=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regul.adj_+3A_x">x</code></td>
<td>
<p> a vector with times corresponding to the observations in the irregular initial time series </p>
</td></tr>
<tr><td><code id="regul.adj_+3A_xmin">xmin</code></td>
<td>
<p> the time corresponding to the first observation in the regular time series </p>
</td></tr>
<tr><td><code id="regul.adj_+3A_frequency">frequency</code></td>
<td>
<p> the frequency of observations in the regular time series </p>
</td></tr>
<tr><td><code id="regul.adj_+3A_deltat">deltat</code></td>
<td>
<p> the interval between two successive observations in the regular time series. This is the inverse of <code>frequency</code>. Only one of both parameters need to be given. If both are provided, <code>frequency</code> supersedes <code>deltat</code> </p>
</td></tr>
<tr><td><code id="regul.adj_+3A_tol">tol</code></td>
<td>
<p> the tolerance in the difference between two matching observations (in the original irregular series and in the regulated series). If <code>tol=0</code> both values must be strictly identical; a higher value for <code>tol</code> allows some fuzzy matching. <code>tol</code> must be a round fraction of <code>deltat</code> and cannot be higher than it, otherwise, it is adjusted to the closest acceptable value. By default, <code>tol=deltat</code> </p>
</td></tr>
<tr><td><code id="regul.adj_+3A_tol.type">tol.type</code></td>
<td>
<p> the type of window to use for the time-tolerance: <code>"left"</code>, <code>"right"</code>, <code>"both"</code> (by default) or <code>"none"</code>. If <code>tol.type="left"</code>, corresponding <code>x</code> values are seeked in a window ]xregul-tol, xregul]. If <code>tol.type="right"</code>, they are seeked in the window [xregul, xregul+tol[. If <code>tol.type="both"</code>, then they are seeked in the window ]xregul-tol, xregul+tol]. If several observations are in this window, the closest one is used. Finally, if <code>tol.type="none"</code>, then <em>all</em> observations in the regulated time series are interpolated (even if exactly matching observations exist!) </p>
</td></tr>
<tr><td><code id="regul.adj_+3A_nclass">nclass</code></td>
<td>
<p> the number of classes to compute in the histogram. This is indicative, and will be adjusted by the algorithm to produce a nicely-formatted histogram. The default value is <code>nclass=50</code>. It is acceptable in many cases, but if the histogram is not correct, try a larger value </p>
</td></tr>
<tr><td><code id="regul.adj_+3A_col">col</code></td>
<td>
<p> the three colors to use to represent respectively the fist bar (exact coincidence), the middle bars (coincidence in a certain tolerance window) and the last bar (values always interpolated). By default, <code>col=c(4,5,2)</code> </p>
</td></tr>
<tr><td><code id="regul.adj_+3A_xlab">xlab</code></td>
<td>
<p> the label of the x-axis </p>
</td></tr>
<tr><td><code id="regul.adj_+3A_ylab">ylab</code></td>
<td>
<p> the label of the y-axis </p>
</td></tr>
<tr><td><code id="regul.adj_+3A_main">main</code></td>
<td>
<p> the main title of the graph</p>
</td></tr>
<tr><td><code id="regul.adj_+3A_plotit">plotit</code></td>
<td>
<p> if <code>plotit=TRUE</code> then the histogram is plotted. Otherwise, it is only calculated </p>
</td></tr>
<tr><td><code id="regul.adj_+3A_...">...</code></td>
<td>
<p> additional graph parameters for the histogram </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is complementary to <code>regul.screen()</code>. While the later look for the best combination of the number of observations, the interval between observations and the position of the first observation on the time-scale for the regular time series, <code>regul.adj()</code> look for the optimal value for <code>tol</code>, the tolerance window.
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>params</code></td>
<td>
<p> the parameters used for the regular time-scale </p>
</td></tr>
<tr><td><code>match</code></td>
<td>
<p> the number of matching observations in the tolerance window </p>
</td></tr>
<tr><td><code>exact.match</code></td>
<td>
<p> the number of exact matching observations </p>
</td></tr>
<tr><td><code>match.counts</code></td>
<td>
<p> a vector with the number of matching observations for increasing values of <code>tol</code> </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>), Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>) </p>


<h3>See Also</h3>

 <p><code><a href="#topic+regul.screen">regul.screen</a></code>, <code><a href="#topic+regul">regul</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'># This example follows the example for regul.screen()
# where we determined that xmin=9, deltat=21, n=63, with tol=1.05
# is a good choice to regulate the irregular time series in 'releve' 
data(releve)
regul.adj(releve$Day, xmin=9, deltat=21)
# The histogram indicates that it is not useful to increase tol
# more than 1.05, because few observations will be added
# except if we increase it to 5-7, but this value could be
# considered to be too large in comparison with deltat=22
# On the other hand, with tol &lt;= 1, the number of matching
# observations will be almost divided by two!
</code></pre>

<hr>
<h2 id='regul.screen'> Test various regulation parameters </h2><span id='topic+regul.screen'></span>

<h3>Description</h3>

<p>Seek for the best combination of the number of observation, the interval between two successive observation and the position of the first observation in the regulated time series to match as much observations of the initial series as possible
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regul.screen(x, weight=NULL, xmin=min(x), frequency=NULL,
    deltat=(max(x, na.rm = TRUE) - min(x, na.rm = TRUE))/(length(x) - 1),
    tol=deltat/5, tol.type="both")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regul.screen_+3A_x">x</code></td>
<td>
<p> a vector with times corresponding to the observations in the irregular initial time series </p>
</td></tr>
<tr><td><code id="regul.screen_+3A_weight">weight</code></td>
<td>
<p> a vector of the same length as <code>x</code>, with the weight to give to each observation. A value of 0 indicates to ignore an observation. A value of 1 gives a normal weight to an observation. A higher value gives more importance to the corresponding observation. You can increase weight of observations around major peaks and pits, to make sure they are not lost in the regulated time series. If <code>weight=NULL</code> (by default), then a weight of 1 is used for all observations </p>
</td></tr>
<tr><td><code id="regul.screen_+3A_xmin">xmin</code></td>
<td>
<p> a vector with all time values for the first observation in the regulated time series to be tested </p>
</td></tr>
<tr><td><code id="regul.screen_+3A_frequency">frequency</code></td>
<td>
<p> a vector with all the frequencies to be screened </p>
</td></tr>
<tr><td><code id="regul.screen_+3A_deltat">deltat</code></td>
<td>
<p> a vector with all time intervals to screen. <code>deltat</code> is the inverse of <code>frequency</code>. Only one of these two arguments is required. If both are provided, <code>frequency</code> supersedes <code>deltat</code> </p>
</td></tr>
<tr><td><code id="regul.screen_+3A_tol">tol</code></td>
<td>
<p> it is possible to tolerate some differences in the time between two matching observations (in the original irregular series and in the regulated series). If <code>tol=0</code> both values must be strictly identical; a higher value allows some fuzzy matching. <code>tol</code> must be a round fraction of <code>deltat</code> and cannot be higher than it, otherwise, it is adjusted to the closest acceptable value. By default, <code>tol=deltat/5</code> </p>
</td></tr>
<tr><td><code id="regul.screen_+3A_tol.type">tol.type</code></td>
<td>
<p> the type of window to use for the time-tolerance: <code>"left"</code>, <code>"right"</code>, <code>"both"</code> (by default) or <code>"none"</code>. If <code>tol.type="left"</code>, corresponding <code>x</code> values are seeked in a window ]xregul-tol, xregul]. If <code>tol.type="right"</code>, they are seeked in the window [xregul, xregul+tol[. If <code>tol.type="both"</code>, then they are seeked in the window ]xregul-tol, xregul+tol]. If several observations are in this window, the closest one is used. Finally, if <code>tol.type="none"</code>, then <em>all</em> observations in the regulated time series are interpolated (even if exactly matching observations exist!) </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Whatever the efficiency of the interpolation procedure used to regulate an irregular time series, a matching, non-interpolated observation is always better than an interpolated one! With very irregular time series, it is often difficult to decide which is the better regular time-scale in order to interpolate as less observations as possible. <code>regul.screen()</code> tests various combinations of number of observation, interval between two observations and position of the first observation and allows to choose the combination that best matches the original irregular time series. To choose also an optimal value for <code>tol</code>, use <code>regul.adj()</code> concurrently.
</p>


<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr><td><code>tol</code></td>
<td>
<p> a vector with the adjusted values of <code>tol</code> for the various values of <code>deltat</code> </p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p> a table indicating the maximum value of <code>n</code> for all combinations of <code>deltat</code> and <code>xmin</code> to avoid any extrapolation </p>
</td></tr>
<tr><td><code>nbr.match</code></td>
<td>
<p> a table indicating the number of matching observations (in the tolerance window) for all combinations of <code>deltat</code> and <code>xmin</code> </p>
</td></tr>
<tr><td><code>nbr.exact.match</code></td>
<td>
<p> a table indicating the number of exactly matching observations (with a tolerance window equal to zero) for all combinations of <code>deltat</code> and <code>xmin</code> </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>), Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>) </p>


<h3>See Also</h3>

 <p><code><a href="#topic+regul.adj">regul.adj</a></code>, <code><a href="#topic+regul">regul</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(releve)
# This series is very irregular, and it is difficult
# to choose the best regular time-scale
releve$Day
length(releve$Day)
intervals &lt;- releve$Day[2:61]-releve$Day[1:60]
intervals
range(intervals)
mean(intervals)
# A combination of xmin=1, deltat=22 and n=61 seems correct
# But is it the best one?
regul.screen(releve$Day, xmin=0:11, deltat=16:27, tol=1.05)
# Now we can tell that xmin=9, deltat=21, n=63, with tol=1.05
# is a much better choice! 
</code></pre>

<hr>
<h2 id='releve'> A data frame of six phytoplankton taxa followed in time at one station </h2><span id='topic+releve'></span>

<h3>Description</h3>

<p>The <code>releve</code> data frame has 61 rows and 8 columns. Several phytoplankton taxa were numbered in a single station from 1989 to 1992, but at irregular times. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(releve)</code></pre>


<h3>Format</h3>

<p>This data frame contains the following columns:
</p>

<dl>
<dt>Day</dt><dd><p>days number, first observation being day 1</p>
</dd>
<dt>Date</dt><dd><p>strings indicating the date of the observations in &quot;dd/mm/yyyy&quot; format</p>
</dd>
<dt>Astegla</dt><dd><p>the abundance of Asterionella glacialis</p>
</dd>
<dt>Chae</dt><dd><p>the abundance of Chaetoceros sp</p>
</dd>
<dt>Dity</dt><dd><p>the abundance of Ditylum sp</p>
</dd>
<dt>Gymn</dt><dd><p>the abundance of Gymnodinium sp</p>
</dd>
<dt>Melosul</dt><dd><p>the abundance of Melosira sulcata + Paralia sulcata</p>
</dd>
<dt>Navi</dt><dd><p>the abundance of Navicula sp</p>
</dd>
</dl>



<h3>Source</h3>

<p>Belin, C. &amp; B. Raffin, 1998. <em>Les espèces phytoplanctoniques toxiques et nuisibles sur le littoral français de 1984 à 1995, résultats du REPHY (réseau de surveillance du phytoplancton et des phycotoxines)</em>. Rapport IFREMER, RST.DEL/MP-AO-98-16. IFREMER, France.
</p>

<hr>
<h2 id='specs'> Collect parameters (&quot;specifications&quot;) from one object to use them in another analysis </h2><span id='topic+specs'></span>

<h3>Description</h3>

<p>&lsquo;specs&rsquo; is a generic function for reusing specifications included in an object and applying them in another similar analysis 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>specs(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="specs_+3A_x">x</code></td>
<td>
<p> An object that has &quot;specs&quot; data </p>
</td></tr>
<tr><td><code id="specs_+3A_...">...</code></td>
<td>
<p> Additional arguments (redefinition of one or several parameters) </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A &lsquo;specs&rsquo; object that has the <code>print</code> method and that can be entered as an argument to functions using similar &quot;specifications&quot;
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+regul">regul</a></code>, <code><a href="#topic+tsd">tsd</a></code> </p>

<hr>
<h2 id='stat.desc'> Descriptive statistics on a data frame or time series </h2><span id='topic+stat.desc'></span>

<h3>Description</h3>

<p>Compute a table giving various descriptive statistics about the series in a data frame or in a single/multiple time series
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stat.desc(x, basic=TRUE, desc=TRUE, norm=FALSE, p=0.95)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stat.desc_+3A_x">x</code></td>
<td>
<p> a data frame or a time series </p>
</td></tr>
<tr><td><code id="stat.desc_+3A_basic">basic</code></td>
<td>
<p> do we have to return basic statistics (by default, it is TRUE)? These are: the number of values (nbr.val), the number of null values (nbr.null), the number of missing values (nbr.na), the minimal value (min), the maximal value (max), the range (range, that is, max-min) and the sum of all non-missing values (sum) </p>
</td></tr>
<tr><td><code id="stat.desc_+3A_desc">desc</code></td>
<td>
<p> do we have to return various descriptive statistics (by default, it is TRUE)? These are: the median (median), the mean (mean), the standard error on the mean (SE.mean), the confidence interval of the mean (CI.mean) at the <code>p</code> level, the variance (var), the standard deviation (std.dev) and the variation coefficient (coef.var) defined as the standard deviation divided by the mean </p>
</td></tr>
<tr><td><code id="stat.desc_+3A_norm">norm</code></td>
<td>
<p> do we have to return normal distribution statistics (by default, it is FALSE)? the skewness coefficient g1 (skewness), its significant criterium (skew.2SE, that is, g1/2.SEg1; if skew.2SE &gt; 1, then skewness is significantly different than zero), kurtosis coefficient g2 (kurtosis), its significant criterium (kurt.2SE, same remark than for skew.2SE), the statistic of a Shapiro-Wilk test of normality (normtest.W) and its associated probability (normtest.p) </p>
</td></tr>
<tr><td><code id="stat.desc_+3A_p">p</code></td>
<td>
<p> the probability level to use to calculate the confidence interval on the mean (CI.mean). By default, <code>p=0.95</code> </p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data frame with the various statistics in rows and with each column correponding to a variable in the data frame, or to a separate time series
</p>


<h3>Note</h3>

<p> The Shapiro-Wilk test of normality is not available yet in Splus and it returns 'NA' in this environment. If you prefer to get separate statistics for various time intervals in your time series, use <code>stat.slide()</code>. If your data are fish or plankton sampled with a net, consider using the Pennington statistics (see <code>stat.pen()</code>) </p>


<h3>Author(s)</h3>

<p> Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>), Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>) </p>


<h3>See Also</h3>

 <p><code><a href="#topic+stat.slide">stat.slide</a></code>, <code><a href="#topic+stat.pen">stat.pen</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(marbio)
stat.desc(marbio[,13:16], basic=TRUE, desc=TRUE, norm=TRUE, p=0.95)
</code></pre>

<hr>
<h2 id='stat.pen'> Pennington statistics on a data frame or time series </h2><span id='topic+stat.pen'></span>

<h3>Description</h3>

<p>Compute a table giving various descriptive statistics, including Pennington's estimators of the mean, the variance and the variance of the mean, about the series in a data frame or in a single/multiple time series
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stat.pen(x, basic=FALSE, desc=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stat.pen_+3A_x">x</code></td>
<td>
<p> a data frame or a time series </p>
</td></tr>
<tr><td><code id="stat.pen_+3A_basic">basic</code></td>
<td>
<p> do we have to return also basic statistics (by default, it is FALSE)? These are: the number of values (nbr.val), the number of null values (nbr.null), the number of missing values (nbr.na), the minimal value (min), the maximal value (max), the range (range, that is, max-min) and the sum of all non-missing values (sum) </p>
</td></tr>
<tr><td><code id="stat.pen_+3A_desc">desc</code></td>
<td>
<p> do we have to return also various descriptive statistics (by default, it is FALSE)? These are: the median (median), the mean (mean), the standard error on the mean (SE.mean), the confidence interval of the mean (CI.mean) at the <code>p</code> level, the variance (var), the standard deviation (std.dev) and the variation coefficient (coef.var) defined as the standard deviation divided by the mean </p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data frame with the various statistics in rows and with each column correponding to a variable in the data frame, or to a separate time series
</p>


<h3>Note</h3>

 
<p>If you prefer to get separate statistics for various time intervals in your time series, use <code>stat.slide()</code>. Various other descriptive statistics, including test of the normal distribution are also available in <code>stat.desc()</code>
</p>


<h3>Author(s)</h3>

<p> Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>), Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>) </p>


<h3>References</h3>

<p> Aitchison, J., 1955. <em>On the distribution of a positive random variable having a discrete probability mass at the origin.</em> J. Amer. Stat. Ass., 50:901-908.
</p>
<p>Pennington, M., 1983. <em>Efficient estimations of abundance for fish and plankton surveys.</em> Biometrics, 39:281-286.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+stat.slide">stat.slide</a></code>, <code><a href="#topic+stat.desc">stat.desc</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(marbio)
stat.pen(marbio[,c(4, 14:16)], basic=TRUE, desc=TRUE)
</code></pre>

<hr>
<h2 id='stat.slide'> Sliding statistics </h2><span id='topic+stat.slide'></span><span id='topic+lines.stat.slide'></span><span id='topic+plot.stat.slide'></span><span id='topic+print.stat.slide'></span>

<h3>Description</h3>

<p>Statistical parameters are not constant along a time series: mean or variance can vary each year, or during particular intervals (radical or smooth changes due to a pollution, a very cold winter, a shift in the system behaviour, etc. Sliding statistics offer the potential to describe series on successive blocs defined along the space-time axis
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stat.slide(x, y, xcut=NULL, xmin=min(x), n=NULL, frequency=NULL,
        deltat=1/frequency, basic=FALSE, desc=FALSE, norm=FALSE,
        pen=FALSE, p=0.95)
## S3 method for class 'stat.slide'
print(x, ...) 
## S3 method for class 'stat.slide'
plot(x, stat="mean", col=c(1, 2), lty=c(par("lty"), par("lty")),
        leg=FALSE, llab=c("series", stat), lpos=c(1.5, 10), xlab="time", ylab="y",
        main=paste("Sliding statistics"), ...)
## S3 method for class 'stat.slide'
lines(x, stat="mean", col=3, lty=1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stat.slide_+3A_x">x</code></td>
<td>
<p> a vector with time data for <code>stat.slide()</code>, or a 'stat.slide' object
for the methods </p>
</td></tr>
<tr><td><code id="stat.slide_+3A_y">y</code></td>
<td>
<p> a vector with observation at corresponding times </p>
</td></tr>
<tr><td><code id="stat.slide_+3A_xcut">xcut</code></td>
<td>
<p> a vector with the position in time of the breaks between successive blocs. <code>xcut=NULL</code> by default. In the later case, a vector with equally spaced blocs is constructed using <code>xmin</code>, <code>n</code> and <code>frequency</code> or <code>deltat</code>. If a value is provided for <code>xcut</code>, then it supersedes all these other parameters </p>
</td></tr>
<tr><td><code id="stat.slide_+3A_xmin">xmin</code></td>
<td>
<p> the minimal value in the time-scale to use for constructing a vector of equally spaced breaks </p>
</td></tr>
<tr><td><code id="stat.slide_+3A_n">n</code></td>
<td>
<p> the number of breaks to use </p>
</td></tr>
<tr><td><code id="stat.slide_+3A_frequency">frequency</code></td>
<td>
<p> the frequency of the breaks in the time-scale </p>
</td></tr>
<tr><td><code id="stat.slide_+3A_deltat">deltat</code></td>
<td>
<p> the bloc interval touse for constructing an equally-spaced break vector. <code>deltat</code> is 1/<code>frequency</code> </p>
</td></tr>
<tr><td><code id="stat.slide_+3A_basic">basic</code></td>
<td>
<p> do we have to return basic statistics (by default, it is FALSE)? These are: the number of values (nbr.val), the number of null values (nbr.null), the number of missing values (nbr.na), the minimal value (min), the maximal value (max), the range (range, that is, max-min) and the sum of all non-missing values (sum) </p>
</td></tr>
<tr><td><code id="stat.slide_+3A_desc">desc</code></td>
<td>
<p> do we have to return descriptive statistics (by default, it is FALSE)? These are: the median (median), the mean (mean), the standard error on the mean (SE.mean), the confidence interval of the mean (CI.mean) at the <code>p</code> level, the variance (var), the standard deviation (std.dev) and the variation coefficient (coef.var) defined as the standard deviation divided by the mean </p>
</td></tr>
<tr><td><code id="stat.slide_+3A_norm">norm</code></td>
<td>
<p> do we have to return normal distribution statistics (by default, it is FALSE)? the skewness coefficient g1 (skewness), its significant criterium (skew.2SE, that is, g1/2.SEg1; if skew.2SE &gt; 1, then skewness is significantly different than zero), kurtosis coefficient g2 (kurtosis), its significant criterium (kurt.2SE, same remark than for skew.2SE), the statistic of a Shapiro-Wilk test of normality (normtest.W) and its associated probability (normtest.p) </p>
</td></tr>
<tr><td><code id="stat.slide_+3A_pen">pen</code></td>
<td>
<p> do we have to return Pennington and other associated statistics (by default, it is FALSE)? pos.median, pos.mean, pos.var, pos.std.dev, respectively the median, the mean, the standard deviation and the variance, considering only non-null values; geo.mean, the geometric mean that is, the exponential of the mean of the logarithm of the observations, excluding null values. pen.mean, pen.var, pen.std.dev, pen.mean.var, respectively the mean, the variance, the standard deviation and the variance of the mean after Pennington's estimators (see <code>pennington()</code>) </p>
</td></tr>
<tr><td><code id="stat.slide_+3A_p">p</code></td>
<td>
<p> the probability level to use to calculate the confidence interval on the mean (CI.mean). By default, <code>p=0.95</code> </p>
</td></tr>
<tr><td><code id="stat.slide_+3A_stat">stat</code></td>
<td>
<p> the statistic to plot on the graph. You can use &quot;min&quot;, &quot;max&quot;, &quot;median&quot;, &quot;mean&quot; (by default), &quot;pos.median&quot;, &quot;pos.mean&quot;, &quot;geo.mean&quot; and &quot;pen.mean&quot;. The other statistics cannot be superposed on the graph of the series in the current version of the function </p>
</td></tr>
<tr><td><code id="stat.slide_+3A_col">col</code></td>
<td>
<p> the colors to use to plot the initial series and the statistics, respectively. By default, <code>col=c(1,2)</code> </p>
</td></tr>
<tr><td><code id="stat.slide_+3A_lty">lty</code></td>
<td>
<p> the style to use to draw the original series and the statistics. The default style is used if this argument is not provided </p>
</td></tr>
<tr><td><code id="stat.slide_+3A_leg">leg</code></td>
<td>
<p> if <code>leg=TRUE</code>, a legend box is drawn on the graph </p>
</td></tr>
<tr><td><code id="stat.slide_+3A_llab">llab</code></td>
<td>
<p> the labels to use for the legend. By default, it is &quot;series&quot; and the corresponding statistics provided in <code>stat</code>, respectively </p>
</td></tr>
<tr><td><code id="stat.slide_+3A_lpos">lpos</code></td>
<td>
<p> the position of the top-left corner (x,y) of the legend box in the graph coordinates. By default <code>lpos=c(1.5,10)</code> </p>
</td></tr>
<tr><td><code id="stat.slide_+3A_xlab">xlab</code></td>
<td>
<p> the label of the x-axis </p>
</td></tr>
<tr><td><code id="stat.slide_+3A_ylab">ylab</code></td>
<td>
<p> the label of the y-axis </p>
</td></tr>
<tr><td><code id="stat.slide_+3A_main">main</code></td>
<td>
<p> the main title of the graph</p>
</td></tr>
<tr><td><code id="stat.slide_+3A_...">...</code></td>
<td>
<p> additional parameters </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Available statistics are the same as for <code>stat.desc()</code> and <code>stat.pen()</code>. The Shapiro-Wilk test of normality is not available yet in Splus and it returns 'NA' in this environment. If not a priori known, successive blocs can be identified using either <code>local.trend()</code> or <code>decmedian()</code> (see respective functions for further details)
</p>


<h3>Value</h3>

<p>An object of type 'stat.slide' is returned. It has methods <code>print()</code>, <code>plot()</code> and <code>lines()</code>.
</p>


<h3>Author(s)</h3>

<p> Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>), Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>) </p>


<h3>See Also</h3>

 <p><code><a href="#topic+stat.desc">stat.desc</a></code>, <code><a href="#topic+stat.pen">stat.pen</a></code>, <code><a href="#topic+pennington">pennington</a></code>, <code><a href="#topic+local.trend">local.trend</a></code>, <code><a href="#topic+decmedian">decmedian</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(marbio)
# Sliding statistics with fixed-length blocs
statsl &lt;- stat.slide(1:68, marbio[, "ClausocalanusA"], xmin=0, n=7, deltat=10)
statsl
plot(statsl, stat="mean", leg=TRUE, lpos=c(55, 2500), xlab="Station",
        ylab="ClausocalanusA")

# More information on the series, with predefined blocs
statsl2 &lt;- stat.slide(1:68, marbio[, "ClausocalanusA"],
        xcut=c(0, 17, 25, 30, 41, 46, 70), basic=TRUE, desc=TRUE, norm=TRUE,
        pen=TRUE, p=0.95)
statsl2
plot(statsl2, stat="median", xlab="Stations", ylab="Counts",
        main="Clausocalanus A")              # Median
lines(statsl2, stat="min")                   # Minimum
lines(statsl2, stat="max")                   # Maximum
lines(c(17, 17), c(-50, 2600), col=4, lty=2) # Cuts
lines(c(25, 25), c(-50, 2600), col=4, lty=2)
lines(c(30, 30), c(-50, 2600), col=4, lty=2)
lines(c(41, 41), c(-50, 2600), col=4, lty=2)
lines(c(46, 46), c(-50, 2600), col=4, lty=2)
text(c(8.5, 21, 27.5, 35, 43.5, 57), 2300, labels=c("Peripheral Zone", "D1",
        "C", "Front", "D2", "Central Zone")) # Labels
legend(0, 1900, c("series", "median", "range"), col=1:3, lty=1)
# Get cuts back from the object
statsl2$xcut
</code></pre>

<hr>
<h2 id='trend.test'> Test if an increasing or decreasing trend exists in a time series </h2><span id='topic+trend.test'></span>

<h3>Description</h3>

<p>Test if the series has an increasing or decreasing trend, using a non-parametric Spearman test between the observations and time
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trend.test(tseries, R=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trend.test_+3A_tseries">tseries</code></td>
<td>
<p> a univariate or multivariate time series (a 'rts' object in Splus or a 'ts' object in <span class="rlang"><b>R</b></span>) </p>
</td></tr>
<tr><td><code id="trend.test_+3A_r">R</code></td>
<td>
<p> The number of time the series is/are resampled for a bootstrap test. If <code>R1</code> (by default), an usual Spearman test is performed. If <code>R</code> &gt; 1 then a bootstrap test is run </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A 'htest' object if <code>R=1</code>, a 'boot' object with an added <code>boot$p.value</code> item otherwise 
</p>


<h3>Note</h3>

 
<p>In both cases (normal test with <code>R=1</code> and bootstrap test), the p-value can be obtained from <code>obj$p.value</code> (see examples)
</p>


<h3>Author(s)</h3>

<p> Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>), Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>) </p>


<h3>References</h3>

 
<p>Siegel, S. &amp; N.J. Castellan, 1988. <em>Non-parametric statistics.</em> McGraw-Hill, New York. 399  pp.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+local.trend">local.trend</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(marbio)
trend.test(marbio[, 8])
# Run a bootstrap test on the same series
marbio8.trend.test &lt;- trend.test(marbio[, 8], R=99)
# R=999 is a better value... but it is very slow!
marbio8.trend.test  
plot(marbio8.trend.test)
marbio8.trend.test$p.value
</code></pre>

<hr>
<h2 id='tsd'> Decomposition of one or several regular time series using various methods </h2><span id='topic+tsd'></span><span id='topic+extract.tsd'></span><span id='topic+plot.tsd'></span><span id='topic+print.specs.tsd'></span><span id='topic+print.summary.tsd'></span><span id='topic+print.tsd'></span><span id='topic+specs.tsd'></span><span id='topic+summary.tsd'></span>

<h3>Description</h3>

<p>Use a decomposition method to split the series into two or more components.
Decomposition methods are either series filtering/smoothing (difference,
average, median, evf), deseasoning (loess) or model-based decomposition (reg,
i.e., regression).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tsd(x, specs=NULL, method="loess",
    type=if (method == "census") "multiplicative" else "additive",
    lag=1, axes=1:5, order=1, times=1, sides=2, ends="fill", weights=NULL,
    s.window=NULL, s.degree=0, t.window=NULL, t.degree=2, robust=FALSE,
    trend=FALSE, xreg=NULL)
## S3 method for class 'tsd'
print(x, ...)
## S3 method for class 'tsd'
summary(object, ...)
## S3 method for class 'summary.tsd'
print(x, ...)
## S3 method for class 'tsd'
plot(x, series=1, stack=TRUE, resid=TRUE, col=par("col"),
    lty=par("lty"), labels=dimnames(X)[[2]], leg=TRUE, lpos=c(0, 0), xlab="time",
    ylab="series", main=paste("Series decomposition by", x$specs$method, "-",
    x$specs$type), ...)
## S3 method for class 'tsd'
extract(e, n, series=NULL, components=NULL, ...)
## S3 method for class 'tsd'
specs(x, ...)
## S3 method for class 'specs.tsd'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tsd_+3A_x">x</code></td>
<td>
<p> an univariate or multivariate regular time series ('ts' object) to
be decomposed for <code>tsd()</code>, or a 'tsd' object for the methods </p>
</td></tr>
<tr><td><code id="tsd_+3A_specs">specs</code></td>
<td>
<p> specifications are collected from a 'tsd' object, using the
<code>specs</code> method. This allows for reusing parameters issued from a
previous similar analysis </p>
</td></tr>
<tr><td><code id="tsd_+3A_method">method</code></td>
<td>
<p> the method to use to decompose the time series. Currently,
possible values are: <code>"diff"</code>, <code>"average"</code>, <code>"median"</code>,
<code>"evf"</code>, <code>"reg"</code>, <code>"loess"</code> (by default) or <code>"census"</code>.
The corresponding function <code>decXXXX()</code> is applied to each of the series
in <code>x</code> </p>
</td></tr>
<tr><td><code id="tsd_+3A_type">type</code></td>
<td>
<p> the type of model to use: either <code>"additive"</code> (by default)
or <code>"multiplicative"</code>. In the additive model, all components must be
added to reconstruct the initial series. In the multiplicative model, they
must be multiplied (one components has the same unit as the original series,
and the other ones are dimensionless multiplicative factors) </p>
</td></tr>
<tr><td><code id="tsd_+3A_lag">lag</code></td>
<td>
<p> The lag between the two observations used to calculate differences.
By default, <code>lag=1</code> </p>
</td></tr>
<tr><td><code id="tsd_+3A_axes">axes</code></td>
<td>
<p> the number of axes to show in the plot </p>
</td></tr>
<tr><td><code id="tsd_+3A_order">order</code></td>
<td>
<p> (1) for the method 'difference': the order of the difference
corresponds to the number of times it is applied, by default <code>order=1</code>,
(2) for the method 'average': the order of the moving average (the window of
the average being 2*order+1), centered around the current observation or at
left of this observation depending upon the value of the <code>sides</code>
argument. Weights are the same for all observations within the window.
However, if the argument <code>weights</code> is provided, it supersedes
<code>order</code>. One can also use <code>order="periodic"</code>. In this case, a
deseasoning filter is calculated according to the value of <code>frequency</code></p>
</td></tr>
<tr><td><code id="tsd_+3A_times">times</code></td>
<td>
<p> The number of times to apply the method (by default, once) </p>
</td></tr>
<tr><td><code id="tsd_+3A_sides">sides</code></td>
<td>
<p> If 2 (by default), the window is centered around the current
observation. If 1, the window is at left of the current observation
(including it) </p>
</td></tr>
<tr><td><code id="tsd_+3A_ends">ends</code></td>
<td>
<p> either &quot;NAs&quot; (fill first and last values that are not calculable
with NAs), or &quot;fill&quot; (fill them with the average of observations before
applying the filter, by default), or &quot;circular&quot; (use last values for
estimating first ones and vice versa), or &quot;periodic&quot; (use entire periods of
contiguous cycles, deseasoning) </p>
</td></tr>
<tr><td><code id="tsd_+3A_weights">weights</code></td>
<td>
<p> a vector indicating weight to give to all observations in the
window. This argument has the priority over <code>order</code> </p>
</td></tr>
<tr><td><code id="tsd_+3A_s.window">s.window</code></td>
<td>
<p> the width of the window used to extract the seasonal
component. Use an odd value equal or just larger than the number of annual
values (frequency of the time series). Use another value to extract other
cycles (circadian, lunar,...). Using <code>s.window="periodic"</code> ensures a
correct value for extracting a seasonal component when the time scale is in
years units </p>
</td></tr>
<tr><td><code id="tsd_+3A_s.degree">s.degree</code></td>
<td>
<p> the order of the polynome to use to extract the seasonal
component (0 or 1). By default <code>s.degree=0</code> </p>
</td></tr>
<tr><td><code id="tsd_+3A_t.window">t.window</code></td>
<td>
<p> the width of the window to use to extract the general trend
when <code>trend=TRUE</code> (indicate an odd value). If this parameter is not
provided, a reasonable value is first calculated, and then used by the
algorithm. </p>
</td></tr>
<tr><td><code id="tsd_+3A_t.degree">t.degree</code></td>
<td>
<p> the order of the polynome to use to extract the general trend
(0, 1 or 2). By default <code>t.degree=2</code> </p>
</td></tr>
<tr><td><code id="tsd_+3A_robust">robust</code></td>
<td>
<p> if <code>TRUE</code> a robust regression method is used. Otherwise
(<code>FALSE</code>), by default, a classical least-square regression is used </p>
</td></tr>
<tr><td><code id="tsd_+3A_trend">trend</code></td>
<td>
<p> If <code>TRUE</code> a trend is calculated (under R only). Otherwise,
the series is decomposed into a seasonal component and residuals only </p>
</td></tr>
<tr><td><code id="tsd_+3A_xreg">xreg</code></td>
<td>
<p> a second regular time series or a vector of the same length as
<code>x</code> with corresponding values from the regression model </p>
</td></tr>
<tr><td><code id="tsd_+3A_object">object</code></td>
<td>
<p> a 'tsd' object as returned by the function <code>tsd()</code>, or any
of the <code>decXXXX()</code> functions </p>
</td></tr>
<tr><td><code id="tsd_+3A_e">e</code></td>
<td>
<p> a 'tsd' object as returned by the function <code>tsd()</code>, or any of
the <code>decXXXX()</code> functions </p>
</td></tr>
<tr><td><code id="tsd_+3A_series">series</code></td>
<td>
<p> (1) for <code>plot()</code>: the series to plot. By default,
<code>series=1</code>, the first (or possibly unique) series in the 'tsd' object
is plotted. (2) for <code>extract</code>: the name or the index of the series to
extract. If <code>series</code> is provided, then <code>n</code> is ignored. By default,
<code>series=NULL</code>. It is also possible to use negative indices. In this
case, all series are extracted, except those ones </p>
</td></tr>
<tr><td><code id="tsd_+3A_stack">stack</code></td>
<td>
<p> graphs of each component are either stacked (<code>stack=TRUE</code>,
by default), or superposed on the same graph <code>stack=FALSE</code> </p>
</td></tr>
<tr><td><code id="tsd_+3A_resid">resid</code></td>
<td>
<p> do we have to plot also the &quot;residuals&quot; components
(<code>resid=TRUE</code>, by default) or not? Usually, in a stacked graph, you
would like to plot the residuals, while in a superposed graph, you would not </p>
</td></tr>
<tr><td><code id="tsd_+3A_col">col</code></td>
<td>
<p> color of the plot </p>
</td></tr>
<tr><td><code id="tsd_+3A_lty">lty</code></td>
<td>
<p> line type for the plot </p>
</td></tr>
<tr><td><code id="tsd_+3A_labels">labels</code></td>
<td>
<p> the labels to use for all y-axes in a stacked graph, or in the
legend for a superposed graph. By default, the names of the components
(&quot;trend&quot;, &quot;seasonal&quot;, &quot;deseasoned&quot;, &quot;filtered&quot;, &quot;residuals&quot;, ...) are used </p>
</td></tr>
<tr><td><code id="tsd_+3A_leg">leg</code></td>
<td>
<p> only used when <code>stack=FALSE</code>. Do we plot a legend
(<code>leg=TRUE</code> or not? </p>
</td></tr>
<tr><td><code id="tsd_+3A_lpos">lpos</code></td>
<td>
<p> position of the upper-left corner of the legend box in the graph
coordinates (x,y). By default, <code>leg=c(0,0)</code> </p>
</td></tr>
<tr><td><code id="tsd_+3A_xlab">xlab</code></td>
<td>
<p> the label of the x-axis </p>
</td></tr>
<tr><td><code id="tsd_+3A_ylab">ylab</code></td>
<td>
<p> the label of the y-axis </p>
</td></tr>
<tr><td><code id="tsd_+3A_main">main</code></td>
<td>
<p> the main title of the graph</p>
</td></tr>
<tr><td><code id="tsd_+3A_n">n</code></td>
<td>
<p> the number of series to extract (from series 1 to series n). By
default, n equals the number of series in the 'tsd' object. If both
<code>series</code> and <code>components</code> arguments are NULL, all series and
components are extracted and this method has exactly the same effect as
<code>tseries</code> </p>
</td></tr>
<tr><td><code id="tsd_+3A_components">components</code></td>
<td>
<p> the names or indices of the components to extract. If
<code>components=NULL</code> (by default), then all components of the selected
series are extracted. It is also possible to specify negative indices. In
this case, all components are extracted, except those ones </p>
</td></tr>
<tr><td><code id="tsd_+3A_...">...</code></td>
<td>
<p> (1) for <code>tsd()</code>: further arguments to pass to the
corresponding <code>decXXXX()</code> function. (2) for <code>plot()</code>: further
graphical arguments, (3) unused for the other functions or methods </p>
</td></tr>
</table>


<h3>Details</h3>

<p>To eliminate trend from a series, use &quot;diff&quot; or use &quot;loess&quot; with
<code>trend=TRUE</code>. If you know the shape of the trend (linear, exponential,
periodic, etc.), you can also use it with the &quot;reg&quot; (regression) method. To
eliminate or extract seasonal components, you can use &quot;loess&quot; if the seasonal
component is additive, or &quot;census&quot; if it is multiplicative. You can also use
&quot;average&quot; with argument <code>order="periodic"</code> and with either an additive or
a multiplicative model, although the later method is often less powerful than
&quot;loess&quot; or &quot;census&quot;. If you want to extract a seasonal cycle with a given
shape (for instance, a sinusoid), use the &quot;reg&quot; method with a fitted
sinusoidal equation. If you want to identify levels in the series, use the
&quot;median&quot; method. To smooth the series, you can use preferably the &quot;evf&quot;
(eigenvector filtering), or the &quot;average&quot; methods, but you can also use
&quot;median&quot;. To extract most important components from the series (no matter if
they are cycles -seasonal or not-, or long-term trends), you should use the
&quot;evf&quot; method. For more information on each of these methods, see online help
of the corresponding <code>decXXXX()</code> functions.
</p>


<h3>Value</h3>

<p>An object of type 'tsd' is returned. It has methods <code>print()</code>,
<code>summary()</code>, <code>plot()</code>, <code>extract()</code> and <code>specs()</code>.
</p>


<h3>Note</h3>

<p> If you have to decompose a single time series, you could also use the
corresponding <code>decXXXX()</code> function directly. In the case of a multivariate
regular time series, <code>tsd()</code> is more convenient because it decompose all
times series of a set at once! </p>


<h3>Author(s)</h3>

<p> Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>),
Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>) </p>


<h3>References</h3>

 
<p>Kendall, M., 1976. <em>Time-series.</em> Charles Griffin &amp; Co Ltd. 197 pp.
</p>
<p>Laloire, J.C., 1972. <em>Méthodes du traitement des chroniques.</em> Dunod, Paris, 194 pp.
</p>
<p>Legendre, L. &amp; P. Legendre, 1984. <em>Ecologie numérique. Tome 2: La structure
des données écologiques.</em> Masson, Paris. 335 pp.
</p>
<p>Malinvaud, E., 1978. <em>Méthodes statistiques de l'économétrie.</em> Dunod, Paris. 846 pp.
</p>
<p>Philips, L. &amp; R. Blomme, 1973. <em>Analyse chronologique.</em> Université
Catholique de Louvain. Vander ed. 339 pp.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+tseries">tseries</a></code>, <code><a href="#topic+decdiff">decdiff</a></code>, <code><a href="#topic+decaverage">decaverage</a></code>,
<code><a href="#topic+decmedian">decmedian</a></code>, <code><a href="#topic+decevf">decevf</a></code>, <code><a href="#topic+decreg">decreg</a></code>,
<code><a href="#topic+decloess">decloess</a></code>, <code><a href="#topic+deccensus">deccensus</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(releve)
# Regulate the series and extract them as a time series object
rel.regy &lt;- regul(releve$Day, releve[3:8], xmin=6, n=87, units="daystoyears",
        frequency=24, tol=2.2, methods="linear", datemin="21/03/1989",
        dateformat="d/m/Y")
rel.ts &lt;- tseries(rel.regy)

# Decompose all series in the set with the "loess" method
rel.dec &lt;- tsd(rel.ts, method="loess", s.window=13, trend=FALSE)
rel.dec
plot(rel.dec, series=5, col=1:3)    # An plot series 5

# Extract "deseasoned" components
rel.des &lt;- extract(rel.dec, series=3:6, components="deseasoned")
rel.des[1:10,]

# Further decompose these components with a moving average
rel.des.dec &lt;- tsd(rel.des, method="average", order=2, times=10)
plot(rel.des.dec, series=3, col=c(2, 4, 6))
# In this case, a superposed graph is more appropriate:
plot(rel.des.dec, series=3, col=c(2,4), stack=FALSE, resid=FALSE,
        labels=c("without season cycle", "trend"), lpos=c(0, 55000))
# Extract residuals from the latter decomposition
rel.res2 &lt;- extract(rel.des.dec, components="residuals")
</code></pre>

<hr>
<h2 id='tseries'> Convert a 'regul' or a 'tsd'  object into a time series </h2><span id='topic+tseries'></span>

<h3>Description</h3>

<p>Regulated series contained in a 'regul' object or components issued from a time series decomposition with 'tsd' are extracted from their respective object and converted into uni- or multivariate regular time series ('rts' objects in Splus and 'ts' objects in <span class="rlang"><b>R</b></span>)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tseries(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tseries_+3A_x">x</code></td>
<td>
<p> A 'regul' or 'tsd' object </p>
</td></tr>
</table>


<h3>Value</h3>

<p>an uni- or multivariate regular time series
</p>


<h3>Note</h3>

<p>To extract some of the time series contained in the 'regul' or 'tsd' objects, use the <code>extract()</code> method
</p>


<h3>Author(s)</h3>

<p> Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>), Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>) </p>


<h3>See Also</h3>

 <p><code><a href="#topic+is.tseries">is.tseries</a></code>, <code><a href="#topic+regul">regul</a></code>, <code><a href="#topic+tsd">tsd</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(releve)
rel.regy &lt;- regul(releve$Day, releve[3:8], xmin=6, n=87, units="daystoyears",
        frequency=24, tol=2.2, methods="linear", datemin="21/03/1989",
        dateformat="d/m/Y")
# This object is not a time series
is.tseries(rel.regy)     # FALSE
# Extract all time series contained in the 'regul' object
rel.ts &lt;- tseries(rel.regy)
# Now this is a time series
is.tseries(rel.ts)       # TRUE
</code></pre>

<hr>
<h2 id='turnogram'> Calculate and plot a turnogram for a regular time series </h2><span id='topic+turnogram'></span><span id='topic+extract.turnogram'></span><span id='topic+identify.turnogram'></span><span id='topic+plot.turnogram'></span><span id='topic+print.summary.turnogram'></span><span id='topic+print.turnogram'></span><span id='topic+summary.turnogram'></span>

<h3>Description</h3>

<p>The turnogram is the variation of a monotony index with the observation scale (the number of data per time unit). A monotony index indicates if the series has more or less erratic variations than a pure random succession of independent observations. Since a time series almost always has autocorrelation, it is expected to be more monotonous than a purely random series. The monotony index is a way to quantify the density of information beared by a time series. The turnogram determines at which observation scale this density of information is maximum. It is also the scale that optimize the sampling effort (best compromise between less samples versus more information).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>turnogram(series, intervals=c(1, length(series)/5), step=1, complete=FALSE,
        two.tailed=TRUE, FUN=mean, plotit=TRUE, level=0.05, lhorz=TRUE,
        lvert=FALSE, xlog=TRUE)
## S3 method for class 'turnogram'
print(x, ...)
## S3 method for class 'turnogram'
summary(object, ...)
## S3 method for class 'summary.turnogram'
print(x, ...)
## S3 method for class 'turnogram'
plot(x, level=0.05, lhorz=TRUE, lvert=TRUE, lcol=2,
        llty=2, xlog=TRUE, xlab=paste("interval (", x$units.text, ")", sep=""),
        ylab="I (bits)", main=paste(x$type, "turnogram for:", x$data),
        sub=paste(x$fun, "/", x$proba), ...)
## S3 method for class 'turnogram'
identify(x, lvert=TRUE, col=2, lty=2, ...)
## S3 method for class 'turnogram'
extract(e, n, level=e$level, FUN=e$fun, drop=0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="turnogram_+3A_series">series</code></td>
<td>
<p> a single regular time series ('rts' object in Splus or 'ts' object in <span class="rlang"><b>R</b></span>) </p>
</td></tr>
<tr><td><code id="turnogram_+3A_intervals">intervals</code></td>
<td>
<p> the range (mini, maxi) of the intervals to calculate, i.e., to take one obervation every 'interval' one. By default, <code>intervals</code> ranges from 1 to the fifth of the total number of observations </p>
</td></tr>
<tr><td><code id="turnogram_+3A_x">x</code></td>
<td>
<p> a 'turnogram' object </p>
</td></tr>
<tr><td><code id="turnogram_+3A_object">object</code></td>
<td>
<p> a 'turnogram' object </p>
</td></tr>
<tr><td><code id="turnogram_+3A_e">e</code></td>
<td>
<p> a 'turnogram' object </p>
</td></tr>
<tr><td><code id="turnogram_+3A_step">step</code></td>
<td>
<p> the increment used for the intervals. By defaults <code>step=1</code>. To limit calculation or for a first screenning with a large range in the intervals, use a higher value for <code>step</code> </p>
</td></tr>
<tr><td><code id="turnogram_+3A_complete">complete</code></td>
<td>
<p> if <code>complete=TRUE</code>, a complete turnogram is calculated, showing mean, minimal and maximal curves. If it is FALSE (by default), only a simple turnogram always starting from the first observation is calculated </p>
</td></tr>
<tr><td><code id="turnogram_+3A_two.tailed">two.tailed</code></td>
<td>
<p> if <code>two.tailed=TRUE</code> (by default), the monotony index is tested with a bilateral test, otherwise, a left-sided test is used </p>
</td></tr>
<tr><td><code id="turnogram_+3A_fun">FUN</code></td>
<td>
<p> a function to apply to aggregate data in the intervals. It is a function of the type <code>FUN(x, na.rm, ...)</code>. The most used function is <code>mean()</code> (by default), but it is also possible to keep only the first value with <code>first()</code>, the last value with <code>last()</code>, the median or the sum of values in the interval. The later function is useful for cumulative observations, like pluviometry. It should be noted that the turnograms with <code>FUN=mean</code> and with <code>FUN=sum</code> are the same, but that extraction of final series are different for levels &gt; 1 </p>
</td></tr>
<tr><td><code id="turnogram_+3A_plotit">plotit</code></td>
<td>
<p> if <code>plotit=TRUE</code> (by default), the graph of the turnogram is also plotted </p>
</td></tr>
<tr><td><code id="turnogram_+3A_level">level</code></td>
<td>
<p> the significant level to draw on the graph. By default <code>level=0.05</code>, corresponding to a test with P = 5% </p>
</td></tr>
<tr><td><code id="turnogram_+3A_lhorz">lhorz</code></td>
<td>
<p> if <code>lhorz=TRUE</code> (by default) then one (left-sided test), or two (two-sided test) horizontal lines are drawn on the graph, indicating the significant level of the test given by the argument <code>level</code>. Any point above the single line, or outside the interval defined by the two lines is significant </p>
</td></tr>
<tr><td><code id="turnogram_+3A_lvert">lvert</code></td>
<td>
<p> if <code>lvert=TRUE</code> (by default, except for <code>turnogram()</code> function), a vertical line is drawn, indicating the time interval that corresponds to the maximum information and it is also the automatic level of extraction unless this value is changed </p>
</td></tr>
<tr><td><code id="turnogram_+3A_lcol">lcol</code></td>
<td>
<p> the color to use to draw supplemental lines: the horizontal line indicating where the test is significant (if <code>lhorz=TRUE</code>) and the vertical line indicating the extraction level (if <code>lvert=TRUE</code>). By default, color 2 is used </p>
</td></tr>
<tr><td><code id="turnogram_+3A_llty">llty</code></td>
<td>
<p> the style for the supplemental lines. By default, style 2 is used (dashed lines) </p>
</td></tr>
<tr><td><code id="turnogram_+3A_xlog">xlog</code></td>
<td>
<p> if <code>xlog=TRUE</code> (by default), then the x-axis is expressed in logarithms. Otherwise, a linear scale is used </p>
</td></tr>
<tr><td><code id="turnogram_+3A_xlab">xlab</code></td>
<td>
<p> the label of the x-axis </p>
</td></tr>
<tr><td><code id="turnogram_+3A_ylab">ylab</code></td>
<td>
<p> the label of the y-axis </p>
</td></tr>
<tr><td><code id="turnogram_+3A_main">main</code></td>
<td>
<p> the main title of the graph</p>
</td></tr>
<tr><td><code id="turnogram_+3A_sub">sub</code></td>
<td>
<p> the subtitle of the graph</p>
</td></tr>
<tr><td><code id="turnogram_+3A_col">col</code></td>
<td>
<p> color to use for identified items </p>
</td></tr>
<tr><td><code id="turnogram_+3A_lty">lty</code></td>
<td>
<p> line type to use for identified items </p>
</td></tr>
<tr><td><code id="turnogram_+3A_...">...</code></td>
<td>
<p> additional optional graphic arguments </p>
</td></tr>
<tr><td><code id="turnogram_+3A_n">n</code></td>
<td>
<p> the number of observations to take into account in the initial series. Use <code>n=NULL</code> (by default) to use all observations of the series </p>
</td></tr>
<tr><td><code id="turnogram_+3A_drop">drop</code></td>
<td>
<p> the number of observations to drop at the beginning of the series before proceeding with the aggregation of the data for the extracted series. By default, <code>drop=0</code>: no observations are dropped </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The turnogram is a generalisation of the information theory (see <code>turnpoints()</code>). If a series has a lot of erratic peaks and pits that alternate with a high frequency, it is more difficult to interpret than a more monotonous series. These erratic fluctuations can be eliminated by changing the scale of observation (keeping one observation every two, three, four,... from the original series). The turnogram resample the original series this way, and calculate a monotony index for each resampled subseries. This monotony index quantifies the number of peaks and pits presents in the series, compared to the total number of observations. The Gleissberg distribution (see <code>pgleissberg()</code>) indicates the probability to have such a number of extrema in a series given it is purely random. It is possible to test monotony indices: is it a random series or not (two-sided test), or is more monotonous than a random series (left-sided test) thanks to a Chi-2 test proposed by Wallis &amp; Moore (1941). 
</p>
<p>There are various turnograms depending on the way the observations are aggregated inside each time interval. For instance, if one consider one observation every three from the original series, each group of three observations can be aggregated in several different ways. One can take the mean of the three observation, or the median value, or the sum,... One can also decide not to aggregate observations, but to drop some of them. Hence, one can take only the first or the last observation of the group. All these options can be choosen by defining the argument <code>FUN=...</code>. A simple turnogram correspond to the change of the monotony index with the scale of observation, stating always from the first observation. One could also decide to start from the second, or the third observation for an aggregation of the observations three by three... and result could be somewhat different. A complete turnogram investigates all possibles combinations (observation scale versus starting point for the aggregation) and trace the maximal, minimal and mean curves for the change of the monotony index. It is thus more informative than the simple turnogram. However, it takes much more time to compute.
</p>
<p>The most obvious use of the turnogram is for the pretreatment of continuously sampled data. It helps in deciding which is the optimal sampling interval for the series to bear as most information as possible while keeping the dataset as small as possible. It is also interesting to compare the turnogram with other functions like the variogram (see <code>vario()</code>) or the spectrogram (see <code>spectrum()</code>).
</p>


<h3>Value</h3>

<p>An object of type 'turnogram' is returned. It has methods <code>print()</code>, <code>summary()</code>, <code>plot()</code>, <code>identify()</code> and <code>extract()</code>.
</p>


<h3>Author(s)</h3>

<p> Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>), Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>) </p>


<h3>References</h3>

 
<p>Dallot, S. &amp; M. Etienne, 1990. <em>Une méthode non paramétrique d'analyse des series en océanographie biologique: les tournogrammes.</em> Biométrie et océanographie - Société de biométrie, 6, Lille, 26-28 mai 1986. IFREMER, Actes de colloques, 10:13-31.
</p>
<p>Johnson, N.L. &amp; Kotz, S., 1969. <em>Discrete distributions.</em> J. Wiley &amp; sons, New York, 328 pp.
</p>
<p>Kendall, M.G., 1976. <em>Time-series, 2nd ed.</em> Charles Griffin &amp; co, London.
</p>
<p>Wallis, W.A. &amp; G.H. Moore, 1941. <em>A significance test for time series.</em> National Bureau of Economic Research, tech. paper n°1.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+pgleissberg">pgleissberg</a></code>, <code><a href="#topic+turnpoints">turnpoints</a></code>, <code><a href="#topic+first">first</a></code>, <code><a href="#topic+last">last</a></code>, <code><a href="#topic+vario">vario</a></code>, <code><a href="stats.html#topic+spectrum">spectrum</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(bnr)
# Let's transform series 4 into a time series (supposing it is regular)
bnr4 &lt;- as.ts(bnr[, 4])
plot(bnr4, type="l", main="bnr4: raw data", xlab="Time")
# A simple turnogram is calculated
bnr4.turno &lt;- turnogram(bnr4)
summary(bnr4.turno)
# A complete turnogram confirms that "level=3" is a good value: 
turnogram(bnr4, complete=TRUE)
# Data with maximum info. are extracted (thus taking 1 every 3 observations)
bnr4.interv3 &lt;- extract(bnr4.turno)
plot(bnr4, type="l", lty=2, xlab="Time")
lines(bnr4.interv3, col=2)
title("Original bnr4 (dotted) versus max. info. curve (plain)")
# Choose another level (for instance, 6) and extract the corresponding series
bnr4.turno$level &lt;- 6
bnr4.interv6 &lt;- extract(bnr4.turno)
# plot both extracted series on top of the original one
plot(bnr4, type="l", lty=2, xlab="Time")
lines(bnr4.interv3, col=2)
lines(bnr4.interv6, col=3)
legend(70, 580, c("original", "interval=3", "interval=6"), col=1:3, lty=c(2, 1, 1))
# It is hard to tell on the graph which series contains more information
# The turnogram shows us that it is the "interval=3" one!
</code></pre>

<hr>
<h2 id='turnpoints'> Analyze turning points (peaks or pits) </h2><span id='topic+turnpoints'></span><span id='topic+extract.turnpoints'></span><span id='topic+lines.turnpoints'></span><span id='topic+plot.turnpoints'></span><span id='topic+print.summary.turnpoints'></span><span id='topic+print.turnpoints'></span><span id='topic+summary.turnpoints'></span>

<h3>Description</h3>

<p>Determine the number and the position of extrema (turning points, either peaks or pits) in a regular time series. Calculate the quantity of information associated to the observations in this series, according to Kendall's information theory
</p>


<h3>Usage</h3>

<pre><code class='language-R'>turnpoints(x, calc.proba = TRUE)
## S3 method for class 'turnpoints'
print(x, ...)
## S3 method for class 'turnpoints'
summary(object, ...)
## S3 method for class 'summary.turnpoints'
print(x, ...)
## S3 method for class 'turnpoints'
plot(x, level = 0.05, lhorz = TRUE, lcol = 2, llty = 2,
    type = "l", xlab = "data number", ylab = paste("I (bits), level = ",
    level * 100, "%", sep = ""), main = paste("Information (turning points) for:",
    x$data), ...)
## S3 method for class 'turnpoints'
lines(x, max = TRUE, min = TRUE, median = TRUE,
    col = c(4, 4, 2), lty = c(2, 2, 1), ...)
## S3 method for class 'turnpoints'
extract(e, n, no.tp = 0, peak = 1, pit = -1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="turnpoints_+3A_x">x</code></td>
<td>
<p> a vector or a time series for <code>turnpoints()</code>, a 'turnpoints' object for the methods </p>
</td></tr>
<tr><td><code id="turnpoints_+3A_calc.proba">calc.proba</code></td>
<td>
<p> are the probabilities associated with each turning point also calculated? The default, <code>TRUE</code>, should be correct unless you really do not need these. In this case, the <code>plot()</code> method is not usable </p>
</td></tr>
<tr><td><code id="turnpoints_+3A_object">object</code></td>
<td>
<p> a 'turnpoints' object, as returned by the function <code>turnpoints()</code> </p>
</td></tr>
<tr><td><code id="turnpoints_+3A_e">e</code></td>
<td>
<p> a 'turnpoints' object, as returned by the function <code>turnpoints()</code> </p>
</td></tr>
<tr><td><code id="turnpoints_+3A_level">level</code></td>
<td>
<p> the significant level to draw on the graph if <code>lhorz=TRUE</code>. By default, <code>level=0.05</code>, which corresponds to a 5% p-value for the test </p>
</td></tr>
<tr><td><code id="turnpoints_+3A_lhorz">lhorz</code></td>
<td>
<p> if <code>lhorz=TRUE</code> (by default), an horizontal line indicating significant level is drawn on the graph </p>
</td></tr>
<tr><td><code id="turnpoints_+3A_lcol">lcol</code></td>
<td>
<p> the color to use to draw the significant level line, by default, color 2 is used </p>
</td></tr>
<tr><td><code id="turnpoints_+3A_llty">llty</code></td>
<td>
<p> the style to use for the significant level line. By default, style 2 is used (dashed line) </p>
</td></tr>
<tr><td><code id="turnpoints_+3A_type">type</code></td>
<td>
<p> the type of plot, as usual meaning for this graph parameter </p>
</td></tr>
<tr><td><code id="turnpoints_+3A_xlab">xlab</code></td>
<td>
<p> the label of the x-axis </p>
</td></tr>
<tr><td><code id="turnpoints_+3A_ylab">ylab</code></td>
<td>
<p> the label of the y-axis </p>
</td></tr>
<tr><td><code id="turnpoints_+3A_main">main</code></td>
<td>
<p> the main title of the graph</p>
</td></tr>
<tr><td><code id="turnpoints_+3A_max">max</code></td>
<td>
<p> do we plot the maximum envelope line (by default, yes) </p>
</td></tr>
<tr><td><code id="turnpoints_+3A_min">min</code></td>
<td>
<p> do we plot the minimum envelope line (by default, yes) </p>
</td></tr>
<tr><td><code id="turnpoints_+3A_median">median</code></td>
<td>
<p> do we plot the median line inside the envelope (by default, yes) </p>
</td></tr>
<tr><td><code id="turnpoints_+3A_col">col</code></td>
<td>
<p> a vector of three values for the color of the max, min, median lines, respectively. By default <code>col=c(4,4,2)</code> </p>
</td></tr>
<tr><td><code id="turnpoints_+3A_lty">lty</code></td>
<td>
<p> a vector of three values for the style of the max, min, median lines, respectively. By default <code>lty=c(2,2,1)</code>, that is: dashed, dashed and plain lines </p>
</td></tr>
<tr><td><code id="turnpoints_+3A_n">n</code></td>
<td>
<p> the number of points to extract. By default <code>n=length(turnp)</code>, all points are extracted </p>
</td></tr>
<tr><td><code id="turnpoints_+3A_no.tp">no.tp</code></td>
<td>
<p> extract gives a vector representing the position of extrema in the original series. <code>no.tp</code> represents the code to use for points that are not an extremum, by default '0' </p>
</td></tr>
<tr><td><code id="turnpoints_+3A_peak">peak</code></td>
<td>
<p> the code to use to flag a peak, by default '1' </p>
</td></tr>
<tr><td><code id="turnpoints_+3A_pit">pit</code></td>
<td>
<p> the code to use to flag a pit, by default '-1' </p>
</td></tr>
<tr><td><code id="turnpoints_+3A_...">...</code></td>
<td>
<p> Additional parameters </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function tests if the time series is purely random or not. Kendall (1976) proposed a series of tests for this. Moreover, graphical methods using the position of the turning points to draw automatically envelopes around the data are implemented, and also the drawing of median points between these envelopes.
</p>
<p>With a purely random time series, one expect to find, on average, a turning point (peak or pit that is, an observation that is preceeded and followed by, respectively, lower or higher observations) every 1.5 observation. Given it is impossible to determine if first and last observation are turning point, it gives:
</p>
<p style="text-align: center;"><code class="reqn">E(p) = 2/3*(n-2)</code>
</p>

<p>with p, the number of observed turning points and n the number of observations. The variance of p is:
</p>
<p style="text-align: center;"><code class="reqn">var(p) = (16*n - 29)/90</code>
</p>

<p>Ibanez (1982) demonstrated that P(t), the probability to observe a turning point at time t is:
</p>
<p style="text-align: center;"><code class="reqn">P(t) = 2*(1/n(t-1)! * (n-1)!)</code>
</p>

<p>where P is the probability to observe a turning point at time t under the null hypothesis that the time series is purely random, and thus, the distribution of turning points follows a normal distribution.
</p>
<p>The quantity of information I associated with this probability is:
</p>
<p style="text-align: center;"><code class="reqn">I = -log2 P(t)</code>
</p>

<p>It can be interpreted as follows. If I is larger, there are less turning points than expected in a purely random series. There are, thus, longer sequence of increasing or decreasing values along the time scale. This is considered to be more informative.
</p>
<p>As you can easily imagine, from this point on, it is straightforward to construct a test to determine if the series is random (regarding the distribution of the turning points), more or less monotonic (more or less turning points than expected).
</p>


<h3>Value</h3>

<p>An object of type 'turnpoints' is returned. It has methods <code>print()</code>, <code>summary()</code>, <code>plot()</code>, <code>lines()</code> and <code>extract()</code>.
Regarding your specific question, 'info' is the quantity of information I associated with the turning points:
</p>
<table>
<tr><td><code>data</code></td>
<td>
<p> The dataset to which the calculation is done </p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p> The number of observations </p>
</td></tr>
<tr><td><code>points</code></td>
<td>
<p> The value of the points in the series, after elimination of ex-aequos </p>
</td></tr>
<tr><td><code>pos</code></td>
<td>
<p> The position of the points on the time scale in the series (including ex-aequos) </p>
</td></tr>
<tr><td><code>exaequos</code></td>
<td>
<p> Location of exaequos (1), or not (0) </p>
</td></tr>
<tr><td><code>nturns</code></td>
<td>
<p> Total number of tunring points in the whole time series </p>
</td></tr>
<tr><td><code>firstispeak</code></td>
<td>
<p> Is the first turning point a peak (<code>TRUE</code>), or not (<code>FALSE</code>) </p>
</td></tr>
<tr><td><code>peaks</code></td>
<td>
<p> Logical vector. Location of the peaks in the time series without ex-aequos </p>
</td></tr>
<tr><td><code>pits</code></td>
<td>
<p> Logical vector. Location of the pits in the time series without ex-aequos</p>
</td></tr>
<tr><td><code>tppos</code></td>
<td>
<p> Position of the turning points in the initial series (with ex-aequos) </p>
</td></tr>
<tr><td><code>proba</code></td>
<td>
<p> Probability to find a turning point at this location (see details) </p>
</td></tr>
<tr><td><code>info</code></td>
<td>
<p> Quantity of information associated with this point (see details) </p>
</td></tr>
</table>


<h3>WARNING </h3>

<p> the <code>lines()</code> method should be used to draw lines on the graph of the <em>original</em> dataset (<code>plot(data, type="l")</code> for instance), <em>not</em> on the graph of turning points (<code>plot(turnp)</code>)! </p>


<h3>Author(s)</h3>

<p> Frederic Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>), Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>) </p>


<h3>References</h3>

<p>Ibanez, F., 1982. <em>Sur une nouvelle application de la theorie de l'information a la description des series chronologiques planctoniques.</em> J. Exp. Mar. Biol. Ecol., 4:619-632
</p>
<p>Kendall, M.G., 1976. <em>Time-series, 2nd ed.</em> Charles Griffin &amp; Co, London.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+turnogram">turnogram</a></code>, <code><a href="#topic+stat.slide">stat.slide</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(marbio)
plot(marbio[, "Nauplii"], type = "l")
# Calculate turning points for this series
Nauplii.tp &lt;- turnpoints(marbio[, "Nauplii"])
summary(Nauplii.tp)
plot(Nauplii.tp)
# Add envelope and median line to original data
plot(marbio[, "Nauplii"], type = "l")
lines(Nauplii.tp)
# Note that lines() applies to the graph of original dataset
title("Raw data, envelope maxi., mini. and median lines")
</code></pre>

<hr>
<h2 id='vario'> Compute and plot a semi-variogram </h2><span id='topic+vario'></span>

<h3>Description</h3>

<p>Compute a classical semi-variogram for a single regular time series
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vario(x, max.dist=length(x)/3, plotit=TRUE, vario.data=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vario_+3A_x">x</code></td>
<td>
<p> a vector or an univariate time series </p>
</td></tr>
<tr><td><code id="vario_+3A_max.dist">max.dist</code></td>
<td>
<p> the maximum distance to calculate. By default, it is the third of the number of observations </p>
</td></tr>
<tr><td><code id="vario_+3A_plotit">plotit</code></td>
<td>
<p> If <code>plotit=TRUE</code> then the graph of the semi-variogram is plotted </p>
</td></tr>
<tr><td><code id="vario_+3A_vario.data">vario.data</code></td>
<td>
<p> data coming from a previous call to <code>vario()</code>. Call the function again with these data to plot the corresponding graph </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame containing distance and semi-variogram values
</p>


<h3>Author(s)</h3>

<p> Frédéric Ibanez (<a href="mailto:ibanez@obs-vlfr.fr">ibanez@obs-vlfr.fr</a>), Philippe Grosjean (<a href="mailto:phgrosjean@sciviews.org">phgrosjean@sciviews.org</a>) </p>


<h3>References</h3>

 
<p>David, M., 1977. <em>Developments in geomathematics. Tome 2: Geostatistical or reserve estimation.</em> Elsevier Scientific, Amsterdam. 364 pp.
</p>
<p>Delhomme, J.P., 1978. <em>Applications de la théorie des variables régionalisées dans les sciences de l'eau.</em> Bull. BRGM, section 3 n°4:341-375.
</p>
<p>Matheron, G., 1971. <em>La théorie des variables régionalisées et ses applications.</em> Cahiers du Centre de Morphologie Mathématique de Fontainebleau. Fasc. 5 ENSMP, Paris. 212 pp.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+disto">disto</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(bnr)
vario(bnr[, 4])
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
