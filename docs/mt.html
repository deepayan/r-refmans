<!DOCTYPE html><html><head><title>Help for package mt</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {mt}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#abr1'><p>abr1 Data</p></a></li>
<li><a href='#accest'><p>Estimate Classification Accuracy By Resampling Method</p></a></li>
<li><a href='#binest'>
<p>Binary Classification</p></a></li>
<li><a href='#boot.err'><p>Calculate .632 and .632+ Bootstrap Error Rate</p></a></li>
<li><a href='#boxplot.frankvali'>
<p>Boxplot Method for Class 'frankvali'</p></a></li>
<li><a href='#boxplot.maccest'>
<p>Boxplot Method for Class 'maccest'</p></a></li>
<li><a href='#cl.rate'>
<p>Assess Classification Performances</p></a></li>
<li><a href='#classifier'>
<p>Wrapper Function for Classifiers</p></a></li>
<li><a href='#cor.util'>
<p>Correlation Analysis Utilities</p></a></li>
<li><a href='#dat.sel'>
<p>Generate Pairwise Data Set</p></a></li>
<li><a href='#data.visualisation'>
<p>Grouped Data Visualisation by PCA, MDS, PCADA and PLSDA</p></a></li>
<li><a href='#df.util'>
<p>Summary Utilities</p></a></li>
<li><a href='#feat.agg'><p> Rank aggregation by Borda count algorithm</p></a></li>
<li><a href='#feat.freq'><p>Frequency and Stability of Feature Selection</p></a></li>
<li><a href='#feat.mfs'>
<p>Multiple Feature Selection</p></a></li>
<li><a href='#feat.rank.re'>
<p>Feature Ranking with Resampling Method</p></a></li>
<li><a href='#frank.err'>
<p>Feature Ranking and Validation on Feature Subset</p></a></li>
<li><a href='#frankvali'>
<p>Estimates Feature Ranking Error Rate with Resampling</p></a></li>
<li><a href='#fs.anova'>
<p>Feature Selection Using ANOVA</p></a></li>
<li><a href='#fs.auc'>
<p>Feature Selection Using Area under Receiver Operating Curve (AUC)</p></a></li>
<li><a href='#fs.bw'>
<p>Feature Selection Using Between-Group to Within-Group (BW) Ratio</p></a></li>
<li><a href='#fs.kruskal'>
<p>Feature Selection Using Kruskal-Wallis Test</p></a></li>
<li><a href='#fs.pca'>
<p>Feature Selection by PCA</p></a></li>
<li><a href='#fs.pls'>
<p>Feature Selection Using PLS</p></a></li>
<li><a href='#fs.relief'>
<p>Feature Selection Using RELIEF Method</p></a></li>
<li><a href='#fs.rf'><p> Feature Selection Using Random Forests (RF)</p></a></li>
<li><a href='#fs.rfe'>
<p>Feature Selection Using SVM-RFE</p></a></li>
<li><a href='#fs.snr'>
<p>Feature Selection Using Signal-to-Noise Ratio (SNR)</p></a></li>
<li><a href='#fs.welch'>
<p>Feature Selection Using Welch Test</p></a></li>
<li><a href='#fs.wilcox'>
<p>Feature Selection Using Wilcoxon Test</p></a></li>
<li><a href='#get.fs.len'>
<p>Get Length of Feature Subset for Validation</p></a></li>
<li><a href='#grpplot'><p>Plot Matrix-Like Object by Group</p></a></li>
<li><a href='#list.util'>
<p>List Manipulation Utilities</p></a></li>
<li><a href='#maccest'><p>Estimation of Multiple Classification Accuracy</p></a></li>
<li><a href='#mbinest'>
<p>Binary Classification by Multiple Classifier</p></a></li>
<li><a href='#mc.anova'>
<p>Multiple Comparison by 'ANOVA' and Pairwise Comparison by 'HSDTukey Test'</p></a></li>
<li><a href='#mc.fried'>
<p>Multiple Comparison by 'Friedman Test' and Pairwise Comparison by 'Wilcoxon Test'</p></a></li>
<li><a href='#mc.norm'>
<p>Normality Test by Shapiro-Wilk Test</p></a></li>
<li><a href='#mdsplot'><p>Plot Classical Multidimensional Scaling</p></a></li>
<li><a href='#mv.util'>
<p>Missing Value Utilities</p></a></li>
<li><a href='#osc'>
<p>Orthogonal Signal Correction (OSC)</p></a></li>
<li><a href='#osc_sjoblom'>
<p>Orthogonal Signal Correction (OSC) Approach by Sjoblom et al.</p></a></li>
<li><a href='#osc_wise'>
<p>Orthogonal Signal Correction (OSC) Approach by Wise and Gallagher.</p></a></li>
<li><a href='#osc_wold'>
<p>Orthogonal Signal Correction (OSC) Approach by Wold et al.</p></a></li>
<li><a href='#panel.elli'><p> Panel Function for Plotting Ellipse and outlier</p></a></li>
<li><a href='#panel.smooth.line'><p> Panel Function for Plotting Regression Line</p></a></li>
<li><a href='#pca.outlier'><p> Outlier detection by PCA</p></a></li>
<li><a href='#pcalda'>
<p>Classification with PCADA</p></a></li>
<li><a href='#pcaplot'><p>Plot Function for PCA with Grouped Values</p></a></li>
<li><a href='#plot.accest'>
<p>Plot Method for Class 'accest'</p></a></li>
<li><a href='#plot.maccest'>
<p>Plot Method for Class 'maccest'</p></a></li>
<li><a href='#plot.pcalda'>
<p>Plot Method for Class 'pcalda'</p></a></li>
<li><a href='#plot.plsc'>
<p>Plot Method for Class 'plsc' or 'plslda'</p></a></li>
<li><a href='#plsc'>
<p>Classification with PLSDA</p></a></li>
<li><a href='#predict.osc'>
<p>Predict Method for Class 'osc'</p></a></li>
<li><a href='#predict.pcalda'>
<p>Predict Method for Class 'pcalda'</p></a></li>
<li><a href='#predict.plsc'>
<p>Predict Method for Class 'plsc' or 'plslda'</p></a></li>
<li><a href='#preproc'>
<p>Pre-process Data Set</p></a></li>
<li><a href='#pval.util'>
<p>P-values Utilities</p></a></li>
<li><a href='#save.tab'>
<p>Save List of Data Frame or Matrix into CSV File</p></a></li>
<li><a href='#stats.util'>
<p>Statistical Summary Utilities for Two-Classes Data</p></a></li>
<li><a href='#trainind'><p> Generate Index of Training Samples</p></a></li>
<li><a href='#tune.func'><p>Functions for Tuning Appropriate Number of Components</p></a></li>
<li><a href='#valipars'><p>Generate Control Parameters for Resampling</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>2.0-1.20</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-02-12</td>
</tr>
<tr>
<td>Title:</td>
<td>Metabolomics Data Analysis Toolbox</td>
</tr>
<tr>
<td>Author:</td>
<td>Wanchang Lin</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Wanchang Lin &lt;wanchanglin@hotmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Functions for metabolomics data analysis: data preprocessing, 
  orthogonal signal correction, PCA analysis, PCA-DA analysis, 
	PLS-DA analysis, classification, feature selection, correlation 
	analysis, data visualisation and re-sampling strategies.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, MASS, class, e1071, randomForest, pls, ellipse,
lattice, latticeExtra</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/wanchanglin/mt">https://github.com/wanchanglin/mt</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/wanchanglin/mt/issues">https://github.com/wanchanglin/mt/issues</a></td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>ZipData:</td>
<td>No</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-12 11:58:01 UTC; wl</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-12 12:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='abr1'>abr1 Data</h2><span id='topic+abr1'></span>

<h3>Description</h3>

<p>An FIE-MS data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(abr1)
</code></pre>


<h3>Details</h3>

<p><code>abr1</code> is an FIE-MS data matrices developed from analysis of
samples representing a time course of pathogen attack in a model plant
species (Brachypodium distachyon). The data was developed in a single
batch with all samples randomised using a Thermo LTQ linear ion trap.
Both positive and negative ion mode are given (<code>abr1$pos</code> and
<code>abr1$neg</code>).
</p>


<h3>Value</h3>

<p>A list with the following elements:
</p>
<table>
<tr><td><code>fact</code></td>
<td>
<p>A data frame containing experimental meta-data.</p>
</td></tr>
<tr><td><code>pos</code></td>
<td>
<p>A data frame for positive data with 120 observations and
2000 variables.</p>
</td></tr>
<tr><td><code>neg</code></td>
<td>
<p>A data frame for negative data with 120 observations and
2000 variables.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># Load data set
data(abr1)

# Select data set
dat &lt;- abr1$neg

# number of observations and variables
dim(dat)

# Transform data
dat.log   &lt;- preproc(dat, method = "log")
dat.sqrt  &lt;- preproc(dat, method = "sqrt")
dat.asinh &lt;- preproc(dat, method = "asinh")

op &lt;- par(mfrow=c(2,2), pch=16)
matplot(t(dat),main="Original",type="l",col="blue",
     ylab="Intensity")
matplot(t(dat.log),main="Log",type="l",col="green",
     ylab="Intensity")
matplot(t(dat.sqrt),main="Sqrt",type="l",col="red",
     ylab="Intensity")
matplot(t(dat.asinh),main="ArcSinh)",type="l",col="black",
     ylab="Intensity")
par(op)
mtext("Data set", line=2.5, font=3, cex=1.5)

</code></pre>

<hr>
<h2 id='accest'>Estimate Classification Accuracy By Resampling Method</h2><span id='topic+accest'></span><span id='topic+aam.cl'></span><span id='topic+aam.mcl'></span><span id='topic+accest.formula'></span><span id='topic+accest.default'></span><span id='topic+print.accest'></span><span id='topic+summary.accest'></span><span id='topic+print.summary.accest'></span>

<h3>Description</h3>

<p>Estimate classification accuracy rate by resampling method. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>accest(dat, ...)

## Default S3 method:
accest(dat, cl, method, pred.func=predict,pars = valipars(), 
       tr.idx = NULL, ...) 

## S3 method for class 'formula'
accest(formula, data = NULL, ..., subset, na.action = na.omit)

aam.cl(x,y,method, pars = valipars(),...)

aam.mcl(x,y,method, pars = valipars(),...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="accest_+3A_formula">formula</code></td>
<td>

<p>A formula of the form <code>groups ~ x1 + x2 + ...</code>  That is, the
response is the grouping factor and the right hand side specifies
the (non-factor) discriminators.
</p>
</td></tr>
<tr><td><code id="accest_+3A_data">data</code></td>
<td>

<p>Data frame from which variables specified in <code>formula</code> are
preferentially to be taken.
</p>
</td></tr>
<tr><td><code id="accest_+3A_dat">dat</code>, <code id="accest_+3A_x">x</code></td>
<td>

<p>A matrix or data frame containing the explanatory variables if no
formula is given as the principal argument.
</p>
</td></tr>
<tr><td><code id="accest_+3A_cl">cl</code>, <code id="accest_+3A_y">y</code></td>
<td>

<p>A factor specifying the class for each observation if no formula
principal argument is given.
</p>
</td></tr>
<tr><td><code id="accest_+3A_method">method</code></td>
<td>

<p>Classification method whose accuracy rate is to be estimated, such as 
<code>randomForest</code>, <code>svm</code>, <code>knn</code> and <code>lda</code>. 
For details, see <code>note</code> below. Either a function or a character 
string naming the function to be called.
</p>
</td></tr>   
<tr><td><code id="accest_+3A_pred.func">pred.func</code></td>
<td>

<p>Predict method (default is <code>predict</code>). Either a function or a
character string naming the function to be called.
</p>
</td></tr>
<tr><td><code id="accest_+3A_pars">pars</code></td>
<td>

<p>A list of parameters using by the resampling method such as 
<em>Leave-one-out cross-validation</em>, <em>Cross-validation</em>, 
<em>Bootstrap</em> and <em>Randomised validation (holdout)</em>.
See <code><a href="#topic+valipars">valipars</a></code> for details.
</p>
</td></tr>
<tr><td><code id="accest_+3A_tr.idx">tr.idx</code></td>
<td>

<p>User defined index of training samples. Can be generated by <code>trainind</code>.
</p>
</td></tr>
<tr><td><code id="accest_+3A_...">...</code></td>
<td>
<p>Additional parameters to <code>method</code>.</p>
</td></tr>
<tr><td><code id="accest_+3A_subset">subset</code></td>
<td>

<p>Optional vector, specifying a subset of observations to be used.
</p>
</td></tr>
<tr><td><code id="accest_+3A_na.action">na.action</code></td>
<td>

<p>Function which indicates what should happen when the data
contains <code>NA</code>'s, defaults to <code><a href="stats.html#topic+na.omit">na.omit</a></code>.
</p>
</td></tr> 
</table>


<h3>Details</h3>

<p>The accuracy rates of classification are estimated by techniques such
as <em>Random Forest</em>, <em>Support Vector Machine</em>,
<em>k-Nearest Neighbour Classification</em> and <em>Linear
Discriminant Analysis</em> based on resampling methods, including
<em>Leave-one-out cross-validation</em>, <em>Cross-validation</em>,
<em>Bootstrap</em> and <em>Randomised validation (holdout)</em>.
</p>


<h3>Value</h3>

<p><code>accest</code> returns an object including the components:
</p>
<table>
<tr><td><code>method</code></td>
<td>
<p>Classification method used.</p>
</td></tr>
<tr><td><code>acc</code></td>
<td>
<p>Overall accuracy rate.</p>
</td></tr>
<tr><td><code>acc.iter</code></td>
<td>
<p>Average accuracy rate for each iteration.</p>
</td></tr>
<tr><td><code>acc.all</code></td>
<td>
<p>Accuracy rate for each iteration and replication.</p>
</td></tr>
<tr><td><code>auc</code></td>
<td>
<p>Overall area under receiver operating curve (AUC).</p>
</td></tr>
<tr><td><code>auc.iter</code></td>
<td>
<p>Average AUC for each iteration.</p>
</td></tr>
<tr><td><code>auc.all</code></td>
<td>
<p>AUC for each iteration and replication.</p>
</td></tr>
<tr><td><code>mar</code></td>
<td>
<p>Overall prediction margin.</p>
</td></tr>
<tr><td><code>mar.iter</code></td>
<td>
<p>Average prediction margin for each iteration.</p>
</td></tr>
<tr><td><code>mar.all</code></td>
<td>
<p>Prediction margin for each iteration and replication.</p>
</td></tr>
<tr><td><code>err</code></td>
<td>
<p>Overall error rate.</p>
</td></tr>
<tr><td><code>err.iter</code></td>
<td>
<p>Average error rate for each iteration.</p>
</td></tr>
<tr><td><code>err.all</code></td>
<td>
<p>Error rate for each iteration and replication.</p>
</td></tr>
<tr><td><code>sampling</code></td>
<td>
<p>Sampling scheme used.</p>
</td></tr> 
<tr><td><code>niter</code></td>
<td>
<p>Number of iteration.</p>
</td></tr>
<tr><td><code>nreps</code></td>
<td>
<p>Number of replications in each iteration if resampling is 
not <code>loocv</code>. </p>
</td></tr>
<tr><td><code>conf</code></td>
<td>
<p>Overall confusion matrix.</p>
</td></tr>
<tr><td><code>res.all</code></td>
<td>
<p>All results which can be further processed.</p>
</td></tr>      
<tr><td><code>acc.boot</code></td>
<td>
<p> A list of bootstrap accuracy such as <code>.632</code> and
<code>.632+</code> if the resampling method is bootstrap.
</p>
</td></tr> 
</table>
<p><code>aam.cl</code> returns a vector with <code>acc</code> (accuracy), 
<code>auc</code>(area under ROC curve) and <code>mar</code>(class margin).
</p>
<p><code>aam.mcl</code> returns a matrix with columns of <code>acc</code> (accuracy), 
<code>auc</code>(area under ROC curve) and <code>mar</code>(class margin).
</p>


<h3>Note</h3>

<p>The <code>accest</code> can take any classification models if their argument
format is <code>model(formula, data, subset, na.action, ...)</code> and
their corresponding method <code>predict.model(object, newdata, ...)</code>
can either return the only predicted class label or a list with a
component called <code>class</code>, such as <code>lda</code> and <code>pcalda</code>.
</p>
<p>If classifier <code>method</code> provides posterior probabilities, the
prediction margin <code>mar</code> will be generated, otherwise <code>NULL</code>.
</p>
<p>If classifier <code>method</code> provides posterior probabilities and the
classification is for two-class problem, <code>auc</code> will be generated,
otherwise <code>NULL</code>.
</p>
<p><code>aam.cl</code> is a wrapper function of <code>accest</code>, returning
accuracy rate, AUC and classification margin. <code>aam.mcl</code> accepts
multiple classifiers in one running.
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin
</p>


<h3>See Also</h3>

<p><code><a href="#topic+binest">binest</a></code>, <code><a href="#topic+maccest">maccest</a></code>, <code><a href="#topic+valipars">valipars</a></code>, 
<code><a href="#topic+trainind">trainind</a></code>, <code><a href="#topic+classifier">classifier</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Iris data
data(iris)
# Use KNN classifier and bootstrap for resampling
acc &lt;- accest(Species~., data = iris, method = "knn",
              pars = valipars(sampling = "boot",niter = 2, nreps=5))
acc
summary(acc)
acc$acc.boot

# alternatively the traditional interface:
x &lt;- subset(iris, select = -Species)
y &lt;- iris$Species

## -----------------------------------------------------------------------
# Random Forest with 5-fold stratified cv 
pars   &lt;- valipars(sampling = "cv",niter = 4, nreps=5, strat=TRUE)
tr.idx &lt;- trainind(y,pars=pars)
acc1   &lt;- accest(x, y, method = "randomForest", pars = pars, tr.idx=tr.idx)
acc1
summary(acc1)
# plot the accuracy in each iteration
plot(acc1)

## -----------------------------------------------------------------------
# Forensic Glass data in chap.12 of MASS
data(fgl, package = "MASS")    # in MASS package
# Randomised validation (holdout) of SVM for fgl data
acc2 &lt;- accest(type~., data = fgl, method = "svm", cost = 100, gamma = 1, 
              pars = valipars(sampling = "rand",niter = 10, nreps=4,div = 2/3) )
              
acc2
summary(acc2)
# plot the accuracy in each iteration
plot(acc2)

## -----------------------------------------------------------------------
## Examples of amm.cl and aam.mcl
aam.1 &lt;- aam.cl(x,y,method="svm",pars=pars)
aam.2 &lt;- aam.mcl(x,y,method=c("svm","randomForest"),pars=pars)

## If use two classes, AUC will be calculated
idx &lt;- (y == "setosa")
aam.3 &lt;- aam.cl(x[!idx,],factor(y[!idx]),method="svm",pars=pars)
aam.4 &lt;- aam.mcl(x[!idx,],factor(y[!idx]),method=c("svm","randomForest"),pars=pars)

</code></pre>

<hr>
<h2 id='binest'>
Binary Classification
</h2><span id='topic+binest'></span>

<h3>Description</h3>

<p>Binary classification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  binest(dat, cl, choices = NULL, method, pars=valipars(),...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="binest_+3A_dat">dat</code></td>
<td>

<p>A matrix or data frame containing the explanatory variables.
</p>
</td></tr>
<tr><td><code id="binest_+3A_cl">cl</code></td>
<td>

<p>A factor specifying the class for each observation.
</p>
</td></tr>
<tr><td><code id="binest_+3A_choices">choices</code></td>
<td>

<p>The vector or list of class labels to be chosen for binary
classification. For details, see <code><a href="#topic+dat.sel">dat.sel</a></code>.
</p>
</td></tr>
<tr><td><code id="binest_+3A_method">method</code></td>
<td>

<p>Classification method to be used. For details, see
<code><a href="#topic+accest">accest</a></code>.
</p>
</td></tr>             
<tr><td><code id="binest_+3A_pars">pars</code></td>
<td>

<p>A list of parameters of the resampling method. For details, see
<code><a href="#topic+valipars">valipars</a></code>.
</p>
</td></tr>
<tr><td><code id="binest_+3A_...">...</code></td>
<td>
<p>Additional parameters to <code>method</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>com</code></td>
<td>
<p>A matrix of combination of the binary class labels.</p>
</td></tr>
<tr><td><code>acc</code></td>
<td>
<p>A table of classification accuracy for the binary
combination in each iteration.
</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>Classification method used.</p>
</td></tr>
<tr><td><code>sampling</code></td>
<td>
<p>Sampling scheme used.</p>
</td></tr> 
<tr><td><code>niter</code></td>
<td>
<p>Number of iterations.</p>
</td></tr>
<tr><td><code>nreps</code></td>
<td>
<p>Number of replications in each iteration if resampling is
not <code>loocv</code>. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wanchang Lin
</p>


<h3>See Also</h3>

<p><code><a href="#topic+accest">accest</a></code>, <code><a href="#topic+valipars">valipars</a></code>, <code><a href="#topic+dat.sel">dat.sel</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># iris data set
data(iris)
dat &lt;- subset(iris, select = -Species)
cl  &lt;- iris$Species

## PCALDA with cross-validation
pars    &lt;- valipars(sampling="cv",niter = 6, nreps = 5)
binpcalda &lt;- binest(dat,cl,choices=c("setosa"), method="pcalda", pars = pars)

## SVM with leave-one-out cross-validation. SVM kernel is 'linear'.
pars   &lt;- valipars(sampling="loocv")
binsvm &lt;- binest(dat,cl,choices=c("setosa","virginica"), method="svm",
                 pars = pars, kernel="linear")

## randomForest with bootstrap
pars  &lt;- valipars(sampling="boot",niter = 5, nreps = 5)
binrf &lt;- binest(dat,cl,choices=c("setosa","virginica"), 
                method="randomForest", pars = pars)

## KNN with randomised validation. The number of neighbours is 3.
pars   &lt;- valipars(sampling="rand",niter = 5, nreps = 5)
binknn &lt;- binest(dat,cl,choices = list(c("setosa","virginica"),
                                       c("virginica","versicolor")), 
                 method="knn",pars = pars, k = 3)

</code></pre>

<hr>
<h2 id='boot.err'>Calculate .632 and .632+ Bootstrap Error Rate</h2><span id='topic+boot.err'></span>

<h3>Description</h3>

<p>Calculate .632 bootstrap and .632 plus bootstrap error rate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>boot.err(err, resub)       
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="boot.err_+3A_err">err</code></td>
<td>
<p> Average error rate of bootstrap samples.</p>
</td></tr>
<tr><td><code id="boot.err_+3A_resub">resub</code></td>
<td>
<p> A list including apparent error rate, class label and
the predicted class label of the original training data (not resampled
training data). Can be generated by <code><a href="#topic+classifier">classifier</a></code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>ae</code></td>
<td>
<p>Apparent error rate.</p>
</td></tr>
<tr><td><code>boot</code></td>
<td>
<p>Average error rate of bootstrap samples(Same as <code>err</code>)</p>
</td></tr>
<tr><td><code>b632</code></td>
<td>
<p>.632 bootstrap error rate.</p>
</td></tr>
<tr><td><code>b632p</code></td>
<td>
<p>.632 plus bootstrap error rate.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>References</h3>

<p>Witten, I. H. and Frank, E. (2005) <em>Data Mining - Practical
Machine Learning and Techniques</em>. Elsevier.
</p>
<p>Efron, B. and Tibshirani, R. (1993) <em>An Introduction to the
Bootstrap</em>. Chapman &amp; Hall.
</p>
<p>Efron, B. and Tibshirani, R. (1997) Improvements on cross-validation:
the .632+ bootstrap method. <em>Journal of the American Statistical
Association</em>, <b>92</b>, 548-560.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+classifier">classifier</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## iris data set
data(iris)
x      &lt;- subset(iris, select = -Species)
y      &lt;- iris$Species

## 10 bootstrap training samples 
pars   &lt;- valipars(sampling = "boot", niter = 1, nreps = 10)
tr.idx &lt;- trainind(y, pars=pars)[[1]]

## bootstrap error rate
err &lt;- sapply(tr.idx, function(i){
  pred &lt;- classifier(x[i,,drop = FALSE],y[i],x[-i,,drop = FALSE],y[-i], 
                     method = "knn")$err
})

## average bootstrap error rate
err &lt;- mean(err)

## apparent error rate
resub  &lt;- classifier(x,y,method = "knn")

## 
err.boot &lt;- boot.err(err, resub)

</code></pre>

<hr>
<h2 id='boxplot.frankvali'>
Boxplot Method for Class 'frankvali'
</h2><span id='topic+boxplot.frankvali'></span>

<h3>Description</h3>

<p>Boxplot method for error rate of each feature subset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'frankvali'
boxplot(x,  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="boxplot.frankvali_+3A_x">x</code></td>
<td>

<p>An object of class <code>frankvali</code>.
</p>
</td></tr>
<tr><td><code id="boxplot.frankvali_+3A_...">...</code></td>
<td>

<p>Additional arguments to the plot, such as <code>main</code>, <code>xlab</code> and 
<code>ylab</code>. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a method for the generic function <code>boxplot()</code> for class 
<code>frankvali</code>. It plots the error rate of each feature subset.
</p>


<h3>Value</h3>

<p>Returns boxplot of class <code>frankvali</code>.
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+frankvali">frankvali</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(abr1)
dat &lt;- abr1$pos[,110:500]

x   &lt;- preproc(dat, method="log10")  
y   &lt;- factor(abr1$fact$class)        

dat &lt;- dat.sel(x, y, choices=c("1","2"))
x.1 &lt;- dat[[1]]$dat
y.1 &lt;- dat[[1]]$cls

pars &lt;- valipars(sampling="cv",niter=2,nreps=4)
res  &lt;- frankvali(x.1,y.1,fs.method = "fs.rfe",fs.len = "power2", 
                  cl.method = "knn",pars = pars)
res
summary(res)
boxplot(res)                  
</code></pre>

<hr>
<h2 id='boxplot.maccest'>
Boxplot Method for Class 'maccest'
</h2><span id='topic+boxplot.maccest'></span>

<h3>Description</h3>

<p>Boxplot method for the accuracy rate of each classifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'maccest'
boxplot(x,  ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="boxplot.maccest_+3A_x">x</code></td>
<td>

<p>An object of class <code>maccest</code>.
</p>
</td></tr>
<tr><td><code id="boxplot.maccest_+3A_...">...</code></td>
<td>

<p>Additional arguments to the plot, such as <code>main</code>, <code>xlab</code> and 
<code>ylab</code>. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a method for the generic function <code>boxplot()</code> for class 
<code>maccest</code>. It plots the accurary rate for each classifier.
</p>


<h3>Value</h3>

<p>Returns boxplot of class <code>maccest</code>.
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+maccest">maccest</a></code>, <code><a href="#topic+plot.maccest">plot.maccest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Iris data
data(iris)
x      &lt;- subset(iris, select = -Species)
y      &lt;- iris$Species

method &lt;- c("randomForest","svm","knn")
pars   &lt;- valipars(sampling="cv", niter = 2, nreps=5)
tr.idx &lt;- trainind(y, pars=pars)
res    &lt;- maccest(x, y, method=method, pars=pars,
                  comp="anova",kernel="linear") 

res
boxplot(res)
</code></pre>

<hr>
<h2 id='cl.rate'>
Assess Classification Performances
</h2><span id='topic+cl.rate'></span><span id='topic+cl.perf'></span><span id='topic+cl.roc'></span><span id='topic+cl.auc'></span>

<h3>Description</h3>

<p>Assess classification performances.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  cl.rate(obs, pre)
  cl.perf(obs, pre, pos=levels(as.factor(obs))[2])
  cl.roc(stat, label, pos=levels(as.factor(label))[2], plot=TRUE, ...)
  cl.auc(stat, label, pos=levels(as.factor(label))[2])
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cl.rate_+3A_obs">obs</code></td>
<td>

<p>Factor or vector of observed class.
</p>
</td></tr>
<tr><td><code id="cl.rate_+3A_pre">pre</code></td>
<td>

<p>Factor or vector of predicted class.
</p>
</td></tr>
<tr><td><code id="cl.rate_+3A_stat">stat</code></td>
<td>

<p>Factor or vector of statistics for positives/cases. 
</p>
</td></tr>
<tr><td><code id="cl.rate_+3A_label">label</code></td>
<td>

<p>Factor or vector of label for categorical data.
</p>
</td></tr>
<tr><td><code id="cl.rate_+3A_pos">pos</code></td>
<td>

<p>Characteristic string for positive.
</p>
</td></tr>
<tr><td><code id="cl.rate_+3A_plot">plot</code></td>
<td>

<p>Logical flag indicating whether ROC should be plotted.  
</p>
</td></tr>
<tr><td><code id="cl.rate_+3A_...">...</code></td>
<td>
<p> Further arguments for plotting.  </p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>cl.perf</code> gets the classification performances such as accuracy rate and 
false positive rate. <code>cl.roc</code> computes receiver operating characteristics 
(ROC).  <code>cl.auc</code> calculates area under ROC curve. Three functions are only 
for binary class problems.  
</p>


<h3>Value</h3>

<p><code>cl.rate</code> returns a list with components:
</p>
<table>
<tr><td><code>acc</code></td>
<td>
<p> Accuracy rate of classification.</p>
</td></tr>
<tr><td><code>err</code></td>
<td>
<p> Error rate of classification.</p>
</td></tr>
<tr><td><code>con.mat</code></td>
<td>
<p> Confusion matrix. </p>
</td></tr>
<tr><td><code>kappa</code></td>
<td>
<p> Kappa Statistics.</p>
</td></tr>
</table>
<p><code>cl.perf</code> returns a list with components:
</p>
<table>
<tr><td><code>acc</code></td>
<td>
<p> Accuracy rate</p>
</td></tr>
<tr><td><code>tpr</code></td>
<td>
<p> True positive rate</p>
</td></tr>
<tr><td><code>fpr</code></td>
<td>
<p> False positive rate</p>
</td></tr>
<tr><td><code>sens</code></td>
<td>
<p> Sensitivity</p>
</td></tr>
<tr><td><code>spec</code></td>
<td>
<p> Specificity</p>
</td></tr>
<tr><td><code>con.mat</code></td>
<td>
<p> Confusion matrix. </p>
</td></tr>
<tr><td><code>kappa</code></td>
<td>
<p> Kappa Statistics.</p>
</td></tr>
<tr><td><code>positive</code></td>
<td>
<p> Positive level.</p>
</td></tr>
</table>
<p><code>cl.roc</code> returns a list with components:
</p>
<table>
<tr><td><code>perf</code></td>
<td>
<p>A data frame of <code>acc</code>, <code>tpr</code>,<code>fpr</code>,<code>sens</code>,
<code>spec</code> and <code>cutoff</code> (thresholds).</p>
</td></tr>
<tr><td><code>auc</code></td>
<td>
<p> Area under ROC curve</p>
</td></tr>
<tr><td><code>positive</code></td>
<td>
<p> Positive level.</p>
</td></tr>
</table>
<p><code>cl.auc</code> returns a scalar value of AUC.
</p>


<h3>Note</h3>

<p>AUC varies between 0.5 and 1.0 for sensible models; the higher the better. If 
it is less than 0.5, it should be corrected by <code>1 - AUC</code>. Or re-run it by 
using <code>1 - stat</code>. 
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>References</h3>

<p>Fawcett, F. (2006) <em>An introduction to ROC analysis</em>. 
<em>Pattern Recognition Letters</em>.  vol. 27, 861-874.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Measurements of Forensic Glass Fragments
library(MASS)
data(fgl, package = "MASS")    # in MASS package
dat &lt;- subset(fgl, grepl("WinF|WinNF",type))
## dat &lt;- subset(fgl, type %in% c("WinF", "WinNF"))
x   &lt;- subset(dat, select = -type)
y   &lt;- factor(dat$type)

## construct train and test data 
idx   &lt;- sample(1:nrow(x), round((2/3)*nrow(x)), replace = FALSE) 
tr.x  &lt;- x[idx,]
tr.y  &lt;- y[idx]
te.x  &lt;- x[-idx,]        
te.y  &lt;- y[-idx] 

model &lt;- lda(tr.x, tr.y)

## predict the test data results
pred  &lt;- predict(model, te.x)

## classification performances
obs &lt;- te.y
pre &lt;- pred$class   
cl.rate(obs, pre)
cl.perf(obs, pre, pos="WinNF")
## change positive as "WinF"
cl.perf(obs, pre, pos="WinF")

## ROC and AUC
pos  &lt;- "WinNF"            ## or "WinF"
stat &lt;- pred$posterior[,pos]
## levels(obs) &lt;- c(0,1)

cl.auc (stat,obs, pos=pos)
cl.roc (stat,obs, pos=pos)

## test examples for ROC and AUC
label &lt;- rbinom(30,size=1,prob=0.2)
stat  &lt;- rnorm(30)
cl.roc(stat,label, pos=levels(factor(label))[2],plot = TRUE)
cl.auc(stat,label,pos=levels(factor(label))[2])

## if auc is less than 0.5, it should be adjusted by 1 - auc. 
## Or re-run them:
cl.roc(1 - stat,label, pos=levels(factor(label))[2],plot = TRUE)
cl.auc(1 - stat,label,pos=levels(factor(label))[2])

</code></pre>

<hr>
<h2 id='classifier'>
Wrapper Function for Classifiers
</h2><span id='topic+classifier'></span>

<h3>Description</h3>

<p>Wrapper function for classifiers. The classification model is built up on the 
training data and error estimation is performed on the test data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>classifier(dat.tr, cl.tr, dat.te=NULL, cl.te=NULL, method,
           pred.func=predict,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="classifier_+3A_dat.tr">dat.tr</code></td>
<td>

<p>A data frame or matrix of training data. The classification model are built 
on it.
</p>
</td></tr>
<tr><td><code id="classifier_+3A_cl.tr">cl.tr</code></td>
<td>

<p>A factor or vector of training class.
</p>
</td></tr>
<tr><td><code id="classifier_+3A_dat.te">dat.te</code></td>
<td>

<p>A data frame or matrix of test data. Error rates are calculated on this data set.
</p>
</td></tr>
<tr><td><code id="classifier_+3A_cl.te">cl.te</code></td>
<td>

<p>A factor or vector of test class.
</p>
</td></tr>
<tr><td><code id="classifier_+3A_method">method</code></td>
<td>

<p>Classification method to be used. Any classification methods can be employed 
if they have method <code>predict</code> (except <code>knn</code>) with output of predicted class
label or one component with name of <code>class</code> in the returned list, such as 
<code>randomForest</code>, <code>svm</code>, <code>knn</code> and <code>lda</code>. 
Either a function or a character string naming the function to be called
</p>
</td></tr>
<tr><td><code id="classifier_+3A_pred.func">pred.func</code></td>
<td>

<p>Predict method (default is <code>predict</code>). Either a function or a character 
string naming the function to be called. 
</p>
</td></tr>
<tr><td><code id="classifier_+3A_...">...</code></td>
<td>
<p>Additional parameters to <code>method</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list including components:
</p>
<table>
<tr><td><code>err</code></td>
<td>
<p>Error rate of test data.</p>
</td></tr>
<tr><td><code>cl</code></td>
<td>
<p>The original class of test data.</p>
</td></tr>
<tr><td><code>pred</code></td>
<td>
<p>The predicted class of test data.</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>

<p>Posterior probabilities for the classes if <code>method</code> provides posterior 
output.    
</p>
</td></tr>
<tr><td><code>acc</code></td>
<td>
<p> Accuracy rate of classification.</p>
</td></tr>
<tr><td><code>margin</code></td>
<td>

<p>The margin of predictions from classifier <code>method</code> if it provides posterior 
output. 
</p>
<p>The margin of a data point is defined as the proportion of probability for the 
correct class minus maximum proportion of probabilities for the other classes. 
Positive margin means correct classification, and vice versa. This idea come
from package <span class="pkg">randomForest</span>. For more details, see 
<code><a href="randomForest.html#topic+margin">margin</a></code>.
</p>
</td></tr>
<tr><td><code>auc</code></td>
<td>

<p>The area under receiver operating curve (AUC) if classifier <code>method</code> 
produces posterior probabilities and the classification is for two-class 
problem. 
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The definition of margin is based on the posterior probabilities. Classifiers,
such as <code><a href="randomForest.html#topic+randomForest">randomForest</a></code>, <code><a href="e1071.html#topic+svm">svm</a></code>,
<code><a href="MASS.html#topic+lda">lda</a></code>, <code><a href="MASS.html#topic+qda">qda</a></code>, <code><a href="#topic+pcalda">pcalda</a></code> and
<code><a href="#topic+plslda">plslda</a></code>, do output posterior probabilities. But 
<code><a href="class.html#topic+knn">knn</a></code> does not.   
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+accest">accest</a></code>, <code><a href="#topic+maccest">maccest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(abr1)
dat &lt;- preproc(abr1$pos[,110:500], method="log10")  
cls &lt;- factor(abr1$fact$class)        

## tmp &lt;- dat.sel(dat, cls, choices=c("1","2"))
## dat &lt;- tmp[[1]]$dat
## cls &lt;- tmp[[1]]$cls

idx &lt;- sample(1:nrow(dat), round((2/3)*nrow(dat)), replace = FALSE) 
## constrcuct train and test data 
train.dat  &lt;- dat[idx,]
train.cl   &lt;- cls[idx]
test.dat   &lt;- dat[-idx,]       
test.cl    &lt;- cls[-idx] 

## estimates accuracy
res &lt;- classifier(train.dat, train.cl, test.dat, test.cl, 
                  method="randomForest")
res
## get confusion matrix
cl.rate(obs=res$cl, res$pred)   ## same as: cl.rate(obs=test.cl, res$pred)

## Measurements of Forensic Glass Fragments
data(fgl, package = "MASS")    # in MASS package
dat &lt;- subset(fgl, grepl("WinF|WinNF",type))
## dat &lt;- subset(fgl, type %in% c("WinF", "WinNF"))
x   &lt;- subset(dat, select = -type)
y   &lt;- factor(dat$type)

## construct train and test data 
idx   &lt;- sample(1:nrow(x), round((2/3)*nrow(x)), replace = FALSE) 
tr.x  &lt;- x[idx,]
tr.y  &lt;- y[idx]
te.x  &lt;- x[-idx,]        
te.y  &lt;- y[-idx] 

res.1 &lt;- classifier(tr.x, tr.y, te.x, te.y, method="svm")
res.1
cl.rate(obs=res.1$cl, res.1$pred) 

## classification performance for the two-class case.
pos &lt;- "WinF"                              ## select positive level
cl.perf(obs=res.1$cl, pre=res.1$pred, pos=pos)
## ROC and AUC
cl.roc(stat=res.1$posterior[,pos],label=res.1$cl, pos=pos)

</code></pre>

<hr>
<h2 id='cor.util'>
Correlation Analysis Utilities
</h2><span id='topic+cor.cut'></span><span id='topic+cor.hcl'></span><span id='topic+cor.heat'></span><span id='topic+corrgram.circle'></span><span id='topic+corrgram.ellipse'></span><span id='topic+cor.heat.gram'></span><span id='topic+hm.cols'></span>

<h3>Description</h3>

<p>Functions to handle correlation analysis on data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cor.cut(mat,cutoff=0.75,abs.f = FALSE,
        use = "pairwise.complete.obs", method = "pearson",...)
  
cor.hcl(mat, cutoff=0.75, use = "pairwise.complete.obs", 
        method = "pearson",fig.f=TRUE, hang=-1,
        horiz = FALSE, main = "Cluster Dendrogram", 
        ylab = ifelse(!horiz, "1 - correlation",""), 
        xlab = ifelse(horiz, "1 - correlation",""),...)
  
cor.heat(mat, use = "pairwise.complete.obs", method = "pearson",
         dend = c("right", "top", "none"),...)
  
corrgram.circle(co, 
                col.regions = colorRampPalette(c("red", "white", "blue")),
                scales = list(x = list(rot = 90)), ...) 

corrgram.ellipse(co,label=FALSE,
                 col.regions = colorRampPalette(c("red", "white", "blue")),
                 scales = list(x = list(rot = 90)), ...) 

cor.heat.gram(mat.1, mat.2, use = "pairwise.complete.obs", 
              method = "pearson", main="Heatmap of correlation", 
              cex=0.75, ...)
              
hm.cols(low = "green", high = "red", n = 123)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cor.util_+3A_mat">mat</code>, <code id="cor.util_+3A_mat.1">mat.1</code>, <code id="cor.util_+3A_mat.2">mat.2</code></td>
<td>

<p>A data frame or matrix. It should be noticed that <code>mat.1</code> and <code>mat.2</code>
must have the same number of row.
</p>
</td></tr>
<tr><td><code id="cor.util_+3A_cutoff">cutoff</code></td>
<td>
<p>A scalar value of threshold.</p>
</td></tr>
<tr><td><code id="cor.util_+3A_abs.f">abs.f</code></td>
<td>

<p>Logical flag indicating whether the absolute values should be used.
</p>
</td></tr>
<tr><td><code id="cor.util_+3A_fig.f">fig.f</code></td>
<td>

<p>Logical flag indicating whether the dendrogram of correlation matrix should be plotted.
</p>
</td></tr>
<tr><td><code id="cor.util_+3A_hang">hang</code></td>
<td>
<p>The fraction of the plot height by which labels should hang below 
the rest of the plot. A negative value will cause the labels to hang down 
from 0. See <code><a href="stats.html#topic+plot.hclust">plot.hclust</a></code>.</p>
</td></tr>
<tr><td><code id="cor.util_+3A_horiz">horiz</code></td>
<td>
<p>Logical indicating if the dendrogram should be drawn
<em>horizontally</em> or not.</p>
</td></tr>
<tr><td><code id="cor.util_+3A_main">main</code>, <code id="cor.util_+3A_xlab">xlab</code>, <code id="cor.util_+3A_ylab">ylab</code></td>
<td>
<p>Graphical parameters, see <code><a href="graphics.html#topic+plot.default">plot.default</a></code>.</p>
</td></tr>
<tr><td><code id="cor.util_+3A_dend">dend</code></td>
<td>
<p>Character string indicating whether to draw 'right', 'top' or 
'none' dendrograms</p>
</td></tr></table>
<p>.
</p>
<table>
<tr><td><code id="cor.util_+3A_use">use</code></td>
<td>
<p>Argument for <code><a href="stats.html#topic+cor">cor</a></code>. An optional character string giving a method for computing 
covariances in the presence of missing values.  This must be (an 
abbreviation of) one of the strings <code>"everything"</code>, <code>"all.obs"</code>, 
<code>"complete.obs"</code>,  <code>"na.or.complete"</code>, or <code>"pairwise.complete.obs"</code>.</p>
</td></tr>
<tr><td><code id="cor.util_+3A_method">method</code></td>
<td>
<p>Argument for <code><a href="stats.html#topic+cor">cor</a></code>. A character string indicating which correlation coefficient (or 
covariance) is to be computed.  One of  <code>"pearson"</code>, <code>"kendall"</code>, or <code>"spearman"</code>,
can be abbreviated.</p>
</td></tr>
<tr><td><code id="cor.util_+3A_co">co</code></td>
<td>
<p>Correlation matrix</p>
</td></tr>
<tr><td><code id="cor.util_+3A_label">label</code></td>
<td>
 
<p>A logical value indicating whether the correlation coefficient should be 
plotted.
</p>
</td></tr>
<tr><td><code id="cor.util_+3A_...">...</code></td>
<td>

<p>Additional parameters to <span class="pkg">lattice</span>.
</p>
</td></tr>
<tr><td><code id="cor.util_+3A_col.regions">col.regions</code></td>
<td>
<p>Color vector to be used</p>
</td></tr>
<tr><td><code id="cor.util_+3A_scales">scales</code></td>
<td>
<p>A list determining how the x- and y-axes (tick marks and labels) 
are drawn. More details, see <code><a href="lattice.html#topic+xyplot">xyplot</a></code>.
</p>
</td></tr>
<tr><td><code id="cor.util_+3A_cex">cex</code></td>
<td>
<p>A numeric multiplier to control character sizes for axis labels</p>
</td></tr></table>
<p>.
</p>
<table>
<tr><td><code id="cor.util_+3A_low">low</code></td>
<td>
<p> Colour for low value</p>
</td></tr>
<tr><td><code id="cor.util_+3A_high">high</code></td>
<td>
<p> Colour for high value</p>
</td></tr>
<tr><td><code id="cor.util_+3A_n">n</code></td>
<td>
<p>The number of colors (&gt;= 1) to be in the palette</p>
</td></tr> 
</table>


<h3>Details</h3>

<p><code>cor.cut</code> returns the pairs with correlation coefficient larger than <code>cutoff</code>.
</p>
<p><code>cor.hcl</code> computes hierarchical cluster analysis based on correlation 
coefficient. For other graphical parameters, see <code><a href="stats.html#topic+plot.dendrogram">plot.dendrogram</a></code>. 
</p>
<p><code>cor.heat</code> display correlation heatmap using <span class="pkg">lattice</span>.
</p>
<p><code>corrgram.circle</code> and <code>corrgram.ellipse</code> display corrgrams with
circle and ellipse. The functions are modified from codes given in 
Deepayan Sarkar's <code>Lattice: Multivariate Data Visualization with R, 
  13.3.3 Corrgrams as customized level plots, pp:238-241</code>.
</p>
<p><code>cor.heat.gram</code> handles the correlation of two data sets which have the 
same row number. The best application is correlation between MS data 
(metabolites) and meta/clinical data.
</p>
<p><code>hm.cols</code> creates a vector of n contiguous colors for heat map.  
</p>


<h3>Value</h3>

<p><code>cor.cut</code> returns a data frame with three columns, in which the first and second columns
are variable names and their correlation (lager than cutoff) are
given in the third column.
</p>
<p><code>cor.hcl</code> returns a list with components of each cluster group and all correlation
coefficients. 
</p>
<p><code>cor.heat</code> returns an object of class &quot;trellis&quot;.
</p>
<p><code>corrgram.circle</code> returns an object of class &quot;trellis&quot;.
</p>
<p><code>corrgram.ellipse</code> returns an object of class &quot;trellis&quot;.
</p>
<p><code>cor.heat.gram</code> returns a list including the components:
</p>
 
<ul>
<li> <p><code>cor.heat</code>: An object of class  &quot;trellis&quot; for correlation heatmap 
ordered by the hierarchical clustering.
</p>
</li>
<li> <p><code>cor.gram</code>: An object of class  &quot;trellis&quot; for corrgrams with 
circle ordered by the hierarchical clustering.
</p>
</li>
<li> <p><code>cor.short</code>: A matrix of correlation coefficient in short format.
</p>
</li>
<li> <p><code>cor.long</code>: A matrix of correlation coefficient in long format.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>References</h3>

<p>Michael Friendly (2002). 
<em>Corrgrams: Exploratory displays for correlation matrices</em>.
The American Statistician, 56, 316&ndash;324.
</p>
<p>D.J. Murdoch, E.D. Chow (1996).
<em>A graphical display of large correlation matrices</em>.
The American Statistician, 50, 178&ndash;180.
</p>
<p>Deepayan Sarkar (2008).  
<em>Lattice: Multivariate Data Visualization with R</em>. Springer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
cor.cut(iris[,1:4],cutoff=0.8, use="pairwise.complete.obs")
cor.hcl(iris[,1:4],cutoff=0.75,fig.f = TRUE)
ph &lt;- cor.heat(iris[,1:4], dend="top")
ph
update(ph, scales = list(x = list(rot = 45)))

## change heatmap color scheme
cor.heat(iris[,1:4], dend="right", xlab="", ylab="",
  col.regions = colorRampPalette(c("green", "black", "red")))

## or use hm.cols
cor.heat(iris[,1:4], dend="right", xlab="", ylab="", col.regions = hm.cols())

## prepare data set
data(abr1)
cls &lt;- factor(abr1$fact$class)
dat &lt;- preproc(abr1$pos[,110:1930], method="log10")  

## feature selection
res &lt;- fs.rf(dat,cls)
## take top 20 features
fs  &lt;- res$fs.order[1:20]
## construct the data set for correlation analysis
mat &lt;- dat[,fs]

cor.cut(mat,cutoff=0.9)
ch &lt;- cor.hcl(mat,cutoff=0.75,fig.f = TRUE, xlab="Peaks")
## plot dendrogram horizontally with coloured labels.
ch &lt;- cor.hcl(mat,cutoff=0.75,fig.f = TRUE, horiz=TRUE,center=TRUE, 
              nodePar = list(lab.cex = 0.6, lab.col = "forest green", pch = NA),
              xlim=c(2,0))

names(ch)
cor.heat(mat,dend="right")
cor.heat(mat,dend="right",col.regions = colorRampPalette(c("yellow", "red")))

## use corrgram with order by the hierarchical clustering
co  &lt;- cor(mat, use="pairwise.complete.obs")
ord &lt;- order.dendrogram(as.dendrogram(hclust(as.dist(1-co)))) 
corrgram.circle(co[ord,ord], main="Corrgrams with circle")
corrgram.ellipse(co[ord,ord], label = TRUE, main = "Corrgrams with circle",
                 col.regions = hm.cols())

## if without ordering
corrgram.circle(co, main="Corrgrams with circle")

## example of cor.heat.gram 
fs.1  &lt;- res$fs.order[21:50]
mat.1 &lt;- dat[,fs.1]

res.cor &lt;- 
  cor.heat.gram(mat, mat.1, main="Heatmap of correlation between mat.1 and mat.2")
names(res.cor)
res.cor$cor.heat
res.cor$cor.gram

</code></pre>

<hr>
<h2 id='dat.sel'>
Generate Pairwise Data Set
</h2><span id='topic+dat.sel'></span><span id='topic+combn.pw'></span>

<h3>Description</h3>

<p>Generate index or data set of pairwise combination based on class labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  combn.pw(cls, choices = NULL) 
  
  dat.sel(dat, cls, choices = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dat.sel_+3A_dat">dat</code></td>
<td>

<p>A data frame or matrix of data set. 
</p>
</td></tr>
<tr><td><code id="dat.sel_+3A_cls">cls</code></td>
<td>

<p>A factor or vector of class labels or categorical data.
</p>
</td></tr>
<tr><td><code id="dat.sel_+3A_choices">choices</code></td>
<td>

<p>The vector or list of class labels to be chosen for binary combination.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>choices</code> is <code>NULL</code>, all binary combinations will be computed.
If <code>choices</code> has one class label, the comparisons between this one and 
any other classes will be calculated. If <code>choices</code> has more than two 
classes, all binary combinations in <code>choices</code> will be generated. 
For details, see <code>examples</code> below.
</p>


<h3>Value</h3>

<p><code>combn.pw</code> returns a data frame of index (logical values).
</p>
<p><code>dat.set</code> returns a list of list with components:
</p>
<table>
<tr><td><code>dat</code></td>
<td>

<p>Pairwise data set.
</p>
</td></tr>
<tr><td><code>cls</code></td>
<td>

<p>Pairwise class label.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p>Applications of <code>dat.sel</code> in <code><a href="#topic+pca_plot_wrap">pca_plot_wrap</a></code>, 
<code><a href="#topic+lda_plot_wrap">lda_plot_wrap</a></code> and <code><a href="#topic+pls_plot_wrap">pls_plot_wrap</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
x &lt;- subset(iris, select = -Species)
y &lt;- iris$Species
 
## generate data set with class "setosa" and "virginica"
binmat.1 &lt;- dat.sel(x,y,choices=c("setosa","virginica"))
names(binmat.1)

## generate data sets for "setosa" vs other classes. These are: 
## "setosa" and "versicolor", "setosa" and "virginica".
binmat.2 &lt;- dat.sel(x,y,choices=c("setosa"))
names(binmat.2)

## generate data set with combination of each class. These are:  
## "setosa" and "versicolor", "setosa" and "virginica",  
## "versicolor" and "virginica" 
binmat.3 &lt;- dat.sel(x,y,choices= NULL)
names(binmat.3)

data(abr1)
cls &lt;- factor(abr1$fact$class)
dat &lt;- preproc(abr1$pos, method="log")

## There are some examples of 'choices'
choices &lt;- c("2")
choices &lt;- c("2","3","4")
choices &lt;- list(c("2","3"),c("4","5"))
choices &lt;- NULL
idx &lt;- combn.pw(cls,choices=choices)  

dat.pw &lt;- dat.sel(dat, cls,choices=choices)

</code></pre>

<hr>
<h2 id='data.visualisation'>
Grouped Data Visualisation by PCA, MDS, PCADA and PLSDA
</h2><span id='topic+pca_plot_wrap'></span><span id='topic+mds_plot_wrap'></span><span id='topic+lda_plot_wrap'></span><span id='topic+lda_plot_wrap.1'></span><span id='topic+pls_plot_wrap'></span>

<h3>Description</h3>

<p>Grouped data visualisation by PCA, MDS, PCADA and PLSDA.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  pca_plot_wrap(data.list,title="plotting",...) 

  mds_plot_wrap(data.list,method="euclidean",title="plotting",...) 

  lda_plot_wrap(data.list,title="plotting",...) 

  lda_plot_wrap.1(data.list,title="plotting",...) 

  pls_plot_wrap(data.list,title="plotting",...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="data.visualisation_+3A_data.list">data.list</code></td>
<td>
<p> A two-layer list structure, in which the second
layer include a data frame called <code>dat</code> and a factor of class
label called <code>cls</code>. Noticed that names of the first layer of
<code>data.list</code> should be given. <code>data.list</code> can be produced
by <code><a href="#topic+dat.sel">dat.sel</a></code>. </p>
</td></tr>
<tr><td><code id="data.visualisation_+3A_method">method</code></td>
<td>
<p> The distance measure to be used. This must be one of
&quot;euclidean&quot;, &quot;maximum&quot;, &quot;manhattan&quot;, &quot;canberra&quot;, &quot;binary&quot; or
&quot;minkowski&quot;. Any unambiguous substring can be given. It is only for
<code>mds_plot_wrap</code>. </p>
</td></tr>
<tr><td><code id="data.visualisation_+3A_title">title</code></td>
<td>
<p>A part of title string for plotting.</p>
</td></tr>
<tr><td><code id="data.visualisation_+3A_...">...</code></td>
<td>
<p>Further arguments to <code>lattice</code>. See corresponding
entry in <code><a href="lattice.html#topic+xyplot">xyplot</a></code> for non-trivial details of
<code>lattice</code>. One argument is <code>ep</code>: an integer flag for
ellipse. <code>1</code> and <code>2</code> for plotting overall and group
ellipse, respectively. Otherwise, none. For details, see
<code><a href="#topic+panel.elli.1">panel.elli.1</a></code>. </p>
</td></tr> </table>


<h3>Value</h3>

<p><code>mds_plot_wrap</code> returns a handle for MDS plot.
</p>
<p>All other four functions return a list with components: the first one
is an object of class <code>"trellis"</code> for data visualisation; the
second one is also an object of class <code>"trellis"</code> but plotting
the corresponding variables, PCs (principal components), LDs (linear
discrimniants) and LCs (latent components); and the third one is a
matrix of these variables.
</p>


<h3>Note</h3>

<p>There is a slight differences between <code>lda_plot_wrap.1</code> and
<code>lda_plot_wrap</code>. The former plots the two-class grouped data,
which has one linear discriminant (LD1), with strip plot. The later
plots the two-class data by LD1 vs LD2 which is identical to LD1.
Hence <code>lda_plot_wrap</code> is more general and can be applied to
fusion of two and more class data sets.
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pcaplot">pcaplot</a></code>, <code><a href="#topic+mdsplot">mdsplot</a></code>, <code><a href="#topic+plot.pcalda">plot.pcalda</a></code>, 
<code><a href="#topic+plot.plsc">plot.plsc</a></code>, <code><a href="#topic+dat.sel">dat.sel</a></code>, <code><a href="#topic+grpplot">grpplot</a></code>,
<code><a href="#topic+panel.elli.1">panel.elli.1</a></code>. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
x &lt;- subset(iris, select = -Species)
y &lt;- iris$Species
## generate data list by dat.sel
iris.pw &lt;- dat.sel(x,y,choices=NULL)
names(iris.pw)

pca.p &lt;- pca_plot_wrap(iris.pw, ep=2)
pca.p[[1]]     ## visualised by PCA
pca.p[[2]]     ## plot PCA variables
pca.p[[3]]     ## matrix of PCA variables

mds.p &lt;- mds_plot_wrap(iris.pw)
mds.p

pls.p  &lt;- pls_plot_wrap(iris.pw)
pls.p[[1]]
pls.p[[2]]
pls.p[[3]]

lda.p &lt;- lda_plot_wrap.1(iris.pw)
lda.p[[1]]
lda.p[[2]]
lda.p[[3]]
lda_plot_wrap(iris.pw)$lda.p

## only plot iris data
ph &lt;- pca_plot_wrap(list(list(dat=x, cls=y)))$pca.p  
## Not given data names
ph
update(ph, strip=FALSE)       ## strip is an argument of lattice

tmp &lt;- list(iris.dat=list(dat=x, cls=y))
pca_plot_wrap(tmp)$pca.p
pca_plot_wrap(tmp,strip=FALSE)$pca.p
pls_plot_wrap(tmp,strip=FALSE)$pls.p
lda_plot_wrap(tmp,strip=FALSE)$lda.p

data(abr1)
cls &lt;- factor(abr1$fact$class)
dat &lt;- preproc(abr1$pos, method="log")
## pair-wise data set
dat.pw &lt;- dat.sel(dat, cls,choices=c("2","3","4"))

## add mult-class
idx &lt;- grep("2|3|4",cls)
cls.234 &lt;- factor(cls[idx])
dat.234 &lt;- dat[idx,,drop = FALSE]

## combine all
dat.tmp &lt;- c(dat.pw, 
             "2~3~4"=list(list(dat=dat.234,cls=cls.234)),
             all=list(list(dat=dat, cls=cls)))

## PCA
ph &lt;- pca_plot_wrap(dat.tmp, title="abr1", par.strip.text = list(cex=0.75), 
                     scales=list(cex =.75,relation="free"), ep=2) 
## See function grpplot for usage of ep.
ph[[1]]
ph[[2]]                     

##PLSDA
ph &lt;- pls_plot_wrap(dat.tmp, title="abr1", par.strip.text = list(cex=0.75), 
                     scales=list(cex =.75,relation="free"), ep=2) 
ph[[1]]
ph[[2]]                     

## PCADA
ph &lt;- lda_plot_wrap(dat.tmp, title="abr1", par.strip.text = list(cex=0.75), 
                     scales=list(cex =.75,relation="free")) 
ph[[1]]
ph[[2]]    
</code></pre>

<hr>
<h2 id='df.util'>
Summary Utilities
</h2><span id='topic+df.summ'></span><span id='topic+vec.summ'></span><span id='topic+vec.summ.1'></span>

<h3>Description</h3>

<p>Functions to summarise data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>df.summ(dat, method=vec.summ,...)
  
vec.summ(x)

vec.summ.1(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="df.util_+3A_dat">dat</code></td>
<td>

<p>A data frame or matrix of data set. 
</p>
</td></tr>
<tr><td><code id="df.util_+3A_x">x</code></td>
<td>

<p>A vector value.
</p>
</td></tr> 
<tr><td><code id="df.util_+3A_method">method</code></td>
<td>
<p>Summary method such as <code>vec.summ</code> and <code>vec.summ.1</code>.
For user-defined methods, see examples below.
</p>
</td></tr>
<tr><td><code id="df.util_+3A_...">...</code></td>
<td>

<p>Additional parameters to <code>method</code> function. 
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>df.summ</code> returns a summarised data frame. 
</p>
<p><code>vec.summ</code> returns an vector of number of variables (exclusing NAs),
minimum, mean, median, maximum and standard derivation.
</p>
<p><code>vec.summ.1</code> returns an vector of number of variables (exclusing NAs),
mean, median, 95% confidence interval of median, IQR and standard derivation.
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(abr1)
dat &lt;- (abr1$pos)[,110:150]
cls &lt;- factor(abr1$fact$class)

## sort out missing value
dat &lt;- mv.zene(dat)

## summary of an individual column
vec.summ(dat[,2])
vec.summ.1(dat[,2])

## summary of data frame
summ   &lt;- df.summ(dat)                       ## default: vec.summ
summ.1 &lt;- df.summ(dat, method=vec.summ.1)

## summary by groups
by(dat, list(cls=cls), df.summ)

## User-defined summary function: 
vec.segment &lt;- function(x, bar=c("SD", "SE", "CI"))
{  
  bar &lt;- match.arg(bar)

  centre &lt;- mean(x, na.rm = TRUE)

  if (bar == "SD") {
    stderr &lt;- sd(x, na.rm = TRUE)        ## Standard derivation (SD)
    lower  &lt;- centre - stderr
    upper  &lt;- centre + stderr
  } else if (bar == "SE") {      ## Standard error(SE) of mean
    stderr &lt;- sd(x, na.rm = TRUE)/sqrt(sum(!is.na(x)))
    ## stderr &lt;- sqrt(var(x, na.rm = TRUE)/length(x[complete.cases(x)]))
    lower  &lt;- centre - stderr
    upper  &lt;- centre + stderr
  } else if (bar == "CI") {      ## Confidence interval (CI), here 95%.
    conf   &lt;- t.test(x)$conf.int
    lower  &lt;- conf[1]
    upper  &lt;- conf[2]
  } else {
    stop("'method' invalid")
  }

  res &lt;- c(lower=lower, centre=centre, upper=upper)
  return(res)
}

## test it
vec.segment(dat[,2])
summ.2 &lt;- df.summ(dat, method=vec.segment, bar="SE")

## ----------------------------------------------------------
#' iris data
df.summ(iris)

#' Group summary
## library(plyr)
## ddply(iris, .(Species), df.summ)
## (tmp &lt;- dlply(iris, .(Species), df.summ, method=vec.segment))
##do.call("rbind", tmp)

#' or you can use summarise to get the group summary for single variable:
## ddply(iris, .(Species), summarise, 
##      mean=mean(Sepal.Length), std=sd(Sepal.Length))

</code></pre>

<hr>
<h2 id='feat.agg'> Rank aggregation by Borda count algorithm </h2><span id='topic+feat.agg'></span>

<h3>Description</h3>

<p> Use Borda count to get the final feature order. </p>


<h3>Usage</h3>

<pre><code class='language-R'> feat.agg(fs.rank.list) </code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="feat.agg_+3A_fs.rank.list">fs.rank.list</code></td>
<td>
<p> A data frame of feature orders by different
feature selectors. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>fs.order</code></td>
<td>
<p>Final feature order. </p>
</td></tr>
<tr><td><code>fs.rank</code></td>
<td>
<p>Aggregated rank list by Borda count. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Wanchang Lin  </p>


<h3>See Also</h3>

 <p><code><a href="#topic+feat.rank.re">feat.rank.re</a></code>, <code><a href="#topic+feat.mfs">feat.mfs</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(abr1)
dat &lt;- preproc(abr1$pos[,200:400], method="log10")  
cls &lt;- factor(abr1$fact$class)

## feature selection without resampling
fs &lt;- feat.mfs(dat, cls, method=c("fs.anova","fs.rf","fs.rfe"), 
               is.resam=FALSE)
## rank aggregation 
fs.1 &lt;- feat.agg(fs$fs.rank)
names(fs.1)
</code></pre>

<hr>
<h2 id='feat.freq'>Frequency and Stability of Feature Selection </h2><span id='topic+feat.freq'></span>

<h3>Description</h3>

<p>Frequency and stability of feature selection. </p>


<h3>Usage</h3>

<pre><code class='language-R'>  feat.freq(x,rank.cutoff=50,freq.cutoff=0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="feat.freq_+3A_x">x</code></td>
<td>
<p> A matrix or data frame of feature orders. </p>
</td></tr>
<tr><td><code id="feat.freq_+3A_rank.cutoff">rank.cutoff</code></td>
<td>
<p>A numeric value for cutoff of top features.</p>
</td></tr>
<tr><td><code id="feat.freq_+3A_freq.cutoff">freq.cutoff</code></td>
<td>
<p>A numeric value for cutoff of feature frequency.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>freq.all</code></td>
<td>
<p>Feature frequencies. </p>
</td></tr>
<tr><td><code>freq</code></td>
<td>
<p>Feature frequencies larger than <code>freq.cutoff</code>. </p>
</td></tr>
<tr><td><code>stability</code></td>
<td>
<p>Stability rate of feature ranking.</p>
</td></tr>
<tr><td><code>rank.cutoff</code></td>
<td>
<p> Top feature order cut-off used. </p>
</td></tr>
<tr><td><code>freq.cutoff</code></td>
<td>
<p> Feature frequency cut-off used. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Wanchang Lin  </p>


<h3>References</h3>

<p>Davis, C. A., et al., (2006) Reliable gene signatures for microarray
classification: assessment of stability and performance.
<em>Bioinformatics</em>, vol.22, no.19, 2356 - 2363.
</p>
<p>Michiels, S., et al., (2005) Prediction of cancer outcome with
microarrays: a multiple random validation strategy. <em>Lancet</em>,
vol.365, 488 - 492.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+feat.rank.re">feat.rank.re</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## prepare data set
data(abr1)
cls &lt;- factor(abr1$fact$class)
dat &lt;- abr1$pos
## dat &lt;- abr1$pos[,110:1930]

## fill zeros with NAs
dat &lt;- mv.zene(dat)

## missing values summary
mv &lt;- mv.stats(dat, grp=cls) 
mv    ## View the missing value pattern

## filter missing value variables
## dim(dat)
dat &lt;- dat[,mv$mv.var &lt; 0.15]
## dim(dat)

## fill NAs with mean
dat &lt;- mv.fill(dat,method="mean")

## log transformation
dat &lt;- preproc(dat, method="log10")

## select class "1" and "2" for feature ranking
ind &lt;- grepl("1|2", cls)
mat &lt;- dat[ind,,drop=FALSE] 
mat &lt;- as.matrix(mat)
grp &lt;- cls[ind, drop=TRUE]   

## use resampling method of bootstrap 
pars   &lt;- valipars(sampling="boot",niter=10, nreps=5)
z      &lt;- feat.rank.re(mat,grp,method="fs.plsvip",pars = pars)

## compute the frequency and stability of feature selection 
freq &lt;- feat.freq(z$order.list,rank.cutoff=50,freq.cutoff=0.5)

</code></pre>

<hr>
<h2 id='feat.mfs'>
Multiple Feature Selection
</h2><span id='topic+feat.mfs'></span><span id='topic+feat.mfs.stab'></span><span id='topic+feat.mfs.stats'></span>

<h3>Description</h3>

<p> Multiple feature selection with or without resampling
procedures. </p>


<h3>Usage</h3>

<pre><code class='language-R'>feat.mfs(x,y,method,pars = valipars(),is.resam = TRUE, ...)
         
feat.mfs.stab(fs.res,rank.cutoff = 20,freq.cutoff = 0.5)

feat.mfs.stats(fs.stats,cumu.plot=FALSE, main="Stats Plot", 
               ylab="Values", xlab="Index of variable", ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="feat.mfs_+3A_x">x</code></td>
<td>
<p> A matrix or data frame containing the explanatory variables.</p>
</td></tr>
<tr><td><code id="feat.mfs_+3A_y">y</code></td>
<td>
<p> A factor specifying the class for each observation. </p>
</td></tr>
<tr><td><code id="feat.mfs_+3A_method">method</code></td>
<td>
<p> Multiple feature selection/ranking method to be used. </p>
</td></tr>
<tr><td><code id="feat.mfs_+3A_pars">pars</code></td>
<td>
<p> A list of resampling scheme. See <code><a href="#topic+valipars">valipars</a></code>
for details. </p>
</td></tr>
<tr><td><code id="feat.mfs_+3A_is.resam">is.resam</code></td>
<td>
<p> A logical value indicating whether the resampling
should be applied. </p>
</td></tr>
<tr><td><code id="feat.mfs_+3A_fs.res">fs.res</code></td>
<td>
<p> A list obtained by running <code>feat.mfs</code> .</p>
</td></tr> 
<tr><td><code id="feat.mfs_+3A_rank.cutoff">rank.cutoff</code></td>
<td>
<p>Cutoff of top features for frequency calculating.</p>
</td></tr>
<tr><td><code id="feat.mfs_+3A_freq.cutoff">freq.cutoff</code></td>
<td>
<p> Cutoff of feature frequency. </p>
</td></tr>
<tr><td><code id="feat.mfs_+3A_fs.stats">fs.stats</code></td>
<td>
<p>A matrix of feature statistics or values outputted by
<code>feat.mfs</code> </p>
</td></tr>
<tr><td><code id="feat.mfs_+3A_cumu.plot">cumu.plot</code></td>
<td>
<p> A logical value indicating the cumulative scores
should be plotted. </p>
</td></tr>
<tr><td><code id="feat.mfs_+3A_main">main</code>, <code id="feat.mfs_+3A_xlab">xlab</code>, <code id="feat.mfs_+3A_ylab">ylab</code></td>
<td>
<p>Plot parameters</p>
</td></tr>
<tr><td><code id="feat.mfs_+3A_...">...</code></td>
<td>
<p>Additional parameters.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>feat.mfs.stab</code> summarises multiple feature selection only when
resampling strategy is employed (i.e. <code>is.resam</code> is <code>TRUE</code>
when calling <code>feat.mfs</code>). It obtains these results based on
<code>feat.mfs</code>'s returned value called <code>all</code>.
</p>
<p><code>feat.mfs.stats</code> handles the statistical values or scores. Its
purpose is to provide a guidance in selecting the best number of
features by spotting the elbow point. This method should work in
conjunction with plotting of p-values and their corresponding adjusted
values such as FDR and Bonferroni in the multiple hypothesis test.
</p>


<h3>Value</h3>

<p><code>feat.mfs</code> returns a list with components:
</p>
<table>
<tr><td><code>fs.order</code></td>
<td>
<p>A data frame of feature order from best to worst.</p>
</td></tr>
<tr><td><code>fs.rank</code></td>
<td>
<p>A matrix of feature ranking scores.</p>
</td></tr>
<tr><td><code>fs.stats</code></td>
<td>
<p>A matrix of feature statistics or values.</p>
</td></tr>
<tr><td><code>all</code></td>
<td>
<p>A list of output of <code><a href="#topic+feat.rank.re">feat.rank.re</a></code> for each feature 
selection method.</p>
</td></tr>
</table>
<p><code>feat.mfs.stab</code> returns a list with components:
</p>
<table>
<tr><td><code>fs.freq</code></td>
<td>
<p>Feature frequencies larger than <code>freq.cutoff</code>. </p>
</td></tr>
<tr><td><code>fs.subs</code></td>
<td>
<p>Feature with frequencies larger than <code>freq.cutoff</code>. </p>
</td></tr>
<tr><td><code>fs.stab</code></td>
<td>
<p>Stability rate of feature ranking.</p>
</td></tr>
<tr><td><code>fs.cons</code></td>
<td>
<p>A matrix of feature consensus table based on feature
frequency.</p>
</td></tr>
</table>
<p><code>feat.mfs.stats</code> returns a list with components:
</p>
<table>
<tr><td><code>stats.tab</code></td>
<td>
<p>A statistical values with their corresponding names.</p>
</td></tr>
<tr><td><code>stats.long</code></td>
<td>
<p>Long-format of statistical values for plotting.</p>
</td></tr> 
<tr><td><code>stats.p</code></td>
<td>
<p> An object of class &quot;trellis&quot;.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The feature order can be computed directly from the overall statistics
<code>fs.stats</code>. It is, however, slightly different from
<code>fs.order</code> obtained by rank aggregation when resampling is
employed.
</p>
<p>The <code>fs.cons</code> and <code>fs.freq</code> are computed based on
<code>fs.order</code>.
</p>


<h3>Author(s)</h3>

<p> Wanchang Lin  </p>


<h3>See Also</h3>

 <p><code><a href="#topic+feat.rank.re">feat.rank.re</a></code>, <code><a href="#topic+feat.freq">feat.freq</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(lattice)	
data(abr1)
dat &lt;- preproc(abr1$pos[,200:400], method="log10")  
cls &lt;- factor(abr1$fact$class)

tmp &lt;- dat.sel(dat, cls, choices=c("1","2"))
x   &lt;- tmp[[1]]$dat
y   &lt;- tmp[[1]]$cls

fs.method &lt;- c("fs.anova","fs.rf","fs.rfe")
fs.pars   &lt;- valipars(sampling="cv",niter=10,nreps=5)
fs &lt;- feat.mfs(x, y, fs.method, fs.pars)   ## with resampling
names(fs)

## frequency, consensus and stabilities of feature selection 
fs.stab &lt;- feat.mfs.stab(fs)
print(fs.stab$fs.cons,digits=2,na.print="")

## plot feature selection frequency
freq &lt;- fs.stab$fs.freq
dotplot(freq$fs.anova, type="o", main="Feature Selection Frequencies")
barchart(freq$fs.anova)

## rank aggregation 
fs.agg &lt;- feat.agg(fs$fs.rank)

## stats table and plotting
fs.stats &lt;- fs$fs.stats
tmp &lt;- feat.mfs.stats(fs.stats, cumu.plot = TRUE)
tmp$stats.p
fs.tab &lt;- tmp$stats.tab
## convert to matrix
fs.tab &lt;- list2df(un.list(fs.tab))

## without resampling
fs.1 &lt;- feat.mfs(x, y, method=fs.method, is.resam = FALSE)

## End(Not run)
</code></pre>

<hr>
<h2 id='feat.rank.re'>
Feature Ranking with Resampling Method
</h2><span id='topic+feat.rank.re'></span>

<h3>Description</h3>

<p>Feature selection with resampling method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  feat.rank.re(x,y,method=,pars = valipars(),tr.idx=NULL,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="feat.rank.re_+3A_x">x</code></td>
<td>

<p>A matrix or data frame containing the explanatory variables.
</p>
</td></tr>
<tr><td><code id="feat.rank.re_+3A_y">y</code></td>
<td>

<p>A factor specifying the class for each observation.
</p>
</td></tr>
<tr><td><code id="feat.rank.re_+3A_method">method</code></td>
<td>

<p>Feature selection method to be used. For each method used in this 
function, the output must be a list including two components, <code>fs.rank</code>
(rank scores of features) and <code>fs.order</code>(feature orders in descending order).
</p>
</td></tr>             
<tr><td><code id="feat.rank.re_+3A_pars">pars</code></td>
<td>

<p>A list of resampling scheme method such as <em>Leave-one-out cross-validation</em>, 
<em>Cross-validation</em>, <em>Bootstrap</em> and <em>Randomised validation (holdout)</em>.
See <code><a href="#topic+valipars">valipars</a></code> for details.
</p>
</td></tr>
<tr><td><code id="feat.rank.re_+3A_tr.idx">tr.idx</code></td>
<td>

<p>User defined index of training samples. Can be generated by <code>trainind</code>.
</p>
</td></tr>
<tr><td><code id="feat.rank.re_+3A_...">...</code></td>
<td>
<p>Additional parameters to <code>method</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>method</code></td>
<td>
<p>Feature selection method used.</p>
</td></tr>
<tr><td><code>fs.rank</code></td>
<td>
<p>A vector of final feature ranking list.</p>
</td></tr>
<tr><td><code>fs.order</code></td>
<td>
<p>A vector of final feature order from best to worst.</p>
</td></tr>
<tr><td><code>rank.list</code></td>
<td>
<p>Feature rank lists of all computation.</p>
</td></tr>
<tr><td><code>order.list</code></td>
<td>
<p>Feature order lists of all computation.</p>
</td></tr>
<tr><td><code>pars</code></td>
<td>
<p>Resampling parameters.</p>
</td></tr>
<tr><td><code>tr.idx</code></td>
<td>
<p>Index of training samples.</p>
</td></tr>
<tr><td><code>all</code></td>
<td>
<p>All results come from re-sampling.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+valipars">valipars</a></code>, <code><a href="#topic+feat.freq">feat.freq</a></code>, <code><a href="#topic+frankvali">frankvali</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## prepare data set
data(abr1)
cls &lt;- factor(abr1$fact$class)
dat &lt;- abr1$pos
## dat &lt;- abr1$pos[,110:1930]

## fill zeros with NAs
dat &lt;- mv.zene(dat)

## missing values summary
mv &lt;- mv.stats(dat, grp=cls) 
## mv    ## View the missing value pattern

## filter missing value variables
## dim(dat)
dat &lt;- dat[,mv$mv.var &lt; 0.15]
## dim(dat)

## fill NAs with mean
dat &lt;- mv.fill(dat,method="mean")

## log transformation
dat &lt;- preproc(dat, method="log10")

## select class "1" and "2" for feature ranking
ind &lt;- which(cls==1 | cls==2)
x   &lt;- dat[ind,,drop=FALSE] 
y   &lt;- cls[ind, drop=TRUE]   

## feature selection
pars   &lt;- valipars(sampling="boot",niter=2,nreps=5)
tr.idx &lt;- trainind(y,pars=pars)

z      &lt;- feat.rank.re(x,y,method="fs.auc",pars = pars)
names(z)
               
</code></pre>

<hr>
<h2 id='frank.err'>
Feature Ranking and Validation on Feature Subset
</h2><span id='topic+frank.err'></span>

<h3>Description</h3>

<p>Get feature ranking on the training data and validate selected feature subsets by
estimating their classification error rate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>frank.err(dat.tr, cl.tr, dat.te, cl.te, cl.method="svm",
          fs.method="fs.auc", fs.order=NULL, fs.len="power2", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="frank.err_+3A_dat.tr">dat.tr</code></td>
<td>

<p>A data frame or matrix of training data. Feature ranking and classification model 
are carried on this data set.
</p>
</td></tr>
<tr><td><code id="frank.err_+3A_cl.tr">cl.tr</code></td>
<td>

<p>A factor or vector of training class.
</p>
</td></tr>
<tr><td><code id="frank.err_+3A_dat.te">dat.te</code></td>
<td>

<p>A data frame or matrix of test data. Error rates are calculated on this data set.
</p>
</td></tr>
<tr><td><code id="frank.err_+3A_cl.te">cl.te</code></td>
<td>

<p>A factor or vector of test class.
</p>
</td></tr>
<tr><td><code id="frank.err_+3A_cl.method">cl.method</code></td>
<td>

<p>Classification method to be used. Any classification methods can be employed 
if they have method <code>predict</code> (except <code>knn</code>) with output of predicted class
label or one component with name of <code>class</code> in the returned list, such as 
<code>randomForest</code>, <code>svm</code>, <code>knn</code> and <code>lda</code>. 
</p>
</td></tr>
<tr><td><code id="frank.err_+3A_fs.method">fs.method</code></td>
<td>

<p>Feature ranking method. If <code>fs.order</code> is not <code>NULL</code>, it is ignored.
</p>
</td></tr>             
<tr><td><code id="frank.err_+3A_fs.order">fs.order</code></td>
<td>

<p>A vector of feature order. Default is <code>NULL</code> and then the feature selection will be 
performed on the training data. 
</p>
</td></tr>
<tr><td><code id="frank.err_+3A_fs.len">fs.len</code></td>
<td>

<p>The lengths of feature subsets used for validation. For details, see <code><a href="#topic+get.fs.len">get.fs.len</a></code>. 
</p>
</td></tr>
<tr><td><code id="frank.err_+3A_...">...</code></td>
<td>
<p>Additional parameters to <code>fs.method</code> or <code>cl.method</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>cl.method</code></td>
<td>
<p>Classification method used.</p>
</td></tr>
<tr><td><code>fs.len</code></td>
<td>
<p>The lengths of feature subsets used for validation.</p>
</td></tr>
<tr><td><code>error</code></td>
<td>
<p>Error rate for each feature length.</p>
</td></tr>
<tr><td><code>fs.method</code></td>
<td>
<p>Feature ranking method used.</p>
</td></tr>
<tr><td><code>fs.order</code></td>
<td>
<p>Feature order vector.</p>
</td></tr>
<tr><td><code>fs.rank</code></td>
<td>
<p>Feature ranking score vector.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+frankvali">frankvali</a></code>, <code><a href="#topic+get.fs.len">get.fs.len</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(abr1)
dat &lt;- abr1$pos
x   &lt;- preproc(dat[,110:500], method="log10")  
y   &lt;- factor(abr1$fact$class)        

dat &lt;- dat.sel(x, y, choices=c("1","6"))
x.1 &lt;- dat[[1]]$dat
y.1 &lt;- dat[[1]]$cls

idx &lt;- sample(1:nrow(x.1), round((2/3)*nrow(x.1)), replace=FALSE) 
## construct train and test data 
train.dat  &lt;- x.1[idx,]
train.cl   &lt;- y.1[idx]
test.dat   &lt;- x.1[-idx,]   
test.cl    &lt;- y.1[-idx] 

## validate feature selection on some feature subsets
res &lt;- frank.err(train.dat, train.cl, test.dat, test.cl, 
                 cl.method="knn", fs.method="fs.auc",  
                 fs.len="power2")
names(res)
## full feature order list
res$fs.order

## validation on subsets of feature order 
res$error

## or first apply feature selection
fs &lt;- fs.auc(train.dat,train.cl)
## then apply error estimation for each selected feature subset
res.1 &lt;- frank.err(train.dat, train.cl, test.dat, test.cl, 
                   cl.method="knn", fs.order=fs$fs.order,  
                   fs.len="power2")

res.1$error

</code></pre>

<hr>
<h2 id='frankvali'>
Estimates Feature Ranking Error Rate with Resampling
</h2><span id='topic+frankvali'></span><span id='topic+fs.cl'></span><span id='topic+fs.cl.1'></span><span id='topic+frankvali.formula'></span><span id='topic+frankvali.default'></span><span id='topic+print.frankvali'></span><span id='topic+summary.frankvali'></span><span id='topic+print.summary.frankvali'></span>

<h3>Description</h3>

<p>Estimates error rate of feature ranking with resampling methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>frankvali(dat, ...)
## Default S3 method:
frankvali(dat,cl,cl.method = "svm", fs.method="fs.auc",
          fs.order=NULL, fs.len="power2", pars = valipars(),
          tr.idx=NULL,...)

## S3 method for class 'formula'
frankvali(formula, data = NULL, ..., subset, na.action = na.omit)

fs.cl(dat,cl,fs.order=colnames(dat), fs.len=1:ncol(dat), 
      cl.method = "svm", pars = valipars(), all.fs=FALSE, ...)
        
fs.cl.1(dat,cl,fs.order=colnames(dat), cl.method = "svm", 
        pars = valipars(), agg_f=FALSE,...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="frankvali_+3A_formula">formula</code></td>
<td>

<p>A formula of the form <code>groups ~ x1 + x2 + ...</code>  That is, the
response is the grouping factor and the right hand side specifies
the (non-factor) discriminators.
</p>
</td></tr>
<tr><td><code id="frankvali_+3A_data">data</code></td>
<td>

<p>Data frame from which variables specified in <code>formula</code> are
preferentially to be taken.
</p>
</td></tr>
<tr><td><code id="frankvali_+3A_dat">dat</code></td>
<td>

<p>A matrix or data frame containing the explanatory variables if no formula is
given as the principal argument.
</p>
</td></tr>
<tr><td><code id="frankvali_+3A_cl">cl</code></td>
<td>

<p>A factor specifying the class for each observation if no formula principal 
argument is given.
</p>
</td></tr>
<tr><td><code id="frankvali_+3A_cl.method">cl.method</code></td>
<td>

<p>Classification method to be used. Any classification methods can be employed 
if they have method <code>predict</code> (except <code>knn</code>) with output of predicted class
label or one component with name of <code>class</code> in the returned list, such as 
<code>randomForest</code>, <code>svm</code>, <code>knn</code> and <code>lda</code>. 
</p>
</td></tr>
<tr><td><code id="frankvali_+3A_fs.method">fs.method</code></td>
<td>

<p>Feature ranking method to be used.  If <code>fs.order</code> is not <code>NULL</code>, it will be
overridden. 
</p>
</td></tr>             
<tr><td><code id="frankvali_+3A_fs.order">fs.order</code></td>
<td>

<p>A vector of ordered feature order. In <code>frankvali</code> its default is <code>NULL</code> and then the 
feature selection will be performed on the training data. 
</p>
</td></tr>
<tr><td><code id="frankvali_+3A_fs.len">fs.len</code></td>
<td>

<p>Feature length used for validation. For details, see <code><a href="#topic+get.fs.len">get.fs.len</a></code>. 
</p>
</td></tr>
<tr><td><code id="frankvali_+3A_pars">pars</code></td>
<td>

<p>A list of resampling scheme method such as <em>Cross-validation</em>,
<em>Stratified cross-validation</em>, <em>Leave-one-out cross-validation</em>, 
<em>Randomised validation (holdout)</em>, <em>Bootstrap</em>, <em>.632 bootstrap</em>
and <em>.632 plus bootstrap</em>, and control parameters for the calculation of accuracy. 
See <code><a href="#topic+valipars">valipars</a></code> for details.
</p>
</td></tr>
<tr><td><code id="frankvali_+3A_tr.idx">tr.idx</code></td>
<td>

<p>User defined index of training samples. Can be generated by <code>trainind</code>.
</p>
</td></tr>
<tr><td><code id="frankvali_+3A_all.fs">all.fs</code></td>
<td>
 
<p>A logical value indicating whether all features should be used for evaluation.
</p>
</td></tr>
<tr><td><code id="frankvali_+3A_agg_f">agg_f</code></td>
<td>
 
<p>A logical value indicating whether aggregated features should be used for 
evaluation.
</p>
</td></tr>
<tr><td><code id="frankvali_+3A_...">...</code></td>
<td>
<p>Additional parameters to <code>fs.method</code> or <code>cl.method</code>.</p>
</td></tr>
<tr><td><code id="frankvali_+3A_subset">subset</code></td>
<td>

<p>Optional vector, specifying a subset of observations to be used.
</p>
</td></tr>
<tr><td><code id="frankvali_+3A_na.action">na.action</code></td>
<td>

<p>Function which indicates what should happen when the data
contains <code>NA</code>'s, defaults to <code><a href="stats.html#topic+na.omit">na.omit</a></code>.
</p>
</td></tr> 
</table>


<h3>Details</h3>

<p>These functions validate the selected feature subsets by classification and resampling methods.  
</p>
<p>It can take any classification model if its argument format 
is <code>model(formula, data, subset, ...)</code> and their corresponding 
method <code>predict.model(object, newdata, ...)</code> can either return the only 
predicted class label or in a list with name as <code>class</code>, such as 
<code>lda</code> and <code>pcalda</code>.  
</p>
<p>The resampling method can be one of <code>cv</code>, <code>scv</code>, <code>loocv</code>, 
<code>boot</code>, <code>632b</code> and <code>632pb</code>.
</p>
<p>The feature ranking method can take one of <code>fs.rf</code>, <code>fs.auc</code>, <code>fs.welch</code>, 
<code>fs.anova</code>, <code>fs.bw</code>, <code>fs.snr</code>, <code>fs.kruskal</code>, <code>fs.relief</code> and <code>fs.rfe</code>. 
</p>


<h3>Value</h3>

<p><code>frankvali</code> returns an object of class including the components:
</p>
<table>
<tr><td><code>fs.method</code></td>
<td>
<p>Feature ranking method used.</p>
</td></tr>
<tr><td><code>cl.method</code></td>
<td>
<p>Classification method used.</p>
</td></tr>
<tr><td><code>fs.len</code></td>
<td>
<p>Feature lengths used.</p>
</td></tr>
<tr><td><code>fs.rank</code></td>
<td>
<p>Final feature ranking. It is obtained based on <code>fs.list</code>
by Borda vote method.</p>
</td></tr>
<tr><td><code>err.all</code></td>
<td>
<p>Error rate for all computation.</p>
</td></tr>
<tr><td><code>err.iter</code></td>
<td>
<p>Error rate for each iteration.</p>
</td></tr>
<tr><td><code>err.avg</code></td>
<td>
<p>Average error rate for all iterations.</p>
</td></tr>
<tr><td><code>sampling</code></td>
<td>
<p>Sampling scheme used.</p>
</td></tr> 
<tr><td><code>niter</code></td>
<td>
<p>Number of iterations.</p>
</td></tr>
<tr><td><code>nboot</code></td>
<td>

<p>Number of bootstrap replications if the sampling method is 
one of <code>boot</code>, <code>632b</code> and <code>632pb</code>.
</p>
</td></tr>
<tr><td><code>nfold</code></td>
<td>
<p>Fold of cross-validations if the sampling is <code>cv</code> or <code>scv</code>.</p>
</td></tr>
<tr><td><code>nrand</code></td>
<td>
<p>Number of replications if the sampling is <code>random</code>.</p>
</td></tr>
<tr><td><code>fs.list</code></td>
<td>
<p>Feature list of all computation if <code>fs.order</code> is <code>NULL</code>.</p>
</td></tr>
</table>
<p><code>fs.cl</code> and <code>fs.cl.1</code> return a matrix with columns of <code>acc</code> (accuracy), 
<code>auc</code>(area under ROC curve) and <code>mar</code>(class margin).
</p>


<h3>Note</h3>

<p><code>fs.cl</code> is the simplified version of <code>frankvali</code>. Both <code>frankvali</code>
and <code>fs.cl</code> are used for validation of aggregated features from top to
bottom only, but <code>fs.cl.1</code> can be used for validation of either individual 
or aggregated features.
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+feat.rank.re">feat.rank.re</a></code>, <code><a href="#topic+frank.err">frank.err</a></code>, <code><a href="#topic+valipars">valipars</a></code>, 
<code><a href="#topic+boxplot.frankvali">boxplot.frankvali</a></code>, <code><a href="#topic+get.fs.len">get.fs.len</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(abr1)
dat &lt;- abr1$pos
x   &lt;- preproc(dat[,110:500], method="log10")  
y   &lt;- factor(abr1$fact$class)        

dat &lt;- dat.sel(x, y, choices=c("1","2"))
x.1 &lt;- dat[[1]]$dat
y.1 &lt;- dat[[1]]$cls

len  &lt;- c(1:20,seq(25,50,5),seq(60,90,10),seq(100,300,50))
pars &lt;- valipars(sampling="boot",niter=2, nreps=4)
res  &lt;- frankvali(x.1,y.1,cl.method = "knn", fs.method="fs.auc",
                  fs.len=len, pars = pars)
res
summary(res)
boxplot(res)  

## Not run: 
## or apply feature selection with re-sampling procedure at first
fs  &lt;- feat.rank.re(x.1,y.1,method="fs.auc",pars = pars)

## then estimate error of feature selection.
res.1  &lt;- frankvali(x.1,y.1,cl.method = "knn", fs.order=fs$fs.order,
                    fs.len=len, pars = pars)
res.1

## use formula
data.bin &lt;- data.frame(y.1,x.1)

pars &lt;- valipars(sampling="cv",niter=2,nreps=4)
res.2  &lt;- frankvali(y.1~., data=data.bin,fs.method="fs.rfe",fs.len=len, 
                    cl.method = "knn",pars = pars)
res.2

## examples of fs.cl and fs.cl.1
fs &lt;- fs.rf(x.1, y.1)
res.3 &lt;- fs.cl(x.1,y.1,fs.order=fs$fs.order, fs.len=len,
               cl.method = "svm", pars = pars, all.fs=TRUE)

ord &lt;- fs$fs.order[1:50]
## aggregated features
res.4 &lt;- fs.cl.1(x.1,y.1,fs.order=ord, cl.method = "svm", pars = pars,
                 agg_f=TRUE)
               
## individual feature
res.5 &lt;- fs.cl.1(x.1,y.1,fs.order=ord, cl.method = "svm", pars = pars,
                 agg_f=FALSE)
                 

## End(Not run)
</code></pre>

<hr>
<h2 id='fs.anova'>
Feature Selection Using ANOVA
</h2><span id='topic+fs.anova'></span>

<h3>Description</h3>

<p>Feature selection using ANOVA.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  fs.anova(x,y,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fs.anova_+3A_x">x</code></td>
<td>

<p>A data frame or matrix of data set. 
</p>
</td></tr>
<tr><td><code id="fs.anova_+3A_y">y</code></td>
<td>

<p>A factor or vector of class.
</p>
</td></tr>
<tr><td><code id="fs.anova_+3A_...">...</code></td>
<td>

<p>Arguments to pass to method.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>fs.rank</code></td>
<td>
<p>A vector of feature ranking scores.</p>
</td></tr>
<tr><td><code>fs.order</code></td>
<td>
<p>A vector of feature order from best to worst.</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>
<p>A vector of statistics.</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>A vector of p values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## prepare data set
data(abr1)
cls &lt;- factor(abr1$fact$class)
dat &lt;- abr1$pos
## dat &lt;- abr1$pos[,110:1930]

## fill zeros with NAs
dat &lt;- mv.zene(dat)

## missing values summary
mv &lt;- mv.stats(dat, grp=cls) 
mv    ## View the missing value pattern

## filter missing value variables
## dim(dat)
dat &lt;- dat[,mv$mv.var &lt; 0.15]
## dim(dat)

## fill NAs with mean
dat &lt;- mv.fill(dat,method="mean")

## log transformation
dat &lt;- preproc(dat, method="log10")

## select class "1" and "2" for feature ranking
ind &lt;- grepl("1|2", cls)
mat &lt;- dat[ind,,drop=FALSE] 
mat &lt;- as.matrix(mat)
grp &lt;- cls[ind, drop=TRUE]   

## apply ANOVA method for feature selection/ranking
res &lt;- fs.anova(mat,grp)
names(res)

</code></pre>

<hr>
<h2 id='fs.auc'>
Feature Selection Using Area under Receiver Operating Curve (AUC)
</h2><span id='topic+fs.auc'></span>

<h3>Description</h3>

<p>Feature selection using area under receiver operating curve (AUC).  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  fs.auc(x,y,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fs.auc_+3A_x">x</code></td>
<td>

<p>A data frame or matrix of data set. 
</p>
</td></tr>
<tr><td><code id="fs.auc_+3A_y">y</code></td>
<td>

<p>A factor or vector of class.
</p>
</td></tr>
<tr><td><code id="fs.auc_+3A_...">...</code></td>
<td>

<p>Arguments to pass(current ignored).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>fs.rank</code></td>
<td>
<p>A vector of feature ranking scores.</p>
</td></tr>
<tr><td><code>fs.order</code></td>
<td>
<p>A vector of feature order from best to worst.</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>
<p>A vector of measurements.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function is for two-class problem only. 
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## prepare data set
data(abr1)
cls &lt;- factor(abr1$fact$class)
dat &lt;- abr1$pos
## dat &lt;- abr1$pos[,110:1930]

## fill zeros with NAs
dat &lt;- mv.zene(dat)

## missing values summary
mv &lt;- mv.stats(dat, grp=cls) 
mv    ## View the missing value pattern

## filter missing value variables
## dim(dat)
dat &lt;- dat[,mv$mv.var &lt; 0.15]
## dim(dat)

## fill NAs with mean
dat &lt;- mv.fill(dat,method="mean")

## log transformation
dat &lt;- preproc(dat, method="log10")

## select class "1" and "2" for feature ranking
ind &lt;- grepl("1|2", cls)
mat &lt;- dat[ind,,drop=FALSE] 
mat &lt;- as.matrix(mat)
grp &lt;- cls[ind, drop=TRUE]   

## apply AUC method for feature selection/ranking
res &lt;- fs.auc(mat,grp)
names(res)

</code></pre>

<hr>
<h2 id='fs.bw'>
Feature Selection Using Between-Group to Within-Group (BW) Ratio
</h2><span id='topic+fs.bw'></span>

<h3>Description</h3>

<p>Feature selection using ratio of between-group to within-group sums of squares 
(BW).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  fs.bw(x,y,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fs.bw_+3A_x">x</code></td>
<td>

<p>A data frame or matrix of data set. 
</p>
</td></tr>
<tr><td><code id="fs.bw_+3A_y">y</code></td>
<td>

<p>A factor or vector of class.
</p>
</td></tr>
<tr><td><code id="fs.bw_+3A_...">...</code></td>
<td>

<p>Arguments to pass(current ignored).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>fs.rank</code></td>
<td>
<p>A vector of feature ranking scores.</p>
</td></tr>
<tr><td><code>fs.order</code></td>
<td>
<p>A vector of feature order from best to worst.</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>
<p>A vector of measurements.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>References</h3>

<p>Dudoit, S., Fridlyand, J. and Speed, T.P. Comparison of discrimination methods 
for classification of tumours using gene expression data. 
<em>Journal of the American Statistical Association</em>. Vol.97, No.457, 77-87.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## prepare data set
data(abr1)
cls &lt;- factor(abr1$fact$class)
dat &lt;- abr1$pos
## dat &lt;- abr1$pos[,110:1930]

## fill zeros with NAs
dat &lt;- mv.zene(dat)

## missing values summary
mv &lt;- mv.stats(dat, grp=cls) 
mv    ## View the missing value pattern

## filter missing value variables
## dim(dat)
dat &lt;- dat[,mv$mv.var &lt; 0.15]
## dim(dat)

## fill NAs with mean
dat &lt;- mv.fill(dat,method="mean")

## log transformation
dat &lt;- preproc(dat, method="log10")

## select class "1" and "2" for feature ranking
ind &lt;- grepl("1|2", cls)
mat &lt;- dat[ind,,drop=FALSE] 
mat &lt;- as.matrix(mat)
grp &lt;- cls[ind, drop=TRUE]   

## apply BW ratio method for feature selection/ranking
res &lt;- fs.bw(mat,grp)
names(res)

</code></pre>

<hr>
<h2 id='fs.kruskal'>
Feature Selection Using Kruskal-Wallis Test
</h2><span id='topic+fs.kruskal'></span>

<h3>Description</h3>

<p>Feature selection using Kruskal-Wallis test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  fs.kruskal(x,y,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fs.kruskal_+3A_x">x</code></td>
<td>

<p>A data frame or matrix of data set. 
</p>
</td></tr>
<tr><td><code id="fs.kruskal_+3A_y">y</code></td>
<td>

<p>A factor or vector of class.
</p>
</td></tr>
<tr><td><code id="fs.kruskal_+3A_...">...</code></td>
<td>

<p>Arguments to pass to method.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>fs.rank</code></td>
<td>
<p>A vector of feature ranking scores.</p>
</td></tr>
<tr><td><code>fs.order</code></td>
<td>
<p>A vector of feature order from best to worst.</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>
<p>A vector of statistics.</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>A vector of p values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## prepare data set
data(abr1)
cls &lt;- factor(abr1$fact$class)
dat &lt;- abr1$pos
## dat &lt;- abr1$pos[,110:1930]

## fill zeros with NAs
dat &lt;- mv.zene(dat)

## missing values summary
mv &lt;- mv.stats(dat, grp=cls) 
mv    ## View the missing value pattern

## filter missing value variables
## dim(dat)
dat &lt;- dat[,mv$mv.var &lt; 0.15]
## dim(dat)

## fill NAs with mean
dat &lt;- mv.fill(dat,method="mean")

## log transformation
dat &lt;- preproc(dat, method="log10")

## select class "1" and "2" for feature ranking
ind &lt;- grepl("1|2", cls)
mat &lt;- dat[ind,,drop=FALSE] 
mat &lt;- as.matrix(mat)
grp &lt;- cls[ind, drop=TRUE]   

## apply Kruskal-Wallis test method for feature selection/ranking
res &lt;- fs.kruskal(mat,grp)
names(res)

</code></pre>

<hr>
<h2 id='fs.pca'>
Feature Selection by PCA  
</h2><span id='topic+fs.pca'></span>

<h3>Description</h3>

<p>Feature selection using PCA loadings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  fs.pca(x,thres=0.8, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fs.pca_+3A_x">x</code></td>
<td>

<p>A data frame or matrix of data set. 
</p>
</td></tr>
<tr><td><code id="fs.pca_+3A_thres">thres</code></td>
<td>

<p>The threshold of the cumulative percentage of PC's explained variances.
</p>
</td></tr>
<tr><td><code id="fs.pca_+3A_...">...</code></td>
<td>

<p>Additional arguments to <code><a href="stats.html#topic+prcomp">prcomp</a></code>. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Since PCA loadings is a matrix with respect to PCs, the Mahalanobis distance of 
loadings is applied to select the features. (Other ways, for example, the sum 
of absolute values of loadings, or squared root of loadings, can be used.)
</p>
<p>It should be noticed that this feature selection method is unsupervised.
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>fs.rank</code></td>
<td>
<p>A vector of feature ranking scores.</p>
</td></tr>
<tr><td><code>fs.order</code></td>
<td>
<p>A vector of feature order from best to worst.</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>
<p>A vector of measurements.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+feat.rank.re">feat.rank.re</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## prepare data set
data(abr1)
cls &lt;- factor(abr1$fact$class)
dat &lt;- abr1$pos
## dat &lt;- abr1$pos[,110:1930]

## fill zeros with NAs
dat &lt;- mv.zene(dat)

## missing values summary
mv &lt;- mv.stats(dat, grp=cls) 
mv    ## View the missing value pattern

## filter missing value variables
## dim(dat)
dat &lt;- dat[,mv$mv.var &lt; 0.15]
## dim(dat)

## fill NAs with mean
dat &lt;- mv.fill(dat,method="mean")

## log transformation
dat &lt;- preproc(dat, method="log10")

## select class "1" and "2" for feature ranking
ind &lt;- grepl("1|2", cls)
mat &lt;- dat[ind,,drop=FALSE] 
mat &lt;- as.matrix(mat)
grp &lt;- cls[ind, drop=TRUE]   

## feature selection by PCA
res &lt;- fs.pca(dat)
names(res)

</code></pre>

<hr>
<h2 id='fs.pls'>
Feature Selection Using PLS  
</h2><span id='topic+fs.pls'></span><span id='topic+fs.plsvip'></span><span id='topic+fs.plsvip.1'></span><span id='topic+fs.plsvip.2'></span>

<h3>Description</h3>

<p>Feature selection using coefficient of regression and VIP values of PLS.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  fs.pls(x,y, pls="simpls",ncomp=10,...)
  fs.plsvip(x,y, ncomp=10,...)
  fs.plsvip.1(x,y, ncomp=10,...)
  fs.plsvip.2(x,y, ncomp=10,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fs.pls_+3A_x">x</code></td>
<td>

<p>A data frame or matrix of data set. 
</p>
</td></tr>
<tr><td><code id="fs.pls_+3A_y">y</code></td>
<td>

<p>A factor or vector of class.
</p>
</td></tr>
<tr><td><code id="fs.pls_+3A_pls">pls</code></td>
<td>

<p>A method for calculating PLS scores and loadings. The following methods are supported:
</p>

<ul>
<li> <p><code>simpls:</code> SIMPLS algorithm. 
</p>
</li>
<li> <p><code>kernelpls:</code> kernel algorithm.
</p>
</li>
<li> <p><code>oscorespls:</code> orthogonal scores algorithm. 
</p>
</li></ul>

<p>For details, see <code><a href="pls.html#topic+simpls.fit">simpls.fit</a></code>, <code><a href="pls.html#topic+kernelpls.fit">kernelpls.fit</a></code> and
<code><a href="pls.html#topic+oscorespls.fit">oscorespls.fit</a></code> in package <span class="pkg">pls</span>.
</p>
</td></tr>
<tr><td><code id="fs.pls_+3A_ncomp">ncomp</code></td>
<td>

<p>The number of components to be used.  
</p>
</td></tr>
<tr><td><code id="fs.pls_+3A_...">...</code></td>
<td>

<p>Arguments passed to or from other methods.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>fs.pls</code> ranks the features by regression coefficient of PLS. Since the 
coefficient is a matrix due to the dummy multiple response variables designed 
for the classification (category) problem, the Mahalanobis distance of 
coefficient is applied to select the features. (Other ways, for example, the sum 
of absolute values of coefficient, or squared root of coefficient, can be used.)
</p>
<p><code>fs.plsvip</code> and <code>fs.plsvip.1</code> carry out feature selection based on the 
the Mahalanobis distance and absolute values of PLS's VIP, respectively. 
</p>
<p><code>fs.plsvip.2</code> is similar to <code>fs.plsvip</code> and <code>fs.plsvip.1</code>, but 
the category response is not treated as dummy multiple response matrix. 
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>fs.rank</code></td>
<td>
<p>A vector of feature ranking scores.</p>
</td></tr>
<tr><td><code>fs.order</code></td>
<td>
<p>A vector of feature order from best to worst.</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>
<p>A vector of measurements.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+feat.rank.re">feat.rank.re</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## prepare data set
data(abr1)
cls &lt;- factor(abr1$fact$class)
dat &lt;- abr1$pos
## dat &lt;- abr1$pos[,110:1930]

## fill zeros with NAs
dat &lt;- mv.zene(dat)

## missing values summary
mv &lt;- mv.stats(dat, grp=cls) 
mv    ## View the missing value pattern

## filter missing value variables
## dim(dat)
dat &lt;- dat[,mv$mv.var &lt; 0.15]
## dim(dat)

## fill NAs with mean
dat &lt;- mv.fill(dat,method="mean")

## log transformation
dat &lt;- preproc(dat, method="log10")

## select class "1" and "2" for feature ranking
ind &lt;- grepl("1|2", cls)
mat &lt;- dat[ind,,drop=FALSE] 
mat &lt;- as.matrix(mat)
grp &lt;- cls[ind, drop=TRUE]   

## apply PLS methods for feature selection
res.pls      &lt;- fs.pls(mat,grp, ncomp=4)
res.plsvip   &lt;- fs.plsvip(mat,grp, ncomp=4)
res.plsvip.1 &lt;- fs.plsvip.1(mat,grp, ncomp=4)
res.plsvip.2 &lt;- fs.plsvip.2(mat,grp, ncomp=4)

## check differences among these methods
fs.order &lt;- data.frame(pls      = res.pls$fs.order,
                       plsvip   = res.plsvip$fs.order,
                       plsvip.1 = res.plsvip.1$fs.order,
                       plsvip.2 = res.plsvip.2$fs.order)
head(fs.order, 20)

</code></pre>

<hr>
<h2 id='fs.relief'>
Feature Selection Using RELIEF Method
</h2><span id='topic+fs.relief'></span>

<h3>Description</h3>

<p>Feature selection using RELIEF method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  fs.relief(x,y, m=NULL, k=10, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fs.relief_+3A_x">x</code></td>
<td>

<p>A data frame or matrix of data set. 
</p>
</td></tr>
<tr><td><code id="fs.relief_+3A_y">y</code></td>
<td>

<p>A factor or vector of class.
</p>
</td></tr>
<tr><td><code id="fs.relief_+3A_m">m</code></td>
<td>

<p>Number of instances to sample without replacement. Default is <code>NULL</code>
which takes all instances for computation.
</p>
</td></tr>
<tr><td><code id="fs.relief_+3A_k">k</code></td>
<td>

<p>Number of nearest neighbours used to estimate feature relevance. 
</p>
</td></tr>
<tr><td><code id="fs.relief_+3A_...">...</code></td>
<td>

<p>Arguments to pass to method (current ignore).    
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements the <b>Relief</b> algorithm's extension called 
<b>ReliefF</b>, which applies to multi-class problem and searches for <code>k</code> of its 
nearest neighbours from the same class, called <em>hits</em>, and also <code>k</code> 
nearest neighbours from each of the different classes, called <em>misses</em>. 
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>fs.rank</code></td>
<td>
<p>A vector of feature ranking scores.</p>
</td></tr>
<tr><td><code>fs.order</code></td>
<td>
<p>A vector of feature order from best to worst.</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>
<p>A vector of measurements.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>References</h3>

<p>Kira, K. and Rendel, L. (1992). 
The Feature Selection Problem: Traditional Methods and a new algorithm. 
<em>Proc. Tenth National Conference on Artificial Intelligence</em>, MIT Press, 
129 - 134. 
</p>
<p>Kononenko, I., Simes, E., and Robnik-Sikonja, M. (1997). 
Overcoming the Myopia of Induction Learning Algorithms with RELIEFF. 
<em>Applied Intelligence</em>, Vol.7, 1, 39-55. 
</p>
<p>Kononenko, I. (1994) Estimating Attributes: Analysis and Extensions of RELIEF,
<em>European Conference on Machine Learning</em>, Ed. Francesco Bergadano and
Luc De Raedt, 171-182, Springer
</p>
<p>Robnik-Sikonja, M. and Kononenko, I. (2003) Theoretical and Empirical Analysis
of ReliefF and RReliefF, <em>Machine Learning</em>, 53, 23 - 69.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
x &lt;- subset(iris, select = -Species)
y &lt;- iris$Species

fs &lt;- fs.relief(x, y, m=20,k=10)

</code></pre>

<hr>
<h2 id='fs.rf'> Feature Selection Using Random Forests (RF) </h2><span id='topic+fs.rf'></span><span id='topic+fs.rf.1'></span>

<h3>Description</h3>

<p> Feature selection using Random Forests (RF). </p>


<h3>Usage</h3>

<pre><code class='language-R'>  fs.rf(x,y,...)
  fs.rf.1(x,y,fs.len="power2",...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fs.rf_+3A_x">x</code></td>
<td>
<p> A data frame or matrix of data set. </p>
</td></tr>
<tr><td><code id="fs.rf_+3A_y">y</code></td>
<td>
<p> A factor or vector of class. </p>
</td></tr>
<tr><td><code id="fs.rf_+3A_fs.len">fs.len</code></td>
<td>
<p> Method or numeric sequence for feature lengths. For
details, see <code><a href="#topic+get.fs.len">get.fs.len</a></code> </p>
</td></tr>
<tr><td><code id="fs.rf_+3A_...">...</code></td>
<td>
<p> Arguments to pass to <code>randomForests</code>. </p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>fs.rf.1</code> select features based on successively eliminating the least 
important variables.
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>fs.rank</code></td>
<td>
<p>A vector of feature ranking scores.</p>
</td></tr>
<tr><td><code>fs.order</code></td>
<td>
<p>A vector of feature order from best to worst.</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>
<p>A vector of measurements. For <code>fs.rf</code>, it is Random
Forest important score. For <code>fs.rf.1</code>, it is a dummy variable
(current ignored). </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Wanchang Lin  </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(abr1)
cls &lt;- factor(abr1$fact$class)
dat &lt;- abr1$pos

## fill zeros with NAs
dat &lt;- mv.zene(dat)

## missing values summary
mv &lt;- mv.stats(dat, grp=cls) 
mv    ## View the missing value pattern

## filter missing value variables
dat &lt;- dat[,mv$mv.var &lt; 0.15]

## fill NAs with mean
dat &lt;- mv.fill(dat,method="mean")

## log transformation
dat &lt;- preproc(dat, method="log10")

## select class "1" and "2" for feature ranking
ind &lt;- grepl("1|2", cls)
mat &lt;- dat[ind,,drop=FALSE] 
mat &lt;- as.matrix(mat)
grp &lt;- cls[ind, drop=TRUE]   

## apply random forests for feature selection/ranking
res   &lt;- fs.rf(mat,grp)
res.1 &lt;- fs.rf.1(mat,grp)

## compare the results
fs &lt;- cbind(fs.rf=res$fs.order, fs.rf.1=res.1$fs.order)

## plot the important score of 'fs.rf' (not 'fs.rf.1')
score &lt;- res$stats
score &lt;- sort(score, decreasing = TRUE)
plot(score)

</code></pre>

<hr>
<h2 id='fs.rfe'>
Feature Selection Using SVM-RFE 
</h2><span id='topic+fs.rfe'></span>

<h3>Description</h3>

<p>Feature selection using Support Vector Machine based on Recursive Feature Elimination (SVM-RFE) 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  fs.rfe(x,y,fs.len="power2",...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fs.rfe_+3A_x">x</code></td>
<td>

<p>A data frame or matrix of data set. 
</p>
</td></tr>
<tr><td><code id="fs.rfe_+3A_y">y</code></td>
<td>

<p>A factor or vector of class.
</p>
</td></tr>
<tr><td><code id="fs.rfe_+3A_fs.len">fs.len</code></td>
<td>

<p>Method for feature lengths used in SVM-RFE computation.  
For details, see <code><a href="#topic+get.fs.len">get.fs.len</a></code>. 
</p>
</td></tr>
<tr><td><code id="fs.rfe_+3A_...">...</code></td>
<td>

<p>Arguments to pass to <code>svm</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>fs.rank</code></td>
<td>
<p>A vector of feature ranking scroes.</p>
</td></tr>
<tr><td><code>fs.order</code></td>
<td>
<p>A vector of feature order from best to worst.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+feat.rank.re">feat.rank.re</a></code>, <code><a href="#topic+get.fs.len">get.fs.len</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## prepare data set
data(abr1)
cls &lt;- factor(abr1$fact$class)
dat &lt;- abr1$pos
## dat &lt;- abr1$pos[,110:1930]

## fill zeros with NAs
dat &lt;- mv.zene(dat)

## missing values summary
mv &lt;- mv.stats(dat, grp=cls) 
mv    ## View the missing value pattern

## filter missing value variables
## dim(dat)
dat &lt;- dat[,mv$mv.var &lt; 0.15]
## dim(dat)

## fill NAs with mean
dat &lt;- mv.fill(dat,method="mean")

## log transformation
dat &lt;- preproc(dat, method="log10")

## select class "1" and "2" for feature ranking
ind &lt;- grepl("1|2", cls)
mat &lt;- dat[ind,,drop=FALSE] 
mat &lt;- as.matrix(mat)
grp &lt;- cls[ind, drop=TRUE]   

## apply RFE method for feature selection/ranking
res &lt;- fs.rfe(mat,grp)
names(res)

</code></pre>

<hr>
<h2 id='fs.snr'>
Feature Selection Using Signal-to-Noise Ratio (SNR) 
</h2><span id='topic+fs.snr'></span>

<h3>Description</h3>

<p>Feature selection using signal-to-noise ratio (SNR).  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  fs.snr(x,y,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fs.snr_+3A_x">x</code></td>
<td>

<p>A data frame or matrix of data set. 
</p>
</td></tr>
<tr><td><code id="fs.snr_+3A_y">y</code></td>
<td>

<p>A factor or vector of class.
</p>
</td></tr>
<tr><td><code id="fs.snr_+3A_...">...</code></td>
<td>

<p>Arguments to pass(current ignored).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>fs.rank</code></td>
<td>
<p>A vector of feature ranking scores.</p>
</td></tr>
<tr><td><code>fs.order</code></td>
<td>
<p>A vector of feature order from best to worst.</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>
<p>A vector of measurements.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function is for two-class problem only. 
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## prepare data set
data(abr1)
cls &lt;- factor(abr1$fact$class)
dat &lt;- abr1$pos
## dat &lt;- abr1$pos[,110:1930]

## fill zeros with NAs
dat &lt;- mv.zene(dat)

## missing values summary
mv &lt;- mv.stats(dat, grp=cls) 
mv    ## View the missing value pattern

## filter missing value variables
## dim(dat)
dat &lt;- dat[,mv$mv.var &lt; 0.15]
## dim(dat)

## fill NAs with mean
dat &lt;- mv.fill(dat,method="mean")

## log transformation
dat &lt;- preproc(dat, method="log10")

## select class "1" and "2" for feature ranking
ind &lt;- grepl("1|2", cls)
mat &lt;- dat[ind,,drop=FALSE] 
mat &lt;- as.matrix(mat)
grp &lt;- cls[ind, drop=TRUE]   

## apply SNR method for feature selection/ranking
res &lt;- fs.snr(mat,grp)
names(res)

</code></pre>

<hr>
<h2 id='fs.welch'>
Feature Selection Using Welch Test
</h2><span id='topic+fs.welch'></span>

<h3>Description</h3>

<p>Feature selection using Welch test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  fs.welch(x,y,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fs.welch_+3A_x">x</code></td>
<td>

<p>A data frame or matrix of data set. 
</p>
</td></tr>
<tr><td><code id="fs.welch_+3A_y">y</code></td>
<td>

<p>A factor or vector of class.
</p>
</td></tr>
<tr><td><code id="fs.welch_+3A_...">...</code></td>
<td>

<p>Arguments to pass to method.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>fs.rank</code></td>
<td>
<p>A vector of feature ranking scores.</p>
</td></tr>
<tr><td><code>fs.order</code></td>
<td>
<p>A vector of feature order from best to worst.</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>
<p>A vector of statistics.</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>A vector of p values.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function is for two-class problem only. 
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## prepare data set
data(abr1)
cls &lt;- factor(abr1$fact$class)
dat &lt;- abr1$pos
## dat &lt;- abr1$pos[,110:1930]

## fill zeros with NAs
dat &lt;- mv.zene(dat)

## missing values summary
mv &lt;- mv.stats(dat, grp=cls) 
mv    ## View the missing value pattern

## filter missing value variables
## dim(dat)
dat &lt;- dat[,mv$mv.var &lt; 0.15]
## dim(dat)

## fill NAs with mean
dat &lt;- mv.fill(dat,method="mean")

## log transformation
dat &lt;- preproc(dat, method="log10")

## select class "1" and "2" for feature ranking
ind &lt;- grepl("1|2", cls)
mat &lt;- dat[ind,,drop=FALSE] 
mat &lt;- as.matrix(mat)
grp &lt;- cls[ind, drop=TRUE]   

## apply Welch method for feature selection/ranking
res &lt;- fs.welch(mat,grp)
names(res)

</code></pre>

<hr>
<h2 id='fs.wilcox'>
Feature Selection Using Wilcoxon Test
</h2><span id='topic+fs.wilcox'></span>

<h3>Description</h3>

<p>Feature selection using Wilcoxon test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  fs.wilcox(x,y,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fs.wilcox_+3A_x">x</code></td>
<td>

<p>A data frame or matrix of data set. 
</p>
</td></tr>
<tr><td><code id="fs.wilcox_+3A_y">y</code></td>
<td>

<p>A factor or vector of class.
</p>
</td></tr>
<tr><td><code id="fs.wilcox_+3A_...">...</code></td>
<td>

<p>Arguments to pass to method.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>fs.rank</code></td>
<td>
<p>A vector of feature ranking scores.</p>
</td></tr>
<tr><td><code>fs.order</code></td>
<td>
<p>A vector of feature order from best to worst.</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>
<p>A vector of statistics.</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>A vector of p values.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function is for two-class problem only. 
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## prepare data set
data(abr1)
cls &lt;- factor(abr1$fact$class)
dat &lt;- abr1$pos
## dat &lt;- abr1$pos[,110:1930]

## fill zeros with NAs
dat &lt;- mv.zene(dat)

## missing values summary
mv &lt;- mv.stats(dat, grp=cls) 
mv    ## View the missing value pattern

## filter missing value variables
## dim(dat)
dat &lt;- dat[,mv$mv.var &lt; 0.15]
## dim(dat)

## fill NAs with mean
dat &lt;- mv.fill(dat,method="mean")

## log transformation
dat &lt;- preproc(dat, method="log10")

## select class "1" and "2" for feature ranking
ind &lt;- grepl("1|2", cls)
mat &lt;- dat[ind,,drop=FALSE] 
mat &lt;- as.matrix(mat)
grp &lt;- cls[ind, drop=TRUE]   

## apply Welch method for feature selection/ranking
res &lt;- fs.wilcox(mat,grp)
names(res)

</code></pre>

<hr>
<h2 id='get.fs.len'>
Get Length of Feature Subset for Validation
</h2><span id='topic+get.fs.len'></span>

<h3>Description</h3>

<p>Get feature lengths for feature selection validation by classification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  get.fs.len(p,fs.len=c("power2"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get.fs.len_+3A_p">p</code></td>
<td>

<p>Number of features in the data set. 
</p>
</td></tr>
<tr><td><code id="get.fs.len_+3A_fs.len">fs.len</code></td>
<td>

<p>Method or numeric sequence for feature lengths. It can be a numeric vector as
user-defined feature lengths, or methods:
</p>

<ul>
<li> <p><code>full</code>. The feature lengths are <code>p, ..., 2, 1</code>. This is an exhaustive 
method. If <code>p</code> is too large, it will consume a lot of time and hence it is not practical.
</p>
</li>
<li> <p><code>half</code>. The feature lengths are the sequence of 
<code>p, p/2, p/2/2, ..., 1</code>.
</p>
</li>
<li> <p><code>power2</code>. The feature lengths are the sequence of 
<code>p, 2^(log2(p)-1), ..., 2^1, 2^0</code>. 
</p>
</li></ul>

</td></tr>
</table>


<h3>Details</h3>

<p>The generation of feature length is used in the validation of feature subsets 
by classification. The feature length decide the lengths of feature subset starting 
from top of the full feature order list.
</p>


<h3>Value</h3>

<p>An descending order numeric vector of feature lengths. 
</p>


<h3>Note</h3>

<p>The last length of feature returned is always <code>p</code>.
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fs.rfe">fs.rfe</a></code>, <code><a href="#topic+frank.err">frank.err</a></code>,  <code><a href="#topic+frankvali">frankvali</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(abr1)
dat &lt;- abr1$pos

## number of featres
p &lt;- ncol(dat)

## predefined feature lengths. The returned will be descending order 
## vector with the first one is p.
(vec &lt;- get.fs.len(p, fs.len=c(1,2,3,4,5,6,7,8,9,10)))

## use all features as feature lengths
(vec.full &lt;- get.fs.len(p, fs.len="full"))

## use "half"
(vec.half &lt;- get.fs.len(p, fs.len="half"))

## use "power2"
(vec.power2 &lt;- get.fs.len(p, fs.len="power2"))

</code></pre>

<hr>
<h2 id='grpplot'>Plot Matrix-Like Object by Group</h2><span id='topic+grpplot'></span>

<h3>Description</h3>

<p>Plot matrix-like object by group</p>


<h3>Usage</h3>

<pre><code class='language-R'>  grpplot(x, y, plot = "pairs", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="grpplot_+3A_x">x</code></td>
<td>
<p>A matrix or data frame to be plotted. </p>
</td></tr>
<tr><td><code id="grpplot_+3A_y">y</code></td>
<td>
<p>A factor or vector giving group information of columns of <code>x</code>.</p>
</td></tr>
<tr><td><code id="grpplot_+3A_plot">plot</code></td>
<td>

<p>One of plot types: <code>strip</code>, <code>box</code>, <code>density</code> and
<code>pairs</code>. </p>
</td></tr>
<tr><td><code id="grpplot_+3A_...">...</code></td>
<td>
<p>Further arguments. See corresponding entry in
<code><a href="lattice.html#topic+xyplot">xyplot</a></code> for non-trivial details. One argument is
<code>ep</code>: an integer for plotting ellipse. <code>1</code> and
<code>2</code> for plotting overall and group ellipse, respectively.
Otherwise, none. For details, see <code><a href="#topic+panel.elli.1">panel.elli.1</a></code>. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"trellis"</code>.</p>


<h3>Author(s)</h3>

<p>Wanchang Lin </p>


<h3>See Also</h3>

<p><code><a href="#topic+panel.elli.1">panel.elli.1</a></code>, <code><a href="#topic+pcaplot">pcaplot</a></code>,
<code><a href="#topic+pca_plot_wrap">pca_plot_wrap</a></code>,  <code><a href="#topic+lda_plot_wrap">lda_plot_wrap</a></code>, 
<code><a href="#topic+pls_plot_wrap">pls_plot_wrap</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)
  grpplot(iris[,1:4], iris[,5],plot="strip", main="IRIS DATA")
  grpplot(iris[,1:4], iris[,5],plot="box", main="IRIS DATA")
  grpplot(iris[,1:4], iris[,5],plot="density", main="IRIS DATA")
  grpplot(iris[,1:4], iris[,5],plot="pairs",main="IRIS DATA",ep=2)

  ## returns an object of  class "trellis".
  tmp &lt;- grpplot(iris[,c(2,1)], iris[,5],main="IRIS DATA",ep=2)
  tmp
  
  ## change symbol's color, type and size
  grpplot(iris[,c(2,1)], iris[,5],main="IRIS DATA", cex=1.5,
         auto.key=list(space="right", col=c("black","blue","red")),
         par.settings = list(superpose.symbol = list(col=c("black","blue","red"),
                                                     pch=c(1:3))))

</code></pre>

<hr>
<h2 id='list.util'>
List Manipulation Utilities 
</h2><span id='topic+list2df'></span><span id='topic+un.list'></span><span id='topic+shrink.list'></span>

<h3>Description</h3>

<p>Functions to handle manipulation of list. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>list2df(x)

un.list(x, y="")

shrink.list(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="list.util_+3A_x">x</code></td>
<td>
<p>A list to be manipulated. </p>
</td></tr>
<tr><td><code id="list.util_+3A_y">y</code></td>
<td>
<p>A character or string of separator.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>list2df</code> converts a list with components of vector to a data 
frame. Shorter vectors will be filled with 
NA. It is useful to convert rugged vectors into a data frame which can 
be written to an Excel file.
</p>
<p><code>un.list</code> collapses higher-depths list to 1-depth list. 
This function uses recursive programming skill to tackle any depths 
of list.
</p>
<p><code>shrink.list</code> removes all NULL or NA entries from a list.
</p>


<h3>Value</h3>

<p><code>list2df</code> returns a data frame.
<code>un.list</code> returns a list.
<code>shrink.list</code> retuns a list.
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+feat.mfs">feat.mfs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'> ## See examples of function feat.mfs for the usages of list2df and un.list.
a &lt;- list(x=1, y=NA, z=NULL)
b &lt;- list(x=1, y=NA)
c &lt;- list(x=1, z=NULL)

shrink.list(a)
shrink.list(b)
shrink.list(c)

</code></pre>

<hr>
<h2 id='maccest'>Estimation of Multiple Classification Accuracy</h2><span id='topic+maccest'></span><span id='topic+maccest.formula'></span><span id='topic+maccest.default'></span><span id='topic+print.maccest'></span><span id='topic+summary.maccest'></span><span id='topic+print.summary.maccest'></span>

<h3>Description</h3>

<p>Estimation of classification accuracy by multiple classifiers with
resampling procedure and comparisons of multiple classifiers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maccest(dat, ...)
## Default S3 method:
maccest(dat, cl, method="svm", pars = valipars(), 
        tr.idx = NULL, comp="anova",...) 
## S3 method for class 'formula'
maccest(formula, data = NULL, ..., subset, na.action = na.omit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="maccest_+3A_formula">formula</code></td>
<td>

<p>A formula of the form <code>groups ~ x1 + x2 + ...</code>  That is, the
response is the grouping factor and the right hand side specifies
the (non-factor) discriminators.
</p>
</td></tr>
<tr><td><code id="maccest_+3A_data">data</code></td>
<td>

<p>Data frame from which variables specified in <code>formula</code> are
preferentially to be taken.
</p>
</td></tr>
<tr><td><code id="maccest_+3A_dat">dat</code></td>
<td>

<p>A matrix or data frame containing the explanatory variables if no
formula is given as the principal argument.
</p>
</td></tr>
<tr><td><code id="maccest_+3A_cl">cl</code></td>
<td>

<p>A factor specifying the class for each observation if no formula
principal argument is given.
</p>
</td></tr>
<tr><td><code id="maccest_+3A_method">method</code></td>
<td>

<p>A vector of multiple classification methods to be used. Classifiers,
such as <code>randomForest</code>, <code>svm</code>, <code>knn</code> and <code>lda</code>,
can be used. For details, see <code>note</code> below.
</p>
</td></tr>
<tr><td><code id="maccest_+3A_pars">pars</code></td>
<td>

<p>A list of resampling scheme such as <em>Leave-one-out
cross-validation</em>, <em>Cross-validation</em>, <em>Randomised
validation (holdout)</em> and <em>Bootstrap</em>, and control parameters
for the calculation of accuracy. See <code><a href="#topic+valipars">valipars</a></code> for
details.
</p>
</td></tr>
<tr><td><code id="maccest_+3A_tr.idx">tr.idx</code></td>
<td>

<p>User defined index of training samples. Can be generated by
<code>trainind</code>.
</p>
</td></tr>
<tr><td><code id="maccest_+3A_comp">comp</code></td>
<td>

<p>Comparison method of multiple classifier. If <code>comp</code> is
<code>anova</code>, the multiple comparisons are performed by <code>ANOVA</code>
and then the pairwise comparisons are performed by <code>HSDTukey</code>. If
<code>comp</code> is <code>fried</code>, the multiple comparisons are performed by
<code>Friedman Test</code> and the pairwise comparisons are performed by
<code>Wilcoxon Test</code>.
</p>
</td></tr>
<tr><td><code id="maccest_+3A_...">...</code></td>
<td>
<p>Additional parameters to <code>method</code>.</p>
</td></tr>
<tr><td><code id="maccest_+3A_subset">subset</code></td>
<td>

<p>Optional vector, specifying a subset of observations to be used.
</p>
</td></tr>
<tr><td><code id="maccest_+3A_na.action">na.action</code></td>
<td>

<p>Function which indicates what should happen when the data
contains <code>NA</code>'s, defaults to <code><a href="stats.html#topic+na.omit">na.omit</a></code>.
</p>
</td></tr> 
</table>


<h3>Details</h3>

<p>The accuracy rates for classification are obtained used techniques
such as <em>Random Forest</em>, <em>Support Vector Machine</em>,
<em>k-Nearest Neighbour Classification</em>, <em>Linear Discriminant
Analysis</em> and <em>Linear Discriminant Analysis</em> based on sampling
methods, including <em>Leave-one-out cross-validation</em>,
<em>Cross-validation</em>, <em>Randomised validation (holdout)</em> and
<em>Bootstrap</em>.
</p>


<h3>Value</h3>

<p>An object of class <code>maccest</code>, including the components:
</p>
<table>
<tr><td><code>method</code></td>
<td>
<p>Classification method used.</p>
</td></tr>
<tr><td><code>acc</code></td>
<td>
<p>Accuracy rate.</p>
</td></tr>
<tr><td><code>acc.iter</code></td>
<td>
<p>Accuracy rate of each iteration.</p>
</td></tr>
<tr><td><code>acc.std</code></td>
<td>
<p>Standard derivation of accuracy rate.</p>
</td></tr>
<tr><td><code>mar</code></td>
<td>
<p>Prediction margin.</p>
</td></tr>
<tr><td><code>mar.iter</code></td>
<td>
<p>Prediction margin of each iteration.</p>
</td></tr>
<tr><td><code>auc</code></td>
<td>
<p>The area under receiver operating curve (AUC).</p>
</td></tr>
<tr><td><code>auc.iter</code></td>
<td>
<p>AUC of each iteration.</p>
</td></tr>
<tr><td><code>comp</code></td>
<td>
<p>Multiple comparison method used.</p>
</td></tr>
<tr><td><code>h.test</code></td>
<td>
<p>Hypothesis test results of multiple comparison.</p>
</td></tr>
<tr><td><code>gl.pval</code></td>
<td>
<p>Global or overall p-value.</p>
</td></tr>
<tr><td><code>mc.pval</code></td>
<td>
<p>Pairwise comparison p-values.</p>
</td></tr>
<tr><td><code>sampling</code></td>
<td>
<p>Sampling scheme used.</p>
</td></tr> 
<tr><td><code>niter</code></td>
<td>
<p>Number of iteration.</p>
</td></tr>
<tr><td><code>nreps</code></td>
<td>
<p>Number of replications in each iteration. </p>
</td></tr>
<tr><td><code>conf.mat</code></td>
<td>
<p>Overall confusion matrix.</p>
</td></tr>
<tr><td><code>acc.boot</code></td>
<td>
<p> A list of bootrap error such as <code>.632</code> and <code>.632+</code>
if the validation method is bootrap. 
</p>
</td></tr> 
</table>


<h3>Note</h3>

<p>The <code>maccest</code> can take any classification model if its argument
format is <code>model(formula, data, subset, na.action, ...)</code> and
their corresponding method <code>predict.model(object, newdata, ...)</code>
can either return the only predicted class label or in a list with
name as <code>class</code>, such as <code>lda</code> and <code>pcalda</code>.
</p>
<p>As for the multiple comparisons by <code>ANOVA</code>, the following
assumptions should be considered:
</p>

<ul>
<li><p> The samples are randomly and independently selected.
</p>
</li>
<li><p> The populations are normally distributed.
</p>
</li>
<li><p> The populations  all have the same variance.
</p>
</li></ul>

<p>All the comparisons are based on the results of all iteration.
</p>
<p><code><a href="#topic+aam.mcl">aam.mcl</a></code> is a simplified version which returns <code>acc</code>
(accuracy), <code>auc</code>(area under ROC curve) and <code>mar</code>(class
margin).
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+accest">accest</a></code>, <code><a href="#topic+aam.mcl">aam.mcl</a></code>, <code><a href="#topic+valipars">valipars</a></code>,
<code><a href="#topic+plot.maccest">plot.maccest</a></code> <code><a href="#topic+trainind">trainind</a></code>,
<code><a href="#topic+boxplot.maccest">boxplot.maccest</a></code>,<code><a href="#topic+classifier">classifier</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'># Iris data
data(iris)
x      &lt;- subset(iris, select = -Species)
y      &lt;- iris$Species

method &lt;- c("randomForest","svm","pcalda","knn")
pars   &lt;- valipars(sampling="boot", niter = 3, nreps=5, strat=TRUE)
res    &lt;- maccest(Species~., data = iris, method=method, pars = pars, 
                  comp="anova")
## or 
res    &lt;- maccest(x, y, method=method, pars=pars, comp="anova") 

res
summary(res)
plot(res)
boxplot(res)
oldpar &lt;- par(mar = c(5,10,4,2) + 0.1)
plot(res$h.test$tukey,las=1)   ## plot the tukey results
par(oldpar)
</code></pre>

<hr>
<h2 id='mbinest'>
Binary Classification by Multiple Classifier
</h2><span id='topic+mbinest'></span>

<h3>Description</h3>

<p>Binary classification by multiple classifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mbinest(dat, cl, choices = NULL, method, pars=valipars(),...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mbinest_+3A_dat">dat</code></td>
<td>

<p>A matrix or data frame containing the explanatory variables.
</p>
</td></tr>
<tr><td><code id="mbinest_+3A_cl">cl</code></td>
<td>

<p>A factor specifying the class for each observation.
</p>
</td></tr>
<tr><td><code id="mbinest_+3A_choices">choices</code></td>
<td>

<p>The vector or list of class labels to be chosen for binary classification.
For details, see <code><a href="#topic+dat.sel">dat.sel</a></code>.
</p>
</td></tr>
<tr><td><code id="mbinest_+3A_method">method</code></td>
<td>

<p>Multiple classification methods to be used. For details, see <code><a href="#topic+maccest">maccest</a></code>.
</p>
</td></tr>             
<tr><td><code id="mbinest_+3A_pars">pars</code></td>
<td>

<p>A list of parameters of the resampling method. See <code><a href="#topic+valipars">valipars</a></code> for details.
</p>
</td></tr>
<tr><td><code id="mbinest_+3A_...">...</code></td>
<td>
<p>Additional parameters to <code>method</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>all</code></td>
<td>
<p>All results of classification.</p>
</td></tr>
<tr><td><code>com</code></td>
<td>
<p>A matrix of the combinations of the binary class labels.</p>
</td></tr>
<tr><td><code>acc</code></td>
<td>
<p>A table of classification accuracy for the binary combination.</p>
</td></tr>
<tr><td><code>mar</code></td>
<td>
<p>Prediction margin.</p>
</td></tr>
<tr><td><code>auc</code></td>
<td>
<p>The area under receiver operating curve (AUC).</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>Classification methods used.</p>
</td></tr>
<tr><td><code>niter</code></td>
<td>
<p>Number of iterations.</p>
</td></tr>
<tr><td><code>sampling</code></td>
<td>
<p>Sampling scheme used.</p>
</td></tr> 
<tr><td><code>nreps</code></td>
<td>
<p>Number of replications in each iteration if sampling is not <code>loocv</code>. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+maccest">maccest</a></code>, <code><a href="#topic+maccest">maccest</a></code>,<code><a href="#topic+valipars">valipars</a></code>, <code><a href="#topic+dat.sel">dat.sel</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## iris data set
data(iris)
dat    &lt;- subset(iris, select = -Species)
cl     &lt;- iris$Species
method &lt;- c("svm","pcalda")

pars  &lt;- valipars(sampling="cv",niter = 10, nreps = 5)
res   &lt;- mbinest(dat,cl,choices=c("setosa"), method=method,
                  pars = pars, kernel="linear")

## combine prediction accuracy, AUC and margin 
z      &lt;- round(cbind(res$acc,res$auc,res$mar),digits=3)
colnames(z) &lt;- c(paste(method,".acc", sep=""),paste(method,".auc", sep=""),
                 paste(method,".mar", sep=""))

</code></pre>

<hr>
<h2 id='mc.anova'>
Multiple Comparison by 'ANOVA' and Pairwise Comparison by 'HSDTukey Test'
</h2><span id='topic+mc.anova'></span>

<h3>Description</h3>

<p>Performs multiple comparison by <code>ANOVA</code> and pairwise comparison by 
<code>HSDTukey Test</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mc.anova(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mc.anova_+3A_x">x</code></td>
<td>

<p>A matrix or data frame to be tested. 
</p>
</td></tr>
<tr><td><code id="mc.anova_+3A_...">...</code></td>
<td>
<p>Additional arguments pass to <code>anova</code> or <code>HSDTukey test</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>anova</code></td>
<td>
<p>Hypothesis test results of <code>anova</code>.</p>
</td></tr>
<tr><td><code>tukey</code></td>
<td>
<p>Hypothesis test results of <code>HSDTukey.test</code>.</p>
</td></tr>
<tr><td><code>gl.pval</code></td>
<td>
<p>Global or overall p value returned by <code>anova</code>.</p>
</td></tr>
<tr><td><code>mc.pval</code></td>
<td>
<p>Pairwise p value returned by <code>HSDTukey.test</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+maccest">maccest</a></code>, <code><a href="#topic+mc.fried">mc.fried</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Iris data
data(iris)
x      &lt;- subset(iris, select = -Species)
y      &lt;- iris$Species

method &lt;- c("randomForest","svm","pcalda","knn")
pars   &lt;- valipars(sampling="boot", niter = 10, nreps=4)
res    &lt;- maccest(x, y, method=method, pars=pars, comp="anova") 

res
htest &lt;- mc.anova(res$acc.iter)

oldpar &lt;- par(mar = c(5,10,4,2) + 0.1)
plot(htest$tukey,las=1)   ## plot the tukey results
par(oldpar)
</code></pre>

<hr>
<h2 id='mc.fried'>
Multiple Comparison by 'Friedman Test' and Pairwise Comparison by 'Wilcoxon Test'
</h2><span id='topic+mc.fried'></span>

<h3>Description</h3>

<p>Performs multiple comparison by <code>Friedman test</code> and pairwise comparison by 
<code>Wilcoxon Test</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mc.fried(x, p.adjust.method = p.adjust.methods,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mc.fried_+3A_x">x</code></td>
<td>

<p>A matrix or data frame to be tested. 
</p>
</td></tr>
<tr><td><code id="mc.fried_+3A_p.adjust.method">p.adjust.method</code></td>
<td>
<p>Method for adjusting p values (see <code><a href="stats.html#topic+p.adjust">p.adjust</a></code>).</p>
</td></tr>  
<tr><td><code id="mc.fried_+3A_...">...</code></td>
<td>
<p>Additional arguments pass to <code>friedman.test</code> or <code>pairwise.wilcox.test</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>fried</code></td>
<td>
<p>Hypothesis test results of <code>friedman.test</code>.</p>
</td></tr>
<tr><td><code>wilcox</code></td>
<td>
<p>Hypothesis test results of <code>pairwise.wilcox.test</code>.</p>
</td></tr>
<tr><td><code>gl.pval</code></td>
<td>
<p>Global or overall p value returned by <code>friedman.test</code>.</p>
</td></tr>
<tr><td><code>mc.pval</code></td>
<td>
<p>Pairwise p value returned by <code>pairwise.wilcox.test</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+maccest">maccest</a></code>, <code><a href="#topic+mc.anova">mc.anova</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Iris data
data(iris)
x      &lt;- subset(iris, select = -Species)
y      &lt;- iris$Species

method &lt;- c("randomForest","svm","pcalda","knn")
pars   &lt;- valipars(sampling="cv", niter = 10, nreps=4)
res    &lt;- maccest(x, y, method=method, pars=pars,
                  comp="fried",kernel="linear") 

res

htest &lt;- mc.fried(res$acc.iter)

</code></pre>

<hr>
<h2 id='mc.norm'>
Normality Test by Shapiro-Wilk Test
</h2><span id='topic+mc.norm'></span>

<h3>Description</h3>

<p>Perform Shapiro-Wilk normality test by <code>shapiro.test</code> and plot the 
density function and boxplot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mc.norm(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mc.norm_+3A_x">x</code></td>
<td>

<p>A matrix or data frame to be tested. 
</p>
</td></tr>
<tr><td><code id="mc.norm_+3A_...">...</code></td>
<td>
<p>Additional arguments pass to <code>shapiro.test</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of <code>shapiro.test</code>, boxplot and histogram.
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+maccest">maccest</a></code>, <code><a href="#topic+mc.anova">mc.anova</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
x      &lt;- subset(iris, select = -Species)
y      &lt;- iris$Species
method &lt;- c("randomForest","svm","pcalda","knn")
pars   &lt;- valipars(sampling="boot", niter = 10, nreps=10)
res    &lt;- maccest(x, y, method=method, pars=pars,
                  comp="anova") 
res
res$acc.iter
mc.norm(res$acc.iter)
</code></pre>

<hr>
<h2 id='mdsplot'>Plot Classical Multidimensional Scaling</h2><span id='topic+mdsplot'></span>

<h3>Description</h3>

<p>Plot metric MDS with categorical information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mdsplot(x, y, method = "euclidean", dimen = c(1,2), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mdsplot_+3A_x">x</code></td>
<td>
<p>A matrix or data frame to be plotted. </p>
</td></tr>
<tr><td><code id="mdsplot_+3A_y">y</code></td>
<td>
<p>A factor or vector giving group information of columns of <code>x</code>.</p>
</td></tr>
<tr><td><code id="mdsplot_+3A_method">method</code></td>
<td>
 
<p>The distance measure to be used. This must be one of &quot;euclidean&quot;, &quot;maximum&quot;, 
&quot;manhattan&quot;, &quot;canberra&quot;, &quot;binary&quot; or &quot;minkowski&quot;. Any unambiguous substring
can be given. It is only for <code>mds_plot_wrap</code>.
</p>
</td></tr>
<tr><td><code id="mdsplot_+3A_dimen">dimen</code></td>
<td>

<p>A vector of index of dimentonal to be plotted. Only two dimentions are 
are allowed.</p>
</td></tr>
<tr><td><code id="mdsplot_+3A_...">...</code></td>
<td>

<p>Further arguments to <code><a href="stats.html#topic+prcomp">prcomp</a></code> or <code>lattice</code>. See 
corresponding entry in <code><a href="lattice.html#topic+xyplot">xyplot</a></code> for non-trivial details of 
<code>lattice</code>. For <code>pcaplot</code>, one argument is
<code>ep</code>: an integer for plotting 95% ellipse. <code>1</code> and <code>2</code> for 
plotting overall and group ellipse, respectively. Otherwise, none.
For details, see <code><a href="#topic+panel.elli.1">panel.elli.1</a></code>. 
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>mdsplot</code> returns an object of class <code>"trellis"</code>.
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+grpplot">grpplot</a></code>, <code><a href="#topic+panel.elli">panel.elli</a></code>, <code><a href="#topic+mds_plot_wrap">mds_plot_wrap</a></code>,
<code><a href="#topic+pcaplot">pcaplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## examples of 'mdsplot'

data(iris)
x &lt;- iris[,1:4]
y &lt;- iris[,5]
mdsplot(x,y, dimen=c(1,2),ep=2)
mdsplot(x,y, dimen=c(2,1),ep=1)

tmp &lt;- mdsplot(x,y, ep=2, conf.level = 0.9)
tmp

## change symbol's color, type and size
mdsplot(x, y, main="IRIS DATA", cex=1.2,
  auto.key=list(space="right", col=c("black","blue","red"), cex=1.2),
  par.settings = list(superpose.symbol = list(col=c("black","blue","red"),
                                              pch=c(1:3))))
</code></pre>

<hr>
<h2 id='mv.util'>
Missing Value Utilities
</h2><span id='topic+mv.stats'></span><span id='topic+mv.fill'></span><span id='topic+mv.zene'></span>

<h3>Description</h3>

<p>Functions to handle missing values of data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mv.stats(dat,grp=NULL,...) 
  
mv.fill(dat,method="mean",ze_ne = FALSE)

mv.zene(dat)
  
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mv.util_+3A_dat">dat</code></td>
<td>

<p>A data frame or matrix of data set. 
</p>
</td></tr>
<tr><td><code id="mv.util_+3A_grp">grp</code></td>
<td>

<p>A factor or vector of class.
</p>
</td></tr>
<tr><td><code id="mv.util_+3A_method">method</code></td>
<td>
<p>Univariate imputation method for missing value. For details, see 
examples below.
</p>
</td></tr>
<tr><td><code id="mv.util_+3A_ze_ne">ze_ne</code></td>
<td>
 
<p>A logical value indicating whether the zeros or negatives should be 
treated as missing values.
</p>
</td></tr>
<tr><td><code id="mv.util_+3A_...">...</code></td>
<td>

<p>Additional parameters to <code>mv.stats</code> for plotting using <span class="pkg">lattice</span>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>mv.fill</code> returns an imputed data frame. 
</p>
<p><code>mv.zene</code> returns an NA-filled data frame.
</p>
<p><code>mv.stats</code> returns a list including the components:
</p>
 
<ul>
<li> <p><code>mv.overall</code>: Overall missng value rate.
</p>
</li>
<li> <p><code>mv.var</code>: Missing value rate per variable (column).
</p>
</li>
<li> <p><code>mv.grp</code>: A matrix of missing value rate for different groups
if argument <code>grp</code> is given.
</p>
</li>
<li> <p><code>mv.grp.plot</code>:  An object of class <code>trellis</code> for plotting  
of <code>mv.grp</code> if argument <code>grp</code> is given.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(abr1)
dat &lt;- abr1$pos[,1970:1980]
cls &lt;- factor(abr1$fact$class)

## fill zeros with NAs
dat &lt;- mv.zene(dat)

## missing values summary
mv &lt;- mv.stats(dat, grp=cls) 
plot(mv$mv.grp.plot)

## fill NAs with mean
dat.mean &lt;- mv.fill(dat,method="mean")

## fill NAs with median
dat.median &lt;- mv.fill(dat,method="median")

## -----------------------------------------------------------------------
## fill NAs with user-defined methods: two examples given here.
## a.) Random imputation function:
rand &lt;- function(x,...) sample(x[!is.na(x)], sum(is.na(x)), replace=TRUE)

## test this function:
(tmp &lt;- dat[,1])        ## an vector with NAs
## get the randomised values for NAs
rand(tmp)

## fill NAs with method "rand"
dat.rand &lt;- mv.fill(dat,method="rand")

## b.) "Low" imputation function:
"low" &lt;- function(x, ...) {
  max(mean(x,...) - 3 * sd(x,...), min(x, ...)/2)
}
## fill NAs with method "low"
dat.low &lt;- mv.fill(dat, method="low") 

## summary of imputed data set
df.summ(dat.mean)

</code></pre>

<hr>
<h2 id='osc'>
Orthogonal Signal Correction (OSC)
</h2><span id='topic+osc'></span><span id='topic+osc.default'></span><span id='topic+osc.formula'></span><span id='topic+print.osc'></span><span id='topic+summary.osc'></span><span id='topic+print.summary.osc'></span>

<h3>Description</h3>

<p>Data pre-processing by orthogonal signal correction (OSC).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>osc(x, ...)

## Default S3 method:
osc(x, y, method="wold",center=TRUE,osc.ncomp=4,pls.ncomp=10,
   tol=1e-3, iter=20,...)

## S3 method for class 'formula'
osc(formula, data = NULL, ..., subset, na.action = na.omit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="osc_+3A_formula">formula</code></td>
<td>

<p>A formula of the form <code>groups ~ x1 + x2 + ...</code>  That is, the
response is the grouping factor and the right hand side specifies
the (non-factor) discriminators.
</p>
</td></tr>
<tr><td><code id="osc_+3A_data">data</code></td>
<td>

<p>Data frame from which variables specified in <code>formula</code> are
preferentially to be taken.
</p>
</td></tr>
<tr><td><code id="osc_+3A_x">x</code></td>
<td>

<p>A matrix or data frame containing the explanatory variables if no formula is
given as the principal argument.
</p>
</td></tr>
<tr><td><code id="osc_+3A_y">y</code></td>
<td>

<p>A factor specifying the class for each observation if no formula principal 
argument is given.
</p>
</td></tr>
<tr><td><code id="osc_+3A_method">method</code></td>
<td>

<p>A method for calculating OSC weights, loadings and scores. The following methods are supported:
</p>

<ul>
<li> <p><code>wold:</code> Original Wold et al approach. 
</p>
</li>
<li> <p><code>sjoblom:</code> Sjoblom et al approach.
</p>
</li>
<li> <p><code>wise:</code> Wise and Gallagher approach. 
</p>
</li></ul>

</td></tr>
<tr><td><code id="osc_+3A_center">center</code></td>
<td>

<p>A logical value indicating whether the data set should be centred by column-wise.
</p>
</td></tr>
<tr><td><code id="osc_+3A_osc.ncomp">osc.ncomp</code></td>
<td>

<p>The number of components to be used in the OSC calculation.  
</p>
</td></tr>
<tr><td><code id="osc_+3A_pls.ncomp">pls.ncomp</code></td>
<td>

<p>The number of components to be used in the PLS calculation.  
</p>
</td></tr>
<tr><td><code id="osc_+3A_tol">tol</code></td>
<td>

<p>A scalar value of tolerance for OSC computation.
</p>
</td></tr>
<tr><td><code id="osc_+3A_iter">iter</code></td>
<td>

<p>The number of iteration used in OSC calculation.
</p>
</td></tr>
<tr><td><code id="osc_+3A_...">...</code></td>
<td>

<p>Arguments passed to or from other methods.
</p>
</td></tr>
<tr><td><code id="osc_+3A_subset">subset</code></td>
<td>

<p>An index vector specifying the cases to be used in the training
sample.  
</p>
</td></tr>
<tr><td><code id="osc_+3A_na.action">na.action</code></td>
<td>

<p>A function to specify the action to be taken if <code>NA</code>s are found. The 
default action is <code>na.omit</code>, which leads to rejection of cases with 
missing values on any required variable. An alternative is <code>na.fail</code>, 
which causes an error if <code>NA</code> cases are found. 
</p>
</td></tr>	
</table>


<h3>Value</h3>

<p>An object of class <code>osc</code> containing the following components:
</p>
<table>
<tr><td><code>x</code></td>
<td>

<p>A matrix of OSC corrected data set.
</p>
</td></tr>
<tr><td><code>R2</code></td>
<td>

<p>R2 statistics. It is calculated as the fraction of variation in X after OSC 
correction for the calibration (training) data.
</p>
</td></tr>
<tr><td><code>angle</code></td>
<td>

<p>An angle used for checking if scores <code>t</code> is orthogonal to <code>y</code>. An 
angle close to 90 degree means that orthogonality is achieved in the correction process.
</p>
</td></tr>
<tr><td><code>w</code></td>
<td>

<p>A matrix of OSC weights.
</p>
</td></tr>
<tr><td><code>p</code></td>
<td>

<p>A matrix of OSC loadings.
</p>
</td></tr>
<tr><td><code>t</code></td>
<td>

<p>A matrix of OSC scores.
</p>
</td></tr>
<tr><td><code>call</code></td>
<td>

<p>The (matched) function call.
</p>
</td></tr>
<tr><td><code>center</code></td>
<td>

<p>A logical value indicating whether the data set has been centred by column-wise.
</p>
</td></tr>
<tr><td><code>osc.ncomp</code></td>
<td>

<p>The number of component used in OSC computation.
</p>
</td></tr>
<tr><td><code>pls.ncomp</code></td>
<td>

<p>The number of component used in PLS computation.
</p>
</td></tr>
<tr><td><code>method</code></td>
<td>

<p>The OSC algorithm used.
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function may be called giving either a formula and
optional data frame, or a matrix and grouping factor as the first
two arguments. 
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>References</h3>

<p>Wold, S., Antti, H., Lindgren, F., Ohman, J.(1998). Orthogonal signal correction of near infrared
spectra. <em>Chemometrics Intell. Lab. Syst.</em>, 44: 175-185.
</p>
<p>Westerhuis, J. A., de Jong, S., Smilde, A, K. (2001). Direct orthogonal signal correction. 
<em>Chemometrics Intell. Lab. Syst.</em>, 56: 13-25.
</p>
<p>Sjoblom. J., Svensson, O., Josefson, M., Kullberg, H., Wold, S. (1998). An evaluation of
orthogonal signal correction applied to calibration transfer of near infrared
spectra. <em>Chemometrics Intell. Lab. Syst.</em>,44: 229-244.
</p>
<p>Svensson, O., Kourti, T. and MacGregor, J.F. (2002). An investigation of orthogonal 
correction algorithms and their characteristics. <em>Journal of Chemometrics</em>, 16:176-188.
</p>
<p>Wise, B. M. and Gallagher, N.B. <em>http://www.eigenvector.com/MATLAB/OSC.html</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.osc">predict.osc</a></code>, <code><a href="#topic+osc_wold">osc_wold</a></code>, <code><a href="#topic+osc_sjoblom">osc_sjoblom</a></code>,
<code><a href="#topic+osc_wise">osc_wise</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(abr1)
cl   &lt;- factor(abr1$fact$class)
dat  &lt;- abr1$pos

## divide data as training and test data
idx &lt;- sample(1:nrow(dat), round((2/3)*nrow(dat)), replace=FALSE) 

## construct train and test data 
train.dat  &lt;- dat[idx,]
train.t    &lt;- cl[idx]
test.dat   &lt;- dat[-idx,]        
test.t     &lt;- cl[-idx] 

## build OSC model based on the training data
res &lt;- osc(train.dat, train.t, method="wise", osc.ncomp=2, pls.ncomp=4)
names(res)
res
summary(res)

## pre-process test data by OSC
test.dat.1 &lt;- predict(res,test.dat)$x

</code></pre>

<hr>
<h2 id='osc_sjoblom'>
Orthogonal Signal Correction (OSC) Approach by Sjoblom et al.
</h2><span id='topic+osc_sjoblom'></span>

<h3>Description</h3>

<p>Orthogonal signal correction (OSC) approach by Sjoblom et al.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  osc_sjoblom(x, y, center=TRUE,osc.ncomp=4,pls.ncomp=10,
              tol=1e-3,iter=20,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="osc_sjoblom_+3A_x">x</code></td>
<td>

<p>A numeric data frame or matrix to be pre-processed. 
</p>
</td></tr>
<tr><td><code id="osc_sjoblom_+3A_y">y</code></td>
<td>

<p>A vector or factor specifying the class for each observation.
</p>
</td></tr>
<tr><td><code id="osc_sjoblom_+3A_center">center</code></td>
<td>

<p>A logical value indicating whether the data set should be centred by column-wise.
</p>
</td></tr>
<tr><td><code id="osc_sjoblom_+3A_osc.ncomp">osc.ncomp</code></td>
<td>

<p>The number of components to be used in the OSC calculation.  
</p>
</td></tr>
<tr><td><code id="osc_sjoblom_+3A_pls.ncomp">pls.ncomp</code></td>
<td>

<p>The number of components to be used in the PLS calculation.  
</p>
</td></tr>
<tr><td><code id="osc_sjoblom_+3A_tol">tol</code></td>
<td>

<p>A scalar value of tolerance for OSC computation.
</p>
</td></tr>
<tr><td><code id="osc_sjoblom_+3A_iter">iter</code></td>
<td>

<p>The number of iteration used in OSC calculation.
</p>
</td></tr>
<tr><td><code id="osc_sjoblom_+3A_...">...</code></td>
<td>

<p>Arguments passed to or from other methods.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following components:
</p>
<table>
<tr><td><code>x</code></td>
<td>

<p>A matrix of OSC corrected data set.
</p>
</td></tr>
<tr><td><code>R2</code></td>
<td>

<p>R2 statistics. It is calculated as the fraction of variation in X after OSC correction.
</p>
</td></tr>
<tr><td><code>angle</code></td>
<td>

<p>An angle used for checking if scores <code>t</code> is orthogonal to <code>y</code>. An 
angle close to 90 degree means that orthogonality is achieved in the correction process.
</p>
</td></tr>
<tr><td><code>w</code></td>
<td>

<p>A matrix of OSC weights.
</p>
</td></tr>
<tr><td><code>p</code></td>
<td>

<p>A matrix of OSC loadings.
</p>
</td></tr>
<tr><td><code>t</code></td>
<td>

<p>A matrix of OSC scores.
</p>
</td></tr>
<tr><td><code>center</code></td>
<td>

<p>A logical value indicating whether the data set has been centred by column-wise.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>References</h3>

<p>Sjoblom. J., Svensson, O., Josefson, M., Kullberg, H., Wold, S. (1998). An evaluation of
orthogonal signal correction applied to calibration transfer of near infrared
spectra. <em>Chemometrics Intell. Lab. Syst.</em>,44: 229-244.
</p>
<p>Svensson, O., Kourti, T. and MacGregor, J.F. (2002). An investigation of orthogonal 
correction algorithms and their characteristics. <em>Journal of Chemometrics</em>, 16:176-188.
</p>
<p>Westerhuis, J. A., de Jong, S., Smilde, A, K. (2001). Direct orthogonal signal correction. 
<em>Chemometrics Intell. Lab. Syst.</em>, 56: 13-25.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+osc">osc</a></code>, <code><a href="#topic+predict.osc">predict.osc</a></code>, <code><a href="#topic+osc_wold">osc_wold</a></code>,
<code><a href="#topic+osc_wise">osc_wise</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(abr1)
cl   &lt;- factor(abr1$fact$class)
dat  &lt;- abr1$pos

## divide data as training and test data
idx &lt;- sample(1:nrow(dat), round((2/3)*nrow(dat)), replace=FALSE) 

## construct train and test data 
train.dat  &lt;- dat[idx,]
train.t    &lt;- cl[idx]
test.dat   &lt;- dat[-idx,]        
test.t     &lt;- cl[-idx] 

## build OSC model based on the training data
res &lt;- osc_sjoblom(train.dat, train.t)
names(res)

## pre-process test data by OSC
test.dat.1 &lt;- predict.osc(res,test.dat)$x

</code></pre>

<hr>
<h2 id='osc_wise'>
Orthogonal Signal Correction (OSC) Approach by Wise and Gallagher.
</h2><span id='topic+osc_wise'></span>

<h3>Description</h3>

<p>Orthogonal signal correction (OSC) approach by Wise and Gallagher.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  osc_wise(x, y, center=TRUE,osc.ncomp=4,pls.ncomp=10,
           tol=1e-3,iter=20,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="osc_wise_+3A_x">x</code></td>
<td>

<p>A numeric data frame or matrix to be pre-processed. 
</p>
</td></tr>
<tr><td><code id="osc_wise_+3A_y">y</code></td>
<td>

<p>A vector or factor specifying the class for each observation.
</p>
</td></tr>
<tr><td><code id="osc_wise_+3A_center">center</code></td>
<td>

<p>A logical value indicating whether the data set should be centred by column-wise.
</p>
</td></tr>
<tr><td><code id="osc_wise_+3A_osc.ncomp">osc.ncomp</code></td>
<td>

<p>The number of components to be used in the OSC calculation.  
</p>
</td></tr>
<tr><td><code id="osc_wise_+3A_pls.ncomp">pls.ncomp</code></td>
<td>

<p>The number of components to be used in the PLS calculation.  
</p>
</td></tr>
<tr><td><code id="osc_wise_+3A_tol">tol</code></td>
<td>

<p>A scalar value of tolerance for OSC computation.
</p>
</td></tr>
<tr><td><code id="osc_wise_+3A_iter">iter</code></td>
<td>

<p>The number of iteration used in OSC calculation.
</p>
</td></tr>
<tr><td><code id="osc_wise_+3A_...">...</code></td>
<td>

<p>Arguments passed to or from other methods.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following components:
</p>
<table>
<tr><td><code>x</code></td>
<td>

<p>A matrix of OSC corrected data set.
</p>
</td></tr>
<tr><td><code>R2</code></td>
<td>

<p>R2 statistics. It is calculated as the fraction of variation in X after OSC correction.
</p>
</td></tr>
<tr><td><code>angle</code></td>
<td>

<p>An angle used for checking if scores <code>t</code> is orthogonal to <code>y</code>. An 
angle close to 90 degree means that orthogonality is achieved in the correction process.
</p>
</td></tr>
<tr><td><code>w</code></td>
<td>

<p>A matrix of OSC weights.
</p>
</td></tr>
<tr><td><code>p</code></td>
<td>

<p>A matrix of OSC loadings.
</p>
</td></tr>
<tr><td><code>t</code></td>
<td>

<p>A matrix of OSC scores.
</p>
</td></tr>
<tr><td><code>center</code></td>
<td>

<p>A logical value indicating whether the data set has been centred by column-wise.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>References</h3>

<p>Westerhuis, J. A., de Jong, S., Smilde, A, K. (2001). Direct orthogonal signal correction. 
<em>Chemometrics Intell. Lab. Syst.</em>, 56: 13-25.
</p>
<p>Wise, B. M. and Gallagher, N.B. <em>http://www.eigenvector.com/MATLAB/OSC.html</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+osc">osc</a></code>, <code><a href="#topic+predict.osc">predict.osc</a></code>, <code><a href="#topic+osc_sjoblom">osc_sjoblom</a></code>,
<code><a href="#topic+osc_wold">osc_wold</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(abr1)
cl   &lt;- factor(abr1$fact$class)
dat  &lt;- abr1$pos

## divide data as training and test data
idx &lt;- sample(1:nrow(dat), round((2/3)*nrow(dat)), replace=FALSE) 

## construct train and test data 
train.dat  &lt;- dat[idx,]
train.t    &lt;- cl[idx]
test.dat   &lt;- dat[-idx,]        
test.t     &lt;- cl[-idx] 

## build OSC model based on the training data
res &lt;- osc_wise(train.dat, train.t)
names(res)

## pre-process test data by OSC
test.dat.1 &lt;- predict.osc(res,test.dat)$x

</code></pre>

<hr>
<h2 id='osc_wold'>
Orthogonal Signal Correction (OSC) Approach by Wold et al.
</h2><span id='topic+osc_wold'></span>

<h3>Description</h3>

<p>Orthogonal signal correction (OSC) approach by Wold et al.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  osc_wold(x, y, center=TRUE,osc.ncomp=4,pls.ncomp=10,
           tol=1e-3,iter=20,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="osc_wold_+3A_x">x</code></td>
<td>

<p>A numeric data frame or matrix to be pre-processed. 
</p>
</td></tr>
<tr><td><code id="osc_wold_+3A_y">y</code></td>
<td>

<p>A vector or factor specifying the class for each observation.
</p>
</td></tr>
<tr><td><code id="osc_wold_+3A_center">center</code></td>
<td>

<p>A logical value indicating whether the data set should be centred by column-wise.
</p>
</td></tr>
<tr><td><code id="osc_wold_+3A_osc.ncomp">osc.ncomp</code></td>
<td>

<p>The number of components to be used in the OSC calculation.  
</p>
</td></tr>
<tr><td><code id="osc_wold_+3A_pls.ncomp">pls.ncomp</code></td>
<td>

<p>The number of components to be used in the PLS calculation.  
</p>
</td></tr>
<tr><td><code id="osc_wold_+3A_tol">tol</code></td>
<td>

<p>A scalar value of tolerance for OSC computation.
</p>
</td></tr>
<tr><td><code id="osc_wold_+3A_iter">iter</code></td>
<td>

<p>The number of iteration used in OSC calculation.
</p>
</td></tr>
<tr><td><code id="osc_wold_+3A_...">...</code></td>
<td>

<p>Arguments passed to or from other methods.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following components:
</p>
<table>
<tr><td><code>x</code></td>
<td>

<p>A matrix of OSC corrected data set.
</p>
</td></tr>
<tr><td><code>R2</code></td>
<td>

<p>R2 statistics. It is calculated as the fraction of variation in X after OSC correction.
</p>
</td></tr>
<tr><td><code>angle</code></td>
<td>

<p>An angle used for checking if scores <code>t</code> is orthogonal to <code>y</code>. An 
angle close to 90 degree means that orthogonality is achieved in the correction process.
</p>
</td></tr>
<tr><td><code>w</code></td>
<td>

<p>A matrix of OSC weights.
</p>
</td></tr>
<tr><td><code>p</code></td>
<td>

<p>A matrix of OSC loadings.
</p>
</td></tr>
<tr><td><code>t</code></td>
<td>

<p>A matrix of OSC scores.
</p>
</td></tr>
<tr><td><code>center</code></td>
<td>

<p>A logical value indicating whether the data set has been centred by column-wise.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>References</h3>

<p>Wold, S., Antti, H., Lindgren, F., Ohman, J.(1998). Orthogonal signal correction of nearinfrared
spectra. <em>Chemometrics Intell. Lab. Syst.</em>, 44: 175-185.
</p>
<p>Svensson, O., Kourti, T. and MacGregor, J.F. (2002). An investigation of orthogonal 
correction algorithms and their characteristics. <em>Journal of Chemometrics</em>, 16:176-188.
</p>
<p>Westerhuis, J. A., de Jong, S., Smilde, A, K. (2001). Direct orthogonal signal correction. 
<em>Chemometrics Intell. Lab. Syst.</em>, 56: 13-25.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+osc">osc</a></code>, <code><a href="#topic+predict.osc">predict.osc</a></code>, <code><a href="#topic+osc_sjoblom">osc_sjoblom</a></code>,
<code><a href="#topic+osc_wise">osc_wise</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(abr1)
cl   &lt;- factor(abr1$fact$class)
dat  &lt;- abr1$pos

## divide data as training and test data
idx &lt;- sample(1:nrow(dat), round((2/3)*nrow(dat)), replace=FALSE) 

## construct train and test data 
train.dat  &lt;- dat[idx,]
train.t    &lt;- cl[idx]
test.dat   &lt;- dat[-idx,]        
test.t     &lt;- cl[-idx] 

## build OSC model based on the training data
res &lt;- osc_wold(train.dat, train.t)
names(res)

## pre-process test data by OSC
test.dat.1 &lt;- predict.osc(res,test.dat)$x

</code></pre>

<hr>
<h2 id='panel.elli'> Panel Function for Plotting Ellipse and outlier </h2><span id='topic+panel.elli'></span><span id='topic+panel.outl'></span><span id='topic+panel.elli.1'></span>

<h3>Description</h3>

<p><span class="pkg">lattice</span> panel functions for plotting grouped ellipse and outlier
</p>


<h3>Usage</h3>

<pre><code class='language-R'>panel.elli(x, y, groups = NULL,conf.level = 0.975, ...)
panel.elli.1(x, y, subscripts, groups=NULL, conf.level = 0.975,
             ep=0, com.grp=NULL, no.grp=NULL, ell.grp=NULL, ...)
panel.outl(x, y, subscripts, groups=NULL, conf.level = 0.975, labs, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="panel.elli_+3A_x">x</code>, <code id="panel.elli_+3A_y">y</code></td>
<td>
<p>Variables to be plotted.</p>
</td></tr>
<tr><td><code id="panel.elli_+3A_conf.level">conf.level</code></td>
<td>
<p>Confident level for ellipse</p>
</td></tr>
<tr><td><code id="panel.elli_+3A_groups">groups</code>, <code id="panel.elli_+3A_subscripts">subscripts</code></td>
<td>
<p> Internal parameters for Lattice.</p>
</td></tr>
<tr><td><code id="panel.elli_+3A_labs">labs</code></td>
<td>
<p>Labels for potential outliers.</p>
</td></tr>
<tr><td><code id="panel.elli_+3A_ep">ep</code></td>
<td>
<p> An integer for plotting ellipse. <code>1</code> and <code>2</code> for 
plotting overall and group ellipse, respectively. Otherwise, none. </p>
</td></tr>
<tr><td><code id="panel.elli_+3A_com.grp">com.grp</code></td>
<td>
<p>A list of characters to select which combination of
groups to be plotted.</p>
</td></tr>
<tr><td><code id="panel.elli_+3A_no.grp">no.grp</code></td>
<td>
<p>A list of characters to select which individual group
not to be plotted. Note it will be overridden by <code>com.grp</code>.
If no <code>com.grp</code> and <code>no.grp</code>, ellipses of each individual
group  will be plotted.</p>
</td></tr>
<tr><td><code id="panel.elli_+3A_ell.grp">ell.grp</code></td>
<td>
<p>Another categorical vector used for plotting ellipse.
If provided, <code>ep</code>,<code>com.grp</code> and <code>no.grp</code> will be
ignored. It should be different from default <code>groups</code>, but has
the same length of <code>groups</code>. For details, see example below.</p>
</td></tr>
<tr><td><code id="panel.elli_+3A_...">...</code></td>
<td>
<p>Further arguments. See corresponding entry in
<code><a href="lattice.html#topic+xyplot">xyplot</a></code> for non-trivial details.  </p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>panel.elli</code> is modified from function
<code><a href="latticeExtra.html#topic+panel.ellipse">panel.ellipse</a></code> in package <span class="pkg">latticeExtra</span>.
</p>
<p><code>panel.elli.1</code> gives more control on how to plot ellipse for the
current group. It also provides an option to plot ellipse based on
another user-defined groups.
</p>
<p><code>panel.outl</code> plots the labels of data points outside the ellipse.
These data points can be treated as potential outliers.
</p>


<h3>Value</h3>

<p>Retuns objects of class <code>"trellis"</code>.
</p>


<h3>Note</h3>

 <p><code>panel.elli.1</code> can be called by functions <code>grpplot</code>,
<code>pcaplot</code>, <code>mdsplot</code>, <code>pca_plot_wrap</code>,
<code>mds_plot_wrap</code>, <code>pls_plot_wrap</code> and <code>lda_plot_wrap</code> by
passing argument of <code>ep</code>. See examples of these function for
details. </p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+grpplot">grpplot</a></code>, <code><a href="#topic+pcaplot">pcaplot</a></code>, <code><a href="#topic+mdsplot">mdsplot</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lattice) 
data(iris)

## =====================================================================
## Examples of calling 'panel.elli' and 'panel.outl'
xyplot(Sepal.Length ~ Petal.Length, data = iris, groups=Species,
        par.settings = list(superpose.symbol = list(pch=c(15:17)),
                            superpose.line = list(lwd=2, lty=1:3)),
        panel = function(x, y, ...) {
            panel.xyplot(x, y, ...)
            panel.elli(x, y, ..., type="l",lwd=2)
            panel.outl(x,y, ...)
        },
        auto.key = list(x = .1, y = .8, corner = c(0, 0)),
        labs=rownames(iris), conf.level=0.9,adj = -0.5)

## Without groups
xyplot(Sepal.Length ~ Petal.Length, data = iris,
        par.settings = list(plot.symbol = list(cex = 1.1, pch=16)),
        panel = function(x, y, ...) {
            panel.xyplot(x, y, ...)
            panel.elli(x, y, ..., type="l", lwd = 2)
            panel.outl(x,y, ...)
        },
        auto.key = list(x = .1, y = .8, corner = c(0, 0)),
        labs=rownames(iris), conf.level=0.9,adj = -0.5)


## With conditioning
xyplot(Sepal.Length ~ Petal.Length|Species, data = iris, 
        par.settings = list(plot.symbol = list(cex = 1.1, pch=16)),
        layout=c(2,2),
        panel = function(x, y, ...) {
            panel.xyplot(x, y, ...)
            panel.elli(x, y, ..., type="l", lwd = 2)
            panel.outl(x,y, ...)
        },
        auto.key = list(x = .6, y = .8, corner = c(0, 0)),
        adj = 0,labs=rownames(iris), conf.level=0.95)

## =====================================================================
## Examples of 'panel.elli.1'
xyplot(Sepal.Length ~ Petal.Length, data = iris, groups=Species,
        ## ---------------------------------------------------
        ## Select what to be plotted.
        ep=2,
        ## com.grp = list(a="setosa",b=c("versicolor", "virginica")),
        ## no.grp = "setosa", ## Not draw ellipse for "setosa"
        ## ---------------------------------------------------
        par.settings = list(superpose.symbol = list(pch=c(15:17)),
                            superpose.line = list(lwd=2, lty=1:3)),
        panel = function(x, y, ...) {
            panel.xyplot(x, y, ...)
            panel.elli.1(x, y, ...)
            panel.outl(x,y, ...)
        },
        auto.key = list(points = TRUE, rectangles = FALSE, space = "right"),
        adj = 0,labs=rownames(iris), conf.level=0.95)

xyplot(Sepal.Length ~ Petal.Length, data = iris, groups=Species,
        ## ---------------------------------------------------
        ## Select what to be plotted.
        ep=2,
        ## com.grp = list(a="setosa",b=c("versicolor", "virginica")),
        no.grp = c("setosa","versicolor"),## Only draw "virginica"
        ## ---------------------------------------------------
        par.settings = list(superpose.symbol = list(pch=c(15:17)),
                            superpose.line = list(lwd=2, lty=1:3)),
        panel = function(x, y, ...) {
            panel.xyplot(x, y, ...)
            panel.elli.1(x, y, ...)
        },
        auto.key = list(x = .1, y = .8, corner = c(0, 0)))

xyplot(Sepal.Length ~ Petal.Length, data = iris, groups=Species,
        ## ---------------------------------------------------
        ## Select what to be plotted.
        ep=2,
        com.grp = list(a="setosa",b=c("versicolor", "virginica")),
        ## no.grp = "setosa", ## Not draw ellipse for "setosa"
        ## ---------------------------------------------------
        par.settings = list(superpose.symbol = list(pch=c(15:17)),
                            superpose.line = list(lwd=2, lty=1:3)),
        panel = function(x, y, ...) {
            panel.xyplot(x, y, ...)
            panel.elli.1(x, y, ...)
        },
        auto.key = list(x = .1, y = .8, corner = c(0, 0)))

  xyplot(Sepal.Length ~ Petal.Length, data = iris, groups=Species, ep=1,
        par.settings = list(superpose.symbol = list(pch=c(15:17)),
                            superpose.line = list(lwd=2, lty=1:3)),
        panel = function(x, y, ...) {
            panel.xyplot(x, y, ...)
            panel.elli.1(x, y, ...)
        },
        auto.key = list(points = TRUE, rectangles = FALSE, space = "right"))

## =====================================================================
## Another data set from package MASS
require(MASS)
data(Cars93)

## Plot ellipse based on original groups: DriveTrain
xyplot(Price~EngineSize, data=Cars93, groups=DriveTrain, ep=2,
        par.settings = list(superpose.symbol = list(pch=c(15:17)),
                            superpose.line = list(lwd=2, lty=1:3)),
        panel = function(x, y, ...) {
            panel.xyplot(x, y, ...)
            panel.elli.1(x, y, ...)
        },
        auto.key = list(points = TRUE, rectangles = FALSE, space = "right"))

## But we want to plot ellipse using AirBags
xyplot(Price~EngineSize, data=Cars93, groups=DriveTrain, 
        ell.grp=Cars93$AirBags,
        par.settings = list(superpose.symbol = list(pch=c(15:17)),
                            superpose.line = list(lwd=2, lty=1:3)),
        panel = function(x, y, ...) {
            panel.xyplot(x, y, ...)
            panel.elli.1(x, y, ...)
        },
        auto.key = list(points = TRUE, rectangles = FALSE, space = "right"))

</code></pre>

<hr>
<h2 id='panel.smooth.line'> Panel Function for Plotting Regression Line</h2><span id='topic+panel.smooth.line'></span>

<h3>Description</h3>

<p><span class="pkg">lattice</span> panel function for plotting regression line with red colour. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>panel.smooth.line(x, y,...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="panel.smooth.line_+3A_x">x</code>, <code id="panel.smooth.line_+3A_y">y</code></td>
<td>
<p> Variables to be plotted.</p>
</td></tr>
<tr><td><code id="panel.smooth.line_+3A_...">...</code></td>
<td>
<p>Further arguments. See corresponding entry in
<code><a href="lattice.html#topic+xyplot">xyplot</a></code> for non-trivial details.  </p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"trellis"</code>.
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin </p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lattice) 
data(iris)
splom(~iris[,1:4], varname.cex = 1.0, pscales = 0, panel = panel.smooth.line)
</code></pre>

<hr>
<h2 id='pca.outlier'> Outlier detection by PCA </h2><span id='topic+pca.outlier'></span><span id='topic+pca.outlier.1'></span>

<h3>Description</h3>

<p>Outlier detection by the Mahalanobis distances of PC1 and PC2. Also
plot PC1 and PC2 with its confidence ellipse.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  pca.outlier(x, center = TRUE, scale=TRUE,conf.level = 0.975,...) 

  pca.outlier.1(x, center = TRUE, scale=TRUE, conf.level = 0.975, 
              group=NULL, main = "PCA", cex=0.7,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pca.outlier_+3A_x">x</code></td>
<td>
<p> A data frame or matrix. </p>
</td></tr>
<tr><td><code id="pca.outlier_+3A_center">center</code></td>
<td>
<p> A logical value indicating whether the variables should
be shifted to be zero centred before PCA analysis takes place. </p>
</td></tr>
<tr><td><code id="pca.outlier_+3A_scale">scale</code></td>
<td>
<p> A logical value indicating whether the variables should
be scaled to have unit variance before PCA analysis takes place. </p>
</td></tr>
<tr><td><code id="pca.outlier_+3A_conf.level">conf.level</code></td>
<td>
<p> The confidence level for controlling the cutoff of
the Mahalanobis distances. </p>
</td></tr>
<tr><td><code id="pca.outlier_+3A_group">group</code></td>
<td>
<p> A string character or factor indicating group
information of row of <code>x</code>. It is used only for plotting. </p>
</td></tr>
<tr><td><code id="pca.outlier_+3A_main">main</code></td>
<td>
<p>An overall title for PCA plot.</p>
</td></tr>
<tr><td><code id="pca.outlier_+3A_cex">cex</code></td>
<td>
<p> A numerical value giving the amount by which plotting text
and symbols should be magnified relative to the default. </p>
</td></tr>
<tr><td><code id="pca.outlier_+3A_...">...</code></td>
<td>
<p>Further arguments for plotting</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>plot</code></td>
<td>
<p>plot object of class <code>"trellis"</code> by
<code>pca.outlier</code> only. </p>
</td></tr>
<tr><td><code>outlier</code></td>
<td>
<p>Outliers detected.</p>
</td></tr>
<tr><td><code>conf.level</code></td>
<td>
<p>Confidence level used.</p>
</td></tr>
<tr><td><code>mah.dist</code></td>
<td>
<p>Mahalanobis distances of each data sample.</p>
</td></tr>
<tr><td><code>cutoff</code></td>
<td>
<p> Cutoff of Mahalanobis distances used for outlier detection.</p>
</td></tr>
</table>


<h3>Note</h3>

<p> Examples of <code><a href="#topic+panel.elli">panel.elli</a></code> and <code><a href="#topic+panel.outl">panel.outl</a></code>
give more general information about ellipses and outliers. If you
ONLY want to plot outliers based on PCA in a general way, for
example, outliers in different groups or in conditional panel, you can
write an wrapper function combining with <code><a href="#topic+pca.comp">pca.comp</a></code>,
<code><a href="#topic+panel.elli">panel.elli</a></code> and <code><a href="#topic+panel.outl">panel.outl</a></code>. It is quite
similiar to the implementation of <code><a href="#topic+pca_plot_wrap">pca_plot_wrap</a></code>.
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pcaplot">pcaplot</a></code>, <code><a href="#topic+grpplot">grpplot</a></code>,
<code><a href="#topic+panel.outl">panel.outl</a></code>,<code><a href="#topic+panel.elli">panel.elli</a></code>,
<code><a href="#topic+pca_plot_wrap">pca_plot_wrap</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>  data(iris)

  ## call lattice version
  pca.outlier(iris[,1:4], adj=-0.5)
  ## plot group
  pca.outlier(iris[,1:4], adj=-0.5,groups=iris[,5])
  ## more information about groups
  pca.outlier(iris[,1:4],groups=iris[,5],adj = -0.5, xlim=c(-5, 5),
                auto.key = list(x = .05, y = .9, corner = c(0, 0)),
                par.settings = list(superpose.symbol=list(pch=rep(1:25))))

  ## call basic graphic version
  pca.outlier.1(iris[,1:4])
  ## plot group
  pca.outlier.1(iris[,1:4], group=iris[,5])

</code></pre>

<hr>
<h2 id='pcalda'>
Classification with PCADA
</h2><span id='topic+pcalda'></span><span id='topic+pcalda.default'></span><span id='topic+pcalda.formula'></span><span id='topic+print.pcalda'></span><span id='topic+summary.pcalda'></span><span id='topic+print.summary.pcalda'></span>

<h3>Description</h3>

<p>Classification with combination of principal component analysis (PCA) and linear discriminant 
analysis (LDA).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pcalda(x, ...)

## Default S3 method:
pcalda(x, y, center = TRUE, scale. = FALSE, ncomp = NULL,
       tune=FALSE,...)

## S3 method for class 'formula'
pcalda(formula, data = NULL, ..., subset, na.action = na.omit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pcalda_+3A_formula">formula</code></td>
<td>

<p>A formula of the form <code>groups ~ x1 + x2 + ...</code>  That is, the
response is the grouping factor and the right hand side specifies
the (non-factor) discriminators.
</p>
</td></tr>
<tr><td><code id="pcalda_+3A_data">data</code></td>
<td>

<p>Data frame from which variables specified in <code>formula</code> are
preferentially to be taken.
</p>
</td></tr>
<tr><td><code id="pcalda_+3A_x">x</code></td>
<td>

<p>A matrix or data frame containing the explanatory variables if no formula is
given as the principal argument.
</p>
</td></tr>
<tr><td><code id="pcalda_+3A_y">y</code></td>
<td>

<p>A factor specifying the class for each observation if no formula principal 
argument is given.
</p>
</td></tr>
<tr><td><code id="pcalda_+3A_center">center</code></td>
<td>

<p>A logical value indicating whether <code>x</code> should be shifted to zero 
centred by column-wise.
</p>
</td></tr>
<tr><td><code id="pcalda_+3A_scale.">scale.</code></td>
<td>

<p>A logical value indicating whether <code>x</code> should be scaled to have unit 
variance by column-wise before the analysis takes place. 
</p>
</td></tr>
<tr><td><code id="pcalda_+3A_ncomp">ncomp</code></td>
<td>

<p>The number of principal components to be used in the classification. If
<code>NULL</code> and <code>tune=TRUE</code>, it is the row number of <code>x</code> minus the 
number of class indicating in <code>y</code>. If <code>NULL</code> and <code>tune=FALSE</code>, 
it is the half of row number of <code>x</code>.
</p>
</td></tr>
<tr><td><code id="pcalda_+3A_tune">tune</code></td>
<td>

<p>A logical value indicating whether the best number of components should be tuned.
</p>
</td></tr>
<tr><td><code id="pcalda_+3A_...">...</code></td>
<td>

<p>Arguments passed to or from other methods.
</p>
</td></tr>
<tr><td><code id="pcalda_+3A_subset">subset</code></td>
<td>

<p>An index vector specifying the cases to be used in the training
sample.  
</p>
</td></tr>
<tr><td><code id="pcalda_+3A_na.action">na.action</code></td>
<td>

<p>A function to specify the action to be taken if <code>NA</code>s are found. The 
default action is <code>na.omit</code>, which leads to rejection of cases with 
missing values on any required variable. An alternative is <code>na.fail</code>, 
which causes an error if <code>NA</code> cases are found. 
</p>
</td></tr>	
</table>


<h3>Details</h3>

<p>A critical issue of applying linear discriminant analysis (LDA) is both the
singularity and instability of the within-class scatter matrix. In practice, 
there are often a large number of features available, but the total number of 
training patterns is limited and commonly less than the dimension of the feature 
space. To tackle this issue, <code>pcalda</code> combines PCA and LDA for 
classification. It uses PCA for dimension reduction. The rotated data resulted 
from PCA will be the input variable to LDA for classification. 
</p>


<h3>Value</h3>

<p>An object of class <code>pcalda</code> containing the following components:
</p>
<table>
<tr><td><code>x</code></td>
<td>

<p>The rotated data on discriminant variables.
</p>
</td></tr>
<tr><td><code>cl</code></td>
<td>

<p>The observed class labels of training data. 
</p>
</td></tr>
<tr><td><code>pred</code></td>
<td>

<p>The predicted class labels of training data. 
</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>

<p>The posterior probabilities for the predicted classes. 
</p>
</td></tr>
<tr><td><code>conf</code></td>
<td>

<p>The confusion matrix based on training data. 
</p>
</td></tr>
<tr><td><code>acc</code></td>
<td>

<p>The accuracy rate of training data. 
</p>
</td></tr>
<tr><td><code>ncomp</code></td>
<td>

<p>The number of principal components used for classification. 
</p>
</td></tr>
<tr><td><code>pca.out</code></td>
<td>

<p>The output of PCA.
</p>
</td></tr>
<tr><td><code>lda.out</code></td>
<td>

<p>The output of LDA.
</p>
</td></tr>
<tr><td><code>call</code></td>
<td>

<p>The (matched) function call.
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function may be called giving either a formula and
optional data frame, or a matrix and grouping factor as the first
two arguments. 
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.pcalda">predict.pcalda</a></code>, <code><a href="#topic+plot.pcalda">plot.pcalda</a></code>, <code><a href="#topic+tune.func">tune.func</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(abr1)
cl   &lt;- factor(abr1$fact$class)
dat  &lt;- abr1$pos

## divide data as training and test data
idx &lt;- sample(1:nrow(dat), round((2/3)*nrow(dat)), replace=FALSE) 

## construct train and test data 
train.dat  &lt;- dat[idx,]
train.t    &lt;- cl[idx]
test.dat   &lt;- dat[-idx,]        
test.t     &lt;- cl[-idx] 

## apply pcalda
model    &lt;- pcalda(train.dat,train.t)
model
summary(model)

## plot
plot(model,dimen=c(1,2),main = "Training data",abbrev = TRUE)
plot(model,main = "Training data",abbrev = TRUE)

## confusion matrix
pred.te  &lt;- predict(model, test.dat)$class
table(test.t,pred.te)

</code></pre>

<hr>
<h2 id='pcaplot'>Plot Function for PCA with Grouped Values</h2><span id='topic+pcaplot'></span><span id='topic+pca.plot'></span><span id='topic+pca.comp'></span>

<h3>Description</h3>

<p>Plot function for PCA with grouped values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pcaplot(x, y, scale = TRUE, pcs = 1:2, ...)

pca.plot(x, y, scale=TRUE, abbrev = FALSE, ep.plot=FALSE,...)

pca.comp(x, scale=FALSE, pcs=1:2,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pcaplot_+3A_x">x</code></td>
<td>
<p>A matrix or data frame to be plotted. </p>
</td></tr>
<tr><td><code id="pcaplot_+3A_y">y</code></td>
<td>
<p>A factor or vector giving group information of columns of
<code>x</code>.</p>
</td></tr>
<tr><td><code id="pcaplot_+3A_scale">scale</code></td>
<td>
<p> A logical value indicating whether the data set <code>x</code>
should be scaled. </p>
</td></tr>
<tr><td><code id="pcaplot_+3A_pcs">pcs</code></td>
<td>
<p>A vector of index of PCs to be plotted.</p>
</td></tr>
<tr><td><code id="pcaplot_+3A_ep.plot">ep.plot</code></td>
<td>
<p> A logical value indicating whether the ellipse should
be plotted. </p>
</td></tr>
<tr><td><code id="pcaplot_+3A_abbrev">abbrev</code></td>
<td>
<p> Whether the group labels are abbreviated on the plots.
If <code>abbrev &gt; 0</code> this gives <code>minlength</code> in the call to
<code>abbreviate</code>. </p>
</td></tr>
<tr><td><code id="pcaplot_+3A_...">...</code></td>
<td>
<p> Further arguments to <code><a href="stats.html#topic+prcomp">prcomp</a></code> or
<code>lattice</code>. See corresponding entry in <code><a href="lattice.html#topic+xyplot">xyplot</a></code> for
non-trivial details of <code>lattice</code>. For <code>pcaplot</code>, one
argument is <code>ep</code>: an integer for plotting ellipse. <code>1</code> and
<code>2</code> for plotting overall and group ellipse, respectively.
Otherwise, none. For details, see <code><a href="#topic+panel.elli.1">panel.elli.1</a></code>. </p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>pcaplot</code> returns an object of class <code>"trellis"</code>.
</p>
<p><code>pca.comp</code> returns a list with components:
</p>
<table>
<tr><td><code>scores</code></td>
<td>
<p> PCA scores</p>
</td></tr>
<tr><td><code>vars</code></td>
<td>
<p>Proportion of variance</p>
</td></tr>
<tr><td><code>varsn</code></td>
<td>
<p>A vector of string indicating the percentage of variance.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Number of columns of <code>x</code> must be larger than 1. <code>pcaplot</code> uses
<code>lattice</code> to plot PCA while <code>pca.plot</code> uses the basic graphics 
to do so. <code>pca.plot</code> plots PC1 and PC2 only.
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+grpplot">grpplot</a></code>, <code><a href="#topic+panel.elli.1">panel.elli.1</a></code>,
<code><a href="#topic+pca_plot_wrap">pca_plot_wrap</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## examples of 'pcaplot'
data(iris)
pcaplot(iris[,1:4], iris[,5],pcs=c(2,1),ep=2)
## change confidence interval (see 'panel.elli.1')
pcaplot(iris[,1:4], iris[,5],pcs=c(1,2),ep=2, conf.level = 0.9)

pcaplot(iris[,1:4], iris[,5],pcs=c(2,1),ep=1,
        auto.key=list(space="top", columns=3))
pcaplot(iris[,1:4], iris[,5],pcs=c(1,3,4))
tmp &lt;- pcaplot(iris[,1:4], iris[,5],pcs=1:3,ep=2)
tmp

## change symbol's color, type and size
pcaplot(iris[,1:4], iris[,5],pcs=c(2,1),main="IRIS DATA", cex=1.2,
  auto.key=list(space="right", col=c("black","blue","red"), cex=1.2),
  par.settings = list(superpose.symbol = list(col=c("black","blue","red"),
                                              pch=c(1:3))))

## compare pcaplot and pca.plot. 
pcaplot(iris[,1:4], iris[,5],pcs=c(1,2),ep=2)
pca.plot(iris[,1:4], iris[,5], ep.plot = TRUE)

## an example of 'pca.comp'
pca.comp(iris[,1:4], scale = TRUE, pcs=1:3)

</code></pre>

<hr>
<h2 id='plot.accest'>
Plot Method for Class 'accest'
</h2><span id='topic+plot.accest'></span>

<h3>Description</h3>

<p>Plot accuracy rate of each iteration.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'accest'
plot(x, main = NULL, xlab = NULL, ylab = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.accest_+3A_x">x</code></td>
<td>

<p>An object of class <code>accest</code>.
</p>
</td></tr>
<tr><td><code id="plot.accest_+3A_main">main</code></td>
<td>

<p>An overall title for the plot.  
</p>
</td></tr>
<tr><td><code id="plot.accest_+3A_xlab">xlab</code></td>
<td>

<p>A title for the x axis.
</p>
</td></tr>
<tr><td><code id="plot.accest_+3A_ylab">ylab</code></td>
<td>

<p>A title for the y axis.
</p>
</td></tr>
<tr><td><code id="plot.accest_+3A_...">...</code></td>
<td>

<p>Additional arguments to the plot. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a method for the generic function <code>plot()</code> for class 
<code>accest</code>. It plots the accuracy rate against the index of iterations. 
</p>


<h3>Value</h3>

<p>Returns plot of class <code>accest</code>.
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+accest">accest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Iris data
data(iris)
# Stratified cross-validation of PCALDA for Iris data
pars &lt;- valipars(sampling="cv", niter=10, nreps=10, strat=TRUE)
acc  &lt;- accest(Species~., data = iris, method = "pcalda", pars = pars)
              
acc
summary(acc)
plot(acc)
</code></pre>

<hr>
<h2 id='plot.maccest'>
Plot Method for Class 'maccest'
</h2><span id='topic+plot.maccest'></span>

<h3>Description</h3>

<p>Plot accuracy rate with standard derivation of each classifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'maccest'
plot(x, main = NULL, xlab = NULL, ylab = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.maccest_+3A_x">x</code></td>
<td>

<p>An object of class <code>maccest</code>.
</p>
</td></tr>
<tr><td><code id="plot.maccest_+3A_main">main</code></td>
<td>

<p>An overall title for the plot.  
</p>
</td></tr>
<tr><td><code id="plot.maccest_+3A_xlab">xlab</code></td>
<td>

<p>A title for the x axis.
</p>
</td></tr>
<tr><td><code id="plot.maccest_+3A_ylab">ylab</code></td>
<td>

<p>A title for the y axis.
</p>
</td></tr>
<tr><td><code id="plot.maccest_+3A_...">...</code></td>
<td>

<p>Additional arguments to the plot. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a method for the generic function <code>plot()</code> for class 
<code>maccest</code>. It plots the accuracy rate with standard derivation against the 
classifiers. 
</p>


<h3>Value</h3>

<p>Returns plot of class <code>maccest</code>.
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+maccest">maccest</a></code>, <code><a href="#topic+boxplot.maccest">boxplot.maccest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Iris data
data(iris)
x      &lt;- subset(iris, select = -Species)
y      &lt;- iris$Species

method &lt;- c("randomForest","svm","pcalda","knn")
pars   &lt;- valipars(sampling="boot", niter = 10, nreps=4)
res    &lt;- maccest(x, y, method=method, pars=pars,
                  comp="anova",kernel="linear") 

res
plot(res)
</code></pre>

<hr>
<h2 id='plot.pcalda'>
Plot Method for Class 'pcalda'
</h2><span id='topic+plot.pcalda'></span>

<h3>Description</h3>

<p>Plot linear discriminants of <code>pcalda</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pcalda'
plot(x, dimen, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.pcalda_+3A_x">x</code></td>
<td>
<p>An object of class <code>pcalda</code>. </p>
</td></tr>
<tr><td><code id="plot.pcalda_+3A_dimen">dimen</code></td>
<td>
<p> The index of linear discriminants to be used for the plot. </p>
</td></tr>
<tr><td><code id="plot.pcalda_+3A_...">...</code></td>
<td>
<p> Further arguments.  See corresponding entry in
<code><a href="lattice.html#topic+xyplot">xyplot</a></code> for non-trivial details. One argument is
<code>ep</code>: an integer for plotting ellipse. <code>1</code> and <code>2</code> for 
plotting overall and group ellipse, respectively. Otherwise, none.
For details, see <code><a href="#topic+panel.elli.1">panel.elli.1</a></code>. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a method for the generic function <code>plot()</code> for
class <code>pcalda</code>. If the length of <code>dimen</code> is greater
than 2, a pairs plot is used. If the length of <code>dimen</code> is equal
to 2, a scatter plot is drawn. Otherwise, the dot plot is drawn for
the single component.
</p>


<h3>Value</h3>

<p>An object of class <code>"trellis"</code>.</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pcalda">pcalda</a></code>, <code><a href="#topic+predict.pcalda">predict.pcalda</a></code>,
<code><a href="#topic+lda_plot_wrap">lda_plot_wrap</a></code>,<code><a href="#topic+panel.elli.1">panel.elli.1</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(abr1)
cl   &lt;- factor(abr1$fact$class)
dat  &lt;- abr1$pos

model &lt;- pcalda(dat,cl)

## Second component versus first
plot(model,dimen=c(1,2),main = "Training data",ep=2)
## Pairwise scatterplots of several components 
plot(model,main = "Training data",ep=1)

## The first component
plot(model,dimen=c(1),main = "Training data")
</code></pre>

<hr>
<h2 id='plot.plsc'>
Plot Method for Class 'plsc' or 'plslda'
</h2><span id='topic+plot.plsc'></span><span id='topic+plot.plslda'></span>

<h3>Description</h3>

<p>Plot latent components of <code>plsc</code> or <code>plslda</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'plsc'
plot(x, dimen, ...)

## S3 method for class 'plslda'
plot(x, dimen, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.plsc_+3A_x">x</code></td>
<td>
<p> An object of class <code>plsc</code> or <code>plslda</code>. </p>
</td></tr>
<tr><td><code id="plot.plsc_+3A_dimen">dimen</code></td>
<td>
<p> The index of latent components to be used for the plot. </p>
</td></tr>
<tr><td><code id="plot.plsc_+3A_...">...</code></td>
<td>
<p> Further arguments.  See corresponding entry in
<code><a href="lattice.html#topic+xyplot">xyplot</a></code> for non-trivial details. One argument is
<code>ep</code>: an integer for plotting ellipse. <code>1</code> and <code>2</code> for 
plotting overall and group ellipse, respectively. Otherwise, none.
For details, see <code><a href="#topic+panel.elli.1">panel.elli.1</a></code>. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Two functions are methods for the generic function <code>plot()</code> of
class <code>plsc</code> and <code>plslda</code>. 
</p>
<p>If the length of <code>dimen</code> is greater than 2, a pairs plot is used.
If the length of <code>dimen</code> is equal to 2, a scatter plot is drawn.
Otherwise, the dot plot is drawn for the single component.
</p>


<h3>Value</h3>

<p>An object of class <code>"trellis"</code>.</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plsc">plsc</a></code>, <code><a href="#topic+predict.plsc">predict.plsc</a></code>,<code><a href="#topic+plslda">plslda</a></code>, 
<code><a href="#topic+predict.plslda">predict.plslda</a></code>, <code><a href="#topic+pls_plot_wrap">pls_plot_wrap</a></code>,
<code><a href="#topic+panel.elli.1">panel.elli.1</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(abr1)
cl   &lt;- factor(abr1$fact$class)
dat  &lt;- abr1$pos

mod.plsc    &lt;- plsc(dat,cl,ncomp=4)
mod.plslda  &lt;- plslda(dat,cl,ncomp=4)

## Second component versus first
plot(mod.plsc,dimen=c(1,2),main = "Training data", ep = 2)
plot(mod.plslda,dimen=c(1,2),main = "Training data", ep = 2)

## Pairwise scatterplots of several components 
plot(mod.plsc, main = "Training data", ep = 1)
plot(mod.plslda, main = "Training data", ep = 1)

## single component
plot(mod.plsc,dimen=c(1),main = "Training data")
plot(mod.plslda,dimen=c(1),main = "Training data")

</code></pre>

<hr>
<h2 id='plsc'>
Classification with PLSDA
</h2><span id='topic+plsc'></span><span id='topic+plsc.default'></span><span id='topic+plsc.formula'></span><span id='topic+print.plsc'></span><span id='topic+summary.plsc'></span><span id='topic+print.summary.plsc'></span><span id='topic+plslda'></span><span id='topic+plslda.default'></span><span id='topic+plslda.formula'></span><span id='topic+print.plslda'></span><span id='topic+summary.plslda'></span><span id='topic+print.summary.plslda'></span>

<h3>Description</h3>

<p>Classification with partial least squares (PLS) or PLS plus linear discriminant 
analysis (LDA).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plsc(x, ...)

plslda(x, ...)

## Default S3 method:
plsc(x, y, pls="simpls",ncomp=10, tune=FALSE,...)

## S3 method for class 'formula'
plsc(formula, data = NULL, ..., subset, na.action = na.omit)

## Default S3 method:
plslda(x, y, pls="simpls",ncomp=10, tune=FALSE,...)

## S3 method for class 'formula'
plslda(formula, data = NULL, ..., subset, na.action = na.omit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plsc_+3A_formula">formula</code></td>
<td>

<p>A formula of the form <code>groups ~ x1 + x2 + ...</code>  That is, the
response is the grouping factor and the right hand side specifies
the (non-factor) discriminators.
</p>
</td></tr>
<tr><td><code id="plsc_+3A_data">data</code></td>
<td>

<p>Data frame from which variables specified in <code>formula</code> are
preferentially to be taken.
</p>
</td></tr>
<tr><td><code id="plsc_+3A_x">x</code></td>
<td>

<p>A matrix or data frame containing the explanatory variables if no formula is
given as the principal argument.
</p>
</td></tr>
<tr><td><code id="plsc_+3A_y">y</code></td>
<td>

<p>A factor specifying the class for each observation if no formula principal 
argument is given.
</p>
</td></tr>
<tr><td><code id="plsc_+3A_pls">pls</code></td>
<td>

<p>A method for calculating PLS scores and loadings. The following methods are supported:
</p>

<ul>
<li> <p><code>simpls:</code> SIMPLS algorithm. 
</p>
</li>
<li> <p><code>kernelpls:</code> kernel algorithm.
</p>
</li>
<li> <p><code>oscorespls:</code> orthogonal scores algorithm. 
</p>
</li></ul>

<p>For details, see <code><a href="pls.html#topic+simpls.fit">simpls.fit</a></code>, <code><a href="pls.html#topic+kernelpls.fit">kernelpls.fit</a></code> and
<code><a href="pls.html#topic+oscorespls.fit">oscorespls.fit</a></code> in package <span class="pkg">pls</span>.
</p>
</td></tr>
<tr><td><code id="plsc_+3A_ncomp">ncomp</code></td>
<td>

<p>The number of components to be used in the classification.  
</p>
</td></tr>
<tr><td><code id="plsc_+3A_tune">tune</code></td>
<td>

<p>A logical value indicating whether the best number of components should be tuned.
</p>
</td></tr>
<tr><td><code id="plsc_+3A_...">...</code></td>
<td>

<p>Arguments passed to or from other methods.
</p>
</td></tr>
<tr><td><code id="plsc_+3A_subset">subset</code></td>
<td>

<p>An index vector specifying the cases to be used in the training
sample.  
</p>
</td></tr>
<tr><td><code id="plsc_+3A_na.action">na.action</code></td>
<td>

<p>A function to specify the action to be taken if <code>NA</code>s are found. The 
default action is <code>na.omit</code>, which leads to rejection of cases with 
missing values on any required variable. An alternative is <code>na.fail</code>, 
which causes an error if <code>NA</code> cases are found. 
</p>
</td></tr>	
</table>


<h3>Details</h3>

<p><code>plcs</code> implements PLS for classification. In details, the categorical response 
vector <code>y</code> is converted into a numeric matrix for regression by PLS and the 
output of PLS is convert to posteriors by <code>softmax</code> method. 
The classification results are obtained based on the posteriors. <code>plslda</code> 
combines PLS and LDA for classification, in which, PLS is for dimension reduction
and LDA is for classification based on the data transformed by PLS. 
</p>
<p>Three PLS functions,<code><a href="pls.html#topic+simpls.fit">simpls.fit</a></code>, 
<code><a href="pls.html#topic+kernelpls.fit">kernelpls.fit</a></code> and  <code><a href="pls.html#topic+oscorespls.fit">oscorespls.fit</a></code>, are  
implemented in package <span class="pkg">pls</span>.
</p>


<h3>Value</h3>

<p>An object of class <code>plsc</code> or <code>plslda</code> containing the following components:
</p>
<table>
<tr><td><code>x</code></td>
<td>

<p>A matrix of the latent components or scores from PLS.
</p>
</td></tr>
<tr><td><code>cl</code></td>
<td>

<p>The observed class labels of training data. 
</p>
</td></tr>
<tr><td><code>pred</code></td>
<td>

<p>The predicted class labels of training data. 
</p>
</td></tr>
<tr><td><code>conf</code></td>
<td>

<p>The confusion matrix based on training data. 
</p>
</td></tr>
<tr><td><code>acc</code></td>
<td>

<p>The accuracy rate of training data. 
</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>

<p>The posterior probabilities for the predicted classes. 
</p>
</td></tr>
<tr><td><code>ncomp</code></td>
<td>

<p>The number of latent component used for classification. 
</p>
</td></tr>
<tr><td><code>pls.method</code></td>
<td>

<p>The PLS algorithm used.
</p>
</td></tr>
<tr><td><code>pls.out</code></td>
<td>

<p>The output of PLS.
</p>
</td></tr>
<tr><td><code>lda.out</code></td>
<td>

<p>The output of LDA used only by <code>plslda</code>.
</p>
</td></tr>
<tr><td><code>call</code></td>
<td>

<p>The (matched) function call.
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Two functions may be called giving either a formula and
optional data frame, or a matrix and grouping factor as the first
two arguments. 
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>References</h3>

<p>Martens, H. and Nas, T. (1989) <em>Multivariate calibration.</em>
John Wiley &amp; Sons.
</p>


<h3>See Also</h3>

<p><code><a href="pls.html#topic+kernelpls.fit">kernelpls.fit</a></code>, <code><a href="pls.html#topic+simpls.fit">simpls.fit</a></code>, 
<code><a href="pls.html#topic+oscorespls.fit">oscorespls.fit</a></code>, <code><a href="#topic+predict.plsc">predict.plsc</a></code>,
<code><a href="#topic+plot.plsc">plot.plsc</a></code>, <code><a href="#topic+tune.func">tune.func</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(pls)  
data(abr1)
cl   &lt;- factor(abr1$fact$class)
dat  &lt;- preproc(abr1$pos , y=cl, method=c("log10"),add=1)[,110:500]

## divide data as training and test data
idx &lt;- sample(1:nrow(dat), round((2/3)*nrow(dat)), replace=FALSE) 

## construct train and test data 
train.dat  &lt;- dat[idx,]
train.t    &lt;- cl[idx]
test.dat   &lt;- dat[-idx,]        
test.t     &lt;- cl[-idx] 

## apply plsc and plslda
(res   &lt;- plsc(train.dat,train.t, ncomp = 20, tune = FALSE))
## Estimate the mean squared error of prediction (MSEP), root mean squared error
## of prediction (RMSEP) and R^2 (coefficient of multiple determination) for 
## fitted PLSR model 
MSEP(res$pls.out)
RMSEP(res$pls.out)
R2(res$pls.out)

(res.1  &lt;- plslda(train.dat,train.t, ncomp = 20, tune = FALSE))
## Estimate the mean squared error of prediction (MSEP), root mean squared error
## of prediction (RMSEP) and R^2 (coefficient of multiple determination) for 
## fitted PLSR model 
MSEP(res.1$pls.out)
RMSEP(res.1$pls.out)
R2(res.1$pls.out)

## Not run: 
## with function of tuning component numbers
(z.plsc   &lt;- plsc(train.dat,train.t, ncomp = 20, tune = TRUE))
(z.plslda &lt;- plslda(train.dat,train.t, ncomp = 20, tune = TRUE))

## check nomp tuning results
z.plsc$ncomp
plot(z.plsc$acc.tune)
z.plslda$ncomp
plot(z.plslda$acc.tune)

## plot
plot(z.plsc,dimen=c(1,2,3),main = "Training data",abbrev = TRUE)
plot(z.plslda,dimen=c(1,2,3),main = "Training data",abbrev = TRUE)

## predict test data
pred.plsc   &lt;- predict(z.plsc, test.dat)$class
pred.plslda &lt;- predict(z.plslda, test.dat)$class

## classification rate and confusion matrix
cl.rate(test.t, pred.plsc)
cl.rate(test.t, pred.plslda)


## End(Not run)
</code></pre>

<hr>
<h2 id='predict.osc'>
Predict Method for Class 'osc'
</h2><span id='topic+predict.osc'></span>

<h3>Description</h3>

<p>Pre-processing of new data by <code>osc</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'osc'
predict(object, newdata,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.osc_+3A_object">object</code></td>
<td>

<p>Object of class <code>osc</code>.
</p>
</td></tr>
<tr><td><code id="predict.osc_+3A_newdata">newdata</code></td>
<td>

<p>A matrix or data frame of cases to be corrected by OSC.  
</p>
</td></tr>
<tr><td><code id="predict.osc_+3A_...">...</code></td>
<td>

<p>Arguments based from or to other methods.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a method for the generic function <code>predict()</code> for
class <code>osc</code>. If <code>newdata</code> is omitted, the corrected data set used in model of 
<code>osc</code> will be returned.
</p>


<h3>Value</h3>

<p>A list containing the following components:
</p>
<table>
<tr><td><code>x</code></td>
<td>

<p>A matrix of OSC corrected data set.
</p>
</td></tr>
<tr><td><code>Q2</code></td>
<td>

<p>The fraction of variation in X after OSC correction for the new data.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+osc">osc</a></code>, <code><a href="#topic+osc_wold">osc_wold</a></code>, <code><a href="#topic+osc_sjoblom">osc_sjoblom</a></code>,
<code><a href="#topic+osc_wise">osc_wise</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(abr1)
cl   &lt;- factor(abr1$fact$class)
dat  &lt;- abr1$pos

## divide data as training and test data
idx &lt;- sample(1:nrow(dat), round((2/3)*nrow(dat)), replace=FALSE) 

## construct train and test data 
train.dat  &lt;- dat[idx,]
train.t    &lt;- cl[idx]
test.dat   &lt;- dat[-idx,]        
test.t     &lt;- cl[-idx] 

## build OSC model based on the training data
res &lt;- osc(train.dat, train.t, method="wold",osc.ncomp=2, pls.ncomp=4)
names(res)
res
summary(res)

## pre-process test data by OSC
test &lt;- predict(res,test.dat)
test.dat.1 &lt;- test$x
</code></pre>

<hr>
<h2 id='predict.pcalda'>
Predict Method for Class 'pcalda'
</h2><span id='topic+predict.pcalda'></span>

<h3>Description</h3>

<p>Prediction of test data using <code>pcalda</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pcalda'
predict(object, newdata,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.pcalda_+3A_object">object</code></td>
<td>

<p>Object of class <code>pcalda</code>.
</p>
</td></tr>
<tr><td><code id="predict.pcalda_+3A_newdata">newdata</code></td>
<td>

<p>A matrix or data frame of cases to be classified.  
</p>
</td></tr>
<tr><td><code id="predict.pcalda_+3A_...">...</code></td>
<td>

<p>Arguments based from or to other methods.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a method for the generic function <code>predict()</code> for
class <code>pcalda</code>. If <code>newdata</code> is omitted, the results of training data 
in <code>pcalda</code> object will be returned.
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>class</code></td>
<td>

<p>The predicted class (a factor).
</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>

<p>The posterior probabilities for the predicted classes.
</p>
</td></tr>
<tr><td><code>x</code></td>
<td>

<p>The rotated test data by the projection matrix of LDA.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pcalda">pcalda</a></code>, <code><a href="#topic+plot.pcalda">plot.pcalda</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris3)

tr    &lt;- sample(1:50, 25)
train &lt;- rbind(iris3[tr,,1], iris3[tr,,2], iris3[tr,,3])
test  &lt;- rbind(iris3[-tr,,1], iris3[-tr,,2], iris3[-tr,,3])
cl    &lt;- factor(c(rep("s",25), rep("c",25), rep("v",25)))

z     &lt;- pcalda(train, cl)
pred  &lt;- predict(z, test)

## plot the projected data.
grpplot(pred$x, pred$class, main="PCALDA: Iris") 
</code></pre>

<hr>
<h2 id='predict.plsc'>
Predict Method for Class 'plsc' or 'plslda'
</h2><span id='topic+predict.plsc'></span><span id='topic+predict.plslda'></span>

<h3>Description</h3>

<p>Prediction of test data using <code>plsc</code> or <code>plslda</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'plsc'
predict(object, newdata,...)
## S3 method for class 'plslda'
predict(object, newdata,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.plsc_+3A_object">object</code></td>
<td>

<p>Object of class <code>plsc</code> or <code>plslda</code>.
</p>
</td></tr>
<tr><td><code id="predict.plsc_+3A_newdata">newdata</code></td>
<td>

<p>A matrix or data frame of cases to be classified.  
</p>
</td></tr>
<tr><td><code id="predict.plsc_+3A_...">...</code></td>
<td>

<p>Arguments based from or to other methods.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Two functions are methods for the generic function <code>predict()</code> for
class <code>plsc</code> or <code>plslda</code>. If <code>newdata</code> is omitted, the results of 
training data in <code>plsc</code> or <code>plslda</code> object will be returned.
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>class</code></td>
<td>

<p>The predicted class (a factor).
</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>

<p>The posterior probabilities for the predicted classes.
</p>
</td></tr>
<tr><td><code>x</code></td>
<td>

<p>The rotated test data by the projection matrix of PLS.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plsc">plsc</a></code>, <code><a href="#topic+plot.plsc">plot.plsc</a></code>,<code><a href="#topic+plslda">plslda</a></code>, <code><a href="#topic+plot.plslda">plot.plslda</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris3)

tr    &lt;- sample(1:50, 25)
train &lt;- rbind(iris3[tr,,1], iris3[tr,,2], iris3[tr,,3])
test  &lt;- rbind(iris3[-tr,,1], iris3[-tr,,2], iris3[-tr,,3])
cl    &lt;- factor(c(rep("s",25), rep("c",25), rep("v",25)))

## model fit using plsc and plslda without tuning of ncomp
(z.plsc       &lt;- plsc(train, cl))
(z.plslda     &lt;- plslda(train, cl))
## predict for test data
pred.plsc    &lt;- predict(z.plsc, test)
pred.plslda  &lt;- predict(z.plslda, test)

## plot the projected test data.
grpplot(pred.plsc$x, pred.plsc$class, main="PLSC: Iris") 
grpplot(pred.plslda$x, pred.plslda$class, main="PLSLDA: Iris") 
</code></pre>

<hr>
<h2 id='preproc'>
Pre-process Data Set
</h2><span id='topic+preproc'></span><span id='topic+preproc.sd'></span><span id='topic+preproc.const'></span>

<h3>Description</h3>

<p>Pre-process a data frame or matrix by different methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>preproc (x, y=NULL,method="log",add=1)

preproc.sd(x, y=NULL, na.rm = FALSE)

preproc.const(x, y, tol = 1.0e-4)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="preproc_+3A_x">x</code></td>
<td>

<p>A numeric data frame or matrix to be pre-processed. 
</p>
</td></tr>
<tr><td><code id="preproc_+3A_y">y</code></td>
<td>

<p>A factor specifying the group. It is only used by the 
method <code>TICnorm</code> in <code>preproc</code>.
</p>
</td></tr>
<tr><td><code id="preproc_+3A_method">method</code></td>
<td>

<p>A method used to pre-process the data set. The following methods are supported:
</p>

<ul>
<li> <p><code>center:</code> Centering
</p>
</li>
<li> <p><code>auto:</code> Auto scaling
</p>
</li>
<li> <p><code>range:</code> Range scaling
</p>
</li>
<li> <p><code>pareto:</code> Pareto scaling
</p>
</li>
<li> <p><code>vast:</code> Vast scaling
</p>
</li>
<li> <p><code>level:</code> Level scaling
</p>
</li>
<li> <p><code>log:</code> Log transformation (default)
</p>
</li>
<li> <p><code>log10:</code> Log 10 transformation
</p>
</li>
<li> <p><code>sqrt:</code> Square root transformation
</p>
</li>
<li> <p><code>asinh:</code> Inverse hyperbolic sine transformation
</p>
</li>
<li> <p><code>TICnorm:</code> TIC normalisation
</p>
</li></ul>

</td></tr>
<tr><td><code id="preproc_+3A_na.rm">na.rm</code></td>
<td>
<p>A logical value indicating whether NA values should be stripped 
before the computation proceeds.
</p>
</td></tr>
<tr><td><code id="preproc_+3A_add">add</code></td>
<td>

<p>A numeric value for addition used in the logarmath transformation <code>log</code>
and <code>log10</code>.
</p>
</td></tr>
<tr><td><code id="preproc_+3A_tol">tol</code></td>
<td>
<p>	A tolerance to decide if a matrix is singular; it will reject 
variables and linear combinations of unit-variance variables whose variance 
is less than tol^2.
</p>
</td></tr> 
</table>


<h3>Details</h3>

<p><code>preproc</code> transforms data set by provided <code>method</code>.  
</p>
<p><code>preproc.sd</code> removes variables which have (near) zero S.D with or without 
respect to class/grouped information.
</p>
<p><code>preproc.const</code> removes variables appears to be constant within groups / classes.
</p>


<h3>Value</h3>

<p>A pre-processed data set.
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>References</h3>

<p>Berg, R., Hoefsloot, H., Westerhuis, J., Smilde, A. and Werf, M. (2006),
Centering, scaling, and transformations: improving the biological
information content of metabolomics data, <em>BMC Genomics</em>, 7:142
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(abr1)
cl   &lt;- factor(abr1$fact$class)
dat  &lt;- abr1$pos

## normalise data set using "TICnorm"
z.1  &lt;- preproc(dat, y=cl, method="TICnorm")

## scale data set using "log10"
z.2 &lt;- preproc(dat,method="log10", add=1)

## or scale data set using "log" and "TICnorm" sequentially
z.3 &lt;- preproc(dat,method=c("log","TICnorm"), add=0.1)

</code></pre>

<hr>
<h2 id='pval.util'>
P-values Utilities
</h2><span id='topic+pval.test'></span><span id='topic+pval.reject'></span>

<h3>Description</h3>

<p>Functions to handle p-values of data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  pval.test(x,y, method="oneway.test",...)

  pval.reject(adjp,alpha)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pval.util_+3A_x">x</code></td>
<td>

<p>A data frame or matrix of data set. 
</p>
</td></tr>
<tr><td><code id="pval.util_+3A_y">y</code></td>
<td>

<p>A factor or vector of class.
</p>
</td></tr>
<tr><td><code id="pval.util_+3A_method">method</code></td>
<td>
<p>Hypothesis test such as <code>t.test</code> and <code>wilcox.test</code>.</p>
</td></tr>
<tr><td><code id="pval.util_+3A_adjp">adjp</code></td>
<td>
<p>A matrix-like p-values of simultaneously testing.</p>
</td></tr>
<tr><td><code id="pval.util_+3A_alpha">alpha</code></td>
<td>
<p>A vector of cutoff of p-values or Type I error rate.</p>
</td></tr>
<tr><td><code id="pval.util_+3A_...">...</code></td>
<td>

<p>Arguments to pass.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>pval.test</code> returns a vector of p-values.
</p>
<p><code>pval.reject</code> returns a matrix indicating rejected number according to 
cutoff.
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lattice)

## Example for pval.test and pval.reject
## prepare data set
data(abr1)
cls &lt;- factor(abr1$fact$class)
dat &lt;- abr1$pos[,200:500]
dat &lt;- preproc(dat, method="log")

## select class "1" and "2" for feature ranking
ind &lt;- grepl("1|2", cls)
dat &lt;- dat[ind,,drop=FALSE] 
cls &lt;- cls[ind, drop=TRUE]   

## univariate p-values and its adjusted p-values
pval &lt;- sort(pval.test(dat, cls, method="t.test"))

## adjust p-values
pval.ad &lt;- sapply(c("fdr","bonferroni","BY"), function(y){
  p.adjust(pval, method=y)
})
pval.ad &lt;- cbind(raw=pval, pval.ad)
pval.reject(pval.ad,c(0.005, 0.01, 0.05))

## plot the all p-values
tmp &lt;- cbind(pval.ad, idx=1:nrow(pval.ad))
tmp &lt;- data.frame(tmp)

# pval_long &lt;- melt(tmp, id="idx")
pval_long &lt;- data.frame(idx = tmp$idx, stack(tmp, select = -idx))
pval_long &lt;- pval_long[c("idx", "ind", "values")]
names(pval_long) &lt;- c("idx", "variable", "value")

pval.p &lt;- xyplot(value~idx, data=pval_long, groups=variable,
                par.settings = list(superpose.line = list(lty=c(1:7))),
                as.table = TRUE, type="l", 
                par.strip.text = list(cex=0.65), ylim=c(-0.005, 1.0),
                ylab="P-values", xlab="Index of variables",
                main="p-values",
                auto.key = list(lines=TRUE, points = FALSE,space="right"),
                panel = function(x, y,...) {
                  panel.xyplot(x, y, ...)
                  panel.abline(h = 0.05, col = "red",lty =2)
                })
pval.p

</code></pre>

<hr>
<h2 id='save.tab'>
Save List of Data Frame or Matrix into CSV File
</h2><span id='topic+save.tab'></span>

<h3>Description</h3>

<p>Save a list of data frame or matrix into a CSV file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  save.tab(x, filename="temp.csv", firstline="\n")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="save.tab_+3A_x">x</code></td>
<td>

<p>A list of data frame or matrix. 
</p>
</td></tr>
<tr><td><code id="save.tab_+3A_filename">filename</code></td>
<td>

<p>A character string for saved file name.
</p>
</td></tr>
<tr><td><code id="save.tab_+3A_firstline">firstline</code></td>
<td>

<p>A string giving some description of the saved file.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function gives a quick option to save a set of data frame or matrix into a
single table file.
</p>


<h3>Value</h3>

<p>No return value, called for side effects.
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="utils.html#topic+write.table">write.table</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data(abr1)
dat &lt;- preproc(abr1$pos[,200:400], method="log10")  
cls &lt;- factor(abr1$fact$class)

tmp &lt;- dat.sel(dat, cls, choices=c("1","2"))
x   &lt;- tmp[[1]]$dat
y   &lt;- tmp[[1]]$cls

fs.method &lt;- c("fs.anova","fs.rf","fs.rfe")
fs.pars   &lt;- valipars(sampling="cv",niter=10,nreps=5)
fs &lt;- feat.mfs(x, y, fs.method, fs.pars)   ## with resampling
names(fs)
fs &lt;- fs[1:3]

## save consistency of feature selection
filename  &lt;- "fs.csv"
firstline &lt;- paste('\nResults of feature selection', sep='')

save.tab(fs, filename, firstline)

## End(Not run)
</code></pre>

<hr>
<h2 id='stats.util'>
Statistical Summary Utilities for Two-Classes Data
</h2><span id='topic+stats.mat'></span><span id='topic+stats.vec'></span>

<h3>Description</h3>

<p>Functions to summarise two-group data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stats.vec(x,y, method="mean",test.method = "wilcox.test",fc=TRUE,...)
stats.mat(x,y, method="mean",test.method = "wilcox.test",
          padj.method= "fdr",fc=TRUE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stats.util_+3A_x">x</code></td>
<td>
<p> A data frame or matrix of data set for <code>stats.mat</code> or
data vector for <code>stats.vec</code>. </p>
</td></tr>
<tr><td><code id="stats.util_+3A_y">y</code></td>
<td>
<p> A factor or vector of class. Two classes only. </p>
</td></tr>
<tr><td><code id="stats.util_+3A_method">method</code></td>
<td>
<p>method for group center such as <code><a href="base.html#topic+mean">mean</a></code> or
<code><a href="stats.html#topic+median">median</a></code>. </p>
</td></tr>
<tr><td><code id="stats.util_+3A_test.method">test.method</code></td>
<td>
<p>method for p-values from parametric test such as
<code><a href="stats.html#topic+t.test">t.test</a></code> or non-parametric test such as
<code><a href="stats.html#topic+wilcox.test">wilcox.test</a></code>. </p>
</td></tr>
<tr><td><code id="stats.util_+3A_padj.method">padj.method</code></td>
<td>
<p>method for p-values correction. Can be one in
<code><a href="stats.html#topic+p.adjust.methods">p.adjust.methods</a></code>: &quot;holm&quot;, &quot;hochberg&quot;, &quot;hommel&quot;,
&quot;bonferroni&quot;, &quot;BH&quot;, &quot;BY&quot;, &quot;fdr&quot; and &quot;none&quot;. </p>
</td></tr>
<tr><td><code id="stats.util_+3A_fc">fc</code></td>
<td>
<p>a flag for fold-change.</p>
</td></tr> 
<tr><td><code id="stats.util_+3A_...">...</code></td>
<td>
<p> Additional parameters. </p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>stats.vec</code> returns an vector of center, group center, fold change, 
log2 fold change, AUC and  p-value. 
</p>
<p><code>stats.mat</code>, which is an wrapper function of <code>stats.vec</code>,
returns an data frame of center, group center, fold change, log2 
fold-change, AUC, p-value and adjusted p-values.
</p>


<h3>Note</h3>

<p>If <code>x</code> has negative values, the results of fold-change and
log2 fold-change are not reliable.
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
sel &lt;- c("setosa", "versicolor")
grp &lt;- iris[,5]
idx &lt;- match(iris[,5],sel,nomatch = 0) &gt; 0

cls &lt;- factor(iris[idx,5])
dat &lt;- iris[idx,1:4]

## stats for the single vector
stats.vec(dat[[1]],cls, method = "median",test.method = "wilcox.test")

## use matrix format
tab &lt;- stats.mat(dat,cls, method = "median",test.method = "wilcox.test",
                 padj.method = "BH")
sapply(tab, class)

</code></pre>

<hr>
<h2 id='trainind'> Generate Index of Training Samples </h2><span id='topic+trainind'></span>

<h3>Description</h3>

<p>Generate index of training samples. The sampling scheme includes leave-one-out 
cross-validation (<code>loocv</code>), cross-validation (<code>cv</code>), randomised 
validation (<code>random</code>) and bootstrap (<code>boot</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trainind(cl, pars = valipars())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trainind_+3A_cl">cl</code></td>
<td>

<p>A factor or vector of class.
</p>
</td></tr>
<tr><td><code id="trainind_+3A_pars">pars</code></td>
<td>
<p>A list of sampling parameters for generating training index. It has 
the same structure as the output of <code>valipars</code>. See <code>valipars</code> for details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list of training index. 
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+valipars">valipars</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## A trivia example
x &lt;- as.factor(sample(c("a","b"), 20, replace=TRUE))
table(x)
pars &lt;- valipars(sampling="rand", niter=2, nreps=4, strat=TRUE,div=2/3)
(temp &lt;- trainind(x,pars=pars))
(tmp  &lt;- temp[[1]])
x[tmp[[1]]];table(x[tmp[[1]]])     ## train idx
x[tmp[[2]]];table(x[tmp[[2]]])
x[tmp[[3]]];table(x[tmp[[3]]])
x[tmp[[4]]];table(x[tmp[[4]]])

x[-tmp[[1]]];table(x[-tmp[[1]]])   ## test idx
x[-tmp[[2]]];table(x[-tmp[[2]]])
x[-tmp[[3]]];table(x[-tmp[[3]]])
x[-tmp[[4]]];table(x[-tmp[[4]]])

# iris data set
data(iris)
dat &lt;- subset(iris, select = -Species)
cl  &lt;- iris$Species

## generate 5-fold cross-validation samples
cv.idx &lt;- trainind(cl, pars = valipars(sampling="cv", niter=2, nreps=5))

## generate leave-one-out cross-validation samples
loocv.idx &lt;- trainind(cl, pars = valipars(sampling = "loocv"))

## generate bootstrap samples with 25 replications
boot.idx &lt;- trainind(cl, pars = valipars(sampling = "boot", niter=2,
                                           nreps=25))

## generate randomised samples with 1/4 division and 10 replications. 
rand.idx &lt;- trainind(cl, pars = valipars(sampling = "rand", niter=2, 
                                           nreps=10, div = 1/4))


</code></pre>

<hr>
<h2 id='tune.func'>Functions for Tuning Appropriate Number of Components</h2><span id='topic+tune.func'></span><span id='topic+tune.plsc'></span><span id='topic+tune.plslda'></span><span id='topic+tune.pcalda'></span>

<h3>Description</h3>

<p>Tune appropriate number of components (<code>ncomp</code>) for <code>plsc</code>, 
<code>plslda</code> or <code>pcalda</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tune.plsc(x,y, pls="simpls",ncomp=10, tune.pars,...)

tune.plslda(x,y, pls="simpls",ncomp=10, tune.pars,...)

tune.pcalda(x,y, ncomp=NULL, tune.pars,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tune.func_+3A_x">x</code></td>
<td>

<p>A matrix or data frame containing the explanatory variables if no formula is
given as the principal argument.
</p>
</td></tr>
<tr><td><code id="tune.func_+3A_y">y</code></td>
<td>

<p>A factor specifying the class for each observation if no formula principal 
argument is given.
</p>
</td></tr>
<tr><td><code id="tune.func_+3A_pls">pls</code></td>
<td>

<p>A method for calculating PLS scores and loadings. The following methods are supported:
</p>

<ul>
<li> <p><code>simpls:</code> SIMPLS algorithm. 
</p>
</li>
<li> <p><code>kernelpls:</code> kernel algorithm.
</p>
</li>
<li> <p><code>oscorespls:</code> orthogonal scores algorithm. 
</p>
</li></ul>

<p>For details, see <code><a href="pls.html#topic+simpls.fit">simpls.fit</a></code>, <code><a href="pls.html#topic+kernelpls.fit">kernelpls.fit</a></code> and
<code><a href="pls.html#topic+oscorespls.fit">oscorespls.fit</a></code> in package <span class="pkg">pls</span>.
</p>
</td></tr>
<tr><td><code id="tune.func_+3A_ncomp">ncomp</code></td>
<td>

<p>The number of components to be used in the classification.  
</p>
</td></tr>
<tr><td><code id="tune.func_+3A_tune.pars">tune.pars</code></td>
<td>

<p>A list of parameters using by the resampling method. 
See <code><a href="#topic+valipars">valipars</a></code> for details.
</p>
</td></tr>
<tr><td><code id="tune.func_+3A_...">...</code></td>
<td>
<p>Further parameters passed to <code>tune</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list including: 
</p>
<table>
<tr><td><code>ncomp</code></td>
<td>
<p>The best number of components.</p>
</td></tr>
<tr><td><code>acc.tune</code></td>
<td>
<p>Accuracy rate of components.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plsc">plsc</a></code>, <code><a href="#topic+plslda">plslda</a></code>, <code><a href="#topic+pcalda">pcalda</a></code>,<code><a href="#topic+valipars">valipars</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data(abr1)
cl   &lt;- factor(abr1$fact$class)
dat  &lt;- preproc(abr1$pos , y=cl, method=c("log10"),add=1)[,110:500]

## divide data as training and test data
idx &lt;- sample(1:nrow(dat), round((2/3)*nrow(dat)), replace=FALSE) 

## construct train and test data 
train.dat  &lt;- dat[idx,]
train.t    &lt;- cl[idx]
test.dat   &lt;- dat[-idx,]        
test.t     &lt;- cl[-idx] 

## tune the best number of components
ncomp.plsc   &lt;- tune.plsc(dat,cl, pls="simpls",ncomp=20)
ncomp.plslda &lt;- tune.plslda(dat,cl, pls="simpls",ncomp=20)
ncomp.pcalda &lt;- tune.pcalda(dat,cl, ncomp=60)

## model fit
(z.plsc   &lt;- plsc(train.dat,train.t, ncomp=ncomp.plsc$ncomp))
(z.plslda &lt;- plslda(train.dat,train.t, ncomp=ncomp.plslda$ncomp))
(z.pcalda &lt;- pcalda(train.dat,train.t, ncomp=ncomp.pcalda$ncomp))

## or indirect use tune function in model fit
z.plsc   &lt;- plsc(train.dat,train.t, ncomp=20, tune=TRUE)
z.plslda &lt;- plslda(train.dat,train.t, ncomp=20, tune=TRUE)
z.pcalda &lt;- pcalda(train.dat,train.t, ncomp=60, tune=TRUE)

## predict test data
pred.plsc   &lt;- predict(z.plsc, test.dat)$class
pred.plslda &lt;- predict(z.plslda, test.dat)$class
pred.pcalda &lt;- predict(z.pcalda, test.dat)$class

## classification rate and confusion matrix
cl.rate(test.t, pred.plsc)
cl.rate(test.t, pred.plslda)
cl.rate(test.t, pred.pcalda)


## End(Not run)
</code></pre>

<hr>
<h2 id='valipars'>Generate Control Parameters for Resampling</h2><span id='topic+valipars'></span>

<h3>Description</h3>

<p>Generate the control parameters for resampling process. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>valipars(sampling="cv", niter=10, nreps=10, strat=FALSE,div = 2/3) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="valipars_+3A_sampling">sampling</code></td>
<td>
<p>Sampling scheme. Valid options are:
</p>

<ul>
<li> <p><code>loocv</code>. Leave-one-out cross-validation
</p>
</li>
<li> <p><code>cv</code>. Cross-validation  (default)
</p>
</li>
<li> <p><code>rand</code>. Randomised validation (holdout)
</p>
</li>
<li> <p><code>boot</code>. Bootstrap
</p>
</li></ul>

</td></tr>
<tr><td><code id="valipars_+3A_niter">niter</code></td>
<td>
<p>Number of iteration or repeat for validation.</p>
</td></tr>
<tr><td><code id="valipars_+3A_nreps">nreps</code></td>
<td>

<p>Number of replications in each iteration.
</p>
</td></tr>
<tr><td><code id="valipars_+3A_strat">strat</code></td>
<td>

<p>A logical value indicating whether the stratification should be applied to <code>cv</code>, 
<code>rand</code> and <code>boot</code>.
</p>
</td></tr>
<tr><td><code id="valipars_+3A_div">div</code></td>
<td>

<p>Proportion of data used for training in randomised validation method.
</p>
</td></tr>   
</table>


<h3>Details</h3>

<p><code>valipars</code> provides a list of control parameters for the resampling or validation 
in the process of accuracy evaluation or feature selection process. 
</p>


<h3>Value</h3>

<p>An object of class <code>valipars</code> containing all the above
parameters (either the defaults or the user specified values).
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+trainind">trainind</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## generate control parameters for the re-sampling scheme with 5-fold 
## cross-validation and iteration of 10 times
valipars(sampling = "cv", niter = 10, nreps = 5)

## generate control parameters for the re-sampling scheme with 
## 25-replication bootstrap and iteration of 100 times
valipars(sampling = "boot", niter = 100, nreps = 25,strat=TRUE)

## generate control parameters for the re-sampling scheme with 
## leave-one-out cross-validation
valipars(sampling = "loocv")

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
