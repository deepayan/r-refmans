<!DOCTYPE html><html lang="en"><head><title>Help for package shrinkGPR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {shrinkGPR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#calc_pred_moments'><p>Calculate Predictive Moments</p></a></li>
<li><a href='#eval_pred_dens'><p>Evaluate Predictive Densities</p></a></li>
<li><a href='#gen_posterior_samples'><p>Generate Posterior Samples</p></a></li>
<li><a href='#kernel_functions'><p>Kernel Functions for Gaussian Processes</p></a></li>
<li><a href='#LPDS'><p>Log Predictive Density Score</p></a></li>
<li><a href='#predict.shrinkGPR'><p>Generate Predictions</p></a></li>
<li><a href='#shrinkGPR'><p>Gaussian Process Regression with Shrinkage and Normalizing Flows</p></a></li>
<li><a href='#simGPR'><p>Simulate Data for Gaussian Process Regression</p></a></li>
<li><a href='#sylvester'><p>Sylvester Normalizing Flow</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Scalable Gaussian Process Regression with Hierarchical Shrinkage
Priors</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Peter Knaus &lt;peter.knaus@wu.ac.at&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Efficient variational inference methods for fully Bayesian Gaussian 
  Process Regression (GPR) models with hierarchical shrinkage priors, 
  including the triple gamma prior for effective variable selection and 
  covariance shrinkage in high-dimensional settings. The package leverages normalizing 
  flows to approximate complex posterior distributions. For details on implementation, 
  see Knaus (2025) &lt;<a href="https://doi.org/10.48550%2FarXiv.2501.13173">doi:10.48550/arXiv.2501.13173</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>gsl, progress, rlang, utils, methods, torch</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>torch backend, for installation guide see
https://cran.r-project.org/web/packages/torch/vignettes/installation.html</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-29 16:42:54 UTC; Peter</td>
</tr>
<tr>
<td>Author:</td>
<td>Peter Knaus <a href="https://orcid.org/0000-0001-6498-7084"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-30 19:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='calc_pred_moments'>Calculate Predictive Moments</h2><span id='topic+calc_pred_moments'></span>

<h3>Description</h3>

<p><code>calc_pred_moments</code> calculates the predictive means and variances for a fitted <code>shrinkGPR</code> model at new data points.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_pred_moments(object, newdata, nsamp = 100)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calc_pred_moments_+3A_object">object</code></td>
<td>
<p>A <code>shrinkGPR</code> object representing the fitted Gaussian process regression model.</p>
</td></tr>
<tr><td><code id="calc_pred_moments_+3A_newdata">newdata</code></td>
<td>
<p><em>Optional</em> data frame containing the covariates for the new data points. If missing, the training data is used.</p>
</td></tr>
<tr><td><code id="calc_pred_moments_+3A_nsamp">nsamp</code></td>
<td>
<p>Positive integer specifying the number of posterior samples to use for the calculation. Default is 100.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes predictive moments by marginalizing over posterior samples from the fitted model. If the mean equation is included in the model, the corresponding covariates are used.
</p>


<h3>Value</h3>

<p>A list with two elements:
</p>

<ul>
<li> <p><code>means</code>: A matrix of predictive means for each new data point, with the rows being the samples and the columns the data points.
</p>
</li>
<li> <p><code>vars</code>: An array of covariance matrices, with the first dimension corresponding to the samples and second and third dimensions to the data points.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
if (torch::torch_is_installed()) {
  # Simulate data
  set.seed(123)
  torch::torch_manual_seed(123)
  n &lt;- 100
  x &lt;- matrix(runif(n * 2), n, 2)
  y &lt;- sin(2 * pi * x[, 1]) + rnorm(n, sd = 0.1)
  data &lt;- data.frame(y = y, x1 = x[, 1], x2 = x[, 2])

  # Fit GPR model
  res &lt;- shrinkGPR(y ~ x1 + x2, data = data)

  # Calculate predictive moments
  momes &lt;- calc_pred_moments(res, nsamp = 100)
  }

</code></pre>

<hr>
<h2 id='eval_pred_dens'>Evaluate Predictive Densities</h2><span id='topic+eval_pred_dens'></span>

<h3>Description</h3>

<p><code>eval_pred_dens</code> evaluates the predictive density for a set of points based on a fitted <code>shrinkGPR</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>eval_pred_dens(x, mod, data_test, nsamp = 100, log = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="eval_pred_dens_+3A_x">x</code></td>
<td>
<p>Numeric vector of points for which the predictive density is to be evaluated.</p>
</td></tr>
<tr><td><code id="eval_pred_dens_+3A_mod">mod</code></td>
<td>
<p>A <code>shrinkGPR</code> object representing the fitted Gaussian process regression model.</p>
</td></tr>
<tr><td><code id="eval_pred_dens_+3A_data_test">data_test</code></td>
<td>
<p>Data frame with one row containing the covariates for the test set.
Variables in <code>data_test</code> must match those used in model fitting.</p>
</td></tr>
<tr><td><code id="eval_pred_dens_+3A_nsamp">nsamp</code></td>
<td>
<p>Positive integer specifying the number of posterior samples to use for the evaluation. Default is 100.</p>
</td></tr>
<tr><td><code id="eval_pred_dens_+3A_log">log</code></td>
<td>
<p>Logical; if <code>TRUE</code>, returns the log predictive density. Default is <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes predictive densities by marginalizing over posterior samples drawn from the fitted model. If the mean equation is included in the model, the corresponding covariates are incorporated.
</p>


<h3>Value</h3>

<p>A numeric vector containing the predictive densities (or log predictive densities) for the points in <code>x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (torch::torch_is_installed()) {
  # Simulate data
  set.seed(123)
  torch::torch_manual_seed(123)
  n &lt;- 100
  x &lt;- matrix(runif(n * 2), n, 2)
  y &lt;- sin(2 * pi * x[, 1]) + rnorm(n, sd = 0.1)
  data &lt;- data.frame(y = y, x1 = x[, 1], x2 = x[, 2])

  # Fit GPR model
  res &lt;- shrinkGPR(y ~ x1 + x2, data = data)

  # Create point at which to evaluate predictive density
  data_test &lt;- data.frame(x1 = 0.8, x2 = 0.5)
  eval_points &lt;- c(-1.2, -1, 0)

  eval_pred_dens(eval_points, res, data_test)

  # Is vectorized, can also be used in functions like curve
  curve(eval_pred_dens(x, res, data_test), from = -1.5, to = -0.5)
  abline(v = sin(2 * pi * 0.8), col = "red")
  }

</code></pre>

<hr>
<h2 id='gen_posterior_samples'>Generate Posterior Samples</h2><span id='topic+gen_posterior_samples'></span>

<h3>Description</h3>

<p><code>gen_posterior_samples</code> generates posterior samples of the model parameters from a fitted <code>shrinkGPR</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gen_posterior_samples(mod, nsamp = 1000)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gen_posterior_samples_+3A_mod">mod</code></td>
<td>
<p>A <code>shrinkGPR</code> object representing the fitted Gaussian process regression model.</p>
</td></tr>
<tr><td><code id="gen_posterior_samples_+3A_nsamp">nsamp</code></td>
<td>
<p>Positive integer specifying the number of posterior samples to generate. Default is 1000.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function draws posterior samples from the latent space and transforms them into the parameter space of the model. These samples can be used for posterior inference or further analysis.
</p>


<h3>Value</h3>

<p>A list containing posterior samples of the model parameters:
</p>

<ul>
<li> <p><code>thetas</code>: A matrix of posterior samples for the inverse lengthscale parameters.
</p>
</li>
<li> <p><code>sigma2</code>: A matrix of posterior samples for the noise variance.
</p>
</li>
<li> <p><code>lambda</code>: A matrix of posterior samples for the global shrinkage parameter.
</p>
</li>
<li> <p><code>betas</code> (optional): A matrix of posterior samples for the mean equation parameters (if included in the model).
</p>
</li>
<li> <p><code>lambda_mean</code> (optional): A matrix of posterior samples for the mean equation's global shrinkage parameter (if included in the model).
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
if (torch::torch_is_installed()) {
  # Simulate data
  set.seed(123)
  torch::torch_manual_seed(123)
  n &lt;- 100
  x &lt;- matrix(runif(n * 2), n, 2)
  y &lt;- sin(2 * pi * x[, 1]) + rnorm(n, sd = 0.1)
  data &lt;- data.frame(y = y, x1 = x[, 1], x2 = x[, 2])

  # Fit GPR model
  res &lt;- shrinkGPR(y ~ x1 + x2, data = data)

  # Generate posterior samples
  samps &lt;- gen_posterior_samples(res, nsamp = 1000)

  # Plot the posterior samples
  boxplot(samps$thetas)
  }

</code></pre>

<hr>
<h2 id='kernel_functions'>Kernel Functions for Gaussian Processes</h2><span id='topic+kernel_functions'></span><span id='topic+kernel_se'></span><span id='topic+kernel_matern_12'></span><span id='topic+kernel_matern_32'></span><span id='topic+kernel_matern_52'></span>

<h3>Description</h3>

<p>A set of kernel functions for Gaussian processes, including the squared exponential (SE) kernel and Matérn kernels
with smoothness parameters 1/2, 3/2, and 5/2. These kernels compute the covariance structure for Gaussian process regression
models and are designed for compatibility with the <code>shrinkGPR</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kernel_se(thetas, tau, x, x_star = NULL)

kernel_matern_12(thetas, tau, x, x_star = NULL)

kernel_matern_32(thetas, tau, x, x_star = NULL)

kernel_matern_52(thetas, tau, x, x_star = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kernel_functions_+3A_thetas">thetas</code></td>
<td>
<p>A <code>torch_tensor</code> of dimensions <code>n_latent x d</code>, representing the latent length-scale parameters.</p>
</td></tr>
<tr><td><code id="kernel_functions_+3A_tau">tau</code></td>
<td>
<p>A <code>torch_tensor</code> of length <code>n_latent</code>, representing the latent scaling factors.</p>
</td></tr>
<tr><td><code id="kernel_functions_+3A_x">x</code></td>
<td>
<p>A <code>torch_tensor</code> of dimensions <code>N x d</code>, containing the input data points.</p>
</td></tr>
<tr><td><code id="kernel_functions_+3A_x_star">x_star</code></td>
<td>
<p>Either <code>NULL</code> or a <code>torch_tensor</code> of dimensions <code>N_new x d</code>. If <code>NULL</code>, the kernel is computed
for <code>x</code> against itself. Otherwise, it computes the kernel between <code>x</code> and <code>x_star</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These kernel functions are used to define the covariance structure in Gaussian process regression models. Each kernel implements a specific covariance function:
</p>

<ul>
<li> <p><code>kernel_se</code>: Squared exponential (SE) kernel, also known as the radial basis function (RBF) kernel.
It assumes smooth underlying functions.
</p>
</li>
<li> <p><code>kernel_matern_12</code>: Matérn kernel with smoothness parameter <code class="reqn">\nu = 1/2</code>, equivalent to the absolute exponential kernel.
</p>
</li>
<li> <p><code>kernel_matern_32</code>: Matérn kernel with smoothness parameter <code class="reqn">\nu = 3/2</code>.
</p>
</li>
<li> <p><code>kernel_matern_52</code>: Matérn kernel with smoothness parameter <code class="reqn">\nu = 5/2</code>.
</p>
</li></ul>

<p>The <code>sqdist</code> helper function is used internally by these kernels to compute squared distances between data points.
</p>
<p>Note that these functions perform no input checks, as to ensure higher performance.
Users should ensure that the input tensors are of the correct dimensions.
</p>


<h3>Value</h3>

<p>A <code>torch_tensor</code> containing the batched covariance matrices (one for each latent sample):
</p>

<ul>
<li><p> If <code>x_star = NULL</code>, the output is of dimensions <code>n_latent x N x N</code>, representing pairwise covariances between all points in <code>x</code>.
</p>
</li>
<li><p> If <code>x_star</code> is provided, the output is of dimensions <code>n_latent x N_new x N</code>, representing pairwise covariances between <code>x_star</code> and <code>x</code>.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch::torch_is_installed()) {
  # Example inputs
  torch::torch_manual_seed(123)
  n_latent &lt;- 3
  d &lt;- 2
  N &lt;- 5
  thetas &lt;- torch::torch_randn(n_latent, d)$abs()
  tau &lt;- torch::torch_randn(n_latent)$abs()
  x &lt;- torch::torch_randn(N, d)

  # Compute the SE kernel
  K_se &lt;- kernel_se(thetas, tau, x)
  print(K_se)

  # Compute the Matérn 3/2 kernel
  K_matern32 &lt;- kernel_matern_32(thetas, tau, x)
  print(K_matern32)

  # Compute the Matérn 5/2 kernel with x_star
  x_star &lt;- torch::torch_randn(3, d)
  K_matern52 &lt;- kernel_matern_52(thetas, tau, x, x_star)
  print(K_matern52)
}
</code></pre>

<hr>
<h2 id='LPDS'>Log Predictive Density Score</h2><span id='topic+LPDS'></span>

<h3>Description</h3>

<p><code>LPDS</code> calculates the log predictive density score for a fitted <code>shrinkGPR</code> model using a test dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LPDS(mod, data_test, nsamp = 100)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="LPDS_+3A_mod">mod</code></td>
<td>
<p>A <code>shrinkGPR</code> object representing the fitted Gaussian process regression model.</p>
</td></tr>
<tr><td><code id="LPDS_+3A_data_test">data_test</code></td>
<td>
<p>Data frame with one row containing the covariates for the test set.
Variables in <code>data_test</code> must match those used in model fitting.</p>
</td></tr>
<tr><td><code id="LPDS_+3A_nsamp">nsamp</code></td>
<td>
<p>Positive integer specifying the number of posterior samples to use for the evaluation. Default is 100.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The log predictive density score is a measure of model fit that evaluates how well the model predicts unseen data.
It is computed as the log of the marginal predictive density of the observed responses.
</p>


<h3>Value</h3>

<p>A numeric value representing the log predictive density score for the test dataset.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (torch::torch_is_installed()) {
  # Simulate data
  set.seed(123)
  torch::torch_manual_seed(123)
  n &lt;- 100
  x &lt;- matrix(runif(n * 2), n, 2)
  y &lt;- sin(2 * pi * x[, 1]) + rnorm(n, sd = 0.1)
  data &lt;- data.frame(y = y, x1 = x[, 1], x2 = x[, 2])

  # Fit GPR model
  res &lt;- shrinkGPR(y ~ x1 + x2, data = data)

  # Calculate true y value and calculate LPDS at specific point
  x1_new &lt;- 0.8
  x2_new &lt;- 0.5
  y_true &lt;- sin(2 * pi * x1_new)
  data_test &lt;- data.frame(y = y_true, x1 = x1_new, x2 = x2_new)
  LPDS(res, data_test)
  }

</code></pre>

<hr>
<h2 id='predict.shrinkGPR'>Generate Predictions</h2><span id='topic+predict.shrinkGPR'></span>

<h3>Description</h3>

<p><code>predict.shrinkGPR</code> generates posterior predictive samples from a fitted <code>shrinkGPR</code> model at specified covariates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'shrinkGPR'
predict(object, newdata, nsamp = 100, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.shrinkGPR_+3A_object">object</code></td>
<td>
<p>A <code>shrinkGPR</code> object representing the fitted Gaussian process regression model.</p>
</td></tr>
<tr><td><code id="predict.shrinkGPR_+3A_newdata">newdata</code></td>
<td>
<p><em>Optional</em> data frame containing the covariates for the prediction points. If missing, the training data is used.</p>
</td></tr>
<tr><td><code id="predict.shrinkGPR_+3A_nsamp">nsamp</code></td>
<td>
<p>Positive integer specifying the number of posterior samples to generate. Default is 100.</p>
</td></tr>
<tr><td><code id="predict.shrinkGPR_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function generates predictions by sampling from the posterior predictive distribution. If the mean equation is included in the model, the corresponding covariates are incorporated.
</p>


<h3>Value</h3>

<p>A matrix containing posterior predictive samples for each covariate combination in <code>newdata</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (torch::torch_is_installed()) {
  # Simulate data
  set.seed(123)
  torch::torch_manual_seed(123)
  n &lt;- 100
  x &lt;- matrix(runif(n * 2), n, 2)
  y &lt;- sin(2 * pi * x[, 1]) + rnorm(n, sd = 0.1)
  data &lt;- data.frame(y = y, x1 = x[, 1], x2 = x[, 2])

  # Fit GPR model
  res &lt;- shrinkGPR(y ~ x1 + x2, data = data)
  # Example usage for in-sample prediction
  preds &lt;- predict(res)

  # Example usage for out-of-sample prediction
  newdata &lt;- data.frame(x1 = runif(10), x2 = runif(10))
  preds &lt;- predict(res, newdata = newdata)
  }

</code></pre>

<hr>
<h2 id='shrinkGPR'>Gaussian Process Regression with Shrinkage and Normalizing Flows</h2><span id='topic+shrinkGPR'></span>

<h3>Description</h3>

<p><code>shrinkGPR</code> implements Gaussian process regression (GPR) with a hierarchical shrinkage prior for hyperparameter estimation,
incorporating normalizing flows to approximate the posterior distribution. The function facilitates model specification, optimization,
and training, including support for early stopping, user-defined kernels, and flow-based transformations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shrinkGPR(
  formula,
  data,
  a = 0.5,
  c = 0.5,
  formula_mean,
  a_mean = 0.5,
  c_mean = 0.5,
  sigma2_rate = 10,
  kernel_func = kernel_se,
  n_layers = 10,
  n_latent = 10,
  flow_func = sylvester,
  flow_args,
  n_epochs = 1000,
  auto_stop = TRUE,
  cont_model,
  device,
  display_progress = TRUE,
  optim_control
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="shrinkGPR_+3A_formula">formula</code></td>
<td>
<p>object of class &quot;formula&quot;: a symbolic representation of the model for the covariance equation, as in <code><a href="stats.html#topic+lm">lm</a></code>.
The response variable and covariates are specified here.</p>
</td></tr>
<tr><td><code id="shrinkGPR_+3A_data">data</code></td>
<td>
<p><em>optional</em> data frame containing the response variable and the covariates. If not found in <code>data</code>,
the variables are taken from <code>environment(formula)</code>. No <code>NA</code>s are allowed in the response variable or covariates.</p>
</td></tr>
<tr><td><code id="shrinkGPR_+3A_a">a</code></td>
<td>
<p>positive real number controlling the shrinkage prior for the covariance structure. The default is 0.5.</p>
</td></tr>
<tr><td><code id="shrinkGPR_+3A_c">c</code></td>
<td>
<p>positive real number controlling the tail behavior of the shrinkage prior for the covariance structure. The default is 0.5.</p>
</td></tr>
<tr><td><code id="shrinkGPR_+3A_formula_mean">formula_mean</code></td>
<td>
<p><em>optional</em> formula for the mean equation. If provided, the response variable and covariates for the mean structure
are specified separately from the covariance structure.</p>
</td></tr>
<tr><td><code id="shrinkGPR_+3A_a_mean">a_mean</code></td>
<td>
<p>positive real number controlling the shrinkage prior for the mean structure. The default is 0.5.</p>
</td></tr>
<tr><td><code id="shrinkGPR_+3A_c_mean">c_mean</code></td>
<td>
<p>positive real number controlling the tail behavior of the shrinkage prior for the mean structure. The default is 0.5.</p>
</td></tr>
<tr><td><code id="shrinkGPR_+3A_sigma2_rate">sigma2_rate</code></td>
<td>
<p>positive real number controlling the prior rate parameter for the residual variance. The default is 10.</p>
</td></tr>
<tr><td><code id="shrinkGPR_+3A_kernel_func">kernel_func</code></td>
<td>
<p>function specifying the covariance kernel. The default is <code><a href="#topic+kernel_se">kernel_se</a></code>, a squared exponential kernel.
For guidance on how to provide a custom kernel function, see Details.</p>
</td></tr>
<tr><td><code id="shrinkGPR_+3A_n_layers">n_layers</code></td>
<td>
<p>positive integer specifying the number of flow layers in the normalizing flow. The default is 10.</p>
</td></tr>
<tr><td><code id="shrinkGPR_+3A_n_latent">n_latent</code></td>
<td>
<p>positive integer specifying the dimensionality of the latent space for the normalizing flow. The default is 10.</p>
</td></tr>
<tr><td><code id="shrinkGPR_+3A_flow_func">flow_func</code></td>
<td>
<p>function specifying the normalizing flow transformation. The default is <code><a href="#topic+sylvester">sylvester</a></code>.
For guidance on how to provide a custom flow function, see Details.</p>
</td></tr>
<tr><td><code id="shrinkGPR_+3A_flow_args">flow_args</code></td>
<td>
<p><em>optional</em> named list containing arguments for the flow function. If not provided, default arguments are used.
For guidance on how to provide a custom flow function, see Details.</p>
</td></tr>
<tr><td><code id="shrinkGPR_+3A_n_epochs">n_epochs</code></td>
<td>
<p>positive integer specifying the number of training epochs. The default is 1000.</p>
</td></tr>
<tr><td><code id="shrinkGPR_+3A_auto_stop">auto_stop</code></td>
<td>
<p>logical value indicating whether to enable early stopping based on convergence. The default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="shrinkGPR_+3A_cont_model">cont_model</code></td>
<td>
<p><em>optional</em> object returned from a previous <code>shrinkGPR</code> call, enabling continuation of training from the saved state.</p>
</td></tr>
<tr><td><code id="shrinkGPR_+3A_device">device</code></td>
<td>
<p><em>optional</em> device to run the model on, e.g., <code>torch_device("cuda")</code> for GPU or <code>torch_device("cpu")</code> for CPU.
Defaults to GPU if available; otherwise, CPU.</p>
</td></tr>
<tr><td><code id="shrinkGPR_+3A_display_progress">display_progress</code></td>
<td>
<p>logical value indicating whether to display progress bars and messages during training. The default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="shrinkGPR_+3A_optim_control">optim_control</code></td>
<td>
<p><em>optional</em> named list containing optimizer parameters. If not provided, default settings are used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This implementation provides a computationally efficient framework for GPR, enabling flexible modeling of mean and covariance structures.
Users can specify custom kernel functions, flow transformations, and hyperparameter configurations to adapt the model to their data.
</p>
<p>The <code>shrinkGPR</code> function combines Gaussian process regression with shrinkage priors and normalizing flows for efficient
and flexible hyperparameter estimation. It supports custom kernels, hierarchical shrinkage priors for mean and covariance structures,
and flow-based posterior approximations. The <code>auto_stop</code> option allows early stopping based on lack of improvement in ELBO.
</p>
<p><strong>Custom Kernel Functions</strong>
</p>
<p>Users can define custom kernel functions for the covariance structure of the Gaussian process by passing them to the <code>kernel_func</code> argument.
A valid kernel function must follow the same structure as the provided <code>kernel_se</code> (squared exponential kernel). The function should:
</p>

<ol>
<li> <p><strong>Accept the following arguments:</strong>
</p>

<ul>
<li> <p><code>thetas</code>: A <code>torch_tensor</code> of dimensions <code>n_latent x d</code>, representing latent length-scale parameters.
</p>
</li>
<li> <p><code>tau</code>: A <code>torch_tensor</code> of length <code>n_latent</code>, representing latent scaling factors.
</p>
</li>
<li> <p><code>x</code>: A <code>torch_tensor</code> of dimensions <code>N x d</code>, containing the input data points.
</p>
</li>
<li> <p><code>x_star</code>: Either <code>NULL</code> or a <code>torch_tensor</code> of dimensions <code>N_new x d</code>. If <code>NULL</code>, the kernel is computed for <code>x</code> against itself.
Otherwise, it computes the kernel between <code>x</code> and <code>x_star</code>.
</p>
</li></ul>

</li>
<li> <p><strong>Return:</strong>
</p>

<ul>
<li><p> If <code>x_star = NULL</code>, the function must return a <code>torch_tensor</code> of dimensions <code>n_latent x N x N</code>, representing pairwise covariances
between all points in <code>x</code>.
</p>
</li>
<li><p> If <code>x_star</code> is provided, the function must return a <code>torch_tensor</code> of dimensions <code>n_latent x N_new x N</code>,
representing pairwise covariances between <code>x_star</code> and <code>x</code>.
</p>
</li></ul>

</li>
<li> <p><strong>Requirements:</strong>
</p>

<ul>
<li><p> The kernel must compute a valid positive semi-definite covariance matrix.
</p>
</li>
<li><p> It should use efficient tensor operations from the Torch library (e.g., <code>torch_bmm</code>, <code>torch_sum</code>) to ensure compatibility with GPUs or CPUs.
</p>
</li></ul>

</li></ol>

<p><strong>Testing a Custom Kernel Function</strong>
</p>
<p>To test a custom kernel function:
</p>

<ol>
<li> <p><strong>Verify Dimensions:</strong>
</p>

<ul>
<li><p> When <code>x_star = NULL</code>, ensure the output is <code>n_latent x N x N</code>.
</p>
</li>
<li><p> When <code>x_star</code> is provided, ensure the output is <code>n_latent x N_new x N</code>.
</p>
</li></ul>

</li>
<li> <p><strong>Check Positive Semi-Definiteness:</strong>
Validate that the kernel produces a positive semi-definite covariance matrix for valid inputs.
</p>
</li>
<li> <p><strong>Integrate:</strong>
Use the custom kernel with <code>shrinkGPR</code> to confirm its compatibility.
</p>
</li></ol>

<p>Examples of kernel functions can be found in the <code>kernel_funcs.R</code> file in the package source code,
which are documented in the <code><a href="#topic+kernel_functions">kernel_functions</a></code> help file.
</p>
<p><strong>Custom Flow Functions</strong>
</p>
<p>Users can define custom flow functions for use in Gaussian process regression models by following the structure
and conventions of the provided <code>sylvester</code> function. A valid flow function should be implemented as a
<code>nn_module</code> in <code>torch</code> and must meet the following requirements:
</p>
<p><strong>Structure of a Custom Flow Function</strong>
</p>

<ol>
<li> <p><strong>Initialization (<code>initialize</code>)</strong>:
</p>

<ul>
<li><p> Include all required parameters as <code>nn_parameter</code> or <code>nn_buffer</code>, and initialize them appropriately.
</p>
</li>
<li><p> Parameters may include matrices for transformations (e.g., triangular matrices), biases, or other learnable components.
</p>
</li></ul>

</li>
<li> <p><strong>Forward Pass (<code>forward</code>)</strong>:
</p>

<ul>
<li><p> The <code>forward</code> method should accept an input tensor <code>z</code> of dimensions <code>n_latent x D</code>.
</p>
</li>
<li><p> The method must:
</p>

<ul>
<li><p> Compute the transformed tensor <code>z</code>.
</p>
</li>
<li><p> Compute the log determinant of the Jacobian (<code>log|det J|</code>).
</p>
</li></ul>

</li>
<li><p> The method should return a list containing:
</p>

<ul>
<li> <p><code>zk</code>: The transformed samples after applying the flow (<code>n_latent x D</code>).
</p>
</li>
<li> <p><code>log_diag_j</code>: A tensor of size <code>n_latent</code> containing the log determinant of the Jacobian for each sample.
</p>
</li></ul>

</li></ul>

</li>
<li> <p><strong>Output Dimensions</strong>:
</p>

<ul>
<li><p> Input tensor <code>z</code>: <code>n_latent x D</code>.
</p>
</li>
<li><p> Outputs:
</p>

<ul>
<li> <p><code>zk</code>: <code>n_latent x D</code>.
</p>
</li>
<li> <p><code>log_diag_j</code>: <code>n_latent</code>.
</p>
</li></ul>

</li></ul>

</li></ol>

<p>An example of a flow function can be found in the <code>sylvester.R</code> file in the package source code,
which is documented in the <code><a href="#topic+sylvester">sylvester</a></code> help file.
</p>


<h3>Value</h3>

<p>A list object of class <code>shrinkGPR</code>, containing:
</p>
<table role = "presentation">
<tr><td><code>model</code></td>
<td>
<p>The best-performing trained model.</p>
</td></tr>
<tr><td><code>loss</code></td>
<td>
<p>The best loss value (ELBO) achieved during training.</p>
</td></tr>
<tr><td><code>loss_stor</code></td>
<td>
<p>A numeric vector storing the ELBO values at each training iteration.</p>
</td></tr>
<tr><td><code>last_model</code></td>
<td>
<p>The model state at the final iteration.</p>
</td></tr>
<tr><td><code>optimizer</code></td>
<td>
<p>The optimizer object used during training.</p>
</td></tr>
<tr><td><code>model_internals</code></td>
<td>
<p>Internal objects required for predictions and further training, such as model matrices and formulas.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peter Knaus <a href="mailto:peter.knaus@wu.ac.at">peter.knaus@wu.ac.at</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (torch::torch_is_installed()) {
  # Simulate data
  set.seed(123)
  torch::torch_manual_seed(123)
  n &lt;- 100
  x &lt;- matrix(runif(n * 2), n, 2)
  y &lt;- sin(2 * pi * x[, 1]) + rnorm(n, sd = 0.1)
  data &lt;- data.frame(y = y, x1 = x[, 1], x2 = x[, 2])

  # Fit GPR model
  res &lt;- shrinkGPR(y ~ x1 + x2, data = data)

  # Check convergence
  plot(res$loss_stor, type = "l", main = "Loss Over Iterations")

  # Check posterior
  samps &lt;- gen_posterior_samples(res, nsamp = 1000)
  boxplot(samps$thetas) # Second theta is pulled towards zero

  # Predict
  x1_new &lt;- seq(from = 0, to = 1, length.out = 100)
  x2_new &lt;- runif(100)
  y_new &lt;- predict(res, newdata = data.frame(x1 = x1_new, x2 = x2_new), nsamp = 2000)

  # Plot
  quants &lt;- apply(y_new, 2, quantile, c(0.025, 0.5, 0.975))
  plot(x1_new, quants[2, ], type = "l", ylim = c(-1.5, 1.5),
        xlab = "x1", ylab = "y", lwd = 2)
  polygon(c(x1_new, rev(x1_new)), c(quants[1, ], rev(quants[3, ])),
        col = adjustcolor("skyblue", alpha.f = 0.5), border = NA)
  points(x[,1], y)
  curve(sin(2 * pi * x), add = TRUE, col = "forestgreen", lwd = 2, lty = 2)
  }

</code></pre>

<hr>
<h2 id='simGPR'>Simulate Data for Gaussian Process Regression</h2><span id='topic+simGPR'></span>

<h3>Description</h3>

<p><code>simGPR</code> generates simulated data for Gaussian Process Regression (GPR) models, including the true hyperparameters used for simulation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simGPR(
  N = 200,
  d = 3,
  d_mean = 0,
  sigma2 = 0.1,
  tau = 2,
  kernel_func = kernel_se,
  perc_spars = 0.5,
  theta,
  beta,
  device
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="simGPR_+3A_n">N</code></td>
<td>
<p>Positive integer specifying the number of observations to simulate. Default is 200.</p>
</td></tr>
<tr><td><code id="simGPR_+3A_d">d</code></td>
<td>
<p>Positive integer specifying the number of covariates for the covariance structure. Default is 3.</p>
</td></tr>
<tr><td><code id="simGPR_+3A_d_mean">d_mean</code></td>
<td>
<p>Positive integer specifying the number of covariates for the mean structure. Default is 0.</p>
</td></tr>
<tr><td><code id="simGPR_+3A_sigma2">sigma2</code></td>
<td>
<p>Positive numeric value specifying the noise variance. Default is 0.1.</p>
</td></tr>
<tr><td><code id="simGPR_+3A_tau">tau</code></td>
<td>
<p>Positive numeric value specifying the global shrinkage parameter. Default is 2.</p>
</td></tr>
<tr><td><code id="simGPR_+3A_kernel_func">kernel_func</code></td>
<td>
<p>Function specifying the covariance kernel. Default is <code>kernel_se</code>.</p>
</td></tr>
<tr><td><code id="simGPR_+3A_perc_spars">perc_spars</code></td>
<td>
<p>Numeric value in [0, 1] indicating the proportion of elements in <code>theta</code>
and <code>beta</code> to sparsify. Default is 0.5.</p>
</td></tr>
<tr><td><code id="simGPR_+3A_theta">theta</code></td>
<td>
<p><em>Optional</em> numeric vector specifying the true inverse length-scale parameters.
If not provided, they are randomly generated.</p>
</td></tr>
<tr><td><code id="simGPR_+3A_beta">beta</code></td>
<td>
<p><em>Optional</em> numeric vector specifying the true regression coefficients for the mean
structure. If not provided, they are randomly generated.</p>
</td></tr>
<tr><td><code id="simGPR_+3A_device">device</code></td>
<td>
<p><em>Optional</em> <code>torch_device</code> object specifying whether to run the simulation
on CPU or GPU. Defaults to GPU if available.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function simulates data from a Gaussian Process Regression model.
The response variable <code>y</code> is sampled from a multivariate normal distribution with
a covariance matrix determined by the specified kernel function, <code>theta</code>, <code>tau</code>,
and <code>sigma2</code>. If <code>d_mean &gt; 0</code>, a mean structure is included in the simulation, with
covariates <code>x_mean</code> and regression coefficients <code>beta</code>.
</p>


<h3>Value</h3>

<p>A list containing:
</p>

<ul>
<li> <p><code>data</code>: A data frame with <code>y</code> (response variable), <code>x</code> (covariates for the covariance structure),
and optionally <code>x_mean</code> (covariates for the mean structure).
</p>
</li>
<li> <p><code>true_vals</code>: A list containing the true values used for the simulation:
</p>

<ul>
<li> <p><code>theta</code>: The true inverse length-scale parameters.
</p>
</li>
<li> <p><code>sigma2</code>: The true noise variance.
</p>
</li>
<li> <p><code>tau</code>: The true global shrinkage parameter.
</p>
</li>
<li> <p><code>beta</code> (optional): The true regression coefficients for the mean structure.
</p>
</li></ul>

</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (torch::torch_is_installed()) {
  torch::torch_manual_seed(123)

  # Simulate data with default settings
  sim_data &lt;- simGPR()

  # Simulate data with custom settings
  sim_data &lt;- simGPR(N = 100, d = 5, d_mean = 2, perc_spars = 0.3, sigma2 = 0.5)

  # Access the simulated data
  head(sim_data$data)

  # Access the true values used for simulation
  sim_data$true_vals
  }
</code></pre>

<hr>
<h2 id='sylvester'>Sylvester Normalizing Flow</h2><span id='topic+sylvester'></span>

<h3>Description</h3>

<p>The <code>sylvester</code> function implements Sylvester normalizing flows as described by
van den Berg et al. (2018) in &quot;Sylvester Normalizing Flows for Variational Inference.&quot;
This flow applies a sequence of invertible transformations to map a simple base distribution
to a more complex target distribution, allowing for flexible posterior approximations in Gaussian process regression models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sylvester(d, n_householder)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sylvester_+3A_d">d</code></td>
<td>
<p>An integer specifying the latent dimensionality of the input space.</p>
</td></tr>
<tr><td><code id="sylvester_+3A_n_householder">n_householder</code></td>
<td>
<p>An optional integer specifying the number of Householder reflections used to orthogonalize the transformation.
Defaults to <code>d - 1</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Sylvester flow uses two triangular matrices (<code>R1</code> and <code>R2</code>) and Householder reflections to construct invertible transformations.
The transformation is parameterized as follows:
</p>
<p style="text-align: center;"><code class="reqn">z = Q R_1 h(Q^T R_2 zk + b) + zk,</code>
</p>

<p>where:
</p>

<ul>
<li> <p><code>Q</code> is an orthogonal matrix obtained via Householder reflections.
</p>
</li>
<li> <p><code>R1</code> and <code>R2</code> are upper triangular matrices with learned diagonal elements.
</p>
</li>
<li> <p><code>h</code> is a non-linear activation function (default: <code>torch_tanh</code>).
</p>
</li>
<li> <p><code>b</code> is a learned bias vector.
</p>
</li></ul>

<p>The log determinant of the Jacobian is computed to ensure the invertibility of the transformation and is given by:
</p>
<p style="text-align: center;"><code class="reqn">\log |det J| = \sum_{i=1}^d \log |diag_1[i] \cdot diag_2[i] \cdot h'(RQ^T zk + b) + 1|,</code>
</p>

<p>where <code>diag_1</code> and <code>diag_2</code> are the learned diagonal elements of <code>R1</code> and <code>R2</code>, respectively, and <code>h\'</code> is the derivative of the activation function.
</p>


<h3>Value</h3>

<p>An <code>nn_module</code> object representing the Sylvester normalizing flow. The module has the following key components:
</p>

<ul>
<li> <p><code>forward(zk)</code>: The forward pass computes the transformed variable <code>z</code> and the log determinant of the Jacobian.
</p>
</li>
<li><p> Internal parameters include matrices <code>R1</code> and <code>R2</code>, diagonal elements, and Householder reflections used for orthogonalization.
</p>
</li></ul>



<h3>References</h3>

<p>van den Berg, R., Hasenclever, L., Tomczak, J. M., &amp; Welling, M. (2018).
&quot;Sylvester Normalizing Flows for Variational Inference.&quot;
<em>Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI 2018)</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch::torch_is_installed()) {
  # Example: Initialize a Sylvester flow
  d &lt;- 5
  n_householder &lt;- 4
  flow &lt;- sylvester(d, n_householder)

  # Forward pass through the flow
  zk &lt;- torch::torch_randn(10, d)  # Batch of 10 samples
  result &lt;- flow(zk)

  print(result$zk)
  print(result$log_diag_j)
}
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
