<!DOCTYPE html><html><head><title>Help for package BayesVarSel</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {BayesVarSel}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#BayesVarSel-package'><p>Bayes Factors, Model Choice And Variable Selection In Linear Models</p></a></li>
<li><a href='#BMAcoeff'><p>Bayesian Model Averaged estimations of regression coefficients</p></a></li>
<li><a href='#Btest'><p>Bayes factors and posterior probabilities for linear regression models</p></a></li>
<li><a href='#Bvs'><p>Bayesian Variable Selection for linear regression models</p></a></li>
<li><a href='#GibbsBvs'><p>Bayesian Variable Selection for linear regression models using Gibbs</p>
sampling.</a></li>
<li><a href='#GibbsBvsF'><p>Bayesian Variable Selection with Factors for linear regression models using Gibbs</p>
sampling.</a></li>
<li><a href='#Hald'><p>Hald data</p></a></li>
<li><a href='#histBMA'><p>A function for histograms-like representations of objects of class</p>
<code>bma.coeffs</code></a></li>
<li><a href='#Jointness'><p>Computation of Jointness measurements.</p></a></li>
<li><a href='#OBICE'><p>OBICE data</p></a></li>
<li><a href='#Ozone35'><p>Ozone35 dataset</p></a></li>
<li><a href='#plot.Bvs'><p>A function for plotting summaries of an object of class <code>Bvs</code></p></a></li>
<li><a href='#pltltn'><p>Correction for p&gt;&gt;n for an object of class <code>Bvs</code></p></a></li>
<li><a href='#predict.Bvs'><p>Bayesian Model Averaged predictions</p></a></li>
<li><a href='#print.Btest'><p>Print an object of class <code>Btest</code></p></a></li>
<li><a href='#print.Bvs'><p>Print an object of class <code>Bvs</code></p></a></li>
<li><a href='#print.jointness'><p>Print an object of class <code>jointness</code></p></a></li>
<li><a href='#SDM'><p>SDM data</p></a></li>
<li><a href='#summary.Bvs'><p>Summary of an object of class <code>Bvs</code></p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Bayes Factors, Model Choice and Variable Selection in Linear
Models</td>
</tr>
<tr>
<td>Version:</td>
<td>2.2.5</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-03-07</td>
</tr>
<tr>
<td>Description:</td>
<td>Bayes factors and posterior probabilities in Linear models, 
    aimed at provide a formal Bayesian answer to testing and variable 
    selection problems. </td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Gonzalo Garcia-Donato &lt;gonzalo.garciadonato@uclm.es&gt;</td>
</tr>
<tr>
<td>Author:</td>
<td>Gonzalo Garcia-Donato [aut, cre],
  Anabel Forte [aut],
  Carlos Vergara-Hernández [ctb]</td>
</tr>
<tr>
<td>Acknowledgements:</td>
<td>since version 1.9.0 BayesVarSel uses the non-exported
function get_rdX from package lmerTest (distributed under
GPL-2, GPL-3) and authored by Alexandra Kuznetsova, Per Bruun
Brockhoff and Rune Haubo Bojesen Christensen.</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/comodin19/BayesVarSel">https://github.com/comodin19/BayesVarSel</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/comodin19/BayesVarSel/issues">https://github.com/comodin19/BayesVarSel/issues</a></td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>GNU Scientific Library</td>
</tr>
<tr>
<td>Depends:</td>
<td>MASS (&ge; 7.3-45), mvtnorm (&ge; 1.0-5), parallel (&ge; 3.3), R (&ge;
3.5.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, faraway, hdi</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-03-08 11:17:16 UTC; gonzalo.garciadonatouclm.es</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-03-08 12:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='BayesVarSel-package'>Bayes Factors, Model Choice And Variable Selection In Linear Models</h2><span id='topic+BayesVarSel-package'></span><span id='topic+BayesVarSel'></span>

<h3>Description</h3>

<p>Hypothesis testing, model selection and model averaging are important
statistical problems that have in common the explicit consideration of the
uncertainty about which is the true model. The formal Bayesian tool to solve
such problems is the Bayes factor (Kass and Raftery, 1995) that reports the
evidence in the data favoring each of the entertained hypotheses/models and
can be easily translated to posterior probabilities.
</p>


<h3>Details</h3>

<p>This package has been specifically conceived to calculate Bayes factors in
linear models and then to provide a formal Bayesian answer to testing and
variable selection problems. From a theoretical side, the emphasis in the
package is placed on the prior distributions (a very delicate issue in this
context) and BayesVarSel allows using a wide range of them:
Jeffreys-Zellner-Siow (Jeffreys, 1961; Zellner and Siow, 1980,1984) Zellner
(1986); Fernandez et al. (2001), Liang et al. (2008) and Bayarri et al.
(2012).
</p>
<p>The interaction with the package is through a friendly interface that
syntactically mimics the well-known lm command of R. The resulting objects
can be easily explored providing the user very valuable information (like
marginal, joint and conditional inclusion probabilities of potential
variables; the highest posterior probability model, HPM; the median
probability model, MPM) about the structure of the true -data generating-
model. Additionally, BayesVarSel incorporates abilities to handle problems
with a large number of potential explanatory variables through parallel and
heuristic versions (Garcia-Donato and Martinez-Beneito 2013) of the main
commands.
</p>

<table>
<tr>
 <td style="text-align: left;"> Package: </td><td style="text-align: left;"> BayesVarSel</td>
</tr>
<tr>
 <td style="text-align: left;"> Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;"> Version:
</td><td style="text-align: left;"> 2.0.1</td>
</tr>
<tr>
 <td style="text-align: left;"> Date: </td><td style="text-align: left;"> 2020-02-17</td>
</tr>
<tr>
 <td style="text-align: left;"> License: </td><td style="text-align: left;"> GPL-2</td>
</tr>
<tr>
 <td style="text-align: left;"> </td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Gonzalo Garcia-Donato and Anabel Forte
</p>
<p>Maintainer: Anabel Forte <a href="mailto:anabel.forte@uv.es">anabel.forte@uv.es</a>
</p>


<h3>References</h3>

<p>Bayarri, M.J., Berger, J.O., Forte, A. and Garcia-Donato, G.
(2012)&lt;DOI:10.1214/12-aos1013&gt; Criteria for Bayesian Model choice with
Application to Variable Selection. The Annals of Statistics. 40: 1550-1577
</p>
<p>Fernandez, C., Ley, E. and Steel, M.F.J.
(2001)&lt;DOI:10.1016/s0304-4076(00)00076-2&gt; Benchmark priors for Bayesian
model averaging. Journal of Econometrics, 100, 381-427.
</p>
<p>Garcia-Donato, G. and Martinez-Beneito, M.A.
(2013)&lt;DOI:10.1080/01621459.2012.742443&gt; On sampling strategies in Bayesian
variable selection problems with large model spaces. Journal of the American
Statistical Association. 108: 340-352.
</p>
<p>Liang, F., Paulo, R., Molina, G., Clyde, M. and Berger, J.O.
(2008)&lt;DOI:10.1198/016214507000001337&gt; Mixtures of g-priors for Bayesian
Variable Selection. Journal of the American Statistical Association.
103:410-423.
</p>
<p>Zellner, A. and Siow, A. (1980)&lt;DOI:10.1007/bf02888369&gt;. Posterior Odds
Ratio for Selected Regression Hypotheses. In Bayesian Statistics 1 (J.M.
Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.) 585-603.
Valencia: University Press.
</p>
<p>Zellner, A. and Siow, A. (1984) Basic Issues in Econometrics. Chicago:
University of Chicago Press.
</p>
<p>Zellner, A. (1986)&lt;DOI:10.2307/2233941&gt; On Assessing Prior Distributions and
Bayesian Regression Analysis with g-prior Distributions. In Bayesian
Inference and Decision techniques: Essays in Honor of Bruno de Finetti (A.
Zellner, ed.) 389-399. Edward Elgar Publishing Limited.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Btest">Btest</a></code>, <code><a href="#topic+Bvs">Bvs</a></code>,
<code><a href="#topic+GibbsBvs">GibbsBvs</a></code>,
<code><a href="#topic+BMAcoeff">BMAcoeff</a></code>, <code><a href="#topic+predict.Bvs">predict.Bvs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>demo(BayesVarSel.Hald)

</code></pre>

<hr>
<h2 id='BMAcoeff'>Bayesian Model Averaged estimations of regression coefficients</h2><span id='topic+BMAcoeff'></span>

<h3>Description</h3>

<p>Samples of the model averaged objective posterior distribution of regression
coefficients
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BMAcoeff(x, n.sim = 10000, method = "svd")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BMAcoeff_+3A_x">x</code></td>
<td>
<p>An object of class <code>Bvs</code></p>
</td></tr>
<tr><td><code id="BMAcoeff_+3A_n.sim">n.sim</code></td>
<td>
<p>Number of simulations to be produced</p>
</td></tr>
<tr><td><code id="BMAcoeff_+3A_method">method</code></td>
<td>
<p>Text specifying the matrix decomposition used to determine the
matrix root of 'sigma' when simulating from the multivariate t distribution.
Possible methods are eigenvalue decomposition ('&quot;eigen&quot;', default), singular
value decomposition ('&quot;svd&quot;'), and Cholesky decomposition ('&quot;chol&quot;'). See
the help of command <code>rmvnorm</code> in package <code>mvtnorm</code> for more
details</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The distribution that is sampled from is the discrete mixture of the
(objective) posterior distributions of the regression coefficients with
weights proportional to the posterior probabilities of each model. That is,
from
</p>
<p><code class="reqn">latex</code>
</p>
<p>The models used in the mixture above are the retained best models (see the
argument <code>n.keep</code> in <a href="#topic+Bvs">Bvs</a>) if <code>x</code> was generated
with <code>Bvs</code> and the sampled models with the associated frequencies if
<code>x</code> was generated with <code>GibbsBvs</code>. The formula for the objective
posterior distribution within each model <code class="reqn">latex</code> is
taken from Bernardo and Smith (1994) page 442.
</p>
<p>Note: The above mixture is potentially highly multimodal and this command
ends with a multiple plot with the densities of the different regression
coefficients to show the user this peculiarity. Hence which summaries should
be used to describe this distribution is a delicate issue and standard
functions like the mean and variance are not recommendable.
</p>


<h3>Value</h3>

<p><code>BMAcoeff</code> returns an object of class <code>bma.coeffs</code> which
is a matrix with <code>n.sim</code> rows with the simulations. Each column of the
matrix corresponds to a regression coefficient in the full model.
</p>


<h3>Author(s)</h3>

<p>Gonzalo Garcia-Donato and Anabel Forte
</p>
<p>Maintainer: &lt;anabel.forte@uv.es&gt;
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+histBMA">histBMA</a></code> for a histogram-like
representation of the columns in the object. See
<code><a href="#topic+Bvs">Bvs</a></code> and
<code><a href="#topic+GibbsBvs">GibbsBvs</a></code> for creating objects of the class
<code>Bvs</code>. See <code><a href="mvtnorm.html#topic+Mvnorm">Mvnorm</a> for details about argument
method.</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

#Analysis of Crime Data
#load data
data(UScrime)

crime.Bvs&lt;- Bvs(formula= y ~ ., data=UScrime, n.keep=1000)
crime.Bvs.BMA&lt;- BMAcoeff(crime.Bvs, n.sim=10000)
#the best 1000 models are used in the mixture

#We could force all  possible models to be included in the mixture
crime.Bvs.all&lt;- Bvs(formula= y ~ ., data=UScrime, n.keep=2^15)
crime.Bvs.BMA&lt;- BMAcoeff(crime.Bvs.all, n.sim=10000)
#(much slower as this implies ordering many more models...)

#With the Gibbs algorithms:
data(Ozone35)

Oz35.GibbsBvs&lt;- GibbsBvs(formula= y ~ ., data=Ozone35, prior.betas="gZellner",
prior.models="Constant", n.iter=10000, init.model="Full", n.burnin=100,
time.test = FALSE)
Oz35.GibbsBvs.BMA&lt;- BMAcoeff(Oz35.GibbsBvs, n.sim=10000)



## End(Not run)

</code></pre>

<hr>
<h2 id='Btest'>Bayes factors and posterior probabilities for linear regression models</h2><span id='topic+Btest'></span>

<h3>Description</h3>

<p>It Computes the Bayes factors and posterior probabilities of a list of linear
regression models proposed to explain a common response variable over the
same dataset
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Btest(
  models,
  data,
  prior.betas = "Robust",
  prior.models = "Constant",
  priorprobs = NULL,
  null.model = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Btest_+3A_models">models</code></td>
<td>
<p>A named list with the entertained models defined with their
corresponding formulas. If the list is unnamed, default names are given by
the routine. One model must be nested in all the others.</p>
</td></tr>
<tr><td><code id="Btest_+3A_data">data</code></td>
<td>
<p>data frame containing the data.</p>
</td></tr>
<tr><td><code id="Btest_+3A_prior.betas">prior.betas</code></td>
<td>
<p>Prior distribution for regression parameters within each
model (to be literally specified). Possible choices include &quot;Robust&quot;, &quot;Robust.G&quot;, &quot;Liangetal&quot;, &quot;gZellner&quot;,
&quot;ZellnerSiow&quot;, &quot;FLS&quot;, &quot;intrinsic.MGC&quot; and &quot;IHG&quot; (see details).</p>
</td></tr>
<tr><td><code id="Btest_+3A_prior.models">prior.models</code></td>
<td>
<p>Type of prior probabilities of the models (to be literally specified). Possible choices are
&quot;Constant&quot; and &quot;User&quot; (see details).</p>
</td></tr>
<tr><td><code id="Btest_+3A_priorprobs">priorprobs</code></td>
<td>
<p>A named vector ir list (same length and names as in argument
<code>models</code>) with the prior probabilities of the models (used in combination
of <code>prior.models="User"</code>). If the provided object
is not named, then the order in the list of <code>models</code> is used to assign the prior
probabilities</p>
</td></tr>
<tr><td><code id="Btest_+3A_null.model">null.model</code></td>
<td>
<p>The name of the null model (eg. the one nested in all the others).
By default, the names of covariates in the different
models are used to identify the null model. An error is produced if such identification fails.
This identification
is not performed if the definition of the null model is provided, with this argument,
by the user.
Note that the (the <code>null.model</code> must coincide with that model with the largest
sum of squared errors and should be smaller in dimension to any other model).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Bayes factors, BFi0, are expressed in relation with the simplest model
(the one nested in all the others). Then, the posterior probabilities of the
entertained models are obtained as
</p>
<p>Pr(Mi | <code>data</code>)=Pr(Mi)*BFi0/C,
</p>
<p>where Pr(Mi) is the prior probability of model Mi and C is the normalizing
constant.
</p>
<p>The Bayes factor B_i depends on the prior assigned for the parameters in the regression
models Mi and <code>Bvs</code> implements a number of popular choices. The &quot;Robust&quot;
prior by Bayarri, Berger, Forte and Garcia-Donato (2012) is the recommended (and default) choice.
This prior prior can be implemented in a more stable way using the derivations in Greenaway (2019) 
and that are available in BayesVarSel since version 2.2.x setting the argument to &quot;Robust.G&quot;. 
</p>
<p>Additional options are &quot;gZellner&quot; a prior which 
corresponds to the
prior in Zellner (1986) with g=n. Also &quot;Liangetal&quot; prior is the
hyper-g/n with a=3 (see the original paper Liang et al 2008, for details).
&quot;ZellnerSiow&quot; is the multivariate Cauchy prior proposed by Zellner and Siow
(1980, 1984), further studied by Bayarri and Garcia-Donato (2007).
&quot;FLS&quot; is the (benchmark) prior recommended by Fernandez, Ley and Steel (2001) which is
the prior in Zellner (1986) with g=max(n, p*p) p being the number of
covariates to choose from (the most complex model has p+number of fixed
covariates).
&quot;intrinsic.MGC&quot; is the intrinsic prior derived by Moreno, Giron, Casella (2015)
and &quot;IHG&quot; corresponds to the intrinsic hyper-g prior derived in Berger, Garcia-Donato, 
Moreno and Pericchi (2022).
</p>
<p>With respect to the prior over the model space Pr(Mi) three possibilities
are implemented: &quot;Constant&quot;, under which every model has the same prior
probability and &quot;User&quot;. With this last option, the prior probabilities are
defined through the named list <code>priorprobs</code>. These probabilities can be
given unnormalized.
</p>
<p>Limitations: the error &quot;A Bayes factor is infinite.&quot;. Bayes factors can be
extremely big numbers if i) the sample size is even moderately large or if
ii) a model is much better (in terms of fit) than the model taken as the
null model. We are currently working on more robust implementations of the
functions to handle these problems. In the meanwhile you could try using the
g-Zellner prior (which is the most simple one and results, in these cases,
should not vary much with the prior) and/or using more accurate definitions
of the simplest model.
</p>


<h3>Value</h3>

<p><code>Btest</code> returns an object of type <code>Btest</code> which is a
<code>list</code> with the following elements: </p>
<table>
<tr><td><code>BFio</code></td>
<td>
<p>A vector with the
Bayes factor of each model to the simplest model.</p>
</td></tr> <tr><td><code>PostProbi</code></td>
<td>
<p>A
vector with the posterior probabilities of each model.</p>
</td></tr> <tr><td><code>models</code></td>
<td>
<p>A
list with the entertained models. </p>
</td></tr> <tr><td><code>nullmodel</code></td>
<td>
<p>The position of the
simplest model. </p>
</td></tr>
<tr><td><code>prior.betas</code></td>
<td>
<p>prior.betas</p>
</td></tr>
<tr><td><code>prior.models</code></td>
<td>
<p>prior.models</p>
</td></tr>
<tr><td><code>priorprobs</code></td>
<td>
<p>priorprobs</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gonzalo Garcia-Donato and Anabel Forte
</p>
<p>Maintainer: &lt;anabel.forte@uv.es&gt;
</p>


<h3>References</h3>

<p>Bayarri, M.J., Berger, J.O., Forte, A. and Garcia-Donato, G.
(2012)&lt;DOI:10.1214/12-aos1013&gt; Criteria for Bayesian Model choice with
Application to Variable Selection. The Annals of Statistics. 40: 1550-1557.
</p>
<p>Bayarri, M.J. and Garcia-Donato, G. (2007)&lt;DOI:10.1093/biomet/asm014&gt;
Extending conventional priors for testing general hypotheses in linear
models. Biometrika, 94:135-152.
</p>
<p>Barbieri, M and Berger, J (2004)&lt;DOI:10.1214/009053604000000238&gt; Optimal
Predictive Model Selection. The Annals of Statistics, 32, 870-897.
</p>
<p>Berger, J., Garcıa-Donato, G., Moreno, E., and Pericchi, L. (2022). 
The intrinsic hyper-g prior for normal linear models. in preparation.
</p>
<p>Fernandez, C., Ley, E. and Steel, M.F.J.
(2001)&lt;DOI:10.1016/s0304-4076(00)00076-2&gt; Benchmark priors for Bayesian
model averaging. Journal of Econometrics, 100, 381-427.
</p>
<p>Greenaway, M. (2019) Numerically stable approximate Bayesian methods for
generalized linear mixed models and linear model selection. Thesis (Department of Statistics,
University of Sydney).
</p>
<p>Liang, F., Paulo, R., Molina, G., Clyde, M. and Berger,J.O.
(2008)&lt;DOI:10.1198/016214507000001337&gt; Mixtures of g-priors for Bayesian
Variable Selection. Journal of the American Statistical Association.
103:410-423
</p>
<p>Zellner, A. and Siow, A. (1980)&lt;DOI:10.1007/bf02888369&gt; Posterior Odds Ratio
for Selected Regression Hypotheses. In Bayesian Statistics 1 (J.M. Bernardo,
M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.) 585-603. Valencia:
University Press.
</p>
<p>Zellner, A. and Siow, A. (1984) Basic Issues in Econometrics. Chicago:
University of Chicago Press.
</p>
<p>Zellner, A. (1986)&lt;DOI:10.2307/2233941&gt; On Assessing Prior Distributions and
Bayesian Regression Analysis with g-prior Distributions. In Bayesian
Inference and Decision techniques: Essays in Honor of Bruno de Finetti (A.
Zellner, ed.) 389-399. Edward Elgar Publishing Limited.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Bvs">Bvs</a></code> for variable selection within linear
regression models
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
#Analysis of Crime Data
#load data
data(UScrime)
#Model selection among the following models: (note model1 is nested in all the others)
model1&lt;- y ~ 1 + Prob
model2&lt;- y ~ 1 + Prob + Time
model3&lt;- y ~ 1 + Prob + Po1 + Po2
model4&lt;- y ~ 1 + Prob + So
model5&lt;- y ~ .

#Equal prior probabilities for models:
crime.BF&lt;- Btest(models=list(basemodel=model1,
	ProbTimemodel=model2, ProbPolmodel=model3,
	ProbSomodel=model4, fullmodel=model5), data=UScrime)

#Another configuration of prior probabilities of models:
crime.BF2&lt;- Btest(models=list(basemodel=model1, ProbTimemodel=model2,
	ProbPolmodel=model3, ProbSomodel=model4, fullmodel=model5),
	data=UScrime, prior.models = "User", priorprobs=list(basemodel=1/8,
	ProbTimemodel=1/8, ProbPolmodel=1/2, ProbSomodel=1/8, fullmodel=1/8))
#same as:
#crime.BF2&lt;- Btest(models=list(basemodel=model1, ProbTimemodel=model2,
	#ProbPolmodel=model3,ProbSomodel=model4, #fullmodel=model5), data=UScrime,
	#prior.models = "User", priorprobs=list(basemodel=1, ProbTimemodel=1,
	#ProbPolmodel=4, #ProbSomodel=1, fullmodel=1))

## End(Not run)
</code></pre>

<hr>
<h2 id='Bvs'>Bayesian Variable Selection for linear regression models</h2><span id='topic+Bvs'></span>

<h3>Description</h3>

<p>Exact computation of summaries of the posterior distribution using
sequential computation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Bvs(
  formula,
  data,
  null.model = paste(as.formula(formula)[[2]], " ~ 1", sep = ""),
  prior.betas = "Robust",
  prior.models = "ScottBerger",
  n.keep = 10,
  time.test = TRUE,
  priorprobs = NULL,
  parallel = FALSE,
  n.nodes = detectCores()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Bvs_+3A_formula">formula</code></td>
<td>
<p>Formula defining the most complex (full) regression model in the
analysis. See details.</p>
</td></tr>
<tr><td><code id="Bvs_+3A_data">data</code></td>
<td>
<p>data frame containing the data.</p>
</td></tr>
<tr><td><code id="Bvs_+3A_null.model">null.model</code></td>
<td>
<p>A formula defining which is the simplest (null) model.
It should be nested in the full model. By default, the null model is defined
to be the one with just the intercept.</p>
</td></tr>
<tr><td><code id="Bvs_+3A_prior.betas">prior.betas</code></td>
<td>
<p>Prior distribution for regression parameters within each
model (to be literally specified). Possible choices include &quot;Robust&quot;, &quot;Robust.G&quot;, &quot;Liangetal&quot;, &quot;gZellner&quot;,
&quot;ZellnerSiow&quot;, &quot;FLS&quot;, &quot;intrinsic.MGC&quot; and &quot;IHG&quot; (see details).</p>
</td></tr>
<tr><td><code id="Bvs_+3A_prior.models">prior.models</code></td>
<td>
<p>Prior distribution over the model space (to be literally specified). Possible
choices are &quot;Constant&quot;, &quot;ScottBerger&quot; and &quot;User&quot; (see details).</p>
</td></tr>
<tr><td><code id="Bvs_+3A_n.keep">n.keep</code></td>
<td>
<p>How many of the most probable models are to be kept? By
default is set to 10, which is automatically adjusted if 10 is greater than
the total number of models.</p>
</td></tr>
<tr><td><code id="Bvs_+3A_time.test">time.test</code></td>
<td>
<p>If TRUE and the number of variables is moderately large
(&gt;=18) a preliminary test to estimate computational time is performed.</p>
</td></tr>
<tr><td><code id="Bvs_+3A_priorprobs">priorprobs</code></td>
<td>
<p>A p+1 (p is the number of non-fixed covariates)
dimensional vector defining the prior probabilities Pr(M_i) (should be used
in the case where <code>prior.models</code>= &quot;User&quot;; see details.)</p>
</td></tr>
<tr><td><code id="Bvs_+3A_parallel">parallel</code></td>
<td>
<p>A logical parameter specifying whether parallel computation
must be used (if set to TRUE)</p>
</td></tr>
<tr><td><code id="Bvs_+3A_n.nodes">n.nodes</code></td>
<td>
<p>The number of cores to be used if parallel computation is used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The model space is the set of all models, Mi, that contain the intercept and
are nested in that specified by <code>formula</code>. The simplest of such models,
M0, contains only the intercept. Then <code>Bvs</code> provides exact summaries of
the posterior distribution over this model space, that is, summaries of the
discrete distribution which assigns to each model Mi its probability given
the data:
</p>
<p>Pr(Mi | <code>data</code>)=Pr(Mi)*Bi/C,
</p>
<p>where Bi is the Bayes factor of Mi to M0, Pr(Mi) is the prior probability of
Mi and C is the normalizing constant.
</p>
<p>The Bayes factor B_i depends on the prior assigned for the parameters in the regression
models Mi and <code>Bvs</code> implements a number of popular choices. The &quot;Robust&quot;
prior by Bayarri, Berger, Forte and Garcia-Donato (2012) is the recommended (and default) choice.
This prior prior can be implemented in a more stable way using the derivations in Greenaway (2019) 
and that are available in BayesVarSel since version 2.2.x setting the argument to &quot;Robust.G&quot;. 
</p>
<p>Additional options are &quot;gZellner&quot; a prior which 
corresponds to the
prior in Zellner (1986) with g=n. Also &quot;Liangetal&quot; prior is the
hyper-g/n with a=3 (see the original paper Liang et al 2008, for details).
&quot;ZellnerSiow&quot; is the multivariate Cauchy prior proposed by Zellner and Siow
(1980, 1984), further studied by Bayarri and Garcia-Donato (2007).
&quot;FLS&quot; is the (benchmark) prior recommended by Fernandez, Ley and Steel (2001) which is
the prior in Zellner (1986) with g=max(n, p*p) p being the number of
covariates to choose from (the most complex model has p+number of fixed
covariates).
&quot;intrinsic.MGC&quot; is the intrinsic prior derived by Moreno, Giron, Casella (2015)
and &quot;IHG&quot; corresponds to the intrinsic hyper-g prior derived in Berger, Garcia-Donato, 
Moreno and Pericchi (2022).
</p>
<p>With respect to the prior over the model space Pr(Mi) three possibilities
are implemented: &quot;Constant&quot;, under which every model has the same prior
probability, &quot;ScottBerger&quot; under which Pr(Mi) is inversely proportional to
the number of models of that dimension, and &quot;User&quot; (see below). The
&quot;ScottBerger&quot; prior was studied by Scott and Berger (2010) and controls for
multiplicity (default choice since version 1.7.0).
</p>
<p>When the parameter <code>prior.models</code>=&quot;User&quot;, the prior probabilities are
defined through the p+1 dimensional parameter vector <code>priorprobs</code>. Let
k be the number of explanatory variables in the simplest model (the one
defined by <code>fixed.cov</code>) then except for the normalizing constant, the
first component of <code>priorprobs</code> must contain the probability of each
model with k covariates (there is only one); the second component of
<code>priorprobs</code> should contain the probability of each model with k+1
covariates and so on. Finally, the p+1 component in <code>priorprobs</code>
defined the probability of the most complex model (that defined by
<code>formula</code>. That is
</p>
<p><code>priorprobs</code>[j]=Cprior*Pr(M_i such that M_i has j-1+k explanatory variables)
</p>
<p>where Cprior is the normalizing constant for the prior, i.e
<code>Cprior=1/sum(priorprobs*choose(p,0:p)</code>.
</p>
<p>Note that <code>prior.models</code>=&quot;Constant&quot; is equivalent to the combination
<code>prior.models</code>=&quot;User&quot; and <code>priorprobs=rep(1,(p+1))</code> but the
internal functions are not the same and you can obtain small variations in
results due to these differences in the implementation.
</p>
<p>Similarly, <code>prior.models</code> = &quot;ScottBerger&quot; is equivalent to the
combination <code>prior.models</code>= &quot;User&quot; and <code>priorprobs</code> =
<code>1/choose(p,0:p)</code>.
</p>
<p>The case where n&lt;p is handled assigning to the Bayes factors of models with k regressors
with n&lt;k a value of 1. This should be interpreted as a generalization of the
null predictive matching in Bayarri et al (2012). Use <code><a href="#topic+GibbsBvs">GibbsBvs</a></code> for cases where
p&gt;&gt;.
</p>
<p>Limitations: about the error &quot;A Bayes factor is infinite.&quot;. Bayes factors can be
extremely big numbers if i) the sample size is large or if
ii) a competing model is much better (in terms of fit) than the model taken as the
null model. If you see this error, try to use the more stable version of the
robust prior &quot;Robust.g&quot; and/or reconisder using more accurate and realistic definitions
of the simplest model (via the <code>null.model</code> argument).
</p>


<h3>Value</h3>

<p><code>Bvs</code> returns an object of class <code>Bvs</code> with the following
elements: </p>
<table>
<tr><td><code>time</code></td>
<td>
<p>The internal time consumed in solving the problem</p>
</td></tr>
<tr><td><code>lmfull</code></td>
<td>
<p>The <code>lm</code> class object that results when the model
defined by <code>formula</code> is fitted by <code>lm</code></p>
</td></tr>
<tr><td><code>lmnull</code></td>
<td>
<p>The
<code>lm</code> class object that results when the model defined by
<code>null.model</code> is fitted by <code>lm</code></p>
</td></tr>
<tr><td><code>variables</code></td>
<td>
<p>The name of all
the potential explanatory variables (the set of variables to select from).</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>Number of observations</p>
</td></tr> <tr><td><code>p</code></td>
<td>
<p>Number of explanatory variables
to select from</p>
</td></tr> <tr><td><code>k</code></td>
<td>
<p>Number of fixed variables</p>
</td></tr> <tr><td><code>HPMbin</code></td>
<td>
<p>The
binary expression of the Highest Posterior Probability model</p>
</td></tr>
<tr><td><code>modelsprob</code></td>
<td>
<p>A <code>data.frame</code> which summaries the <code>n.keep</code>
most probable, a posteriori models, and their associated probability.</p>
</td></tr>
<tr><td><code>inclprob</code></td>
<td>
<p>A named vector with the inclusion probabilities of all
the variables.</p>
</td></tr> <tr><td><code>jointinclprob</code></td>
<td>
<p>A <code>data.frame</code> with the joint
inclusion probabilities of all the variables.</p>
</td></tr> <tr><td><code>postprobdim</code></td>
<td>
<p>Posterior
probabilities of the dimension of the true model</p>
</td></tr> <tr><td><code>call</code></td>
<td>
<p>The
<code>call</code> to the function</p>
</td></tr> 
<tr><td><code>C</code></td>
<td>
<p>The value of the normalizing constant (C=sum BiPr(Mi), for Mi in the model space)</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p><code>full</code> or <code>parallel</code> in case of
parallel computation</p>
</td></tr>
<tr><td><code>prior.betas</code></td>
<td>
<p>prior.betas</p>
</td></tr>
<tr><td><code>prior.models</code></td>
<td>
<p>prior.models</p>
</td></tr>
<tr><td><code>priorprobs</code></td>
<td>
<p>priorprobs</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gonzalo Garcia-Donato and Anabel Forte
</p>
<p>Maintainer: &lt;anabel.forte@uv.es&gt;
</p>


<h3>References</h3>

<p>Bayarri, M.J., Berger, J.O., Forte, A. and Garcia-Donato, G.
(2012)&lt;DOI:10.1214/12-aos1013&gt; Criteria for Bayesian Model choice with
Application to Variable Selection. The Annals of Statistics. 40: 1550-1557.
</p>
<p>Berger, J., Garcıa-Donato, G., Moreno, E., and Pericchi, L. (2022). 
The intrinsic hyper-g prior for normal linear models. in preparation.
</p>
<p>Bayarri, M.J. and Garcia-Donato, G. (2007)&lt;DOI:10.1093/biomet/asm014&gt;
Extending conventional priors for testing general hypotheses in linear
models. Biometrika, 94:135-152.
</p>
<p>Barbieri, M and Berger, J (2004)&lt;DOI:10.1214/009053604000000238&gt; Optimal
Predictive Model Selection. The Annals of Statistics, 32, 870-897.
</p>
<p>Fernandez, C., Ley, E. and Steel, M.F.J.
(2001)&lt;DOI:10.1016/s0304-4076(00)00076-2&gt; Benchmark priors for Bayesian
model averaging. Journal of Econometrics, 100, 381-427.
</p>
<p>Greenaway, M. (2019) Numerically stable approximate Bayesian methods for
generalized linear mixed models and linear model selection. Thesis (Department of Statistics,
University of Sydney).
</p>
<p>Liang, F., Paulo, R., Molina, G., Clyde, M. and Berger,J.O.
(2008)&lt;DOI:10.1198/016214507000001337&gt; Mixtures of g-priors for Bayesian
Variable Selection. Journal of the American Statistical Association.
103:410-423
</p>
<p>Moreno, E., Giron, J. and Casella, G. (2015) Posterior model consistency
in variable selection as the model dimension grows. Statistical Science. 30: 228-241.
</p>
<p>Zellner, A. and Siow, A. (1980)&lt;DOI:10.1007/bf02888369&gt; Posterior Odds Ratio
for Selected Regression Hypotheses. In Bayesian Statistics 1 (J.M. Bernardo,
M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.) 585-603. Valencia:
University Press.
</p>
<p>Zellner, A. and Siow, A. (1984). Basic Issues in Econometrics. Chicago:
University of Chicago Press.
</p>
<p>Zellner, A. (1986)&lt;DOI:10.2307/2233941&gt; On Assessing Prior Distributions and
Bayesian Regression Analysis with g-prior Distributions. In Bayesian
Inference and Decision techniques: Essays in Honor of Bruno de Finetti (A.
Zellner, ed.) 389-399. Edward Elgar Publishing Limited.
</p>


<h3>See Also</h3>

<p>Use <code><a href="#topic+print.Bvs">print.Bvs</a></code> for the best visited models and an 
estimation of their posterior probabilities and  <code><a href="#topic+summary.Bvs">summary.Bvs</a></code> for
summaries of the posterior distribution.
</p>
<p><code><a href="#topic+plot.Bvs">plot.Bvs</a></code> for several plots of the result,
<code><a href="#topic+BMAcoeff">BMAcoeff</a></code> for obtaining model averaged simulations
of regression coefficients and <code><a href="#topic+predict.Bvs">predict.Bvs</a></code> for
predictions.
</p>
<p><code><a href="#topic+GibbsBvs">GibbsBvs</a></code> for a heuristic approximation based on
Gibbs sampling (recommended when p&gt;20, no other possibilities when p&gt;31).
</p>
<p>See <code><a href="#topic+GibbsBvsF">GibbsBvsF</a></code> if there are factors among the explanatory variables
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
#Analysis of Crime Data
#load data
data(UScrime)

#Default arguments are Robust prior for the regression parameters
#and constant prior over the model space
#Here we keep the 1000 most probable models a posteriori:
crime.Bvs&lt;- Bvs(formula= y ~ ., data=UScrime, n.keep=1000)

#A look at the results:
crime.Bvs

summary(crime.Bvs)

#A plot with the posterior probabilities of the dimension of the
#true model:
plot(crime.Bvs, option="dimension")

#Two image plots of the conditional inclusion probabilities:
plot(crime.Bvs, option="conditional")
plot(crime.Bvs, option="not")

## End(Not run)

</code></pre>

<hr>
<h2 id='GibbsBvs'>Bayesian Variable Selection for linear regression models using Gibbs
sampling.</h2><span id='topic+GibbsBvs'></span>

<h3>Description</h3>

<p>Approximate computation of summaries of the posterior distribution using a
Gibbs sampling algorithm to explore the model space and frequency of
&quot;visits&quot; to construct the estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GibbsBvs(
  formula,
  data,
  null.model = paste(as.formula(formula)[[2]], " ~ 1", sep = ""),
  prior.betas = "Robust",
  prior.models = "ScottBerger",
  n.iter = 10000,
  init.model = "Full",
  n.burnin = 500,
  n.thin = 1,
  time.test = TRUE,
  priorprobs = NULL,
  seed = runif(1, 0, 16091956)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GibbsBvs_+3A_formula">formula</code></td>
<td>
<p>Formula defining the most complex regression model in the
analysis. See details.</p>
</td></tr>
<tr><td><code id="GibbsBvs_+3A_data">data</code></td>
<td>
<p>data frame containing the data.</p>
</td></tr>
<tr><td><code id="GibbsBvs_+3A_null.model">null.model</code></td>
<td>
<p>A formula defining which is the simplest (null) model.
It should be nested in the full model. By default, the null model is defined
to be the one with just the intercept.</p>
</td></tr>
<tr><td><code id="GibbsBvs_+3A_prior.betas">prior.betas</code></td>
<td>
<p>Prior distribution for regression parameters within each
model (to be literally specified). Possible choices include &quot;Robust&quot;, &quot;Robust.G&quot;, &quot;Liangetal&quot;, &quot;gZellner&quot;,
&quot;ZellnerSiow&quot;, &quot;FLS&quot;, &quot;intrinsic.MGC&quot; and &quot;IHG&quot; (see details).</p>
</td></tr>
<tr><td><code id="GibbsBvs_+3A_prior.models">prior.models</code></td>
<td>
<p>Prior distribution over the model space (to be literally specified). Possible
choices are &quot;Constant&quot;, &quot;ScottBerger&quot; and &quot;User&quot; (see details).</p>
</td></tr>
<tr><td><code id="GibbsBvs_+3A_n.iter">n.iter</code></td>
<td>
<p>The total number of iterations performed after the burn in
process.</p>
</td></tr>
<tr><td><code id="GibbsBvs_+3A_init.model">init.model</code></td>
<td>
<p>The model at which the simulation process starts. Options
include &quot;Null&quot; (the model only with the covariates specified in
<code>fixed.cov</code>), &quot;Full&quot; (the model defined by <code>formula</code>), &quot;Random&quot; (a
randomly selected model) and a vector with p (the number of covariates to
select from) zeros and ones defining a model. When p&gt;n the dimension of the
init.model must be smaller than n. Otherwise the function produces
an error.</p>
</td></tr>
<tr><td><code id="GibbsBvs_+3A_n.burnin">n.burnin</code></td>
<td>
<p>Length of burn in, i.e. number of iterations to discard at
the beginning.</p>
</td></tr>
<tr><td><code id="GibbsBvs_+3A_n.thin">n.thin</code></td>
<td>
<p>Thinning rate. Must be a positive integer.  Set 'n.thin' &gt; 1
to save memory and computation time if 'n.iter' is large. Default is 1. This
parameter jointly with <code>n.iter</code> sets the number of simulations kept and
used to construct the estimates so is important to keep in mind that a large
value for 'n.thin' can reduce the precision of the results</p>
</td></tr>
<tr><td><code id="GibbsBvs_+3A_time.test">time.test</code></td>
<td>
<p>If TRUE and the number of variables is large (&gt;=21) a
preliminary test to estimate computational time is performed.</p>
</td></tr>
<tr><td><code id="GibbsBvs_+3A_priorprobs">priorprobs</code></td>
<td>
<p>A p+1 dimensional vector defining the prior probabilities
Pr(M_i) (should be used in the case where <code>prior.models</code>=&quot;User&quot;; see
the details in <code><a href="#topic+Bvs">Bvs</a></code>.)</p>
</td></tr>
<tr><td><code id="GibbsBvs_+3A_seed">seed</code></td>
<td>
<p>A seed to initialize the random number generator</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a heuristic approximation to the function
<code><a href="#topic+Bvs">Bvs</a></code> so the details there apply also here.
</p>
<p>The algorithm implemented is a Gibbs sampling-based searching algorithm
originally proposed by George and McCulloch (1997). Garcia-Donato and
Martinez-Beneito (2013) have shown that this simple sampling strategy in
combination with estimates based on frequency of visits (the one here
implemented) provides very reliable results.
</p>


<h3>Value</h3>

<p><code>GibbsBvs</code> returns an object of class <code>Bvs</code> with the
following elements: </p>
<table>
<tr><td><code>time</code></td>
<td>
<p>The internal time consumed in solving the
problem</p>
</td></tr> <tr><td><code>lmfull</code></td>
<td>
<p>The <code>lm</code> class object that results when the
model defined by <code>formula</code> is fitted by <code>lm</code></p>
</td></tr> <tr><td><code>lmnull</code></td>
<td>
<p>The
<code>lm</code> class object that results when the model defined by
<code>fixed.cov</code> is fitted by <code>lm</code></p>
</td></tr> <tr><td><code>variables</code></td>
<td>
<p>The name of all
the potential explanatory variables</p>
</td></tr> <tr><td><code>n</code></td>
<td>
<p>Number of observations</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>Number of explanatory variables to select from</p>
</td></tr> <tr><td><code>k</code></td>
<td>
<p>Number
of fixed variables</p>
</td></tr> <tr><td><code>HPMbin</code></td>
<td>
<p>The binary expression of the most
probable model found.</p>
</td></tr> <tr><td><code>inclprob</code></td>
<td>
<p>A named vector with the
estimates of the inclusion probabilities of all the variables.</p>
</td></tr>
<tr><td><code>jointinclprob</code></td>
<td>
<p>A <code>data.frame</code> with the estimates of the joint
inclusion probabilities of all the variables.</p>
</td></tr> <tr><td><code>postprobdim</code></td>
<td>
<p>Estimates
of posterior probabilities of the dimension of the true model.</p>
</td></tr>
<tr><td><code>modelslogBF</code></td>
<td>
<p>A matrix with both the binary representation of the
visited models after the burning period and the Bayes factor (log scale) of
that model to the null model.</p>
</td></tr><tr><td><code>priorprobs</code></td>
<td>
<p>If <code>prior.models</code>=&quot;User&quot; then this vector is stored here. Else, the #' type of prior as defined in <code>prior.models</code></p>
</td></tr><tr><td><code>call</code></td>
<td>
<p>The <code>call</code> to the
function.</p>
</td></tr>
<tr><td><code>C</code></td>
<td>
<p>An estimation of the normalizing constant (C=sum Bi Pr(Mi), for Mi in the model space) using the method in George and McCulloch (1997).</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p><code>gibbs</code></p>
</td></tr>
<tr><td><code>prior.betas</code></td>
<td>
<p>prior.betas</p>
</td></tr>
<tr><td><code>prior.models</code></td>
<td>
<p>prior.models</p>
</td></tr>
<tr><td><code>priorprobs</code></td>
<td>
<p>priorprobs</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gonzalo Garcia-Donato and Anabel Forte
</p>


<h3>References</h3>

<p>Garcia-Donato, G. and Martinez-Beneito, M.A.
(2013)&lt;DOI:10.1080/01621459.2012.742443&gt; On sampling strategies in Bayesian
variable selection problems with large model spaces. Journal of the American
Statistical Association, 108: 340-352.
</p>
<p>George E. and McCulloch R. (1997) Approaches for Bayesian variable
selection. Statistica Sinica, 7, 339:372.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.Bvs">plot.Bvs</a></code> for several plots of the result,
<code><a href="#topic+BMAcoeff">BMAcoeff</a></code> for obtaining model averaged simulations
of regression coefficients and <code><a href="#topic+predict.Bvs">predict.Bvs</a></code> for
predictions. 
</p>
<p>See <code><a href="#topic+GibbsBvsF">GibbsBvsF</a></code> if there are factors among the explanatory variables.
</p>
<p>See <code><a href="#topic+pltltn">pltltn</a></code> for corrections on estimations for the
situation where p&gt;&gt;n. See the help in  <code><a href="#topic+pltltn">pltltn</a></code> for an application
in this situation.
</p>
<p>Consider <code><a href="#topic+Bvs">Bvs</a></code> for exact
version obtained enumerating all entertained models (recommended when
p&lt;20).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
#Analysis of Ozone35 data

data(Ozone35)

#We use here the (Zellner) g-prior for
#regression parameters and constant prior
#over the model space
#In this Gibbs sampling scheme, we perform 10100 iterations,
#of which the first 100 are discharged (burnin) and of the remaining
#only one each 10 is kept.
#as initial model we use the Full model
Oz35.GibbsBvs&lt;- GibbsBvs(formula= y ~ ., data=Ozone35, prior.betas="gZellner",
prior.models="Constant", n.iter=10000, init.model="Full", n.burnin=100,
time.test = FALSE)

#Note: this is a heuristic approach and results are estimates
#of the exact answer.

#with the print we can see which is the most probable model
#among the visited
Oz35.GibbsBvs

#The estimation of inclusion probabilities and
#the model-averaged estimation of parameters:
summary(Oz35.GibbsBvs)

#Plots:
plot(Oz35.GibbsBvs, option="conditional")

## End(Not run)

</code></pre>

<hr>
<h2 id='GibbsBvsF'>Bayesian Variable Selection with Factors for linear regression models using Gibbs
sampling.</h2><span id='topic+GibbsBvsF'></span>

<h3>Description</h3>

<p>Numerical and factor variable selection from a Bayesian perspective. The posterior distribution is approximated
with Gibbs sampling
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GibbsBvsF(
  formula,
  data,
  null.model = paste(as.formula(formula)[[2]], " ~ 1", sep = ""),
  prior.betas = "Robust",
  prior.models = "SBSB",
  n.iter = 10000,
  init.model = "Full",
  n.burnin = 500,
  n.thin = 1,
  time.test = TRUE,
  seed = runif(1, 0, 16091956)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GibbsBvsF_+3A_formula">formula</code></td>
<td>
<p>Formula defining the most complex linear model in the
analysis. See details.</p>
</td></tr>
<tr><td><code id="GibbsBvsF_+3A_data">data</code></td>
<td>
<p>data frame containing the data.</p>
</td></tr>
<tr><td><code id="GibbsBvsF_+3A_null.model">null.model</code></td>
<td>
<p>A formula defining which is the simplest (null) model.
It should be nested in the full model. It is compulsory that the null model
contains the intercept and by default, the null model is defined
to be the one with just the intercept</p>
</td></tr>
<tr><td><code id="GibbsBvsF_+3A_prior.betas">prior.betas</code></td>
<td>
<p>Prior distribution for regression parameters within each
model (to be literally specified). Possible choices include &quot;Robust&quot;, &quot;Robust.G&quot;, &quot;Liangetal&quot;, &quot;gZellner&quot;,
&quot;ZellnerSiow&quot; (see details in <code><a href="#topic+Bvs">Bvs</a></code>).</p>
</td></tr>
<tr><td><code id="GibbsBvsF_+3A_prior.models">prior.models</code></td>
<td>
<p>Prior distribution over the model space (to be literally specified). Possible
choices (see details) are &quot;Const&quot;, &quot;SB&quot;, &quot;ConstConst&quot;, &quot;SBConst&quot; and &quot;SBSB&quot; (the default).</p>
</td></tr>
<tr><td><code id="GibbsBvsF_+3A_n.iter">n.iter</code></td>
<td>
<p>The total number of iterations performed after the burn in
process.</p>
</td></tr>
<tr><td><code id="GibbsBvsF_+3A_init.model">init.model</code></td>
<td>
<p>The model at which the simulation process starts. Options
include &quot;Null&quot; (the model only with the covariates specified in
<code>fixed.cov</code>), &quot;Full&quot; (the model defined by <code>formula</code>), &quot;Random&quot; (a
randomly selected model) and a vector with (pnum+sum_j L_j) zeros and ones defining a model.</p>
</td></tr>
<tr><td><code id="GibbsBvsF_+3A_n.burnin">n.burnin</code></td>
<td>
<p>Length of burn in, i.e. number of iterations to discard at
the beginning.</p>
</td></tr>
<tr><td><code id="GibbsBvsF_+3A_n.thin">n.thin</code></td>
<td>
<p>Thinning rate. Must be a positive integer.  Set 'n.thin' &gt; 1
to save memory and computation time if 'n.iter' is large. Default is 1. This
parameter jointly with <code>n.iter</code> sets the number of simulations kept and
used to construct the estimates so is important to keep in mind that a large
value for 'n.thin' can reduce the precision of the results</p>
</td></tr>
<tr><td><code id="GibbsBvsF_+3A_time.test">time.test</code></td>
<td>
<p>If TRUE and the number of variables is large (&gt;=21) a
preliminary test to estimate computational time is performed.</p>
</td></tr>
<tr><td><code id="GibbsBvsF_+3A_seed">seed</code></td>
<td>
<p>A seed to initialize the random number generator</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In practical terms, <code>GibbsBvsF</code> can be understood as a version of <code><a href="#topic+GibbsBvs">GibbsBvs</a></code> in the presence of factors.
The methodology implemented in <code>GibbsBvsF</code> to handle variable selection problems with factors
has been proposed in Garcia-Donato and Paulo (2018) leading to a method
for which results  do not depend on how the factors are
coded (eg. via <code><a href="stats.html#topic+contrast">contrast</a></code>).
</p>
<p>Internally, a rank defficient representation
of factors using dummies is used and the number of competing models considered is
</p>
<p>2^(pnum+sum_j L_j),
</p>
<p>where pnum is the number of numerical variables and L_j is the number of levels in factor j.
</p>
<p>A main difference with <code>Bvs</code> and <code>GibbsBvs</code> (due to the presence of factors) concerns the prior
probabilities on the model space:
</p>
<p>The options <code>prior.models="SBSB"</code>, <code>prior.models="ConstConst"</code> and <code>prior.models="SBConst"</code>
acknowledge the &quot;grouped&quot; nature of the dummy variables representing
factors through the use of two stage
priors described in Garcia-Donato and Paulo (2021). In the first stage probabilities over factors and numerical
variables are specified and (conditional on these) within the second stage
the probablities are apportioned over the different submodels defined
by the dummies. The default (and recommended, for the reasons argued in Garcia-Donato and Paulo,2021) 
option is &quot;SBSB&quot; which uses in both stages an assignment
of the type Scott-Berger so inversely proportional to the number of models of the same dimension. The
option &quot;ConstConst&quot; implements a uniform prior for both stages while &quot;SBConst&quot; uses a Scott-Berger prior
in the first stage and it is uniform in the second stage. Within all these priors, the prior inclusion probabilities
of factors and numerical variables are 1/2.
</p>
<p>The options <code>prior.models="Const"</code> and
<code>prior.models="SB"</code> do not have a staged structure and &quot;Const&quot; apportions the prior probabilities
uniformly over all possible models (2^(pnum+sum_j L_j)) and in &quot;SB&quot; the probability
is inversely proportional to the number of any model of the same dimension. In these cases, prior inclusion probabilities
of factors and numerical variables depend on the number of levels of factors and, in general, are not 1/2.
</p>


<h3>Value</h3>

<p><code>GibbsBvsF</code> returns an object of class <code>Bvs</code> with the
following elements: </p>
<table>
<tr><td><code>time</code></td>
<td>
<p>The internal time consumed in solving the
problem</p>
</td></tr> <tr><td><code>lmfull</code></td>
<td>
<p>The <code>lm</code> class object that results when the
model defined by <code>formula</code> is fitted by <code>lm</code></p>
</td></tr>
<tr><td><code>lmnull</code></td>
<td>
<p>The
<code>lm </code> class object that results when the model defined by
<code>fixed.cov</code> is fitted by <code>lm</code></p>
</td></tr>
<tr><td><code>variables</code></td>
<td>
<p>The name of all the potential explanatory variables (numerical or factors)</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>Number of observations</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>Number of explanatory variables (both numerical and factors) to select from</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>Number of fixed variables</p>
</td></tr>
<tr><td><code>HPMbin</code></td>
<td>
<p>The binary expression of the most
probable model found.</p>
</td></tr>
<tr><td><code>inclprob</code></td>
<td>
<p>A named vector with the
estimates of the inclusion probabilities of all the variables.</p>
</td></tr>
<tr><td><code>jointinclprob</code></td>
<td>
<p>A <code>data.frame</code> with the estimates of the joint
inclusion probabilities of all the variables.</p>
</td></tr>
<tr><td><code>postprobdim</code></td>
<td>
<p>Estimates
of posterior probabilities of the number of active variables in the true model (hence ranking from
<code>k </code> to <code>k+p</code>).</p>
</td></tr>
<tr><td><code>modelslogBF</code></td>
<td>
<p>A matrix with both the binary representation of the
active variables in the MCMC after the burning period and the Bayes factor (log scale) of
that model to the null model.</p>
</td></tr>
<tr><td><code>modelswllogBF</code></td>
<td>
<p>A matrix with both the binary representation of the
active variables (at the level of the levels in the factors) in the MCMC after the burning period and the Bayes factor (log scale) of
that model to the null model.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>The <code>call</code> to the
function.</p>
</td></tr>
<tr><td><code>C</code></td>
<td>
<p>An estimation of the normalizing constant (C=sum Bi Pr(Mi), for Mi in the model space) using the method in George and McCulloch (1997).</p>
</td></tr>
<tr><td><code>positions</code></td>
<td>
<p>A binary matrix with <code>p</code> rows and (pnum+sum_j L_j) columns. The 1's identify, for each variable (row) the position (column)
of dummies (in case of factor) or of the numerical variable grouped on that variable. (Its use is conceived for internal purposes).</p>
</td></tr>
<tr><td><code>positionsx</code></td>
<td>
<p>A <code>p</code> dimensional binary vector, stating which of the competing variables is a numerical variable. (Its use is conceived for internal purposes).</p>
</td></tr>
<tr><td><code>prior.betas</code></td>
<td>
<p>prior.betas</p>
</td></tr>
<tr><td><code>prior.models</code></td>
<td>
<p>prior.models</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p><code>gibbsWithFactors</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gonzalo Garcia-Donato and Anabel Forte
</p>


<h3>References</h3>

<p>Garcia-Donato, G. and Martinez-Beneito, M.A.
(2013)&lt;DOI:10.1080/01621459.2012.742443&gt; On sampling strategies in Bayesian
variable selection problems with large model spaces. Journal of the American
Statistical Association, 108: 340-352.
</p>
<p>Garcia-Donato, G. and Paulo, R. (2021) Variable selection in the presence of factors: 
a model selection perspective. Journal of American Statistical Association, Ahead-of- print(1-11).
</p>
<p>George E. and McCulloch R. (1997) Approaches for Bayesian variable
selection. Statistica Sinica, 7, 339:372.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.Bvs">plot.Bvs</a></code> for several plots of the result.
</p>
<p>Under construction: <code><a href="#topic+BMAcoeff">BMAcoeff</a></code> for obtaining model averaged simulations
of regression coefficients and <code><a href="#topic+predict.Bvs">predict.Bvs</a></code> for
predictions.
</p>
<p>See <code><a href="#topic+GibbsBvs">GibbsBvs</a></code> and <code><a href="#topic+Bvs">Bvs</a></code> when no factors are involved.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
data(diabetes, package="faraway")

#remove NA's and the column with the id of samples:
diabetes2&lt;- na.omit(diabetes)[,-1]

#For reproducibility:
set.seed(16091956)
#Now run the main instruction
diabetesVS&lt;- GibbsBvsF(formula= glyhb ~ ., data=diabetes2, n.iter=100000, n.burnin=5000)

summary(diabetesVS)

#A plot of the dimension of the true model,
plot(diabetesVS, option="dimension")

#A joint inclusion plot
plot(diabetesVS, option="joint")

#Now a similar exercise but with fixed variables:
diabetesVS2&lt;- GibbsBvsF(formula= glyhb ~ ., null.model= glyhb ~ chol+stab.glu,
		                   data=diabetes2, n.iter=100000, n.burnin=5000)


#and with fixed factors:
diabetesVS3&lt;- GibbsBvsF(formula= glyhb ~ ., null.model= glyhb ~ chol+stab.glu+location,
		                   data=diabetes2, n.iter=100000, n.burnin=5000)



## End(Not run)

</code></pre>

<hr>
<h2 id='Hald'>Hald data</h2><span id='topic+Hald'></span>

<h3>Description</h3>

<p>The following data relates to an engineering application that was interested
in the effect of the cement composition on heat evolved during hardening
(for more details, see Woods et al., 1932).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Hald
</code></pre>


<h3>Format</h3>

<p>A data frame with 13 observations on the following 5 variables.
</p>
 <dl>
<dt>y</dt><dd><p>Heat evolved per gram of cement (in calories)</p>
</dd>
<dt>x1</dt><dd><p>Amount of tricalcium aluminate</p>
</dd> <dt>x2</dt><dd><p>Amount
of tricalcium silicate</p>
</dd> <dt>x3</dt><dd><p>Amount of tetracalcium alumino
ferrite</p>
</dd> <dt>x4</dt><dd><p>Amount of dicalcium silicate</p>
</dd> </dl>



<h3>References</h3>

<p>Woods, H., Steinour, H. and Starke, H.
(1932)&lt;DOI:10.1021/ie50275a002&gt; Effect of Composition of Porland Cement on
Heat Evolved During Hardening. Industrial and Engineering Chemistry
Research, 24, 1207-1214.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Hald)

</code></pre>

<hr>
<h2 id='histBMA'>A function for histograms-like representations of objects of class
<code>bma.coeffs</code></h2><span id='topic+histBMA'></span>

<h3>Description</h3>

<p>The columns in <code>bma.coeffs</code> are simulations of the model averaged
posterior distribution. This normally is a mixture of a discrete (at zero)
and several continuous distributions. This plot provides a convenient
graphical summary of such distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>histBMA(
  x,
  covariate,
  n.breaks = 100,
  text = TRUE,
  gray.0 = 0.6,
  gray.no0 = 0.8
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="histBMA_+3A_x">x</code></td>
<td>
<p>An object of class <code>bma.coeffs</code></p>
</td></tr>
<tr><td><code id="histBMA_+3A_covariate">covariate</code></td>
<td>
<p>The name of an explanatory variable whose accompanying
coefficient is to be represented. This must be the name of one of the
columns in <code>x</code></p>
</td></tr>
<tr><td><code id="histBMA_+3A_n.breaks">n.breaks</code></td>
<td>
<p>The number of equally lentgh bars for the histogram</p>
</td></tr>
<tr><td><code id="histBMA_+3A_text">text</code></td>
<td>
<p>If set to TRUE the probability of the coefficient being zero is
added in top of the bar at zero. Note: this probability is based on the
models used in <code>bma.coeffs</code> (see details in that function)</p>
</td></tr>
<tr><td><code id="histBMA_+3A_gray.0">gray.0</code></td>
<td>
<p>A numeric value between 0 and 1 that specifies the darkness,
in a gray scale (0 is white and 1 is black) of the bar at zero</p>
</td></tr>
<tr><td><code id="histBMA_+3A_gray.no0">gray.no0</code></td>
<td>
<p>A numeric value between 0 and 1 that specifies the darkness,
in a gray scale (0 is white and 1 is black) of the bars different from zero</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function produces a histogram but with the peculiarity that the zero
values in the simulation are represented as bar centered at zero. The area
of all the bars is one and of these, the area of the bar at zero (colored
with <code>gray.0</code>) is, conditionally on the retained models (see details in
<code><a href="#topic+BMAcoeff">BMAcoeff</a></code>), the probability of that coefficient be
exactly zero. This number is included in the top of the zero bar if
<code>text</code> is set to TRUE.
</p>


<h3>Author(s)</h3>

<p>Gonzalo Garcia-Donato and Anabel Forte
</p>
<p>Maintainer: &lt;anabel.forte@uv.es&gt;
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+BMAcoeff">BMAcoeff</a></code>. Also see
<code><a href="#topic+Bvs">Bvs</a></code> and
<code><a href="#topic+GibbsBvs">GibbsBvs</a></code> for creating objects of the class
<code>BMAcoeff</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

## Not run: 

#Analysis of Crime Data
#load data
data(UScrime)

crime.Bvs&lt;- Bvs(formula= y ~ ., data=UScrime, n.keep=1000)
crime.Bvs.BMA&lt;- BMAcoeff(crime.Bvs, n.sim=10000)
#the best 1000 models are used in the mixture

#Observe the bimodality of the coefficient associated with regressor M
histBMA(crime.Bvs.BMA, "M")

#Note 1:
#The value in top of the bar at zero (0.251 in this case) is the probability of beta_M is
#zero conditional on a model space containing the 1000 models used in the mixture. This value
#should be closed to the exact value
#1-crime.Bvs$inclprob["M"]
#which in this case is 0.2954968
#if n.keep above is close to 2^15

#Note 2:
#The BMA posterior distribution of beta_M has two modes approximately located at 0 and 10
#If we summarize this distribution using the mean
mean(crime.Bvs.BMA[ ,"M"])
#or median
median(crime.Bvs.BMA[ ,"M"])
#we obtain values around 7 (or 7.6) which do not represent this distribution.

#With the Gibbs algorithms:
data(Ozone35)

Oz35.GibbsBvs&lt;- GibbsBvs(formula="y~.", data=Ozone35, prior.betas="gZellner",
prior.models="Constant", n.iter=10000, init.model="Full", n.burnin=100,
time.test = FALSE)
Oz35.GibbsBvs.BMA&lt;- BMAcoeff(Oz35.GibbsBvs, n.sim=10000)

histBMA(Oz35.GibbsBvs.BMA, "x6.x7")
#In this case (Gibbs sampling), the value in top of the bar at zero (0.366)
#basically should coincide (if n.sim is large enough)
#with the estimated complement of the inclusion probability
#1-Oz35.GibbsBvs$inclprob["x6.x7"]
#which in this case is 0.3638

## End(Not run)

</code></pre>

<hr>
<h2 id='Jointness'>Computation of Jointness measurements.</h2><span id='topic+Jointness'></span>

<h3>Description</h3>

<p><code>Jointness</code> computes the joint inclusion probabilitiy of two given
covariates as well as the jointness measurements of Ley and Steel (2007)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Jointness(x, covariates = "All")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Jointness_+3A_x">x</code></td>
<td>
<p>An object of class <code>Bvs</code></p>
</td></tr>
<tr><td><code id="Jointness_+3A_covariates">covariates</code></td>
<td>
<p>It can be either &quot;All&quot;(default) or a vector contaning the
name of two covariates.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>jointness</code> is returned.
</p>
<p>If <code>covariates</code> is &quot;All&quot; this object is a list with three matrices
containg different jointness measurements for all pairs of covariates is
returned.  In particular, for covariates i and j the jointness measurements
are:
</p>
<p>The Joint inclusion probabilities:
</p>
<p><code class="reqn">P(i and j)</code>
</p>
<p>And the two measurements of Ley and Steel (2007)
</p>
<p><code class="reqn">J*= P(i and j)/P(i or j)</code>
</p>
<p><code class="reqn">J*=P(i and j)/(P(i or j)-P(i and j))</code>
</p>
<p>If <code>covariates</code> is a vector of length 2, <code>Jointness</code> return a
list of four elements. The first three of them is a list of three values containing the
measurements above but just for the given pair of covariates. The fourth
element is the <code>covariates</code> vector.
</p>
<p>If method <code>print.jointness</code> is used a message with the meaning of the
measurement si printed.
</p>


<h3>Author(s)</h3>

<p>Gonzalo Garcia-Donato and Anabel Forte
</p>
<p>Maintainer: &lt;anabel.forte@uv.es&gt;
</p>


<h3>References</h3>

<p>Ley, E. and Steel, M.F.J. (2007)&lt;DOI:10.1016/j.jmacro.2006.12.002&gt;Jointness
in Bayesian variable selection with applications to growth regression.
Journal of Macroeconomics, 29(3):476-493.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Bvs">Bvs</a></code> and
<code><a href="#topic+GibbsBvs">GibbsBvs</a></code> for performing variable selection and
obtaining an object of class <code>Bvs</code>.
</p>
<p><code><a href="#topic+plot.Bvs">plot.Bvs</a></code> for different descriptive plots of the
results, <code><a href="#topic+BMAcoeff">BMAcoeff</a></code> for obtaining model averaged
simulations of regression coefficients and
<code><a href="#topic+predict.Bvs">predict.Bvs</a></code> for predictions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
#Analysis of Crime Data
#load data

data(UScrime)

crime.Bvs&lt;- Bvs(formula= y ~ ., data=UScrime, n.keep=1000)

#A look at the jointness measurements:
Jointness(crime.Bvs, covariates="All")

Jointness(crime.Bvs, covariates=c("Ineq","Prob"))
#---------
#The joint inclusion probability for Ineq and Prob is:  0.65
#---------
#The ratio between the probability of including both
#covariates and the probability of including at least one of then is: 0.66
#---------
#The probability of including both covariates at the same times is 1.95 times
#the probability of including one of them alone


## End(Not run)
</code></pre>

<hr>
<h2 id='OBICE'>OBICE data</h2><span id='topic+OBICE'></span>

<h3>Description</h3>

<p>Dataset corresponding to the OBICE study (Zurriaga et al 2011) where factors associated with childhood obesity
are studied. The data were collected in 2007 and 2008 through several questionnaries and 
n=1188 children were enroled in the study. It contains 155 variables.
This is a case and control study with 437 cases (obese) and 751 controls (not obese).
Purposedly the dataset is distributed without any post-processing
hence, many variables may contain unavailable observations coded in different way.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>OBICE
</code></pre>


<h3>Format</h3>

<p>A data frame with 1188 entries and 121 variables. The more relevant are described. Contact us if you need specific information of any other.
</p>

<dl>
<dt>Acostarse</dt><dd><p>:does he/she eat before going to bed? (yes/no) </p>
</dd>
<dt>ActFisica</dt><dd><p>:(physic activity) factor coded as 1 None; 2 less than monthly; 3 less than weekly; 4 less than 2/week; 5 at least 2/week </p>
</dd>
<dt>ActivDepor</dt><dd><p>:weekly hours devoted to sports activity </p>
</dd>
<dt>Almuerzo</dt><dd><p>: </p>
</dd>
<dt>Bebida</dt><dd><p>:(main dring accompanying the main meal) 1 water tap; 2 bottle water; 3 soda; 4 natural juices; 5 bottle juices; 6 Milk (and derivatives); 7 Other </p>
</dd>
<dt>Caso01</dt><dd><p>: </p>
</dd>
<dt>Cena</dt><dd><p>: </p>
</dd>
<dt>Chuches</dt><dd><p>:Sweets and soft drinks weekly consumption (how many times) </p>
</dd>
<dt>CincoComidas</dt><dd><p>:does he/she have regularly 5 meals per day? (0 is No; 1 is Yes) </p>
</dd>
<dt>clSocEl</dt><dd><p>: </p>
</dd>
<dt>clSocElla</dt><dd><p>: </p>
</dd>
<dt>clSocXiquet</dt><dd><p>: </p>
</dd>
<dt>ComedorEsc</dt><dd><p>: </p>
</dd>
<dt>Comida</dt><dd><p>: </p>
</dd>
<dt>Daceite</dt><dd><p>: </p>
</dd>
<dt>Dcereal</dt><dd><p>: </p>
</dd>
<dt>Desayuno</dt><dd><p>: </p>
</dd>
<dt>Descubrimiento</dt><dd><p>: </p>
</dd>
<dt>Dgalleta</dt><dd><p>: </p>
</dd>
<dt>Dislipemias</dt><dd><p>: </p>
</dd>
<dt>DislipeRelacion</dt><dd><p>: </p>
</dd>
<dt>Dleche</dt><dd><p>: </p>
</dd>
<dt>Dotros</dt><dd><p>: </p>
</dd>
<dt>Dpan</dt><dd><p>: </p>
</dd>
<dt>Dzumoenv</dt><dd><p>: </p>
</dd>
<dt>Dzumonat</dt><dd><p>: </p>
</dd>
<dt>Edad</dt><dd><p>:years old </p>
</dd>
<dt>EntreHoras</dt><dd><p>: </p>
</dd>
<dt>EstudiosMadre</dt><dd><p>: </p>
</dd>
<dt>EstudiosMadreSinCon</dt><dd><p>: </p>
</dd>
<dt>EstudiosPadre</dt><dd><p>: </p>
</dd>
<dt>EstudiosPadreSinCon</dt><dd><p>: </p>
</dd>
<dt>Faperitivos</dt><dd><p>: </p>
</dd>
<dt>Faperitivosmp</dt><dd><p>: </p>
</dd>
<dt>Farroz</dt><dd><p>: </p>
</dd>
<dt>Farrozmp</dt><dd><p>: </p>
</dd>
<dt>Fcarnes</dt><dd><p>: </p>
</dd>
<dt>Fcarnesmp</dt><dd><p>: </p>
</dd>
<dt>Fchucherias</dt><dd><p>: </p>
</dd>
<dt>Fchucheriasmp</dt><dd><p>: </p>
</dd>
<dt>Fdulces</dt><dd><p>: </p>
</dd>
<dt>Fdulcesmp</dt><dd><p>: </p>
</dd>
<dt>Ffiambres</dt><dd><p>: </p>
</dd>
<dt>Ffiambresmp</dt><dd><p>: </p>
</dd>
<dt>Ffritos</dt><dd><p>: </p>
</dd>
<dt>Ffritosmp</dt><dd><p>: </p>
</dd>
<dt>Ffruta</dt><dd><p>: </p>
</dd>
<dt>Ffrutamp</dt><dd><p>: </p>
</dd>
<dt>Fhuevos</dt><dd><p>: </p>
</dd>
<dt>Fhuevosmp</dt><dd><p>: </p>
</dd>
<dt>Flacteos</dt><dd><p>: </p>
</dd>
<dt>Flacteosmp</dt><dd><p>: </p>
</dd>
<dt>Flegumbres</dt><dd><p>: </p>
</dd>
<dt>Flegumbresmp</dt><dd><p>: </p>
</dd>
<dt>Fpan</dt><dd><p>: </p>
</dd>
<dt>Fpanmp</dt><dd><p>: </p>
</dd>
<dt>Fpescado</dt><dd><p>: </p>
</dd>
<dt>Fpescadomp</dt><dd><p>: </p>
</dd>
<dt>Fprecocina</dt><dd><p>: </p>
</dd>
<dt>Fprecocinamp</dt><dd><p>: </p>
</dd>
<dt>Frefrescos</dt><dd><p>: </p>
</dd>
<dt>Frefrescosmp</dt><dd><p>: </p>
</dd>
<dt>Fruta</dt><dd><p>:usual consumption of fruit? (0 is No; 1 is Yes) </p>
</dd>
<dt>FrutaVerdura</dt><dd><p>: </p>
</dd>
<dt>Fverduras</dt><dd><p>: </p>
</dd>
<dt>Fverdurasmp</dt><dd><p>: </p>
</dd>
<dt>HorasPantDia</dt><dd><p>: </p>
</dd>
<dt>HorasPCDiaPond</dt><dd><p>:daily hours playing videogames and/or in internet (weekends included) </p>
</dd>
<dt>HorasPCsem1</dt><dd><p>: </p>
</dd>
<dt>HorasPCsem2</dt><dd><p>: </p>
</dd>
<dt>HorasTV</dt><dd><p>: </p>
</dd>
<dt>HorasTVDiaPond</dt><dd><p>:daily hours watching TV (weekends included) </p>
</dd>
<dt>HorasTVsem1</dt><dd><p>: </p>
</dd>
<dt>HorasTVsem2</dt><dd><p>: </p>
</dd>
<dt>HoraSuenyo</dt><dd><p>:daily hours sleeping </p>
</dd>
<dt>HTA</dt><dd><p>: </p>
</dd>
<dt>HTARelacion</dt><dd><p>: </p>
</dd>
<dt>IMC</dt><dd><p>: </p>
</dd>
<dt>IndEdadComedorEscolar</dt><dd><p>: </p>
</dd>
<dt>IntolGlucosa</dt><dd><p>: </p>
</dd>
<dt>IntolRelacion</dt><dd><p>: </p>
</dd>
<dt>LactMater</dt><dd><p>: </p>
</dd>
<dt>LactMaterna</dt><dd><p>: breast-feeding (1 is Yes; 0 is No) </p>
</dd>
<dt>LactMatMeses</dt><dd><p>: </p>
</dd>
<dt>LactMatSemanas</dt><dd><p>: </p>
</dd>
<dt>MadreObesa</dt><dd><p>: </p>
</dd>
<dt>MadreObesa01</dt><dd><p>:is the mother obese? (0 is No; 1 is Yes) </p>
</dd>
<dt>Merienda</dt><dd><p>:Afternoon snack  (1 is Yes; 0 is No) </p>
</dd>
<dt>NumComidas</dt><dd><p>: </p>
</dd>
<dt>NumContOK</dt><dd><p>: </p>
</dd>
<dt>NumControles</dt><dd><p>: </p>
</dd>
<dt>NumHnos</dt><dd><p>: </p>
</dd>
<dt>NumHnosOb</dt><dd><p>: </p>
</dd>
<dt>NumPadresEsp02</dt><dd><p>: </p>
</dd>
<dt>NumPadresObesos</dt><dd><p>: </p>
</dd>
<dt>OrdenadorDiario</dt><dd><p>: </p>
</dd>
<dt>OrdenadorFinDe</dt><dd><p>: </p>
</dd>
<dt>OsteoRelacion</dt><dd><p>: </p>
</dd>
<dt>OtrosPatol</dt><dd><p>: </p>
</dd>
<dt>PadreObeso</dt><dd><p>:is the father obese? (0 is No; 1 is Yes) </p>
</dd>
<dt>PesoActual</dt><dd><p>:current weight (in kilograms) </p>
</dd>
<dt>PesoNac</dt><dd><p>:weight born (in grams) </p>
</dd>
<dt>PorcHnosObesos</dt><dd><p>: </p>
</dd>
<dt>porcHnosObesosOK</dt><dd><p>: </p>
</dd>
<dt>Postre</dt><dd><p>: </p>
</dd>
<dt>ProbOsteo</dt><dd><p>: </p>
</dd>
<dt>ProbPsico</dt><dd><p>: </p>
</dd>
<dt>ProbResp</dt><dd><p>: </p>
</dd>
<dt>PsicoRelacion</dt><dd><p>: </p>
</dd>
<dt>ResoponF01</dt><dd><p>: </p>
</dd>
<dt>RespRelacion</dt><dd><p>: </p>
</dd>
<dt>Semlact</dt><dd><p>: </p>
</dd>
<dt>Sexo</dt><dd><p>:female (1); male (0) </p>
</dd>
<dt>TallaAct</dt><dd><p>:current height (in meters) </p>
</dd>
<dt>TallaNac</dt><dd><p>:height born (in centimeters) </p>
</dd>
<dt>Tipocaso</dt><dd><p>: </p>
</dd>
<dt>Tipocaso.y</dt><dd><p>: </p>
</dd>
<dt>TipoObeso</dt><dd><p>: </p>
</dd>
<dt>TVDiario</dt><dd><p>: </p>
</dd>
<dt>TVFinSemana</dt><dd><p>: </p>
</dd>
<dt>Verduras</dt><dd><p>:usual consumption of vegetables? (0 is No; 1 is Yes) </p>
</dd>
</dl>



<h3>References</h3>

<p>Zurriaga, O., Perez-Panades, J., Quiles, J. , Gil, M.,
Anes, Y., Quiñones, C., Margolles, M., Lopez-Maside, A.,
Vega-Alonso, A., Miralles M. and Recent OBICE Research Group (2011)
Factors associated with childhood obesity in Spain. The OBICE
study: a case–control study based on sentinel networks. Public Health Nutrition
14(6), 1105–1113.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(OBICE)

</code></pre>

<hr>
<h2 id='Ozone35'>Ozone35 dataset</h2><span id='topic+Ozone35'></span>

<h3>Description</h3>

<p>Polution data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Ozone35
</code></pre>


<h3>Format</h3>

<p>A data frame with 178 observations on the following 36 variables.
</p>
 <dl>
<dt>y</dt><dd><p>Response = Daily maximum 1-hour-average ozone
reading (ppm) at Upland, CA</p>
</dd> <dt>x4</dt><dd><p>500-millibar pressure height
(m) measured at Vandenberg AFB</p>
</dd> <dt>x5</dt><dd><p>Wind speed (mph) at Los
Angeles International Airport (LAX)</p>
</dd> <dt>x6</dt><dd><p>Humidity (percentage)
at LAX</p>
</dd> <dt>x7</dt><dd><p>Temperature (Fahrenheit degrees) measured at
Sandburg, CA</p>
</dd> <dt>x8</dt><dd><p>Inversion base height (feet) at LAX</p>
</dd>
<dt>x9</dt><dd><p>Pressure gradient (mm Hg) from LAX to Daggett, CA</p>
</dd>
<dt>x10</dt><dd><p>Visibility (miles) measured at LAX</p>
</dd>
<dt>x4.x4</dt><dd><p>=x4*x4</p>
</dd> <dt>x4.x5</dt><dd><p>=x4*x5</p>
</dd>
<dt>x4.x6</dt><dd><p>=x4*x6</p>
</dd> <dt>x4.x7</dt><dd><p>=x4*x7</p>
</dd>
<dt>x4.x8</dt><dd><p>=x4*x8</p>
</dd> <dt>x4.x9</dt><dd><p>=x4*x9</p>
</dd>
<dt>x4.x10</dt><dd><p>=x4*x10</p>
</dd> <dt>x5.x5</dt><dd><p>=x5*x5</p>
</dd>
<dt>x5.x6</dt><dd><p>=x5*x6</p>
</dd> <dt>x5.x7</dt><dd><p>=x5*x7</p>
</dd>
<dt>x5.x8</dt><dd><p>=x5*x8</p>
</dd> <dt>x5.x9</dt><dd><p>=x5*x9</p>
</dd>
<dt>x5.x10</dt><dd><p>=x5*x10</p>
</dd> <dt>x6.x6</dt><dd><p>=x6*x6</p>
</dd>
<dt>x6.x7</dt><dd><p>=x6*x7</p>
</dd> <dt>x6.x8</dt><dd><p>=x6*x8</p>
</dd>
<dt>x6.x9</dt><dd><p>=x6*x9</p>
</dd> <dt>x6.x10</dt><dd><p>=x6*x10</p>
</dd>
<dt>x7.x7</dt><dd><p>=x7*x7</p>
</dd> <dt>x7.x8</dt><dd><p>=x7*x8</p>
</dd>
<dt>x7.x9</dt><dd><p>=x7*x9</p>
</dd> <dt>x7.x10</dt><dd><p>=x7*x10</p>
</dd>
<dt>x8.x8</dt><dd><p>=x8*x8</p>
</dd> <dt>x8.x9</dt><dd><p>=x8*x9</p>
</dd>
<dt>x8.x10</dt><dd><p>=x8*x10</p>
</dd> <dt>x9.x9</dt><dd><p>=x9*x9</p>
</dd>
<dt>x9.x10</dt><dd><p>=x9*x10</p>
</dd> <dt>x10.x10</dt><dd><p>=x10*x10</p>
</dd> </dl>



<h3>Details</h3>

<p>This dataset has been used by Garcia-Donato and Martinez-Beneito (2013) to
illustrate the potential of the Gibbs sampling method (in <code>BayesVarSel</code>
implemented in <code><a href="#topic+GibbsBvs">GibbsBvs</a></code>).
</p>
<p>This data were previously used by Casella and Moreno (2006) and Berger and
Molina (2005) and concern N = 178 measures of ozone concentration in the
atmosphere. Of the 10 main effects originally considered, we only make use
of those with an atmospheric meaning x4 to x10, as was done by Liang et al.
(2008). We then have 7 main effects which, jointly with the quadratic terms
and second order interactions, produce the above-mentioned p = 35 possible
regressors.
</p>


<h3>References</h3>

<p>Berger, J. and Molina, G. (2005)&lt;DOI:j.1467-9574.2005.00275.x&gt;
Posterior model probabilities via path-based pairwise priors. Statistica
Neerlandica, 59:3-15.
</p>
<p>Casella, G. and Moreno, E. (2006)&lt;DOI:10.1198/016214505000000646&gt; Objective
Bayesian variable selection. Journal of the American Statistical
Association, 101(473).
</p>
<p>Garcia-Donato, G. and Martinez-Beneito, M.A.
(2013)&lt;DOI:10.1080/01621459.2012.742443&gt; On sampling strategies in Bayesian
variable selection problems with large model spaces. Journal of the American
Statistical Association, 108: 340-352.
</p>
<p>Liang, F., Paulo, R., Molina, G., Clyde, M. and Berger, J.O.
(2008)&lt;DOI:10.1198/016214507000001337&gt; Mixtures of g-priors for Bayesian
Variable Selection. Journal of the American Statistical Association.
103:410-423.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Ozone35)

</code></pre>

<hr>
<h2 id='plot.Bvs'>A function for plotting summaries of an object of class <code>Bvs</code></h2><span id='topic+plot.Bvs'></span>

<h3>Description</h3>

<p>Four different plots to summarize graphically the results in an object of
class <code>Bvs</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Bvs'
plot(x, option = "dimension", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.Bvs_+3A_x">x</code></td>
<td>
<p>An object of class <code>Bvs</code></p>
</td></tr>
<tr><td><code id="plot.Bvs_+3A_option">option</code></td>
<td>
<p>One of &quot;dimension&quot;, &quot;joint&quot;, &quot;conditional&quot;, &quot;not&quot; or &quot;trace&quot;</p>
</td></tr>
<tr><td><code id="plot.Bvs_+3A_...">...</code></td>
<td>
<p>Additional graphical parameters to be passed</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>option</code>=&quot;dimension&quot; this function returns a barplot of the
posterior distribution of the dimension of the true model. If
<code>option</code>=&quot;joint&quot; an image plot of the joint inclusion probabilities is
returned. If <code>option</code>=&quot;conditional&quot; an image plot of the conditional
inclusion probabilities. These should be read as the probabilty that the
variable in the column is part of the true model if the corresponding
variables on the row is. If <code>option</code>=&quot;not&quot; the image plot that
is returned is that of the the probabilty that the variable in the column is
part of the true model if the corresponding variables on the row is not. Finally,
if <code>option</code>=&quot;trace&quot;, only available if x$method == &quot;Gibbs&quot;, returns a plot of the trace of the inclusion probabilities to check for convergence.
</p>


<h3>Value</h3>

<p>If <code>option</code>=&quot;joint&quot;, &quot;conditional&quot; or &quot;not&quot; <code>plot</code> also
returns an object of class <code>matrix</code> with the numeric values of the
printed probabilities.
</p>


<h3>Author(s)</h3>

<p>Gonzalo Garcia-Donato and Anabel Forte
</p>
<p>Maintainer: &lt;anabel.forte@uv.es&gt;
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+Bvs">Bvs</a></code>, <code><a href="#topic+GibbsBvs">GibbsBvs</a></code> for creating objects of the class
<code>Bvs</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

#Analysis of Crime Data
#load data
data(UScrime)

#Default arguments are Robust prior for the regression parameters
#and constant prior over the model space
#Here we keep the 1000 most probable models a posteriori:
crime.Bvs&lt;- Bvs(formula= y ~ ., data=UScrime, n.keep=1000)

#A look at the results:
crime.Bvs

summary(crime.Bvs)

#A plot with the posterior probabilities of the dimension of the
#true model:
plot(crime.Bvs, option="dimension")

#An image plot of the joint inclusion probabilities:
plot(crime.Bvs, option="joint")

#Two image plots of the conditional inclusion probabilities:
plot(crime.Bvs, option="conditional")
plot(crime.Bvs, option="not")


</code></pre>

<hr>
<h2 id='pltltn'>Correction for p&gt;&gt;n for an object of class <code>Bvs</code></h2><span id='topic+pltltn'></span>

<h3>Description</h3>

<p>In cases where p&gt;&gt;n and the true model is expected to be sparse, it is very unlikely that the Gibbs sampling
will sample models in the singular subset of the model space (models with k&gt;n). Nevertheless, depending on
how large is p/n and the strenght of the signal, this part of the model space could be very influential in the
final response.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pltltn(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pltltn_+3A_object">object</code></td>
<td>
<p>An object of class <code>Bvs</code> obtained with <code>GibbsBvs</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>From an object created with GibbsBvs and prior probabilities specified as Scott-Berger,
this function provides an estimation of the posterior probability of models with k&gt;n which is a measure of the
importance of these models. In summary, when this probability is large,  the sample size is not large enough to beat
such large p.
Additionally, <code>pltltn</code> gives corrections of the posterior inclusion probabilities and posterior probabilities
of dimension of the true model.
</p>


<h3>Value</h3>

<p><code>pltltn</code> returns a list with the following elements:
</p>
<table>
<tr><td><code>pS</code></td>
<td>
<p>An estimation of the probability that the true model is irregular (k&gt;n)</p>
</td></tr>
<tr><td><code>postprobdim</code></td>
<td>
<p>A corrected estimation of the posterior probabilities over the dimensions</p>
</td></tr>
<tr><td><code>inclprob</code></td>
<td>
<p>A corrected estimation of the posterior inclusion probabilities</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gonzalo Garcia-Donato
</p>
<p>Maintainer: &lt;gonzalo.garciadonato@uclm.es&gt;
</p>


<h3>References</h3>

<p>Berger, J.O., Garcia-Donato, G., Martínez-Beneito M.A. and Peña, V. (2016)
Bayesian variable selection in high dimensional problems without assumptions on prior model probabilities.
arXiv:1607.02993
</p>


<h3>See Also</h3>

<p>See
<code><a href="#topic+GibbsBvs">GibbsBvs</a></code> for creating objects of the class
<code>Bvs</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
data(riboflavin, package="hdi")

set.seed(16091956)
#the following sentence took 37.3 minutes in a single core
#(a trick to see the evolution of the algorithm is to monitor
#the files created by the function.
#you can see the working directory running
#tempdir()
#copy this path in the clipboard. Then open another R session
#and from there (once the simulating process is running and the burnin has completed)
#write
#system("wc (path from clipboard)/AllBF")
#the number here appearing is the number of completed iterations
#
testRB&lt;- GibbsBvs(formula=y~.,
                  data=riboflavin,
                  prior.betas="Robust",
                  init.model="null",
                  time.test=F,
                  n.iter=10000,
                  n.burnin=1000)

set.seed(16091956)
system.time(
testRB&lt;- GibbsBvs(formula=y~.,
                  data=riboflavin,
                  prior.betas="Robust",
                  init.model="null",
                  time.test=F,
                  n.iter=10000,
                  n.burnin=1000)
)

#notice the large sparsity of the result since
#the great majority of covariates are not influential:
boxplot(testRB$inclprob)
testRB$inclprob[testRB$inclprob&gt;.5]
#xYOAB_at xYXLE_at
#  0.9661   0.6502
#we can discharge all covariates except xYOAB_at and xYXLE_at
#the method does not reach to inform about xYXLE_at and its posterior
#probability is only slightly bigger than its prior probability

#We see that dimensions of visited models are small:
plot(testRB, option="d", xlim=c(0,100))
#so the part of the model space with singular models (k&gt;n)
#has not been explored.
#To correct this issue we run:
corrected.testRB&lt;- pltltn(testRB)
#Estimate of the posterior probability of the
# model space with singular models is: 0
#Meaning that it is extremely unlikely that the true model is such that k&gt;n
#The corrected inclusion probabilities can be accessed through
#corrected.testRB but, in this case, these are essentially the same as in the
#original object (due to the unimportance of the singular part of the model space)


## End(Not run)

</code></pre>

<hr>
<h2 id='predict.Bvs'>Bayesian Model Averaged predictions</h2><span id='topic+predict.Bvs'></span>

<h3>Description</h3>

<p>Samples of the model averaged objective predictive distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Bvs'
predict(object, newdata, n.sim = 10000, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.Bvs_+3A_object">object</code></td>
<td>
<p>An object of class <code>Bvs</code></p>
</td></tr>
<tr><td><code id="predict.Bvs_+3A_newdata">newdata</code></td>
<td>
<p>A data frame in which to look for variables with which to
predict</p>
</td></tr>
<tr><td><code id="predict.Bvs_+3A_n.sim">n.sim</code></td>
<td>
<p>Number of simulations to be produced</p>
</td></tr>
<tr><td><code id="predict.Bvs_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed (currently none implemented).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The distribution that is sampled from is the discrete mixture of the
(objective) predictive distribution with weights proportional to the
posterior probabilities of each model. That is, from
</p>
<p><code class="reqn">latex</code>
</p>
<p>The models used in the mixture above are the retained best models (see the
argument <code>n.keep</code> in <a href="#topic+Bvs">Bvs</a>) if <code>x</code> was generated
with <code>Bvs</code> and the sampled models with the associated frequencies if
<code>x</code> was generated with <code>GibbsBvs</code>. The formula for the objective
predictive distribution within each model <code class="reqn">latex</code> is
taken from Bernardo and Smith (1994) page 442.
</p>


<h3>Value</h3>

<p><code>predict</code> returns a matrix with <code>n.sim</code> rows with the
simulations. Each column of the matrix corresponds to each of the
configurations for the covariates defined in <code>newdata</code>.
</p>


<h3>Author(s)</h3>

<p>Gonzalo Garcia-Donato and Anabel Forte
</p>
<p>Maintainer: &lt;anabel.forte@uv.es&gt;
</p>


<h3>References</h3>

<p>Bernardo, J. M. and Smith, A. F. M.
(1994)&lt;DOI:10.1002/9780470316870&gt; Bayesian Theory. Chichester: Wiley.
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+Bvs">Bvs</a></code>
and <code><a href="#topic+GibbsBvs">GibbsBvs</a></code> for creating objects of the class
<code>Bvs</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

#Analysis of Crime Data
#load data
data(UScrime)

crime.Bvs&lt;- Bvs(formula= y ~ ., data=UScrime, n.keep=1000)
#predict a future observation associated with the first two sets of covariates
crime.Bvs.predict&lt;- predict(crime.Bvs, newdata=UScrime[1:2,], n.sim=10000)
#(Notice the best 1000 models are used in the mixture)

#Here you can use standard summaries to describe the underlying predictive distribution
#summary(crime.Bvs.predict)
#
#To study more in deep the first set:
plot(density(crime.Bvs.predict[,1]))
#Point prediction
median(crime.Bvs.predict[,1])
#A credible 95% interval for the prediction:
#lower bound:
quantile(crime.Bvs.predict[,1], probs=0.025)
#upper bound:
quantile(crime.Bvs.predict[,1], probs=0.975)


## End(Not run)

</code></pre>

<hr>
<h2 id='print.Btest'>Print an object of class <code>Btest</code></h2><span id='topic+print.Btest'></span>

<h3>Description</h3>

<p>Print an object of class <code>Btest</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Btest'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.Btest_+3A_x">x</code></td>
<td>
<p>Object of class Btest</p>
</td></tr>
<tr><td><code id="print.Btest_+3A_...">...</code></td>
<td>
<p>Additional parameters to be passed</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>See <code><a href="#topic+Btest">Btest</a></code> for creating objects of the class <code>Btest</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> ## Not run: 
#Analysis of Crime Data
#load data
data(UScrime)
#Model selection among the following models: (note model1 is nested in all the others)
model1&lt;- y ~ 1 + Prob
model2&lt;- y ~ 1 + Prob + Time
model3&lt;- y ~ 1 + Prob + Po1 + Po2
model4&lt;- y ~ 1 + Prob + So
model5&lt;- y ~ .

#Equal prior probabilities for models:
crime.BF&lt;- Btest(models=list(basemodel=model1,
	ProbTimemodel=model2, ProbPolmodel=model3,
	ProbSomodel=model4, fullmodel=model5), data=UScrime)
	crime.BF
	
## End(Not run)
</code></pre>

<hr>
<h2 id='print.Bvs'>Print an object of class <code>Bvs</code></h2><span id='topic+print.Bvs'></span>

<h3>Description</h3>

<p>Print an object of class <code>Bvs</code>. The ten most probable models (among the visited ones if the object was created with
GibbsBvs) are shown jointly with their Bayes factors and an estimation of their posterior probability based on the estimation
of the normalizing constant.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Bvs'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.Bvs_+3A_x">x</code></td>
<td>
<p>An object of class <code>Bvs</code></p>
</td></tr>
<tr><td><code id="print.Bvs_+3A_...">...</code></td>
<td>
<p>Additional parameters to be passed</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gonzalo Garcia-Donato and Anabel Forte
</p>
<p>Maintainer: &lt;anabel.forte@uv.es&gt;
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+Bvs">Bvs</a></code>,
<code><a href="#topic+GibbsBvs">GibbsBvs</a></code> for creating objects of the class
<code>Bvs</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
#Analysis of Crime Data
#load data
data(UScrime)

#Default arguments are Robust prior for the regression parameters
#and constant prior over the model space
#Here we keep the 1000 most probable models a posteriori:
crime.Bvs&lt;- Bvs(formula= y ~ ., data=UScrime, n.keep=1000)

#A look at the results:
print(crime.Bvs)

## End(Not run)

</code></pre>

<hr>
<h2 id='print.jointness'>Print an object of class <code>jointness</code></h2><span id='topic+print.jointness'></span>

<h3>Description</h3>

<p>Print an object of class <code>jointness</code>. Show the different jointness measurements with a small explanation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'jointness'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.jointness_+3A_x">x</code></td>
<td>
<p>An object of class <code>jointness</code></p>
</td></tr>
<tr><td><code id="print.jointness_+3A_...">...</code></td>
<td>
<p>Additional parameters to be passed</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gonzalo Garcia-Donato and Anabel Forte
</p>
<p>Maintainer: &lt;anabel.forte@uv.es&gt;
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+Jointness">Jointness</a></code> for creating objects of the class
<code>jointness</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
#Analysis of Crime Data
#load data
data(UScrime)

#Default arguments are Robust prior for the regression parameters
#and constant prior over the model space
#Here we keep the 1000 most probable models a posteriori:
crime.Bvs&lt;- Bvs(formula= y ~ ., data=UScrime, n.keep=1000)

#A look at the results:
jointness(crime.Bvs)

## End(Not run)

</code></pre>

<hr>
<h2 id='SDM'>SDM data</h2><span id='topic+SDM'></span>

<h3>Description</h3>

<p>The following data set contains 67 variables potentially related with Growth. The name of this dataset is related to its authors since it was firstly used in Sala i Martin, Doppelhofer and Miller (2004).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SDM
</code></pre>


<h3>Format</h3>

<p>A data frame with 88 observations on the following 68 variables
</p>
 <dl>
<dt><code>y</code></dt><dd><p>Growth of GDP per capita at purchasing power parities between 1960 and 1996.</p>
</dd>
<dt><code>ABSLATIT</code></dt><dd><p>Absolute latitude.</p>
</dd>
<dt><code>AIRDIST</code></dt><dd><p>Logarithm of minimal distance (in km) from New York, Rotterdam, or Tokyo.</p>
</dd>
<dt><code>AVELF</code></dt><dd><p>Average of five different indices of ethnolinguistic fractionalization which is the probability of two random people in a country not speaking the same language.</p>
</dd>
<dt><code>BRIT</code></dt><dd><p>Dummy for former British colony after 1776.</p>
</dd>
<dt><code>BUDDHA</code></dt><dd><p>Fraction of population Buddhist in 1960.</p>
</dd>
<dt><code>CATH00</code></dt><dd><p>Fraction of population Catholic in 1960.</p>
</dd>
<dt><code>CIV72</code></dt><dd><p>Index of civil liberties index in 1972.</p>
</dd>
<dt><code>COLONY</code></dt><dd><p>Dummy for former colony.</p>
</dd>
<dt><code>CONFUC</code></dt><dd><p>Fraction of population Confucian.</p>
</dd>
<dt><code>DENS60</code></dt><dd><p>Population per area in 1960.</p>
</dd>
<dt><code>DENS65C</code></dt><dd><p>Coastal (within 100 km of coastline) population per coastal area in 1965.</p>
</dd>
<dt><code>DENS65I</code></dt><dd><p>Interior (more than 100 km from coastline) population per interior area in 1965.</p>
</dd>
<dt><code>DPOP6090</code></dt><dd><p>Average growth rate of population between 1960 and 1990.</p>
</dd>
<dt><code>EAST</code></dt><dd><p>Dummy for East Asian countries.</p>
</dd>
<dt><code>ECORG</code></dt><dd><p>Degree Capitalism index.</p>
</dd>
<dt><code>ENGFRAC</code></dt><dd><p>Fraction of population speaking English.</p>
</dd>
<dt><code>EUROPE</code></dt><dd><p>Dummy for European economies.</p>
</dd>
<dt><code>FERTLDC1</code></dt><dd><p>Fertility in 1960's.</p>
</dd>
<dt><code>GDE1</code></dt><dd><p>Average share public expenditures on defense as fraction of GDP between 1960 and 1965.</p>
</dd>
<dt><code>GDPCH60L</code></dt><dd><p>Logarithm of GDP per capita in 1960.</p>
</dd>
<dt><code>GEEREC1</code></dt><dd><p>Average share public expenditures on education as fraction of GDP between 1960 and 1965.</p>
</dd>
<dt><code>GGCFD3</code></dt><dd><p>Average share of expenditures on public investment as fraction of GDP between 1960 and 1965.</p>
</dd>
<dt><code>GOVNOM1</code></dt><dd><p>Average share of nominal government spending to nominal GDP between 1960 and 1964.</p>
</dd>
<dt><code>GOVSH61</code></dt><dd><p>Average share government spending to GDP between 1960 and 1964.</p>
</dd>
<dt><code>GVR61</code></dt><dd><p>Share of expenditures on government consumption to GDP in 1961.</p>
</dd>
<dt><code>H60</code></dt><dd><p>Enrollment rates in higher education.</p>
</dd>
<dt><code>HERF00</code></dt><dd><p>Religion measure.</p>
</dd>
<dt><code>HINDU00</code></dt><dd><p>Fraction of the population Hindu in 1960.</p>
</dd>
<dt><code>IPRICE1</code></dt><dd><p>Average investment price level between 1960 and 1964 on purchasing power parity basis.</p>
</dd>
<dt><code>LAAM</code></dt><dd><p>Dummy for Latin American countries.</p>
</dd>
<dt><code>LANDAREA</code></dt><dd><p>Area in km.</p>
</dd>
<dt><code>LANDLOCK</code></dt><dd><p>Dummy for landlocked countries.</p>
</dd>
<dt><code>LHCPC</code></dt><dd><p>Log of hydrocarbon deposits in 1993.</p>
</dd>
<dt><code>LIFE060</code></dt><dd><p>Life expectancy in 1960.</p>
</dd>
<dt><code>LT100CR</code></dt><dd><p>Proportion of country's land area within 100 km of ocean or ocean-navigable river.</p>
</dd>
<dt><code>MALFAL66</code></dt><dd><p>Index of malaria prevalence in 1966.</p>
</dd>
<dt><code>MINING</code></dt><dd><p>Fraction of GDP in mining.</p>
</dd>
<dt><code>MUSLIM00</code></dt><dd><p>Fraction of population Muslim in 1960.</p>
</dd>
<dt><code>NEWSTATE</code></dt><dd><p>Timing of national independence measure: 0 if before 1914; 1 if between 1914 and 1945; 2 if between 1946 and 1989; and 3 if after 1989.</p>
</dd>
<dt><code>OIL</code></dt><dd><p>Dummy for oil-producing country.</p>
</dd>
<dt><code>OPENDEC1</code></dt><dd><p>Ratio of exports plus imports to GDP, averaged over 1965 to 1974.</p>
</dd>
<dt><code>ORTH00</code></dt><dd><p>Fraction of population Orthodox in 1960.</p>
</dd>
<dt><code>OTHFRAC</code></dt><dd><p>Fraction of population speaking foreign language.</p>
</dd>
<dt><code>P60</code></dt><dd><p>Enrollment rate in primary education in 1960.</p>
</dd>
<dt><code>PI6090</code></dt><dd><p>Average inflation rate between 1960 and 1990.</p>
</dd>
<dt><code>SQPI6090</code></dt><dd><p>Square of average inflation rate between 1960 and 1990.</p>
</dd>
<dt><code>PRIGHTS</code></dt><dd><p>Political rights index.</p>
</dd>
<dt><code>POP1560</code></dt><dd><p>Fraction of population younger than 15 years in 1960.</p>
</dd>
<dt><code>POP60</code></dt><dd><p>Population in 1960</p>
</dd>
<dt><code>POP6560</code></dt><dd><p>Fraction of population older than 65 years in 1960.</p>
</dd>
<dt><code>PRIEXP70</code></dt><dd><p>Fraction of primary exports in total exports in 1970.</p>
</dd>
<dt><code>PROT00</code></dt><dd><p>Fraction of population Protestant in 1960.</p>
</dd>
<dt><code>RERD</code></dt><dd><p>Real exchange rate distortions.</p>
</dd>
<dt><code>REVCOUP</code></dt><dd><p>Number of revolutions and military coups.</p>
</dd>
<dt><code>SAFRICA</code></dt><dd><p>Dummy for Sub-Saharan African countries.</p>
</dd>
<dt><code>SCOUT</code></dt><dd><p>Measure of outward orientation.</p>
</dd>
<dt><code>SIZE60</code></dt><dd><p>Logarithm of aggregate GDP in 1960.</p>
</dd>
<dt><code>SOCIALIST</code></dt><dd><p>Dummy for countries under Socialist rule for considerable time during 1950 to 1995.</p>
</dd>
<dt><code>SPAIN</code></dt><dd><p>Dummy variable for former Spanish colonies.</p>
</dd>
<dt><code>TOT1DEC1</code></dt><dd><p>Growth of terms of trade in the 1960's.</p>
</dd>
<dt><code>TOTIND</code></dt><dd><p>Terms of trade ranking</p>
</dd>
<dt><code>TROPICAR</code></dt><dd><p>Proportion of country's land area within geographical tropics.</p>
</dd>
<dt><code>TROPPOP</code></dt><dd><p>Proportion of country's population living in geographical tropics.</p>
</dd>
<dt><code>WARTIME</code></dt><dd><p>Fraction of time spent in war between 1960 and 1990.</p>
</dd>
<dt><code>WARTORN</code></dt><dd><p>Indicator for countries that participated in external war between 1960 and 1990.</p>
</dd>
<dt><code>YRSOPEN</code></dt><dd><p>Number of years economy has been open between 1950 and 1994.</p>
</dd>
<dt><code>ZTROPICS</code></dt><dd><p>Fraction tropical climate zone.</p>
</dd></dl>



<h3>References</h3>

<p>Sala i Martin, X., Doppelhofer, G., Miller, R.I. (2004)
&lt;DOI: 10.1257/0002828042002570&gt;.
Determinants of long-term growth: a Bayesian averaging of classical estimates (BACE) approach.
American Economic Review 94: 813&ndash;835.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(SDM)

</code></pre>

<hr>
<h2 id='summary.Bvs'>Summary of an object of class <code>Bvs</code></h2><span id='topic+summary.Bvs'></span>

<h3>Description</h3>

<p>Summary of an object of class <code>Bvs</code>, providing inclusion probabilities and a representation of
the Median Probability Model and the Highest Posterior probability Model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Bvs'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.Bvs_+3A_object">object</code></td>
<td>
<p>An object of class <code>Bvs</code></p>
</td></tr>
<tr><td><code id="summary.Bvs_+3A_...">...</code></td>
<td>
<p>Additional parameters to be passed</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Gonzalo Garcia-Donato and Anabel Forte
</p>
<p>Maintainer: &lt;anabel.forte@uv.es&gt;
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+Bvs">Bvs</a></code>,
<code><a href="#topic+GibbsBvs">GibbsBvs</a></code> for creating objects of the class
<code>Bvs</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
#Analysis of Crime Data
#load data
data(UScrime)

#Default arguments are Robust prior for the regression parameters
#and constant prior over the model space
#Here we keep the 1000 most probable models a posteriori:
crime.Bvs&lt;- Bvs(formula= y ~ ., data=UScrime, n.keep=1000)

#A look at the results:
summary(crime.Bvs)

## End(Not run)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
