<!DOCTYPE html><html><head><title>Help for package textTinyR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {textTinyR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#batch_compute'><p>Compute batches</p></a></li>
<li><a href='#big_tokenize_transform'><p>String tokenization and transformation for big data sets</p></a></li>
<li><a href='#bytes_converter'><p>bytes converter of a text file ( KB, MB or GB )</p></a></li>
<li><a href='#cluster_frequency'><p>Frequencies of an existing cluster object</p></a></li>
<li><a href='#COS_TEXT'><p>Cosine similarity for text documents</p></a></li>
<li><a href='#cosine_distance'><p>cosine distance of two character strings (each string consists of more than one words)</p></a></li>
<li><a href='#Count_Rows'><p>Number of rows of a file</p></a></li>
<li><a href='#dense_2sparse'><p>convert a dense matrix to a sparse matrix</p></a></li>
<li><a href='#dice_distance'><p>dice similarity of words using n-grams</p></a></li>
<li><a href='#dims_of_word_vecs'><p>dimensions of a word vectors file</p></a></li>
<li><a href='#Doc2Vec'><p>Conversion of text documents to word-vector-representation features ( Doc2Vec )</p></a></li>
<li><a href='#JACCARD_DICE'><p>Jaccard or Dice similarity for text documents</p></a></li>
<li><a href='#levenshtein_distance'><p>levenshtein distance of two words</p></a></li>
<li><a href='#load_sparse_binary'><p>load a sparse matrix in binary format</p></a></li>
<li><a href='#matrix_sparsity'><p>sparsity percentage of a sparse matrix</p></a></li>
<li><a href='#read_characters'><p>read a specific number of characters from a text file</p></a></li>
<li><a href='#read_rows'><p>read a specific number of rows from a text file</p></a></li>
<li><a href='#save_sparse_binary'><p>save a sparse matrix in binary format</p></a></li>
<li><a href='#select_predictors'><p>Exclude highly correlated predictors</p></a></li>
<li><a href='#sparse_Means'><p>RowMens and colMeans for a sparse matrix</p></a></li>
<li><a href='#sparse_Sums'><p>RowSums and colSums for a sparse matrix</p></a></li>
<li><a href='#sparse_term_matrix'><p>Term matrices and statistics ( document-term-matrix, term-document-matrix)</p></a></li>
<li><a href='#TEXT_DOC_DISSIM'><p>Dissimilarity calculation of text documents</p></a></li>
<li><a href='#text_file_parser'><p>text file parser</p></a></li>
<li><a href='#text_intersect'><p>intersection of words or letters in tokenized text</p></a></li>
<li><a href='#token_stats'><p>token statistics</p></a></li>
<li><a href='#tokenize_transform_text'><p>String tokenization and transformation  ( character string or path to a file )</p></a></li>
<li><a href='#tokenize_transform_vec_docs'><p>String tokenization and transformation ( vector of documents )</p></a></li>
<li><a href='#utf_locale'><p>utf-locale for the available languages</p></a></li>
<li><a href='#vocabulary_parser'><p>returns the vocabulary counts for small or medium ( xml and not only ) files</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Text Processing for Small or Big Data Files</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.8</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-12-04</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/mlampros/textTinyR/issues">https://github.com/mlampros/textTinyR/issues</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/mlampros/textTinyR">https://github.com/mlampros/textTinyR</a></td>
</tr>
<tr>
<td>Description:</td>
<td>It offers functions for splitting, parsing, tokenizing and creating a vocabulary for big text data files. Moreover, it includes functions for building a document-term matrix and extracting information from those (term-associations, most frequent terms). It also embodies functions for calculating token statistics (collocations, look-up tables, string dissimilarities) and functions to work with sparse matrices. Lastly, it includes functions for Word Vector Representations (i.e. 'GloVe', 'fasttext') and incorporates functions for the calculation of (pairwise) text document dissimilarities. The source code is based on 'C++11' and exported in R through the 'Rcpp', 'RcppArmadillo' and 'BH' packages.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Copyright:</td>
<td>inst/COPYRIGHTS</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>libarmadillo: apt-get install -y libarmadillo-dev
(deb)</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Depends:</td>
<td>R(&ge; 3.2.3), Matrix</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.10), R6, data.table, utils</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo (&ge; 0.7.8), BH</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, covr, knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-12-04 16:20:58 UTC; lampros</td>
</tr>
<tr>
<td>Author:</td>
<td>Lampros Mouselimis
    <a href="https://orcid.org/0000-0002-8024-1546"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Lampros Mouselimis &lt;mouselimislampros@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-12-04 17:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='batch_compute'>Compute batches</h2><span id='topic+batch_compute'></span>

<h3>Description</h3>

<p>Compute batches
</p>


<h3>Usage</h3>

<pre><code class='language-R'>batch_compute(n_rows, n_batches)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="batch_compute_+3A_n_rows">n_rows</code></td>
<td>
<p>a numeric specifying the number of rows</p>
</td></tr>
<tr><td><code id="batch_compute_+3A_n_batches">n_batches</code></td>
<td>
<p>a numeric specifying the number of output batches</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(textTinyR)

btch = batch_compute(n_rows = 1000, n_batches = 10)
</code></pre>

<hr>
<h2 id='big_tokenize_transform'>String tokenization and transformation for big data sets</h2><span id='topic+big_tokenize_transform'></span>

<h3>Description</h3>

<p>String tokenization and transformation for big data sets
</p>
<p>String tokenization and transformation for big data sets
</p>


<h3>Usage</h3>

<pre><code class='language-R'># utl &lt;- big_tokenize_transform$new(verbose = FALSE)
</code></pre>


<h3>Details</h3>

<p>the <em>big_text_splitter</em> function splits a text file into sub-text-files using either the batches parameter (big-text-splitter-bytes) or both the batches and the end_query parameter (big-text-splitter-query). The end_query parameter (if not NULL) should be a character string specifying a word that appears repeatedly at the end of each line in the text file.
</p>
<p>the <em>big_text_parser</em> function parses text files from an input folder and saves those processed files to an output folder. The <em>big_text_parser</em> is appropriate for files with a structure using the start- and end- query parameters.
</p>
<p>the <em>big_text_tokenizer</em> function tokenizes and transforms the text files of a folder and saves those files to either a folder or a single file. There is also the option to save a frequency vocabulary of those transformed tokens to a file.
</p>
<p>the <em>vocabulary_accumulator</em> function takes the resulted vocabulary files of the <em>big_text_tokenizer</em> and returns the vocabulary sums sorted in decreasing order. The parameter <em>max_num_chars</em> limits the number of the corpus using the number of characters of each word.
</p>
<p>The <em>ngram_sequential</em> or <em>ngram_overlap</em> stemming method applies to each single batch and not to the whole corpus of the text file. Thus, it is possible that the stems of the same words for randomly selected batches might differ.
</p>


<h3>Methods</h3>


<dl>
<dt><code>big_tokenize_transform$new(verbose = FALSE)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>big_text_splitter(input_path_file = NULL, output_path_folder = NULL, end_query = NULL, batches = NULL, trimmed_line = FALSE)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>big_text_parser(input_path_folder = NULL, output_path_folder = NULL, start_query = NULL, end_query = NULL, min_lines = 1, trimmed_line = FALSE)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>big_text_tokenizer(input_path_folder = NULL, batches = NULL, read_file_delimiter = " ", to_lower = FALSE, to_upper = FALSE, utf_locale = "", remove_char = "", remove_punctuation_string = FALSE, remove_punctuation_vector = FALSE, remove_numbers = FALSE, trim_token = FALSE, split_string = FALSE, split_separator = " .,;:()?!", remove_stopwords = FALSE, language = "english", min_num_char = 1, max_num_char = Inf, stemmer = NULL, min_n_gram = 1, max_n_gram = 1, skip_n_gram = 1, skip_distance = 0, n_gram_delimiter = " ", concat_delimiter = NULL, path_2folder = "", stemmer_ngram = 4, stemmer_gamma = 0.0, stemmer_truncate = 3, stemmer_batches = 1, threads = 1, save_2single_file = FALSE, increment_batch_nr = 1, vocabulary_path_folder = NULL)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>vocabulary_accumulator(input_path_folder = NULL, vocabulary_path_file = NULL, max_num_chars = 100)</code></dt><dd></dd>
</dl>



<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-big_tokenize_transform-new"><code>big_tokenize_transform$new()</code></a>
</p>
</li>
<li> <p><a href="#method-big_tokenize_transform-big_text_splitter"><code>big_tokenize_transform$big_text_splitter()</code></a>
</p>
</li>
<li> <p><a href="#method-big_tokenize_transform-big_text_parser"><code>big_tokenize_transform$big_text_parser()</code></a>
</p>
</li>
<li> <p><a href="#method-big_tokenize_transform-big_text_tokenizer"><code>big_tokenize_transform$big_text_tokenizer()</code></a>
</p>
</li>
<li> <p><a href="#method-big_tokenize_transform-vocabulary_accumulator"><code>big_tokenize_transform$vocabulary_accumulator()</code></a>
</p>
</li>
<li> <p><a href="#method-big_tokenize_transform-clone"><code>big_tokenize_transform$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-big_tokenize_transform-new"></a>



<h4>Method <code>new()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>big_tokenize_transform$new(verbose = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>verbose</code></dt><dd><p>either TRUE or FALSE. If TRUE then information will be printed in the console</p>
</dd>
</dl>

</div>


<hr>
<a id="method-big_tokenize_transform-big_text_splitter"></a>



<h4>Method <code>big_text_splitter()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>big_tokenize_transform$big_text_splitter(
  input_path_file = NULL,
  output_path_folder = NULL,
  end_query = NULL,
  batches = NULL,
  trimmed_line = FALSE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>input_path_file</code></dt><dd><p>a character string specifying the path to the input file</p>
</dd>
<dt><code>output_path_folder</code></dt><dd><p>a character string specifying the folder where the output files should be saved</p>
</dd>
<dt><code>end_query</code></dt><dd><p>a character string. The <em>end_query</em> is the last word of the subset of the data and should appear frequently at the end of each line in the text file.</p>
</dd>
<dt><code>batches</code></dt><dd><p>a numeric value specifying the number of batches to use. The batches will be used to split the initial data into subsets. Those subsets will be either saved in files (<em>big_text_splitter</em> function) or will be used internally for low memory processing (<em>big_text_tokenizer</em> function).</p>
</dd>
<dt><code>trimmed_line</code></dt><dd><p>either TRUE or FALSE. If FALSE then each line of the text file will be trimmed both sides before applying the start_query and end_query</p>
</dd>
</dl>

</div>


<hr>
<a id="method-big_tokenize_transform-big_text_parser"></a>



<h4>Method <code>big_text_parser()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>big_tokenize_transform$big_text_parser(
  input_path_folder = NULL,
  output_path_folder = NULL,
  start_query = NULL,
  end_query = NULL,
  min_lines = 1,
  trimmed_line = FALSE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>input_path_folder</code></dt><dd><p>a character string specifying the folder where the input files are saved</p>
</dd>
<dt><code>output_path_folder</code></dt><dd><p>a character string specifying the folder where the output files should be saved</p>
</dd>
<dt><code>start_query</code></dt><dd><p>a character string. The <em>start_query</em> is the first word of the subset of the data and should appear frequently at the beginning of each line int the text file.</p>
</dd>
<dt><code>end_query</code></dt><dd><p>a character string. The <em>end_query</em> is the last word of the subset of the data and should appear frequently at the end of each line in the text file.</p>
</dd>
<dt><code>min_lines</code></dt><dd><p>a numeric value specifying the minimum number of lines. For instance if min_lines = 2, then only subsets of text with more than 1 lines will be kept.</p>
</dd>
<dt><code>trimmed_line</code></dt><dd><p>either TRUE or FALSE. If FALSE then each line of the text file will be trimmed both sides before applying the start_query and end_query</p>
</dd>
</dl>

</div>


<hr>
<a id="method-big_tokenize_transform-big_text_tokenizer"></a>



<h4>Method <code>big_text_tokenizer()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>big_tokenize_transform$big_text_tokenizer(
  input_path_folder = NULL,
  batches = NULL,
  read_file_delimiter = "\n",
  to_lower = FALSE,
  to_upper = FALSE,
  utf_locale = "",
  remove_char = "",
  remove_punctuation_string = FALSE,
  remove_punctuation_vector = FALSE,
  remove_numbers = FALSE,
  trim_token = FALSE,
  split_string = FALSE,
  split_separator = " \r\n\t.,;:()?!//",
  remove_stopwords = FALSE,
  language = "english",
  min_num_char = 1,
  max_num_char = Inf,
  stemmer = NULL,
  min_n_gram = 1,
  max_n_gram = 1,
  skip_n_gram = 1,
  skip_distance = 0,
  n_gram_delimiter = " ",
  concat_delimiter = NULL,
  path_2folder = "",
  stemmer_ngram = 4,
  stemmer_gamma = 0,
  stemmer_truncate = 3,
  stemmer_batches = 1,
  threads = 1,
  save_2single_file = FALSE,
  increment_batch_nr = 1,
  vocabulary_path_folder = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>input_path_folder</code></dt><dd><p>a character string specifying the folder where the input files are saved</p>
</dd>
<dt><code>batches</code></dt><dd><p>a numeric value specifying the number of batches to use. The batches will be used to split the initial data into subsets. Those subsets will be either saved in files (<em>big_text_splitter</em> function) or will be used internally for low memory processing (<em>big_text_tokenizer</em> function).</p>
</dd>
<dt><code>read_file_delimiter</code></dt><dd><p>the delimiter to use when the input file will be red (for instance a tab-delimiter or a new-line delimiter).</p>
</dd>
<dt><code>to_lower</code></dt><dd><p>either TRUE or FALSE. If TRUE the character string will be converted to lower case</p>
</dd>
<dt><code>to_upper</code></dt><dd><p>either TRUE or FALSE. If TRUE the character string will be converted to upper case</p>
</dd>
<dt><code>utf_locale</code></dt><dd><p>the language specific locale to use in case that either the <em>to_lower</em> or the <em>to_upper</em> parameter is TRUE and the text file language is other than english. For instance if the language of a text file is greek then the <em>utf_locale</em> parameter should be <em>'el_GR.UTF-8'</em> ( <em>language_country.encoding</em> ). A wrong utf-locale does not raise an error, however the runtime of the function increases.</p>
</dd>
<dt><code>remove_char</code></dt><dd><p>a character string with specific characters that should be removed from the text file. If the <em>remove_char</em> is &quot;&quot; then no removal of characters take place</p>
</dd>
<dt><code>remove_punctuation_string</code></dt><dd><p>either TRUE or FALSE. If TRUE then the punctuation of the character string will be removed (applies before the split function)</p>
</dd>
<dt><code>remove_punctuation_vector</code></dt><dd><p>either TRUE or FALSE. If TRUE then the punctuation of the vector of the character strings will be removed  (after the string split has taken place)</p>
</dd>
<dt><code>remove_numbers</code></dt><dd><p>either TRUE or FALSE. If TRUE then any numbers in the character string will be removed</p>
</dd>
<dt><code>trim_token</code></dt><dd><p>either TRUE or FALSE. If TRUE then the string will be trimmed (left and/or right)</p>
</dd>
<dt><code>split_string</code></dt><dd><p>either TRUE or FALSE. If TRUE then the character string will be split using the <em>split_separator</em> as delimiter. The user can also specify multiple delimiters.</p>
</dd>
<dt><code>split_separator</code></dt><dd><p>a character string specifying the character delimiter(s)</p>
</dd>
<dt><code>remove_stopwords</code></dt><dd><p>either TRUE, FALSE or a character vector of user defined stop words. If TRUE then by using the <em>language</em> parameter the corresponding stop words vector will be uploaded.</p>
</dd>
<dt><code>language</code></dt><dd><p>a character string which defaults to english. If the <em>remove_stopwords</em> parameter is TRUE then the corresponding stop words vector will be uploaded. Available languages
are <em>afrikaans</em>, <em>arabic</em>, <em>armenian</em>, <em>basque</em>, <em>bengali</em>, <em>breton</em>, <em>bulgarian</em>, <em>catalan</em>,
<em>croatian</em>, <em>czech</em>, <em>danish</em>, <em>dutch</em>, <em>english</em>, <em>estonian</em>,
<em>finnish</em>, <em>french</em>, <em>galician</em>, <em>german</em>, <em>greek</em>, <em>hausa</em>, <em>hebrew</em>, <em>hindi</em>, <em>hungarian</em>,
<em>indonesian</em>, <em>irish</em>, <em>italian</em>, <em>latvian</em>, <em>marathi</em>,
<em>norwegian</em>, <em>persian</em>, <em>polish</em>, <em>portuguese</em>, <em>romanian</em>, <em>russian</em>, <em>slovak</em>, <em>slovenian</em>,
<em>somalia</em>, <em>spanish</em>, <em>swahili</em>, <em>swedish</em>, <em>turkish</em>, <em>yoruba</em>, <em>zulu</em></p>
</dd>
<dt><code>min_num_char</code></dt><dd><p>an integer specifying the minimum number of characters to keep. If the <em>min_num_char</em> is greater than 1 then character strings with more than 1 characters will be returned</p>
</dd>
<dt><code>max_num_char</code></dt><dd><p>an integer specifying the maximum number of characters to keep. The <em>max_num_char</em> should be less than or equal to <em>Inf</em> (in this function the Inf value translates to a word-length of 1000000000)</p>
</dd>
<dt><code>stemmer</code></dt><dd><p>a character string specifying the stemming method. One of the following <em>porter2_stemmer</em>, <em>ngram_sequential</em>, <em>ngram_overlap</em>. See details for more information.</p>
</dd>
<dt><code>min_n_gram</code></dt><dd><p>an integer specifying the minimum number of n-grams. The minimum number of min_n_gram is 1.</p>
</dd>
<dt><code>max_n_gram</code></dt><dd><p>an integer specifying the maximum number of n-grams. The minimum number of max_n_gram is 1.</p>
</dd>
<dt><code>skip_n_gram</code></dt><dd><p>an integer specifying the number of skip-n-grams. The minimum number of skip_n_gram is 1. The skip_n_gram gives the (max.) n-grams using the <em>skip_distance</em> parameter. If <em>skip_n_gram</em> is greater than 1 then both <em>min_n_gram</em> and <em>max_n_gram</em> should be set to 1.</p>
</dd>
<dt><code>skip_distance</code></dt><dd><p>an integer specifying the skip distance between the words. The minimum value for the skip distance is 0, in which case simple n-grams will be returned.</p>
</dd>
<dt><code>n_gram_delimiter</code></dt><dd><p>a character string specifying the n-gram delimiter (applies to both n-gram and skip-n-gram cases)</p>
</dd>
<dt><code>concat_delimiter</code></dt><dd><p>either NULL or a character string specifying the delimiter to use in order to concatenate the end-vector of character strings to a single character string (recommended in case that the end-vector should be saved to a file)</p>
</dd>
<dt><code>path_2folder</code></dt><dd><p>a character string specifying the path to the folder where the file(s) will be saved</p>
</dd>
<dt><code>stemmer_ngram</code></dt><dd><p>a numeric value greater than 1. Applies to both <em>ngram_sequential</em> and <em>ngram_overlap</em> methods. In case of <em>ngram_sequential</em> the first stemmer_ngram characters will be picked, whereas in the case of <em>ngram_overlap</em> the overlapping stemmer_ngram characters will be build.</p>
</dd>
<dt><code>stemmer_gamma</code></dt><dd><p>a float number greater or equal to 0.0. Applies only to <em>ngram_sequential</em>. Is a threshold value, which defines how much frequency deviation of two N-grams is acceptable. It is kept either zero or to a minimum value.</p>
</dd>
<dt><code>stemmer_truncate</code></dt><dd><p>a numeric value greater than 0. Applies only to <em>ngram_sequential</em>. The ngram_sequential is modified to use relative frequencies (float numbers between 0.0 and 1.0 for the ngrams of a specific word in the corpus) and the stemmer_truncate parameter controls the number of rounding digits for the ngrams of the word. The main purpose was to give the same relative frequency to words appearing approximately the same on the corpus.</p>
</dd>
<dt><code>stemmer_batches</code></dt><dd><p>a numeric value greater than 0. Applies only to <em>ngram_sequential</em>. Splits the corpus into batches with the option to run the batches in multiple threads.</p>
</dd>
<dt><code>threads</code></dt><dd><p>an integer specifying the number of cores to run in parallel</p>
</dd>
<dt><code>save_2single_file</code></dt><dd><p>either TRUE or FALSE. If TRUE then the output data will be saved in a single file. Otherwise the data will be saved in multiple files with incremented enumeration</p>
</dd>
<dt><code>increment_batch_nr</code></dt><dd><p>a numeric value. The enumeration of the output files will start from the <em>increment_batch_nr</em>. If the <em>save_2single_file</em> parameter is TRUE then the <em>increment_batch_nr</em> parameter won't be taken into consideration.</p>
</dd>
<dt><code>vocabulary_path_folder</code></dt><dd><p>either NULL or a character string specifying the output folder where the vocabulary batches should be saved (after tokenization and transformation is applied). Applies to the <em>big_text_tokenizer</em> method.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-big_tokenize_transform-vocabulary_accumulator"></a>



<h4>Method <code>vocabulary_accumulator()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>big_tokenize_transform$vocabulary_accumulator(
  input_path_folder = NULL,
  vocabulary_path_file = NULL,
  max_num_chars = 100
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>input_path_folder</code></dt><dd><p>a character string specifying the folder where the input files are saved</p>
</dd>
<dt><code>vocabulary_path_file</code></dt><dd><p>either NULL or a character string specifying the output file where the vocabulary should be saved (after tokenization and transformation is applied). Applies to the <em>vocabulary_accumulator</em> method.</p>
</dd>
<dt><code>max_num_chars</code></dt><dd><p>a numeric value to limit the words of the output vocabulary to a maximum number of characters (applies to the <em>vocabulary_accumulator</em> function)</p>
</dd>
</dl>

</div>


<hr>
<a id="method-big_tokenize_transform-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>big_tokenize_transform$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

library(textTinyR)

fs &lt;- big_tokenize_transform$new(verbose = FALSE)

#---------------
# file splitter:
#---------------

fs$big_text_splitter(input_path_file = "input.txt",
                     output_path_folder = "/folder/output/",
                     end_query = "endword", batches = 5,
                     trimmed_line = FALSE)


#-------------
# file parser:
#-------------

fs$big_text_parser(input_path_folder = "/folder/output/",
                    output_path_folder = "/folder/parser/",
                    start_query = "startword", end_query = "endword",
                    min_lines = 1, trimmed_line = TRUE)


#----------------
# file tokenizer:
#----------------


 fs$big_text_tokenizer(input_path_folder = "/folder/parser/",
                       batches = 5, split_string=TRUE,
                       to_lower = TRUE, trim_token = TRUE,
                       max_num_char = 100, remove_stopwords = TRUE,
                       stemmer = "porter2_stemmer", threads = 1,
                       path_2folder="/folder/output_token/",
                       vocabulary_path_folder="/folder/VOCAB/")

#-------------------
# vocabulary counts:
#-------------------


fs$vocabulary_accumulator(input_path_folder = "/folder/VOCAB/",
                           vocabulary_path_file = "/folder/vocab.txt",
                           max_num_chars = 50)


## End(Not run)
</code></pre>

<hr>
<h2 id='bytes_converter'>bytes converter of a text file ( KB, MB or GB )</h2><span id='topic+bytes_converter'></span>

<h3>Description</h3>

<p>bytes converter of a text file ( KB, MB or GB )
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bytes_converter(input_path_file = NULL, unit = "MB")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bytes_converter_+3A_input_path_file">input_path_file</code></td>
<td>
<p>a character string specifying the path to the input file</p>
</td></tr>
<tr><td><code id="bytes_converter_+3A_unit">unit</code></td>
<td>
<p>a character string specifying the unit. One of <em>KB</em>, <em>MB</em>, <em>GB</em></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a number
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

library(textTinyR)

bc = bytes_converter(input_path_file = 'some_file.txt', unit = "MB")


## End(Not run)
</code></pre>

<hr>
<h2 id='cluster_frequency'>Frequencies of an existing cluster object</h2><span id='topic+cluster_frequency'></span>

<h3>Description</h3>

<p>Frequencies of an existing cluster object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster_frequency(tokenized_list_text, cluster_vector, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluster_frequency_+3A_tokenized_list_text">tokenized_list_text</code></td>
<td>
<p>a list of tokenized text documents. This can be the result of the <em>textTinyR::tokenize_transform_vec_docs</em> function with the <em>as_token</em> parameter set to TRUE (the <em>token</em> object of the output)</p>
</td></tr>
<tr><td><code id="cluster_frequency_+3A_cluster_vector">cluster_vector</code></td>
<td>
<p>a numeric vector. This can be the result of the <em>ClusterR::KMeans_rcpp</em> function (the <em>clusters</em> object of the output)</p>
</td></tr>
<tr><td><code id="cluster_frequency_+3A_verbose">verbose</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then information will be printed out in the R session.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function takes a list of tokenized text and a numeric vector of clusters and returns the sorted frequency of each cluster. The length of the <em>tokenized_list_text</em> object must be equal to the length of the <em>cluster_vector</em> object
</p>


<h3>Value</h3>

<p>a list of data.tables
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(textTinyR)

tok_lst = list(c('the', 'the', 'tokens', 'of', 'first', 'document'),
               c('the', 'tokens', 'of', 'of', 'second', 'document'),
               c('the', 'tokens', 'of', 'third', 'third', 'document'))

vec_clust = rep(1:6, 3)

res = cluster_frequency(tok_lst, vec_clust)
</code></pre>

<hr>
<h2 id='COS_TEXT'>Cosine similarity for text documents</h2><span id='topic+COS_TEXT'></span>

<h3>Description</h3>

<p>Cosine similarity for text documents
</p>


<h3>Usage</h3>

<pre><code class='language-R'>COS_TEXT(
  text_vector1 = NULL,
  text_vector2 = NULL,
  threads = 1,
  separator = " "
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="COS_TEXT_+3A_text_vector1">text_vector1</code></td>
<td>
<p>a character string vector representing text documents (it should have the same length as the text_vector2)</p>
</td></tr>
<tr><td><code id="COS_TEXT_+3A_text_vector2">text_vector2</code></td>
<td>
<p>a character string vector representing text documents (it should have the same length as the text_vector1)</p>
</td></tr>
<tr><td><code id="COS_TEXT_+3A_threads">threads</code></td>
<td>
<p>a numeric value specifying the number of cores to run in parallel</p>
</td></tr>
<tr><td><code id="COS_TEXT_+3A_separator">separator</code></td>
<td>
<p>specifies the separator used between words of each character string in the text vectors</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function calculates the <em>cosine</em> distance between pairs of text sequences of two character string vectors
</p>


<h3>Value</h3>

<p>a numeric vector
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(textTinyR)

vec1 = c('use this', 'function to compute the')

vec2 = c('cosine distance', 'between text sequences')

out = COS_TEXT(text_vector1 = vec1, text_vector2 = vec2, separator = " ")
</code></pre>

<hr>
<h2 id='cosine_distance'>cosine distance of two character strings (each string consists of more than one words)</h2><span id='topic+cosine_distance'></span>

<h3>Description</h3>

<p>cosine distance of two character strings (each string consists of more than one words)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cosine_distance(sentence1, sentence2, split_separator = " ")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cosine_distance_+3A_sentence1">sentence1</code></td>
<td>
<p>a character string consisting of multiple words</p>
</td></tr>
<tr><td><code id="cosine_distance_+3A_sentence2">sentence2</code></td>
<td>
<p>a character string consisting of multiple words</p>
</td></tr>
<tr><td><code id="cosine_distance_+3A_split_separator">split_separator</code></td>
<td>
<p>a character string specifying the delimiter(s) to split the sentence</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a float number
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(textTinyR)

sentence1 = 'this is one sentence'

sentence2 = 'this is a similar sentence'

cds = cosine_distance(sentence1, sentence2)
</code></pre>

<hr>
<h2 id='Count_Rows'>Number of rows of a file</h2><span id='topic+Count_Rows'></span>

<h3>Description</h3>

<p>Number of rows of a file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Count_Rows(PATH, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Count_Rows_+3A_path">PATH</code></td>
<td>
<p>a character string specifying the path to a file</p>
</td></tr>
<tr><td><code id="Count_Rows_+3A_verbose">verbose</code></td>
<td>
<p>either TRUE or FALSE</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function returns the number of rows for a file. It doesn't load the data in memory.
</p>


<h3>Value</h3>

<p>a numeric value
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(textTinyR)

PATH = system.file("example_files", "word_vecs.txt", package = "textTinyR")

num_rows = Count_Rows(PATH)
</code></pre>

<hr>
<h2 id='dense_2sparse'>convert a dense matrix to a sparse matrix</h2><span id='topic+dense_2sparse'></span>

<h3>Description</h3>

<p>convert a dense matrix to a sparse matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dense_2sparse(dense_mat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dense_2sparse_+3A_dense_mat">dense_mat</code></td>
<td>
<p>a dense matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a sparse matrix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(textTinyR)

tmp = matrix(sample(0:1, 100, replace = TRUE), 10, 10)

sp_mat = dense_2sparse(tmp)
</code></pre>

<hr>
<h2 id='dice_distance'>dice similarity of words using n-grams</h2><span id='topic+dice_distance'></span>

<h3>Description</h3>

<p>dice similarity of words using n-grams
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dice_distance(word1, word2, n_grams = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dice_distance_+3A_word1">word1</code></td>
<td>
<p>a character string</p>
</td></tr>
<tr><td><code id="dice_distance_+3A_word2">word2</code></td>
<td>
<p>a character string</p>
</td></tr>
<tr><td><code id="dice_distance_+3A_n_grams">n_grams</code></td>
<td>
<p>a value specifying the consecutive n-grams of the words</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a float number
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(textTinyR)

word1 = 'one_word'

word2 = 'two_words'

dts = dice_distance(word1, word2, n_grams = 2)
</code></pre>

<hr>
<h2 id='dims_of_word_vecs'>dimensions of a word vectors file</h2><span id='topic+dims_of_word_vecs'></span>

<h3>Description</h3>

<p>dimensions of a word vectors file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dims_of_word_vecs(input_file = NULL, read_delimiter = "\n")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dims_of_word_vecs_+3A_input_file">input_file</code></td>
<td>
<p>a character string specifying a valid path to a text file</p>
</td></tr>
<tr><td><code id="dims_of_word_vecs_+3A_read_delimiter">read_delimiter</code></td>
<td>
<p>a character string specifying the row delimiter of the text file</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function takes a valid path to a file and a file delimiter as input and estimates the dimensions of the word vectors by using the first row of the file.
</p>


<h3>Value</h3>

<p>a numeric value
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(textTinyR)

PATH = system.file("example_files", "word_vecs.txt", package = "textTinyR")

dimensions = dims_of_word_vecs(input_file = PATH)
</code></pre>

<hr>
<h2 id='Doc2Vec'>Conversion of text documents to word-vector-representation features ( Doc2Vec )</h2><span id='topic+Doc2Vec'></span>

<h3>Description</h3>

<p>Conversion of text documents to word-vector-representation features ( Doc2Vec )
</p>
<p>Conversion of text documents to word-vector-representation features ( Doc2Vec )
</p>


<h3>Usage</h3>

<pre><code class='language-R'># utl &lt;- Doc2Vec$new(token_list = NULL, word_vector_FILE = NULL,

       #                    print_every_rows = 10000, verbose = FALSE,

       #                    copy_data = FALSE)
</code></pre>


<h3>Details</h3>

<p>the <em>pre_processed_wv</em> method should be used after the initialization of the <em>Doc2Vec</em> class, if the <em>copy_data</em> parameter is set to TRUE, in order to inspect the pre-processed word-vectors.
</p>
<p>The <em>global_term_weights</em> method is part of the <em>sparse_term_matrix</em> R6 class of the <em>textTinyR package</em>. One can come to the correct <em>global_term_weights</em> by using the
<em>sparse_term_matrix</em> class and by setting the <em>tf_idf</em> parameter to FALSE and the <em>normalize</em> parameter to NULL. In <em>Doc2Vec</em> class, if method equals to <em>idf</em> then the <em>global_term_weights</em> parameter should not be equal to NULL.
</p>
<p>Explanation of the various <em>methods</em> :
</p>

<dl>
<dt>sum_sqrt</dt><dd><p>Assuming that a single sublist of the token list will be taken into consideration : the wordvectors of each word of the sublist of tokens will be accumulated to a vector equal to the length of the wordvector (INITIAL_WORD_VECTOR). Then a scalar will be computed using this INITIAL_WORD_VECTOR in the following way : the INITIAL_WORD_VECTOR will be raised to the power of 2.0, then the resulted wordvector will be summed and the square-root will be calculated. The INITIAL_WORD_VECTOR will be divided by the resulted scalar</p>
</dd>
<dt>min_max_norm</dt><dd><p>Assuming that a single sublist of the token list will be taken into consideration : the wordvectors of each word of the sublist of tokens will be first <em>min-max</em> normalized and then will be accumulated to a vector equal to the length of the initial wordvector</p>
</dd>
<dt>idf</dt><dd><p>Assuming that a single sublist of the token list will be taken into consideration : the word-vector of each term in the sublist will be multiplied with the corresponding <em>idf</em> of the <em>global weights term</em></p>
</dd>
</dl>
<p>There might be slight differences in the output data for each method depending on the input value of the <em>copy_data</em> parameter (if it's either TRUE or FALSE).

</p>


<h3>Value</h3>

<p>a matrix
</p>


<h3>Methods</h3>


<dl>
<dt><code>Doc2Vec$new(token_list = NULL, word_vector_FILE = NULL, print_every_rows = 10000, verbose = FALSE, copy_data = FALSE)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>doc2vec_methods(method = "sum_sqrt", global_term_weights = NULL, threads = 1)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>pre_processed_wv()</code></dt><dd></dd>
</dl>



<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-documents_to_wordvectors-new"><code>Doc2Vec$new()</code></a>
</p>
</li>
<li> <p><a href="#method-documents_to_wordvectors-doc2vec_methods"><code>Doc2Vec$doc2vec_methods()</code></a>
</p>
</li>
<li> <p><a href="#method-documents_to_wordvectors-pre_processed_wv"><code>Doc2Vec$pre_processed_wv()</code></a>
</p>
</li>
<li> <p><a href="#method-documents_to_wordvectors-clone"><code>Doc2Vec$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-documents_to_wordvectors-new"></a>



<h4>Method <code>new()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>Doc2Vec$new(
  token_list = NULL,
  word_vector_FILE = NULL,
  print_every_rows = 10000,
  verbose = FALSE,
  copy_data = FALSE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>token_list</code></dt><dd><p>either NULL or a list of tokenized text documents</p>
</dd>
<dt><code>word_vector_FILE</code></dt><dd><p>a valid path to a text file, where the word-vectors are saved</p>
</dd>
<dt><code>print_every_rows</code></dt><dd><p>a numeric value greater than 1 specifying the print intervals. Frequent output in the R session can slow down the function especially in case of big files.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>either TRUE or FALSE. If TRUE then information will be printed out in the R session.</p>
</dd>
<dt><code>copy_data</code></dt><dd><p>either TRUE or FALSE. If FALSE then a pointer will be created and no copy of the initial data takes place (memory efficient especially for big datasets). This is an alternative way to pre-process the data.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-documents_to_wordvectors-doc2vec_methods"></a>



<h4>Method <code>doc2vec_methods()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>Doc2Vec$doc2vec_methods(
  method = "sum_sqrt",
  global_term_weights = NULL,
  threads = 1
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>method</code></dt><dd><p>a character string specifying the method to use. One of <em>sum_sqrt</em>, <em>min_max_norm</em> or <em>idf</em>. See the details section for more information.</p>
</dd>
<dt><code>global_term_weights</code></dt><dd><p>either NULL or the output of the <em>global_term_weights</em> method of the textTinyR package. See the details section for more information.</p>
</dd>
<dt><code>threads</code></dt><dd><p>a numeric value specifying the number of cores to run in parallel</p>
</dd>
</dl>

</div>


<hr>
<a id="method-documents_to_wordvectors-pre_processed_wv"></a>



<h4>Method <code>pre_processed_wv()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>Doc2Vec$pre_processed_wv()</pre></div>


<hr>
<a id="method-documents_to_wordvectors-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>Doc2Vec$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>
library(textTinyR)

#---------------------------------
# tokenized text in form of a list
#---------------------------------

tok_text = list(c('the', 'result', 'of'), c('doc2vec', 'are', 'vector', 'features'))

#-------------------------
# path to the word vectors
#-------------------------

PATH = system.file("example_files", "word_vecs.txt", package = "textTinyR")


init = Doc2Vec$new(token_list = tok_text, word_vector_FILE = PATH)


out = init$doc2vec_methods(method = "sum_sqrt")
</code></pre>

<hr>
<h2 id='JACCARD_DICE'>Jaccard or Dice similarity for text documents</h2><span id='topic+JACCARD_DICE'></span>

<h3>Description</h3>

<p>Jaccard or Dice similarity for text documents
</p>


<h3>Usage</h3>

<pre><code class='language-R'>JACCARD_DICE(
  token_list1 = NULL,
  token_list2 = NULL,
  method = "jaccard",
  threads = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="JACCARD_DICE_+3A_token_list1">token_list1</code></td>
<td>
<p>a list of tokenized text documents (it should have the same length as the token_list2)</p>
</td></tr>
<tr><td><code id="JACCARD_DICE_+3A_token_list2">token_list2</code></td>
<td>
<p>a list of tokenized text documents (it should have the same length as the token_list1)</p>
</td></tr>
<tr><td><code id="JACCARD_DICE_+3A_method">method</code></td>
<td>
<p>a character string specifying the similarity metric. One of 'jaccard', 'dice'</p>
</td></tr>
<tr><td><code id="JACCARD_DICE_+3A_threads">threads</code></td>
<td>
<p>a numeric value specifying the number of cores to run in parallel</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function calculates either the <em>jaccard</em> or the <em>dice</em> distance between pairs of tokenized text of two lists
</p>


<h3>Value</h3>

<p>a numeric vector
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(textTinyR)

lst1 = list(c('use', 'this', 'function', 'to'), c('either', 'compute', 'the', 'jaccard'))

lst2 = list(c('or', 'the', 'dice', 'distance'), c('for', 'two', 'same', 'sized', 'lists'))

out = JACCARD_DICE(token_list1 = lst1, token_list2 = lst2, method = 'jaccard', threads = 1)
</code></pre>

<hr>
<h2 id='levenshtein_distance'>levenshtein distance of two words</h2><span id='topic+levenshtein_distance'></span>

<h3>Description</h3>

<p>levenshtein distance of two words
</p>


<h3>Usage</h3>

<pre><code class='language-R'>levenshtein_distance(word1, word2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="levenshtein_distance_+3A_word1">word1</code></td>
<td>
<p>a character string</p>
</td></tr>
<tr><td><code id="levenshtein_distance_+3A_word2">word2</code></td>
<td>
<p>a character string</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a float number
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(textTinyR)

word1 = 'one_word'

word2 = 'two_words'

lvs = levenshtein_distance(word1, word2)
</code></pre>

<hr>
<h2 id='load_sparse_binary'>load a sparse matrix in binary format</h2><span id='topic+load_sparse_binary'></span>

<h3>Description</h3>

<p>load a sparse matrix in binary format
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_sparse_binary(file_name = "save_sparse.mat")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="load_sparse_binary_+3A_file_name">file_name</code></td>
<td>
<p>a character string specifying the binary file</p>
</td></tr>
</table>


<h3>Value</h3>

<p>loads a sparse matrix from a file
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

library(textTinyR)

load_sparse_binary(file_name = "save_sparse.mat")


## End(Not run)
</code></pre>

<hr>
<h2 id='matrix_sparsity'>sparsity percentage of a sparse matrix</h2><span id='topic+matrix_sparsity'></span>

<h3>Description</h3>

<p>sparsity percentage of a sparse matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matrix_sparsity(sparse_matrix)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matrix_sparsity_+3A_sparse_matrix">sparse_matrix</code></td>
<td>
<p>a sparse matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric value (percentage)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(textTinyR)

tmp = matrix(sample(0:1, 100, replace = TRUE), 10, 10)

sp_mat = dense_2sparse(tmp)

dbl = matrix_sparsity(sp_mat)
</code></pre>

<hr>
<h2 id='read_characters'>read a specific number of characters from a text file</h2><span id='topic+read_characters'></span>

<h3>Description</h3>

<p>read a specific number of characters from a text file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_characters(input_file = NULL, characters = 100, write_2file = "")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read_characters_+3A_input_file">input_file</code></td>
<td>
<p>a character string specifying a valid path to a text file</p>
</td></tr>
<tr><td><code id="read_characters_+3A_characters">characters</code></td>
<td>
<p>a numeric value specifying the number of characters to read</p>
</td></tr>
<tr><td><code id="read_characters_+3A_write_2file">write_2file</code></td>
<td>
<p>either an empty string (&quot;&quot;) or a character string specifying a valid output file to write the subset of the input file</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

library(textTinyR)

txfl = read_characters(input_file = 'input.txt', characters = 100)


## End(Not run)
</code></pre>

<hr>
<h2 id='read_rows'>read a specific number of rows from a text file</h2><span id='topic+read_rows'></span>

<h3>Description</h3>

<p>read a specific number of rows from a text file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_rows(
  input_file = NULL,
  read_delimiter = "\n",
  rows = 100,
  write_2file = ""
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read_rows_+3A_input_file">input_file</code></td>
<td>
<p>a character string specifying a valid path to a text file</p>
</td></tr>
<tr><td><code id="read_rows_+3A_read_delimiter">read_delimiter</code></td>
<td>
<p>a character string specifying the row delimiter of the text file</p>
</td></tr>
<tr><td><code id="read_rows_+3A_rows">rows</code></td>
<td>
<p>a numeric value specifying the number of rows to read</p>
</td></tr>
<tr><td><code id="read_rows_+3A_write_2file">write_2file</code></td>
<td>
<p>either &quot;&quot; or a character string specifying a valid output file to write the subset of the input file</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

library(textTinyR)

txfl = read_rows(input_file = 'input.txt', rows = 100)


## End(Not run)
</code></pre>

<hr>
<h2 id='save_sparse_binary'>save a sparse matrix in binary format</h2><span id='topic+save_sparse_binary'></span>

<h3>Description</h3>

<p>save a sparse matrix in binary format
</p>


<h3>Usage</h3>

<pre><code class='language-R'>save_sparse_binary(sparse_matrix, file_name = "save_sparse.mat")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="save_sparse_binary_+3A_sparse_matrix">sparse_matrix</code></td>
<td>
<p>a sparse matrix</p>
</td></tr>
<tr><td><code id="save_sparse_binary_+3A_file_name">file_name</code></td>
<td>
<p>a character string specifying the binary file</p>
</td></tr>
</table>


<h3>Value</h3>

<p>writes the sparse matrix to a file
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(textTinyR)

tmp = matrix(sample(0:1, 100, replace = TRUE), 10, 10)

sp_mat = dense_2sparse(tmp)

# save_sparse_binary(sp_mat, file_name = "save_sparse.mat")
</code></pre>

<hr>
<h2 id='select_predictors'>Exclude highly correlated predictors</h2><span id='topic+select_predictors'></span>

<h3>Description</h3>

<p>Exclude highly correlated predictors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>select_predictors(
  response_vector,
  predictors_matrix,
  response_lower_thresh = 0.1,
  predictors_upper_thresh = 0.75,
  threads = 1,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="select_predictors_+3A_response_vector">response_vector</code></td>
<td>
<p>a numeric vector (the length should be equal to the rows of the <em>predictors_matrix</em> parameter)</p>
</td></tr>
<tr><td><code id="select_predictors_+3A_predictors_matrix">predictors_matrix</code></td>
<td>
<p>a numeric matrix (the rows should be equal to the length of the <em>response_vector</em> parameter)</p>
</td></tr>
<tr><td><code id="select_predictors_+3A_response_lower_thresh">response_lower_thresh</code></td>
<td>
<p>a numeric value. This parameter allows the user to keep all the predictors having a correlation with the response <em>greater</em> than the <em>response_lower_thresh</em> value.</p>
</td></tr>
<tr><td><code id="select_predictors_+3A_predictors_upper_thresh">predictors_upper_thresh</code></td>
<td>
<p>a numeric value. This parameter allows the user to keep all the predictors having a correlation comparing to the other predictors <em>less</em> than the <em>predictors_upper_thresh</em> value.</p>
</td></tr>
<tr><td><code id="select_predictors_+3A_threads">threads</code></td>
<td>
<p>a numeric value specifying the number of cores to run in parallel</p>
</td></tr>
<tr><td><code id="select_predictors_+3A_verbose">verbose</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then information will be printed out in the R session.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function works in the following way : The correlation of the predictors with the response is first calculated and the resulted correlations are sorted in decreasing order. Then iteratively predictors with correlation
higher than the <em>predictors_upper_thresh</em> value are removed by favoring those predictors which are more correlated with the response variable. If the <em>response_lower_thresh</em> value is greater than 0.0 then only predictors
having a correlation higher than or equal to the <em>response_lower_thresh</em> value will be kept, otherwise they will be excluded.
This function returns the indices of the <em>predictors</em> and is useful in case of multicollinearity.
</p>
<p>If during computation the correlation between the response variable and a potential predictor is equal to NA or +/- Inf, then a correlation of 0.0 will be assigned to this particular pair.
</p>


<h3>Value</h3>

<p>a vector of column-indices
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(textTinyR)

set.seed(1)
resp = runif(100)

set.seed(2)
col = runif(100)

matr = matrix(c(col, col^4, col^6, col^8, col^10), nrow = 100, ncol = 5)

out = select_predictors(resp, matr, predictors_upper_thresh = 0.75)
</code></pre>

<hr>
<h2 id='sparse_Means'>RowMens and colMeans for a sparse matrix</h2><span id='topic+sparse_Means'></span>

<h3>Description</h3>

<p>RowMens and colMeans for a sparse matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sparse_Means(sparse_matrix, rowMeans = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sparse_Means_+3A_sparse_matrix">sparse_matrix</code></td>
<td>
<p>a sparse matrix</p>
</td></tr>
<tr><td><code id="sparse_Means_+3A_rowmeans">rowMeans</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then the row-means will be calculated, otherwise the column-means</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector with either the row- or the column-sums of the matrix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(textTinyR)

tmp = matrix(sample(0:1, 100, replace = TRUE), 10, 10)

sp_mat = dense_2sparse(tmp)

spsm = sparse_Means(sp_mat, rowMeans = FALSE)
</code></pre>

<hr>
<h2 id='sparse_Sums'>RowSums and colSums for a sparse matrix</h2><span id='topic+sparse_Sums'></span>

<h3>Description</h3>

<p>RowSums and colSums for a sparse matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sparse_Sums(sparse_matrix, rowSums = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sparse_Sums_+3A_sparse_matrix">sparse_matrix</code></td>
<td>
<p>a sparse matrix</p>
</td></tr>
<tr><td><code id="sparse_Sums_+3A_rowsums">rowSums</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then the row-sums will be calculated, otherwise the column-sums</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector with either the row- or the column-sums of the matrix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(textTinyR)

tmp = matrix(sample(0:1, 100, replace = TRUE), 10, 10)

sp_mat = dense_2sparse(tmp)

spsm = sparse_Sums(sp_mat, rowSums = FALSE)
</code></pre>

<hr>
<h2 id='sparse_term_matrix'>Term matrices and statistics ( document-term-matrix, term-document-matrix)</h2><span id='topic+sparse_term_matrix'></span>

<h3>Description</h3>

<p>Term matrices and statistics ( document-term-matrix, term-document-matrix)
</p>
<p>Term matrices and statistics ( document-term-matrix, term-document-matrix)
</p>


<h3>Usage</h3>

<pre><code class='language-R'># utl &lt;- sparse_term_matrix$new(vector_data = NULL, file_data = NULL,

#                                      document_term_matrix = TRUE)
</code></pre>


<h3>Details</h3>

<p>the <em>Term_Matrix</em> function takes either a character vector of strings or a text file and after tokenization and transformation returns either a document-term-matrix or a term-document-matrix
</p>
<p>the <em>triplet_data</em> function returns the triplet data, which is used internally (in c++), to construct the Term Matrix. The triplet data could be usefull for secondary purposes, such as in word vector representations.
</p>
<p>the <em>global_term_weights</em> function returns a list of length two. The first sublist includes the <em>terms</em> and the second sublist the <em>global-term-weights</em>. The <em>tf_idf</em> parameter should be set to FALSE and the <em>normalize</em> parameter to NULL. This function is normally used in conjuction with word-vector-embeddings.
</p>
<p>the <em>Term_Matrix_Adjust</em> function removes sparse terms from a sparse matrix using a sparsity threshold
</p>
<p>the <em>term_associations</em> function finds the associations between the given terms (Terms argument) and all the other terms in the corpus by calculating their correlation. There is also the option to keep a specific number of terms from the output table using the <em>keep_terms</em> parameter.
</p>
<p>the <em>most_frequent_terms</em> function returns the most frequent terms of the corpus using the output of the sparse matrix. The user has the option to keep a specific number of terms from the output table using the <em>keep_terms</em> parameter.
</p>
<p>Stemming of the english language is done using the porter2-stemmer, for details see <a href="https://github.com/smassung/porter2_stemmer">https://github.com/smassung/porter2_stemmer</a>
</p>


<h3>Methods</h3>


<dl>
<dt><code>sparse_term_matrix$new(vector_data = NULL, file_data = NULL, document_term_matrix = TRUE)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>Term_Matrix(sort_terms = FALSE, to_lower = FALSE, to_upper = FALSE, utf_locale = "", remove_char = "", remove_punctuation_string = FALSE, remove_punctuation_vector = FALSE, remove_numbers = FALSE, trim_token = FALSE, split_string = FALSE, split_separator = " .,;:()?!", remove_stopwords = FALSE, language = "english", min_num_char = 1, max_num_char = Inf, stemmer = NULL, min_n_gram = 1, max_n_gram = 1, skip_n_gram = 1, skip_distance = 0, n_gram_delimiter = " ", print_every_rows = 1000, normalize = NULL, tf_idf = FALSE, threads = 1, verbose = FALSE)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>triplet_data()</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>global_term_weights()</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>Term_Matrix_Adjust(sparsity_thresh = 1.0)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>term_associations(Terms = NULL, keep_terms = NULL, verbose = FALSE)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>most_frequent_terms(keep_terms = NULL, threads = 1, verbose = FALSE)</code></dt><dd></dd>
</dl>



<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-sparse_term_matrix-new"><code>sparse_term_matrix$new()</code></a>
</p>
</li>
<li> <p><a href="#method-sparse_term_matrix-Term_Matrix"><code>sparse_term_matrix$Term_Matrix()</code></a>
</p>
</li>
<li> <p><a href="#method-sparse_term_matrix-triplet_data"><code>sparse_term_matrix$triplet_data()</code></a>
</p>
</li>
<li> <p><a href="#method-sparse_term_matrix-global_term_weights"><code>sparse_term_matrix$global_term_weights()</code></a>
</p>
</li>
<li> <p><a href="#method-sparse_term_matrix-Term_Matrix_Adjust"><code>sparse_term_matrix$Term_Matrix_Adjust()</code></a>
</p>
</li>
<li> <p><a href="#method-sparse_term_matrix-term_associations"><code>sparse_term_matrix$term_associations()</code></a>
</p>
</li>
<li> <p><a href="#method-sparse_term_matrix-most_frequent_terms"><code>sparse_term_matrix$most_frequent_terms()</code></a>
</p>
</li>
<li> <p><a href="#method-sparse_term_matrix-clone"><code>sparse_term_matrix$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-sparse_term_matrix-new"></a>



<h4>Method <code>new()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>sparse_term_matrix$new(
  vector_data = NULL,
  file_data = NULL,
  document_term_matrix = TRUE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>vector_data</code></dt><dd><p>either NULL or a character vector of documents</p>
</dd>
<dt><code>file_data</code></dt><dd><p>either NULL or a valid character path to a text file</p>
</dd>
<dt><code>document_term_matrix</code></dt><dd><p>either TRUE or FALSE. If TRUE then a document-term-matrix will be returned, otherwise a term-document-matrix</p>
</dd>
</dl>

</div>


<hr>
<a id="method-sparse_term_matrix-Term_Matrix"></a>



<h4>Method <code>Term_Matrix()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>sparse_term_matrix$Term_Matrix(
  sort_terms = FALSE,
  to_lower = FALSE,
  to_upper = FALSE,
  utf_locale = "",
  remove_char = "",
  remove_punctuation_string = FALSE,
  remove_punctuation_vector = FALSE,
  remove_numbers = FALSE,
  trim_token = FALSE,
  split_string = FALSE,
  split_separator = " \r\n\t.,;:()?!//",
  remove_stopwords = FALSE,
  language = "english",
  min_num_char = 1,
  max_num_char = Inf,
  stemmer = NULL,
  min_n_gram = 1,
  max_n_gram = 1,
  skip_n_gram = 1,
  skip_distance = 0,
  n_gram_delimiter = " ",
  print_every_rows = 1000,
  normalize = NULL,
  tf_idf = FALSE,
  threads = 1,
  verbose = FALSE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>sort_terms</code></dt><dd><p>either TRUE or FALSE specifying if the initial terms should be sorted ( so that the output sparse matrix is sorted in alphabetical order )</p>
</dd>
<dt><code>to_lower</code></dt><dd><p>either TRUE or FALSE. If TRUE the character string will be converted to lower case</p>
</dd>
<dt><code>to_upper</code></dt><dd><p>either TRUE or FALSE. If TRUE the character string will be converted to upper case</p>
</dd>
<dt><code>utf_locale</code></dt><dd><p>the language specific locale to use in case that either the <em>to_lower</em> or the <em>to_upper</em> parameter is TRUE and the text file language is other than english. For instance if the language of a text file is greek then the <em>utf_locale</em> parameter should be <em>'el_GR.UTF-8'</em> ( <em>language_country.encoding</em> ). A wrong utf-locale does not raise an error, however the runtime of the function increases.</p>
</dd>
<dt><code>remove_char</code></dt><dd><p>a string specifying the specific characters that should be removed from a text file. If the <em>remove_char</em> is &quot;&quot; then no removal of characters take place</p>
</dd>
<dt><code>remove_punctuation_string</code></dt><dd><p>either TRUE or FALSE. If TRUE then the punctuation of the character string will be removed (applies before the split function)</p>
</dd>
<dt><code>remove_punctuation_vector</code></dt><dd><p>either TRUE or FALSE. If TRUE then the punctuation of the vector of the character strings will be removed  (after the string split has taken place)</p>
</dd>
<dt><code>remove_numbers</code></dt><dd><p>either TRUE or FALSE. If TRUE then any numbers in the character string will be removed</p>
</dd>
<dt><code>trim_token</code></dt><dd><p>either TRUE or FALSE. If TRUE then the string will be trimmed (left and/or right)</p>
</dd>
<dt><code>split_string</code></dt><dd><p>either TRUE or FALSE. If TRUE then the character string will be split using the <em>split_separator</em> as delimiter. The user can also specify multiple delimiters.</p>
</dd>
<dt><code>split_separator</code></dt><dd><p>a character string specifying the character delimiter(s)</p>
</dd>
<dt><code>remove_stopwords</code></dt><dd><p>either TRUE, FALSE or a character vector of user defined stop words. If TRUE then by using the <em>language</em> parameter the corresponding stop words vector will be uploaded.</p>
</dd>
<dt><code>language</code></dt><dd><p>a character string which defaults to english. If the <em>remove_stopwords</em> parameter is TRUE then the corresponding stop words vector will be uploaded. Available languages
are <em>afrikaans</em>, <em>arabic</em>, <em>armenian</em>, <em>basque</em>, <em>bengali</em>, <em>breton</em>, <em>bulgarian</em>, <em>catalan</em>,
<em>croatian</em>, <em>czech</em>, <em>danish</em>, <em>dutch</em>, <em>english</em>, <em>estonian</em>,
<em>finnish</em>, <em>french</em>, <em>galician</em>, <em>german</em>, <em>greek</em>, <em>hausa</em>, <em>hebrew</em>, <em>hindi</em>, <em>hungarian</em>,
<em>indonesian</em>, <em>irish</em>, <em>italian</em>, <em>latvian</em>, <em>marathi</em>,
<em>norwegian</em>, <em>persian</em>, <em>polish</em>, <em>portuguese</em>, <em>romanian</em>, <em>russian</em>, <em>slovak</em>, <em>slovenian</em>,
<em>somalia</em>, <em>spanish</em>, <em>swahili</em>, <em>swedish</em>, <em>turkish</em>, <em>yoruba</em>, <em>zulu</em></p>
</dd>
<dt><code>min_num_char</code></dt><dd><p>an integer specifying the minimum number of characters to keep. If the <em>min_num_char</em> is greater than 1 then character strings with more than 1 characters will be returned</p>
</dd>
<dt><code>max_num_char</code></dt><dd><p>an integer specifying the maximum number of characters to keep. The <em>max_num_char</em> should be less than or equal to <em>Inf</em> (in this function the Inf value translates to a word-length of 1000000000)</p>
</dd>
<dt><code>stemmer</code></dt><dd><p>a character string specifying the stemming method. Available method is the <em>porter2_stemmer</em>. See details for more information.</p>
</dd>
<dt><code>min_n_gram</code></dt><dd><p>an integer specifying the minimum number of n-grams. The minimum number of min_n_gram is 1.</p>
</dd>
<dt><code>max_n_gram</code></dt><dd><p>an integer specifying the maximum number of n-grams. The minimum number of max_n_gram is 1.</p>
</dd>
<dt><code>skip_n_gram</code></dt><dd><p>an integer specifying the number of skip-n-grams. The minimum number of skip_n_gram is 1. The skip_n_gram gives the (max.) n-grams using the <em>skip_distance</em> parameter. If <em>skip_n_gram</em> is greater than 1 then both <em>min_n_gram</em> and <em>max_n_gram</em> should be set to 1.</p>
</dd>
<dt><code>skip_distance</code></dt><dd><p>an integer specifying the skip distance between the words. The minimum value for the skip distance is 0, in which case simple n-grams will be returned.</p>
</dd>
<dt><code>n_gram_delimiter</code></dt><dd><p>a character string specifying the n-gram delimiter (applies to both n-gram and skip-n-gram cases)</p>
</dd>
<dt><code>print_every_rows</code></dt><dd><p>a numeric value greater than 1 specifying the print intervals. Frequent output in the R session can slow down the function in case of big files.</p>
</dd>
<dt><code>normalize</code></dt><dd><p>either NULL or one of 'l1' or 'l2' normalization.</p>
</dd>
<dt><code>tf_idf</code></dt><dd><p>either TRUE or FALSE. If TRUE then the term-frequency-inverse-document-frequency will be returned</p>
</dd>
<dt><code>threads</code></dt><dd><p>an integer specifying the number of cores to run in parallel</p>
</dd>
<dt><code>verbose</code></dt><dd><p>either TRUE or FALSE. If TRUE then information will be printed out</p>
</dd>
</dl>

</div>


<hr>
<a id="method-sparse_term_matrix-triplet_data"></a>



<h4>Method <code>triplet_data()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>sparse_term_matrix$triplet_data()</pre></div>


<hr>
<a id="method-sparse_term_matrix-global_term_weights"></a>



<h4>Method <code>global_term_weights()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>sparse_term_matrix$global_term_weights()</pre></div>


<hr>
<a id="method-sparse_term_matrix-Term_Matrix_Adjust"></a>



<h4>Method <code>Term_Matrix_Adjust()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>sparse_term_matrix$Term_Matrix_Adjust(sparsity_thresh = 1)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>sparsity_thresh</code></dt><dd><p>a float number between 0.0 and 1.0 specifying the sparsity threshold in the <em>Term_Matrix_Adjust</em> function</p>
</dd>
</dl>

</div>


<hr>
<a id="method-sparse_term_matrix-term_associations"></a>



<h4>Method <code>term_associations()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>sparse_term_matrix$term_associations(
  Terms = NULL,
  keep_terms = NULL,
  verbose = FALSE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>Terms</code></dt><dd><p>a character vector specifying the character strings for which the associations will be calculated ( <em>term_associations</em> function )</p>
</dd>
<dt><code>keep_terms</code></dt><dd><p>either NULL or a numeric value specifying the number of terms to keep ( both in <em>term_associations</em> and <em>most_frequent_terms</em> functions )</p>
</dd>
<dt><code>verbose</code></dt><dd><p>either TRUE or FALSE. If TRUE then information will be printed out</p>
</dd>
</dl>

</div>


<hr>
<a id="method-sparse_term_matrix-most_frequent_terms"></a>



<h4>Method <code>most_frequent_terms()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>sparse_term_matrix$most_frequent_terms(
  keep_terms = NULL,
  threads = 1,
  verbose = FALSE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>keep_terms</code></dt><dd><p>either NULL or a numeric value specifying the number of terms to keep ( both in <em>term_associations</em> and <em>most_frequent_terms</em> functions )</p>
</dd>
<dt><code>threads</code></dt><dd><p>an integer specifying the number of cores to run in parallel</p>
</dd>
<dt><code>verbose</code></dt><dd><p>either TRUE or FALSE. If TRUE then information will be printed out</p>
</dd>
</dl>

</div>


<hr>
<a id="method-sparse_term_matrix-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>sparse_term_matrix$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

library(textTinyR)


sm &lt;- sparse_term_matrix$new(file_data = "/folder/my_data.txt",
                              document_term_matrix = TRUE)

#--------------
# term matrix :
#--------------

sm$Term_Matrix(sort_terms = TRUE, to_lower = TRUE,
                trim_token = TRUE, split_string = TRUE,
                remove_stopwords = TRUE, normalize = 'l1',
                stemmer = 'porter2_stemmer', threads = 1 )

#---------------
# triplet data :
#---------------

sm$triplet_data()


#----------------------
# global-term-weights :
#----------------------

sm$global_term_weights()


#-------------------------
# removal of sparse terms:
#-------------------------

sm$Term_Matrix_Adjust(sparsity_thresh = 0.995)


#-----------------------------------------------
# associations between terms of a sparse matrix:
#-----------------------------------------------

sm$term_associations(Terms = c("word", "sentence"), keep_terms = 10)


#---------------------------------------------
# most frequent terms using the sparse matrix:
#---------------------------------------------

sm$most_frequent_terms(keep_terms = 10, threads = 1)


## End(Not run)
</code></pre>

<hr>
<h2 id='TEXT_DOC_DISSIM'>Dissimilarity calculation of text documents</h2><span id='topic+TEXT_DOC_DISSIM'></span>

<h3>Description</h3>

<p>Dissimilarity calculation of text documents
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TEXT_DOC_DISSIM(
  first_matr = NULL,
  second_matr = NULL,
  method = "euclidean",
  batches = NULL,
  threads = 1,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TEXT_DOC_DISSIM_+3A_first_matr">first_matr</code></td>
<td>
<p>a numeric matrix where each row represents a text document ( has same dimensions as the <em>second_matr</em> )</p>
</td></tr>
<tr><td><code id="TEXT_DOC_DISSIM_+3A_second_matr">second_matr</code></td>
<td>
<p>a numeric matrix where each row represents a text document ( has same dimensions as the <em>first_matr</em> )</p>
</td></tr>
<tr><td><code id="TEXT_DOC_DISSIM_+3A_method">method</code></td>
<td>
<p>a dissimilarity metric in form of a character string. One of <em>euclidean</em>, <em>manhattan</em>, <em>chebyshev</em>, <em>canberra</em>, <em>braycurtis</em>, <em>pearson_correlation</em>, <em>cosine</em>,
<em>simple_matching_coefficient</em>, <em>hamming</em>, <em>jaccard_coefficient</em>, <em>Rao_coefficient</em></p>
</td></tr>
<tr><td><code id="TEXT_DOC_DISSIM_+3A_batches">batches</code></td>
<td>
<p>a numeric value specifying the number of batches</p>
</td></tr>
<tr><td><code id="TEXT_DOC_DISSIM_+3A_threads">threads</code></td>
<td>
<p>a numeric value specifying the number of cores to run in parallel</p>
</td></tr>
<tr><td><code id="TEXT_DOC_DISSIM_+3A_verbose">verbose</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then information will be printed in the console</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Row-wise dissimilarity calculation of text documents. The text document sequences should be converted to numeric matrices using for instance LSI (Latent Semantic Indexing).
If the numeric matrices are too big to be pre-processed, then one should use the <em>batches</em> parameter to split the data in batches before applying one of the dissimilarity metrics.
For parallelization (<em>threads</em>) OpenMP will be used.
</p>


<h3>Value</h3>

<p>a numeric vector
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

library(textTinyR)


# example input LSI matrices (see details section)
#-------------------------------------------------

set.seed(1)
LSI_matrix1 = matrix(runif(10000), 100, 100)

set.seed(2)
LSI_matrix2 = matrix(runif(10000), 100, 100)


txt_out = TEXT_DOC_DISSIM(first_matr = LSI_matrix1,

                          second_matr = LSI_matrix2, 'euclidean')

## End(Not run)
</code></pre>

<hr>
<h2 id='text_file_parser'>text file parser</h2><span id='topic+text_file_parser'></span>

<h3>Description</h3>

<p>text file parser
</p>


<h3>Usage</h3>

<pre><code class='language-R'>text_file_parser(
  input_path_file = NULL,
  output_path_file = "",
  start_query = NULL,
  end_query = NULL,
  min_lines = 1,
  trimmed_line = FALSE,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="text_file_parser_+3A_input_path_file">input_path_file</code></td>
<td>
<p>either a path to an input file or a vector of character strings ( normally the latter would represent ordered lines of a text file in form of a character vector )</p>
</td></tr>
<tr><td><code id="text_file_parser_+3A_output_path_file">output_path_file</code></td>
<td>
<p>either an empty character string (&quot;&quot;) or a character string specifying a path to an output file ( it applies only if the <em>input_path_file</em> parameter is a valid path to a file )</p>
</td></tr>
<tr><td><code id="text_file_parser_+3A_start_query">start_query</code></td>
<td>
<p>a character string or a vector of character strings. The <em>start_query</em> (if it's a single character string) is the first word of the subset of the data and should appear frequently at the beginning of each line in the text file.</p>
</td></tr>
<tr><td><code id="text_file_parser_+3A_end_query">end_query</code></td>
<td>
<p>a character string or a vector of character strings. The <em>end_query</em> (if it's a single character string) is the last word of the subset of the data and should appear frequently at the end of each line in the text file.</p>
</td></tr>
<tr><td><code id="text_file_parser_+3A_min_lines">min_lines</code></td>
<td>
<p>a numeric value specifying the minimum number of lines ( applies only if the <em>input_path_file</em> is a valid path to a file) . For instance if min_lines = 2, then only subsets of text with more than 1 lines will be pre-processed.</p>
</td></tr>
<tr><td><code id="text_file_parser_+3A_trimmed_line">trimmed_line</code></td>
<td>
<p>either TRUE or FALSE. If FALSE then each line of the text file will be trimmed both sides before applying the start_query and end_query</p>
</td></tr>
<tr><td><code id="text_file_parser_+3A_verbose">verbose</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then information will be printed in the console</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The text file should have a structure (such as an xml-structure), so that subsets can be extracted using the <em>start_query</em> and <em>end_query</em> parameters ( the same applies in case of a vector of character strings)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

library(textTinyR)

# In case that the 'input_path_file' is a valid path
#---------------------------------------------------

fp = text_file_parser(input_path_file = '/folder/input_data.txt',
                       output_path_file = '/folder/output_data.txt',
                       start_query = 'word_a', end_query = 'word_w',
                       min_lines = 1, trimmed_line = FALSE)


# In case that the 'input_path_file' is a character vector of strings
#--------------------------------------------------------------------

PATH_url = "https://FILE.xml"
con = url(PATH_url, method = "libcurl")
tmp_dat = read.delim(con, quote = "\"", comment.char = "", stringsAsFactors = FALSE)

vec_docs = unlist(lapply(1:length(as.vector(tmp_dat[, 1])), function(x)
                    trimws(tmp_dat[x, 1], which = "both")))

parse_data = text_file_parser(input_path_file = vec_docs,
                                start_query = c("&lt;query1&gt;", "&lt;query2&gt;", "&lt;query3&gt;"),
                                end_query = c("&lt;/query1&gt;", "&lt;/query2&gt;", "&lt;/query3&gt;"),
                                min_lines = 1, trimmed_line = TRUE)
                                

## End(Not run)
</code></pre>

<hr>
<h2 id='text_intersect'>intersection of words or letters in tokenized text</h2><span id='topic+text_intersect'></span>

<h3>Description</h3>

<p>intersection of words or letters in tokenized text
</p>
<p>intersection of words or letters in tokenized text
</p>


<h3>Usage</h3>

<pre><code class='language-R'># utl &lt;- text_intersect$new(token_list1 = NULL, token_list2 = NULL)
</code></pre>


<h3>Details</h3>

<p>This class includes methods for text or character intersection. If both <em>distinct</em> and <em>letters</em> are FALSE then the simple (count or ratio) word intersection will be computed.
</p>


<h3>Value</h3>

<p>a numeric vector
</p>


<h3>Methods</h3>


<dl>
<dt><code>text_intersect$new(file_data = NULL)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>count_intersect(distinct = FALSE, letters = FALSE)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>ratio_intersect(distinct = FALSE, letters = FALSE)</code></dt><dd></dd>
</dl>



<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-text_intersect-new"><code>text_intersect$new()</code></a>
</p>
</li>
<li> <p><a href="#method-text_intersect-count_intersect"><code>text_intersect$count_intersect()</code></a>
</p>
</li>
<li> <p><a href="#method-text_intersect-ratio_intersect"><code>text_intersect$ratio_intersect()</code></a>
</p>
</li>
<li> <p><a href="#method-text_intersect-clone"><code>text_intersect$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-text_intersect-new"></a>



<h4>Method <code>new()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>text_intersect$new(token_list1 = NULL, token_list2 = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>token_list1</code></dt><dd><p>a list, where each sublist is a tokenized text sequence (<em>token_list1</em> should be of same length with <em>token_list2</em>)</p>
</dd>
<dt><code>token_list2</code></dt><dd><p>a list, where each sublist is a tokenized text sequence (<em>token_list2</em> should be of same length with <em>token_list1</em>)</p>
</dd>
</dl>

</div>


<hr>
<a id="method-text_intersect-count_intersect"></a>



<h4>Method <code>count_intersect()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>text_intersect$count_intersect(distinct = FALSE, letters = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>distinct</code></dt><dd><p>either TRUE or FALSE. If TRUE then the intersection of <em>distinct</em> words (or letters) will be taken into account</p>
</dd>
<dt><code>letters</code></dt><dd><p>either TRUE or FALSE. If TRUE then the intersection of letters in the text sequences will be computed</p>
</dd>
</dl>

</div>


<hr>
<a id="method-text_intersect-ratio_intersect"></a>



<h4>Method <code>ratio_intersect()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>text_intersect$ratio_intersect(distinct = FALSE, letters = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>distinct</code></dt><dd><p>either TRUE or FALSE. If TRUE then the intersection of <em>distinct</em> words (or letters) will be taken into account</p>
</dd>
<dt><code>letters</code></dt><dd><p>either TRUE or FALSE. If TRUE then the intersection of letters in the text sequences will be computed</p>
</dd>
</dl>

</div>


<hr>
<a id="method-text_intersect-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>text_intersect$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>References</h3>

<p>https://www.kaggle.com/c/home-depot-product-search-relevance/discussion/20427 by Igor Buinyi
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(textTinyR)

tok1 = list(c('compare', 'this', 'text'),

            c('and', 'this', 'text'))

tok2 = list(c('with', 'another', 'set'),

            c('of', 'text', 'documents'))


init = text_intersect$new(tok1, tok2)


init$count_intersect(distinct = TRUE, letters = FALSE)


init$ratio_intersect(distinct = FALSE, letters = TRUE)
</code></pre>

<hr>
<h2 id='token_stats'>token statistics</h2><span id='topic+token_stats'></span>

<h3>Description</h3>

<p>token statistics
</p>
<p>token statistics
</p>


<h3>Usage</h3>

<pre><code class='language-R'># utl &lt;- token_stats$new(x_vec = NULL, path_2folder = NULL, path_2file = NULL,

#                               file_delimiter = ' ', n_gram_delimiter = "_")
</code></pre>


<h3>Details</h3>

<p>the <em>path_2vector</em> function returns the words of a <em>folder</em> or <em>file</em> to a vector ( using the <em>file_delimiter</em> to input the data ). Usage: read a vocabulary from a text file
</p>
<p>the <em>freq_distribution</em> function returns a named-unsorted vector frequency_distribution in R for EITHER a <em>folder</em>, a <em>file</em> OR a character string <em>vector</em>. A specific subset of the result can be retrieved using the <em>print_frequency</em> function
</p>
<p>the <em>count_character</em> function returns the number of characters for each word of the corpus for EITHER a <em>folder</em>, a <em>file</em> OR a character string <em>vector</em>. A specific number of character words can be retrieved using the <em>print_count_character</em> function
</p>
<p>the <em>collocation_words</em> function returns a co-occurence frequency table for n-grams for EITHER a <em>folder</em>, a <em>file</em> OR a character string <em>vector</em>. A collocation is defined as a sequence of two or more consecutive words, that has characteristics of a syntactic and semantic unit, and whose exact and unambiguous meaning or connotation cannot be derived directly from the meaning or connotation of its components ( <a href="http://nlp.stanford.edu/fsnlp/promo/colloc.pdf">http://nlp.stanford.edu/fsnlp/promo/colloc.pdf</a>, page 172 ). The input to the function should be text n-grams separated by a delimiter (for instance 3- or 4-ngrams ). I can retrieve a specific frequency table by using the <em>print_collocations</em> function
</p>
<p>the <em>string_dissimilarity_matrix</em> function returns a string-dissimilarity-matrix using either the <em>dice</em>, <em>levenshtein</em> or <em>cosine</em> distance. The input can be a character string <em>vector</em> only. In case that the method is <em>dice</em> then the dice-coefficient (similarity) is calculated between two strings for a specific number of character n-grams ( <em>dice_n_gram</em> ).
</p>
<p>the <em>look_up_table</em> returns a look-up-list where the list-names are the n-grams and the list-vectors are the words associated with those n-grams. The words for each n-gram can be retrieved using the <em>print_words_lookup_tbl</em> function. The input can be a character string <em>vector</em> only.
</p>


<h3>Methods</h3>


<dl>
<dt><code>token_stats$new(x_vec = NULL, path_2folder = NULL, path_2file = NULL, file_delimiter = ' ', n_gram_delimiter = "_")</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>path_2vector()</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>freq_distribution()</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>print_frequency(subset = NULL)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>count_character()</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>print_count_character(number = NULL)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>collocation_words()</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>print_collocations(word = NULL)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>string_dissimilarity_matrix(dice_n_gram = 2, method = "dice", split_separator = " ", dice_thresh = 1.0, upper = TRUE, diagonal = TRUE, threads = 1)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>look_up_table(n_grams = NULL)</code></dt><dd></dd>
<dt><code>--------------</code></dt><dd></dd>
<dt><code>print_words_lookup_tbl(n_gram = NULL)</code></dt><dd></dd>
</dl>



<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-token_stats-new"><code>token_stats$new()</code></a>
</p>
</li>
<li> <p><a href="#method-token_stats-path_2vector"><code>token_stats$path_2vector()</code></a>
</p>
</li>
<li> <p><a href="#method-token_stats-freq_distribution"><code>token_stats$freq_distribution()</code></a>
</p>
</li>
<li> <p><a href="#method-token_stats-print_frequency"><code>token_stats$print_frequency()</code></a>
</p>
</li>
<li> <p><a href="#method-token_stats-count_character"><code>token_stats$count_character()</code></a>
</p>
</li>
<li> <p><a href="#method-token_stats-print_count_character"><code>token_stats$print_count_character()</code></a>
</p>
</li>
<li> <p><a href="#method-token_stats-collocation_words"><code>token_stats$collocation_words()</code></a>
</p>
</li>
<li> <p><a href="#method-token_stats-print_collocations"><code>token_stats$print_collocations()</code></a>
</p>
</li>
<li> <p><a href="#method-token_stats-string_dissimilarity_matrix"><code>token_stats$string_dissimilarity_matrix()</code></a>
</p>
</li>
<li> <p><a href="#method-token_stats-look_up_table"><code>token_stats$look_up_table()</code></a>
</p>
</li>
<li> <p><a href="#method-token_stats-print_words_lookup_tbl"><code>token_stats$print_words_lookup_tbl()</code></a>
</p>
</li>
<li> <p><a href="#method-token_stats-clone"><code>token_stats$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-token_stats-new"></a>



<h4>Method <code>new()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>token_stats$new(
  x_vec = NULL,
  path_2folder = NULL,
  path_2file = NULL,
  file_delimiter = "\n",
  n_gram_delimiter = "_"
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>x_vec</code></dt><dd><p>either NULL or a string character vector</p>
</dd>
<dt><code>path_2folder</code></dt><dd><p>either NULL or a valid path to a folder (each file in the folder should include words separated by a delimiter)</p>
</dd>
<dt><code>path_2file</code></dt><dd><p>either NULL or a valid path to a file</p>
</dd>
<dt><code>file_delimiter</code></dt><dd><p>either NULL or a character string specifying the file delimiter</p>
</dd>
<dt><code>n_gram_delimiter</code></dt><dd><p>either NULL or a character string specifying the n-gram delimiter. It is used in the <em>collocation_words</em> function</p>
</dd>
</dl>

</div>


<hr>
<a id="method-token_stats-path_2vector"></a>



<h4>Method <code>path_2vector()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>token_stats$path_2vector()</pre></div>


<hr>
<a id="method-token_stats-freq_distribution"></a>



<h4>Method <code>freq_distribution()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>token_stats$freq_distribution()</pre></div>


<hr>
<a id="method-token_stats-print_frequency"></a>



<h4>Method <code>print_frequency()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>token_stats$print_frequency(subset = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>subset</code></dt><dd><p>either NULL or a vector specifying the subset of data to keep (number of rows of the <em>print_frequency</em> function)</p>
</dd>
</dl>

</div>


<hr>
<a id="method-token_stats-count_character"></a>



<h4>Method <code>count_character()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>token_stats$count_character()</pre></div>


<hr>
<a id="method-token_stats-print_count_character"></a>



<h4>Method <code>print_count_character()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>token_stats$print_count_character(number = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>number</code></dt><dd><p>a numeric value for the <em>print_count_character</em> function. All words with number of characters equal to the <em>number</em> parameter will be returned.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-token_stats-collocation_words"></a>



<h4>Method <code>collocation_words()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>token_stats$collocation_words()</pre></div>


<hr>
<a id="method-token_stats-print_collocations"></a>



<h4>Method <code>print_collocations()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>token_stats$print_collocations(word = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>word</code></dt><dd><p>a character string for the <em>print_collocations</em> and <em>print_prob_next</em> functions</p>
</dd>
</dl>

</div>


<hr>
<a id="method-token_stats-string_dissimilarity_matrix"></a>



<h4>Method <code>string_dissimilarity_matrix()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>token_stats$string_dissimilarity_matrix(
  dice_n_gram = 2,
  method = "dice",
  split_separator = " ",
  dice_thresh = 1,
  upper = TRUE,
  diagonal = TRUE,
  threads = 1
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>dice_n_gram</code></dt><dd><p>a numeric value specifying the n-gram for the dice method of the <em>string_dissimilarity_matrix</em> function</p>
</dd>
<dt><code>method</code></dt><dd><p>a character string specifying the method to use in the <em>string_dissimilarity_matrix</em> function. One of <em>dice</em>, <em>levenshtein</em> or <em>cosine</em>.</p>
</dd>
<dt><code>split_separator</code></dt><dd><p>a character string specifying the string split separator if method equal <em>cosine</em> in the <em>string_dissimilarity_matrix</em> function. The <em>cosine</em> method uses sentences, so for a sentence : &quot;this_is_a_word_sentence&quot; the <em>split_separator</em> should be &quot;_&quot;</p>
</dd>
<dt><code>dice_thresh</code></dt><dd><p>a float number to use to threshold the data if method is <em>dice</em> in the <em>string_dissimilarity_matrix</em> function. It takes values between 0.0 and 1.0. The closer the thresh is to 0.0 the more values of the dissimilarity matrix will take the value of 1.0.</p>
</dd>
<dt><code>upper</code></dt><dd><p>either TRUE or FALSE. If TRUE then both lower and upper parts of the dissimilarity matrix of the <em>string_dissimilarity_matrix</em> function will be shown. Otherwise the upper part will be filled with NA's</p>
</dd>
<dt><code>diagonal</code></dt><dd><p>either TRUE or FALSE. If TRUE then the diagonal of the dissimilarity matrix of the <em>string_dissimilarity_matrix</em> function will be shown. Otherwise the diagonal will be filled with NA's</p>
</dd>
<dt><code>threads</code></dt><dd><p>a numeric value specifying the number of cores to use in parallel in the <em>string_dissimilarity_matrix</em> function</p>
</dd>
</dl>

</div>


<hr>
<a id="method-token_stats-look_up_table"></a>



<h4>Method <code>look_up_table()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>token_stats$look_up_table(n_grams = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>n_grams</code></dt><dd><p>a numeric value specifying the n-grams in the <em>look_up_table</em> function</p>
</dd>
</dl>

</div>


<hr>
<a id="method-token_stats-print_words_lookup_tbl"></a>



<h4>Method <code>print_words_lookup_tbl()</code></h4>



<h5>Usage</h5>

<div class="r"><pre>token_stats$print_words_lookup_tbl(n_gram = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>n_gram</code></dt><dd><p>a character string specifying the n-gram to use in the <em>print_words_lookup_tbl</em> function</p>
</dd>
</dl>

</div>


<hr>
<a id="method-token_stats-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>token_stats$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Examples</h3>

<pre><code class='language-R'>

library(textTinyR)

expl = c('one_word_token', 'two_words_token', 'three_words_token', 'four_words_token')

tk &lt;- token_stats$new(x_vec = expl, path_2folder = NULL, path_2file = NULL)

#-------------------------
# frequency distribution:
#-------------------------

tk$freq_distribution()

# tk$print_frequency()


#------------------
# count characters:
#------------------

cnt &lt;- tk$count_character()

# tk$print_count_character(number = 4)


#----------------------
# collocation of words:
#----------------------

col &lt;- tk$collocation_words()

# tk$print_collocations(word = 'five')


#-----------------------------
# string dissimilarity matrix:
#-----------------------------

dism &lt;- tk$string_dissimilarity_matrix(method = 'levenshtein')


#---------------------
# build a look-up-table:
#---------------------

lut &lt;- tk$look_up_table(n_grams = 3)

# tk$print_words_lookup_tbl(n_gram = 'e_w')
</code></pre>

<hr>
<h2 id='tokenize_transform_text'>String tokenization and transformation  ( character string or path to a file )</h2><span id='topic+tokenize_transform_text'></span>

<h3>Description</h3>

<p>String tokenization and transformation  ( character string or path to a file )
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokenize_transform_text(
  object = NULL,
  batches = NULL,
  read_file_delimiter = "\n",
  to_lower = FALSE,
  to_upper = FALSE,
  utf_locale = "",
  remove_char = "",
  remove_punctuation_string = FALSE,
  remove_punctuation_vector = FALSE,
  remove_numbers = FALSE,
  trim_token = FALSE,
  split_string = FALSE,
  split_separator = " \r\n\t.,;:()?!//",
  remove_stopwords = FALSE,
  language = "english",
  min_num_char = 1,
  max_num_char = Inf,
  stemmer = NULL,
  min_n_gram = 1,
  max_n_gram = 1,
  skip_n_gram = 1,
  skip_distance = 0,
  n_gram_delimiter = " ",
  concat_delimiter = NULL,
  path_2folder = "",
  stemmer_ngram = 4,
  stemmer_gamma = 0,
  stemmer_truncate = 3,
  stemmer_batches = 1,
  threads = 1,
  vocabulary_path_file = NULL,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokenize_transform_text_+3A_object">object</code></td>
<td>
<p>either a character string (text data) or a character-string-path to a file (for big .txt files it's recommended to use a path to a file).</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_batches">batches</code></td>
<td>
<p>a numeric value. If the <em>batches</em> parameter is not NULL then the <em>object</em> parameter should be a valid path to a file and the <em>path_2folder</em> parameter should be a valid path to a folder. The batches parameter should be used in case of small to medium data sets (for zero memory consumption). For big data sets the <em>big_tokenize_transform</em> R6 class and especially the <em>big_text_tokenizer</em> function should be used.</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_read_file_delimiter">read_file_delimiter</code></td>
<td>
<p>the delimiter to use when the input file will be red (for instance a tab-delimiter or a new-line delimiter).</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_to_lower">to_lower</code></td>
<td>
<p>either TRUE or FALSE. If TRUE the character string will be converted to lower case</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_to_upper">to_upper</code></td>
<td>
<p>either TRUE or FALSE. If TRUE the character string will be converted to upper case</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_utf_locale">utf_locale</code></td>
<td>
<p>the language specific locale to use in case that either the <em>to_lower</em> or the <em>to_upper</em> parameter is TRUE and the text file language is other than english. For instance if the language of a text file is greek then the <em>utf_locale</em> parameter should be <em>'el_GR.UTF-8'</em> ( <em>language_country.encoding</em> ). A wrong utf-locale does not raise an error, however the runtime of the function increases.</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_remove_char">remove_char</code></td>
<td>
<p>a character string with specific characters that should be removed from the text file. If the <em>remove_char</em> is &quot;&quot; then no removal of characters take place</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_remove_punctuation_string">remove_punctuation_string</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then the punctuation of the character string will be removed (applies before the split function)</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_remove_punctuation_vector">remove_punctuation_vector</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then the punctuation of the vector of the character strings will be removed  (after the string split has taken place)</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_remove_numbers">remove_numbers</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then any numbers in the character string will be removed</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_trim_token">trim_token</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then the string will be trimmed (left and/or right)</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_split_string">split_string</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then the character string will be split using the <em>split_separator</em> as delimiter. The user can also specify multiple delimiters.</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_split_separator">split_separator</code></td>
<td>
<p>a character string specifying the character delimiter(s)</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_remove_stopwords">remove_stopwords</code></td>
<td>
<p>either TRUE, FALSE or a character vector of user defined stop words. If TRUE then by using the <em>language</em> parameter the corresponding stop words vector will be uploaded.</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_language">language</code></td>
<td>
<p>a character string which defaults to english. If the <em>remove_stopwords</em> parameter is TRUE then the corresponding stop words vector will be uploaded. Available languages
are <em>afrikaans</em>, <em>arabic</em>, <em>armenian</em>, <em>basque</em>, <em>bengali</em>, <em>breton</em>, <em>bulgarian</em>, <em>catalan</em>,
<em>croatian</em>, <em>czech</em>, <em>danish</em>, <em>dutch</em>, <em>english</em>, <em>estonian</em>,
<em>finnish</em>, <em>french</em>, <em>galician</em>, <em>german</em>, <em>greek</em>, <em>hausa</em>, <em>hebrew</em>, <em>hindi</em>, <em>hungarian</em>,
<em>indonesian</em>, <em>irish</em>, <em>italian</em>, <em>latvian</em>, <em>marathi</em>,
<em>norwegian</em>, <em>persian</em>, <em>polish</em>, <em>portuguese</em>, <em>romanian</em>, <em>russian</em>, <em>slovak</em>, <em>slovenian</em>,
<em>somalia</em>, <em>spanish</em>, <em>swahili</em>, <em>swedish</em>, <em>turkish</em>, <em>yoruba</em>, <em>zulu</em></p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_min_num_char">min_num_char</code></td>
<td>
<p>an integer specifying the minimum number of characters to keep. If the <em>min_num_char</em> is greater than 1 then character strings with more than 1 characters will be returned</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_max_num_char">max_num_char</code></td>
<td>
<p>an integer specifying the maximum number of characters to keep. The <em>max_num_char</em> should be less than or equal to <em>Inf</em> (in this function the Inf value translates to a word-length of 1000000000)</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_stemmer">stemmer</code></td>
<td>
<p>a character string specifying the stemming method. One of the following <em>porter2_stemmer</em>, <em>ngram_sequential</em>, <em>ngram_overlap</em>. See details for more information.</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_min_n_gram">min_n_gram</code></td>
<td>
<p>an integer specifying the minimum number of n-grams. The minimum number of min_n_gram is 1.</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_max_n_gram">max_n_gram</code></td>
<td>
<p>an integer specifying the maximum number of n-grams. The minimum number of max_n_gram is 1.</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_skip_n_gram">skip_n_gram</code></td>
<td>
<p>an integer specifying the number of skip-n-grams. The minimum number of skip_n_gram is 1. The skip_n_gram gives the (max.) n-grams using the <em>skip_distance</em> parameter. If <em>skip_n_gram</em> is greater than 1 then both <em>min_n_gram</em> and <em>max_n_gram</em> should be set to 1.</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_skip_distance">skip_distance</code></td>
<td>
<p>an integer specifying the skip distance between the words. The minimum value for the skip distance is 0, in which case simple n-grams will be returned.</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_n_gram_delimiter">n_gram_delimiter</code></td>
<td>
<p>a character string specifying the n-gram delimiter (applies to both n-gram and skip-n-gram cases)</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_concat_delimiter">concat_delimiter</code></td>
<td>
<p>either NULL or a character string specifying the delimiter to use in order to concatenate the end-vector of character strings to a single character string (recommended in case that the end-vector should be saved to a file)</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_path_2folder">path_2folder</code></td>
<td>
<p>a character string specifying the path to the folder where the file(s) will be saved</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_stemmer_ngram">stemmer_ngram</code></td>
<td>
<p>a numeric value greater than 1. Applies to both <em>ngram_sequential</em> and <em>ngram_overlap</em> methods. In case of <em>ngram_sequential</em> the first <em>n</em> characters will be picked, whereas in the case of <em>ngram_overlap</em> the overlapping stemmer_ngram characters will be build.</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_stemmer_gamma">stemmer_gamma</code></td>
<td>
<p>a float number greater or equal to 0.0. Applies only to <em>ngram_sequential</em>. Is a threshold value, which defines how much frequency deviation of two N-grams is acceptable. It is kept either zero or to a minimum value.</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_stemmer_truncate">stemmer_truncate</code></td>
<td>
<p>a numeric value greater than 0. Applies only to <em>ngram_sequential</em>. The ngram_sequential is modified to use relative frequencies (float numbers between 0.0 and 1.0 for the ngrams of a specific word in the corpus) and the stemmer_truncate parameter controls the number of rounding digits for the ngrams of the word. The main purpose was to give the same relative frequency to words appearing approximately the same on the corpus.</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_stemmer_batches">stemmer_batches</code></td>
<td>
<p>a numeric value greater than 0. Applies only to <em>ngram_sequential</em>. Splits the corpus into batches with the option to run the batches in multiple threads.</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_threads">threads</code></td>
<td>
<p>an integer specifying the number of cores to run in parallel</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_vocabulary_path_file">vocabulary_path_file</code></td>
<td>
<p>either NULL or a character string specifying the output path to a file where the vocabulary should be saved once the text is tokenized</p>
</td></tr>
<tr><td><code id="tokenize_transform_text_+3A_verbose">verbose</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then information will be printed out</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is memory efficient to read the data using a <em>path file</em> in case of a big file, rather than importing the data in the R-session and then calling the <em>tokenize_transform_text</em> function.
</p>
<p>It is memory efficient to give a <em>path_2folder</em> in case that a big file should be saved, rather than return the vector of all character strings in the R-session.
</p>
<p>The <em>skip-grams</em> are a generalization of n-grams in which the components (typically words) need not to be consecutive in the text under consideration, but may leave gaps that are skipped over. They provide one way of overcoming the <em>data sparsity problem</em> found with conventional n-gram analysis.
</p>
<p>Many character string pre-processing functions (such as the <em>utf-locale</em> or the <em>split-string</em> function ) are based on the <em>boost</em> library ( <a href="https://www.boost.org/">https://www.boost.org/</a> ).
</p>
<p>Stemming of the english language is done using the porter2-stemmer, for details see <a href="https://github.com/smassung/porter2_stemmer">https://github.com/smassung/porter2_stemmer</a>
</p>
<p>N-gram stemming is language independent and supported by the following two functions:
</p>

<dl>
<dt>ngram_overlap</dt><dd><p>The <em>ngram_overlap</em> stemming method is based on <em>N-Gram Morphemes for Retrieval, Paul McNamee and James Mayfield</em>, <a href="http://clef.isti.cnr.it/2007/working_notes/mcnameeCLEF2007.pdf">http://clef.isti.cnr.it/2007/working_notes/mcnameeCLEF2007.pdf</a></p>
</dd>
<dt>ngram_sequential</dt><dd><p>The <em>ngram_sequential</em> stemming method is a modified version based on <em>Generation, Implementation and Appraisal of an N-gram based Stemming Algorithm, B. P. Pande, Pawan Tamta, H. S. Dhami</em>, <a href="https://arxiv.org/pdf/1312.4824.pdf">https://arxiv.org/pdf/1312.4824.pdf</a></p>
</dd>
</dl>

<p>The list of stop-words in the available languages was downloaded from the following link, <a href="https://github.com/6/stopwords-json">https://github.com/6/stopwords-json</a>
</p>


<h3>Value</h3>

<p>a character vector
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(textTinyR)

token_str = "CONVERT to lower, remove.. punctuation11234, trim token and split "

res = tokenize_transform_text(object = token_str, to_lower = TRUE, split_string = TRUE)
</code></pre>

<hr>
<h2 id='tokenize_transform_vec_docs'>String tokenization and transformation ( vector of documents )</h2><span id='topic+tokenize_transform_vec_docs'></span>

<h3>Description</h3>

<p>String tokenization and transformation ( vector of documents )
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokenize_transform_vec_docs(
  object = NULL,
  as_token = FALSE,
  to_lower = FALSE,
  to_upper = FALSE,
  utf_locale = "",
  remove_char = "",
  remove_punctuation_string = FALSE,
  remove_punctuation_vector = FALSE,
  remove_numbers = FALSE,
  trim_token = FALSE,
  split_string = FALSE,
  split_separator = " \r\n\t.,;:()?!//",
  remove_stopwords = FALSE,
  language = "english",
  min_num_char = 1,
  max_num_char = Inf,
  stemmer = NULL,
  min_n_gram = 1,
  max_n_gram = 1,
  skip_n_gram = 1,
  skip_distance = 0,
  n_gram_delimiter = " ",
  concat_delimiter = NULL,
  path_2folder = "",
  threads = 1,
  vocabulary_path_file = NULL,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokenize_transform_vec_docs_+3A_object">object</code></td>
<td>
<p>a character string vector of documents</p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_as_token">as_token</code></td>
<td>
<p>if TRUE then the output of the function is a list of (split) token. Otherwise is a vector of character strings (sentences)</p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_to_lower">to_lower</code></td>
<td>
<p>either TRUE or FALSE. If TRUE the character string will be converted to lower case</p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_to_upper">to_upper</code></td>
<td>
<p>either TRUE or FALSE. If TRUE the character string will be converted to upper case</p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_utf_locale">utf_locale</code></td>
<td>
<p>the language specific locale to use in case that either the <em>to_lower</em> or the <em>to_upper</em> parameter is TRUE and the text file language is other than english. For instance if the language of a text file is greek then the <em>utf_locale</em> parameter should be <em>'el_GR.UTF-8'</em> ( <em>language_country.encoding</em> ). A wrong utf-locale does not raise an error, however the runtime of the function increases.</p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_remove_char">remove_char</code></td>
<td>
<p>a character string with specific characters that should be removed from the text file. If the <em>remove_char</em> is &quot;&quot; then no removal of characters take place</p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_remove_punctuation_string">remove_punctuation_string</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then the punctuation of the character string will be removed (applies before the split function)</p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_remove_punctuation_vector">remove_punctuation_vector</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then the punctuation of the vector of the character strings will be removed  (after the string split has taken place)</p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_remove_numbers">remove_numbers</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then any numbers in the character string will be removed</p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_trim_token">trim_token</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then the string will be trimmed (left and/or right)</p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_split_string">split_string</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then the character string will be split using the <em>split_separator</em> as delimiter. The user can also specify multiple delimiters.</p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_split_separator">split_separator</code></td>
<td>
<p>a character string specifying the character delimiter(s)</p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_remove_stopwords">remove_stopwords</code></td>
<td>
<p>either TRUE, FALSE or a character vector of user defined stop words. If TRUE then by using the <em>language</em> parameter the corresponding stop words vector will be uploaded.</p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_language">language</code></td>
<td>
<p>a character string which defaults to english. If the <em>remove_stopwords</em> parameter is TRUE then the corresponding stop words vector will be uploaded. Available languages
are <em>afrikaans</em>, <em>arabic</em>, <em>armenian</em>, <em>basque</em>, <em>bengali</em>, <em>breton</em>, <em>bulgarian</em>, <em>catalan</em>,
<em>croatian</em>, <em>czech</em>, <em>danish</em>, <em>dutch</em>, <em>english</em>, <em>estonian</em>,
<em>finnish</em>, <em>french</em>, <em>galician</em>, <em>german</em>, <em>greek</em>, <em>hausa</em>, <em>hebrew</em>, <em>hindi</em>, <em>hungarian</em>,
<em>indonesian</em>, <em>irish</em>, <em>italian</em>, <em>latvian</em>, <em>marathi</em>,
<em>norwegian</em>, <em>persian</em>, <em>polish</em>, <em>portuguese</em>, <em>romanian</em>, <em>russian</em>, <em>slovak</em>, <em>slovenian</em>,
<em>somalia</em>, <em>spanish</em>, <em>swahili</em>, <em>swedish</em>, <em>turkish</em>, <em>yoruba</em>, <em>zulu</em></p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_min_num_char">min_num_char</code></td>
<td>
<p>an integer specifying the minimum number of characters to keep. If the <em>min_num_char</em> is greater than 1 then character strings with more than 1 characters will be returned</p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_max_num_char">max_num_char</code></td>
<td>
<p>an integer specifying the maximum number of characters to keep. The <em>max_num_char</em> should be less than or equal to <em>Inf</em> (in this function the Inf value translates to a word-length of 1000000000)</p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_stemmer">stemmer</code></td>
<td>
<p>a character string specifying the stemming method. Available method is the <em>porter2_stemmer</em>. See details for more information.</p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_min_n_gram">min_n_gram</code></td>
<td>
<p>an integer specifying the minimum number of n-grams. The minimum number of min_n_gram is 1.</p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_max_n_gram">max_n_gram</code></td>
<td>
<p>an integer specifying the maximum number of n-grams. The minimum number of max_n_gram is 1.</p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_skip_n_gram">skip_n_gram</code></td>
<td>
<p>an integer specifying the number of skip-n-grams. The minimum number of skip_n_gram is 1. The skip_n_gram gives the (max.) n-grams using the <em>skip_distance</em> parameter. If <em>skip_n_gram</em> is greater than 1 then both <em>min_n_gram</em> and <em>max_n_gram</em> should be set to 1.</p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_skip_distance">skip_distance</code></td>
<td>
<p>an integer specifying the skip distance between the words. The minimum value for the skip distance is 0, in which case simple n-grams will be returned.</p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_n_gram_delimiter">n_gram_delimiter</code></td>
<td>
<p>a character string specifying the n-gram delimiter (applies to both n-gram and skip-n-gram cases)</p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_concat_delimiter">concat_delimiter</code></td>
<td>
<p>either NULL or a character string specifying the delimiter to use in order to concatenate the end-vector of character strings to a single character string (recommended in case that the end-vector should be saved to a file)</p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_path_2folder">path_2folder</code></td>
<td>
<p>a character string specifying the path to the folder where the file(s) will be saved</p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_threads">threads</code></td>
<td>
<p>an integer specifying the number of cores to run in parallel</p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_vocabulary_path_file">vocabulary_path_file</code></td>
<td>
<p>either NULL or a character string specifying the output path to a file where the vocabulary should be saved once the text is tokenized</p>
</td></tr>
<tr><td><code id="tokenize_transform_vec_docs_+3A_verbose">verbose</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then information will be printed out</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is memory efficient to give a <em>path_2folder</em> in case that a big file should be saved, rather than return the vector of all character strings in the R-session.
</p>
<p>The <em>skip-grams</em> are a generalization of n-grams in which the components (typically words) need not to be consecutive in the text under consideration, but may leave gaps that are skipped over. They provide one way of overcoming the <em>data sparsity problem</em> found with conventional n-gram analysis.
</p>
<p>Many character string pre-processing functions (such as the <em>utf-locale</em> or the <em>split-string</em> function ) are based on the <em>boost</em> library ( <a href="https://www.boost.org/">https://www.boost.org/</a> ).
</p>
<p>Stemming of the english language is done using the porter2-stemmer, for details see <a href="https://github.com/smassung/porter2_stemmer">https://github.com/smassung/porter2_stemmer</a>
</p>
<p>The list of stop-words in the available languages was downloaded from the following link, <a href="https://github.com/6/stopwords-json">https://github.com/6/stopwords-json</a>
</p>


<h3>Value</h3>

<p>a character vector
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(textTinyR)

token_doc_vec = c("CONVERT to lower", "remove.. punctuation11234", "trim token and split ")

res = tokenize_transform_vec_docs(object = token_doc_vec, to_lower = TRUE, split_string = TRUE)
</code></pre>

<hr>
<h2 id='utf_locale'>utf-locale for the available languages</h2><span id='topic+utf_locale'></span>

<h3>Description</h3>

<p>utf-locale for the available languages
</p>


<h3>Usage</h3>

<pre><code class='language-R'>utf_locale(language = "english")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="utf_locale_+3A_language">language</code></td>
<td>
<p>a character string specifying the language for which the utf-locale should be returned</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a limited list of language-locale. The locale depends mostly on the text input.
</p>


<h3>Value</h3>

<p>a utf locale
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(textTinyR)

utf_locale(language = "english")
</code></pre>

<hr>
<h2 id='vocabulary_parser'>returns the vocabulary counts for small or medium ( xml and not only ) files</h2><span id='topic+vocabulary_parser'></span>

<h3>Description</h3>

<p>returns the vocabulary counts for small or medium ( xml and not only ) files
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vocabulary_parser(
  input_path_file = NULL,
  start_query = NULL,
  end_query = NULL,
  vocabulary_path_file = NULL,
  min_lines = 1,
  trimmed_line = FALSE,
  to_lower = FALSE,
  to_upper = FALSE,
  utf_locale = "",
  max_num_char = Inf,
  remove_char = "",
  remove_punctuation_string = FALSE,
  remove_punctuation_vector = FALSE,
  remove_numbers = FALSE,
  trim_token = FALSE,
  split_string = FALSE,
  split_separator = " \r\n\t.,;:()?!//",
  remove_stopwords = FALSE,
  language = "english",
  min_num_char = 1,
  stemmer = NULL,
  min_n_gram = 1,
  max_n_gram = 1,
  skip_n_gram = 1,
  skip_distance = 0,
  n_gram_delimiter = " ",
  threads = 1,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vocabulary_parser_+3A_input_path_file">input_path_file</code></td>
<td>
<p>a character string specifying a valid path to the input file</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_start_query">start_query</code></td>
<td>
<p>a character string. The <em>start_query</em> is the first word of the subset of the data and should appear frequently at the beginning of each line in the text file.</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_end_query">end_query</code></td>
<td>
<p>a character string. The <em>end_query</em> is the last word of the subset of the data and should appear frequently at the end of each line in the text file.</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_vocabulary_path_file">vocabulary_path_file</code></td>
<td>
<p>a character string specifying the output file where the vocabulary should be saved (after tokenization and transformation is applied).</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_min_lines">min_lines</code></td>
<td>
<p>a numeric value specifying the minimum number of lines. For instance if min_lines = 2, then only subsets of text with more than 1 lines will be kept.</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_trimmed_line">trimmed_line</code></td>
<td>
<p>either TRUE or FALSE. If FALSE then each line of the text file will be trimmed both sides before applying the start_query and end_query</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_to_lower">to_lower</code></td>
<td>
<p>either TRUE or FALSE. If TRUE the character string will be converted to lower case</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_to_upper">to_upper</code></td>
<td>
<p>either TRUE or FALSE. If TRUE the character string will be converted to upper case</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_utf_locale">utf_locale</code></td>
<td>
<p>the language specific locale to use in case that either the <em>to_lower</em> or the <em>to_upper</em> parameter is TRUE and the text file language is other than english. For instance if the language of a text file is greek then the <em>utf_locale</em> parameter should be <em>'el_GR.UTF-8'</em> ( <em>language_country.encoding</em> ). A wrong utf-locale does not raise an error, however the runtime of the function increases.</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_max_num_char">max_num_char</code></td>
<td>
<p>an integer specifying the maximum number of characters to keep. The <em>max_num_char</em> should be less than or equal to <em>Inf</em> (in this function the Inf value translates to a word-length of 1000000000)</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_remove_char">remove_char</code></td>
<td>
<p>a character string with specific characters that should be removed from the text file. If the <em>remove_char</em> is &quot;&quot; then no removal of characters take place</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_remove_punctuation_string">remove_punctuation_string</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then the punctuation of the character string will be removed (applies before the split function)</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_remove_punctuation_vector">remove_punctuation_vector</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then the punctuation of the vector of the character strings will be removed  (after the string split has taken place)</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_remove_numbers">remove_numbers</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then any numbers in the character string will be removed</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_trim_token">trim_token</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then the string will be trimmed (left and/or right)</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_split_string">split_string</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then the character string will be split using the <em>split_separator</em> as delimiter. The user can also specify multiple delimiters.</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_split_separator">split_separator</code></td>
<td>
<p>a character string specifying the character delimiter(s)</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_remove_stopwords">remove_stopwords</code></td>
<td>
<p>either TRUE, FALSE or a character vector of user defined stop words. If TRUE then by using the <em>language</em> parameter the corresponding stop words vector will be uploaded.</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_language">language</code></td>
<td>
<p>a character string which defaults to english. If the <em>remove_stopwords</em> parameter is TRUE then the corresponding stop words vector will be uploaded. Available languages
are <em>afrikaans</em>, <em>arabic</em>, <em>armenian</em>, <em>basque</em>, <em>bengali</em>, <em>breton</em>, <em>bulgarian</em>, <em>catalan</em>,
<em>croatian</em>, <em>czech</em>, <em>danish</em>, <em>dutch</em>, <em>english</em>, <em>estonian</em>,
<em>finnish</em>, <em>french</em>, <em>galician</em>, <em>german</em>, <em>greek</em>, <em>hausa</em>, <em>hebrew</em>, <em>hindi</em>, <em>hungarian</em>,
<em>indonesian</em>, <em>irish</em>, <em>italian</em>, <em>latvian</em>, <em>marathi</em>,
<em>norwegian</em>, <em>persian</em>, <em>polish</em>, <em>portuguese</em>, <em>romanian</em>, <em>russian</em>, <em>slovak</em>, <em>slovenian</em>,
<em>somalia</em>, <em>spanish</em>, <em>swahili</em>, <em>swedish</em>, <em>turkish</em>, <em>yoruba</em>, <em>zulu</em></p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_min_num_char">min_num_char</code></td>
<td>
<p>an integer specifying the minimum number of characters to keep. If the <em>min_num_char</em> is greater than 1 then character strings with more than 1 characters will be returned</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_stemmer">stemmer</code></td>
<td>
<p>a character string specifying the stemming method. Available method is the <em>porter2_stemmer</em>. See details for more information.</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_min_n_gram">min_n_gram</code></td>
<td>
<p>an integer specifying the minimum number of n-grams. The minimum number of min_n_gram is 1.</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_max_n_gram">max_n_gram</code></td>
<td>
<p>an integer specifying the maximum number of n-grams. The minimum number of max_n_gram is 1.</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_skip_n_gram">skip_n_gram</code></td>
<td>
<p>an integer specifying the number of skip-n-grams. The minimum number of skip_n_gram is 1. The skip_n_gram gives the (max.) n-grams using the <em>skip_distance</em> parameter. If <em>skip_n_gram</em> is greater than 1 then both <em>min_n_gram</em> and <em>max_n_gram</em> should be set to 1.</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_skip_distance">skip_distance</code></td>
<td>
<p>an integer specifying the skip distance between the words. The minimum value for the skip distance is 0, in which case simple n-grams will be returned.</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_n_gram_delimiter">n_gram_delimiter</code></td>
<td>
<p>a character string specifying the n-gram delimiter (applies to both n-gram and skip-n-gram cases)</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_threads">threads</code></td>
<td>
<p>an integer specifying the number of cores to run in parallel</p>
</td></tr>
<tr><td><code id="vocabulary_parser_+3A_verbose">verbose</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then information will be printed in the console</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The text file should have a structure (such as an xml-structure), so that subsets can be extracted using the <em>start_query</em> and <em>end_query</em> parameters
</p>
<p>For big files the <em>vocabulary_accumulator</em> method of the <em>big_tokenize_transform</em> class is appropriate
</p>
<p>Stemming of the english language is done using the porter2-stemmer, for details see <a href="https://github.com/smassung/porter2_stemmer">https://github.com/smassung/porter2_stemmer</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

library(textTinyR)

 vps = vocabulary_parser(input_path_file = '/folder/input_data.txt',
                         start_query = 'start_word', end_query = 'end_word',
                         vocabulary_path_file = '/folder/vocab.txt',
                         to_lower = TRUE, split_string = TRUE)

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
