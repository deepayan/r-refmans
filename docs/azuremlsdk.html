<!DOCTYPE html><html lang="en"><head><title>Help for package azuremlsdk</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {azuremlsdk}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#aci_webservice_deployment_config'><p>Create a deployment config for deploying an ACI web service</p></a></li>
<li><a href='#aks_webservice_deployment_config'><p>Create a deployment config for deploying an AKS web service</p></a></li>
<li><a href='#attach_aks_compute'><p>Attach an existing AKS cluster to a workspace</p></a></li>
<li><a href='#azureml'><p>azureml module</p>
User can access functions/modules in azureml that are not exposed through the
exported R functions.</a></li>
<li><a href='#bandit_policy'><p>Define a Bandit policy for early termination of HyperDrive runs</p></a></li>
<li><a href='#bayesian_parameter_sampling'><p>Define Bayesian sampling over a hyperparameter search space</p></a></li>
<li><a href='#cancel_run'><p>Cancel a run</p></a></li>
<li><a href='#choice'><p>Specify a discrete set of options to sample from</p></a></li>
<li><a href='#complete_run'><p>Mark a run as completed.</p></a></li>
<li><a href='#container_registry'><p>Specify Azure Container Registry details</p></a></li>
<li><a href='#convert_to_dataset_with_csv_files'><p>Convert the current dataset into a FileDataset containing CSV files.</p></a></li>
<li><a href='#convert_to_dataset_with_parquet_files'><p>Convert the current dataset into a FileDataset containing Parquet files.</p></a></li>
<li><a href='#cran_package'><p>Specifies a CRAN package to install in environment</p></a></li>
<li><a href='#create_aks_compute'><p>Create an AksCompute cluster</p></a></li>
<li><a href='#create_aml_compute'><p>Create an AmlCompute cluster</p></a></li>
<li><a href='#create_child_run'><p>Create a child run</p></a></li>
<li><a href='#create_child_runs'><p>Create one or many child runs</p></a></li>
<li><a href='#create_file_dataset_from_files'><p>Create a FileDataset to represent file streams.</p></a></li>
<li><a href='#create_tabular_dataset_from_delimited_files'><p>Create an unregistered, in-memory Dataset from delimited files.</p></a></li>
<li><a href='#create_tabular_dataset_from_json_lines_files'><p>Create a TabularDataset to represent tabular data in JSON Lines files (http://jsonlines.org/).</p></a></li>
<li><a href='#create_tabular_dataset_from_parquet_files'><p>Create an unregistered, in-memory Dataset from parquet files.</p></a></li>
<li><a href='#create_tabular_dataset_from_sql_query'><p>Create a TabularDataset to represent tabular data in SQL databases.</p></a></li>
<li><a href='#create_workspace'><p>Create a new Azure Machine Learning workspace</p></a></li>
<li><a href='#data_path'><p>Represents a path to data in a datastore.</p></a></li>
<li><a href='#data_type_bool'><p>Configure conversion to bool.</p></a></li>
<li><a href='#data_type_datetime'><p>Configure conversion to datetime.</p></a></li>
<li><a href='#data_type_double'><p>Configure conversion to 53-bit double.</p></a></li>
<li><a href='#data_type_long'><p>Configure conversion to 64-bit integer.</p></a></li>
<li><a href='#data_type_string'><p>Configure conversion to string.</p></a></li>
<li><a href='#dataset_consumption_config'><p>Represent how to deliver the dataset to a compute target.</p></a></li>
<li><a href='#define_timestamp_columns_for_dataset'><p>Define timestamp columns for the dataset.</p></a></li>
<li><a href='#delete_compute'><p>Delete a cluster</p></a></li>
<li><a href='#delete_local_webservice'><p>Delete a local web service from the local machine</p></a></li>
<li><a href='#delete_model'><p>Delete a model from its associated workspace</p></a></li>
<li><a href='#delete_secrets'><p>Delete secrets from a keyvault</p></a></li>
<li><a href='#delete_webservice'><p>Delete a web service from a given workspace</p></a></li>
<li><a href='#delete_workspace'><p>Delete a workspace</p></a></li>
<li><a href='#deploy_model'><p>Deploy a web service from registered model(s)</p></a></li>
<li><a href='#detach_aks_compute'><p>Detach an AksCompute cluster from its associated workspace</p></a></li>
<li><a href='#download_file_from_run'><p>Download a file from a run</p></a></li>
<li><a href='#download_files_from_run'><p>Download files from a run</p></a></li>
<li><a href='#download_from_datastore'><p>Download data from a datastore to the local file system</p></a></li>
<li><a href='#download_from_file_dataset'><p>Download file streams defined by the dataset as local files.</p></a></li>
<li><a href='#download_model'><p>Download a model to the local file system</p></a></li>
<li><a href='#drop_columns_from_dataset'><p>Drop the specified columns from the dataset.</p></a></li>
<li><a href='#estimator'><p>Create an estimator</p></a></li>
<li><a href='#experiment'><p>Create an Azure Machine Learning experiment</p></a></li>
<li><a href='#filter_dataset_after_time'><p>Filter Tabular Dataset with time stamp columns after a specified start time.</p></a></li>
<li><a href='#filter_dataset_before_time'><p>Filter Tabular Dataset with time stamp columns before a specified end time.</p></a></li>
<li><a href='#filter_dataset_between_time'><p>Filter Tabular Dataset between a specified start and end time.</p></a></li>
<li><a href='#filter_dataset_from_recent_time'><p>Filter Tabular Dataset to contain only the specified duration (amount) of recent data.</p></a></li>
<li><a href='#generate_entry_script'><p>Generates the control script for the experiment.</p></a></li>
<li><a href='#generate_new_webservice_key'><p>Regenerate one of a web service's keys</p></a></li>
<li><a href='#get_aks_compute_credentials'><p>Get the credentials for an AksCompute cluster</p></a></li>
<li><a href='#get_best_run_by_primary_metric'><p>Return the best performing run amongst all completed runs</p></a></li>
<li><a href='#get_child_run_hyperparameters'><p>Get the hyperparameters for all child runs</p></a></li>
<li><a href='#get_child_run_metrics'><p>Get the metrics from all child runs</p></a></li>
<li><a href='#get_child_runs'><p>Get all children for the current run selected by specified filters</p></a></li>
<li><a href='#get_child_runs_sorted_by_primary_metric'><p>Get the child runs sorted in descending order by</p>
best primary metric</a></li>
<li><a href='#get_compute'><p>Get an existing compute cluster</p></a></li>
<li><a href='#get_current_run'><p>Get the context object for a run</p></a></li>
<li><a href='#get_dataset_by_id'><p>Get Dataset by ID.</p></a></li>
<li><a href='#get_dataset_by_name'><p>Get a registered Dataset from the workspace by its registration name.</p></a></li>
<li><a href='#get_datastore'><p>Get an existing datastore</p></a></li>
<li><a href='#get_default_datastore'><p>Get the default datastore for a workspace</p></a></li>
<li><a href='#get_default_keyvault'><p>Get the default keyvault for a workspace</p></a></li>
<li><a href='#get_environment'><p>Get an existing environment</p></a></li>
<li><a href='#get_file_dataset_paths'><p>Get a list of file paths for each file stream defined by the dataset.</p></a></li>
<li><a href='#get_input_dataset_from_run'><p>Return the named list for input datasets.</p></a></li>
<li><a href='#get_model'><p>Get a registered model</p></a></li>
<li><a href='#get_model_package_container_registry'><p>Get the Azure container registry that a packaged model uses</p></a></li>
<li><a href='#get_model_package_creation_logs'><p>Get the model package creation logs</p></a></li>
<li><a href='#get_run'><p>Get an experiment run</p></a></li>
<li><a href='#get_run_details'><p>Get the details of a run</p></a></li>
<li><a href='#get_run_details_with_logs'><p>Get the details of a run along with the log files' contents</p></a></li>
<li><a href='#get_run_file_names'><p>List the files that are stored in association with a run</p></a></li>
<li><a href='#get_run_metrics'><p>Get the metrics logged to a run</p></a></li>
<li><a href='#get_runs_in_experiment'><p>Return a generator of the runs for an experiment</p></a></li>
<li><a href='#get_secrets'><p>Get secrets from a keyvault</p></a></li>
<li><a href='#get_secrets_from_run'><p>Get secrets from the keyvault associated with a run's workspace</p></a></li>
<li><a href='#get_webservice'><p>Get a deployed web service</p></a></li>
<li><a href='#get_webservice_keys'><p>Retrieve auth keys for a web service</p></a></li>
<li><a href='#get_webservice_logs'><p>Retrieve the logs for a web service</p></a></li>
<li><a href='#get_webservice_token'><p>Retrieve the auth token for a web service</p></a></li>
<li><a href='#get_workspace'><p>Get an existing workspace</p></a></li>
<li><a href='#get_workspace_details'><p>Get the details of a workspace</p></a></li>
<li><a href='#github_package'><p>Specifies a Github package to install in environment</p></a></li>
<li><a href='#grid_parameter_sampling'><p>Define grid sampling over a hyperparameter search space</p></a></li>
<li><a href='#hyperdrive_config'><p>Create a configuration for a HyperDrive run</p></a></li>
<li><a href='#inference_config'><p>Create an inference configuration for model deployments</p></a></li>
<li><a href='#install_azureml'><p>Install azureml sdk package</p></a></li>
<li><a href='#interactive_login_authentication'><p>Manages authentication and acquires an authorization token in interactive login workflows.</p></a></li>
<li><a href='#invoke_webservice'><p>Call a web service with the provided input</p></a></li>
<li><a href='#keep_columns_from_dataset'><p>Keep the specified columns and drops all others from the dataset.</p></a></li>
<li><a href='#list_nodes_in_aml_compute'><p>Get the details (e.g IP address, port etc) of all the compute nodes in the</p>
compute target</a></li>
<li><a href='#list_secrets'><p>List the secrets in a keyvault</p></a></li>
<li><a href='#list_supported_vm_sizes'><p>List the supported VM sizes in a region</p></a></li>
<li><a href='#list_workspaces'><p>List all workspaces that the user has access to in a subscription ID</p></a></li>
<li><a href='#load_dataset_into_data_frame'><p>Load all records from the dataset into a dataframe.</p></a></li>
<li><a href='#load_workspace_from_config'><p>Load workspace configuration details from a config file</p></a></li>
<li><a href='#local_webservice_deployment_config'><p>Create a deployment config for deploying a local web service</p></a></li>
<li><a href='#log_accuracy_table_to_run'><p>Log an accuracy table metric to a run</p></a></li>
<li><a href='#log_confusion_matrix_to_run'><p>Log a confusion matrix metric to a run</p></a></li>
<li><a href='#log_image_to_run'><p>Log an image metric to a run</p></a></li>
<li><a href='#log_list_to_run'><p>Log a vector metric value to a run</p></a></li>
<li><a href='#log_metric_to_run'><p>Log a metric to a run</p></a></li>
<li><a href='#log_predictions_to_run'><p>Log a predictions metric to a run</p></a></li>
<li><a href='#log_residuals_to_run'><p>Log a residuals metric to a run</p></a></li>
<li><a href='#log_row_to_run'><p>Log a row metric to a run</p></a></li>
<li><a href='#log_table_to_run'><p>Log a table metric to a run</p></a></li>
<li><a href='#lognormal'><p>Specify a normal distribution of the form <code>exp(normal(mu, sigma))</code></p></a></li>
<li><a href='#loguniform'><p>Specify a log uniform distribution</p></a></li>
<li><a href='#median_stopping_policy'><p>Define a median stopping policy for early termination of HyperDrive runs</p></a></li>
<li><a href='#merge_results'><p>Combine the results from the parallel training.</p></a></li>
<li><a href='#mount_file_dataset'><p>Create a context manager for mounting file streams defined by the dataset as local files.</p></a></li>
<li><a href='#normal'><p>Specify a real value that is normally-distributed with mean <code>mu</code> and standard</p>
deviation <code>sigma</code></a></li>
<li><a href='#package_model'><p>Create a model package that packages all the assets needed to host a</p>
model as a web service</a></li>
<li><a href='#plot_run_details'><p>Generate table of run details</p></a></li>
<li><a href='#primary_metric_goal'><p>Define supported metric goals for hyperparameter tuning</p></a></li>
<li><a href='#promote_headers_behavior'><p>Defines options for how column headers are processed when reading data from files to create a dataset.</p></a></li>
<li><a href='#pull_model_package_image'><p>Pull the Docker image from a <code>ModelPackage</code> to your local</p>
Docker environment</a></li>
<li><a href='#qlognormal'><p>Specify a normal distribution of the form</p>
<code>round(exp(normal(mu, sigma)) / q) * q</code></a></li>
<li><a href='#qloguniform'><p>Specify a uniform distribution of the form</p>
<code style="white-space: pre;">&#8288;round(exp(uniform(min_value, max_value) / q) * q&#8288;</code></a></li>
<li><a href='#qnormal'><p>Specify a normal distribution of the <code style="white-space: pre;">&#8288;form round(normal(mu, sigma) / q) * q&#8288;</code></p></a></li>
<li><a href='#quniform'><p>Specify a uniform distribution of the form</p>
<code>round(uniform(min_value, max_value) / q) * q</code></a></li>
<li><a href='#r_environment'><p>Create an environment</p></a></li>
<li><a href='#randint'><p>Specify a set of random integers in the range <code style="white-space: pre;">&#8288;[0, upper)&#8288;</code></p></a></li>
<li><a href='#random_parameter_sampling'><p>Define random sampling over a hyperparameter search space</p></a></li>
<li><a href='#random_split_dataset'><p>Split file streams in the dataset into two parts randomly and approximately by the percentage specified.</p></a></li>
<li><a href='#register_azure_blob_container_datastore'><p>Register an Azure blob container as a datastore</p></a></li>
<li><a href='#register_azure_data_lake_gen2_datastore'><p>Initialize a new Azure Data Lake Gen2 Datastore.</p></a></li>
<li><a href='#register_azure_file_share_datastore'><p>Register an Azure file share as a datastore</p></a></li>
<li><a href='#register_azure_postgre_sql_datastore'><p>Initialize a new Azure PostgreSQL Datastore.</p></a></li>
<li><a href='#register_azure_sql_database_datastore'><p>Initialize a new Azure SQL database Datastore.</p></a></li>
<li><a href='#register_dataset'><p>Register a Dataset in the workspace</p></a></li>
<li><a href='#register_do_azureml_parallel'><p>Registers AMLCompute as a parallel backend with the foreach package.</p></a></li>
<li><a href='#register_environment'><p>Register an environment in the workspace</p></a></li>
<li><a href='#register_model'><p>Register a model to a given workspace</p></a></li>
<li><a href='#register_model_from_run'><p>Register a model for operationalization.</p></a></li>
<li><a href='#reload_local_webservice_assets'><p>Reload a local web service's entry script and dependencies</p></a></li>
<li><a href='#resource_configuration'><p>Initialize the  ResourceConfiguration.</p></a></li>
<li><a href='#save_model_package_files'><p>Save a Dockerfile and dependencies from a <code>ModelPackage</code> to</p>
your local file system</a></li>
<li><a href='#service_principal_authentication'><p>Manages authentication using a service principle instead of a user identity.</p></a></li>
<li><a href='#set_default_datastore'><p>Set the default datastore for a workspace</p></a></li>
<li><a href='#set_secrets'><p>Add secrets to a keyvault</p></a></li>
<li><a href='#skip_from_dataset'><p>Skip file streams from the top of the dataset by the specified count.</p></a></li>
<li><a href='#split_tasks'><p>Splits the job into parallel tasks.</p></a></li>
<li><a href='#start_logging_run'><p>Create an interactive logging run</p></a></li>
<li><a href='#submit_child_run'><p>Submit an experiment and return the active child run</p></a></li>
<li><a href='#submit_experiment'><p>Submit an experiment and return the active created run</p></a></li>
<li><a href='#take_from_dataset'><p>Take a sample of file streams from top of the dataset by the specified count.</p></a></li>
<li><a href='#take_sample_from_dataset'><p>Take a random sample of file streams in the dataset approximately by the probability specified.</p></a></li>
<li><a href='#truncation_selection_policy'><p>Define a truncation selection policy for early termination of HyperDrive runs</p></a></li>
<li><a href='#uniform'><p>Specify a uniform distribution of options to sample from</p></a></li>
<li><a href='#unregister_all_dataset_versions'><p>Unregister all versions under the registration name of this dataset from the workspace.</p></a></li>
<li><a href='#unregister_datastore'><p>Unregister a datastore from its associated workspace</p></a></li>
<li><a href='#update_aci_webservice'><p>Update a deployed ACI web service</p></a></li>
<li><a href='#update_aks_webservice'><p>Update a deployed AKS web service</p></a></li>
<li><a href='#update_aml_compute'><p>Update scale settings for an AmlCompute cluster</p></a></li>
<li><a href='#update_local_webservice'><p>Update a local web service</p></a></li>
<li><a href='#upload_files_to_datastore'><p>Upload files to the Azure storage a datastore points to</p></a></li>
<li><a href='#upload_files_to_run'><p>Upload files to a run</p></a></li>
<li><a href='#upload_folder_to_run'><p>Upload a folder to a run</p></a></li>
<li><a href='#upload_to_datastore'><p>Upload a local directory to the Azure storage a datastore points to</p></a></li>
<li><a href='#view_run_details'><p>Initialize run details widget</p></a></li>
<li><a href='#wait_for_deployment'><p>Wait for a web service to finish deploying</p></a></li>
<li><a href='#wait_for_model_package_creation'><p>Wait for a model package to finish creating</p></a></li>
<li><a href='#wait_for_provisioning_completion'><p>Wait for a cluster to finish provisioning</p></a></li>
<li><a href='#wait_for_run_completion'><p>Wait for the completion of a run</p></a></li>
<li><a href='#write_workspace_config'><p>Write out the workspace configuration details to a config file</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Interface to the 'Azure Machine Learning' 'SDK'</td>
</tr>
<tr>
<td>Version:</td>
<td>1.10.0</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/azure/azureml-sdk-for-r">https://github.com/azure/azureml-sdk-for-r</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/azure/azureml-sdk-for-r/issues">https://github.com/azure/azureml-sdk-for-r/issues</a></td>
</tr>
<tr>
<td>Description:</td>
<td>Interface to the 'Azure Machine Learning' Software Development Kit
    ('SDK'). Data scientists can use the 'SDK' to train, deploy, automate, and
    manage machine learning models on the 'Azure Machine Learning' service. To
    learn more about 'Azure Machine Learning' visit the website:
    <a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/overview-what-is-azure-ml">https://docs.microsoft.com/en-us/azure/machine-learning/service/overview-what-is-azure-ml</a>.</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>ggplot2, reticulate (&ge; 1.12), plyr (&ge; 1.8), DT, rstudioapi
(&ge; 0.7), htmltools, servr, shiny, shinycssloaders</td>
</tr>
<tr>
<td>Suggests:</td>
<td>rmarkdown, knitr, testthat, dplyr, jsonlite, foreach,
iterators, utils</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-09-16 21:35:24 UTC; dipeck</td>
</tr>
<tr>
<td>Author:</td>
<td>Diondra Peck [cre, aut],
  Minna Xiao [aut],
  AzureML R SDK Team [ctb],
  Microsoft [cph, fnd],
  Google Inc. [cph] (Examples and Tutorials),
  The TensorFlow Authors [cph] (Examples and Tutorials),
  RStudio Inc. [cph] (Examples and Tutorials)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Diondra Peck &lt;Diondra.Peck@microsoft.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-09-22 15:40:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='aci_webservice_deployment_config'>Create a deployment config for deploying an ACI web service</h2><span id='topic+aci_webservice_deployment_config'></span>

<h3>Description</h3>

<p>Deploy a web service to Azure Container Instances for testing or
debugging. Use ACI for low-scale CPU-based workloads that
require less than 48 GB of RAM.
</p>
<p>Deploy to ACI if one of the following conditions is true:
</p>

<ul>
<li><p> You need to quickly deploy and validate your model. You do not need
to create ACI containers ahead of time. They are created as part of
the deployment process.
</p>
</li>
<li><p> You are testing a model that is under development.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>aci_webservice_deployment_config(
  cpu_cores = NULL,
  memory_gb = NULL,
  tags = NULL,
  properties = NULL,
  description = NULL,
  location = NULL,
  auth_enabled = NULL,
  ssl_enabled = NULL,
  enable_app_insights = NULL,
  ssl_cert_pem_file = NULL,
  ssl_key_pem_file = NULL,
  ssl_cname = NULL,
  dns_name_label = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="aci_webservice_deployment_config_+3A_cpu_cores">cpu_cores</code></td>
<td>
<p>The number of cpu cores to allocate for
the web service. Can be a decimal. Defaults to <code>0.1</code>.</p>
</td></tr>
<tr><td><code id="aci_webservice_deployment_config_+3A_memory_gb">memory_gb</code></td>
<td>
<p>The amount of memory (in GB) to allocate for
the web service. Can be a decimal. Defaults to <code>0.5</code>.</p>
</td></tr>
<tr><td><code id="aci_webservice_deployment_config_+3A_tags">tags</code></td>
<td>
<p>A named list of key-value tags for the web service,
e.g. <code>list("key" = "value")</code>.</p>
</td></tr>
<tr><td><code id="aci_webservice_deployment_config_+3A_properties">properties</code></td>
<td>
<p>A named list of key-value properties for the web
service, e.g. <code>list("key" = "value")</code>. These properties cannot
be changed after deployment, but new key-value pairs can be added.</p>
</td></tr>
<tr><td><code id="aci_webservice_deployment_config_+3A_description">description</code></td>
<td>
<p>A string of the description to give the web service.</p>
</td></tr>
<tr><td><code id="aci_webservice_deployment_config_+3A_location">location</code></td>
<td>
<p>A string of the Azure region to deploy the web service
to. If not specified the workspace location will be used. More details
on available regions can be found <a href="https://azure.microsoft.com/en-us/global-infrastructure/services/?regions=all&amp;products=container-instances">here</a>.</p>
</td></tr>
<tr><td><code id="aci_webservice_deployment_config_+3A_auth_enabled">auth_enabled</code></td>
<td>
<p>If <code>TRUE</code> enable key-based authentication for the
web service. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="aci_webservice_deployment_config_+3A_ssl_enabled">ssl_enabled</code></td>
<td>
<p>If <code>TRUE</code> enable SSL for the web service. Defaults
to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="aci_webservice_deployment_config_+3A_enable_app_insights">enable_app_insights</code></td>
<td>
<p>If <code>TRUE</code> enable AppInsights for the web service.
Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="aci_webservice_deployment_config_+3A_ssl_cert_pem_file">ssl_cert_pem_file</code></td>
<td>
<p>A string of the cert file needed if SSL is enabled.</p>
</td></tr>
<tr><td><code id="aci_webservice_deployment_config_+3A_ssl_key_pem_file">ssl_key_pem_file</code></td>
<td>
<p>A string of the key file needed if SSL is enabled.</p>
</td></tr>
<tr><td><code id="aci_webservice_deployment_config_+3A_ssl_cname">ssl_cname</code></td>
<td>
<p>A string of the cname if SSL is enabled.</p>
</td></tr>
<tr><td><code id="aci_webservice_deployment_config_+3A_dns_name_label">dns_name_label</code></td>
<td>
<p>A string of the dns name label for the scoring
endpoint.
If not specified a unique dns name label will be generated for the scoring
endpoint.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>AciServiceDeploymentConfiguration</code> object.
</p>


<h3>See Also</h3>

<p><code>deploy_model()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
deployment_config &lt;- aci_webservice_deployment_config(cpu_cores = 1, memory_gb = 1)

## End(Not run)
</code></pre>

<hr>
<h2 id='aks_webservice_deployment_config'>Create a deployment config for deploying an AKS web service</h2><span id='topic+aks_webservice_deployment_config'></span>

<h3>Description</h3>

<p>Deploy a web service to Azure Kubernetes Service for high-scale
prodution deployments. Provides fast response time and autoscaling
of the deployed service. Using GPU for inference when deployed as a
web service is only supported on AKS.
</p>
<p>Deploy to AKS if you need one or more of the following capabilities:
</p>

<ul>
<li><p> Fast response time
</p>
</li>
<li><p> Autoscaling of the deployed service
</p>
</li>
<li><p> Hardware acceleration options
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>aks_webservice_deployment_config(
  autoscale_enabled = NULL,
  autoscale_min_replicas = NULL,
  autoscale_max_replicas = NULL,
  autoscale_refresh_seconds = NULL,
  autoscale_target_utilization = NULL,
  auth_enabled = NULL,
  cpu_cores = NULL,
  memory_gb = NULL,
  enable_app_insights = NULL,
  scoring_timeout_ms = NULL,
  replica_max_concurrent_requests = NULL,
  max_request_wait_time = NULL,
  num_replicas = NULL,
  primary_key = NULL,
  secondary_key = NULL,
  tags = NULL,
  properties = NULL,
  description = NULL,
  gpu_cores = NULL,
  period_seconds = NULL,
  initial_delay_seconds = NULL,
  timeout_seconds = NULL,
  success_threshold = NULL,
  failure_threshold = NULL,
  namespace = NULL,
  token_auth_enabled = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="aks_webservice_deployment_config_+3A_autoscale_enabled">autoscale_enabled</code></td>
<td>
<p>If <code>TRUE</code> enable autoscaling for the web service.
Defaults to <code>TRUE</code> if <code>num_replicas = NULL</code>.</p>
</td></tr>
<tr><td><code id="aks_webservice_deployment_config_+3A_autoscale_min_replicas">autoscale_min_replicas</code></td>
<td>
<p>An int of the minimum number of containers
to use when autoscaling the web service. Defaults to <code>1</code>.</p>
</td></tr>
<tr><td><code id="aks_webservice_deployment_config_+3A_autoscale_max_replicas">autoscale_max_replicas</code></td>
<td>
<p>An int of the maximum number of containers
to use when autoscaling the web service. Defaults to <code>10</code>.</p>
</td></tr>
<tr><td><code id="aks_webservice_deployment_config_+3A_autoscale_refresh_seconds">autoscale_refresh_seconds</code></td>
<td>
<p>An int of how often in seconds the
autoscaler should attempt to scale the web service. Defaults to <code>1</code>.</p>
</td></tr>
<tr><td><code id="aks_webservice_deployment_config_+3A_autoscale_target_utilization">autoscale_target_utilization</code></td>
<td>
<p>An int of the target utilization
(in percent out of 100) the autoscaler should attempt to maintain for
the web service. Defaults to <code>70</code>.</p>
</td></tr>
<tr><td><code id="aks_webservice_deployment_config_+3A_auth_enabled">auth_enabled</code></td>
<td>
<p>If <code>TRUE</code> enable key-based authentication for the
web service. Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="aks_webservice_deployment_config_+3A_cpu_cores">cpu_cores</code></td>
<td>
<p>The number of cpu cores to allocate for
the web service. Can be a decimal. Defaults to <code>0.1</code>.</p>
</td></tr>
<tr><td><code id="aks_webservice_deployment_config_+3A_memory_gb">memory_gb</code></td>
<td>
<p>The amount of memory (in GB) to allocate for
the web service. Can be a decimal. Defaults to <code>0.5</code>.</p>
</td></tr>
<tr><td><code id="aks_webservice_deployment_config_+3A_enable_app_insights">enable_app_insights</code></td>
<td>
<p>If <code>TRUE</code> enable AppInsights for the web service.
Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="aks_webservice_deployment_config_+3A_scoring_timeout_ms">scoring_timeout_ms</code></td>
<td>
<p>An int of the timeout (in milliseconds) to
enforce for scoring calls to the web service. Defaults to <code>60000</code>.</p>
</td></tr>
<tr><td><code id="aks_webservice_deployment_config_+3A_replica_max_concurrent_requests">replica_max_concurrent_requests</code></td>
<td>
<p>An int of the number of maximum
concurrent requests per node to allow for the web service. Defaults to <code>1</code>.</p>
</td></tr>
<tr><td><code id="aks_webservice_deployment_config_+3A_max_request_wait_time">max_request_wait_time</code></td>
<td>
<p>An int of the maximum amount of time a request
will stay in the queue (in milliseconds) before returning a 503 error.
Defaults to <code>500</code>.</p>
</td></tr>
<tr><td><code id="aks_webservice_deployment_config_+3A_num_replicas">num_replicas</code></td>
<td>
<p>An int of the number of containers to allocate for the
web service. If this parameter is not set then the autoscaler is enabled by
default.</p>
</td></tr>
<tr><td><code id="aks_webservice_deployment_config_+3A_primary_key">primary_key</code></td>
<td>
<p>A string of the primary auth key to use for the web service.</p>
</td></tr>
<tr><td><code id="aks_webservice_deployment_config_+3A_secondary_key">secondary_key</code></td>
<td>
<p>A string of the secondary auth key to use for the web
service.</p>
</td></tr>
<tr><td><code id="aks_webservice_deployment_config_+3A_tags">tags</code></td>
<td>
<p>A named list of key-value tags for the web service,
e.g. <code>list("key" = "value")</code>.</p>
</td></tr>
<tr><td><code id="aks_webservice_deployment_config_+3A_properties">properties</code></td>
<td>
<p>A named list of key-value properties for the web
service, e.g. <code>list("key" = "value")</code>. These properties cannot
be changed after deployment, but new key-value pairs can be added.</p>
</td></tr>
<tr><td><code id="aks_webservice_deployment_config_+3A_description">description</code></td>
<td>
<p>A string of the description to give the web service</p>
</td></tr>
<tr><td><code id="aks_webservice_deployment_config_+3A_gpu_cores">gpu_cores</code></td>
<td>
<p>An int of the number of gpu cores to allocate for the
web service. Defaults to <code>1</code>.</p>
</td></tr>
<tr><td><code id="aks_webservice_deployment_config_+3A_period_seconds">period_seconds</code></td>
<td>
<p>An int of how often in seconds to perform the
liveness probe. Default to <code>10</code>. Minimum value is <code>1</code>.</p>
</td></tr>
<tr><td><code id="aks_webservice_deployment_config_+3A_initial_delay_seconds">initial_delay_seconds</code></td>
<td>
<p>An int of the number of seconds after
the container has started before liveness probes are initiated.
Defaults to <code>310</code>.</p>
</td></tr>
<tr><td><code id="aks_webservice_deployment_config_+3A_timeout_seconds">timeout_seconds</code></td>
<td>
<p>An int of the number of seconds after which the
liveness probe times out. Defaults to <code>2</code>. Minimum value is <code>1</code>.</p>
</td></tr>
<tr><td><code id="aks_webservice_deployment_config_+3A_success_threshold">success_threshold</code></td>
<td>
<p>An int of the minimum consecutive successes
for the liveness probe to be considered successful after having failed.
Defaults to <code>1</code>. Minimum value is <code>1</code>.</p>
</td></tr>
<tr><td><code id="aks_webservice_deployment_config_+3A_failure_threshold">failure_threshold</code></td>
<td>
<p>An int of the number of times Kubernetes will try
the liveness probe when a Pod starts and the probe fails, before giving up.
Defaults to <code>3</code>. Minimum value is <code>1</code>.</p>
</td></tr>
<tr><td><code id="aks_webservice_deployment_config_+3A_namespace">namespace</code></td>
<td>
<p>A string of the Kubernetes namespace in which to deploy the web service:
up to 63 lowercase alphanumeric ('a'-'z', '0'-'9') and hyphen ('-') characters. The first
last characters cannot be hyphens.</p>
</td></tr>
<tr><td><code id="aks_webservice_deployment_config_+3A_token_auth_enabled">token_auth_enabled</code></td>
<td>
<p>If <code>TRUE</code>, enable token-based authentication for the web service.
If enabled, users can access the web service by fetching an access token using their Azure
Active Directory credentials. Defaults to <code>FALSE</code>. Both <code>token_auth_enabled</code> and
<code>auth_enabled</code> cannot be set to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>AKS compute target</h4>

<p>When deploying to AKS, you deploy to an AKS cluster that is connected to your workspace.
There are two ways to connect an AKS cluster to your workspace:
</p>

<ul>
<li><p> Create the AKS cluster using Azure ML (see <code>create_aks_compute()</code>).
</p>
</li>
<li><p> Attach an existing AKS cluster to your workspace (see <code>attach_aks_compute()</code>).
</p>
</li></ul>

<p>Pass the <code>AksCompute</code> object to the <code>deployment_target</code> parameter of <code>deploy_model()</code>.
</p>



<h4>Token-based authentication</h4>

<p>We strongly recommend that you create your Azure ML workspace in the same region as your
AKS cluster. To authenticate with a token, the web service will make a call to the region
in which your workspace is created. If your workspace's region is unavailable, then you will
not be able to fetch a token for your web service, even if your cluster is in a different region
than your workspace. This effectively results in token-based auth being unavailable until your
workspace's region is available again. In addition, the greater the distance between your
cluster's region and your workspace's region, the longer it will take to fetch a token.
</p>



<h3>Value</h3>

<p>The <code>AksServiceDeploymentConfiguration</code> object.
</p>


<h3>See Also</h3>

<p><code>deploy_model()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
deployment_config &lt;- aks_webservice_deployment_config(cpu_cores = 1, memory_gb = 1)

## End(Not run)
</code></pre>

<hr>
<h2 id='attach_aks_compute'>Attach an existing AKS cluster to a workspace</h2><span id='topic+attach_aks_compute'></span>

<h3>Description</h3>

<p>If you already have an AKS cluster in your Azure subscription, and it is
version 1.12.##, you can attach it to your workspace to use for deployments.
The existing AKS cluster can be in a different Azure region than your
workspace.
</p>
<p>If you want to secure your AKS cluster using an Azure Virtual Network, you
must create the virtual network first. For more information, see
<a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-enable-virtual-network#aksvnet">Secure Azure ML experimentation and inference jobs within an Azure Virtual Network</a>
</p>
<p>If you want to re-attach an AKS cluster, for example to to change SSL or other
cluster configuration settings, you must first remove the existing attachment
with <code>detach_aks_compute()</code>.
</p>
<p>Attaching a cluster will take approximately 5 minutes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>attach_aks_compute(
  workspace,
  resource_group,
  cluster_name,
  cluster_purpose = c("FastProd", "DevTest")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="attach_aks_compute_+3A_workspace">workspace</code></td>
<td>
<p>The <code>Workspace</code> object to attach the AKS cluster to.</p>
</td></tr>
<tr><td><code id="attach_aks_compute_+3A_resource_group">resource_group</code></td>
<td>
<p>A string of the resource group in which the AKS cluster
is located.</p>
</td></tr>
<tr><td><code id="attach_aks_compute_+3A_cluster_name">cluster_name</code></td>
<td>
<p>A string of the name of the AKS cluster.</p>
</td></tr>
<tr><td><code id="attach_aks_compute_+3A_cluster_purpose">cluster_purpose</code></td>
<td>
<p>The targeted usage of the cluster. The possible values are
&quot;DevTest&quot; or &quot;FastProd&quot;. This is used to provision Azure Machine Learning components
to ensure the desired level of fault-tolerance and QoS. If your cluster has less
than 12 virtual CPUs, you will need to specify &quot;DevTest&quot; for this argument. We
recommend that your cluster have at least 2 virtual CPUs for dev/test usage.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>AksCompute</code> object.
</p>


<h3>Examples</h3>

<div class="r"><pre>ws &lt;- load_workspace_from_config()
compute_target &lt;- attach_aks_compute(ws,
                                     resource_group = 'myresourcegroup',
                                     cluster_name = 'myakscluster')
</pre></div>
<p>If the cluster has less than 12 virtual CPUs, you will need to also specify the
<code>cluster_purpose</code> parameter in the <code>attach_aks_compute()</code> call: <code>cluster_purpose = 'DevTest'</code>.
</p>


<h3>See Also</h3>

<p><code>detach_aks_compute()</code>
</p>

<hr>
<h2 id='azureml'>azureml module
User can access functions/modules in azureml that are not exposed through the
exported R functions.</h2><span id='topic+azureml'></span>

<h3>Description</h3>

<p>azureml module
User can access functions/modules in azureml that are not exposed through the
exported R functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>azureml
</code></pre>


<h3>Format</h3>

<p>An object of class <code>python.builtin.module</code> (inherits from <code>python.builtin.object</code>) of length 5.
</p>

<hr>
<h2 id='bandit_policy'>Define a Bandit policy for early termination of HyperDrive runs</h2><span id='topic+bandit_policy'></span>

<h3>Description</h3>

<p>Bandit is an early termination policy based on slack factor/slack amount
and evaluation interval. The policy early terminates any runs where the
primary metric is not within the specified slack factor/slack amount with
respect to the best performing training run.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bandit_policy(
  slack_factor = NULL,
  slack_amount = NULL,
  evaluation_interval = 1L,
  delay_evaluation = 0L
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bandit_policy_+3A_slack_factor">slack_factor</code></td>
<td>
<p>A double of the ratio of the allowed distance from
the best performing run.</p>
</td></tr>
<tr><td><code id="bandit_policy_+3A_slack_amount">slack_amount</code></td>
<td>
<p>A double of the absolute distance allowed from the
best performing run.</p>
</td></tr>
<tr><td><code id="bandit_policy_+3A_evaluation_interval">evaluation_interval</code></td>
<td>
<p>An integer of the frequency for applying policy.</p>
</td></tr>
<tr><td><code id="bandit_policy_+3A_delay_evaluation">delay_evaluation</code></td>
<td>
<p>An integer of the number of intervals for which to
delay the first evaluation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>BanditPolicy</code> object.
</p>


<h3>Details</h3>

<p>The Bandit policy takes the following configuration parameters:
</p>

<ul>
<li> <p><code>slack_factor</code> or <code>slack_amount</code>: The slack allowed with respect to
the best performing training run. <code>slack_factor</code> specifies the
allowable slack as a ration. <code>slack_amount</code> specifies the allowable
slack as an absolute amount, instead of a ratio.
</p>
</li>
<li> <p><code>evaluation_interval</code>: Optional. The frequency for applying the policy.
Each time the training script logs the primary metric counts as one
interval.
</p>
</li>
<li> <p><code>delay_evaluation</code>: Optional. The number of intervals to delay the
policy evaluation. Use this parameter to avoid premature termination
of training runs. If specified, the policy applies every multiple of
<code>evaluation_interval</code> that is greater than or equal to <code>delay_evaluation</code>.
</p>
</li></ul>

<p>Any run that doesn't fall within the slack factor or slack amount of the
evaluation metric with respect to the best performing run will be
terminated.
</p>
<p>Consider a Bandit policy with <code>slack_factor = 0.2</code> and
<code>evaluation_interval = 100</code>. Assume that run X is the currently best
performing run with an AUC (performance metric) of 0.8 after 100 intervals.
Further, assume the best AUC reported for a run is Y. This policy compares
the value <code>(Y + Y * 0.2)</code> to 0.8, and if smaller, cancels the run.
If <code>delay_evaluation = 200</code>, then the first time the policy will be applied
is at interval 200.
</p>
<p>Now, consider a Bandit policy with <code>slack_amount = 0.2</code> and
<code>evaluation_interval = 100</code>. If run 3 is the currently best performing run
with an AUC (performance metric) of 0.8 after 100 intervals, then any run
with an AUC less than 0.6 (<code>0.8 - 0.2</code>) after 100 iterations will be
terminated. Similarly, the <code>delay_evaluation</code> can also be used to delay the
first termination policy evaluation for a specific number of sequences.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># In this example, the early termination policy is applied at every interval
# when metrics are reported, starting at evaluation interval 5. Any run whose
# best metric is less than (1 / (1 + 0.1)) or 91\% of the best performing run will
# be terminated
## Not run: 
early_termination_policy = bandit_policy(slack_factor = 0.1,
                                         evaluation_interval = 1L,
                                         delay_evaluation = 5L)

## End(Not run)
</code></pre>

<hr>
<h2 id='bayesian_parameter_sampling'>Define Bayesian sampling over a hyperparameter search space</h2><span id='topic+bayesian_parameter_sampling'></span>

<h3>Description</h3>

<p>Bayesian sampling is based on the Bayesian optimization algorithm and makes
intelligent choices on the hyperparameter values to sample next. It picks
the sample based on how the previous samples performed, such that the new
sample improves the reported primary metric.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bayesian_parameter_sampling(parameter_space)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bayesian_parameter_sampling_+3A_parameter_space">parameter_space</code></td>
<td>
<p>A named list containing each parameter and its
distribution, e.g. <code>list("parameter" = distribution)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>BayesianParameterSampling</code> object.
</p>


<h3>Details</h3>

<p>When you use Bayesian sampling, the number of concurrent runs has an impact
on the effectiveness of the tuning process. Typically, a smaller number of
concurrent runs can lead to better sampling convergence, since the smaller
degree of parallelism increases the number of runs that benefit from
previously completed runs.
</p>
<p>Bayesian sampling only supports <code>choice()</code>, <code>uniform()</code>, and <code>quniform()</code>
distributions over the search space.
</p>
<p>Bayesian sampling does not support any early termination policy. When
using Bayesian parameter sampling, <code>early_termination_policy</code> must be
<code>NULL</code>.
</p>


<h3>See Also</h3>

<p><code>choice()</code>, <code>uniform()</code>, <code>quniform()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
param_sampling &lt;- bayesian_parameter_sampling(list("learning_rate" = uniform(0.05, 0.1),
                                                   "batch_size" = choice(c(16, 32, 64, 128))))

## End(Not run)
</code></pre>

<hr>
<h2 id='cancel_run'>Cancel a run</h2><span id='topic+cancel_run'></span>

<h3>Description</h3>

<p>Cancel an ongoing run.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cancel_run(run)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cancel_run_+3A_run">run</code></td>
<td>
<p>The <code>Run</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if cancellation was successful, else <code>FALSE</code>.
</p>

<hr>
<h2 id='choice'>Specify a discrete set of options to sample from</h2><span id='topic+choice'></span>

<h3>Description</h3>

<p>Specify a discrete set of options to sample the hyperparameters
from.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>choice(options)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="choice_+3A_options">options</code></td>
<td>
<p>A vector of values to choose from.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of the stochastic expression.
</p>


<h3>See Also</h3>

<p><code>random_parameter_sampling()</code>, <code>grid_parameter_sampling()</code>,
<code>bayesian_parameter_sampling()</code>
</p>

<hr>
<h2 id='complete_run'>Mark a run as completed.</h2><span id='topic+complete_run'></span>

<h3>Description</h3>

<p>Mark the run as completed. Use for an interactive logging run.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>complete_run(run)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="complete_run_+3A_run">run</code></td>
<td>
<p>The <code>Run</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>


<h3>See Also</h3>

<p><code><a href="#topic+start_logging_run">start_logging_run()</a></code>
</p>

<hr>
<h2 id='container_registry'>Specify Azure Container Registry details</h2><span id='topic+container_registry'></span>

<h3>Description</h3>

<p>Returns a <code>ContainerRegistry</code> object with the details for an
Azure Container Registry (ACR). This is needed when a custom
Docker image used for training or deployment is located in
a private image registry. Provide a <code>ContainerRegistry</code> object
to the <code>image_registry_details</code> parameter of either <code>r_environment()</code>
or <code>estimator()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>container_registry(address = NULL, username = NULL, password = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="container_registry_+3A_address">address</code></td>
<td>
<p>A string of the DNS name or IP address of the
Azure Container Registry (ACR).</p>
</td></tr>
<tr><td><code id="container_registry_+3A_username">username</code></td>
<td>
<p>A string of the username for ACR.</p>
</td></tr>
<tr><td><code id="container_registry_+3A_password">password</code></td>
<td>
<p>A string of the password for ACR.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>ContainerRegistry</code> object.
</p>


<h3>See Also</h3>

<p><code>r_environment()</code>, <code>estimator()</code>
</p>

<hr>
<h2 id='convert_to_dataset_with_csv_files'>Convert the current dataset into a FileDataset containing CSV files.</h2><span id='topic+convert_to_dataset_with_csv_files'></span>

<h3>Description</h3>

<p>Convert the current dataset into a FileDataset containing CSV files.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convert_to_dataset_with_csv_files(dataset, separator = ",")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="convert_to_dataset_with_csv_files_+3A_dataset">dataset</code></td>
<td>
<p>The Tabular Dataset object.</p>
</td></tr>
<tr><td><code id="convert_to_dataset_with_csv_files_+3A_separator">separator</code></td>
<td>
<p>The separator to use to separate values in the resulting file.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new FileDataset object with a set of CSV files containing the data
in this dataset.
</p>

<hr>
<h2 id='convert_to_dataset_with_parquet_files'>Convert the current dataset into a FileDataset containing Parquet files.</h2><span id='topic+convert_to_dataset_with_parquet_files'></span>

<h3>Description</h3>

<p>Convert the current dataset into a FileDataset containing Parquet files.
The resulting dataset will contain one or more Parquet files, each corresponding
to a partition of data from the current dataset. These files are not materialized
until they are downloaded or read from.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convert_to_dataset_with_parquet_files(dataset)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="convert_to_dataset_with_parquet_files_+3A_dataset">dataset</code></td>
<td>
<p>The Tabular Dataset object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new FileDataset object with a set of Parquet files containing the
data in this dataset.
</p>

<hr>
<h2 id='cran_package'>Specifies a CRAN package to install in environment</h2><span id='topic+cran_package'></span>

<h3>Description</h3>

<p>Specifies a CRAN package to install in run environment
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cran_package(name, version = NULL, repo = "https://cloud.r-project.org")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cran_package_+3A_name">name</code></td>
<td>
<p>The package name</p>
</td></tr>
<tr><td><code id="cran_package_+3A_version">version</code></td>
<td>
<p>A string of the package version. If not provided, version
will default to latest</p>
</td></tr>
<tr><td><code id="cran_package_+3A_repo">repo</code></td>
<td>
<p>The base URL of the repository to use, e.g., the URL of a
CRAN mirror. If not provided, the package will be pulled from
&quot;https://cloud.r-project.org&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list containing the package specifications
</p>


<h3>Examples</h3>

<pre>pkg1 &lt;- cran_package("ggplot2", version = "3.3.0")
pkg2 &lt;- cran_package("stringr")
pkg3 &lt;- cran_package("ggplot2", version = "0.9.1",
                     repo = "http://cran.us.r-project.org")

env &lt;- r_environment(name = "r_env",
                     cran_packages = list(pkg1, pkg2, pkg3))
</pre>


<h3>See Also</h3>

<p><code><a href="#topic+r_environment">r_environment()</a></code>
</p>

<hr>
<h2 id='create_aks_compute'>Create an AksCompute cluster</h2><span id='topic+create_aks_compute'></span>

<h3>Description</h3>

<p>Provision an Azure Kubernetes Service instance (AksCompute) as a compute
target for web service deployment. AksCompute is recommended for high-scale
production deployments and provides fast response time and autoscaling of
the deployed service. Cluster autoscaling isn't supported through the Azure
ML R SDK. To change the nodes in the AksCompute cluster, use the UI for the
cluster in the Azure portal. Once created, the cluster can be reused for
multiple deployments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_aks_compute(
  workspace,
  cluster_name,
  agent_count = NULL,
  vm_size = NULL,
  ssl_cname = NULL,
  ssl_cert_pem_file = NULL,
  ssl_key_pem_file = NULL,
  location = NULL,
  vnet_resourcegroup_name = NULL,
  vnet_name = NULL,
  subnet_name = NULL,
  service_cidr = NULL,
  dns_service_ip = NULL,
  docker_bridge_cidr = NULL,
  cluster_purpose = c("FastProd", "DevTest")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_aks_compute_+3A_workspace">workspace</code></td>
<td>
<p>The <code>Workspace</code> object.</p>
</td></tr>
<tr><td><code id="create_aks_compute_+3A_cluster_name">cluster_name</code></td>
<td>
<p>A string of the name of the cluster.</p>
</td></tr>
<tr><td><code id="create_aks_compute_+3A_agent_count">agent_count</code></td>
<td>
<p>An integer of the number of agents (VMs) to host
containers. Defaults to <code>3</code>.</p>
</td></tr>
<tr><td><code id="create_aks_compute_+3A_vm_size">vm_size</code></td>
<td>
<p>A string of the size of agent VMs. More details can be found
<a href="https://docs.microsoft.com/en-us/azure/templates/microsoft.compute/2019-12-01/virtualmachines#virtualmachineidentity-object">here</a>.
Note that not all sizes are available in all regions, as detailed in the
aformentioned link. Defaults to <code>'Standard_D3_v2'</code>.</p>
</td></tr>
<tr><td><code id="create_aks_compute_+3A_ssl_cname">ssl_cname</code></td>
<td>
<p>A string of a CName to use if enabling SSL validation on
the cluster. Must provide all three - CName, cert file, and key file - to
enable SSL validation.</p>
</td></tr>
<tr><td><code id="create_aks_compute_+3A_ssl_cert_pem_file">ssl_cert_pem_file</code></td>
<td>
<p>A string of a file path to a file containing cert
information for SSL validation. Must provide all three - CName, cert file,
and key file - to enable SSL validation.</p>
</td></tr>
<tr><td><code id="create_aks_compute_+3A_ssl_key_pem_file">ssl_key_pem_file</code></td>
<td>
<p>A string of a file path to a file containing key
information for SSL validation. Must provide all three - CName, cert file,
and key file - to enable SSL validation.</p>
</td></tr>
<tr><td><code id="create_aks_compute_+3A_location">location</code></td>
<td>
<p>A string of the location to provision the cluster in. If not
specified, defaults to the workspace location. Available regions for this
compute can be found here:
&quot;https://azure.microsoft.com/global-infrastructure/services/?regions=all&amp;products=kubernetes-service&quot;.</p>
</td></tr>
<tr><td><code id="create_aks_compute_+3A_vnet_resourcegroup_name">vnet_resourcegroup_name</code></td>
<td>
<p>A string of the name of the resource group
where the virtual network is located.</p>
</td></tr>
<tr><td><code id="create_aks_compute_+3A_vnet_name">vnet_name</code></td>
<td>
<p>A string of the name of the virtual network.</p>
</td></tr>
<tr><td><code id="create_aks_compute_+3A_subnet_name">subnet_name</code></td>
<td>
<p>A string of the name of the subnet inside the vnet.</p>
</td></tr>
<tr><td><code id="create_aks_compute_+3A_service_cidr">service_cidr</code></td>
<td>
<p>A string of a CIDR notation IP range from which to assign
service cluster IPs.</p>
</td></tr>
<tr><td><code id="create_aks_compute_+3A_dns_service_ip">dns_service_ip</code></td>
<td>
<p>A string of the container's DNS server IP address.</p>
</td></tr>
<tr><td><code id="create_aks_compute_+3A_docker_bridge_cidr">docker_bridge_cidr</code></td>
<td>
<p>A string of a CIDR notation IP for Docker bridge.</p>
</td></tr>
<tr><td><code id="create_aks_compute_+3A_cluster_purpose">cluster_purpose</code></td>
<td>
<p>A string describing targeted usage of the cluster.
This is used to provision Azure Machine Learning components to ensure the desired level of fault-tolerance and QoS.
'FastProd' will provision components to handle higher levels of traffic with production quality fault-tolerance. This will default the AKS cluster to have 3 nodes.
'DevTest' will provision components at a minimal level for testing. This will default the AKS cluster to have 1 node.
'FastProd'is the default value.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code>AksCompute</code> object.
</p>


<h3>Details</h3>

<p>For more information on using an AksCompute resource within a virtual
network, see
<a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-enable-virtual-network#use-azure-kubernetes-service-aks">Secure Azure ML experimentation and inference jobs within an Azure Virtual Network</a>.
</p>


<h3>Examples</h3>

<div class="r"><pre># Create an AksCompute cluster using the default configuration (you can also
# provide parameters to customize this)

ws &lt;- load_workspace_from_config()

compute_target &lt;- create_aks_compute(ws, cluster_name = 'mycluster')
wait_for_provisioning_completion(compute_target)
</pre></div>

<hr>
<h2 id='create_aml_compute'>Create an AmlCompute cluster</h2><span id='topic+create_aml_compute'></span>

<h3>Description</h3>

<p>Provision Azure Machine Learning Compute (AmlCompute) as a compute target
for training. AmlCompute is a managed-compute infrastructure that allows the
user to easily create a single or multi-node compute. To create a persistent
AmlCompute resource that can be reused across jobs, make sure to specify the
<code>vm_size</code> and <code>max_nodes</code> parameters. The compute can then be shared with
other users in the workspace and is kept between jobs. If <code>min_nodes = 0</code>,
the compute autoscales down to zero nodes when it isn't used, and scales up
automatically when a job is submitted.
</p>
<p>AmlCompute has default limits, such as the number of cores that can be
allocated. For more information, see
<a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas">Manage and request quotas for Azure resources</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_aml_compute(
  workspace,
  cluster_name,
  vm_size,
  vm_priority = "dedicated",
  min_nodes = 0,
  max_nodes = NULL,
  idle_seconds_before_scaledown = NULL,
  admin_username = NULL,
  admin_user_password = NULL,
  admin_user_ssh_key = NULL,
  vnet_resourcegroup_name = NULL,
  vnet_name = NULL,
  subnet_name = NULL,
  tags = NULL,
  description = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_aml_compute_+3A_workspace">workspace</code></td>
<td>
<p>The <code>Workspace</code> object.</p>
</td></tr>
<tr><td><code id="create_aml_compute_+3A_cluster_name">cluster_name</code></td>
<td>
<p>A string of the name of the cluster.</p>
</td></tr>
<tr><td><code id="create_aml_compute_+3A_vm_size">vm_size</code></td>
<td>
<p>A string of the size of agent VMs. More details can be found
<a href="https://docs.microsoft.com/en-us/azure/templates/microsoft.compute/2019-12-01/virtualmachines#virtualmachineidentity-object">here</a>.
Note that not all sizes are available in all regions, as detailed in the
aformentioned link. Defaults to <code>'Standard_NC6'</code>.</p>
</td></tr>
<tr><td><code id="create_aml_compute_+3A_vm_priority">vm_priority</code></td>
<td>
<p>A string of either <code>'dedicated'</code> or <code>'lowpriority'</code> to
use either dedicated or low-priority VMs. Defaults to <code>'dedicated'</code>.</p>
</td></tr>
<tr><td><code id="create_aml_compute_+3A_min_nodes">min_nodes</code></td>
<td>
<p>An integer of the minimum number of nodes to use on the
cluster. If not specified, will default to <code>0</code>.</p>
</td></tr>
<tr><td><code id="create_aml_compute_+3A_max_nodes">max_nodes</code></td>
<td>
<p>An integer of the maximum number of nodes to use on the
cluster.</p>
</td></tr>
<tr><td><code id="create_aml_compute_+3A_idle_seconds_before_scaledown">idle_seconds_before_scaledown</code></td>
<td>
<p>An integer of the node idle time in
seconds before scaling down the cluster. Defaults to <code>120</code>.</p>
</td></tr>
<tr><td><code id="create_aml_compute_+3A_admin_username">admin_username</code></td>
<td>
<p>A string of the name of the administrator user account
that can be used to SSH into nodes.</p>
</td></tr>
<tr><td><code id="create_aml_compute_+3A_admin_user_password">admin_user_password</code></td>
<td>
<p>A string of the password of the administrator user
account.</p>
</td></tr>
<tr><td><code id="create_aml_compute_+3A_admin_user_ssh_key">admin_user_ssh_key</code></td>
<td>
<p>A string of the SSH public key of the administrator
user account.</p>
</td></tr>
<tr><td><code id="create_aml_compute_+3A_vnet_resourcegroup_name">vnet_resourcegroup_name</code></td>
<td>
<p>A string of the name of the resource group
where the virtual network is located.</p>
</td></tr>
<tr><td><code id="create_aml_compute_+3A_vnet_name">vnet_name</code></td>
<td>
<p>A string of the name of the virtual network.</p>
</td></tr>
<tr><td><code id="create_aml_compute_+3A_subnet_name">subnet_name</code></td>
<td>
<p>A string of the name of the subnet inside the vnet.</p>
</td></tr>
<tr><td><code id="create_aml_compute_+3A_tags">tags</code></td>
<td>
<p>A named list of tags for the cluster, e.g.
<code>list("tag" = "value")</code>.'</p>
</td></tr>
<tr><td><code id="create_aml_compute_+3A_description">description</code></td>
<td>
<p>A string of the description for the cluster.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>AmlCompute</code> object.
</p>


<h3>Details</h3>

<p>For more information on using an Azure Machine Learning Compute resource
in a virtual network, see
<a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-enable-virtual-network#use-a-machine-learning-compute-instance">Secure Azure ML experimentation and inference jobs within an Azure Virtual Network</a>.
</p>


<h3>See Also</h3>

<p><code>wait_for_provisioning_completion()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
ws &lt;- load_workspace_from_config()
compute_target &lt;- create_aml_compute(ws,
                                     cluster_name = 'mycluster',
                                     vm_size = 'STANDARD_D2_V2',
                                     max_nodes = 1)
wait_for_provisioning_completion(compute_target, show_output = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='create_child_run'>Create a child run</h2><span id='topic+create_child_run'></span>

<h3>Description</h3>

<p>Create a child run. This is used to isolate part of a run into a subsection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_child_run(parent_run, name = NULL, run_id = NULL, outputs = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_child_run_+3A_parent_run">parent_run</code></td>
<td>
<p>The parent <code>Run</code> object.</p>
</td></tr>
<tr><td><code id="create_child_run_+3A_name">name</code></td>
<td>
<p>An optional name for the child run, typically specified for a &quot;part&quot;</p>
</td></tr>
<tr><td><code id="create_child_run_+3A_run_id">run_id</code></td>
<td>
<p>An optional run ID for the child, otherwise it is auto-generated.
Typically this parameter is not set.</p>
</td></tr>
<tr><td><code id="create_child_run_+3A_outputs">outputs</code></td>
<td>
<p>Optional outputs directory to track for the child.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The child run, a <code>Run</code> object.
</p>

<hr>
<h2 id='create_child_runs'>Create one or many child runs</h2><span id='topic+create_child_runs'></span>

<h3>Description</h3>

<p>Create one or many child runs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_child_runs(parent_run, count = NULL, tag_key = NULL, tag_values = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_child_runs_+3A_parent_run">parent_run</code></td>
<td>
<p>The parent <code>Run</code> object.</p>
</td></tr>
<tr><td><code id="create_child_runs_+3A_count">count</code></td>
<td>
<p>An optional number of children to create.</p>
</td></tr>
<tr><td><code id="create_child_runs_+3A_tag_key">tag_key</code></td>
<td>
<p>An optional key to populate the Tags entry in all created children.</p>
</td></tr>
<tr><td><code id="create_child_runs_+3A_tag_values">tag_values</code></td>
<td>
<p>An optional list of values that will map onto Tags for the list of runs created.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The list of child runs, <code>Run</code> objects.
</p>

<hr>
<h2 id='create_file_dataset_from_files'>Create a FileDataset to represent file streams.</h2><span id='topic+create_file_dataset_from_files'></span>

<h3>Description</h3>

<p>Create a FileDataset to represent file streams.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_file_dataset_from_files(path, validate = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_file_dataset_from_files_+3A_path">path</code></td>
<td>
<p>A data path in a registered datastore or a local path.</p>
</td></tr>
<tr><td><code id="create_file_dataset_from_files_+3A_validate">validate</code></td>
<td>
<p>Indicates whether to validate if data can be loaded from the
returned dataset. Defaults to True. Validation requires that the data source
is accessible from the current compute.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The FileDataset object
</p>


<h3>See Also</h3>

<p><code><a href="#topic+data_path">data_path</a></code>
</p>

<hr>
<h2 id='create_tabular_dataset_from_delimited_files'>Create an unregistered, in-memory Dataset from delimited files.</h2><span id='topic+create_tabular_dataset_from_delimited_files'></span>

<h3>Description</h3>

<p>Create an unregistered, in-memory Dataset from delimited files.
Use this method to read delimited text files when you want to control the options used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_tabular_dataset_from_delimited_files(
  path,
  validate = TRUE,
  include_path = FALSE,
  infer_column_types = TRUE,
  set_column_types = NULL,
  separator = ",",
  header = TRUE,
  partition_format = NULL,
  support_multi_line = FALSE,
  empty_as_string = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_tabular_dataset_from_delimited_files_+3A_path">path</code></td>
<td>
<p>A data path in a registered datastore, a local path, or an HTTP URL.</p>
</td></tr>
<tr><td><code id="create_tabular_dataset_from_delimited_files_+3A_validate">validate</code></td>
<td>
<p>Boolean to validate if data can be loaded from the returned dataset.
Defaults to True. Validation requires that the data source is accessible from the
current compute.</p>
</td></tr>
<tr><td><code id="create_tabular_dataset_from_delimited_files_+3A_include_path">include_path</code></td>
<td>
<p>Whether to include a column containing the path of the file
from which the data was read. This is useful when you are reading multiple files,
and want to know which file a particular record originated from, or to keep
useful information in file path.</p>
</td></tr>
<tr><td><code id="create_tabular_dataset_from_delimited_files_+3A_infer_column_types">infer_column_types</code></td>
<td>
<p>Indicates whether column data types are inferred.</p>
</td></tr>
<tr><td><code id="create_tabular_dataset_from_delimited_files_+3A_set_column_types">set_column_types</code></td>
<td>
<p>A named list to set column data type, where key is
column name and value is data type.</p>
</td></tr>
<tr><td><code id="create_tabular_dataset_from_delimited_files_+3A_separator">separator</code></td>
<td>
<p>The separator used to split columns.</p>
</td></tr>
<tr><td><code id="create_tabular_dataset_from_delimited_files_+3A_header">header</code></td>
<td>
<p>Controls how column headers are promoted when reading from files. Defaults to True for all
files having the same header. Files will read as having no header When header=False. More options can
be specified using <code>PromoteHeadersBehavior</code>.</p>
</td></tr>
<tr><td><code id="create_tabular_dataset_from_delimited_files_+3A_partition_format">partition_format</code></td>
<td>
<p>Specify the partition format in path and create string columns from
format 'x' and datetime column from format 'x:yyyy/MM/dd/HH/mm/ss', where 'yyyy', 'MM',
'dd', 'HH', 'mm' and 'ss' are used to extrat year, month, day, hour, minute and second for the datetime
type. The format should start from the postition of first partition key until the end of file path.
For example, given a file path '../USA/2019/01/01/data.csv' and data is partitioned by country and time,
we can define '/Country/PartitionDate:yyyy/MM/dd/data.csv' to create columns 'Country'
of string type and 'PartitionDate' of datetime type.</p>
</td></tr>
<tr><td><code id="create_tabular_dataset_from_delimited_files_+3A_support_multi_line">support_multi_line</code></td>
<td>
<p>By default (support_multi_line=FALSE), all line breaks,
including those in quoted field values, will be interpreted as a record break. Reading data this way is
faster and more optimized for parallel execution on multiple CPU cores. However, it may result in silently
producing more records with misaligned field values. This should be set to TRUE when the delimited files
are known to contain quoted line breaks.</p>
</td></tr>
<tr><td><code id="create_tabular_dataset_from_delimited_files_+3A_empty_as_string">empty_as_string</code></td>
<td>
<p>Specify if empty field values should be loaded as empty strings.
The default (FALSE) will read empty field values as nulls. Passing this as TRUE will read empty
field values as empty strings. If the values are converted to numeric or datetime then this has no effect,
as empty values will be converted to nulls.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The Tabular Dataset object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+data_path">data_path</a></code>
</p>

<hr>
<h2 id='create_tabular_dataset_from_json_lines_files'>Create a TabularDataset to represent tabular data in JSON Lines files (http://jsonlines.org/).</h2><span id='topic+create_tabular_dataset_from_json_lines_files'></span>

<h3>Description</h3>

<p>Create a TabularDataset to represent tabular data in JSON Lines files (http://jsonlines.org/).
&ldquo;from_json_lines_files&ldquo;' creates a Tabular Dataset object , which defines the operations to
load data from JSON Lines files into tabular representation. For the data to be accessible
by Azure Machine Learning, the JSON Lines files specified by <code>path</code> must be located in
a Datastore or behind public web urls. Column data types are read from data types saved
in the JSON Lines files. Providing 'set_column_types' will override the data type
for the specified columns in the returned Tabular Dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_tabular_dataset_from_json_lines_files(
  path,
  validate = TRUE,
  include_path = FALSE,
  set_column_types = NULL,
  partition_format = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_tabular_dataset_from_json_lines_files_+3A_path">path</code></td>
<td>
<p>The path to the source files, which can be single value or list
of http url string or tuple of Datastore and relative path.</p>
</td></tr>
<tr><td><code id="create_tabular_dataset_from_json_lines_files_+3A_validate">validate</code></td>
<td>
<p>Boolean to validate if data can be loaded from the returned
dataset. Defaults to True. Validation requires that the data source is
accessible from the current compute.</p>
</td></tr>
<tr><td><code id="create_tabular_dataset_from_json_lines_files_+3A_include_path">include_path</code></td>
<td>
<p>Boolean to keep path information as column in the dataset.
Defaults to False. This is useful when reading multiple files, and want to
know which file a particular record originated from, or to keep useful
information in file path.</p>
</td></tr>
<tr><td><code id="create_tabular_dataset_from_json_lines_files_+3A_set_column_types">set_column_types</code></td>
<td>
<p>A named list to set column data type, where key is
column name and value is data type.</p>
</td></tr>
<tr><td><code id="create_tabular_dataset_from_json_lines_files_+3A_partition_format">partition_format</code></td>
<td>
<p>Specify the partition format in path and create string columns from
format 'x' and datetime column from format 'x:yyyy/MM/dd/HH/mm/ss', where 'yyyy', 'MM',
'dd', 'HH', 'mm' and 'ss' are used to extrat year, month, day, hour, minute and second for the datetime
type. The format should start from the postition of first partition key until the end of file path.
For example, given a file path '../USA/2019/01/01/data.csv' and data is partitioned by country and time,
we can define '/Country/PartitionDate:yyyy/MM/dd/data.csv' to create columns 'Country'
of string type and 'PartitionDate' of datetime type.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The Tabular Dataset object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+data_path">data_path</a></code>
</p>

<hr>
<h2 id='create_tabular_dataset_from_parquet_files'>Create an unregistered, in-memory Dataset from parquet files.</h2><span id='topic+create_tabular_dataset_from_parquet_files'></span>

<h3>Description</h3>

<p>Create an unregistered, in-memory Dataset from parquet files.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_tabular_dataset_from_parquet_files(
  path,
  validate = TRUE,
  include_path = FALSE,
  set_column_types = NULL,
  partition_format = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_tabular_dataset_from_parquet_files_+3A_path">path</code></td>
<td>
<p>A data path in a registered datastore or a local path.</p>
</td></tr>
<tr><td><code id="create_tabular_dataset_from_parquet_files_+3A_validate">validate</code></td>
<td>
<p>Boolean to validate if data can be loaded from the returned dataset.
Defaults to True. Validation requires that the data source is accessible from the
current compute.</p>
</td></tr>
<tr><td><code id="create_tabular_dataset_from_parquet_files_+3A_include_path">include_path</code></td>
<td>
<p>Whether to include a column containing the path of the file
from which the data was read. This is useful when you are reading multiple files,
and want to know which file a particular record originated from, or to keep useful
information in file path.</p>
</td></tr>
<tr><td><code id="create_tabular_dataset_from_parquet_files_+3A_set_column_types">set_column_types</code></td>
<td>
<p>A named list to set column data type, where key is
column name and value is data type.</p>
</td></tr>
<tr><td><code id="create_tabular_dataset_from_parquet_files_+3A_partition_format">partition_format</code></td>
<td>
<p>Specify the partition format in path and create string columns from
format 'x' and datetime column from format 'x:yyyy/MM/dd/HH/mm/ss', where 'yyyy', 'MM',
'dd', 'HH', 'mm' and 'ss' are used to extrat year, month, day, hour, minute and second for the datetime
type. The format should start from the postition of first partition key until the end of file path.
For example, given a file path '../USA/2019/01/01/data.csv' and data is partitioned by country and time,
we can define '/Country/PartitionDate:yyyy/MM/dd/data.csv' to create columns 'Country'
of string type and 'PartitionDate' of datetime type.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The Tabular Dataset object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+data_path">data_path</a></code>
</p>

<hr>
<h2 id='create_tabular_dataset_from_sql_query'>Create a TabularDataset to represent tabular data in SQL databases.</h2><span id='topic+create_tabular_dataset_from_sql_query'></span>

<h3>Description</h3>

<p>Create a TabularDataset to represent tabular data in SQL databases.
&ldquo;from_sql_query&ldquo;' creates a Tabular Dataset object , which defines the operations to
load data from SQL databases into tabular representation. For the data to be accessible
by Azure Machine Learning, the SQL database specified by <code>query</code> must be located in
a Datastore and the datastore type must be of a SQL kind. Column data types are
read from data types in SQL query result. Providing 'set_column_types' will
override the data type  for the specified columns in the returned Tabular Dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_tabular_dataset_from_sql_query(
  query,
  validate = TRUE,
  set_column_types = NULL,
  query_timeout = 30L
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_tabular_dataset_from_sql_query_+3A_query">query</code></td>
<td>
<p>A SQL-kind datastore and a query</p>
</td></tr>
<tr><td><code id="create_tabular_dataset_from_sql_query_+3A_validate">validate</code></td>
<td>
<p>Boolean to validate if data can be loaded from the returned dataset.
Defaults to True. Validation requires that the data source is accessible from
the current compute.</p>
</td></tr>
<tr><td><code id="create_tabular_dataset_from_sql_query_+3A_set_column_types">set_column_types</code></td>
<td>
<p>A named list to set column data type, where key is
column name and value is data type.</p>
</td></tr>
<tr><td><code id="create_tabular_dataset_from_sql_query_+3A_query_timeout">query_timeout</code></td>
<td>
<p>Sets the wait time (as an int, in seconds) before terminating the attempt to execute a command
and generating an error. The default is 30 seconds.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>TabularDataset</code> object
</p>


<h3>Examples</h3>

<pre># create tabular dataset from a SQL database in datastore
datastore &lt;- get_datastore(ws, 'sql-db')
query &lt;- data_path(datastore, 'SELECT * FROM my_table')
tab_ds &lt;- create_tabular_dataset_from_sql_query(query, query_timeout = 10)

# use `set_column_types` param to set column data types
data_types &lt;- list(ID = data_type_string(),
                   Date = data_type_datetime('%d/%m/%Y %I:%M:%S %p'),
                   Count = data_type_long(),
                   Latitude = data_type_double(),
                   Found = data_type_bool())

set_tab_ds &lt;- create_tabular_dataset_from_sql_query(query, set_column_types = data_types)
</pre>


<h3>See Also</h3>

<p><code><a href="#topic+data_path">data_path()</a></code> <code><a href="#topic+data_type_datetime">data_type_datetime()</a></code> <code><a href="#topic+data_type_bool">data_type_bool()</a></code>
<code><a href="#topic+data_type_double">data_type_double()</a></code> <code><a href="#topic+data_type_string">data_type_string()</a></code> <code><a href="#topic+data_type_long">data_type_long()</a></code>
</p>

<hr>
<h2 id='create_workspace'>Create a new Azure Machine Learning workspace</h2><span id='topic+create_workspace'></span>

<h3>Description</h3>

<p>Create a new Azure Machine Learning workspace. Throws an exception if the
workspace already exists or any of the workspace requirements are not
satisfied. When you create new workspace, it automatically creates several
Azure resources that are used in the workspace:
</p>

<ul>
<li><p> Azure Container Registry: Registers Docker containers that you use during
training and when you deploy a model. To minimize costs, ACR is
lazy-loaded until deployment images are created.
</p>
</li>
<li><p> Azure Storage account: Used as the default datastore for the workspace.
</p>
</li>
<li><p> Azure Application Insights: Stores monitoring information about your
models.
</p>
</li>
<li><p> Azure Key Vault: Stores secrets that are used by compute targets and other
sensitive information that's needed by the workspace.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>create_workspace(
  name,
  auth = NULL,
  subscription_id = NULL,
  resource_group = NULL,
  location = NULL,
  create_resource_group = TRUE,
  friendly_name = NULL,
  storage_account = NULL,
  key_vault = NULL,
  app_insights = NULL,
  container_registry = NULL,
  cmk_keyvault = NULL,
  resource_cmk_uri = NULL,
  hbi_workspace = FALSE,
  exist_ok = FALSE,
  show_output = TRUE,
  sku = "basic"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_workspace_+3A_name">name</code></td>
<td>
<p>A string of the new workspace name. Workspace name has to be
between 2 and 32 characters of letters and numbers.</p>
</td></tr>
<tr><td><code id="create_workspace_+3A_auth">auth</code></td>
<td>
<p>The <code>ServicePrincipalAuthentication</code> or <code>InteractiveLoginAuthentication</code>
object. For more details refer to https://aka.ms/aml-notebook-auth. If NULL,
the default Azure CLI credentials will be used or the API will prompt for credentials.</p>
</td></tr>
<tr><td><code id="create_workspace_+3A_subscription_id">subscription_id</code></td>
<td>
<p>A string of the subscription ID of the containing
subscription for the new workspace. The parameter is required if the user has
access to more than one subscription.</p>
</td></tr>
<tr><td><code id="create_workspace_+3A_resource_group">resource_group</code></td>
<td>
<p>A string of the Azure resource group that is containing
the workspace. The parameter defaults to a mutation of the workspace name.</p>
</td></tr>
<tr><td><code id="create_workspace_+3A_location">location</code></td>
<td>
<p>A string of the location of the workspace. The parameter
defaults to the resource group location. The location has to be a supported
region for Azure Machine Learning Services.</p>
</td></tr>
<tr><td><code id="create_workspace_+3A_create_resource_group">create_resource_group</code></td>
<td>
<p>If <code>TRUE</code> the resource group will be created
if it doesn't exist.</p>
</td></tr>
<tr><td><code id="create_workspace_+3A_friendly_name">friendly_name</code></td>
<td>
<p>A string of the friendly name for the workspace that
can be displayed in the UI.</p>
</td></tr>
<tr><td><code id="create_workspace_+3A_storage_account">storage_account</code></td>
<td>
<p>A string of an existing storage account in the Azure
resource ID format. The storage will be used by the workspace to save run
outputs, code, logs etc. If <code>NULL</code> a new storage will be created.</p>
</td></tr>
<tr><td><code id="create_workspace_+3A_key_vault">key_vault</code></td>
<td>
<p>A string of an existing key vault in the Azure resource ID
format. The key vault will be used by the workspace to store credentials
added to the workspace by the users. If <code>NULL</code> a new key vault will be
created.</p>
</td></tr>
<tr><td><code id="create_workspace_+3A_app_insights">app_insights</code></td>
<td>
<p>A string of an existing Application Insights in the Azure
resource ID format. The Application Insights will be used by the workspace to
log webservices events. If <code>NULL</code> a new Application Insights will be created.</p>
</td></tr>
<tr><td><code id="create_workspace_+3A_container_registry">container_registry</code></td>
<td>
<p>A string of an existing container registry in the
Azure resource ID format. The container registry will be used by the
workspace to pull and push both experimentation and webservices images. If
<code>NULL</code> a new container registry will be created.</p>
</td></tr>
<tr><td><code id="create_workspace_+3A_cmk_keyvault">cmk_keyvault</code></td>
<td>
<p>A string representing the key vault containing the customer
managed key in the Azure resource ID format:
'/subscriptions//resourcegroups//providers/microsoft.keyvault/vaults/'. For
example: '/subscriptions/d139f240-94e6-4175-87a7-954b9d27db16/resourcegroups/myresourcegroup/providers/microsoft.keyvault/vaults/mykeyvault'.</p>
</td></tr>
<tr><td><code id="create_workspace_+3A_resource_cmk_uri">resource_cmk_uri</code></td>
<td>
<p>The key URI of the customer managed key to encrypt the data at rest.
The URI format is: 'https://&lt;keyvault-dns-name&gt;/keys/&lt;key-name&gt;/&lt;key-version&gt;'.
For example, 'https://mykeyvault.vault.azure.net/keys/mykey/bc5dce6d01df49w2na7ffb11a2ee008b'.
Refer to https://docs.microsoft.com/azure-stack/user/azure-stack-key-vault-manage-portal for steps on how
to create a key and get its URI.</p>
</td></tr>
<tr><td><code id="create_workspace_+3A_hbi_workspace">hbi_workspace</code></td>
<td>
<p>Specifies whether the customer data is of High Business
Impact(HBI), i.e., contains sensitive business information. The default value
is FALSE. When set to TRUE, downstream services will selectively disable logging.</p>
</td></tr>
<tr><td><code id="create_workspace_+3A_exist_ok">exist_ok</code></td>
<td>
<p>If <code>TRUE</code> the method will not fail if the workspace already
exists.</p>
</td></tr>
<tr><td><code id="create_workspace_+3A_show_output">show_output</code></td>
<td>
<p>If <code>TRUE</code> the method will print out incremental progress
of method.</p>
</td></tr>
<tr><td><code id="create_workspace_+3A_sku">sku</code></td>
<td>
<p>A string indicating if the workspace will be &quot;basic&quot; or
&quot;enterprise&quot; edition.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>Workspace</code> object.
</p>


<h3>Examples</h3>

<p>This example requires only minimal specification, and all dependent
resources as well as the resource group will be created automatically.</p>
<pre>ws &lt;- create_workspace(name = 'myworkspace',
                       subscription_id = '&lt;azure-subscription-id&gt;',
                       resource_group = 'myresourcegroup',
                       location = 'eastus2')
</pre>
<p>This example shows how to reuse existing Azure resources by making
use of all parameters utilizing the Azure resource ID format. The specific
Azure resource IDs can be retrieved through the Azure Portal or SDK. This
assumes that the resource group, storage account, key vault, App Insights
and container registry already exist.</p>
<pre>prefix = "subscriptions/&lt;azure-subscription-id&gt;/resourcegroups/myresourcegroup/providers/"
ws &lt;- create_workspace(
       name = 'myworkspace',
       subscription_id = '&lt;azure-subscription-id&gt;',
       resource_group = 'myresourcegroup',
       create_resource_group = FALSE,
       location = 'eastus2',
       friendly_name = 'My workspace',
       storage_account = paste0(prefix, 'microsoft.storage/storageaccounts/mystorageaccount'),
       key_vault = paste0(prefix, 'microsoft.keyvault/vaults/mykeyvault'),
       app_insights = paste0(prefix, 'microsoft.insights/components/myappinsights'),
       container_registry = paste0(
         prefix,
         'microsoft.containerregistry/registries/mycontainerregistry'))
</pre>


<h3>See Also</h3>

<p><code><a href="#topic+get_workspace">get_workspace()</a></code> <code><a href="#topic+service_principal_authentication">service_principal_authentication()</a></code> <code><a href="#topic+interactive_login_authentication">interactive_login_authentication()</a></code>
</p>

<hr>
<h2 id='data_path'>Represents a path to data in a datastore.</h2><span id='topic+data_path'></span>

<h3>Description</h3>

<p>The path represented by DataPath object can point to a directory or a data artifact (blob, file).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_path(datastore, path_on_datastore = NULL, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="data_path_+3A_datastore">datastore</code></td>
<td>
<p>The Datastore to reference.</p>
</td></tr>
<tr><td><code id="data_path_+3A_path_on_datastore">path_on_datastore</code></td>
<td>
<p>The relative path in the backing storage for the data reference.</p>
</td></tr>
<tr><td><code id="data_path_+3A_name">name</code></td>
<td>
<p>An optional name for the DataPath.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>DataPath</code> object.
</p>


<h3>Examples</h3>

<pre>my_data &lt;- register_azure_blob_container_datastore(
    workspace = ws,
    datastore_name = blob_datastore_name,
    container_name = ws_blob_datastore$container_name,
    account_name = ws_blob_datastore$account_name,
    account_key = ws_blob_datastore$account_key,
    create_if_not_exists = TRUE)

datapath &lt;- data_path(my_data, &lt;path_on_my_datastore&gt;)
dataset &lt;- create_file_dataset_from_files(datapath)
</pre>


<h3>See Also</h3>

<p><code><a href="#topic+create_file_dataset_from_files">create_file_dataset_from_files</a></code>
<code><a href="#topic+create_tabular_dataset_from_parquet_files">create_tabular_dataset_from_parquet_files</a></code>
<code><a href="#topic+create_tabular_dataset_from_delimited_files">create_tabular_dataset_from_delimited_files</a></code>
<code><a href="#topic+create_tabular_dataset_from_json_lines_files">create_tabular_dataset_from_json_lines_files</a></code>
<code><a href="#topic+create_tabular_dataset_from_sql_query">create_tabular_dataset_from_sql_query</a></code>
</p>

<hr>
<h2 id='data_type_bool'>Configure conversion to bool.</h2><span id='topic+data_type_bool'></span>

<h3>Description</h3>

<p>Configure conversion to bool.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_type_bool()
</code></pre>


<h3>Value</h3>

<p>Converted DataType object.
</p>

<hr>
<h2 id='data_type_datetime'>Configure conversion to datetime.</h2><span id='topic+data_type_datetime'></span>

<h3>Description</h3>

<p>Configure conversion to datetime.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_type_datetime(formats = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="data_type_datetime_+3A_formats">formats</code></td>
<td>
<p>Formats to try for datetime conversion. For example <code style="white-space: pre;">&#8288;%d-%m-%Y&#8288;</code> for data in &quot;day-month-year&quot;,
and <code style="white-space: pre;">&#8288;%Y-%m-%dT%H:%M:%S.%f&#8288;</code> for &quot;combined date an time representation&quot; according to ISO 8601.
</p>

<ul>
<li><p> %Y: Year with 4 digits
</p>
</li>
<li><p> %y: Year with 2 digits
</p>
</li>
<li><p> %m: Month in digits
</p>
</li>
<li><p> %b: Month represented by its abbreviated name in 3 letters, like Aug
</p>
</li>
<li><p> %B: Month represented by its full name, like August
</p>
</li>
<li><p> %d: Day in digits
</p>
</li>
<li><p> %H: Hour as represented in 24-hour clock time
</p>
</li>
<li><p> %I: Hour as represented in 12-hour clock time
</p>
</li>
<li><p> %M: Minute in 2 digits
</p>
</li>
<li><p> %S: Second in 2 digits
</p>
</li>
<li><p> %f: Microsecond
</p>
</li>
<li><p> %p: AM/PM designator
</p>
</li>
<li><p> %z: Timezone, for example: -0700
</p>
</li></ul>

<p>Format specifiers will be inferred if not specified.
Inference requires that the data source is accessible from current compute.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Converted DataType object.
</p>

<hr>
<h2 id='data_type_double'>Configure conversion to 53-bit double.</h2><span id='topic+data_type_double'></span>

<h3>Description</h3>

<p>Configure conversion to 53-bit double.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_type_double()
</code></pre>


<h3>Value</h3>

<p>Converted DataType object.
</p>

<hr>
<h2 id='data_type_long'>Configure conversion to 64-bit integer.</h2><span id='topic+data_type_long'></span>

<h3>Description</h3>

<p>Configure conversion to 64-bit integer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_type_long()
</code></pre>


<h3>Value</h3>

<p>Converted DataType object.
</p>

<hr>
<h2 id='data_type_string'>Configure conversion to string.</h2><span id='topic+data_type_string'></span>

<h3>Description</h3>

<p>Configure conversion to string.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_type_string()
</code></pre>


<h3>Value</h3>

<p>Converted DataType object.
</p>

<hr>
<h2 id='dataset_consumption_config'>Represent how to deliver the dataset to a compute target.</h2><span id='topic+dataset_consumption_config'></span>

<h3>Description</h3>

<p>Represent how to deliver the dataset to a compute target.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_consumption_config(
  name,
  dataset,
  mode = "direct",
  path_on_compute = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dataset_consumption_config_+3A_name">name</code></td>
<td>
<p>The name of the dataset in the run, which can be different to the
registered name. The name will be registered as environment variable and can
be used in data plane.</p>
</td></tr>
<tr><td><code id="dataset_consumption_config_+3A_dataset">dataset</code></td>
<td>
<p>The dataset that will be consumed in the run.</p>
</td></tr>
<tr><td><code id="dataset_consumption_config_+3A_mode">mode</code></td>
<td>
<p>Defines how the dataset should be delivered to the compute target. There are three modes:
</p>
<p>'direct': consume the dataset as dataset.
'download': download the dataset and consume the dataset as downloaded path.
'mount': mount the dataset and consume the dataset as mount path.</p>
</td></tr>
<tr><td><code id="dataset_consumption_config_+3A_path_on_compute">path_on_compute</code></td>
<td>
<p>The target path on the compute to make the data available at.
The folder structure of the source data will be kept, however, we might add prefixes
to this folder structure to avoid collision.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>DatasetConsumptionConfig</code> object.
</p>


<h3>Examples</h3>

<pre>est &lt;- estimator(source_directory = ".",
                 entry_script = "train.R",
                 inputs = list(dataset_consumption_config('mydataset', dataset, mode = 'download')),
                 compute_target = compute_target)
</pre>


<h3>See Also</h3>

<p><code><a href="#topic+estimator">estimator</a></code>
</p>

<hr>
<h2 id='define_timestamp_columns_for_dataset'>Define timestamp columns for the dataset.</h2><span id='topic+define_timestamp_columns_for_dataset'></span>

<h3>Description</h3>

<p>Define timestamp columns for the dataset.
The method defines columns to be used as timestamps. Timestamp columns on a dataset
make it possible to treat the data as time-series data and enable additional capabilities.
When a dataset has both <code>fine_grain_timestamp</code> and <code style="white-space: pre;">&#8288;coarse_grain_timestamp defined&#8288;</code>
specified, the two columns should represent the same timeline.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>define_timestamp_columns_for_dataset(
  dataset,
  fine_grain_timestamp,
  coarse_grain_timestamp = NULL,
  validate = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="define_timestamp_columns_for_dataset_+3A_dataset">dataset</code></td>
<td>
<p>The Tabular Dataset object.</p>
</td></tr>
<tr><td><code id="define_timestamp_columns_for_dataset_+3A_fine_grain_timestamp">fine_grain_timestamp</code></td>
<td>
<p>The name of column as fine grain timestamp. Use None to clear it.</p>
</td></tr>
<tr><td><code id="define_timestamp_columns_for_dataset_+3A_coarse_grain_timestamp">coarse_grain_timestamp</code></td>
<td>
<p>The name of column coarse grain timestamp (optional).
The default is None.</p>
</td></tr>
<tr><td><code id="define_timestamp_columns_for_dataset_+3A_validate">validate</code></td>
<td>
<p>Indicates whether to validate if specified columns exist in dataset.
The default is False. Validation requires that the data source is accessible
from the current compute.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The Tabular Dataset with timestamp columns defined.
</p>

<hr>
<h2 id='delete_compute'>Delete a cluster</h2><span id='topic+delete_compute'></span>

<h3>Description</h3>

<p>Remove the compute object from its associated workspace and delete the
corresponding cloud-based resource.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>delete_compute(cluster)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="delete_compute_+3A_cluster">cluster</code></td>
<td>
<p>The <code>AmlCompute</code> or <code>AksCompute</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
ws &lt;- load_workspace_from_config()
compute_target &lt;- get_compute(ws, cluster_name = 'mycluster')
delete_compute(compute_target)

## End(Not run)
</code></pre>

<hr>
<h2 id='delete_local_webservice'>Delete a local web service from the local machine</h2><span id='topic+delete_local_webservice'></span>

<h3>Description</h3>

<p>Delete a local web service from the local machine. This function call
is not asynchronous; it runs until the service is deleted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>delete_local_webservice(webservice, delete_cache = TRUE, delete_image = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="delete_local_webservice_+3A_webservice">webservice</code></td>
<td>
<p>The <code>LocalWebservice</code> object.</p>
</td></tr>
<tr><td><code id="delete_local_webservice_+3A_delete_cache">delete_cache</code></td>
<td>
<p>If <code>TRUE</code>, delete the temporary files cached for
the service.</p>
</td></tr>
<tr><td><code id="delete_local_webservice_+3A_delete_image">delete_image</code></td>
<td>
<p>If <code>TRUE</code>, delete the service's Docker image.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>

<hr>
<h2 id='delete_model'>Delete a model from its associated workspace</h2><span id='topic+delete_model'></span>

<h3>Description</h3>

<p>Delete the registered model from its associated workspace. Note that
you cannot delete a registered model that is being used by an active
web service deployment.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>delete_model(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="delete_model_+3A_model">model</code></td>
<td>
<p>The <code>Model</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>

<hr>
<h2 id='delete_secrets'>Delete secrets from a keyvault</h2><span id='topic+delete_secrets'></span>

<h3>Description</h3>

<p>Delete secrets from the keyvault associated with the workspace for
a specified set of secret names.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>delete_secrets(keyvault, secrets)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="delete_secrets_+3A_keyvault">keyvault</code></td>
<td>
<p>The <code>Keyvault</code> object.</p>
</td></tr>
<tr><td><code id="delete_secrets_+3A_secrets">secrets</code></td>
<td>
<p>A vector of secret names.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>

<hr>
<h2 id='delete_webservice'>Delete a web service from a given workspace</h2><span id='topic+delete_webservice'></span>

<h3>Description</h3>

<p>Delete a deployed ACI or AKS web service from the given workspace.
This function call is not asynchronous; it runs until the resource is
deleted.
</p>
<p>To delete a <code>LocalWebservice</code> see <code>delete_local_webservice()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>delete_webservice(webservice)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="delete_webservice_+3A_webservice">webservice</code></td>
<td>
<p>The <code>AciWebservice</code> or <code>AksWebservice</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>

<hr>
<h2 id='delete_workspace'>Delete a workspace</h2><span id='topic+delete_workspace'></span>

<h3>Description</h3>

<p>Delete the Azure Machine Learning workspace resource. <code>delete_workspace()</code>
can also delete the workspace's associated resources.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>delete_workspace(
  workspace,
  delete_dependent_resources = FALSE,
  no_wait = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="delete_workspace_+3A_workspace">workspace</code></td>
<td>
<p>The <code>Workspace</code> object of the workspace to delete.</p>
</td></tr>
<tr><td><code id="delete_workspace_+3A_delete_dependent_resources">delete_dependent_resources</code></td>
<td>
<p>If <code>TRUE</code> the workspace's associated
resources, i.e. ACR, storage account, key value, and application insights
will also be deleted.</p>
</td></tr>
<tr><td><code id="delete_workspace_+3A_no_wait">no_wait</code></td>
<td>
<p>If <code>FALSE</code> do not wait for the workspace deletion to complete.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>

<hr>
<h2 id='deploy_model'>Deploy a web service from registered model(s)</h2><span id='topic+deploy_model'></span>

<h3>Description</h3>

<p>Deploy a web service from zero or more registered models. Types of web
services that can be deployed are <code>LocalWebservice</code>, which will deploy
a model locally, and <code>AciWebservice</code> and <code>AksWebservice</code>, which will
deploy a model to Azure Container Instances (ACI) and Azure Kubernetes
Service (AKS), respectively.The type of web service deployed will be
determined by the <code>deployment_config</code> specified. Returns a <code>Webservice</code>
object corresponding to the deployed web service.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deploy_model(
  workspace,
  name,
  models,
  inference_config,
  deployment_config = NULL,
  deployment_target = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="deploy_model_+3A_workspace">workspace</code></td>
<td>
<p>The <code>Workspace</code> object.</p>
</td></tr>
<tr><td><code id="deploy_model_+3A_name">name</code></td>
<td>
<p>A string of the name to give the deployed service. Must be
unique to the workspace, only consist of lowercase letters, numbers, or
dashes, start with a letter, and be between 3 and 32 characters long.</p>
</td></tr>
<tr><td><code id="deploy_model_+3A_models">models</code></td>
<td>
<p>A list of <code>Model</code> objects. Can be an empty list.</p>
</td></tr>
<tr><td><code id="deploy_model_+3A_inference_config">inference_config</code></td>
<td>
<p>The <code>InferenceConfig</code> object used to describe
how to configure the model to make predictions.</p>
</td></tr>
<tr><td><code id="deploy_model_+3A_deployment_config">deployment_config</code></td>
<td>
<p>The deployment configuration of type
<code>LocalWebserviceDeploymentConfiguration</code>,
<code>AciServiceDeploymentConfiguration</code>, or
<code>AksServiceDeploymentConfiguration</code> used to configure the web service.
The deployment configuration is specific to the compute target that will
host the web service. For example, when you deploy a model locally, you
must specify the port where the service accepts requests. If <code>NULL</code>, an
empty configuration object will be used based on the desired target
specified by <code>deployment_target</code>.</p>
</td></tr>
<tr><td><code id="deploy_model_+3A_deployment_target">deployment_target</code></td>
<td>
<p>The compute target to deploy the model to.
You will only need to specify this parameter if you are deploy to AKS,
in which case provide an <code>AksCompute</code> object. If you are deploying locally
or to ACI, leave this parameter as <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>LocalWebservice</code>, <code>AciWebservice</code>, or <code>AksWebservice</code> object.
</p>


<h3>Details</h3>

<p>If you encounter any issue in deploying your web service, please visit this
<a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-troubleshoot-deployment">troubleshooting guide</a>.
</p>


<h3>See Also</h3>

<p><code>inference_config()</code>, <code>aci_webservice_deployment_config()</code>,
<code>aks_webservice_deployment_config()</code>, <code>local_webservice_deployment_config()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
ws &lt;- load_workspace_from_config()
model &lt;- get_model(ws, name = "my_model")
r_env &lt;- r_environment(name = "r_env")
inference_config &lt;- inference_config(entry_script = "score.R",
                                     source_directory = ".",
                                     environment = r_env)
deployment_config &lt;- aci_webservice_deployment_config(cpu_cores = 1, memory_gb = 1)
service &lt;- deploy_model(ws,
                        name = "my_webservice",
                        models = list(model),
                        inference_config = inference_config,
                        deployment_config = deployment_config)
wait_for_deployment(service, show_output = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='detach_aks_compute'>Detach an AksCompute cluster from its associated workspace</h2><span id='topic+detach_aks_compute'></span>

<h3>Description</h3>

<p>Detach the AksCompute cluster from its associated workspace. No
underlying cloud resource will be deleted; the association will
just be removed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>detach_aks_compute(cluster)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="detach_aks_compute_+3A_cluster">cluster</code></td>
<td>
<p>The <code>AksCompute</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>

<hr>
<h2 id='download_file_from_run'>Download a file from a run</h2><span id='topic+download_file_from_run'></span>

<h3>Description</h3>

<p>Download a file from the run record. You can download any file that
was uploaded to the run record via <code>upload_files_to_run()</code> or
<code>upload_folder_to_run()</code>, or any file that was written out to
the <code>./outputs</code> or <code>./logs</code> folders during a run.
</p>
<p>You can see what files are available to download from the run record
by calling <code>get_run_file_names()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>download_file_from_run(run, name, output_file_path = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="download_file_from_run_+3A_run">run</code></td>
<td>
<p>The <code>Run</code> object.</p>
</td></tr>
<tr><td><code id="download_file_from_run_+3A_name">name</code></td>
<td>
<p>A string of the name of the artifact to be downloaded.</p>
</td></tr>
<tr><td><code id="download_file_from_run_+3A_output_file_path">output_file_path</code></td>
<td>
<p>A string of the local path where to download
the artifact to.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>


<h3>See Also</h3>

<p><code><a href="#topic+download_files_from_run">download_files_from_run()</a></code>
</p>

<hr>
<h2 id='download_files_from_run'>Download files from a run</h2><span id='topic+download_files_from_run'></span>

<h3>Description</h3>

<p>Download files from the run record. You can download any files that
were uploaded to the run record via <code>upload_files_to_run()</code> or
<code>upload_folder_to_run()</code>, or any files that were written out to
the <code>./outputs</code> or <code>./logs</code> folders during a run.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>download_files_from_run(
  run,
  prefix = NULL,
  output_directory = NULL,
  output_paths = NULL,
  batch_size = 100L
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="download_files_from_run_+3A_run">run</code></td>
<td>
<p>The <code>Run</code> object.</p>
</td></tr>
<tr><td><code id="download_files_from_run_+3A_prefix">prefix</code></td>
<td>
<p>A string of the the filepath prefix (folder name) from
which to download all artifacts. If not specified, all the artifacts
in the run record will be downloaded.</p>
</td></tr>
<tr><td><code id="download_files_from_run_+3A_output_directory">output_directory</code></td>
<td>
<p>(Optional) A string of the directory that all
artifact paths use as a prefix.</p>
</td></tr>
<tr><td><code id="download_files_from_run_+3A_output_paths">output_paths</code></td>
<td>
<p>(Optional) A list of strings of the local filepaths
where the artifacts will be downloaded to.</p>
</td></tr>
<tr><td><code id="download_files_from_run_+3A_batch_size">batch_size</code></td>
<td>
<p>An int of the number of files to download per batch.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>


<h3>See Also</h3>

<p><code><a href="#topic+download_file_from_run">download_file_from_run()</a></code>
</p>

<hr>
<h2 id='download_from_datastore'>Download data from a datastore to the local file system</h2><span id='topic+download_from_datastore'></span>

<h3>Description</h3>

<p>Download data from the datastore to the local file system.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>download_from_datastore(
  datastore,
  target_path,
  prefix = NULL,
  overwrite = FALSE,
  show_progress = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="download_from_datastore_+3A_datastore">datastore</code></td>
<td>
<p>The <code>AzureBlobDatastore</code> or <code>AzureFileDatastore</code> object.</p>
</td></tr>
<tr><td><code id="download_from_datastore_+3A_target_path">target_path</code></td>
<td>
<p>A string of the local directory to download the file to.</p>
</td></tr>
<tr><td><code id="download_from_datastore_+3A_prefix">prefix</code></td>
<td>
<p>A string of the path to the folder in the blob container
or file store to download. If <code>NULL</code>, will download everything in the blob
container or file share</p>
</td></tr>
<tr><td><code id="download_from_datastore_+3A_overwrite">overwrite</code></td>
<td>
<p>If <code>TRUE</code>, overwrites any existing data at <code>target_path</code>.</p>
</td></tr>
<tr><td><code id="download_from_datastore_+3A_show_progress">show_progress</code></td>
<td>
<p>If <code>TRUE</code>, show progress of upload in the console.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An integer of the number of files successfully downloaded.
</p>

<hr>
<h2 id='download_from_file_dataset'>Download file streams defined by the dataset as local files.</h2><span id='topic+download_from_file_dataset'></span>

<h3>Description</h3>

<p>Download file streams defined by the dataset as local files. If target_path starts
with a /, then it will be treated as an absolute path. If it doesn't start with a /,
then it will be treated as a relative path relative to the current working directory.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>download_from_file_dataset(dataset, target_path = NULL, overwrite = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="download_from_file_dataset_+3A_dataset">dataset</code></td>
<td>
<p>The Dataset object</p>
</td></tr>
<tr><td><code id="download_from_file_dataset_+3A_target_path">target_path</code></td>
<td>
<p>The local directory to download the files to. If NULL,
the data will be downloaded into a temporary directory.</p>
</td></tr>
<tr><td><code id="download_from_file_dataset_+3A_overwrite">overwrite</code></td>
<td>
<p>Indicates whether to overwirte existing files. The default
is FALSE. Existing files will be overwritten if <code>overwrite</code> is set to TRUE;
otherwise an exception will be raised.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of file paths for each file downloaded.
</p>

<hr>
<h2 id='download_model'>Download a model to the local file system</h2><span id='topic+download_model'></span>

<h3>Description</h3>

<p>Download a registered model to the <code>target_dir</code> of your local file
system.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>download_model(model, target_dir = ".", exist_ok = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="download_model_+3A_model">model</code></td>
<td>
<p>The <code>Model</code> object.</p>
</td></tr>
<tr><td><code id="download_model_+3A_target_dir">target_dir</code></td>
<td>
<p>A string of the path to the directory on your local
file system for where to download the model to. Defaults to &quot;.&quot;.</p>
</td></tr>
<tr><td><code id="download_model_+3A_exist_ok">exist_ok</code></td>
<td>
<p>If <code>FALSE</code>, replace the downloaded folder/file if they
already exist.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A string of the path to the file or folder of the downloaded
model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
ws &lt;- load_workspace_from_config()
model &lt;- get_model(ws, name = "my_model", version = 2)
download_model(model, target_dir = tempdir(), exist_ok = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='drop_columns_from_dataset'>Drop the specified columns from the dataset.</h2><span id='topic+drop_columns_from_dataset'></span>

<h3>Description</h3>

<p>Drop the specified columns from the dataset. If a timeseries column is dropped,
the corresponding capabilities will be dropped for the returned dataset as well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>drop_columns_from_dataset(dataset, columns)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="drop_columns_from_dataset_+3A_dataset">dataset</code></td>
<td>
<p>The Tabular Dataset object.</p>
</td></tr>
<tr><td><code id="drop_columns_from_dataset_+3A_columns">columns</code></td>
<td>
<p>A name or a list of names for the columns to drop.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new TabularDataset object with the specified columns dropped.
</p>

<hr>
<h2 id='estimator'>Create an estimator</h2><span id='topic+estimator'></span>

<h3>Description</h3>

<p>An Estimator wraps run configuration information for specifying details
of executing an R script. Running an Estimator experiment
(using <code>submit_experiment()</code>) will return a <code>ScriptRun</code> object and
execute your training script on the specified compute target.
</p>
<p>To define the environment to use for training, you can either directly
provide the environment-related parameters (e.g. <code>cran_packages</code>,
<code>custom_docker_image</code>) to <code>estimator()</code>, or you can provide an
<code>Environment</code> object to the <code>environment</code> parameter. For more information
on the predefined Docker images that are used for training if
<code>custom_docker_image</code> is not specified, see the documentation
<a href="https://azure.github.io/azureml-sdk-for-r/reference/r_environment.html#predefined-docker-images">here</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimator(
  source_directory,
  compute_target = NULL,
  vm_size = NULL,
  vm_priority = NULL,
  entry_script = NULL,
  script_params = NULL,
  cran_packages = NULL,
  github_packages = NULL,
  custom_url_packages = NULL,
  custom_docker_image = NULL,
  image_registry_details = NULL,
  use_gpu = FALSE,
  environment_variables = NULL,
  shm_size = NULL,
  max_run_duration_seconds = NULL,
  environment = NULL,
  inputs = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="estimator_+3A_source_directory">source_directory</code></td>
<td>
<p>A string of the local directory containing
experiment configuration and code files needed for the training job.</p>
</td></tr>
<tr><td><code id="estimator_+3A_compute_target">compute_target</code></td>
<td>
<p>The <code>AmlCompute</code> object for the compute target
where training will happen.</p>
</td></tr>
<tr><td><code id="estimator_+3A_vm_size">vm_size</code></td>
<td>
<p>A string of the VM size of the compute target that will be
created for the training job. The list of available VM sizes
are listed <a href="https://docs.microsoft.com/azure/cloud-services/cloud-services-sizes-specs">here</a>.
Provide this parameter if you want to create AmlCompute as the compute target
at run time, instead of providing an existing cluster to the <code>compute_target</code>
parameter. If <code>vm_size</code> is specified, a single-node cluster is automatically
created for your run and is deleted automatically once the run completes.</p>
</td></tr>
<tr><td><code id="estimator_+3A_vm_priority">vm_priority</code></td>
<td>
<p>A string of either <code>'dedicated'</code> or <code>'lowpriority'</code> to
specify the VM priority of the compute target that will be created for the
training job. Defaults to <code>'dedicated'</code>. This takes effect only when the
<code>vm_size</code> parameter is specified.</p>
</td></tr>
<tr><td><code id="estimator_+3A_entry_script">entry_script</code></td>
<td>
<p>A string representing the relative path to the file used
to start training.</p>
</td></tr>
<tr><td><code id="estimator_+3A_script_params">script_params</code></td>
<td>
<p>A named list of the command-line arguments to pass to
the training script specified in <code>entry_script</code>.</p>
</td></tr>
<tr><td><code id="estimator_+3A_cran_packages">cran_packages</code></td>
<td>
<p>A list of <code>cran_package</code> objects to be installed.</p>
</td></tr>
<tr><td><code id="estimator_+3A_github_packages">github_packages</code></td>
<td>
<p>A list of <code>github_package</code> objects to be installed.</p>
</td></tr>
<tr><td><code id="estimator_+3A_custom_url_packages">custom_url_packages</code></td>
<td>
<p>A character vector of packages to be installed
from local directory or custom URL.</p>
</td></tr>
<tr><td><code id="estimator_+3A_custom_docker_image">custom_docker_image</code></td>
<td>
<p>A string of the name of the Docker image from
which the image to use for training will be built. If not set, a predefined
image will be used as the base image. To use an image from a
private Docker repository, you will also have to specify the
<code>image_registry_details</code> parameter.</p>
</td></tr>
<tr><td><code id="estimator_+3A_image_registry_details">image_registry_details</code></td>
<td>
<p>A <code>ContainerRegistry</code> object of the details of
the Docker image registry for the custom Docker image.</p>
</td></tr>
<tr><td><code id="estimator_+3A_use_gpu">use_gpu</code></td>
<td>
<p>Indicates whether the environment to run the experiment should
support GPUs. If <code>TRUE</code>, a predefined GPU-based Docker image will be used in the
environment. If <code>FALSE</code>, a predefined CPU-based image will be used. Predefined
Docker images (CPU or GPU) will only be used if the <code>custom_docker_image</code> parameter
is not set.</p>
</td></tr>
<tr><td><code id="estimator_+3A_environment_variables">environment_variables</code></td>
<td>
<p>A named list of environment variables names
and values. These environment variables are set on the process where the user
script is being executed.</p>
</td></tr>
<tr><td><code id="estimator_+3A_shm_size">shm_size</code></td>
<td>
<p>A string for the size of the Docker container's shared
memory block. For more information, see
<a href="https://docs.docker.com/engine/reference/run/">Docker run reference</a>.
If not set, a default value of <code>'2g'</code> is used.</p>
</td></tr>
<tr><td><code id="estimator_+3A_max_run_duration_seconds">max_run_duration_seconds</code></td>
<td>
<p>An integer of the maximum allowed time for
the run. Azure ML will attempt to automatically cancel the run if it takes
longer than this value.</p>
</td></tr>
<tr><td><code id="estimator_+3A_environment">environment</code></td>
<td>
<p>The <code>Environment</code> object that configures the R
environment where the experiment is executed. This parameter is mutually
exclusive with the other environment-related parameters <code>custom_docker_image</code>
, <code>image_registry_details</code>, <code>use_gpu</code>, <code>environment_variables</code>, <code>shm_size</code>,
<code>cran_packages</code>, <code>github_packages</code>, and <code>custom_url_packages</code> and if set
will take precedence over those parameters.</p>
</td></tr>
<tr><td><code id="estimator_+3A_inputs">inputs</code></td>
<td>
<p>A list of DataReference objects or DatasetConsumptionConfig
objects to use as input.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>Estimator</code> object.
</p>


<h3>Examples</h3>

<pre>r_env &lt;- r_environment(name = "r-env",
                       cran_packages = list(cran_package("dplyr"),
                                            cran_package("ggplot2")))
est &lt;- estimator(source_directory = ".",
                 entry_script = "train.R",
                 compute_target = compute_target,
                 environment = r_env)
</pre>


<h3>See Also</h3>

<p><code><a href="#topic+r_environment">r_environment()</a></code>, <code><a href="#topic+container_registry">container_registry()</a></code>, <code><a href="#topic+submit_experiment">submit_experiment()</a></code>,
<code><a href="#topic+dataset_consumption_config">dataset_consumption_config()</a></code>, <code><a href="#topic+cran_package">cran_package()</a></code>
</p>

<hr>
<h2 id='experiment'>Create an Azure Machine Learning experiment</h2><span id='topic+experiment'></span>

<h3>Description</h3>

<p>An experiment is a grouping of many runs from a specified script.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>experiment(workspace, name)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="experiment_+3A_workspace">workspace</code></td>
<td>
<p>The <code>Workspace</code> object.</p>
</td></tr>
<tr><td><code id="experiment_+3A_name">name</code></td>
<td>
<p>A string of the experiment name. The name must be between
3-36 characters, start with a letter or number, and can only contain
letters, numbers, underscores, and dashes.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>Experiment</code> object.
</p>


<h3>See Also</h3>

<p><code>submit_experiment()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
ws &lt;- load_workspace_from_config()
exp &lt;- experiment(ws, name = 'myexperiment')

## End(Not run)
</code></pre>

<hr>
<h2 id='filter_dataset_after_time'>Filter Tabular Dataset with time stamp columns after a specified start time.</h2><span id='topic+filter_dataset_after_time'></span>

<h3>Description</h3>

<p>Filter Tabular Dataset with time stamp columns after a specified start time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>filter_dataset_after_time(dataset, start_time, include_boundary = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="filter_dataset_after_time_+3A_dataset">dataset</code></td>
<td>
<p>The Tabular Dataset object</p>
</td></tr>
<tr><td><code id="filter_dataset_after_time_+3A_start_time">start_time</code></td>
<td>
<p>The lower bound for filtering data.</p>
</td></tr>
<tr><td><code id="filter_dataset_after_time_+3A_include_boundary">include_boundary</code></td>
<td>
<p>Boolean indicating if the row associated with the
boundary time (<code>start_time</code>) should be included.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The filtered Tabular Dataset
</p>

<hr>
<h2 id='filter_dataset_before_time'>Filter Tabular Dataset with time stamp columns before a specified end time.</h2><span id='topic+filter_dataset_before_time'></span>

<h3>Description</h3>

<p>Filter Tabular Dataset with time stamp columns before a specified end time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>filter_dataset_before_time(dataset, end_time, include_boundary = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="filter_dataset_before_time_+3A_dataset">dataset</code></td>
<td>
<p>The Tabular Dataset object</p>
</td></tr>
<tr><td><code id="filter_dataset_before_time_+3A_end_time">end_time</code></td>
<td>
<p>The upper bound for filtering data.</p>
</td></tr>
<tr><td><code id="filter_dataset_before_time_+3A_include_boundary">include_boundary</code></td>
<td>
<p>Boolean indicating if the row associated with the
boundary time (<code>start_time</code>) should be included.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The filtered Tabular Dataset
</p>

<hr>
<h2 id='filter_dataset_between_time'>Filter Tabular Dataset between a specified start and end time.</h2><span id='topic+filter_dataset_between_time'></span>

<h3>Description</h3>

<p>Filter Tabular Dataset between a specified start and end time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>filter_dataset_between_time(
  dataset,
  start_time,
  end_time,
  include_boundary = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="filter_dataset_between_time_+3A_dataset">dataset</code></td>
<td>
<p>The Tabular Dataset object</p>
</td></tr>
<tr><td><code id="filter_dataset_between_time_+3A_start_time">start_time</code></td>
<td>
<p>The lower bound for filtering data.</p>
</td></tr>
<tr><td><code id="filter_dataset_between_time_+3A_end_time">end_time</code></td>
<td>
<p>The upper bound for filtering data.</p>
</td></tr>
<tr><td><code id="filter_dataset_between_time_+3A_include_boundary">include_boundary</code></td>
<td>
<p>Boolean indicating if the row associated with the
boundary time (<code>start_time</code> and <code>end_time</code>) should be included.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The filtered Tabular Dataset
</p>

<hr>
<h2 id='filter_dataset_from_recent_time'>Filter Tabular Dataset to contain only the specified duration (amount) of recent data.</h2><span id='topic+filter_dataset_from_recent_time'></span>

<h3>Description</h3>

<p>Filter Tabular Dataset to contain only the specified duration (amount) of recent data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>filter_dataset_from_recent_time(dataset, time_delta, include_boundary = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="filter_dataset_from_recent_time_+3A_dataset">dataset</code></td>
<td>
<p>The Tabular Dataset object</p>
</td></tr>
<tr><td><code id="filter_dataset_from_recent_time_+3A_time_delta">time_delta</code></td>
<td>
<p>The duration (amount) of recent data to retrieve.</p>
</td></tr>
<tr><td><code id="filter_dataset_from_recent_time_+3A_include_boundary">include_boundary</code></td>
<td>
<p>Boolean indicating if the row associated with the
boundary time (<code>time_delta</code>) should be included.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The filtered Tabular Dataset
</p>

<hr>
<h2 id='generate_entry_script'>Generates the control script for the experiment.</h2><span id='topic+generate_entry_script'></span>

<h3>Description</h3>

<p>Generates the control script for the experiment.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_entry_script(source_directory)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="generate_entry_script_+3A_source_directory">source_directory</code></td>
<td>
<p>The directory which contains all the files
needed for the experiment.</p>
</td></tr>
</table>

<hr>
<h2 id='generate_new_webservice_key'>Regenerate one of a web service's keys</h2><span id='topic+generate_new_webservice_key'></span>

<h3>Description</h3>

<p>Regenerate either the primary or secondary authentication key for
an <code>AciWebservice</code> or <code>AksWebservice</code>.The web service must have
been deployed with key-based authentication enabled.
</p>
<p>Not supported for <code>LocalWebservice</code> deployments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_new_webservice_key(webservice, key_type)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="generate_new_webservice_key_+3A_webservice">webservice</code></td>
<td>
<p>The <code>AciWebservice</code> or <code>AksWebservice</code> object.</p>
</td></tr>
<tr><td><code id="generate_new_webservice_key_+3A_key_type">key_type</code></td>
<td>
<p>A string of which key to regenerate. Options are
&quot;Primary&quot; or &quot;Secondary&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>

<hr>
<h2 id='get_aks_compute_credentials'>Get the credentials for an AksCompute cluster</h2><span id='topic+get_aks_compute_credentials'></span>

<h3>Description</h3>

<p>Retrieve the credentials for an AksCompute cluster.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_aks_compute_credentials(cluster)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_aks_compute_credentials_+3A_cluster">cluster</code></td>
<td>
<p>The <code>AksCompute</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Named list of the cluster details.
</p>

<hr>
<h2 id='get_best_run_by_primary_metric'>Return the best performing run amongst all completed runs</h2><span id='topic+get_best_run_by_primary_metric'></span>

<h3>Description</h3>

<p>Find and return the run that corresponds to the best performing run
amongst all the completed runs.
</p>
<p>The best performing run is identified solely based on the primary metric
parameter specified in the <code>HyperDriveConfig</code> (<code>primary_metric_name</code>).
The <code>PrimaryMetricGoal</code> governs whether the minimum or maximum of the
primary metric is used. To do a more detailed analysis of all the
run metrics launched by this HyperDrive run, use <code>get_child_run_metrics()</code>.
Only one of the runs is returned from <code>get_best_run_by_primary_metric()</code>,
even if several of the runs launched by this HyperDrive run reached
the same best metric.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_best_run_by_primary_metric(
  hyperdrive_run,
  include_failed = TRUE,
  include_canceled = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_best_run_by_primary_metric_+3A_hyperdrive_run">hyperdrive_run</code></td>
<td>
<p>The <code>HyperDriveRun</code> object.</p>
</td></tr>
<tr><td><code id="get_best_run_by_primary_metric_+3A_include_failed">include_failed</code></td>
<td>
<p>If <code>TRUE</code>, include the failed runs.</p>
</td></tr>
<tr><td><code id="get_best_run_by_primary_metric_+3A_include_canceled">include_canceled</code></td>
<td>
<p>If <code>TRUE</code>, include the canceled runs.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>Run</code> object.
</p>

<hr>
<h2 id='get_child_run_hyperparameters'>Get the hyperparameters for all child runs</h2><span id='topic+get_child_run_hyperparameters'></span>

<h3>Description</h3>

<p>Return the hyperparameters for all the child runs of the
HyperDrive run.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_child_run_hyperparameters(hyperdrive_run)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_child_run_hyperparameters_+3A_hyperdrive_run">hyperdrive_run</code></td>
<td>
<p>The <code>HyperDriveRun</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The named list of hyperparameters where element name
is the run_id, e.g. <code>list("run_id" = hyperparameters)</code>.
</p>

<hr>
<h2 id='get_child_run_metrics'>Get the metrics from all child runs</h2><span id='topic+get_child_run_metrics'></span>

<h3>Description</h3>

<p>Return the metrics from all the child runs of the
HyperDrive run.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_child_run_metrics(hyperdrive_run)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_child_run_metrics_+3A_hyperdrive_run">hyperdrive_run</code></td>
<td>
<p>The <code>HyperDriveRun</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The named list of metrics where element name is
the run_id, e.g. <code>list("run_id" = metrics)</code>.
</p>

<hr>
<h2 id='get_child_runs'>Get all children for the current run selected by specified filters</h2><span id='topic+get_child_runs'></span>

<h3>Description</h3>

<p>Get all children for the current run selected by specified filters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_child_runs(
  parent_run,
  recursive = FALSE,
  tags = NULL,
  properties = NULL,
  type = NULL,
  status = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_child_runs_+3A_parent_run">parent_run</code></td>
<td>
<p>The parent <code>Run</code> object.</p>
</td></tr>
<tr><td><code id="get_child_runs_+3A_recursive">recursive</code></td>
<td>
<p>Boolean indicating whether to recurse through all descendants.</p>
</td></tr>
<tr><td><code id="get_child_runs_+3A_tags">tags</code></td>
<td>
<p>If specified, returns runs matching specified &quot;tag&quot; or list(tag = value).</p>
</td></tr>
<tr><td><code id="get_child_runs_+3A_properties">properties</code></td>
<td>
<p>If specified, returns runs matching specified &quot;property&quot; or list(property = value).</p>
</td></tr>
<tr><td><code id="get_child_runs_+3A_type">type</code></td>
<td>
<p>If specified, returns runs matching this type.</p>
</td></tr>
<tr><td><code id="get_child_runs_+3A_status">status</code></td>
<td>
<p>If specified, returns runs with status specified &quot;status&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of child runs, <code>Run</code> objects.
</p>

<hr>
<h2 id='get_child_runs_sorted_by_primary_metric'>Get the child runs sorted in descending order by
best primary metric</h2><span id='topic+get_child_runs_sorted_by_primary_metric'></span>

<h3>Description</h3>

<p>Return a list of child runs of the HyperDrive run sorted by their best
primary metric. The sorting is done according to the primary metric and
its goal: if it is maximize, then the child runs are returned in descending
order of their best primary metric. If <code>reverse = TRUE</code>, the order is
reversed. Each child in the result has run id, hyperparameters, best primary
metric value, and status.
</p>
<p>Child runs without the primary metric are discarded when
<code>discard_no_metric = TRUE</code>. Otherwise, they are appended to the list behind
other child runs with the primary metric. Note that the reverse option has no
impact on them.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_child_runs_sorted_by_primary_metric(
  hyperdrive_run,
  top = 0L,
  reverse = FALSE,
  discard_no_metric = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_child_runs_sorted_by_primary_metric_+3A_hyperdrive_run">hyperdrive_run</code></td>
<td>
<p>The <code>HyperDriveRun</code> object.</p>
</td></tr>
<tr><td><code id="get_child_runs_sorted_by_primary_metric_+3A_top">top</code></td>
<td>
<p>An integer of the number of top child runs to be returned. If <code>0</code>
(the default value), all child runs will be returned.</p>
</td></tr>
<tr><td><code id="get_child_runs_sorted_by_primary_metric_+3A_reverse">reverse</code></td>
<td>
<p>If <code>TRUE</code>, the order will be reversed. This sorting only
impacts child runs with the primary metric.</p>
</td></tr>
<tr><td><code id="get_child_runs_sorted_by_primary_metric_+3A_discard_no_metric">discard_no_metric</code></td>
<td>
<p>If <code>FALSE</code>, child runs without the primary metric
will be appended to the list returned.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The named list of child runs.
</p>

<hr>
<h2 id='get_compute'>Get an existing compute cluster</h2><span id='topic+get_compute'></span>

<h3>Description</h3>

<p>Returns an <code>AmlCompute</code> or <code>AksCompute</code> object for an existing compute
resource. If the compute target doesn't exist, the function will return
<code>NULL</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_compute(workspace, cluster_name)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_compute_+3A_workspace">workspace</code></td>
<td>
<p>The <code>Workspace</code> object.</p>
</td></tr>
<tr><td><code id="get_compute_+3A_cluster_name">cluster_name</code></td>
<td>
<p>A string of the name of the cluster.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>AmlCompute</code> or <code>AksCompute</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
ws &lt;- load_workspace_from_config()
compute_target &lt;- get_compute(ws, cluster_name = 'mycluster')

## End(Not run)
</code></pre>

<hr>
<h2 id='get_current_run'>Get the context object for a run</h2><span id='topic+get_current_run'></span>

<h3>Description</h3>

<p>This function is commonly used to retrieve the authenticated
run object inside of a script to be submitted for execution
via <code>submit_experiment()</code>. Note that the logging functions
(<code style="white-space: pre;">&#8288;log_*&#8288;</code> methods, <code>upload_files_to_run()</code>, <code>upload_folder_to_run()</code>)
will by default log the specified metrics or files to the
run returned from <code>get_current_run()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_current_run(allow_offline = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_current_run_+3A_allow_offline">allow_offline</code></td>
<td>
<p>If <code>TRUE</code>, allow the service context to
fall back to offline mode so that the training script can be
tested locally without submitting a job with the SDK.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>Run</code> object.
</p>

<hr>
<h2 id='get_dataset_by_id'>Get Dataset by ID.</h2><span id='topic+get_dataset_by_id'></span>

<h3>Description</h3>

<p>Get a Dataset which is saved to the workspace using its ID.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_dataset_by_id(workspace, id)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_dataset_by_id_+3A_workspace">workspace</code></td>
<td>
<p>The existing AzureML workspace in which the Dataset is saved.</p>
</td></tr>
<tr><td><code id="get_dataset_by_id_+3A_id">id</code></td>
<td>
<p>The ID of the dataset</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The Dataset object
</p>

<hr>
<h2 id='get_dataset_by_name'>Get a registered Dataset from the workspace by its registration name.</h2><span id='topic+get_dataset_by_name'></span>

<h3>Description</h3>

<p>Get a registered Dataset from the workspace by its registration name.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_dataset_by_name(workspace, name, version = "latest")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_dataset_by_name_+3A_workspace">workspace</code></td>
<td>
<p>The existing AzureML workspace in which the Dataset was registered.</p>
</td></tr>
<tr><td><code id="get_dataset_by_name_+3A_name">name</code></td>
<td>
<p>The registration name.</p>
</td></tr>
<tr><td><code id="get_dataset_by_name_+3A_version">version</code></td>
<td>
<p>The registration version. Defaults to &quot;latest&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The registered Dataset object.
</p>

<hr>
<h2 id='get_datastore'>Get an existing datastore</h2><span id='topic+get_datastore'></span>

<h3>Description</h3>

<p>Get the corresponding datastore object for an existing
datastore by name from the given workspace.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_datastore(workspace, datastore_name)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_datastore_+3A_workspace">workspace</code></td>
<td>
<p>The <code>Workspace</code> object.</p>
</td></tr>
<tr><td><code id="get_datastore_+3A_datastore_name">datastore_name</code></td>
<td>
<p>A string of the name of the datastore.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>azureml.data.azure_sql_database.AzureBlobDatastore</code>,
<code>azureml.data.azure_sql_database.AzureFileDatastore</code>,
<code>azureml.data.azure_sql_database.AzureSqlDatabaseDatastore</code>,
<code>azureml.data.azure_data_lake_datastore.AzureDataLakeGen2Datastore</code>,
<code>azureml.data.azure_postgre_sql_datastore.AzurePostgreSqlDatastore</code>, or
<code>azureml.data.azure_sql_database.AzureSqlDatabaseDatastore</code> object.
</p>

<hr>
<h2 id='get_default_datastore'>Get the default datastore for a workspace</h2><span id='topic+get_default_datastore'></span>

<h3>Description</h3>

<p>Returns the default datastore associated with the workspace.
</p>
<p>When you create a workspace, an Azure blob container and Azure file share
are registered to the workspace with the names <code>workspaceblobstore</code> and
<code>workspacefilestore</code>, respectively. They store the connection information
of the blob container and the file share that is provisioned in the storage
account attached to the workspace. The <code>workspaceblobstore</code> is set as the
default datastore, and remains the default datastore unless you set a new
datastore as the default with <code>set_default_datastore()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_default_datastore(workspace)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_default_datastore_+3A_workspace">workspace</code></td>
<td>
<p>The <code>Workspace</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The default <code>Datastore</code> object.
</p>


<h3>Examples</h3>

<p>Get the default datastore for the datastore:</p>
<pre>ws &lt;- load_workspace_from_config()
ds &lt;- get_default_datastore(ws)
</pre>
<p>If you have not changed the default datastore for the workspace, the
following code will return the same datastore object as the above
example:</p>
<pre>ws &lt;- load_workspace_from_config()
ds &lt;- get_datastore(ws, datastore_name = 'workspaceblobstore')
</pre>


<h3>See Also</h3>

<p><code><a href="#topic+set_default_datastore">set_default_datastore()</a></code>
</p>

<hr>
<h2 id='get_default_keyvault'>Get the default keyvault for a workspace</h2><span id='topic+get_default_keyvault'></span>

<h3>Description</h3>

<p>Returns a <code>Keyvault</code> object representing the default
<a href="https://docs.microsoft.com/en-us/azure/key-vault/key-vault-overview">Azure Key Vault</a>
associated with the workspace.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_default_keyvault(workspace)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_default_keyvault_+3A_workspace">workspace</code></td>
<td>
<p>The <code>Workspace</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>Keyvault</code> object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+set_secrets">set_secrets()</a></code> <code><a href="#topic+get_secrets">get_secrets()</a></code> <code><a href="#topic+list_secrets">list_secrets()</a></code> <code><a href="#topic+delete_secrets">delete_secrets()</a></code>
</p>

<hr>
<h2 id='get_environment'>Get an existing environment</h2><span id='topic+get_environment'></span>

<h3>Description</h3>

<p>Returns an <code>Environment</code> object for an existing environment in
the workspace.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_environment(workspace, name, version = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_environment_+3A_workspace">workspace</code></td>
<td>
<p>The <code>Workspace</code> object.</p>
</td></tr>
<tr><td><code id="get_environment_+3A_name">name</code></td>
<td>
<p>A string of the name of the environment.</p>
</td></tr>
<tr><td><code id="get_environment_+3A_version">version</code></td>
<td>
<p>A string of the version of the environment.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>Environment</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
ws &lt;- load_workspace_from_config()
env &lt;- get_environment(ws, name = 'myenv', version = '1')

## End(Not run)
</code></pre>

<hr>
<h2 id='get_file_dataset_paths'>Get a list of file paths for each file stream defined by the dataset.</h2><span id='topic+get_file_dataset_paths'></span>

<h3>Description</h3>

<p>Get a list of file paths for each file stream defined by the dataset. The file
paths are relative paths for local files when the file srteam are downloaded
or mounted. A common prefix will be removed from the file paths based on how
data source was specified to create the dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_file_dataset_paths(dataset)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_file_dataset_paths_+3A_dataset">dataset</code></td>
<td>
<p>The Dataset object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of file paths.
</p>

<hr>
<h2 id='get_input_dataset_from_run'>Return the named list for input datasets.</h2><span id='topic+get_input_dataset_from_run'></span>

<h3>Description</h3>

<p>Return the named list for input datasets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_input_dataset_from_run(name, run = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_input_dataset_from_run_+3A_name">name</code></td>
<td>
<p>The name of the input dataset</p>
</td></tr>
<tr><td><code id="get_input_dataset_from_run_+3A_run">run</code></td>
<td>
<p>The run taking the dataset as input</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset object corresponding to the &quot;name&quot;
</p>

<hr>
<h2 id='get_model'>Get a registered model</h2><span id='topic+get_model'></span>

<h3>Description</h3>

<p>Returns a <code>Model</code> object for an existing model that has been
previously registered to the given workspace.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_model(
  workspace,
  name = NULL,
  id = NULL,
  tags = NULL,
  properties = NULL,
  version = NULL,
  run_id = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_model_+3A_workspace">workspace</code></td>
<td>
<p>The <code>Workspace</code> object.</p>
</td></tr>
<tr><td><code id="get_model_+3A_name">name</code></td>
<td>
<p>Retrieve the latest model with the corresponding
name (a string), if it exists.</p>
</td></tr>
<tr><td><code id="get_model_+3A_id">id</code></td>
<td>
<p>Retrieve the model with the corresponding ID (a string),
if it exists.</p>
</td></tr>
<tr><td><code id="get_model_+3A_tags">tags</code></td>
<td>
<p>(Optional) Retrieve the model filtered based on the
provided tags (a list), searching by either 'key' or
'list(key, value)'.</p>
</td></tr>
<tr><td><code id="get_model_+3A_properties">properties</code></td>
<td>
<p>(Optional) Retrieve the model filter based on the
provided properties (a list), searching by either 'key' or
'list(key, value)'.</p>
</td></tr>
<tr><td><code id="get_model_+3A_version">version</code></td>
<td>
<p>(Optional) An int of the version of a model to
retrieve, when provided along with <code>name</code>. The specific version of
the specified named model will be returned, if it exists.</p>
</td></tr>
<tr><td><code id="get_model_+3A_run_id">run_id</code></td>
<td>
<p>(Optional) Retrieve the model filterd by the provided
run ID (a string) the model was registered from, if it exists.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>Model</code> object.
</p>

<hr>
<h2 id='get_model_package_container_registry'>Get the Azure container registry that a packaged model uses</h2><span id='topic+get_model_package_container_registry'></span>

<h3>Description</h3>

<p>Return a <code>ContainerRegistry</code> object for where the image
(or base image, for Dockerfile packages) is stored in an
Azure container registry.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_model_package_container_registry(package)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_model_package_container_registry_+3A_package">package</code></td>
<td>
<p>The <code>ModelPackage</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>ContainerRegistry</code> object.
</p>


<h3>See Also</h3>

<p><code>container_registry()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Given a ModelPackage object,
# get the container registry information
## Not run: 
container_registry &lt;- get_model_package_container_registry(package)
address &lt;- container_registry$address
username &lt;- container_registry$username
password &lt;- container_registry$password

## End(Not run)

# To then authenticate Docker with the Azure container registry from
# a shell or command-line session, use the following command, replacing
# &lt;address&gt;, &lt;username&gt;, and &lt;password&gt; with the values retrieved
# from above:
# ```bash
# docker login &lt;address&gt; -u &lt;username&gt; -p &lt;password&gt;
# ```
</code></pre>

<hr>
<h2 id='get_model_package_creation_logs'>Get the model package creation logs</h2><span id='topic+get_model_package_creation_logs'></span>

<h3>Description</h3>

<p>Retrieve the creation logs from packaging a model with
<code>package_model()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_model_package_creation_logs(package, decode = TRUE, offset = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_model_package_creation_logs_+3A_package">package</code></td>
<td>
<p>The <code>ModelPackage</code> object.</p>
</td></tr>
<tr><td><code id="get_model_package_creation_logs_+3A_decode">decode</code></td>
<td>
<p>If <code>TRUE</code>, decode the raw log bytes to a string.</p>
</td></tr>
<tr><td><code id="get_model_package_creation_logs_+3A_offset">offset</code></td>
<td>
<p>An int of the byte offset from which to start
reading the logs.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A string or character vector of package creation logs.
</p>

<hr>
<h2 id='get_run'>Get an experiment run</h2><span id='topic+get_run'></span>

<h3>Description</h3>

<p>Given the associated experiment and run ID, return the
run object for a previously submitted/tracked run.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_run(experiment, run_id)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_run_+3A_experiment">experiment</code></td>
<td>
<p>The <code>Experiment</code> object.</p>
</td></tr>
<tr><td><code id="get_run_+3A_run_id">run_id</code></td>
<td>
<p>A string of the run ID for the run.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>Run</code> object.
</p>

<hr>
<h2 id='get_run_details'>Get the details of a run</h2><span id='topic+get_run_details'></span>

<h3>Description</h3>

<p>Get the definition, status information, current log files, and
other details of the run.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_run_details(run)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_run_details_+3A_run">run</code></td>
<td>
<p>The <code>Run</code> object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The returned list contains the following named elements:
</p>

<ul>
<li> <p><em>runId</em>: ID of the run.
</p>
</li>
<li> <p><em>target</em>: The compute target of the run.
</p>
</li>
<li> <p><em>status</em>: The run's current status.
</p>
</li>
<li> <p><em>startTimeUtc</em>: UTC time of when the run was started, in ISO8601.
</p>
</li>
<li> <p><em>endTimeUtc</em>: UTC time of when the run was finished (either
Completed or Failed), in ISO8601. This element does not exist if
the run is still in progress.
</p>
</li>
<li> <p><em>properties</em>: Immutable key-value pairs associated with the run.
</p>
</li>
<li> <p><em>logFiles</em>: Log files from the run.
</p>
</li></ul>



<h3>Value</h3>

<p>A named list of the details for the run.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+get_run_details_with_logs">get_run_details_with_logs()</a></code>
</p>

<hr>
<h2 id='get_run_details_with_logs'>Get the details of a run along with the log files' contents</h2><span id='topic+get_run_details_with_logs'></span>

<h3>Description</h3>

<p>Get the details of a run along with the log files' contents
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_run_details_with_logs(run)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_run_details_with_logs_+3A_run">run</code></td>
<td>
<p>The <code>Run</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list of the run details and log file contents.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+get_run_details">get_run_details()</a></code>
</p>

<hr>
<h2 id='get_run_file_names'>List the files that are stored in association with a run</h2><span id='topic+get_run_file_names'></span>

<h3>Description</h3>

<p>Get the list of files stored in a run record.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_run_file_names(run)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_run_file_names_+3A_run">run</code></td>
<td>
<p>The <code>Run</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of strings of the paths for existing artifacts
in the run record.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+download_file_from_run">download_file_from_run()</a></code> <code><a href="#topic+download_files_from_run">download_files_from_run()</a></code>
</p>

<hr>
<h2 id='get_run_metrics'>Get the metrics logged to a run</h2><span id='topic+get_run_metrics'></span>

<h3>Description</h3>

<p>Retrieve the metrics logged to a run that were logged with
the <code style="white-space: pre;">&#8288;log_*()&#8288;</code> methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_run_metrics(
  run,
  name = NULL,
  recursive = FALSE,
  run_type = NULL,
  populate = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_run_metrics_+3A_run">run</code></td>
<td>
<p>The <code>Run</code> object.</p>
</td></tr>
<tr><td><code id="get_run_metrics_+3A_name">name</code></td>
<td>
<p>The name of the metric.</p>
</td></tr>
<tr><td><code id="get_run_metrics_+3A_recursive">recursive</code></td>
<td>
<p>If specified, returns runs matching specified <em>&quot;property&quot;</em> or <em>&quot;property&quot;</em>: <em>&quot;value&quot;</em>.</p>
</td></tr>
<tr><td><code id="get_run_metrics_+3A_run_type">run_type</code></td>
<td>
<p>run type</p>
</td></tr>
<tr><td><code id="get_run_metrics_+3A_populate">populate</code></td>
<td>
<p>Boolean indicating whether to fetch the contents of external data linked to the metric.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list of the metrics associated with the run,
e.g. <code>list("metric_name" = metric)</code>.
</p>


<h3>Examples</h3>

<pre>ws &lt;- load_workspace_from_config()
exp &lt;- experiment(ws, name = 'myexperiment')
run &lt;- get_run(exp, run_id = "myrunid")
metrics &lt;- get_run_metrics(run)
</pre>

<hr>
<h2 id='get_runs_in_experiment'>Return a generator of the runs for an experiment</h2><span id='topic+get_runs_in_experiment'></span>

<h3>Description</h3>

<p>Return a generator of the runs for an experiment, in reverse
chronological order.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_runs_in_experiment(
  experiment,
  type = NULL,
  tags = NULL,
  properties = NULL,
  include_children = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_runs_in_experiment_+3A_experiment">experiment</code></td>
<td>
<p>The <code>Experiment</code> object.</p>
</td></tr>
<tr><td><code id="get_runs_in_experiment_+3A_type">type</code></td>
<td>
<p>Filter the returned generator of runs by the provided type.</p>
</td></tr>
<tr><td><code id="get_runs_in_experiment_+3A_tags">tags</code></td>
<td>
<p>Filter runs by tags. A named list eg. list(&quot;tag&quot; = &quot;value&quot;).</p>
</td></tr>
<tr><td><code id="get_runs_in_experiment_+3A_properties">properties</code></td>
<td>
<p>Filter runs by properties. A named list
eg. list(&quot;property&quot; = &quot;value&quot;).</p>
</td></tr>
<tr><td><code id="get_runs_in_experiment_+3A_include_children">include_children</code></td>
<td>
<p>By default, fetch only top-level runs.
Set to TRUE to list all runs.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The list of runs matching supplied filters.
</p>

<hr>
<h2 id='get_secrets'>Get secrets from a keyvault</h2><span id='topic+get_secrets'></span>

<h3>Description</h3>

<p>Returns the secret values from the keyvault associated with the
workspace for a given set of secret names. For runs submitted using
<code>submit_experiment()</code>, you can use <code>get_secrets_from_run()</code> instead,
as that method shortcuts workspace instantiation (since a submitted
run is aware of its workspace).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_secrets(keyvault, secrets)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_secrets_+3A_keyvault">keyvault</code></td>
<td>
<p>The <code>Keyvault</code> object.</p>
</td></tr>
<tr><td><code id="get_secrets_+3A_secrets">secrets</code></td>
<td>
<p>A vector of secret names.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list of found and not found secrets, where element
name corresponds to the secret name. If a secret was not found, the
corresponding element will be <code>NULL</code>.
</p>

<hr>
<h2 id='get_secrets_from_run'>Get secrets from the keyvault associated with a run's workspace</h2><span id='topic+get_secrets_from_run'></span>

<h3>Description</h3>

<p>From within the script of a run submitted using
<code>submit_experiment()</code>, you can use <code>get_secrets_from_run()</code>
to get secrets that are stored in the keyvault of the associated
workspace.
</p>
<p>Note that this method is slightly different than <code>get_secrets()</code>,
which first requires you to instantiate the workspace object.
Since a submitted run is aware of its workspace,
<code>get_secrets_from_run()</code> shortcuts workspace instantiation and
returns the secret value directly.
</p>
<p>Be careful not to expose the secret(s) values by writing or
printing them out.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_secrets_from_run(run, secrets)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_secrets_from_run_+3A_run">run</code></td>
<td>
<p>The <code>Run</code> object.</p>
</td></tr>
<tr><td><code id="get_secrets_from_run_+3A_secrets">secrets</code></td>
<td>
<p>A vector of strings of secret names to retrieve
the values for.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list of found and not found secrets.
If a secret was not found, the corresponding element will be <code>NULL</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+set_secrets">set_secrets()</a></code>
</p>

<hr>
<h2 id='get_webservice'>Get a deployed web service</h2><span id='topic+get_webservice'></span>

<h3>Description</h3>

<p>Return the corresponding Webservice object of a deployed web service from
a given workspace.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_webservice(workspace, name)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_webservice_+3A_workspace">workspace</code></td>
<td>
<p>The <code>Workspace</code> object.</p>
</td></tr>
<tr><td><code id="get_webservice_+3A_name">name</code></td>
<td>
<p>A string of the name of the web service to retrieve.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>LocalWebservice</code>, <code>AciWebservice</code>, or <code>AksWebservice</code> object.
</p>

<hr>
<h2 id='get_webservice_keys'>Retrieve auth keys for a web service</h2><span id='topic+get_webservice_keys'></span>

<h3>Description</h3>

<p>Get the authentication keys for a web service that is deployed
with key-based authentication enabled. In order to enable
key-based authentication, set the <code>auth_enabled = TRUE</code> parameter
when you are creating or updating a deployment (either
<code>aci_webservice_deployment_config()</code> or
<code>aks_webservice_deployment_config()</code> for creation and
<code>update_aci_webservice()</code> or <code>update_aks_webservice()</code> for updating).
Note that key-based auth is enabled by default for <code>AksWebservice</code>
but not for <code>AciWebservice</code>.
</p>
<p>To check if a web service has key-based auth enabled, you can
access the following boolean property from the Webservice object:
<code>service$auth_enabled</code>
</p>
<p>Not supported for <code>LocalWebservice</code> deployments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_webservice_keys(webservice)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_webservice_keys_+3A_webservice">webservice</code></td>
<td>
<p>The <code>AciWebservice</code> or <code>AksWebservice</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of two strings corresponding to the primary and
secondary authentication keys.
</p>


<h3>See Also</h3>

<p><code>generate_new_webservice_key()</code>
</p>

<hr>
<h2 id='get_webservice_logs'>Retrieve the logs for a web service</h2><span id='topic+get_webservice_logs'></span>

<h3>Description</h3>

<p>You can get the detailed Docker engine log messages from your
web service deployment. You can view the logs for local, ACI,
and AKS deployments.
</p>
<p>For example, if your web service deployment fails, you can
inspect the logs to help troubleshoot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_webservice_logs(webservice, num_lines = 5000L)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_webservice_logs_+3A_webservice">webservice</code></td>
<td>
<p>The <code>LocalWebservice</code>, <code>AciWebservice</code>, or
<code>AksWebservice</code> object.</p>
</td></tr>
<tr><td><code id="get_webservice_logs_+3A_num_lines">num_lines</code></td>
<td>
<p>An int of the maximum number of log lines to
retrieve.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A string of the logs for the web service.
</p>

<hr>
<h2 id='get_webservice_token'>Retrieve the auth token for a web service</h2><span id='topic+get_webservice_token'></span>

<h3>Description</h3>

<p>Get the authentication token, scoped to the current user,
for a web service that was deployed with token-based authentication
enabled. Token-based authentication requires clients to use an Azure
Active Directory account to request an authentication token, which is
used to make requests to the deployed service. Only available for
AKS deployments.
</p>
<p>In order to enable token-based authentication, set the
<code>token_auth_enabled = TRUE</code> parameter when you are creating or
updating a deployment (<code>aks_webservice_deployment_config()</code> for creation
or <code>update_aks_webservice()</code> for updating). Note that you cannot have both
key-based authentication and token-based authentication enabled.
Token-based authentication is not enabled by default.
</p>
<p>To check if a web service has token-based auth enabled, you can
access the following boolean property from the Webservice object:
<code>service$token_auth_enabled</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_webservice_token(webservice)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_webservice_token_+3A_webservice">webservice</code></td>
<td>
<p>The <code>AksWebservice</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code>AksServiceAccessToken</code> object.
</p>

<hr>
<h2 id='get_workspace'>Get an existing workspace</h2><span id='topic+get_workspace'></span>

<h3>Description</h3>

<p>Returns a <code>Workspace</code> object for an existing Azure Machine Learning
workspace. Throws an exception if the workpsace doesn't exist or the
required fields don't lead to a uniquely identifiable workspace.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_workspace(name, auth = NULL, subscription_id = NULL, resource_group = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_workspace_+3A_name">name</code></td>
<td>
<p>A string of the workspace name to get.</p>
</td></tr>
<tr><td><code id="get_workspace_+3A_auth">auth</code></td>
<td>
<p>The <code>ServicePrincipalAuthentication</code> or <code>InteractiveLoginAuthentication</code>
object. For more details refer to https://aka.ms/aml-notebook-auth. If NULL,
the default Azure CLI credentials will be used or the API will prompt for credentials.</p>
</td></tr>
<tr><td><code id="get_workspace_+3A_subscription_id">subscription_id</code></td>
<td>
<p>A string of the subscription ID to use. The parameter
is required if the user has access to more than one subscription.</p>
</td></tr>
<tr><td><code id="get_workspace_+3A_resource_group">resource_group</code></td>
<td>
<p>A string of the resource group to use. If <code>NULL</code> the
method will search all resource groups in the subscription.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>Workspace</code> object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+create_workspace">create_workspace()</a></code> <code><a href="#topic+service_principal_authentication">service_principal_authentication()</a></code> <code><a href="#topic+interactive_login_authentication">interactive_login_authentication()</a></code>
</p>

<hr>
<h2 id='get_workspace_details'>Get the details of a workspace</h2><span id='topic+get_workspace_details'></span>

<h3>Description</h3>

<p>Returns the details of the workspace.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_workspace_details(workspace)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_workspace_details_+3A_workspace">workspace</code></td>
<td>
<p>The <code>Workspace</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Named list of the workspace details.
</p>


<h3>Details</h3>

<p>The returned named list contains the following elements:
</p>

<ul>
<li> <p><em>id</em>: URI pointing to the workspace resource, containing subscription ID,
resource group, and workspace name.
</p>
</li>
<li> <p><em>name</em>: Workspace name.
</p>
</li>
<li> <p><em>location</em>: Workspace region.
</p>
</li>
<li> <p><em>type</em>: URI of the format <code>"{providerName}/workspaces"</code>.
</p>
</li>
<li> <p><em>workspaceid</em>: Workspace ID.
</p>
</li>
<li> <p><em>description</em>: Workspace description.
</p>
</li>
<li> <p><em>friendlyName</em>: Workspace friendly name.
</p>
</li>
<li> <p><em>creationTime</em>: Time the workspace was created, in ISO8601.
</p>
</li>
<li> <p><em>containerRegistry</em>: Workspace container registry.
</p>
</li>
<li> <p><em>keyVault</em>: Workspace key vault.
</p>
</li>
<li> <p><em>applicationInsights</em>: Workspace App Insights.
</p>
</li>
<li> <p><em>identityPrincipalId</em>: Workspace identity principal ID.
</p>
</li>
<li> <p><em>identityTenantId</em>: Workspace tenant ID.
</p>
</li>
<li> <p><em>identityType</em>: Workspace identity type.
</p>
</li>
<li> <p><em>storageAccount</em>: Workspace storage account.
</p>
</li></ul>


<hr>
<h2 id='github_package'>Specifies a Github package to install in environment</h2><span id='topic+github_package'></span>

<h3>Description</h3>

<p>Specifies a Github package to install in run environment
</p>


<h3>Usage</h3>

<pre><code class='language-R'>github_package(repository, auth_token = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="github_package_+3A_repository">repository</code></td>
<td>
<p>Repository address of the github package</p>
</td></tr>
<tr><td><code id="github_package_+3A_auth_token">auth_token</code></td>
<td>
<p>Personal access token to install from a private repo.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list containing the package specifications
</p>


<h3>Examples</h3>

<pre>pkg1 &lt;- github_package("Azure/azureml-sdk-for-r")

env &lt;- r_environment(name = "r_env",
                     github_packages = list(pkg1))
</pre>


<h3>See Also</h3>

<p><code><a href="#topic+r_environment">r_environment()</a></code>
</p>

<hr>
<h2 id='grid_parameter_sampling'>Define grid sampling over a hyperparameter search space</h2><span id='topic+grid_parameter_sampling'></span>

<h3>Description</h3>

<p>Grid sampling performs a simple grid search over all feasible values in
the defined search space. It can only be used with hyperparameters
specified using <code>choice()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>grid_parameter_sampling(parameter_space)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="grid_parameter_sampling_+3A_parameter_space">parameter_space</code></td>
<td>
<p>A named list containing each parameter and its
distribution, e.g. <code>list("parameter" = distribution)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>GridParameterSampling</code> object.
</p>


<h3>See Also</h3>

<p><code>choice()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
param_sampling &lt;- grid_parameter_sampling(list("num_hidden_layers" = choice(c(1, 2, 3)),
                                               "batch_size" = choice(c(16, 32))))

## End(Not run)
</code></pre>

<hr>
<h2 id='hyperdrive_config'>Create a configuration for a HyperDrive run</h2><span id='topic+hyperdrive_config'></span>

<h3>Description</h3>

<p>The HyperDrive configuration includes information about hyperparameter
space sampling, termination policy, primary metric, estimator, and
the compute target to execute the experiment runs on.
</p>
<p>To submit the HyperDrive experiment, pass the <code>HyperDriveConfig</code> object
returned from this method to <code>submit_experiment()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hyperdrive_config(
  hyperparameter_sampling,
  primary_metric_name,
  primary_metric_goal,
  max_total_runs,
  max_concurrent_runs = NULL,
  max_duration_minutes = 10080L,
  policy = NULL,
  estimator = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hyperdrive_config_+3A_hyperparameter_sampling">hyperparameter_sampling</code></td>
<td>
<p>The hyperparameter sampling space.
Can be a <code>RandomParameterSampling</code>, <code>GridParameterSampling</code>, or
<code>BayesianParameterSampling</code> object.</p>
</td></tr>
<tr><td><code id="hyperdrive_config_+3A_primary_metric_name">primary_metric_name</code></td>
<td>
<p>A string of the name of the primary metric
reported by the experiment runs.</p>
</td></tr>
<tr><td><code id="hyperdrive_config_+3A_primary_metric_goal">primary_metric_goal</code></td>
<td>
<p>The <code>PrimaryMetricGoal</code> object. This
parameter determines if the primary metric is to be minimized or
maximized when evaluating runs.</p>
</td></tr>
<tr><td><code id="hyperdrive_config_+3A_max_total_runs">max_total_runs</code></td>
<td>
<p>An integer of the maximum total number of runs
to create. This is the upper bound; there may be fewer runs when the
sample space is smaller than this value. If both <code>max_total_runs</code> and
<code>max_duration_minutes</code> are specified, the hyperparameter tuning experiment
terminates when the first of these two thresholds is reached.</p>
</td></tr>
<tr><td><code id="hyperdrive_config_+3A_max_concurrent_runs">max_concurrent_runs</code></td>
<td>
<p>An integer of the maximum number of runs to
execute concurrently. If <code>NULL</code>, all runs are launched in parallel.
The number of concurrent runs is gated on the resources available in the
specified compute target. Hence, you need to ensure that the compute target
has the available resources for the desired concurrency.</p>
</td></tr>
<tr><td><code id="hyperdrive_config_+3A_max_duration_minutes">max_duration_minutes</code></td>
<td>
<p>An integer of the maximum duration of the
HyperDrive run. Once this time is exceeded, any runs still executing are
cancelled. If both <code>max_total_runs</code> and <code>max_duration_minutes</code> are specified,
the hyperparameter tuning experiment terminates when the first of these two
thresholds is reached.</p>
</td></tr>
<tr><td><code id="hyperdrive_config_+3A_policy">policy</code></td>
<td>
<p>The early termination policy to use. Can be either a
<code>BanditPolicy</code>, <code>MedianStoppingPolicy</code>, or <code>TruncationSelectionPolicy</code>
object. If <code>NULL</code> (the default), no early termination policy will be used.
</p>
<p>The <code>MedianStoppingPolicy</code> with <code style="white-space: pre;">&#8288;delay_evaluation of = 5&#8288;</code> is a good
termination policy to start with. These are conservative settings that can
provide 25%-35% savings with no loss on primary metric
(based on our evaluation data).</p>
</td></tr>
<tr><td><code id="hyperdrive_config_+3A_estimator">estimator</code></td>
<td>
<p>The <code>Estimator</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>HyperDriveConfig</code> object.
</p>


<h3>See Also</h3>

<p><code>submit_experiment()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Load the workspace
ws &lt;- load_workspace_from_config()

# Get the compute target
compute_target &lt;- get_compute(ws, cluster_name = 'mycluster')

# Define the primary metric goal
goal = primary_metric_goal("MAXIMIZE")

# Define the early termination policy
early_termination_policy = median_stopping_policy(evaluation_interval = 1L,
                                                  delay_evaluation = 5L)

# Create the estimator
est &lt;- estimator(source_directory = '.',
                 entry_script = 'train.R',
                 compute_target = compute_target)

# Create the HyperDrive configuration
hyperdrive_run_config = hyperdrive_config(
                                   hyperparameter_sampling = param_sampling,
                                   primary_metric_name = 'accuracy',
                                   primary_metric_goal = goal,
                                   max_total_runs = 100,
                                   max_concurrent_runs = 4,
                                   policy = early_termination_policy,
                                   estimator = est)

# Submit the HyperDrive experiment
exp &lt;- experiment(ws, name = 'myexperiment')
run = submit_experiment(exp, hyperdrive_run_config)

## End(Not run)
</code></pre>

<hr>
<h2 id='inference_config'>Create an inference configuration for model deployments</h2><span id='topic+inference_config'></span>

<h3>Description</h3>

<p>The inference configuration describes how to configure the model to make
predictions. It references your scoring script (<code>entry_script</code>) and is
used to locate all the resources required for the deployment. Inference
configurations use Azure Machine Learning environments (see <code>r_environment()</code>)
to define the software dependencies needed for your deployment.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>inference_config(
  entry_script,
  source_directory = ".",
  description = NULL,
  environment = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="inference_config_+3A_entry_script">entry_script</code></td>
<td>
<p>A string of the path to the local file that contains
the code to run for making predictions.</p>
</td></tr>
<tr><td><code id="inference_config_+3A_source_directory">source_directory</code></td>
<td>
<p>A string of the path to the local folder
that contains the files to package and deploy alongside your model, such as
helper files for your scoring script (<code>entry_script</code>). The folder must
contain the <code>entry_script</code>.</p>
</td></tr>
<tr><td><code id="inference_config_+3A_description">description</code></td>
<td>
<p>(Optional) A string of the description to give this
configuration.</p>
</td></tr>
<tr><td><code id="inference_config_+3A_environment">environment</code></td>
<td>
<p>An <code>Environment</code> object to use for the deployment. The
environment does not have to be registered.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>InferenceConfig</code> object.
</p>


<h3>Defining the entry script</h3>

<p>To deploy a model, you must provide an entry script that accepts requests,
scores the requests by using the model, and returns the results. The
entry script is specific to your model. It must understand the format of
the incoming request data, the format of the data expected by your model,
and the format of the data returned to clients. If the request data is in a
format that is not usable by your model, the script can transform it into
an acceptable format. It can also transform the response before returning
it to the client.
</p>
<p>The entry script must contain an <code>init()</code> method that loads your model and
then returns a function that uses the model to make a prediction based on
the input data passed to the function. Azure ML runs the <code>init()</code> method
once, when the Docker container for your web service is started. The
prediction function returned by <code>init()</code> will be run every time the service
is invoked to make a prediction on some input data. The inputs and outputs
of this prediction function typically use JSON for serialization and
deserialization.
</p>
<p>To locate the model in your entry script (when you load the model in the
script's <code>init()</code> method), use <code>AZUREML_MODEL_DIR</code>, an environment variable
containing the path to the model location. The environment variable is
created during service deployment, and you can use it to find the location
of your deployed model(s).
</p>
<p>To get the path to a file in a model, combine the environment variable
with the filename you're looking for. The filenames of the model files
are preserved during registration and deployment.
</p>
<p>Single model example:</p>
<pre>model_path &lt;- file.path(Sys.getenv("AZUREML_MODEL_DIR"), "my_model.rds")
</pre>
<p>Multiple model example:</p>
<pre>model1_path &lt;- file.path(Sys.getenv("AZUREML_MODEL_DIR"), "my_model/1/my_model.rds")
</pre>


<h3>See Also</h3>

<p><code>r_environment()</code>, <code>deploy_model()</code>
</p>

<hr>
<h2 id='install_azureml'>Install azureml sdk package</h2><span id='topic+install_azureml'></span>

<h3>Description</h3>

<p>Install azureml sdk package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>install_azureml(
  version = "1.10.0",
  envname = "r-reticulate",
  conda_python_version = "3.6",
  restart_session = TRUE,
  remove_existing_env = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="install_azureml_+3A_version">version</code></td>
<td>
<p>azureml sdk package version</p>
</td></tr>
<tr><td><code id="install_azureml_+3A_envname">envname</code></td>
<td>
<p>name of environment to create, if environment other
than default is desired</p>
</td></tr>
<tr><td><code id="install_azureml_+3A_conda_python_version">conda_python_version</code></td>
<td>
<p>version of python for conda environment</p>
</td></tr>
<tr><td><code id="install_azureml_+3A_restart_session">restart_session</code></td>
<td>
<p>restart R session after installation</p>
</td></tr>
<tr><td><code id="install_azureml_+3A_remove_existing_env">remove_existing_env</code></td>
<td>
<p>delete the conda environment if already exists</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>

<hr>
<h2 id='interactive_login_authentication'>Manages authentication and acquires an authorization token in interactive login workflows.</h2><span id='topic+interactive_login_authentication'></span>

<h3>Description</h3>

<p>Interactive login authentication is suitable for local experimentation on your own computer, and is the
default authentication model when using Azure Machine Learning SDK.
The constructor of the class will prompt you to login. The constructor then will save the credentials
for any subsequent attempts. If you are already logged in with the Azure CLI or have logged-in before, the
constructor will load the existing credentials without prompt.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>interactive_login_authentication(
  force = FALSE,
  tenant_id = NULL,
  cloud = "AzureCloud"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="interactive_login_authentication_+3A_force">force</code></td>
<td>
<p>Indicates whether &quot;az login&quot; will be run even if the old &quot;az login&quot; is still valid.</p>
</td></tr>
<tr><td><code id="interactive_login_authentication_+3A_tenant_id">tenant_id</code></td>
<td>
<p>The string id of the active directory tenant that the service
identity belongs to. This is can be used to specify a specific tenant when
you have access to multiple tenants. If unspecified, the default tenant will be used.</p>
</td></tr>
<tr><td><code id="interactive_login_authentication_+3A_cloud">cloud</code></td>
<td>
<p>The name of the target cloud. Can be one of &quot;AzureCloud&quot;, &quot;AzureChinaCloud&quot;, or
&quot;AzureUSGovernment&quot;. If no cloud is specified, &quot;AzureCloud&quot; is used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>InteractiveLoginAuthentication</code> object
</p>


<h3>Examples</h3>

<pre>interactive_auth &lt;- interactive_login_authentication(tenant_id="your-tenant-id")

ws &lt;- get_workspace("&lt;your workspace name&gt;",
                    "&lt;your subscription ID&gt;",
                    "&lt;your resource group&gt;",
                    auth = interactive_auth)
</pre>


<h3>See Also</h3>

<p><code><a href="#topic+get_workspace">get_workspace()</a></code> <code><a href="#topic+service_principal_authentication">service_principal_authentication()</a></code>
</p>

<hr>
<h2 id='invoke_webservice'>Call a web service with the provided input</h2><span id='topic+invoke_webservice'></span>

<h3>Description</h3>

<p>Invoke the web service with the provided input and to receive
predictions from the deployed model. The structure of the
provided input data needs to match what the service's scoring
script and model expect. See the &quot;Details&quot; section of
<code>inference_config()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>invoke_webservice(webservice, input_data)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="invoke_webservice_+3A_webservice">webservice</code></td>
<td>
<p>The <code>LocalWebservice</code>, <code>AciWebservice</code>, or
<code>AksWebservice</code> object.</p>
</td></tr>
<tr><td><code id="invoke_webservice_+3A_input_data">input_data</code></td>
<td>
<p>The input data to invoke the web service with. This is
the data your model expects as an input to run predictions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Instead of invoking the web service using <code>invoke_webservice()</code>, you can
also consume the web service using the service's REST API. If you've
enabled key-based authentication for your service, you will need to provide
a service key as a token in your request header
(see <code>get_webservice_keys()</code>). If you've enabled token-based
authentication, you will need to provide an JWT token as a bearer
token in your request header (see <code>get_webservice_token()</code>).
</p>
<p>To get the REST API address for the service's scoring endpoint, you can
access the following property from the Webservice object:
<code>service$scoring_uri</code>
</p>


<h3>Value</h3>

<p>A named list of the result of calling the web service. This will
return the predictions run from your model.
</p>

<hr>
<h2 id='keep_columns_from_dataset'>Keep the specified columns and drops all others from the dataset.</h2><span id='topic+keep_columns_from_dataset'></span>

<h3>Description</h3>

<p>Keep the specified columns and drops all others from the dataset.
If a timeseries column is dropped, the corresponding capabilities will be
dropped for the returned dataset as well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>keep_columns_from_dataset(dataset, columns, validate = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="keep_columns_from_dataset_+3A_dataset">dataset</code></td>
<td>
<p>The Tabular Dataset object</p>
</td></tr>
<tr><td><code id="keep_columns_from_dataset_+3A_columns">columns</code></td>
<td>
<p>The name or a list of names for the columns to keep.</p>
</td></tr>
<tr><td><code id="keep_columns_from_dataset_+3A_validate">validate</code></td>
<td>
<p>Indicates whether to validate if data can be loaded from the
returned dataset. The default is False. Validation requires that the data
source is accessible from current compute.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new Tabular Dataset object with only the specified columns kept.
</p>

<hr>
<h2 id='list_nodes_in_aml_compute'>Get the details (e.g IP address, port etc) of all the compute nodes in the
compute target</h2><span id='topic+list_nodes_in_aml_compute'></span>

<h3>Description</h3>

<p>Get the details (e.g IP address, port etc) of all the compute nodes in the
compute target
</p>


<h3>Usage</h3>

<pre><code class='language-R'>list_nodes_in_aml_compute(cluster)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="list_nodes_in_aml_compute_+3A_cluster">cluster</code></td>
<td>
<p>cluster object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Details of all the compute nodes in the cluster in data frame
</p>

<hr>
<h2 id='list_secrets'>List the secrets in a keyvault</h2><span id='topic+list_secrets'></span>

<h3>Description</h3>

<p>Returns the list of secret names for all the secrets in the keyvault
associated with the workspace.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>list_secrets(keyvault)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="list_secrets_+3A_keyvault">keyvault</code></td>
<td>
<p>The <code>Keyvault</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of secret names.
</p>

<hr>
<h2 id='list_supported_vm_sizes'>List the supported VM sizes in a region</h2><span id='topic+list_supported_vm_sizes'></span>

<h3>Description</h3>

<p>List the supported VM sizes in a region
</p>


<h3>Usage</h3>

<pre><code class='language-R'>list_supported_vm_sizes(workspace, location = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="list_supported_vm_sizes_+3A_workspace">workspace</code></td>
<td>
<p>The <code>Workspace</code> object.</p>
</td></tr>
<tr><td><code id="list_supported_vm_sizes_+3A_location">location</code></td>
<td>
<p>A string of the location of the cluster. If not specified,
will default to the workspace location.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame of supported VM sizes in a region with name of the VM, VCPUs,
RAM.
</p>

<hr>
<h2 id='list_workspaces'>List all workspaces that the user has access to in a subscription ID</h2><span id='topic+list_workspaces'></span>

<h3>Description</h3>

<p>List all workspaces that the user has access to in the specified
<code>subscription_id</code> parameter. The list of workspaces can be filtered
based on the resource group.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>list_workspaces(subscription_id, resource_group = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="list_workspaces_+3A_subscription_id">subscription_id</code></td>
<td>
<p>A string of the specified subscription ID to
list the workspaces in.</p>
</td></tr>
<tr><td><code id="list_workspaces_+3A_resource_group">resource_group</code></td>
<td>
<p>A string of the specified resource group to list
the workspaces. If <code>NULL</code> the method will list all the workspaces within
the specified subscription in.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list of <code>Workspace</code> objects where element name corresponds
to the workspace name.
</p>

<hr>
<h2 id='load_dataset_into_data_frame'>Load all records from the dataset into a dataframe.</h2><span id='topic+load_dataset_into_data_frame'></span>

<h3>Description</h3>

<p>Load all records from the dataset into a dataframe.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_dataset_into_data_frame(
  dataset,
  on_error = "null",
  out_of_range_datetime = "null"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="load_dataset_into_data_frame_+3A_dataset">dataset</code></td>
<td>
<p>The Tabular Dataset object.</p>
</td></tr>
<tr><td><code id="load_dataset_into_data_frame_+3A_on_error">on_error</code></td>
<td>
<p>How to handle any error values in the dataset, such as those
produced by an error while parsing values. Valid values are 'null' which replaces
them with NULL; and 'fail' which will result in an exception.</p>
</td></tr>
<tr><td><code id="load_dataset_into_data_frame_+3A_out_of_range_datetime">out_of_range_datetime</code></td>
<td>
<p>How to handle date-time values that are outside
the range supported by Pandas. Valid values are 'null' which replaces them with
NULL; and 'fail' which will result in an exception.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame.
</p>

<hr>
<h2 id='load_workspace_from_config'>Load workspace configuration details from a config file</h2><span id='topic+load_workspace_from_config'></span>

<h3>Description</h3>

<p>Returns a <code>Workspace</code> object for an existing Azure Machine Learning
workspace by reading the workspace configuration from a file. The method
provides a simple way of reusing the same workspace across multiple files or
projects. Users can save the workspace ARM properties using
<code>write_workspace_config()</code>, and use this method to load the same workspace
in different files or projects without retyping the workspace ARM properties.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_workspace_from_config(path = NULL, file_name = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="load_workspace_from_config_+3A_path">path</code></td>
<td>
<p>A string of the path to the config file or starting directory
for search. The parameter defaults to starting the search in the current
directory.</p>
</td></tr>
<tr><td><code id="load_workspace_from_config_+3A_file_name">file_name</code></td>
<td>
<p>A string that will override the config file name to
search for when path is a directory path.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>Workspace</code> object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+write_workspace_config">write_workspace_config()</a></code>
</p>

<hr>
<h2 id='local_webservice_deployment_config'>Create a deployment config for deploying a local web service</h2><span id='topic+local_webservice_deployment_config'></span>

<h3>Description</h3>

<p>You can deploy a model locally for limited testing and troubleshooting.
To do so, you will need to have Docker installed on your local machine.
</p>
<p>If you are using an Azure Machine Learning Compute Instance for
development, you can also deploy locally on your compute instance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>local_webservice_deployment_config(port = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="local_webservice_deployment_config_+3A_port">port</code></td>
<td>
<p>An int of the local port on which to expose the service's
HTTP endpoint.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>LocalWebserviceDeploymentConfiguration</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
deployment_config &lt;- local_webservice_deployment_config(port = 8890)

## End(Not run)
</code></pre>

<hr>
<h2 id='log_accuracy_table_to_run'>Log an accuracy table metric to a run</h2><span id='topic+log_accuracy_table_to_run'></span>

<h3>Description</h3>

<p>The accuracy table metric is a multi-use non-scalar metric that can be
used to produce multiple types of line charts that vary continuously
over the space of predicted probabilities. Examples of these charts are
ROC, precision-recall, and lift curves.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_accuracy_table_to_run(name, value, description = "", run = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="log_accuracy_table_to_run_+3A_name">name</code></td>
<td>
<p>A string of the name of the metric.</p>
</td></tr>
<tr><td><code id="log_accuracy_table_to_run_+3A_value">value</code></td>
<td>
<p>A named list containing name, version, and data properties.</p>
</td></tr>
<tr><td><code id="log_accuracy_table_to_run_+3A_description">description</code></td>
<td>
<p>(Optional) A string of the metric description.</p>
</td></tr>
<tr><td><code id="log_accuracy_table_to_run_+3A_run">run</code></td>
<td>
<p>The <code>Run</code> object. If not specified, will default
to the current run from the service context.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The calculation of the accuracy table is similar to the calculation of
an ROC curve. An ROC curve stores true positive rates and false positive
rates at many different probability thresholds. The accuracy table stores
the raw number of true positives, false positives, true negatives, and
false negatives at many probability thresholds.
</p>
<p>There are two methods used for selecting thresholds: &quot;probability&quot; and
&quot;percentile.&quot; They differ in how they sample from the space of predicted
probabilities.
</p>
<p>Probability thresholds are uniformly spaced thresholds between 0 and 1.
If NUM_POINTS were 5 the probability thresholds would be
c(0.0, 0.25, 0.5, 0.75, 1.0).
</p>
<p>Percentile thresholds are spaced according to the distribution of predicted
probabilities. Each threshold corresponds to the percentile of the data at
a probability threshold. For example, if NUM_POINTS were 5, then the first
threshold would be at the 0th percentile, the second at the 25th percentile,
the third at the 50th, and so on.
</p>
<p>The probability tables and percentile tables are both 3D lists where the
first dimension represents the class label, the second dimension represents
the sample at one threshold (scales with NUM_POINTS), and the third dimension
always has 4 values: TP, FP, TN, FN, and always in that order.
</p>
<p>The confusion values (TP, FP, TN, FN) are computed with the one vs. rest
strategy. See the following link for more details:
https://en.wikipedia.org/wiki/Multiclass_classification.
</p>
<p>N = # of samples in validation dataset (200 in example),
M = # thresholds = # samples taken from the probability space (5 in example),
C = # classes in full dataset (3 in example)
</p>
<p>Some invariants of the accuracy table:
</p>

<ul>
<li><p> TP + FP + TN + FN = N for all thresholds for all classes
</p>
</li>
<li><p> TP + FN is the same at all thresholds for any class
</p>
</li>
<li><p> TN + FP is the same at all thresholds for any class
</p>
</li>
<li><p> Probability tables and percentile tables have shape (C, M, 4)
</p>
</li></ul>

<p>Note: M can be any value and controls the resolution of the charts.
This is independent of the dataset, is defined when calculating metrics,
and trades off storage space, computation time, and resolution.
</p>
<p>Class labels should be strings, confusion values should be integers,
and thresholds should be doubles.
</p>


<h3>Value</h3>

<p>None
</p>

<hr>
<h2 id='log_confusion_matrix_to_run'>Log a confusion matrix metric to a run</h2><span id='topic+log_confusion_matrix_to_run'></span>

<h3>Description</h3>

<p>Log a confusion matrix metric to a run
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_confusion_matrix_to_run(name, value, description = "", run = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="log_confusion_matrix_to_run_+3A_name">name</code></td>
<td>
<p>A string of the name of the metric.</p>
</td></tr>
<tr><td><code id="log_confusion_matrix_to_run_+3A_value">value</code></td>
<td>
<p>A named list containing name, version, and data properties.</p>
</td></tr>
<tr><td><code id="log_confusion_matrix_to_run_+3A_description">description</code></td>
<td>
<p>(Optional) A string of the metric description.</p>
</td></tr>
<tr><td><code id="log_confusion_matrix_to_run_+3A_run">run</code></td>
<td>
<p>The <code>Run</code> object. If not specified, will default
to the current run from the service context.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>

<hr>
<h2 id='log_image_to_run'>Log an image metric to a run</h2><span id='topic+log_image_to_run'></span>

<h3>Description</h3>

<p>Log an image to the run with the give metric name. Use
<code>log_image_to_run()</code> to log an image file or ggplot2 plot to the
run. These images will be visible and comparable in the run
record.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_image_to_run(name, path = NULL, plot = NULL, description = "", run = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="log_image_to_run_+3A_name">name</code></td>
<td>
<p>A string of the name of the metric.</p>
</td></tr>
<tr><td><code id="log_image_to_run_+3A_path">path</code></td>
<td>
<p>A string of the path or stream of the image.</p>
</td></tr>
<tr><td><code id="log_image_to_run_+3A_plot">plot</code></td>
<td>
<p>The ggplot2 plot to log as an image.</p>
</td></tr>
<tr><td><code id="log_image_to_run_+3A_description">description</code></td>
<td>
<p>(Optional) A string of the metric description.</p>
</td></tr>
<tr><td><code id="log_image_to_run_+3A_run">run</code></td>
<td>
<p>The <code>Run</code> object. If not specified, will default
to the current run from the service context.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>

<hr>
<h2 id='log_list_to_run'>Log a vector metric value to a run</h2><span id='topic+log_list_to_run'></span>

<h3>Description</h3>

<p>Log a vector with the given metric name to the run.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_list_to_run(name, value, description = "", run = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="log_list_to_run_+3A_name">name</code></td>
<td>
<p>A string of the name of metric.</p>
</td></tr>
<tr><td><code id="log_list_to_run_+3A_value">value</code></td>
<td>
<p>The vector of elements to log.</p>
</td></tr>
<tr><td><code id="log_list_to_run_+3A_description">description</code></td>
<td>
<p>(Optional) A string of the metric description.</p>
</td></tr>
<tr><td><code id="log_list_to_run_+3A_run">run</code></td>
<td>
<p>The <code>Run</code> object. If not specified, will default
to the current run from the service context.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>


<h3>Examples</h3>

<pre>log_list_to_run("Accuracies", c(0.6, 0.7, 0.87))
</pre>

<hr>
<h2 id='log_metric_to_run'>Log a metric to a run</h2><span id='topic+log_metric_to_run'></span>

<h3>Description</h3>

<p>Log a numerical or string value with the given metric name
to the run. Logging a metric to a run causes that metric to
be stored in the run record in the experiment. You can log
the same metric multiple times within a run, the result being
considered a vector of that metric.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_metric_to_run(name, value, run = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="log_metric_to_run_+3A_name">name</code></td>
<td>
<p>A string of the name of the metric.</p>
</td></tr>
<tr><td><code id="log_metric_to_run_+3A_value">value</code></td>
<td>
<p>The value of the metric.</p>
</td></tr>
<tr><td><code id="log_metric_to_run_+3A_run">run</code></td>
<td>
<p>The <code>Run</code> object. If not specified, will default
to the current run from the service context.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>


<h3>Examples</h3>

<pre>log_metric_to_run("Accuracy", 0.95)
</pre>

<hr>
<h2 id='log_predictions_to_run'>Log a predictions metric to a run</h2><span id='topic+log_predictions_to_run'></span>

<h3>Description</h3>

<p><code>log_predictions_to_run()</code> logs a metric score that can be used to
compare the distributions of true target values to the distribution
of predicted values for a regression task.
</p>
<p>The predictions are binned and standard deviations are calculated
for error bars on a line chart.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_predictions_to_run(name, value, description = "", run = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="log_predictions_to_run_+3A_name">name</code></td>
<td>
<p>A string of the name of the metric.</p>
</td></tr>
<tr><td><code id="log_predictions_to_run_+3A_value">value</code></td>
<td>
<p>A named list containing name, version, and data properties.</p>
</td></tr>
<tr><td><code id="log_predictions_to_run_+3A_description">description</code></td>
<td>
<p>(Optional) A string of the metric description.</p>
</td></tr>
<tr><td><code id="log_predictions_to_run_+3A_run">run</code></td>
<td>
<p>The <code>Run</code> object. If not specified, will default
to the current run from the service context.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>


<h3>Examples</h3>

<pre>data &lt;- list("bin_averages" = c(0.25, 0.75),
             "bin_errors" = c(0.013, 0.042),
             "bin_counts" = c(56, 34),
             "bin_edges" = c(0.0, 0.5, 1.0))
predictions &lt;- list("schema_type" = "predictions",
                    "schema_version" = "v1",
                    "data" = data)
log_predictions_to_run("mypredictions", predictions)
</pre>

<hr>
<h2 id='log_residuals_to_run'>Log a residuals metric to a run</h2><span id='topic+log_residuals_to_run'></span>

<h3>Description</h3>

<p><code>log_residuals_to_run()</code> logs the data needed to display a histogram
of residuals for a regression task. The residuals are <code>predicted - actual</code>.
</p>
<p>There should be one more edge than the number of counts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_residuals_to_run(name, value, description = "", run = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="log_residuals_to_run_+3A_name">name</code></td>
<td>
<p>A string of the name of the metric.</p>
</td></tr>
<tr><td><code id="log_residuals_to_run_+3A_value">value</code></td>
<td>
<p>A named list containing name, version, and data properties.</p>
</td></tr>
<tr><td><code id="log_residuals_to_run_+3A_description">description</code></td>
<td>
<p>(Optional) A string of the metric description.</p>
</td></tr>
<tr><td><code id="log_residuals_to_run_+3A_run">run</code></td>
<td>
<p>The <code>Run</code> object. If not specified, will default
to the current run from the service context.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>


<h3>Examples</h3>

<pre>data &lt;- list("bin_edges" = c(50, 100, 200, 300, 350),
             "bin_counts" = c(0.88, 20, 30, 50.99))
residuals &lt;- list("schema_type" = "residuals",
                    "schema_version" = "v1",
                    "data" = data)
log_predictions_to_run("myresiduals", predictions)
</pre>

<hr>
<h2 id='log_row_to_run'>Log a row metric to a run</h2><span id='topic+log_row_to_run'></span>

<h3>Description</h3>

<p>Using <code>log_row_to_run()</code> creates a metric with multiple columns
as described in <code>...</code>. Each named parameter generates a column
with the value specified. <code>log_row_to_run()</code> can be called once
to log an arbitrary tuple, or multiple times in a loop to generate
a complete table.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_row_to_run(name, description = "", run = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="log_row_to_run_+3A_name">name</code></td>
<td>
<p>A string of the name of metric.</p>
</td></tr>
<tr><td><code id="log_row_to_run_+3A_description">description</code></td>
<td>
<p>(Optional) A string of the metric description.</p>
</td></tr>
<tr><td><code id="log_row_to_run_+3A_run">run</code></td>
<td>
<p>The <code>Run</code> object. If not specified, will default
to the current run from the service context.</p>
</td></tr>
<tr><td><code id="log_row_to_run_+3A_...">...</code></td>
<td>
<p>Each named parameter generates a column with the value
specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>


<h3>Examples</h3>

<p>Log an arbitrary tuple:</p>
<pre>log_row_to_run("Y over X", x = 1, y = 0.4)
</pre>
<p>Log the complete table:</p>
<pre>citrus &lt;- c("orange", "lemon", "lime")
sizes &lt;- c(10, 7, 3)
for (i in seq_along(citrus)) {
    log_row_to_run("citrus", fruit = citrus[i], size = sizes[i])
}
</pre>

<hr>
<h2 id='log_table_to_run'>Log a table metric to a run</h2><span id='topic+log_table_to_run'></span>

<h3>Description</h3>

<p>Log a table metric with the given metric name to the run. The
table value is a named list where each element corresponds to
a column of the table.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_table_to_run(name, value, description = "", run = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="log_table_to_run_+3A_name">name</code></td>
<td>
<p>A string of the name of metric.</p>
</td></tr>
<tr><td><code id="log_table_to_run_+3A_value">value</code></td>
<td>
<p>The table value of the metric (a named list where the
element name corresponds to the column name).</p>
</td></tr>
<tr><td><code id="log_table_to_run_+3A_description">description</code></td>
<td>
<p>(Optional) A string of the metric description.</p>
</td></tr>
<tr><td><code id="log_table_to_run_+3A_run">run</code></td>
<td>
<p>The <code>Run</code> object. If not specified, will default
to the current run from the service context.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>


<h3>Examples</h3>

<pre>log_table_to_run("Y over X",
                 list("x" = c(1, 2, 3), "y" = c(0.6, 0.7, 0.89)))
</pre>

<hr>
<h2 id='lognormal'>Specify a normal distribution of the form <code>exp(normal(mu, sigma))</code></h2><span id='topic+lognormal'></span>

<h3>Description</h3>

<p>Specify a normal distribution of the form <code>exp(normal(mu, sigma))</code>.
</p>
<p>The logarithm of the return value is normally distributed. When optimizing,
this variable is constrained to be positive.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lognormal(mu, sigma)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lognormal_+3A_mu">mu</code></td>
<td>
<p>A double of the mean of the normal distribution.</p>
</td></tr>
<tr><td><code id="lognormal_+3A_sigma">sigma</code></td>
<td>
<p>A double of the standard deviation of the normal distribution.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of the stochastic expression.
</p>


<h3>See Also</h3>

<p><code>random_parameter_sampling()</code>, <code>grid_parameter_sampling()</code>,
<code>bayesian_parameter_sampling()</code>
</p>

<hr>
<h2 id='loguniform'>Specify a log uniform distribution</h2><span id='topic+loguniform'></span>

<h3>Description</h3>

<p>Specify a log uniform distribution.
</p>
<p>A value is drawn according to <code>exp(uniform(min_value, max_value))</code> so that
the logarithm of the return value is uniformly distributed. When optimizing,
this variable is constrained to the interval
<code style="white-space: pre;">&#8288;[exp(min_value), exp(max_value)]&#8288;</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loguniform(min_value, max_value)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="loguniform_+3A_min_value">min_value</code></td>
<td>
<p>A double where the minimum value in the range will be
<code>exp(min_value)</code> (inclusive).</p>
</td></tr>
<tr><td><code id="loguniform_+3A_max_value">max_value</code></td>
<td>
<p>A double where the maximum value in the range will be
<code>exp(min_value)</code> (inclusive).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of the stochastic expression.
</p>


<h3>See Also</h3>

<p><code>random_parameter_sampling()</code>, <code>grid_parameter_sampling()</code>,
<code>bayesian_parameter_sampling()</code>
</p>

<hr>
<h2 id='median_stopping_policy'>Define a median stopping policy for early termination of HyperDrive runs</h2><span id='topic+median_stopping_policy'></span>

<h3>Description</h3>

<p>Median stopping is an early termination policy based on running averages of
primary metrics reported by the runs. This policy computes running averages
across all training runs and terminates runs whose performance is worse than
the median of the running averages. Specifically, a run will be canceled at
interval N if its best primary metric reported up to interval N is worse than
the median of the running averages for intervals 1:N across all runs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>median_stopping_policy(evaluation_interval = 1L, delay_evaluation = 0L)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="median_stopping_policy_+3A_evaluation_interval">evaluation_interval</code></td>
<td>
<p>An integer of the frequency for applying policy.</p>
</td></tr>
<tr><td><code id="median_stopping_policy_+3A_delay_evaluation">delay_evaluation</code></td>
<td>
<p>An integer of the number of intervals for which to
delay the first evaluation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>MedianStoppingPolicy</code> object.
</p>


<h3>Details</h3>

<p>The median stopping policy takes the following optional configuration
parameters:
</p>

<ul>
<li> <p><code>evaluation_interval</code>: Optional. The frequency for applying the policy.
Each time the training script logs the primary metric counts as one
interval.
</p>
</li>
<li> <p><code>delay_evaluation</code>: Optional. The number of intervals to delay the
policy evaluation. Use this parameter to avoid premature termination
of training runs. If specified, the policy applies every multiple of
<code>evaluation_interval</code> that is greater than or equal to <code>delay_evaluation</code>.
</p>
</li></ul>

<p>This policy is inspired from the research publication
<a href="https://ai.google/research/pubs/pub46180">Google Vizier: A Service for Black-Box Optimization</a>.
</p>
<p>If you are looking for a conservative policy that provides savings without
terminating promising jobs, you can use a <code>MedianStoppingPolicy</code> with
<code>evaluation_interval = 1</code> and <code>delay_evaluation = 5</code>. These are conservative
settings that can provide approximately 25%-35% savings with no loss on
the primary metric (based on our evaluation data).
</p>


<h3>Examples</h3>

<pre><code class='language-R'># In this example, the early termination policy is applied at every
# interval starting at evaluation interval 5. A run will be terminated at
# interval 5 if its best primary metric is worse than the median of the
# running averages over intervals 1:5 across all training runs
## Not run: 
early_termination_policy = median_stopping_policy(evaluation_interval = 1L,
                                                  delay_evaluation = 5L)

## End(Not run)
</code></pre>

<hr>
<h2 id='merge_results'>Combine the results from the parallel training.</h2><span id='topic+merge_results'></span>

<h3>Description</h3>

<p>Combine the results from the parallel training.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>merge_results(node_count, process_count_per_node, run, source_directory)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="merge_results_+3A_node_count">node_count</code></td>
<td>
<p>Number of nodes in the AmlCompute cluster.</p>
</td></tr>
<tr><td><code id="merge_results_+3A_process_count_per_node">process_count_per_node</code></td>
<td>
<p>Number of processes per node.</p>
</td></tr>
<tr><td><code id="merge_results_+3A_run">run</code></td>
<td>
<p>The run object whose output needs to be combined.</p>
</td></tr>
<tr><td><code id="merge_results_+3A_source_directory">source_directory</code></td>
<td>
<p>The directory where the output from the run
would be downloaded.</p>
</td></tr>
</table>

<hr>
<h2 id='mount_file_dataset'>Create a context manager for mounting file streams defined by the dataset as local files.</h2><span id='topic+mount_file_dataset'></span>

<h3>Description</h3>

<p>Create a context manager for mounting file streams defined by the dataset as local files.
A context manager will be returned to manage the lifecycle of the mount.
To mount, you will need to enter the context manager and to unmount, exit from
the context manager. Mount is only supported on Unix or Unix-like operating systems
and libfuse must be present. If you are running inside a docker container, the docker
container must be started with the <code>--privileged</code> flag or started with
<code style="white-space: pre;">&#8288;--cap-add SYS_ADMIN --device /dev/fuse&#8288;</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mount_file_dataset(dataset, mount_point = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mount_file_dataset_+3A_dataset">dataset</code></td>
<td>
<p>The Dataset object.</p>
</td></tr>
<tr><td><code id="mount_file_dataset_+3A_mount_point">mount_point</code></td>
<td>
<p>The local directory to mount the files to. If NULL, the
data will be mounted into a temporary directory.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a context manager for managing the lifecycle of the mount of
type <code>azureml.dataprep.fuse.daemon.MountContext</code>.
</p>

<hr>
<h2 id='normal'>Specify a real value that is normally-distributed with mean <code>mu</code> and standard
deviation <code>sigma</code></h2><span id='topic+normal'></span>

<h3>Description</h3>

<p>Specify a real value that is normally-distributed with mean <code>mu</code> and
standard deviation <code>sigma</code>.
</p>
<p>When optimizing, this is an unconstrained variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normal(mu, sigma)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="normal_+3A_mu">mu</code></td>
<td>
<p>A double of the mean of the normal distribution.</p>
</td></tr>
<tr><td><code id="normal_+3A_sigma">sigma</code></td>
<td>
<p>A double of the standard deviation of the normal distribution.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of the stochastic expression.
</p>


<h3>See Also</h3>

<p><code>random_parameter_sampling()</code>, <code>grid_parameter_sampling()</code>,
<code>bayesian_parameter_sampling()</code>
</p>

<hr>
<h2 id='package_model'>Create a model package that packages all the assets needed to host a
model as a web service</h2><span id='topic+package_model'></span>

<h3>Description</h3>

<p>In some cases, you might want to create a Docker image without deploying
the model (for example, if you plan to deploy to Azure App Service). Or
you might want to download the image and run it on a local Docker installation.
You might even want to download the files used to build the image, inspect
them, modify them, and build the image manually.
</p>
<p>Model packaging enables you to do these things. <code>package_model()</code> packages all
the assets needed to host a model as a web service and allows you to download
either a fully built Docker image or the files needed to build one. There are
two ways to use model packaging:
</p>

<ul>
<li> <p><strong>Download a packaged model</strong>: Download a Docker image that contains the model
and other files needed to host it as a web service.
</p>
</li>
<li> <p><strong>Generate a Dockerfile</strong>: Download the Dockerfile, model, entry script, and
other assets needed to build a Docker image. You can then inspect the files or
make changes before you build the image locally. To use this method, make sure
to set <code>generate_dockerfile = TRUE</code>.
With either scenario, you will need to have Docker installed in your
development environment.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>package_model(workspace, models, inference_config, generate_dockerfile = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="package_model_+3A_workspace">workspace</code></td>
<td>
<p>The <code>Workspace</code> object.</p>
</td></tr>
<tr><td><code id="package_model_+3A_models">models</code></td>
<td>
<p>A list of <code>Model</code> objects to include in the package. Can
be an empty list.</p>
</td></tr>
<tr><td><code id="package_model_+3A_inference_config">inference_config</code></td>
<td>
<p>The <code>InferenceConfig</code> object to configure the
operation of the models.</p>
</td></tr>
<tr><td><code id="package_model_+3A_generate_dockerfile">generate_dockerfile</code></td>
<td>
<p>If <code>TRUE</code>, will create a Dockerfile that
can be run locally instead of building an image.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>ModelPackage</code> object.
</p>


<h3>See Also</h3>

<p><code>wait_for_model_package_creation()</code>, <code>get_model_package_container_registry()</code>,
<code>get_model_package_creation_logs()</code>, <code>pull_model_package_image()</code>,
<code>save_model_package_files()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Package a registered model
## Not run: 
ws &lt;- load_workspace_from_config()
model &lt;- get_model(ws, name = "my_model")
r_env &lt;- r_environment(name = "r_env")
inference_config &lt;- inference_config(entry_script = "score.R",
                                     source_directory = ".",
                                     environment = r_env)
package &lt;- package_model(ws,
                         models = list(model),
                         inference_config = inference_config)
wait_for_model_package_creation(show_output = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='plot_run_details'>Generate table of run details</h2><span id='topic+plot_run_details'></span>

<h3>Description</h3>

<p>Plot a table of run details including
</p>

<ul>
<li><p> ID
</p>
</li>
<li><p> Status
</p>
</li>
<li><p> Start Time
</p>
</li>
<li><p> Duration
</p>
</li>
<li><p> Script Name
</p>
</li>
<li><p> Arguments
</p>
</li>
<li><p> Link to Web Portal view
</p>
</li>
<li><p> Errors
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>plot_run_details(run)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_run_details_+3A_run">run</code></td>
<td>
<p>The <code>Run</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Datatable containing run details
</p>

<hr>
<h2 id='primary_metric_goal'>Define supported metric goals for hyperparameter tuning</h2><span id='topic+primary_metric_goal'></span>

<h3>Description</h3>

<p>A metric goal is used to determine whether a higher value for a metric
is better or worse. Metric goals are used when comparing runs based on
the primary metric. For example, you may want to maximize accuracy or
minimize error.
</p>
<p>The primary metric name and goal are specified to <code>hyperdrive_config()</code>
when you configure a HyperDrive run.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>primary_metric_goal(goal)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="primary_metric_goal_+3A_goal">goal</code></td>
<td>
<p>A string of the metric goal (either &quot;MAXIMIZE&quot; or &quot;MINIMIZE&quot;).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>PrimaryMetricGoal</code> object.
</p>

<hr>
<h2 id='promote_headers_behavior'>Defines options for how column headers are processed when reading data from files to create a dataset.</h2><span id='topic+promote_headers_behavior'></span>

<h3>Description</h3>

<p>Defines options for how column headers are processed when reading data from files to create a dataset.
These enumeration values are used in the Dataset class method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>promote_headers_behavior(option)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="promote_headers_behavior_+3A_option">option</code></td>
<td>
<p>An integer corresponding to an option for how column headers are to be processed
</p>

<ul>
<li><p> 0: NO_HEADERS No column headers are read
</p>
</li>
<li><p> 1: ONLY_FIRST_FILE_HAS_HEADERS Read headers only from first row of first file, everything else is data.
</p>
</li>
<li><p> 2: COMBINE_ALL_FILES_HEADERS Read headers from first row of each file, combining identically named columns.
</p>
</li>
<li><p> 3: ALL_FILES_HAVE_SAME_HEADERS Read headers from first row of first file, drops first row from other files.
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>The PromoteHeadersBehavior object.
</p>

<hr>
<h2 id='pull_model_package_image'>Pull the Docker image from a <code>ModelPackage</code> to your local
Docker environment</h2><span id='topic+pull_model_package_image'></span>

<h3>Description</h3>

<p>Pull the Docker image from a created <code>ModelPackage</code> to your
local Docker environment. The output of this call will
display the name of the image. For example:
<code style="white-space: pre;">&#8288;Status: Downloaded newer image for myworkspacef78fd10.azurecr.io/package:20190822181338&#8288;</code>.
</p>
<p>This can only be used with a Docker image <code>ModelPackage</code> (where
<code>package_model()</code> was called with <code>generate_dockerfile = FALSE</code>).
</p>
<p>After you've pulled the image, you can start a local container based
on this image using Docker commands.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pull_model_package_image(package)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pull_model_package_image_+3A_package">package</code></td>
<td>
<p>The <code>ModelPackage</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>


<h3>See Also</h3>

<p><code>package_model()</code>
</p>

<hr>
<h2 id='qlognormal'>Specify a normal distribution of the form
<code>round(exp(normal(mu, sigma)) / q) * q</code></h2><span id='topic+qlognormal'></span>

<h3>Description</h3>

<p>Specify a normal distribution of the form
<code>round(exp(normal(mu, sigma)) / q) * q</code>.
</p>
<p>Suitable for a discrete variable with respect to which the objective is
smooth and gets smoother with the size of the variable, which is bounded
from one side.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qlognormal(mu, sigma, q)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="qlognormal_+3A_mu">mu</code></td>
<td>
<p>A double of the mean of the normal distribution.</p>
</td></tr>
<tr><td><code id="qlognormal_+3A_sigma">sigma</code></td>
<td>
<p>A double of the standard deviation of the normal distribution.</p>
</td></tr>
<tr><td><code id="qlognormal_+3A_q">q</code></td>
<td>
<p>An integer of the smoothing factor.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of the stochastic expression.
</p>


<h3>See Also</h3>

<p><code>random_parameter_sampling()</code>, <code>grid_parameter_sampling()</code>,
<code>bayesian_parameter_sampling()</code>
</p>

<hr>
<h2 id='qloguniform'>Specify a uniform distribution of the form
<code style="white-space: pre;">&#8288;round(exp(uniform(min_value, max_value) / q) * q&#8288;</code></h2><span id='topic+qloguniform'></span>

<h3>Description</h3>

<p>Specify a uniform distribution of the form
<code style="white-space: pre;">&#8288;round(exp(uniform(min_value, max_value) / q) * q&#8288;</code>.
</p>
<p>This is suitable for a discrete variable with respect to which the objective
is &quot;smooth&quot;, and gets smoother with the size of the value, but which should
be bounded both above and below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qloguniform(min_value, max_value, q)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="qloguniform_+3A_min_value">min_value</code></td>
<td>
<p>A double of the minimum value in the range (inclusive).</p>
</td></tr>
<tr><td><code id="qloguniform_+3A_max_value">max_value</code></td>
<td>
<p>A double of the maximum value in the range (inclusive).</p>
</td></tr>
<tr><td><code id="qloguniform_+3A_q">q</code></td>
<td>
<p>An integer of the smoothing factor.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of the stochastic expression.
</p>


<h3>See Also</h3>

<p><code>random_parameter_sampling()</code>, <code>grid_parameter_sampling()</code>,
<code>bayesian_parameter_sampling()</code>
</p>

<hr>
<h2 id='qnormal'>Specify a normal distribution of the <code style="white-space: pre;">&#8288;form round(normal(mu, sigma) / q) * q&#8288;</code></h2><span id='topic+qnormal'></span>

<h3>Description</h3>

<p>Specify a normal distribution of the form <code>round(normal(mu, sigma) / q) * q</code>.
</p>
<p>Suitable for a discrete variable that probably takes a value around <code>mu</code>,
but is fundamentally unbounded.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qnormal(mu, sigma, q)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="qnormal_+3A_mu">mu</code></td>
<td>
<p>A double of the mean of the normal distribution.</p>
</td></tr>
<tr><td><code id="qnormal_+3A_sigma">sigma</code></td>
<td>
<p>A double of the standard deviation of the normal distribution.</p>
</td></tr>
<tr><td><code id="qnormal_+3A_q">q</code></td>
<td>
<p>An integer of the smoothing factor.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of the stochastic expression.
</p>


<h3>See Also</h3>

<p><code>random_parameter_sampling()</code>, <code>grid_parameter_sampling()</code>,
<code>bayesian_parameter_sampling()</code>
</p>

<hr>
<h2 id='quniform'>Specify a uniform distribution of the form
<code>round(uniform(min_value, max_value) / q) * q</code></h2><span id='topic+quniform'></span>

<h3>Description</h3>

<p>Specify a uniform distribution of the form
<code>round(uniform(min_value, max_value) / q) * q</code>.
</p>
<p>This is suitable for a discrete value with respect to which the objective
is still somewhat &quot;smooth&quot;, but which should be bounded both above and below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quniform(min_value, max_value, q)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="quniform_+3A_min_value">min_value</code></td>
<td>
<p>A double of the minimum value in the range (inclusive).</p>
</td></tr>
<tr><td><code id="quniform_+3A_max_value">max_value</code></td>
<td>
<p>A double of the maximum value in the range (inclusive).</p>
</td></tr>
<tr><td><code id="quniform_+3A_q">q</code></td>
<td>
<p>An integer of the smoothing factor.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of the stochastic expression.
</p>


<h3>See Also</h3>

<p><code>random_parameter_sampling()</code>, <code>grid_parameter_sampling()</code>,
<code>bayesian_parameter_sampling()</code>
</p>

<hr>
<h2 id='r_environment'>Create an environment</h2><span id='topic+r_environment'></span>

<h3>Description</h3>

<p>Configure the R environment to be used for training or web service
deployments. When you submit a run or deploy a model, Azure ML builds a
Docker image and creates a conda environment with your specifications from
your <code>Environment</code> object within that Docker container.
</p>
<p>If the <code>custom_docker_image</code> parameter
is not set, Azure ML will build a predefined base image (CPU or GPU
depending on the <code>use_gpu</code> flag) and install any R packages specified in the
<code>cran_packages</code>, <code>github_packages</code>, or <code>custom_url_packages</code> parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r_environment(
  name,
  version = NULL,
  environment_variables = NULL,
  r_version = NULL,
  rscript_path = NULL,
  snapshot_date = NULL,
  cran_packages = NULL,
  github_packages = NULL,
  custom_url_packages = NULL,
  bioconductor_packages = NULL,
  custom_docker_image = NULL,
  image_registry_details = NULL,
  use_gpu = FALSE,
  shm_size = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="r_environment_+3A_name">name</code></td>
<td>
<p>A string of the name of the environment.</p>
</td></tr>
<tr><td><code id="r_environment_+3A_version">version</code></td>
<td>
<p>A string of the version of the environment.</p>
</td></tr>
<tr><td><code id="r_environment_+3A_environment_variables">environment_variables</code></td>
<td>
<p>A named list of environment variables names
and values. These environment variables are set on the process where the user
script is being executed.</p>
</td></tr>
<tr><td><code id="r_environment_+3A_r_version">r_version</code></td>
<td>
<p>The version of R to be installed.</p>
</td></tr>
<tr><td><code id="r_environment_+3A_rscript_path">rscript_path</code></td>
<td>
<p>The Rscript path to use if an environment build is not required.
The path specified gets used to call the user script.</p>
</td></tr>
<tr><td><code id="r_environment_+3A_snapshot_date">snapshot_date</code></td>
<td>
<p>Date of MRAN snapshot to use.</p>
</td></tr>
<tr><td><code id="r_environment_+3A_cran_packages">cran_packages</code></td>
<td>
<p>A list of <code>cran_package</code> objects to be installed.</p>
</td></tr>
<tr><td><code id="r_environment_+3A_github_packages">github_packages</code></td>
<td>
<p>A list of <code>github_package</code> objects to be installed.</p>
</td></tr>
<tr><td><code id="r_environment_+3A_custom_url_packages">custom_url_packages</code></td>
<td>
<p>A character vector of packages to be installed
from local directory or custom URL.</p>
</td></tr>
<tr><td><code id="r_environment_+3A_bioconductor_packages">bioconductor_packages</code></td>
<td>
<p>A character vector of packages to be installed
from Bioconductor.</p>
</td></tr>
<tr><td><code id="r_environment_+3A_custom_docker_image">custom_docker_image</code></td>
<td>
<p>A string of the name of the Docker image from
which the image to use for training or deployment will be built. If not set,
a predefined Docker image will be used. To use an image from a private Docker
repository, you will also have to specify the <code>image_registry_details</code> parameter.</p>
</td></tr>
<tr><td><code id="r_environment_+3A_image_registry_details">image_registry_details</code></td>
<td>
<p>A <code>ContainerRegistry</code> object of the details of
the Docker image registry for the custom Docker image.</p>
</td></tr>
<tr><td><code id="r_environment_+3A_use_gpu">use_gpu</code></td>
<td>
<p>Indicates whether the environment should support GPUs.
If <code>TRUE</code>, a predefined GPU-based Docker image will be used in the environment.
If <code>FALSE</code>, a predefined CPU-based image will be used. Predefined Docker images
(CPU or GPU) will only be used if the <code>custom_docker_image</code> parameter is not set.</p>
</td></tr>
<tr><td><code id="r_environment_+3A_shm_size">shm_size</code></td>
<td>
<p>A string for the size of the Docker container's shared
memory block. For more information, see
<a href="https://docs.docker.com/engine/reference/run/">Docker run reference</a>
If not set, a default value of <code>'2g'</code> is used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>Environment</code> object.
</p>


<h3>Details</h3>

<p>Once built, the Docker image appears in the Azure Container Registry
associated with your workspace, by default. The repository name has the form
<em>azureml/azureml_&lt;uuid&gt;</em>. The unique identifier (<em>uuid</em>) part corresponds to
a hash computed from the environment configuration. This allows the service
to determine whether an image corresponding to the given environment already
exists for reuse.
</p>
<p>If you make changes to an existing environment, such as adding an R package,
a new version of the environment is created when you either submit a run,
deploy a model, or manually register the environment. The versioning allows
you to view changes to the environment over time.
</p>


<h3>Predefined Docker images</h3>

<p>When submitting a training job or deploying a model, Azure ML runs your
training script or scoring script within a Docker container. If no custom
Docker image is specified with the <code>custom_docker_image</code> parameter, Azure
ML will build a predefined CPU or GPU Docker image. The predefine images extend
the Ubuntu 16.04 <a href="https://github.com/Azure/AzureML-Containers">Azure ML base images</a>
and include the following dependencies:
</p>

<table>
<tr>
 <td style="text-align: right;">
<strong>Dependencies</strong> </td><td style="text-align: right;"> <strong>Version</strong> </td><td style="text-align: right;"> <strong>Remarks</strong></td>
</tr>
<tr>
 <td style="text-align: right;">
azuremlsdk </td><td style="text-align: right;"> latest </td><td style="text-align: right;"> (from GitHub)</td>
</tr>
<tr>
 <td style="text-align: right;">
R </td><td style="text-align: right;"> 3.6.0 </td><td style="text-align: right;"> -</td>
</tr>
<tr>
 <td style="text-align: right;">
Commonly used R packages </td><td style="text-align: right;"> - </td><td style="text-align: right;"> 80+ of the most popular R packages for
data science, including the IRKernel, dplyr, shiny, ggplot2, tidyr, caret,
and nnet. For the full list of packages included, see
<a href="https://github.com/Azure/azureml-sdk-for-r/blob/master/misc/r-packages-docker.md">here</a>.</td>
</tr>
<tr>
 <td style="text-align: right;">
Python </td><td style="text-align: right;"> 3.7.0 </td><td style="text-align: right;"> -</td>
</tr>
<tr>
 <td style="text-align: right;">
azureml-defaults </td><td style="text-align: right;"> latest </td><td style="text-align: right;"> <code>azureml-defaults</code> contains the
<code>azureml-core</code> and <code>applicationinsights</code> packages of the Python SDK that
are required for tasks such as logging metrics, uploading artifacts, and
deploying models. (from pip)</td>
</tr>
<tr>
 <td style="text-align: right;">
rpy2 </td><td style="text-align: right;"> latest </td><td style="text-align: right;"> (from conda)</td>
</tr>
<tr>
 <td style="text-align: right;">
CUDA (GPU image only) </td><td style="text-align: right;"> 10.0 </td><td style="text-align: right;"> CuDNN (version 7) is also included
</td>
</tr>

</table>



<h3>See Also</h3>

<p><code>estimator()</code>, <code>inference_config()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The following example defines an environment that will build the default
# base CPU image.
## Not run: 
r_env &lt;- r_environment(name = 'myr_env',
                       version = '1')

## End(Not run)
</code></pre>

<hr>
<h2 id='randint'>Specify a set of random integers in the range <code style="white-space: pre;">&#8288;[0, upper)&#8288;</code></h2><span id='topic+randint'></span>

<h3>Description</h3>

<p>Specify a set of random integers in the range <code style="white-space: pre;">&#8288;[0, upper)&#8288;</code>
to sample the hyperparameters from.
</p>
<p>The semantics of this distribution is that there is no more
correlation in the loss function between nearby integer values,
as compared with more distant integer values. This is an
appropriate distribution for describing random seeds, for example.
If the loss function is probably more correlated for nearby integer
values, then you should probably use one of the &quot;quantized&quot; continuous
distributions, such as either <code>quniform()</code>, <code>qloguniform()</code>, <code>qnormal()</code>,
or <code>qlognormal()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>randint(upper)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="randint_+3A_upper">upper</code></td>
<td>
<p>An integer of the upper bound for the range of
integers (exclusive).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of the stochastic expression.
</p>


<h3>See Also</h3>

<p><code>random_parameter_sampling()</code>, <code>grid_parameter_sampling()</code>,
<code>bayesian_parameter_sampling()</code>
</p>

<hr>
<h2 id='random_parameter_sampling'>Define random sampling over a hyperparameter search space</h2><span id='topic+random_parameter_sampling'></span>

<h3>Description</h3>

<p>In random sampling, hyperparameter values are randomly selected from the
defined search space. Random sampling allows the search space to include
both discrete and continuous hyperparameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>random_parameter_sampling(parameter_space, properties = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="random_parameter_sampling_+3A_parameter_space">parameter_space</code></td>
<td>
<p>A named list containing each parameter and its
distribution, e.g. <code>list("parameter" = distribution)</code>.</p>
</td></tr>
<tr><td><code id="random_parameter_sampling_+3A_properties">properties</code></td>
<td>
<p>A named list of additional properties for the algorithm.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>RandomParameterSampling</code> object.
</p>


<h3>Details</h3>

<p>In this sampling algorithm, parameter values are chosen from a set of
discrete values or a distribution over a continuous range. Functions you can
use include:
<code>choice()</code>, <code>randint()</code>, <code>uniform()</code>, <code>quniform()</code>, <code>loguniform()</code>,
<code>qloguniform()</code>, <code>normal()</code>, <code>qnormal()</code>, <code>lognormal()</code>, and <code>qlognormal()</code>.
</p>


<h3>See Also</h3>

<p><code>choice()</code>, <code>randint()</code>, <code>uniform()</code>, <code>quniform()</code>, <code>loguniform()</code>,
<code>qloguniform()</code>, <code>normal()</code>, <code>qnormal()</code>, <code>lognormal()</code>, <code>qlognormal()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
param_sampling &lt;- random_parameter_sampling(list("learning_rate" = normal(10, 3),
                                                 "keep_probability" = uniform(0.05, 0.1),
                                                 "batch_size" = choice(c(16, 32, 64, 128))))

## End(Not run)
</code></pre>

<hr>
<h2 id='random_split_dataset'>Split file streams in the dataset into two parts randomly and approximately by the percentage specified.</h2><span id='topic+random_split_dataset'></span>

<h3>Description</h3>

<p>Split file streams in the dataset into two parts randomly and approximately by the percentage specified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>random_split_dataset(dataset, percentage, seed = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="random_split_dataset_+3A_dataset">dataset</code></td>
<td>
<p>The Dataset object.</p>
</td></tr>
<tr><td><code id="random_split_dataset_+3A_percentage">percentage</code></td>
<td>
<p>The approximate percentage to split the Dataset by. This must
be a number between 0.0 and 1.0.</p>
</td></tr>
<tr><td><code id="random_split_dataset_+3A_seed">seed</code></td>
<td>
<p>An optional seed to use for the random generator.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new Dataset object representing the two datasets after the split.
</p>

<hr>
<h2 id='register_azure_blob_container_datastore'>Register an Azure blob container as a datastore</h2><span id='topic+register_azure_blob_container_datastore'></span>

<h3>Description</h3>

<p>Register an Azure blob container as a datastore. You can choose to use
either the SAS token or the storage account key.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>register_azure_blob_container_datastore(
  workspace,
  datastore_name,
  container_name,
  account_name,
  sas_token = NULL,
  account_key = NULL,
  protocol = NULL,
  endpoint = NULL,
  overwrite = FALSE,
  create_if_not_exists = FALSE,
  skip_validation = FALSE,
  blob_cache_timeout = NULL,
  grant_workspace_access = FALSE,
  subscription_id = NULL,
  resource_group = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="register_azure_blob_container_datastore_+3A_workspace">workspace</code></td>
<td>
<p>The <code>Workspace</code> object.</p>
</td></tr>
<tr><td><code id="register_azure_blob_container_datastore_+3A_datastore_name">datastore_name</code></td>
<td>
<p>A string of the name of the datastore. The name
must be case insensitive and can only contain alphanumeric characters and
underscores.</p>
</td></tr>
<tr><td><code id="register_azure_blob_container_datastore_+3A_container_name">container_name</code></td>
<td>
<p>A string of the name of the Azure blob container.</p>
</td></tr>
<tr><td><code id="register_azure_blob_container_datastore_+3A_account_name">account_name</code></td>
<td>
<p>A string of the storage account name.</p>
</td></tr>
<tr><td><code id="register_azure_blob_container_datastore_+3A_sas_token">sas_token</code></td>
<td>
<p>A string of the account SAS token.</p>
</td></tr>
<tr><td><code id="register_azure_blob_container_datastore_+3A_account_key">account_key</code></td>
<td>
<p>A string of the storage account key.</p>
</td></tr>
<tr><td><code id="register_azure_blob_container_datastore_+3A_protocol">protocol</code></td>
<td>
<p>A string of the protocol to use to connect to the
blob container. If <code>NULL</code>, defaults to <code>'https'</code>.</p>
</td></tr>
<tr><td><code id="register_azure_blob_container_datastore_+3A_endpoint">endpoint</code></td>
<td>
<p>A string of the endpoint of the blob container.
If <code>NULL</code>, defaults to <code>'core.windows.net'</code>.</p>
</td></tr>
<tr><td><code id="register_azure_blob_container_datastore_+3A_overwrite">overwrite</code></td>
<td>
<p>If <code>TRUE</code>, overwrites an existing datastore. If
the datastore does not exist, it will create one.</p>
</td></tr>
<tr><td><code id="register_azure_blob_container_datastore_+3A_create_if_not_exists">create_if_not_exists</code></td>
<td>
<p>If <code>TRUE</code>, creates the blob container
if it does not exists.</p>
</td></tr>
<tr><td><code id="register_azure_blob_container_datastore_+3A_skip_validation">skip_validation</code></td>
<td>
<p>If <code>TRUE</code>, skips validation of storage keys.</p>
</td></tr>
<tr><td><code id="register_azure_blob_container_datastore_+3A_blob_cache_timeout">blob_cache_timeout</code></td>
<td>
<p>An integer of the cache timeout in seconds
when this blob is mounted. If <code>NULL</code>, defaults to no timeout (i.e.
blobs will be cached for the duration of the job when read).</p>
</td></tr>
<tr><td><code id="register_azure_blob_container_datastore_+3A_grant_workspace_access">grant_workspace_access</code></td>
<td>
<p>If <code>TRUE</code>, grants workspace Managed Identities
(MSI) access to the user storage account. This should be set to <code>TRUE</code> if the
storage account is in VNET. If <code>TRUE</code>, Azure ML will use the workspace MSI
token to grant access to the user storage account. It may take a while for
the granted access to reflect.</p>
</td></tr>
<tr><td><code id="register_azure_blob_container_datastore_+3A_subscription_id">subscription_id</code></td>
<td>
<p>A string of the subscription id of the storage
account.</p>
</td></tr>
<tr><td><code id="register_azure_blob_container_datastore_+3A_resource_group">resource_group</code></td>
<td>
<p>A string of the resource group of the storage account.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>AzureBlobDatastore</code> object.
</p>


<h3>Details</h3>

<p>In general we recommend Azure Blob storage over Azure File storage. Both
standard and premium storage are available for blobs. Although more
expensive, we suggest premium storage due to faster throughput speeds that
may improve the speed of your training runs, particularly if you train
against a large dataset.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
ws &lt;- load_workspace_from_config()
ds &lt;- register_azure_blob_container_datastore(ws,
                                              datastore_name = 'mydatastore',
                                              container_name = 'myazureblobcontainername',
                                              account_name = 'mystorageaccoutname',
                                              account_key = 'mystorageaccountkey')

## End(Not run)
</code></pre>

<hr>
<h2 id='register_azure_data_lake_gen2_datastore'>Initialize a new Azure Data Lake Gen2 Datastore.</h2><span id='topic+register_azure_data_lake_gen2_datastore'></span>

<h3>Description</h3>

<p>Initialize a new Azure Data Lake Gen2 Datastore.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>register_azure_data_lake_gen2_datastore(
  workspace,
  datastore_name,
  filesystem,
  account_name,
  tenant_id,
  client_id,
  client_secret,
  resource_url = NULL,
  authority_url = NULL,
  protocol = NULL,
  endpoint = NULL,
  overwrite = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="register_azure_data_lake_gen2_datastore_+3A_workspace">workspace</code></td>
<td>
<p>The workspace this datastore belongs to.</p>
</td></tr>
<tr><td><code id="register_azure_data_lake_gen2_datastore_+3A_datastore_name">datastore_name</code></td>
<td>
<p>The datastore name.</p>
</td></tr>
<tr><td><code id="register_azure_data_lake_gen2_datastore_+3A_filesystem">filesystem</code></td>
<td>
<p>The name of the Data Lake Gen2 filesystem.</p>
</td></tr>
<tr><td><code id="register_azure_data_lake_gen2_datastore_+3A_account_name">account_name</code></td>
<td>
<p>The storage account name.</p>
</td></tr>
<tr><td><code id="register_azure_data_lake_gen2_datastore_+3A_tenant_id">tenant_id</code></td>
<td>
<p>The Directory ID/Tenant ID of the service principal.</p>
</td></tr>
<tr><td><code id="register_azure_data_lake_gen2_datastore_+3A_client_id">client_id</code></td>
<td>
<p>The Client ID/Application ID of the service principal.</p>
</td></tr>
<tr><td><code id="register_azure_data_lake_gen2_datastore_+3A_client_secret">client_secret</code></td>
<td>
<p>The secret of the service principal.</p>
</td></tr>
<tr><td><code id="register_azure_data_lake_gen2_datastore_+3A_resource_url">resource_url</code></td>
<td>
<p>The resource URL, which determines what operations will be
performed on the data lake store, defaults to https://storage.azure.com/ which
allows us to perform filesystem operations.</p>
</td></tr>
<tr><td><code id="register_azure_data_lake_gen2_datastore_+3A_authority_url">authority_url</code></td>
<td>
<p>The authority URL used to authenticate the user, defaults to
&quot;https://login.microsoftonline.com&quot;.</p>
</td></tr>
<tr><td><code id="register_azure_data_lake_gen2_datastore_+3A_protocol">protocol</code></td>
<td>
<p>Protocol to use to connect to the blob container. If None,
defaults to &quot;https&quot;.</p>
</td></tr>
<tr><td><code id="register_azure_data_lake_gen2_datastore_+3A_endpoint">endpoint</code></td>
<td>
<p>The endpoint of the blob container. If None, defaults to
&quot;core.windows.net&quot;.</p>
</td></tr>
<tr><td><code id="register_azure_data_lake_gen2_datastore_+3A_overwrite">overwrite</code></td>
<td>
<p>Whether to overwrite an existing datastore. If the datastore
does not exist, it will create one. The default is FALSE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>azureml.data.azure_data_lake_datastore.AzureDataLakeGen2Datastore</code>
object.
</p>


<h3>Examples</h3>

<div class="r"><pre># Create and register an Azure Data Lake Gen2 Datastore to a workspace.

my_adlsgen2_ds &lt;- register_azure_data_lake_gen2_datastore(workspace = your_workspace,
                                                          datastore_name = &lt;name for this datastore&gt;,
                                                          filesystem = 'test',
                                                          tenant_id = your_workspace$auth$tenant_id,
                                                          client_id = your_workspace$auth$service_principal_id,
                                                          client_secret = your_workspace$auth$service_principal_password)
</pre></div>


<h3>See Also</h3>

<p><code><a href="#topic+unregister_datastore">unregister_datastore()</a></code>, <code><a href="#topic+get_datastore">get_datastore()</a></code>
</p>

<hr>
<h2 id='register_azure_file_share_datastore'>Register an Azure file share as a datastore</h2><span id='topic+register_azure_file_share_datastore'></span>

<h3>Description</h3>

<p>Register an Azure file share as a datastore. You can choose to use
either the SAS token or the storage account key.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>register_azure_file_share_datastore(
  workspace,
  datastore_name,
  file_share_name,
  account_name,
  sas_token = NULL,
  account_key = NULL,
  protocol = NULL,
  endpoint = NULL,
  overwrite = FALSE,
  create_if_not_exists = FALSE,
  skip_validation = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="register_azure_file_share_datastore_+3A_workspace">workspace</code></td>
<td>
<p>The <code>Workspace</code> object.</p>
</td></tr>
<tr><td><code id="register_azure_file_share_datastore_+3A_datastore_name">datastore_name</code></td>
<td>
<p>A string of the name of the datastore. The name
must be case insensitive and can only contain alphanumeric characters and
underscores.</p>
</td></tr>
<tr><td><code id="register_azure_file_share_datastore_+3A_file_share_name">file_share_name</code></td>
<td>
<p>A string of the name of the Azure file share.</p>
</td></tr>
<tr><td><code id="register_azure_file_share_datastore_+3A_account_name">account_name</code></td>
<td>
<p>A string of the storage account name.</p>
</td></tr>
<tr><td><code id="register_azure_file_share_datastore_+3A_sas_token">sas_token</code></td>
<td>
<p>A string of the account SAS token.</p>
</td></tr>
<tr><td><code id="register_azure_file_share_datastore_+3A_account_key">account_key</code></td>
<td>
<p>A string of the storage account key.</p>
</td></tr>
<tr><td><code id="register_azure_file_share_datastore_+3A_protocol">protocol</code></td>
<td>
<p>A string of the protocol to use to connect to the
file store. If <code>NULL</code>, defaults to <code>'https'</code>.</p>
</td></tr>
<tr><td><code id="register_azure_file_share_datastore_+3A_endpoint">endpoint</code></td>
<td>
<p>A string of the endpoint of the file store.
If <code>NULL</code>, defaults to <code>'core.windows.net'</code>.</p>
</td></tr>
<tr><td><code id="register_azure_file_share_datastore_+3A_overwrite">overwrite</code></td>
<td>
<p>If <code>TRUE</code>, overwrites an existing datastore. If
the datastore does not exist, it will create one.</p>
</td></tr>
<tr><td><code id="register_azure_file_share_datastore_+3A_create_if_not_exists">create_if_not_exists</code></td>
<td>
<p>If <code>TRUE</code>, creates the file share
if it does not exists.</p>
</td></tr>
<tr><td><code id="register_azure_file_share_datastore_+3A_skip_validation">skip_validation</code></td>
<td>
<p>If <code>TRUE</code>, skips validation of storage keys.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>AzureFileDatastore</code> object.
</p>


<h3>Details</h3>

<p>In general we recommend Azure Blob storage over Azure File storage. Both
standard and premium storage are available for blobs. Although more
expensive, we suggest premium storage due to faster throughput speeds that
may improve the speed of your training runs, particularly if you train
against a large dataset.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
ws &lt;- load_workspace_from_config()
ds &lt;- register_azure_file_share_datastore(ws,
                                          datastore_name = 'mydatastore',
                                          file_share_name = 'myazurefilesharename',
                                          account_name = 'mystorageaccoutname',
                                          account_key = 'mystorageaccountkey')

## End(Not run)
</code></pre>

<hr>
<h2 id='register_azure_postgre_sql_datastore'>Initialize a new Azure PostgreSQL Datastore.</h2><span id='topic+register_azure_postgre_sql_datastore'></span>

<h3>Description</h3>

<p>Initialize a new Azure PostgreSQL Datastore.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>register_azure_postgre_sql_datastore(
  workspace,
  datastore_name,
  server_name,
  database_name,
  user_id,
  user_password,
  port_number = NULL,
  endpoint = NULL,
  overwrite = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="register_azure_postgre_sql_datastore_+3A_workspace">workspace</code></td>
<td>
<p>The workspace this datastore belongs to.</p>
</td></tr>
<tr><td><code id="register_azure_postgre_sql_datastore_+3A_datastore_name">datastore_name</code></td>
<td>
<p>The datastore name.</p>
</td></tr>
<tr><td><code id="register_azure_postgre_sql_datastore_+3A_server_name">server_name</code></td>
<td>
<p>The PostgreSQL server name.</p>
</td></tr>
<tr><td><code id="register_azure_postgre_sql_datastore_+3A_database_name">database_name</code></td>
<td>
<p>The PostgreSQL database name.</p>
</td></tr>
<tr><td><code id="register_azure_postgre_sql_datastore_+3A_user_id">user_id</code></td>
<td>
<p>The User ID of the PostgreSQL server.</p>
</td></tr>
<tr><td><code id="register_azure_postgre_sql_datastore_+3A_user_password">user_password</code></td>
<td>
<p>The User Password of the PostgreSQL server.</p>
</td></tr>
<tr><td><code id="register_azure_postgre_sql_datastore_+3A_port_number">port_number</code></td>
<td>
<p>The Port Number of the PostgreSQL server.</p>
</td></tr>
<tr><td><code id="register_azure_postgre_sql_datastore_+3A_endpoint">endpoint</code></td>
<td>
<p>The endpoint of the PostgreSQL server. If NULL, defaults to
postgres.database.azure.com.</p>
</td></tr>
<tr><td><code id="register_azure_postgre_sql_datastore_+3A_overwrite">overwrite</code></td>
<td>
<p>Whether to overwrite an existing datastore. If the datastore
does not exist, it will create one. The default is FALSE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>azureml.data.azure_postgre_sql_datastore.AzurePostgreSqlDatastore</code>
object.
</p>

<hr>
<h2 id='register_azure_sql_database_datastore'>Initialize a new Azure SQL database Datastore.</h2><span id='topic+register_azure_sql_database_datastore'></span>

<h3>Description</h3>

<p>Initialize a new Azure SQL database Datastore.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>register_azure_sql_database_datastore(
  workspace,
  datastore_name,
  server_name,
  database_name,
  tenant_id,
  client_id,
  client_secret,
  resource_url = NULL,
  authority_url = NULL,
  endpoint = NULL,
  overwrite = FALSE,
  username = NULL,
  password = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="register_azure_sql_database_datastore_+3A_workspace">workspace</code></td>
<td>
<p>The workspace this datastore belongs to.</p>
</td></tr>
<tr><td><code id="register_azure_sql_database_datastore_+3A_datastore_name">datastore_name</code></td>
<td>
<p>The datastore name.</p>
</td></tr>
<tr><td><code id="register_azure_sql_database_datastore_+3A_server_name">server_name</code></td>
<td>
<p>The SQL server name.</p>
</td></tr>
<tr><td><code id="register_azure_sql_database_datastore_+3A_database_name">database_name</code></td>
<td>
<p>The SQL database name.</p>
</td></tr>
<tr><td><code id="register_azure_sql_database_datastore_+3A_tenant_id">tenant_id</code></td>
<td>
<p>The Directory ID/Tenant ID of the service principal.</p>
</td></tr>
<tr><td><code id="register_azure_sql_database_datastore_+3A_client_id">client_id</code></td>
<td>
<p>The Client ID/Application ID of the service principal.</p>
</td></tr>
<tr><td><code id="register_azure_sql_database_datastore_+3A_client_secret">client_secret</code></td>
<td>
<p>The secret of the service principal.</p>
</td></tr>
<tr><td><code id="register_azure_sql_database_datastore_+3A_resource_url">resource_url</code></td>
<td>
<p>The resource URL, which determines what operations will
be performed on the SQL database store, if NULL, defaults to
https://database.windows.net/.</p>
</td></tr>
<tr><td><code id="register_azure_sql_database_datastore_+3A_authority_url">authority_url</code></td>
<td>
<p>The authority URL used to authenticate the user, defaults
to https://login.microsoftonline.com.</p>
</td></tr>
<tr><td><code id="register_azure_sql_database_datastore_+3A_endpoint">endpoint</code></td>
<td>
<p>The endpoint of the SQL server. If NULL, defaults to
database.windows.net.</p>
</td></tr>
<tr><td><code id="register_azure_sql_database_datastore_+3A_overwrite">overwrite</code></td>
<td>
<p>Whether to overwrite an existing datastore. If the datastore does
not exist, it will create one. The default is FALSE.</p>
</td></tr>
<tr><td><code id="register_azure_sql_database_datastore_+3A_username">username</code></td>
<td>
<p>The username of the database user to access the database.</p>
</td></tr>
<tr><td><code id="register_azure_sql_database_datastore_+3A_password">password</code></td>
<td>
<p>The password of the database user to access the database.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>azureml.data.azure_sql_database_datastore.AzureSqlDatabaseDatastore</code>
object.
</p>

<hr>
<h2 id='register_dataset'>Register a Dataset in the workspace</h2><span id='topic+register_dataset'></span>

<h3>Description</h3>

<p>Register the Dataset in the workspace, making it available to other users of the workspace.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>register_dataset(
  workspace,
  dataset,
  name,
  description = NULL,
  tags = NULL,
  create_new_version = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="register_dataset_+3A_workspace">workspace</code></td>
<td>
<p>The AzureML workspace in which the Dataset is to be registered.</p>
</td></tr>
<tr><td><code id="register_dataset_+3A_dataset">dataset</code></td>
<td>
<p>The dataset to be registered.</p>
</td></tr>
<tr><td><code id="register_dataset_+3A_name">name</code></td>
<td>
<p>The name of the Dataset in the workspace.</p>
</td></tr>
<tr><td><code id="register_dataset_+3A_description">description</code></td>
<td>
<p>A description of the Dataset.</p>
</td></tr>
<tr><td><code id="register_dataset_+3A_tags">tags</code></td>
<td>
<p>Named list of tags to give the Dataset. Defaults to NULL.</p>
</td></tr>
<tr><td><code id="register_dataset_+3A_create_new_version">create_new_version</code></td>
<td>
<p>Boolean to register the dataset as a new version under the specified name.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The registered Dataset object.
</p>

<hr>
<h2 id='register_do_azureml_parallel'>Registers AMLCompute as a parallel backend with the foreach package.</h2><span id='topic+register_do_azureml_parallel'></span>

<h3>Description</h3>

<p>Registers AMLCompute as a parallel backend with the foreach package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>register_do_azureml_parallel(workspace, compute_target)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="register_do_azureml_parallel_+3A_workspace">workspace</code></td>
<td>
<p>The Workspace object which has the compute_target.</p>
</td></tr>
<tr><td><code id="register_do_azureml_parallel_+3A_compute_target">compute_target</code></td>
<td>
<p>The AMLCompute target to use for parallelization.</p>
</td></tr>
</table>

<hr>
<h2 id='register_environment'>Register an environment in the workspace</h2><span id='topic+register_environment'></span>

<h3>Description</h3>

<p>The environment is automatically registered with your workspace when you
submit an experiment or deploy a web service. You can also manually register
the environment with <code>register_environment()</code>. This operation makes the
environment into an entity that is tracked and versioned in the cloud, and
can be shared between workspace users.
</p>
<p>Whe used for the first time in training or deployment, the environment is
registered with the workspace, built, and deployed on the compute target.
The environments are cached by the service. Reusing a cached environment
takes much less time than using a new service or one that has bee updated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>register_environment(environment, workspace)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="register_environment_+3A_environment">environment</code></td>
<td>
<p>The <code>Environment</code> object.</p>
</td></tr>
<tr><td><code id="register_environment_+3A_workspace">workspace</code></td>
<td>
<p>The <code>Workspace</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>Environment</code> object.
</p>

<hr>
<h2 id='register_model'>Register a model to a given workspace</h2><span id='topic+register_model'></span>

<h3>Description</h3>

<p>Register a model to the given workspace. A registered model is a logical
container for one or more files that make up your model. For example, if
you have a model that's stored in multiple files, you can register them
as a single model in your workspace. After registration, you can then
download or deploy the registered model and receive all the files that
were registered.
</p>
<p>Models are identified by name and version. Each time you register a
model with the same name as an existing one, your workspace's model
registry assumes that it's a new version. The version is incremented,
and the new model is registered under the same name.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>register_model(
  workspace,
  model_path,
  model_name,
  datasets = NULL,
  tags = NULL,
  properties = NULL,
  description = NULL,
  child_paths = NULL,
  sample_input_dataset = NULL,
  sample_output_dataset = NULL,
  resource_configuration = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="register_model_+3A_workspace">workspace</code></td>
<td>
<p>The <code>Workspace</code> object.</p>
</td></tr>
<tr><td><code id="register_model_+3A_model_path">model_path</code></td>
<td>
<p>A string of the path on the local file system where
the model assets are located. This can be a direct pointer to a single
file or folder. If pointing to a folder, the <code>child_paths</code> parameter can
be used to specify individual files to bundle together as the <code>Model</code>
object, as opposed to using the entire contents of the folder.</p>
</td></tr>
<tr><td><code id="register_model_+3A_model_name">model_name</code></td>
<td>
<p>A string of the name to register the model with.</p>
</td></tr>
<tr><td><code id="register_model_+3A_datasets">datasets</code></td>
<td>
<p>A list of two-element lists where the first element is the
dataset-model relationship and the second is the corresponding dataset, e.g.
<code>list(list("training", train_ds), list("inferencing", infer_ds))</code>. Valid
values for the data-model relationship are 'training', 'validation', and 'inferencing'.</p>
</td></tr>
<tr><td><code id="register_model_+3A_tags">tags</code></td>
<td>
<p>A named list of key-value tags to give the model, e.g.
<code>list("key" = "value")</code></p>
</td></tr>
<tr><td><code id="register_model_+3A_properties">properties</code></td>
<td>
<p>A named list of key-value properties to give the model,
e.g. <code>list("key" = "value")</code>.</p>
</td></tr>
<tr><td><code id="register_model_+3A_description">description</code></td>
<td>
<p>A string of the text description of the model.</p>
</td></tr>
<tr><td><code id="register_model_+3A_child_paths">child_paths</code></td>
<td>
<p>A list of strings of child paths of a folder specified
by <code>model_name</code>. Must be provided in conjunction with a <code>model_path</code>
pointing to a folder; only the specified files will be bundled into the
<code>Model</code> object.</p>
</td></tr>
<tr><td><code id="register_model_+3A_sample_input_dataset">sample_input_dataset</code></td>
<td>
<p>Sample input dataset for the registered model.</p>
</td></tr>
<tr><td><code id="register_model_+3A_sample_output_dataset">sample_output_dataset</code></td>
<td>
<p>Sample output dataset for the registered model.</p>
</td></tr>
<tr><td><code id="register_model_+3A_resource_configuration">resource_configuration</code></td>
<td>
<p>'ResourceConfiguration&ldquo; object to run the registered model.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>Model</code> object.
</p>


<h3>Examples</h3>

<p>Registering a model from a single file:</p>
<pre>ws &lt;- load_workspace_from_config()
model &lt;- register_model(ws,
                        model_path = "my_model.rds",
                        model_name = "my_model",
                        datasets = list(list("training", train_dataset)))
</pre>


<h3>See Also</h3>

<p><code><a href="#topic+resource_configuration">resource_configuration()</a></code>
</p>

<hr>
<h2 id='register_model_from_run'>Register a model for operationalization.</h2><span id='topic+register_model_from_run'></span>

<h3>Description</h3>

<p>Register a model for operationalization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>register_model_from_run(
  run,
  model_name,
  model_path = NULL,
  tags = NULL,
  properties = NULL,
  description = NULL,
  datasets = NULL,
  sample_input_dataset = NULL,
  sample_output_dataset = NULL,
  resource_configuration = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="register_model_from_run_+3A_run">run</code></td>
<td>
<p>The <code>Run</code> object.</p>
</td></tr>
<tr><td><code id="register_model_from_run_+3A_model_name">model_name</code></td>
<td>
<p>The name of the model.</p>
</td></tr>
<tr><td><code id="register_model_from_run_+3A_model_path">model_path</code></td>
<td>
<p>The relative cloud path to the model, for example,
&quot;outputs/modelname&quot;. When not specified, <code>model_name</code> is used as the path.</p>
</td></tr>
<tr><td><code id="register_model_from_run_+3A_tags">tags</code></td>
<td>
<p>A dictionary of key value tags to assign to the model.</p>
</td></tr>
<tr><td><code id="register_model_from_run_+3A_properties">properties</code></td>
<td>
<p>A dictionary of key value properties to assign to the model.
These properties cannot be changed after model creation, however new key-value pairs can be added.</p>
</td></tr>
<tr><td><code id="register_model_from_run_+3A_description">description</code></td>
<td>
<p>An optional description of the model.</p>
</td></tr>
<tr><td><code id="register_model_from_run_+3A_datasets">datasets</code></td>
<td>
<p>A list of two-element lists where the first element is the
dataset-model relationship and the second is the corresponding dataset, e.g.
<code>list(list("training", train_ds), list("inferencing", infer_ds))</code>. Valid
values for the data-model relationship are 'training', 'validation', and 'inferencing'.</p>
</td></tr>
<tr><td><code id="register_model_from_run_+3A_sample_input_dataset">sample_input_dataset</code></td>
<td>
<p>Sample input dataset for the registered model.</p>
</td></tr>
<tr><td><code id="register_model_from_run_+3A_sample_output_dataset">sample_output_dataset</code></td>
<td>
<p>Sample output dataset for the registered model.</p>
</td></tr>
<tr><td><code id="register_model_from_run_+3A_resource_configuration">resource_configuration</code></td>
<td>
<p>'ResourceConfiguration&ldquo; object to run the registered model.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The registered Model.
</p>


<h3>Examples</h3>

<pre>registered_model &lt;- register_model_from_run(run = run,
                                            model_name = "my model",
                                            model_path = 'outputs/model.rds',
                                            tags = list("version" = "0"),
                                            datasets = list(list("training", train_dataset),
                                                            list("validation", validation_dataset)),
                                            resource_configuration = resource_configuration(2, 2, 0))
</pre>


<h3>See Also</h3>

<p><code><a href="#topic+resource_configuration">resource_configuration()</a></code>
</p>

<hr>
<h2 id='reload_local_webservice_assets'>Reload a local web service's entry script and dependencies</h2><span id='topic+reload_local_webservice_assets'></span>

<h3>Description</h3>

<p>This restarts the service's container with copies of updated assets,
including the entry script and local dependencies, but it does not
rebuild the underlying image. Accordingly, changes to the environment
will not be reflected in the reloaded local web service. To handle those
changes call <code>update_local_webservice()</code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reload_local_webservice_assets(webservice, wait = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="reload_local_webservice_assets_+3A_webservice">webservice</code></td>
<td>
<p>The <code>LocalWebservice</code> object.</p>
</td></tr>
<tr><td><code id="reload_local_webservice_assets_+3A_wait">wait</code></td>
<td>
<p>If <code>TRUE</code>, wait for the service's container to reach a
healthy state. Defaults to <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>

<hr>
<h2 id='resource_configuration'>Initialize the  ResourceConfiguration.</h2><span id='topic+resource_configuration'></span>

<h3>Description</h3>

<p>Initialize the  ResourceConfiguration.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>resource_configuration(cpu = NULL, memory_in_gb = NULL, gpu = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="resource_configuration_+3A_cpu">cpu</code></td>
<td>
<p>The number of CPU cores to allocate for this resource. Can be a decimal.</p>
</td></tr>
<tr><td><code id="resource_configuration_+3A_memory_in_gb">memory_in_gb</code></td>
<td>
<p>The amount of memory (in GB) to allocate for this resource.
Can be a decimal If <code>TRUE</code>, decode the raw log bytes to a string.</p>
</td></tr>
<tr><td><code id="resource_configuration_+3A_gpu">gpu</code></td>
<td>
<p>The number of GPUs to allocate for this resource.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>ResourceConfiguration</code> object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+register_model_from_run">register_model_from_run</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
rc &lt;- resource_configuration(2, 2, 0)

registered_model &lt;- register_model_from_run(run, "my_model_name",
                                            "path_to_my_model",
                                            resource_configuration = rc)

## End(Not run)
</code></pre>

<hr>
<h2 id='save_model_package_files'>Save a Dockerfile and dependencies from a <code>ModelPackage</code> to
your local file system</h2><span id='topic+save_model_package_files'></span>

<h3>Description</h3>

<p>Download the Dockerfile, model, and other assets needed to build
an image locally from a created <code>ModelPackage</code>.
</p>
<p>This can only be used with a Dockerfile <code>ModelPackage</code> (where
<code>package_model()</code> was called with <code>generate_dockerfile = TRUE</code> to
indicated that you wanted only the files and not a fully built image).
</p>
<p><code>save_model_package_files()</code> downloads the files needed to build the
image to the <code>output_directory</code>. The Dockerfile included in the saved
files references a base image stored in an Azure container registry.
When you build the image on your local Docker installation, you will
need the address, username, and password to authenticate to the registry.
You can get this information using <code>get_model_package_container_registry()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>save_model_package_files(package, output_directory)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="save_model_package_files_+3A_package">package</code></td>
<td>
<p>The <code>ModelPackage</code> object.</p>
</td></tr>
<tr><td><code id="save_model_package_files_+3A_output_directory">output_directory</code></td>
<td>
<p>A string of the local directory that
will be created to contain the contents of the package.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>


<h3>See Also</h3>

<p><code>package_model()</code>, <code>get_model_package_container_registry()</code>
</p>

<hr>
<h2 id='service_principal_authentication'>Manages authentication using a service principle instead of a user identity.</h2><span id='topic+service_principal_authentication'></span>

<h3>Description</h3>

<p>Service Principal authentication is suitable for automated workflows like for CI/CD scenarios.
This type of authentication decouples the authentication process from any specific user login, and
allows for managed access control.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>service_principal_authentication(
  tenant_id,
  service_principal_id,
  service_principal_password,
  cloud = "AzureCloud"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="service_principal_authentication_+3A_tenant_id">tenant_id</code></td>
<td>
<p>The string id of the active directory tenant that the service
identity belongs to.</p>
</td></tr>
<tr><td><code id="service_principal_authentication_+3A_service_principal_id">service_principal_id</code></td>
<td>
<p>The service principal ID string.</p>
</td></tr>
<tr><td><code id="service_principal_authentication_+3A_service_principal_password">service_principal_password</code></td>
<td>
<p>The service principal password/key string.</p>
</td></tr>
<tr><td><code id="service_principal_authentication_+3A_cloud">cloud</code></td>
<td>
<p>The name of the target cloud. Can be one of &quot;AzureCloud&quot;, &quot;AzureChinaCloud&quot;, or
&quot;AzureUSGovernment&quot;. If no cloud is specified, &quot;AzureCloud&quot; is used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ServicePrincipalAuthentication</code> object
</p>


<h3>Examples</h3>

<p>Service principal authentication involves creating an App Registration in
Azure Active Directory. First, you generate a client secret, and then you grant
your service principal role access to your machine learning workspace. Then,
you use the <code>ServicePrincipalAuthentication</code> object to manage your authentication flow.</p>
<pre>svc_pr_password &lt;- Sys.getenv("AZUREML_PASSWORD")
svc_pr &lt;- service_principal_authentication(tenant_id="my-tenant-id",
                                           service_principal_id="my-application-id",
                                           service_principal_password=svc_pr_password)

ws &lt;- get_workspace("&lt;your workspace name&gt;",
                    "&lt;your subscription ID&gt;",
                    "&lt;your resource group&gt;",
                    auth = svc_pr)
</pre>


<h3>See Also</h3>

<p><code><a href="#topic+get_workspace">get_workspace()</a></code> <code><a href="#topic+interactive_login_authentication">interactive_login_authentication()</a></code>
</p>

<hr>
<h2 id='set_default_datastore'>Set the default datastore for a workspace</h2><span id='topic+set_default_datastore'></span>

<h3>Description</h3>

<p>Set the default datastore associated with the workspace.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_default_datastore(workspace, datastore_name)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="set_default_datastore_+3A_workspace">workspace</code></td>
<td>
<p>The <code>Workspace</code> object.</p>
</td></tr>
<tr><td><code id="set_default_datastore_+3A_datastore_name">datastore_name</code></td>
<td>
<p>The name of the datastore to be set as default.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>


<h3>See Also</h3>

<p><code><a href="#topic+get_default_datastore">get_default_datastore()</a></code>
</p>

<hr>
<h2 id='set_secrets'>Add secrets to a keyvault</h2><span id='topic+set_secrets'></span>

<h3>Description</h3>

<p>Add a named list of secrets into the keyvault associated with the
workspace.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_secrets(keyvault, secrets)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="set_secrets_+3A_keyvault">keyvault</code></td>
<td>
<p>The <code>Keyvault</code> object.</p>
</td></tr>
<tr><td><code id="set_secrets_+3A_secrets">secrets</code></td>
<td>
<p>The named list of secrets to be added to the keyvault,
where element name corresponds to the secret name.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
ws &lt;- load_workspace_from_config()
my_secret &lt;- Sys.getenv("MY_SECRET")
keyvault &lt;- get_default_keyvault(ws)
set_secrets(list("mysecret" = my_secret))

## End(Not run)
</code></pre>

<hr>
<h2 id='skip_from_dataset'>Skip file streams from the top of the dataset by the specified count.</h2><span id='topic+skip_from_dataset'></span>

<h3>Description</h3>

<p>Skip file streams from the top of the dataset by the specified count.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>skip_from_dataset(dataset, count)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="skip_from_dataset_+3A_dataset">dataset</code></td>
<td>
<p>The Dataset object.</p>
</td></tr>
<tr><td><code id="skip_from_dataset_+3A_count">count</code></td>
<td>
<p>The number of file streams to skip.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new Dataset object representing the dataset with file streams skipped.
</p>

<hr>
<h2 id='split_tasks'>Splits the job into parallel tasks.</h2><span id='topic+split_tasks'></span>

<h3>Description</h3>

<p>Splits the job into parallel tasks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>split_tasks(args_list, node_count, process_count_per_node)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="split_tasks_+3A_args_list">args_list</code></td>
<td>
<p>The list of arguments which are distributed across all the
processes.</p>
</td></tr>
<tr><td><code id="split_tasks_+3A_node_count">node_count</code></td>
<td>
<p>Number of nodes in the AmlCompute cluster.</p>
</td></tr>
<tr><td><code id="split_tasks_+3A_process_count_per_node">process_count_per_node</code></td>
<td>
<p>Number of processes per node.</p>
</td></tr>
</table>

<hr>
<h2 id='start_logging_run'>Create an interactive logging run</h2><span id='topic+start_logging_run'></span>

<h3>Description</h3>

<p>Create an interactive run that allows the user to log
metrics and artifacts to a run locally.
</p>
<p>Any metrics that are logged during the interactive run session
are added to the run record in the experiment. If an output
directory is specified, the contents of that directory is
uploaded as run artifacts upon run completion.
</p>
<p>This method is useful if you would like to add experiment
tracking and artifact logging to the corresponding run record
in Azure ML for local runs without have to submit an experiment
run to a compute target with <code>submit_experiment()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>start_logging_run(experiment, outputs = NULL, snapshot_directory = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="start_logging_run_+3A_experiment">experiment</code></td>
<td>
<p>The <code>Experiment</code> object.</p>
</td></tr>
<tr><td><code id="start_logging_run_+3A_outputs">outputs</code></td>
<td>
<p>(Optional) A string of the local path to an
outputs directory to track.</p>
</td></tr>
<tr><td><code id="start_logging_run_+3A_snapshot_directory">snapshot_directory</code></td>
<td>
<p>(Optional) Directory to take snapshot of.
Setting to <code>NULL</code> will take no snapshot.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>Run</code> object of the started run.
</p>


<h3>See Also</h3>

<p><code>complete_run()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
ws &lt;- load_workspace_from_config()
exp &lt;- experiment(ws, name = 'myexperiment')
run &lt;- start_logging_run(exp)
log_metric_to_run("Accuracy", 0.9)
complete_run(run)

## End(Not run)
</code></pre>

<hr>
<h2 id='submit_child_run'>Submit an experiment and return the active child run</h2><span id='topic+submit_child_run'></span>

<h3>Description</h3>

<p>Submit an experiment and return the active child run.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>submit_child_run(parent_run, config = NULL, tags = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="submit_child_run_+3A_parent_run">parent_run</code></td>
<td>
<p>The parent <code>Run</code> object.</p>
</td></tr>
<tr><td><code id="submit_child_run_+3A_config">config</code></td>
<td>
<p>The <code>RunConfig</code> object</p>
</td></tr>
<tr><td><code id="submit_child_run_+3A_tags">tags</code></td>
<td>
<p>Tags to be added to the submitted run, e.g., &quot;tag&quot;: &quot;value&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>Run</code> object.
</p>

<hr>
<h2 id='submit_experiment'>Submit an experiment and return the active created run</h2><span id='topic+submit_experiment'></span>

<h3>Description</h3>

<p><code>submit_experiment()</code> is an asynchronous call to Azure Machine Learning
service to execute a trial on local or remote compute. Depending on the
configuration, <code>submit_experiment()</code> will automatically prepare your
execution environments, execute your code, and capture your source code
and results in the experiment's run history.
</p>
<p>To submit an experiment you first need to create a configuration object
describing how the experiment is to be run. The configuration depends on
the type of trial required. For a script run, provide an <code>Estimator</code> object
to the <code>config</code> parameter. For a HyperDrive run for hyperparameter tuning,
provide a <code>HyperDriveConfig</code> to <code>config</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>submit_experiment(experiment, config, tags = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="submit_experiment_+3A_experiment">experiment</code></td>
<td>
<p>The <code>Experiment</code> object.</p>
</td></tr>
<tr><td><code id="submit_experiment_+3A_config">config</code></td>
<td>
<p>The <code>Estimator</code> or <code>HyperDriveConfig</code> object.</p>
</td></tr>
<tr><td><code id="submit_experiment_+3A_tags">tags</code></td>
<td>
<p>A named list of tags for the submitted run, e.g.
<code>list("tag" = "value")</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>ScriptRun</code> or <code>HyperDriveRun</code> object.
</p>


<h3>See Also</h3>

<p><code>estimator()</code>, <code>hyperdrive_config()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># This example submits an Estimator experiment
## Not run: 
ws &lt;- load_workspace_from_config()
compute_target &lt;- get_compute(ws, cluster_name = 'mycluster')
exp &lt;- experiment(ws, name = 'myexperiment')
est &lt;- estimator(source_directory = '.',
                 entry_script = 'train.R',
                 compute_target = compute_target)
run &lt;- submit_experiment(exp, est)

## End(Not run)
</code></pre>

<hr>
<h2 id='take_from_dataset'>Take a sample of file streams from top of the dataset by the specified count.</h2><span id='topic+take_from_dataset'></span>

<h3>Description</h3>

<p>Take a sample of file streams from top of the dataset by the specified count.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>take_from_dataset(dataset, count)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="take_from_dataset_+3A_dataset">dataset</code></td>
<td>
<p>The Dataset object.</p>
</td></tr>
<tr><td><code id="take_from_dataset_+3A_count">count</code></td>
<td>
<p>The number of file streams to take.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new Dataset object representing the sampled dataset.
</p>

<hr>
<h2 id='take_sample_from_dataset'>Take a random sample of file streams in the dataset approximately by the probability specified.</h2><span id='topic+take_sample_from_dataset'></span>

<h3>Description</h3>

<p>Take a random sample of file streams in the dataset approximately by the probability specified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>take_sample_from_dataset(dataset, probability, seed = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="take_sample_from_dataset_+3A_dataset">dataset</code></td>
<td>
<p>The Dataset object.</p>
</td></tr>
<tr><td><code id="take_sample_from_dataset_+3A_probability">probability</code></td>
<td>
<p>The probability of a file stream being included in the sample.</p>
</td></tr>
<tr><td><code id="take_sample_from_dataset_+3A_seed">seed</code></td>
<td>
<p>An optional seed to use for the random generator.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new Dataset object representing the sampled dataset.
</p>

<hr>
<h2 id='truncation_selection_policy'>Define a truncation selection policy for early termination of HyperDrive runs</h2><span id='topic+truncation_selection_policy'></span>

<h3>Description</h3>

<p>Truncation selection cancels a given percentage of lowest performing runs at
each evaluation interval. Runs are compared based on their performance on the
primary metric and the lowest X% are terminated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>truncation_selection_policy(
  truncation_percentage,
  evaluation_interval = 1L,
  delay_evaluation = 0L
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="truncation_selection_policy_+3A_truncation_percentage">truncation_percentage</code></td>
<td>
<p>An integer of the percentage of lowest
performing runs to terminate at each interval.</p>
</td></tr>
<tr><td><code id="truncation_selection_policy_+3A_evaluation_interval">evaluation_interval</code></td>
<td>
<p>An integer of the frequency for applying policy.</p>
</td></tr>
<tr><td><code id="truncation_selection_policy_+3A_delay_evaluation">delay_evaluation</code></td>
<td>
<p>An integer of the number of intervals for which to
delay the first evaluation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>TruncationSelectionPolicy</code> object.
</p>


<h3>Details</h3>

<p>This policy periodically cancels the given percentage of runs that rank the
lowest for their performance on the primary metric. The policy strives for
fairness in ranking the runs by accounting for improving model performance
with training time. When ranking a relatively young run, the policy uses the
corresponding (and earlier) performance of older runs for comparison.
Therefore, runs aren't terminated for having a lower performance because they
have run for less time than other runs.
</p>
<p>The truncation selection policy takes the following configuration parameters:
</p>

<ul>
<li> <p><code>truncation_percentage</code>: An integer of the percentage of lowest performing
runs to terminate at each evaluation interval.
</p>
</li>
<li> <p><code>evaluation_interval</code>: Optional. The frequency for applying the policy.
Each time the training script logs the primary metric counts as one
interval.
</p>
</li>
<li> <p><code>delay_evaluation</code>: Optional. The number of intervals to delay the
policy evaluation. Use this parameter to avoid premature termination
of training runs. If specified, the policy applies every multiple of
<code>evaluation_interval</code> that is greater than or equal to <code>delay_evaluation</code>.
</p>
</li></ul>

<p>For example, when evaluating a run at a interval N, its performance is only
compared with the performance of other runs up to interval N even if they
reported metrics for intervals greater than N.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># In this example, the early termination policy is applied at every interval
# starting at evaluation interval 5. A run will be terminated at interval 5
# if its performance at interval 5 is in the lowest 20% of performance of all
# runs at interval 5
## Not run: 
early_termination_policy = truncation_selection_policy(
                                                 truncation_percentage = 20L,
                                                 evaluation_interval = 1L,
                                                 delay_evaluation = 5L)

## End(Not run)
</code></pre>

<hr>
<h2 id='uniform'>Specify a uniform distribution of options to sample from</h2><span id='topic+uniform'></span>

<h3>Description</h3>

<p>Specify a uniform distribution of options to sample the
hyperparameters from.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>uniform(min_value, max_value)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="uniform_+3A_min_value">min_value</code></td>
<td>
<p>A double of the minimum value in the range
(inclusive).</p>
</td></tr>
<tr><td><code id="uniform_+3A_max_value">max_value</code></td>
<td>
<p>A double of the maximum value in the range
(inclusive).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of the stochastic expression.
</p>


<h3>See Also</h3>

<p><code>random_parameter_sampling()</code>, <code>grid_parameter_sampling()</code>,
<code>bayesian_parameter_sampling()</code>
</p>

<hr>
<h2 id='unregister_all_dataset_versions'>Unregister all versions under the registration name of this dataset from the workspace.</h2><span id='topic+unregister_all_dataset_versions'></span>

<h3>Description</h3>

<p>Unregister all versions under the registration name of this dataset from the workspace.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unregister_all_dataset_versions(dataset)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="unregister_all_dataset_versions_+3A_dataset">dataset</code></td>
<td>
<p>The dataset to be unregistered.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>

<hr>
<h2 id='unregister_datastore'>Unregister a datastore from its associated workspace</h2><span id='topic+unregister_datastore'></span>

<h3>Description</h3>

<p>Unregister the datastore from its associated workspace. The
underlying Azure storage will not be deleted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unregister_datastore(datastore)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="unregister_datastore_+3A_datastore">datastore</code></td>
<td>
<p>The <code>AzureBlobDatastore</code> or <code>AzureFileDatastore</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>

<hr>
<h2 id='update_aci_webservice'>Update a deployed ACI web service</h2><span id='topic+update_aci_webservice'></span>

<h3>Description</h3>

<p>Update an ACI web service with the provided properties. You can update the
web service to use a new model, a new entry script, or new dependencies
that can be specified in an inference configuration.
</p>
<p>Values left as <code>NULL</code> will remain unchanged in the web service.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>update_aci_webservice(
  webservice,
  tags = NULL,
  properties = NULL,
  description = NULL,
  auth_enabled = NULL,
  ssl_enabled = NULL,
  ssl_cert_pem_file = NULL,
  ssl_key_pem_file = NULL,
  ssl_cname = NULL,
  enable_app_insights = NULL,
  models = NULL,
  inference_config = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="update_aci_webservice_+3A_webservice">webservice</code></td>
<td>
<p>The <code>AciWebservice</code> object.</p>
</td></tr>
<tr><td><code id="update_aci_webservice_+3A_tags">tags</code></td>
<td>
<p>A named list of key-value tags for the web service,
e.g. <code>list("key" = "value")</code>. Will replace existing tags.</p>
</td></tr>
<tr><td><code id="update_aci_webservice_+3A_properties">properties</code></td>
<td>
<p>A named list of key-value properties to add for the web
service, e.g. <code>list("key" = "value")</code>.</p>
</td></tr>
<tr><td><code id="update_aci_webservice_+3A_description">description</code></td>
<td>
<p>A string of the description to give the web service.</p>
</td></tr>
<tr><td><code id="update_aci_webservice_+3A_auth_enabled">auth_enabled</code></td>
<td>
<p>If <code>TRUE</code> enable key-based authentication for the
web service.</p>
</td></tr>
<tr><td><code id="update_aci_webservice_+3A_ssl_enabled">ssl_enabled</code></td>
<td>
<p>Whether or not to enable SSL for this Webservice.</p>
</td></tr>
<tr><td><code id="update_aci_webservice_+3A_ssl_cert_pem_file">ssl_cert_pem_file</code></td>
<td>
<p>A string of the cert file needed if SSL is enabled.</p>
</td></tr>
<tr><td><code id="update_aci_webservice_+3A_ssl_key_pem_file">ssl_key_pem_file</code></td>
<td>
<p>A string of the key file needed if SSL is enabled.</p>
</td></tr>
<tr><td><code id="update_aci_webservice_+3A_ssl_cname">ssl_cname</code></td>
<td>
<p>A string of the cname if SSL is enabled.</p>
</td></tr>
<tr><td><code id="update_aci_webservice_+3A_enable_app_insights">enable_app_insights</code></td>
<td>
<p>If <code>TRUE</code> enable AppInsights for the web service.</p>
</td></tr>
<tr><td><code id="update_aci_webservice_+3A_models">models</code></td>
<td>
<p>A list of <code>Model</code> objects to package into the updated service.</p>
</td></tr>
<tr><td><code id="update_aci_webservice_+3A_inference_config">inference_config</code></td>
<td>
<p>An <code>InferenceConfig</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>

<hr>
<h2 id='update_aks_webservice'>Update a deployed AKS web service</h2><span id='topic+update_aks_webservice'></span>

<h3>Description</h3>

<p>Update an AKS web service with the provided properties. You can update the
web service to use a new model, a new entry script, or new dependencies
that can be specified in an inference configuration.
</p>
<p>Values left as <code>NULL</code> will remain unchanged in the web service.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>update_aks_webservice(
  webservice,
  autoscale_enabled = NULL,
  autoscale_min_replicas = NULL,
  autoscale_max_replicas = NULL,
  autoscale_refresh_seconds = NULL,
  autoscale_target_utilization = NULL,
  auth_enabled = NULL,
  cpu_cores = NULL,
  memory_gb = NULL,
  enable_app_insights = NULL,
  scoring_timeout_ms = NULL,
  replica_max_concurrent_requests = NULL,
  max_request_wait_time = NULL,
  num_replicas = NULL,
  tags = NULL,
  properties = NULL,
  description = NULL,
  models = NULL,
  inference_config = NULL,
  gpu_cores = NULL,
  period_seconds = NULL,
  initial_delay_seconds = NULL,
  timeout_seconds = NULL,
  success_threshold = NULL,
  failure_threshold = NULL,
  namespace = NULL,
  token_auth_enabled = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="update_aks_webservice_+3A_webservice">webservice</code></td>
<td>
<p>The <code>AksWebservice</code> object.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_autoscale_enabled">autoscale_enabled</code></td>
<td>
<p>If <code>TRUE</code> enable autoscaling for the web service.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_autoscale_min_replicas">autoscale_min_replicas</code></td>
<td>
<p>An int of the minimum number of containers
to use when autoscaling the web service.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_autoscale_max_replicas">autoscale_max_replicas</code></td>
<td>
<p>An int of the maximum number of containers
to use when autoscaling the web service.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_autoscale_refresh_seconds">autoscale_refresh_seconds</code></td>
<td>
<p>An int of how often in seconds the autoscaler
should attempt to scale the web service.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_autoscale_target_utilization">autoscale_target_utilization</code></td>
<td>
<p>An int of the target utilization
(in percent out of 100) the autoscaler should attempt to maintain for the
web service.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_auth_enabled">auth_enabled</code></td>
<td>
<p>If <code>TRUE</code> enable key-based authentication for the
web service. Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_cpu_cores">cpu_cores</code></td>
<td>
<p>The number of cpu cores to allocate for
the web service. Can be a decimal. Defaults to <code>0.1</code>.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_memory_gb">memory_gb</code></td>
<td>
<p>The amount of memory (in GB) to allocate for
the web service. Can be a decimal. Defaults to <code>0.5</code>.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_enable_app_insights">enable_app_insights</code></td>
<td>
<p>If <code>TRUE</code> enable AppInsights for the web service.
Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_scoring_timeout_ms">scoring_timeout_ms</code></td>
<td>
<p>An int of the timeout (in milliseconds) to enforce for
scoring calls to the web service.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_replica_max_concurrent_requests">replica_max_concurrent_requests</code></td>
<td>
<p>An int of the number of maximum concurrent
requests per node to allow for the web service.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_max_request_wait_time">max_request_wait_time</code></td>
<td>
<p>An int of the maximum amount of time a request
will stay in the queue (in milliseconds) before returning a 503 error.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_num_replicas">num_replicas</code></td>
<td>
<p>An int of the number of containers to allocate for the
web service. If this parameter is not set then the autoscaler is enabled by
default.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_tags">tags</code></td>
<td>
<p>A named list of key-value tags for the web service,
e.g. <code>list("key" = "value")</code>. Will replace existing tags.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_properties">properties</code></td>
<td>
<p>A named list of key-value properties to add for the web
service, e.g. <code>list("key" = "value")</code>.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_description">description</code></td>
<td>
<p>A string of the description to give the web service.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_models">models</code></td>
<td>
<p>A list of <code>Model</code> objects to package into the updated service.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_inference_config">inference_config</code></td>
<td>
<p>An <code>InferenceConfig</code> object.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_gpu_cores">gpu_cores</code></td>
<td>
<p>An int of the number of gpu cores to allocate for the
web service.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_period_seconds">period_seconds</code></td>
<td>
<p>An int of how often in seconds to perform the
liveness probe. Minimum value is <code>1</code>.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_initial_delay_seconds">initial_delay_seconds</code></td>
<td>
<p>An int of the number of seconds after
the container has started before liveness probes are initiated.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_timeout_seconds">timeout_seconds</code></td>
<td>
<p>An int of the number of seconds after which the
liveness probe times out. Minimum value is <code>1</code>.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_success_threshold">success_threshold</code></td>
<td>
<p>An int of the minimum consecutive successes
for the liveness probe to be considered successful after having failed.
Minimum value is <code>1</code>.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_failure_threshold">failure_threshold</code></td>
<td>
<p>An int of the number of times Kubernetes will try
the liveness probe when a Pod starts and the probe fails, before giving up.
Minimum value is <code>1</code>.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_namespace">namespace</code></td>
<td>
<p>A string of the Kubernetes namespace in which to deploy the
web service: up to 63 lowercase alphanumeric ('a'-'z', '0'-'9') and
hyphen ('-') characters. The first last characters cannot be hyphens.</p>
</td></tr>
<tr><td><code id="update_aks_webservice_+3A_token_auth_enabled">token_auth_enabled</code></td>
<td>
<p>If <code>TRUE</code>, enable token-based authentication for
the web service. If enabled, users can access the web service by fetching
an access token using their Azure Active Directory credentials.
Both <code>token_auth_enabled</code> and <code>auth_enabled</code> cannot be set to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>

<hr>
<h2 id='update_aml_compute'>Update scale settings for an AmlCompute cluster</h2><span id='topic+update_aml_compute'></span>

<h3>Description</h3>

<p>Update the scale settings for an existing AmlCompute cluster.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>update_aml_compute(
  cluster,
  min_nodes = NULL,
  max_nodes = NULL,
  idle_seconds_before_scaledown = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="update_aml_compute_+3A_cluster">cluster</code></td>
<td>
<p>The <code>AmlCompute</code> cluster.</p>
</td></tr>
<tr><td><code id="update_aml_compute_+3A_min_nodes">min_nodes</code></td>
<td>
<p>An integer of the minimum number of nodes to use on
the cluster.</p>
</td></tr>
<tr><td><code id="update_aml_compute_+3A_max_nodes">max_nodes</code></td>
<td>
<p>An integer of the maximum number of nodes to use on
the cluster.</p>
</td></tr>
<tr><td><code id="update_aml_compute_+3A_idle_seconds_before_scaledown">idle_seconds_before_scaledown</code></td>
<td>
<p>An integer of the node idle time
in seconds before scaling down the cluster.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>

<hr>
<h2 id='update_local_webservice'>Update a local web service</h2><span id='topic+update_local_webservice'></span>

<h3>Description</h3>

<p>Update a local web service with the provided properties. You can update the
web service to use a new model, a new entry script, or new dependencies
that can be specified in an inference configuration.
</p>
<p>Values left as <code>NULL</code> will remain unchanged in the service.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>update_local_webservice(
  webservice,
  models = NULL,
  deployment_config = NULL,
  wait = FALSE,
  inference_config = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="update_local_webservice_+3A_webservice">webservice</code></td>
<td>
<p>The <code>LocalWebservice</code> object.</p>
</td></tr>
<tr><td><code id="update_local_webservice_+3A_models">models</code></td>
<td>
<p>A list of <code>Model</code> objects to package into the updated service.</p>
</td></tr>
<tr><td><code id="update_local_webservice_+3A_deployment_config">deployment_config</code></td>
<td>
<p>A <code>LocalWebserviceDeploymentConfiguration</code> to
apply to the web service.</p>
</td></tr>
<tr><td><code id="update_local_webservice_+3A_wait">wait</code></td>
<td>
<p>If <code>TRUE</code>, wait for the service's container to reach a
healthy state. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="update_local_webservice_+3A_inference_config">inference_config</code></td>
<td>
<p>An <code>InferenceConfig</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>

<hr>
<h2 id='upload_files_to_datastore'>Upload files to the Azure storage a datastore points to</h2><span id='topic+upload_files_to_datastore'></span>

<h3>Description</h3>

<p>Upload the data from the local file system to the Azure storage that the
datastore points to.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>upload_files_to_datastore(
  datastore,
  files,
  relative_root = NULL,
  target_path = NULL,
  overwrite = FALSE,
  show_progress = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="upload_files_to_datastore_+3A_datastore">datastore</code></td>
<td>
<p>The <code>AzureBlobDatastore</code> or <code>AzureFileDatastore</code> object.</p>
</td></tr>
<tr><td><code id="upload_files_to_datastore_+3A_files">files</code></td>
<td>
<p>A character vector of the absolute path to files to upload.</p>
</td></tr>
<tr><td><code id="upload_files_to_datastore_+3A_relative_root">relative_root</code></td>
<td>
<p>A string of the base path from which is used to
determine the path of the files in the Azure storage. For example, if
we upload <code style="white-space: pre;">&#8288;/path/to/file.txt&#8288;</code>, and we define the base path to be <code style="white-space: pre;">&#8288;/path&#8288;</code>,
when <code>file.txt</code> is uploaded to the blob storage or file share, it will
have the path of <code style="white-space: pre;">&#8288;/to/file.txt&#8288;</code>. If <code>target_path</code> is also given, then it
will be used as the prefix for the derived path from above. The base path
must be a common path of all of the files, otherwise an exception will be
thrown.</p>
</td></tr>
<tr><td><code id="upload_files_to_datastore_+3A_target_path">target_path</code></td>
<td>
<p>A string of the location in the blob container or file
share to upload the data to. Defaults to <code>NULL</code>, in which case the data is
uploaded to the root.</p>
</td></tr>
<tr><td><code id="upload_files_to_datastore_+3A_overwrite">overwrite</code></td>
<td>
<p>If <code>TRUE</code>, overwrites any existing data at <code>target_path</code>.</p>
</td></tr>
<tr><td><code id="upload_files_to_datastore_+3A_show_progress">show_progress</code></td>
<td>
<p>If <code>TRUE</code>, show progress of upload in the console.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>DataReference</code> object for the target path uploaded.
</p>

<hr>
<h2 id='upload_files_to_run'>Upload files to a run</h2><span id='topic+upload_files_to_run'></span>

<h3>Description</h3>

<p>Upload files to the run record.
</p>
<p>Note: Runs automatically capture files in the specified output
directory, which defaults to &quot;./outputs&quot;. Use <code>upload_files_to_run()</code>
only when additional files need to be uploaded or an output directory
is not specified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>upload_files_to_run(names, paths, timeout_seconds = NULL, run = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="upload_files_to_run_+3A_names">names</code></td>
<td>
<p>A character vector of the names of the files to upload.</p>
</td></tr>
<tr><td><code id="upload_files_to_run_+3A_paths">paths</code></td>
<td>
<p>A character vector of relative local paths to the files
to be upload.</p>
</td></tr>
<tr><td><code id="upload_files_to_run_+3A_timeout_seconds">timeout_seconds</code></td>
<td>
<p>An int of the timeout in seconds for uploading
the files.</p>
</td></tr>
<tr><td><code id="upload_files_to_run_+3A_run">run</code></td>
<td>
<p>The <code>Run</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>


<h3>Examples</h3>

<pre>ws &lt;- load_workspace_from_config()
exp &lt;- experiment(ws, name = 'myexperiment')

# Start an interactive logging run
run &lt;- start_logging_run(exp)

# Upload files to the run record
filename1 &lt;- "important_file_1"
filename2 &lt;- "important_file_2"
upload_files_to_run(names = c(filename1, filename2),
                    paths = c("path/on/disk/file_1.txt", "other/path/on/disk/file_2.txt"))

# Download a file from the run record
download_file_from_run(filename1, "file_1.txt")
</pre>


<h3>See Also</h3>

<p><code><a href="#topic+upload_folder_to_run">upload_folder_to_run()</a></code> <code><a href="#topic+download_file_from_run">download_file_from_run()</a></code> <code><a href="#topic+download_files_from_run">download_files_from_run()</a></code>
</p>

<hr>
<h2 id='upload_folder_to_run'>Upload a folder to a run</h2><span id='topic+upload_folder_to_run'></span>

<h3>Description</h3>

<p>Upload the specified folder to the given prefix name to the run
record.
</p>
<p>Note: Runs automatically capture files in the specified output
directory, which defaults to &quot;./outputs&quot;. Use <code>upload_folder_to_run()</code>
only when additional files need to be uploaded or an output directory
is not specified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>upload_folder_to_run(name, path, run = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="upload_folder_to_run_+3A_name">name</code></td>
<td>
<p>A string of the name of the folder of files to upload.</p>
</td></tr>
<tr><td><code id="upload_folder_to_run_+3A_path">path</code></td>
<td>
<p>A string of the relative local path to the folder to upload.</p>
</td></tr>
<tr><td><code id="upload_folder_to_run_+3A_run">run</code></td>
<td>
<p>The <code>Run</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>


<h3>Examples</h3>

<pre>ws &lt;- load_workspace_from_config()
exp &lt;- experiment(ws, name = 'myexperiment')

# Start an interactive logging run
run &lt;- start_logging_run(exp)

# Upload folder to the run record
upload_folder_to_run(name = "important_files",
                     path = "path/on/disk")

# Download a file from the run record
download_file_from_run("important_files/existing_file.txt", "local_file.txt")
</pre>


<h3>See Also</h3>

<p><code><a href="#topic+upload_files_to_run">upload_files_to_run()</a></code> <code><a href="#topic+download_file_from_run">download_file_from_run()</a></code> <code><a href="#topic+download_files_from_run">download_files_from_run()</a></code>
</p>

<hr>
<h2 id='upload_to_datastore'>Upload a local directory to the Azure storage a datastore points to</h2><span id='topic+upload_to_datastore'></span>

<h3>Description</h3>

<p>Upload a local directory to the Azure storage the datastore points to.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>upload_to_datastore(
  datastore,
  src_dir,
  target_path = NULL,
  overwrite = FALSE,
  show_progress = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="upload_to_datastore_+3A_datastore">datastore</code></td>
<td>
<p>The <code>AzureBlobDatastore</code> or <code>AzureFileDatastore</code> object.</p>
</td></tr>
<tr><td><code id="upload_to_datastore_+3A_src_dir">src_dir</code></td>
<td>
<p>A string of the local directory to upload.</p>
</td></tr>
<tr><td><code id="upload_to_datastore_+3A_target_path">target_path</code></td>
<td>
<p>A string of the location in the blob container or
file share to upload the data to. Defaults to <code>NULL</code>, in which case the data
is uploaded to the root.</p>
</td></tr>
<tr><td><code id="upload_to_datastore_+3A_overwrite">overwrite</code></td>
<td>
<p>If <code>TRUE</code>, overwrites any existing data at <code>target_path</code>.</p>
</td></tr>
<tr><td><code id="upload_to_datastore_+3A_show_progress">show_progress</code></td>
<td>
<p>If <code>TRUE</code>, show progress of upload in the console.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>DataReference</code> object for the target path uploaded.
</p>

<hr>
<h2 id='view_run_details'>Initialize run details widget</h2><span id='topic+view_run_details'></span>

<h3>Description</h3>

<p>Initializes a ShinyApp in RStudio Viewer (or the default browser if Viewer
is unavailable) showing details of the submitted run. If using RStudio, the
plot will auto-update with information collected from the server. For more
details about the run, click the web view link. The widget will stop running
once the run has reached a terminal state: &quot;Failed&quot;, &quot;Completed&quot;, or
&quot;Canceled&quot;.
</p>
<p>If you are running this method from an RMarkdown file, the
run details table will show up in the code chunk output
instead of the Viewer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>view_run_details(run, auto_refresh = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="view_run_details_+3A_run">run</code></td>
<td>
<p>Run object</p>
</td></tr>
<tr><td><code id="view_run_details_+3A_auto_refresh">auto_refresh</code></td>
<td>
<p>Boolean indicating whether or not widget should update
run details automatically. The default is TRUE when using RStudio.</p>
</td></tr>
</table>

<hr>
<h2 id='wait_for_deployment'>Wait for a web service to finish deploying</h2><span id='topic+wait_for_deployment'></span>

<h3>Description</h3>

<p>Automatically poll on the running web service deployment and
wait for the web service to reach a terminal state. Will throw
an exception if it reaches a non-successful terminal state.
</p>
<p>Typically called after running <code>deploy_model()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wait_for_deployment(webservice, show_output = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="wait_for_deployment_+3A_webservice">webservice</code></td>
<td>
<p>The <code>LocalWebservice</code>, <code>AciWebservice</code>, or
<code>AksWebservice</code> object.</p>
</td></tr>
<tr><td><code id="wait_for_deployment_+3A_show_output">show_output</code></td>
<td>
<p>If <code>TRUE</code>, print more verbose output. Defaults
to <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>


<h3>See Also</h3>

<p><code>deploy_model()</code>
</p>

<hr>
<h2 id='wait_for_model_package_creation'>Wait for a model package to finish creating</h2><span id='topic+wait_for_model_package_creation'></span>

<h3>Description</h3>

<p>Wait for a model package creation to reach a terminal state.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wait_for_model_package_creation(package, show_output = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="wait_for_model_package_creation_+3A_package">package</code></td>
<td>
<p>The <code>ModelPackage</code> object.</p>
</td></tr>
<tr><td><code id="wait_for_model_package_creation_+3A_show_output">show_output</code></td>
<td>
<p>If <code>TRUE</code>, print more verbose output. Defaults to
<code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>

<hr>
<h2 id='wait_for_provisioning_completion'>Wait for a cluster to finish provisioning</h2><span id='topic+wait_for_provisioning_completion'></span>

<h3>Description</h3>

<p>Wait for a cluster to finish provisioning. Typically invoked after a
<code>create_aml_compute()</code> or <code>create_aks_compute()</code> call.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wait_for_provisioning_completion(cluster, show_output = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="wait_for_provisioning_completion_+3A_cluster">cluster</code></td>
<td>
<p>The <code>AmlCompute</code> or <code>AksCompute</code> object.</p>
</td></tr>
<tr><td><code id="wait_for_provisioning_completion_+3A_show_output">show_output</code></td>
<td>
<p>If <code>TRUE</code>, more verbose output will be provided.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>


<h3>See Also</h3>

<p><code>create_aml_compute()</code>, <code>create_aks_compute()</code>
</p>

<hr>
<h2 id='wait_for_run_completion'>Wait for the completion of a run</h2><span id='topic+wait_for_run_completion'></span>

<h3>Description</h3>

<p>Wait for the run to reach a terminal state. Typically called
after submitting an experiment run with <code>submit_experiment()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wait_for_run_completion(run, show_output = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="wait_for_run_completion_+3A_run">run</code></td>
<td>
<p>The <code>Run</code> object.</p>
</td></tr>
<tr><td><code id="wait_for_run_completion_+3A_show_output">show_output</code></td>
<td>
<p>If <code>TRUE</code>, print verbose output to console.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>


<h3>See Also</h3>

<p><code><a href="#topic+submit_experiment">submit_experiment()</a></code>
</p>

<hr>
<h2 id='write_workspace_config'>Write out the workspace configuration details to a config file</h2><span id='topic+write_workspace_config'></span>

<h3>Description</h3>

<p>Write out the workspace ARM properties to a config file. Workspace ARM
properties can be loaded later using <code>load_workspace_from_config()</code>.
The method provides a simple way of reusing the same workspace across
multiple files or projects. Users can save the workspace ARM properties
using this function, and use <code>load_workspace_from_config()</code> to load the
same workspace in different files or projects without retyping the
workspace ARM properties.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write_workspace_config(workspace, path = NULL, file_name = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="write_workspace_config_+3A_workspace">workspace</code></td>
<td>
<p>The <code>Workspace</code> object whose config has to be written down.</p>
</td></tr>
<tr><td><code id="write_workspace_config_+3A_path">path</code></td>
<td>
<p>A string of the location to write the config.json file. The config
file will be located in a directory called '.azureml'. The parameter defaults to
the current working directory, so by default config.json will be located at '.azureml/'.</p>
</td></tr>
<tr><td><code id="write_workspace_config_+3A_file_name">file_name</code></td>
<td>
<p>A string of the name to use for the config file. The
parameter defaults to <code>'config.json'</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None
</p>


<h3>See Also</h3>

<p><code><a href="#topic+load_workspace_from_config">load_workspace_from_config()</a></code>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
