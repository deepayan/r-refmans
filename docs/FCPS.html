<!DOCTYPE html><html><head><title>Help for package FCPS</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {FCPS}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#FCPS-package'>
<p>Fundamental Clustering Problems Suite</p></a></li>
<li><a href='#ADPclustering'>
<p>(Adaptive) Density Peak Clustering algorithm using automatic parameter selection</p></a></li>
<li><a href='#AgglomerativeNestingClustering'>
<p>AGNES clustering</p></a></li>
<li><a href='#APclustering'>
<p>Affinity Propagation Clustering</p></a></li>
<li><a href='#Atom'>
<p>Atom introduced in [Ultsch, 2004].</p></a></li>
<li><a href='#AutomaticProjectionBasedClustering'>
<p>Automatic Projection-Based Clustering</p></a></li>
<li><a href='#Chainlink'>
<p>Chainlink introduced in [Ultsch et al., 1994; Ultsch, 1995].</p></a></li>
<li><a href='#ClusterabilityMDplot'>
<p>Clusterability MDplot</p></a></li>
<li><a href='#ClusterApply'><p>Applies a function over grouped data</p></a></li>
<li><a href='#ClusterARI'>
<p>Adjusted Rand index</p></a></li>
<li><a href='#ClusterChallenge'>
<p>Generates a Fundamental Clustering Challenge based on specific artificial datasets.</p></a></li>
<li><a href='#ClusterCount'><p>ClusterCount</p></a></li>
<li><a href='#ClusterCreateClassification'>
<p>Create Classification for Cluster.. functions</p></a></li>
<li><a href='#ClusterDaviesBouldinIndex'>
<p>Davies Bouldin Index</p></a></li>
<li><a href='#ClusterDendrogram'>
<p>Cluster Dendrogram</p></a></li>
<li><a href='#ClusterDistances'>
<p>ClusterDistances</p></a></li>
<li><a href='#ClusterDunnIndex'>
<p>Dunn Index</p></a></li>
<li><a href='#ClusterEqualWeighting'>
<p>ClusterEqualWeighting</p></a></li>
<li><a href='#ClusteringAccuracy'>
<p>ClusterAccuracy</p></a></li>
<li><a href='#ClusterInterDistances'>
<p>Computes Inter-Cluster Distances</p></a></li>
<li><a href='#ClusterMCC'>
<p>Matthews Correlation Coefficient (MCC)</p></a></li>
<li><a href='#ClusterNoEstimation'>
<p>Estimates Number of Clusters using up to 26 Indicators</p></a></li>
<li><a href='#ClusterNormalize'>
<p>Cluster Normalize</p></a></li>
<li><a href='#ClusterPlotMDS'>
<p>Plot Clustering using Dimensionality Reduction by MDS</p></a></li>
<li><a href='#ClusterRedefine'>
<p>Redfines Clustering</p></a></li>
<li><a href='#ClusterRename'>
<p>Renames Clustering</p></a></li>
<li><a href='#ClusterRenameDescendingSize'><p>Cluster Rename Descending Size</p></a></li>
<li><a href='#ClusterShannonInfo'>
<p>Shannon Information</p></a></li>
<li><a href='#ClusterUpsamplingMinority'>
<p>Cluster Up Sampling using SMOTE for minority cluster</p></a></li>
<li><a href='#CrossEntropyClustering'><p>Cross-Entropy Clustering</p></a></li>
<li><a href='#DBSCAN'>
<p>DBSCAN</p></a></li>
<li><a href='#DBSclusteringAndVisualization'>
<p>Databionic Swarm (DBS) Clustering and Visualization</p></a></li>
<li><a href='#DensityPeakClustering'>
<p>Density Peak Clustering algorithm using the Decision Graph</p></a></li>
<li><a href='#DivisiveAnalysisClustering'><p>Large DivisiveAnalysisClustering Clustering</p></a></li>
<li><a href='#EngyTime'>
<p>EngyTime introduced in [Baggenstoss, 2002].</p></a></li>
<li><a href='#EntropyOfDataField'>
<p>Entropy Of a Data Field [Wang et al., 2011].</p></a></li>
<li><a href='#EstimateRadiusByDistance'>
<p>Estimate Radius By Distance</p></a></li>
<li><a href='#FannyClustering'>
<p>Fuzzy Analysis Clustering [Rousseeuw/Kaufman, 1990, p. 253-279]</p></a></li>
<li><a href='#GapStatistic'>
<p>Gap Statistic</p></a></li>
<li><a href='#GenieClustering'>
<p>Genie Clustering by Gini Index</p></a></li>
<li><a href='#GolfBall'>
<p>GolfBall introduced in [Ultsch, 2005]</p></a></li>
<li><a href='#HCLclustering'><p>On-line Update (Hard Competitive learning) method</p></a></li>
<li><a href='#HDDClustering'>
<p>HDD clustering is a model-based clustering method of [Bouveyron et al., 2007].</p></a></li>
<li><a href='#Hepta'>
<p>Hepta introduced in [Ultsch, 2003]</p></a></li>
<li><a href='#HierarchicalClusterData'>
<p>Internal function of Hierarchical Clusterering of Data</p></a></li>
<li><a href='#HierarchicalClusterDists'>
<p>Internal Function of Hierarchical Clustering with Distances</p></a></li>
<li><a href='#HierarchicalClustering'>
<p>Hierarchical Clustering</p></a></li>
<li><a href='#HierarchicalDBSCAN'>
<p>Hierarchical DBSCAN</p></a></li>
<li><a href='#kmeansClustering'>
<p>K-Means Clustering</p></a></li>
<li><a href='#kmeansDist'>
<p>k-means Clustering using a distance matrix</p></a></li>
<li><a href='#LargeApplicationClustering'><p>Large Application Clustering</p></a></li>
<li><a href='#Leukemia'>
<p>Leukemia distance matrix and classificiation used in [Thrun, 2018]</p></a></li>
<li><a href='#Lsun3D'>
<p>Lsun3D inspired by FCPS introduced in [Thrun, 2018]</p></a></li>
<li><a href='#MarkovClustering'>
<p>Markov Clustering</p></a></li>
<li><a href='#MeanShiftClustering'><p>Mean Shift Clustering</p></a></li>
<li><a href='#MinimalEnergyClustering'>
<p>Minimal Energy Clustering</p></a></li>
<li><a href='#MinimaxLinkageClustering'>
<p>Minimax Linkage Hierarchical Clustering</p></a></li>
<li><a href='#ModelBasedClustering'>
<p>Model Based Clustering</p></a></li>
<li><a href='#ModelBasedVarSelClustering'>
<p>Model Based Clustering with Variable Selection</p></a></li>
<li><a href='#MoGclustering'>
<p>Mixture of Gaussians Clustering using EM</p></a></li>
<li><a href='#MSTclustering'>
<p>MST-kNN clustering algorithm [Inostroza-Ponta, 2008].</p></a></li>
<li><a href='#NetworkClustering'>
<p>Network Clustering</p></a></li>
<li><a href='#NeuralGasClustering'><p>Neural gas algorithm for clustering</p></a></li>
<li><a href='#OPTICSclustering'>
<p>OPTICS Clustering</p></a></li>
<li><a href='#PAMclustering'>
<p>Partitioning Around Medoids (PAM)</p></a></li>
<li><a href='#pdfClustering'>
<p>Probability Density Distribution Clustering</p></a></li>
<li><a href='#PenalizedRegressionBasedClustering'>
<p>Penalized Regression-Based Clustering of [Wu et al., 2016].</p></a></li>
<li><a href='#ProjectionPursuitClustering'>
<p>Cluster Identification using Projection Pursuit as described in [Hofmeyr/Pavlidis, 2019].</p></a></li>
<li><a href='#QTclustering'>
<p>Stochastic QT Clustering</p></a></li>
<li><a href='#RobustTrimmedClustering'><p>Robust Trimmed Clustering</p></a></li>
<li><a href='#SharedNearestNeighborClustering'>
<p>SNN clustering</p></a></li>
<li><a href='#SOMclustering'>
<p>self-organizing maps based clustering implemented by [Wherens, Buydens, 2017].</p></a></li>
<li><a href='#SOTAclustering'><p>SOTA Clustering</p></a></li>
<li><a href='#SparseClustering'>
<p>Sparse Clustering</p></a></li>
<li><a href='#SpectralClustering'><p> Spectral Clustering</p></a></li>
<li><a href='#Spectrum'>
<p>Fast Adaptive Spectral Clustering [John et al, 2020]</p></a></li>
<li><a href='#StatPDEdensity'>
<p>Pareto Density Estimation</p></a></li>
<li><a href='#SubspaceClustering'><p>Algorithms for Subspace clustering</p></a></li>
<li><a href='#TandemClustering'>
<p>Tandem Clustering</p></a></li>
<li><a href='#Target'>
<p>Target introduced in [Ultsch, 2005].</p></a></li>
<li><a href='#Tetra'>
<p>Tetra introduced in [Ultsch, 1993]</p></a></li>
<li><a href='#TwoDiamonds'>
<p>TwoDiamonds introduced in  [Ultsch, 2003a, 2003b]</p></a></li>
<li><a href='#WingNut'>
<p>WingNut introduced in [Ultsch, 2005]</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Fundamental Clustering Problems Suite</td>
</tr>
<tr>
<td>Version:</td>
<td>1.3.4</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-10-18</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Michael Thrun &lt;m.thrun@gmx.net&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Over sixty clustering algorithms are provided in this package with consistent input and output, which enables the user to try out algorithms swiftly. Additionally, 26 statistical approaches for the estimation of the number of clusters as well as the mirrored density plot (MD-plot) of clusterability are implemented. The packages is published in Thrun, M.C., Stier Q.: "Fundamental Clustering Algorithms Suite" (2021), SoftwareX, &lt;<a href="https://doi.org/10.1016%2Fj.softx.2020.100642">doi:10.1016/j.softx.2020.100642</a>&gt;. Moreover, the fundamental clustering problems suite (FCPS) offers a variety of clustering challenges any algorithm should handle when facing real world data, see Thrun, M.C., Ultsch A.: "Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems" (2020), Data in Brief, &lt;<a href="https://doi.org/10.1016%2Fj.dib.2020.105501">doi:10.1016/j.dib.2020.105501</a>&gt;.</td>
</tr>
<tr>
<td>Imports:</td>
<td>mclust, ggplot2, DataVisualizations, methods</td>
</tr>
<tr>
<td>Suggests:</td>
<td>mlpack, kernlab, cclust, dbscan, kohonen, MCL, ADPclust,
cluster, DatabionicSwarm, orclus, subspace, flexclust,
ABCanalysis, apcluster, pracma,EMCluster, pdfCluster,
parallelDist, plotly, ProjectionBasedClustering,
GeneralizedUmatrix, mstknnclust, densityClust, parallel,
energy, R.utils, tclust, Spectrum, genie, protoclust,
fastcluster, clusterability, signal, reshape2, PPCI, clustrd,
smacof, rgl,prclust, CEC, dendextend, moments,prabclus,
VarSelLCM, sparcl, mixtools, HDclassif, clustvarsel, yardstick,
knitr, rmarkdown, igraph, leiden, clustMixType, clusterSim,
NetworkToolbox,ClusterR,partitionComparison</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://www.deepbionics.org/">https://www.deepbionics.org/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/Mthrun/FCPS/issues">https://github.com/Mthrun/FCPS/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>Pandoc (&gt;= 1.12.3)</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-18 13:42:08 UTC; MCT</td>
</tr>
<tr>
<td>Author:</td>
<td>Michael Thrun <a href="https://orcid.org/0000-0001-9542-5543"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre, cph],
  Peter Nahrgang [ctr, ctb],
  Felix Pape [ctr, ctb],
  Vasyl Pihur [ctb],
  Guy Brock [ctb],
  Susmita Datta [ctb],
  Somnath Datta [ctb],
  Luis Winckelmann [com],
  Alfred Ultsch [dtc, ctb],
  Quirin Stier [ctb, rev]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-19 13:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='FCPS-package'>
Fundamental Clustering Problems Suite
</h2><span id='topic+FCPS-package'></span><span id='topic+ClusteringAlgorithms'></span>

<h3>Description</h3>

<p>Over sixty clustering algorithms are provided in this package with consistent input and output, which enables the user to try out algorithms swiftly. Additionally, 26 statistical approaches for the estimation of the number of clusters as well as the mirrored density plot (MD-plot) of clusterability are implemented. The packages is published in Thrun, M.C., Stier Q.: &quot;Fundamental Clustering Algorithms Suite&quot; (2021), SoftwareX, &lt;DOI:10.1016/j.softx.2020.100642&gt;. Moreover, the fundamental clustering problems suite (FCPS) offers a variety of clustering challenges any algorithm should handle when facing real world data, see Thrun, M.C., Ultsch A.: &quot;Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems&quot; (2020), Data in Brief, &lt;DOI:10.1016/j.dib.2020.105501&gt;.
</p>
<p>The package consists of many algorithms and fundamental datasets for clustering published in [Thrun/Stier, 2021]. Originally, the 'Fundamental Clustering Problems Suite' (FCPS) offered a variety of clustering problems
any algorithm shall be able to handle when facing real world data.  Nine of the here presented artificial datasets were priorly named FCPS with a fixed sample size in Ultsch, A.: &quot;Clustering with SOM: U*C&quot;, In Workshop on Self-Organizing Maps, 2005. FCPS often served in the paper as an elementary benchmark for clustering algorithms. The FCPS package extends datasets, enables variable sample sizes for these datasets, and provides a standardized and easy access to many clustering algorithms.
</p>
<p><a href="https://www.deepbionics.org/">https://www.deepbionics.org/</a>
</p>


<h3>Details</h3>

<p>FCPS datasets consists of data sets with known a priori classification to be reproduced by the algorithms.
All data sets are intentionally created to be simple and might be visualized in two or three dimensions.
Each data sets represents a certain problem that is solved by known clustering algorithms with varying success. 
This is done in order to reveal benefits and shortcomings of algorithms in question. Standard clustering methods,
e.g. single-linkage, ward and k-means, are not able to solve all FCPS problems satisfactorily. &quot;Lsun3D and each of the nine artificial data sets of &quot;Fundamental Clustering Problems Suite&quot; (FCPS) 
were defined separately for a specific clustering problem as cited (in [Thrun/Ultsch, 2020]). The original 
sample size defined in the respective first publication mentioning the data was used in [Thrun/Ultsch, 2020],
but using the R function &quot;ClusterChallenge&quot; (...) any sample size can be drawn for all artificial data sets.
[Thrun/Ultsch, 2020]
</p>

<p>Index:  This package was not yet installed at build time.<br />
</p>


<h3>Author(s)</h3>

<p>NA
</p>
<p>Maintainer: Michael Thrun &lt;m.thrun@gmx.net&gt;
</p>


<h3>References</h3>

<p>[Thrun/Ultsch, 2020]  Thrun, M. C., &amp; Ultsch, A.: Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems, Data in Brief, Vol. 30(C), pp. 105501, <a href="https://doi.org/10.1016/j.dib.2020.105501">doi:10.1016/j.dib.2020.105501</a>, 2020.
</p>
<p>[Thrun/Stier, 2021]  Thrun, M. C., &amp; Stier, Q.: Fundamental Clustering Algorithms Suite SoftwareX, Vol. 13(C), in press, pp. 100642. <a href="https://doi.org/10.1016/j.softx.2020.100642">doi:10.1016/j.softx.2020.100642</a>, 2021.
</p>
<p>[Ultsch, 2005] Ultsch, A.: Clustering with SOM: U*C, In Proc. Workshop on Self-Organizing Maps, pp. 75-82, Paris, France, 2005.
</p>

<hr>
<h2 id='ADPclustering'>
(Adaptive) Density Peak Clustering algorithm using automatic parameter selection
</h2><span id='topic+ADPclustering'></span>

<h3>Description</h3>

<p>The algorithm was introduced in [Rodriguez/Laio, 2014] and here implemented by [Wang/Xu, 2017]. The algorithm is adaptive in the sense that only <code>ClusterNo</code> has to be set instead of the paramters of [Rodriguez/Laio, 2014] implemented in <code><a href="#topic+ADPclustering">ADPclustering</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ADPclustering(Data,ClusterNo=NULL,PlotIt=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ADPclustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.</p>
</td></tr>
<tr><td><code id="ADPclustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>Optional, either: A number k which defines k different Clusters to be build by the algorithm, or a range of <code>ClusterNo</code> to let the algorithm choose from.</p>
</td></tr>
<tr><td><code id="ADPclustering_+3A_plotit">PlotIt</code></td>
<td>
<p>default: FALSE, If TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="ADPclustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The ADP algorithm decides the k number of clusters. This is contrary to the other version of the algorithm from another package which can be called with <code><a href="#topic+DensityPeakClustering">DensityPeakClustering</a></code>.
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Rodriguez/Laio, 2014]  Rodriguez, A., &amp; Laio, A.: Clustering by fast search and find of density peaks, Science, Vol. 344(6191), pp. 1492-1496. 2014.
</p>
<p>[Wang/Xu, 2017]  Wang, X.-F., &amp; Xu, Y.: Fast clustering using adaptive density peak detection, Statistical methods in medical research, Vol. 26(6), pp. 2800-2811. 2017.</p>


<h3>See Also</h3>

<p><code><a href="#topic+DensityPeakClustering">DensityPeakClustering</a></code>
</p>
<p><code><a href="ADPclust.html#topic+adpclust">adpclust</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
out=ADPclustering(Hepta$Data,PlotIt=FALSE)
</code></pre>

<hr>
<h2 id='AgglomerativeNestingClustering'>
AGNES clustering
</h2><span id='topic+AgglomerativeNestingClustering'></span>

<h3>Description</h3>

<p>Agglomerative hierarchical clustering (AGNES)of [Rousseeuw/Kaufman, 1990, pp. 199-252]
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AgglomerativeNestingClustering(DataOrDistances, ClusterNo,

PlotIt = FALSE, Standardization = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AgglomerativeNestingClustering_+3A_dataordistances">DataOrDistances</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases or d-dimensional data points. Every case has d attributes, variables or features. Alternatively, symmetric [1:n,1:n] distance matrix</p>
</td></tr>
<tr><td><code id="AgglomerativeNestingClustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be built by the algorithm.
if <code>ClusterNo=0</code>, the dendrogram is generated instead of a clustering to estimate the numbers of clusters.
</p>
</td></tr>
<tr><td><code id="AgglomerativeNestingClustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE if <code>ClusterNo!=0</code>, If TRUE or <code>ClusterNo=0</code> plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="AgglomerativeNestingClustering_+3A_standardization">Standardization</code></td>
<td>

<p><code>DataOrDistances</code> is standardized before calculating the dissimilarities. Measurements are standardized for each variable (column), by subtracting the variable's mean value and dividing by the variable's mean absolute deviation. If <code>DataOrDistances</code> is already a distance matrix, then this argument will be ignored.
</p>
</td></tr>
<tr><td><code id="AgglomerativeNestingClustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n] numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>Dendrogram</code></td>
<td>
<p>Dendrogram of hierarchical clustering algorithm</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Rousseeuw/Kaufman, 1990]	Rousseeuw, P. J., &amp; Kaufman, L.: Finding groups in data, Belgium, John Wiley &amp; Sons Inc., ISBN: 0471735787, doi 10.1002/9780470316801, Online ISBN: 9780470316801, 1990.
</p>
<p>[Struyf et al., 1996] Struyf,A., Hubert, M. and Rousseeuw, Peter J.: Clustering in an Object-Oriented Environment, Journal of Statistical Software, Vol. 1, doi: 10.18637/jss.v001.i04, 1996.
</p>
<p>[Struyf et al., 1997] Struyf, A., Hubert, M. and Rousseeuw, P.J.: Integrating Robust Clustering Techniques in S-PLUS, Computational Statistics and Data Analysis, Vol. 26, pp. 17&ndash;37, 1997.
</p>


<h3>See Also</h3>

<p><code><a href="cluster.html#topic+agnes">agnes</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
CA=AgglomerativeNestingClustering(Hepta$Data,ClusterNo=7,PlotIt=FALSE)
## Not run: 
ClusterDendrogram(CA$Dendrogram,7,main='AGNES clustering')

print(CA$Object)
plot(CA$Object)

## End(Not run)
</code></pre>

<hr>
<h2 id='APclustering'>
Affinity Propagation Clustering
</h2><span id='topic+APclustering'></span>

<h3>Description</h3>

<p>Affinity propagation clustering published by [Frey/Dueck, 2007] and implemented by [Bodenhofer et al., 2011].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>APclustering(DataOrDistances,

InputPreference=NA,ExemplarPreferences=NA,

DistanceMethod="euclidean",

Seed=7568,PlotIt=FALSE,Data,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="APclustering_+3A_dataordistances">DataOrDistances</code></td>
<td>

<p>[1:n,1:d] with: if d=n and symmetric then distance matrix assumed, otherwise:
[1:n,1:d] matrix of dataset to be clustered. It consists of n cases or d-dimensional data points. Every case has d attributes, variables or features.
In the latter case the Euclidean distances will be calculated.
</p>
</td></tr>
<tr><td><code id="APclustering_+3A_inputpreference">InputPreference</code></td>
<td>

<p>Default parameter set, see <span class="pkg">apcluster</span>
</p>
</td></tr>
<tr><td><code id="APclustering_+3A_exemplarpreferences">ExemplarPreferences</code></td>
<td>

<p>Default parameter set, see <span class="pkg">apcluster</span>
</p>
</td></tr>
<tr><td><code id="APclustering_+3A_distancemethod">DistanceMethod</code></td>
<td>

<p>DistanceMethod as in <code><a href="stats.html#topic+dist">dist</a></code> for  <code><a href="apcluster.html#topic+similarities">similarities</a></code>.
</p>
</td></tr>
<tr><td><code id="APclustering_+3A_seed">Seed</code></td>
<td>

<p>Set as integervalue to have reproducible results, see <span class="pkg">apcluster</span>
</p>
</td></tr>
<tr><td><code id="APclustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, If TRUE and dataset of [1:n,1:d] dimensions then a plot of the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code> will be generated.</p>
</td></tr>
<tr><td><code id="APclustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] data matrix in the case that <code>DataOrDistances</code> is missing and partial matching does not work.</p>
</td></tr>
<tr><td><code id="APclustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Distancematrix D is converted to similarity matrix S with S=-(D^2).
</p>
<p>If data matrix is used, then euclidean similarities are calculated by <code><a href="apcluster.html#topic+similarities">similarities</a></code> and a specifed distance method.
</p>
<p>The AP algorithm decides the k number of clusters.
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Frey/Dueck, 2007]	Frey, B. J., &amp; Dueck, D.: Clustering by passing messages between data points, Science, Vol. 315(5814), pp. 972-976, &lt;doi:10.1126/science.1136800&gt;, 2007.
</p>
<p>[Bodenhofer et al., 2011]	Bodenhofer, U., Kothmeier, A., &amp; Hochreiter, S.: APCluster: an R package for affinity propagation clustering, Bioinformatics, Vol. 27(17), pp, 2463-2464, 2011.
</p>
<p>Further details in
<a href="http://www.bioinf.jku.at/software/apcluster/">http://www.bioinf.jku.at/software/apcluster/</a>
</p>


<h3>See Also</h3>

<p><span class="pkg">apcluster</span>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
res=APclustering(Hepta$Data, PlotIt = FALSE)
</code></pre>

<hr>
<h2 id='Atom'>
Atom introduced in [Ultsch, 2004].
</h2><span id='topic+Atom'></span>

<h3>Description</h3>

<p>Two nested spheres with different variances that are not linear not separable.
Detailed description of dataset and its clustering challenge is provided in [Thrun/Ultsch, 2020].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("Atom")</code></pre>


<h3>Details</h3>

<p>Size 800, Dimensions 3, stored in <code>Atom$Data</code>
</p>
<p>Classes 2, stored in <code>Atom$Cls</code>
</p>


<h3>References</h3>

<p>[Ultsch, 2004]  Ultsch, A.: Strategies for an artificial life system to cluster high dimensional data, Abstracting and Synthesizing the Principles of Living Systems, GWAL-6, U. Brggemann, H. Schaub, and F. Detje, Eds, pp. 128-137. 2004.
</p>
<p>[Thrun/Ultsch, 2020]  Thrun, M. C., &amp; Ultsch, A.: Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems, Data in Brief, Vol. 30(C), pp. 105501, <a href="https://doi.org/10.1016/j.dib.2020.105501">doi:10.1016/j.dib.2020.105501</a>, 2020.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Atom)
str(Atom)
</code></pre>

<hr>
<h2 id='AutomaticProjectionBasedClustering'>
Automatic Projection-Based Clustering
</h2><span id='topic+AutomaticProjectionBasedClustering'></span>

<h3>Description</h3>

<p>Projection-based clustering <code><a href="#topic+AutomaticProjectionBasedClustering">AutomaticProjectionBasedClustering</a></code> projects the data (nonlinear) into two dimensions and tries only to preserve relevant neighborhoods prior to clustering. The cluster analysis itself includes the high-dimensional distances in the clustering process. Performs non-interactive projection-based clustering based on non-linear projection methods [Thrun/Ultsch, 2017], [Thrun/Ultsch, 2020a].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AutomaticProjectionBasedClustering(DataOrDistances,ClusterNo,Type="NerV",

StructureType = TRUE,PlotIt=FALSE,PlotTree=FALSE,PlotMap=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AutomaticProjectionBasedClustering_+3A_dataordistances">DataOrDistances</code></td>
<td>

<p>Either nonsymmetric [1:n,1:d] numerical matrix of a dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.
</p>
<p>or
</p>
<p>symmetric [1:n,1:n] distance matrix, e.g. <code>as.matrix(dist(Data,method))</code>
</p>
</td></tr>
<tr><td><code id="AutomaticProjectionBasedClustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be built by the algorithm.</p>
</td></tr>
<tr><td><code id="AutomaticProjectionBasedClustering_+3A_type">Type</code></td>
<td>

<p>Type of Projection method, either
</p>
<p><code>NerV</code> [Venna et al., 2010] 
</p>
<p><code>Pswarm</code> [Thrun/Ultsch, 2020b] 
</p>
<p><code>MDS</code> [Torgerson, 1952]  
</p>
<p><code>Uwot</code> [McInnes et al., 2018]
</p>
<p><code>CCA</code> [Demartines/Herault, 1995] 
</p>
<p><code>Sammon</code> [Sammon, 1969] 
</p>
<p><code>t-SNE</code> [Van der Maaten/Hinton, 2008]
</p>
</td></tr>
<tr><td><code id="AutomaticProjectionBasedClustering_+3A_structuretype">StructureType</code></td>
<td>

<p>Either compact (TRUE) or connected (FALSE), see discussion in [Thrun, 2018] 
</p>
</td></tr>
<tr><td><code id="AutomaticProjectionBasedClustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, if TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="AutomaticProjectionBasedClustering_+3A_plottree">PlotTree</code></td>
<td>

<p>TRUE: Plots the dendrogram, FALSE: no plot
</p>
</td></tr>
<tr><td><code id="AutomaticProjectionBasedClustering_+3A_plotmap">PlotMap</code></td>
<td>

<p>Plots the topographic map [Thrun et al., 2016].
</p>
</td></tr>
<tr><td><code id="AutomaticProjectionBasedClustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The first idea of using non-PCA projections for clustering was published by [Bock, 1987] as a definition. However, to the knowledge of the author, it was not applied to any data. The coexistence of projection and clustering was introduced in [Thrun/Ultsch, 2017].
</p>
<p>Projection-based clustering is based on a nonlinear projection of high-dimensional data into a two-dimensional space [Thrun/Ultsch, 2020b].  Typical projection-methods like t-distributed stochastic neighbor embedding (t-SNE) [Van der Maaten/Hinton, 2008], or neighbor retrieval visualizer (NerV) [Venna et al., 2010] are used project data explicitly into two dimensions disregarding the subspaces of higher dimension than two and preserving only relevant neighborhoods in high-dimensional data. In the next step, the Delaunay graph [Delaunay, 1934] between the projected points is calculated, and each vertex between two projected points is weighted with the high-dimensional distance between the corresponding high-dimensional data points. Thereafter the shortest path between every pair of points is computed using the Dijkstra algorithm [Dijkstra, 1959]. The shortest paths are then used in the clustering process, which involves two choices depending on the structure type in the high-dimensional data [Thrun/Ultsch, 2020b]. This Boolean choice can be decided by looking at the topographic map of high-dimensional structures [Thrun/Ultsch, 2020a]. In a benchmarking of 34 comparable clustering methods, projection-based clustering was the only algorithm that always was able to find the high-dimensional distance or density-based structure of the dataset [Thrun/Ultsch, 2020b].
</p>
<p>It should be noted that it is preferable to use a visualization for the Generalized U-Matrix like the topographic map <code><a href="GeneralizedUmatrix.html#topic+plotTopographicMap">plotTopographicMap</a></code> of [Thrun et al., 2016] to evaluate the choice of the boolean parameter <code>StructureType</code> and the clustering, improve it or set the number of clusters appropriately. A comparison with 32 clustering algorithms showed that PBC is always able to find the correct cluster structure while the best of the 32 clustering algorithms varies depending on the dataset [Thrun/Ultsch, 2020].
</p>
<p>The first systematic comparison to other DR clustering methods like Projection-Pursuit Methods <code><a href="#topic+ProjectionPursuitClustering">ProjectionPursuitClustering</a></code>, supspace clustering methods <code><a href="#topic+SubspaceClustering">SubspaceClustering</a></code>,  and CA-based clustering methods can be found in [Thrun/Ultsch, 2020a]. For PCA-based clustering methods please see <code><a href="#topic+TandemClustering">TandemClustering</a></code>.
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.
. Points which cannot be assigned to a cluster will be reported with 0.
</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Bock, 1987]  Bock, H.: On the interface between cluster analysis, principal component analysis, and multidimensional scaling, Multivariate statistical modeling and data analysis, (pp. 17-34), Springer, 1987.
</p>
<p>[Thrun/Ultsch, 2017]  Thrun, M. C., &amp; Ultsch, A.: Projection based Clustering, Proc. International Federation of Classification Societies (IFCS), pp. 250-251, Tokai University, Japanese Classification Society (JCS), Tokyo, Japan August 7-10, 2017.
</p>
<p>[Thrun/Ultsch, 2020a]  Thrun, M. C., &amp; Ultsch, A.: Using Projection based Clustering to Find Distance and Density based Clusters in High-Dimensional Data, Journal of Classification, in press, doi 10.1007/s00357-020-09373-2, 2020.
</p>
<p>[Thrun et al., 2016]  Thrun, M. C., Lerch, F., Loetsch, J., &amp; Ultsch, A.: Visualization and 3D Printing of Multivariate Data of Biomarkers, in Skala, V. (Ed.), International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision (WSCG), Vol. 24, pp. 7-16, Plzen, http://wscg.zcu.cz/wscg2016/short/A43-full.pdf, 2016.
</p>
<p>[McInnes et al., 2018]  McInnes, L., Healy, J., &amp; Melville, J.: Umap: Uniform manifold approximation and projection for dimension reduction, arXiv preprint arXiv:1802.03426, 2018.
</p>
<p>[Demartines/Herault, 1995]  Demartines, P., &amp; Herault, J.: CCA:&quot; Curvilinear component analysis&quot;, Proc. 15 Colloque sur le traitement du signal et des images, Vol. 199, GRETSI, Groupe d Etudes du Traitement du Signal et des Images, France 18-21 September, 1995.
</p>
<p>[Sammon, 1969]  Sammon, J. W.: A nonlinear mapping for data structure analysis, IEEE Transactions on computers, Vol. 18(5), pp. 401-409. doi doi:10.1109/t-c.1969.222678, 1969.
</p>
<p>[Thrun/Ultsch, 2020b]  Thrun, M. C., &amp; Ultsch, A.: Swarm Intelligence for Self-Organized Clustering, Journal of Artificial Intelligence, Vol. in press, pp. doi 10.1016/j.artint.2020.103237, 2020.
</p>
<p>[Torgerson, 1952]  Torgerson, W. S.: Multidimensional scaling: I. Theory and method, Psychometrika, Vol. 17(4), pp. 401-419. 1952.
</p>
<p>[Venna et al., 2010]  Venna, J., Peltonen, J., Nybo, K., Aidos, H., &amp; Kaski, S.: Information retrieval perspective to nonlinear dimensionality reduction for data visualization, The Journal of Machine Learning Research, Vol. 11, pp. 451-490. 2010.
</p>
<p>[Van der Maaten/Hinton, 2008]  Van der Maaten, L., &amp; Hinton, G.: Visualizing Data using t-SNE, Journal of Machine Learning Research, Vol. 9(11), pp. 2579-2605. 2008.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data('Hepta')
out=AutomaticProjectionBasedClustering(Hepta$Data,ClusterNo=7,PlotIt=FALSE)


</code></pre>

<hr>
<h2 id='Chainlink'>
Chainlink introduced in [Ultsch et al., 1994; Ultsch, 1995]. 
</h2><span id='topic+Chainlink'></span>

<h3>Description</h3>

<p>Two chains of rings. Detailed description of dataset and its clustering challenge is provided in [Thrun/Ultsch, 2020].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("Chainlink")</code></pre>


<h3>Details</h3>

<p>Size 1000, Dimensions 3, stored in <code>Chainlink$Data</code>
</p>
<p>Classes 2, stored in <code>Chainlink$Cls</code>
</p>


<h3>References</h3>

<p>[Ultsch et al., 1994]  Ultsch, A., Guimaraes, G., Korus, D., &amp; Li, H.: Knowledge extraction from artificial neural networks and applications, Parallele Datenverarbeitung mit dem Transputer, (pp. 148-162), Springer, 1994.
</p>
<p>[Ultsch, 1995]  Ultsch, A.: Self organizing neural networks perform different from statistical k-means clustering, Proc. Society for Information and Classification (GFKL), Vol. 1995, Basel 8th-10th March 1995.
</p>
<p>[Thrun/Ultsch, 2020]  Thrun, M. C., &amp; Ultsch, A.: Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems, Data in Brief, Vol. 30(C), pp. 105501, <a href="https://doi.org/10.1016/j.dib.2020.105501">doi:10.1016/j.dib.2020.105501</a>, 2020.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Chainlink)
str(Chainlink)
</code></pre>

<hr>
<h2 id='ClusterabilityMDplot'>
Clusterability MDplot
</h2><span id='topic+ClusterabilityMDplot'></span>

<h3>Description</h3>

<p>Clusterability mirrored-density plot. Clusterability aims to quantify the degree of cluster structures [Adolfsson et al., 2019].
A dataset has a high probabilty to possess cluster structures, if the first component of the PCA projection is multimodal [Adolfsson et al., 2019]. As the dip test is less exact than the MDplot [Thrun et al., 2020] , pvalues above 0.05 can be given for MDplots which are clearly multimodal. 
</p>
<p>An alternative investigation of clusterability can be performed by inspecting the topographic map of the Generalized U-Matrix for a specfic projection method using the <span class="pkg">ProjectionBasesdClustering</span> and <span class="pkg">GeneralizedUmatrix</span> packages on CRAN, see [Thrun/Ultsch, 2021] for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ClusterabilityMDplot(DataOrDistance,Method,

na.rm=FALSE,PlotIt=TRUE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ClusterabilityMDplot_+3A_dataordistance">DataOrDistance</code></td>
<td>

<p>Either a dataset[1:n,1:d] of n cases and d features or a symmetric distance matrix [1:d,1:d] 
or multiple data sets or distances in a list
</p>
</td></tr>
<tr><td><code id="ClusterabilityMDplot_+3A_method">Method</code></td>
<td>

<p>&quot;none&quot; performs no dimension reduction.
</p>
<p>&quot;pca&quot; uses the scores from the first principal component.
</p>
<p>&quot;distance&quot; computes pairwise distances (using distance_metric as the metric).
</p>
</td></tr>
<tr><td><code id="ClusterabilityMDplot_+3A_na.rm">na.rm</code></td>
<td>
<p>Statistical testing will not work with missing values, if TRUE values are imputed with averages</p>
</td></tr>
<tr><td><code id="ClusterabilityMDplot_+3A_plotit">PlotIt</code></td>
<td>
<p>TRUE: print plot, otherwise do not plot directly, instead use <code>Handle</code> for further adjustment</p>
</td></tr>
<tr><td><code id="ClusterabilityMDplot_+3A_...">...</code></td>
<td>
<p>Further arguments for function<code>MDplot4multiplevectors</code> of package <span class="pkg">DataVisualizations</span> like <code>"main"</code>, and <code>"Ordering"</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Use the method of [Adolfsson et al., 2019] specified as pca plus dip-test (PCA dip) per default without scaling or standardization of data because this step should never be done automatically. In [Thrun, 2020] the standardization and scaling did not improve the results.
</p>
<p>If list is named, than the names of the list will be used and the MDplots will be re-ordered according to multimodality in the plot, otherwise only the pvalues of [Adolfsson et al., 2019] will be the names and the ordering of the MDplots is the same as the list.
</p>
<p>Beware, as shown below, this test fails for almost touching clusters of Tetra and is difficult to intepret on WingNut but with overlayed with a roubustly estimated unimodal Gaussian distribution it can be interpreted as multimodal). However,  it does not fail for chaining data contrary to the claim in [Adolfsson et al., 2019].
</p>
<p>Based on [Thrun, 2020], the author of this function disagrees with [Adolfsson et al., 2019] as to the preference which clusterablity method should be used because the approach &quot;distance&quot; is not preferable for density-based cluster structures.
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Handle</code></td>
<td>
<p>GGobject, plotter handle of <span class="pkg">ggplot2</span></p>
</td></tr>
<tr><td><code>Pvalue</code></td>
<td>
<p>One or more p-values of dip test depending on <code>DataOrDistance</code></p>
</td></tr>
</table>


<h3>Note</h3>

<p>&quot;none&quot; seems to call dip.test in clusterabilitytest with high-dimensional data. In that case dip.test just vectorizes the matrix of the data which does not make any sense. Since this could be a bug, the &quot;none&quot; option should not be used.
</p>
<p>Imputation does not work for distance matrices. Imputation is still experimental. It is adviced to impute missing values before using this function
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Adolfsson et al., 2019]  Adolfsson, A., Ackerman, M., &amp; Brownstein, N. C.: To cluster, or not to cluster: An analysis of clusterability methods, Pattern Recognition, Vol. 88, pp. 13-26. 2019.
</p>
<p>[Thrun et al., 2020]  Thrun, M. C., Gehlert, T. &amp; Ultsch, A.: Analyzing the Fine Structure of Distributions, PLoS ONE, Vol. 15(10), pp. 1-66, DOI <a href="https://doi.org/10.1371/journal.pone.0238835">doi:10.1371/journal.pone.0238835</a>, 2020. 
</p>
<p>[Thrun/Ultsch, 2021]  Thrun, M. C., and Ultsch, A.: Swarm Intelligence for Self-Organized Clustering, Artificial Intelligence, Vol. 290, pp. 103237, <a href="https://doi.org/10.1016/j.artint.2020.103237">doi:10.1016/j.artint.2020.103237</a>, 2021.
</p>
<p>[Thrun, 2020]  Thrun, M. C.: Improving the Sensitivity of Statistical Testing for Clusterability with Mirrored-Density Plot, in Archambault, D., Nabney, I. &amp; Peltonen, J. (eds.), Machine Learning Methods in Visualisation for Big Data, The Eurographics Association, <a href="https://diglib.eg.org:443/handle/10.2312/mlvis20201102">https://diglib.eg.org:443/handle/10.2312/mlvis20201102</a>, Norrkoping, Sweden, May, 2020.
</p>


<h3>See Also</h3>

<p><code><a href="DataVisualizations.html#topic+MDplot">MDplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##one dataset
data(Hepta)

ClusterabilityMDplot(Hepta$Data)

##multiple datasets
data(Atom)
data(Chainlink)
data(Lsun3D)
data(GolfBall)
data(EngyTime)
data(Target)
data(Tetra)
data(WingNut)
data(TwoDiamonds)

DataV = list(
  Atom = Atom$Data,
  Chainlink = Chainlink$Data,
  Hepta = Hepta$Data,
  Lsun3D = Lsun3D$Data,
  GolfBall = GolfBall$Data,
  EngyTime = EngyTime$Data,
  Target = Target$Data,
  Tetra = Tetra$Data,
  WingNut = WingNut$Data,
  TwoDiamonds = TwoDiamonds$Data
)

ClusterabilityMDplot(DataV)


</code></pre>

<hr>
<h2 id='ClusterApply'>Applies a function over grouped data</h2><span id='topic+ClusterApply'></span>

<h3>Description</h3>

<p>Applies a given function to each dimension <code>d</code> of data separately for each cluster</p>


<h3>Usage</h3>

<pre><code class='language-R'>ClusterApply(DataOrDistances,FUN,Cls,Simple=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ClusterApply_+3A_dataordistances">DataOrDistances</code></td>
<td>
<p>  [1:n,1:d] with: if d=n and symmetric then distance matrix assumed, otherwise:
</p>
<p>[1:n,1:d] matrix of defining the dataset that consists of <code>n</code> cases or d-dimensional data points. Every case has <code>d</code> attributes, variables or features.
</p>
</td></tr>
<tr><td><code id="ClusterApply_+3A_fun">FUN</code></td>
<td>
<p>Function to be applied to each cluster of data and each column of data</p>
</td></tr>
<tr><td><code id="ClusterApply_+3A_cls">Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code id="ClusterApply_+3A_simple">Simple</code></td>
<td>
<p>Boolean, if TRUE, simplifies output</p>
</td></tr>
<tr><td><code id="ClusterApply_+3A_...">...</code></td>
<td>
<p>Additional parameters to be passed on to FUN</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Applies a given function to each feature of each cluster of data using the clustering stored in <code>Cls</code> which is the cluster identifiers for all rows in data. If missing, all data are in first cluster, The main output is <code>FUNPerCluster[i]</code> which is the result of <code>FUN</code> for the data points in cluster of <code>UniqueClusters[i]</code> named with the function's name used.
</p>
<p>In case of a distance matrix an automatic classical multidimensional scaling transformation of distances to data is computed. Number of dimensions is selected by the minimal stress w.r.t. the possible output dimensions of cmdscale.
</p>
<p>If <code>FUN</code> has not function name, then ResultPerCluster is given back.
</p>


<h3>Value</h3>

<p>if(Simple==FALSE)
List with
</p>
<table>
<tr><td><code>UniqueClusters</code></td>
<td>
<p>The unique clusters in Cls</p>
</td></tr>
<tr><td><code>FUNPerCluster</code></td>
<td>
<p>a matrix of [1:k,1:d] of d features and k clusters, the list element is named by the function <code>FUN</code> used </p>
</td></tr>
</table>
<p>if(Simple==TRUE)
</p>
<p>a matrix of [1:k,1:d] of d features and k clusters
</p>


<h3>Author(s)</h3>

<p>Felix Pape, Michael Thrun</p>


<h3>Examples</h3>

<pre><code class='language-R'>##one dataset
data(Hepta)
Data=Hepta$Data
Cls=Hepta$Cls
#mean per cluster
ClusterApply(Data,mean,Cls)

#Simplified
ClusterApply(Data,mean,Cls,Simple=TRUE)

# Mean per cluster of MDS transformation
# Beware, this is not the same!

ClusterApply(as.matrix(dist(Data)),mean,Cls)


## Not run: 
Iris=datasets::iris
Distances=as.matrix(Iris[,1:4])
SomeFactors=Iris$Species
V=ClusterCreateClassification(SomeFactors)
Cls=V$Cls
V$ClusterNames
ClusterApply(Distances,mean,Cls)

## End(Not run)
#special case of identity
## Not run: 
suppressPackageStartupMessages(library('prabclus',quietly = TRUE))
data(tetragonula)
#Generated Specific Distance Matrix
ta &lt;- alleleconvert(strmatrix=as.matrix(tetragonula[1:236,]))
tai &lt;- alleleinit(allelematrix=ta,distance="none")
Distance=alleledist((unbuild.charmatrix(tai$charmatrix,236,13)),236,13)

MDStrans=ClusterApply(Distance,identity)$identityPerCluster

## End(Not run)
</code></pre>

<hr>
<h2 id='ClusterARI'>
Adjusted Rand index
</h2><span id='topic+ClusterARI'></span>

<h3>Description</h3>

<p>Adjusted Rand index for two clusterings that should be compared to each other. 
This index has expected value zero for independant clusterings and maximum value 1 (for identical clusterings).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ClusterARI(Cls1, Cls2,Fast=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ClusterARI_+3A_cls1">Cls1</code></td>
<td>

<p>1:n numerical vector of numbers defining the classification as the main output of the first clustering  or trial for the n cases of data. It has k unique numbers representing the arbitrary labels of the clustering.
</p>
</td></tr>
<tr><td><code id="ClusterARI_+3A_cls2">Cls2</code></td>
<td>

<p>1:n numerical vector of numbers defining the classification as the main output of the second clustering algorithm trial for the n cases of data. It has p unique numbers representing the arbitrary labels of the clustering.
</p>
</td></tr>
<tr><td><code id="ClusterARI_+3A_fast">Fast</code></td>
<td>

<p>TRUE:uses mclust package which maybe does not integrate all published insights 
about ARI
FALSE: uses partitionComparison package
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>&quot;The expected value of the Rand Index of two random partitions does not take a constant value (e.g. zero). 
Thus, Hubert and Arabie proposed an adjustment [Hubert &amp; Arabie]  which assumes a generalized hypergeometric distribution as null hypothesis: 
the two clusterings are drawn randomly with a fixed number of clusters and a fixed number of elements in each cluster 
(the number of clusters in the two clusterings need not be the same). 
Then the adjusted Rand Index is the (normalized) difference of the Rand Index and its expected value under the null hypothesis. 
The significance of this measure has to be put into question because of the strong assumptions it makes on the distribution. 
Meila [Meila, 2003] notes, that some pairs of clusterings may result in negative index values&quot; [Wagner and Wagner, 2007].
</p>


<h3>Value</h3>

<p>value of adjusted rand index
</p>


<h3>Note</h3>

<p>the equation of adjusted random index ignores the labels themselve and measures only the agreement. Hence, one can compare clusterin solutions for k!=p unique numbers that represent the labels, see second example
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Rand, 1971]  Rand, W. M.: Objective criteria for the evaluation of clustering methods, Journal of the American Statistical Association, Vol. 66(336), pp. 846-850, 1971.
</p>
<p>[Hubert &amp; Arabie]  Hubert, L. and Arabie, P.: Comparing partitions, Journal of Classification. Vol. 2 (1), pp. 193-218. doi:10.1007/BF01908075, 1985.
</p>
<p>[Ball/Geyer-Schulz, 2018]  Ball, F., &amp; Geyer-Schulz, A.: Invariant Graph Partition Comparison Measures, Symmetry, Vol. 10(10), pp. 1-27, 2018.
</p>
<p>[Meila, 2003]	Meila, Marina: Comparing Clusterings. COLT 2003.
</p>
<p>[Wagner and Wagner, 2007]	Wagner, Silke; Wagner, Dorothea. Comparing clusterings: an overview. Karlsruhe: Universitaet Karlsruhe, Fakultaet fr Informatik, 2007.
</p>


<h3>See Also</h3>

<p><code><a href="partitionComparison.html#topic+adjustedRandIndex">adjustedRandIndex</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Hepta)
#compare to baseline
Cls2=kmeansClustering(Hepta$Data,7,Type = "Steinley")$Cls
ClusterARI(Hepta$Cls,Cls2)
#compare different solutions
Cls3=kmeansClustering(Hepta$Data,5)$Cls
ClusterARI(Cls3,Cls2)

</code></pre>

<hr>
<h2 id='ClusterChallenge'>
Generates a Fundamental Clustering Challenge based on specific artificial datasets.
</h2><span id='topic+ClusterChallenge'></span>

<h3>Description</h3>

<p>Lsun3D and FCPS datasets were introduced in various publications for a specific fixed size. This function generalizes them for any sample size.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ClusterChallenge(Name,SampleSize,

PlotIt=FALSE,PointSize=1,Plotter3D="rgl",...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ClusterChallenge_+3A_name">Name</code></td>
<td>

<p>string, either 'Atom', 'Chainlink, 'EngyTime', 'GolfBall', 'Hepta', 'Lsun3D',  'Target'
'Tetra'
'TwoDiamonds'
'WingNut
</p>
</td></tr>
<tr><td><code id="ClusterChallenge_+3A_samplesize">SampleSize</code></td>
<td>

<p>Size of Sample higher than 300, preferable above 500
</p>
</td></tr>
<tr><td><code id="ClusterChallenge_+3A_plotit">PlotIt</code></td>
<td>

<p>TRUE: Plots the challenge with <code><a href="#topic+ClusterPlotMDS">ClusterPlotMDS</a></code>
</p>
</td></tr>
<tr><td><code id="ClusterChallenge_+3A_pointsize">PointSize</code></td>
<td>

<p>If PlotIt=TRUE: see <code><a href="#topic+ClusterPlotMDS">ClusterPlotMDS</a></code>
</p>
</td></tr>
<tr><td><code id="ClusterChallenge_+3A_plotter3d">Plotter3D</code></td>
<td>

<p>If PlotIt=TRUE: see <code><a href="#topic+ClusterPlotMDS">ClusterPlotMDS</a></code>
</p>
</td></tr>
<tr><td><code id="ClusterChallenge_+3A_...">...</code></td>
<td>

<p>If PlotIt=TRUE: further arguments for <code><a href="#topic+ClusterPlotMDS">ClusterPlotMDS</a></code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A detailed description of the datasets can be found in [Thrun/Ultsch 2020]. Sampling works by combining Pareto Density Estimation with rejection sampling.
</p>


<h3>Value</h3>

<p>LIST, with
</p>
<table>
<tr><td><code>Name</code></td>
<td>
<p>[1:SampleSize,1:d] data matrix</p>
</td></tr>
<tr><td><code>Cls</code></td>
<td>
<p>[1:SampleSize] numerical vector of classification</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Thrun/Ultsch, 2020]  Thrun, M. C., &amp; Ultsch, A.: Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems, Data in Brief, Vol. in press, pp. 105501, <a href="https://doi.org/10.1016/j.dib.2020.105501">doi:10.1016/j.dib.2020.105501</a>, 2020.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ClusterPlotMDS">ClusterPlotMDS</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
ClusterChallenge("Chainlink",2000,PlotIt=TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='ClusterCount'>ClusterCount</h2><span id='topic+ClusterCount'></span>

<h3>Description</h3>

<p>Calulates statistics for clustering in each group of the data points
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ClusterCount(Cls,Ordered=TRUE,NonFinite=9999)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ClusterCount_+3A_cls">Cls</code></td>
<td>
<p>1:n numerical vector of numbers defining the classification as the main output of the clustering algorithm for the n cases of data. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code id="ClusterCount_+3A_ordered">Ordered</code></td>
<td>

<p>Optional, boolean, if TRUE: the ouput is ordered increasingly by cluster labels in <code>UniqueClusters</code>.
</p>
</td></tr>
<tr><td><code id="ClusterCount_+3A_nonfinite">NonFinite</code></td>
<td>

<p>Optional, If non finite values are given in the numerical vector, they are set to the scalar value defined here</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The ordering of the output is defined by the first occurence of every cluster label in <code>Cls</code> in the  setting of <code>Ordered=FALSE</code>. 
</p>
<p>The function can be overloaded with non-numerical vectors. In this case, a cast via as.character() is applied to <code>Cls</code>, a warning is stated, and the statistics are still computed.
</p>


<h3>Value</h3>

<table>
<tr><td><code>UniqueClusters</code></td>
<td>
<p>[1:k] numerical vector of the k unique clusters in Cls</p>
</td></tr>
<tr><td><code>CountPerCluster</code></td>
<td>
<p>Named vector [1:k] with the number of data points in the corresponding unique clusters. Names are the <code>UniqueClusters</code></p>
</td></tr>
<tr><td><code>NumberOfClusters</code></td>
<td>
<p>The number of clusters k</p>
</td></tr>
<tr><td><code>ClusterPercentages</code></td>
<td>
<p>[1:k] numerical vector of the percentages of datapoints belonging to a cluster for each cluster</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
Cls=Hepta$Cls
ClusterCount(Cls)
</code></pre>

<hr>
<h2 id='ClusterCreateClassification'>
Create Classification for Cluster.. functions
</h2><span id='topic+ClusterCreateClassification'></span>

<h3>Description</h3>

<p>Creates a Cls from arbitrary list of objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ClusterCreateClassification(Objects,Decreasing)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ClusterCreateClassification_+3A_objects">Objects</code></td>
<td>

<p>Listed objects, for example factor
</p>
</td></tr>
<tr><td><code id="ClusterCreateClassification_+3A_decreasing">Decreasing</code></td>
<td>

<p>Boolean that can be missing. If given, sorts <code>ClusterNames</code> with either decreasing or increasing.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ClusterNames</code> can be sorted before the classification stored <code>Cls</code> is created. See example.
</p>


<h3>Value</h3>

<p>LIST, with
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n] numerical vector with n numbers defining the labels of the classification. It has 1 to k unique numbers representing the arbitrary labels of the classification.</p>
</td></tr>
<tr><td><code>ClusterNames</code></td>
<td>
<p>ClusterNames defined which names belongs to which unique number</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
  Iris=datasets::iris
  SomeFactors=Iris$Species
  V=ClusterCreateClassification(SomeFactors)
  Cls=V$Cls
  V$ClusterNames
  table(Cls,SomeFactors)
  
  #Increasing alphabetical order
  V=ClusterCreateClassification(SomeFactors,Decreasing=FALSE)
  Cls=V$Cls
  V$ClusterNames
  table(Cls,SomeFactors)
  
## End(Not run)
</code></pre>

<hr>
<h2 id='ClusterDaviesBouldinIndex'>
Davies Bouldin Index
</h2><span id='topic+ClusterDaviesBouldinIndex'></span>

<h3>Description</h3>

<p>Internal (i.e. without prior classification) cluster quality measure called Davies Bouldin index for a given clustering published in [Davies/Bouldin, 1979].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ClusterDaviesBouldinIndex(Cls, Data,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ClusterDaviesBouldinIndex_+3A_cls">Cls</code></td>
<td>

<p>[1:n] numerical vector of numbers defining the classification as the main output of the clustering algorithm for the n cases of data. It has k unique numbers representing the arbitrary labels of the clustering.
</p>
</td></tr>
<tr><td><code id="ClusterDaviesBouldinIndex_+3A_data">Data</code></td>
<td>

<p>matrix, [1:d,1:n] dataset of d variables and n cases
</p>
</td></tr>
<tr><td><code id="ClusterDaviesBouldinIndex_+3A_...">...</code></td>
<td>
<p>Further arguments passed on to the <code><a href="clusterSim.html#topic+index.DB">index.DB</a></code> function of <code>clusterSim</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Wrapper for <code><a href="clusterSim.html#topic+index.DB">index.DB</a></code>. Davies Bouldin index is defined in [Davies/Bouldin, 1979]. Best clustering scheme essentially minimizes the Davies-Bouldin index because it is defined as the function of the ratio of the within cluster scatter, to the between cluster separation.[Davies/Bouldin, 1979].
</p>


<h3>Value</h3>

<p>List of 
</p>
<table>
<tr><td><code>DaviesBouldinIndex</code></td>
<td>

<p>scalar,Davies Bouldin index
</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>

<p>further information stored in  <code><a href="clusterSim.html#topic+index.DB">index.DB</a></code>
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Davies/Bouldin, 1979]  Davies, D. L., &amp; Bouldin, D. W.: A cluster separation measure, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 1(2), pp. 224-227. doi 10.1109/TPAMI.1979.4766909, 1979.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Hepta")
Cls=kmeansClustering(Hepta$Data,ClusterNo = 7,Type="Hartigan")$Cls
ClusterDaviesBouldinIndex(Cls,Hepta$Data)[1]


data("Hepta")
ClsWellSeperated=kmeansClustering(Hepta$Data,ClusterNo = 7,Type="Steinley")$Cls
ClusterDaviesBouldinIndex(ClsWellSeperated,Hepta$Data)[1]

</code></pre>

<hr>
<h2 id='ClusterDendrogram'>
Cluster Dendrogram
</h2><span id='topic+ClusterDendrogram'></span>

<h3>Description</h3>

<p>Presents a dendrogram of a given tree using a colorsequence for the branches defined from the highest cluster size to the lowest cluster size.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ClusterDendrogram(TreeOrDendrogram, ClusterNo, 

Colorsequence,main='Name of Algorithm')
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ClusterDendrogram_+3A_treeordendrogram">TreeOrDendrogram</code></td>
<td>

<p>Either object of hcclust defining the tree, third list element of hierarchical cluster algorithms of this package
</p>
<p>or
</p>
<p>Object of class dendrogram,  second list element of hierarchical cluster algorithms.
</p>
</td></tr>
<tr><td><code id="ClusterDendrogram_+3A_clusterno">ClusterNo</code></td>
<td>

<p>k number of clusters for cutree.
</p>
</td></tr>
<tr><td><code id="ClusterDendrogram_+3A_colorsequence">Colorsequence</code></td>
<td>

<p>[1:k] character vector of colors, per default the colorsquence defined in the <span class="pkg">DataVisualizations</span> is used
</p>
</td></tr>
<tr><td><code id="ClusterDendrogram_+3A_main">main</code></td>
<td>
<p>Title of plot</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Reqires the package <span class="pkg">dendextend</span> to work correctly.
</p>


<h3>Value</h3>

<p>In mode invisible:
</p>
<p>[1:n]  numerical vector defining the clustering of k clusters; this classification is the main output of the algorithm.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+cutree">cutree</a></code>, <code><a href="stats.html#topic+hclust">hclust</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Lsun3D)
listofh=HierarchicalClustering(Lsun3D$Data,0,'SingleL')
Tree=listofh$Object
#given colors are per default:
#"magenta" "yellow"  "black"   "red"
ClusterDendrogram(Tree, 4,main='Single Linkage Clustering')

listofh=HierarchicalClustering(Lsun3D$Data,4)
ClusterCount(listofh$Cls)
#c1 is magenta, c2 is red, c3 is yellow, c4 is black
#because the order of the cluster sizes is
#c1,c3,c4,c2
</code></pre>

<hr>
<h2 id='ClusterDistances'>
ClusterDistances
</h2><span id='topic+ClusterIntraDistances'></span><span id='topic+IntraClusterDistances'></span><span id='topic+ClusterDistances'></span>

<h3>Description</h3>

<p>Computes intra-cluster distances which are the distance in-between each cluster.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ClusterDistances(FullDistanceMatrix, Cls,

Names, PlotIt = FALSE)

ClusterIntraDistances(FullDistanceMatrix, Cls,

Names, PlotIt = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ClusterDistances_+3A_fulldistancematrix">FullDistanceMatrix</code></td>
<td>

<p>[1:n,1:n] symmetric distance matrix
</p>
</td></tr>
<tr><td><code id="ClusterDistances_+3A_cls">Cls</code></td>
<td>

<p>[1:n] numerical vector of k classes
</p>
</td></tr>
<tr><td><code id="ClusterDistances_+3A_names">Names</code></td>
<td>

<p>Optional [1:k] character vector naming k classes
</p>
</td></tr>
<tr><td><code id="ClusterDistances_+3A_plotit">PlotIt</code></td>
<td>

<p>Optional, Plots if TRUE
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Cluster distances are given back as a matrix, one column per cluster and the vector of the full distance matrix without the diagonal elements and the upper half of the symmetric matrix.
Details and definitons can be found in [Thrun, 2021].
</p>


<h3>Value</h3>

<p>Matrix [1:m,1:(k+1)] of k clusters, each columns consists of the distances in a cluster, filled up with NaN at the end to be of the same length as the vector of the upper triangle of the complete distance matrix.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Thrun, 2021]  Thrun, M. C.: The Exploitation of Distance Distributions for Clustering, International Journal of Computational Intelligence and Applications, Vol. 20(3), pp. 2150016, DOI: <a href="https://doi.org/10.1142/S1469026821500164">doi:10.1142/S1469026821500164</a>, 2021.
</p>


<h3>See Also</h3>

<p><code><a href="DataVisualizations.html#topic+MDplot">MDplot</a></code>
</p>
<p><code><a href="#topic+ClusterInterDistances">ClusterInterDistances</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Hepta)
Distance=as.matrix(dist(Hepta$Data))

interdists=ClusterDistances(Distance,Hepta$Cls)
</code></pre>

<hr>
<h2 id='ClusterDunnIndex'>
Dunn Index
</h2><span id='topic+ClusterDunnIndex'></span>

<h3>Description</h3>

<p>Internal (i.e. without prior classification) cluster quality measure called Dunn index for a given clustering published in [Dunn, 1974].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ClusterDunnIndex(Cls,DataOrDistances,

DistanceMethod="euclidean",Silent=TRUE,Force=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ClusterDunnIndex_+3A_cls">Cls</code></td>
<td>

<p>[1:n] numerical vector of numbers defining the classification as the main output of the clustering algorithm for the n cases of data. It has k unique numbers representing the arbitrary labels of the clustering.
</p>
</td></tr>
<tr><td><code id="ClusterDunnIndex_+3A_dataordistances">DataOrDistances</code></td>
<td>

<p>matrix, DataOrDistance[1:n,1:n] symmetric matrix of dissimilarities, if variable unsymmetric 
DataOrDistance[1:d,1:n] is assumed as a dataset and the euclidean distances are calculated of d variables and n cases
</p>
</td></tr>
<tr><td><code id="ClusterDunnIndex_+3A_distancemethod">DistanceMethod</code></td>
<td>

<p>Optional, one of 39 distance methods of <code>parDist</code> of package parallelDist,  if Data matrix is chosen above
</p>
</td></tr>
<tr><td><code id="ClusterDunnIndex_+3A_silent">Silent</code></td>
<td>

<p>TRUE: Warnings are shown
</p>
</td></tr>
<tr><td><code id="ClusterDunnIndex_+3A_force">Force</code></td>
<td>

<p>TRUE: force computing in case of numerical instability
</p>
</td></tr>
<tr><td><code id="ClusterDunnIndex_+3A_...">...</code></td>
<td>
<p>Further arguments passed on to the <code>parDist</code> function, e.g. user_defined distance functions</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Dunn index is defined as <code>Dunn=min(InterDist)/max(IntraDist)</code>. Well seperated clusters have usually a dunn index above 1, for details please see [Dunn, 1974].
</p>


<h3>Value</h3>

<p>List of 
</p>
<table>
<tr><td><code>Dunn</code></td>
<td>

<p>scalar, Dunn Index
</p>
</td></tr>
<tr><td><code>IntraDist</code></td>
<td>

<p>[1:k] numerical vector of minimal intra cluster distances per given cluster
</p>
</td></tr>
<tr><td><code>InterDist</code></td>
<td>

<p>[1:k] numerical vector of minimal inter cluster distances per given cluster
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Dunn, 1974]  Dunn, J. C.: Well_separated clusters and optimal fuzzy partitions, Journal of cybernetics, Vol. 4(1), pp. 95-104. 1974.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Hepta")
Cls=kmeansClustering(Hepta$Data,ClusterNo = 7,Type="Hartigan")$Cls
ClusterDunnIndex(Cls,Hepta$Data)


data("Hepta")
ClsWellSeperated=kmeansClustering(Hepta$Data,ClusterNo = 7,Type="Steinley")$Cls
ClusterDunnIndex(ClsWellSeperated,Hepta$Data)

</code></pre>

<hr>
<h2 id='ClusterEqualWeighting'>
ClusterEqualWeighting
</h2><span id='topic+ClusterEqualWeighting'></span>

<h3>Description</h3>

<p>Weights clusters equally
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ClusterEqualWeighting(Cls, Data, MinClusterSize)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ClusterEqualWeighting_+3A_cls">Cls</code></td>
<td>

<p>1:n numerical vector of numbers defining the classification as the main output of the clustering algorithm for the n cases of data. It has k unique numbers representing the arbitrary labels of the clustering.
</p>
</td></tr>
<tr><td><code id="ClusterEqualWeighting_+3A_data">Data</code></td>
<td>

<p>Optional, [1:n,1:d] matrix of dataset consisting of n cases of d-dimensional data points. Every case has d attributes, variables or features.
</p>
</td></tr>
<tr><td><code id="ClusterEqualWeighting_+3A_minclustersize">MinClusterSize</code></td>
<td>

<p>Optional, scalar defining the number of cases m that each cluster should have
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Balance clusters such that their sizes are the same by subsampling the larger cluster. If <code>MinClusterSize</code> is missing the number of cases per cluster is set to the smallest cluster size. For clusters sizes smaller than <code>MinClusterSize</code>, sampling with replacement is turned on, i.e. up sampling. For clusters sizes equal to <code>MinClusterSize</code>, no sampling is performed.
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>BalancedCls</code></td>
<td>
<p>Vector of Cls such that all clusters have the same sizes spezified by <code>MinClusterSize</code></p>
</td></tr>
<tr><td><code>BalancedInd</code></td>
<td>
<p>index such that BalancedCls = Cls[BalancedInd]</p>
</td></tr>
<tr><td><code>BalancedData</code></td>
<td>
<p>NULL if missing, otherwise, Data[BalancedInd,]</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Alfred Ultsch (matlab), reimplemented by Michael Thrun 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Hepta)
ClusterEqualWeighting(Hepta$Cls,Hepta$Data,5)
</code></pre>

<hr>
<h2 id='ClusteringAccuracy'>
ClusterAccuracy
</h2><span id='topic+ClusterAccuracy'></span>

<h3>Description</h3>

<p>ClusterAccuracy
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ClusterAccuracy(PriorCls,CurrentCls,K=9)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ClusteringAccuracy_+3A_priorcls">PriorCls</code></td>
<td>

<p>Ground truth,[1:n] numerical vector with n numbers defining the classification. It has k unique numbers representing the arbitrary labels of the clustering.
</p>
</td></tr>
<tr><td><code id="ClusteringAccuracy_+3A_currentcls">CurrentCls</code></td>
<td>

<p>Main output of the clustering, [1:n]  numerical vector with n numbers defining the classification. It has k unique numbers representing the arbitrary labels of the clustering.
</p>
</td></tr>
<tr><td><code id="ClusteringAccuracy_+3A_k">K</code></td>
<td>

<p>Maximal number of classes for computation.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Here, accuracy is defined as the normalized sum over all true positive labeled data points of a clustering algorithm. The best of all permutation of labels with the highest accuracy is selected in every trial because algorithms arbitrarily define the labels [Thrun et al., 2018]. Beware that in contrast to <code><a href="#topic+ClusterMCC">ClusterMCC</a></code>, the labels can be arbitrary. However, accuracy is a only a valid quality measure if the clusters are balanced (of) nearly equal size). Ohterwise please use <code><a href="#topic+ClusterMCC">ClusterMCC</a></code>.
</p>
<p>In contrast to the F-measure, &quot;Accuracy  tends  to  be  naturally  unbiased,   because  it can  be  expressed  in  terms  of a  binomial  distribution: A success in  the  underlying  Bernoulli  trial  would  be defined as sampling an example for which a classifier under consideration  makes  the  right  prediction. By definition, the success probability is  identical  to  the  accuracy  of  the classifier.  The i.i.d. assumption implies that each example of  the  test  set  is  sampled  independently,  so  the  expected fraction  of  correctly  classified  samples  is  identical  to  the probability  of  seeing  a  success  above. Averaging  over multiple  folds  is  identical  to  increasing  the  number  of repetitions of the Binomial trial.  This does not affect the posterior distribution of accuracy 
if the test sets are of equal size, or if we weight each estimate by the size of each test set.&quot; [Forman/Scholz, 2010]
</p>


<h3>Value</h3>

<p>Single scalar of Accuracy between zero and one
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Thrun et al., 2018]	Michael C. Thrun, Felix Pape, Alfred Ultsch: Benchmarking Cluster Analysis Methods in the Case of Distance and Density-based Structures Defined by a Prior Classification Using PDE-Optimized Violin Plots, ECDA, Potsdam, 2018
</p>
<p>[Forman/Scholz, 2010]  Forman, G., and Scholz, M.: Apples-to-apples in cross-validation studies: pitfalls in classifier performance measurement, ACM SIGKDD Explorations Newsletter, Vol. 12(1), pp. 49-57. 2010.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ClusterMCC">ClusterMCC</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#Influence of random sets/ random starts on k-means

data('Hepta')
Cls=kmeansClustering(Hepta$Data,7,Type = "Hartigan",nstart=1)
table(Cls$Cls,Hepta$Cls)
ClusterAccuracy(Hepta$Cls,Cls$Cls)



data('Hepta')
Cls=kmeansClustering(Hepta$Data,7,Type = "Hartigan",nstart=100)
table(Cls$Cls,Hepta$Cls)
ClusterAccuracy(Hepta$Cls,Cls$Cls)

</code></pre>

<hr>
<h2 id='ClusterInterDistances'>
Computes Inter-Cluster Distances
</h2><span id='topic+ClusterInterDistances'></span><span id='topic+InterClusterDistances'></span>

<h3>Description</h3>

<p>Computes inter-cluster distances which are the distance between each cluster and all other clusters
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ClusterInterDistances(FullDistanceMatrix, Cls,

Names,PlotIt=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ClusterInterDistances_+3A_fulldistancematrix">FullDistanceMatrix</code></td>
<td>

<p>[1:n,1:n] symmetric distance matrix
</p>
</td></tr>
<tr><td><code id="ClusterInterDistances_+3A_cls">Cls</code></td>
<td>

<p>[1:n] numerical vector of numbers defining the classification as the main output of the clustering algorithm for the n cases of data. It has k unique numbers representing the arbitrary labels of the clustering.
</p>
</td></tr>
<tr><td><code id="ClusterInterDistances_+3A_names">Names</code></td>
<td>

<p>Optional [1:k] character vector naming k classes
</p>
</td></tr>
<tr><td><code id="ClusterInterDistances_+3A_plotit">PlotIt</code></td>
<td>

<p>Optional, Plots if TRUE
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Cluster distances are given back as a matrix, one column per cluster and the vector of the full distance matrix without the diagonal elements and the upper half of the symmetric matrix. Details and definitons can be found in [Thrun, 2021].
</p>


<h3>Value</h3>

<p>Matrix [1:m,1:(k+1)] of k clusters, each columns consists of the distances between a cluster and all other clusters, filled up with NaN at the end to be of the same lenght as the vector of the upper triangle of the complete distance matrix.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Thrun, 2021]  Thrun, M. C.: The Exploitation of Distance Distributions for Clustering, International Journal of Computational Intelligence and Applications, Vol. 20(3), pp. 2150016, DOI: <a href="https://doi.org/10.1142/S1469026821500164">doi:10.1142/S1469026821500164</a>, 2021.
</p>


<h3>See Also</h3>

<p><code><a href="DataVisualizations.html#topic+MDplot">MDplot</a></code>
</p>
<p><code>ClusterDistances</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Hepta)
Distance=as.matrix(dist(Hepta$Data))

interdists=ClusterInterDistances(Distance,Hepta$Cls)
</code></pre>

<hr>
<h2 id='ClusterMCC'>
Matthews Correlation Coefficient (MCC)
</h2><span id='topic+ClusterMCC'></span>

<h3>Description</h3>

<p>Matthews correlation coefficient eneralized to the multiclass case (a.k.a.  R_K statistic).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ClusterMCC(PriorCls, CurrentCls,Force=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ClusterMCC_+3A_priorcls">PriorCls</code></td>
<td>

<p>Ground truth,[1:n] numerical vector with n numbers defining the classification. It has k unique numbers representing the labels of the clustering.
</p>
</td></tr>
<tr><td><code id="ClusterMCC_+3A_currentcls">CurrentCls</code></td>
<td>

<p>Main output of the clustering, [1:n]  numerical vector with n numbers defining the classification. It has k unique numbers representing the labels of the clustering.
</p>
</td></tr>
<tr><td><code id="ClusterMCC_+3A_force">Force</code></td>
<td>

<p>Boolean, if is TRUE: forces code even if one or more than one of the k numbers given in <code>PriorCls</code> is missing in <code>CurrentCls</code> or vice versa. In this case, one label per missing number is added ad the end of the vectors.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Contrary to accuracy, the MCC is balanced measure which can be used even if the classes are of very different sizes. When there are more than two labels the MCC will no longer range between -1 and +1. Instead the minimum value will be between -1 and 0 depending on the true distribution. The maximum value is always +1.
Beware that in contrast to <code><a href="#topic+ClusterAccuracy">ClusterAccuracy</a></code>, the labels cannot be arbitrary. Instead each label of <code>PriorCls</code> and <code>CurrentCls</code> has to be mapped to the same cluster of data points. Typically this has to be ensured manually.
</p>


<h3>Value</h3>

<p>Single scalar of MCC in a range described in details.
</p>


<h3>Note</h3>

<p>If No. of Clusters is not equivalent, internally the number is allgined with zero datapoints belonging to the missing clusters.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>Matthews, B. W.: Comparison of the predicted and observed secondary structure of T4 phage lysozyme, Biochimica et Biophysica Acta (BBA), Protein Structure, Vol. 405(2), pp. 442-451, 1975.
</p>
<p>Boughorbel, S.B: Optimal classifier for imbalanced data using Matthews Correlation Coefficient metric, PLOS ONE, Vol. 12(6), pp. e0177678, 2017.
</p>
<p>Chicco, D.; Toetsch, N. and Jurman, G.: The Matthews correlation coefficient (MCC) is more reliable than balanced accuracy, bookmaker informedness, and markedness in two_class confusion matrix evaluation. BioData Mining. Vol. 14., 2021.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ClusterAccuracy">ClusterAccuracy</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Beware that algorithm arbitrary defines the labels
data(Hepta)
V=kmeansClustering(Hepta$Data,Type = "Hartigan",7)
table(V$Cls,Hepta$Cls)
#result is only valid if the above issue is resolved manually
ClusterMCC(Hepta$Cls,V$Cls)
</code></pre>

<hr>
<h2 id='ClusterNoEstimation'>
Estimates Number of Clusters using up to 26 Indicators
</h2><span id='topic+ClusterNoEstimation'></span>

<h3>Description</h3>

<p>Calculation of up to 26 indicators and the recommendations based on them for the number of clusters in data sets. 
For a given dataset and clusterings for this dataset, key indicators mentioned in details are calculated and based on this a recommendation regarding the number of clusters is given for each indicator.
</p>
<p>An alternative estimation of the cluster number can be done by counting the valleys of the topographic map of the generalized U-Matrix for a specfic projection method using the <span class="pkg">ProjectionBasesdClustering</span> and <span class="pkg">GeneralizedUmatrix</span> packages on CRAN, see [Thrun/Ultsch, 2021] for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ClusterNoEstimation(DataOrDistances, ClsMatrix = NULL, MaxClusterNo, 

ClusterIndex = "all", Method = NULL, MinClusterNo = 2,

Silent = TRUE,PlotIt=FALSE,SelectByABC=TRUE,Colorsequence,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ClusterNoEstimation_+3A_dataordistances">DataOrDistances</code></td>
<td>

<p>Either [1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.
</p>
<p>or
</p>
<p>Symmetric [1:n,1:n] distance matrix
</p>
</td></tr>
<tr><td><code id="ClusterNoEstimation_+3A_clsmatrix">ClsMatrix</code></td>
<td>

<p>[1:n,1:(MaxClusterNo)] matrix of clusterings each columns is defined as: 
</p>
<p>1:n numerical vector of numbers defining the classification as the main output of the clustering algorithm for the n cases of data. It has k unique numbers representing the arbitrary labels of the clustering.
</p>
<p>(see also details (2) and (3)), must be specified if method = NULL
</p>
</td></tr>
<tr><td><code id="ClusterNoEstimation_+3A_maxclusterno">MaxClusterNo</code></td>
<td>

<p>Highest number of clusters to be checked
</p>
</td></tr>
<tr><td><code id="ClusterNoEstimation_+3A_method">Method</code></td>
<td>

<p>Cluster procedure, with which the clusterings are created (see details (4) for possible methods), must be specified if ClsMatrix = NULL
</p>
</td></tr>
</table>
<p>Optional:
</p>
<table>
<tr><td><code id="ClusterNoEstimation_+3A_clusterindex">ClusterIndex</code></td>
<td>

<p>String or vector of strings with the indicators to be calculated (see details (1)), default = &quot;all
</p>
</td></tr>
<tr><td><code id="ClusterNoEstimation_+3A_minclusterno">MinClusterNo</code></td>
<td>

<p>Lowest number of clusters to be checked, default = 2
</p>
</td></tr>
<tr><td><code id="ClusterNoEstimation_+3A_silent">Silent</code></td>
<td>

<p>If TRUE status messages are output, default = FALSE
</p>
</td></tr>
<tr><td><code id="ClusterNoEstimation_+3A_plotit">PlotIt</code></td>
<td>

<p>If TRUE plots fanplot with proposed cluster numbers
</p>
</td></tr>
<tr><td><code id="ClusterNoEstimation_+3A_selectbyabc">SelectByABC</code></td>
<td>

<p>If PlotIt=TRUE, TRUE: Plots group A of ABCanalysis of the most important ones (highest overlap in indicators), FALSE: plots all indicators
</p>
</td></tr>
<tr><td><code id="ClusterNoEstimation_+3A_colorsequence">Colorsequence</code></td>
<td>

<p>Optional, character vector of sufficient length of colors for the fan plot.If the sequence is too long the first part of the sequence is used.
</p>
</td></tr>
<tr><td><code id="ClusterNoEstimation_+3A_...">...</code></td>
<td>

<p>Optional, further arguents used if clustering methods if <code>Method</code> is set.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each column of <code>ClsMatrix</code> has to have at least two unqiue clusters defined. Otherwise the function will stop.
</p>
<p>(1)
</p>
<p>The following 26 indicators can be calculated:
&quot;ball&quot;, &quot;beale&quot;, &quot;calinski&quot;, &quot;ccc&quot;, &quot;cindex&quot;, &quot;db&quot;, &quot;duda&quot;, 
&quot;dunn&quot;, &quot;frey&quot;, &quot;friedman&quot;, &quot;hartigan&quot;, &quot;kl&quot;, &quot;marriot&quot;, &quot;mcclain&quot;, 
&quot;pseudot2&quot;, &quot;ptbiserial&quot;, &quot;ratkowsky&quot;, &quot;rubin&quot;, &quot;scott&quot;, &quot;sdbw&quot;, 
&quot;sdindex&quot;, &quot;silhouette&quot;, &quot;ssi&quot;, &quot;tracew&quot;, &quot;trcovw&quot;, &quot;xuindex&quot;.
</p>
<p>These can be specified individually or as a vector via the parameter index.
If you enter 'all', all key figures are calculated.
</p>
<p>(2)
</p>
<p>The indicators kl, duda, pseudot2, beale, frey and mcclain require a clustering for MaxClusterNo+1 clusters. If these key figures are to be calculated, this clustering must be specified in cls.
</p>
<p>(3)
</p>
<p>The indicator kl requires a clustering for MinClusterNo-1 clusters. If this key figure is to be calculated, this clustering must also be specified in cls. For the case MinClusterNo = 2 no clustering for 1 has to be given.
</p>
<p>(4)
</p>
<p>The following methods can be used to create clusterings:
</p>
<p>&quot;kmeans,&quot; &quot;DBSclustering&quot;,&quot;DivisiveAnalysisClustering&quot;,&quot;FannyClustering&quot;, &quot;ModelBasedClustering&quot;,&quot;SpectralClustering&quot; or all methods found in <code><a href="#topic+HierarchicalClustering">HierarchicalClustering</a></code>.
</p>
<p>(5)
</p>
<p>The indicators duda, pseudot2, beale and frey are only intended for use in hierarchical cluster procedures.
</p>
<p>If a distances matrix is given, then <span class="pkg">ProjectionBasedClustering</span> is required to be accessible.
</p>


<h3>Value</h3>

<table>
<tr><td><code>Indicators</code></td>
<td>

<p>A table of the calculated indicators except Duda, Pseudot2 and Beale
</p>
</td></tr>
<tr><td><code>ClusterNo</code></td>
<td>

<p>The recommended number of clusters for each calculated indicator
</p>
</td></tr>
<tr><td><code>ClsMatrix</code></td>
<td>

<p>[1:n,MinClusterNo:(MaxClusterNo)] Output of the clusterings used for the calculation
</p>
</td></tr>
<tr><td><code>HierarchicalIndicators</code></td>
<td>

<p>Either NULL or the values for the indicators Duda, Pseudot2 and Beale in case of hierarchical cluster procedures, if calculated
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Code of &quot;calinski&quot;, &quot;cindex&quot;, &quot;db&quot;, &quot;hartigan&quot;,
&quot;ratkowsky&quot;, &quot;scott&quot;, &quot;marriot&quot;, &quot;ball&quot;, &quot;trcovw&quot;, &quot;tracew&quot;, &quot;friedman&quot;,
&quot;rubin&quot;, &quot;ssi&quot; of package cclust ist adapted for the purpose of this function.
</p>
<p>Colorsequence works if <span class="pkg">DataVisualizations</span> 1.1.13 is installed (currently only on github available).
</p>


<h3>Author(s)</h3>

<p>Peter Nahrgang, revised by Michael Thrun (2021)
</p>


<h3>References</h3>

<p>Charrad, Malika, et al. &quot;Package 'NbClust', J. Stat. Soft Vol.  61, pp. 1-36, 2014.
</p>
<p>Dimtriadou, E. &quot;cclust: Convex Clustering Methods and Clustering Indexes.&quot; R package version 0.6-16, URL <a href="https://CRAN.R-project.org/package=cclust">https://CRAN.R-project.org/package=cclust</a>, 2009.
</p>
<p>[Thrun/Ultsch, 2021]  Thrun, M. C., and Ultsch, A.: Swarm Intelligence for Self-Organized Clustering, Artificial Intelligence, Vol. 290, pp. 103237, <a href="https://doi.org/10.1016/j.artint.2020.103237">doi:10.1016/j.artint.2020.103237</a>, 2021.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Reading the iris dataset from the standard R-Package datasets
data &lt;- as.matrix(iris[,1:4])
MaxClusterNo = 7
# Creating the clusterings for the data set
#(here with method complete) for the number of clusters 2 to 8
hc &lt;- hclust(dist(data), method = "complete")
clsm &lt;- matrix(data = 0, nrow = dim(data)[1],

ncol = MaxClusterNo)
for (i in 2:(MaxClusterNo+1)) {
  clsm[,i-1] &lt;- cutree(hc,i)
}

# Calculation of all indicators and recommendations for the number of clusters
indicatorsList=ClusterNoEstimation(Data = data, 

ClsMatrix = clsm, MaxClusterNo = MaxClusterNo)

# Alternatively, the same calculation as above can be executed with the following call
ClusterNoEstimation(Data = data, MaxClusterNo = 7, Method = "CompleteL")
# In this variant, the function clusterumbers also takes over the clustering
</code></pre>

<hr>
<h2 id='ClusterNormalize'>
Cluster Normalize
</h2><span id='topic+ClusterNormalize'></span>

<h3>Description</h3>

<p>Values in Cls are consistently recoded to positive consecutive integers
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ClusterNormalize(Cls)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ClusterNormalize_+3A_cls">Cls</code></td>
<td>
<p>[1:n numerical vector of numbers defining the classification as the main output of the clustering algorithm for the n cases of data. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For recoding depending on cluster size please see <code><a href="#topic+ClusterRenameDescendingSize">ClusterRenameDescendingSize</a></code>.
</p>


<h3>Value</h3>

<p>The renamed classification. A vector of clusters recoded to positive consecutive integers.
</p>


<h3>Author(s)</h3>

<p>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ClusterRenameDescendingSize">ClusterRenameDescendingSize</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Lsun3D')
Cls=Lsun3D$Cls
#not desceending cluster numbers
Cls[Cls==1]=543
Cls[Cls==4]=1

# Now ordered consecutively
ClusterNormalize(Cls)
</code></pre>

<hr>
<h2 id='ClusterPlotMDS'>
Plot Clustering using Dimensionality Reduction by MDS
</h2><span id='topic+ClusterPlotMDS'></span>

<h3>Description</h3>

<p>This function uses a projection method to perform dimensionality reduction (DR) on order to visualize the data as 3D data points colored by a clustering.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ClusterPlotMDS(DataOrDistances, Cls, main = "Clustering",

DistanceMethod = "euclidean", OutputDimension = 3,

PointSize=1,Plotter3D="rgl",Colorsequence, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ClusterPlotMDS_+3A_dataordistances">DataOrDistances</code></td>
<td>

<p>Either nonsymmetric [1:n,1:d] datamatrix of n cases and d features or symmetric [1:n,1:n] distance matrix
</p>
</td></tr>
<tr><td><code id="ClusterPlotMDS_+3A_cls">Cls</code></td>
<td>

<p>1:n numerical vector of numbers defining the classification as the main output of the clustering algorithm for the n cases of data. It has k unique numbers representing the arbitrary labels of the clustering.
</p>
</td></tr>
<tr><td><code id="ClusterPlotMDS_+3A_main">main</code></td>
<td>

<p>String, title of plot
</p>
</td></tr>
<tr><td><code id="ClusterPlotMDS_+3A_distancemethod">DistanceMethod</code></td>
<td>

<p>Method to compute distances, default &quot;euclidean&quot;
</p>
</td></tr>
<tr><td><code id="ClusterPlotMDS_+3A_outputdimension">OutputDimension</code></td>
<td>

<p>Either two or three depending on user choice
</p>
</td></tr>
<tr><td><code id="ClusterPlotMDS_+3A_pointsize">PointSize</code></td>
<td>

<p>Scalar defining the size of points
</p>
</td></tr>
<tr><td><code id="ClusterPlotMDS_+3A_plotter3d">Plotter3D</code></td>
<td>

<p>In case of 3 dimensions, choose either &quot;plotly&quot; or &quot;rgl&quot;,
</p>
</td></tr>
<tr><td><code id="ClusterPlotMDS_+3A_colorsequence">Colorsequence</code></td>
<td>

<p>[1:k] character vector of colors, per default the colorsquence defined in the <span class="pkg">DataVisualizations</span> is used
</p>
</td></tr>
<tr><td><code id="ClusterPlotMDS_+3A_...">...</code></td>
<td>

<p>Please see <code><a href="DataVisualizations.html#topic+Plot3D">Plot3D</a></code> in <span class="pkg">DataVisualizations</span>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If dataset has more than 3 dimesions, mds is performed as defined in the <span class="pkg">smacof</span> [De Leeuw/Mair, 2011].
If <span class="pkg">smacof</span> package is not installed, classical metric MDS (see Def. in [Thrun, 2018]) is performed.
In both cases, the first OutputDimension are visualized. Points are colored by the labels (Cls).
</p>
<p>In the special case that the dataset has not more than 3 dimensions, all dimensions are visualized and no DR is performed.
</p>


<h3>Value</h3>

<p>The rgl or plotly plot handler depending on <code>Plotter3D</code>
</p>


<h3>Note</h3>

<p>If <span class="pkg">DataVisualizations</span> is not installed a 2D plot using native plot function is shown.
</p>
<p>If  <span class="pkg">MASS</span> is not installed, classicial metric MDS is used, see [Thrun, 2018] for definition.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[De Leeuw/Mair, 2011]  De Leeuw, J., &amp; Mair, P.: Multidimensional scaling using majorization: SMACOF in R, Journal of statistical Software, Vol. 31(3), pp. 1-30. 2011.
</p>
<p>[Thrun, 2018] Thrun, M. C.: Projection Based Clustering through Self-Organization and Swarm Intelligence, doctoral dissertation 2017, Springer, ISBN: 978-3-658-20539-3, Heidelberg, 2018.
</p>


<h3>See Also</h3>

<p><code><a href="DataVisualizations.html#topic+Plot3D">Plot3D</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Hepta)
ClusterPlotMDS(Hepta$Data,Hepta$Cls)

data(Leukemia)
ClusterPlotMDS(Leukemia$DistanceMatrix,Leukemia$Cls)


</code></pre>

<hr>
<h2 id='ClusterRedefine'>
Redfines Clustering
</h2><span id='topic+ClusterRedefine'></span>

<h3>Description</h3>

<p>Redfines some or all Clusters of Clustering such that the names of the numerical vectors are defined by 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ClusterRedefine(Cls, NewLabels,OldLabels)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ClusterRedefine_+3A_cls">Cls</code></td>
<td>

<p>1:n numerical vector of numbers defining the classification as the main output of the clustering algorithm for the n cases of data. It has k unique numbers representing the arbitrary labels of the clustering.
</p>
</td></tr>
<tr><td><code id="ClusterRedefine_+3A_newlabels">NewLabels</code></td>
<td>

<p>[1:p], p&lt;=k labels (identifiers) of clusters to be changed with
</p>
</td></tr>
<tr><td><code id="ClusterRedefine_+3A_oldlabels">OldLabels</code></td>
<td>

<p>Optional,  [1:p], p&lt;=k labels(identifiers) of clusters to be changed, default [1:k] unique cluster Ids of <code>Cls</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The same ordering of <code>NewLabels</code> and  <code>OldLabels</code> is assumend, i.e., the mapping is defined by OldLabels[i] -&gt; NewLabels[i] with <code>i</code> in [1:p].   <code>NewLabels</code> can also be a vector for strings, for example for plotting.
</p>


<h3>Value</h3>

<p>Cls[1:n] numerical vector named after the row names of data
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Lsun3D')
Cls=Lsun3D$Cls
Data=Lsun3D$Data#
#prior
ClsNew=unique(Cls)+10
#Redfined Clustering
NewCls=ClusterRedefine(Cls,ClsNew)

table(Cls,NewCls)

#require(DataVisualizations)
n=length(unique(Cls))
NewCls=ClusterRedefine(Cls,LETTERS[1:n])
#DataVisualizations package required
if(requireNamespace("DataVisualizations"))
  DataVisualizations::Classplot(Data[,1],Data[,2],
  Cls,Names=NewCls,Plotter="ggplot",Size =1.5)

</code></pre>

<hr>
<h2 id='ClusterRename'>
Renames Clustering
</h2><span id='topic+ClusterRename'></span>

<h3>Description</h3>

<p>Renames Clustering such that the names of the numerical vectors are the row names of DataOrDistances
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ClusterRename(Cls, DataOrDistances)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ClusterRename_+3A_cls">Cls</code></td>
<td>

<p>1:n numerical vector of numbers defining the classification as the main output of the clustering algorithm for the n cases of data. It has k unique numbers representing the arbitrary labels of the clustering.
</p>
</td></tr>
<tr><td><code id="ClusterRename_+3A_dataordistances">DataOrDistances</code></td>
<td>

<p>Either nonsymmetric [1:n,1:d] datamatrix of n cases and d features or symmetric [1:n,1:n] distance matrix
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If DataOrDistances is missing or if inconsistent length, nothing is done.
</p>


<h3>Value</h3>

<p>Cls[1:n] numerical vector named after the row names of data
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
Cls=Hepta$Cls
Data=Hepta$Data#
#prior
Cls
#Named Clustering
ClusterRename(Cls,Data)
</code></pre>

<hr>
<h2 id='ClusterRenameDescendingSize'>Cluster Rename Descending Size</h2><span id='topic+ClusterRenameDescendingSize'></span>

<h3>Description</h3>

<p>Renames the clusters of a classification in descending order.</p>


<h3>Usage</h3>

<pre><code class='language-R'>ClusterRenameDescendingSize(Cls,

ProvideClusterNames=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ClusterRenameDescendingSize_+3A_cls">Cls</code></td>
<td>
<p>[1:n numerical vector of numbers defining the classification as the main output of the clustering algorithm for the n cases of data. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code id="ClusterRenameDescendingSize_+3A_provideclusternames">ProvideClusterNames</code></td>
<td>
<p>TRUE: Provides in seperate output new and old k numbers, FALSE: simple output </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Beware: output changes in this function depending on <code>ProvideClusterNames</code> in order to be congruent to prior code in a large varierity of other packages.
</p>


<h3>Value</h3>

<p>ProvideClusterNames==FALSE:
</p>
<table>
<tr><td><code>RenamedCls</code></td>
<td>
<p>The renamed classification. A vector of clusters, were the largest cluster is C1 and so forth</p>
</td></tr>
</table>
<p>ProvideClusterNames==TRUE: List V with
</p>
<table>
<tr><td><code>RenamedCls</code></td>
<td>
<p>The renamed classification. A vector of clusters, were the largest cluster is C1 and so forth</p>
</td></tr>
<tr><td><code>ClusterName</code></td>
<td>
<p>[1:k,1:2] matrix of k new numbers and prior numbers</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun, Alfred Ultsch</p>


<h3>See Also</h3>

<p><code><a href="#topic+ClusterNormalize">ClusterNormalize</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Lsun3D')
Cls=Lsun3D$Cls
#not desceending cluster numbers
Cls[Cls==1]=543
Cls[Cls==4]=1

# Now ordered per cluster size and descending
ClusterRenameDescendingSize(Cls)
</code></pre>

<hr>
<h2 id='ClusterShannonInfo'>
Shannon Information
</h2><span id='topic+ClusterShannonInfo'></span>

<h3>Description</h3>

<p>Shannon Information [Shannon, 1948] for each column in ClsMatrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ClusterShannonInfo(ClsMatrix)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ClusterShannonInfo_+3A_clsmatrix">ClsMatrix</code></td>
<td>

<p>[1:n,1:C] matrix of C clusterings each columns is defined as: 
</p>
<p>1:n numerical vector of numbers defining the classification as the main output of the clustering algorithm for the n cases of data. It has k unique numbers representing the arbitrary labels of the clustering.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Info[1:d]  = sum(-p * log(p)/MaxInfo) for all unique cases with probability p in ClsMatrix[,c] for a column with k clusters  MaxInfo = -(1/k)*log(1/k)
</p>


<h3>Value</h3>

<table>
<tr><td><code>Info</code></td>
<td>

<p>[1:max.nc,1:C] matrix of Shannin informaton as defined in details, each column represents one <code>Cls</code> of <code>ClsMatrix</code>,each row yields the information of one cluster up the <code>ClusterNo</code> k, if k&lt;max.nc (highest number of clusters) then NaN are filled.
</p>
</td></tr>
<tr><td><code>ClusterNo</code></td>
<td>

<p>Number of Clusters k found for each <code>Cls</code>  respectively
</p>
</td></tr>
<tr><td><code>MaxInfo</code></td>
<td>

<p>max per column of <code>Info</code>
</p>
</td></tr>
<tr><td><code>MinInfo</code></td>
<td>

<p>min per column of <code>Info</code>
</p>
</td></tr>
<tr><td><code>MedianInfo</code></td>
<td>

<p>median per column of <code>Info</code>
</p>
</td></tr>
<tr><td><code>MeanInfo</code></td>
<td>

<p>mean per column of <code>Info</code>
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>reeimplemented from Alfred's Ultsch Matlab version but not verified yet.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Shannon, 1948]  Shannon, C. E.: A Mathematical Theory of Communication, Bell System Technical Journal, Vol. 27(3), pp. 379-423. doi doi:10.1002/j.1538-7305.1948.tb01338.x, 1948.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Reading the iris dataset from the standard R-Package datasets
data &lt;- as.matrix(iris[,1:4])
max.nc = 7
# Creating the clusterings for the data set
#(here with method complete) for the number of classes 2 to 8
hc &lt;- hclust(dist(data), method = "complete")
clsm &lt;- matrix(data = 0, nrow = dim(data)[1],

ncol = max.nc)
for (i in 2:(max.nc+1)) {
  clsm[,i-1] &lt;- cutree(hc,i)
}

ClusterShannonInfo(clsm)
</code></pre>

<hr>
<h2 id='ClusterUpsamplingMinority'>
Cluster Up Sampling using SMOTE for minority cluster
</h2><span id='topic+ClusterUpsamplingMinority'></span>

<h3>Description</h3>

<p>Wrapper for one specific internal function of L. Torgo who implemented there the relevant part of the SMOTE algorithm [Chawla et al., 2002].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ClusterUpsamplingMinority(Cls, Data, MinorityCluster,

Percentage = 200, knn = 5, PlotIt = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ClusterUpsamplingMinority_+3A_cls">Cls</code></td>
<td>

<p>1:n numerical vector of numbers defining the classification as the main output of the clustering algorithm for the n cases of data. It has k unique numbers representing the arbitrary labels of the clustering.
</p>
</td></tr>
<tr><td><code id="ClusterUpsamplingMinority_+3A_data">Data</code></td>
<td>

<p>[1:n,1:d] datamatrix of n cases and d features
</p>
</td></tr>
<tr><td><code id="ClusterUpsamplingMinority_+3A_minoritycluster">MinorityCluster</code></td>
<td>

<p>scalar defining the number of the cluster to be upsampeled
</p>
</td></tr>
<tr><td><code id="ClusterUpsamplingMinority_+3A_percentage">Percentage</code></td>
<td>

<p>pecentage above 100 of who many samples should be taken
</p>
</td></tr>
<tr><td><code id="ClusterUpsamplingMinority_+3A_knn">knn</code></td>
<td>

<p>k nearest neighbors of SMOTE algorithm</p>
</td></tr>
<tr><td><code id="ClusterUpsamplingMinority_+3A_plotit">PlotIt</code></td>
<td>

<p>TRUE: plots the result using   <code><a href="#topic+ClusterPlotMDS">ClusterPlotMDS</a></code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>the number of items <code>m</code> is defined by the scalar <code>Percentage</code> and the up sampling is combined with the <code>Data</code> and the <code>Cls</code> to  <code>DataExt</code> and <code>ClsExt</code> such that the sample is placed thereafter.
</p>


<h3>Value</h3>

<p>List with 
</p>
<table>
<tr><td><code>ClsExt</code></td>
<td>
<p>1:(n+m) numerical vector of numbers defining the classification as the main output of the clustering algorithm for the n cases of data. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>DataExt</code></td>
<td>
<p>[1:(n+m),1:d] datamatrix of n cases and d features</p>
</td></tr>
</table>
<p>.
</p>


<h3>Author(s)</h3>

<p>L. Torgo
</p>


<h3>References</h3>

<p>[Chawla et al., 2002]  Chawla, N. V., Bowyer, K. W., Hall, L. O., &amp; Kegelmeyer, W. P.: SMOTE: synthetic minority over-sampling technique, Journal of artificial intelligence research, Vol. 16, pp. 321-357. 2002.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Lsun3D)
Data=Lsun3D$Data
Cls=Lsun3D$Cls
table(Cls)

V=ClusterUpsamplingMinority(Cls,Data,4,1000)
table(V$ClsExt)
</code></pre>

<hr>
<h2 id='CrossEntropyClustering'>Cross-Entropy Clustering</h2><span id='topic+CrossEntropyClustering'></span>

<h3>Description</h3>

<p>Cross-entropy clustering published by [Tabor/Spurek, 2014] and implemented by [Spurek et al., 2017].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CrossEntropyClustering(Data, ClusterNo,PlotIt=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CrossEntropyClustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.</p>
</td></tr>
<tr><td><code id="CrossEntropyClustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be built by the algorithm.</p>
</td></tr>
<tr><td><code id="CrossEntropyClustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, If TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="CrossEntropyClustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Contrary to most of the other implemented algorithms in this package, the results on the easiest clustering challenge of Hepta are unstable for cross-entropy clustering
in the sense that the clustering is not always correct. Reproducibilty experiments should be performed (see [Tabor/Spurek, 2014]).
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun</p>


<h3>References</h3>

<p>[Spurek et al., 2017]  Spurek, P., Kamieniecki, K., Tabor, J., Misztal, K., &amp; mieja, M.: R package cec, Neurocomputing, Vol. 237, pp. 410-413. 2017.
</p>
<p>[Tabor/Spurek, 2014]  Tabor, J., &amp; Spurek, P.: Cross-entropy clustering, Pattern Recognition, Vol. 47(9), pp. 3046-3059. 2014.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
out=CrossEntropyClustering(Hepta$Data,ClusterNo=7,PlotIt=FALSE)
</code></pre>

<hr>
<h2 id='DBSCAN'>
DBSCAN
</h2><span id='topic+DBSCAN'></span><span id='topic+DBscan'></span>

<h3>Description</h3>

<p>Density-Based Spatial Clustering of Applications with Noise of [Ester et al., 1996].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DBSCAN(Data,Radius,minPts,Rcpp=TRUE,

PlotIt=FALSE,UpperLimitRadius,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DBSCAN_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.</p>
</td></tr>
<tr><td><code id="DBSCAN_+3A_radius">Radius</code></td>
<td>

<p>Eps [Ester et al., 1996, p. 227] neighborhood in the R-ball graph/unit disk graph), size of the epsilon neighborhood.
If NULL, automatic estimation is performed using insights of [Ultsch, 2005].
</p>
</td></tr>
<tr><td><code id="DBSCAN_+3A_minpts">minPts</code></td>
<td>

<p>Number of minimum points in the eps region (for core points). 
In principle minimum number of points in the unit disk, if the unit disk is within the cluster (core) [Ester et al., 1996, p. 228].
If NULL, 2.5 percent of points is selected.
</p>
</td></tr>
<tr><td><code id="DBSCAN_+3A_rcpp">Rcpp</code></td>
<td>

<p>If TRUE: fast Rcpp implementation of mlpack is used. FALSE uses dbscan package.
</p>
</td></tr>
<tr><td><code id="DBSCAN_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, If TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="DBSCAN_+3A_upperlimitradius">UpperLimitRadius</code></td>
<td>
<p>Limit for radius search, experimental</p>
</td></tr>
<tr><td><code id="DBSCAN_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector defining the clustering; this classification is the main output of the algorithm. Points which cannot be assigned to a cluster will be reported as members of the noise cluster with 0.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Ester et al., 1996]  Ester, M., Kriegel, H.-P., Sander, J., &amp; Xu, X.: A density-based algorithm for discovering clusters in large spatial databases with noise, Proc. Kdd, Vol. 96, pp. 226-231, 1996.
</p>
<p>[Ultsch, 2005]  Ultsch, A.: Pareto density estimation: A density estimation for knowledge discovery, In Baier, D. &amp; Werrnecke, K. D. (Eds.), Innovations in classification, data science, and information systems, (Vol. 27, pp. 91-100), Berlin, Germany, Springer, 2005.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')

out=DBSCAN(Hepta$Data,Radius=NULL,minPts=NULL,PlotIt=FALSE)

## Not run: 
#search for right parameter setting by grid search
data("WingNut")
Data = WingNut$Data
DBSGrid &lt;- expand.grid(
  Radius = seq(from = 0.01, to = 0.3, by = 0.02),
  minPTs = seq(from = 1, to = 50, by = 2)
)
BestAcc = c()
for (i in seq_len(nrow(DBSGrid))) {
  parameters &lt;- DBSGrid[i,]
  Cls9 = DBSCAN(
    Data,
    minPts = parameters$minPTs,
    
    Radius = parameters$Radius,
    PlotIt = F,
    
    UpperLimitRadius = parameters$Radius
  )$Cls
  if (length(unique(Cls9)) &lt; 5)
    BestAcc[i] = ClusterAccuracy(WingNut$Cls,
                                    
                                    Cls9) * 100
  else
    BestAcc[i] = 50
}
max(BestAcc)
which.max(BestAcc)
parameters &lt;- DBSGrid[13,]

Cls9 = DBSCAN(
  Data,
  minPts = parameters$minPTs,
  Radius = parameters$Radius,
  UpperLimitRadius = parameters$Radius, 
  PlotIt = TRUE
)$Cls

## End(Not run)
</code></pre>

<hr>
<h2 id='DBSclusteringAndVisualization'>
Databionic Swarm (DBS) Clustering and Visualization
</h2><span id='topic+DatabionicSwarmClustering'></span><span id='topic+DBSclusteringAndVisualization'></span>

<h3>Description</h3>

<p>Swarm-based clustering by exploting self-organization, emergence, swarm intelligence and game theory published in [Thrun/Ultsch, 2021].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DatabionicSwarmClustering(DataOrDistances, ClusterNo = 0,

StructureType = TRUE, DistancesMethod = NULL,

PlotTree = FALSE, PlotMap = FALSE,PlotIt=FALSE, 

Parallel = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DBSclusteringAndVisualization_+3A_dataordistances">DataOrDistances</code></td>
<td>

<p>Either nonsymmetric [1:n,1:d] numerical matrix of a dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.
</p>
<p>or
</p>
<p>symmetric [1:n,1:n] distance matrix, e.g. <code>as.matrix(dist(Data,method))</code>
</p>
</td></tr>
<tr><td><code id="DBSclusteringAndVisualization_+3A_clusterno">ClusterNo</code></td>
<td>

<p>Number of Clusters, if zero a the topographic map is ploted. Number of valleys equals number of clusters.
</p>
</td></tr>
<tr><td><code id="DBSclusteringAndVisualization_+3A_structuretype">StructureType</code></td>
<td>

<p>Either TRUE or FALSE, has to be tested against the visualization. If colored points of clusters a divided by mountain ranges, parameter is incorrect.
</p>
</td></tr>
<tr><td><code id="DBSclusteringAndVisualization_+3A_distancesmethod">DistancesMethod</code></td>
<td>

<p>Optional, if data matrix given, annon Euclidean distance can be selected
</p>
</td></tr>
<tr><td><code id="DBSclusteringAndVisualization_+3A_plottree">PlotTree</code></td>
<td>

<p>Optional, if TRUE: dendrogram is plotted.
</p>
</td></tr>
<tr><td><code id="DBSclusteringAndVisualization_+3A_plotmap">PlotMap</code></td>
<td>

<p>Optional, if TRUE: topographic map is plotted if <span class="pkg">GeneralizedUmatrix</span> is installed. See details.
</p>
</td></tr>
<tr><td><code id="DBSclusteringAndVisualization_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, If TRUE and dataset of [1:n,1:d] dimensions then a plot of the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code> will be generated.</p>
</td></tr>
<tr><td><code id="DBSclusteringAndVisualization_+3A_parallel">Parallel</code></td>
<td>
<p>FALSE: default implementatiomn, TRUE faster Cpp parallel implementation, for this the subsequent packages have
to be installed from github, as they are not available on CRAN yet.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function does not enable the user first to project the data and then to test the Boolean parameter defining the type of structure contrary to the <span class="pkg">DatabionicSwarm</span> which is an inappropriate approach in case of exploratory data analysis. 
</p>
<p>Instead, this function is implemented for the purpose of automatic benchmarking because in such a case nobody will investigate many trials with one visualization per trial.
</p>
<p>If one would like to perform a clustering exploratively (in the sense that a prior clustering is not given for evaluation purposes), then please use the <span class="pkg">DatabionicSwarm</span> package directly and read the vignette there.
Databionic swarm is like k-means a stochastic algorithm meaning that the clustering and visualization may change between trials.
</p>
<p>If <code>PlotMap==TRUE</code> and <code>ClusterNo=0</code> a topview of the topographic map is shown, in which the points are not labeled, i.e. colored by the same color. If <code>PlotMap==TRUE</code> and <code>ClusterNo&gt;0</code>, then the points are colored by their cluster labels. If you would like to look an 3D topogrpahic map that can be interactively rotated or use 3D printing of the high-dimensional structures [Thrun et al., 2016], please see <code><a href="GeneralizedUmatrix.html#topic+plotTopographicMap">plotTopographicMap</a></code> for further details.
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>1:n numerical vector of numbers defining the classification as the main output of the clustering algorithm for the n cases of data. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>List of further output of DBS</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Current implementation is not efficient enough to cluster more than N=4000 cases as in that case it takes longer than a day for a result.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Thrun/Ultsch, 2021]  Thrun, M. C., and Ultsch, A.: Swarm Intelligence for Self-Organized Clustering, Artificial Intelligence, Vol. 290, pp. 103237, <a href="https://doi.org/10.1016/j.artint.2020.103237">doi:10.1016/j.artint.2020.103237</a>, 2021.
</p>
<p>[Thrun/Ultsch, 2021] Thrun, M. C., &amp; Ultsch, A.: Swarm Intelligence for Self-Organized Clustering (Extended Abstract), in Bessiere, C. (Ed.), 29th International Joint Conference on Artificial Intelligence (IJCAI), Vol. IJCAI-20, pp. 5125&ndash;5129, <a href="https://doi.org/10.24963/ijcai.2020/720">doi:10.24963/ijcai.2020/720</a>, Yokohama, Japan, Jan., 2021. 
</p>
<p>[Thrun et al., 2016] Thrun, M. C., Lerch, F., Ltsch, J., &amp; Ultsch, A. : Visualization and 3D Printing of Multivariate Data of Biomarkers, in Skala, V. (Ed.), International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision (WSCG), Vol. 24, Plzen, 2016. 
</p>


<h3>See Also</h3>

<p><code><a href="DatabionicSwarm.html#topic+Pswarm">Pswarm</a></code>, <code><a href="DatabionicSwarm.html#topic+DBSclustering">DBSclustering</a></code>,<code><a href="DatabionicSwarm.html#topic+GeneratePswarmVisualization">GeneratePswarmVisualization</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Generate random but small non-structured data set
data = cbind(
  sample(1:100, 300, replace = TRUE),
  sample(1:100, 300, replace = TRUE),
  sample(1:100, 300, replace = TRUE)
)
# Make sure there are no structures
# (sample size is small and still could generate structures randomly)
if(requireNamespace('DataVisualizations',quietly = TRUE)){
Data = DataVisualizations::RobustNormalization(data, Centered = TRUE)
#DataVisualizations::Plot3D(Data)

# No structres are visible
# Topographic map looks like "egg carton"
# with every point in its own valley
ClsV = DatabionicSwarmClustering(Data, 0, PlotMap = TRUE)
}else{
# only for testing purposes of CRAN!
# in case CRAN tests with no suggest packages available
# please use alpways some kind of standardization!
ClsV = DatabionicSwarmClustering(data, 0, PlotMap = TRUE)
}


# Distance based cluster structures
# 7 valleys are visible, thus ClusterNo=7

data(Hepta)
#DataVisualizations::Plot3D(Hepta$Data)

ClsV = DatabionicSwarmClustering(Hepta$Data, 0, PlotMap = TRUE)


#entagled, complex, and non-linear seperable structures 
## Not run: 
#takes too long for CRAN tests
data(Chainlink)
#DataVisualizations::Plot3D(Chainlink$Data)

# 2 valleys are visible, thus ClusterNo=2
ClsV = DatabionicSwarmClustering(Chainlink$Data, 0, PlotMap = TRUE)

# Experiment with parameter StructureType only
# reveals that clustering is appropriate
# if StructureType=FALSE
ClsV2 = DatabionicSwarmClustering(Chainlink$Data,
                                2,
                                StructureType = FALSE,
                                PlotMap = TRUE)

# Here clusters (colored points)
# are not seperated by valleys
ClsV = DatabionicSwarmClustering(Chainlink$Data,
                                2,
                                StructureType = TRUE,
                                PlotMap = TRUE)

## End(Not run)

</code></pre>

<hr>
<h2 id='DensityPeakClustering'>
Density Peak Clustering algorithm using the Decision Graph
</h2><span id='topic+DensityPeakClustering'></span>

<h3>Description</h3>

<p>Density peaks clustering of [Rodriguez/Laio, 2014] is here implemented by [Pedersen et al., 2017] with estimation of [Wang et al, 2015] meaning its non adaptive in the sense of <code><a href="#topic+ADPclustering">ADPclustering</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DensityPeakClustering(DataOrDistances, Rho,Delta,Dc,Knn=7, 

DistanceMethod = "euclidean", PlotIt = FALSE, Data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DensityPeakClustering_+3A_dataordistances">DataOrDistances</code></td>
<td>

<p>Either [1:n,1:n] symmetric distance matrix or [1:n,1:d] non symmetric data matrix of n cases and d variables
</p>
</td></tr>
<tr><td><code id="DensityPeakClustering_+3A_rho">Rho</code></td>
<td>

<p>Local density of a point, see [Rodriguez/Laio, 2014] for explanation
</p>
</td></tr>
<tr><td><code id="DensityPeakClustering_+3A_delta">Delta</code></td>
<td>

<p>Minimum distance between a point and any other point, see [Rodriguez/Laio, 2014] for explanation
</p>
</td></tr>
<tr><td><code id="DensityPeakClustering_+3A_dc">Dc</code></td>
<td>

<p>Optional, cutoff distance, will either be estimated by [Pedersen et al., 2017] or  [Wang et al, 2015] (see example below)
</p>
</td></tr>
<tr><td><code id="DensityPeakClustering_+3A_knn">Knn</code></td>
<td>

<p>Optional k nearest neighbors
</p>
</td></tr>
<tr><td><code id="DensityPeakClustering_+3A_distancemethod">DistanceMethod</code></td>
<td>

<p>Optional distance method of data, default is euclid, see <code><a href="parallelDist.html#topic+parDist">parDist</a></code> for details
</p>
</td></tr>
<tr><td><code id="DensityPeakClustering_+3A_plotit">PlotIt</code></td>
<td>

<p>Optional TRUE: Plots 2d or 3d result with clustering
</p>
</td></tr>
<tr><td><code id="DensityPeakClustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] data matrix in the case that <code>DataOrDistances</code> is missing and partial matching does not work.</p>
</td></tr>
<tr><td><code id="DensityPeakClustering_+3A_...">...</code></td>
<td>

<p>Optional, further arguments for  <code><a href="densityClust.html#topic+densityClust">densityClust</a></code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The densityClust algorithm does not decide the k number of clusters, this has to be done by the parameter setting. This is contrary to the other version of the algorithm from another package which can be called with <code><a href="#topic+ADPclustering">ADPclustering</a></code>.
</p>
<p>The plot shows the density peaks (Cluster centers). Set Rho and Delta as boundaries below the number of relevant cluster centers for your problem. (see example below).
</p>


<h3>Value</h3>

<p>If Rho and Delta are set:
</p>
<p>list of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n numerical vector of numbers defining the classification as the main output of the clustering algorithm for the n cases of data. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p> output of [Pedersen et al., 2017] algorithm</p>
</td></tr>
</table>
<p>If Rho and Delta are missing:
</p>
<table>
<tr><td><code>p</code></td>
<td>
<p>object of <code><a href="plotly.html#topic+plot_ly">plot_ly</a></code> for the decision graph is returned</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Wang et al., 2015] Wang, S., Wang, D., Li, C., &amp; Li, Y.: Comment on&quot; Clustering by fast search and find of density peaks&quot;, arXiv preprint arXiv:1501.04267, 2015.
</p>
<p>[Pedersen et al., 2017]  Thomas Lin Pedersen, Sean Hughes and Xiaojie Qiu: densityClust: Clustering by Fast Search and Find of Density Peaks. R package version 0.3. https://CRAN.R-project.org/package=densityClust, 2017.
</p>
<p>[Rodriguez/Laio, 2014]  Rodriguez, A., &amp; Laio, A.: Clustering by fast search and find of density peaks, Science, Vol. 344(6191), pp. 1492-1496. 2014.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ADPclustering">ADPclustering</a></code>
</p>
<p><code><a href="densityClust.html#topic+densityClust">densityClust</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Hepta)
H=EntropyOfDataField(Hepta$Data,  seq(from=0,to=1.5,by=0.05),PlotIt=FALSE)
Sigmamin=names(H)[which.min(H)]
Dc=3/sqrt(2)*as.numeric(names(H)[which.min(H)])
# Look at the plot and estimate rho and delta

DensityPeakClustering(Hepta$Data, Knn = 7,Dc=Dc)
Cls=DensityPeakClustering(Hepta$Data,Dc=Dc,Rho = 0.028,

Delta = 22,Knn = 7,PlotIt = TRUE)$Cls

</code></pre>

<hr>
<h2 id='DivisiveAnalysisClustering'>Large DivisiveAnalysisClustering Clustering</h2><span id='topic+DivisiveAnalysisClustering'></span>

<h3>Description</h3>

<p>Divisive Analysis Clustering (diana) of  [Rousseeuw/Kaufman, 1990, pp. 253-279]
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DivisiveAnalysisClustering(DataOrDistances, ClusterNo,

PlotIt=FALSE,Standardization=TRUE,PlotTree=FALSE,Data,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DivisiveAnalysisClustering_+3A_dataordistances">DataOrDistances</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features. Alternatively, symmetric [1:n,1:n] distance matrix</p>
</td></tr>
<tr><td><code id="DivisiveAnalysisClustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be build by the algorithm.
if <code>ClusterNo=0</code> and <code>PlotTree=TRUE</code>, the dendrogram is generated instead of a clustering to estimate the numbers of clusters.
</p>
</td></tr>
<tr><td><code id="DivisiveAnalysisClustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, If TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="DivisiveAnalysisClustering_+3A_standardization">Standardization</code></td>
<td>

<p><code>DataOrDistances</code> Is standardized before calculating the dissimilarities. Measurements are standardized for each variable (column), by subtracting the variable's mean value and dividing by the variable's mean absolute deviation.If <code>DataOrDistances</code> Is already a distance matrix, then this argument will be ignored.
</p>
</td></tr>
<tr><td><code id="DivisiveAnalysisClustering_+3A_plottree">PlotTree</code></td>
<td>

<p>TRUE: Plots the dendrogram, FALSE: no plot
</p>
</td></tr>
<tr><td><code id="DivisiveAnalysisClustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] data matrix in the case that <code>DataOrDistances</code> is missing and partial matching does not work.</p>
</td></tr>
<tr><td><code id="DivisiveAnalysisClustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>Dendrogram</code></td>
<td>
<p>Dendrogram of hierarchical clustering algorithm</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun</p>


<h3>References</h3>

<p>[Rousseeuw/Kaufman, 1990]	Rousseeuw, P. J., &amp; Kaufman, L.: Finding groups in data, Belgium, John Wiley &amp; Sons Inc., ISBN: 0471735787, doi: 10.1002/9780470316801, Online ISBN: 9780470316801, 1990.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
CA=DivisiveAnalysisClustering(Hepta$Data,ClusterNo=7,PlotIt=FALSE)

print(CA$Object)
plot(CA$Object)
ClusterDendrogram(CA$Dendrogram,7,main='DIANA')

</code></pre>

<hr>
<h2 id='EngyTime'>
EngyTime introduced in [Baggenstoss, 2002].
</h2><span id='topic+EngyTime'></span>

<h3>Description</h3>

<p>Gaussian mixture. Detailed description of dataset and its clustering challenge is provided in [Thrun/Ultsch, 2020].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("EngyTime")</code></pre>


<h3>Details</h3>

<p>Size 4096, Dimensions 2, stored in <code>EngyTime$Data</code>
</p>
<p>Classes 2, stored in <code>EngyTime$Cls</code>
</p>


<h3>References</h3>

<p>[Baggenstoss, 2002]  Baggenstoss, P. M.: Statistical modeling using gaussian mixtures and hmms with matlab, Naval Undersea Warfare Center, Newport RI, 2002.
</p>
<p>[Thrun/Ultsch, 2020]  Thrun, M. C., &amp; Ultsch, A.: Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems, Data in Brief, Vol. 30(C), pp. 105501, <a href="https://doi.org/10.1016/j.dib.2020.105501">doi:10.1016/j.dib.2020.105501</a>, 2020.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(EngyTime)
str(EngyTime)
</code></pre>

<hr>
<h2 id='EntropyOfDataField'>
Entropy Of a Data Field [Wang et al., 2011].
</h2><span id='topic+EntropyOfDataField'></span>

<h3>Description</h3>

<p>Calculates the Potential Entropy Of a Data Field for a given ranges of impact factors sigma
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EntropyOfDataField(Data, 

sigmarange = c(0.01, 0.1, 0.5, 1, 2, 5, 8, 10, 100)

, PlotIt = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EntropyOfDataField_+3A_data">Data</code></td>
<td>

<p>[1:n,1:d] data matrix
</p>
</td></tr>
<tr><td><code id="EntropyOfDataField_+3A_sigmarange">sigmarange</code></td>
<td>

<p>Numeric vector [1:s] of relevant sigmas
</p>
</td></tr>
<tr><td><code id="EntropyOfDataField_+3A_plotit">PlotIt</code></td>
<td>

<p>FALSE: disable plot, TRUE: Plot with upper boundary of H after [Wang et al., 2011].
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In theory there should be a curve with a clear minimum of Entropy [Wang et al.,2011]. Then the choice for the impact factor sigma is the minimum of the entropy to define the correct data field. It follows, that the influence radius is 3/sqrt(2)*sigma (3B rule of gaussian distribution) for clustering algorithms like density peak clustering [Wang et al.,2011].
</p>


<h3>Value</h3>

<p>[1:s] named vector of the Entropy of data field. The names are the impact factor sigma.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Wang et al., 2015] Wang, S., Wang, D., Li, C., &amp; Li, Y.: Comment on&quot; Clustering by fast search and find of density peaks&quot;, arXiv preprint arXiv:1501.04267, 2015.
</p>
<p>[Wang et al., 2011]  Wang, S., Gan, W., Li, D., &amp; Li, D.: Data field for hierarchical clustering, International Journal of Data Warehousing and Mining (IJDWM), Vol. 7(4), pp. 43-63. 2011.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Hepta)
H=EntropyOfDataField(Hepta$Data,PlotIt=FALSE)
Sigmamin=names(H)[which.min(H)]
Dc=3/sqrt(2)*as.numeric(names(H)[which.min(H)])
</code></pre>

<hr>
<h2 id='EstimateRadiusByDistance'>
Estimate Radius By Distance
</h2><span id='topic+EstimateRadiusByDistance'></span>

<h3>Description</h3>

<p>Published in [Thrun et al, 2016] for the case of automatically estimating the radius of the P-matrix. Can also be used to estimate the radius parameter for distance based clustering algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>EstimateRadiusByDistance(DistanceMatrix)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EstimateRadiusByDistance_+3A_distancematrix">DistanceMatrix</code></td>
<td>

<p>[1:n,1:n]  symmetric distance Matrix of n cases
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For density-based clustering algorithms like <code><a href="#topic+DBSCAN">DBSCAN</a></code> it is not always usefull.
</p>


<h3>Value</h3>

<p>Numerical scalar defining the radius
</p>


<h3>Note</h3>

<p>Symmetric matrix is assumed.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Thrun et al., 2016]  Thrun, M. C., Lerch, F., Loetsch, J., &amp; Ultsch, A.: Visualization and 3D Printing of Multivariate Data of Biomarkers, in Skala, V. (Ed.), International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision (WSCG), Vol. 24, pp. 7-16, Plzen, http://wscg.zcu.cz/wscg2016/short/A43-full.pdf, 2016.
</p>


<h3>See Also</h3>

<p><code><a href="GeneralizedUmatrix.html#topic+GeneratePmatrix">GeneratePmatrix</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
DistanceMatrix=as.matrix(dist(Hepta$Data))
Radius=EstimateRadiusByDistance(DistanceMatrix)
</code></pre>

<hr>
<h2 id='FannyClustering'>
Fuzzy Analysis Clustering [Rousseeuw/Kaufman, 1990, p. 253-279] 
</h2><span id='topic+FannyClustering'></span>

<h3>Description</h3>

<p>...
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FannyClustering(DataOrDistances,ClusterNo,

PlotIt=FALSE,Standardization=TRUE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FannyClustering_+3A_dataordistances">DataOrDistances</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases or d-dimensional data points. Every case has d attributes, variables or features. Alternatively, symmetric [1:n,1:n] distance matrix</p>
</td></tr>
<tr><td><code id="FannyClustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be build by the algorithm.</p>
</td></tr>
<tr><td><code id="FannyClustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, If TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="FannyClustering_+3A_standardization">Standardization</code></td>
<td>

<p><code>DataOrDistances</code> is standardized before calculating the dissimilarities. Measurements are standardized for each variable (column), by subtracting the variable's mean value and dividing by the variable's mean absolute deviation. If <code>DataOrDistances</code> is already a distance matrix, then this argument will be ignored.
</p>
</td></tr>
<tr><td><code id="FannyClustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>...
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.
Points which cannot be assigned to a cluster will be reported with 0.
</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the second output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Rousseeuw/Kaufman, 1990]	Rousseeuw, P. J., &amp; Kaufman, L.: Finding groups in data, Belgium, John Wiley &amp; Sons Inc., ISBN: 0471735787, doi: 10.1002/9780470316801, Online ISBN: 9780470316801, 1990.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
out=FannyClustering(Hepta$Data,ClusterNo=7,PlotIt=FALSE)
</code></pre>

<hr>
<h2 id='GapStatistic'>
Gap Statistic
</h2><span id='topic+GapStatistic'></span>

<h3>Description</h3>

<p>Gap Statistic
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GapStatistic(Data, ClusterNoMax, ClusterFun, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GapStatistic_+3A_data">Data</code></td>
<td>

<p>[1:n,1:d] data matrix
</p>
</td></tr>
<tr><td><code id="GapStatistic_+3A_clusternomax">ClusterNoMax</code></td>
<td>

<p>max no of clusters to beinvestigated
</p>
</td></tr>
<tr><td><code id="GapStatistic_+3A_clusterfun">ClusterFun</code></td>
<td>

<p>which clustering algorithm to investigate
</p>
</td></tr>
<tr><td><code id="GapStatistic_+3A_...">...</code></td>
<td>

<p>further arguments passed on
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>does not work on hepta, see example
</p>


<h3>Value</h3>

<p>tobedocumented
</p>


<h3>Note</h3>

<p>Wrapper only
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>Tibshirani, R., Walther, G. and Hastie, T: Estimating the number of data clusters via the Gap statistic, Journal of the Royal Statistical Society B, Vol. 63, pp. 411-423, 2003.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Hepta)
#GapStatistic(Hepta$Data,10,ClusterFun = kmeans)
</code></pre>

<hr>
<h2 id='GenieClustering'>
Genie Clustering by Gini Index
</h2><span id='topic+GenieClustering'></span>

<h3>Description</h3>

<p>Outlier Resistant Hierarchical Clustering Algorithm of [Gagolewski/Bartoszuk, 2016].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GenieClustering(DataOrDistances, ClusterNo = 0,
DistanceMethod="euclidean", ColorTreshold = 0,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GenieClustering_+3A_dataordistances">DataOrDistances</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features. Alternatively, symmetric [1:n,1:n] distance matrix</p>
</td></tr>
<tr><td><code id="GenieClustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be build by the algorithm.</p>
</td></tr>
<tr><td><code id="GenieClustering_+3A_distancemethod">DistanceMethod</code></td>
<td>
<p>See  <code><a href="parallelDist.html#topic+parDist">parDist</a></code>, for example 'euclidean','mahalanobis','manhatten' (cityblock),'fJaccard','binary', 'canberra', 'maximum'. Any unambiguous substring can be given.</p>
</td></tr>
<tr><td><code id="GenieClustering_+3A_colortreshold">ColorTreshold</code></td>
<td>
<p>Draws cutline w.r.t. dendogram y-axis (height), height of line as scalar should be given</p>
</td></tr>
<tr><td><code id="GenieClustering_+3A_...">...</code></td>
<td>
<p>furter argument to genie like:
</p>
<p><code>thresholdGini</code>  Single numeric value in [0,1], threshold for the Gini index, 1 gives the standard single linkage algorithm
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Wrapper for Genie algorithm.
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>If, ClusterNo&gt;0: [1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering. Otherwise for ClusterNo=0: NULL</p>
</td></tr>
<tr><td><code>Dendrogram</code></td>
<td>
<p>Dendrogram of hierarchical clustering algorithm</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Ultrametric tree of hierarchical clustering algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Gagolewski/Bartoszuk, 2016]  Gagolewski M., Bartoszuk M., Cena A., Genie: A new, fast, and outlier-resistant hierarchical clustering
algorithm, Information Sciences, Vol. 363, pp. 8-23, 2016.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+HierarchicalClustering">HierarchicalClustering</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
Clust=GenieClustering(Hepta$Data,ClusterNo=7)
</code></pre>

<hr>
<h2 id='GolfBall'>
GolfBall introduced in [Ultsch, 2005]
</h2><span id='topic+GolfBall'></span>

<h3>Description</h3>

<p>No clusters at all. Detailed description of dataset and its clustering challenge is provided in [Thrun/Ultsch, 2020].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("GolfBall")</code></pre>


<h3>Details</h3>

<p>Size 4002, Dimensions 3, stored in <code>GolfBall$Data</code>
</p>
<p>Classes 1, stored in <code>GolfBall$Cls</code>
</p>


<h3>References</h3>

<p>[Ultsch, 2005]  Ultsch, A.: Clustering wih SOM: U* C, Proc. Proceedings of the 5th Workshop on Self-Organizing Maps, Vol. 2, pp. 75-82, 2005.
</p>
<p>[Thrun/Ultsch, 2020]  Thrun, M. C., &amp; Ultsch, A.: Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems, Data in Brief, Vol. 30(C), pp. 105501, <a href="https://doi.org/10.1016/j.dib.2020.105501">doi:10.1016/j.dib.2020.105501</a>, 2020.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(GolfBall)
str(GolfBall)
</code></pre>

<hr>
<h2 id='HCLclustering'>On-line Update (Hard Competitive learning) method</h2><span id='topic+HCLclustering'></span>

<h3>Description</h3>

<p>Hard Competitive learning clustering published by [Ripley, 2007].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HCLclustering(Data, ClusterNo,PlotIt=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="HCLclustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.</p>
</td></tr>
<tr><td><code id="HCLclustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be build by the algorithm.</p>
</td></tr>
<tr><td><code id="HCLclustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, If TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="HCLclustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun</p>


<h3>References</h3>

<p>[Dimitriadou, 2002]  Dimitriadou, E.: cclust-convex clustering methods and clustering indexes. R package, 2002,
</p>
<p>[Ripley, 2007]  Ripley, B. D.: Pattern recognition and neural networks, Cambridge university press, ISBN: 0521717701, 2007.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
out=HCLclustering(Hepta$Data,ClusterNo=7,PlotIt=FALSE)
</code></pre>

<hr>
<h2 id='HDDClustering'>
HDD clustering is a model-based clustering method of [Bouveyron et al., 2007]. 
</h2><span id='topic+HDDClustering'></span>

<h3>Description</h3>

<p>HDD clustering is based on the Gaussian Mixture Model and on the idea that the data lives in subspaces with a lower dimension than the dimension of the original space. It uses the EM algorithm to estimate the parameters of the model [Berge et al., 2012].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HDDClustering(Data, ClusterNo, PlotIt=F,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="HDDClustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases
of d-dimensional data points. Every case has d attributes, variables or
features.</p>
</td></tr>
<tr><td><code id="HDDClustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>Optional, Numeric indicating either the number of cluster or a vector of 1:k to indicate the maximal expected number of clusters.</p>
</td></tr>
<tr><td><code id="HDDClustering_+3A_plotit">PlotIt</code></td>
<td>
<p>(optional) Boolean. Default = FALSE = No plotting performed.</p>
</td></tr>
<tr><td><code id="HDDClustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used, see <code><a href="HDclassif.html#topic+hddc">hddc</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>HDD clustering maximises the BIC criterion for a range of possible number of cluster up to <code>ClusterNo</code>. Per default the most general model is used, alternetively the parameter <code>model="ALL"</code> can be used to evaluate all possible models with BIC [Berge et al., 2012]. If specific properties of <code>Data</code> are known priorly please see <code><a href="HDclassif.html#topic+hddc">hddc</a></code> for specific model selection.
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as
the main output of the clustering algorithm. It has k unique numbers
representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output
of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Quirin Stier
</p>


<h3>References</h3>

<p>[Berge et al., 2012]  L. Berge, C. Bouveyron and S. Girard, HDclassif:
an R Package for Model-Based Clustering and Discriminant Analysis of
High-Dimensional Data, Journal of Statistical Software, vol. 42 (6), pp. 1-29,
2012.
</p>
<p>[Bouveyron et al., 2007]  Bouveyron, C. Girard, S. and Schmid, C: High-Dimensional Data Clustering, Computational Statistics and Data Analysis, vol. 52 (1), pp. 502-519, 2007.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Hepta
data("Hepta")
Data = Hepta$Data
#Non-default parameter model
#can be set to evaulate all possible models
V = HDDClustering(Data=Data,ClusterNo=7,model="ALL")
Cls = V$Cls

ClusterAccuracy(Hepta$Cls, Cls)

## Not run: 
library(HDclassif)
data(Crabs)
Data = Crabs[,-1]
V = HDDClustering(Data=Data,ClusterNo=4,com_dim=1)

## End(Not run)
</code></pre>

<hr>
<h2 id='Hepta'>
Hepta introduced in [Ultsch, 2003]
</h2><span id='topic+Hepta'></span>

<h3>Description</h3>

<p>Clearly defined clusters, different variances. Detailed description of dataset and its clustering challenge is provided in [Thrun/Ultsch, 2020].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("Hepta")</code></pre>


<h3>Details</h3>

<p>Size 212, Dimensions 3, stored in <code>Hepta$Data</code>
</p>
<p>Classes 7, stored in <code>Hepta$Cls</code>
</p>


<h3>References</h3>

<p>[Ultsch, 2003]  Ultsch, A.: Maps for the visualization of high-dimensional data spaces, Proc. Workshop on Self organizing Maps (WSOM), pp. 225-230, Kyushu, Japan, 2003.
</p>
<p>[Thrun/Ultsch, 2020]  Thrun, M. C., &amp; Ultsch, A.: Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems, Data in Brief, Vol. 30(C), pp. 105501, <a href="https://doi.org/10.1016/j.dib.2020.105501">doi:10.1016/j.dib.2020.105501</a>, 2020.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Hepta)
str(Hepta)
</code></pre>

<hr>
<h2 id='HierarchicalClusterData'>
Internal function of Hierarchical Clusterering of Data
</h2><span id='topic+HierarchicalClusterData'></span><span id='topic+HierarchicalCluster'></span>

<h3>Description</h3>

<p>Please use <code><a href="#topic+HierarchicalClustering">HierarchicalClustering</a></code>. Hierarchical cluster analysis on a set of dissimilarities and methods for analyzing it. Uses stats package function 'hclust'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HierarchicalClusterData(Data,ClusterNo=0,

Type="ward.D2",DistanceMethod="euclidean",

ColorTreshold=0,Fast=FALSE,Cls=NULL,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="HierarchicalClusterData_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.</p>
</td></tr>
<tr><td><code id="HierarchicalClusterData_+3A_clusterno">ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be build by the algorithm.</p>
</td></tr>
<tr><td><code id="HierarchicalClusterData_+3A_type">Type</code></td>
<td>
<p>Methode der Clusterung: &quot;ward.D&quot;, &quot;ward.D2&quot;, &quot;single&quot;, &quot;complete&quot;, &quot;average&quot;, &quot;mcquitty&quot;, &quot;median&quot; or &quot;centroid&quot;.</p>
</td></tr>
<tr><td><code id="HierarchicalClusterData_+3A_distancemethod">DistanceMethod</code></td>
<td>
<p>see  <code><a href="parallelDist.html#topic+parDist">parDist</a></code>, for example 'euclidean','mahalanobis','manhatten' (cityblock),'fJaccard','binary', 'canberra', 'maximum'. Any unambiguous substring can be given.</p>
</td></tr>
<tr><td><code id="HierarchicalClusterData_+3A_colortreshold">ColorTreshold</code></td>
<td>
<p>Draws cutline w.r.t. dendrogram y-axis (height), height of line as scalar should be given</p>
</td></tr>
<tr><td><code id="HierarchicalClusterData_+3A_fast">Fast</code></td>
<td>
<p>If TRUE and fastcluster installed, then a faster implementation of the methods above can be used</p>
</td></tr>
<tr><td><code id="HierarchicalClusterData_+3A_cls">Cls</code></td>
<td>
<p>[1:n] classification vector for coloring of dendrogram in plot</p>
</td></tr>
<tr><td><code id="HierarchicalClusterData_+3A_...">...</code></td>
<td>
<p>In case of plotting further argument for <code>plot</code>, see <code><a href="stats.html#topic+as.dendrogram">as.dendrogram</a></code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>If, ClusterNo&gt;0: [1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering. Otherwise for ClusterNo=0: NULL</p>
</td></tr>
<tr><td><code>Dendrogram</code></td>
<td>
<p>Dendrogram of hierarchical clustering algorithm</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Ultrametric tree of hierarchical clustering algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>See Also</h3>

<p><code><a href="#topic+HierarchicalClusterData">HierarchicalClusterData</a></code>
</p>
<p><code><a href="#topic+HierarchicalClusterDists">HierarchicalClusterDists</a></code>
</p>
<p><code><a href="#topic+HierarchicalClustering">HierarchicalClustering</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
#out=HierarchicalClusterData(Hepta$Data,ClusterNo=7)
</code></pre>

<hr>
<h2 id='HierarchicalClusterDists'>
Internal Function of Hierarchical Clustering with Distances
</h2><span id='topic+HierarchicalClusterDists'></span>

<h3>Description</h3>

<p>Please use <code><a href="#topic+HierarchicalClustering">HierarchicalClustering</a></code>. Cluster analysis on a set of dissimilarities and methods for analyzing it. Uses stats package function 'hclust'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HierarchicalClusterDists(pDist,ClusterNo=0,Type="ward.D2",

ColorTreshold=0,Fast=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="HierarchicalClusterDists_+3A_pdist">pDist</code></td>
<td>
<p>Distances as either matrix [1:n,1:n] or dist object</p>
</td></tr>
<tr><td><code id="HierarchicalClusterDists_+3A_clusterno">ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be built by the algorithm.</p>
</td></tr>
<tr><td><code id="HierarchicalClusterDists_+3A_type">Type</code></td>
<td>
<p>Method of cluster analysis: &quot;ward.D&quot;, &quot;ward.D2&quot;, &quot;single&quot;, &quot;complete&quot;, &quot;average&quot;, &quot;mcquitty&quot;, &quot;median&quot; or &quot;centroid&quot;.</p>
</td></tr>
<tr><td><code id="HierarchicalClusterDists_+3A_colortreshold">ColorTreshold</code></td>
<td>
<p>Draws cutline w.r.t. dendogram y-axis (height), height of line as scalar should be given</p>
</td></tr>
<tr><td><code id="HierarchicalClusterDists_+3A_fast">Fast</code></td>
<td>
<p>If TRUE and fastcluster installed, then a faster implementation of the methods above can be used</p>
</td></tr>
<tr><td><code id="HierarchicalClusterDists_+3A_...">...</code></td>
<td>
<p>In case of plotting further argument for <code>plot</code>, see <code><a href="stats.html#topic+as.dendrogram">as.dendrogram</a></code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>If, ClusterNo&gt;0: [1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering. Otherwise for ClusterNo=0: NULL</p>
</td></tr>
<tr><td><code>Dendrogram</code></td>
<td>
<p>Dendrogram of hierarchical clustering algorithm</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Ultrametric tree of hierarchical clustering algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun</p>


<h3>See Also</h3>

<p><code><a href="#topic+HierarchicalClusterData">HierarchicalClusterData</a></code>
</p>
<p><code><a href="#topic+HierarchicalClusterDists">HierarchicalClusterDists</a></code>
</p>
<p><code><a href="#topic+HierarchicalClustering">HierarchicalClustering</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
#out=HierarchicalClusterDists(as.matrix(dist(Hepta$Data)),ClusterNo=7)
</code></pre>

<hr>
<h2 id='HierarchicalClustering'>
Hierarchical Clustering
</h2><span id='topic+HierarchicalClustering'></span>

<h3>Description</h3>

<p>Wrapper for various agglomerative hierarchical clustering algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HierarchicalClustering(DataOrDistances,ClusterNo,Type='SingleL',Fast=TRUE,Data,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="HierarchicalClustering_+3A_dataordistances">DataOrDistances</code></td>
<td>

<p>Either nonsymmetric [1:n,1:d] numerical matrix of a dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.
</p>
<p>or
</p>
<p>symmetric [1:n,1:n] distance matrix, e.g. <code>as.matrix(dist(Data,method))</code>
</p>
</td></tr>
<tr><td><code id="HierarchicalClustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be built by the algorithm.</p>
</td></tr>
<tr><td><code id="HierarchicalClustering_+3A_type">Type</code></td>
<td>
<p>Method of cluster analysis: &quot;Ward&quot;, &quot;SingleL&quot;, &quot;CompleteL&quot;, &quot;AverageL&quot; (UPGMA), &quot;WPGMA&quot; (mcquitty), &quot;MedianL&quot; (WPGMC), &quot;CentroidL&quot; (UPGMC), &quot;Minimax&quot;, &quot;MinEnergy&quot;, &quot;Gini&quot;,&quot;HDBSCAN&quot;, or &quot;Sparse&quot;</p>
</td></tr>
<tr><td><code id="HierarchicalClustering_+3A_fast">Fast</code></td>
<td>
<p>If TRUE and fastcluster installed, then a faster implementation of the methods above can be used except for &quot;Minimax&quot;, &quot;MinEnergy&quot;, &quot;Gini&quot; or &quot;HDBSCAN&quot;</p>
</td></tr>
<tr><td><code id="HierarchicalClustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] data matrix in the case that <code>DataOrDistances</code> is missing and partial matching does not work.</p>
</td></tr>
<tr><td><code id="HierarchicalClustering_+3A_...">...</code></td>
<td>

<p>Further arguments passed on to either <code><a href="#topic+HierarchicalClusterData">HierarchicalClusterData</a></code>, <code><a href="#topic+HierarchicalClusterDists">HierarchicalClusterDists</a></code>, <code><a href="#topic+MinimalEnergyClustering">MinimalEnergyClustering</a></code> or <code><a href="#topic+GenieClustering">GenieClustering</a></code> (for &quot;Gini&quot;), <code><a href="#topic+HierarchicalDBSCAN">HierarchicalDBSCAN</a></code> (for HDBSCAN) or <code><a href="#topic+SparseClustering">SparseClustering</a></code> (for Sparse).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Please see <code><a href="#topic+HierarchicalClusterData">HierarchicalClusterData</a></code> and <code><a href="#topic+HierarchicalClusterDists">HierarchicalClusterDists</a></code> or the other functions listed above.
</p>
<p>It should be noted that in case of &quot;HDBSCAN&quot; the number of clusters is manually selected by <code>cutree</code> to have the same convention as the other algorithms. Usually, &quot;HDBSCAN&quot; selects the number of clusters automatically.
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>If, ClusterNo&gt;0: [1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering. Otherwise for ClusterNo=0: NULL</p>
</td></tr>
<tr><td><code>Dendrogram</code></td>
<td>
<p>Dendrogram of hierarchical clustering algorithm</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Ultrametric tree of hierarchical clustering algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>See Also</h3>

<p><code><a href="#topic+HierarchicalClusterData">HierarchicalClusterData</a></code>
</p>
<p><code><a href="#topic+HierarchicalClusterDists">HierarchicalClusterDists</a></code>,
</p>
<p><code><a href="#topic+MinimalEnergyClustering">MinimalEnergyClustering</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
out=HierarchicalClustering(Hepta$Data,ClusterNo=7)
</code></pre>

<hr>
<h2 id='HierarchicalDBSCAN'>
Hierarchical DBSCAN
</h2><span id='topic+HierarchicalDBSCAN'></span><span id='topic+Hierarchical_DBscan'></span><span id='topic+Hierarchical_DBSCAN'></span>

<h3>Description</h3>

<p>Hierarchical DBSCAN clustering [Campello et al., 2015].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HierarchicalDBSCAN(DataOrDistances,minPts=4,

PlotTree=FALSE,PlotIt=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="HierarchicalDBSCAN_+3A_dataordistances">DataOrDistances</code></td>
<td>
<p>Either a [1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.
</p>
<p>or a [1:n,1:n] symmetric distance matrix.
</p>
</td></tr>
<tr><td><code id="HierarchicalDBSCAN_+3A_minpts">minPts</code></td>
<td>

<p>Classic smoothing factor in density estimates [Campello et al., 2015, p.9]
</p>
</td></tr>
<tr><td><code id="HierarchicalDBSCAN_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, If TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="HierarchicalDBSCAN_+3A_plottree">PlotTree</code></td>
<td>
<p>Default: FALSE, If TRUE plots the dendrogram. If minPts is missing, PlotTree is set to TRUE.</p>
</td></tr>
<tr><td><code id="HierarchicalDBSCAN_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>&quot;Computes the hierarchical cluster tree representing density estimates along with the stability-based flat cluster extraction proposed by Campello et al. (2013). HDBSCAN essentially computes the hierarchy of all DBSCAN* clusterings, and then uses a stability-based extraction method to find optimal cuts in the hierarchy, thus producing a flat solution.&quot;[Hahsler et al., 2019]
</p>
<p>It is claimed by the inventors that the minPts parameter is noncritical [Campello et al., 2015, p.35]. minPts is reported to be set to 4 on all experiments [Campello et al., 2015, p.35].
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector defining the clustering; this classification is the main output of the algorithm. Points which cannot be assigned to a cluster will be reported as members of the noise cluster with 0.</p>
</td></tr>
<tr><td><code>Dendrogram</code></td>
<td>
<p>Dendrogram of hierarchical clustering algorithm</p>
</td></tr>
<tr><td><code>Tree</code></td>
<td>
<p>Ultrametric tree of hierarchical clustering algorithm</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Campello et al., 2015]  Campello, R. J., Moulavi, D., Zimek, A., &amp; Sander, J.: Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data (TKDD), Vol. 10(1), pp. 1-51. 2015.
</p>
<p>[Hahsler et al., 2019]   Hahsler M, Piekenbrock M, Doran D: dbscan: Fast Density-Based Clustering with R. Journal of Statistical Software, 91(1), pp. 1-30. doi: 10.18637/jss.v091.i01, 2019
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')

out=HierarchicalDBSCAN(Hepta$Data,PlotIt=FALSE)


data('Leukemia')
set.seed(1234)
CA=HierarchicalDBSCAN(Leukemia$DistanceMatrix)
#ClusterCount(CA$Cls)
#ClusterDendrogram(CA$Dendrogram,5,main='H-DBscan')


</code></pre>

<hr>
<h2 id='kmeansClustering'>
K-Means Clustering
</h2><span id='topic+kmeansClustering'></span>

<h3>Description</h3>

<p>Perform k-means clustering on a data matrix. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kmeansClustering(DataOrDistances, ClusterNo,

 Type = 'LBG',RandomNo=5000, CategoricalData,
 
 PlotIt=FALSE, Verbose = FALSE,... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kmeansClustering_+3A_dataordistances">DataOrDistances</code></td>
<td>
<p>Either nonsymmetric [1:n,1:d] datamatrix of n cases and d numerical features or
</p>
<p>symmetric [1:n,1:n] distance matrix</p>
</td></tr>
<tr><td><code id="kmeansClustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be built by the algorithm.</p>
</td></tr>
<tr><td><code id="kmeansClustering_+3A_type">Type</code></td>
<td>
 
<p>Choice of Kmeans algorithm, currently either &quot; <code>Hartigan</code>&quot; [Hartigan/Wong, 1979], &quot;<code>LBG</code>&quot; [Linde et al., 1980], &quot;<code>Sparse</code>&quot; sparse k-means proposed in [Witten/Tibshirani, 2010], &quot;<code>Steinley</code>&quot; best method of [Steinley/Brusco, 2007] proposed in Steinley 2003, &quot;<code>Lloyd</code>&quot; [Lloyd, 1982], &quot;<code>Forgy</code>&quot;[Forgy, 1965], <code>MacQueen</code> [MacQueen, 1967], <code>kcentroids</code> [Leisch, 2006], &quot;<code>kprototypes</code>&quot; [Szepannek, 2018], &quot;<code>Pelleg-moore</code>&quot; [Pelleg &amp; Moores,2000], &quot;<code>Elkan</code>&quot; [Elkan, 2003], &quot;<code>kmeans++</code>&quot;&quot; [Arthur &amp; Vassilvitskii], <code>Hamerly</code>&quot;[Hamerly, 2010] ,<code>Dualtree</code>&quot;  or <code>Dualtree-covertree</code> [Curtin, 2017]&quot;
</p>
</td></tr>
<tr><td><code id="kmeansClustering_+3A_randomno">RandomNo</code></td>
<td>
<p>Only for &quot; <code>Steinley</code>&quot; or in case of distance matrix, number of random initializations with searching for minimal SSE, see [Steinley/Brusco, 2007]</p>
</td></tr>
<tr><td><code id="kmeansClustering_+3A_categoricaldata">CategoricalData</code></td>
<td>
<p>Only for &quot; <code>kprototypes</code>&quot;, [1:n,1:m] matrix of categorical features]</p>
</td></tr>
<tr><td><code id="kmeansClustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, If TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="kmeansClustering_+3A_verbose">Verbose</code></td>
<td>
<p>Print details, if true</p>
</td></tr>
<tr><td><code id="kmeansClustering_+3A_...">...</code></td>
<td>
<p>Further arguments like <code>iter.max</code>, <code>nstart</code>, for <code>kcentroids</code> please see <code>kcca</code> function of the <span class="pkg">flexclust</span> package, or  <code><a href="sparcl.html#topic+KMeansSparseCluster">KMeansSparseCluster</a></code> 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses either <span class="pkg">stats</span> package function 'kmeans', <span class="pkg">cclust</span> package implemention, <span class="pkg">flexclust</span> package implemention or own code.
In case of a distance matrix, RandomNo should be significantly lower than 5000, otherwise a long computation time is to be expected.
</p>


<h3>Value</h3>

<p>List V of 
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.
</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p> Object of the clustering algorithm used if existent, otherwise
</p>
<p>SumDistsToCentroids: Vector of within-cluster sum of squares, one component per cluster
</p>
</td></tr>
<tr><td><code>Centroids</code></td>
<td>
<p>the final cluster centers.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The version using a distance matrix is still in the test phase and not yet verified.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Hartigan/Wong, 1979]  Hartigan, J. A., &amp; Wong, M. A.: Algorithm AS 136: A k-means clustering algorithm, Journal of the Royal Statistical Society. Series C (Applied Statistics), Vol. 28(1), pp. 100-108. 1979.
</p>
<p>[Linde et al., 1980]  Linde, Y., Buzo, A., &amp; Gray, R.: An algorithm for vector quantizer design, IEEE Transactions on communications, Vol. 28(1), pp. 84-95. 1980.
</p>
<p>[Steinley/Brusco, 2007]  Steinley, D., &amp; Brusco, M. J.: Initializing k-means batch clustering: A critical evaluation of several techniques, Journal of Classification, Vol. 24(1), pp. 99-121. 2007.
</p>
<p>[Forgy, 1965]  Forgy, E. W.: Cluster analysis of multivariate data: efficiency versus interpretability of classifications, Biometrics, Vol. 21, pp. 768-769. 1965.
</p>
<p>[MacQueen, 1967]  MacQueen, J.: Some methods for classification and analysis of multivariate observations, Proc. Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, Vol. 1, pp. 281-297, Oakland, CA, USA., 1967.
</p>
<p>[Pelleg &amp; Moores,2000] Pelleg, Dan, and Andrew W. Moore. X-means: Extending k-means with efficient estimation of the number of clusters, ICML. Vol. 1. 2000.
</p>
<p>[Elkan, 2003] Elkan, Charles: Using the triangle inequality to acceler- ate k-means, In Tom Fawcett and Nina Mishra, editors, ICML, pages Vol.3, 147-153. AAAI Press, 2003.
</p>
<p>[Lloyd, 1982]  Lloyd, S.: Least squares quantization in PCM, IEEE transactions on information theory, Vol. 28(2), pp. 129-137. 1982.
</p>
<p>[Leisch, 2006]  Leisch, F.: A toolbox for k-centroids cluster analysis, Computational Statistics &amp; Data Analysis, Vol. 51(2), pp. 526-544. 2006.
</p>
<p>[Arthur &amp; Vassilvitskii] Arthur, David, and  Vassilvitskii, Sergei: K-means++ the advantages of careful seeding, Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. 2007
</p>
<p>[Witten/Tibshirani, 2010] Witten, D. and Tibshirani, R.: A Framework for
Feature Selection in Clustering. Journal of the American Statistical
Association, Vol. 105(490), pp. 713-726, 2010.
</p>
<p>[Hamerly, 2010]  Hamerly, Greg: Making k-means even faster, Proceedings of the 2010 SIAM international conference on data mining, Society for Industrial and Applied Mathematics, pp. 130-140, 2010.
</p>
<p>[Szepannek, 2018] Szepannek, G.: clustMixType: User-Friendly Clustering of Mixed-Type Data in R, The R Journal, Vol. 10/2, pp. 200-208, doi:10.32614/RJ2018048, 2018.
</p>
<p>[Curtin, 2017]  Curtin, Ryan R: A dual-tree algorithm for fast k-means clustering with large k, Proceedings of the 2017 SIAM International Conference on Data Mining, Society for Industrial and Applied Mathematics, 2017.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
out=kmeansClustering(Hepta$Data,ClusterNo=7,PlotIt=FALSE)


data('Leukemia')
# As expected does not perform well
# For non-spherical cluster structures:
out=kmeansClustering(Leukemia$DistanceMatrix,ClusterNo=6,RandomNo =10,PlotIt=TRUE)




data('Hepta')
out=kmeansClustering(Hepta$Data,ClusterNo=7,PlotIt=FALSE,Type="Steinley")



data('Hepta')
out=kmeansClustering(Hepta$Data,ClusterNo = 7,
Type = "kprototypes",CategoricalData = as.matrix(Hepta$Cls))


</code></pre>

<hr>
<h2 id='kmeansDist'>
k-means Clustering using a distance matrix
</h2><span id='topic+kmeansDist'></span>

<h3>Description</h3>

<p>Perform k-means clustering on a distance matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kmeansDist(Distance, ClusterNo=2,Centers=NULL,

RandomNo=1,maxIt = 2000, 

PlotIt=FALSE,verbose = F)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kmeansDist_+3A_distance">Distance</code></td>
<td>
<p> Distance matrix. For n data points of the dimension n x n   </p>
</td></tr>
<tr><td><code id="kmeansDist_+3A_clusterno">ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be built by the algorithm.</p>
</td></tr>
<tr><td><code id="kmeansDist_+3A_centers">Centers</code></td>
<td>
<p>Default(NULL) a set of initial (distinct) cluster centres.</p>
</td></tr>
<tr><td><code id="kmeansDist_+3A_randomno">RandomNo</code></td>
<td>
<p>If&gt;1: Number of random initializations with searching for minimal SSE is defined by this scalar</p>
</td></tr>
<tr><td><code id="kmeansDist_+3A_maxit">maxIt</code></td>
<td>
<p>Optional: Maximum number of iterations before the algorithm terminates.</p>
</td></tr>
<tr><td><code id="kmeansDist_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, If TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="kmeansDist_+3A_verbose">verbose</code></td>
<td>
<p>Optional: Algorithm always outputs current iteration.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>Cls[1:n]</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>centerids[1:k]</code></td>
<td>
<p>Indices of the centroids from which the cluster Cls was created</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Currently an experimental version
</p>


<h3>Author(s)</h3>

<p>Felix Pape, Michael Thrun
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
#out=kmeansDist(as.matrix(dist(Hepta$Data)),ClusterNo=7,PlotIt=FALSE,RandomNo = 10)

## Not run: 
data('Leukemia')
#as expected does not perform well
#for non-spherical cluster structures:
#out=kmeansDist(Leukemia$DistanceMatrix,ClusterNo=6,PlotIt=TRUE,RandomNo=10)

## End(Not run)
</code></pre>

<hr>
<h2 id='LargeApplicationClustering'>Large Application Clustering</h2><span id='topic+LargeApplicationClustering'></span>

<h3>Description</h3>

<p>Clustering Large Applications  (clara) of  [Rousseeuw/Kaufman, 1990, pp. 126-163]
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LargeApplicationClustering(Data, ClusterNo,

PlotIt=FALSE,Standardization=TRUE,Samples=50,Random=TRUE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LargeApplicationClustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.</p>
</td></tr>
<tr><td><code id="LargeApplicationClustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be built by the algorithm.</p>
</td></tr>
<tr><td><code id="LargeApplicationClustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, If TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="LargeApplicationClustering_+3A_standardization">Standardization</code></td>
<td>

<p><code>Data</code> is standardized before calculating the dissimilarities. Measurements are standardized for each variable (column), by subtracting the variable's mean value and dividing by the variable's mean absolute deviation.
</p>
</td></tr>
<tr><td><code id="LargeApplicationClustering_+3A_samples">Samples</code></td>
<td>
<p>Integer, say N, the number of samples to be drawn from the dataset. Default value set as recommended by documentation of <code><a href="cluster.html#topic+clara">clara</a></code></p>
</td></tr>
<tr><td><code id="LargeApplicationClustering_+3A_random">Random</code></td>
<td>
<p>Logical indicating if R's random number generator should be used instead of the primitive clara()-builtin one.</p>
</td></tr>
<tr><td><code id="LargeApplicationClustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is recommended to use <code>set.seed</code> if clustering output should be always the same instead of setting Random=FALSE in order to use the primitive clara()-builtin random number generator.
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun</p>


<h3>References</h3>

<p>[Rousseeuw/Kaufman, 1990]	Rousseeuw, P. J., &amp; Kaufman, L.: Finding groups in data, Belgium, John Wiley &amp; Sons Inc., ISBN: 0471735787, doi 10.1002/9780470316801, Online ISBN: 9780470316801, 1990.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
out=LargeApplicationClustering(Hepta$Data,ClusterNo=7,PlotIt=FALSE)
</code></pre>

<hr>
<h2 id='Leukemia'>
Leukemia distance matrix and classificiation used in [Thrun, 2018]
</h2><span id='topic+Leukemia'></span>

<h3>Description</h3>

<p>Data is anonymized. Original dataset was published in [Haferlach et al., 2010]. Original dataset had around 12.000 dimensions.
Detailed description of preprocessed dataset and its clustering challenge is provided in [Thrun/Ultsch, 2020].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("Leukemia")</code></pre>


<h3>Details</h3>

<p>554x554 distance matrix.
Cls defines the following clusters:
</p>
<p>1= APL Outlier
</p>
<p>2=APL
</p>
<p>3=Healthy
</p>
<p>4=AML
</p>
<p>5=CLL
</p>
<p>6=CLL Outlier
</p>


<h3>References</h3>

<p>[Thrun, 2018]  Thrun, M. C.: Projection Based Clustering through Self-Organization and Swarm Intelligence, doctoral dissertation 2017, Springer, Heidelberg, ISBN: 978-3-658-20539-3, <a href="https://doi.org/10.1007/978-3-658-20540-9">doi:10.1007/978-3-658-20540-9</a>, 2018. 
</p>
<p>[Haferlach et al., 2010]  Haferlach, T., Kohlmann, A., Wieczorek, L., Basso, G., Te Kronnie, G., Bene, M.-C., . . . Mills, K. I.: Clinical utility of microarray-based gene expression profiling in the diagnosis and subclassification of leukemia: report from the International Microarray Innovations in Leukemia Study Group, Journal of Clinical Oncology, Vol. 28(15), pp. 2529-2537. 2010.
</p>
<p>[Thrun/Ultsch, 2020]  Thrun, M. C., &amp; Ultsch, A.: Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems, Data in Brief, Vol. 30(C), pp. 105501, <a href="https://doi.org/10.1016/j.dib.2020.105501">doi:10.1016/j.dib.2020.105501</a>, 2020.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Leukemia)
str(Leukemia)
Cls=Leukemia$Cls
Distance=Leukemia$DistanceMatrix
isSymmetric(Distance)
</code></pre>

<hr>
<h2 id='Lsun3D'>
Lsun3D inspired by FCPS introduced in [Thrun, 2018]
</h2><span id='topic+Lsun3D'></span>

<h3>Description</h3>

<p>Clearly defined clusters, different variances.  Detailed description of dataset and its clustering challenge is provided in [Thrun/Ultsch, 2020].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("Lsun3D")</code></pre>


<h3>Details</h3>

<p>Size 404, Dimensions 3
</p>
<p>Dataset defines discontinuites, where the clusters have different variances.
Three main clusters, and four outliers (in cluster 4). For a more detailed description see [Thrun, 2018].
</p>


<h3>References</h3>

<p>[Thrun, 2018]  Thrun, M. C.: Projection Based Clustering through Self-Organization and Swarm Intelligence, doctoral dissertation 2017, Springer, Heidelberg, ISBN: 978-3-658-20539-3, <a href="https://doi.org/10.1007/978-3-658-20540-9">doi:10.1007/978-3-658-20540-9</a>, 2018. 
</p>
<p>[Thrun/Ultsch, 2020]  Thrun, M. C., &amp; Ultsch, A.: Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems, Data in Brief, Vol. 30(C), pp. 105501, <a href="https://doi.org/10.1016/j.dib.2020.105501">doi:10.1016/j.dib.2020.105501</a>, 2020.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Lsun3D)
str(Lsun3D)
Cls=Lsun3D$Cls
Data=Lsun3D$Data
</code></pre>

<hr>
<h2 id='MarkovClustering'>
Markov Clustering
</h2><span id='topic+MarkovClustering'></span>

<h3>Description</h3>

<p>Graph clustering algorithm introduced by [van Dongen, 2000].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MarkovClustering(DataOrDistances=NULL,Adjacency=NULL,

Radius=TRUE,DistanceMethod="euclidean",addLoops = TRUE,PlotIt=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MarkovClustering_+3A_dataordistances">DataOrDistances</code></td>
<td>
<p>NULL or: Either [1:n,1:n] symmetric distance matrix or [1:n,1:d] not symmetric data matrix of n cases and d variables</p>
</td></tr>
<tr><td><code id="MarkovClustering_+3A_adjacency">Adjacency</code></td>
<td>
<p>Used if <code>Data</code> is NULL, matrix [1:n,1:n] defining which points are adjacent to each other by the number 1; not adjacent: 0</p>
</td></tr>
<tr><td><code id="MarkovClustering_+3A_radius">Radius</code></td>
<td>
<p>Scalar, Radius for unit disk graph (r-ball graph) if adjacency matrix is missing. Automatic estimation can be done either with =TRUE [Ultsch, 2005] or FALSE [Thrun et al., 2016] if Data instead of Distances are given.</p>
</td></tr>
<tr><td><code id="MarkovClustering_+3A_distancemethod">DistanceMethod</code></td>
<td>

<p>Optional distance method of data, default is euclid, see <code><a href="parallelDist.html#topic+parDist">parDist</a></code> for details
</p>
</td></tr>
<tr><td><code id="MarkovClustering_+3A_addloops">addLoops</code></td>
<td>

<p>Logical; if TRUE, self-loops with weight 1 are added to each vertex of x (see <code>mcl</code> of CRAN package <code>MCL</code>).
</p>
</td></tr>
<tr><td><code id="MarkovClustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, If TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="MarkovClustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>DataOrDistances</code> is used to compute the <code>Adjecency</code> matrix if this input is missing. Then a unit-disk (R-ball) graph is calculated.
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.
Points which cannot be assigned to a cluster will be reported with 0.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[van Dongen, 2000]  van Dongen, S.M. Graph Clustering by Flow Simulation. Ph.D. thesis, Universtiy of Utrecht. Utrecht University Repository: http://dspace.library.uu.nl/handle/1874/848, 2000
</p>
<p>[Thrun et al., 2016]  Thrun, M. C., Lerch, F., Loetsch, J., &amp; Ultsch, A. : Visualization and 3D Printing of Multivariate Data of Biomarkers, in Skala, V. (Ed.), International Conference in Central Europe on Computer Graphics, Visualization  and Computer Vision (WSCG), Vol. 24, Plzen, 2016. 
</p>
<p>[Ultsch, 2005]  Ultsch, A.: Pareto density estimation: A density estimation for knowledge discovery, In Baier, D. &amp; Werrnecke, K. D. (Eds.), Innovations in classification, data science, and information systems, (Vol. 27, pp. 91-100), Berlin, Germany, Springer, 2005.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

data('Hepta')
out=MarkovClustering(Data=Hepta$Data,PlotIt=FALSE)

</code></pre>

<hr>
<h2 id='MeanShiftClustering'>Mean Shift Clustering</h2><span id='topic+MeanShiftClustering'></span>

<h3>Description</h3>

<p>Mean Shift Clustering of  [Cheng, 1995]	
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MeanShiftClustering(Data,

PlotIt=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MeanShiftClustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.</p>
</td></tr>
<tr><td><code id="MeanShiftClustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, If TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="MeanShiftClustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>the radius used for search can be specified with the &quot;<code>radius</code>&quot; parameter. The maximum number of iterations before algorithm termination is controlled with the &quot;<code>max_iterations</code>&quot; parameter.
</p>
<p>If the distance between two centroids is less than the given radius, one will be removed. A radius of 0 or less means an estimate will be calculated and used for the radius. Default value &quot;0&quot; (numeric).
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun</p>


<h3>References</h3>

<p>[Cheng, 1995]	Cheng, Yizong: Mean Shift, Mode Seeking, and Clustering, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 17 (8), pp. 790-799, doi:10.1109/34.400568, 1995.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
out=MeanShiftClustering(Hepta$Data,PlotIt=FALSE,radius=1)
</code></pre>

<hr>
<h2 id='MinimalEnergyClustering'>
Minimal Energy Clustering 
</h2><span id='topic+MinimalEnergyClustering'></span>

<h3>Description</h3>

<p>Hierchical Clustering using the minimal energy approach of [Szekely/Rizzo, 2005].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MinimalEnergyClustering(DataOrDistances, ClusterNo = 0,
DistanceMethod="euclidean", ColorTreshold = 0,Data,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MinimalEnergyClustering_+3A_dataordistances">DataOrDistances</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features. Alternatively, symmetric [1:n,1:n] distance matrix</p>
</td></tr>
<tr><td><code id="MinimalEnergyClustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be build by the algorithm.</p>
</td></tr>
<tr><td><code id="MinimalEnergyClustering_+3A_distancemethod">DistanceMethod</code></td>
<td>
<p>See  <code><a href="parallelDist.html#topic+parDist">parDist</a></code>, for example 'euclidean','mahalanobis','manhatten' (cityblock),'fJaccard','binary', 'canberra', 'maximum'. Any unambiguous substring can be given.</p>
</td></tr>
<tr><td><code id="MinimalEnergyClustering_+3A_colortreshold">ColorTreshold</code></td>
<td>
<p>Draws cutline w.r.t. dendogram y-axis (height), height of line as scalar should be given</p>
</td></tr>
<tr><td><code id="MinimalEnergyClustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] data matrix in the case that <code>DataOrDistances</code> is missing and partial matching does not work.</p>
</td></tr>
<tr><td><code id="MinimalEnergyClustering_+3A_...">...</code></td>
<td>
<p>In case of plotting further argument for <code>plot</code>, see <code><a href="stats.html#topic+as.dendrogram">as.dendrogram</a></code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>If ClusterNo&gt;0: [1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering. Otherwise ClusterNo=0: NULL</p>
</td></tr>
<tr><td><code>Dendrogram</code></td>
<td>
<p>Dendrogram of hierarchical clustering algorithm</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Ultrametric tree of hierarchical clustering algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Szekely/Rizzo, 2005]  Szekely, G. J. and Rizzo, M. L.: Hierarchical Clustering via Joint Between-Within Distances: Extending Ward's Minimum Variance Method, Journal of Classification, 22(2) 151-183.http://dx.doi.org/10.1007/s00357-005-0012-9, 2005.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+HierarchicalClustering">HierarchicalClustering</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
out=MinimalEnergyClustering(Hepta$Data,ClusterNo=7)
</code></pre>

<hr>
<h2 id='MinimaxLinkageClustering'>
Minimax Linkage Hierarchical Clustering
</h2><span id='topic+MinimaxLinkageClustering'></span>

<h3>Description</h3>

<p>In the minimax linkage hierarchical clustering every cluster has an associated prototype element that represents that cluster [Bien/Tibshirani, 2011].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MinimaxLinkageClustering(DataOrDistances, ClusterNo = 0,
DistanceMethod="euclidean", ColorTreshold = 0,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MinimaxLinkageClustering_+3A_dataordistances">DataOrDistances</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases or d-dimensional data points. Every case has d attributes, variables or features. Alternatively, symmetric [1:n,1:n] distance matrix</p>
</td></tr>
<tr><td><code id="MinimaxLinkageClustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be build by the algorithm.</p>
</td></tr>
<tr><td><code id="MinimaxLinkageClustering_+3A_distancemethod">DistanceMethod</code></td>
<td>
<p>See  <code><a href="parallelDist.html#topic+parDist">parDist</a></code>, for example 'euclidean','mahalanobis','manhatten' (cityblock),'fJaccard','binary', 'canberra', 'maximum'. Any unambiguous substring can be given.</p>
</td></tr>
<tr><td><code id="MinimaxLinkageClustering_+3A_colortreshold">ColorTreshold</code></td>
<td>
<p>Draws cutline w.r.t. dendogram y-axis (height), height of line as scalar should be given</p>
</td></tr>
<tr><td><code id="MinimaxLinkageClustering_+3A_...">...</code></td>
<td>
<p>In case of plotting further argument for <code>plot</code>, see <code><a href="stats.html#topic+as.dendrogram">as.dendrogram</a></code>
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>If, ClusterNo&gt;0: [1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering. Otherwise for ClusterNo=0: NULL</p>
</td></tr>
<tr><td><code>Dendrogram</code></td>
<td>
<p>Dendrogram of hierarchical clustering algorithm</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Ultrametric tree of hierarchical clustering algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Bien/Tibshirani, 2011] Bien, J., and Tibshirani, R.: Hierarchical Clustering with Prototypes via Minimax Linkage, The Journal of the American Statistical Association, Vol. 106(495), pp. 1075-1084, 2011.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+HierarchicalClustering">HierarchicalClustering</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
out=MinimaxLinkageClustering(Hepta$Data,ClusterNo=7)
</code></pre>

<hr>
<h2 id='ModelBasedClustering'>
Model Based Clustering
</h2><span id='topic+ModelBasedClustering'></span>

<h3>Description</h3>

<p>Calls Model based clustering of [Fraley/Raftery, 2006] which models a  Mixture Of Gaussians (MoG).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ModelBasedClustering(Data,ClusterNo=2,PlotIt=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ModelBasedClustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.</p>
</td></tr>
<tr><td><code id="ModelBasedClustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be built by the algorithm.</p>
</td></tr>
<tr><td><code id="ModelBasedClustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, If TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="ModelBasedClustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>see [Thrun, 2017, p. 23] or [Fraley/Raftery, 2002] and [Fraley/Raftery, 2006].
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Note</h3>

<p>MoGclustering used in [Thrun, 2017] was renamed to <code><a href="#topic+ModelBasedClustering">ModelBasedClustering</a></code> in this package.</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Thrun, 2017]  Thrun, M. C.:A System for Projection Based Clustering through Self-Organization and Swarm Intelligence, (Doctoral dissertation), Philipps-Universitaet Marburg, Marburg, 2017.
</p>
<p>[Fraley/Raftery, 2002]  Fraley, C., and Raftery, A. E.: Model-based clustering, discriminant analysis, and density estimation, Journal of the American Statistical Association, Vol. 97(458), pp. 611-631. 2002.
</p>
<p>[Fraley/Raftery, 2006]  Fraley, C., and Raftery, A. E.MCLUST version 3: an R package for normal mixture modeling and model-based clustering,DTIC Document, 2006.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+MoGclustering">MoGclustering</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
out=ModelBasedClustering(Hepta$Data,PlotIt=FALSE)
</code></pre>

<hr>
<h2 id='ModelBasedVarSelClustering'>
Model Based Clustering with Variable Selection
</h2><span id='topic+ModelBasedVarSelClustering'></span>

<h3>Description</h3>

<p>Model-based clustering with variable selection and estimation of the number of
clusters which is either based on [Marbac/Sedki, 2017],[Marbac et al., 2020], or on [Scrucca and Raftery, 2014].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ModelBasedVarSelClustering(Data,ClusterNo,Type,PlotIt=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ModelBasedVarSelClustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases
of d-dimensional data points. Every case has d attributes, variables or
features.</p>
</td></tr>
<tr><td><code id="ModelBasedVarSelClustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>Numeric which defines number of cluster to search for.</p>
</td></tr>
<tr><td><code id="ModelBasedVarSelClustering_+3A_type">Type</code></td>
<td>
<p>String, either <code>VarSelLCM</code> [Marbac/Sedki, 2017],[Marbac et al., 2020], or <code>clustvarsel</code> [Scrucca and Raftery, 2014].</p>
</td></tr>
<tr><td><code id="ModelBasedVarSelClustering_+3A_plotit">PlotIt</code></td>
<td>
<p>(optional) Boolean. Default = FALSE = No plotting performed.</p>
</td></tr>
<tr><td><code id="ModelBasedVarSelClustering_+3A_...">...</code></td>
<td>
<p>Further arguments passed on to <a href="VarSelLCM.html#topic+VarSelCluster">VarSelCluster</a> or <a href="clustvarsel.html#topic+clustvarsel">clustvarsel</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as
the main output of the clustering algorithm. It has k unique numbers
representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this
algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Quirin Stier, Michael Thrun
</p>


<h3>References</h3>

<p>[Marbac/Sedki, 2017] Marbac, M. and Sedki, M.: Variable selection for
model-based clustering using the integrated complete-data likelihood. Statistics
and Computing, 27(4), pp. 1049-1063, 2017.
</p>
<p>[Marbac et al., 2020]  Marbac, M., Sedki, M., &amp; Patin, T.: Variable selection for mixed data clustering: application in human population genomics, Journal of Classification, Vol. 37(1), pp. 124-142. 2020.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Hepta
data("Hepta")
Data = Hepta$Data
V = ModelBasedVarSelClustering(Data, ClusterNo=7,Type="VarSelLCM")
Cls = V$Cls
ClusterAccuracy(Hepta$Cls, Cls, K = 7)

V = ModelBasedVarSelClustering(Data, ClusterNo=7,Type="clustvarsel")
Cls = V$Cls
ClusterAccuracy(Hepta$Cls, Cls, K = 7)

## Not run: 
# Hearts
heart=VarSelLCM::heart
ztrue &lt;- heart[,"Class"]
Data &lt;- heart[,-13]
V &lt;- ModelBasedVarSelClustering(Data,2,Type="VarSelLCM")
Cls = V$Cls
ClusterAccuracy(ztrue, Cls, K = 2)

## End(Not run)
</code></pre>

<hr>
<h2 id='MoGclustering'>
Mixture of Gaussians Clustering using EM
</h2><span id='topic+MoGclustering'></span>

<h3>Description</h3>

<p>MixtureOfGaussians (MoG) clustering based on Expectation Maximization (EM) of [Chen et al., 2012] or algorithms closely resembling EM of [Benaglia/Chauveau/Hunter, 2009].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MoGclustering(Data,ClusterNo=2,Type,PlotIt=FALSE,Silent=TRUE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MoGclustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.</p>
</td></tr>
<tr><td><code id="MoGclustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be built by the algorithm.</p>
</td></tr>
<tr><td><code id="MoGclustering_+3A_type">Type</code></td>
<td>
<p>string defining approach to select:  initialization approach of &quot;EM&quot; or &quot;kmeans&quot; of [Chen et al., 2012], or other methods &quot;mvnormalmixEM&quot; [McLachlan/Peel, 2000],
&quot;npEM&quot;[Benaglia et al., 2009] or its extension &quot;mvnpEM&quot;
[Chauveau/Hoang, 2016].
</p>
</td></tr>
<tr><td><code id="MoGclustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, if TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="MoGclustering_+3A_silent">Silent</code></td>
<td>
<p>(optional) Boolean: print output or not (Default = FALSE = no
output)</p>
</td></tr>
<tr><td><code id="MoGclustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used, see package mixtools <span class="pkg">EMCluster</span> or <span class="pkg">mixtools</span> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Algorithms for clustering through EM or its close resembles.
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Note</h3>

<p>MoG used in [Thrun, 2017] was renamed to <code><a href="#topic+ModelBasedClustering">ModelBasedClustering</a></code> in this package. <code>Type="mvnormalmixEM"</code> sometimes fails </p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Chen et al., 2012]  Chen, W., Maitra, R., &amp; Melnykov, V.: EMCluster: EM Algorithm for Model-Based Clustering of Finite Mixture Gaussian Distribution, R Package, URL http://cran. r-project. org/package= EMCluster, 2012.
</p>
<p>[Chauveau/Hoang, 2016]  Chauveau, D., &amp; Hoang, V. T. L.: Nonparametric mixture models with conditionally independent multivariate component densities, Computational Statistics &amp; Data Analysis, Vol. 103, pp. 1-16. 2016.
</p>
<p>[Benaglia et al., 2009] Benaglia, T., Chauveau, D., and Hunter, D. R.: An EM-like algorithm for semi-and nonparametric estimation in multivariate mixtures. Journal of Computational and Graphical Statistics, 18(2), pp. 505-526, 2009.
</p>
<p>[McLachlan/Peel, 2000] D. McLachlan, G. J. and Peel, D.: Finite Mixture Models, John Wiley and Sons, Inc, 2000.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ModelBasedClustering">ModelBasedClustering</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
Data = Hepta$Data
out=MoGclustering(Data,ClusterNo=7,Type="EM",PlotIt=FALSE)
V=out$Cls


V1 = MoGclustering(Data,ClusterNo=7,Type="mvnpEM")
Cls1 = V1$Cls

V2 = MoGclustering(Data,ClusterNo=7,Type="npEM")
Cls2 = V2$Cls


## Not run: 
#does not work always
  V3 = MoGclustering(Data,ClusterNo=7,Type="mvnormalmixEM")
  Cls3 = V3$Cls

## End(Not run)

</code></pre>

<hr>
<h2 id='MSTclustering'>
MST-kNN clustering algorithm [Inostroza-Ponta, 2008].
</h2><span id='topic+MSTclustering'></span>

<h3>Description</h3>

<p>Performs the MST-kNN clustering algorithm which generate a clustering solution with automatic k determination using two proximity graphs: Minimal Spanning Tree (MST) and k-Nearest Neighbor (kNN) which are recursively intersected.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MSTclustering(DataOrDistances, DistanceMethod = "euclidean",PlotIt=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MSTclustering_+3A_dataordistances">DataOrDistances</code></td>
<td>

<p>Either [1:n,1:n] symmetric distance matrix or [1:n,1:d] not symmetric data matrix of n cases and d variables
</p>
</td></tr>
<tr><td><code id="MSTclustering_+3A_distancemethod">DistanceMethod</code></td>
<td>

<p>Optional distance method of data, default is euclid, see <code><a href="parallelDist.html#topic+parDist">parDist</a></code> for details
</p>
</td></tr>
<tr><td><code id="MSTclustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, if TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="MSTclustering_+3A_...">...</code></td>
<td>

<p>Optional, further arguments for   <code><a href="mstknnclust.html#topic+mst.knn">mst.knn</a></code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Does not work on Hepta with euclidean distances.
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Inostroza-Ponta, 2008]  Inostroza-Ponta, M.: An integrated and scalable approach based on combinatorial optimization techniques for the analysis of microarray data, University of Newcastle, ISBN, 2008</p>


<h3>See Also</h3>

<p><code><a href="mstknnclust.html#topic+mst.knn">mst.knn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Hepta)

MSTclustering(Hepta$Data)

</code></pre>

<hr>
<h2 id='NetworkClustering'>
Network Clustering
</h2><span id='topic+NetworkClustering'></span>

<h3>Description</h3>

<p>Either leiden [Traag et al., 2019] or louvain [Blondel et al., 2008] clustering
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NetworkClustering(DataOrDistances=NULL,Adjacency=NULL,

Type="louvain",Radius=FALSE,PlotIt=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NetworkClustering_+3A_dataordistances">DataOrDistances</code></td>
<td>
<p>NULL or: [1:n,1:d] matrix of dataset to be clustered. It consists of n cases or d-dimensional data points. Every case has d attributes, variables or features. Alternatively, symmetric [1:n,1:n] distance matrix</p>
</td></tr>
<tr><td><code id="NetworkClustering_+3A_adjacency">Adjacency</code></td>
<td>
<p>Used if <code>DataOrDistances</code> is NULL, matrix [1:n,1:n] defining which points are adjacent to each other by the number 1; not adjacent: 0</p>
</td></tr>
<tr><td><code id="NetworkClustering_+3A_type">Type</code></td>
<td>

<p>Either &quot;louvain&quot; or &quot;leiden&quot;
</p>
</td></tr>
<tr><td><code id="NetworkClustering_+3A_radius">Radius</code></td>
<td>
<p>Scalar, Radius for unit disk graph (r-ball graph) if adjacency matrix is missing. Automatic estimation can be done either with =TRUE [Ultsch, 2005] or FALSE [Thrun et al., 2016]</p>
</td></tr>
<tr><td><code id="NetworkClustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, If TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="NetworkClustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>DataOrDistances</code> is used to compute the <code>Adjecency</code> matrix if this input is missing. Then a unit-disk (R-ball) graph is calculated.
<code>Radius=TRUE</code> only works if data matrix is given.
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.
Points which cannot be assigned to a cluster will be reported with 0.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Note</h3>

<p>leiden requires igraph package and an installed python version. automatic installation may not work. manual call in console has to be  in this case <code>conda install -c conda-forge leidenalg</code>
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Blondel et al., 2008]  Blondel, V. D., Guillaume, J.-L., Lambiotte, R., &amp; Lefebvre, E.: Fast unfolding of communities in large networks, Journal of statistical mechanics: theory and experiment, Vol. 2008(10), pp. P10008. 2008.
</p>
<p>[Traag et al., 2019]  Traag, V. A., Waltman, L., &amp; van Eck, N. J.: From Louvain to Leiden: guaranteeing well-connected communities, Scientific reports, Vol. 9(1), pp. 1-12. 2019.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

data('Hepta')
#out=NetworkClustering(Hepta$Data,PlotIt=FALSE)

</code></pre>

<hr>
<h2 id='NeuralGasClustering'>Neural gas algorithm for clustering</h2><span id='topic+NeuralGasClustering'></span>

<h3>Description</h3>

<p>Neural gas clustering published by [Martinetz et al., 1993]] and implemented by [Bodenhofer et al., 2011].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NeuralGasClustering(Data, ClusterNo,PlotIt=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NeuralGasClustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.</p>
</td></tr>
<tr><td><code id="NeuralGasClustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be built by the algorithm.</p>
</td></tr>
<tr><td><code id="NeuralGasClustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, If TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="NeuralGasClustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun</p>


<h3>References</h3>

<p>[Dimitriadou, 2002]  Dimitriadou, E.: cclust-convex clustering methods and clustering indexes. R package, 2002,
</p>
<p>[Martinetz et al., 1993]  Martinetz, T. M., Berkovich, S. G., &amp; Schulten, K. J.: 'Neural-gas' network for vector quantization and its application to time-series prediction, IEEE Transactions on Neural Networks, Vol. 4(4), pp. 558-569. 1993.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
out=NeuralGasClustering(Hepta$Data,ClusterNo=7,PlotIt=FALSE)
</code></pre>

<hr>
<h2 id='OPTICSclustering'>
OPTICS Clustering
</h2><span id='topic+OPTICSclustering'></span>

<h3>Description</h3>

<p>OPTICS (Ordering points to identify the clustering structure) clustering algorithm [Ankerst et al.,1999].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>OPTICSclustering(Data, MaxRadius,RadiusThreshold, minPts = 5, PlotIt=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="OPTICSclustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.</p>
</td></tr>
<tr><td><code id="OPTICSclustering_+3A_maxradius">MaxRadius</code></td>
<td>

<p>Upper limit neighborhood in the R-ball graph/unit disk graph), size of the epsilon neighborhood  (eps) [Ester et al., 1996, p. 227].
If NULL, automatic estimation is done using insights of [Ultsch, 2005].
</p>
</td></tr>
<tr><td><code id="OPTICSclustering_+3A_radiusthreshold">RadiusThreshold</code></td>
<td>

<p>Threshold to identify clusters (RadiusThreshold &lt;= MaxRadius), if NULL <code>0.9*MaxRadius</code> is set.
</p>
</td></tr>
<tr><td><code id="OPTICSclustering_+3A_minpts">minPts</code></td>
<td>

<p>Number of minimum points in the eps region (for core points). 
In principle minimum number of points in the unit disk, if the unit disk is within the cluster (core) [Ester et al., 1996, p. 228].
If NULL, its 2.5 percent of points.
</p>
</td></tr>
<tr><td><code id="OPTICSclustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, If TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="OPTICSclustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>...
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector defining the clustering; this classification is the main output of the algorithm. Points which cannot be assigned to a cluster will be reported as members of the noise cluster with 0.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Ankerst et al.,1999]   Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel, Joerg Sander: OPTICS: Ordering Points To Identify the Clustering Structure, ACM SIGMOD international conference on Management of data, ACM Press, pp. 49-60, 1999.
</p>
<p>[Ester et al., 1996] Ester, M., Kriegel, H.-P., Sander, J., &amp; Xu, X.: A density-based algorithm for discovering clusters in large spatial databases with noise, Proc. Kdd, Vol. 96, pp. 226-231, 1996.
</p>
<p>[Ultsch, 2005] Ultsch, A.: Pareto density estimation: A density estimation for knowledge discovery, In Baier, D. &amp; Werrnecke, K. D. (Eds.), Innovations in classification, data science, and information systems, (Vol. 27, pp. 91-100), Berlin, Germany, Springer, 2005.
</p>


<h3>See Also</h3>

<p><code><a href="dbscan.html#topic+optics">optics</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
out=OPTICSclustering(Hepta$Data,MaxRadius=NULL,RadiusThreshold=NULL,minPts=NULL,PlotIt = FALSE)
</code></pre>

<hr>
<h2 id='PAMclustering'>
Partitioning Around Medoids (PAM)
</h2><span id='topic+PAMclustering'></span><span id='topic+PAMClustering'></span>

<h3>Description</h3>

<p>Partitioning (clustering) of the data into k clusters around medoids, a more robust version of k-means [Rousseeuw/Kaufman, 1990, p. 68-125] .
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PAMclustering(DataOrDistances,ClusterNo,

PlotIt=FALSE,Standardization=TRUE,Data,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PAMclustering_+3A_dataordistances">DataOrDistances</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features. Alternatively, symmetric [1:n,1:n] distance matrix</p>
</td></tr>
<tr><td><code id="PAMclustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be built by the algorithm.</p>
</td></tr>
<tr><td><code id="PAMclustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, If TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="PAMclustering_+3A_standardization">Standardization</code></td>
<td>

<p><code>DataOrDistances</code> is standardized before calculating the dissimilarities. Measurements are standardized for each variable (column), by subtracting the variable's mean value and dividing by the variable's mean absolute deviation.If <code>DataOrDistances</code> is already a distance matrix, then this argument will be ignored.
</p>
</td></tr>
<tr><td><code id="PAMclustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] data matrix in the case that <code>DataOrDistances</code> is missing and partial matching does not work.</p>
</td></tr>
<tr><td><code id="PAMclustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>[Rousseeuw/Kaufman, 1990, chapter 2] or [Reynolds et al., 1992].
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Rousseeuw/Kaufman, 1990]	Rousseeuw, P. J., &amp; Kaufman, L.: Finding groups in data, Belgium, John Wiley &amp; Sons Inc., ISBN: 0471735787, doi:10.1002/9780470316801, Online ISBN: 9780470316801, 1990.
</p>
<p>[Reynolds et al., 1992]	Reynolds, A., Richards, G.,de la Iglesia, B. and Rayward-Smith, V.: Clustering rules: A comparison of partitioning and hierarchical clustering algorithms, Journal of Mathematical Modelling and Algorithms 5, 475-504, DOI:10.1007/s10852-005-9022-1, 1992.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
out=PAMclustering(Hepta$Data,ClusterNo=7,PlotIt=FALSE)
</code></pre>

<hr>
<h2 id='pdfClustering'>
Probability Density Distribution Clustering
</h2><span id='topic+pdfClustering'></span>

<h3>Description</h3>

<p>Clustering via non parametric density estimation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pdfClustering(Data, PlotIt = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pdfClustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.</p>
</td></tr>
<tr><td><code id="pdfClustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, if TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="pdfClustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Cluster analysis is performed by the density-based procedures described in Azzalini and Torelli (2007) and Menardi and Azzalini (2014), and summarized in Azzalini and Menardi (2014).
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>Azzalini, A., Menardi, G. (2014). Clustering via nonparametric density estimation: the R package pdfCluster. Journal of Statistical Software, 57(11), 1-26, URL http://www.jstatsoft.org/v57/i11/.
</p>
<p>Azzalini A., Torelli N. (2007). Clustering via nonparametric density estimation. Statistics and Computing. 17, 71-80.
</p>
<p>Menardi, G., Azzalini, A. (2014). An advancement in clustering via nonparametric density estimation. Statistics and Computing. DOI: 10.1007/s11222-013-9400-x.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
out=pdfClustering(Hepta$Data,PlotIt=FALSE)
</code></pre>

<hr>
<h2 id='PenalizedRegressionBasedClustering'>
Penalized Regression-Based Clustering of [Wu et al., 2016].
</h2><span id='topic+PenalizedRegressionBasedClustering'></span>

<h3>Description</h3>

<p>Clustering is performed through penalized regression with grouping pursuit
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PenalizedRegressionBasedClustering(Data, FirstLambda, 

SecondLambda, Tau, PlotIt = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PenalizedRegressionBasedClustering_+3A_data">Data</code></td>
<td>

<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.
</p>
</td></tr>
<tr><td><code id="PenalizedRegressionBasedClustering_+3A_firstlambda">FirstLambda</code></td>
<td>

<p>Set 1 for quadratic penalty based algorithm, 0.4 for revised ADMM.
</p>
</td></tr>
<tr><td><code id="PenalizedRegressionBasedClustering_+3A_secondlambda">SecondLambda</code></td>
<td>

<p>The magnitude of grouping penalty.
</p>
</td></tr>
<tr><td><code id="PenalizedRegressionBasedClustering_+3A_tau">Tau</code></td>
<td>

<p>Tuning parameter: tau, related to grouping penalty.
</p>
</td></tr>
<tr><td><code id="PenalizedRegressionBasedClustering_+3A_plotit">PlotIt</code></td>
<td>

<p>Default: FALSE, if TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code>
</p>
</td></tr>
<tr><td><code id="PenalizedRegressionBasedClustering_+3A_...">...</code></td>
<td>

<p>Further arguments for <code><a href="prclust.html#topic+PRclust">PRclust</a></code>, enables also usage of [Pan et al., 2013].
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Parameters are rather challenging to choose.
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.
</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Data matrix is internally transposed in order to fit the definition of the algorithm.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Pan et al., 2013]  Pan, W., Shen, X., &amp; Liu, B.: Cluster analysis: unsupervised learning via supervised learning with a non-convex penalty, The Journal of Machine Learning Research, Vol. 14(1), pp. 1865-1889. 2013.
</p>
<p>[Wu et al., 2016]  Wu, C., Kwon, S., Shen, X., &amp; Pan, W.: A new algorithm and theory for penalized regression-based clustering, The Journal of Machine Learning Research, Vol. 17(1), pp. 6479-6503. 2016.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Hepta)
Data=Hepta$Data
out=PenalizedRegressionBasedClustering(Data,0.4,1,2,PlotIt=FALSE)
table(out$Cls,Hepta$Cls)
</code></pre>

<hr>
<h2 id='ProjectionPursuitClustering'>
Cluster Identification using Projection Pursuit as described in [Hofmeyr/Pavlidis, 2019].
</h2><span id='topic+ProjectionPursuitClustering'></span>

<h3>Description</h3>

<p>Summarizes recent projection pursuit methods for clustering based on [Hofmeyr/Pavlidis, 2015], [Hofmeyr, 2016] and [Pavlidis et al., 2016]  .
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ProjectionPursuitClustering(Data,ClusterNo,Type="MinimumDensity",

PlotIt=FALSE,PlotSolution=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ProjectionPursuitClustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.</p>
</td></tr>
<tr><td><code id="ProjectionPursuitClustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be built by the algorithm.</p>
</td></tr>
<tr><td><code id="ProjectionPursuitClustering_+3A_type">Type</code></td>
<td>

<p>Either  <code>MinimumDensity</code>[Pavlidis et al., 2016]
</p>
<p><code>MaximumClusterbility</code>[Hofmeyr/Pavlidis, 2015]], or
</p>
<p><code>NormalisedCut</code> [Hofmeyr, 2016] or KernelPCA [Hofmeyr/Pavlidis, 2019].
</p>
</td></tr>
<tr><td><code id="ProjectionPursuitClustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, if TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="ProjectionPursuitClustering_+3A_plotsolution">PlotSolution</code></td>
<td>

<p>Plots the partioning solution as a tree as described in 
</p>
</td></tr>
<tr><td><code id="ProjectionPursuitClustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The details of the options for projection pursuit and partioning of data are defined in [Hofmeyr/Pavlidis, 2019].
</p>
<p>&quot;KernelPCA&quot; uses additionally the package kernlab and is implemented as given in the fifth example on page 21, section &quot;extension&quot; of [Hofmeyr/Pavlidis, 2019].
</p>
<p>The first idea of using non-PCA projections for clustering was published by [Bock, 1987] as an definition. However, to the knowledge of the author it was not applied to any data. The first systematic comparison to Projection-Pursuit Methods <code><a href="#topic+ProjectionPursuitClustering">ProjectionPursuitClustering</a></code> and <code><a href="#topic+AutomaticProjectionBasedClustering">AutomaticProjectionBasedClustering</a></code> can be found in [Thrun/Ultsch, 2018]. For PCA-based clustering methods please see <code><a href="#topic+TandemClustering">TandemClustering</a></code>
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.
Points which cannot be assigned to a cluster will be reported with 0.
</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Hofmeyr/Pavlidis, 2015]  Hofmeyr, D., &amp; Pavlidis, N.: Maximum clusterability divisive clustering, Proc. 2015 IEEE Symposium Series on Computational Intelligence, pp. 780-786, IEEE, 2015.
</p>
<p>[Hofmeyr/Pavlidis, 2019]  Hofmeyr, D., &amp; Pavlidis, N.: PPCI: an R Package for Cluster Identification using Projection Pursuit, The R Journal, 2019.
</p>
<p>[Hofmeyr, 2016]  Hofmeyr, D. P.: Clustering by minimum cut hyperplanes, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 39(8), pp. 1547-1560. 2016.
</p>
<p>[Pavlidis et al., 2016]  Pavlidis, N. G., Hofmeyr, D. P., &amp; Tasoulis, S. K.: Minimum density hyperplanes, The Journal of Machine Learning Research, Vol. 17(1), pp. 5414-5446. 2016.
</p>
<p>[Thrun/Ultsch, 2018]  Thrun, M. C., &amp; Ultsch, A.: Using Projection based Clustering to Find Distance and Density based Clusters in High-Dimensional Data, Journal of Classification, Vol. in revision, 2018.
</p>
<p>[Bock, 1987]  Bock, H.: On the interface between cluster analysis, principal component analysis, and multidimensional scaling, Multivariate statistical modeling and data analysis, (pp. 17-34), Springer, 1987.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
out=ProjectionPursuitClustering(Hepta$Data,ClusterNo=7,PlotIt=FALSE)
</code></pre>

<hr>
<h2 id='QTclustering'>
Stochastic QT Clustering
</h2><span id='topic+QTclustering'></span><span id='topic+QTClustering'></span>

<h3>Description</h3>

<p>Stochastic quality clustering of [Heyer et al., 1999] with an improved implementation by [Scharl/Leisch, 2006].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>QTclustering(Data,Radius,PlotIt=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="QTclustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.</p>
</td></tr>
<tr><td><code id="QTclustering_+3A_radius">Radius</code></td>
<td>
<p>Maximum radius of clusters. If NULL, automatic estimation can be done with [Thrun et al., 2016] if not otherwise set.</p>
</td></tr>
<tr><td><code id="QTclustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, if TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="QTclustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n] numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.
Points which cannot be assigned to a cluster will be reported with 0.
</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Heyer et al., 1999]  Heyer, L. J., Kruglyak, S., &amp; Yooseph, S.: Exploring expression data: identification and analysis of coexpressed genes, Genome research, Vol. 9(11), pp. 1106-1115. 1999.
</p>
<p>[Scharl/Leisch, 2006]  Scharl, T., &amp; Leisch, F.: The stochastic QT-clust algorithm: evaluation of stability and variance on time-course microarray data, in Rizzi , A. &amp; Vichi, M. (eds.), Proc. Proceedings in Computational Statistics (Compstat), pp. 1015-1022, Physica Verlag, Heidelberg, Germany, 2006.
</p>
<p>[Thrun et al., 2016]  Thrun, M. C., Lerch, F., Loetsch, J., &amp; Ultsch, A. : Visualization and 3D Printing of Multivariate Data of Biomarkers, in Skala, V. (Ed.), International Conference in Central Europe on Computer Graphics, Visualization  and Computer Vision (WSCG), Vol. 24, Plzen, 2016. 
</p>
<p>[Ultsch, 2005]  Ultsch, A.: Pareto density estimation: A density estimation for knowledge discovery, In Baier, D. &amp; Werrnecke, K. D. (Eds.), Innovations in classification, data science, and information systems, (Vol. 27, pp. 91-100), Berlin, Germany, Springer, 2005.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
out=QTclustering(Hepta$Data,Radius=NULL,PlotIt=FALSE)
</code></pre>

<hr>
<h2 id='RobustTrimmedClustering'>Robust Trimmed Clustering</h2><span id='topic+RobustTrimmedClustering'></span>

<h3>Description</h3>

<p>Robust Trimmed Clustering invented by [Garcia-Escudero et al., 2008] and implemented by [Fritz et al., 2012].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RobustTrimmedClustering(Data, ClusterNo,

Alpha=0.05,PlotIt=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RobustTrimmedClustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.</p>
</td></tr>
<tr><td><code id="RobustTrimmedClustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be built by the algorithm.</p>
</td></tr>
<tr><td><code id="RobustTrimmedClustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, if TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="RobustTrimmedClustering_+3A_alpha">Alpha</code></td>
<td>

<p>No trimming is done equals to alpha =0, otherwise proportion of datapoints to be trimmed, <code><a href="tclust.html#topic+tclust">tclust</a></code> uses 0.05 as default.
</p>
</td></tr>
<tr><td><code id="RobustTrimmedClustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, e.g.  ,<code>nstart</code> (number of random initializations),<code>iter.max</code> (maximum number of concentration steps),<code>restr</code> and <code>restr.fact</code> described in details. If not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>&quot;This iterative algorithm initializes k clusters randomly and performs &quot;concentration steps&quot; in order to improve the current cluster assignment. The number of maximum concentration steps to be performed is given by iter.max. For approximately obtaining the global optimum, the system is initialized nstart times and concentration steps are performed until convergence or iter.max is reached. When processing more complex data sets higher values of nstart and iter.max have to be specified (obviously implying extra computation time). ... The larger <code>restr.fact</code> is chosen, the looser is the restriction on the scatter matrices, allowing for more heterogeneity among the clusters. On the contrary, small values of restr.fact close to 1 imply very equally scattered clusters. This idea of constraining cluster scatters to avoid spurious solutions goes back to Hathaway (1985), who proposed it in mixture fitting problems&quot; [Fritz et al., 2012]. The type of constraint <code>restr</code> can be set to &quot;eigen&quot;, &quot;deter&quot; or &quot;sigma.&quot;. Please see <code><a href="tclust.html#topic+tclust">tclust</a></code> for further parameter description.
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun</p>


<h3>References</h3>

<p>[Garcia-Escudero et al., 2008]  Garcia-Escudero, L. A., Gordaliza, A., Matran, C., &amp; Mayo-Iscar, A.: A general trimming approach to robust cluster analysis, The annals of Statistics, Vol. 36(3), pp. 1324-1345. 2008.
</p>
<p>[Fritz et al., 2012]  Fritz, H., Garcia-Escudero, L. A., &amp; Mayo-Iscar, A.: tclust: An R package for a trimming approach to cluster analysis, Journal of statistical Software, Vol. 47(12), pp. 1-26. 2012.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
out=RobustTrimmedClustering(Hepta$Data,ClusterNo=7,Alpha=0,PlotIt=FALSE)
</code></pre>

<hr>
<h2 id='SharedNearestNeighborClustering'>
SNN clustering 
</h2><span id='topic+SharedNearestNeighborClustering'></span>

<h3>Description</h3>

<p>Shared Nearest Neighbor Clustering of [Ertoz et al., 2003].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SharedNearestNeighborClustering(Data,Knn,

Radius,minPts,PlotIt=FALSE,

UpperLimitRadius,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SharedNearestNeighborClustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.</p>
</td></tr>
<tr><td><code id="SharedNearestNeighborClustering_+3A_knn">Knn</code></td>
<td>

<p>Number of neighbors to consider to calculate the shared nearest neighbors.
</p>
</td></tr>
<tr><td><code id="SharedNearestNeighborClustering_+3A_radius">Radius</code></td>
<td>

<p>Eps [Ester et al., 1996, p. 227] neighborhood in the R-ball graph/unit disk graph), size of the epsilon neighborhood.
If NULL, automatic estimation is done using insights of [Ultsch, 2005].
</p>
</td></tr>
<tr><td><code id="SharedNearestNeighborClustering_+3A_minpts">minPts</code></td>
<td>

<p>Number of minimum points in the eps region (for core points). 
In principle minimum number of points in the unit disk, if the unit disk is within the cluster (core) [Ester et al., 1996, p. 228].
if NULL, its 2.5 percent of points.
</p>
</td></tr>
<tr><td><code id="SharedNearestNeighborClustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, if TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="SharedNearestNeighborClustering_+3A_upperlimitradius">UpperLimitRadius</code></td>
<td>
<p>Limit for radius search, experimental</p>
</td></tr>
<tr><td><code id="SharedNearestNeighborClustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>..
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector defining the clustering; this classification is the main output of the algorithm. Points which cannot be assigned to a cluster will be reported as members of the noise cluster with 0.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Ertoz et al., 2003]   Levent Ertoz, Michael Steinbach, Vipin Kumar: Finding Clusters of Different Sizes, Shapes, and Densities in Noisy, High Dimensional Data, SIAM International Conference on Data Mining, 47-59, 2003.
</p>


<h3>See Also</h3>

<p><code><a href="dbscan.html#topic+sNNclust">sNNclust</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
out=SharedNearestNeighborClustering(
Hepta$Data, Knn=7,Radius=NULL,minPts=NULL,PlotIt = FALSE)
</code></pre>

<hr>
<h2 id='SOMclustering'>
self-organizing maps based clustering implemented by [Wherens, Buydens, 2017]. 
</h2><span id='topic+SOMclustering'></span>

<h3>Description</h3>

<p>Either the variant k-batch or k-online is possible in which every unit can be seen approximately as an cluster.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SOMclustering(Data,LC=c(1,2),ClusterNo=NULL,

Mode="online",PlotIt=FALSE,rlen=100,alpha = c(0.05, 0.01),...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SOMclustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.</p>
</td></tr>
<tr><td><code id="SOMclustering_+3A_lc">LC</code></td>
<td>
<p>Lines and Columns of a very small SOM, usually every unit is a cluster, will be ignored if ClusterNo is not NULL.</p>
</td></tr>
<tr><td><code id="SOMclustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>Optional, A number k which defines k different clusters to be built by the algorithm. LC will then be set accordingly.</p>
</td></tr>
<tr><td><code id="SOMclustering_+3A_mode">Mode</code></td>
<td>
<p>Either &quot;batch&quot; or &quot;online&quot;</p>
</td></tr>
<tr><td><code id="SOMclustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, if TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="SOMclustering_+3A_rlen">rlen</code></td>
<td>
<p>Please see <code><a href="kohonen.html#topic+supersom">supersom</a></code></p>
</td></tr>
<tr><td><code id="SOMclustering_+3A_alpha">alpha</code></td>
<td>
<p>Please see <code><a href="kohonen.html#topic+supersom">supersom</a></code></p>
</td></tr>
<tr><td><code id="SOMclustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm in 
<code><a href="kohonen.html#topic+unit.distances">somgrid</a></code>, if not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This clustering algorithm is based on very small maps and, hence, not emergent (c.f. [Thrun, 2018, p.37]). A 3x3 map means 9 units leading to 9 clusters.
</p>
<p>Batch is a deterministic clustering approach whereas online is a stochastic clustering approach and research indicates that online should be preferred (c.f. [Thrun, 2018, p.37]).
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector defining the classification as the main output of the clustering algorithm</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Wherens, Buydens, 2017] R. Wehrens and L.M.C. Buydens, J. Stat. Softw. 21 (5), 2007; R. Wehrens and J. Kruisselbrink, submitted, 2017.
</p>
<p>[Thrun, 2018] Thrun, M.C., Projection Based Clustering through Self-Organization and Swarm Intelligence. 2018, Heidelberg: Springer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
out=SOMclustering(Hepta$Data,ClusterNo=7,PlotIt=FALSE)
</code></pre>

<hr>
<h2 id='SOTAclustering'>SOTA Clustering</h2><span id='topic+SOTAclustering'></span><span id='topic+sotaClustering'></span>

<h3>Description</h3>

<p>Self-organizing Tree Algorithm (SOTA) introduced by [Herrero et al., 2001].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SOTAclustering(Data, ClusterNo,PlotIt=FALSE,UnrestGrowth,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SOTAclustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.</p>
</td></tr>
<tr><td><code id="SOTAclustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be built by the algorithm.</p>
</td></tr>
<tr><td><code id="SOTAclustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, if TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="SOTAclustering_+3A_unrestgrowth">UnrestGrowth</code></td>
<td>

<p>TRUE: forces the <code>ClusterNo</code> option to uphold.
FALSE: enables the algorithm to find its own number of clusters, in this cases ClusterNo should contain a high number because it is internally set as the number of iterations which is either reached or the max diversity criteria is satisfied priorly.
</p>
</td></tr>
<tr><td><code id="SOTAclustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>sotaObject</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Note</h3>

<p>*Luis Winckelman intergrated several function from clValid because it's ORPHANED.
</p>


<h3>Author(s)</h3>

<p>Luis Winckelmann*, Vasyl Pihur, Guy Brock, Susmita Datta, Somnath Datta</p>


<h3>References</h3>

<p>[Herrero et al., 2001]  Herrero, J., Valencia, A., &amp; Dopazo, J.: A hierarchical unsupervised growing neural network for clustering gene expression patterns, Bioinformatics, Vol. 17(2), pp. 126-136. 2001.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Does Work
data('Hepta')
out=SOTAclustering(Hepta$Data,ClusterNo=7)
table(Hepta$Cls,out$Cls)


#Does not work well
data('Lsun3D')
out=SOTAclustering(Lsun3D$Data,ClusterNo=100,PlotIt=FALSE,UnrestGrowth=FALSE)

</code></pre>

<hr>
<h2 id='SparseClustering'>
Sparse Clustering
</h2><span id='topic+SparseClustering'></span>

<h3>Description</h3>

<p>Implements the sparse clustering methods of [Witten/Tibshirani, 2010].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SparseClustering(DataOrDistances, ClusterNo, Type="Hierarchical",

PlotIt=F,Silent=FALSE, NoPerms=10,Wbounds, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SparseClustering_+3A_dataordistances">DataOrDistances</code></td>
<td>
<p>Either a [1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.
</p>
<p>or a [1:n,1:n] symmetric distance matrix.
</p>
</td></tr>
<tr><td><code id="SparseClustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>Numeric indicating number to cluster to find in Tree/
Dendrogramm in case of Type=&quot;Hierachical&quot; or numer of cluster to use in
Type=&quot;kmeans&quot;</p>
</td></tr>
<tr><td><code id="SparseClustering_+3A_type">Type</code></td>
<td>
<p>(optional) Char selecting methods Hierarchical or kmeans.
Default: &quot;Hierarchical&quot;</p>
</td></tr>
<tr><td><code id="SparseClustering_+3A_plotit">PlotIt</code></td>
<td>
<p>(optional) Boolean. Default = FALSE = No plotting performed.</p>
</td></tr>
<tr><td><code id="SparseClustering_+3A_silent">Silent</code></td>
<td>
<p>(optional) Boolean: print output or not (Default = FALSE = no
output)</p>
</td></tr>
<tr><td><code id="SparseClustering_+3A_noperms">NoPerms</code></td>
<td>
<p>(optional), numeric scalar, Number of permutations.</p>
</td></tr>
<tr><td><code id="SparseClustering_+3A_wbounds">Wbounds</code></td>
<td>
<p>(optional) numeric vector, range of tuning parameters to consider. This is the L1 bound on w, the feature weights [Witten/Tibshirani, 2010].</p>
</td></tr>
<tr><td><code id="SparseClustering_+3A_...">...</code></td>
<td>
<p>Further arguments passed on to sparcl <a href="sparcl.html#topic+HierarchicalSparseCluster">HierarchicalSparseCluster</a> or <a href="sparcl.html#topic+KMeansSparseCluster">KMeansSparseCluster</a> depending on <code>Type</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
<tr><td><code>Tree</code></td>
<td>
<p>Object Tree if Type=&quot;Hierachical&quot; is used.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Quality of clustering results varies between sparse hierarchical if data is given in comparison to the case that distances are given.</p>


<h3>Author(s)</h3>

<p>Quirin Stier, Michael Thrun
</p>


<h3>References</h3>

<p>[Witten/Tibshirani, 2010] Witten, D. and Tibshirani, R.: A Framework for
Feature Selection in Clustering. Journal of the American Statistical
Association, Vol. 105(490), pp. 713-726, 2010.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Hepta
data("Hepta")
Data = Hepta$Data
V1 = SparseClustering(Data, ClusterNo=7, Type="kmeans")
Cls1 = V1$Cls

V2 = SparseClustering(Data, ClusterNo=7, Type="Hierarchical")
Cls2 = V2$Cls

InputDistances = parallelDist::parDist(Data, method="euclidean")
DistanceMatrix = as.matrix(InputDistances)
V3 = SparseClustering(DistanceMatrix, ClusterNo=7, Type="Hierarchical")
Cls3 = V3$Cls

## Not run: 
set.seed(1)
Data = matrix(rnorm(100*50),ncol=50)
y    = c(rep(1,50),rep(2,50))
Data[y==1,1:25] = Data[y==1,1:25]+2

V1 = SparseClustering(Data, ClusterNo=2, Type="kmeans")
Cls1 = V1$Cls

## End(Not run)

</code></pre>

<hr>
<h2 id='SpectralClustering'> Spectral Clustering </h2><span id='topic+SpectralClustering'></span>

<h3>Description</h3>

<p>Clusters the Data into &quot;ClusterNo&quot; different clusters using the Spectral Clustering method</p>


<h3>Usage</h3>

<pre><code class='language-R'>SpectralClustering(Data, ClusterNo,PlotIt=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SpectralClustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.</p>
</td></tr>
<tr><td><code id="SpectralClustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be built by the algorithm.</p>
</td></tr>
<tr><td><code id="SpectralClustering_+3A_plotit">PlotIt</code></td>
<td>
<p>default: FALSE, if TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="SpectralClustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.
e.g.:
</p>
<p><code>kernel</code> : Kernelmethod, possible options: rbfdot          Radial Basis kernel function &quot;Gaussian&quot; polydot         Polynomial kernel function vanilladot      Linear kernel function tanhdot         Hyperbolic tangent kernel function laplacedot      Laplacian kernel function besseldot       Bessel kernel function anovadot        ANOVA RBF kernel function splinedot       Spline kernel stringdot       String kernel
</p>
<p><code>kpar</code> : Kernelparameter: a character string or the list of hyper-parameters (kernel parameters). The default character string &quot;automatic&quot; uses a heuristic to determine a suitable value for the width parameter of the RBF kernel. &quot;local&quot; (local scaling) uses a more advanced heuristic and sets a width parameter for every point in the data set. A list can also be used containing the parameters to be used with the kernel function.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun</p>


<h3>References</h3>

<p>[Ng et al., 2002]  Ng, A. Y., Jordan, M. I., &amp; Weiss, Y.: On spectral clustering: Analysis and an algorithm, Advances in neural information processing systems, Vol. 2, pp. 849-856. 2002.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
out=SpectralClustering(Hepta$Data,ClusterNo=7,PlotIt=FALSE)
</code></pre>

<hr>
<h2 id='Spectrum'>
Fast Adaptive Spectral Clustering [John et al, 2020]
</h2><span id='topic+Spectrum'></span>

<h3>Description</h3>

<p>Spectrum is a self-tuning spectral clustering method for single or multi-view data. In this wrapper restricted to the standard use in other clustering algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Spectrum(Data, Type = 2, ClusterNo = NULL, 

PlotIt = FALSE, Silent = TRUE,PlotResults = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Spectrum_+3A_data">Data</code></td>
<td>

<p>1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.
</p>
</td></tr>
<tr><td><code id="Spectrum_+3A_type">Type</code></td>
<td>

<p>Type=1: default eigengap method (Gaussian clusters)
</p>
<p>Type=2: multimodality gap method (Gaussian/ non-Gaussian clusters)
</p>
<p>Type=3: Allows to setClusterNo
</p>
</td></tr>
<tr><td><code id="Spectrum_+3A_clusterno">ClusterNo</code></td>
<td>
<p>Optional, A number k which defines k different clusters to be built by the algorithm.
For default <code>ClusterNo=NULL</code> please see details.
</p>
</td></tr>
<tr><td><code id="Spectrum_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, If TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="Spectrum_+3A_silent">Silent</code></td>
<td>

<p>Silent progress of algorithm=TRUE
</p>
</td></tr>
<tr><td><code id="Spectrum_+3A_plotresults">PlotResults</code></td>
<td>

<p>Plots result of spectrum with plot function
</p>
</td></tr>
<tr><td><code id="Spectrum_+3A_...">...</code></td>
<td>

<p>Method: numerical value: 1 = default eigengap method (Gaussian clusters), 2 = multimodality
gap method (Gaussian/ non-Gaussian clusters), 3 = no automatic
method (see fixk param)
</p>
<p>Other parameters defined in Spectrum packages
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Spectrum is a partitioning algorithm and either uses the eigengap or multimodality gap heuristics to determine the number of clusters, please see Spectrum package for details
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[John et al, 2020] John, C. R., Watson, D., Barnes, M. R., Pitzalis, C., &amp; Lewis, M. J.: Spectrum: Fast density-aware spectral clustering for single and multi-omic data. Bioinformatics, Vol. 36(4), pp. 1159-1166, 2020.
</p>


<h3>See Also</h3>

<p><code><a href="Spectrum.html#topic+Spectrum">Spectrum</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
out=Spectrum(Hepta$Data,PlotIt=FALSE)

out=Spectrum(Hepta$Data,PlotIt=TRUE)

</code></pre>

<hr>
<h2 id='StatPDEdensity'>
Pareto Density Estimation
</h2><span id='topic+StatPDEdensity'></span>

<h3>Description</h3>

<p>Density estimation for ggplot with a clear model behind it.
</p>


<h3>Format</h3>

<p>The format is:
Classes 'StatPDEdensity', 'Stat', 'ggproto' &lt;ggproto object: Class StatPDEdensity, Stat&gt;
aesthetics: function
compute_group: function
compute_layer: function
compute_panel: function
default_aes: uneval
extra_params: na.rm
finish_layer: function
non_missing_aes: 
parameters: function
required_aes: x y
retransform: TRUE
setup_data: function
setup_params: function
super:  &lt;ggproto object: Class Stat&gt; 
</p>


<h3>Details</h3>

<p>PDE was published in [Ultsch, 2005], short explanation in [Thrun, Ultsch 2018] and the PDE optimized violin plot was published in [Thrun et al., 2018].
</p>


<h3>References</h3>

<p>[Ultsch,2005]  Ultsch, A.: Pareto density estimation: A density estimation for knowledge discovery, in  Baier, D.; Werrnecke, K. D., (Eds), Innovations in classification, data science, and information systems, Proc Gfkl 2003, pp 91-100, Springer, Berlin, 2005.
</p>
<p>[Thrun, Ultsch 2018]   Thrun, M. C., &amp; Ultsch, A. : Effects of the payout system of income taxes to municipalities in Germany, in Papiez, M. &amp; Smiech,, S. (eds.), Proc. 12th Professor Aleksander Zelias International Conference on Modelling and Forecasting of Socio-Economic Phenomena, pp. 533-542, Cracow: Foundation of the Cracow University of Economics, Cracow, Poland, 2018. 
</p>
<p>[Thrun et al, 2018]	Thrun, M. C., Pape, F., &amp; Ultsch, A. : Benchmarking Cluster Analysis Methods using PDE-Optimized Violin Plots, Proc. European Conference on Data Analysis (ECDA), accepted, Paderborn, Germany, 2018. 
</p>

<hr>
<h2 id='SubspaceClustering'>Algorithms for Subspace clustering</h2><span id='topic+SubspaceClustering'></span>

<h3>Description</h3>

<p>Subspace (projected) clustering is a technique which finds clusters within different subspaces (a selection of one or more dimensions).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SubspaceClustering(Data,ClusterNo,DimSubspace,

Type='Orclus',PlotIt=FALSE,OrclusInitialClustersNo=ClusterNo+2,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SubspaceClustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases or d-dimensional data points. Every case has d attributes, variables or features.</p>
</td></tr>
<tr><td><code id="SubspaceClustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be built by the proclus or orclust algorithm.</p>
</td></tr>
<tr><td><code id="SubspaceClustering_+3A_dimsubspace">DimSubspace</code></td>
<td>
<p>Numerical number defining the dimensionality in which clusters should be search in in the orclust algorithm, for proclus it is an optional parameter</p>
</td></tr>
<tr><td><code id="SubspaceClustering_+3A_type">Type</code></td>
<td>

<p>'Orclus',  subspace clustering based on arbitrarily oriented projected cluster generation [Aggarwal and Yu, 2000]
</p>
<p>'ProClus' ProClus algorithm for subspace clustering [Aggarwal/Wolf, 1999] 
</p>
<p>'Clique' ProClus algorithm finds subspaces of high-density clusters [Agrawal et al., 1999] and [Agrawal et al., 2005]
</p>
<p>'SubClu' SubClu algorithm is a density-connected approach for subspace clustering [Kailing et al.,2004]  
</p>
</td></tr>
<tr><td><code id="SubspaceClustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, if TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="SubspaceClustering_+3A_orclusinitialclustersno">OrclusInitialClustersNo</code></td>
<td>
<p>Only for Orclus algorithm: Initial number of clusters (that are computed in the entire data space) must be greater than k. The number of clusters is iteratively decreased by a factor until the final number of k clusters is reached.</p>
</td></tr>
<tr><td><code id="SubspaceClustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.
</p>
<p>For Subclue: &quot;epsilon&quot; and &quot;minSupport&quot;, see <code><a href="#topic+DBSCAN">DBSCAN</a></code>
</p>
<p>For Clique: &quot;xi&quot; (number of intervals for each dimension) and &quot;tau&quot; (Density Threshold), see <code><a href="#topic+DBSCAN">DBSCAN</a></code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Subspace clustering algorithms have the goal to finde one or more subspaces with the assumation that sufficient dimensionality reduction is dimensionality reduction without loss of information. Hence subspace clustering aums at finding a linear subspace sucht that the subspace contains as much predictive information as the input space. The subspace is usually higher than two but lower than the input space. In contrast, projection-based clustering <code><a href="#topic+AutomaticProjectionBasedClustering">AutomaticProjectionBasedClustering</a></code> projects the data (nonlinear) into two dimensions and tries only to preerve relevant neighborhoods.
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Note</h3>

<p>JAVA_HOME has to be set for rJava to the ProClus algorithm (in windows set PATH env. variable to .../bin path of Java. The architecture of R and Java have to match. Java automatically downloads the Java version of the browser which may not be installed in the architecture in R. In such a case choose a Java version manually.
</p>


<h3>Author(s)</h3>

<p>Michael Thrun</p>


<h3>References</h3>

<p>[Aggarwal/Wolf et al., 1999]  Aggarwal, C. C., Wolf, J. L., Yu, P. S., Procopiuc, C., &amp; Park, J. S.: Fast algorithms for projected clustering, Proc. ACM SIGMoD Record, Vol. 28, pp. 61-72, ACM, 1999.
</p>
<p>[Aggarwal/Yu, 2000]  Aggarwal, C. C., &amp; Yu, P. S.: Finding generalized projected clusters in high dimensional spaces, (Vol. 29), ACM, ISBN: 1581132174, 2000.
</p>
<p>[Agrawal et al., 1999]: Rakesh Agrawal, Johannes Gehrke, Dimitrios Gunopulos, and Prabhakar Raghavan: Automatic Subspace Clustering of High Dimensional Data for Data Mining Applications, In Proc. ACM SIGMOD, 1999.
</p>
<p>[Agrawal et al., 2005]  Agrawal, R., Gehrke, J., Gunopulos, D., &amp; Raghavan, P.: Automatic subspace clustering of high dimensional data, Data Mining and Knowledge Discovery, Vol. 11(1), pp. 5-33. 2005.
</p>
<p>[Kailing et al.,2004]  Kailing, Karin, Hans-Peter Kriegel, and Peer Kroeger: Density-connected subspace clustering for high-dimensional data, Proceedings of the 2004 SIAM international conference on data mining. Society for Industrial and Applied Mathematics, 2004
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
out=SubspaceClustering(Hepta$Data,ClusterNo=7,PlotIt=FALSE)
</code></pre>

<hr>
<h2 id='TandemClustering'>
Tandem Clustering
</h2><span id='topic+TandemClustering'></span>

<h3>Description</h3>

<p>Summarizes clustering methods that combine k-means and pca
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TandemClustering(Data,ClusterNo,Type="Reduced",PlotIt=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TandemClustering_+3A_data">Data</code></td>
<td>
<p>[1:n,1:d] matrix of dataset to be clustered. It consists of n cases of d-dimensional data points. Every case has d attributes, variables or features.</p>
</td></tr>
<tr><td><code id="TandemClustering_+3A_clusterno">ClusterNo</code></td>
<td>
<p>A number k which defines k different clusters to be built by the algorithm.</p>
</td></tr>
<tr><td><code id="TandemClustering_+3A_type">Type</code></td>
<td>

<p><code>Reduced</code>: Reduced k-means (RKM) [De Soete/Carroll, 1994].
</p>
<p><code>Factorial</code>: Factorial k-mean (FKM) [Vichi/Kiers, 2001] 
</p>
<p><code>KernelPCA</code>: Kernel PCA with minimum normalised cut hyperplanes [Hofmeyr/Pavlidis, 2019] 
</p>
</td></tr>
<tr><td><code id="TandemClustering_+3A_plotit">PlotIt</code></td>
<td>
<p>Default: FALSE, if TRUE plots the first three dimensions of the dataset with colored three-dimensional data points defined by the clustering stored in <code>Cls</code></p>
</td></tr>
<tr><td><code id="TandemClustering_+3A_...">...</code></td>
<td>
<p>Further arguments to be set for the clustering algorithm, if not set, default arguments are used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the ClusterNo exceeds the number of dimensions, than the function is called recursively with ClusterNo=2. In each iteration the cluster with the highest number of overall points is clustered again, until the number of clusters is met. 
</p>
<p>&quot;KernelPCA&quot; uses addtionally the package kernlab and is implemented as given in the fifth example on page 18, section &quot;extension&quot; of [Hofmeyr/Pavlidis, 2019] 
</p>
<p>The first idea of using non-PCA projections for clustering was published by [Bock, 1987] as an definition. However, to the knowledge of the author it was not applied to any data. The first systematic comparison to Projection-Pursuit Methods <code><a href="#topic+ProjectionPursuitClustering">ProjectionPursuitClustering</a></code> and <code><a href="#topic+AutomaticProjectionBasedClustering">AutomaticProjectionBasedClustering</a></code> can be found in [Thrun/Ultsch, 2018]. 
</p>


<h3>Value</h3>

<p>List of
</p>
<table>
<tr><td><code>Cls</code></td>
<td>
<p>[1:n]  numerical vector with n numbers defining the classification as the main output of the clustering algorithm. It has k unique numbers representing the arbitrary labels of the clustering.
Points which cannot be assigned to a cluster will be reported with 0.
</p>
</td></tr>
<tr><td><code>Object</code></td>
<td>
<p>Object defined by clustering algorithm as the other output of this algorithm</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[De Soete/Carroll, 1994]  De Soete, G., &amp; Carroll, J. D.: K-means clustering in a low-dimensional Euclidean space, New approaches in classification and data analysis, (pp. 212-219), Springer, 1994.
</p>
<p>[Hofmeyr/Pavlidis, 2019]  Hofmeyr, D., &amp; Pavlidis, N.: PPCI: an R Package for Cluster Identification using Projection Pursuit, The R Journal, 2019.
</p>
<p>[Vichi/Kiers, 2001]  Vichi, M., &amp; Kiers, H. A.: Factorial k-means analysis for two-way data, Computational Statistics &amp; Data Analysis, Vol. 37(1), pp. 49-64. 2001.
</p>
<p>[Thrun/Ultsch, 2018]  Thrun, M. C., &amp; Ultsch, A.: Using Projection based Clustering to Find Distance and Density based Clusters in High-Dimensional Data, Journal of Classification, Vol. in revision, 2018.
</p>
<p>[Bock, 1987]  Bock, H.: On the interface between cluster analysis, principal component analysis, and multidimensional scaling, Multivariate statistical modeling and data analysis, (pp. 17-34), Springer, 1987.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data('Hepta')
out=TandemClustering(Hepta$Data,ClusterNo=7,PlotIt=FALSE)
</code></pre>

<hr>
<h2 id='Target'>
Target introduced in [Ultsch, 2005].
</h2><span id='topic+Target'></span>

<h3>Description</h3>

<p>Detailed description of dataset and its clustering challenge of outliers is provided in [Thrun/Ultsch, 2020]
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("Target")</code></pre>


<h3>Details</h3>

<p>Size 770, Dimensions 2, stored in <code>Target$Data</code>
</p>
<p>Classes 6, stored in <code>Target$Cls</code>
</p>


<h3>References</h3>

<p>[Ultsch, 2005]  Ultsch, A.: U* C: Self-organized Clustering with Emergent Feature Maps, Proc. Lernen, Wissensentdeckung und Adaptivitaet (LWA/FGML), pp. 240-244, Saarbruecken, Germany, 2005.
</p>
<p>[Thrun/Ultsch, 2020]  Thrun, M. C., &amp; Ultsch, A.: Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems, Data in Brief, Vol. 30(C), pp. 105501, <a href="https://doi.org/10.1016/j.dib.2020.105501">doi:10.1016/j.dib.2020.105501</a>, 2020.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Target)
str(Target)
</code></pre>

<hr>
<h2 id='Tetra'>
Tetra introduced in [Ultsch, 1993]
</h2><span id='topic+Tetra'></span>

<h3>Description</h3>

<p>Almost touching clusters. Detailed description of dataset and its clustering challenge is provided in [Thrun/Ultsch, 2020].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("Tetra")</code></pre>


<h3>Details</h3>

<p>Size 400, Dimensions 3, stored in <code>Tetra$Data</code>
</p>
<p>Classes 4, stored in <code>Tetra$Cls</code>
</p>


<h3>References</h3>

<p>[Ultsch, 1993]  Ultsch, A.: Self-organizing neural networks for visualisation and classification, Information and classification, (pp. 307-313), Springer, 1993.
</p>
<p>[Thrun/Ultsch, 2020]  Thrun, M. C., &amp; Ultsch, A.: Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems, Data in Brief, Vol. 30(C), pp. 105501, <a href="https://doi.org/10.1016/j.dib.2020.105501">doi:10.1016/j.dib.2020.105501</a>, 2020.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Tetra)
str(Tetra)
</code></pre>

<hr>
<h2 id='TwoDiamonds'>
TwoDiamonds introduced in  [Ultsch, 2003a, 2003b]
</h2><span id='topic+TwoDiamonds'></span>

<h3>Description</h3>

<p>Cluster border defined by density. Detailed description of dataset and its clustering challenge is provided in [Thrun/Ultsch, 2020].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("TwoDiamonds")</code></pre>


<h3>Details</h3>

<p>Size 800, Dimensions 2, stored in <code>TwoDiamonds$Data</code>
</p>
<p>Classes 2, stored in <code>TwoDiamonds$Cls</code>
</p>


<h3>References</h3>

<p>[Ultsch, 2003a]  Ultsch, A.Optimal density estimation in data containing clusters of unknown structure, technical report, Vol. 34,University of Marburg, Department of Mathematics and Computer Science, 2003.
</p>
<p>[Ultsch, 2003b]  Ultsch, A.: U*-matrix: a tool to visualize clusters in high dimensional data, Fachbereich Mathematik und Informatik, 2003.
</p>
<p>[Thrun/Ultsch, 2020]  Thrun, M. C., &amp; Ultsch, A.: Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems, Data in Brief, Vol. 30(C), pp. 105501, <a href="https://doi.org/10.1016/j.dib.2020.105501">doi:10.1016/j.dib.2020.105501</a>, 2020.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(TwoDiamonds)
str(TwoDiamonds)
</code></pre>

<hr>
<h2 id='WingNut'>
WingNut introduced in [Ultsch, 2005]
</h2><span id='topic+WingNut'></span>

<h3>Description</h3>

<p>Density vs. distance. Detailed description of dataset and its clustering challenge is provided in [Thrun/Ultsch, 2020].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("WingNut")</code></pre>


<h3>Details</h3>

<p>Size 1016, Dimensions 2, stored in <code>WingNut$Data</code>
</p>
<p>Classes 2, stored in <code>WingNut$Cls</code>
</p>


<h3>References</h3>

<p>[Ultsch, 2005]  Ultsch, A.: Clustering wih SOM: U* C, Proc. Proceedings of the 5th Workshop on Self-Organizing Maps, Vol. 2, pp. 75-82, 2005.
</p>
<p>[Thrun/Ultsch, 2020]  Thrun, M. C., &amp; Ultsch, A.: Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems, Data in Brief, Vol. 30(C), pp. 105501, <a href="https://doi.org/10.1016/j.dib.2020.105501">doi:10.1016/j.dib.2020.105501</a>, 2020.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(WingNut)
str(WingNut)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
