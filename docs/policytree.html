<!DOCTYPE html><html><head><title>Help for package policytree</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {policytree}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#policytree-package'><p>policytree: Policy Learning via Doubly Robust Empirical Welfare Maximization over Trees</p></a></li>
<li><a href='#conditional_means.causal_forest'><p>Estimate mean rewards <code class="reqn">\mu</code> for each treatment <code class="reqn">a</code></p></a></li>
<li><a href='#create_dot_body'><p>Writes each node information</p>
If it is a leaf node: show it in different color, show number of samples, show leaf id
If it is a non-leaf node: show its splitting variable and splitting value</a></li>
<li><a href='#double_robust_scores.causal_forest'><p>Matrix <code class="reqn">\Gamma</code> of scores for each treatment <code class="reqn">a</code></p></a></li>
<li><a href='#export_graphviz'><p>Export a tree in DOT format.</p>
This function generates a GraphViz representation of the tree,
which is then written into <code>dot_string</code>.</a></li>
<li><a href='#gen_data_epl'><p>Example data generating process from Policy Learning With Observational Data</p></a></li>
<li><a href='#gen_data_mapl'><p>Example data generating process from Offline Multi-Action Policy Learning: Generalization and Optimization</p></a></li>
<li><a href='#hybrid_policy_tree'><p>Hybrid tree search</p></a></li>
<li><a href='#make_tree'><p>A utility function for generating random trees for test purposes.</p></a></li>
<li><a href='#multi_causal_forest'><p>(deprecated) One vs. all causal forest for multiple treatment effect estimation</p></a></li>
<li><a href='#plot.policy_tree'><p>Plot a policy_tree tree object.</p></a></li>
<li><a href='#policy_tree'><p>Fit a policy with exact tree search</p></a></li>
<li><a href='#predict_test_tree'><p>Predict with the above test tree.</p></a></li>
<li><a href='#predict.policy_tree'><p>Predict method for policy_tree</p></a></li>
<li><a href='#print.policy_tree'><p>Print a policy_tree object.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Policy Learning via Doubly Robust Empirical Welfare Maximization
over Trees</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.2</td>
</tr>
<tr>
<td>Description:</td>
<td>Learn optimal policies via doubly robust empirical
 welfare maximization over trees. Given doubly robust reward estimates, this package
 finds a rule-based treatment prescription policy, where the policy takes the form of
 a shallow decision tree that is globally (or close to) optimal.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 3.0.4), DiagrammeR</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, BH</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, grf (&ge; 2.0.0)</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/grf-labs/policytree">https://github.com/grf-labs/policytree</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/grf-labs/policytree/issues">https://github.com/grf-labs/policytree/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-06-23 02:02:26 UTC; erikcs</td>
</tr>
<tr>
<td>Author:</td>
<td>Erik Sverdrup [aut, cre],
  Ayush Kanodia [aut],
  Zhengyuan Zhou [aut],
  Susan Athey [aut],
  Stefan Wager [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Erik Sverdrup &lt;erikcs@stanford.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-06-23 04:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='policytree-package'>policytree: Policy Learning via Doubly Robust Empirical Welfare Maximization over Trees</h2><span id='topic+policytree'></span><span id='topic+policytree-package'></span>

<h3>Description</h3>

<p>A package for learning simple rule-based policies, where the rule takes the form of a shallow decision tree. Applications include settings which require interpretable predictions, such as for example a medical treatment prescription. This package uses doubly robust reward estimates from <code>grf</code> to find a shallow, but globally optimal decision tree.
</p>
<p>Some helpful links for getting started:
</p>

<ul>
<li><p> The R package documentation contains usage examples and method references (<a href="https://grf-labs.github.io/policytree/">https://grf-labs.github.io/policytree/</a>).
</p>
</li>
<li><p> For community questions and answers around usage, see the GitHub issues page (<a href="https://github.com/grf-labs/policytree/issues">https://github.com/grf-labs/policytree/issues</a>).
</p>
</li></ul>



<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Erik Sverdrup <a href="mailto:erikcs@stanford.edu">erikcs@stanford.edu</a>
</p>
<p>Authors:
</p>

<ul>
<li><p> Ayush Kanodia
</p>
</li>
<li><p> Zhengyuan Zhou
</p>
</li>
<li><p> Susan Athey
</p>
</li>
<li><p> Stefan Wager
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/grf-labs/policytree">https://github.com/grf-labs/policytree</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/grf-labs/policytree/issues">https://github.com/grf-labs/policytree/issues</a>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
# Multi-action policy learning example.
n &lt;- 250
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
W &lt;- as.factor(sample(c("A", "B", "C"), n, replace = TRUE))
Y &lt;- X[, 1] + X[, 2] * (W == "B") + X[, 3] * (W == "C") + runif(n)
multi.forest &lt;- grf::multi_arm_causal_forest(X, Y, W)

# Compute doubly robust reward estimates.
Gamma.matrix &lt;- double_robust_scores(multi.forest)

# Fit a depth 2 tree on a random training subset.
train &lt;- sample(1:n, 200)
opt.tree &lt;- policy_tree(X[train, ], Gamma.matrix[train, ], depth = 2)
opt.tree

# Predict treatment on held out data.
predict(opt.tree, X[-train, ])


</code></pre>

<hr>
<h2 id='conditional_means.causal_forest'>Estimate mean rewards <code class="reqn">\mu</code> for each treatment <code class="reqn">a</code></h2><span id='topic+conditional_means.causal_forest'></span><span id='topic+conditional_means.causal_survival_forest'></span><span id='topic+conditional_means.instrumental_forest'></span><span id='topic+conditional_means.multi_arm_causal_forest'></span><span id='topic+conditional_means'></span>

<h3>Description</h3>

<p><code class="reqn">\mu_a = m(x) + (1-e_a(x))\tau_a(x)</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'causal_forest'
conditional_means(object, ...)

## S3 method for class 'causal_survival_forest'
conditional_means(object, ...)

## S3 method for class 'instrumental_forest'
conditional_means(object, ...)

## S3 method for class 'multi_arm_causal_forest'
conditional_means(object, outcome = 1, ...)

conditional_means(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="conditional_means.causal_forest_+3A_object">object</code></td>
<td>
<p>An appropriate causal forest type object</p>
</td></tr>
<tr><td><code id="conditional_means.causal_forest_+3A_...">...</code></td>
<td>
<p>Additional arguments</p>
</td></tr>
<tr><td><code id="conditional_means.causal_forest_+3A_outcome">outcome</code></td>
<td>
<p>Only used with multi arm causal forets. In the event the forest is trained
with multiple outcomes Y, a column number/name specifying the outcome of interest.
Default is 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of estimated mean rewards
</p>


<h3>Methods (by class)</h3>


<ul>
<li> <p><code>conditional_means(causal_forest)</code>: Mean rewards <code class="reqn">\mu</code> for control/treated
</p>
</li>
<li> <p><code>conditional_means(causal_survival_forest)</code>: Mean rewards <code class="reqn">\mu</code> for control/treated
</p>
</li>
<li> <p><code>conditional_means(instrumental_forest)</code>: Mean rewards <code class="reqn">\mu</code> for control/treated
</p>
</li>
<li> <p><code>conditional_means(multi_arm_causal_forest)</code>: Mean rewards <code class="reqn">\mu</code> for each treatment <code class="reqn">a</code>
</p>
</li></ul>


<h3>Examples</h3>

<pre><code class='language-R'>
# Compute conditional means for a multi-arm causal forest
n &lt;- 500
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
W &lt;- as.factor(sample(c("A", "B", "C"), n, replace = TRUE))
Y &lt;- X[, 1] + X[, 2] * (W == "B") + X[, 3] * (W == "C") + runif(n)
forest &lt;- grf::multi_arm_causal_forest(X, Y, W)
mu.hats &lt;- conditional_means(forest)
head(mu.hats)

# Compute conditional means for a causal forest
n &lt;- 500
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
W &lt;- rbinom(n, 1, 0.5)
Y &lt;- pmax(X[, 1], 0) * W + X[, 2] + pmin(X[, 3], 0) + rnorm(n)
c.forest &lt;- grf::causal_forest(X, Y, W)
mu.hats &lt;- conditional_means(c.forest)

</code></pre>

<hr>
<h2 id='create_dot_body'>Writes each node information
If it is a leaf node: show it in different color, show number of samples, show leaf id
If it is a non-leaf node: show its splitting variable and splitting value</h2><span id='topic+create_dot_body'></span>

<h3>Description</h3>

<p>Writes each node information
If it is a leaf node: show it in different color, show number of samples, show leaf id
If it is a non-leaf node: show its splitting variable and splitting value
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_dot_body(tree, index = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_dot_body_+3A_tree">tree</code></td>
<td>
<p>the tree to convert</p>
</td></tr>
<tr><td><code id="create_dot_body_+3A_index">index</code></td>
<td>
<p>the index of the current node</p>
</td></tr>
</table>

<hr>
<h2 id='double_robust_scores.causal_forest'>Matrix <code class="reqn">\Gamma</code> of scores for each treatment <code class="reqn">a</code></h2><span id='topic+double_robust_scores.causal_forest'></span><span id='topic+double_robust_scores.causal_survival_forest'></span><span id='topic+double_robust_scores.instrumental_forest'></span><span id='topic+double_robust_scores.multi_arm_causal_forest'></span><span id='topic+double_robust_scores'></span>

<h3>Description</h3>

<p>Computes a matrix of double robust scores
<code class="reqn">\Gamma_{ia} = \mu_a(x) + \frac{1}{e_a(x)} (Y_i - \mu_a(x)) 1(A_i=a)</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'causal_forest'
double_robust_scores(object, ...)

## S3 method for class 'causal_survival_forest'
double_robust_scores(object, ...)

## S3 method for class 'instrumental_forest'
double_robust_scores(object, compliance.score = NULL, ...)

## S3 method for class 'multi_arm_causal_forest'
double_robust_scores(object, outcome = 1, ...)

double_robust_scores(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="double_robust_scores.causal_forest_+3A_object">object</code></td>
<td>
<p>An appropriate causal forest type object</p>
</td></tr>
<tr><td><code id="double_robust_scores.causal_forest_+3A_...">...</code></td>
<td>
<p>Additional arguments</p>
</td></tr>
<tr><td><code id="double_robust_scores.causal_forest_+3A_compliance.score">compliance.score</code></td>
<td>
<p>An estimate of the causal effect of Z on W.
i.e., Delta(X) = E(W | X, Z = 1) - E(W | X, Z = 0), for each sample i = 1, ..., n. If NULL (default)
then this is estimated with a causal forest.</p>
</td></tr>
<tr><td><code id="double_robust_scores.causal_forest_+3A_outcome">outcome</code></td>
<td>
<p>Only used with multi arm causal forets. In the event the forest is trained
with multiple outcomes Y, a column number/name specifying the outcome of interest.
Default is 1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is the matrix used for CAIPWL (Cross-fitted Augmented Inverse Propensity Weighted Learning)
</p>


<h3>Value</h3>

<p>A matrix of scores for each treatment
</p>


<h3>Methods (by class)</h3>


<ul>
<li> <p><code>double_robust_scores(causal_forest)</code>: Scores <code class="reqn">(\Gamma_0, \Gamma_1)</code>
</p>
</li>
<li> <p><code>double_robust_scores(causal_survival_forest)</code>: Scores <code class="reqn">(\Gamma_0, \Gamma_1)</code>
</p>
</li>
<li> <p><code>double_robust_scores(instrumental_forest)</code>: Scores <code class="reqn">(-\Gamma, \Gamma)</code>
</p>
</li>
<li> <p><code>double_robust_scores(multi_arm_causal_forest)</code>: Matrix <code class="reqn">\Gamma</code> of scores for each treatment <code class="reqn">a</code>
</p>
</li></ul>


<h3>Note</h3>

<p>For instrumental_forest this method returns <code class="reqn">(-\Gamma_i, \Gamma_i)</code> where <code class="reqn">\Gamma_i</code>
is the double robust estimator of the treatment effect as in eqn. (44) in Athey and Wager (2021).
</p>


<h3>References</h3>

<p>Athey, Susan, and Stefan Wager. &quot;Policy Learning With Observational Data.&quot; Econometrica 89.1 (2021): 133-161.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Compute double robust scores for a multi-arm causal forest
n &lt;- 500
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
W &lt;- as.factor(sample(c("A", "B", "C"), n, replace = TRUE))
Y &lt;- X[, 1] + X[, 2] * (W == "B") + X[, 3] * (W == "C") + runif(n)
forest &lt;- grf::multi_arm_causal_forest(X, Y, W)
scores &lt;- double_robust_scores(forest)
head(scores)

# Compute double robust scores for a causal forest
n &lt;- 500
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
W &lt;- rbinom(n, 1, 0.5)
Y &lt;- pmax(X[, 1], 0) * W + X[, 2] + pmin(X[, 3], 0) + rnorm(n)
c.forest &lt;- grf::causal_forest(X, Y, W)
scores &lt;- double_robust_scores(c.forest)

</code></pre>

<hr>
<h2 id='export_graphviz'>Export a tree in DOT format.
This function generates a GraphViz representation of the tree,
which is then written into <code>dot_string</code>.</h2><span id='topic+export_graphviz'></span>

<h3>Description</h3>

<p>Export a tree in DOT format.
This function generates a GraphViz representation of the tree,
which is then written into <code>dot_string</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>export_graphviz(tree)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="export_graphviz_+3A_tree">tree</code></td>
<td>
<p>the tree to convert</p>
</td></tr>
</table>

<hr>
<h2 id='gen_data_epl'>Example data generating process from Policy Learning With Observational Data</h2><span id='topic+gen_data_epl'></span>

<h3>Description</h3>

<p>The DGP from section 5.2 in Athey and Wager (2021)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gen_data_epl(n, type = c("continuous", "jump"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gen_data_epl_+3A_n">n</code></td>
<td>
<p>Number of observations</p>
</td></tr>
<tr><td><code id="gen_data_epl_+3A_type">type</code></td>
<td>
<p>tau is &quot;continuous&quot; (default - equation 46) or exhibits &quot;jumps&quot; (equation 47)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list
</p>


<h3>References</h3>

<p>Athey, Susan, and Stefan Wager. &quot;Policy Learning With Observational Data.&quot; Econometrica 89.1 (2021): 133-161.
</p>

<hr>
<h2 id='gen_data_mapl'>Example data generating process from Offline Multi-Action Policy Learning: Generalization and Optimization</h2><span id='topic+gen_data_mapl'></span>

<h3>Description</h3>

<p>The DGP from section 6.4.1 in Zhou, Athey, and Wager (2023):
There are <code class="reqn">d=3</code> actions <code class="reqn">(a_0,a_1,a_2)</code> which depend
on 3 regions the covariates <code class="reqn">X \sim U[0,1]^p</code> reside in. Observed outcomes:
<code class="reqn">Y \sim N(\mu_{a_i}(X_i), 4)</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gen_data_mapl(n, p = 10, sigma2 = 4)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gen_data_mapl_+3A_n">n</code></td>
<td>
<p>Number of observations <code class="reqn">X</code>.</p>
</td></tr>
<tr><td><code id="gen_data_mapl_+3A_p">p</code></td>
<td>
<p>Number of features (minimum 7). Default is 10.</p>
</td></tr>
<tr><td><code id="gen_data_mapl_+3A_sigma2">sigma2</code></td>
<td>
<p>Noise variance. Default is 4.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with realized action <code class="reqn">a_i</code>, region <code class="reqn">r_i</code>,
conditional mean <code class="reqn">\mu</code>, outcome <code class="reqn">Y</code> and covariates <code class="reqn">X</code>
</p>


<h3>References</h3>

<p>Zhou, Zhengyuan, Susan Athey, and Stefan Wager. &quot;Offline multi-action policy learning:
Generalization and optimization.&quot; Operations Research 71.1 (2023).
</p>

<hr>
<h2 id='hybrid_policy_tree'>Hybrid tree search</h2><span id='topic+hybrid_policy_tree'></span>

<h3>Description</h3>

<p>Finds a depth k tree by looking ahead l steps.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hybrid_policy_tree(
  X,
  Gamma,
  depth = 3,
  search.depth = 2,
  split.step = 1,
  min.node.size = 1,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hybrid_policy_tree_+3A_x">X</code></td>
<td>
<p>The covariates used. Dimension <code class="reqn">N*p</code> where <code class="reqn">p</code> is the number of features.</p>
</td></tr>
<tr><td><code id="hybrid_policy_tree_+3A_gamma">Gamma</code></td>
<td>
<p>The rewards for each action. Dimension <code class="reqn">N*d</code> where <code class="reqn">d</code> is the number of actions.</p>
</td></tr>
<tr><td><code id="hybrid_policy_tree_+3A_depth">depth</code></td>
<td>
<p>The depth of the fitted tree. Default is 3.</p>
</td></tr>
<tr><td><code id="hybrid_policy_tree_+3A_search.depth">search.depth</code></td>
<td>
<p>Depth to look ahead when splitting. Default is 2.</p>
</td></tr>
<tr><td><code id="hybrid_policy_tree_+3A_split.step">split.step</code></td>
<td>
<p>An optional approximation parameter, the number of possible splits
to consider when performing tree search. split.step = 1 (default) considers every possible split, split.step = 10
considers splitting at every 10'th sample and may yield a substantial speedup for dense features.
Manually rounding or re-encoding continuous covariates with very high cardinality in a
problem specific manner allows for finer-grained control of the accuracy/runtime tradeoff and may in some cases
be the preferred approach.</p>
</td></tr>
<tr><td><code id="hybrid_policy_tree_+3A_min.node.size">min.node.size</code></td>
<td>
<p>An integer indicating the smallest terminal node size permitted. Default is 1.</p>
</td></tr>
<tr><td><code id="hybrid_policy_tree_+3A_verbose">verbose</code></td>
<td>
<p>Give verbose output. Default is TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Builds deeper trees by iteratively using exact tree search to look ahead l splits. For example,
with <code>depth = 3</code> and <code>search.depth = 2</code>, the root split is determined by a depth 2 exact tree,
and two new depth 2 trees are fit in the two immediate children using exact tree search,
leading to a total depth of 3 (the resulting tree may be shallower than the
specified <code>depth</code> depending on whether leaf nodes were pruned or not).
This algorithm scales with some coefficient multiple of the runtime of a <code>search.depth</code> <code>policy_tree</code>,
which means that for this approach to be feasible it needs an (n, p, d) configuration in which
a <code>search.depth</code> <code>policy_tree</code> runs in reasonable time.
</p>
<p>The algorithm: desired depth is given by <code>depth</code>. Each node is split using exact tree search
with depth  = <code>search.depth</code>. When we reach a node where the current level + <code>search.depth</code> is equal to <code>depth</code>,
we stop and attach the <code>search.depth</code> subtree to this node.
We also stop if the best <code>search.depth</code> split yielded a leaf node.
</p>


<h3>Value</h3>

<p>A policy_tree object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Fit a depth three tree on doubly robust treatment effect estimates from a causal forest.
n &lt;- 1500
p &lt;- 5
X &lt;- round(matrix(rnorm(n * p), n, p), 2)
W &lt;- rbinom(n, 1, 1 / (1 + exp(X[, 3])))
tau &lt;- 1 / (1 + exp((X[, 1] + X[, 2]) / 2)) - 0.5
Y &lt;- X[, 3] + W * tau + rnorm(n)
c.forest &lt;- grf::causal_forest(X, Y, W)
dr.scores &lt;- double_robust_scores(c.forest)

tree &lt;- hybrid_policy_tree(X, dr.scores, depth = 3)

# Predict treatment assignment.
predicted &lt;- predict(tree, X)

</code></pre>

<hr>
<h2 id='make_tree'>A utility function for generating random trees for test purposes.</h2><span id='topic+make_tree'></span>

<h3>Description</h3>

<p>Build a depth <code>depth</code> tree by drawing random split variables
and split values from the Nxp matrix <code>X</code>. In leaf nodes a random
action is drawn from 1:<code>d</code>. (Minimum leaf size will be 1)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_tree(X, depth, d)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make_tree_+3A_x">X</code></td>
<td>
<p>data matrix.</p>
</td></tr>
<tr><td><code id="make_tree_+3A_depth">depth</code></td>
<td>
<p>tree depth.</p>
</td></tr>
<tr><td><code id="make_tree_+3A_d">d</code></td>
<td>
<p>number of actions.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A policy_tree tree object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
depth &lt;- 2
n &lt;- 100
p &lt;- 10
d &lt;- 3
X &lt;- matrix(rnorm(n * p), n, p)
Y &lt;- matrix(rnorm(n * d), n, d)
tree &lt;- make_tree(X, depth = depth, d = d)
pp &lt;- predict_test_tree(tree, X)

## End(Not run)
</code></pre>

<hr>
<h2 id='multi_causal_forest'>(deprecated) One vs. all causal forest for multiple treatment effect estimation</h2><span id='topic+multi_causal_forest'></span>

<h3>Description</h3>

<p>Since policytree version 1.1 this function is deprecated in favor of the new estimator
<code>multi_arm_causal_forest</code> available in GRF (version 2+). This function will continue to work
for now but passes its arguments onto the &quot;conformable&quot; <code>multi_arm_causal_forest</code> in GRF, with a warning.
(Note: for policy learning this forest works as before,
but for individual point predictions, they differ as <code>multi_arm_causal_forest</code> predicts contrasts.
See the GRF documentation example for details.)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multi_causal_forest(
  X,
  Y,
  W,
  Y.hat = NULL,
  W.hat = NULL,
  num.trees = 2000,
  sample.weights = NULL,
  clusters = NULL,
  equalize.cluster.weights = FALSE,
  sample.fraction = 0.5,
  mtry = min(ceiling(sqrt(ncol(X)) + 20), ncol(X)),
  min.node.size = 5,
  honesty = TRUE,
  honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE,
  alpha = 0.05,
  imbalance.penalty = 0,
  stabilize.splits = TRUE,
  ci.group.size = 2,
  tune.parameters = "none",
  tune.num.trees = 200,
  tune.num.reps = 50,
  tune.num.draws = 1000,
  compute.oob.predictions = TRUE,
  orthog.boosting = FALSE,
  num.threads = NULL,
  seed = runif(1, 0, .Machine$integer.max)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multi_causal_forest_+3A_x">X</code></td>
<td>
<p>The covariates used in the causal regression.</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_y">Y</code></td>
<td>
<p>The outcome (must be a numeric vector with no NAs).</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_w">W</code></td>
<td>
<p>The treatment assignment (must be a categorical vector with no NAs).</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_y.hat">Y.hat</code></td>
<td>
<p>Estimates of the expected responses E[Y | Xi], marginalizing
over treatment. If Y.hat = NULL, these are estimated using
a separate regression forest. See section 6.1.1 of the GRF paper for
further discussion of this quantity. Default is NULL.</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_w.hat">W.hat</code></td>
<td>
<p>Matrix with estimates of the treatment propensities E[Wk | Xi]. If W.hat = NULL,
these are estimated using a k separate regression forests. Default is NULL.</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_num.trees">num.trees</code></td>
<td>
<p>Number of trees grown in the forest. Note: Getting accurate
confidence intervals generally requires more trees than
getting accurate predictions. Default is 2000.</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_sample.weights">sample.weights</code></td>
<td>
<p>(experimental) Weights given to each sample in estimation.
If NULL, each observation receives the same weight.
Note: To avoid introducing confounding, weights should be
independent of the potential outcomes given X. Default is NULL.</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_clusters">clusters</code></td>
<td>
<p>Vector of integers or factors specifying which cluster each observation corresponds to.
Default is NULL (ignored).</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_equalize.cluster.weights">equalize.cluster.weights</code></td>
<td>
<p>If FALSE, each unit is given the same weight (so that bigger
clusters get more weight). If TRUE, each cluster is given equal weight in the forest. In this case,
during training, each tree uses the same number of observations from each drawn cluster: If the
smallest cluster has K units, then when we sample a cluster during training, we only give a random
K elements of the cluster to the tree-growing procedure. When estimating average treatment effects,
each observation is given weight 1/cluster size, so that the total weight of each cluster is the
same. Note that, if this argument is FALSE, sample weights may also be directly adjusted via the
sample.weights argument. If this argument is TRUE, sample.weights must be set to NULL. Default is
FALSE.</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_sample.fraction">sample.fraction</code></td>
<td>
<p>Fraction of the data used to build each tree.
Note: If honesty = TRUE, these subsamples will
further be cut by a factor of honesty.fraction. Default is 0.5.</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_mtry">mtry</code></td>
<td>
<p>Number of variables tried for each split. Default is
<code class="reqn">\sqrt p + 20</code> where p is the number of variables.</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_min.node.size">min.node.size</code></td>
<td>
<p>A target for the minimum number of observations in each tree leaf. Note that nodes
with size smaller than min.node.size can occur, as in the original randomForest package.
Default is 5.</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_honesty">honesty</code></td>
<td>
<p>Whether to use honest splitting (i.e., sub-sample splitting). Default is TRUE.
For a detailed description of honesty, honesty.fraction, honesty.prune.leaves, and recommendations for
parameter tuning, see the grf
<a href="https://grf-labs.github.io/grf/REFERENCE.html#honesty-honesty-fraction-prune-empty-leaves">algorithm reference</a>.</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_honesty.fraction">honesty.fraction</code></td>
<td>
<p>The fraction of data that will be used for determining splits if honesty = TRUE. Corresponds
to set J1 in the notation of the paper. Default is 0.5 (i.e. half of the data is used for
determining splits).</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_honesty.prune.leaves">honesty.prune.leaves</code></td>
<td>
<p>If true, prunes the estimation sample tree such that no leaves
are empty. If false, keep the same tree as determined in the splits sample (if an empty leave is encountered, that
tree is skipped and does not contribute to the estimate). Setting this to false may improve performance on
small/marginally powered data, but requires more trees (note: tuning does not adjust the number of trees).
Only applies if honesty is enabled. Default is TRUE.</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_alpha">alpha</code></td>
<td>
<p>A tuning parameter that controls the maximum imbalance of a split. Default is 0.05.</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_imbalance.penalty">imbalance.penalty</code></td>
<td>
<p>A tuning parameter that controls how harshly imbalanced splits are penalized. Default is 0.</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_stabilize.splits">stabilize.splits</code></td>
<td>
<p>Whether or not the treatment should be taken into account when
determining the imbalance of a split. Default is TRUE.</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_ci.group.size">ci.group.size</code></td>
<td>
<p>The forest will grow ci.group.size trees on each subsample.
In order to provide confidence intervals, ci.group.size must
be at least 2. Default is 2.</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_tune.parameters">tune.parameters</code></td>
<td>
<p>A vector of parameter names to tune.
If &quot;all&quot;: all tunable parameters are tuned by cross-validation. The following parameters are
tunable: (&quot;sample.fraction&quot;, &quot;mtry&quot;, &quot;min.node.size&quot;, &quot;honesty.fraction&quot;,
&quot;honesty.prune.leaves&quot;, &quot;alpha&quot;, &quot;imbalance.penalty&quot;). If honesty is false these parameters are not tuned.
Default is &quot;none&quot; (no parameters are tuned).</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_tune.num.trees">tune.num.trees</code></td>
<td>
<p>The number of trees in each 'mini forest' used to fit the tuning model. Default is 200.</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_tune.num.reps">tune.num.reps</code></td>
<td>
<p>The number of forests used to fit the tuning model. Default is 50.</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_tune.num.draws">tune.num.draws</code></td>
<td>
<p>The number of random parameter values considered when using the model
to select the optimal parameters. Default is 1000.</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_compute.oob.predictions">compute.oob.predictions</code></td>
<td>
<p>Whether OOB predictions on training set should be precomputed. Default is TRUE.</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_orthog.boosting">orthog.boosting</code></td>
<td>
<p>Deprecated and unused after version 1.0.4.</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in training. By default, the number of threads is set
to the maximum hardware concurrency.</p>
</td></tr>
<tr><td><code id="multi_causal_forest_+3A_seed">seed</code></td>
<td>
<p>The seed of the C++ random number generator.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A warning will be issued and this function passes its arguments onto the new
estimator <code>multi_arm_causal_forest</code> and returns that object.
</p>

<hr>
<h2 id='plot.policy_tree'>Plot a policy_tree tree object.</h2><span id='topic+plot.policy_tree'></span>

<h3>Description</h3>

<p>Plot a policy_tree tree object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'policy_tree'
plot(x, leaf.labels = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.policy_tree_+3A_x">x</code></td>
<td>
<p>The tree to plot.</p>
</td></tr>
<tr><td><code id="plot.policy_tree_+3A_leaf.labels">leaf.labels</code></td>
<td>
<p>An optional character vector of leaf labels for each treatment.</p>
</td></tr>
<tr><td><code id="plot.policy_tree_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># Plot a policy_tree object
## Not run: 
n &lt;- 250
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
W &lt;- as.factor(sample(c("A", "B", "C"), n, replace = TRUE))
Y &lt;- X[, 1] + X[, 2] * (W == "B") + X[, 3] * (W == "C") + runif(n)
multi.forest &lt;- grf::multi_arm_causal_forest(X = X, Y = Y, W = W)
Gamma.matrix &lt;- double_robust_scores(multi.forest)
tree &lt;- policy_tree(X, Gamma.matrix, depth = 2)
plot(tree)

# Provide optional names for the treatment names in each leaf node
# `action.names` is by default the column names of the reward matrix
plot(tree, leaf.labels = tree$action.names)
# Providing a custom character vector
plot(tree, leaf.labels = c("treatment A", "treatment B", "placebo C"))

# Saving a plot in a vectorized SVG format can be done with the `DiagrammeRsvg` package.
install.packages("DiagrammeRsvg")
tree.plot = plot(tree)
cat(DiagrammeRsvg::export_svg(tree.plot), file = 'plot.svg')

## End(Not run)
</code></pre>

<hr>
<h2 id='policy_tree'>Fit a policy with exact tree search</h2><span id='topic+policy_tree'></span>

<h3>Description</h3>

<p>Finds the optimal (maximizing the sum of rewards) depth k tree by exhaustive search. If the optimal
action is the same in both the left and right leaf of a node, the node is pruned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>policy_tree(
  X,
  Gamma,
  depth = 2,
  split.step = 1,
  min.node.size = 1,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="policy_tree_+3A_x">X</code></td>
<td>
<p>The covariates used. Dimension <code class="reqn">N*p</code> where <code class="reqn">p</code> is the number of features.</p>
</td></tr>
<tr><td><code id="policy_tree_+3A_gamma">Gamma</code></td>
<td>
<p>The rewards for each action. Dimension <code class="reqn">N*d</code> where <code class="reqn">d</code> is the number of actions.</p>
</td></tr>
<tr><td><code id="policy_tree_+3A_depth">depth</code></td>
<td>
<p>The depth of the fitted tree. Default is 2.</p>
</td></tr>
<tr><td><code id="policy_tree_+3A_split.step">split.step</code></td>
<td>
<p>An optional approximation parameter, the number of possible splits
to consider when performing tree search. split.step = 1 (default) considers every possible split, split.step = 10
considers splitting at every 10'th sample and may yield a substantial speedup for dense features.
Manually rounding or re-encoding continuous covariates with very high cardinality in a
problem specific manner allows for finer-grained control of the accuracy/runtime tradeoff and may in some cases
be the preferred approach.</p>
</td></tr>
<tr><td><code id="policy_tree_+3A_min.node.size">min.node.size</code></td>
<td>
<p>An integer indicating the smallest terminal node size permitted. Default is 1.</p>
</td></tr>
<tr><td><code id="policy_tree_+3A_verbose">verbose</code></td>
<td>
<p>Give verbose output. Default is TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Exact tree search is intended as a way to find shallow (i.e. depth 2 or 3) globally optimal
tree-based polices on datasets of &quot;moderate&quot; size.
The amortized runtime of exact tree search is <code class="reqn">O(p^k n^k (log n + d) + pnlog n)</code> where p is
the number of features, n the number of distinct observations, d the number of treatments, and k &gt;= 1
the tree depth. Due to the exponents in this expression, exact tree search will not scale to datasets
of arbitrary size.
</p>
<p>As an example, the runtime of a depth two tree scales quadratically with the number of observations, implying
that doubling the number of samples will quadruple the runtime.
n refers to the number of distinct observations, substantial speedups can be gained
when the features are discrete (with all binary features, the runtime will be ~ linear in n),
and it is therefore beneficial to round down/re-encode very dense data to a lower cardinality
(the optional parameter <code>split.step</code> emulates this, though rounding/re-encoding allow for finer-grained control).
</p>


<h3>Value</h3>

<p>A policy_tree object.
</p>


<h3>References</h3>

<p>Athey, Susan, and Stefan Wager. &quot;Policy Learning With Observational Data.&quot;
Econometrica 89.1 (2021): 133-161.
</p>
<p>Sverdrup, Erik, Ayush Kanodia, Zhengyuan Zhou, Susan Athey, and Stefan Wager.
&quot;policytree: Policy learning via doubly robust empirical welfare maximization over trees.&quot;
Journal of Open Source Software 5, no. 50 (2020): 2232.
</p>
<p>Zhou, Zhengyuan, Susan Athey, and Stefan Wager. &quot;Offline multi-action policy learning:
Generalization and optimization.&quot; Operations Research 71.1 (2023).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hybrid_policy_tree">hybrid_policy_tree</a></code> for building deeper trees.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Construct doubly robust scores using a causal forest.
n &lt;- 10000
p &lt;- 10
# Discretizing continuous covariates decreases runtime for policy learning.
X &lt;- round(matrix(rnorm(n * p), n, p), 2)
colnames(X) &lt;- make.names(1:p)
W &lt;- rbinom(n, 1, 1 / (1 + exp(X[, 3])))
tau &lt;- 1 / (1 + exp((X[, 1] + X[, 2]) / 2)) - 0.5
Y &lt;- X[, 3] + W * tau + rnorm(n)
c.forest &lt;- grf::causal_forest(X, Y, W)

# Retrieve doubly robust scores.
dr.scores &lt;- double_robust_scores(c.forest)

# Learn a depth-2 tree on a training set.
train &lt;- sample(1:n, n / 2)
tree &lt;- policy_tree(X[train, ], dr.scores[train, ], 2)
tree

# Evaluate the tree on a test set.
test &lt;- -train

# One way to assess the policy is to see whether the leaf node (group) the test set samples
# are predicted to belong to have mean outcomes in accordance with the prescribed policy.

# Get the leaf node assigned to each test sample.
node.id &lt;- predict(tree, X[test, ], type = "node.id")

# Doubly robust estimates of E[Y(control)] and E[Y(treated)] by leaf node.
values &lt;- aggregate(dr.scores[test, ], by = list(leaf.node = node.id),
                    FUN = function(dr) c(mean = mean(dr), se = sd(dr) / sqrt(length(dr))))
print(values, digits = 1)

# Take cost of treatment into account by, for example, offsetting the objective
# with an estimate of the average treatment effect.
ate &lt;- grf::average_treatment_effect(c.forest)
cost.offset &lt;- ate[["estimate"]]
dr.scores[, "treated"] &lt;- dr.scores[, "treated"] - cost.offset
tree.cost &lt;- policy_tree(X, dr.scores, 2)

# Predict treatment assignment for each sample.
predicted &lt;- predict(tree, X)

# If there are too many covariates to make tree search computationally feasible, then one
# approach is to consider for example only the top features according to GRF's variable importance.
var.imp &lt;- grf::variable_importance(c.forest)
top.5 &lt;- order(var.imp, decreasing = TRUE)[1:5]
tree.top5 &lt;- policy_tree(X[, top.5], dr.scores, 2, split.step = 50)

</code></pre>

<hr>
<h2 id='predict_test_tree'>Predict with the above test tree.</h2><span id='topic+predict_test_tree'></span>

<h3>Description</h3>

<p>Predict with the above test tree.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_test_tree(tree, newdata)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_test_tree_+3A_tree">tree</code></td>
<td>
<p>tree from make_tree</p>
</td></tr>
<tr><td><code id="predict_test_tree_+3A_newdata">newdata</code></td>
<td>
<p>data matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of predictions
</p>

<hr>
<h2 id='predict.policy_tree'>Predict method for policy_tree</h2><span id='topic+predict.policy_tree'></span>

<h3>Description</h3>

<p>Predict values based on fitted policy_tree object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'policy_tree'
predict(object, newdata, type = c("action.id", "node.id"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.policy_tree_+3A_object">object</code></td>
<td>
<p>policy_tree object</p>
</td></tr>
<tr><td><code id="predict.policy_tree_+3A_newdata">newdata</code></td>
<td>
<p>Points at which predictions should be made. Note that this matrix should have the
same number of columns as the training matrix, and that the columns must appear in the same order.</p>
</td></tr>
<tr><td><code id="predict.policy_tree_+3A_type">type</code></td>
<td>
<p>The type of prediction required, &quot;action.id&quot; is the action id and
&quot;node.id&quot; is the integer id of the leaf node the sample falls into. Default is &quot;action.id&quot;.</p>
</td></tr>
<tr><td><code id="predict.policy_tree_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of predictions. For type = &quot;action.id&quot; each element is an integer from 1 to d where d is
the number of columns in the reward matrix. For type = &quot;node.id&quot; each element is an integer corresponding
to the node the sample falls into (level-ordered).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Construct doubly robust scores using a causal forest.
n &lt;- 10000
p &lt;- 10
# Discretizing continuous covariates decreases runtime for policy learning.
X &lt;- round(matrix(rnorm(n * p), n, p), 2)
colnames(X) &lt;- make.names(1:p)
W &lt;- rbinom(n, 1, 1 / (1 + exp(X[, 3])))
tau &lt;- 1 / (1 + exp((X[, 1] + X[, 2]) / 2)) - 0.5
Y &lt;- X[, 3] + W * tau + rnorm(n)
c.forest &lt;- grf::causal_forest(X, Y, W)

# Retrieve doubly robust scores.
dr.scores &lt;- double_robust_scores(c.forest)

# Learn a depth-2 tree on a training set.
train &lt;- sample(1:n, n / 2)
tree &lt;- policy_tree(X[train, ], dr.scores[train, ], 2)
tree

# Evaluate the tree on a test set.
test &lt;- -train

# One way to assess the policy is to see whether the leaf node (group) the test set samples
# are predicted to belong to have mean outcomes in accordance with the prescribed policy.

# Get the leaf node assigned to each test sample.
node.id &lt;- predict(tree, X[test, ], type = "node.id")

# Doubly robust estimates of E[Y(control)] and E[Y(treated)] by leaf node.
values &lt;- aggregate(dr.scores[test, ], by = list(leaf.node = node.id),
                    FUN = function(dr) c(mean = mean(dr), se = sd(dr) / sqrt(length(dr))))
print(values, digits = 1)

# Take cost of treatment into account by, for example, offsetting the objective
# with an estimate of the average treatment effect.
ate &lt;- grf::average_treatment_effect(c.forest)
cost.offset &lt;- ate[["estimate"]]
dr.scores[, "treated"] &lt;- dr.scores[, "treated"] - cost.offset
tree.cost &lt;- policy_tree(X, dr.scores, 2)

# Predict treatment assignment for each sample.
predicted &lt;- predict(tree, X)

# If there are too many covariates to make tree search computationally feasible, then one
# approach is to consider for example only the top features according to GRF's variable importance.
var.imp &lt;- grf::variable_importance(c.forest)
top.5 &lt;- order(var.imp, decreasing = TRUE)[1:5]
tree.top5 &lt;- policy_tree(X[, top.5], dr.scores, 2, split.step = 50)

</code></pre>

<hr>
<h2 id='print.policy_tree'>Print a policy_tree object.</h2><span id='topic+print.policy_tree'></span>

<h3>Description</h3>

<p>Print a policy_tree object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'policy_tree'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.policy_tree_+3A_x">x</code></td>
<td>
<p>The tree to print.</p>
</td></tr>
<tr><td><code id="print.policy_tree_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
