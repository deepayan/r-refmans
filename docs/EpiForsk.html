<!DOCTYPE html><html><head><title>Help for package EpiForsk</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {EpiForsk}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#EpiForsk-package'><p>EpiForsk</p></a></li>
<li><a href='#.datatable.aware'><p>make package data table aware</p></a></li>
<li><a href='#adls_timevarying_region_data'><p>Simulated Time-Varying Residence Data</p></a></li>
<li><a href='#andh_forest_data'><p>Example Data for Husby's Forest Plot Vignette</p></a></li>
<li><a href='#braid_rows'><p>Bind lists of list of multiple data frames by row</p></a></li>
<li><a href='#CATESurface'><p>Calculate CATE on a surface in the covariate space</p></a></li>
<li><a href='#CausalForestDynamicSubgroups'><p>Calculate CATE in dynamically determined subgroups</p></a></li>
<li><a href='#ceiling_dec'><p>Round numbers up to a given number of decimal places</p></a></li>
<li><a href='#CForBenefit'><p>c-for-benefit</p></a></li>
<li><a href='#charlson_score'><p>Charlson Score Constructor</p></a></li>
<li><a href='#ci_fct'><p>solve optimization problem for  CI bounds</p></a></li>
<li><a href='#ci_fct_error_handler'><p>Handle errors returned by ci_fct</p></a></li>
<li><a href='#CovariateBalance'><p>Plots for checking covariate balance in causal forest</p></a></li>
<li><a href='#decimalplaces'><p>Determine number of decimal places</p></a></li>
<li><a href='#DiscreteCovariateNames'><p>Extract discrete covariate names</p></a></li>
<li><a href='#DiscreteCovariatesToOneHot'><p>One-hot encode factors</p></a></li>
<li><a href='#fct_confint'><p>Confidence set for functions of model parameters</p></a></li>
<li><a href='#flatten_date_intervals'><p>Flatten Date Intervals</p></a></li>
<li><a href='#floor_dec'><p>Round numbers down to a given number of decimal places</p></a></li>
<li><a href='#freq_function'><p>Frequency Tables with Percentage and Odds Ratios</p></a></li>
<li><a href='#freq_function_repeated'><p>Wrapper for <code>freq_function()</code> to get frequencies for many variables in one</p>
go.</a></li>
<li><a href='#lms'><p>Wrapper around lm for sibling design</p></a></li>
<li><a href='#logasympBF'><p>Asymptotic Bayes factors</p></a></li>
<li><a href='#many_merge'><p>Merging Many Data Frames with Name Handling</p></a></li>
<li><a href='#multi_join'><p>Join many data frames with name handling</p></a></li>
<li><a href='#odds_ratio_function'><p>Easier to perform logistic and log-linear regressions giving a standardized</p>
output table</a></li>
<li><a href='#odds_ratio_function_repeated'><p>Wrapper for the <code>odds_ratio_function()</code>to perform several similar analyses</p>
in one go</a></li>
<li><a href='#RATEOmnibusTest'><p>RATE based omnibus test of heterogeneity</p></a></li>
<li><a href='#RATETest'><p>wrapper for rank_average_treatment_effect</p></a></li>
<li><a href='#summary.svy_vglm'><p>Summary function for svy_vglm objects</p></a></li>
<li><a href='#try_catch_warnings'><p>Try Catch with Warning Handling</p></a></li>
<li><a href='#vcovHC'><p>Heteroscedasticity-Consistent Covariance Matrix</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Code Sharing at the Department of Epidemiological Research at
Statens Serum Institut</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.1</td>
</tr>
<tr>
<td>Description:</td>
<td>This is a collection of assorted functions and examples collected 
    from various projects. Currently we have functionalities for simplifying 
    overlapping time intervals, Charlson comorbidity score constructors for 
    Danish data, getting frequency for multiple variables, getting standardized
    output from logistic and log-linear regressions, sibling design linear 
    regression functionalities a method for calculating the confidence intervals 
    for functions of parameters from a GLM, Bayes equivalent for hypothesis 
    testing with asymptotic Bayes factor, and several help functions for 
    generalized random forest analysis using 'grf'. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Imports:</td>
<td>broom, cowplot, data.table, dplyr, forcats, ggplot2, glue,
grf, gridExtra, Hmisc, MatchIt, methods, nnet, patchwork,
policytree, progressr, purrr, rlang, stringr, survey, survival,
svyVGAM, tidyr, VGAM</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.2)</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Suggests:</td>
<td>cli, CVXR, furrr, future, ggsci, knitr, parallel, rmarkdown,
testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-26 10:46:13 UTC; b246294</td>
</tr>
<tr>
<td>Author:</td>
<td>Anders Husby <a href="https://orcid.org/0000-0002-7634-8455"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Anna Laksafoss <a href="https://orcid.org/0000-0002-9898-2924"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Emilia Myrup Thiesson
    <a href="https://orcid.org/0000-0001-6258-4177"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Kim Daniel Jakobsen
    <a href="https://orcid.org/0000-0003-0086-9980"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre],
  Mikael Andersson <a href="https://orcid.org/0000-0002-0114-2057"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Klaus Rostgaard <a href="https://orcid.org/0000-0001-6220-9414"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kim Daniel Jakobsen &lt;kija@ssi.dk&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-26 13:40:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='EpiForsk-package'>EpiForsk</h2><span id='topic+EpiForsk'></span><span id='topic+EpiForsk-package'></span>

<h3>Description</h3>

<p>This is a collection of assorted functions and examples collected
from various projects. Currently we have functionalities for simplifying
overlapping time intervals, Charlson comorbidity score constructors for
Danish data, getting frequency for multiple variables, getting standardized
output from logistic and log-linear regressions, sibling design linear
regression functionalities a method for calculating the confidence intervals
for functions of parameters from a GLM, Bayes equivalent for hypothesis
testing with asymptotic Bayes factor, and several help functions for
generalized random forest analysis using the grf package.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Kim Daniel Jakobsen <a href="mailto:kija@ssi.dk">kija@ssi.dk</a> (<a href="https://orcid.org/0000-0003-0086-9980">ORCID</a>)
</p>
<p>Authors:
</p>

<ul>
<li><p> Anders Husby <a href="mailto:andh@ssi.dk">andh@ssi.dk</a> (<a href="https://orcid.org/0000-0002-7634-8455">ORCID</a>)
</p>
</li>
<li><p> Anna Laksafoss <a href="mailto:adls@ssi.dk">adls@ssi.dk</a> (<a href="https://orcid.org/0000-0002-9898-2924">ORCID</a>)
</p>
</li>
<li><p> Emilia Myrup Thiesson <a href="mailto:emth@ssi.dk">emth@ssi.dk</a> (<a href="https://orcid.org/0000-0001-6258-4177">ORCID</a>)
</p>
</li>
<li><p> Mikael Andersson <a href="mailto:aso@ssi.dk">aso@ssi.dk</a> (<a href="https://orcid.org/0000-0002-0114-2057">ORCID</a>)
</p>
</li>
<li><p> Klaus Rostgaard <a href="mailto:klp@ssi.dk">klp@ssi.dk</a> (<a href="https://orcid.org/0000-0001-6220-9414">ORCID</a>)
</p>
</li></ul>


<hr>
<h2 id='.datatable.aware'>make package data table aware</h2><span id='topic+.datatable.aware'></span>

<h3>Description</h3>

<p>This package uses data.table as a fast alternative to dplyr in cases where
performance is essential.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.datatable.aware
</code></pre>


<h3>Format</h3>

<p>An object of class <code>logical</code> of length 1.
</p>

<hr>
<h2 id='adls_timevarying_region_data'>Simulated Time-Varying Residence Data</h2><span id='topic+adls_timevarying_region_data'></span>

<h3>Description</h3>

<p>A dataset of simulated time-varying residence and gender data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adls_timevarying_region_data
</code></pre>


<h3>Format</h3>



<h4><code>andh_forest_data</code></h4>

<p>A data frame with 546 rows and 7 columns describing 100 people:
</p>

<dl>
<dt>id</dt><dd><p>an id number</p>
</dd>
<dt>dob</dt><dd><p>date of birth</p>
</dd>
<dt>region</dt><dd><p>region of Denmark</p>
</dd>
<dt>move_in</dt><dd><p>date of moving to region</p>
</dd>
<dt>move_out</dt><dd><p>date of moving away from region</p>
</dd>
<dt>gender</dt><dd><p>gender of the person</p>
</dd>
<dt>claim</dt><dd><p>whether or not the person made a claim here</p>
</dd>
</dl>



<hr>
<h2 id='andh_forest_data'>Example Data for Husby's Forest Plot Vignette</h2><span id='topic+andh_forest_data'></span>

<h3>Description</h3>

<p>A data example for the construction of a multi faceted forest plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>andh_forest_data
</code></pre>


<h3>Format</h3>



<h4><code>andh_forest_data</code></h4>

<p>A data frame with 18 rows and 12 columns:
</p>

<dl>
<dt>type</dt><dd><p>text formattaing, bold/plain</p>
</dd>
<dt>indent</dt><dd><p>number of indents in final formatting</p>
</dd>
<dt>text</dt><dd><p>description text</p>
</dd>
<dt>A_est</dt><dd><p>point estimate in first figure column</p>
</dd>
<dt>A_l</dt><dd><p>lower limit of confidence interval in first figure column</p>
</dd>
<dt>A_u</dt><dd><p>upper limit of confidence interval in first figure column</p>
</dd>
<dt>B_est</dt><dd><p>point estimate in second figure column</p>
</dd>
<dt>B_l</dt><dd><p>lower limit of confidence interval in second figure column</p>
</dd>
<dt>B_u</dt><dd><p>upper limit of confidence interval in second figure column</p>
</dd>
<dt>C_est</dt><dd><p>point estimate in third figure column</p>
</dd>
<dt>C_l</dt><dd><p>lower limit of confidence interval in third figure column</p>
</dd>
<dt>C_u</dt><dd><p>upper limit of confidence interval in third figure column</p>
</dd>
</dl>



<hr>
<h2 id='braid_rows'>Bind lists of list of multiple data frames by row</h2><span id='topic+braid_rows'></span>

<h3>Description</h3>

<p>Row binds the matching innermost data frames in a list of lists. This is
essentially a list inversion <code><a href="purrr.html#topic+list_transpose">purrr::list_transpose()</a></code> with row-binding
<code><a href="dplyr.html#topic+bind_rows">dplyr::bind_rows()</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>braid_rows(list)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="braid_rows_+3A_list">list</code></td>
<td>
<p>A list of lists of <code>data.frame</code>s where matching innermost
elements must be bound together row-wise.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of <code>data.frame</code>s with the combined information from the inputted list.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># A simple example
lst &lt;- lapply(1:5, function(x) {
  list(
    "A" = data.frame("first" = x, "second" = rnorm(x)),
    "B" = data.frame("info" = 1, "other" = 3)
  )
})
braid_rows(lst)

# An example with an additional layer and jagged innermost info
lapply(c(28, 186, 35), function(len) {
  lapply(c("a", "b"), function(x) {
    res &lt;- list(
      "descriptive" = data.frame(
         risk = len,
         event = x,
         var = 1,
         other = 2
       ),
      "results" = data.frame(
         risk = len,
         event = x,
         important = 4:7,
         new = 3:6
      )
    )
    if (len &lt; 30) {
      res &lt;- c(res, list("additional" = data.frame(helps = "extra data")))
    }
    return(res)
  }) |&gt; braid_rows()
}) |&gt; braid_rows()


</code></pre>

<hr>
<h2 id='CATESurface'>Calculate CATE on a surface in the covariate space</h2><span id='topic+CATESurface'></span>

<h3>Description</h3>

<p>Calculates CATE estimates from a causal forest object on a specified surface
within the covariate space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CATESurface(
  forest,
  continuous_covariates,
  discrete_covariates,
  estimate_variance = TRUE,
  grid = 100,
  fixed_covariate_fct = median,
  other_discrete = NULL,
  max_predict_size = 1e+05,
  num_threads = 2
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CATESurface_+3A_forest">forest</code></td>
<td>
<p>An object of class <code>causal_forest</code>, as returned by
<a href="grf.html#topic+causal_forest">causal_forest</a>(). Alternatively, and object of class
<code>regression_forest</code>, as returned by <a href="grf.html#topic+regression_forest">regression_forest</a>().</p>
</td></tr>
<tr><td><code id="CATESurface_+3A_continuous_covariates">continuous_covariates</code></td>
<td>
<p>character, continuous covariates to use for the
surface. Must match names in <code>forest$X.orig</code>.</p>
</td></tr>
<tr><td><code id="CATESurface_+3A_discrete_covariates">discrete_covariates</code></td>
<td>
<p>character, discrete covariates to use for the
surface. Note that discrete covariates are currently assumed to be one-hot
encoded with columns named <code style="white-space: pre;">&#8288;{fct_nm}_{lvl_nm}&#8288;</code>. Names supplied to
discrete_covariates should match <code>fct_nm</code>.</p>
</td></tr>
<tr><td><code id="CATESurface_+3A_estimate_variance">estimate_variance</code></td>
<td>
<p>boolean, If <code>TRUE</code>, the variance of CATE estimates
is computed.</p>
</td></tr>
<tr><td><code id="CATESurface_+3A_grid">grid</code></td>
<td>
<p>list, points in which to predict CATE along continuous
covariates. Index i in the list should contain a numeric vectors with
either a single integer, specifying the number of equally spaced points
within the range of the i'th continuous covariate in which to calculate the
CATE, or a numeric vector with manually specified points in which to
calculate the CATE along the i'th continuous covariate. If all elements of
grid specify a number of points, this can be supplied using a numeric
vector. If the list is named, the names must match the continuous
covariates. grid will be reordered to match the order of
continuous_covariates.</p>
</td></tr>
<tr><td><code id="CATESurface_+3A_fixed_covariate_fct">fixed_covariate_fct</code></td>
<td>
<p>Function applied to covariates not in the
sub-surface which returns the fixed value of the covariate used to
calculate the CATE. Must be specified in one of the following ways:
</p>

<ul>
<li><p> A named function, e.g. <code>mean</code>.
</p>
</li>
<li><p> An anonymous function, e.g. <code>\(x) x + 1</code> or <code>function(x) x + 1</code>.
</p>
</li>
<li><p> A formula, e.g. <code>~ .x + 1</code>. You must use <code>.x</code> to refer to the
first argument. Only recommended if you require backward compatibility with
older versions of R.
</p>
</li>
<li><p> A string, integer, or list, e.g. <code>"idx"</code>, <code>1</code>, or <code>list("idx", 1)</code> which
are shorthand for <code>\(x) purrr::pluck(x, "idx")</code>, <code>\(x)
  purrr::pluck(x, 1)</code>, and <code>\(x) purrr::pluck(x, "idx", 1)</code>
respectively. Optionally supply <code>.default</code> to set a default value if the
indexed element is <code>NULL</code> or does not exist.
</p>
</li></ul>
</td></tr>
<tr><td><code id="CATESurface_+3A_other_discrete">other_discrete</code></td>
<td>
<p>A data frame, data frame extension (e.g. a tibble), or
a lazy data frame (e.g. from dbplyr or dtplyr) with columns <code>covs</code> and
<code>lvl</code>. Used to specify the level of each discrete covariate to use when
calculating the CATE. assumes the use of one-hot encoding. <code>covs</code> must
contain the name of discrete covariates, and <code>lvl</code> the level to use. Set to
<code>NULL</code> if none of the fixed covariates are discrete using one-hot-encoding.</p>
</td></tr>
<tr><td><code id="CATESurface_+3A_max_predict_size">max_predict_size</code></td>
<td>
<p>integer, maximum number of examples to predict at a
time. If the surface has more points than max_predict_size, the prediction
is split up into an appropriate number of chunks.</p>
</td></tr>
<tr><td><code id="CATESurface_+3A_num_threads">num_threads</code></td>
<td>
<p>Number of threads used in training. If set to <code>NULL</code>, the
software automatically selects an appropriate amount.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Tibble with the predicted CATE's on the specified surface in the
covariate space. The tibble has columns for each covariate used to train
the input forest, as well as columns output from
<a href="grf.html#topic+predict.causal_forest">predict.causal_forest</a>().
</p>


<h3>Author(s)</h3>

<p>KIJA
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 1000
p &lt;- 3
X &lt;- matrix(rnorm(n * p), n, p) |&gt; as.data.frame()
X_d &lt;- data.frame(
  X_d1 = factor(sample(1:3, n, replace = TRUE)),
  X_d2 = factor(sample(1:3, n, replace = TRUE))
)
X_d &lt;- DiscreteCovariatesToOneHot(X_d)
X &lt;- cbind(X, X_d)
W &lt;- rbinom(n, 1, 0.5)
event_prob &lt;- 1 / (1 + exp(2 * (pmax(2 * X[, 1], 0) * W - X[, 2])))
Y &lt;- rbinom(n, 1, event_prob)
cf &lt;- grf::causal_forest(X, Y, W)
cate_surface &lt;- CATESurface(
  cf,
  continuous_covariates = paste0("V", 1:2),
  discrete_covariates = "X_d1",
  grid = list(
    V1 = 10,
    V2 = -5:5
  ),
  other_discrete = data.frame(
    covs = "X_d2",
    lvl = "4"
  )
)


</code></pre>

<hr>
<h2 id='CausalForestDynamicSubgroups'>Calculate CATE in dynamically determined subgroups</h2><span id='topic+CausalForestDynamicSubgroups'></span>

<h3>Description</h3>

<p>Determines subgroups ranked by CATE estimates from a causal_forest object,
then calculates comparable CATE estimates in each subgroup and tests for
differences.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CausalForestDynamicSubgroups(forest, n_rankings = 3, n_folds = 5, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CausalForestDynamicSubgroups_+3A_forest">forest</code></td>
<td>
<p>An object of class <code>causal_forest</code>, as returned by
<a href="grf.html#topic+causal_forest">causal_forest</a>().</p>
</td></tr>
<tr><td><code id="CausalForestDynamicSubgroups_+3A_n_rankings">n_rankings</code></td>
<td>
<p>Integer, scalar with number of groups to rank CATE's into.</p>
</td></tr>
<tr><td><code id="CausalForestDynamicSubgroups_+3A_n_folds">n_folds</code></td>
<td>
<p>Integer, scalar with number of folds to split data into.</p>
</td></tr>
<tr><td><code id="CausalForestDynamicSubgroups_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <a href="grf.html#topic+causal_forest">causal_forest</a>() and
<a href="grf.html#topic+regression_forest">regression_forest</a>().</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To evaluate heterogeneity in treatment effect one can split data
into groups by estimated CATE (for an alternative, see also
<a href="#topic+RATEOmnibusTest">RATEOmnibusTest</a>). To compare estimates one must use a
model which is not trained on the subjects we wish to compare. To achieve
this, data is partitioned into n_folds folds and a causal forest is trained
for each fold where the fold is left out. If the data has no existing
clustering, one <a href="grf.html#topic+causal_forest">causal_forest</a>() is trained with the folds as
clustering structure. This enables predictions on each fold where trees
using data from the fold are left out for the prediction. In the case of
preexisting clustering in the data, folds are sampled within each cluster
and combined across clusters afterwards.
</p>


<h3>Value</h3>

<p>A list with elements
</p>

<ul>
<li><p> forest_subgroups: A tibble with CATE estimates, ranking, and AIPW-scores
for each subject.
</p>
</li>
<li><p> forest_rank_ate: A tibble with the ATE estimate and standard error of
each subgroup.
</p>
</li>
<li><p> forest_rank_diff_test: A tibble with estimates of the difference in ATE
between subgroups and p-values for a formal test of no difference.
</p>
</li>
<li><p> heatmap_data: A tibble with data used to draw a heatmap of covariate
distribution in each subgroup.
</p>
</li>
<li><p> forest_rank_ate_plot: ggplot with the ATE estimates in each subgroup.
</p>
</li>
<li><p> heatmap: ggplot with heatmap of covariate distribution in each subgroup.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>KIJA
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 800
p &lt;- 3
X &lt;- matrix(rnorm(n * p), n, p) |&gt; as.data.frame()
W &lt;- rbinom(n, 1, 0.5)
event_prob &lt;- 1 / (1 + exp(2 * (pmax(2 * X[, 1], 0) * W - X[, 2])))
Y &lt;- rbinom(n, 1, event_prob)
cf &lt;- grf::causal_forest(X, Y, W)
cf_ds &lt;- CausalForestDynamicSubgroups(cf, 2, 4)


</code></pre>

<hr>
<h2 id='ceiling_dec'>Round numbers up to a given number of decimal places</h2><span id='topic+ceiling_dec'></span>

<h3>Description</h3>

<p>Round numbers up to a given number of decimal places
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ceiling_dec(x, digits = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ceiling_dec_+3A_x">x</code></td>
<td>
<p>a numeric vector</p>
</td></tr>
<tr><td><code id="ceiling_dec_+3A_digits">digits</code></td>
<td>
<p>integer indicating the number of decimal places</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The rounded up numeric vector
</p>

<hr>
<h2 id='CForBenefit'>c-for-benefit</h2><span id='topic+CForBenefit'></span>

<h3>Description</h3>

<p>Calculates the c-for-benefit, as proposed by D. van Klaveren et al. (2018),
by matching patients based on patient characteristics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CForBenefit(
  forest,
  match = c("covariates", "CATE"),
  match_method = "nearest",
  match_distance = "mahalanobis",
  tau_hat_method = c("risk_diff", "tau_avg"),
  CI = c("simple", "bootstrap", "none"),
  level = 0.95,
  n_bootstraps = 999L,
  time_limit = Inf,
  time_limit_CI = Inf,
  verbose = TRUE,
  Y = NULL,
  W = NULL,
  X = NULL,
  p_0 = NULL,
  p_1 = NULL,
  tau_hat = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CForBenefit_+3A_forest">forest</code></td>
<td>
<p>An object of class <code>causal_forest</code>, as returned by
<a href="grf.html#topic+causal_forest">causal_forest</a>().</p>
</td></tr>
<tr><td><code id="CForBenefit_+3A_match">match</code></td>
<td>
<p>character, <code>"covariates"</code> to match on covariates or <code>"CATE"</code> to
match on estimated CATE.</p>
</td></tr>
<tr><td><code id="CForBenefit_+3A_match_method">match_method</code></td>
<td>
<p>see <a href="MatchIt.html#topic+matchit">matchit</a>.</p>
</td></tr>
<tr><td><code id="CForBenefit_+3A_match_distance">match_distance</code></td>
<td>
<p>see <a href="MatchIt.html#topic+matchit">matchit</a>.</p>
</td></tr>
<tr><td><code id="CForBenefit_+3A_tau_hat_method">tau_hat_method</code></td>
<td>
<p>character, <code>"risk_diff"</code> to calculate the expected
treatment effect in matched groups as the risk under treatment for the
treated subject minus the risk under control for the untreated subject.
<code>"tau_avg"</code> to calculate it as the average treatment effect of matched
subject.</p>
</td></tr>
<tr><td><code id="CForBenefit_+3A_ci">CI</code></td>
<td>
<p>character, <code>"none"</code> for no confidence interval, <code>"simple"</code> to use a
normal approximation, and <code>"bootstrap"</code> to use the bootstrap.</p>
</td></tr>
<tr><td><code id="CForBenefit_+3A_level">level</code></td>
<td>
<p>numeric, confidence level of the confidence interval.</p>
</td></tr>
<tr><td><code id="CForBenefit_+3A_n_bootstraps">n_bootstraps</code></td>
<td>
<p>numeric, number of bootstraps to use for the bootstrap
confidence interval computation.</p>
</td></tr>
<tr><td><code id="CForBenefit_+3A_time_limit">time_limit</code></td>
<td>
<p>numeric, maximum allowed time to compute C-for-benefit. If
limit is reached, execution stops.</p>
</td></tr>
<tr><td><code id="CForBenefit_+3A_time_limit_ci">time_limit_CI</code></td>
<td>
<p>numeric, maximum time allowed to compute the bootstrap
confidence interval. If limit is reached, the user is asked if execution
should continue or be stopped.</p>
</td></tr>
<tr><td><code id="CForBenefit_+3A_verbose">verbose</code></td>
<td>
<p>boolean, TRUE to display progress bar, FALSE to not display
progress bar.</p>
</td></tr>
<tr><td><code id="CForBenefit_+3A_y">Y</code></td>
<td>
<p>a vector of outcomes. If provided, replaces <code>forest$Y.orig</code>.</p>
</td></tr>
<tr><td><code id="CForBenefit_+3A_w">W</code></td>
<td>
<p>a vector of treatment assignment; 1 for active treatment; 0 for
control If provided, replaces <code>forest$W.orig</code>.</p>
</td></tr>
<tr><td><code id="CForBenefit_+3A_x">X</code></td>
<td>
<p>a matrix of patient characteristics. If provided, replaces
<code>forest$X.orig</code>.</p>
</td></tr>
<tr><td><code id="CForBenefit_+3A_p_0">p_0</code></td>
<td>
<p>a vector of outcome probabilities under control.</p>
</td></tr>
<tr><td><code id="CForBenefit_+3A_p_1">p_1</code></td>
<td>
<p>a vector of outcome probabilities under active treatment.</p>
</td></tr>
<tr><td><code id="CForBenefit_+3A_tau_hat">tau_hat</code></td>
<td>
<p>a vector of individualized treatment effect predictions. If
provided, replaces forest$predictions.</p>
</td></tr>
<tr><td><code id="CForBenefit_+3A_...">...</code></td>
<td>
<p>additional arguments for <a href="MatchIt.html#topic+matchit">matchit</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The c-for-benefit statistic is inspired by the c-statistic used with
prediction models to measure discrimination. The c-statistic takes all
pairs of observations discordant on the outcome, and calculates the
proportion of these where the subject with the higher predicted probability
was the one who observed the outcome. In order to extend this to treatment
effects, van Klaveren et al. suggest matching a treated subject to a
control subject on the predicted treatments effect (or alternatively the
covariates) and defining the observed effect as the difference between the
outcomes of the treated subject and the control subject. The c-for-benefit
statistic is then defined as the proportion of matched pairs with unequal
observed effect in which the subject pair receiving greater treatment
effect also has the highest expected treatment effect. <br /> When calculating
the expected treatment effect, van Klaveren et al. use the average CATE
from the matched subjects in a pair (tau_hat_method = &quot;mean&quot;). However,
this doesn't match the observed effect used, unless the baseline risks are
equal. The observed effect is the difference between the observed outcome
for the subject receiving treatment and the observed outcome for the
subject receiving control. Their outcomes are governed by the exposed risk
and the baseline risk respectively. The baseline risks are ideally equal
when covariate matching, although instability of the forest estimates can
cause significantly different baseline risks due to non-exact matching.
When matching on CATE, we should not expect baseline risks to be equal.
Instead, we can more closely match the observed treatment effect by using
the difference between the exposed risk for the subject receiving treatment
and the baseline risk of the subject receiving control (tau_hat_method =
&quot;treatment&quot;).
</p>


<h3>Value</h3>

<p>a list with the following components:
</p>

<ul>
<li><p> type: a list with the input provided to the function which determines
how C-for-benefit is computed.
</p>
</li>
<li><p> matched_patients: a tibble containing the matched patients.
</p>
</li>
<li><p> c_for_benefit: the resulting C-for-benefit value.
</p>
</li>
<li><p> lower_CI: the lower bound of the confidence interval (if CI = TRUE).
</p>
</li>
<li><p> upper_CI: the upper bound of the confidence interval (if CI = TRUE).
</p>
</li></ul>



<h3>Author(s)</h3>

<p>KIJA
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 800
p &lt;- 3
X &lt;- matrix(rnorm(n * p), n, p)
W &lt;- rbinom(n, 1, 0.5)
event_prob &lt;- 1 / (1 + exp(2 * (pmax(2 * X[, 1], 0) * W - X[, 2])))
Y &lt;- rbinom(n, 1, event_prob)
cf &lt;- grf::causal_forest(X, Y, W)
CB_out &lt;- CForBenefit(
forest = cf, CI = "bootstrap", n_bootstraps = 20L, verbose = TRUE,
match_method = "nearest", match_distance = "mahalanobis"
)


</code></pre>

<hr>
<h2 id='charlson_score'>Charlson Score Constructor</h2><span id='topic+charlson_score'></span>

<h3>Description</h3>

<p>Charlson comorbidity score for Danish ICD-10 and ICD-8 data. This is a
SAS-macro ASO translated to R in March of 2022
</p>


<h3>Usage</h3>

<pre><code class='language-R'>charlson_score(
  data,
  Person_ID,
  diagnosis_variable,
  time_variable = NULL,
  end_date = NULL,
  days_before_end_date = NULL,
  amount_output = "total"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="charlson_score_+3A_data">data</code></td>
<td>
<p>A data.frame with at least an id variable and a variable with all
diagnosis codes. The data should be in the long format (only one variable
with diagnoses, but several lines per person is OK).</p>
</td></tr>
<tr><td><code id="charlson_score_+3A_person_id">Person_ID</code></td>
<td>
<p>&lt;<code><a href="dplyr.html#topic+dplyr_data_masking">data-masking</a></code>&gt; An unquoted
expression naming the id variable in <code>data</code>. This variable must always be
specified.</p>
</td></tr>
<tr><td><code id="charlson_score_+3A_diagnosis_variable">diagnosis_variable</code></td>
<td>
<p>&lt;<code><a href="dplyr.html#topic+dplyr_data_masking">data-masking</a></code>&gt; An unquoted
expression naming the diagnosis variable in <code>data</code>. This variable must
always be specified.</p>
</td></tr>
<tr><td><code id="charlson_score_+3A_time_variable">time_variable</code></td>
<td>
<p>&lt;<code><a href="dplyr.html#topic+dplyr_data_masking">data-masking</a></code>&gt; An unquoted
expression naming the diagnosis time variable in <code>data</code> if needed. The
<code>time_variable</code> must be in a date format.
</p>
<p>When <code>time_variable</code> is specified, <code>end_date</code> must also be specified.</p>
</td></tr>
<tr><td><code id="charlson_score_+3A_end_date">end_date</code></td>
<td>
<p>&lt;<code><a href="dplyr.html#topic+dplyr_data_masking">data-masking</a></code>&gt; An unquoted
expression naming the end of time-period to search for relevant diagnoses
or a single date specifying the end date. If <code>end_date</code> names a variable,
this variable must be in a date format.</p>
</td></tr>
<tr><td><code id="charlson_score_+3A_days_before_end_date">days_before_end_date</code></td>
<td>
<p>A numeric specifying the number of days look-back
from <code>end_date</code> to search for relevant diagnoses.</p>
</td></tr>
<tr><td><code id="charlson_score_+3A_amount_output">amount_output</code></td>
<td>
<p>A character specifying whether all created index
variables should be returned. When <code>amount_output</code> is &quot;total&quot; (the default)
only the resulting Charlson scores are returned, otherwise all disease-
specific index variables are returned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>charlson_score()</code> function calculates the Charlson Charlson Comorbidity
Index for each person. Three different variations on the score has been
implemented:
</p>

<ul>
<li><p> cc: Article from Quan et al. (Coding Algorithms for Defining
Comorbidities in ICD-9 and ICD-10 Administrative Data, Med Care 2005:43:
1130-1139), the same HTR and others have used - ICD10 only
</p>
</li>
<li><p> ch: Article from Christensen et al. (Comparison of Charlson
comorbidity index with SAPS and APACHE sources for prediction of mortality
following intensive care, Clinical Epidemiology 2011:3 203-211), include
ICD8 and ICD10 but the included diagnoses are not the same as in Quan
</p>
</li>
<li><p> cd: Article from Sundarajan et al. (New ICD-10 version of Charlson
Comorbidity Index predicted in-hospital mortality, Journal of clinical
Epidemiology 57 (2004) 1288-1294, include ICD10 = Charlson-Deyo including
cancer
</p>
</li></ul>



<h3>Value</h3>

<p>If <code>Person_ID</code> and <code>diagnosis_variable</code> are the only specifications, the
function will calculate the different versions of the Charlson score on all
data available for each person, regardless of timing etc. This is OK if only
relevant records are included.
</p>


<h3>NOTE</h3>

<p>The diagnoses to use in this function at the current state should be either
ICD-8, but preferably ICD-10. The ICD-10 codes should start with two letters,
where the first one is &quot;D&quot;. Furthermore, the code should only have letters
and digits (i.e. the form &quot;DA000&quot; not &quot;DA00.0&quot;)
</p>


<h3>Author(s)</h3>

<p>ASO &amp; ADLS
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# An example dataset

test_data &lt;- data.frame(
  IDs = c(
    1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,
    17, 18, 19, 20, 21, 22, 22, 23, 23, 24, 24, 24, 24, 24
  ),
  Diags = c(
    "DZ36", "DZ38", "DZ40", "DZ42", "DC20", "DI252",
    "DP290", "DI71", "DH340", "DG30", "DJ40", "DM353",
    "DK26", "DK700", "DK711", "DE106", "DE112", "DG82",
    "DZ940", "DC80", "DB20", "DK74", "DK704", "DE101",
    "DE102", "DB20", "DK74", "DK704", "DE101", "DE102"
 ),
 time = as.Date(c(
   "2001-01-30", "2004-05-20", "2007-01-02", "2013-12-01",
   "2017-04-30", "2001-01-30", "2004-05-20", "2007-01-02",
   "2013-12-01", "2017-04-30", "2001-01-30", "2004-05-20",
   "2007-01-02", "2013-12-01", "2017-04-30", "2001-01-30",
   "2004-05-20", "2007-01-02", "2013-12-01", "2017-04-30",
   "2001-01-30", "2004-05-20", "2007-01-02", "2013-12-01",
   "2017-04-30", "2001-01-30", "2004-05-20", "2007-01-02",
   "2013-12-01", "2017-04-30"
 )),
 match_date = as.Date(c(
   "2001-10-15", "2005-10-15", "2011-10-15", "2021-10-15",
   "2021-10-15", "2001-10-15", "2005-10-15", "2011-10-15",
   "2021-10-15", "2021-10-15", "2001-10-15", "2005-10-15",
   "2011-10-15", "2021-10-15", "2021-10-15", "2001-10-15",
   "2005-10-15", "2011-10-15", "2021-10-15", "2021-10-15",
   "2001-10-15", "2005-10-15", "2011-10-15", "2021-10-15",
   "2021-10-15", "2001-10-15", "2005-10-15", "2011-10-15",
   "2021-10-15", "2021-10-15"
 ))
)

# Minimal example
charlson_score(
  data = test_data,
  Person_ID = IDs,
  diagnosis_variable = Diags
)

# Minimal example with all index diagnosis variables
charlson_score(
  data = test_data,
  Person_ID = IDs,
  diagnosis_variable = Diags,
  amount_output = "all"
)

# Imposing uniform date restrictions to diagnoses
charlson_score(
  data = test_data,
  Person_ID = IDs,
  diagnosis_variable = Diags,
  time_variable = time,
  end_date = as.Date("2012-01-01")
)

# Imposing differing date restriction to diagnoses
charlson_score(
  data = test_data,
  Person_ID = IDs,
  diagnosis_variable = Diags,
  time_variable = time,
  end_date = match_date
)

# Imposing both a start and end to the lookup period for
# relevant diagnoses
charlson_score(
  data = test_data,
  Person_ID = IDs,
  diagnosis_variable = Diags,
  time_variable = time,
  end_date = match_date,
  days_before_end_date = 365.25
)


</code></pre>

<hr>
<h2 id='ci_fct'>solve optimization problem for  CI bounds</h2><span id='topic+ci_fct'></span>

<h3>Description</h3>

<p>solve optimization problem for each coordinate of f, to obtain the uniform
limit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ci_fct(i, f, xtx_red, beta_hat, which_parm, level, n_grid, k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ci_fct_+3A_i">i</code></td>
<td>
<p>An index for the point at which to solve for confidence limits.</p>
</td></tr>
<tr><td><code id="ci_fct_+3A_f">f</code></td>
<td>
<p>A function taking the parameter vector as its single argument, and
returning a numeric vector.</p>
</td></tr>
<tr><td><code id="ci_fct_+3A_xtx_red">xtx_red</code></td>
<td>
<p>Reduced form of matrix <em>X^TX</em>.</p>
</td></tr>
<tr><td><code id="ci_fct_+3A_beta_hat">beta_hat</code></td>
<td>
<p>Vector of parameter estimates.</p>
</td></tr>
<tr><td><code id="ci_fct_+3A_which_parm">which_parm</code></td>
<td>
<p>Vector indicating which parameters to include.</p>
</td></tr>
<tr><td><code id="ci_fct_+3A_level">level</code></td>
<td>
<p>The confidence level required.</p>
</td></tr>
<tr><td><code id="ci_fct_+3A_n_grid">n_grid</code></td>
<td>
<p>Either <code>NULL</code> or an integer vector of length 1 or the number of
<code>TRUE</code>/indices in which_parm. Specifies the number of grid points in each
dimension of a grid with endpoints defined by len. If <code>NULL</code> or <code>0L</code>, will
instead sample k points uniformly on a sphere.</p>
</td></tr>
<tr><td><code id="ci_fct_+3A_k">k</code></td>
<td>
<p>If n_grid is <code>NULL</code> or <code>0L</code>, the number of points to sample
uniformly from a sphere.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>One row tibble with estimate and confidence limits.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>1+1
</code></pre>

<hr>
<h2 id='ci_fct_error_handler'>Handle errors returned by ci_fct</h2><span id='topic+ci_fct_error_handler'></span>

<h3>Description</h3>

<p>Handle errors returned by ci_fct
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ci_fct_error_handler(e, which_parm, env)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ci_fct_error_handler_+3A_e">e</code></td>
<td>
<p>error returned by ci_fct</p>
</td></tr>
<tr><td><code id="ci_fct_error_handler_+3A_which_parm">which_parm</code></td>
<td>
<p>Either a logical vector the same length as the coefficient
vector, with <code>TRUE</code> indicating a coefficient is used by <code>f</code>, or an integer
vector with the indices of the coefficients used by <code>f</code>.</p>
</td></tr>
<tr><td><code id="ci_fct_error_handler_+3A_env">env</code></td>
<td>
<p>environment to assign n_grid and k</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns NULL if no stop command is executed.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>1+1
</code></pre>

<hr>
<h2 id='CovariateBalance'>Plots for checking covariate balance in causal forest</h2><span id='topic+CovariateBalance'></span>

<h3>Description</h3>

<p>Generate plots showing balance in the covariates before and after propensity
score weighting with a causal forest object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CovariateBalance(
  cf,
  plots = c("all", "Love", "density", "ecdf"),
  balance_table = TRUE,
  covariates = NULL,
  names = NULL,
  factor = NULL,
  treatment_name = "W",
  love_breaks = NULL,
  love_xlim = NULL,
  love_scale_color = NULL,
  cd_nrow = NULL,
  cd_ncol = NULL,
  cd_x_scale_width = NULL,
  cd_bar_width = NULL,
  cd_scale_fill = NULL,
  ec_nrow = NULL,
  ec_ncol = NULL,
  ec_x_scale_width = NULL,
  ec_scale_color = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CovariateBalance_+3A_cf">cf</code></td>
<td>
<p>An object of class causal_forest (and inheriting from class grf).</p>
</td></tr>
<tr><td><code id="CovariateBalance_+3A_plots">plots</code></td>
<td>
<p>Character, <code>"all"</code> returns both Love plots and density plots,
<code>"Love"</code> returns only Love plots, <code>"density"</code> returns only density plots.</p>
</td></tr>
<tr><td><code id="CovariateBalance_+3A_balance_table">balance_table</code></td>
<td>
<p>Boolean, TRUE to return a table with balance statistics.</p>
</td></tr>
<tr><td><code id="CovariateBalance_+3A_covariates">covariates</code></td>
<td>
<p>A vector to select covariates to show in balance plots. If
<code>cf$X.orig</code> is an unnamed matrix, use a numeric vector to select variables.
Otherwise use a character vector. Names provided in the <code>names</code> argument
takes priority over existing names in <code>cf$X.orig</code>. If discrete covariates
have been one-hot encoded using <a href="#topic+DiscreteCovariatesToOneHot">DiscreteCovariatesToOneHot</a>
the name of these discrete covariates can be provided in <code>covariates</code> to
select it and to collect all levels into a bar plot to show the
distribution.</p>
</td></tr>
<tr><td><code id="CovariateBalance_+3A_names">names</code></td>
<td>
<p>A named character vector. The vector itself should contain
covariate names from the causal_forest object, while the names attribute
should contain the names to use when plotting. If discrete covariates have
been one-hot encoded using <a href="#topic+DiscreteCovariatesToOneHot">DiscreteCovariatesToOneHot</a>,
providing just the name of a discrete covariate will modify the name of all
levels for plotting. If the vector is unnamed, the provided vector will act
as the new covariate names, given in the order of <code>cf$X_orig</code>. If <code>NULL</code>
(the default), the original names are used.</p>
</td></tr>
<tr><td><code id="CovariateBalance_+3A_factor">factor</code></td>
<td>
<p>A named list with covariates to be converted to factor. Note
that one-hot encoded covariates are automatically converted, so need not be
specified in the factor argument. Each component of the list must contain
the factor levels, using a named vector to supply custom labels.</p>
</td></tr>
<tr><td><code id="CovariateBalance_+3A_treatment_name">treatment_name</code></td>
<td>
<p>Character, name of treatment.</p>
</td></tr>
<tr><td><code id="CovariateBalance_+3A_love_breaks">love_breaks</code></td>
<td>
<p>Numeric, breaks used in the plot of absolute standardized
mean differences.</p>
</td></tr>
<tr><td><code id="CovariateBalance_+3A_love_xlim">love_xlim</code></td>
<td>
<p>Numeric, <code>x</code>-limits used in the plot of absolute
standardized mean differences.</p>
</td></tr>
<tr><td><code id="CovariateBalance_+3A_love_scale_color">love_scale_color</code></td>
<td>
<p>Function, <code>scale_color_.</code> function to use in the plot
of absolute standardized mean differences.</p>
</td></tr>
<tr><td><code id="CovariateBalance_+3A_cd_nrow">cd_nrow</code>, <code id="CovariateBalance_+3A_cd_ncol">cd_ncol</code></td>
<td>
<p>Numeric, the dimensions of the grid to create in
covariate distribution plots. If both are <code>NULL</code> it will use the same logic
as <a href="ggplot2.html#topic+facet_wrap">facet_wrap</a> to set the dimensions.</p>
</td></tr>
<tr><td><code id="CovariateBalance_+3A_cd_x_scale_width">cd_x_scale_width</code></td>
<td>
<p>Numeric, the distance between major <code>x</code>-axis tics in
the covariate distribution plots. If <code>NULL</code>, a width is chosen to display
approximately six major tics. If length 1, the same width is used for all
covariate plots. If the same length as the number of covariates included,
each number is used as the width for different covariates, in the order of
the covariates after selection with the tidy-select expression in
<code>covariates</code>.</p>
</td></tr>
<tr><td><code id="CovariateBalance_+3A_cd_bar_width">cd_bar_width</code></td>
<td>
<p>Numeric, the width of the bars in the covariate
distribution plots (barplots for categorical variables, histograms for
continuous variables). If <code>NULL</code>, a width is chosen to display
approximately 50 bars in histograms, while 0.9 times the resolution of the
data is used in bar plots. If length 1, the same width is used for all
covariate plots. This is not recommended if there are both categorical and
continuous covariates. If the same length as the number of covariates
included, each number is used as the bar width for different covariates, in
the order of the covariates after selection with the tidy-select expression
in <code>covariates</code>.</p>
</td></tr>
<tr><td><code id="CovariateBalance_+3A_cd_scale_fill">cd_scale_fill</code></td>
<td>
<p>Function, <code>scale_fill_.</code> function to use in covariate
distribution plots.</p>
</td></tr>
<tr><td><code id="CovariateBalance_+3A_ec_nrow">ec_nrow</code>, <code id="CovariateBalance_+3A_ec_ncol">ec_ncol</code></td>
<td>
<p>Numeric, the dimensions of the grid to create in
empirical CDF plots. If both are <code>NULL</code> it will use the same logic
as <a href="ggplot2.html#topic+facet_wrap">facet_wrap</a> to set the dimensions.</p>
</td></tr>
<tr><td><code id="CovariateBalance_+3A_ec_x_scale_width">ec_x_scale_width</code></td>
<td>
<p>Numeric, the distance between major <code>x</code>-axis tics in
the empirical CDF plots. If <code>NULL</code>, a width is chosen to display
approximately six major tics. If length 1, the same width is used for all
plots. If the same length as the number of covariates included, each number
is used as the width for different covariates, in the order of the
covariates after selection with the tidy-select expression in <code>covariates</code>.</p>
</td></tr>
<tr><td><code id="CovariateBalance_+3A_ec_scale_color">ec_scale_color</code></td>
<td>
<p>Function, <code>scale_color_.</code> function to use in empirical
CDF plots.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If an unnamed character vector is provided in <code>names</code>, it must have length
<code>ncol(cf$X.orig)</code>. Names of covarates not selected by <code>covariates</code> can be set
to <code>NA</code>. If a named character vector is provided in <code>names</code>, all renamed
covariates will be kept regardless if they are selected in <code>covariates</code>.
Thus to select only renamed covariates, <code>character(0)</code> can be used in
<code>covariates</code>. The plot theme can be adjusted using ggplot2 active theme
modifiers, see <a href="ggplot2.html#topic+theme_get">theme_get</a>.
</p>


<h3>Value</h3>

<p>A list with up to five elements:
</p>

<ul>
<li><p> love_data: data used to plot the absolute standardized mean differences.
</p>
</li>
<li><p> love: plot object for absolute standardized mean differences.
</p>
</li>
<li><p> cd_data: data used to plot covariate distributions.
</p>
</li>
<li><p> cd_unadjusted: plot of unadjusted covariate distributions in the exposure
groups.
</p>
</li>
<li><p> cd_adjusted: plot of adjusted covariate distributions in the exposure
groups.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>KIJA
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 1000
p &lt;- 5
X &lt;- matrix(rnorm(n * p), n, p) |&gt;
as.data.frame() |&gt;
dplyr::bind_cols(
  DiscreteCovariatesToOneHot(
    dplyr::tibble(
      D1 = factor(
        sample(1:3, n, replace = TRUE, prob = c(0.2, 0.3, 0.5)),
        labels = c("first", "second", "third")
      ),
      D2 = factor(
        sample(1:2, n, replace = TRUE, prob = c(0.2, 0.8)),
        labels = c("a", "b")
      )
    )
  )
) |&gt;
dplyr::select(
  V1,
  V2,
  dplyr::starts_with("D1"),
  V3,
  V4,
  dplyr::starts_with("D2"),
  V5
)
expo_prob &lt;- 1 / (1 + exp(0.4 * X[, 1] + 0.2 * X[, 2] - 0.6 * X[, 3] +
                          0.4 * X[, 6] + 0.6 * X[, 8] - 0.2 * X[, 9]))
W &lt;- rbinom(n, 1, expo_prob)
event_prob &lt;- 1 / (1 + exp(2 * (pmax(2 * X[, 1], 0) * W - X[, 2] +
                           X[, 6] + 3 * X[, 9])))
Y &lt;- rbinom(n, 1, event_prob)
cf &lt;- grf::causal_forest(X, Y, W)
cb1 &lt;- CovariateBalance(cf)
cb2 &lt;- CovariateBalance(
  cf,
  covariates = character(0),
  names = c(
  "medium imbalance" = "V1",
  "low imbalance" = "V2",
  "high imbalance" = "V3",
  "no imbalance" = "V4",
  "discrete 1" = "D1",
  "discrete 2" = "D2"
  )
)
cb3 &lt;- CovariateBalance(
  cf,
  covariates = character(0),
  names = c(
    "medium imbalance" = "V1",
    "low imbalance" = "V2",
    "high imbalance" = "V3",
    "no imbalance" = "V4"
  ),
  treatment_name = "Treatment",
  love_breaks = seq(0, 0.5, 0.1),
  love_xlim = c(0, 0.5),
  cd_nrow = 2,
  cd_x_scale_width = 1,
  cd_bar_width = 0.3
)


</code></pre>

<hr>
<h2 id='decimalplaces'>Determine number of decimal places</h2><span id='topic+decimalplaces'></span>

<h3>Description</h3>

<p>Determine number of decimal places
</p>


<h3>Usage</h3>

<pre><code class='language-R'>decimalplaces(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="decimalplaces_+3A_x">x</code></td>
<td>
<p>Numeric, a single decimal number</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The number of decimal places in x
</p>

<hr>
<h2 id='DiscreteCovariateNames'>Extract discrete covariate names</h2><span id='topic+DiscreteCovariateNames'></span>

<h3>Description</h3>

<p>Detect elements in covariates which match a string from the discrete_covariates
argument.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DiscreteCovariateNames(covariates, discrete_covariates = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DiscreteCovariateNames_+3A_covariates">covariates</code></td>
<td>
<p>character, names of covariates</p>
</td></tr>
<tr><td><code id="DiscreteCovariateNames_+3A_discrete_covariates">discrete_covariates</code></td>
<td>
<p>character, names of discrete covariates. Currently
it is assumed that discrete covariates are one-hot encoded with naming in
covariates following <code style="white-space: pre;">&#8288;{fct_nm}_{lvl_nm}&#8288;</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector with elements from covariates matching the names
supplied in discrete_covariates.
</p>


<h3>Author(s)</h3>

<p>KIJA
</p>


<h3>Examples</h3>

<pre><code class='language-R'>one_hot_df &lt;- mtcars |&gt;
  dplyr::mutate(across(c(2, 8:11), factor)) |&gt;
  as.data.frame() |&gt;
  DiscreteCovariatesToOneHot(cyl)
EpiForsk:::DiscreteCovariateNames(colnames(one_hot_df), c("cyl"))
</code></pre>

<hr>
<h2 id='DiscreteCovariatesToOneHot'>One-hot encode factors</h2><span id='topic+DiscreteCovariatesToOneHot'></span>

<h3>Description</h3>

<p>Convert factors in a data frame to one-hot encoding.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DiscreteCovariatesToOneHot(df, factors = dplyr::everything())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DiscreteCovariatesToOneHot_+3A_df">df</code></td>
<td>
<p>A data frame, data frame extension (e.g. a tibble), or a lazy data
frame (e.g. from dbplyr or dtplyr).</p>
</td></tr>
<tr><td><code id="DiscreteCovariatesToOneHot_+3A_factors">factors</code></td>
<td>
<p>&lt;<code><a href="dplyr.html#topic+dplyr_tidy_select">tidy-select</a></code>&gt; One or more unquoted
expressions naming factors in df to one-hot encode.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data frame with one-hot encoded factors. One-hot encoded columns
have names <code style="white-space: pre;">&#8288;{fct_nm}_{lvl_nm}&#8288;</code>.
</p>


<h3>Author(s)</h3>

<p>KIJA
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mtcars |&gt;
dplyr::mutate(dplyr::across(c(2, 8:11), factor)) |&gt;
 as.data.frame() |&gt;
 DiscreteCovariatesToOneHot(cyl)
mtcars |&gt;
dplyr::mutate(dplyr::across(c(2, 8:11), factor)) |&gt;
 as.data.frame() |&gt;
 DiscreteCovariatesToOneHot(c(2, 8:11))

</code></pre>

<hr>
<h2 id='fct_confint'>Confidence set for functions of model parameters</h2><span id='topic+fct_confint'></span><span id='topic+fct_confint.lm'></span><span id='topic+fct_confint.glm'></span><span id='topic+fct_confint.lms'></span><span id='topic+fct_confint.default'></span>

<h3>Description</h3>

<p>Computes confidence sets of functions of model parameters by computing a
confidence set of the model parameters and returning the codomain of the
provided function given the confidence set of model parameters as domain.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fct_confint(
  object,
  f,
  which_parm = rep(TRUE, length(coef(object))),
  level = 0.95,
  ...
)

## S3 method for class 'lm'
fct_confint(
  object,
  f,
  which_parm = rep(TRUE, length(coef(object))),
  level = 0.95,
  return_beta = FALSE,
  n_grid = NULL,
  k = NULL,
  len = 0.1,
  parallel = c("sequential", "multisession", "multicore", "cluster"),
  n_cores = 10L,
  ...
)

## S3 method for class 'glm'
fct_confint(
  object,
  f,
  which_parm = rep(TRUE, length(coef(object))),
  level = 0.95,
  return_beta = FALSE,
  n_grid = NULL,
  k = NULL,
  len = 0.1,
  parallel = c("sequential", "multisession", "multicore", "cluster"),
  n_cores = 10L,
  ...
)

## S3 method for class 'lms'
fct_confint(
  object,
  f,
  which_parm = rep(TRUE, length(coef(object))),
  level = 0.95,
  return_beta = FALSE,
  len = 0.1,
  n_grid = 0L,
  k = 1000L,
  parallel = c("sequential", "multisession", "multicore", "cluster"),
  n_cores = 10,
  ...
)

## Default S3 method:
fct_confint(
  object,
  f,
  which_parm = rep(TRUE, length(coef(object))),
  level = 0.95,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fct_confint_+3A_object">object</code></td>
<td>
<p>A fitted model object.</p>
</td></tr>
<tr><td><code id="fct_confint_+3A_f">f</code></td>
<td>
<p>A function taking the parameter vector as its single argument, and
returning a numeric vector.</p>
</td></tr>
<tr><td><code id="fct_confint_+3A_which_parm">which_parm</code></td>
<td>
<p>Either a logical vector the same length as the coefficient
vector, with <code>TRUE</code> indicating a coefficient is used by <code>f</code>, or an integer
vector with the indices of the coefficients used by <code>f</code>.</p>
</td></tr>
<tr><td><code id="fct_confint_+3A_level">level</code></td>
<td>
<p>The confidence level required.</p>
</td></tr>
<tr><td><code id="fct_confint_+3A_...">...</code></td>
<td>
<p>Additional argument(s) passed to methods.</p>
</td></tr>
<tr><td><code id="fct_confint_+3A_return_beta">return_beta</code></td>
<td>
<p>Logical, if <code>TRUE</code> returns both the confidence limits and
the parameter values used from the boundary of the parameter confidence
set.</p>
</td></tr>
<tr><td><code id="fct_confint_+3A_n_grid">n_grid</code></td>
<td>
<p>Either <code>NULL</code> or an integer vector of length 1 or the number of
<code>TRUE</code>/indices in which_parm. Specifies the number of grid points in each
dimension of a grid with endpoints defined by len. If <code>NULL</code> or <code>0L</code>, will
instead sample k points uniformly on a sphere.</p>
</td></tr>
<tr><td><code id="fct_confint_+3A_k">k</code></td>
<td>
<p>If n_grid is <code>NULL</code> or <code>0L</code>, the number of points to sample
uniformly from a sphere.</p>
</td></tr>
<tr><td><code id="fct_confint_+3A_len">len</code></td>
<td>
<p>numeric, the radius of the sphere or box used to define directions
in which to look for boundary points of the parameter confidence set.</p>
</td></tr>
<tr><td><code id="fct_confint_+3A_parallel">parallel</code></td>
<td>
<p>Character, specify how futures are resolved. Default is
&quot;sequential&quot;. Can be &quot;multisession&quot; to resolve in parallel in separate R
sessions, &quot;multicore&quot; (not supported on Windows) to resolve in parallel in
forked R processes, or &quot;cluster&quot; to resolve in parallel in separate R
sessions running on one or more machines.</p>
</td></tr>
<tr><td><code id="fct_confint_+3A_n_cores">n_cores</code></td>
<td>
<p>An integer specifying the number of threads to use for
parallel computing.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Assume the response Y and predictors X are given by a generalized
linear model, that is, they fulfill the assumptions
</p>
<p style="text-align: center;"><code class="reqn">E(Y|X)=\mu(X^T\beta)</code>
</p>

<p style="text-align: center;"><code class="reqn">V(Y|X)=\psi \nu(\mu(X^T\beta))</code>
</p>

<p style="text-align: center;"><code class="reqn">Y|X\sim\varepsilon(\theta,\nu_{\psi}).</code>
</p>

<p>Here <code class="reqn">\mu</code> is the mean value function, <code class="reqn">\nu</code> is the variance
function, and <code class="reqn">\psi</code> is the dispersion parameter in the exponential
dispersion model
<code class="reqn">\varepsilon(\theta,\nu_{\psi})</code>, where
<code class="reqn">\theta</code> is the canonical parameter and <code class="reqn">\nu_{\psi}</code> is
the structure measure. Then it follows from the central limit theorem that
</p>
<p style="text-align: center;"><code class="reqn">\hat\beta\sim N(\beta, (X^TWX)^{-1})</code>
</p>

<p>will be a good approximation in large samples, where <code class="reqn">X^TWX</code> is the
Fisher information of the exponential dispersion model.
</p>
<p>From this, the combinant
</p>
<p style="text-align: center;"><code class="reqn">(\hat\beta-\beta)^TX^TWX(\hat\beta-\beta)</code>
</p>

<p>is an approximate pivot, with a <code class="reqn">\chi_p^2</code> distribution. Then
</p>
<p style="text-align: center;"><code class="reqn">C_{\beta}=\{\beta|(\hat\beta-\beta)^TX^TWX(\hat\beta-\beta)&lt;\chi_p^2(1-\alpha)\}</code>
</p>

<p>is an approximate <code class="reqn">(1-\alpha)</code>-confidence set for the parameter vector
<code class="reqn">\beta</code>. Similarly, confidence sets for sub-vectors of <code class="reqn">\beta</code> can
be obtained by the fact that marginal distributions of normal distributions
are again normally distributed, where the mean vector and covariance matrix
are appropriate subvectors and submatrices.
</p>
<p>Finally, a confidence set for the transformed parameters <code class="reqn">f(\beta)</code>
is obtained as
</p>
<p style="text-align: center;"><code class="reqn">\{f(\beta)|\beta\in C_{\beta}\}</code>
</p>

<p>Note this is a conservative confidence set, since parameters outside the
confidence set of <code class="reqn">\beta</code> can be mapped to the confidence set of the
transformed parameter.
</p>
<p>To determine <code class="reqn">C_{\beta}</code>, <code>fct_confint()</code> uses a convex optimization
program when f is follows DCP rules. Otherwise, it finds the boundary by
taking a number of points around <code class="reqn">\hat\beta</code> and projecting them onto
the boundary. In this case, the confidence set of the transformed parameter
will only be valid if the boundary of <code class="reqn">C_{\beta}</code> is mapped to the
boundary of the confidence set for the transformed parameter.
</p>
<p>The points projected to the boundary are either laid out in a grid around
<code class="reqn">\hat\beta</code>, with the number of points in each direction determined
by <code>n_grid</code>, or uniformly at random on a hypersphere, with the number of
points determined by <code>k</code>. The radius of the grid/sphere is determined by
<code>len</code>.
</p>
<p>To print a progress bar with information about the fitting process, wrap
the call to fct_confint in with_progress, i.e.
<code>progressr::with_progress({result &lt;- fct_confint(object, f)})</code>
</p>


<h3>Value</h3>

<p>A tibble with columns estimate, conf.low, and conf.high or if
return_beta is <code>TRUE</code>, a list with the tibble and the beta values on the
boundary used to calculate the confidence limits.
</p>


<h3>Author(s)</h3>

<p>KIJA
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- 1:5 |&gt;
  purrr::map(
    \(x) {
      name = paste0("cov", x);
      dplyr::tibble("{name}" := rnorm(100, 1))
    }
  ) |&gt;
  purrr::list_cbind() |&gt;
  dplyr::mutate(
  y = rowSums(dplyr::across(dplyr::everything())) + rnorm(100)
  )
lm &lt;- lm(
 as.formula(
  paste0("y ~ 0 + ", paste0(names(data)[names(data) != "y"], collapse = " + "))
 ),
 data
)
fct_confint(lm, sum)
fct_confint(lm, sum, which_parm = 1:3, level = 0.5)

</code></pre>

<hr>
<h2 id='flatten_date_intervals'>Flatten Date Intervals</h2><span id='topic+flatten_date_intervals'></span>

<h3>Description</h3>

<p>A tidyverse compatible function for simplifying time interval data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>flatten_date_intervals(
  data,
  id,
  in_date,
  out_date,
  status = NULL,
  overlap_handling = "most_recent",
  lag = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="flatten_date_intervals_+3A_data">data</code></td>
<td>
<p>A data frame, data frame extension (e.g. a tibble), or a lazy
data frame (e.g. from dbplyr or dtplyr).</p>
</td></tr>
<tr><td><code id="flatten_date_intervals_+3A_id">id</code></td>
<td>
<p>&lt;<code><a href="dplyr.html#topic+dplyr_tidy_select">tidy-select</a></code>&gt; One or more unquoted
expression naming the id variables in data.</p>
</td></tr>
<tr><td><code id="flatten_date_intervals_+3A_in_date">in_date</code></td>
<td>
<p>&lt;<code><a href="dplyr.html#topic+dplyr_data_masking">data-masking</a></code>&gt; One unquoted
expressions naming the start date variable in data.</p>
</td></tr>
<tr><td><code id="flatten_date_intervals_+3A_out_date">out_date</code></td>
<td>
<p>&lt;<code><a href="dplyr.html#topic+dplyr_data_masking">data-masking</a></code>&gt; One unquoted
expression naming the end date variable in data.</p>
</td></tr>
<tr><td><code id="flatten_date_intervals_+3A_status">status</code></td>
<td>
<p>&lt;<code><a href="dplyr.html#topic+dplyr_tidy_select">tidy-select</a></code>&gt; One or more unquoted
expressions naming a status variable in data, such as region or
hospitalization reason.</p>
</td></tr>
<tr><td><code id="flatten_date_intervals_+3A_overlap_handling">overlap_handling</code></td>
<td>
<p>A character naming the method for handling overlaps
within an individuals time when <code>status</code> has been specified.
</p>

<ul>
<li><p> &quot;none&quot;: No special handling of the overlapping time intervals within
person is done.
</p>
</li>
<li><p> &quot;first&quot;: The <code>status</code> mentioned first, that is, has the smallest
<code>in_date</code>, dominates.
</p>
</li>
<li><p> &quot;most_recent&quot; (default): The most recent <code>status</code>, that is, the one with
the largest <code>in_date</code>, dominates. When the most recent <code>status</code> is fully
contained within an older (and different) <code>status</code> then the <code>out_date</code>
associated with the most recent <code>in_date</code> is kept, but the remaining time
from the older <code>status</code> is removed. See examples below.
</p>
</li></ul>

<p>We currently don't have a method that lets the most recent status dominate
and then potentially return to an older longer running status. If this is
needed, please contact ADLS.</p>
</td></tr>
<tr><td><code id="flatten_date_intervals_+3A_lag">lag</code></td>
<td>
<p>A numeric, giving the number of days allowed between time
intervals that should be collapsed into one.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This functions identifies overlapping time intervals within individual and
collapses them into distinct and disjoint intervals. When <code>status</code> is
specified these intervals are both individual and status specific.
</p>
<p>If <code>lag</code> is specified then intervals must be more then <code>lag</code> time units apart
to be considered distinct.
</p>


<h3>Value</h3>

<p>A data frame with the <code>id</code>, <code>status</code> if specified and simplified <code>in_date</code>
and <code>out_date</code>. The returned data is sorted by <code>id</code> and <code>in_date</code>.
</p>


<h3>Author(s)</h3>

<p>ADLS, EMTH &amp; ASO
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
### The flatten function works with both dates and numeric

dat &lt;- data.frame(
   ID    = c(1, 1, 1, 2, 2, 3, 3, 4),
   START = c(1, 2, 5, 3, 6, 2, 3, 6),
   END   = c(3, 3, 7, 4, 9, 3, 5, 8))
dat |&gt; flatten_date_intervals(ID, START, END)

dat &lt;- data.frame(
   ID    = c(1, 1, 1, 2, 2, 3, 3, 4, 4),
   START = as.Date(c("2012-02-15", "2005-12-13", "2006-01-24",
                     "2002-03-14", "1997-02-27",
                     "2008-08-13", "1998-09-23",
                     "2005-01-12", "2007-05-10")),
   END   = as.Date(c("2012-06-03", "2007-02-05", "2006-08-22",
                     "2005-02-26", "1999-04-16",
                     "2008-08-22", "2015-01-29",
                     "2007-05-07", "2008-12-12")))
dat |&gt; flatten_date_intervals(ID, START, END)



###  Allow for a 5 days lag between

dat |&gt; flatten_date_intervals(ID, START, END, lag = 5)



### Adding status information

dat &lt;- data.frame(
   ID     = c(1, 1, 1, 2, 2, 3, 3, 4, 4),
   START  = as.Date(c("2012-02-15", "2005-12-13", "2006-01-24",
                      "2002-03-14", "1997-02-27",
                      "2008-08-13", "1998-09-23",
                      "2005-01-12", "2007-05-10")),
   END    = as.Date(c("2012-06-03", "2007-02-05", "2006-08-22",
                      "2005-02-26", "1999-04-16",
                      "2008-08-22", "2015-01-29",
                     "2007-05-07", "2008-12-12")),
   REGION = c("H", "H", "N", "S", "S", "M", "N", "S", "S"))

# Note the difference between the the different overlap_handling methods
dat |&gt; flatten_date_intervals(ID, START, END, REGION, "none")
dat |&gt; flatten_date_intervals(ID, START, END, REGION, "first")
dat |&gt; flatten_date_intervals(ID, START, END, REGION, "most_recent")

</code></pre>

<hr>
<h2 id='floor_dec'>Round numbers down to a given number of decimal places</h2><span id='topic+floor_dec'></span>

<h3>Description</h3>

<p>Round numbers down to a given number of decimal places
</p>


<h3>Usage</h3>

<pre><code class='language-R'>floor_dec(x, digits = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="floor_dec_+3A_x">x</code></td>
<td>
<p>a numeric vector</p>
</td></tr>
<tr><td><code id="floor_dec_+3A_digits">digits</code></td>
<td>
<p>integer indicating the number of decimal places</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The rounded down numeric vector
</p>

<hr>
<h2 id='freq_function'>Frequency Tables with Percentage and Odds Ratios</h2><span id='topic+freq_function'></span>

<h3>Description</h3>

<p>A method for making 1- and 2-way frequency tables with percentages and odds
ratios.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>freq_function(
  normaldata,
  var1,
  var2 = NULL,
  by_vars = NULL,
  include_NA = FALSE,
  values_to_remove = NULL,
  weightvar = NULL,
  textvar = NULL,
  number_decimals = 2,
  output = c("all", "numeric", "col", "colw", "row", "roww", "total", "totalw"),
  chisquare = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="freq_function_+3A_normaldata">normaldata</code></td>
<td>
<p>A data frame or data frame extension (e.g. a tibble).</p>
</td></tr>
<tr><td><code id="freq_function_+3A_var1">var1</code></td>
<td>
<p>A character string naming the first variable to get frequencies.</p>
</td></tr>
<tr><td><code id="freq_function_+3A_var2">var2</code></td>
<td>
<p>An optional character naming the second variable to get
frequencies. If <code>NULL</code> (standard) a 1-way frequency table of only <code>var1</code> is
created, and if <code>var2</code> is specified a 2-way table is returned.</p>
</td></tr>
<tr><td><code id="freq_function_+3A_by_vars">by_vars</code></td>
<td>
<p>An optional character vector naming variables in <code>normal_data</code>
to stratify the calculations and output by. That is, ALL calculations will
be made within the combinations of variables in the vector, hence it's
possible to get N and % for many groups in one go.</p>
</td></tr>
<tr><td><code id="freq_function_+3A_include_na">include_NA</code></td>
<td>
<p>A logical. If <code>FALSE</code> (standard) missing variables (<code>NA</code>'s)
will be removed from <code>var1</code> and <code>var2</code>. Any missing values in <code>by_vars</code>
will not be removed. If <code>TRUE</code> all missing values will be included in
calculations and the output.</p>
</td></tr>
<tr><td><code id="freq_function_+3A_values_to_remove">values_to_remove</code></td>
<td>
<p>An optional character vector. When specified all
values from <code>var1</code> and <code>var2</code> found in <code>values_to_remove</code> will be removed
from the calculations and output.</p>
</td></tr>
<tr><td><code id="freq_function_+3A_weightvar">weightvar</code></td>
<td>
<p>An optional character naming a column in <code>normaldata</code> with
numeric weights for each observation. If <code>NULL</code> (standard) all observations
have weight 1.</p>
</td></tr>
<tr><td><code id="freq_function_+3A_textvar">textvar</code></td>
<td>
<p>An optional character. When specified <code>textvar</code> is added to
the resulting table as a comment. When <code>NULL</code> (standard) no such text
addition is made.</p>
</td></tr>
<tr><td><code id="freq_function_+3A_number_decimals">number_decimals</code></td>
<td>
<p>A numeric indicating the number of decimals to show on
percentages and weighted frequencies in the combined frequency and percent
variables.</p>
</td></tr>
<tr><td><code id="freq_function_+3A_output">output</code></td>
<td>
<p>A character indicating the output type wanted:
</p>

<ul>
<li> <p><code>"all"</code> - will give ALL output from tables. In many cases unnecessary and
hard to get an overview of. This is set as the standard.
</p>
</li>
<li> <p><code>"numeric"</code> - will give frequencies and percents as numeric variables
only, thus the number_decimals option is not in effect. This option might
be useful when making figures/graphs.
</p>
</li>
<li> <p><code>"col"</code> - will only give unweighted number of observations and weighted
column percent (if weights are used, otherwise unweighted)
</p>
</li>
<li> <p><code>"colw"</code> - will only give weighted number of observations and weighted
column percent (if weights are used, otherwise unweighted)
</p>
</li>
<li> <p><code>"row"</code>- will only give unweighted number of observations and weighted
row percent (if weights are used, otherwise unweighted). Only works in
two-way tables (<code>var2</code> is specified)
</p>
</li>
<li> <p><code>"roww"</code> - will only give weighted number of oberservations and weighted
column percent (if weights are used, otherwise unweighted). Only works in
two-way tables (<code>var2</code> is specified)
</p>
</li>
<li> <p><code>"total"</code> - will only give unweighted number of observations and
weighted percent of the total (if weights are used, otherwise unweighted).
Only works in two-way tables (<code>var2</code> is specified)
</p>
</li>
<li> <p><code>"totalw"</code> - will only give weighted number of observations and
weighted percent of the total (if weights are used, otherwise unweighted).
Only works in two-way tables (<code>var2</code> is specified)
</p>
</li>
<li><p> Any other text will give the default (&quot;all&quot;)
</p>
</li></ul>
</td></tr>
<tr><td><code id="freq_function_+3A_chisquare">chisquare</code></td>
<td>
<p>A logical. <code>FALSE</code> (standard) will not calculate p-value for
the chi-square test for two-way tables (<code>var2</code> is specified). If <code>TRUE</code>,
the table will include the chi-square p-value as well as the chi-square
statistic and the corresponding degrees of freedom. It will be included in
the output whichever output option have been specified. No chi-square test
is performed or included in one-way tables (<code>var2</code> is unspecified)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A frequency table as a data frame object.
</p>


<h3>Author(s)</h3>

<p>ASO
</p>


<h3>See Also</h3>

<p><code><a href="#topic+freq_function_repeated">freq_function_repeated()</a></code> to to get frequencies for multiple
variables in one go.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("starwars", package = "dplyr")

test_table1 &lt;- freq_function(
  starwars,
  var1 = "homeworld"
)

test_table2 &lt;- freq_function(
  starwars,
  var1 = "sex",
  var2 = "eye_color",
  output = "total"
)

test_table3 &lt;- freq_function(
  starwars,
  var1 = "hair_color",
  var2 = "skin_color",
  by_vars = "gender",
  output = "col",
  number_decimals = 5
)

</code></pre>

<hr>
<h2 id='freq_function_repeated'>Wrapper for <code>freq_function()</code> to get frequencies for many variables in one
go.</h2><span id='topic+freq_function_repeated'></span>

<h3>Description</h3>

<p>A method for making multiple 1- and 2-way frequency tables with percentages
and odds ratios.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>freq_function_repeated(
  normaldata,
  var1,
  var2 = NULL,
  by_vars = NULL,
  include_NA = FALSE,
  values_to_remove = NULL,
  weightvar = NULL,
  textvar = NULL,
  number_decimals = 2,
  output = c("all", "numeric", "col", "colw", "row", "roww", "total", "totalw"),
  chisquare = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="freq_function_repeated_+3A_normaldata">normaldata</code></td>
<td>
<p>A data frame or data frame extension (e.g. a tibble).</p>
</td></tr>
<tr><td><code id="freq_function_repeated_+3A_var1">var1</code></td>
<td>
<p>A character vector with the names of the first variable to get
frequencies from for each frequency table.</p>
</td></tr>
<tr><td><code id="freq_function_repeated_+3A_var2">var2</code></td>
<td>
<p>An optional character naming the second variable to get
frequencies. If <code>NULL</code> (standard) 1-way frequency tables of only variables
in <code>var1</code> are created, and if <code>var2</code> is specified 2-way tables are
returned.</p>
</td></tr>
<tr><td><code id="freq_function_repeated_+3A_by_vars">by_vars</code></td>
<td>
<p>An optional character vector naming variables in <code>normal_data</code>
to stratify the calculations and output by. That is, ALL calculations will
be made within the combinations of variables in the vector, hence it's
possible to get N and % for many groups in one go.</p>
</td></tr>
<tr><td><code id="freq_function_repeated_+3A_include_na">include_NA</code></td>
<td>
<p>A logical. If <code>FALSE</code> (standard) missing variables (<code>NA</code>'s)
will be removed from <code>var1</code> and <code>var2</code>. Any missing values in <code>by_vars</code>
will not be removed. If <code>TRUE</code> all missing values will be included in
calculations and the output.</p>
</td></tr>
<tr><td><code id="freq_function_repeated_+3A_values_to_remove">values_to_remove</code></td>
<td>
<p>An optional character vector. When specified all
values from <code>var1</code> and <code>var2</code> found in <code>values_to_remove</code> will be removed
from the calculations and output.</p>
</td></tr>
<tr><td><code id="freq_function_repeated_+3A_weightvar">weightvar</code></td>
<td>
<p>An optional character naming a column in <code>normaldata</code> with
numeric weights for each observation. If <code>NULL</code> (standard) all observations
have weight 1.</p>
</td></tr>
<tr><td><code id="freq_function_repeated_+3A_textvar">textvar</code></td>
<td>
<p>An optional character. When specified <code>textvar</code> is added to
the resulting table as a comment. When <code>NULL</code> (standard) no such text
addition is made.</p>
</td></tr>
<tr><td><code id="freq_function_repeated_+3A_number_decimals">number_decimals</code></td>
<td>
<p>A numeric indicating the number of decimals to show on
percentages and weighted frequencies in the combined frequency and percent
variables.</p>
</td></tr>
<tr><td><code id="freq_function_repeated_+3A_output">output</code></td>
<td>
<p>A character indicating the output type wanted:
</p>

<ul>
<li> <p><code>"all"</code> - will give ALL output from tables. In many cases unnecessary and
hard to get an overview of. This is set as the standard.
</p>
</li>
<li> <p><code>"numeric"</code> - will give frequencies and percents as numeric variables
only, thus the number_decimals option is not in effect. This option might
be useful when making figures/graphs.
</p>
</li>
<li><p> &quot;col&quot; - will only give unweighted number of observations and weighted
column percent (if weights are used, otherwise unweighted)
</p>
</li>
<li> <p><code>"colw"</code> - will only give weighted number of observations and weighted
column percent (if weights are used, otherwise unweighted)
</p>
</li>
<li> <p><code>"row"</code>- will only give unweighted number of observations and weighted
row percent (if weights are used, otherwise unweighted). Only works in
two-way tables (<code>var2</code> is specified)
</p>
</li>
<li> <p><code>"roww"</code> - will only give weighted number of oberservations and weighted
column percent (if weights are used, otherwise unweighted). Only works in
two-way tables (<code>var2</code> is specified)
</p>
</li>
<li> <p><code>"total"</code> - will only give unweighted number of observations and
weighted percent of the total (if weights are used, otherwise unweighted).
Only works in two-way tables (<code>var2</code> is specified)
</p>
</li>
<li> <p><code>"totalw"</code> - will only give weighted number of observations and
weighted percent of the total (if weights are used, otherwise unweighted).
Only works in two-way tables (<code>var2</code> is specified)
</p>
</li>
<li><p> Any other text will give the default (&quot;all&quot;)
</p>
</li></ul>
</td></tr>
<tr><td><code id="freq_function_repeated_+3A_chisquare">chisquare</code></td>
<td>
<p>A logical. <code>FALSE</code> (standard) will not calculate p-value for
the chi-square test for two-way tables (<code>var2</code> is specified). If <code>TRUE</code>,
the table will include the chi-square p-value as well as the chi-square
statistic and the corresponding degrees of freedom. It will be included in
the output whichever output option have been specified. No chi-square test
is performed or included in one-way tables (<code>var2</code> is unspecified)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Multiple frequency tables stored in a data frame object.
</p>


<h3>Author(s)</h3>

<p>ASO
</p>


<h3>See Also</h3>

<p><code><a href="#topic+freq_function">freq_function()</a></code> for the function that creates frequency tables for
single variables.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Examples
data("starwars", package = "dplyr")
test_table1 &lt;- freq_function_repeated(
  starwars,
  var1 = c("sex","homeworld","eye_color"),
  include_NA = TRUE
)
test_table2 &lt;- freq_function_repeated(
  starwars,
  var1 = c("homeworld","eye_color","skin_color"),
  var2 = "sex",
  output = "col",
  number_decimals = 3
)
test_table3 &lt;- freq_function_repeated(
  starwars,
  var1 = c("homeworld","eye_color","skin_color"),
  var2 = "sex",
  by_vars = c("gender"),
  output = "row"
)

</code></pre>

<hr>
<h2 id='lms'>Wrapper around lm for sibling design</h2><span id='topic+lms'></span><span id='topic+print.lms'></span>

<h3>Description</h3>

<p>Fits a linear model using demeaned data. Useful for sibling design.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lms(formula, data, grp_id, obs_id = NULL, ...)

## S3 method for class 'lms'
print(x, digits = max(3L, getOption("digits") - 3L), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lms_+3A_formula">formula</code></td>
<td>
<p>A formula, used to create a model matrix with demeaned columns.</p>
</td></tr>
<tr><td><code id="lms_+3A_data">data</code></td>
<td>
<p>A data frame, data frame extension (e.g. a tibble), or a lazy
data frame (e.g. from dbplyr or dtplyr).</p>
</td></tr>
<tr><td><code id="lms_+3A_grp_id">grp_id</code></td>
<td>
<p>&lt;<code><a href="dplyr.html#topic+dplyr_data_masking">data-masking</a></code>&gt; One unquoted expression
naming the id variable in data defining the groups to demean,
e.g. sibling groups.</p>
</td></tr>
<tr><td><code id="lms_+3A_obs_id">obs_id</code></td>
<td>
<p>&lt;<code><a href="dplyr.html#topic+dplyr_data_masking">data-masking</a></code>&gt; Optional, One unquoted
expression naming an id variable to keep track of the input data order.</p>
</td></tr>
<tr><td><code id="lms_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to <a href="stats.html#topic+lm">lm</a>(). In print,
additional arguments are ignored without warning.</p>
</td></tr>
<tr><td><code id="lms_+3A_x">x</code></td>
<td>
<p>An S3 object with class lms.</p>
</td></tr>
<tr><td><code id="lms_+3A_digits">digits</code></td>
<td>
<p>The number of significant digits to be passed to
<a href="base.html#topic+format">format</a>(<a href="stats.html#topic+coef">coef</a>(x), .) when <a href="base.html#topic+print">print</a>()ing.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>lms</code> estimates parameters in the linear model
</p>
<p style="text-align: center;"><code class="reqn">y_{ij_i}=\alpha_i+x_{ij_i}^T\beta + \varepsilon_{ij_i}</code>
</p>

<p>where <code class="reqn">\alpha_i</code> is a group (e.g. sibling group)
specific intercept and <code class="reqn">x_{ij_i}</code> are covariate values for
observation <code class="reqn">j_i</code> in group i.
<code class="reqn">\varepsilon_{ij_i}\sim N(0, \sigma^2)</code>
is a normally distributed error term. It is assumed that interest is in
estimating the vector <code class="reqn">\beta</code> while <code class="reqn">\alpha_{i}</code>
are nuisance parameters. Estimation of <code class="reqn">\beta</code> uses the mean deviation
method, where
</p>
<p style="text-align: center;"><code class="reqn">y_{ij_i}^{'}=y_{ij_i}-y_i</code>
</p>

<p>is regressed on
</p>
<p style="text-align: center;"><code class="reqn">x_{ij_i}^{'}=x_{ij_i}-x_i.</code>
</p>

<p>Here <code class="reqn">y_i</code> and <code class="reqn">x_i</code> refers to the mean of y and x in group i.
<br /> <code>lms</code> can keep track of observations by providing a unique identifier
column to <code>obs_id</code>. <code>lms</code> will return <code>obs_id</code> so it matches the order of
observations in model.<br />
<code>lms</code> only supports syntactic covariate names. Using a non-syntactic name
risks returning an error, e.g if names end in + or -.
</p>


<h3>Value</h3>

<p>A list with class <code>c("lms", "lm")</code>. Contains the output from <code>lm</code> applied
to demeaned data according to <code>formula</code>, as well as the original data and the
provided formula.
</p>


<h3>Author(s)</h3>

<p>KIJA
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
sib_id &lt;- sample(200, 1000, replace = TRUE)
sib_out &lt;- rnorm(200)
x1 &lt;- rnorm(1000)
x2 &lt;- rnorm(1000) + sib_out[sib_id] + x1
y &lt;- rnorm(1000, 1, 0.5) + 2 * sib_out[sib_id] - x1 + 2 * x2
data &lt;- data.frame(
  x1 = x1,
  x2 = x2,
  y = y,
  sib_id = sib_id,
  obs_id = 1:1000
)
mod_lm &lt;- lm(y ~ x1 + x2, data) # OLS model
mod_lm_grp &lt;- lm(y ~ x1 + x2 + factor(sib_id), data) # OLS with grp
mod_lms &lt;- lms(y ~ x1 + x2, data, sib_id, obs_id) # conditional model
summary(mod_lm)
coef(mod_lm_grp)[1:3]
summary(mod_lms)
print(mod_lms)

</code></pre>

<hr>
<h2 id='logasympBF'>Asymptotic Bayes factors</h2><span id='topic+logasympBF'></span><span id='topic+asympBF'></span><span id='topic+invlogasympBF'></span><span id='topic+invasympBF'></span><span id='topic+watershed'></span><span id='topic+invwatershed'></span>

<h3>Description</h3>

<p>The Bayesian equivalent of a significance test for H1: an
unrestricted parameter value versus H0: of a specific parameter value based
only on data D can be obtained from Bayes factor (BF). Then <code>BF = Probability(H1|D) / Probability(H0|D)</code> and is a Bayesian equivalent of a
likelihood ratio. It is based on the same asymptotics as the ubiqutous
chi-square tests. This BF only depends on the difference in deviance
between the models corresponding to H0 and H1 (chisquare) and the dimension
d of H1. This BF is monotone in chisquare (and hence the p-value p) for
fixed d. It is thus a tool to turn p-values into evidence, also
retrospectively. The expression for BF depends on a parameter lambda which
expresses the ratio between the information in the prior and the data
(likelihood). By default <code>lambda = min(d / chisquare, lambdamax = 0.255)</code>.
Thus, as chisquare goes to infinity we effectively maximize BF and hence
the evidence favoring H1, and opposite for small chisquare has a
well-defined watershed where we have equal preferences for H1 and H0. The
value 0.255 corresponds to a watershed of 2, that is we prefer H1 when
<code>chisquare &gt; d * 2</code> and prefer H0 when <code>chisquare &lt; d * 2</code>, similar to
having a BF that is a continuous version of the Akaike Information
Criterion for model selection. For derivations and details, see Rostgaard
(2023).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logasympBF(chisq = NA, p = NA, d = 1, lambda = NA, lambdamax = 0.255)

asympBF(chisq = NA, p = NA, d = 1, lambda = NA, lambdamax = 0.255)

invlogasympBF(logasympBF = NA, d = 1, lambda = NA, lambdamax = 0.255)

invasympBF(bf, d = 1, lambda = NA, lambdamax = 0.255)

watershed(chisq)

invwatershed(lambda)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logasympBF_+3A_chisq">chisq</code></td>
<td>
<p>a non-negative number denoting the difference in deviance
between the statistical models corresponding to H0 and H1</p>
</td></tr>
<tr><td><code id="logasympBF_+3A_p">p</code></td>
<td>
<p>the p value corresponding to the test statistic chisq on d degrees
of freedom</p>
</td></tr>
<tr><td><code id="logasympBF_+3A_d">d</code></td>
<td>
<p>the dimension of H1, <code style="white-space: pre;">&#8288;d =&gt; 1&#8288;</code></p>
</td></tr>
<tr><td><code id="logasympBF_+3A_lambda">lambda</code></td>
<td>
<p>a strictly positive number corresponding to the ratio between
the information in the prior and the data</p>
</td></tr>
<tr><td><code id="logasympBF_+3A_lambdamax">lambdamax</code></td>
<td>
<p>an upper limit on lambda</p>
</td></tr>
<tr><td><code id="logasympBF_+3A_logasympbf">logasympBF</code></td>
<td>
<p>log(bf)</p>
</td></tr>
<tr><td><code id="logasympBF_+3A_bf">bf</code></td>
<td>
<p>Bayes factor, a strictly positive number</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For fixed dimension d of the alternative hypothesis H1 <code>asympBF(.) = exp(logasympBF(.))</code> maps a test statistic (chisquare) or a p-value p into a
Bayes factor (the ratio between the probabilities of the models
corresponding to each hypothesis). <code>asympBF(.)</code> is monotonely increasing in
chisquare, attaining the value 1 when <code>chisquare = d * watershed</code>. The
watershed is thus a device to specify what the user considers a practical
null result by choosing <code>lambdamax = watershed(watershed)</code>.
</p>
<p>For sufficiently large values of chisquare, lambda is estimated as
d/chisquare. This behavior can be overruled by specifying lambda. Using
<code>invasympBF(.) = exp(invlogasympBF(.))</code> we can map a Bayes factor, bf to a
value of chisquare.
</p>
<p>Likewise, we can obtain the watershed corresponding to a given lambdamax as
<code>invwatershed(lambdamax)</code>.
</p>
<p>Generally in these functions we recode or ignore illegal values of
parameters, rather than returning an error code. <code>chisquare</code> is always
recoded as <code>abs(chisquare)</code>. <code>d</code> is set to <code>1</code> as default, and missing or
entered values of <code>d &lt; 1</code> are recoded as <code>d = 1</code>. Entered values of
<code>lambdamax &lt;= 0</code> or missing are ignored. Entered values of <code>lambda &lt;= 0</code> or
missing are ignored in <code>invwatershed(.)</code>. we use <code>abs(lambda)</code> as argument,
<code>lambda = 0</code> results in an error.
</p>


<h3>Author(s)</h3>

<p>KLP &amp; KIJA
</p>


<h3>References</h3>

<p>Klaus Rostgaard (2023): Simple nested Bayesian hypothesis testing
for meta-analysis, Cox, Poisson and logistic regression models. Scientific
Reports. https://doi.org/10.1038/s41598-023-31838-8
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example code

# 1. the example(s) from Rostgaard (2023)
asympBF(p = 0.19, d = 8) # 0.148411
asympBF(p = 0.19, d = 8, lambdamax = 100) # 0.7922743
asympBF(p = 0.19, d = 8, lambda = 100 / 4442) # 5.648856e-05
# a maximal value of a parameter considered practically null
deltalogHR &lt;- -0.2 * log(0.80)
sigma &lt;- (log(1.19) - log(0.89)) / 3.92
chisq &lt;- (deltalogHR / sigma) ** 2
chisq # 0.3626996
watershed(chisq)
# leads nowhere useful chisq=0.36&lt;2

# 2. tests for interaction/heterogeneity - real world examples
asympBF(p = 0.26, d = 24) # 0.00034645
asympBF(p = 0.06, d = 11) # 0.3101306
asympBF(p = 0.59, d = 7) # 0.034872

# 3. other examples
asympBF(p = 0.05) # 2.082664
asympBF(p = 0.05, d = 8) # 0.8217683
chisq &lt;- invasympBF(19)
chisq # 9.102697
pchisq(chisq, df = 1, lower.tail = FALSE) # 0.002552328
chisq &lt;- invasympBF(19, d = 8)
chisq # 23.39056
pchisq(chisq, df = 8, lower.tail = FALSE) # 0.002897385
</code></pre>

<hr>
<h2 id='many_merge'>Merging Many Data Frames with Name Handling</h2><span id='topic+many_merge'></span>

<h3>Description</h3>

<p>Function to join/merge multiple data.frames with one or more common variable
names.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>many_merge(by, first_data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="many_merge_+3A_by">by</code></td>
<td>
<p>A join specification created with
<code><a href="dplyr.html#topic+join_by">join_by()</a></code>, or a character vector of variables
to join by. The <code>by</code> must be present in all data frames <code>first_data</code> and
<code>...</code>.</p>
</td></tr>
<tr><td><code id="many_merge_+3A_first_data">first_data</code></td>
<td>
<p>A data frame (presented on the left in the final table).</p>
</td></tr>
<tr><td><code id="many_merge_+3A_...">...</code></td>
<td>
<p>Data frames to merge onto <code>first_data</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>many_merge()</code> function returns a data frame.
</p>


<h3>Author(s)</h3>

<p>ASO
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Create some dummy data
testdata_id &lt;- c(1:10)
var1 &lt;- rep(letters[1:5], times = 2)
var2 &lt;- letters[1:10]
var3 &lt;- rep(letters[11:12], times = 5)
var4 &lt;- letters[13:22]
var5 &lt;- letters[11:20]

# Rename alle the variables to "var"
data1 &lt;- data.frame(testdata_id, var = var1)
data2 &lt;- data.frame(testdata_id, var = var2)
data3 &lt;- data.frame(testdata_id, var = var3)
data4 &lt;- data.frame(testdata_id, var = var4)
data5 &lt;- data.frame(testdata_id, var = var5)

# Many merge
final_data &lt;- many_merge(
  by = c("testdata_id"),
  data1,
  data2,
  data3,
  data4,
  data5
)

</code></pre>

<hr>
<h2 id='multi_join'>Join many data frames with name handling</h2><span id='topic+multi_join'></span>

<h3>Description</h3>

<p>Function to join multiple data.frames with one or more common
variable names.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multi_join(..., .by)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multi_join_+3A_...">...</code></td>
<td>
<p>Data frames to join. Each argument in <code>...</code> must either be
a data.frame or a list of data.frames.</p>
</td></tr>
<tr><td><code id="multi_join_+3A_.by">.by</code></td>
<td>
<p>A character vector of variables to join by. The <code>.by</code> must be
present in all data frames in <code>...</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>multi_join()</code> function returns a data frame.
</p>


<h3>Author(s)</h3>

<p>KIJA
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create some dummy data
testdata_id &lt;- c(1:10)
a1 &lt;- 1:10; b1 &lt;- rep(letters[1:5], times = 2); c1 &lt;- runif(10)
a2 &lt;- 11:20; b2 &lt;- letters[1:10]
a3 &lt;- 21:30; b3 &lt;- rep(letters[11:12], times = 5)
a4 &lt;- 31:40; b4 &lt;- letters[13:22]
a5 &lt;- 41:50; b5 &lt;- letters[11:20]

# Define data.frames with common key and shared column names
data1 &lt;- data.frame(testdata_id, a = a1, b = b1, c = c1)
data2 &lt;- data.frame(testdata_id, b = b2, a = a2)
data3 &lt;- data.frame(testdata_id, a = a3, b = b3)
data4 &lt;- data.frame(testdata_id, a = a4, b = b4)
data5 &lt;- data.frame(testdata_id, a = a5, b = b5)

# multi join
final_data &lt;- multi_join(
  data1,
  data2,
  data3,
  data4,
  data5,
  .by = "testdata_id"
)

</code></pre>

<hr>
<h2 id='odds_ratio_function'>Easier to perform logistic and log-linear regressions giving a standardized
output table</h2><span id='topic+odds_ratio_function'></span>

<h3>Description</h3>

<p>odds_ratio_function analyses specified data given user specifications,
including outcome, exposures and possible weights. It can handle survey-data,
but not complex sampling schemes (if specified as survey-data, the model will
create a simple survey-object from the data, using weights as specified - if
not specified, the weights are 1 for each observation) The standard
regression is logistic regression (yielding Odds Ratios=OR) but it is
possible to perform a log-linear regression (yielding Risk Ratios=RR)
instead, if specified and requirements are met.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>odds_ratio_function(
  normaldata,
  outcomevar,
  expvars,
  number_decimals = 2,
  alpha = 0.05,
  regtype = c("logistic", "log-linear"),
  matchgroup = NULL,
  matchtiemethod = c("exact", "approximate", "efron", "breslow"),
  values_to_remove = NULL,
  weightvar = NULL,
  surveydata = FALSE,
  textvar = NULL,
  model_object = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="odds_ratio_function_+3A_normaldata">normaldata</code></td>
<td>
<p>A data frame or data frame extension (e.g. a tibble).</p>
</td></tr>
<tr><td><code id="odds_ratio_function_+3A_outcomevar">outcomevar</code></td>
<td>
<p>A character string naming of factor variable in normaldata
to use as the outcome.</p>
</td></tr>
<tr><td><code id="odds_ratio_function_+3A_expvars">expvars</code></td>
<td>
<p>A character vector with the names of the exposure variables
(either numeric or factors). Any transformations or interactions to be
included must also be specified, e.g.
<code>c("Var1", "I(Var1^2)", "Var2", "Var3*Var4")</code>.</p>
</td></tr>
<tr><td><code id="odds_ratio_function_+3A_number_decimals">number_decimals</code></td>
<td>
<p>An integer giving the number of decimals to show in
the standardized output (default is two decimals).</p>
</td></tr>
<tr><td><code id="odds_ratio_function_+3A_alpha">alpha</code></td>
<td>
<p>A scalar, between 0 and 1 specifying the desired significance
level of the confidence intervals (default is 0.05 which will yield the
usual 95% confidence interval).</p>
</td></tr>
<tr><td><code id="odds_ratio_function_+3A_regtype">regtype</code></td>
<td>
<p>A character string specifying the analysis method. Can either
be &quot;logistic&quot; for logistic regression (the default) or &quot;log-linear&quot; for
log-linear regression. Log-linear regression can only be used with
binomial, unconditional analysis.</p>
</td></tr>
<tr><td><code id="odds_ratio_function_+3A_matchgroup">matchgroup</code></td>
<td>
<p>Character string specifying a variable in normaldata to
condition the analysis on. Can only be used in binomial logistic regression
models (default is NULL).</p>
</td></tr>
<tr><td><code id="odds_ratio_function_+3A_matchtiemethod">matchtiemethod</code></td>
<td>
<p>Character string specifying the method for ties when
using a matched/conditional analysis. The default options is &quot;exact&quot;,
however this option does not take weights into account for the analysis, so
if weights (other than 1) are used, another option should be selected.
Other options are &quot;approximate&quot;, &quot;efron&quot;, and &quot;breslow&quot; - for further
explanations, see documentation for <a href="survival.html#topic+clogit">clogit</a>.</p>
</td></tr>
<tr><td><code id="odds_ratio_function_+3A_values_to_remove">values_to_remove</code></td>
<td>
<p>A Character vector specifying values to remove from
ALL variables used in the regression before the analysis (default is NULL).
This is useful if some value(s) are used consistently to encode
missing/irrelevant in the data (e.g. c(&quot;888&quot;, &quot;987&quot;) - normal missing (NA)
don't need to be specified as it will be removed automatically. Do NOT
remove the reference values as this will lead to unexpected results!</p>
</td></tr>
<tr><td><code id="odds_ratio_function_+3A_weightvar">weightvar</code></td>
<td>
<p>A character string specifying a numeric variable in
normaldata with pre-calculated weights for observations in the analysis.
The default value NULL corresponds to weight 1 for all observations.</p>
</td></tr>
<tr><td><code id="odds_ratio_function_+3A_surveydata">surveydata</code></td>
<td>
<p>A Boolean specifying whether the data comes from a survey
(default is FALSE).</p>
</td></tr>
<tr><td><code id="odds_ratio_function_+3A_textvar">textvar</code></td>
<td>
<p>A character string with text (like a note) to be added to the
output. The default value NULL corresponds to no added note.</p>
</td></tr>
<tr><td><code id="odds_ratio_function_+3A_model_object">model_object</code></td>
<td>
<p>A Boolean. If TRUE, returns the raw output object from
the analysis instead of the standard output. This might be useful to see
information not included in the standardized output (default is FALSE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A standardized analysis object with results from a model.
</p>


<h3>Author(s)</h3>

<p>ASO
</p>


<h3>See Also</h3>

<p><code><a href="#topic+odds_ratio_function_repeated">odds_ratio_function_repeated()</a></code> to perform several analysis in one
go.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Binomial outcome
data(logan, package = "survival")

resp &lt;- levels(logan$occupation)
n &lt;- nrow(logan)
indx &lt;- rep(1:n, length(resp))
logan2 &lt;- data.frame(
  logan[indx,],
  id = indx,
  tocc = factor(rep(resp, each=n))
)
logan2$case &lt;- (logan2$occupation == logan2$tocc)
logan2$case &lt;- as.factor(logan2$case)
logan2$case &lt;- relevel(logan2$case, ref = "FALSE")

# Standard binomial logistic regression but using interaction for exposures:
func_est1 &lt;- odds_ratio_function(
  logan2,
  outcomevar = "case",
  expvars = c("tocc", "education", "tocc:education")
)


# Conditional binomial logistic regression with some extra text added:
func_est2 &lt;- odds_ratio_function(
  logan2,
  outcomevar = "case",
  expvars = c("tocc", "tocc:education"),
  matchgroup = "id",
  textvar = "Testing function"
)


# Standard binomial logistic regression as survey data with no prepared
# weights:
func_est3 &lt;- odds_ratio_function(
  logan2,
  outcomevar = "case",
  expvars = c("tocc", "education"),
  surveydata = TRUE
)

# Example changing significance level and the number of decimals in fixed
# output and adding some text:
func_est4 &lt;- odds_ratio_function(
  logan2,
  outcomevar = "case",
  expvars = c("tocc", "education"),
  number_decimals = 5,
  alpha = 0.01,
  textvar = "Testing function"
)

# Getting raw output from the regression function:
func_est5 &lt;- odds_ratio_function(
  logan2,
  outcomevar = "case",
  expvars = c("tocc", "education"),
  model_object = TRUE
)

### Polytomous/multinomial outcome
data(api, package = "survey")

# As normal data, but using weights:
func_est6 &lt;- odds_ratio_function(
  apiclus2,
  outcomevar = "stype",
  expvars = c("ell", "meals", "mobility", "sch.wide"),
  weightvar = "pw"
)

# As survey data with weights:
func_est7 &lt;- odds_ratio_function(
  apiclus2,
  outcomevar = "stype",
  expvars = c("ell", "meals", "mobility"),
  weightvar = "pw", surveydata = TRUE
)

# Binomial logistic regression with same data (by removing all observations
# with a specific value of outcome):
func_est8 &lt;- odds_ratio_function(
  apiclus2,
  outcomevar = "stype",
  expvars = c("ell", "meals", "mobility"),
  weightvar = "pw",
  values_to_remove = c("E")
)

</code></pre>

<hr>
<h2 id='odds_ratio_function_repeated'>Wrapper for the <code>odds_ratio_function()</code>to perform several similar analyses
in one go</h2><span id='topic+odds_ratio_function_repeated'></span>

<h3>Description</h3>

<p>The function is intended to make it easy to get OR's for several similar
models in one go, where either the same analysis is performed except for one
variable or the same analysis is performed but by each variable (each level
of the variable is analysed separately).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>odds_ratio_function_repeated(
  normaldata,
  outcomevar,
  expvars,
  adjustment_fixed = NULL,
  by_var = NULL,
  number_decimals = 2,
  alpha = 0.05,
  regtype = c("logistic", "log-linear"),
  matchgroup = NULL,
  matchtiemethod = c("exact", "approximate", "efron", "breslow"),
  values_to_remove = NULL,
  weightvar = NULL,
  surveydata = FALSE,
  textvar = NULL,
  model_object = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="odds_ratio_function_repeated_+3A_normaldata">normaldata</code></td>
<td>
<p>A data frame or data frame extension (e.g. a tibble).</p>
</td></tr>
<tr><td><code id="odds_ratio_function_repeated_+3A_outcomevar">outcomevar</code></td>
<td>
<p>A character vector naming factor variables in normaldata
to use as outcomes in separate models.</p>
</td></tr>
<tr><td><code id="odds_ratio_function_repeated_+3A_expvars">expvars</code></td>
<td>
<p>A character vector naming exposure variables (either numeric
or factors) to use in separate models.</p>
</td></tr>
<tr><td><code id="odds_ratio_function_repeated_+3A_adjustment_fixed">adjustment_fixed</code></td>
<td>
<p>A character vector naming adjustment variables to
include in all models. NULL is the default resulting in no fixed
adjustment.</p>
</td></tr>
<tr><td><code id="odds_ratio_function_repeated_+3A_by_var">by_var</code></td>
<td>
<p>A character vector specifying a factor on which to run the
analyses completely separate for all levels. It only works with one
variable (default is NULL). NOTE: NA and &quot;&quot; levels will not be used but all
other levels will have separate models.</p>
</td></tr>
<tr><td><code id="odds_ratio_function_repeated_+3A_number_decimals">number_decimals</code></td>
<td>
<p>An integer giving the number of decimals to show in
the standardized output (default is two decimals).</p>
</td></tr>
<tr><td><code id="odds_ratio_function_repeated_+3A_alpha">alpha</code></td>
<td>
<p>A scalar, between 0 and 1 specifying the desired significance
level of the confidence intervals (default is 0.05 which will yield the
usual 95% confidence interval).</p>
</td></tr>
<tr><td><code id="odds_ratio_function_repeated_+3A_regtype">regtype</code></td>
<td>
<p>A character string specifying the analysis method. Can either
be &quot;logistic&quot; for logistic regression (the default) or &quot;log-linear&quot; for
log-linear regression. Log-linear regression can only be used with
binomial, unconditional analysis.</p>
</td></tr>
<tr><td><code id="odds_ratio_function_repeated_+3A_matchgroup">matchgroup</code></td>
<td>
<p>Character string specifying a variable in normaldata to
condition the analysis on. Can only be used in binomial logistic regression
models (default is NULL).</p>
</td></tr>
<tr><td><code id="odds_ratio_function_repeated_+3A_matchtiemethod">matchtiemethod</code></td>
<td>
<p>Character string specifying the method for ties when
using a matched/conditional analysis. The default options is &quot;exact&quot;,
however this option does not take weights into account for the analysis, so
if weights (other than 1) are used, another option should be selected.
Other options are &quot;approximate&quot;, &quot;efron&quot;, and &quot;breslow&quot; - for further
explanations, see documentation for <a href="survival.html#topic+clogit">clogit</a>.</p>
</td></tr>
<tr><td><code id="odds_ratio_function_repeated_+3A_values_to_remove">values_to_remove</code></td>
<td>
<p>Character vector specifying values to remove from
ALL variables used in the regression before the analysis (default is NULL).
This is useful if some value(s) are used consistently to encode
missing/irrelevant in the data (e.g. c(&quot;888&quot;, &quot;987&quot;) - normal missing (NA)
don't need to be specified as it will be removed automatically. Do NOT
remove the reference values as this will lead to unexpected results!</p>
</td></tr>
<tr><td><code id="odds_ratio_function_repeated_+3A_weightvar">weightvar</code></td>
<td>
<p>A character string specifying a numeric variable in
normaldata with pre-calculated weights for observations in the analysis.
The default value NULL corresponds to weight 1 for all observations.</p>
</td></tr>
<tr><td><code id="odds_ratio_function_repeated_+3A_surveydata">surveydata</code></td>
<td>
<p>A Boolean specifying whether the data comes from a survey
(default is FALSE).</p>
</td></tr>
<tr><td><code id="odds_ratio_function_repeated_+3A_textvar">textvar</code></td>
<td>
<p>A character string with text (like a note) to be added to the
output. The default value NULL corresponds to no added note.</p>
</td></tr>
<tr><td><code id="odds_ratio_function_repeated_+3A_model_object">model_object</code></td>
<td>
<p>A Boolean. If TRUE, returns the raw output object from
the analysis instead of the standard output. This might be useful to see
information not included in the standardized output (default is FALSE).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It's possible to have same variable in <code>expvars</code> and
<code>adjustment_fixed</code>.
</p>
<p>When a model results in an error, the function will not stop - it continues
with other models until done BUT in the output the error text can be seen.
</p>


<h3>Value</h3>

<p>A standardized analysis object with results from multiple models.
</p>


<h3>Author(s)</h3>

<p>ASO
</p>


<h3>See Also</h3>

<p><code><a href="#topic+odds_ratio_function">odds_ratio_function()</a></code> to perform a single logistic or log-linear
regression giving a standardized output table.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Data to use
data("infert", package = "datasets")
infert2 &lt;- infert |&gt;
  dplyr::mutate(
    Age_grp = relevel(as.factor(dplyr::case_when(
      age &lt; 25 ~ "&lt;25",
      25 &lt;= age &amp; age &lt; 35 ~ "25-&lt;35",
      age &gt;= 35 ~ "35+"
    )), ref="25-&lt;35"),
    Parity_grp = relevel(as.factor(dplyr::case_when(
      parity == 1 ~ "1",
      parity &gt;= 2 &amp; parity &lt;= 3 ~ "2-3",
      parity &gt; 3 ~ "4+"
    )), ref="2-3"),
    induced = relevel(as.factor(induced), ref="0"),
    case = relevel(as.factor(case), ref="0"),
    spontaneous = relevel(as.factor(spontaneous), ref="0")
  )

# Two outcomes (Parity_grp, case) with their own set of models, three
# variables included in separate models (spontaneous,induced and education)
# and one variable that is included in all models (Age_grp)
test &lt;- odds_ratio_function_repeated(
  normaldata = infert2,
  outcomevar = c("Parity_grp","case"),
  expvars = c("spontaneous","induced","education"),
  adjustment_fixed = c("Age_grp")
)

# One outcome (case), two variables included in separate models
# (spontaneous and induced), one variable included in all models (Age_grp)
# and all analyses made for each level of another variable (Parity_grp)
test2 &lt;- odds_ratio_function_repeated(
  normaldata = infert2,
  outcomevar = c("case"),
  expvars = c("spontaneous","induced"),
  adjustment_fixed = c("Age_grp"),
  by_var = "Parity_grp"
)

</code></pre>

<hr>
<h2 id='RATEOmnibusTest'>RATE based omnibus test of heterogeneity</h2><span id='topic+RATEOmnibusTest'></span>

<h3>Description</h3>

<p>Provides the P-value for a formal test of heterogeneity based on the RATE
statistic by Yadlowsky et al.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RATEOmnibusTest(
  forest,
  level = 0.95,
  target = c("AUTOC", "QINI"),
  q = seq(0.1, 1, 0.1),
  R = 500,
  num.threads = 1,
  seed = NULL,
  honesty = TRUE,
  stabilize.splits = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RATEOmnibusTest_+3A_forest">forest</code></td>
<td>
<p>An object of class <code>causal_forest</code>, as returned by
<a href="grf.html#topic+causal_forest">causal_forest</a>(), with binary treatment.</p>
</td></tr>
<tr><td><code id="RATEOmnibusTest_+3A_level">level</code></td>
<td>
<p>numeric, level of RATE confidence interval.</p>
</td></tr>
<tr><td><code id="RATEOmnibusTest_+3A_target">target</code></td>
<td>
<p>character, see <a href="grf.html#topic+rank_average_treatment_effect">rank_average_treatment_effect</a>.</p>
</td></tr>
<tr><td><code id="RATEOmnibusTest_+3A_q">q</code></td>
<td>
<p>numeric, see <a href="grf.html#topic+rank_average_treatment_effect">rank_average_treatment_effect</a>.</p>
</td></tr>
<tr><td><code id="RATEOmnibusTest_+3A_r">R</code></td>
<td>
<p>integer, see <a href="grf.html#topic+rank_average_treatment_effect">rank_average_treatment_effect</a></p>
</td></tr>
<tr><td><code id="RATEOmnibusTest_+3A_num.threads">num.threads</code></td>
<td>
<p>passed to <a href="grf.html#topic+causal_forest">causal_forest</a>. Number of threads used in
training. Default value is 1.</p>
</td></tr>
<tr><td><code id="RATEOmnibusTest_+3A_seed">seed</code></td>
<td>
<p>numeric, either length 1, in which case the same seed is used for
both new forests, or length 2, to train each forest with a different seed.
Default is <code>NULL</code>, in which case two seeds are randomly sampled.</p>
</td></tr>
<tr><td><code id="RATEOmnibusTest_+3A_honesty">honesty</code></td>
<td>
<p>Boolean, <code>TRUE</code> if forest was trained using honesty. Otherwise <code>FALSE</code>.
Argument controls if honesty is used to train the new forests on the random
half-samples, so misspecification will lead to invalid results. Default is
<code>TRUE</code>, the default in <a href="grf.html#topic+causal_forest">causal_forest</a>.</p>
</td></tr>
<tr><td><code id="RATEOmnibusTest_+3A_stabilize.splits">stabilize.splits</code></td>
<td>
<p>Boolean, <code>TRUE</code> if forest was trained taking treatment into
account when determining the imbalance of a split. Otherwise <code>FALSE</code>.
Argument controls if treatment is taken into account when determining the
imbalance of a split during training of the new forests on the random
half-samples, so misspecification will lead to invalid results. Default is
<code>TRUE</code>, the default in <a href="grf.html#topic+causal_forest">causal_forest</a>.</p>
</td></tr>
<tr><td><code id="RATEOmnibusTest_+3A_...">...</code></td>
<td>
<p>additional arguments for <a href="grf.html#topic+causal_forest">causal_forest</a>. By default, the
arguments used by forest will be used to train new forests on the random
half-samples. Arguments provided through <code>...</code> will override these. Note that
sample.weights and clusters are passed to both <a href="grf.html#topic+causal_forest">causal_forest</a> and
<a href="grf.html#topic+rank_average_treatment_effect.fit">rank_average_treatment_effect.fit</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>RATE evaluates the ability of a provided prioritization rule to
prioritize treatment to subjects with a large benefit. In order to test for
heterogeneity, we want estimated CATE's to define the prioritization rule.
However, to obtain valid inference the prioritization scores must be
constructed independently of the evaluating forest training data. To
accomplice this, we split the data and train separate forests on each part.
Then we estimate double robust scores on the observations used to train
each forest, and obtain prioritization scores by predicting CATE's with
each forest on the samples not used for training.
</p>


<h3>Value</h3>

<p>A list of class <code>rank_average_treatment_effect</code> with elements
</p>

<ul>
<li><p> estimate: the RATE estimate.
</p>
</li>
<li><p> std.err: bootstrapped standard error of RATE.
</p>
</li>
<li><p> target: the type of estimate.
</p>
</li>
<li><p> TOC: a data.frame with the Targeting Operator Characteristic curve
estimated on grid q, along with bootstrapped SEs.
</p>
</li>
<li><p> confint: a data.frame with the lower and upper bounds of the RATE
confidence interval.
</p>
</li>
<li><p> pval: the p-value for the test that RATE is non-positive.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>KIJA
</p>


<h3>References</h3>

<p>Yadlowsky S, Fleming S, Shah N, Brunskill E, Wager S. Evaluating
Treatment Prioritization Rules via Rank-Weighted Average Treatment Effects.
2021. http://arxiv.org/abs/2111.07966.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 800
p &lt;- 3
X &lt;- matrix(rnorm(n * p), n, p)
W &lt;- rbinom(n, 1, 0.5)
event_prob &lt;- 1 / (1 + exp(2 * (pmax(2 * X[, 1], 0) * W - X[, 2])))
Y &lt;- rbinom(n, 1, event_prob)
clusters &lt;- sample(1:4, n, replace = TRUE)
cf &lt;- grf::causal_forest(X, Y, W, clusters = clusters)
rate &lt;- RATEOmnibusTest(cf, target = "QINI")
rate


</code></pre>

<hr>
<h2 id='RATETest'>wrapper for <a href="grf.html#topic+rank_average_treatment_effect">rank_average_treatment_effect</a></h2><span id='topic+RATETest'></span>

<h3>Description</h3>

<p>Provides confidence interval and p-value together with the standard output
from <a href="grf.html#topic+rank_average_treatment_effect">rank_average_treatment_effect</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RATETest(
  forest,
  priorities,
  level = 0.95,
  cov_type = c("continuous", "discrete"),
  target = "AUTOC",
  q = seq(0.1, 1, by = 0.1),
  R = 500,
  subset = NULL,
  debiasing.weights = NULL,
  compliance.score = NULL,
  num.trees.for.weights = 500
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RATETest_+3A_forest">forest</code></td>
<td>
<p>An object of class <code>causal_forest</code>, as returned by
<a href="grf.html#topic+causal_forest">causal_forest</a>().</p>
</td></tr>
<tr><td><code id="RATETest_+3A_priorities">priorities</code></td>
<td>
<p>character, name of covariate to test for heterogeneity.</p>
</td></tr>
<tr><td><code id="RATETest_+3A_level">level</code></td>
<td>
<p>numeric, level of RATE confidence interval.</p>
</td></tr>
<tr><td><code id="RATETest_+3A_cov_type">cov_type</code></td>
<td>
<p>character, either &quot;continuous&quot; or &quot;discrete&quot;. If &quot;discrete&quot;,
and q is not manually set, TOC will be evaluated at the quantiles
corresponding to transitions from one level to the next.</p>
</td></tr>
<tr><td><code id="RATETest_+3A_target">target</code></td>
<td>
<p>character, see <a href="grf.html#topic+rank_average_treatment_effect">rank_average_treatment_effect</a>.</p>
</td></tr>
<tr><td><code id="RATETest_+3A_q">q</code></td>
<td>
<p>numeric, see <a href="grf.html#topic+rank_average_treatment_effect">rank_average_treatment_effect</a>.</p>
</td></tr>
<tr><td><code id="RATETest_+3A_r">R</code></td>
<td>
<p>integer, see <a href="grf.html#topic+rank_average_treatment_effect">rank_average_treatment_effect</a>.</p>
</td></tr>
<tr><td><code id="RATETest_+3A_subset">subset</code></td>
<td>
<p>numeric, see <a href="grf.html#topic+rank_average_treatment_effect">rank_average_treatment_effect</a>.</p>
</td></tr>
<tr><td><code id="RATETest_+3A_debiasing.weights">debiasing.weights</code></td>
<td>
<p>numeric, see
<a href="grf.html#topic+rank_average_treatment_effect">rank_average_treatment_effect</a>.</p>
</td></tr>
<tr><td><code id="RATETest_+3A_compliance.score">compliance.score</code></td>
<td>
<p>numeric, see
<a href="grf.html#topic+rank_average_treatment_effect">rank_average_treatment_effect</a>.</p>
</td></tr>
<tr><td><code id="RATETest_+3A_num.trees.for.weights">num.trees.for.weights</code></td>
<td>
<p>integer, see
<a href="grf.html#topic+rank_average_treatment_effect">rank_average_treatment_effect</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of class 'rank_average_treatment_effect' with elements
</p>

<ul>
<li><p> estimate: the RATE estimate.
</p>
</li>
<li><p> std.err: bootstrapped standard error of RATE.
</p>
</li>
<li><p> target: the type of estimate.
</p>
</li>
<li><p> TOC: a data.frame with the Targeting Operator Characteristic curve
estimated on grid q, along with bootstrapped SEs.
</p>
</li>
<li><p> confint: a data.frame with the lower and upper bounds of the RATE
confidence interval.
</p>
</li>
<li><p> pval: the p-value for the test that RATE is non-positive.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>KIJA
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 800
p &lt;- 3
X &lt;- matrix(rnorm(n * p), n, p)
W &lt;- rbinom(n, 1, 0.5)
event_prob &lt;- 1 / (1 + exp(2 * (pmax(2 * X[, 1], 0) * W - X[, 2])))
Y &lt;- rbinom(n, 1, event_prob)
cf &lt;- grf::causal_forest(X, Y, W)
rate &lt;- RATETest(cf, 1)
rate$pval


</code></pre>

<hr>
<h2 id='summary.svy_vglm'>Summary function for svy_vglm objects</h2><span id='topic+summary.svy_vglm'></span>

<h3>Description</h3>

<p>Internal summary function for svy_vglm objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'svy_vglm'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.svy_vglm_+3A_object">object</code></td>
<td>
<p>An svy_vglm object</p>
</td></tr>
<tr><td><code id="summary.svy_vglm_+3A_...">...</code></td>
<td>
<p>additional arguments. Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A &quot;summary.svy_vglm&quot; object is returned.
</p>

<hr>
<h2 id='try_catch_warnings'>Try Catch with Warning Handling</h2><span id='topic+try_catch_warnings'></span>

<h3>Description</h3>

<p>Try Catch with Warning Handling
</p>


<h3>Usage</h3>

<pre><code class='language-R'>try_catch_warnings(expr, character = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="try_catch_warnings_+3A_expr">expr</code></td>
<td>
<p>An expression to be evaluated.</p>
</td></tr>
<tr><td><code id="try_catch_warnings_+3A_character">character</code></td>
<td>
<p>A logical indicating if the returned error and warning
should be characters (<code>character</code> = <code>TRUE</code>) or language
(<code>character</code> = <code>FALSE</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>try_catch_warnings()</code> funciton returns a list with three elements
</p>

<ul>
<li> <p><code>values</code> is the evaluated <code>expr</code> or <code>NULL</code> if the evaluations throws an
error.
</p>
</li>
<li> <p><code>warning</code> is any warning given while evaluating <code>expr</code>. When <code>character</code> =
<code>FALSE</code>, the default, <code>warning</code> is a <a href="base.html#topic+simpleWarning">simpleWarning</a>, otherwise
it is a character.
</p>
</li>
<li> <p><code>error</code> is any error given while trying to evaluate <code>expr</code>. When
<code>character</code> = <code>FALSE</code>, the default, <code>error</code> is a <a href="base.html#topic+simpleError">simpleError</a>,
otherwise it is a character.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># No errors or warnings
try_catch_warnings(log(2))

# Warnings
try_catch_warnings(log(-1))

# Errors
try_catch_warnings(stop("Error Message"))
try_catch_warnings(stop("Error Message"), character = TRUE)

</code></pre>

<hr>
<h2 id='vcovHC'>Heteroscedasticity-Consistent Covariance Matrix</h2><span id='topic+vcovHC'></span>

<h3>Description</h3>

<p>Calculate Heteroscedasticity-Consistent Covariance Matrix from a linear
model using the HC3 method from sandwich.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vcovHC(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vcovHC_+3A_x">x</code></td>
<td>
<p>lm object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix containing the covariance matrix estimate.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>1+1
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
