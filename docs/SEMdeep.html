<!DOCTYPE html><html lang="en"><head><title>Help for package SEMdeep</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {SEMdeep}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#classificationReport'><p>Prediction evaluation report of a classification model</p></a></li>
<li><a href='#crossValidation'><p>Cross-validation of linear SEM, ML or DNN training models</p></a></li>
<li><a href='#getConnectionWeight'><p>Connection Weight method for neural network variable importance</p></a></li>
<li><a href='#getGradientWeight'><p>Gradient Weight method for neural network variable importance</p></a></li>
<li><a href='#getShapleyR2'><p>Compute variable importance using Shapley (R2) values</p></a></li>
<li><a href='#getSignificanceTest'><p>Test for the significance of neural network inputs</p></a></li>
<li><a href='#getVariableImportance'><p>Variable importance for Machine Learning models</p></a></li>
<li><a href='#mapGraph'><p>Map additional variables (nodes) to a graph object</p></a></li>
<li><a href='#nplot'><p>Create a plot for a neural network model</p></a></li>
<li><a href='#predict.DNN'><p>SEM-based out-of-sample prediction using layer-wise DNN</p></a></li>
<li><a href='#predict.ML'><p>SEM-based out-of-sample prediction using node-wise ML</p></a></li>
<li><a href='#predict.SEM'><p>SEM-based out-of-sample prediction using layer-wise ordering</p></a></li>
<li><a href='#SEMdnn'><p>Layer-wise SEM train with a Deep Neural Netwok (DNN)</p></a></li>
<li><a href='#SEMml'><p>Nodewise SEM train using Machine Learning (ML)</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Structural Equation Modeling with Deep Neural Network and
Machine Learning</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2025-01-20</td>
</tr>
<tr>
<td>Description:</td>
<td>Training and validation of a custom (or data-driven) Structural
    Equation Models using layer-wise Deep Neural Networks or node-wise
	Machine Learning algorithms, which extend the fitting procedures of
	the	'SEMgraph' R package &lt;<a href="https://doi.org/10.32614%2FCRAN.package.SEMgraph">doi:10.32614/CRAN.package.SEMgraph</a>&gt;.</td>
</tr>
<tr>
<td>Depends:</td>
<td>SEMgraph (&ge; 1.2.2), igraph (&ge; 2.0.0), R (&ge; 4.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>cito, corpcor, doSNOW, foreach, kernelshap, lavaan, glmnet,
NeuralNetTools, nnet, progress, ranger, rpart, torch, xgboost</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/BarbaraTarantino/SEMdeep">https://github.com/BarbaraTarantino/SEMdeep</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Author:</td>
<td>Mario Grassi [aut],
  Barbara Tarantino [cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Barbara Tarantino &lt;barbara.tarantino01@universitadipavia.it&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-23 14:45:29 UTC; mario</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-23 19:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='classificationReport'>Prediction evaluation report of a classification model</h2><span id='topic+classificationReport'></span>

<h3>Description</h3>

<p>This function builds a report showing the main classification 
metrics. It provides an overview of key evaluation metrics like precision, 
recall, F1-score, accuracy, Matthew's correlation coefficient (mcc) and
support (testing size) for each class in the dataset and averages (macro or
weighted) for all classes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>classificationReport(yobs, yhat, CM = NULL, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="classificationReport_+3A_yobs">yobs</code></td>
<td>
<p>A vector with the true target variable values.</p>
</td></tr>
<tr><td><code id="classificationReport_+3A_yhat">yhat</code></td>
<td>
<p>A matrix with the predicted target variables values.</p>
</td></tr>
<tr><td><code id="classificationReport_+3A_cm">CM</code></td>
<td>
<p>An optional (external) confusion matrix CxC.</p>
</td></tr>
<tr><td><code id="classificationReport_+3A_verbose">verbose</code></td>
<td>
<p>A logical value (default = FALSE). If TRUE, the confusion
matrix is printed on the screen, and if C=2, the density plots of the
predicted probability for each group are also printed.</p>
</td></tr>
<tr><td><code id="classificationReport_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given one vector with the true target variable labels, 
and the a matrix with the predicted target variable values for each class, 
a series of classification metrics is computed. 
For example, suppose a 2x2 table with notation
</p>

<table>
<tr>
 <td style="text-align: right;"> </td><td style="text-align: center;"> Predicted </td><td style="text-align: center;"> </td>
</tr>
<tr>
 <td style="text-align: right;"> Observed </td><td style="text-align: center;"> Yes Event </td><td style="text-align: center;"> No Event
</td>
</tr>
<tr>
 <td style="text-align: right;"> Yes Event </td><td style="text-align: center;"> A </td><td style="text-align: center;"> C </td>
</tr>
<tr>
 <td style="text-align: right;"> No Event </td><td style="text-align: center;"> B </td><td style="text-align: center;"> D </td>
</tr>
<tr>
 <td style="text-align: right;"> </td>
</tr>

</table>

<p>The formulas used here for the label = &quot;Yes Event&quot; are:
</p>
<p style="text-align: center;"><code class="reqn">pre = A/(A+B)</code>
</p>
 <p style="text-align: center;"><code class="reqn">rec = A/(A+C)</code>
</p>
 
<p style="text-align: center;"><code class="reqn">F1 = (2*pre*rec)/(pre+rec)</code>
</p>

<p style="text-align: center;"><code class="reqn">acc = (A+D)/(A+B+C+D)</code>
</p>

<p style="text-align: center;"><code class="reqn">mcc = (A*D-B*C)/sqrt((A+B)*(C+D)*(A+C)*(B+D))</code>
</p>

<p>Metrics analogous to those described above are calculated for the label
&quot;No Event&quot;, and the weighted average (averaging the support-weighted mean
per label) and macro average (averaging the unweighted mean per label) are
also provided.
</p>


<h3>Value</h3>

<p>A list of 3 objects:
</p>

<ol>
<li><p> &quot;CM&quot;, the confusion matrix between observed and predicted counts.
</p>
</li>
<li><p> &quot;stats&quot;, a data.frame with the classification evaluation statistics.
</p>
</li>
<li><p> &quot;cls&quot;, a data.frame with the predicted probabilities, predicted
labels and true labels of the categorical target variable.
</p>
</li></ol>



<h3>Author(s)</h3>

<p>Barbara Tarantino <a href="mailto:barbara.tarantino@unipv.it">barbara.tarantino@unipv.it</a>
</p>


<h3>References</h3>

<p>Sammut, C. &amp; Webb, G. I. (eds.) (2017). Encyclopedia of Machine Learning 
and Data Mining. New York: Springer. ISBN: 978-1-4899-7685-7
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# Load Sachs data (pkc)
ig&lt;- sachs$graph
data&lt;- sachs$pkc
data&lt;- transformData(data)$data
group&lt;- sachs$group

#...with train-test (0.5-0.5) samples
set.seed(123)
train&lt;- sample(1:nrow(data), 0.5*nrow(data))

#...with a categorical (as.factor) variable (C=2)
outcome&lt;- factor(ifelse(group == 0, "control", "case"))
res&lt;- SEMml(ig, data[train, ], outcome[train], algo="rf")
pred&lt;- predict(res, data[-train, ], outcome[-train], verbose=TRUE)

yobs&lt;- outcome[-train]
yhat&lt;- pred$Yhat[ ,levels(outcome)]
cls&lt;- classificationReport(yobs, yhat)
cls$CM
cls$stats
head(cls$cls)

#...with predicted probabiliy density plots, if C=2
cls&lt;- classificationReport(yobs, yhat, verbose=TRUE)

#...with a categorical (as.factor) variable (C=3)
group[1:400]&lt;- 2; table(group)
outcome&lt;- factor(ifelse(group == 0, "control",
				ifelse(group == 1, "case1", "case2")))
res&lt;- SEMml(ig, data[train, ], outcome[train], algo="rf")
pred&lt;- predict(res, data[-train, ], outcome[-train], verbose=TRUE)

yobs&lt;- outcome[-train]
yhat&lt;- pred$Yhat[ ,levels(outcome)]
cls&lt;- classificationReport(yobs, yhat)
cls$CM
cls$stats
head(cls$cls)


</code></pre>

<hr>
<h2 id='crossValidation'>Cross-validation of linear SEM, ML or DNN training models</h2><span id='topic+crossValidation'></span>

<h3>Description</h3>

<p>The function does a R-repeated K-fold cross-validation
of <code>SEMrun()</code>, <code>SEMml()</code> or <code>SEMdnn()</code> models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crossValidation(
  models,
  outcome = NULL,
  K = 5,
  R = 1,
  metric = NULL,
  ncores = 2,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="crossValidation_+3A_models">models</code></td>
<td>
<p>A named list of model fitting objects from <code>SEMrun()</code>,
<code>SEMml()</code> or <code>SEMdnn()</code> function, with default group=NULL (for
<code>SEMrun()</code> or outcome=NULL (for <code>SEMml()</code> or <code>SEMdnn()</code>).</p>
</td></tr>
<tr><td><code id="crossValidation_+3A_outcome">outcome</code></td>
<td>
<p>A character vector (as.factor) of labels for a categorical
output (target). If NULL (default), the categorical output (target) will
not be considered.</p>
</td></tr>
<tr><td><code id="crossValidation_+3A_k">K</code></td>
<td>
<p>A numerical value indicating the number of k-fold to create.</p>
</td></tr>
<tr><td><code id="crossValidation_+3A_r">R</code></td>
<td>
<p>A numerical value indicating the number of repetitions for the k-fold
cross-validation.</p>
</td></tr>
<tr><td><code id="crossValidation_+3A_metric">metric</code></td>
<td>
<p>A character value indicating the metric for boxplots display, i.e.:
&quot;amse&quot;, &quot;r2&quot;, or &quot;srmr&quot;, for continuous outcomes, and &quot;f1&quot;, &quot;accuracy&quot; or &quot;mcc&quot;,
for a categorical outcome (default = NULL).</p>
</td></tr>
<tr><td><code id="crossValidation_+3A_ncores">ncores</code></td>
<td>
<p>Number of cpu cores (default = 2).</p>
</td></tr>
<tr><td><code id="crossValidation_+3A_verbose">verbose</code></td>
<td>
<p>Output to console boxplots and summarized results (default = FALSE).</p>
</td></tr>
<tr><td><code id="crossValidation_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Easy-to-use model comparison and selection of SEM, ML or DNN models,
in which several models are defined and compared in a R-repeated K-fold
cross-validation procedure. The winner model is selected by reporting the mean
predicted performances across all runs, as outline in de Rooij &amp; Weeda (2020).
</p>


<h3>Value</h3>

<p>A list of 2 objects: (1) &quot;stats&quot;, a list with performance evaluation metrics.
If <code>outcome=FALSE</code>, mean and (0.025;0.0975)-quantiles of amse, r2, and srmr
across folds and repetitions are reported; if <code>outcome=TRUE</code>, mean and
(0.025;0.0975)-quantiles of f1, accuracy and mcc from confusion matrix averaged across
all repetitions are reported; and (2) &quot;PE&quot;, a data.frame of repeated cross-validation
results.
</p>


<h3>Author(s)</h3>

<p>Mario Grassi <a href="mailto:mario.grassi@unipv.it">mario.grassi@unipv.it</a>
</p>


<h3>References</h3>

<p>de Rooij M, Weeda W. Cross-Validation: A Method Every Psychologist Should Know.
Advances in Methods and Practices in Psychological Science. 2020;3(2):248-263.
doi:10.1177/2515245919898466
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# Load Amyotrophic Lateral Sclerosis (ALS)
ig&lt;- alsData$graph
data&lt;- alsData$exprs
data&lt;- transformData(data)$data
group&lt;- alsData$group

# ... with continuous outcomes 

res1 &lt;- SEMml(ig, data, algo="tree")
res2 &lt;- SEMml(ig, data, algo="rf")
res3 &lt;- SEMml(ig, data, algo="xgb")
res4 &lt;- SEMml(ig, data, algo="nn")

models &lt;- list(res1,res2,res3,res4)
names(models) &lt;- c("tree","rf","xgb","nn")

res.cv1 &lt;- crossValidation(models, outcome=NULL, K=5, R=10)
print(res.cv1$stats)

#... with a categorical (as.factor) outcome

outcome &lt;- factor(ifelse(group == 0, "control", "case"))
res.cv2 &lt;- crossValidation(models, outcome=outcome, K=5, R=10)
print(res.cv2$stats)


</code></pre>

<hr>
<h2 id='getConnectionWeight'>Connection Weight method for neural network variable importance</h2><span id='topic+getConnectionWeight'></span>

<h3>Description</h3>

<p>The function computes the matrix multiplications of hidden
weight matrices (Wx,...,Wy), i.e., the product of the raw input-hidden and
hidden-output connection weights between each input and output neuron and
sums the products across all hidden neurons, as proposed by Olden (2004).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getConnectionWeight(object, thr = NULL, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getConnectionWeight_+3A_object">object</code></td>
<td>
<p>A neural network object from <code>SEMdnn()</code> function.</p>
</td></tr>
<tr><td><code id="getConnectionWeight_+3A_thr">thr</code></td>
<td>
<p>A numeric value [0-1] indicating the threshold to apply to the
Olden's connection weights to color the graph. If thr = NULL (default), the
threshold is set to thr = 0.5*max(abs(connection weights)).</p>
</td></tr>
<tr><td><code id="getConnectionWeight_+3A_verbose">verbose</code></td>
<td>
<p>A logical value. If FALSE (default), the processed graph 
will not be plotted to screen.</p>
</td></tr>
<tr><td><code id="getConnectionWeight_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In a neural network, the connections between inputs and outputs are 
represented by the connection weights between the neurons. 
The importance values assigned to each input variable using the Olden method are
in units that are based directly on the summed product of the connection weights.
The amount and direction of the link weights largely determine the proportional
contributions of the input variables to the neural network's prediction output.
Input variables with larger connection weights indicate higher intensities
of signal transfer and are therefore more important in the prediction process.
Positive connection weights represent excitatory effects on neurons (raising the
intensity of the incoming signal) and increase the value of the predicted response, 
while negative connection weights represent inhibitory effects on neurons 
(reducing the intensity of the incoming signal). The weights that change sign
(e.g., positive to negative) between the input-hidden to hidden-output layers
would have a cancelling effect, and vice versa weights with the same sign would
have a synergistic effect.
Note that in order to map the connection weights to the DAG edges, the element-wise
product, W*A is performed between the Olden's weights entered in a matrix, W(pxp)
and the binary (1,0) adjacency matrix, A(pxp) of the input DAG.
</p>


<h3>Value</h3>

<p>A list of three object: (i) est: a data.frame including the connections together
with their connection weights(W), (ii) gest: if the outcome vector is given, a data.frame
of connection weights for outcome lavels, and (iii) dag: DAG with colored edges/nodes. If
abs(W) &gt; thr and W &lt; 0, the edge W &gt; 0, the edge is activated and it is highlighted
in red. If the outcome vector is given, nodes with absolute connection weights summed
over the outcome levels, i.e. sum(abs(W[outcome levels])) &gt; thr, will be highlighted
in pink.
</p>


<h3>Author(s)</h3>

<p>Mario Grassi <a href="mailto:mario.grassi@unipv.it">mario.grassi@unipv.it</a>
</p>


<h3>References</h3>

<p>Olden, Julian &amp; Jackson, Donald. (2002). Illuminating the &quot;black box&quot;: A 
randomization approach for understanding variable contributions in artificial
neural networks. Ecological Modelling. 154. 135-150. 10.1016/S0304-3800(02)00064-9. 
</p>
<p>Olden, Julian. (2004). An accurate comparison of methods for quantifying 
variable importance in artificial neural networks using simulated data. 
Ecological Modelling. 178. 10.1016/S0304-3800(04)00156-5.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

if (torch::torch_is_installed()){

# load ALS data
ig&lt;- alsData$graph
data&lt;- alsData$exprs
data&lt;- transformData(data)$data

#ncores&lt;- parallel::detectCores(logical = FALSE)
dnn0&lt;- SEMdnn(ig, data, outcome = NULL, thr = NULL,
			#hidden = 5*K, link = "selu", bias = TRUE,
			hidden = c(10,10,10), link = "selu", bias = TRUE,
			validation = 0,  epochs = 32, ncores = 2)

cw05&lt;- getConnectionWeight(dnn0, thr = 0.5, verbose = TRUE)
table(E(cw05$dag)$color)
}


</code></pre>

<hr>
<h2 id='getGradientWeight'>Gradient Weight method for neural network variable importance</h2><span id='topic+getGradientWeight'></span>

<h3>Description</h3>

<p>The function computes the gradient matrix, i.e., the average
conditional effects of the input variables w.r.t the neural network model,
as discussed by Amesöder et al (2024).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getGradientWeight(object, thr = NULL, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getGradientWeight_+3A_object">object</code></td>
<td>
<p>A neural network object from <code>SEMdnn()</code> function.</p>
</td></tr>
<tr><td><code id="getGradientWeight_+3A_thr">thr</code></td>
<td>
<p>A numeric value [0-1] indicating the threshold to apply to the
gradient weights to color the graph. If thr = NULL (default), the threshold
is set to thr = 0.5*max(abs(gradient weights)).</p>
</td></tr>
<tr><td><code id="getGradientWeight_+3A_verbose">verbose</code></td>
<td>
<p>A logical value. If FALSE (default), the processed graph
will not be plotted to screen.</p>
</td></tr>
<tr><td><code id="getGradientWeight_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The partial derivatives method calculates the derivative (the gradient)
of each output variable (y) with respect to each input variable (x) evaluated at
each observation (i=1,...,n) of the training data. The contribution of each input 
is evaluated in terms of both magnitude taking into account not only the connection
weights and activation functions, but also the values of each observation of the
input variables. 
Once the gradients for each variable and observation, a summary gradient is calculated
by averaging over the observation units. Finally, the average weights are entered into
a matrix, W(pxp) and the element-wise product with the binary (1,0) adjacency matrix,
A(pxp) of the input DAG, W*A maps the weights on the DAG edges.
Note that the operations required to compute partial derivatives are time consuming
compared to other methods such as Olden's (connection weight). The computational
time increases with the size of the neural network or the size of the data. Therefore,
the function uses a progress bar to check the progress of the gradient evaluation per
observation.
</p>


<h3>Value</h3>

<p>A list of three object: (i) est: a data.frame including the connections together
with their gradient weights, (ii) gest: if the outcome vector is given, a data.frame of
gradient weights for outcome lavels, and (iii) dag: DAG with colored edges/nodes. If
abs(grad) &gt; thr and grad &lt; 0, the edge is inhibited and it is highlighted in blue;
otherwise, if abs(grad) &gt; thr and grad &gt; 0, the edge is activated and it is highlighted
in red. If the outcome vector is given, nodes with absolute connection weights summed
over the outcome levels, i.e. sum(abs(grad[outcome levels])) &gt; thr, will be highlighted
in pink.
</p>


<h3>Author(s)</h3>

<p>Mario Grassi <a href="mailto:mario.grassi@unipv.it">mario.grassi@unipv.it</a>
</p>


<h3>References</h3>

<p>Amesöder, C., Hartig, F. and Pichler, M. (2024), ‘cito': an R package for training
neural networks using ‘torch'. Ecography, 2024: e07143. https://doi.org/10.1111/ecog.07143
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

if (torch::torch_is_installed()){

# load ALS data
ig&lt;- alsData$graph
data&lt;- alsData$exprs
data&lt;- transformData(data)$data

#ncores&lt;- parallel::detectCores(logical = FALSE)
dnn0&lt;- SEMdnn(ig, data, outcome = NULL, thr = NULL,
			#hidden = 5*K, link = "selu", bias = TRUE,
			hidden = c(10,10,10), link = "selu", bias = TRUE,
			validation = 0,  epochs = 32, ncores = 2)

gw05&lt;- getGradientWeight(dnn0, thr = 0.5, verbose = TRUE)
table(E(gw05$dag)$color)
}


</code></pre>

<hr>
<h2 id='getShapleyR2'>Compute variable importance using Shapley (R2) values</h2><span id='topic+getShapleyR2'></span>

<h3>Description</h3>

<p>This function computes variable contributions for individual
predictions using the Shapley values, a method from cooperative game
theory where the variable values of an observation work together to achieve
the prediction. In addition, to make variable contributions easily explainable, 
the function decomposes the entire model R-Squared (R2 or the coefficient
of determination) into variable-level attributions of the variance
(Redell, 2019).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getShapleyR2(
  object,
  newdata,
  newoutcome = NULL,
  thr = NULL,
  ncores = 2,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getShapleyR2_+3A_object">object</code></td>
<td>
<p>A model fitting object from <code>SEMml()</code>, <code>SEMdnn()</code> or
<code>SEMrun()</code> functions.</p>
</td></tr>
<tr><td><code id="getShapleyR2_+3A_newdata">newdata</code></td>
<td>
<p>A matrix containing new data with rows corresponding to subjects,
and columns to variables.</p>
</td></tr>
<tr><td><code id="getShapleyR2_+3A_newoutcome">newoutcome</code></td>
<td>
<p>A new character vector (as.factor) of labels for a categorical
output (target)(default = NULL).</p>
</td></tr>
<tr><td><code id="getShapleyR2_+3A_thr">thr</code></td>
<td>
<p>A numeric value [0-1] indicating the threshold to apply to the
signed Shapley R2 to color the graph. If thr = NULL (default), the
threshold is set to thr = 0.5*max(abs(signed Shapley R2 values)).</p>
</td></tr>
<tr><td><code id="getShapleyR2_+3A_ncores">ncores</code></td>
<td>
<p>number of cpu cores (default = 2)</p>
</td></tr>
<tr><td><code id="getShapleyR2_+3A_verbose">verbose</code></td>
<td>
<p>A logical value. If FALSE (default), the processed
graph will not be plotted to screen.</p>
</td></tr>
<tr><td><code id="getShapleyR2_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Lundberg &amp; Lee (2017) proposed a unified approach to both
local explainability (the variable contribution of a single variable within
a single sample) and global explainability (the variable contribution of the
entire model) by applying the fair distribution of payoffs principles from
game theory put forth by Shapley (1953). Now called SHAP (SHapley Additive
exPlanations), this suggested framework explains predictions of ML models,
where input variables take the place of players, and their contribution to
a particular prediction is measured using Shapley values. 
Successively, Redell (2019) presented a metric that combines the additive 
property of Shapley values with the robustness of the  R-squared (R2) of
Gelman (2018) to produce a variance decomposition that accurately captures
the contribution of each variable to the explanatory power of the model. 
We also use the signed R2, in order to denote the regulation of connections
in line with a linear SEM, since the edges in the DAG indicate node regulation
(activation, if positive; inhibition, if negative). This has been recovered for
each edge using sign(beta), i.e., the sign of the coefficient estimates from a
linear model (lm) fitting of the output node on the input nodes, as suggested
by Joseph (2019).
Additionally, in order to ascertain the local significance of node regulation
with respect to the DAG, the Shapley decomposition of the R-squared values for
each outcome node (r=1,...,R) can be computed by summing the ShapleyR2 indices
of their input nodes.
Finally, It should be noted that the operations required to compute kernel SHAP
values are inherently time-consuming, with the computational time increasing
in proportion to the number of predictor variables and the number of observations.
Therefore, the function uses a progress bar to check the progress of the kernel
SHAP evaluation per observation.
</p>


<h3>Value</h3>

<p>A list od four object: (i) shapx: the list of individual Shapley values
of predictors variables per each response variable; (ii) est: a data.frame including
the connections together with their signed Shapley R-squred values; (iii) gest:
if the outcome vector is given, a data.frame of signed Shapley R-squred values per
outcome levels; and (iv) dag: DAG with colored edges/nodes. If abs(sign_r2) &gt; thr
and sign_r2 &lt; 0, the edge is inhibited and it is highlighted in blue; otherwise,
if abs(sign_r2) &gt; thr and sign_r2 &gt; 0, the edge is activated and it is highlighted
in red. If the outcome vector is given, nodes with absolute connection weights
summed over the outcome levels, i.e. sum(abs(sign_r2[outcome levels])) &gt; thr, will
be highlighted in pink.
</p>


<h3>Author(s)</h3>

<p>Mario Grassi <a href="mailto:mario.grassi@unipv.it">mario.grassi@unipv.it</a>
</p>


<h3>References</h3>

<p>Shapley, L. (1953) A Value for n-Person Games. In: Kuhn, H. and Tucker, A., 
Eds., Contributions to the Theory of Games II, Princeton University Press,
Princeton, 307-317. 
</p>
<p>Scott M. Lundberg, Su-In Lee. (2017). A unified approach to interpreting 
model predictions. In Proceedings of the 31st International Conference on 
Neural Information Processing Systems (NIPS'17). Curran Associates Inc., 
Red Hook, NY, USA, 4768–4777.
</p>
<p>Redell, N. (2019). Shapley Decomposition of R-Squared in Machine Learning 
Models. arXiv: Methodology.
</p>
<p>Gelman, A., Goodrich, B., Gabry, J., &amp; Vehtari, A. (2019). R-squared for 
Bayesian Regression Models. The American Statistician, 73(3), 307–309.
</p>
<p>Joseph, A. Parametric inference with universal function approximators (2019).
Bank of England working papers 784, Bank of England, revised 22 Jul 2020.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# load ALS data
ig&lt;- alsData$graph
data&lt;- alsData$exprs
data&lt;- transformData(data)$data

#...with train-test (0.5-0.5) samples
set.seed(123)
train&lt;- sample(1:nrow(data), 0.5*nrow(data))

rf0&lt;- SEMml(ig, data[train, ], algo="rf")

res&lt;- getShapleyR2(rf0, data[-train, ], thr=NULL, verbose=TRUE)
table(E(res$dag)$color)

# shapley R2 per response variables
R2&lt;- abs(res$est[,4])
Y&lt;- res$est[,1]
R2Y&lt;- aggregate(R2~Y,data=data.frame(R2,Y),FUN="sum");R2Y
r2&lt;- mean(R2Y$R2);r2


</code></pre>

<hr>
<h2 id='getSignificanceTest'>Test for the significance of neural network inputs</h2><span id='topic+getSignificanceTest'></span>

<h3>Description</h3>

<p>The function computes a formal test for the significance of
neural network input nodes, based on a linear relationship between the
observed output and the predicted values of an input variable, when all
other input variables are maintained at their mean values, as proposed by
Mohammadi (2018).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getSignificanceTest(object, thr = NULL, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getSignificanceTest_+3A_object">object</code></td>
<td>
<p>A neural network object from <code>SEMdnn()</code> function.</p>
</td></tr>
<tr><td><code id="getSignificanceTest_+3A_thr">thr</code></td>
<td>
<p>A numeric value [0-1] indicating the threshold to apply to the
t-test values to color the graph. If thr = NULL (default), the threshold
is set to thr = 0.5*max(abs(t-test values)).</p>
</td></tr>
<tr><td><code id="getSignificanceTest_+3A_verbose">verbose</code></td>
<td>
<p>A logical value. If FALSE (default), the processed graph 
will not be plotted to screen.</p>
</td></tr>
<tr><td><code id="getSignificanceTest_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A neural network with an arbitrary architecture is trained, 
taking into account factors like the number of neurons, hidden layers, 
and activation function. Then, network's output is simulated to get 
the predicted values of the output variable, fixing all the inputs
(with the exception of one nonconstant input variable) at their mean
values. Subsequently, the network's predictions are stored after this
process is completed for each input variable. As last step, multiple
regression analysis is applied node-wise (mapping the input DAG) on the
observed output nodes with the predicted values of the input nodes as
explanatory variables. The statistical significance of the coefficients
is evaluated with the standard t-student critical values, which represent
the importance of the input variables.
</p>


<h3>Value</h3>

<p>A list of three object: (i) est: a data.frame including the connections together
with their t_test weights, (ii) gest: if the outcome vector is given, a data.frame of
t_test weights for outcome lavels, and (iii) dag: DAG with colored edges/nodes. If
abs(t_test) &gt; thr and t_test &lt; 0, the edge is inhibited and it is highlighted in blue;
otherwise, if abs(t_test) &gt; thr and t_test &gt; 0, the edge is activated and it is highlighted
in red. If the outcome vector is given, nodes with absolute connection weights summed
over the outcome levels, i.e. sum(abs(t_test[outcome levels])) &gt; thr, will be highlighted
in pink.
</p>


<h3>Author(s)</h3>

<p>Mario Grassi <a href="mailto:mario.grassi@unipv.it">mario.grassi@unipv.it</a>
</p>


<h3>References</h3>

<p>S. Mohammadi. A new test for the significance of neural network
inputs. Neurocomputing 2018; 273: 304-322.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

if (torch::torch_is_installed()){

# load ALS data
ig&lt;- alsData$graph
data&lt;- alsData$exprs
data&lt;- transformData(data)$data

#ncores&lt;- parallel::detectCores(logical = FALSE)
dnn0 &lt;- SEMdnn(ig, data, outcome = NULL, thr = NULL,
			#hidden = 5*K, link = "selu", bias = TRUE,
			hidden = c(10,10,10), link = "selu", bias = TRUE,
			validation = 0,  epochs = 32, ncores = 2)

st05&lt;- getSignificanceTest(dnn0, thr = 2, verbose = TRUE)
table(E(st05$dag)$color)
}


</code></pre>

<hr>
<h2 id='getVariableImportance'>Variable importance for Machine Learning models</h2><span id='topic+getVariableImportance'></span>

<h3>Description</h3>

<p>Extraction of ML variable importance measures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getVariableImportance(object, thr = NULL, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getVariableImportance_+3A_object">object</code></td>
<td>
<p>A model fitting object from <code>SEMml()</code> function.</p>
</td></tr>
<tr><td><code id="getVariableImportance_+3A_thr">thr</code></td>
<td>
<p>A numeric value [0-1] indicating the threshold to apply to the
variable importance values to color the graph. If thr = NULL (default), the
threshold is set to thr = 0.5*max(abs(variable importance values)).</p>
</td></tr>
<tr><td><code id="getVariableImportance_+3A_verbose">verbose</code></td>
<td>
<p>A logical value. If FALSE (default), the processed graph 
will not be plotted to screen.</p>
</td></tr>
<tr><td><code id="getVariableImportance_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The variable (predictor) importance will be computed considering:
(i) the absolute value of the z-statistic of the model parameters for &quot;sem&quot;;
(ii) the variable importance measures from the <code><a href="rpart.html#topic+rpart">rpart</a></code>,
<code><a href="ranger.html#topic+importance">importance</a></code> or <code><a href="xgboost.html#topic+xgb.importance">xgb.importance</a></code> functions
for &quot;tree&quot;, &quot;rf&quot; or &quot;xgb&quot;; and (iii) the Olden's connection weights for &quot;nn&quot; or &quot;dnn&quot;
methods.
</p>


<h3>Value</h3>

<p>A list of three object: (i) est: a data.frame including the connections together
with their variable importances (VarImp)), (ii) gest: if the outcome vector is given,
a data.frame of VarImp for outcome lavels, and (iii) dag: DAG with colored edges/nodes.
If abs(VarImp) &gt; thr will be highlighted in red (VarImp &gt; 0) or blue (VarImp &lt; 0). If
the outcome vector is given, nodes with variable importances summed over the outcome
levels, i.e. sum(VarImp[outcome levels])) &gt; thr, will be highlighted in pink.
</p>


<h3>Author(s)</h3>

<p>Mario Grassi <a href="mailto:mario.grassi@unipv.it">mario.grassi@unipv.it</a>
</p>


<h3>References</h3>

<p>add references
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# load ALS data
ig&lt;- alsData$graph
data&lt;- alsData$exprs
data&lt;- transformData(data)$data

#ncores&lt;- parallel::detectCores(logical = FALSE)
ml0&lt;- SEMml(ig, data, outcome=NULL, algo="rf", ncores=2)

vi05&lt;- getVariableImportance(ml0, thr=0.5, verbose=TRUE)
table(E(vi05$dag)$color)


</code></pre>

<hr>
<h2 id='mapGraph'>Map additional variables (nodes) to a graph object</h2><span id='topic+mapGraph'></span>

<h3>Description</h3>

<p>The function insert additional nodes to a graph object.
Among the node types, additional source or sink nodes can be added. 
Regarding the former, source nodes can represent: (i) data variables; 
(ii) a group variable; (iii) Latent Variables (LV). For the latter,
sink nodes represent the levels of a categorical outcome variable and
are linked with all graph nodes.' Moreover, <code>mapGraph()</code> can also
create a new graph object starting from a compact symbolic formula.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mapGraph(graph, type, C = NULL, LV = NULL, f = NULL, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mapGraph_+3A_graph">graph</code></td>
<td>
<p>An igraph object.</p>
</td></tr>
<tr><td><code id="mapGraph_+3A_type">type</code></td>
<td>
<p>A character value specifying the type of mapping. Five 
types can be specified.
</p>

<ol>
<li><p> &quot;source&quot;, source nodes are linked to sink nodes of the graph.
</p>
</li>
<li><p> &quot;group&quot;, an additional group source node is added to the graph.
</p>
</li>
<li><p> &quot;outcome&quot;, additional c=1,2,...,C sink nodes are added to the graph.
</p>
</li>
<li><p> &quot;LV&quot;, additional latent variable (LV) source nodes are added to the graph. 
</p>
</li>
<li><p> &quot;clusterLV&quot;, a series of clusters for the data are computed
and a different LV source node is added separately for each cluster.
</p>
</li></ol>
</td></tr>
<tr><td><code id="mapGraph_+3A_c">C</code></td>
<td>
<p>the number of labels of the categorical sink node (default = NULL).</p>
</td></tr>
<tr><td><code id="mapGraph_+3A_lv">LV</code></td>
<td>
<p>The number of LV source nodes to add to the graph. This argument 
needs to be specified when <code>type = "LV"</code>. When <code>type = "clusterLV"</code>
the LV number is defined internally equal to the number of clusters.
(default = NULL).</p>
</td></tr>
<tr><td><code id="mapGraph_+3A_f">f</code></td>
<td>
<p>A formula object (default = NULL). A new graph object is created
according to the specified formula object.</p>
</td></tr>
<tr><td><code id="mapGraph_+3A_verbose">verbose</code></td>
<td>
<p>If TRUE disply the mapped graph (default = FALSE)</p>
</td></tr>
<tr><td><code id="mapGraph_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>mapGraph returns invisibly the graphical object with the
mapped node variables.
</p>


<h3>Author(s)</h3>

<p>Mario Grassi <a href="mailto:mario.grassi@unipv.it">mario.grassi@unipv.it</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Load Amyotrophic Lateral Sclerosis (ALS)
ig&lt;- alsData$graph; gplot(ig)

# ... map source nodes to sink nodes of ALS graph 
ig1 &lt;- mapGraph(ig, type = "source"); gplot(ig1, l="dot")

# ... map group source node to ALS graph 
ig2 &lt;- mapGraph(ig, type = "group"); gplot(ig2, l="fdp")

# ... map outcome sink (C=2) to ALS graph 
ig3 &lt;- mapGraph(ig, type = "outcome", C=2); gplot(ig3, l="fdp")

# ... map LV source nodes to ALS graph 
ig4 &lt;- mapGraph(ig, type = "LV", LV = 3); gplot(ig4, l="fdp")

# ... map LV source nodes to the cluster nodes of ALS graph 
ig5 &lt;- mapGraph(ig, type = "clusterLV"); gplot(ig5, l="dot")

# ... create a new graph with the formula variables
formula &lt;- as.formula("z4747 ~ z1432 + z5603 + z5630")
ig6 &lt;- mapGraph(f=formula); gplot(ig6)
</code></pre>

<hr>
<h2 id='nplot'>Create a plot for a neural network model</h2><span id='topic+nplot'></span>

<h3>Description</h3>

<p>The function draws a neural network plot as a neural
interpretation diagram using with the <code><a href="NeuralNetTools.html#topic+plotnet">plotnet</a></code>
function of the <span class="pkg">NeuralNetTools</span> R package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nplot(dnn.fit, bias = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nplot_+3A_dnn.fit">dnn.fit</code></td>
<td>
<p>A neural network model from <span class="pkg">cito</span> R package.</p>
</td></tr>
<tr><td><code id="nplot_+3A_bias">bias</code></td>
<td>
<p>A logical value, indicating whether to draw biases in 
the layers (default = FALSE).</p>
</td></tr>
<tr><td><code id="nplot_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The induced subgraph of the input graph mapped on data 
variables. Based on the estimated connection weights, if the connection
weight W &gt; 0, the connection is activated and it is highlighted in red;  
if W &lt; 0, the connection is inhibited and it is highlighted in blue.
</p>


<h3>Value</h3>

<p>The function invisibly returns the graphical object representing
the neural network architecture designed by NeuralNetTools.
</p>


<h3>Author(s)</h3>

<p>Mario Grassi <a href="mailto:mario.grassi@unipv.it">mario.grassi@unipv.it</a>
</p>


<h3>References</h3>

<p>Beck, M.W. 2018. NeuralNetTools: Visualization and Analysis Tools for Neural 
Networks. Journal of Statistical Software. 85(11):1-20.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

if (torch::torch_is_installed()){

# load ALS data
ig&lt;- alsData$graph
data&lt;- alsData$exprs
data&lt;- transformData(data)$data

#ncores&lt;- parallel::detectCores(logical = FALSE)
dnn0 &lt;- SEMdnn(ig, data, train=1:nrow(data), grad = FALSE,
			#hidden = 5*K, link = "selu", bias = TRUE,
			hidden = c(10, 10, 10), link = "selu", bias =TRUE,
			validation = 0, epochs = 32, ncores = 2)

 for (j in 1:length(dnn0$model)) {
   nnj &lt;- dnn0$model[[j]][[1]]
   nplot(nnj)
   Sys.sleep(5)
 }
}


</code></pre>

<hr>
<h2 id='predict.DNN'>SEM-based out-of-sample prediction using layer-wise DNN</h2><span id='topic+predict.DNN'></span>

<h3>Description</h3>

<p>Predict method for DNN objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'DNN'
predict(object, newdata, newoutcome = NULL, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.DNN_+3A_object">object</code></td>
<td>
<p>A model fitting object from <code>SEMdnn()</code> function.</p>
</td></tr>
<tr><td><code id="predict.DNN_+3A_newdata">newdata</code></td>
<td>
<p>A matrix containing new data with rows corresponding to
subjects, and columns to variables.</p>
</td></tr>
<tr><td><code id="predict.DNN_+3A_newoutcome">newoutcome</code></td>
<td>
<p>A new character vector (as.factor) of labels for a categorical
output (target) (default = NULL).</p>
</td></tr>
<tr><td><code id="predict.DNN_+3A_verbose">verbose</code></td>
<td>
<p>Print predicted out-of-sample MSE values (default = FALSE).</p>
</td></tr>
<tr><td><code id="predict.DNN_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of three objects:
</p>

<ol>
<li><p> &quot;PE&quot;, vector of the amse = average MSE over all (sink and mediators)
graph nodes; r2 = 1 - amse; and srmr= Standardized Root Means Square Residual
between the out-of-bag correlation matrix and the model correlation matrix.
</p>
</li>
<li><p> &quot;mse&quot;, vector of the Mean Squared Error (MSE) for each out-of-bag
prediction of the sink and mediators graph nodes.
</p>
</li>
<li><p> &quot;Yhat&quot;, the matrix of continuous predicted values of graph nodes  
(excluding source nodes) based on out-of-bag samples. 
</p>
</li></ol>



<h3>Author(s)</h3>

<p>Mario Grassi <a href="mailto:mario.grassi@unipv.it">mario.grassi@unipv.it</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

if (torch::torch_is_installed()){

# Load Amyotrophic Lateral Sclerosis (ALS)
ig&lt;- alsData$graph
data&lt;- alsData$exprs
data&lt;- transformData(data)$data
group&lt;- alsData$group 

#...with train-test (0.5-0.5) samples
set.seed(123)
train&lt;- sample(1:nrow(data), 0.5*nrow(data))
#ncores&lt;- parallel::detectCores(logical = FALSE)

start&lt;- Sys.time()
dnn0 &lt;- SEMdnn(ig, data[train, ],
			# hidden = 5*K, link = "selu", bias = TRUE, 
			hidden = c(10,10,10), link = "selu", bias = TRUE, 
			validation = 0, epochs = 32, ncores = 2)
end&lt;- Sys.time()
print(end-start)
pred.dnn &lt;- predict(dnn0, data[-train, ], verbose=TRUE)

# SEMrun vs. SEMdnn MSE comparison
sem0 &lt;- SEMrun(ig, data[train, ], algo="ricf", n_rep=0)
pred.sem &lt;- predict(sem0, data[-train,], verbose=TRUE)

#...with a categorical (as.factor) outcome
outcome &lt;- factor(ifelse(group == 0, "control", "case")); table(outcome) 

start&lt;- Sys.time()
dnn1 &lt;- SEMdnn(ig, data[train, ], outcome[train],
			#hidden = 5*K, link = "selu", bias = TRUE,
			hidden = c(10,10,10), link = "selu", bias = TRUE,
			validation = 0,  epochs = 32, ncores = 2)
end&lt;- Sys.time()
print(end-start)

pred &lt;- predict(dnn1, data[-train, ], outcome[-train], verbose=TRUE)
yhat &lt;- pred$Yhat[ ,levels(outcome)]; head(yhat)
yobs &lt;- outcome[-train]; head(yobs)
classificationReport(yobs, yhat, verbose=TRUE)$stats
}


</code></pre>

<hr>
<h2 id='predict.ML'>SEM-based out-of-sample prediction using node-wise ML</h2><span id='topic+predict.ML'></span>

<h3>Description</h3>

<p>Predict method for ML objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ML'
predict(object, newdata, newoutcome = NULL, ncores = 2, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.ML_+3A_object">object</code></td>
<td>
<p>A model fitting object from <code>SEMml()</code> function.</p>
</td></tr>
<tr><td><code id="predict.ML_+3A_newdata">newdata</code></td>
<td>
<p>A matrix containing new data with rows corresponding to subjects,
and columns to variables.</p>
</td></tr>
<tr><td><code id="predict.ML_+3A_newoutcome">newoutcome</code></td>
<td>
<p>A new character vector (as.factor) of labels for a categorical
output (target)(default = NULL).</p>
</td></tr>
<tr><td><code id="predict.ML_+3A_ncores">ncores</code></td>
<td>
<p>number of cpu cores (default = 2)</p>
</td></tr>
<tr><td><code id="predict.ML_+3A_verbose">verbose</code></td>
<td>
<p>Print predicted out-of-sample MSE values (default = FALSE).</p>
</td></tr>
<tr><td><code id="predict.ML_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of 3 objects:
</p>

<ol>
<li><p> &quot;PE&quot;, vector of the amse = average MSE over all (sink and mediators)
graph nodes; r2 = 1 - amse; and srmr= Standardized Root Means Squared Residual
between the out-of-bag correlation matrix and the model correlation matrix.
</p>
</li>
<li><p> &quot;mse&quot;, vector of the Mean Squared Error (MSE) for each out-of-bag
prediction of the sink and mediators graph nodes.
</p>
</li>
<li><p> &quot;Yhat&quot;, the matrix of continuous predicted values of graph nodes  
(excluding source nodes) based on out-of-bag samples.
</p>
</li></ol>



<h3>Author(s)</h3>

<p>Mario Grassi <a href="mailto:mario.grassi@unipv.it">mario.grassi@unipv.it</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# Load Amyotrophic Lateral Sclerosis (ALS)
ig&lt;- alsData$graph
data&lt;- alsData$exprs
data&lt;- transformData(data)$data
group&lt;- alsData$group

#...with train-test (0.5-0.5) samples
set.seed(123)
train&lt;- sample(1:nrow(data), 0.5*nrow(data))

start&lt;- Sys.time()
# ... tree
res1&lt;- SEMml(ig, data[train, ], algo="tree")
mse1&lt;- predict(res1, data[-train, ], verbose=TRUE)

# ... rf
res2&lt;- SEMml(ig, data[train, ], algo="rf")
mse2&lt;- predict(res2, data[-train, ], verbose=TRUE)

# ... xgb
res3&lt;- SEMml(ig, data[train, ], algo="xgb")
mse3&lt;- predict(res3, data[-train, ], verbose=TRUE)

# ... nn
res4&lt;- SEMml(ig, data[train, ], algo="nn")
mse4&lt;- predict(res4, data[-train, ], verbose=TRUE)
end&lt;- Sys.time()
print(end-start)

#...with a categorical (as.factor) outcome
outcome &lt;- factor(ifelse(group == 0, "control", "case")); table(outcome) 

res5 &lt;- SEMml(ig, data[train, ], outcome[train], algo="tree")
pred &lt;- predict(res5, data[-train, ], outcome[-train], verbose=TRUE)
yhat &lt;- pred$Yhat[ ,levels(outcome)]; head(yhat)
yobs &lt;- outcome[-train]; head(yobs)
classificationReport(yobs, yhat, verbose=TRUE)$stats


</code></pre>

<hr>
<h2 id='predict.SEM'>SEM-based out-of-sample prediction using layer-wise ordering</h2><span id='topic+predict.SEM'></span>

<h3>Description</h3>

<p>Given the values of (observed) x-variables in a SEM,
this function may be used to predict the values of (observed) y-variables.
The predictive procedure consists of two steps: (1) construction of the
topological layer (TL) ordering of the input graph; (2) prediction of
the node y values in a layer, where the nodes included in the previous
layers act as predictors x.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SEM'
predict(object, newdata, newoutcome = NULL, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.SEM_+3A_object">object</code></td>
<td>
<p>An object, as that created by the function <code>SEMrun()</code>
with the argument <code>group</code> set to the default <code>group = NULL</code>.</p>
</td></tr>
<tr><td><code id="predict.SEM_+3A_newdata">newdata</code></td>
<td>
<p>A matrix with new data, with rows corresponding to subjects,
and columns to variables.</p>
</td></tr>
<tr><td><code id="predict.SEM_+3A_newoutcome">newoutcome</code></td>
<td>
<p>A new character vector (as.factor) of labels for a categorical
output (target)(default = NULL).</p>
</td></tr>
<tr><td><code id="predict.SEM_+3A_verbose">verbose</code></td>
<td>
<p>A logical value. If FALSE (default), the processed graph 
will not be plotted to screen.</p>
</td></tr>
<tr><td><code id="predict.SEM_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function first creates a layer-based structure of the
input graph. Then, a SEM-based predictive approach (Rooij et al., 2022) 
is used to produce predictions while accounting for the graph structure
based on the topological layer (j=1,…,L) of the input graph. In each iteration,
the response (output) variables, y are the nodes in the j=1,...,(L-1) layer and
the predictor (input) variables, x are the nodes belonging to the successive,
(j+1),...,L layers.
Predictions (for y given x) are based on the (joint y and x) model-implied 
variance-covariance (Sigma) matrix and mean vector (Mu) of the fitted SEM,
and the standard expression for the conditional mean of a multivariate normal
distribution. Thus, the layer structure described in the SEM is taken into
consideration, which differs from ordinary least squares (OLS) regression.
</p>


<h3>Value</h3>

<p>A list of 3 objects:
</p>

<ol>
<li><p> &quot;PE&quot;, vector of the amse = average MSE over all (sink and mediators)
graph nodes; r2 = 1 - amse; and srmr= Standardized Root Means Square Residual
between the out-of-bag correlation matrix and the model correlation matrix.
</p>
</li>
<li><p> &quot;mse&quot;, vector of the Mean Squared Error (MSE) for each out-of-bag
prediction of the sink and mediators graph nodes.
</p>
</li>
<li><p> &quot;Yhat&quot;, the matrix of continuous predicted values of graph nodes  
(excluding source nodes) based on out-of-bag samples. 
</p>
</li></ol>



<h3>Author(s)</h3>

<p>Mario Grassi <a href="mailto:mario.grassi@unipv.it">mario.grassi@unipv.it</a>
</p>


<h3>References</h3>

<p>de Rooij M, Karch JD, Fokkema M, Bakk Z, Pratiwi BC, and Kelderman H
(2023). SEM-Based Out-of-Sample Predictions, Structural Equation Modeling:
A Multidisciplinary Journal, 30:1, 132-148
&lt;https://doi.org/10.1080/10705511.2022.2061494&gt;
</p>
<p>Grassi M, Palluzzi F, Tarantino B (2022). SEMgraph: An R Package for Causal Network
Analysis of High-Throughput Data with Structural Equation Models.
Bioinformatics, 38 (20), 4829–4830 &lt;https://doi.org/10.1093/bioinformatics/btac567&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load ALS data
data&lt;- alsData$exprs
data&lt;- transformData(data)$data
group&lt;- alsData$group

#...with train-test (0.5-0.5) samples
set.seed(123)
train&lt;- sample(1:nrow(data), 0.5*nrow(data))

# predictors, source+mediator; outcomes, mediator+sink

ig &lt;- alsData$graph; gplot(ig)
sem0 &lt;- SEMrun(ig, data[train,], algo="ricf", n_rep=0)
pred0 &lt;- predict(sem0, newdata=data[-train,], verbose=TRUE) 

# predictors, source+mediator+group; outcomes, source+mediator+sink

ig1 &lt;- mapGraph(ig, type = "group"); gplot(ig1)
data1 &lt;- cbind(group, data); head(data1[,5])
sem1 &lt;- SEMrun(ig1, data1[train,], algo="ricf", n_rep=0)
pred1 &lt;- predict(sem1, newdata= data1[-train,], verbose=TRUE) 

# predictors, source nodes; outcomes, sink nodes

ig2 &lt;- mapGraph(ig, type = "source"); gplot(ig2)
sem2 &lt;- SEMrun(ig2, data[train,], algo="ricf", n_rep=0)
pred2 &lt;- predict(sem2, newdata=data[-train,], verbose=TRUE)

</code></pre>

<hr>
<h2 id='SEMdnn'>Layer-wise SEM train with a Deep Neural Netwok (DNN)</h2><span id='topic+SEMdnn'></span>

<h3>Description</h3>

<p>The function builds the topological layer (TL) ordering
of the input graph to fit a series of Deep Neural Networks (DNN) 
models, where the nodes in one layer act as response variables (output) 
y and the nodes in the sucessive layers act as predictors (input) x. 
Each fit uses the <code><a href="cito.html#topic+dnn">dnn</a></code> function of the <span class="pkg">cito</span> R
package, based on the deep learning framework 'torch'.
</p>
<p>The <span class="pkg">torch</span> package is native to R, so it's computationally efficient
and the installation is very simple, as there is no need to install Python
or any other API, and DNNs can be trained on CPU, GPU and MacOS GPUs.
In order to install <span class="pkg">torch</span> please follow these steps:
</p>
<p><code>install.packages("torch")</code>
</p>
<p><code>library(torch)</code>
</p>
<p><code>install_torch(reinstall = TRUE)</code>
</p>
<p>For setup GPU or if you have problems installing <span class="pkg">torch</span> package, check out the
<a href="https://torch.mlverse.org/docs/articles/installation.html/">installation</a>
help from the torch developer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SEMdnn(
  graph,
  data,
  outcome = NULL,
  thr = NULL,
  nboot = 0,
  hidden = c(10L, 10L, 10L),
  link = "relu",
  bias = TRUE,
  dropout = 0,
  loss = "mse",
  validation = 0,
  lambda = 0,
  alpha = 0.5,
  optimizer = "adam",
  lr = 0.01,
  epochs = 100,
  device = "cpu",
  ncores = 2,
  early_stopping = FALSE,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SEMdnn_+3A_graph">graph</code></td>
<td>
<p>An igraph object.</p>
</td></tr>
<tr><td><code id="SEMdnn_+3A_data">data</code></td>
<td>
<p>A matrix with rows corresponding to subjects, and columns
to graph nodes (variables).</p>
</td></tr>
<tr><td><code id="SEMdnn_+3A_outcome">outcome</code></td>
<td>
<p>A character vector (as.factor) of labels for a categorical
output (target). If NULL (default), the categorical output (target) will
not be considered.</p>
</td></tr>
<tr><td><code id="SEMdnn_+3A_thr">thr</code></td>
<td>
<p>A numeric value [0-1] indicating the threshold to apply to the
Olden's connection weights to color the graph. If thr = NULL (default), the
threshold is set to thr = 0.5*max(abs(connection weights)).</p>
</td></tr>
<tr><td><code id="SEMdnn_+3A_nboot">nboot</code></td>
<td>
<p>number of bootstrap samples that will be used to compute cheap
(lower, upper) CIs for all input variable weights. As a default, nboot = 0.</p>
</td></tr>
<tr><td><code id="SEMdnn_+3A_hidden">hidden</code></td>
<td>
<p>hidden units in layers; the number of layers corresponds with
the length of the hidden units. As a default, hidden = c(10L, 10L, 10L).</p>
</td></tr>
<tr><td><code id="SEMdnn_+3A_link">link</code></td>
<td>
<p>A character value describing the activation function to use, which 
might be a single length or be a vector with many activation functions assigned
to each layer. As a default, link = &quot;selu&quot;.</p>
</td></tr>
<tr><td><code id="SEMdnn_+3A_bias">bias</code></td>
<td>
<p>A logical vector, indicating whether to employ biases in the layers, 
which can be either vectors of logicals for each layer (number of hidden layers
+ 1 (final layer)) or of length one.  As a default, bias = TRUE.</p>
</td></tr>
<tr><td><code id="SEMdnn_+3A_dropout">dropout</code></td>
<td>
<p>A numerical value for the dropout rate, which is the probability
that a node will be excluded from training.  As a default, dropout = 0.</p>
</td></tr>
<tr><td><code id="SEMdnn_+3A_loss">loss</code></td>
<td>
<p>A character value specifying the at which the network should 
be optimized. For regression problem used in SEMdnn(), the user can specify:
(a) &quot;mse&quot; (mean squared error), &quot;mae&quot; (mean absolute error), or &quot;gaussian&quot;
(normal likelihood). As a default, loss = &quot;mse&quot;.</p>
</td></tr>
<tr><td><code id="SEMdnn_+3A_validation">validation</code></td>
<td>
<p>A numerical value indicating the proportion of the data set 
that should be used as a validation set (randomly selected, default = 0).</p>
</td></tr>
<tr><td><code id="SEMdnn_+3A_lambda">lambda</code></td>
<td>
<p>A numerical value indicating the strength of the regularization,
<code class="reqn">\lambda</code>(L1 + L2) for lambda penalty (default = 0).</p>
</td></tr>
<tr><td><code id="SEMdnn_+3A_alpha">alpha</code></td>
<td>
<p>A numerical vector to add L1/L2 regularization into the training. 
Set the alpha parameter for each layer to (1-<code class="reqn">\alpha</code>)L1 + <code class="reqn">\alpha</code>L2. 
It must fall between 0 and 1 (default = 0.5).</p>
</td></tr>
<tr><td><code id="SEMdnn_+3A_optimizer">optimizer</code></td>
<td>
<p>A character value indicating the optimizer to use for 
training the network. The user can specify: &quot;adam&quot; (ADAM algorithm), &quot;adagrad&quot;
(adaptive gradient algorithm), &quot;rmsprop&quot; (root mean squared propagation),
&quot;rprop” (resilient backpropagation), &quot;sgd&quot; (stochastic gradient descent).
As a default, optimizer = &quot;adam&quot;.</p>
</td></tr>
<tr><td><code id="SEMdnn_+3A_lr">lr</code></td>
<td>
<p>A numerical value indicating the learning rate given to the optimizer 
(default = 0.01).</p>
</td></tr>
<tr><td><code id="SEMdnn_+3A_epochs">epochs</code></td>
<td>
<p>A numerical value indicating the epochs during which the training 
is conducted (default = 100).</p>
</td></tr>
<tr><td><code id="SEMdnn_+3A_device">device</code></td>
<td>
<p>A character value describing the CPU/GPU device (&quot;cpu&quot;, &quot;cuda&quot;, &quot;mps&quot;)
on which the  neural network should be trained on. As a default, device = &quot;cpu&quot;.</p>
</td></tr>
<tr><td><code id="SEMdnn_+3A_ncores">ncores</code></td>
<td>
<p>number of cpu cores (default = 2)</p>
</td></tr>
<tr><td><code id="SEMdnn_+3A_early_stopping">early_stopping</code></td>
<td>
<p>If set to integer, training will terminate if the loss 
increases over a predetermined number of consecutive epochs and apply validation
loss when available. Default is FALSE, no early stopping is applied.</p>
</td></tr>
<tr><td><code id="SEMdnn_+3A_verbose">verbose</code></td>
<td>
<p>The training loss values of the DNN model are displayed as output,
comparing the training, validation and baseline in the last epoch (default = FALSE).</p>
</td></tr>
<tr><td><code id="SEMdnn_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By mapping data onto the input graph, <code>SEMdnn()</code> creates a set 
of DNN models based on the topological layer (j=1,…,L) structure of the input
graph. In each iteration, the response (output) variables, y are the nodes in
the j=1,...,(L-1) layer and the predictor (input) variables, x are the nodes
belonging to the successive, (j+1),...,L layers. 
Each DNN model is a Multilayer Perceptron (MLP) network, where every neuron node
is connected to every other neuron node in the hidden layer above and every other
hidden layer below. Each neuron's value is determined by calculating a weighted
summation of its outputs from the hidden layer before it, and then applying an
activation function.  The calculated value of every neuron is used as the input
for the neurons in the layer below it, until the output layer is reached.
</p>
<p>If boot != 0, the function will implement the cheap bootstrapping proposed by
Lam (2002) to generate uncertainties, i.e. 90
for DNN parameters. Bootstrapping can be enabled by setting a small number
(1 to 10) of bootstrap samples. Note, however, that the computation can be
time-consuming for massive DNNs, even with cheap bootstrapping!
</p>


<h3>Value</h3>

<p>An S3 object of class &quot;DNN&quot; is returned. It is a list of 5 objects:
</p>

<ol>
<li><p> &quot;fit&quot;, a list of DNN model objects, including: the estimated covariance 
matrix (Sigma), the estimated model errors (Psi), the fitting indices (fitIdx),
and the parameterEstimates, i.e., the data.frame of Olden's connection weights. 
</p>
</li>
<li><p> &quot;gest&quot;, the data.frame of estimated connection weights (parameterEstimates)
of outcome levels, if outcome != NULL.
</p>
</li>
<li><p> &quot;model&quot;, a list of all j=1,...,(L-1) fitted MLP network models.
</p>
</li>
<li><p> &quot;graph&quot;, the induced DAG of the input graph mapped on data variables.
The DAG is colored based on the Olden's connection weights (W), if abs(W) &gt; thr
and W &lt; 0, the edge is inhibited and it is highlighted in blue; otherwise, if
abs(W) &gt; thr and W &gt; 0, the edge is activated and it is highlighted in red.
If the outcome vector is given, nodes with absolute connection weights summed
over the outcome levels, i.e. sum(abs(W[outcome levels])) &gt; thr, will be
highlighted in pink.
</p>
</li>
<li><p> &quot;data&quot;, input data subset mapping graph nodes.
</p>
</li></ol>



<h3>Author(s)</h3>

<p>Mario Grassi <a href="mailto:mario.grassi@unipv.it">mario.grassi@unipv.it</a>
</p>


<h3>References</h3>

<p>Amesöder, C., Hartig, F. and Pichler, M. (2024), ‘cito': an R package for training
neural networks using ‘torch'. Ecography, 2024: e07143. https://doi.org/10.1111/ecog.07143
</p>
<p>Grassi M, Palluzzi F, Tarantino B (2022). SEMgraph: An R Package for Causal Network
Analysis of High-Throughput Data with Structural Equation Models.
Bioinformatics, 38 (20), 4829–4830. &lt;https://doi.org/10.1093/bioinformatics/btac567&gt;
</p>
<p>Lam, H. (2022). Cheap bootstrap for input uncertainty quantification. WSC '22:
Proceedings of the Winter Simulation Conference, 2318 - 2329.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

if (torch::torch_is_installed()){

# load ALS data
ig&lt;- alsData$graph
data&lt;- alsData$exprs
data&lt;- transformData(data)$data
group&lt;- alsData$group

#...with train-test (0.5-0.5) samples
set.seed(123)
train&lt;- sample(1:nrow(data), 0.5*nrow(data))
#ncores&lt;- parallel::detectCores(logical = FALSE)

start&lt;- Sys.time()
dnn0&lt;- SEMdnn(ig, data[train, ], thr = NULL,
			#hidden = 5*K, link = "selu", bias = TRUE,
			hidden = c(10,10,10), link = "selu", bias = TRUE,
			validation = 0, epochs = 32, ncores = 2)
end&lt;- Sys.time()
print(end-start)

#str(dnn0, max.level=2)
dnn0$fit$fitIdx
parameterEstimates(dnn0$fit)
gplot(dnn0$graph)
table(E(dnn0$graph)$color)

#...with source nodes -&gt; graph layer structure -&gt; sink nodes

#Topological layer (TL) ordering
K&lt;- c(12,  5,  3,  2,  1,  8)
K&lt;- rev(K[-c(1,length(K))]);K

ig1&lt;- mapGraph(ig, type="source"); gplot(ig1)

start&lt;- Sys.time()
dnn1&lt;- SEMdnn(ig1, data[train, ], thr = NULL,
			hidden = 5*K, link = "selu", bias = TRUE,
		validation = 0, epochs = 32, ncores = 2)
end&lt;- Sys.time()
print(end-start)

#Visualization of the neural network structure
nn1 &lt;- dnn1$model[[1]][[1]]
nplot(nn1, bias=FALSE)

#str(dnn1, max.level=2)
dnn1$fit$fitIdx
mean(dnn1$fit$Psi)
parameterEstimates(dnn1$fit)
gplot(dnn1$graph)
table(E(dnn1$graph)$color)

#...with a categorical outcome, a train set (0.5) and a validation set (0.2)
outcome&lt;- factor(ifelse(group == 0, "control", "case")); table(outcome) 

start&lt;- Sys.time()
dnn2&lt;- SEMdnn(ig, data[train, ], outcome[train], thr = NULL,
			#hidden = 5*K, link = "selu", bias = TRUE,
			hidden = c(10,10,10), link = "selu", bias = TRUE,
			validation = 0.2, epochs = 32, ncores = 2)
end&lt;- Sys.time()
print(end-start)

#str(dnn2, max.level=2)
dnn2$fit$fitIdx
parameterEstimates(dnn2$fit)
gplot(dnn2$graph) 
table(E(dnn2$graph)$color)
table(V(dnn2$graph)$color)
}


</code></pre>

<hr>
<h2 id='SEMml'>Nodewise SEM train using Machine Learning (ML)</h2><span id='topic+SEMml'></span>

<h3>Description</h3>

<p>The function converts a graph to a collection of 
nodewise-based models: each mediator or sink variable can be expressed as 
a function of its parents. Based on the assumed type of relationship, 
i.e. linear or non-linear, <code>SEMml()</code> fits a ML model to each
node (variable) with non-zero incoming connectivity. 
The model fitting is performed equation-by equation (r=1,...,R) 
times, where R is the number of mediators and sink nodes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SEMml(
  graph,
  data,
  outcome = NULL,
  algo = "sem",
  thr = NULL,
  nboot = 0,
  ncores = 2,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SEMml_+3A_graph">graph</code></td>
<td>
<p>An igraph object.</p>
</td></tr>
<tr><td><code id="SEMml_+3A_data">data</code></td>
<td>
<p>A matrix with rows corresponding to subjects, and columns 
to graph nodes (variables).</p>
</td></tr>
<tr><td><code id="SEMml_+3A_outcome">outcome</code></td>
<td>
<p>A character vector (as.fctor) of labels for a categorical
output (target). If NULL (default), the categorical output (target) will
not be considered.</p>
</td></tr>
<tr><td><code id="SEMml_+3A_algo">algo</code></td>
<td>
<p>ML method used for nodewise-network predictions.
Six algorithms can be specified:
</p>

<ul>
<li> <p><code>algo="sem"</code> (default) for a linear SEM, see <code><a href="SEMgraph.html#topic+SEMrun">SEMrun</a></code>. 
</p>
</li>
<li> <p><code>algo="tree"</code> for a CART model, see <code><a href="rpart.html#topic+rpart">rpart</a></code>.
</p>
</li>
<li> <p><code>algo="rf"</code> for a random forest model, see <code><a href="ranger.html#topic+ranger">ranger</a></code>.
</p>
</li>
<li> <p><code>algo="xgb"</code> for a XGBoost model, see <code><a href="xgboost.html#topic+xgboost">xgboost</a></code>.
</p>
</li>
<li> <p><code>algo="nn"</code> for a small neural network model (1 hidden layer and 10 nodes), see <code><a href="nnet.html#topic+nnet">nnet</a></code>.
</p>
</li>
<li> <p><code>algo="dnn"</code> for a large neural network model (1 hidden layers and 1000 nodes), see <code><a href="cito.html#topic+dnn">dnn</a></code>.
</p>
</li></ul>
</td></tr>
<tr><td><code id="SEMml_+3A_thr">thr</code></td>
<td>
<p>A numeric value [0-1] indicating the threshold to apply to the
variable importance values to color the graph. If thr = NULL (default), the
threshold is set to thr = 0.5*max(abs(variable importance values)).</p>
</td></tr>
<tr><td><code id="SEMml_+3A_nboot">nboot</code></td>
<td>
<p>number of bootstrap samples that will be used to compute cheap
(lower, upper) CIs for all input variable weights. As a default, nboot = 0.</p>
</td></tr>
<tr><td><code id="SEMml_+3A_ncores">ncores</code></td>
<td>
<p>number of cpu cores (default = 2)</p>
</td></tr>
<tr><td><code id="SEMml_+3A_verbose">verbose</code></td>
<td>
<p>A logical value. If FALSE (default), the processed graph
will not be plotted to screen.</p>
</td></tr>
<tr><td><code id="SEMml_+3A_...">...</code></td>
<td>
<p>Currently ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By mapping data onto the input graph, <code>SEMml()</code> creates
a set of nodewise-based models based on the directed links, i.e., 
a set of edges pointing in the same direction, between two nodes 
in the input graph that are causally relevant to each other. 
The mediator or sink variables can be characterized in detail as 
functions of their parents. An ML model (sem, tree, rf, xgb, nn, dnn) 
can then be fitted to each variable with non-zero inbound connectivity. 
With R representing the number of mediators and sink nodes in the 
network, the model fitting process is performed equation-by-equation 
(r=1,...,R) times.
</p>
<p>If boot != 0, the function will implement the cheap bootstrapping proposed by
Lam (2002) to generate uncertainties, i.e. 90
for ML parameters. Bootstrapping can be enabled by setting a small number
(1 to 10) of bootstrap samples. Note, however, that the computation can be
time-consuming for massive MLs, even with cheap bootstrapping!
</p>


<h3>Value</h3>

<p>An S3 object of class &quot;ML&quot; is returned. It is a list of 5 objects:
</p>

<ol>
<li><p> &quot;fit&quot;, a list of ML model objects, including: the estimated covariance 
matrix (Sigma), the estimated model errors (Psi), the fitting indices (fitIdx),
and the parameterEstimates, i.e., the variable importance measures (VarImp).
</p>
</li>
<li><p> &quot;gest&quot;, the data.frame of variable importances (parameterEstimates)
of outcome levels, if outcome != NULL.
</p>
</li>
<li><p> &quot;model&quot;, a list of all the fitted non-linear nodewise-based models 
(tree, rf, xgb, nn or dnn).
</p>
</li>
<li><p> &quot;graph&quot;, the induced DAG of the input graph  mapped on data variables. 
The DAG with colored edge/nodes based on the variable importance measures,
i.e., if abs(VarImp) &gt; thr will be highlighted in red (VarImp &gt; 0) or blue
(VarImp &lt; 0). If the outcome vector is given, nodes with variable importances
summed over the outcome levels, i.e. sum(VarImp[outcome levels])) &gt; thr,
will be highlighted in pink.
</p>
</li>
<li><p> &quot;data&quot;, input data subset mapping graph nodes.
</p>
</li></ol>

<p>Using the default <code>algo="sem"</code>, the usual output of a linear nodewise-based,
SEM, see <code><a href="SEMgraph.html#topic+SEMrun">SEMrun</a></code> (algo=&quot;cggm&quot;), will be returned.
</p>


<h3>Author(s)</h3>

<p>Mario Grassi <a href="mailto:mario.grassi@unipv.it">mario.grassi@unipv.it</a>
</p>


<h3>References</h3>

<p>Grassi M., Palluzzi F., and Tarantino B. (2022). SEMgraph: An R Package for Causal 
Network Analysis of High-Throughput Data with Structural Equation Models. 
Bioinformatics, 38 (20), 4829–4830 &lt;https://doi.org/10.1093/bioinformatics/btac567&gt;
</p>
<p>Breiman L., Friedman J.H., Olshen R.A., and Stone, C.J. (1984) Classification
and Regression Trees. Chapman and Hall/CRC.
</p>
<p>Breiman L. (2001). Random Forests, Machine Learning 45(1), 5-32.
</p>
<p>Chen T., and Guestrin C. (2016). XGBoost: A Scalable Tree Boosting System. 
Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge 
Discovery and Data Mining.
</p>
<p>Ripley B.D. (1996). Pattern Recognition and Neural Networks. Cambridge University Press.
</p>
<p>Lam, H. (2022). Cheap bootstrap for input uncertainty quantification. WSC '22:
Proceedings of the Winter Simulation Conference, 2318 - 2329.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# Load Amyotrophic Lateral Sclerosis (ALS)
ig&lt;- alsData$graph
data&lt;- alsData$exprs
data&lt;- transformData(data)$data
group&lt;- alsData$group

#...with train-test (0.5-0.5) samples
set.seed(123)
train&lt;- sample(1:nrow(data), 0.5*nrow(data))

start&lt;- Sys.time()
# ... tree
res1&lt;- SEMml(ig, data[train, ], algo="tree")

# ... rf
res2&lt;- SEMml(ig, data[train, ], algo="rf")

# ... xgb
res3&lt;- SEMml(ig, data[train, ], algo="xgb")

# ... nn
res4&lt;- SEMml(ig, data[train, ], algo="nn")

end&lt;- Sys.time()
print(end-start)

#visualizaation of the colored dag for algo="nn"
gplot(res4$graph, l="dot", main="nn")

#Comparison of fitting indices (in train data)
res1$fit$fitIdx #tree
res2$fit$fitIdx #rf
res3$fit$fitIdx #xgb
res4$fit$fitIdx #nn

#Comparison of parameter estimates (in train data)
parameterEstimates(res1$fit) #tree
parameterEstimates(res2$fit) #rf
parameterEstimates(res3$fit) #xgb
parameterEstimates(res4$fit) #nn

#Comparison of VarImp (in train data)
table(E(res1$graph)$color) #tree
table(E(res2$graph)$color) #rf
table(E(res3$graph)$color) #xgb
table(E(res4$graph)$color) #nn

#Comparison of AMSE, R2, SRMR (in test data)
print(predict(res1, data[-train, ])$PE) #tree
print(predict(res2, data[-train, ])$PE) #rf
print(predict(res3, data[-train, ])$PE) #xgb
print(predict(res4, data[-train, ])$PE) #nn

#...with a categorical (as.factor) outcome
outcome &lt;- factor(ifelse(group == 0, "control", "case")); table(outcome) 

res5 &lt;- SEMml(ig, data[train, ], outcome[train], algo="tree")
gplot(res5$graph)
table(E(res5$graph)$color)
table(V(res5$graph)$color)

pred &lt;- predict(res5, data[-train, ], outcome[-train], verbose=TRUE)
yhat &lt;- pred$Yhat[ ,levels(outcome)]; head(yhat)
yobs &lt;- outcome[-train]; head(yobs)
classificationReport(yobs, yhat, verbose=TRUE)$stats


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
