<!DOCTYPE html><html><head><title>Help for package intrinsicDimension</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {intrinsicDimension}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#addNoise'>
<p>Add Noise to Data Set</p></a></li>
<li><a href='#asPointwiseEstimator'><p> Turn a local estimator into a pointwise estimator</p></a></li>
<li><a href='#cornerPlane'><p> Corner Plane</p></a></li>
<li><a href='#cutHyperPlane'><p> Piece of Noisy Hyperplane</p></a></li>
<li><a href='#cutHyperSphere'><p> Piece of Noisy Hypersphere</p></a></li>
<li><a href='#dancoDimEst'>
<p>Dimension Estimation With the DANCo and MIND Methods</p></a></li>
<li><a href='#essLocalDimEst'><p> Expected Simplex Skewness Local Dimension Estimation</p></a></li>
<li><a href='#essReference'><p> ESS Reference Values</p></a></li>
<li><a href='#hyperCube'><p> Hypercube</p></a></li>
<li><a href='#ide'><p> Intrinsic Dimension Estimation</p></a></li>
<li><a href='#intrinsicDimension-package'>
<p>Intrinsic Dimension Estimation and Data on Manifolds</p></a></li>
<li><a href='#knnDimEst'><p> Dimension Estimation from kNN Distances</p></a></li>
<li><a href='#M_rozza'><p> Manifolds from Rozza et al. (2012)</p></a></li>
<li><a href='#mHeinManifold'><p> 12-dimensional manifold from Hein and Audibert (2005)</p></a></li>
<li><a href='#neighborhoods'><p> Obtaining neighborhoods (local data) from a data set</p></a></li>
<li><a href='#Noisefun'><p> Transition Functions Describing Noise</p></a></li>
<li><a href='#oblongNormal'><p> Oblong Normal Distribution</p></a></li>
<li><a href='#pcaLocalDimEst'><p> Local Dimension Estimation with PCA</p></a></li>
<li><a href='#pcaOtpmPointwiseDimEst'><p> Dimension Estimation With Optimally Topology Preserving Maps</p></a></li>
<li><a href='#Spherical'><p> Isotropic Distributions With or Without Noise</p></a></li>
<li><a href='#swissRoll'>
<p>Swiss roll with or without 3-sphere inside</p></a></li>
<li><a href='#tp'><p> Dimension Estimation via Translated Poisson Distributions</p></a></li>
<li><a href='#twinPeaks'><p> Twin Peaks</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Intrinsic Dimension Estimation</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2019-05-23</td>
</tr>
<tr>
<td>Author:</td>
<td>Kerstin Johnsson, Lund University</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kerstin Johnsson &lt;kerstin.johnsson@hotmail.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>yaImpute</td>
</tr>
<tr>
<td>Description:</td>
<td>A variety of methods for estimating intrinsic dimension of data sets (i.e the manifold or Hausdorff dimension of the support of the distribution that generated the data) as reviewed in Johnsson, K. (2016, ISBN:978-91-7623-921-6) and Johnsson, K., Soneson, C. and Fontes, M. (2015) &lt;<a href="https://doi.org/10.1109%2FTPAMI.2014.2343220">doi:10.1109/TPAMI.2014.2343220</a>&gt;. Furthermore, to evaluate the performance of these estimators, functions for generating data sets with given intrinsic dimensions are provided.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-06-07 10:00:08 UTC; johnsson</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-06-07 10:20:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='addNoise'>
Add Noise to Data Set
</h2><span id='topic+addNoise'></span>

<h3>Description</h3>

<p>Embeds the data in <code>n</code> dimensions and adds normal isotropic noise to 
the data set. Hence <code>n</code> has to be at least equal to the dimension (the
number of columns) of the data set, otherwise the function terminates with 
an error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addNoise(data, n = ncol(data), sd)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="addNoise_+3A_data">data</code></td>
<td>
<p> data set. Each row corresponds to a data point. </p>
</td></tr>
<tr><td><code id="addNoise_+3A_n">n</code></td>
<td>
<p> dimension of noise. </p>
</td></tr>
<tr><td><code id="addNoise_+3A_sd">sd</code></td>
<td>
<p> standard deviation of noise. The covariance matrix of the noise
is <code class="reqn">sd^2 \cdot I</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Matrix of same size as <code>data</code>.
</p>


<h3>Author(s)</h3>

<p>Kerstin Johnsson, Lund University
</p>


<h3>Examples</h3>

<pre><code class='language-R'>datap &lt;- hyperCubeEdges(100, 1, 2)
datap &lt;- addNoise(datap, 3, .05)
par(mfrow = c(1, 2))
plot(datap[, 1], datap[, 2])
plot(datap[, 1], datap[, 3])
</code></pre>

<hr>
<h2 id='asPointwiseEstimator'> Turn a local estimator into a pointwise estimator
</h2><span id='topic+asPointwiseEstimator'></span>

<h3>Description</h3>

<p>Returns a function that can be used as a pointwise estimator of intrinsic dimension that uses local data sets with a fixed number of data points.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>asPointwiseEstimator(estimator, neighborhood.size, indices=NULL, eps=0.0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="asPointwiseEstimator_+3A_estimator">estimator</code></td>
<td>
<p> A local intrinsic dimension estimator.</p>
</td></tr>
<tr><td><code id="asPointwiseEstimator_+3A_neighborhood.size">neighborhood.size</code></td>
<td>
<p> The number of neighbors used for each dimension estimate.</p>
</td></tr>
<tr><td><code id="asPointwiseEstimator_+3A_indices">indices</code></td>
<td>
<p> A vector with indices of the points in <code>data</code> (as sent to the estimator function) that should be used as center for neighborhoods. </p>
</td></tr>
<tr><td><code id="asPointwiseEstimator_+3A_eps">eps</code></td>
<td>
<p> If non-zero, the relative error in distance allowed when finding nearest neighbors. See Details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>ann</code> function of the package <code>yaImpute</code> is used for finding the <code>k</code> nearest neighbors. The <code>eps</code> parameter to <code>neighborhoods</code> is used in the <code>ann</code> function.
</p>


<h3>Value</h3>

<p>A function that can be used as a pointwise dimension estimator.
</p>


<h3>Author(s)</h3>

<p>Kerstin Johnsson, Lund University
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- swissRoll3Sph(300, 300) 
 # the first 300 data points are on the swiss roll (ID=2) , the last 300 on the 3-sphere (ID=3)
essPointwiseDimEst &lt;- asPointwiseEstimator(essLocalDimEst, neighborhood.size=10,
                                           indices = c(1:10, 301:310))
ess.pw.res &lt;- essPointwiseDimEst(data)
ess.pw.res$dim.est
</code></pre>

<hr>
<h2 id='cornerPlane'> Corner Plane </h2><span id='topic+cornerPlane'></span>

<h3>Description</h3>

<p>Generates a sample from a uniform distribution on a bent plane. Half of the
plane is in the xz-plane and half of the plane is bent over the x-axis, so
that the resulting surface has an edge along the x-axis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cornerPlane(Ns, theta = pi/4)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cornerPlane_+3A_ns">Ns</code></td>
<td>
<p> number of data points. </p>
</td></tr>
<tr><td><code id="cornerPlane_+3A_theta">theta</code></td>
<td>
<p> angle at the x-axis. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>Ns</code> x 3 matrix with columns x, y and z.
</p>


<h3>Author(s)</h3>

<p>Kerstin Johnsson, Lund University
</p>


<h3>Examples</h3>

<pre><code class='language-R'>datap &lt;- cornerPlane(400)
par(mfrow = c(1, 2))
plot(datap[,1], datap[,2])
plot(datap[,1], datap[,3])
</code></pre>

<hr>
<h2 id='cutHyperPlane'> Piece of Noisy Hyperplane </h2><span id='topic+cutHyperPlane'></span>

<h3>Description</h3>

<p>Generates <code>Ns</code> data points within the unit ball from a hyperplane 
through the origin with noise added. <code>n</code> has to be at least <code>d</code>,
otherwise the function terminates with an error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cutHyperPlane(Ns, d, n, sd)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cutHyperPlane_+3A_ns">Ns</code></td>
<td>
<p> number of data points. </p>
</td></tr>
<tr><td><code id="cutHyperPlane_+3A_d">d</code></td>
<td>
<p> dimension of hyperplane. </p>
</td></tr>
<tr><td><code id="cutHyperPlane_+3A_n">n</code></td>
<td>
<p> dimension of noise. </p>
</td></tr>
<tr><td><code id="cutHyperPlane_+3A_sd">sd</code></td>
<td>
<p> standard deviation of noise. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data set is generated the following way: First data points are sampled
uniformly in a <code>d</code>-ball. After this, <code>(n-d)</code>-dimensional orthogonal noise with
standard deviation <code>sd</code> in each direction is added. No noise is added in the
directions parallel to the hyperplane since on an infinite plane adding 
isotropic noise to a uniform distribution does not change the
distribution. Finally all data points within distance 1 from the origin are
considered as candidates for the data set that will be returned, out of the 
candidates <code>Ns</code> data points are chosen randomly to be returned.
If there are less than <code>Ns</code> candidates more candidates will be generated
in the same way.
</p>
<p>The data generated by this function can be used to evaluate how much
local dimension estimators are affected by noise.
</p>


<h3>Value</h3>

<p>A <code>Ns</code> x <code>n</code> matrix.
</p>


<h3>Warning</h3>

 
<p>If <code>sd</code> is high, <code>cutHyperPlane</code> will be slow and might not even
be able to return a data set. If so, it will return <code>NULL</code>.
</p>


<h3>Author(s)</h3>

<p>Kerstin Johnsson, Lund University
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cutHyperSphere">cutHyperSphere</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>datap &lt;- cutHyperPlane(100, 2, 3, 0.01)
par(mfrow = c(1, 2))
plot(datap[, 1], datap[, 2])
plot(datap[, 1], datap[, 3])

</code></pre>

<hr>
<h2 id='cutHyperSphere'> Piece of Noisy Hypersphere </h2><span id='topic+cutHyperSphere'></span>

<h3>Description</h3>

<p>Generates <code>Ns</code> data points cut out from a noisy hypersphere. <code>n</code>
has to be at least <code>d+1</code>, otherwise the function terminates with an
error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cutHyperSphere(Ns, rat, d, n, sd)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cutHyperSphere_+3A_ns">Ns</code></td>
<td>
<p> number of data points. </p>
</td></tr>
<tr><td><code id="cutHyperSphere_+3A_rat">rat</code></td>
<td>
<p> ratio between cut-off radius and radius of sphere. </p>
</td></tr>
<tr><td><code id="cutHyperSphere_+3A_d">d</code></td>
<td>
<p> (intrinsic) dimension of hypersphere. </p>
</td></tr>
<tr><td><code id="cutHyperSphere_+3A_n">n</code></td>
<td>
<p> dimension of noise. </p>
</td></tr>
<tr><td><code id="cutHyperSphere_+3A_sd">sd</code></td>
<td>
<p> standard deviation of noise. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The returned data are within distance <code>rat</code> the point
<code class="reqn">1/\sqrt{d+1}(1 ... 1)</code> and are obtained from a unit distribution on the
<code>d</code>-sphere overlaid with <code>n</code>-dimensional normal noise.
</p>
<p>The data generated by this function can be used to evaluate the performance
of local dimension estimators.
</p>


<h3>Value</h3>

<p>A <code>Ns</code> by <code>n</code> matrix.
</p>


<h3>Warning</h3>

 
<p>If <code>sd</code> is high, <code>cutHyperSphere</code> will be slow and might not even
be able to return a data set. If so, it will return <code>NULL</code>.
</p>


<h3>Author(s)</h3>

<p>Kerstin Johnsson, Lund University
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cutHyperPlane">cutHyperPlane</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>datap &lt;- cutHyperSphere(100, rat = .5, 1, 3, 0.01)
par(mfrow = c(1, 2))
plot(datap[, 1], datap[, 2])
plot(datap[, 1], datap[, 3])

datap &lt;- cutHyperSphere(100, rat = 2, 1, 3, 0.11)
par(mfrow = c(1, 2))
plot(datap[, 1], datap[, 2])
plot(datap[, 1], datap[, 3])
</code></pre>

<hr>
<h2 id='dancoDimEst'>
Dimension Estimation With the DANCo and MIND Methods
</h2><span id='topic+dancoDimEst'></span>

<h3>Description</h3>

<p>Intrinsic dimension estimation with the DANCo (Ceruti et al. 2012),
MIND_MLi and MIND_MLk (Rozza et al. 2012) methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dancoDimEst(data, k, D, ver = "DANCo", calibration.data = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dancoDimEst_+3A_data">data</code></td>
<td>
<p> a data set for which the intrinsic dimension is estimated. </p>
</td></tr>
<tr><td><code id="dancoDimEst_+3A_k">k</code></td>
<td>
<p> neighborhood parameter. </p>
</td></tr>
<tr><td><code id="dancoDimEst_+3A_d">D</code></td>
<td>
<p> maximal dimension. </p>
</td></tr>
<tr><td><code id="dancoDimEst_+3A_ver">ver</code></td>
<td>
<p> possible values: 'DANCo', 'MIND_MLi', 'MIND_MLk'. </p>
</td></tr>
<tr><td><code id="dancoDimEst_+3A_calibration.data">calibration.data</code></td>
<td>
<p> precomputed calibration data. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>cal = NULL</code> or the <code>cal$maxdim &lt; D</code> new calibration data will be computed as needed.
</p>


<h3>Value</h3>

<p>A <code>DimEst</code> object with slots:
</p>
<table>
<tr><td><code>dim.est</code></td>
<td>
<p>the intrinsic dimension estimate.</p>
</td></tr>
<tr><td><code>kl.divergence</code></td>
<td>
<p>the KL divergence between data and reference data for the estimated dimension (if ver == 'DANCo').</p>
</td></tr>
<tr><td><code>calibration.data</code></td>
<td>
<p>calibration data that can be reused when applying DANCo to data sets
of the same size with the same neighborhood parameter k.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kerstin Johnsson, Lund University
</p>


<h3>References</h3>

<p>Ceruti, C. et al. (2012) DANCo: Dimensionality from Angle and Norm Concentration.
<em>arXiv preprint</em> 1206.3881.
</p>
<p>Rozza, A et al. (2012) Novel high intrinsic dimensionality estimators. <em>Machine learning</em>
<b>89</b>, 37-65.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- hyperBall(50, 10)
res &lt;- dancoDimEst(data, 8, 20)
print(res)

## Reusing calibration data
data2 &lt;- hyperBall(50, 5)
dancoDimEst(data2, 8, 20, calibration.data=res$calibration.data)
</code></pre>

<hr>
<h2 id='essLocalDimEst'> Expected Simplex Skewness Local Dimension Estimation
</h2><span id='topic+essLocalDimEst'></span>

<h3>Description</h3>

<p>Local intrinsic dimension estimation with the ESS method
</p>


<h3>Usage</h3>

<pre><code class='language-R'>essLocalDimEst(data, ver, d = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="essLocalDimEst_+3A_data">data</code></td>
<td>
<p> Local data set for which dimension should be estimated.</p>
</td></tr>
<tr><td><code id="essLocalDimEst_+3A_ver">ver</code></td>
<td>
<p> Possible values: <code>'a'</code> and <code>'b'</code>. See Johnsson et al. (2015).</p>
</td></tr>
<tr><td><code id="essLocalDimEst_+3A_d">d</code></td>
<td>
<p> For <code>ver = 'a'</code>, any value of <code>d</code> is possible, for <code>ver = 'b'</code>, only <code>d = 1</code> is supported. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The ESS method assumes that the data is local, i.e. that it is a neighborhood taken from a larger data set,
such that the curvature and the noise within the neighborhood is relatively small. In the ideal case
(no noise, no curvature) this is equivalent to the data being uniformly distributed over a hyper ball.
</p>


<h3>Value</h3>

<p>A <code>DimEst</code> object with two slots:
</p>
<table>
<tr><td><code>dim.est</code></td>
<td>
<p>The interpolated dimension estimate.</p>
</td></tr>
<tr><td><code>ess</code></td>
<td>
<p>The ESS value produced by the algorithm.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kerstin Johnsson, Lund University
</p>


<h3>References</h3>

<p>Johnsson, K., Soneson, C., &amp; Fontes, M. (2015). Low Bias Local Intrinsic Dimension Estimation from Expected Simplex Skewness. IEEE Trans. Pattern Anal. Mach. Intell., 37(1), 196-202.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- hyperBall(100, 4, 15, .05)
essLocalDimEst(data, ver = 'a', d = 1)
</code></pre>

<hr>
<h2 id='essReference'> ESS Reference Values
</h2><span id='topic+essReference'></span>

<h3>Description</h3>

<p>Reference values for the ESS dimension estimation method
</p>


<h3>Usage</h3>

<pre><code class='language-R'>essReference(ver, d, maxdim, mindim=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="essReference_+3A_ver">ver</code></td>
<td>
<p> Possible values: <code>'a'</code> and <code>'b'</code>. See Johnsson et al. (2015).</p>
</td></tr>
<tr><td><code id="essReference_+3A_d">d</code></td>
<td>
<p> For <code>ver = 'a'</code>, any value of <code>d</code> is possible, for <code>ver = 'b'</code>, only <code>d = 1</code> is supported. </p>
</td></tr>
<tr><td><code id="essReference_+3A_maxdim">maxdim</code></td>
<td>
<p> The largest dimension for which reference values should be computed. </p>
</td></tr>
<tr><td><code id="essReference_+3A_mindim">mindim</code></td>
<td>
<p> The smallest dimension for which reference values should be computed. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The ESS reference values are used by the ESS algorithm (<a href="#topic+essLocalDimEst">essLocalDimEst</a>) to compute the final dimension estimate.
</p>


<h3>Value</h3>

<p>A vector of length maxdim-(mindim-1), where each slot represents the reference value.
</p>


<h3>Author(s)</h3>

<p>Kerstin Johnsson, Lund University
</p>


<h3>References</h3>

<p>Johnsson, K., Soneson, C., &amp; Fontes, M. (2015). Low Bias Local Intrinsic Dimension Estimation from Expected Simplex Skewness. IEEE Trans. Pattern Anal. Mach. Intell., 37(1), 196-202.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>essReference('a', 3, maxdim=500)
essReference('b', 1, maxdim=30, mindim=3)
</code></pre>

<hr>
<h2 id='hyperCube'> Hypercube </h2><span id='topic+hyperCube'></span><span id='topic+hyperCubeFaces'></span><span id='topic+hyperCubeEdges'></span>

<h3>Description</h3>

<p>Generates a sample from a uniform distribution on a hypercube, the faces
of a hypercube or the &ldquo;edges&rdquo; of a hyper cube.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hyperCube(Ns, n, side = 1)
hyperCubeFaces(Ns, n)
hyperCubeEdges(Ns, d, n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hyperCube_+3A_ns">Ns</code></td>
<td>
<p> number of data points. </p>
</td></tr>
<tr><td><code id="hyperCube_+3A_d">d</code></td>
<td>
<p> dimension of edges. </p>
</td></tr>
<tr><td><code id="hyperCube_+3A_n">n</code></td>
<td>
<p> dimension of the hypercube. </p>
</td></tr>
<tr><td><code id="hyperCube_+3A_side">side</code></td>
<td>
<p> the length of the side of the hyper cube. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The hypercube is <code class="reqn">[0,1]^n</code>. The edges of dimension <code>d</code> of the 
hypercube are the <code>d</code>-dimensional boundaries of the hypercube. The 
hypercube faces are the hyper cube edges of dimension <code>n-1</code>. 
</p>


<h3>Value</h3>

<p>A <code>Ns</code> by <code>n</code> matrix.
</p>


<h3>Author(s)</h3>

<p>Kerstin Johnsson, Lund University.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>datap &lt;- hyperCubeEdges(200, 1, 3)
par(mfrow = c(1, 3))
plot(datap[, 1], datap[, 2])
plot(datap[, 1], datap[, 3])
plot(datap[, 2], datap[, 3])

</code></pre>

<hr>
<h2 id='ide'> Intrinsic Dimension Estimation
</h2><span id='topic+localIntrinsicDimension'></span><span id='topic+globalIntrinsicDimension'></span><span id='topic+pointwiseIntrinsicDimension'></span>

<h3>Description</h3>

<p>Intrinsic dimension estimation with method given as parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>localIntrinsicDimension(.data, .method, ...)
globalIntrinsicDimension(.data, .method, ...)
pointwiseIntrinsicDimension(.data, .method, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ide_+3A_.data">.data</code></td>
<td>
<p> Data set for which dimension should be estimated.</p>
</td></tr>
<tr><td><code id="ide_+3A_.method">.method</code></td>
<td>
<p> For <code>local.dimension.estimation</code>, one of
<code>'essLocalDimEst'</code>, 
<code>'dancoDimEst'</code>, <code>'pcaLocalDimEst'</code>,
<code>'maxLikLocalDimEst'</code>, 
<code>'knnDimEst'</code>. For <code>global.dimension.estimation</code>, one of
<code>'dancoDimEst'</code>, <code>'maxLikGlobalDimEst'</code>, <code>'knnDimEst'</code>.
For <code>pointwise.dimension.estimation</code>,
<code>'pcaOtpmLocalDimEst'</code> or <code>'maxLikPointwiseDimEst'</code>.</p>
</td></tr>
<tr><td><code id="ide_+3A_...">...</code></td>
<td>
<p> arguments passed to intrinsic dimension estimator. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the <code>localIntrinsicDimension</code> function, <code>.data</code> should be a
local data set, i.e. a piece of a data set that is well approximated by a
hyperplane (meaning that the curvature should be low in the local data set).
</p>
<p>The function <code>pointwiseIntrinsicDimension</code> estimates local dimension
around each data point in the data set.
</p>


<h3>Value</h3>

<p>For <code>localIntrinsicDimension</code> and <code>globalIntrinsicDimension</code>, a <code>DimEst</code> object with the slot <code>dim.est</code> containing the dimension estimate and possibly additional slots containing additional information about the estimation process.
For <code>pointwiseIntrinsicDimension</code>, a <code>DimEstPointwise</code> object, inheriting <code>data.frame</code>, with one slot <code>dim.est</code> containing the dimension estimates and possibly additional slots containing additional information about the estimation process.
</p>


<h3>Author(s)</h3>

<p>Kerstin Johnsson, Lund University
</p>


<h3>References</h3>

<p>Johnsson, K (2016). Structures in high-dimensional data: Intrinsic dimension and cluster analysis. PhD thesis. Lund University.
</p>
<p>Johnsson, K., Soneson, C. and Fontes, M. (2015). Low Bias Local Intrinsic 
Dimension Estimation from Expected Simplex Skewness. <em>IEEE Trans. Pattern Anal. 
Mach. Intell.</em>, <b>37</b>(1), 196-202.
</p>
<p>Ceruti, C. et al. (2012). DANCo: Dimensionality from Angle and Norm Concentration.
<em>arXiv preprint</em> 1206.3881.
</p>
<p>Rozza, A et al. (2012). Novel high intrinsic dimensionality estimators. <em>Machine learning</em>
<b>89</b>, 37-65.
</p>
<p>Fukunaga, K. and Olsen, D. R. (1971). An algorithm for finding intrinsic dimensionality
of data. <em>IEEE Trans. Comput.</em>, <b>c-20</b>(2):176-183.
</p>
<p>Fan, M. et al. (2010). Intrinsic dimension estimation of data by principal component 
analysis. <em>arXiv preprint</em> 1002.2050.
</p>
<p>Bruske, J. and Sommer, G. (1998) Intrinsic dimensionality estimation with
optimally topology preserving maps. <em>IEEE Trans. on Pattern Anal. and Mach.
Intell.</em>, <b>20</b>(5), 572-575.
</p>
<p>Haro, G., Randall, G. and Sapiro, G. (2008) Translated Poisson Mixture Model
for Stratification Learning. <em>Int. J. Comput. Vis.</em>, <b>80</b>, 358-374.
</p>
<p>Hill, B. M. (1975) A simple general approach to inference about the tail of a distribution. <em>Ann. Stat.</em>, <b>3</b>(5) 1163-1174.
</p>
<p>Levina, E. and Bickel., P. J. (2005) Maximum likelihood estimation of intrinsic dimension. <em>Advances in Neural Information Processing Systems 17</em>, 777-784. MIT Press.
</p>
<p>Carter, K.M., Raich, R. and Hero, A.O. (2010) On local intrinsic dimension 
estimation and its applications. <em>IEEE Trans. on Sig. Proc.</em>, 
<b>58</b>(2), 650-663.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+essLocalDimEst">essLocalDimEst</a></code>, <code><a href="#topic+dancoDimEst">dancoDimEst</a></code>, <code><a href="#topic+pcaLocalDimEst">pcaLocalDimEst</a></code>, <code><a href="#topic+knnDimEst">knnDimEst</a></code>
<code><a href="#topic+pcaOtpmPointwiseDimEst">pcaOtpmPointwiseDimEst</a></code>, <code><a href="#topic+maxLikGlobalDimEst">maxLikGlobalDimEst</a></code>, <code><a href="#topic+maxLikLocalDimEst">maxLikLocalDimEst</a></code>,
<code><a href="#topic+maxLikPointwiseDimEst">maxLikPointwiseDimEst</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- hyperBall(100, 4, 15, .05)
localIntrinsicDimension(data, .method='essLocalDimEst', ver = 'a', d = 1)
globalIntrinsicDimension(data, 'dancoDimEst', k = 8, D = 20)
pointwiseIntrinsicDimension(data, .method='maxLikPointwiseDimEst', k = 8, dnoise = NULL)
</code></pre>

<hr>
<h2 id='intrinsicDimension-package'>
Intrinsic Dimension Estimation and Data on Manifolds
</h2><span id='topic+intrinsicDimension'></span>

<h3>Description</h3>

<p>The intrinsic dimension of a data set is a measure of its complexity. In technical terms it typically means the manifold or Hausdorff (fractal) dimension of the support of the probability distribution generating the data. This package contains functions for estimating intrinsic dimension and generating ground truth data sets with known intrinsic dimension.
</p>


<h3>Details</h3>

<p>Data sets that can be accurately described with a few parameters have low intrinsic dimension. It is expected that the performance of many machine learning algorithms is dependent on the intrinsic dimension of the data. Is has also been proposed to use estimates of intrinsic dimension for applications such as network anomaly detection and image analysis.
</p>
<p>This package contains implementations of a variety of approaches for intrinsic dimension estimation: modeling distances by for example Maximum Likelihood, approximating hyperplanes using Principal Component Analysis (PCA) and modeling angular information and concentration of measure (ESS and DANCo methods). Ground truth data, i.e. data with known intrinsic dimension, can be generated with a number of functions modeling manifolds. The manifold dimension is the intrinsic dimension.
</p>
<p>The package distinguishes between local, global and pointwise estimators of intrinsic dimension. Local estimators estimate dimension of a _local data set_, for example a neighborhood from a larger data set. For this estimate to be accurate the noise and the curvature of the data has to be small relative to the neighborhood diameter. A global estimator takes the entire data set and returns one estimate of intrinsic dimension. Global estimators has the potential to handle higher noise and curvature levels than local estimators, but require that the entire data set has the same intrinsic dimension. Pointwise estimators are essentially local estimators applied neighborhoods around each point in the data set, but sometimes information beyond the neighborhood is used, as in PCA with Optimally Topology Preserving Maps. Any local estimator can be converted into a pointwise estimator.
</p>
<p>Functions for estimating intrinsic dimension: <code><a href="#topic+localIntrinsicDimension">localIntrinsicDimension</a></code>, <code><a href="#topic+globalIntrinsicDimension">globalIntrinsicDimension</a></code>, <code><a href="#topic+pointwiseIntrinsicDimension">pointwiseIntrinsicDimension</a></code>, <code><a href="#topic+essLocalDimEst">essLocalDimEst</a></code>, <code><a href="#topic+dancoDimEst">dancoDimEst</a></code>, <code><a href="#topic+pcaLocalDimEst">pcaLocalDimEst</a></code>, <code><a href="#topic+pcaOtpmPointwiseDimEst">pcaOtpmPointwiseDimEst</a></code>,
<code><a href="#topic+maxLikGlobalDimEst">maxLikGlobalDimEst</a></code>, <code><a href="#topic+maxLikLocalDimEst">maxLikLocalDimEst</a></code>, <code><a href="#topic+maxLikPointwiseDimEst">maxLikPointwiseDimEst</a></code>, <code><a href="#topic+knnDimEst">knnDimEst</a></code>.
</p>
<p>Functions for generating data points from (usually uniform) distributions on 
manifolds (possibly with noise): <code><a href="#topic+hyperBall">hyperBall</a></code>, <code><a href="#topic+hyperSphere">hyperSphere</a></code>, <code><a href="#topic+hyperCube">hyperCube</a></code>,
<code><a href="#topic+isotropicNormal">isotropicNormal</a></code>, <code><a href="#topic+hyperCubeFaces">hyperCubeFaces</a></code>, <code><a href="#topic+hyperCubeEdges">hyperCubeEdges</a></code>, <code><a href="#topic+cutHyperPlane">cutHyperPlane</a></code>, <code><a href="#topic+cutHyperSphere">cutHyperSphere</a></code>, <code><a href="#topic+oblongNormal">oblongNormal</a></code>, <code><a href="#topic+swissRoll">swissRoll</a></code>, <code><a href="#topic+swissRoll3Sph">swissRoll3Sph</a></code>, <code><a href="#topic+twinPeaks">twinPeaks</a></code>, <code><a href="#topic+hyperTwinPeaks">hyperTwinPeaks</a></code>,
<code><a href="#topic+cornerPlane">cornerPlane</a></code>, <code><a href="#topic+mHeinManifold">mHeinManifold</a></code>, <code><a href="#topic+m14Manifold">m14Manifold</a></code>, <code><a href="#topic+m15Manifold">m15Manifold</a></code>.
</p>
<p>Functions for applying local estimators to non-local data: <code><a href="#topic+asPointwiseEstimator">asPointwiseEstimator</a></code>, <code><a href="#topic+neighborhoods">neighborhoods</a></code>
</p>


<h3>Author(s)</h3>

<p>Kerstin Johnsson, Lund University
</p>
<p>Maintainer: Kerstin Johnsson &lt;kerstin.johnsson@hotmail.com&gt;
</p>


<h3>References</h3>

<p>Johnsson, K (2016). Structures in high-dimensional data: Intrinsic dimension and cluster analysis. PhD thesis. Lund University. <a href="http://portal.research.lu.se/portal/sv/publications/structures-in-highdimensional-data-intrinsic-dimension-and-cluster-analysis(8404f72e-e760-436d-ad7f-1be15af4b3d1).html">http://portal.research.lu.se/portal/sv/publications/structures-in-highdimensional-data-intrinsic-dimension-and-cluster-analysis(8404f72e-e760-436d-ad7f-1be15af4b3d1).html</a>
</p>

<hr>
<h2 id='knnDimEst'> Dimension Estimation from kNN Distances </h2><span id='topic+knnDimEst'></span>

<h3>Description</h3>

<p>Estimates the intrinsic dimension of a data set using weighted average kNN
distances.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>knnDimEst(data, k, ps, M, gamma = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="knnDimEst_+3A_data">data</code></td>
<td>
<p>data set with each row describing a data point.</p>
</td></tr>
<tr><td><code id="knnDimEst_+3A_k">k</code></td>
<td>
<p>number of distances to neighbors used at a time.</p>
</td></tr>
<tr><td><code id="knnDimEst_+3A_ps">ps</code></td>
<td>
<p>vector with sample sizes; each sample size has to be larger than
k and smaller than <code>nrow(data)</code>.</p>
</td></tr>
<tr><td><code id="knnDimEst_+3A_m">M</code></td>
<td>
<p>number of bootstrap samples for each sample size. </p>
</td></tr>
<tr><td><code id="knnDimEst_+3A_gamma">gamma</code></td>
<td>
<p>weighting constant.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a somewhat simplified version of the kNN dimension estimation method
described by Carter et al. (2010), the difference being that block 
bootstrapping is not used.
</p>


<h3>Value</h3>

<p> A <code>DimEst</code> object with slots:
</p>
<table>
<tr><td><code>dim.est</code></td>
<td>
<p>the intrinsic dimension estimate (integer).</p>
</td></tr>
<tr><td><code>residual</code></td>
<td>
<p>the residual, see Carter et al. (2010).</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>Kerstin Johnsson, Lund University.
</p>


<h3>References</h3>

<p>Carter, K.M., Raich, R. and Hero, A.O. (2010) On local intrinsic dimension 
estimation and its applications. <em>IEEE Trans. on Sig. Proc.</em>, 
<b>58</b>(2), 650-663.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>N &lt;- 50
data &lt;- hyperBall(N, 5)

k &lt;- 2
ps &lt;- seq(max(k + 1, round(N/2)), N - 1, by = 3)
knnDimEst(data, k, ps, M = 10, gamma = 2)

</code></pre>

<hr>
<h2 id='M_rozza'> Manifolds from Rozza et al. (2012) </h2><span id='topic+m14Manifold'></span><span id='topic+m15Manifold'></span>

<h3>Description</h3>

<p>Generates data sets from Rozza et al. (2012). M14 is an 18-dimensional manifold with intrinsic dimension 72.
M14 is a 24-dimensional manifold with extrinsic dimension 96. Note that M14 and M15 are not uniformly sampled.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>m14Manifold(Ns)
m15Manifold(Ns)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="M_rozza_+3A_ns">Ns</code></td>
<td>
<p> number of data points. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>72</code>-dimensional or <code>96</code>-dimensional data set respectively.
</p>


<h3>Author(s)</h3>

<p>Kerstin Johnsson, Lund University.
</p>


<h3>References</h3>

<p>Rozza, A. et al. (2012) Novel high intrinsic dimensionality estimators.
<em>Machine Learning</em>, 89:37-65.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>datap &lt;- m14Manifold(800)
par(mfrow = c(1, 3))
plot(datap[,1], datap[,3])
plot(datap[,2], datap[,3])
plot(datap[,1], datap[,2])
datap &lt;- m15Manifold(800)
par(mfrow = c(1, 3))
plot(datap[,1], datap[,3])
plot(datap[,2], datap[,3])
plot(datap[,1], datap[,2])
</code></pre>

<hr>
<h2 id='mHeinManifold'> 12-dimensional manifold from Hein and Audibert (2005) </h2><span id='topic+mHeinManifold'></span>

<h3>Description</h3>

<p>Generates a 12-dimensional manifold with extrinsic dimension 72
(not uniformly sampled).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mHeinManifold(Ns)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mHeinManifold_+3A_ns">Ns</code></td>
<td>
<p> number of data points. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>72</code>-dimensional data set.
</p>


<h3>Author(s)</h3>

<p>Kerstin Johnsson, Lund University.
</p>


<h3>References</h3>

<p>Hein, M. and Audibert, J-Y. (2005) Intrinsic Dimensionality Estimation of
Submanifolds in R^d. <em>Proceedings of ICML</em>, 289-296.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>datap &lt;- mHeinManifold(800)
par(mfrow = c(1, 3))
plot(datap[,1], datap[,3])
plot(datap[,2], datap[,3])
plot(datap[,1], datap[,2])
</code></pre>

<hr>
<h2 id='neighborhoods'> Obtaining neighborhoods (local data) from a data set
</h2><span id='topic+neighborhoods'></span>

<h3>Description</h3>

<p>Get a list of neighborhoods, each containing the <code>k</code> nearest neighbors (not including itself) to a point in the data set. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neighborhoods(data, k, indices, eps=0.0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="neighborhoods_+3A_data">data</code></td>
<td>
<p> A data set.</p>
</td></tr>
<tr><td><code id="neighborhoods_+3A_k">k</code></td>
<td>
<p> The number of neighbors in each neighborhood.</p>
</td></tr>
<tr><td><code id="neighborhoods_+3A_indices">indices</code></td>
<td>
<p> A vector with indices of the points in <code>data</code> that should be used as center for neighborhoods. </p>
</td></tr>
<tr><td><code id="neighborhoods_+3A_eps">eps</code></td>
<td>
<p> If non-zero, the relative error in distance allowed when finding nearest neighbors. See Details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>ann</code> function of the package <code>yaImpute</code> is used for finding the <code>k</code> nearest neighbors. The <code>eps</code> parameter to <code>neighborhoods</code> is used in the <code>ann</code> function.
</p>


<h3>Value</h3>

<p>A list of neighborhoods where each item corresponds to one index in <code>indices</code> and each item contains a data set with <code>k</code> data points.
</p>


<h3>Author(s)</h3>

<p>Kerstin Johnsson, Lund University
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- swissRoll3Sph(300, 300)
neighborhoods(data, 10, 1:10)
</code></pre>

<hr>
<h2 id='Noisefun'> Transition Functions Describing Noise </h2><span id='topic+dnoiseNcChi'></span><span id='topic+dnoiseGaussH'></span>

<h3>Description</h3>

<p>Transition functions <code class="reqn">f(s|r)</code> describing the shift in lengths of vectors
when Gaussian noise is added. Given a length <code class="reqn">r</code>, <code class="reqn">f(s|r)</code> is the
probability density for the length after noise is added to one endpoint.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dnoiseNcChi(r, s, sigma, k)
dnoiseGaussH(r, s, sigma, k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Noisefun_+3A_r">r</code></td>
<td>
<p> length or vector of lengths of original vector. </p>
</td></tr>
<tr><td><code id="Noisefun_+3A_s">s</code></td>
<td>
<p> length or vector of lengths of perturbed vector. </p>
</td></tr>
<tr><td><code id="Noisefun_+3A_sigma">sigma</code></td>
<td>
<p> noise standard deviation. </p>
</td></tr>
<tr><td><code id="Noisefun_+3A_k">k</code></td>
<td>
<p> dimension of noise. </p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>dnoiseNcChi</code> is the true transition function density when the noise
is Gaussian, the other transition functions are approximations of this.
<code>dnoiseGaussH</code> is the Gaussian approximation used in Haro et al.
</p>
<p>If Gaussian noise is added to both endpoints of the vector, <code>sigma</code>
should be replaced by 
<code>sqrt(2)*sigma</code>.
</p>


<h3>Value</h3>

<p>Vector of probability densities.
</p>


<h3>Note</h3>

<p>Only <code>r</code> or <code>s</code> can be a vector.
</p>


<h3>Author(s)</h3>

<p>Kerstin Johnsson, Lund University
</p>


<h3>References</h3>

<p>Haro, G., Randall, G. and Sapiro, G. (2008) Translated Poisson Mixture Model
for Stratification Learning. <em>Int. J. Comput. Vis.</em>, <b>80</b>, 358-374.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+maxLikPointwiseDimEst">maxLikPointwiseDimEst</a></code>, <code><a href="#topic+maxLikGlobalDimEst">maxLikGlobalDimEst</a></code>, <code><a href="#topic+maxLikLocalDimEst">maxLikLocalDimEst</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># High SNR, high-dim noise
sigma &lt;- 0.05
x &lt;- seq(0, 1.5, length.out = 200)
y &lt;- dnoiseNcChi(x, s = .5, sigma, k = 20)
plot(x, y, type = 'l', main = 'Noise dim = 20')
y2 &lt;- dnoiseGaussH(x, s = .5, sigma, k = 20)
lines(x, y2, lty = 2)

# Low SNR
par(mfrow = c(2, 3))
sigma &lt;- 0.2
x &lt;- seq(0, 1.5, length.out = 200)
y &lt;- dnoiseNcChi(x, s = .5, sigma, k = 4)
plot(x, y, type = 'l', main = 'Noise approximations')
y2 &lt;- dnoiseGaussH(x, s = .5, sigma, k)
lines(x, y2, lty = 2)

# High SNR, low-dim noise
sigma &lt;- 0.05
x &lt;- seq(0, 1.5, length.out = 200)
y &lt;- dnoiseNcChi(x, s = .5, sigma, k = 4)
plot(x, y, type = 'l', main = 'Noise dim = 4')
y2 &lt;- dnoiseGaussH(x, s = .5, sigma, k)
lines(x, y2, lty = 2)

</code></pre>

<hr>
<h2 id='oblongNormal'> Oblong Normal Distribution </h2><span id='topic+oblongNormal'></span>

<h3>Description</h3>

<p>Generates a sample from a certain anisotropic normal distribution centered
around the origin.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> oblongNormal(Ns, n) </code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="oblongNormal_+3A_ns">Ns</code></td>
<td>
<p> number of data points. </p>
</td></tr>
<tr><td><code id="oblongNormal_+3A_n">n</code></td>
<td>
<p> dimension of the distribution (and the data points). </p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the first half of the dimensions (rounded down if <code>n</code> is odd)
the standard deviation is 1 and in the rest the standard deviation is 0.25 . 
</p>


<h3>Value</h3>

<p>A <code>Ns</code> by <code>n</code> matrix.
</p>


<h3>Author(s)</h3>

<p>Kerstin Johnsson, Lund University
</p>


<h3>Examples</h3>

<pre><code class='language-R'>datap &lt;- oblongNormal(100, 10)
par(mfrow = c(1, 2))
plot(datap[, 1], datap[, 2])
plot(datap[, 1], datap[, 6])
</code></pre>

<hr>
<h2 id='pcaLocalDimEst'> Local Dimension Estimation with PCA </h2><span id='topic+pcaLocalDimEst'></span>

<h3>Description</h3>

<p>Estimates local manifold dimension using the largest singular values of the covariance matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pcaLocalDimEst(data, ver, alphaFO = .05, alphaFan = 10, betaFan = .8, PFan = .95,
     ngap = 5, maxdim = min(dim(data)), verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pcaLocalDimEst_+3A_data">data</code></td>
<td>
<p> a local data set for which dimension should be estimated.</p>
</td></tr>
<tr><td><code id="pcaLocalDimEst_+3A_ver">ver</code></td>
<td>
<p> possible values: <code>'FO'</code>, <code>'fan'</code>, <code>'maxgap'</code>, <code>'cal'</code>. <code>'cal'</code> is often very slow. </p>
</td></tr>
<tr><td><code id="pcaLocalDimEst_+3A_alphafo">alphaFO</code></td>
<td>
<p> only for <code>ver = 'FO'</code>. An eigenvalue is considered significant if it is larger than alpha times the largest eigenvalue. </p>
</td></tr>
<tr><td><code id="pcaLocalDimEst_+3A_alphafan">alphaFan</code></td>
<td>
<p> only for <code>ver = 'Fan'</code>. The alpha parameter (large gap threshold).</p>
</td></tr>
<tr><td><code id="pcaLocalDimEst_+3A_betafan">betaFan</code></td>
<td>
<p> only for <code>ver = 'Fan'</code>. The beta parameter (total covariance threshold).</p>
</td></tr>
<tr><td><code id="pcaLocalDimEst_+3A_pfan">PFan</code></td>
<td>
<p> only for <code>ver = 'Fan'</code>. Total covariance in non-noise.</p>
</td></tr>
<tr><td><code id="pcaLocalDimEst_+3A_ngap">ngap</code></td>
<td>
<p> only for <code>ver = 'cal'</code>. How many of the largest gaps that should be considered. </p>
</td></tr>
<tr><td><code id="pcaLocalDimEst_+3A_maxdim">maxdim</code></td>
<td>
<p> only for <code>ver = 'cal'</code>. The maximal manifold dimension of the data. </p>
</td></tr>
<tr><td><code id="pcaLocalDimEst_+3A_verbose">verbose</code></td>
<td>
<p> should information about the process be printed out? </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Version <code>'FO'</code> is the method by Fukunaga-Olsen, version <code>'fan'</code> is the method by Fan et al..
</p>
<p>Version <code>'maxgap'</code> returns the position of the largest relative gap in the sequence of singular values.
</p>
<p>Version <code>'cal'</code> considers the positions of the <code>ngap</code> largest relative gaps in the
sequence of singular values and generates calibration data to determine which one of them is most likely.
</p>
<p>All versions assume that the data is local, i.e. that it is a neighborhood taken from a larger data set,
such that the curvature and the noise within the neighborhood is relatively small. In the ideal case
(no noise, no curvature) this is equivalent to the data being uniformly distributed over a hyper ball.
</p>


<h3>Value</h3>

<p>A <code>DimEst</code> object with slots:
</p>
<table>
<tr><td><code>dim.est</code></td>
<td>
<p>the dimension estimate</p>
</td></tr>
<tr><td><code>gap.size</code></td>
<td>
<p>if <code>ver</code> is not <code>'cal'</code>, the size of the gap in singular values corresponding to the estimated dimension</p>
</td></tr>
<tr><td><code>likelihood</code></td>
<td>
<p>if <code>ver</code> is <code>cal</code>, the likelihood of the estimated dimension.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kerstin Johnsson, Lund University
</p>


<h3>References</h3>

<p>Fukunaga, K. and Olsen, D. R. (1971). An algorithm for finding intrinsic dimensionality
of data. <em>IEEE Trans. Comput.</em>, <b>c-20</b>(2):176-183.
</p>
<p>Fan, M. et al. (2010). Intrinsic dimension estimation of data by principal component 
analysis. <em>arXiv preprint</em> 1002.2050.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pcaOtpmPointwiseDimEst">pcaOtpmPointwiseDimEst</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- cutHyperPlane(100, 4, 10, .05)
pcaLocalDimEst(data, 'fan')
pcaLocalDimEst(data, 'FO')
pcaLocalDimEst(data, 'maxgap')

</code></pre>

<hr>
<h2 id='pcaOtpmPointwiseDimEst'> Dimension Estimation With Optimally Topology Preserving Maps </h2><span id='topic+pcaOtpmPointwiseDimEst'></span>

<h3>Description</h3>

<p>Intrinsic dimension estimation with the method proposed in Bruske and Sommer
(1998). A graph called optimally topology preserving map (OTPM) is constructed
and on this local PCA is made with the Fukunaga-Olsen criterion to determine
which eigenvalues that are significant.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> pcaOtpmPointwiseDimEst(data, N, alpha = .05) </code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pcaOtpmPointwiseDimEst_+3A_data">data</code></td>
<td>
<p> a data set for which dimension should be estimated. </p>
</td></tr>
<tr><td><code id="pcaOtpmPointwiseDimEst_+3A_n">N</code></td>
<td>
<p> the number of the nodes in the OTPM.</p>
</td></tr>
<tr><td><code id="pcaOtpmPointwiseDimEst_+3A_alpha">alpha</code></td>
<td>
<p> the significance level for the Fukunaga-Olsen method. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>DimEstPointwise</code> object, inheriting <code>data.frame</code>, with two columns:
</p>
<table>
<tr><td><code>dim.est</code></td>
<td>
<p>The dimension estimate at each point.</p>
</td></tr>
<tr><td><code>nbr.nb</code></td>
<td>
<p>The number of neighboring nodes used for the dimension estimate at each point.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kerstin Johnsson, Lund University
</p>


<h3>References</h3>

<p>Bruske, J. and Sommer, G. (1998) Intrinsic dimensionality estimation with
optimally topology preserving maps. <em>IEEE Trans. on Pattern Anal. and Mach.
Intell.</em>, <b>20</b>(5), 572-575.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pcaLocalDimEst">pcaLocalDimEst</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- hyperBall(1000, 5)
pcaOtpmPointwiseDimEst(data, 400)
</code></pre>

<hr>
<h2 id='Spherical'> Isotropic Distributions With or Without Noise </h2><span id='topic+hyperBall'></span><span id='topic+hyperSphere'></span><span id='topic+isotropicNormal'></span>

<h3>Description</h3>

<p>Generates a sample from isotropic distributions in <code>d</code> dimensions with
<code>n</code>-dimensional noise added to it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hyperBall(Ns, d, n = d, sd = 0)
hyperSphere(Ns, d, n = d + 1, sd = 0)
isotropicNormal(Ns, d, n = d, sd = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Spherical_+3A_ns">Ns</code></td>
<td>
<p> number of points. </p>
</td></tr>
<tr><td><code id="Spherical_+3A_d">d</code></td>
<td>
<p> intrinsic dimension of the support of the distribution 
(the manifold.) </p>
</td></tr>
<tr><td><code id="Spherical_+3A_n">n</code></td>
<td>
<p> dimension of noise. </p>
</td></tr>
<tr><td><code id="Spherical_+3A_sd">sd</code></td>
<td>
<p> standard deviation of noise. </p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>hyperBall</code> draws a sample from a uniform distribution on a hyper ball of 
radius 1.
<code>hyperSphere</code> draws a sample from a uniform distribution on a hypersphere
of radius 1.
<code>isotropicNormal</code> draws a sample from a isotropic normal distribution with
identity covariance matrix.
</p>


<h3>Author(s)</h3>

<p>Kerstin Johnsson, Lund University
</p>


<h3>Examples</h3>

<pre><code class='language-R'>datap &lt;- hyperSphere(100, 1, 3, sd = .1)
par(mfrow = c(1, 2))
plot(datap[, 1], datap[, 2])
plot(datap[, 1], datap[, 3])    

</code></pre>

<hr>
<h2 id='swissRoll'>
Swiss roll with or without 3-sphere inside
</h2><span id='topic+swissRoll'></span><span id='topic+swissRoll3Sph'></span>

<h3>Description</h3>

<p>Generates a sample from a uniform distribution on a Swiss roll-surface, 
possibly together with a sample from a uniform distribution on a 3-sphere
inside the Swiss roll.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>swissRoll(Ns, a = 1, b = 2, nturn = 1.5, h = 4)
swissRoll3Sph(Ns, Nsph, a = 1, b = 2, nturn = 1.5, h = 4)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="swissRoll_+3A_ns">Ns</code></td>
<td>
<p> number of data points on the Swiss roll. </p>
</td></tr>
<tr><td><code id="swissRoll_+3A_nsph">Nsph</code></td>
<td>
<p> number of data points on the 3-sphere. </p>
</td></tr>
<tr><td><code id="swissRoll_+3A_a">a</code></td>
<td>
<p> minimal radius of Swiss roll and radius of 3-sphere. </p>
</td></tr>
<tr><td><code id="swissRoll_+3A_b">b</code></td>
<td>
<p> maximal radius of Swiss roll. </p>
</td></tr>
<tr><td><code id="swissRoll_+3A_nturn">nturn</code></td>
<td>
<p> number of turns of the surface. </p>
</td></tr>
<tr><td><code id="swissRoll_+3A_h">h</code></td>
<td>
<p> height of Swiss roll. </p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>swissRoll</code> returns three-dimensional data points.
<code>swissRoll3Sph</code> returns four-dimensional data points with the Swiss roll
in the three first dimensions (columns). The <code>Ns</code> first data points
lie on the Swiss roll and the <code>Nsph</code> last data points lie on the 
3-sphere.
</p>


<h3>Author(s)</h3>

<p>Kerstin Johnsson, Lund University.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>datap &lt;- swissRoll3Sph(300, 100)
par(mfrow = c(1, 3))
plot(datap[,1], datap[,2])
plot(datap[,1], datap[,3])
plot(datap[,1], datap[,4])
</code></pre>

<hr>
<h2 id='tp'> Dimension Estimation via Translated Poisson Distributions </h2><span id='topic+maxLikGlobalDimEst'></span><span id='topic+maxLikPointwiseDimEst'></span><span id='topic+maxLikLocalDimEst'></span><span id='topic+hill'></span>

<h3>Description</h3>

<p>Estimates the intrinsic dimension of a data set using models of translated
Poisson distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maxLikGlobalDimEst(data, k, dnoise = NULL, sigma = 0, n = NULL,
        integral.approximation = 'Haro', unbiased = FALSE,
        neighborhood.based = TRUE,
        neighborhood.aggregation = 'maximum.likelihood', iterations = 5, K = 5)
maxLikPointwiseDimEst(data, k, dnoise = NULL, sigma = 0, n = NULL, indices = NULL,
             integral.approximation = 'Haro', unbiased = FALSE, iterations = 5)
maxLikLocalDimEst(data, dnoise = NULL, sigma = 0, n = NULL,
       integral.approximation = 'Haro',
       unbiased = FALSE, iterations = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tp_+3A_data">data</code></td>
<td>
<p>data set with each row describing a data point.</p>
</td></tr>
<tr><td><code id="tp_+3A_k">k</code></td>
<td>
<p>the number of distances that should be used for each dimension estimation.</p>
</td></tr>
<tr><td><code id="tp_+3A_dnoise">dnoise</code></td>
<td>
<p>a function or a name of a function giving the translation density. If NULL, no noise is modeled, and the estimator turns into the Hill estimator (see References). Translation densities <code><a href="#topic+dnoiseGaussH">dnoiseGaussH</a></code> and <code><a href="#topic+dnoiseNcChi">dnoiseNcChi</a></code> are provided in the package. <code>dnoiseGaussH</code> is an approximation of <code>dnoiseNcChi</code>, but faster.</p>
</td></tr>
<tr><td><code id="tp_+3A_sigma">sigma</code></td>
<td>
<p>(estimated) standard deviation of the (isotropic) noise.</p>
</td></tr>
<tr><td><code id="tp_+3A_n">n</code></td>
<td>
<p>dimension of the noise.</p>
</td></tr>
<tr><td><code id="tp_+3A_indices">indices</code></td>
<td>
<p>the indices of the data points for which local dimension
estimation should be made.</p>
</td></tr>
<tr><td><code id="tp_+3A_integral.approximation">integral.approximation</code></td>
<td>
<p> how to approximate the integrals in eq. (5) in Haro et al. (2008). Possible values: <code>'Haro'</code>, <code>'guaranteed.convergence'</code>, <code>'iteration'</code>. See Details.</p>
</td></tr>
<tr><td><code id="tp_+3A_unbiased">unbiased</code></td>
<td>
<p> if <code>TRUE</code>, a factor <code>k-2</code> is used instead of the factor <code>k-1</code> that was used in Haro et al. (2008). This makes the estimator is unbiased in the case of data without noise or boundary.</p>
</td></tr>
<tr><td><code id="tp_+3A_neighborhood.based">neighborhood.based</code></td>
<td>
<p> if TRUE, dimension estimation is first made for neighborhoods around each data point and final value is aggregated from this. Otherwise dimension estimation is made once, based on distances in entire data set.</p>
</td></tr>
<tr><td><code id="tp_+3A_neighborhood.aggregation">neighborhood.aggregation</code></td>
<td>
<p> if <code>neighborhood.based</code>, how should dimension estimates from different neighborhoods be combined. Possible values: <code>'maximum.liklihood'</code> follows Haro et al. (2008) in maximizing likelihood by using the harmonic mean, <code>'mean'</code> follows Levina and Bickel (2005) and takes the mean, <code>'robust'</code> takes the median, to remove influence from possible outliers.</p>
</td></tr>
<tr><td><code id="tp_+3A_iterations">iterations</code></td>
<td>
<p> for <code>integral.approxmation = 'iteration'</code>, how many iterations should be made.</p>
</td></tr>
<tr><td><code id="tp_+3A_k">K</code></td>
<td>
<p> for <code>neighborhood.based = FALSE</code>, how many distances for each data point should be considered when looking for the <code>k</code> shortest distances in the entire data set.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The estimators are based on the referenced paper by Haro et al. (2008), using the assumption that there is a single manifold. The estimator in the paper is obtained using default parameters and <code>dnoise = dnoiseGaussH</code>.
</p>
<p>With <code>integral.approximation = 'Haro'</code> the Taylor expansion approximation of <code>r^(m-1)</code> that Haro et al. (2008) used are employed. With <code>integral.approximation = 'guaranteed.convergence'</code>, <code>r</code> is factored out and kept and <code>r^(m-2)</code> is approximated with the corresponding Taylor expansion. This guarantees convergence of the integrals. Divergence might be an issue when the noise is not sufficiently small in comparison to the smallest distances. With <code>integral.approximation = 'iteration'</code>, five iterations is used to determine <code>m</code>. 
</p>
<p><code>maxLikLocalDimEst</code> assumes that the data set is local i.e. a piece of a data set cut out by a sphere with a radius such that the data set is well approximated by a hyperplane (meaning that the curvature should be low in the local data set). See <code><a href="#topic+localIntrinsicDimension">localIntrinsicDimension</a></code>.
</p>


<h3>Value</h3>

<p>For <code>maxLikGlobalDimEst</code> and <code>maxLikLocalDimEst</code>, a <code>DimEst</code> object with one slot:
</p>
<table>
<tr><td><code>dim.est</code></td>
<td>
<p> the dimension estimate</p>
</td></tr>
</table>
<p>For <code>maxLikPointwiseDimEst</code>, a <code>DimEstPointwise</code> object, inheriting <code>data.frame</code>, with one slot:
</p>
<table>
<tr><td><code>dim.est</code></td>
<td>
<p> the dimension estimate for each data point. Row <code>i</code> has the local dimension estimate at point <code>data[indices[i], ]</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kerstin Johnsson, Lund University.
</p>


<h3>References</h3>

<p>Haro, G., Randall, G. and Sapiro, G. (2008) Translated Poisson Mixture Model
for Stratification Learning. <em>Int. J. Comput. Vis.</em>, <b>80</b>, 358-374.
</p>
<p>Hill, B. M. (1975) A simple general approach to inference about the tail of a distribution. <em>Ann. Stat.</em>, <b>3</b>(5) 1163-1174.
</p>
<p>Levina, E. and Bickel., P. J. (2005) Maximum likelihood estimation of intrinsic dimension. <em>Advances in Neural Information Processing Systems 17</em>, 777-784. MIT Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- hyperBall(100, d = 7, n = 13, sd = 0.01)
maxLikGlobalDimEst(data, 10, dnoiseNcChi, 0.01, 13)
maxLikGlobalDimEst(data, 10, dnoiseGaussH, 0.01, 13)
maxLikGlobalDimEst(data, 10, dnoiseGaussH, 0.01, 13)
maxLikGlobalDimEst(data, 10, dnoiseGaussH, 0.01, 13, neighborhood.aggregation = 'robust')
maxLikGlobalDimEst(data, 10, dnoiseGaussH, 0.01, 13,
        integral.approximation = 'guaranteed.convergence',
        neighborhood.aggregation = 'robust')
maxLikGlobalDimEst(data, 10, dnoiseGaussH, 0.01, 13,
        integral.approximation = 'iteration', unbiased = TRUE)

data &lt;- hyperBall(1000, d = 7, n = 13, sd = 0.01)
maxLikGlobalDimEst(data, 500, dnoiseGaussH, 0.01, 13,
        neighborhood.based = FALSE)
maxLikGlobalDimEst(data, 500, dnoiseGaussH, 0.01, 13,
        integral.approximation = 'guaranteed.convergence',
        neighborhood.based = FALSE)
maxLikGlobalDimEst(data, 500, dnoiseGaussH, 0.01, 13,
        integral.approximation = 'iteration',
        neighborhood.based = FALSE)
        
data &lt;- hyperBall(100, d = 7, n = 13, sd = 0.01)
maxLikPointwiseDimEst(data, 10, dnoiseNcChi, 0.01, 13, indices=1:10)

data &lt;- cutHyperPlane(50, d = 7, n = 13, sd = 0.01)
maxLikLocalDimEst(data, dnoiseNcChi, 0.1, 3)
maxLikLocalDimEst(data, dnoiseGaussH, 0.1, 3)
maxLikLocalDimEst(data, dnoiseNcChi, 0.1, 3,
       integral.approximation = 'guaranteed.convergence')

</code></pre>

<hr>
<h2 id='twinPeaks'> Twin Peaks </h2><span id='topic+twinPeaks'></span><span id='topic+hyperTwinPeaks'></span>

<h3>Description</h3>

<p>Generates data points from a two- or higher-dimensional Twin Peaks manifold.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>twinPeaks(Ns, h = 1)
hyperTwinPeaks(Ns, n, h = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="twinPeaks_+3A_ns">Ns</code></td>
<td>
<p> number of data points. </p>
</td></tr>
<tr><td><code id="twinPeaks_+3A_n">n</code></td>
<td>
<p> dimension of the (hyper) plane from which the peaks stand out. For
<code>twinPeaks</code> <code>n</code> is 2. </p>
</td></tr>
<tr><td><code id="twinPeaks_+3A_h">h</code></td>
<td>
<p> height of the peaks. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The height of the points is computed as <code class="reqn">\prod_1^n \sin(x_i)</code>, where <code class="reqn">x_1,...,x_n</code> are the coordinates of the point in
the (hyper) plane.
</p>


<h3>Value</h3>

<p>A <code>n+1</code>-dimensional data set, where the last dimension represents the
height of the points.
</p>


<h3>Author(s)</h3>

<p>Kerstin Johnsson, Lund University.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>datap &lt;- twinPeaks(400)
par(mfrow = c(1, 3))
plot(datap[,1], datap[,3])
plot(datap[,2], datap[,3])
plot(datap[,1], datap[,2])
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
