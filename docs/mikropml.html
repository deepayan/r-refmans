<!DOCTYPE html><html><head><title>Help for package mikropml</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {mikropml}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#mikropml-package'><p>mikropml: User-Friendly R Package for Robust Machine Learning Pipelines</p></a></li>
<li><a href='#abort_packages_not_installed'><p>Throw error if required packages are not installed.</p></a></li>
<li><a href='#bootstrap_performance'><p>Calculate a bootstrap confidence interval for the performance on a single train/test split</p></a></li>
<li><a href='#calc_balanced_precision'><p>Calculate balanced precision given actual and baseline precision</p></a></li>
<li><a href='#calc_baseline_precision'><p>Calculate the fraction of positives, i.e. baseline precision for a PRC curve</p></a></li>
<li><a href='#calc_mean_perf'><p>Generic function to calculate mean performance curves for multiple models</p></a></li>
<li><a href='#calc_model_sensspec'><p>Calculate and summarize performance for ROC and PRC plots</p></a></li>
<li><a href='#calc_perf_bootstrap_split'><p>Calculate performance for a single split from <code>rsample::bootstraps()</code></p></a></li>
<li><a href='#calc_perf_metrics'><p>Get performance metrics for test data</p></a></li>
<li><a href='#calc_pvalue'><p>Calculate the p-value for a permutation test</p></a></li>
<li><a href='#change_to_num'><p>Change columns to numeric if possible</p></a></li>
<li><a href='#check_all'><p>Check all params that don't return a value</p></a></li>
<li><a href='#check_cat_feats'><p>Check if any features are categorical</p></a></li>
<li><a href='#check_corr_thresh'><p>check that corr_thresh is either NULL or a number between 0 and 1</p></a></li>
<li><a href='#check_dataset'><p>Check that the dataset is not empty and has more than 1 column.</p></a></li>
<li><a href='#check_features'><p>Check features</p></a></li>
<li><a href='#check_group_partitions'><p>Check the validity of the group_partitions list</p></a></li>
<li><a href='#check_groups'><p>Check grouping vector</p></a></li>
<li><a href='#check_kfold'><p>Check that kfold is an integer of reasonable size</p></a></li>
<li><a href='#check_method'><p>Check if the method is supported. If not, throws error.</p></a></li>
<li><a href='#check_ntree'><p>Check ntree</p></a></li>
<li><a href='#check_outcome_column'><p>Check that outcome column exists. Pick outcome column if not specified.</p></a></li>
<li><a href='#check_outcome_value'><p>Check that the outcome variable is valid. Pick outcome value if necessary.</p></a></li>
<li><a href='#check_packages_installed'><p>Check whether package(s) are installed</p></a></li>
<li><a href='#check_perf_metric_function'><p>Check perf_metric_function is NULL or a function</p></a></li>
<li><a href='#check_perf_metric_name'><p>Check perf_metric_name is NULL or a function</p></a></li>
<li><a href='#check_permute'><p>Check that permute is a logical</p></a></li>
<li><a href='#check_remove_var'><p>Check remove_var</p></a></li>
<li><a href='#check_seed'><p>check that the seed is either NA or a number</p></a></li>
<li><a href='#check_training_frac'><p>Check that the training fraction is between 0 and 1</p></a></li>
<li><a href='#check_training_indices'><p>Check the validity of the training indices</p></a></li>
<li><a href='#cluster_corr_mat'><p>Cluster a matrix of correlated features</p></a></li>
<li><a href='#collapse_correlated_features'><p>Collapse correlated features</p></a></li>
<li><a href='#combine_hp_performance'><p>Combine hyperparameter performance metrics for multiple train/test splits</p></a></li>
<li><a href='#compare_models'><p>Perform permutation tests to compare the performance metric</p>
across all pairs of a group variable.</a></li>
<li><a href='#create_grouped_data_partition'><p>Split into train and test set while splitting by groups.</p>
When <code>group_partitions</code> is <code>NULL</code>, all samples from each group will go into
either the training set or the testing set.
Otherwise, the groups will be split according to <code>group_partitions</code></a></li>
<li><a href='#create_grouped_k_multifolds'><p>Splitting into folds for cross-validation when using groups</p></a></li>
<li><a href='#define_cv'><p>Define cross-validation scheme and training parameters</p></a></li>
<li><a href='#find_permuted_perf_metric'><p>Get permuted performance metric difference for a single feature</p>
(or group of features)</a></li>
<li><a href='#flatten_corr_mat'><p>Flatten correlation matrix to pairs</p></a></li>
<li><a href='#get_binary_corr_mat'><p>Identify correlated features as a binary matrix</p></a></li>
<li><a href='#get_caret_dummyvars_df'><p>Get dummyvars dataframe (i.e. design matrix)</p></a></li>
<li><a href='#get_caret_processed_df'><p>Get preprocessed dataframe for continuous variables</p></a></li>
<li><a href='#get_corr_feats'><p>Identify correlated features</p></a></li>
<li><a href='#get_difference'><p>Calculate the difference in the mean of the metric for two groups</p></a></li>
<li><a href='#get_feature_importance'><p>Get feature importance using the permutation method</p></a></li>
<li><a href='#get_groups_from_clusters'><p>Assign features to groups</p></a></li>
<li><a href='#get_hp_performance'><p>Get hyperparameter performance metrics</p></a></li>
<li><a href='#get_hyperparams_from_df'><p>Split hyperparameters dataframe into named lists for each parameter</p></a></li>
<li><a href='#get_hyperparams_list'><p>Set hyperparameters based on ML method and dataset characteristics</p></a></li>
<li><a href='#get_outcome_type'><p>Get outcome type.</p></a></li>
<li><a href='#get_partition_indices'><p>Select indices to partition the data into training &amp; testing sets.</p></a></li>
<li><a href='#get_perf_metric_fn'><p>Get default performance metric function</p></a></li>
<li><a href='#get_perf_metric_name'><p>Get default performance metric name</p></a></li>
<li><a href='#get_performance_tbl'><p>Get model performance metrics as a one-row tibble</p></a></li>
<li><a href='#get_seeds_trainControl'><p>Get seeds for <code>caret::trainControl()</code></p></a></li>
<li><a href='#get_tuning_grid'><p>Generate the tuning grid for tuning hyperparameters</p></a></li>
<li><a href='#group_correlated_features'><p>Group correlated features</p></a></li>
<li><a href='#is_whole_number'><p>Check whether a numeric vector contains whole numbers.</p></a></li>
<li><a href='#keep_groups_in_cv_partitions'><p>Whether groups can be kept together in partitions during cross-validation</p></a></li>
<li><a href='#lower_bound'><p>Get the lower and upper bounds for an empirical confidence interval</p></a></li>
<li><a href='#mutate_all_types'><p>Mutate all columns with <code>utils::type.convert()</code>.'</p></a></li>
<li><a href='#otu_data_preproc'><p>Mini OTU abundance dataset - preprocessed</p></a></li>
<li><a href='#otu_mini_bin'><p>Mini OTU abundance dataset</p></a></li>
<li><a href='#otu_mini_bin_results_glmnet'><p>Results from running the pipeline with L2 logistic regression on <code>otu_mini_bin</code> with feature importance and grouping</p></a></li>
<li><a href='#otu_mini_bin_results_rf'><p>Results from running the pipeline with random forest on <code>otu_mini_bin</code></p></a></li>
<li><a href='#otu_mini_bin_results_rpart2'><p>Results from running the pipeline with rpart2 on <code>otu_mini_bin</code></p></a></li>
<li><a href='#otu_mini_bin_results_svmRadial'><p>Results from running the pipeline with svmRadial on <code>otu_mini_bin</code></p></a></li>
<li><a href='#otu_mini_bin_results_xgbTree'><p>Results from running the pipeline with xbgTree on <code>otu_mini_bin</code></p></a></li>
<li><a href='#otu_mini_cont_results_glmnet'><p>Results from running the pipeline with glmnet on <code>otu_mini_bin</code> with <code>Otu00001</code></p>
as the outcome</a></li>
<li><a href='#otu_mini_cont_results_nocv'><p>Results from running the pipeline with glmnet on <code>otu_mini_bin</code> with <code>Otu00001</code></p>
as the outcome column,
using a custom train control scheme that does not perform cross-validation</a></li>
<li><a href='#otu_mini_cv'><p>Cross validation on <code>train_data_mini</code> with grouped features.</p></a></li>
<li><a href='#otu_mini_multi'><p>Mini OTU abundance dataset with 3 categorical variables</p></a></li>
<li><a href='#otu_mini_multi_group'><p>Groups for otu_mini_multi</p></a></li>
<li><a href='#otu_mini_multi_results_glmnet'><p>Results from running the pipeline with glmnet on <code>otu_mini_multi</code> for</p>
multiclass outcomes</a></li>
<li><a href='#otu_small'><p>Small OTU abundance dataset</p></a></li>
<li><a href='#pbtick'><p>Update progress if the progress bar is not <code>NULL</code>.</p></a></li>
<li><a href='#permute_p_value'><p>Calculated a permuted p-value comparing two models</p></a></li>
<li><a href='#plot_hp_performance'><p>Plot hyperparameter performance metrics</p></a></li>
<li><a href='#plot_mean_roc'><p>Plot ROC and PRC curves</p></a></li>
<li><a href='#plot_model_performance'><p>Plot performance metrics for multiple ML runs with different parameters</p></a></li>
<li><a href='#preprocess_data'><p>Preprocess data prior to running machine learning</p></a></li>
<li><a href='#process_cat_feats'><p>Process categorical features</p></a></li>
<li><a href='#process_cont_feats'><p>Preprocess continuous features</p></a></li>
<li><a href='#process_novar_feats'><p>Process features with no variation</p></a></li>
<li><a href='#radix_sort'><p>Call <code>sort()</code> with <code>method = 'radix'</code></p></a></li>
<li><a href='#randomize_feature_order'><p>Randomize feature order to eliminate any position-dependent effects</p></a></li>
<li><a href='#reexports'><p>caret contr.ltfr</p></a></li>
<li><a href='#remove_singleton_columns'><p>Remove columns appearing in only <code>threshold</code> row(s) or fewer.</p></a></li>
<li><a href='#replace_spaces'><p>Replace spaces in all elements of a character vector with underscores</p></a></li>
<li><a href='#rm_missing_outcome'><p>Remove missing outcome values</p></a></li>
<li><a href='#run_ml'><p>Run the machine learning pipeline</p></a></li>
<li><a href='#select_apply'><p>Use future apply if available</p></a></li>
<li><a href='#set_hparams_glmnet'><p>Set hyperparameters for regression models for use with glmnet</p></a></li>
<li><a href='#set_hparams_rf'><p>Set hyparameters for random forest models</p></a></li>
<li><a href='#set_hparams_rpart2'><p>Set hyperparameters for decision tree models</p></a></li>
<li><a href='#set_hparams_svmRadial'><p>Set hyperparameters for SVM with radial kernel</p></a></li>
<li><a href='#set_hparams_xgbTree'><p>Set hyperparameters for SVM with radial kernel</p></a></li>
<li><a href='#shared_ggprotos'><p>Get plot layers shared by <code>plot_mean_roc</code> and <code>plot_mean_prc</code></p></a></li>
<li><a href='#shuffle_group'><p>Shuffle the rows in a column</p></a></li>
<li><a href='#split_outcome_features'><p>Split dataset into outcome and features</p></a></li>
<li><a href='#tidy_perf_data'><p>Tidy the performance dataframe</p></a></li>
<li><a href='#train_model'><p>Train model using <code>caret::train()</code>.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>User-Friendly R Package for Supervised Machine Learning
Pipelines</td>
</tr>
<tr>
<td>Version:</td>
<td>1.6.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-08-21</td>
</tr>
<tr>
<td>Description:</td>
<td>An interface to build machine learning models for
    classification and regression problems. 'mikropml' implements the ML
    pipeline described by Topçuoğlu et al. (2020)
    &lt;<a href="https://doi.org/10.1128%2FmBio.00434-20">doi:10.1128/mBio.00434-20</a>&gt; with reasonable default options for data
    preprocessing, hyperparameter tuning, cross-validation, testing, model
    evaluation, and interpretation steps.  See the website
    <a href="https://www.schlosslab.org/mikropml/">https://www.schlosslab.org/mikropml/</a> for more information,
    documentation, and examples.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://www.schlosslab.org/mikropml/">https://www.schlosslab.org/mikropml/</a>,
<a href="https://github.com/SchlossLab/mikropml">https://github.com/SchlossLab/mikropml</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/SchlossLab/mikropml/issues">https://github.com/SchlossLab/mikropml/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>caret, dplyr, e1071, glmnet, kernlab, MLmetrics, randomForest,
rlang, rpart, stats, utils, xgboost</td>
</tr>
<tr>
<td>Suggests:</td>
<td>assertthat, doFuture, forcats, foreach, future, future.apply,
furrr, ggplot2, knitr, progress, progressr, purrr, rmarkdown,
rsample, testthat, tidyr</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-08-21 14:16:32 UTC; sovacool</td>
</tr>
<tr>
<td>Author:</td>
<td>Begüm Topçuoğlu <a href="https://orcid.org/0000-0003-3140-537X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Zena Lapp <a href="https://orcid.org/0000-0003-4674-2176"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Kelly Sovacool <a href="https://orcid.org/0000-0003-3283-829X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Evan Snitkin <a href="https://orcid.org/0000-0001-8409-278X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Jenna Wiens <a href="https://orcid.org/0000-0002-1057-7722"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Patrick Schloss <a href="https://orcid.org/0000-0002-6935-4275"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Nick Lesniak <a href="https://orcid.org/0000-0001-9359-5194"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb],
  Courtney Armour <a href="https://orcid.org/0000-0002-5250-1224"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb],
  Sarah Lucas <a href="https://orcid.org/0000-0003-1676-5801"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kelly Sovacool &lt;sovacool@umich.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-08-21 15:10:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='mikropml-package'>mikropml: User-Friendly R Package for Robust Machine Learning Pipelines</h2><span id='topic+mikropml'></span><span id='topic+mikropml-package'></span>

<h3>Description</h3>

<p><code>mikropml</code> implements supervised machine learning pipelines using regression,
support vector machines, decision trees, random forest, or gradient-boosted trees.
The main functions are <code>preprocess_data()</code> to process your data prior to
running machine learning, and <code>run_ml()</code> to run machine learning.
</p>


<h3>Authors</h3>


<ul>
<li><p> Begüm D. Topçuoğlu (<a href="https://orcid.org/0000-0003-3140-537X">ORCID</a>)
</p>
</li>
<li><p> Zena Lapp (<a href="https://orcid.org/0000-0003-4674-2176">ORCID</a>)
</p>
</li>
<li><p> Kelly L. Sovacool (<a href="https://orcid.org/0000-0003-3283-829X">ORCID</a>)
</p>
</li>
<li><p> Evan Snitkin (<a href="https://orcid.org/0000-0001-8409-278X">ORCID</a>)
</p>
</li>
<li><p> Jenna Wiens (<a href="https://orcid.org/0000-0002-1057-7722">ORCID</a>)
</p>
</li>
<li><p> Patrick D. Schloss (<a href="https://orcid.org/0000-0002-6935-4275">ORCID</a>)
</p>
</li></ul>



<h3>See vignettes</h3>


<ul>
<li> <p><a href="http://www.schlosslab.org/mikropml/articles/introduction.html">Introduction</a>
</p>
</li>
<li> <p><a href="http://www.schlosslab.org/mikropml/articles/preprocess.html">Preprocessing data</a>
</p>
</li>
<li> <p><a href="http://www.schlosslab.org/mikropml/articles/tuning.html">Hyperparameter tuning</a>
</p>
</li>
<li> <p><a href="http://www.schlosslab.org/mikropml/articles/parallel.html">Parallel processing</a>
</p>
</li>
<li> <p><a href="http://www.schlosslab.org/mikropml/articles/paper.html">The mikropml paper</a>
</p>
</li></ul>



<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Kelly Sovacool <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a> (<a href="https://orcid.org/0000-0003-3283-829X">ORCID</a>)
</p>
<p>Authors:
</p>

<ul>
<li><p> Begüm Topçuoğlu <a href="mailto:topcuoglu.begum@gmail.com">topcuoglu.begum@gmail.com</a> (<a href="https://orcid.org/0000-0003-3140-537X">ORCID</a>)
</p>
</li>
<li><p> Zena Lapp <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a> (<a href="https://orcid.org/0000-0003-4674-2176">ORCID</a>)
</p>
</li>
<li><p> Evan Snitkin (<a href="https://orcid.org/0000-0001-8409-278X">ORCID</a>)
</p>
</li>
<li><p> Jenna Wiens (<a href="https://orcid.org/0000-0002-1057-7722">ORCID</a>)
</p>
</li>
<li><p> Patrick Schloss <a href="mailto:pschloss@umich.edu">pschloss@umich.edu</a> (<a href="https://orcid.org/0000-0002-6935-4275">ORCID</a>)
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Nick Lesniak <a href="mailto:nlesniak@umich.edu">nlesniak@umich.edu</a> (<a href="https://orcid.org/0000-0001-9359-5194">ORCID</a>) [contributor]
</p>
</li>
<li><p> Courtney Armour <a href="mailto:armourc@umich.edu">armourc@umich.edu</a> (<a href="https://orcid.org/0000-0002-5250-1224">ORCID</a>) [contributor]
</p>
</li>
<li><p> Sarah Lucas <a href="mailto:salucas@umich.edu">salucas@umich.edu</a> (<a href="https://orcid.org/0000-0003-1676-5801">ORCID</a>) [contributor]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://www.schlosslab.org/mikropml/">https://www.schlosslab.org/mikropml/</a>
</p>
</li>
<li> <p><a href="https://github.com/SchlossLab/mikropml">https://github.com/SchlossLab/mikropml</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/SchlossLab/mikropml/issues">https://github.com/SchlossLab/mikropml/issues</a>
</p>
</li></ul>


<hr>
<h2 id='abort_packages_not_installed'>Throw error if required packages are not installed.</h2><span id='topic+abort_packages_not_installed'></span>

<h3>Description</h3>

<p>Reports which packages need to be installed and the parent function name.
See <a href="https://stackoverflow.com/questions/15595478/how-to-get-the-name-of-the-calling-function-inside-the-called-routine">https://stackoverflow.com/questions/15595478/how-to-get-the-name-of-the-calling-function-inside-the-called-routine</a>
This is only intended to be used inside a function. It will error otherwise.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>abort_packages_not_installed(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="abort_packages_not_installed_+3A_...">...</code></td>
<td>
<p>names of packages to check</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kelly Sovacool <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
abort_packages_not_installed("base")
abort_packages_not_installed("not-a-package-name", "caret", "dplyr", "non_package")

## End(Not run)
</code></pre>

<hr>
<h2 id='bootstrap_performance'>Calculate a bootstrap confidence interval for the performance on a single train/test split</h2><span id='topic+bootstrap_performance'></span>

<h3>Description</h3>

<p>Uses <code><a href="rsample.html#topic+bootstraps">rsample::bootstraps()</a></code>, <code><a href="rsample.html#topic+int_pctl">rsample::int_pctl()</a></code>, and <code><a href="furrr.html#topic+future_map">furrr::future_map()</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootstrap_performance(
  ml_result,
  outcome_colname,
  bootstrap_times = 10000,
  alpha = 0.05
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bootstrap_performance_+3A_ml_result">ml_result</code></td>
<td>
<p>result returned from a single <code><a href="#topic+run_ml">run_ml()</a></code> call</p>
</td></tr>
<tr><td><code id="bootstrap_performance_+3A_outcome_colname">outcome_colname</code></td>
<td>
<p>Column name as a string of the outcome variable
(default <code>NULL</code>; the first column will be chosen automatically).</p>
</td></tr>
<tr><td><code id="bootstrap_performance_+3A_bootstrap_times">bootstrap_times</code></td>
<td>
<p>the number of boostraps to create (default: <code>10000</code>)</p>
</td></tr>
<tr><td><code id="bootstrap_performance_+3A_alpha">alpha</code></td>
<td>
<p>the alpha level for the confidence interval (default <code>0.05</code> to create a 95% confidence interval)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data frame with an estimate (<code>.estimate</code>), lower bound (<code>.lower</code>),
and upper bound (<code>.upper</code>) for each performance metric (<code>term</code>).
</p>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>bootstrap_performance(otu_mini_bin_results_glmnet, "dx",
  bootstrap_times = 10, alpha = 0.10
)
## Not run: 
outcome_colname &lt;- "dx"
run_ml(otu_mini_bin, "rf", outcome_colname = "dx") %&gt;%
  bootstrap_performance(outcome_colname,
    bootstrap_times = 10000,
    alpha = 0.05
  )

## End(Not run)
</code></pre>

<hr>
<h2 id='calc_balanced_precision'>Calculate balanced precision given actual and baseline precision</h2><span id='topic+calc_balanced_precision'></span>

<h3>Description</h3>

<p>Implements Equation 1 from Wu <em>et al.</em> 2021 <a href="https://doi.org/10.1016/j.ajhg.2021.08.012">doi:10.1016/j.ajhg.2021.08.012</a>.
It is the same as Equation 7 if <code>AUPRC</code> (aka <code>prAUC</code>) is used in place of <code>precision</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_balanced_precision(precision, prior)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_balanced_precision_+3A_precision">precision</code></td>
<td>
<p>actual precision of the model.</p>
</td></tr>
<tr><td><code id="calc_balanced_precision_+3A_prior">prior</code></td>
<td>
<p>baseline precision, aka frequency of positives.
Can be calculated with <a href="#topic+calc_baseline_precision">calc_baseline_precision</a></p>
</td></tr>
</table>


<h3>Value</h3>

<p>the expected precision if the data were balanced
</p>


<h3>Author(s)</h3>

<p>Kelly Sovacool <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prior &lt;- calc_baseline_precision(otu_mini_bin,
  outcome_colname = "dx",
  pos_outcome = "cancer"
)
calc_balanced_precision(otu_mini_bin_results_rf$performance$Precision, prior)

otu_mini_bin_results_rf$performance %&gt;%
  dplyr::mutate(
    balanced_precision = calc_balanced_precision(Precision, prior),
    aubprc = calc_balanced_precision(prAUC, prior)
  ) %&gt;%
  dplyr::select(AUC, Precision, balanced_precision, aubprc)

# cumulative performance for a single model
sensspec_1 &lt;- calc_model_sensspec(
  otu_mini_bin_results_glmnet$trained_model,
  otu_mini_bin_results_glmnet$test_data,
  "dx"
)
head(sensspec_1)
prior &lt;- calc_baseline_precision(otu_mini_bin,
  outcome_colname = "dx",
  pos_outcome = "cancer"
)
sensspec_1 %&gt;%
  dplyr::mutate(balanced_precision = calc_balanced_precision(precision, prior)) %&gt;%
  dplyr::rename(recall = sensitivity) %&gt;%
  calc_mean_perf(group_var = recall, sum_var = balanced_precision) %&gt;%
  plot_mean_prc(ycol = mean_balanced_precision)
</code></pre>

<hr>
<h2 id='calc_baseline_precision'>Calculate the fraction of positives, i.e. baseline precision for a PRC curve</h2><span id='topic+calc_baseline_precision'></span>

<h3>Description</h3>

<p>Calculate the fraction of positives, i.e. baseline precision for a PRC curve
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_baseline_precision(dataset, outcome_colname = NULL, pos_outcome = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_baseline_precision_+3A_dataset">dataset</code></td>
<td>
<p>Data frame with an outcome variable and other columns as features.</p>
</td></tr>
<tr><td><code id="calc_baseline_precision_+3A_outcome_colname">outcome_colname</code></td>
<td>
<p>Column name as a string of the outcome variable
(default <code>NULL</code>; the first column will be chosen automatically).</p>
</td></tr>
<tr><td><code id="calc_baseline_precision_+3A_pos_outcome">pos_outcome</code></td>
<td>
<p>the positive outcome from <code>outcome_colname</code>,
e.g. &quot;cancer&quot; for the <code>otu_mini_bin</code> dataset.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the baseline precision based on the fraction of positives
</p>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># calculate the baseline precision
data.frame(y = c("a", "b", "a", "b")) %&gt;%
  calc_baseline_precision(
    outcome_colname = "y",
    pos_outcome = "a"
  )


calc_baseline_precision(otu_mini_bin,
  outcome_colname = "dx",
  pos_outcome = "cancer"
)


# if you're not sure which outcome was used as the 'positive' outcome during
# model training, you can access it from the trained model and pass it along:
calc_baseline_precision(otu_mini_bin,
  outcome_colname = "dx",
  pos_outcome = otu_mini_bin_results_glmnet$trained_model$levels[1]
)

</code></pre>

<hr>
<h2 id='calc_mean_perf'>Generic function to calculate mean performance curves for multiple models</h2><span id='topic+calc_mean_perf'></span>

<h3>Description</h3>

<p>Used by <code>calc_mean_roc()</code> and <code>calc_mean_prc()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_mean_perf(sensspec_dat, group_var = specificity, sum_var = sensitivity)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_mean_perf_+3A_sensspec_dat">sensspec_dat</code></td>
<td>
<p>data frame created by concatenating results of
<code>calc_model_sensspec()</code> for multiple models.</p>
</td></tr>
<tr><td><code id="calc_mean_perf_+3A_group_var">group_var</code></td>
<td>
<p>variable to group by (e.g. specificity or recall).</p>
</td></tr>
<tr><td><code id="calc_mean_perf_+3A_sum_var">sum_var</code></td>
<td>
<p>variable to summarize (e.g. sensitivity or precision).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data frame with mean &amp; standard deviation of <code>sum_var</code> summarized over <code>group_var</code>
</p>


<h3>Author(s)</h3>

<p>Courtney Armour
</p>
<p>Kelly Sovacool
</p>

<hr>
<h2 id='calc_model_sensspec'>Calculate and summarize performance for ROC and PRC plots</h2><span id='topic+calc_model_sensspec'></span><span id='topic+calc_mean_roc'></span><span id='topic+calc_mean_prc'></span><span id='topic+sensspec'></span>

<h3>Description</h3>

<p>Use these functions to calculate cumulative sensitivity,
specificity, recall, etc. on single models, concatenate the results
together from multiple models, and compute mean ROC and PRC.
You can then plot mean ROC and PRC curves to visualize the results.
<strong>Note</strong>: These functions assume a binary outcome.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_model_sensspec(trained_model, test_data, outcome_colname = NULL)

calc_mean_roc(sensspec_dat)

calc_mean_prc(sensspec_dat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_model_sensspec_+3A_trained_model">trained_model</code></td>
<td>
<p>Trained model from <code><a href="caret.html#topic+train">caret::train()</a></code>.</p>
</td></tr>
<tr><td><code id="calc_model_sensspec_+3A_test_data">test_data</code></td>
<td>
<p>Held out test data: dataframe of outcome and features.</p>
</td></tr>
<tr><td><code id="calc_model_sensspec_+3A_outcome_colname">outcome_colname</code></td>
<td>
<p>Column name as a string of the outcome variable
(default <code>NULL</code>; the first column will be chosen automatically).</p>
</td></tr>
<tr><td><code id="calc_model_sensspec_+3A_sensspec_dat">sensspec_dat</code></td>
<td>
<p>data frame created by concatenating results of
<code>calc_model_sensspec()</code> for multiple models.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data frame with summarized performance
</p>


<h3>Functions</h3>


<ul>
<li> <p><code>calc_model_sensspec()</code>: Get sensitivity, specificity, and precision for a model.
</p>
</li>
<li> <p><code>calc_mean_roc()</code>: Calculate mean sensitivity over specificity for multiple models
</p>
</li>
<li> <p><code>calc_mean_prc()</code>: Calculate mean precision over recall for multiple models
</p>
</li></ul>


<h3>Author(s)</h3>

<p>Courtney Armour
</p>
<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(dplyr)
# get cumulative performance for a single model
sensspec_1 &lt;- calc_model_sensspec(
  otu_mini_bin_results_glmnet$trained_model,
  otu_mini_bin_results_glmnet$test_data,
  "dx"
)
head(sensspec_1)

# get performance for multiple models
get_sensspec_seed &lt;- function(seed) {
  ml_result &lt;- run_ml(otu_mini_bin, "glmnet", seed = seed)
  sensspec &lt;- calc_model_sensspec(
    ml_result$trained_model,
    ml_result$test_data,
    "dx"
  ) %&gt;%
    dplyr::mutate(seed = seed)
  return(sensspec)
}
sensspec_dat &lt;- purrr::map_dfr(seq(100, 102), get_sensspec_seed)

# calculate mean sensitivity over specificity
roc_dat &lt;- calc_mean_roc(sensspec_dat)
head(roc_dat)

# calculate mean precision over recall
prc_dat &lt;- calc_mean_prc(sensspec_dat)
head(prc_dat)

# plot ROC &amp; PRC
roc_dat %&gt;% plot_mean_roc()
baseline_prec &lt;- calc_baseline_precision(otu_mini_bin, "dx", "cancer")
prc_dat %&gt;%
  plot_mean_prc(baseline_precision = baseline_prec)

# balanced precision
prior &lt;- calc_baseline_precision(otu_mini_bin,
  outcome_colname = "dx",
  pos_outcome = "cancer"
)
bprc_dat &lt;- sensspec_dat %&gt;%
  dplyr::mutate(balanced_precision = calc_balanced_precision(precision, prior)) %&gt;%
  dplyr::rename(recall = sensitivity) %&gt;%
  calc_mean_perf(group_var = recall, sum_var = balanced_precision)
bprc_dat %&gt;% plot_mean_prc(ycol = mean_balanced_precision) + ylab("Mean Bal. Precision")

## End(Not run)
</code></pre>

<hr>
<h2 id='calc_perf_bootstrap_split'>Calculate performance for a single split from <code><a href="rsample.html#topic+bootstraps">rsample::bootstraps()</a></code></h2><span id='topic+calc_perf_bootstrap_split'></span>

<h3>Description</h3>

<p>Used by <code><a href="#topic+bootstrap_performance">bootstrap_performance()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_perf_bootstrap_split(
  test_data_split,
  trained_model,
  outcome_colname,
  perf_metric_function,
  perf_metric_name,
  class_probs,
  method,
  seed
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_perf_bootstrap_split_+3A_test_data_split">test_data_split</code></td>
<td>
<p>a single bootstrap of the test set from <code><a href="rsample.html#topic+bootstraps">rsample::bootstraps()</a></code></p>
</td></tr>
<tr><td><code id="calc_perf_bootstrap_split_+3A_trained_model">trained_model</code></td>
<td>
<p>Trained model from <code><a href="caret.html#topic+train">caret::train()</a></code>.</p>
</td></tr>
<tr><td><code id="calc_perf_bootstrap_split_+3A_outcome_colname">outcome_colname</code></td>
<td>
<p>Column name as a string of the outcome variable
(default <code>NULL</code>; the first column will be chosen automatically).</p>
</td></tr>
<tr><td><code id="calc_perf_bootstrap_split_+3A_perf_metric_function">perf_metric_function</code></td>
<td>
<p>Function to calculate the performance metric to
be used for cross-validation and test performance. Some functions are
provided by caret (see <code><a href="caret.html#topic+postResample">caret::defaultSummary()</a></code>).
Defaults: binary classification = <code>twoClassSummary</code>,
multi-class classification = <code>multiClassSummary</code>,
regression = <code>defaultSummary</code>.</p>
</td></tr>
<tr><td><code id="calc_perf_bootstrap_split_+3A_perf_metric_name">perf_metric_name</code></td>
<td>
<p>The column name from the output of the function
provided to perf_metric_function that is to be used as the performance metric.
Defaults: binary classification = <code>"ROC"</code>,
multi-class classification = <code>"logLoss"</code>,
regression = <code>"RMSE"</code>.</p>
</td></tr>
<tr><td><code id="calc_perf_bootstrap_split_+3A_class_probs">class_probs</code></td>
<td>
<p>Whether to use class probabilities (TRUE for categorical outcomes, FALSE for numeric outcomes).</p>
</td></tr>
<tr><td><code id="calc_perf_bootstrap_split_+3A_method">method</code></td>
<td>
<p>ML method.
Options: <code>c("glmnet", "rf", "rpart2", "svmRadial", "xgbTree")</code>.
</p>

<ul>
<li><p> glmnet: linear, logistic, or multiclass regression
</p>
</li>
<li><p> rf: random forest
</p>
</li>
<li><p> rpart2: decision tree
</p>
</li>
<li><p> svmRadial: support vector machine
</p>
</li>
<li><p> xgbTree: xgboost
</p>
</li></ul>
</td></tr>
<tr><td><code id="calc_perf_bootstrap_split_+3A_seed">seed</code></td>
<td>
<p>Random seed (default: <code>NA</code>).
Your results will only be reproducible if you set a seed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a long data frame of performance metrics for <code><a href="rsample.html#topic+int_pctl">rsample::int_pctl()</a></code>
</p>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>

<hr>
<h2 id='calc_perf_metrics'>Get performance metrics for test data</h2><span id='topic+calc_perf_metrics'></span>

<h3>Description</h3>

<p>Get performance metrics for test data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_perf_metrics(
  test_data,
  trained_model,
  outcome_colname,
  perf_metric_function,
  class_probs
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_perf_metrics_+3A_test_data">test_data</code></td>
<td>
<p>Held out test data: dataframe of outcome and features.</p>
</td></tr>
<tr><td><code id="calc_perf_metrics_+3A_trained_model">trained_model</code></td>
<td>
<p>Trained model from <code><a href="caret.html#topic+train">caret::train()</a></code>.</p>
</td></tr>
<tr><td><code id="calc_perf_metrics_+3A_outcome_colname">outcome_colname</code></td>
<td>
<p>Column name as a string of the outcome variable
(default <code>NULL</code>; the first column will be chosen automatically).</p>
</td></tr>
<tr><td><code id="calc_perf_metrics_+3A_perf_metric_function">perf_metric_function</code></td>
<td>
<p>Function to calculate the performance metric to
be used for cross-validation and test performance. Some functions are
provided by caret (see <code><a href="caret.html#topic+postResample">caret::defaultSummary()</a></code>).
Defaults: binary classification = <code>twoClassSummary</code>,
multi-class classification = <code>multiClassSummary</code>,
regression = <code>defaultSummary</code>.</p>
</td></tr>
<tr><td><code id="calc_perf_metrics_+3A_class_probs">class_probs</code></td>
<td>
<p>Whether to use class probabilities (TRUE for categorical outcomes, FALSE for numeric outcomes).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Dataframe of performance metrics.
</p>


<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
results &lt;- run_ml(otu_small, "glmnet", kfold = 2, cv_times = 2)
calc_perf_metrics(results$test_data,
  results$trained_model,
  "dx",
  multiClassSummary,
  class_probs = TRUE
)

## End(Not run)
</code></pre>

<hr>
<h2 id='calc_pvalue'>Calculate the p-value for a permutation test</h2><span id='topic+calc_pvalue'></span>

<h3>Description</h3>

<p>compute Monte Carlo p-value with correction
based on formula from Page 158 of 'Bootstrap methods and their application'
By Davison &amp; Hinkley 1997
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_pvalue(vctr, test_stat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_pvalue_+3A_vctr">vctr</code></td>
<td>
<p>vector of statistics</p>
</td></tr>
<tr><td><code id="calc_pvalue_+3A_test_stat">test_stat</code></td>
<td>
<p>the test statistic</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the number of observations in <code>vctr</code> that are greater than
<code>test_stat</code> divided by the number of observations in <code>vctr</code>
</p>


<h3>Author(s)</h3>

<p>Kelly Sovacool <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>

<hr>
<h2 id='change_to_num'>Change columns to numeric if possible</h2><span id='topic+change_to_num'></span>

<h3>Description</h3>

<p>Change columns to numeric if possible
</p>


<h3>Usage</h3>

<pre><code class='language-R'>change_to_num(features)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="change_to_num_+3A_features">features</code></td>
<td>
<p>dataframe of features for machine learning</p>
</td></tr>
</table>


<h3>Value</h3>

<p>dataframe with numeric columns where possible
</p>


<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
class(change_to_num(data.frame(val = c("1", "2", "3")))[[1]])

## End(Not run)
</code></pre>

<hr>
<h2 id='check_all'>Check all params that don't return a value</h2><span id='topic+check_all'></span>

<h3>Description</h3>

<p>Check all params that don't return a value
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_all(
  dataset,
  method,
  permute,
  kfold,
  training_frac,
  perf_metric_function,
  perf_metric_name,
  groups,
  group_partitions,
  corr_thresh,
  seed,
  hyperparameters
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_all_+3A_dataset">dataset</code></td>
<td>
<p>Data frame with an outcome variable and other columns as features.</p>
</td></tr>
<tr><td><code id="check_all_+3A_method">method</code></td>
<td>
<p>ML method.
Options: <code>c("glmnet", "rf", "rpart2", "svmRadial", "xgbTree")</code>.
</p>

<ul>
<li><p> glmnet: linear, logistic, or multiclass regression
</p>
</li>
<li><p> rf: random forest
</p>
</li>
<li><p> rpart2: decision tree
</p>
</li>
<li><p> svmRadial: support vector machine
</p>
</li>
<li><p> xgbTree: xgboost
</p>
</li></ul>
</td></tr>
<tr><td><code id="check_all_+3A_kfold">kfold</code></td>
<td>
<p>Fold number for k-fold cross-validation (default: <code>5</code>).</p>
</td></tr>
<tr><td><code id="check_all_+3A_training_frac">training_frac</code></td>
<td>
<p>Fraction of data for training set (default: <code>0.8</code>). Rows
from the dataset will be randomly selected for the training set, and all
remaining rows will be used in the testing set. Alternatively, if you
provide a vector of integers, these will be used as the row indices for the
training set. All remaining rows will be used in the testing set.</p>
</td></tr>
<tr><td><code id="check_all_+3A_perf_metric_function">perf_metric_function</code></td>
<td>
<p>Function to calculate the performance metric to
be used for cross-validation and test performance. Some functions are
provided by caret (see <code><a href="caret.html#topic+postResample">caret::defaultSummary()</a></code>).
Defaults: binary classification = <code>twoClassSummary</code>,
multi-class classification = <code>multiClassSummary</code>,
regression = <code>defaultSummary</code>.</p>
</td></tr>
<tr><td><code id="check_all_+3A_perf_metric_name">perf_metric_name</code></td>
<td>
<p>The column name from the output of the function
provided to perf_metric_function that is to be used as the performance metric.
Defaults: binary classification = <code>"ROC"</code>,
multi-class classification = <code>"logLoss"</code>,
regression = <code>"RMSE"</code>.</p>
</td></tr>
<tr><td><code id="check_all_+3A_groups">groups</code></td>
<td>
<p>Vector of groups to keep together when splitting the data into
train and test sets. If the number of groups in the training set is larger
than <code>kfold</code>, the groups will also be kept together for cross-validation.
Length matches the number of rows in the dataset (default: <code>NULL</code>).</p>
</td></tr>
<tr><td><code id="check_all_+3A_group_partitions">group_partitions</code></td>
<td>
<p>Specify how to assign <code>groups</code> to the training and
testing partitions (default: <code>NULL</code>). If <code>groups</code> specifies that some
samples belong to group <code>"A"</code> and some belong to group <code>"B"</code>, then setting
<code>group_partitions = list(train = c("A", "B"), test = c("B"))</code> will result
in all samples from group <code>"A"</code> being placed in the training set, some
samples from <code>"B"</code> also in the training set, and the remaining samples from
<code>"B"</code> in the testing set. The partition sizes will be as close to
<code>training_frac</code> as possible. If the number of groups in the training set is
larger than <code>kfold</code>, the groups will also be kept together for
cross-validation.</p>
</td></tr>
<tr><td><code id="check_all_+3A_corr_thresh">corr_thresh</code></td>
<td>
<p>For feature importance, group correlations
above or equal to <code>corr_thresh</code> (range <code>0</code> to <code>1</code>; default: <code>1</code>).</p>
</td></tr>
<tr><td><code id="check_all_+3A_seed">seed</code></td>
<td>
<p>Random seed (default: <code>NA</code>).
Your results will only be reproducible if you set a seed.</p>
</td></tr>
<tr><td><code id="check_all_+3A_hyperparameters">hyperparameters</code></td>
<td>
<p>Dataframe of hyperparameters
(default <code>NULL</code>; sensible defaults will be chosen automatically).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>

<hr>
<h2 id='check_cat_feats'>Check if any features are categorical</h2><span id='topic+check_cat_feats'></span>

<h3>Description</h3>

<p>Check if any features are categorical
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_cat_feats(feats)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_cat_feats_+3A_feats">feats</code></td>
<td>
<p>features</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
check_cat_feats(otu_mini_bin)

## End(Not run)
</code></pre>

<hr>
<h2 id='check_corr_thresh'>check that corr_thresh is either NULL or a number between 0 and 1</h2><span id='topic+check_corr_thresh'></span>

<h3>Description</h3>

<p>check that corr_thresh is either NULL or a number between 0 and 1
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_corr_thresh(corr_thresh)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_corr_thresh_+3A_corr_thresh">corr_thresh</code></td>
<td>
<p>correlation threshold</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
check_corr_thresh(1)
check_corr_thresh(0.8)
check_corr_thresh(2019)
check_corr_thresh(NULL)

## End(Not run)
</code></pre>

<hr>
<h2 id='check_dataset'>Check that the dataset is not empty and has more than 1 column.</h2><span id='topic+check_dataset'></span>

<h3>Description</h3>

<p>Errors if there are no rows or fewer than 2 columns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_dataset(dataset)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_dataset_+3A_dataset">dataset</code></td>
<td>
<p>Data frame with an outcome variable and other columns as features.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
check_dataset(otu_small)

## End(Not run)
</code></pre>

<hr>
<h2 id='check_features'>Check features</h2><span id='topic+check_features'></span>

<h3>Description</h3>

<p>Check features
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_features(features, check_missing = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_features_+3A_features">features</code></td>
<td>
<p>features for machine learning</p>
</td></tr>
<tr><td><code id="check_features_+3A_check_missing">check_missing</code></td>
<td>
<p>check whether the features have missing data (default: TRUE)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
check_features(otu_mini_bin[, 2:11])

## End(Not run)
</code></pre>

<hr>
<h2 id='check_group_partitions'>Check the validity of the group_partitions list</h2><span id='topic+check_group_partitions'></span>

<h3>Description</h3>

<p>Check the validity of the group_partitions list
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_group_partitions(dataset, groups, group_partitions)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_group_partitions_+3A_dataset">dataset</code></td>
<td>
<p>Data frame with an outcome variable and other columns as features.</p>
</td></tr>
<tr><td><code id="check_group_partitions_+3A_groups">groups</code></td>
<td>
<p>Vector of groups to keep together when splitting the data into
train and test sets. If the number of groups in the training set is larger
than <code>kfold</code>, the groups will also be kept together for cross-validation.
Length matches the number of rows in the dataset (default: <code>NULL</code>).</p>
</td></tr>
<tr><td><code id="check_group_partitions_+3A_group_partitions">group_partitions</code></td>
<td>
<p>Specify how to assign <code>groups</code> to the training and
testing partitions (default: <code>NULL</code>). If <code>groups</code> specifies that some
samples belong to group <code>"A"</code> and some belong to group <code>"B"</code>, then setting
<code>group_partitions = list(train = c("A", "B"), test = c("B"))</code> will result
in all samples from group <code>"A"</code> being placed in the training set, some
samples from <code>"B"</code> also in the training set, and the remaining samples from
<code>"B"</code> in the testing set. The partition sizes will be as close to
<code>training_frac</code> as possible. If the number of groups in the training set is
larger than <code>kfold</code>, the groups will also be kept together for
cross-validation.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
check_group_partitions(
  otu_mini_bin,
  sample(LETTERS[1:8],
    size = nrow(otu_mini_bin),
    replace = TRUE
  ),
  list(train = c("A", "B"), test = c("C", "D"))
)

## End(Not run)
</code></pre>

<hr>
<h2 id='check_groups'>Check grouping vector</h2><span id='topic+check_groups'></span>

<h3>Description</h3>

<p>Check grouping vector
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_groups(dataset, groups, kfold)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_groups_+3A_dataset">dataset</code></td>
<td>
<p>Data frame with an outcome variable and other columns as features.</p>
</td></tr>
<tr><td><code id="check_groups_+3A_groups">groups</code></td>
<td>
<p>Vector of groups to keep together when splitting the data into
train and test sets. If the number of groups in the training set is larger
than <code>kfold</code>, the groups will also be kept together for cross-validation.
Length matches the number of rows in the dataset (default: <code>NULL</code>).</p>
</td></tr>
<tr><td><code id="check_groups_+3A_kfold">kfold</code></td>
<td>
<p>Fold number for k-fold cross-validation (default: <code>5</code>).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
check_groups(mikropml::otu_mini_bin,
  sample(LETTERS, nrow(mikropml::otu_mini_bin), replace = TRUE),
  kfold = 2
)

## End(Not run)
</code></pre>

<hr>
<h2 id='check_kfold'>Check that kfold is an integer of reasonable size</h2><span id='topic+check_kfold'></span>

<h3>Description</h3>

<p>Check that kfold is an integer of reasonable size
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_kfold(kfold, dataset)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_kfold_+3A_kfold">kfold</code></td>
<td>
<p>Fold number for k-fold cross-validation (default: <code>5</code>).</p>
</td></tr>
<tr><td><code id="check_kfold_+3A_dataset">dataset</code></td>
<td>
<p>Data frame with an outcome variable and other columns as features.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
check_kfold(5, otu_small)

## End(Not run)
</code></pre>

<hr>
<h2 id='check_method'>Check if the method is supported. If not, throws error.</h2><span id='topic+check_method'></span>

<h3>Description</h3>

<p>Check if the method is supported. If not, throws error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_method(method, hyperparameters)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_method_+3A_method">method</code></td>
<td>
<p>ML method.
Options: <code>c("glmnet", "rf", "rpart2", "svmRadial", "xgbTree")</code>.
</p>

<ul>
<li><p> glmnet: linear, logistic, or multiclass regression
</p>
</li>
<li><p> rf: random forest
</p>
</li>
<li><p> rpart2: decision tree
</p>
</li>
<li><p> svmRadial: support vector machine
</p>
</li>
<li><p> xgbTree: xgboost
</p>
</li></ul>
</td></tr>
<tr><td><code id="check_method_+3A_hyperparameters">hyperparameters</code></td>
<td>
<p>Dataframe of hyperparameters
(default <code>NULL</code>; sensible defaults will be chosen automatically).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
check_method("rf")

## End(Not run)
</code></pre>

<hr>
<h2 id='check_ntree'>Check ntree</h2><span id='topic+check_ntree'></span>

<h3>Description</h3>

<p>Check ntree
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_ntree(ntree)
</code></pre>


<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
check_ntree(NULL)

## End(Not run)
</code></pre>

<hr>
<h2 id='check_outcome_column'>Check that outcome column exists. Pick outcome column if not specified.</h2><span id='topic+check_outcome_column'></span>

<h3>Description</h3>

<p>Check that outcome column exists. Pick outcome column if not specified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_outcome_column(
  dataset,
  outcome_colname,
  check_values = TRUE,
  show_message = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_outcome_column_+3A_dataset">dataset</code></td>
<td>
<p>Data frame with an outcome variable and other columns as features.</p>
</td></tr>
<tr><td><code id="check_outcome_column_+3A_outcome_colname">outcome_colname</code></td>
<td>
<p>Column name as a string of the outcome variable
(default <code>NULL</code>; the first column will be chosen automatically).</p>
</td></tr>
<tr><td><code id="check_outcome_column_+3A_check_values">check_values</code></td>
<td>
<p>whether to check the outcome values or just get the column (default:TRUE)</p>
</td></tr>
<tr><td><code id="check_outcome_column_+3A_show_message">show_message</code></td>
<td>
<p>whether to show which column is being used as the output column (default: TRUE)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>outcome colname
</p>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
check_outcome_column(otu_small, NULL)
check_outcome_column(otu_small, "dx")

## End(Not run)
</code></pre>

<hr>
<h2 id='check_outcome_value'>Check that the outcome variable is valid. Pick outcome value if necessary.</h2><span id='topic+check_outcome_value'></span>

<h3>Description</h3>

<p>Check that the outcome variable is valid. Pick outcome value if necessary.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_outcome_value(dataset, outcome_colname)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_outcome_value_+3A_dataset">dataset</code></td>
<td>
<p>Data frame with an outcome variable and other columns as features.</p>
</td></tr>
<tr><td><code id="check_outcome_value_+3A_outcome_colname">outcome_colname</code></td>
<td>
<p>Column name as a string of the outcome variable
(default <code>NULL</code>; the first column will be chosen automatically).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>outcome value
</p>


<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>
<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
check_outcome_value(otu_small, "dx", "cancer")

## End(Not run)
</code></pre>

<hr>
<h2 id='check_packages_installed'>Check whether package(s) are installed</h2><span id='topic+check_packages_installed'></span>

<h3>Description</h3>

<p>Check whether package(s) are installed
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_packages_installed(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_packages_installed_+3A_...">...</code></td>
<td>
<p>names of packages to check</p>
</td></tr>
</table>


<h3>Value</h3>

<p>named vector with status of each packages; installed (<code>TRUE</code>) or not (<code>FALSE</code>)
</p>


<h3>Author(s)</h3>

<p>Kelly Sovacool <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>
<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
check_packages_installed("base")
check_packages_installed("not-a-package-name")
all(check_packages_installed("parallel", "doFuture"))

## End(Not run)
</code></pre>

<hr>
<h2 id='check_perf_metric_function'>Check perf_metric_function is NULL or a function</h2><span id='topic+check_perf_metric_function'></span>

<h3>Description</h3>

<p>Check perf_metric_function is NULL or a function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_perf_metric_function(perf_metric_function)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_perf_metric_function_+3A_perf_metric_function">perf_metric_function</code></td>
<td>
<p>performance metric function</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
check_perf_metric_function(NULL)

## End(Not run)
</code></pre>

<hr>
<h2 id='check_perf_metric_name'>Check perf_metric_name is NULL or a function</h2><span id='topic+check_perf_metric_name'></span>

<h3>Description</h3>

<p>Check perf_metric_name is NULL or a function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_perf_metric_name(perf_metric_name)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_perf_metric_name_+3A_perf_metric_name">perf_metric_name</code></td>
<td>
<p>performance metric function</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
check_perf_metric_name(NULL)

## End(Not run)
</code></pre>

<hr>
<h2 id='check_permute'>Check that permute is a logical</h2><span id='topic+check_permute'></span>

<h3>Description</h3>

<p>Check that permute is a logical
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_permute(permute)
</code></pre>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
do_permute &lt;- TRUE
check_permute(do_permute)

## End(Not run)
</code></pre>

<hr>
<h2 id='check_remove_var'>Check remove_var</h2><span id='topic+check_remove_var'></span>

<h3>Description</h3>

<p>Check remove_var
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_remove_var(remove_var)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_remove_var_+3A_remove_var">remove_var</code></td>
<td>
<p>Whether to remove variables with near-zero variance
(<code>'nzv'</code>; default), zero variance (<code>'zv'</code>), or none (<code>NULL</code>).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
check_remove_var(NULL)

## End(Not run)
</code></pre>

<hr>
<h2 id='check_seed'>check that the seed is either NA or a number</h2><span id='topic+check_seed'></span>

<h3>Description</h3>

<p>check that the seed is either NA or a number
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_seed(seed)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_seed_+3A_seed">seed</code></td>
<td>
<p>random seed</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
check_seed(2019)
check_seed(NULL)

## End(Not run)
</code></pre>

<hr>
<h2 id='check_training_frac'>Check that the training fraction is between 0 and 1</h2><span id='topic+check_training_frac'></span>

<h3>Description</h3>

<p>Check that the training fraction is between 0 and 1
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_training_frac(frac)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_training_frac_+3A_frac">frac</code></td>
<td>
<p>fraction (numeric)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
check_training_frac(0.8)

## End(Not run)
</code></pre>

<hr>
<h2 id='check_training_indices'>Check the validity of the training indices</h2><span id='topic+check_training_indices'></span>

<h3>Description</h3>

<p>Check the validity of the training indices
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_training_indices(training_inds, dataset)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_training_indices_+3A_training_inds">training_inds</code></td>
<td>
<p>vector of integers corresponding to samples for the training set</p>
</td></tr>
<tr><td><code id="check_training_indices_+3A_dataset">dataset</code></td>
<td>
<p>data frame containing the entire dataset</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
training_indices &lt;- otu_small %&gt;%
  nrow() %&gt;%
  sample(., size = 160)
check_training_indices(training_indices, otu_small)

## End(Not run)
</code></pre>

<hr>
<h2 id='cluster_corr_mat'>Cluster a matrix of correlated features</h2><span id='topic+cluster_corr_mat'></span>

<h3>Description</h3>

<p>Cluster a matrix of correlated features
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster_corr_mat(bin_corr_mat, hclust_method = "single", cut_height = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cluster_corr_mat_+3A_bin_corr_mat">bin_corr_mat</code></td>
<td>
<p>a binary correlation matrix created by <code>get_binary_corr_mat()</code>.</p>
</td></tr>
<tr><td><code id="cluster_corr_mat_+3A_hclust_method">hclust_method</code></td>
<td>
<p>the <code>method</code> to use in <code>stats::hclust()</code> (default: 'single').</p>
</td></tr>
<tr><td><code id="cluster_corr_mat_+3A_cut_height">cut_height</code></td>
<td>
<p>the cut height (<code>h</code>) to use in <code>stats::cutree()</code> (default: 0).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a named vector from <code>stats::cutree()</code>. Each element is a cluster and
the name is a feature in that cluster.
</p>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>
<p>Pat Schloss, <a href="mailto:pschloss@umich.edu">pschloss@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
corr_mat &lt;- matrix(
  data = c(1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1),
  nrow = 4,
  dimnames = list(
    c("a", "b", "c", "d"),
    c("a", "b", "c", "d")
  )
)
corr_mat
cluster_corr_mat(corr_mat)

## End(Not run)
</code></pre>

<hr>
<h2 id='collapse_correlated_features'>Collapse correlated features</h2><span id='topic+collapse_correlated_features'></span>

<h3>Description</h3>

<p>Collapse correlated features
</p>


<h3>Usage</h3>

<pre><code class='language-R'>collapse_correlated_features(features, group_neg_corr = TRUE, progbar = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="collapse_correlated_features_+3A_features">features</code></td>
<td>
<p>dataframe of features for machine learning</p>
</td></tr>
<tr><td><code id="collapse_correlated_features_+3A_group_neg_corr">group_neg_corr</code></td>
<td>
<p>Whether to group negatively correlated features
together (e.g. c(0,1) and c(1,0)).</p>
</td></tr>
<tr><td><code id="collapse_correlated_features_+3A_progbar">progbar</code></td>
<td>
<p>optional progress bar (default: <code>NULL</code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>features where perfectly correlated ones are collapsed
</p>


<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
collapse_correlated_features(mikropml::otu_small[, 2:ncol(otu_small)])

## End(Not run)
</code></pre>

<hr>
<h2 id='combine_hp_performance'>Combine hyperparameter performance metrics for multiple train/test splits</h2><span id='topic+combine_hp_performance'></span>

<h3>Description</h3>

<p>Combine hyperparameter performance metrics for multiple train/test splits generated by, for instance, <a href="http://www.schlosslab.org/mikropml/articles/parallel.html">looping in R</a> or using a <a href="https://github.com/SchlossLab/mikropml-snakemake-workflow">snakemake workflow</a> on a high-performance computer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>combine_hp_performance(trained_model_lst)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="combine_hp_performance_+3A_trained_model_lst">trained_model_lst</code></td>
<td>
<p>List of trained models.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Named list:
</p>

<ul>
<li> <p><code>dat</code>: Dataframe of performance metric for each group of hyperparameters
</p>
</li>
<li> <p><code>params</code>: Hyperparameters tuned.
</p>
</li>
<li> <p><code>Metric</code>: Performance metric used.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
results &lt;- lapply(seq(100, 102), function(seed) {
  run_ml(otu_small, "glmnet", seed = seed, cv_times = 2, kfold = 2)
})
models &lt;- lapply(results, function(x) x$trained_model)
combine_hp_performance(models)

## End(Not run)
</code></pre>

<hr>
<h2 id='compare_models'>Perform permutation tests to compare the performance metric
across all pairs of a group variable.</h2><span id='topic+compare_models'></span>

<h3>Description</h3>

<p>A wrapper for <code>permute_p_value()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compare_models(merged_data, metric, group_name, nperm = 10000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compare_models_+3A_merged_data">merged_data</code></td>
<td>
<p>the concatenated performance data from <code>run_ml</code></p>
</td></tr>
<tr><td><code id="compare_models_+3A_metric">metric</code></td>
<td>
<p>metric to compare, must be numeric</p>
</td></tr>
<tr><td><code id="compare_models_+3A_group_name">group_name</code></td>
<td>
<p>column with group variables to compare</p>
</td></tr>
<tr><td><code id="compare_models_+3A_nperm">nperm</code></td>
<td>
<p>number of permutations, default=10000</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a table of p-values for all pairs of group variable
</p>


<h3>Author(s)</h3>

<p>Courtney R Armour, <a href="mailto:armourc@umich.edu">armourc@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>df &lt;- dplyr::tibble(
  model = c("rf", "rf", "glmnet", "glmnet", "svmRadial", "svmRadial"),
  AUC = c(.2, 0.3, 0.8, 0.9, 0.85, 0.95)
)
set.seed(123)
compare_models(df, "AUC", "model", nperm = 10)
</code></pre>

<hr>
<h2 id='create_grouped_data_partition'>Split into train and test set while splitting by groups.
When <code>group_partitions</code> is <code>NULL</code>, all samples from each group will go into
either the training set or the testing set.
Otherwise, the groups will be split according to <code>group_partitions</code></h2><span id='topic+create_grouped_data_partition'></span>

<h3>Description</h3>

<p>Split into train and test set while splitting by groups.
When <code>group_partitions</code> is <code>NULL</code>, all samples from each group will go into
either the training set or the testing set.
Otherwise, the groups will be split according to <code>group_partitions</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_grouped_data_partition(
  groups,
  group_partitions = NULL,
  training_frac = 0.8
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_grouped_data_partition_+3A_groups">groups</code></td>
<td>
<p>Vector of groups to keep together when splitting the data into
train and test sets. If the number of groups in the training set is larger
than <code>kfold</code>, the groups will also be kept together for cross-validation.
Length matches the number of rows in the dataset (default: <code>NULL</code>).</p>
</td></tr>
<tr><td><code id="create_grouped_data_partition_+3A_group_partitions">group_partitions</code></td>
<td>
<p>Specify how to assign <code>groups</code> to the training and
testing partitions (default: <code>NULL</code>). If <code>groups</code> specifies that some
samples belong to group <code>"A"</code> and some belong to group <code>"B"</code>, then setting
<code>group_partitions = list(train = c("A", "B"), test = c("B"))</code> will result
in all samples from group <code>"A"</code> being placed in the training set, some
samples from <code>"B"</code> also in the training set, and the remaining samples from
<code>"B"</code> in the testing set. The partition sizes will be as close to
<code>training_frac</code> as possible. If the number of groups in the training set is
larger than <code>kfold</code>, the groups will also be kept together for
cross-validation.</p>
</td></tr>
<tr><td><code id="create_grouped_data_partition_+3A_training_frac">training_frac</code></td>
<td>
<p>Fraction of data for training set (default: <code>0.8</code>). Rows
from the dataset will be randomly selected for the training set, and all
remaining rows will be used in the testing set. Alternatively, if you
provide a vector of integers, these will be used as the row indices for the
training set. All remaining rows will be used in the testing set.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector of row indices for the training set
</p>


<h3>Author(s)</h3>

<p>Zena Lapp, zenalapp@umich.edu
</p>
<p>Kelly Sovacool, sovacool@umich.edu
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
groups &lt;- c("A", "B", "A", "B", "C", "C", "A", "A", "D")
set.seed(0)
create_grouped_data_partition(groups, training_frac = 0.8)
groups &lt;- rep.int(c("A", "B", "C"), 3)
create_grouped_data_partition(groups,
  group_partitions = list(train = c("A"), test = c("A", "B", "C"))
)

## End(Not run)
</code></pre>

<hr>
<h2 id='create_grouped_k_multifolds'>Splitting into folds for cross-validation when using groups</h2><span id='topic+create_grouped_k_multifolds'></span>

<h3>Description</h3>

<p>Like <a href="caret.html#topic+createMultiFolds">createMultiFolds</a> but still splitting by groups using <a href="caret.html#topic+groupKFold">groupKFold</a>. Code modified from <a href="caret.html#topic+createMultiFolds">createMultiFolds</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_grouped_k_multifolds(groups, kfold = 10, cv_times = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_grouped_k_multifolds_+3A_groups">groups</code></td>
<td>
<p>equivalent to y in caret::createMultiFolds</p>
</td></tr>
<tr><td><code id="create_grouped_k_multifolds_+3A_kfold">kfold</code></td>
<td>
<p>equivalent to k in caret::createMultiFolds</p>
</td></tr>
<tr><td><code id="create_grouped_k_multifolds_+3A_cv_times">cv_times</code></td>
<td>
<p>equivalent to cv_times in caret::createMultiFolds</p>
</td></tr>
</table>


<h3>Value</h3>

<p>indices of folds for CV
</p>


<h3>Author(s)</h3>

<p>Zena Lapp, zenalapp@umich.edu
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
set.seed(0)
groups &lt;- c("A", "B", "A", "B", "C", "C", "A", "A", "D")
folds &lt;- create_grouped_k_multifolds(groups, kfold = 2, cv_times = 2)

## End(Not run)
</code></pre>

<hr>
<h2 id='define_cv'>Define cross-validation scheme and training parameters</h2><span id='topic+define_cv'></span>

<h3>Description</h3>

<p>Define cross-validation scheme and training parameters
</p>


<h3>Usage</h3>

<pre><code class='language-R'>define_cv(
  train_data,
  outcome_colname,
  hyperparams_list,
  perf_metric_function,
  class_probs,
  kfold = 5,
  cv_times = 100,
  groups = NULL,
  group_partitions = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="define_cv_+3A_train_data">train_data</code></td>
<td>
<p>Dataframe for training model.</p>
</td></tr>
<tr><td><code id="define_cv_+3A_outcome_colname">outcome_colname</code></td>
<td>
<p>Column name as a string of the outcome variable
(default <code>NULL</code>; the first column will be chosen automatically).</p>
</td></tr>
<tr><td><code id="define_cv_+3A_hyperparams_list">hyperparams_list</code></td>
<td>
<p>Named list of lists of hyperparameters.</p>
</td></tr>
<tr><td><code id="define_cv_+3A_perf_metric_function">perf_metric_function</code></td>
<td>
<p>Function to calculate the performance metric to
be used for cross-validation and test performance. Some functions are
provided by caret (see <code><a href="caret.html#topic+postResample">caret::defaultSummary()</a></code>).
Defaults: binary classification = <code>twoClassSummary</code>,
multi-class classification = <code>multiClassSummary</code>,
regression = <code>defaultSummary</code>.</p>
</td></tr>
<tr><td><code id="define_cv_+3A_class_probs">class_probs</code></td>
<td>
<p>Whether to use class probabilities (TRUE for categorical outcomes, FALSE for numeric outcomes).</p>
</td></tr>
<tr><td><code id="define_cv_+3A_kfold">kfold</code></td>
<td>
<p>Fold number for k-fold cross-validation (default: <code>5</code>).</p>
</td></tr>
<tr><td><code id="define_cv_+3A_cv_times">cv_times</code></td>
<td>
<p>Number of cross-validation partitions to create (default: <code>100</code>).</p>
</td></tr>
<tr><td><code id="define_cv_+3A_groups">groups</code></td>
<td>
<p>Vector of groups to keep together when splitting the data into
train and test sets. If the number of groups in the training set is larger
than <code>kfold</code>, the groups will also be kept together for cross-validation.
Length matches the number of rows in the dataset (default: <code>NULL</code>).</p>
</td></tr>
<tr><td><code id="define_cv_+3A_group_partitions">group_partitions</code></td>
<td>
<p>Specify how to assign <code>groups</code> to the training and
testing partitions (default: <code>NULL</code>). If <code>groups</code> specifies that some
samples belong to group <code>"A"</code> and some belong to group <code>"B"</code>, then setting
<code>group_partitions = list(train = c("A", "B"), test = c("B"))</code> will result
in all samples from group <code>"A"</code> being placed in the training set, some
samples from <code>"B"</code> also in the training set, and the remaining samples from
<code>"B"</code> in the testing set. The partition sizes will be as close to
<code>training_frac</code> as possible. If the number of groups in the training set is
larger than <code>kfold</code>, the groups will also be kept together for
cross-validation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Caret object for trainControl that controls cross-validation
</p>


<h3>Author(s)</h3>

<p>Begüm Topçuoğlu, <a href="mailto:topcuoglu.begum@gmail.com">topcuoglu.begum@gmail.com</a>
</p>
<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>training_inds &lt;- get_partition_indices(otu_small %&gt;% dplyr::pull("dx"),
  training_frac = 0.8,
  groups = NULL
)
train_data &lt;- otu_small[training_inds, ]
test_data &lt;- otu_small[-training_inds, ]
cv &lt;- define_cv(train_data,
  outcome_colname = "dx",
  hyperparams_list = get_hyperparams_list(otu_small, "glmnet"),
  perf_metric_function = caret::multiClassSummary,
  class_probs = TRUE,
  kfold = 5
)
</code></pre>

<hr>
<h2 id='find_permuted_perf_metric'>Get permuted performance metric difference for a single feature
(or group of features)</h2><span id='topic+find_permuted_perf_metric'></span>

<h3>Description</h3>

<p>Requires the <code>future.apply</code> package
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_permuted_perf_metric(
  test_data,
  trained_model,
  outcome_colname,
  perf_metric_function,
  perf_metric_name,
  class_probs,
  feat,
  test_perf_value,
  nperms = 100,
  alpha = 0.05,
  progbar = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find_permuted_perf_metric_+3A_test_data">test_data</code></td>
<td>
<p>Held out test data: dataframe of outcome and features.</p>
</td></tr>
<tr><td><code id="find_permuted_perf_metric_+3A_trained_model">trained_model</code></td>
<td>
<p>Trained model from <code><a href="caret.html#topic+train">caret::train()</a></code>.</p>
</td></tr>
<tr><td><code id="find_permuted_perf_metric_+3A_outcome_colname">outcome_colname</code></td>
<td>
<p>Column name as a string of the outcome variable
(default <code>NULL</code>; the first column will be chosen automatically).</p>
</td></tr>
<tr><td><code id="find_permuted_perf_metric_+3A_perf_metric_function">perf_metric_function</code></td>
<td>
<p>Function to calculate the performance metric to
be used for cross-validation and test performance. Some functions are
provided by caret (see <code><a href="caret.html#topic+postResample">caret::defaultSummary()</a></code>).
Defaults: binary classification = <code>twoClassSummary</code>,
multi-class classification = <code>multiClassSummary</code>,
regression = <code>defaultSummary</code>.</p>
</td></tr>
<tr><td><code id="find_permuted_perf_metric_+3A_perf_metric_name">perf_metric_name</code></td>
<td>
<p>The column name from the output of the function
provided to perf_metric_function that is to be used as the performance metric.
Defaults: binary classification = <code>"ROC"</code>,
multi-class classification = <code>"logLoss"</code>,
regression = <code>"RMSE"</code>.</p>
</td></tr>
<tr><td><code id="find_permuted_perf_metric_+3A_class_probs">class_probs</code></td>
<td>
<p>Whether to use class probabilities (TRUE for categorical outcomes, FALSE for numeric outcomes).</p>
</td></tr>
<tr><td><code id="find_permuted_perf_metric_+3A_feat">feat</code></td>
<td>
<p>feature or group of correlated features to permute.</p>
</td></tr>
<tr><td><code id="find_permuted_perf_metric_+3A_test_perf_value">test_perf_value</code></td>
<td>
<p>value of the true performance metric on the held-out
test data.</p>
</td></tr>
<tr><td><code id="find_permuted_perf_metric_+3A_nperms">nperms</code></td>
<td>
<p>number of permutations to perform (default: <code>100</code>).</p>
</td></tr>
<tr><td><code id="find_permuted_perf_metric_+3A_alpha">alpha</code></td>
<td>
<p>alpha level for the confidence interval
(default: <code>0.05</code> to obtain a 95% confidence interval)</p>
</td></tr>
<tr><td><code id="find_permuted_perf_metric_+3A_progbar">progbar</code></td>
<td>
<p>optional progress bar (default: <code>NULL</code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector of mean permuted performance and mean difference between test
and permuted performance (test minus permuted performance)
</p>


<h3>Author(s)</h3>

<p>Begüm Topçuoğlu, <a href="mailto:topcuoglu.begum@gmail.com">topcuoglu.begum@gmail.com</a>
</p>
<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>
<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>

<hr>
<h2 id='flatten_corr_mat'>Flatten correlation matrix to pairs</h2><span id='topic+flatten_corr_mat'></span>

<h3>Description</h3>

<p>Flatten correlation matrix to pairs
</p>


<h3>Usage</h3>

<pre><code class='language-R'>flatten_corr_mat(cormat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="flatten_corr_mat_+3A_cormat">cormat</code></td>
<td>
<p>correlation matrix computed with stats::cor</p>
</td></tr>
</table>


<h3>Value</h3>

<p>flattened correlation matrix (pairs of features their correlation)
</p>


<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>

<hr>
<h2 id='get_binary_corr_mat'>Identify correlated features as a binary matrix</h2><span id='topic+get_binary_corr_mat'></span>

<h3>Description</h3>

<p>Identify correlated features as a binary matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_binary_corr_mat(
  features,
  corr_thresh = 1,
  group_neg_corr = TRUE,
  corr_method = "spearman"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_binary_corr_mat_+3A_features">features</code></td>
<td>
<p>a dataframe with each column as a feature for ML</p>
</td></tr>
<tr><td><code id="get_binary_corr_mat_+3A_corr_thresh">corr_thresh</code></td>
<td>
<p>For feature importance, group correlations
above or equal to <code>corr_thresh</code> (range <code>0</code> to <code>1</code>; default: <code>1</code>).</p>
</td></tr>
<tr><td><code id="get_binary_corr_mat_+3A_group_neg_corr">group_neg_corr</code></td>
<td>
<p>Whether to group negatively correlated features
together (e.g. c(0,1) and c(1,0)).</p>
</td></tr>
<tr><td><code id="get_binary_corr_mat_+3A_corr_method">corr_method</code></td>
<td>
<p>correlation method. options or the same as those supported
by <code>stats::cor</code>: spearman, pearson, kendall. (default: spearman)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A binary matrix of correlated features
</p>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
features &lt;- data.frame(
  a = 1:3, b = 2:4, c = c(1, 0, 1),
  d = (5:7), e = c(5, 1, 4)
)
get_binary_corr_mat(features)

## End(Not run)
</code></pre>

<hr>
<h2 id='get_caret_dummyvars_df'>Get dummyvars dataframe (i.e. design matrix)</h2><span id='topic+get_caret_dummyvars_df'></span>

<h3>Description</h3>

<p>Get dummyvars dataframe (i.e. design matrix)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_caret_dummyvars_df(features, full_rank = FALSE, progbar = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_caret_dummyvars_df_+3A_features">features</code></td>
<td>
<p>dataframe of features for machine learning</p>
</td></tr>
<tr><td><code id="get_caret_dummyvars_df_+3A_full_rank">full_rank</code></td>
<td>
<p>whether matrix should be full rank or not (see '<a href="caret.html#topic+dummyVars">caret::dummyVars</a>)</p>
</td></tr>
<tr><td><code id="get_caret_dummyvars_df_+3A_progbar">progbar</code></td>
<td>
<p>optional progress bar (default: <code>NULL</code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>design matrix
</p>


<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
df &lt;- data.frame(
  outcome = c("normal", "normal", "cancer"),
  var1 = 1:3,
  var2 = c("a", "b", "c"),
  var3 = c("no", "yes", "no"),
  var4 = c(0, 1, 0)
)
get_caret_dummyvars_df(df, TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='get_caret_processed_df'>Get preprocessed dataframe for continuous variables</h2><span id='topic+get_caret_processed_df'></span>

<h3>Description</h3>

<p>Get preprocessed dataframe for continuous variables
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_caret_processed_df(features, method)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_caret_processed_df_+3A_features">features</code></td>
<td>
<p>Dataframe of features for machine learning</p>
</td></tr>
<tr><td><code id="get_caret_processed_df_+3A_method">method</code></td>
<td>
<p>Methods to preprocess the data, described in
<code><a href="caret.html#topic+preProcess">caret::preProcess()</a></code> (default: <code>c("center","scale")</code>, use <code>NULL</code> for
no normalization).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Named list:
</p>

<ul>
<li> <p><code>processed</code>: Dataframe of processed features.
</p>
</li>
<li> <p><code>removed</code>: Names of any features removed during preprocessing.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>get_caret_processed_df(mikropml::otu_small[, 2:ncol(otu_small)], c("center", "scale"))
</code></pre>

<hr>
<h2 id='get_corr_feats'>Identify correlated features</h2><span id='topic+get_corr_feats'></span>

<h3>Description</h3>

<p>Identify correlated features
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_corr_feats(
  features,
  corr_thresh = 1,
  group_neg_corr = TRUE,
  corr_method = "spearman"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_corr_feats_+3A_features">features</code></td>
<td>
<p>a dataframe with each column as a feature for ML</p>
</td></tr>
<tr><td><code id="get_corr_feats_+3A_corr_thresh">corr_thresh</code></td>
<td>
<p>For feature importance, group correlations
above or equal to <code>corr_thresh</code> (range <code>0</code> to <code>1</code>; default: <code>1</code>).</p>
</td></tr>
<tr><td><code id="get_corr_feats_+3A_group_neg_corr">group_neg_corr</code></td>
<td>
<p>Whether to group negatively correlated features
together (e.g. c(0,1) and c(1,0)).</p>
</td></tr>
<tr><td><code id="get_corr_feats_+3A_corr_method">corr_method</code></td>
<td>
<p>correlation method. options or the same as those supported
by <code>stats::cor</code>: spearman, pearson, kendall. (default: spearman)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Dataframe of correlated features where the columns are feature1,
feature2, and the correlation between those two features
(anything exceeding corr_thresh).
</p>


<h3>Author(s)</h3>

<p>Begüm Topçuoğlu, <a href="mailto:topcuoglu.begum@gmail.com">topcuoglu.begum@gmail.com</a>
</p>
<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>

<hr>
<h2 id='get_difference'>Calculate the difference in the mean of the metric for two groups</h2><span id='topic+get_difference'></span>

<h3>Description</h3>

<p>Calculate the difference in the mean of the metric for two groups
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_difference(sub_data, group_name, metric)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_difference_+3A_sub_data">sub_data</code></td>
<td>
<p>subset of the merged performance data frame for two groups</p>
</td></tr>
<tr><td><code id="get_difference_+3A_group_name">group_name</code></td>
<td>
<p>name of column with group variable</p>
</td></tr>
<tr><td><code id="get_difference_+3A_metric">metric</code></td>
<td>
<p>metric to compare</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric difference in the average metric between the two groups
</p>


<h3>Author(s)</h3>

<p>Courtney Armour, <a href="mailto:armourc@umich.edu">armourc@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
df &lt;- dplyr::tibble(
  condition = c("a", "a", "b", "b"),
  AUC = c(.2, 0.3, 0.8, 0.9)
)
get_difference(df, "condition", "AUC")

## End(Not run)
</code></pre>

<hr>
<h2 id='get_feature_importance'>Get feature importance using the permutation method</h2><span id='topic+get_feature_importance'></span>

<h3>Description</h3>

<p>Calculates feature importance using a trained model and test data. Requires
the <code>future.apply</code> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_feature_importance(
  trained_model,
  test_data,
  outcome_colname,
  perf_metric_function,
  perf_metric_name,
  class_probs,
  method,
  seed = NA,
  corr_thresh = 1,
  groups = NULL,
  nperms = 100,
  corr_method = "spearman"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_feature_importance_+3A_trained_model">trained_model</code></td>
<td>
<p>Trained model from <code><a href="caret.html#topic+train">caret::train()</a></code>.</p>
</td></tr>
<tr><td><code id="get_feature_importance_+3A_test_data">test_data</code></td>
<td>
<p>Held out test data: dataframe of outcome and features.</p>
</td></tr>
<tr><td><code id="get_feature_importance_+3A_outcome_colname">outcome_colname</code></td>
<td>
<p>Column name as a string of the outcome variable
(default <code>NULL</code>; the first column will be chosen automatically).</p>
</td></tr>
<tr><td><code id="get_feature_importance_+3A_perf_metric_function">perf_metric_function</code></td>
<td>
<p>Function to calculate the performance metric to
be used for cross-validation and test performance. Some functions are
provided by caret (see <code><a href="caret.html#topic+postResample">caret::defaultSummary()</a></code>).
Defaults: binary classification = <code>twoClassSummary</code>,
multi-class classification = <code>multiClassSummary</code>,
regression = <code>defaultSummary</code>.</p>
</td></tr>
<tr><td><code id="get_feature_importance_+3A_perf_metric_name">perf_metric_name</code></td>
<td>
<p>The column name from the output of the function
provided to perf_metric_function that is to be used as the performance metric.
Defaults: binary classification = <code>"ROC"</code>,
multi-class classification = <code>"logLoss"</code>,
regression = <code>"RMSE"</code>.</p>
</td></tr>
<tr><td><code id="get_feature_importance_+3A_class_probs">class_probs</code></td>
<td>
<p>Whether to use class probabilities (TRUE for categorical outcomes, FALSE for numeric outcomes).</p>
</td></tr>
<tr><td><code id="get_feature_importance_+3A_method">method</code></td>
<td>
<p>ML method.
Options: <code>c("glmnet", "rf", "rpart2", "svmRadial", "xgbTree")</code>.
</p>

<ul>
<li><p> glmnet: linear, logistic, or multiclass regression
</p>
</li>
<li><p> rf: random forest
</p>
</li>
<li><p> rpart2: decision tree
</p>
</li>
<li><p> svmRadial: support vector machine
</p>
</li>
<li><p> xgbTree: xgboost
</p>
</li></ul>
</td></tr>
<tr><td><code id="get_feature_importance_+3A_seed">seed</code></td>
<td>
<p>Random seed (default: <code>NA</code>).
Your results will only be reproducible if you set a seed.</p>
</td></tr>
<tr><td><code id="get_feature_importance_+3A_corr_thresh">corr_thresh</code></td>
<td>
<p>For feature importance, group correlations
above or equal to <code>corr_thresh</code> (range <code>0</code> to <code>1</code>; default: <code>1</code>).</p>
</td></tr>
<tr><td><code id="get_feature_importance_+3A_groups">groups</code></td>
<td>
<p>Vector of feature names to group together during permutation.
Each element should be a string with feature names separated by a pipe
character (<code>|</code>). If this is <code>NULL</code> (default), correlated features will be
grouped together based on <code>corr_thresh</code>.</p>
</td></tr>
<tr><td><code id="get_feature_importance_+3A_nperms">nperms</code></td>
<td>
<p>number of permutations to perform (default: <code>100</code>).</p>
</td></tr>
<tr><td><code id="get_feature_importance_+3A_corr_method">corr_method</code></td>
<td>
<p>correlation method. options or the same as those supported
by <code>stats::cor</code>: spearman, pearson, kendall. (default: spearman)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For permutation tests, the p-value is the number of permutation statistics
that are greater than the test statistic, divided by the number of
permutations. In our case, the permutation statistic is the model performance
(e.g. AUROC) after randomizing the order of observations for one feature, and
the test statistic is the actual performance on the test data. By default we
perform 100 permutations per feature; increasing this will increase the
precision of estimating the null distribution, but also increases runtime.
The p-value represents the probability of obtaining the actual performance in
the event that the null hypothesis is true, where the null hypothesis is that
the feature is not important for model performance.
</p>
<p>We strongly recommend providing multiple cores to speed up computation time.
See <a href="http://www.schlosslab.org/mikropml/articles/parallel.html">our vignette on parallel processing</a>
for more details.
</p>


<h3>Value</h3>

<p>Data frame with performance metrics for when each feature (or group
of correlated features; <code>feat</code>) is permuted (<code>perf_metric</code>), differences
between the actual test performance metric on and the permuted performance
metric (<code>perf_metric_diff</code>; test minus permuted performance), and the
p-value (<code>pvalue</code>: the probability of obtaining the actual performance
value under the null hypothesis). Features with a larger <code>perf_metric_diff</code>
are more important. The performance metric name (<code>perf_metric_name</code>) and
seed (<code>seed</code>) are also returned.
</p>


<h3>Author(s)</h3>

<p>Begüm Topçuoğlu, <a href="mailto:topcuoglu.begum@gmail.com">topcuoglu.begum@gmail.com</a>
</p>
<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>
<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# If you called `run_ml()` with `feature_importance = FALSE` (the default),
# you can use `get_feature_importance()` later as long as you have the
# trained model and test data.
results &lt;- run_ml(otu_small, "glmnet", kfold = 2, cv_times = 2)
names(results$trained_model$trainingData)[1] &lt;- "dx"
feat_imp &lt;- get_feature_importance(results$trained_model,
  results$trained_model$trainingData,
  results$test_data,
  "dx",
  multiClassSummary,
  "AUC",
  class_probs = TRUE,
  method = "glmnet"
)

# We strongly recommend providing multiple cores to speed up computation time.
# Do this before calling `get_feature_importance()`.
doFuture::registerDoFuture()
future::plan(future::multicore, workers = 2)

# Optionally, you can group features together with a custom grouping
feat_imp &lt;- get_feature_importance(results$trained_model,
  results$trained_model$trainingData,
  results$test_data,
  "dx",
  multiClassSummary,
  "AUC",
  class_probs = TRUE,
  method = "glmnet",
  groups = c(
    "Otu00007", "Otu00008", "Otu00009", "Otu00011", "Otu00012",
    "Otu00015", "Otu00016", "Otu00018", "Otu00019", "Otu00020", "Otu00022",
    "Otu00023", "Otu00025", "Otu00028", "Otu00029", "Otu00030", "Otu00035",
    "Otu00036", "Otu00037", "Otu00038", "Otu00039", "Otu00040", "Otu00047",
    "Otu00050", "Otu00052", "Otu00054", "Otu00055", "Otu00056", "Otu00060",
    "Otu00003|Otu00002|Otu00005|Otu00024|Otu00032|Otu00041|Otu00053",
    "Otu00014|Otu00021|Otu00017|Otu00031|Otu00057",
    "Otu00013|Otu00006", "Otu00026|Otu00001|Otu00034|Otu00048",
    "Otu00033|Otu00010",
    "Otu00042|Otu00004", "Otu00043|Otu00027|Otu00049", "Otu00051|Otu00045",
    "Otu00058|Otu00044", "Otu00059|Otu00046"
  )
)

# the function can show a progress bar if you have the `progressr` package installed.
## optionally, specify the progress bar format:
progressr::handlers(progressr::handler_progress(
  format = ":message :bar :percent | elapsed: :elapsed | eta: :eta",
  clear = FALSE,
  show_after = 0
))
## tell progressr to always report progress
progressr::handlers(global = TRUE)
## run the function and watch the live progress udpates
feat_imp &lt;- get_feature_importance(results$trained_model,
  results$trained_model$trainingData,
  results$test_data,
  "dx",
  multiClassSummary,
  "AUC",
  class_probs = TRUE,
  method = "glmnet"
)

# You can specify any correlation method supported by `stats::cor`:
feat_imp &lt;- get_feature_importance(results$trained_model,
  results$trained_model$trainingData,
  results$test_data,
  "dx",
  multiClassSummary,
  "AUC",
  class_probs = TRUE,
  method = "glmnet",
  corr_method = "pearson"
)

## End(Not run)

</code></pre>

<hr>
<h2 id='get_groups_from_clusters'>Assign features to groups</h2><span id='topic+get_groups_from_clusters'></span>

<h3>Description</h3>

<p>Assign features to groups
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_groups_from_clusters(cluster_ids)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_groups_from_clusters_+3A_cluster_ids">cluster_ids</code></td>
<td>
<p>named vector created by <code>cluster_corr_mat()</code>.
Each element is a cluster and the name is a feature in that cluster.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector where each element is a group of correlated features
separated by pipes (<code>|</code>)
</p>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
corr_mat &lt;- matrix(
  data = c(1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1),
  nrow = 4,
  dimnames = list(
    c("a", "b", "c", "d"),
    c("a", "b", "c", "d")
  )
)
corr_mat
get_groups_from_clusters(cluster_corr_mat(corr_mat))

## End(Not run)
</code></pre>

<hr>
<h2 id='get_hp_performance'>Get hyperparameter performance metrics</h2><span id='topic+get_hp_performance'></span>

<h3>Description</h3>

<p>Get hyperparameter performance metrics
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_hp_performance(trained_model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_hp_performance_+3A_trained_model">trained_model</code></td>
<td>
<p>trained model (e.g. from <code>run_ml()</code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Named list:
</p>

<ul>
<li> <p><code>dat</code>: Dataframe of performance metric for each group of hyperparameters.
</p>
</li>
<li> <p><code>params</code>: Hyperparameters tuned.
</p>
</li>
<li> <p><code>metric</code>: Performance metric used.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>
<p>Kelly Sovacool <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>get_hp_performance(otu_mini_bin_results_glmnet$trained_model)
</code></pre>

<hr>
<h2 id='get_hyperparams_from_df'>Split hyperparameters dataframe into named lists for each parameter</h2><span id='topic+get_hyperparams_from_df'></span>

<h3>Description</h3>

<p>Using <code><a href="#topic+get_hyperparams_list">get_hyperparams_list</a></code> is preferred over this function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_hyperparams_from_df(hyperparams_df, ml_method)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_hyperparams_from_df_+3A_hyperparams_df">hyperparams_df</code></td>
<td>
<p>dataframe of hyperparameters with columns <code>param</code>, <code>value</code>, and <code>method</code></p>
</td></tr>
<tr><td><code id="get_hyperparams_from_df_+3A_ml_method">ml_method</code></td>
<td>
<p>machine learning method</p>
</td></tr>
</table>


<h3>Value</h3>

<p>named list of lists of hyperparameters
</p>


<h3>Author(s)</h3>

<p>Begüm Topçuoğlu, <a href="mailto:topcuoglu.begum@gmail.com">topcuoglu.begum@gmail.com</a>
</p>
<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
hparams_df &lt;- dplyr::tibble(
  param = c("alpha", "lambda", "lambda"),
  value = c(1, 0, 1),
  method = rep("glmnet", 3)
)
get_hyperparams_from_df(hparams_df, "glmnet")

## End(Not run)
</code></pre>

<hr>
<h2 id='get_hyperparams_list'>Set hyperparameters based on ML method and dataset characteristics</h2><span id='topic+get_hyperparams_list'></span>

<h3>Description</h3>

<p>For more details see the vignette on <a href="http://www.schlosslab.org/mikropml/articles/tuning.html">hyperparameter tuning</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_hyperparams_list(dataset, method)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_hyperparams_list_+3A_dataset">dataset</code></td>
<td>
<p>Data frame with an outcome variable and other columns as features.</p>
</td></tr>
<tr><td><code id="get_hyperparams_list_+3A_method">method</code></td>
<td>
<p>ML method.
Options: <code>c("glmnet", "rf", "rpart2", "svmRadial", "xgbTree")</code>.
</p>

<ul>
<li><p> glmnet: linear, logistic, or multiclass regression
</p>
</li>
<li><p> rf: random forest
</p>
</li>
<li><p> rpart2: decision tree
</p>
</li>
<li><p> svmRadial: support vector machine
</p>
</li>
<li><p> xgbTree: xgboost
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>Named list of hyperparameters.
</p>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>get_hyperparams_list(otu_mini_bin, "rf")
get_hyperparams_list(otu_small, "rf")
get_hyperparams_list(otu_mini_bin, "rpart2")
get_hyperparams_list(otu_small, "rpart2")
</code></pre>

<hr>
<h2 id='get_outcome_type'>Get outcome type.</h2><span id='topic+get_outcome_type'></span>

<h3>Description</h3>

<p>If the outcome is numeric, the type is continuous.
Otherwise, the outcome type is binary if there are only two outcomes or
multiclass if there are more than two outcomes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_outcome_type(outcomes_vec)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_outcome_type_+3A_outcomes_vec">outcomes_vec</code></td>
<td>
<p>Vector of outcomes.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Outcome type (continuous, binary, or multiclass).
</p>


<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>get_outcome_type(c(1, 2, 1))
get_outcome_type(c("a", "b", "b"))
get_outcome_type(c("a", "b", "c"))
</code></pre>

<hr>
<h2 id='get_partition_indices'>Select indices to partition the data into training &amp; testing sets.</h2><span id='topic+get_partition_indices'></span>

<h3>Description</h3>

<p>Use this function to get the row indices for the training set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_partition_indices(
  outcomes,
  training_frac = 0.8,
  groups = NULL,
  group_partitions = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_partition_indices_+3A_outcomes">outcomes</code></td>
<td>
<p>vector of outcomes</p>
</td></tr>
<tr><td><code id="get_partition_indices_+3A_training_frac">training_frac</code></td>
<td>
<p>Fraction of data for training set (default: <code>0.8</code>). Rows
from the dataset will be randomly selected for the training set, and all
remaining rows will be used in the testing set. Alternatively, if you
provide a vector of integers, these will be used as the row indices for the
training set. All remaining rows will be used in the testing set.</p>
</td></tr>
<tr><td><code id="get_partition_indices_+3A_groups">groups</code></td>
<td>
<p>Vector of groups to keep together when splitting the data into
train and test sets. If the number of groups in the training set is larger
than <code>kfold</code>, the groups will also be kept together for cross-validation.
Length matches the number of rows in the dataset (default: <code>NULL</code>).</p>
</td></tr>
<tr><td><code id="get_partition_indices_+3A_group_partitions">group_partitions</code></td>
<td>
<p>Specify how to assign <code>groups</code> to the training and
testing partitions (default: <code>NULL</code>). If <code>groups</code> specifies that some
samples belong to group <code>"A"</code> and some belong to group <code>"B"</code>, then setting
<code>group_partitions = list(train = c("A", "B"), test = c("B"))</code> will result
in all samples from group <code>"A"</code> being placed in the training set, some
samples from <code>"B"</code> also in the training set, and the remaining samples from
<code>"B"</code> in the testing set. The partition sizes will be as close to
<code>training_frac</code> as possible. If the number of groups in the training set is
larger than <code>kfold</code>, the groups will also be kept together for
cross-validation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>groups</code> is <code>NULL</code>, uses <a href="caret.html#topic+createDataPartition">createDataPartition</a>.
Otherwise, uses <code>create_grouped_data_partition()</code>.
</p>
<p>Set the seed prior to calling this function if you would like your data
partitions to be reproducible (recommended).
</p>


<h3>Value</h3>

<p>Vector of row indices for the training set.
</p>


<h3>Author(s)</h3>

<p>Kelly Sovacool, sovacool@umich.edu
</p>


<h3>Examples</h3>

<pre><code class='language-R'>training_inds &lt;- get_partition_indices(otu_mini_bin$dx)
train_data &lt;- otu_mini_bin[training_inds, ]
test_data &lt;- otu_mini_bin[-training_inds, ]
</code></pre>

<hr>
<h2 id='get_perf_metric_fn'>Get default performance metric function</h2><span id='topic+get_perf_metric_fn'></span>

<h3>Description</h3>

<p>Get default performance metric function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_perf_metric_fn(outcome_type)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_perf_metric_fn_+3A_outcome_type">outcome_type</code></td>
<td>
<p>Type of outcome (one of: <code>"continuous"</code>,<code>"binary"</code>,<code>"multiclass"</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Performance metric function.
</p>


<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>get_perf_metric_fn("continuous")
get_perf_metric_fn("binary")
get_perf_metric_fn("multiclass")
</code></pre>

<hr>
<h2 id='get_perf_metric_name'>Get default performance metric name</h2><span id='topic+get_perf_metric_name'></span>

<h3>Description</h3>

<p>Get default performance metric name for cross-validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_perf_metric_name(outcome_type)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_perf_metric_name_+3A_outcome_type">outcome_type</code></td>
<td>
<p>Type of outcome (one of: <code>"continuous"</code>,<code>"binary"</code>,<code>"multiclass"</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Performance metric name.
</p>


<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>get_perf_metric_name("continuous")
get_perf_metric_name("binary")
get_perf_metric_name("multiclass")
</code></pre>

<hr>
<h2 id='get_performance_tbl'>Get model performance metrics as a one-row tibble</h2><span id='topic+get_performance_tbl'></span>

<h3>Description</h3>

<p>Get model performance metrics as a one-row tibble
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_performance_tbl(
  trained_model,
  test_data,
  outcome_colname,
  perf_metric_function,
  perf_metric_name,
  class_probs,
  method,
  seed = NA
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_performance_tbl_+3A_trained_model">trained_model</code></td>
<td>
<p>Trained model from <code><a href="caret.html#topic+train">caret::train()</a></code>.</p>
</td></tr>
<tr><td><code id="get_performance_tbl_+3A_test_data">test_data</code></td>
<td>
<p>Held out test data: dataframe of outcome and features.</p>
</td></tr>
<tr><td><code id="get_performance_tbl_+3A_outcome_colname">outcome_colname</code></td>
<td>
<p>Column name as a string of the outcome variable
(default <code>NULL</code>; the first column will be chosen automatically).</p>
</td></tr>
<tr><td><code id="get_performance_tbl_+3A_perf_metric_function">perf_metric_function</code></td>
<td>
<p>Function to calculate the performance metric to
be used for cross-validation and test performance. Some functions are
provided by caret (see <code><a href="caret.html#topic+postResample">caret::defaultSummary()</a></code>).
Defaults: binary classification = <code>twoClassSummary</code>,
multi-class classification = <code>multiClassSummary</code>,
regression = <code>defaultSummary</code>.</p>
</td></tr>
<tr><td><code id="get_performance_tbl_+3A_perf_metric_name">perf_metric_name</code></td>
<td>
<p>The column name from the output of the function
provided to perf_metric_function that is to be used as the performance metric.
Defaults: binary classification = <code>"ROC"</code>,
multi-class classification = <code>"logLoss"</code>,
regression = <code>"RMSE"</code>.</p>
</td></tr>
<tr><td><code id="get_performance_tbl_+3A_class_probs">class_probs</code></td>
<td>
<p>Whether to use class probabilities (TRUE for categorical outcomes, FALSE for numeric outcomes).</p>
</td></tr>
<tr><td><code id="get_performance_tbl_+3A_method">method</code></td>
<td>
<p>ML method.
Options: <code>c("glmnet", "rf", "rpart2", "svmRadial", "xgbTree")</code>.
</p>

<ul>
<li><p> glmnet: linear, logistic, or multiclass regression
</p>
</li>
<li><p> rf: random forest
</p>
</li>
<li><p> rpart2: decision tree
</p>
</li>
<li><p> svmRadial: support vector machine
</p>
</li>
<li><p> xgbTree: xgboost
</p>
</li></ul>
</td></tr>
<tr><td><code id="get_performance_tbl_+3A_seed">seed</code></td>
<td>
<p>Random seed (default: <code>NA</code>).
Your results will only be reproducible if you set a seed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A one-row tibble with a column for the cross-validation performance,
columns for each of the performance metrics for the test data,
plus the <code>method</code>, and <code>seed</code>.
</p>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>
<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
results &lt;- run_ml(otu_small, "glmnet", kfold = 2, cv_times = 2)
names(results$trained_model$trainingData)[1] &lt;- "dx"
get_performance_tbl(results$trained_model, results$test_data,
  "dx",
  multiClassSummary, "AUC",
  class_probs = TRUE,
  method = "glmnet"
)

## End(Not run)

</code></pre>

<hr>
<h2 id='get_seeds_trainControl'>Get seeds for <code>caret::trainControl()</code></h2><span id='topic+get_seeds_trainControl'></span>

<h3>Description</h3>

<p>Adapted from <a href="https://stackoverflow.com/a/32598959">this Stack Overflow post</a>
and the <a href="caret.html#topic+trainControl">trainControl</a> documentation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_seeds_trainControl(hyperparams_list, kfold, cv_times, ncol_train)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_seeds_trainControl_+3A_hyperparams_list">hyperparams_list</code></td>
<td>
<p>Named list of lists of hyperparameters.</p>
</td></tr>
<tr><td><code id="get_seeds_trainControl_+3A_kfold">kfold</code></td>
<td>
<p>Fold number for k-fold cross-validation (default: <code>5</code>).</p>
</td></tr>
<tr><td><code id="get_seeds_trainControl_+3A_cv_times">cv_times</code></td>
<td>
<p>Number of cross-validation partitions to create (default: <code>100</code>).</p>
</td></tr>
<tr><td><code id="get_seeds_trainControl_+3A_ncol_train">ncol_train</code></td>
<td>
<p>number of columns in training data</p>
</td></tr>
</table>


<h3>Value</h3>

<p>seeds for <code>caret::trainControl()</code>
</p>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>
<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
get_seeds_trainControl(
  get_hyperparams_list(otu_small, "glmnet"),
  5, 100, 60
)

## End(Not run)
</code></pre>

<hr>
<h2 id='get_tuning_grid'>Generate the tuning grid for tuning hyperparameters</h2><span id='topic+get_tuning_grid'></span>

<h3>Description</h3>

<p>Generate the tuning grid for tuning hyperparameters
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_tuning_grid(hyperparams_list, method)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_tuning_grid_+3A_hyperparams_list">hyperparams_list</code></td>
<td>
<p>Named list of lists of hyperparameters.</p>
</td></tr>
<tr><td><code id="get_tuning_grid_+3A_method">method</code></td>
<td>
<p>ML method.
Options: <code>c("glmnet", "rf", "rpart2", "svmRadial", "xgbTree")</code>.
</p>

<ul>
<li><p> glmnet: linear, logistic, or multiclass regression
</p>
</li>
<li><p> rf: random forest
</p>
</li>
<li><p> rpart2: decision tree
</p>
</li>
<li><p> svmRadial: support vector machine
</p>
</li>
<li><p> xgbTree: xgboost
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>The tuning grid.
</p>


<h3>Author(s)</h3>

<p>Begüm Topçuoğlu, <a href="mailto:topcuoglu.begum@gmail.com">topcuoglu.begum@gmail.com</a>
</p>
<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ml_method &lt;- "glmnet"
hparams_list &lt;- get_hyperparams_list(otu_small, ml_method)
get_tuning_grid(hparams_list, ml_method)
</code></pre>

<hr>
<h2 id='group_correlated_features'>Group correlated features</h2><span id='topic+group_correlated_features'></span>

<h3>Description</h3>

<p>Group correlated features
</p>


<h3>Usage</h3>

<pre><code class='language-R'>group_correlated_features(
  features,
  corr_thresh = 1,
  group_neg_corr = TRUE,
  corr_method = "spearman"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="group_correlated_features_+3A_features">features</code></td>
<td>
<p>a dataframe with each column as a feature for ML</p>
</td></tr>
<tr><td><code id="group_correlated_features_+3A_corr_thresh">corr_thresh</code></td>
<td>
<p>For feature importance, group correlations
above or equal to <code>corr_thresh</code> (range <code>0</code> to <code>1</code>; default: <code>1</code>).</p>
</td></tr>
<tr><td><code id="group_correlated_features_+3A_group_neg_corr">group_neg_corr</code></td>
<td>
<p>Whether to group negatively correlated features
together (e.g. c(0,1) and c(1,0)).</p>
</td></tr>
<tr><td><code id="group_correlated_features_+3A_corr_method">corr_method</code></td>
<td>
<p>correlation method. options or the same as those supported
by <code>stats::cor</code>: spearman, pearson, kendall. (default: spearman)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector where each element is a group of correlated features
separated by pipes (<code>|</code>)
</p>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>features &lt;- data.frame(
  a = 1:3, b = 2:4, c = c(1, 0, 1),
  d = (5:7), e = c(5, 1, 4), f = c(-1, 0, -1)
)
group_correlated_features(features)
</code></pre>

<hr>
<h2 id='is_whole_number'>Check whether a numeric vector contains whole numbers.</h2><span id='topic+is_whole_number'></span>

<h3>Description</h3>

<p>Because <code>is.integer</code> checks for the class, <em>not</em> whether the number is an
integer in the mathematical sense.
This code was copy-pasted from the <code>is.integer</code> docs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_whole_number(x, tol = .Machine$double.eps^0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is_whole_number_+3A_x">x</code></td>
<td>
<p>numeric vector</p>
</td></tr>
<tr><td><code id="is_whole_number_+3A_tol">tol</code></td>
<td>
<p>tolerance (default: <code>.Machine$double.eps^0.5</code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>logical vector
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
is_whole_number(c(1, 2, 3))
is.integer(c(1, 2, 3))
is_whole_number(c(1.0, 2.0, 3.0))
is_whole_number(1.2)

## End(Not run)
</code></pre>

<hr>
<h2 id='keep_groups_in_cv_partitions'>Whether groups can be kept together in partitions during cross-validation</h2><span id='topic+keep_groups_in_cv_partitions'></span>

<h3>Description</h3>

<p>Whether groups can be kept together in partitions during cross-validation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>keep_groups_in_cv_partitions(groups, group_partitions, kfold)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="keep_groups_in_cv_partitions_+3A_groups">groups</code></td>
<td>
<p>Vector of groups to keep together when splitting the data into
train and test sets. If the number of groups in the training set is larger
than <code>kfold</code>, the groups will also be kept together for cross-validation.
Length matches the number of rows in the dataset (default: <code>NULL</code>).</p>
</td></tr>
<tr><td><code id="keep_groups_in_cv_partitions_+3A_group_partitions">group_partitions</code></td>
<td>
<p>Specify how to assign <code>groups</code> to the training and
testing partitions (default: <code>NULL</code>). If <code>groups</code> specifies that some
samples belong to group <code>"A"</code> and some belong to group <code>"B"</code>, then setting
<code>group_partitions = list(train = c("A", "B"), test = c("B"))</code> will result
in all samples from group <code>"A"</code> being placed in the training set, some
samples from <code>"B"</code> also in the training set, and the remaining samples from
<code>"B"</code> in the testing set. The partition sizes will be as close to
<code>training_frac</code> as possible. If the number of groups in the training set is
larger than <code>kfold</code>, the groups will also be kept together for
cross-validation.</p>
</td></tr>
<tr><td><code id="keep_groups_in_cv_partitions_+3A_kfold">kfold</code></td>
<td>
<p>Fold number for k-fold cross-validation (default: <code>5</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if possible, <code>FALSE</code> otherwise
</p>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>

<hr>
<h2 id='lower_bound'>Get the lower and upper bounds for an empirical confidence interval</h2><span id='topic+lower_bound'></span><span id='topic+upper_bound'></span><span id='topic+bounds'></span>

<h3>Description</h3>

<p>Get the lower and upper bounds for an empirical confidence interval
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lower_bound(x, alpha)

upper_bound(x, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lower_bound_+3A_x">x</code></td>
<td>
<p>vector of test statistics, such as from permutation tests or bootstraps</p>
</td></tr>
<tr><td><code id="lower_bound_+3A_alpha">alpha</code></td>
<td>
<p>alpha level for the confidence interval
(default: <code>0.05</code> to obtain a 95% confidence interval)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the value of the lower or upper bound for the confidence interval
</p>


<h3>Functions</h3>


<ul>
<li> <p><code>lower_bound()</code>: Get the lower bound for an empirical confidence interval
</p>
</li>
<li> <p><code>upper_bound()</code>: Get the upper bound for an empirical confidence interval
</p>
</li></ul>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
x &lt;- 1:10000
lower_bound(x, 0.05)
upper_bound(x, 0.05)

## End(Not run)
</code></pre>

<hr>
<h2 id='mutate_all_types'>Mutate all columns with <code>utils::type.convert()</code>.'</h2><span id='topic+mutate_all_types'></span>

<h3>Description</h3>

<p>Turns factors into characters and numerics where possible.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mutate_all_types(dat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mutate_all_types_+3A_dat">dat</code></td>
<td>
<p>data.frame to convert</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame with no factors
</p>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dat &lt;- data.frame(
  c1 = as.factor(c("a", "b", "c")),
  c2 = as.factor(1:3)
)
class(dat$c1)
class(dat$c2)
dat &lt;- mutate_all_types(dat)
class(dat$c1)
class(dat$c2)

## End(Not run)
</code></pre>

<hr>
<h2 id='otu_data_preproc'>Mini OTU abundance dataset - preprocessed</h2><span id='topic+otu_data_preproc'></span>

<h3>Description</h3>

<p>This is the result of running <code>preprocess_data("otu_mini_bin")</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>otu_data_preproc
</code></pre>


<h3>Format</h3>

<p>An object of class <code>list</code> of length 3.
</p>

<hr>
<h2 id='otu_mini_bin'>Mini OTU abundance dataset</h2><span id='topic+otu_mini_bin'></span>

<h3>Description</h3>

<p>A dataset containing relatives abundances of OTUs for human stool samples
with a binary outcome, <code>dx</code>.
This is a subset of <code>otu_small</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>otu_mini_bin
</code></pre>


<h3>Format</h3>

<p>A data frame
The <code>dx</code> column is the diagnosis: healthy or cancerous (colorectal).
All other columns are OTU relative abundances.
</p>

<hr>
<h2 id='otu_mini_bin_results_glmnet'>Results from running the pipeline with L2 logistic regression on <code>otu_mini_bin</code> with feature importance and grouping</h2><span id='topic+otu_mini_bin_results_glmnet'></span>

<h3>Description</h3>

<p>Results from running the pipeline with L2 logistic regression on <code>otu_mini_bin</code> with feature importance and grouping
</p>


<h3>Usage</h3>

<pre><code class='language-R'>otu_mini_bin_results_glmnet
</code></pre>


<h3>Format</h3>

<p>An object of class <code>list</code> of length 4.
</p>

<hr>
<h2 id='otu_mini_bin_results_rf'>Results from running the pipeline with random forest on <code>otu_mini_bin</code></h2><span id='topic+otu_mini_bin_results_rf'></span>

<h3>Description</h3>

<p>Results from running the pipeline with random forest on <code>otu_mini_bin</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>otu_mini_bin_results_rf
</code></pre>


<h3>Format</h3>

<p>An object of class <code>list</code> of length 4.
</p>

<hr>
<h2 id='otu_mini_bin_results_rpart2'>Results from running the pipeline with rpart2 on <code>otu_mini_bin</code></h2><span id='topic+otu_mini_bin_results_rpart2'></span>

<h3>Description</h3>

<p>Results from running the pipeline with rpart2 on <code>otu_mini_bin</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>otu_mini_bin_results_rpart2
</code></pre>


<h3>Format</h3>

<p>An object of class <code>list</code> of length 4.
</p>

<hr>
<h2 id='otu_mini_bin_results_svmRadial'>Results from running the pipeline with svmRadial on <code>otu_mini_bin</code></h2><span id='topic+otu_mini_bin_results_svmRadial'></span>

<h3>Description</h3>

<p>Results from running the pipeline with svmRadial on <code>otu_mini_bin</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>otu_mini_bin_results_svmRadial
</code></pre>


<h3>Format</h3>

<p>An object of class <code>list</code> of length 4.
</p>

<hr>
<h2 id='otu_mini_bin_results_xgbTree'>Results from running the pipeline with xbgTree on <code>otu_mini_bin</code></h2><span id='topic+otu_mini_bin_results_xgbTree'></span>

<h3>Description</h3>

<p>Results from running the pipeline with xbgTree on <code>otu_mini_bin</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>otu_mini_bin_results_xgbTree
</code></pre>


<h3>Format</h3>

<p>An object of class <code>list</code> of length 4.
</p>

<hr>
<h2 id='otu_mini_cont_results_glmnet'>Results from running the pipeline with glmnet on <code>otu_mini_bin</code> with <code>Otu00001</code>
as the outcome</h2><span id='topic+otu_mini_cont_results_glmnet'></span>

<h3>Description</h3>

<p>Results from running the pipeline with glmnet on <code>otu_mini_bin</code> with <code>Otu00001</code>
as the outcome
</p>


<h3>Usage</h3>

<pre><code class='language-R'>otu_mini_cont_results_glmnet
</code></pre>


<h3>Format</h3>

<p>An object of class <code>list</code> of length 4.
</p>

<hr>
<h2 id='otu_mini_cont_results_nocv'>Results from running the pipeline with glmnet on <code>otu_mini_bin</code> with <code>Otu00001</code>
as the outcome column,
using a custom train control scheme that does not perform cross-validation</h2><span id='topic+otu_mini_cont_results_nocv'></span>

<h3>Description</h3>

<p>Results from running the pipeline with glmnet on <code>otu_mini_bin</code> with <code>Otu00001</code>
as the outcome column,
using a custom train control scheme that does not perform cross-validation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>otu_mini_cont_results_nocv
</code></pre>


<h3>Format</h3>

<p>An object of class <code>list</code> of length 4.
</p>

<hr>
<h2 id='otu_mini_cv'>Cross validation on <code>train_data_mini</code> with grouped features.</h2><span id='topic+otu_mini_cv'></span>

<h3>Description</h3>

<p>Cross validation on <code>train_data_mini</code> with grouped features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>otu_mini_cv
</code></pre>


<h3>Format</h3>

<p>An object of class <code>list</code> of length 27.
</p>

<hr>
<h2 id='otu_mini_multi'>Mini OTU abundance dataset with 3 categorical variables</h2><span id='topic+otu_mini_multi'></span>

<h3>Description</h3>

<p>A dataset containing relatives abundances of OTUs for human stool samples
</p>


<h3>Usage</h3>

<pre><code class='language-R'>otu_mini_multi
</code></pre>


<h3>Format</h3>

<p>A data frame
The <code>dx</code> column is the colorectal cancer diagnosis: adenoma, carcinoma, normal.
All other columns are OTU relative abundances.
</p>

<hr>
<h2 id='otu_mini_multi_group'>Groups for otu_mini_multi</h2><span id='topic+otu_mini_multi_group'></span>

<h3>Description</h3>

<p>Groups for otu_mini_multi
</p>


<h3>Usage</h3>

<pre><code class='language-R'>otu_mini_multi_group
</code></pre>


<h3>Format</h3>

<p>An object of class <code>character</code> of length 490.
</p>

<hr>
<h2 id='otu_mini_multi_results_glmnet'>Results from running the pipeline with glmnet on <code>otu_mini_multi</code> for
multiclass outcomes</h2><span id='topic+otu_mini_multi_results_glmnet'></span>

<h3>Description</h3>

<p>Results from running the pipeline with glmnet on <code>otu_mini_multi</code> for
multiclass outcomes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>otu_mini_multi_results_glmnet
</code></pre>


<h3>Format</h3>

<p>An object of class <code>list</code> of length 4.
</p>

<hr>
<h2 id='otu_small'>Small OTU abundance dataset</h2><span id='topic+otu_small'></span>

<h3>Description</h3>

<p>A dataset containing relatives abundances of 60 OTUs for 60 human stool samples.
This is a subset of the data provided in <code>extdata/otu_large.csv</code>, which was
used in <a href="https://journals.asm.org/doi/10.1128/mbio.00434-20">Topçuoğlu <em>et al.</em> 2020</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>otu_small
</code></pre>


<h3>Format</h3>

<p>A data frame with 60 rows and 61 variables.
The <code>dx</code> column is the diagnosis: healthy or cancerous (colorectal).
All other columns are OTU relative abundances.
</p>

<hr>
<h2 id='pbtick'>Update progress if the progress bar is not <code>NULL</code>.</h2><span id='topic+pbtick'></span>

<h3>Description</h3>

<p>This allows for flexible code that only initializes a progress bar if the
<code>progressr</code> package is installed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pbtick(pb, message = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pbtick_+3A_pb">pb</code></td>
<td>
<p>a progress bar created with <code>progressr</code>.</p>
</td></tr>
<tr><td><code id="pbtick_+3A_message">message</code></td>
<td>
<p>optional message to report (default: <code>NULL</code>).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kelly Sovacool <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
f &lt;- function() {
  if (isTRUE(check_packages_installed("progressr"))) {
    pb &lt;- progressr::progressor(steps = 5, message = "looping")
  } else {
    pb &lt;- NULL
  }
  for (i in 1:5) {
    pbtick(pb)
    Sys.sleep(0.5)
  }
}
progressr::with_progress(
  f()
)

## End(Not run)
</code></pre>

<hr>
<h2 id='permute_p_value'>Calculated a permuted p-value comparing two models</h2><span id='topic+permute_p_value'></span>

<h3>Description</h3>

<p>Calculated a permuted p-value comparing two models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>permute_p_value(
  merged_data,
  metric,
  group_name,
  group_1,
  group_2,
  nperm = 10000
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="permute_p_value_+3A_merged_data">merged_data</code></td>
<td>
<p>the concatenated performance data from <code>run_ml</code></p>
</td></tr>
<tr><td><code id="permute_p_value_+3A_metric">metric</code></td>
<td>
<p>metric to compare, must be numeric</p>
</td></tr>
<tr><td><code id="permute_p_value_+3A_group_name">group_name</code></td>
<td>
<p>column with group variables to compare</p>
</td></tr>
<tr><td><code id="permute_p_value_+3A_group_1">group_1</code></td>
<td>
<p>name of one group to compare</p>
</td></tr>
<tr><td><code id="permute_p_value_+3A_group_2">group_2</code></td>
<td>
<p>name of other group to compare</p>
</td></tr>
<tr><td><code id="permute_p_value_+3A_nperm">nperm</code></td>
<td>
<p>number of permutations, default=10000</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric p-value comparing two models
</p>


<h3>Author(s)</h3>

<p>Begüm Topçuoğlu, <a href="mailto:topcuoglu.begum@gmail.com">topcuoglu.begum@gmail.com</a>
</p>
<p>Courtney R Armour, <a href="mailto:armourc@umich.edu">armourc@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>df &lt;- dplyr::tibble(
  model = c("rf", "rf", "glmnet", "glmnet", "svmRadial", "svmRadial"),
  AUC = c(.2, 0.3, 0.8, 0.9, 0.85, 0.95)
)
set.seed(123)
permute_p_value(df, "AUC", "model", "rf", "glmnet", nperm = 100)
</code></pre>

<hr>
<h2 id='plot_hp_performance'>Plot hyperparameter performance metrics</h2><span id='topic+plot_hp_performance'></span>

<h3>Description</h3>

<p>Plot hyperparameter performance metrics
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_hp_performance(dat, param_col, metric_col)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_hp_performance_+3A_dat">dat</code></td>
<td>
<p>dataframe of hyperparameters and performance metric (e.g. from <code>get_hp_performance()</code> or <code>combine_hp_performance()</code>)</p>
</td></tr>
<tr><td><code id="plot_hp_performance_+3A_param_col">param_col</code></td>
<td>
<p>hyperparameter to be plotted. must be a column in <code>dat</code>.</p>
</td></tr>
<tr><td><code id="plot_hp_performance_+3A_metric_col">metric_col</code></td>
<td>
<p>performance metric. must be a column in <code>dat</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot of hyperparameter performance.
</p>


<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>
<p>Kelly Sovacool <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># plot for a single `run_ml()` call
hp_metrics &lt;- get_hp_performance(otu_mini_bin_results_glmnet$trained_model)
hp_metrics
plot_hp_performance(hp_metrics$dat, lambda, AUC)
## Not run: 
# plot for multiple `run_ml()` calls
results &lt;- lapply(seq(100, 102), function(seed) {
  run_ml(otu_small, "glmnet", seed = seed)
})
models &lt;- lapply(results, function(x) x$trained_model)
hp_metrics &lt;- combine_hp_performance(models)
plot_hp_performance(hp_metrics$dat, lambda, AUC)

## End(Not run)
</code></pre>

<hr>
<h2 id='plot_mean_roc'>Plot ROC and PRC curves</h2><span id='topic+plot_mean_roc'></span><span id='topic+plot_mean_prc'></span><span id='topic+plot_curves'></span>

<h3>Description</h3>

<p>Plot ROC and PRC curves
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_mean_roc(dat, ribbon_fill = "#C6DBEF", line_color = "#08306B")

plot_mean_prc(
  dat,
  baseline_precision = NULL,
  ycol = mean_precision,
  ribbon_fill = "#C7E9C0",
  line_color = "#00441B"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_mean_roc_+3A_dat">dat</code></td>
<td>
<p>sensitivity, specificity, and precision data calculated by <code>calc_mean_roc()</code></p>
</td></tr>
<tr><td><code id="plot_mean_roc_+3A_ribbon_fill">ribbon_fill</code></td>
<td>
<p>ribbon fill color (default: &quot;#D9D9D9&quot;)</p>
</td></tr>
<tr><td><code id="plot_mean_roc_+3A_line_color">line_color</code></td>
<td>
<p>line color (default: &quot;#000000&quot;)</p>
</td></tr>
<tr><td><code id="plot_mean_roc_+3A_baseline_precision">baseline_precision</code></td>
<td>
<p>baseline precision from <code>calc_baseline_precision()</code></p>
</td></tr>
<tr><td><code id="plot_mean_roc_+3A_ycol">ycol</code></td>
<td>
<p>column for the y axis (Default: <code>mean_precision</code>)</p>
</td></tr>
</table>


<h3>Functions</h3>


<ul>
<li> <p><code>plot_mean_roc()</code>: Plot mean sensitivity over specificity
</p>
</li>
<li> <p><code>plot_mean_prc()</code>: Plot mean precision over recall
</p>
</li></ul>


<h3>Author(s)</h3>

<p>Courtney Armour
</p>
<p>Kelly Sovacool <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(dplyr)
# get performance for multiple models
get_sensspec_seed &lt;- function(seed) {
  ml_result &lt;- run_ml(otu_mini_bin, "glmnet", seed = seed)
  sensspec &lt;- calc_model_sensspec(
    ml_result$trained_model,
    ml_result$test_data,
    "dx"
  ) %&gt;%
    mutate(seed = seed)
  return(sensspec)
}
sensspec_dat &lt;- purrr::map_dfr(seq(100, 102), get_sensspec_seed)

# plot ROC &amp; PRC
sensspec_dat %&gt;%
  calc_mean_roc() %&gt;%
  plot_mean_roc()
baseline_prec &lt;- calc_baseline_precision(otu_mini_bin, "dx", "cancer")
sensspec_dat %&gt;%
  calc_mean_prc() %&gt;%
  plot_mean_prc(baseline_precision = baseline_prec)

## End(Not run)
</code></pre>

<hr>
<h2 id='plot_model_performance'>Plot performance metrics for multiple ML runs with different parameters</h2><span id='topic+plot_model_performance'></span>

<h3>Description</h3>

<p>ggplot2 is required to use this function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_model_performance(performance_df)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_model_performance_+3A_performance_df">performance_df</code></td>
<td>
<p>dataframe of performance results from multiple calls to <code>run_ml()</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot2 plot of performance.
</p>


<h3>Author(s)</h3>

<p>Begüm Topçuoglu, <a href="mailto:topcuoglu.begum@gmail.com">topcuoglu.begum@gmail.com</a>
</p>
<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# call `run_ml()` multiple times with different seeds
results_lst &lt;- lapply(seq(100, 104), function(seed) {
  run_ml(otu_small, "glmnet", seed = seed)
})
# extract and combine the performance results
perf_df &lt;- lapply(results_lst, function(result) {
  result[["performance"]]
}) %&gt;%
  dplyr::bind_rows()
# plot the performance results
p &lt;- plot_model_performance(perf_df)


# call `run_ml()` with different ML methods
param_grid &lt;- expand.grid(
  seeds = seq(100, 104),
  methods = c("glmnet", "rf")
)
results_mtx &lt;- mapply(
  function(seed, method) {
    run_ml(otu_mini_bin, method, seed = seed, kfold = 2)
  },
  param_grid$seeds, param_grid$methods
)
# extract and combine the performance results
perf_df2 &lt;- dplyr::bind_rows(results_mtx["performance", ])
# plot the performance results
p &lt;- plot_model_performance(perf_df2)

# you can continue adding layers to customize the plot
p +
  theme_classic() +
  scale_color_brewer(palette = "Dark2") +
  coord_flip()

## End(Not run)
</code></pre>

<hr>
<h2 id='preprocess_data'>Preprocess data prior to running machine learning</h2><span id='topic+preprocess_data'></span>

<h3>Description</h3>

<p>Function to preprocess your data for input into <code><a href="#topic+run_ml">run_ml()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>preprocess_data(
  dataset,
  outcome_colname,
  method = c("center", "scale"),
  remove_var = "nzv",
  collapse_corr_feats = TRUE,
  to_numeric = TRUE,
  group_neg_corr = TRUE,
  prefilter_threshold = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="preprocess_data_+3A_dataset">dataset</code></td>
<td>
<p>Data frame with an outcome variable and other columns as features.</p>
</td></tr>
<tr><td><code id="preprocess_data_+3A_outcome_colname">outcome_colname</code></td>
<td>
<p>Column name as a string of the outcome variable
(default <code>NULL</code>; the first column will be chosen automatically).</p>
</td></tr>
<tr><td><code id="preprocess_data_+3A_method">method</code></td>
<td>
<p>Methods to preprocess the data, described in
<code><a href="caret.html#topic+preProcess">caret::preProcess()</a></code> (default: <code>c("center","scale")</code>, use <code>NULL</code> for
no normalization).</p>
</td></tr>
<tr><td><code id="preprocess_data_+3A_remove_var">remove_var</code></td>
<td>
<p>Whether to remove variables with near-zero variance
(<code>'nzv'</code>; default), zero variance (<code>'zv'</code>), or none (<code>NULL</code>).</p>
</td></tr>
<tr><td><code id="preprocess_data_+3A_collapse_corr_feats">collapse_corr_feats</code></td>
<td>
<p>Whether to keep only one of perfectly correlated
features.</p>
</td></tr>
<tr><td><code id="preprocess_data_+3A_to_numeric">to_numeric</code></td>
<td>
<p>Whether to change features to numeric where possible.</p>
</td></tr>
<tr><td><code id="preprocess_data_+3A_group_neg_corr">group_neg_corr</code></td>
<td>
<p>Whether to group negatively correlated features
together (e.g. c(0,1) and c(1,0)).</p>
</td></tr>
<tr><td><code id="preprocess_data_+3A_prefilter_threshold">prefilter_threshold</code></td>
<td>
<p>Remove features which only have non-zero &amp; non-NA
values N rows or fewer (default: 1). Set this to -1 to keep all columns at
this step. This step will also be skipped if <code>to_numeric</code> is set to
<code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Named list including:
</p>

<ul>
<li> <p><code>dat_transformed</code>: Preprocessed data.
</p>
</li>
<li> <p><code>grp_feats</code>: If features were grouped together, a named list of the features corresponding to each group.
</p>
</li>
<li> <p><code>removed_feats</code>: Any features that were removed during preprocessing (e.g. because there was zero variance or near-zero variance for those features).
</p>
</li></ul>

<p>If the <code>progressr</code> package is installed, a progress bar with time elapsed
and estimated time to completion can be displayed.
</p>


<h3>More details</h3>

<p>See the <a href="http://www.schlosslab.org/mikropml/articles/preprocess.html">preprocessing vignette</a>
for more details.
</p>
<p>Note that if any values in <code>outcome_colname</code> contain spaces, they will be
converted to underscores for compatibility with <code>caret</code>.
</p>


<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>
<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>preprocess_data(mikropml::otu_small, "dx")

# the function can show a progress bar if you have the progressr package installed
## optionally, specify the progress bar format
progressr::handlers(progressr::handler_progress(
  format = ":message :bar :percent | elapsed: :elapsed | eta: :eta",
  clear = FALSE,
  show_after = 0
))
## tell progressor to always report progress
## Not run: 
progressr::handlers(global = TRUE)
## run the function and watch the live progress udpates
dat_preproc &lt;- preprocess_data(mikropml::otu_small, "dx")

## End(Not run)
</code></pre>

<hr>
<h2 id='process_cat_feats'>Process categorical features</h2><span id='topic+process_cat_feats'></span>

<h3>Description</h3>

<p>Process categorical features
</p>


<h3>Usage</h3>

<pre><code class='language-R'>process_cat_feats(features, progbar = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="process_cat_feats_+3A_features">features</code></td>
<td>
<p>dataframe of features for machine learning</p>
</td></tr>
<tr><td><code id="process_cat_feats_+3A_progbar">progbar</code></td>
<td>
<p>optional progress bar (default: <code>NULL</code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of two dataframes: categorical (processed) and continuous features (unprocessed)
</p>


<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
process_cat_feats(mikropml::otu_small[, 2:ncol(otu_small)])

## End(Not run)
</code></pre>

<hr>
<h2 id='process_cont_feats'>Preprocess continuous features</h2><span id='topic+process_cont_feats'></span>

<h3>Description</h3>

<p>Preprocess continuous features
</p>


<h3>Usage</h3>

<pre><code class='language-R'>process_cont_feats(features, method)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="process_cont_feats_+3A_features">features</code></td>
<td>
<p>Dataframe of features for machine learning</p>
</td></tr>
<tr><td><code id="process_cont_feats_+3A_method">method</code></td>
<td>
<p>Methods to preprocess the data, described in
<code><a href="caret.html#topic+preProcess">caret::preProcess()</a></code> (default: <code>c("center","scale")</code>, use <code>NULL</code> for
no normalization).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>dataframe of preprocessed features
</p>


<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
process_cont_feats(mikropml::otu_small[, 2:ncol(otu_small)], c("center", "scale"))

## End(Not run)
</code></pre>

<hr>
<h2 id='process_novar_feats'>Process features with no variation</h2><span id='topic+process_novar_feats'></span>

<h3>Description</h3>

<p>Process features with no variation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>process_novar_feats(features, progbar = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="process_novar_feats_+3A_features">features</code></td>
<td>
<p>dataframe of features for machine learning</p>
</td></tr>
<tr><td><code id="process_novar_feats_+3A_progbar">progbar</code></td>
<td>
<p>optional progress bar (default: <code>NULL</code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of two dataframes: features with variability (unprocessed) and without (processed)
</p>


<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
process_novar_feats(mikropml::otu_small[, 2:ncol(otu_small)])

## End(Not run)
</code></pre>

<hr>
<h2 id='radix_sort'>Call <code>sort()</code> with <code>method = 'radix'</code></h2><span id='topic+radix_sort'></span>

<h3>Description</h3>

<p>THE BASE SORT FUNCTION USES A DIFFERENT METHOD DEPENDING ON YOUR LOCALE.
However, the order for the radix method is always stable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>radix_sort(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="radix_sort_+3A_...">...</code></td>
<td>
<p>All arguments forwarded to <code>sort()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>see https://stackoverflow.com/questions/42272119/r-cmd-check-fails-devtoolstest-works-fine
</p>
<p><code>stringr::str_sort()</code> solves this problem with the <code>locale</code> parameter having
a default value, but I don't want to add that as another dependency.
</p>


<h3>Value</h3>

<p>Whatever you passed in, now in a stable sorted order regardless of your locale.
</p>


<h3>Author(s)</h3>

<p>Kelly Sovacool <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>

<hr>
<h2 id='randomize_feature_order'>Randomize feature order to eliminate any position-dependent effects</h2><span id='topic+randomize_feature_order'></span>

<h3>Description</h3>

<p>Randomize feature order to eliminate any position-dependent effects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>randomize_feature_order(dataset, outcome_colname)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="randomize_feature_order_+3A_dataset">dataset</code></td>
<td>
<p>Data frame with an outcome variable and other columns as features.</p>
</td></tr>
<tr><td><code id="randomize_feature_order_+3A_outcome_colname">outcome_colname</code></td>
<td>
<p>Column name as a string of the outcome variable
(default <code>NULL</code>; the first column will be chosen automatically).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Dataset with feature order randomized.
</p>


<h3>Author(s)</h3>

<p>Nick Lesniak, <a href="mailto:nlesniak@umich.edu">nlesniak@umich.edu</a>
</p>
<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- data.frame(
  outcome = c("1", "2", "3"),
  a = 4:6, b = 7:9, c = 10:12, d = 13:15
)
randomize_feature_order(dat, "outcome")
</code></pre>

<hr>
<h2 id='reexports'>caret contr.ltfr</h2><span id='topic+reexports'></span><span id='topic+contr.ltfr'></span><span id='topic++25+3E+25'></span><span id='topic+.data'></span><span id='topic++21+21'></span><span id='topic++3A+3D'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>caret</dt><dd><p><code><a href="caret.html#topic+dummyVars">contr.ltfr</a></code></p>
</dd>
<dt>dplyr</dt><dd><p><code><a href="dplyr.html#topic+reexports">%&gt;%</a></code></p>
</dd>
<dt>rlang</dt><dd><p><code><a href="rlang.html#topic+dyn-dots">:=</a></code>, <code><a href="rlang.html#topic+injection-operator">!!</a></code>, <code><a href="rlang.html#topic+dot-data">.data</a></code></p>
</dd>
</dl>

<hr>
<h2 id='remove_singleton_columns'>Remove columns appearing in only <code>threshold</code> row(s) or fewer.</h2><span id='topic+remove_singleton_columns'></span>

<h3>Description</h3>

<p>Removes columns which only have non-zero &amp; non-NA values in <code>threshold</code> row(s) or fewer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>remove_singleton_columns(dat, threshold = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="remove_singleton_columns_+3A_dat">dat</code></td>
<td>
<p>dataframe</p>
</td></tr>
<tr><td><code id="remove_singleton_columns_+3A_threshold">threshold</code></td>
<td>
<p>Number of rows. If a column only has non-zero &amp; non-NA values
in <code>threshold</code> row(s) or fewer, it will be removed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>dataframe without singleton columns
</p>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>
<p>Courtney Armour
</p>


<h3>Examples</h3>

<pre><code class='language-R'>remove_singleton_columns(data.frame(a = 1:3, b = c(0, 1, 0), c = 4:6))
remove_singleton_columns(data.frame(a = 1:3, b = c(0, 1, 0), c = 4:6), threshold = 0)
remove_singleton_columns(data.frame(a = 1:3, b = c(0, 1, NA), c = 4:6))
remove_singleton_columns(data.frame(a = 1:3, b = c(1, 1, 1), c = 4:6))
</code></pre>

<hr>
<h2 id='replace_spaces'>Replace spaces in all elements of a character vector with underscores</h2><span id='topic+replace_spaces'></span>

<h3>Description</h3>

<p>Replace spaces in all elements of a character vector with underscores
</p>


<h3>Usage</h3>

<pre><code class='language-R'>replace_spaces(x, new_char = "_")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="replace_spaces_+3A_x">x</code></td>
<td>
<p>a character vector</p>
</td></tr>
<tr><td><code id="replace_spaces_+3A_new_char">new_char</code></td>
<td>
<p>the character to replace spaces (default: <code style="white-space: pre;">&#8288;_&#8288;</code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>character vector with all spaces replaced with <code>new_char</code>
</p>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- data.frame(
  dx = c("outcome 1", "outcome 2", "outcome 1"),
  a = 1:3, b = c(5, 7, 1)
)
dat$dx &lt;- replace_spaces(dat$dx)
dat
</code></pre>

<hr>
<h2 id='rm_missing_outcome'>Remove missing outcome values</h2><span id='topic+rm_missing_outcome'></span>

<h3>Description</h3>

<p>Remove missing outcome values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rm_missing_outcome(dataset, outcome_colname)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rm_missing_outcome_+3A_dataset">dataset</code></td>
<td>
<p>Data frame with an outcome variable and other columns as features.</p>
</td></tr>
<tr><td><code id="rm_missing_outcome_+3A_outcome_colname">outcome_colname</code></td>
<td>
<p>Column name as a string of the outcome variable
(default <code>NULL</code>; the first column will be chosen automatically).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>dataset with no missing outcomes
</p>


<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
rm_missing_outcome(mikropml::otu_mini_bin, "dx")

test_df &lt;- mikropml::otu_mini_bin
test_df[1:100, "dx"] &lt;- NA
rm_missing_outcome(test_df, "dx")

## End(Not run)
</code></pre>

<hr>
<h2 id='run_ml'>Run the machine learning pipeline</h2><span id='topic+run_ml'></span>

<h3>Description</h3>

<p>This function splits the data set into a train &amp; test set,
trains machine learning (ML) models using k-fold cross-validation,
evaluates the best model on the held-out test set,
and optionally calculates feature importance using the framework
outlined in Topçuoğlu <em>et al.</em> 2020 (<a href="https://doi.org/10.1128/mBio.00434-20">doi:10.1128/mBio.00434-20</a>).
Required inputs are a data frame (must contain an outcome variable and all
other columns as features) and the ML method.
See <code>vignette('introduction')</code> for more details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>run_ml(
  dataset,
  method,
  outcome_colname = NULL,
  hyperparameters = NULL,
  find_feature_importance = FALSE,
  calculate_performance = TRUE,
  kfold = 5,
  cv_times = 100,
  cross_val = NULL,
  training_frac = 0.8,
  perf_metric_function = NULL,
  perf_metric_name = NULL,
  groups = NULL,
  group_partitions = NULL,
  corr_thresh = 1,
  seed = NA,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="run_ml_+3A_dataset">dataset</code></td>
<td>
<p>Data frame with an outcome variable and other columns as features.</p>
</td></tr>
<tr><td><code id="run_ml_+3A_method">method</code></td>
<td>
<p>ML method.
Options: <code>c("glmnet", "rf", "rpart2", "svmRadial", "xgbTree")</code>.
</p>

<ul>
<li><p> glmnet: linear, logistic, or multiclass regression
</p>
</li>
<li><p> rf: random forest
</p>
</li>
<li><p> rpart2: decision tree
</p>
</li>
<li><p> svmRadial: support vector machine
</p>
</li>
<li><p> xgbTree: xgboost
</p>
</li></ul>
</td></tr>
<tr><td><code id="run_ml_+3A_outcome_colname">outcome_colname</code></td>
<td>
<p>Column name as a string of the outcome variable
(default <code>NULL</code>; the first column will be chosen automatically).</p>
</td></tr>
<tr><td><code id="run_ml_+3A_hyperparameters">hyperparameters</code></td>
<td>
<p>Dataframe of hyperparameters
(default <code>NULL</code>; sensible defaults will be chosen automatically).</p>
</td></tr>
<tr><td><code id="run_ml_+3A_find_feature_importance">find_feature_importance</code></td>
<td>
<p>Run permutation importance (default: <code>FALSE</code>).
<code>TRUE</code> is recommended if you would like to identify features important for
predicting your outcome, but it is resource-intensive.</p>
</td></tr>
<tr><td><code id="run_ml_+3A_calculate_performance">calculate_performance</code></td>
<td>
<p>Whether to calculate performance metrics (default: <code>TRUE</code>).
You might choose to skip this if you do not perform cross-validation during model training.</p>
</td></tr>
<tr><td><code id="run_ml_+3A_kfold">kfold</code></td>
<td>
<p>Fold number for k-fold cross-validation (default: <code>5</code>).</p>
</td></tr>
<tr><td><code id="run_ml_+3A_cv_times">cv_times</code></td>
<td>
<p>Number of cross-validation partitions to create (default: <code>100</code>).</p>
</td></tr>
<tr><td><code id="run_ml_+3A_cross_val">cross_val</code></td>
<td>
<p>a custom cross-validation scheme from <code>caret::trainControl()</code>
(default: <code>NULL</code>, uses <code>kfold</code> cross validation repeated <code>cv_times</code>).
<code>kfold</code> and <code>cv_times</code> are ignored if the user provides a custom cross-validation scheme.
See the <code>caret::trainControl()</code> docs for information on how to use it.</p>
</td></tr>
<tr><td><code id="run_ml_+3A_training_frac">training_frac</code></td>
<td>
<p>Fraction of data for training set (default: <code>0.8</code>). Rows
from the dataset will be randomly selected for the training set, and all
remaining rows will be used in the testing set. Alternatively, if you
provide a vector of integers, these will be used as the row indices for the
training set. All remaining rows will be used in the testing set.</p>
</td></tr>
<tr><td><code id="run_ml_+3A_perf_metric_function">perf_metric_function</code></td>
<td>
<p>Function to calculate the performance metric to
be used for cross-validation and test performance. Some functions are
provided by caret (see <code><a href="caret.html#topic+postResample">caret::defaultSummary()</a></code>).
Defaults: binary classification = <code>twoClassSummary</code>,
multi-class classification = <code>multiClassSummary</code>,
regression = <code>defaultSummary</code>.</p>
</td></tr>
<tr><td><code id="run_ml_+3A_perf_metric_name">perf_metric_name</code></td>
<td>
<p>The column name from the output of the function
provided to perf_metric_function that is to be used as the performance metric.
Defaults: binary classification = <code>"ROC"</code>,
multi-class classification = <code>"logLoss"</code>,
regression = <code>"RMSE"</code>.</p>
</td></tr>
<tr><td><code id="run_ml_+3A_groups">groups</code></td>
<td>
<p>Vector of groups to keep together when splitting the data into
train and test sets. If the number of groups in the training set is larger
than <code>kfold</code>, the groups will also be kept together for cross-validation.
Length matches the number of rows in the dataset (default: <code>NULL</code>).</p>
</td></tr>
<tr><td><code id="run_ml_+3A_group_partitions">group_partitions</code></td>
<td>
<p>Specify how to assign <code>groups</code> to the training and
testing partitions (default: <code>NULL</code>). If <code>groups</code> specifies that some
samples belong to group <code>"A"</code> and some belong to group <code>"B"</code>, then setting
<code>group_partitions = list(train = c("A", "B"), test = c("B"))</code> will result
in all samples from group <code>"A"</code> being placed in the training set, some
samples from <code>"B"</code> also in the training set, and the remaining samples from
<code>"B"</code> in the testing set. The partition sizes will be as close to
<code>training_frac</code> as possible. If the number of groups in the training set is
larger than <code>kfold</code>, the groups will also be kept together for
cross-validation.</p>
</td></tr>
<tr><td><code id="run_ml_+3A_corr_thresh">corr_thresh</code></td>
<td>
<p>For feature importance, group correlations
above or equal to <code>corr_thresh</code> (range <code>0</code> to <code>1</code>; default: <code>1</code>).</p>
</td></tr>
<tr><td><code id="run_ml_+3A_seed">seed</code></td>
<td>
<p>Random seed (default: <code>NA</code>).
Your results will only be reproducible if you set a seed.</p>
</td></tr>
<tr><td><code id="run_ml_+3A_...">...</code></td>
<td>
<p>All additional arguments are passed on to <code>caret::train()</code>, such as
case weights via the <code>weights</code> argument or <code>ntree</code> for <code>rf</code> models.
See the <code>caret::train()</code> docs for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Named list with results:
</p>

<ul>
<li> <p><code>trained_model</code>: Output of <code><a href="caret.html#topic+train">caret::train()</a></code>, including the best model.
</p>
</li>
<li> <p><code>test_data</code>: Part of the data that was used for testing.
</p>
</li>
<li> <p><code>performance</code>: Data frame of performance metrics. The first column is the
cross-validation performance metric, and the last two columns are the ML
method used and the seed (if one was set), respectively.
All other columns are performance metrics calculated on the test data.
This contains only one row, so you can easily combine performance
data frames from multiple calls to <code>run_ml()</code>
(see <code>vignette("parallel")</code>).
</p>
</li>
<li> <p><code>feature_importance</code>: If feature importances were calculated, a data frame
where each row is a feature or correlated group. The columns are the
performance metric of the permuted data, the difference between the true
performance metric and the performance metric of the permuted data
(true - permuted), the feature name, the ML method,
the performance metric name, and the seed (if provided).
For AUC and RMSE, the higher perf_metric_diff is, the more important that
feature is for predicting the outcome. For log loss, the lower
perf_metric_diff is, the more important that feature is for
predicting the outcome.
</p>
</li></ul>



<h3>More details</h3>

<p>For more details, please see
<a href="http://www.schlosslab.org/mikropml/articles/">the vignettes</a>.
</p>


<h3>Author(s)</h3>

<p>Begüm Topçuoğlu, <a href="mailto:topcuoglu.begum@gmail.com">topcuoglu.begum@gmail.com</a>
</p>
<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>
<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

# regression
run_ml(otu_small, "glmnet",
  seed = 2019
)

# random forest w/ feature importance
run_ml(otu_small, "rf",
  outcome_colname = "dx",
  find_feature_importance = TRUE
)

# custom cross validation &amp; hyperparameters
run_ml(otu_mini_bin[, 2:11],
  "glmnet",
  outcome_colname = "Otu00001",
  seed = 2019,
  hyperparameters = list(lambda = c(1e-04), alpha = 0),
  cross_val = caret::trainControl(method = "none"),
  calculate_performance = FALSE
)

## End(Not run)
</code></pre>

<hr>
<h2 id='select_apply'>Use future apply if available</h2><span id='topic+select_apply'></span>

<h3>Description</h3>

<p>Use future apply if available
</p>


<h3>Usage</h3>

<pre><code class='language-R'>select_apply(fun = "apply")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="select_apply_+3A_fun">fun</code></td>
<td>
<p>apply function to use (apply, lapply, sapply, etc.)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>output of apply function
</p>


<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
select_apply(fun = "sapply")

## End(Not run)
</code></pre>

<hr>
<h2 id='set_hparams_glmnet'>Set hyperparameters for regression models for use with glmnet</h2><span id='topic+set_hparams_glmnet'></span>

<h3>Description</h3>

<p>Alpha is set to <code>0</code> for ridge (L2). An alpha of <code>1</code> would make it lasso (L1).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_hparams_glmnet()
</code></pre>


<h3>Value</h3>

<p>default lambda &amp; alpha values
</p>


<h3>Author(s)</h3>

<p>Zena Lapp, zenalapp@umich.edu
</p>

<hr>
<h2 id='set_hparams_rf'>Set hyparameters for random forest models</h2><span id='topic+set_hparams_rf'></span>

<h3>Description</h3>

<p>Set hyparameters for random forest models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_hparams_rf(n_features)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_hparams_rf_+3A_n_features">n_features</code></td>
<td>
<p>number of features in the dataset</p>
</td></tr>
</table>


<h3>Value</h3>

<p>named list of hyperparameters
</p>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
set_hparams_rf(16)
set_hparams_rf(2000)
set_hparams_rf(1)

## End(Not run)
</code></pre>

<hr>
<h2 id='set_hparams_rpart2'>Set hyperparameters for decision tree models</h2><span id='topic+set_hparams_rpart2'></span>

<h3>Description</h3>

<p>Set hyperparameters for decision tree models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_hparams_rpart2(n_samples)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_hparams_rpart2_+3A_n_samples">n_samples</code></td>
<td>
<p>number of samples in the dataset</p>
</td></tr>
</table>


<h3>Value</h3>

<p>named list of hyperparameters
</p>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
set_hparams_rpart2(100)
set_hparams_rpart2(20)

## End(Not run)
</code></pre>

<hr>
<h2 id='set_hparams_svmRadial'>Set hyperparameters for SVM with radial kernel</h2><span id='topic+set_hparams_svmRadial'></span>

<h3>Description</h3>

<p>Set hyperparameters for SVM with radial kernel
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_hparams_svmRadial()
</code></pre>


<h3>Value</h3>

<p>named list of hyperparameters
</p>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
set_hparams_svmRadial()

## End(Not run)
</code></pre>

<hr>
<h2 id='set_hparams_xgbTree'>Set hyperparameters for SVM with radial kernel</h2><span id='topic+set_hparams_xgbTree'></span>

<h3>Description</h3>

<p>Set hyperparameters for SVM with radial kernel
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_hparams_xgbTree(n_samples)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_hparams_xgbTree_+3A_n_samples">n_samples</code></td>
<td>
<p>number of samples in the dataset</p>
</td></tr>
</table>


<h3>Value</h3>

<p>named list of hyperparameters
</p>


<h3>Author(s)</h3>

<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
set_hparams_xgbTree()

## End(Not run)
</code></pre>

<hr>
<h2 id='shared_ggprotos'>Get plot layers shared by <code>plot_mean_roc</code> and <code>plot_mean_prc</code></h2><span id='topic+shared_ggprotos'></span>

<h3>Description</h3>

<p>Get plot layers shared by <code>plot_mean_roc</code> and <code>plot_mean_prc</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shared_ggprotos(ribbon_fill = "#D9D9D9", line_color = "#000000")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shared_ggprotos_+3A_ribbon_fill">ribbon_fill</code></td>
<td>
<p>ribbon fill color (default: &quot;#D9D9D9&quot;)</p>
</td></tr>
<tr><td><code id="shared_ggprotos_+3A_line_color">line_color</code></td>
<td>
<p>line color (default: &quot;#000000&quot;)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of ggproto objects to add to a ggplot
</p>


<h3>Author(s)</h3>

<p>Kelly Sovacool <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>

<hr>
<h2 id='shuffle_group'>Shuffle the rows in a column</h2><span id='topic+shuffle_group'></span>

<h3>Description</h3>

<p>Shuffle the rows in a column
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shuffle_group(dat, col_name)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shuffle_group_+3A_dat">dat</code></td>
<td>
<p>a data frame containing <code>col_name</code></p>
</td></tr>
<tr><td><code id="shuffle_group_+3A_col_name">col_name</code></td>
<td>
<p>column name to shuffle</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>dat</code> with the rows of <code>col_name</code> shuffled
</p>


<h3>Author(s)</h3>

<p>Courtney R Armour, <a href="mailto:armourc@umich.edu">armourc@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
set.seed(123)
df &lt;- dplyr::tibble(
  condition = c("a", "a", "b", "b"),
  AUC = c(.2, 0.3, 0.8, 0.9)
)
shuffle_group(df, "condition")

## End(Not run)
</code></pre>

<hr>
<h2 id='split_outcome_features'>Split dataset into outcome and features</h2><span id='topic+split_outcome_features'></span>

<h3>Description</h3>

<p>Split dataset into outcome and features
</p>


<h3>Usage</h3>

<pre><code class='language-R'>split_outcome_features(dataset, outcome_colname)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="split_outcome_features_+3A_dataset">dataset</code></td>
<td>
<p>Data frame with an outcome variable and other columns as features.</p>
</td></tr>
<tr><td><code id="split_outcome_features_+3A_outcome_colname">outcome_colname</code></td>
<td>
<p>Column name as a string of the outcome variable
(default <code>NULL</code>; the first column will be chosen automatically).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of length two: outcome, features (as dataframes)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
split_outcome_features(mikropml::otu_mini_bin, "dx")

## End(Not run)
</code></pre>

<hr>
<h2 id='tidy_perf_data'>Tidy the performance dataframe</h2><span id='topic+tidy_perf_data'></span>

<h3>Description</h3>

<p>Used by <code>plot_model_performance()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tidy_perf_data(performance_df)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tidy_perf_data_+3A_performance_df">performance_df</code></td>
<td>
<p>dataframe of performance results from multiple calls to <code>run_ml()</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Tidy dataframe with model performance metrics.
</p>


<h3>Author(s)</h3>

<p>Begüm Topçuoglu, <a href="mailto:topcuoglu.begum@gmail.com">topcuoglu.begum@gmail.com</a>
</p>
<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# call `run_ml()` multiple times with different seeds
results_lst &lt;- lapply(seq(100, 104), function(seed) {
  run_ml(otu_small, "glmnet", seed = seed)
})
# extract and combine the performance results
perf_df &lt;- lapply(results_lst, function(result) {
  result[["performance"]]
}) %&gt;%
  dplyr::bind_rows()
# make it pretty!
tidy_perf_data(perf_df)

## End(Not run)
</code></pre>

<hr>
<h2 id='train_model'>Train model using <code><a href="caret.html#topic+train">caret::train()</a></code>.</h2><span id='topic+train_model'></span>

<h3>Description</h3>

<p>Train model using <code><a href="caret.html#topic+train">caret::train()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>train_model(
  train_data,
  outcome_colname,
  method,
  cv,
  perf_metric_name,
  tune_grid,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="train_model_+3A_train_data">train_data</code></td>
<td>
<p>Training data. Expected to be a subset of the full dataset.</p>
</td></tr>
<tr><td><code id="train_model_+3A_outcome_colname">outcome_colname</code></td>
<td>
<p>Column name as a string of the outcome variable
(default <code>NULL</code>; the first column will be chosen automatically).</p>
</td></tr>
<tr><td><code id="train_model_+3A_method">method</code></td>
<td>
<p>ML method.
Options: <code>c("glmnet", "rf", "rpart2", "svmRadial", "xgbTree")</code>.
</p>

<ul>
<li><p> glmnet: linear, logistic, or multiclass regression
</p>
</li>
<li><p> rf: random forest
</p>
</li>
<li><p> rpart2: decision tree
</p>
</li>
<li><p> svmRadial: support vector machine
</p>
</li>
<li><p> xgbTree: xgboost
</p>
</li></ul>
</td></tr>
<tr><td><code id="train_model_+3A_cv">cv</code></td>
<td>
<p>Cross-validation caret scheme from <code>define_cv()</code>.</p>
</td></tr>
<tr><td><code id="train_model_+3A_perf_metric_name">perf_metric_name</code></td>
<td>
<p>The column name from the output of the function
provided to perf_metric_function that is to be used as the performance metric.
Defaults: binary classification = <code>"ROC"</code>,
multi-class classification = <code>"logLoss"</code>,
regression = <code>"RMSE"</code>.</p>
</td></tr>
<tr><td><code id="train_model_+3A_tune_grid">tune_grid</code></td>
<td>
<p>Tuning grid from <code>get_tuning_grid()</code>.#'</p>
</td></tr>
<tr><td><code id="train_model_+3A_...">...</code></td>
<td>
<p>All additional arguments are passed on to <code>caret::train()</code>, such as
case weights via the <code>weights</code> argument or <code>ntree</code> for <code>rf</code> models.
See the <code>caret::train()</code> docs for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Trained model from <code><a href="caret.html#topic+train">caret::train()</a></code>.
</p>


<h3>Author(s)</h3>

<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
training_data &lt;- otu_mini_bin_results_glmnet$trained_model$trainingData %&gt;%
  dplyr::rename(dx = .outcome)
method &lt;- "rf"
hyperparameters &lt;- get_hyperparams_list(otu_mini_bin, method)
cross_val &lt;- define_cv(training_data,
  "dx",
  hyperparameters,
  perf_metric_function = caret::multiClassSummary,
  class_probs = TRUE,
  cv_times = 2
)
tune_grid &lt;- get_tuning_grid(hyperparameters, method)

rf_model &lt;- train_model(
  training_data,
  "dx",
  method,
  cross_val,
  "AUC",
  tune_grid,
  ntree = 1000
)
rf_model$results %&gt;% dplyr::select(mtry, AUC, prAUC)

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
