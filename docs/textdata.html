<!DOCTYPE html><html><head><title>Help for package textdata</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {textdata}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#textdata-package'><p>textdata: Download and Load Various Text Datasets</p></a></li>
<li><a href='#cache_info'><p>List folders and their sizes in cache</p></a></li>
<li><a href='#catalogue'><p>Catalogue of all available data sources</p></a></li>
<li><a href='#dataset_ag_news'><p>AG's News Topic Classification Dataset</p></a></li>
<li><a href='#dataset_dbpedia'><p>DBpedia Ontology Dataset</p></a></li>
<li><a href='#dataset_imdb'><p>IMDB Large Movie Review Dataset</p></a></li>
<li><a href='#dataset_sentence_polarity'><p>v1.0 sentence polarity dataset</p></a></li>
<li><a href='#dataset_trec'><p>TREC dataset</p></a></li>
<li><a href='#embedding_glove'><p>Global Vectors for Word Representation</p></a></li>
<li><a href='#lexicon_afinn'><p>AFINN-111 dataset</p></a></li>
<li><a href='#lexicon_bing'><p>Bing sentiment lexicon</p></a></li>
<li><a href='#lexicon_loughran'><p>Loughran-McDonald sentiment lexicon</p></a></li>
<li><a href='#lexicon_nrc'><p>NRC word-emotion association lexicon</p></a></li>
<li><a href='#lexicon_nrc_eil'><p>NRC Emotion Intensity Lexicon (aka Affect Intensity Lexicon) v0.5</p></a></li>
<li><a href='#lexicon_nrc_vad'><p>The NRC Valence, Arousal, and Dominance Lexicon</p></a></li>
<li><a href='#load_dataset'><p>Internal Functions</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Download and Load Various Text Datasets</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4.4</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides a framework to download, parse, and store text
    datasets on the disk and load them when needed. Includes various
    sentiment lexicons and labeled text data sets for classification and
    analysis.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/EmilHvitfeldt/textdata">https://github.com/EmilHvitfeldt/textdata</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/EmilHvitfeldt/textdata/issues">https://github.com/EmilHvitfeldt/textdata/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>fs, rappdirs, readr, tibble</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, knitr, rmarkdown, testthat (&ge; 2.1.0)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1.9000</td>
</tr>
<tr>
<td>Collate:</td>
<td>'cache_info.R' 'dataset_ag_news.R' 'dataset_dbpedia.R'
'dataset_imdb.R' 'dataset_sentence_polarity.R' 'dataset_trec.R'
'embedding_glove.R' 'lexicon_nrc_vad.R' 'lexicon_nrc_eil.R'
'lexicon_nrc.R' 'lexicon_bing.R' 'lexicon_loughran.R'
'lexicon_afinn.R' 'download_functions.R' 'info.R'
'load_dataset.R' 'printer.R' 'process_functions.R'
'textdata-package.R'</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-09-02 01:14:19 UTC; emilhvitfeldt</td>
</tr>
<tr>
<td>Author:</td>
<td>Emil Hvitfeldt <a href="https://orcid.org/0000-0002-0679-1945"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Julia Silge <a href="https://orcid.org/0000-0002-3671-836X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Emil Hvitfeldt &lt;emilhhvitfeldt@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-09-02 06:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='textdata-package'>textdata: Download and Load Various Text Datasets</h2><span id='topic+textdata'></span><span id='topic+textdata-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>Provides a framework to download, parse, and store text datasets on the disk and load them when needed. Includes various sentiment lexicons and labeled text data sets for classification and analysis.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Emil Hvitfeldt <a href="mailto:emilhhvitfeldt@gmail.com">emilhhvitfeldt@gmail.com</a> (<a href="https://orcid.org/0000-0002-0679-1945">ORCID</a>)
</p>
<p>Other contributors:
</p>

<ul>
<li><p> Julia Silge <a href="mailto:julia.silge@gmail.com">julia.silge@gmail.com</a> (<a href="https://orcid.org/0000-0002-3671-836X">ORCID</a>) [contributor]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/EmilHvitfeldt/textdata">https://github.com/EmilHvitfeldt/textdata</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/EmilHvitfeldt/textdata/issues">https://github.com/EmilHvitfeldt/textdata/issues</a>
</p>
</li></ul>


<hr>
<h2 id='cache_info'>List folders and their sizes in cache</h2><span id='topic+cache_info'></span>

<h3>Description</h3>

<p>This function will return a tibble with the name and sizes of all folder in
specified directory. Will default to textdata's default cache.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cache_info(dir = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cache_info_+3A_dir">dir</code></td>
<td>
<p>Character, path to directory where data will be stored. If
<code>NULL</code>, <a href="rappdirs.html#topic+user_cache_dir">user_cache_dir</a> will be used to determine path.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with 2 variables:
</p>

<dl>
<dt>name</dt><dd><p>Name of the folder</p>
</dd>
<dt>size</dt><dd><p>Size of the folder</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
cache_info()

## End(Not run)
</code></pre>

<hr>
<h2 id='catalogue'>Catalogue of all available data sources</h2><span id='topic+catalogue'></span>

<h3>Description</h3>

<p>Catalogue of all available data sources
</p>


<h3>Usage</h3>

<pre><code class='language-R'>catalogue
</code></pre>


<h3>Format</h3>

<p>An object of class <code>data.frame</code> with 15 rows and 8 columns.
</p>

<hr>
<h2 id='dataset_ag_news'>AG's News Topic Classification Dataset</h2><span id='topic+dataset_ag_news'></span>

<h3>Description</h3>

<p>The AG's news topic classification dataset is constructed by choosing 4
largest classes from the original corpus. Each class contains 30,000 training
samples and 1,900 testing samples. The total number of training samples is
120,000 and testing 7,600.
Version 3, Updated 09/09/2015
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_ag_news(
  dir = NULL,
  split = c("train", "test"),
  delete = FALSE,
  return_path = FALSE,
  clean = FALSE,
  manual_download = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_ag_news_+3A_dir">dir</code></td>
<td>
<p>Character, path to directory where data will be stored. If
<code>NULL</code>, <a href="rappdirs.html#topic+user_cache_dir">user_cache_dir</a> will be used to determine path.</p>
</td></tr>
<tr><td><code id="dataset_ag_news_+3A_split">split</code></td>
<td>
<p>Character. Return training (&quot;train&quot;) data or testing (&quot;test&quot;)
data. Defaults to &quot;train&quot;.</p>
</td></tr>
<tr><td><code id="dataset_ag_news_+3A_delete">delete</code></td>
<td>
<p>Logical, set <code>TRUE</code> to delete dataset.</p>
</td></tr>
<tr><td><code id="dataset_ag_news_+3A_return_path">return_path</code></td>
<td>
<p>Logical, set <code>TRUE</code> to return the path of the dataset.</p>
</td></tr>
<tr><td><code id="dataset_ag_news_+3A_clean">clean</code></td>
<td>
<p>Logical, set <code>TRUE</code> to remove intermediate files. This can
greatly reduce the size. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="dataset_ag_news_+3A_manual_download">manual_download</code></td>
<td>
<p>Logical, set <code>TRUE</code> if you have manually
downloaded the file and placed it in the folder designated by running
this function with <code>return_path = TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The classes in this dataset are
</p>

<ul>
<li><p> World
</p>
</li>
<li><p> Sports
</p>
</li>
<li><p> Business
</p>
</li>
<li><p> Sci/Tech
</p>
</li></ul>



<h3>Value</h3>

<p>A tibble with 120,000 or 30,000 rows for &quot;train&quot; and &quot;test&quot;
respectively and 3 variables:
</p>

<dl>
<dt>class</dt><dd><p>Character, denoting new class</p>
</dd>
<dt>title</dt><dd><p>Character, title of article</p>
</dd>
<dt>description</dt><dd><p>Character, description of article</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html">http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html</a>
</p>
<p><a href="https://github.com/srhrshr/torchDatasets/raw/master/dbpedia_csv.tar.gz">https://github.com/srhrshr/torchDatasets/raw/master/dbpedia_csv.tar.gz</a>
</p>


<h3>See Also</h3>

<p>Other topic: 
<code><a href="#topic+dataset_dbpedia">dataset_dbpedia</a>()</code>,
<code><a href="#topic+dataset_trec">dataset_trec</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dataset_ag_news()

# Custom directory
dataset_ag_news(dir = "data/")

# Deleting dataset
dataset_ag_news(delete = TRUE)

# Returning filepath of data
dataset_ag_news(return_path = TRUE)

# Access both training and testing dataset
train &lt;- dataset_ag_news(split = "train")
test &lt;- dataset_ag_news(split = "test")

## End(Not run)

</code></pre>

<hr>
<h2 id='dataset_dbpedia'>DBpedia Ontology Dataset</h2><span id='topic+dataset_dbpedia'></span>

<h3>Description</h3>

<p>DBpedia ontology dataset classification dataset. It contains 560,000 training
samples and 70,000 testing samples for each of 14 nonoverlapping classes
from DBpedia.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_dbpedia(
  dir = NULL,
  split = c("train", "test"),
  delete = FALSE,
  return_path = FALSE,
  clean = FALSE,
  manual_download = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_dbpedia_+3A_dir">dir</code></td>
<td>
<p>Character, path to directory where data will be stored. If
<code>NULL</code>, <a href="rappdirs.html#topic+user_cache_dir">user_cache_dir</a> will be used to determine path.</p>
</td></tr>
<tr><td><code id="dataset_dbpedia_+3A_split">split</code></td>
<td>
<p>Character. Return training (&quot;train&quot;) data or testing (&quot;test&quot;)
data. Defaults to &quot;train&quot;.</p>
</td></tr>
<tr><td><code id="dataset_dbpedia_+3A_delete">delete</code></td>
<td>
<p>Logical, set <code>TRUE</code> to delete dataset.</p>
</td></tr>
<tr><td><code id="dataset_dbpedia_+3A_return_path">return_path</code></td>
<td>
<p>Logical, set <code>TRUE</code> to return the path of the dataset.</p>
</td></tr>
<tr><td><code id="dataset_dbpedia_+3A_clean">clean</code></td>
<td>
<p>Logical, set <code>TRUE</code> to remove intermediate files. This can
greatly reduce the size. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="dataset_dbpedia_+3A_manual_download">manual_download</code></td>
<td>
<p>Logical, set <code>TRUE</code> if you have manually
downloaded the file and placed it in the folder designated by running
this function with <code>return_path = TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The classes are
</p>

<ul>
<li><p> Company
</p>
</li>
<li><p> EducationalInstitution
</p>
</li>
<li><p> Artist
</p>
</li>
<li><p> Athlete
</p>
</li>
<li><p> OfficeHolder
</p>
</li>
<li><p> MeanOfTransportation
</p>
</li>
<li><p> Building
</p>
</li>
<li><p> NaturalPlace
</p>
</li>
<li><p> Village
</p>
</li>
<li><p> Animal
</p>
</li>
<li><p> Plant
</p>
</li>
<li><p> Album
</p>
</li>
<li><p> Film
</p>
</li>
<li><p> WrittenWork
</p>
</li></ul>



<h3>Value</h3>

<p>A tibble with 560,000 or 70,000 rows for &quot;train&quot; and &quot;test&quot;
respectively and 3 variables:
</p>

<dl>
<dt>class</dt><dd><p>Character, denoting the class class</p>
</dd>
<dt>title</dt><dd><p>Character, title of article</p>
</dd>
<dt>description</dt><dd><p>Character, description of article</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf">https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf</a>
</p>
<p><a href="https://www.dbpedia.org/">https://www.dbpedia.org/</a>
</p>
<p><a href="https://github.com/srhrshr/torchDatasets/raw/master/dbpedia_csv.tar.gz">https://github.com/srhrshr/torchDatasets/raw/master/dbpedia_csv.tar.gz</a>
</p>


<h3>See Also</h3>

<p>Other topic: 
<code><a href="#topic+dataset_ag_news">dataset_ag_news</a>()</code>,
<code><a href="#topic+dataset_trec">dataset_trec</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dataset_dbpedia()

# Custom directory
dataset_dbpedia(dir = "data/")

# Deleting dataset
dataset_dbpedia(delete = TRUE)

# Returning filepath of data
dataset_dbpedia(return_path = TRUE)

# Access both training and testing dataset
train &lt;- dataset_dbpedia(split = "train")
test &lt;- dataset_dbpedia(split = "test")

## End(Not run)

</code></pre>

<hr>
<h2 id='dataset_imdb'>IMDB Large Movie Review Dataset</h2><span id='topic+dataset_imdb'></span>

<h3>Description</h3>

<p>The core dataset contains 50,000 reviews split evenly into 25k train and
25k test sets. The overall distribution of labels is balanced (25k pos and
25k neg).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_imdb(
  dir = NULL,
  split = c("train", "test"),
  delete = FALSE,
  return_path = FALSE,
  clean = FALSE,
  manual_download = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_imdb_+3A_dir">dir</code></td>
<td>
<p>Character, path to directory where data will be stored. If
<code>NULL</code>, <a href="rappdirs.html#topic+user_cache_dir">user_cache_dir</a> will be used to determine path.</p>
</td></tr>
<tr><td><code id="dataset_imdb_+3A_split">split</code></td>
<td>
<p>Character. Return training (&quot;train&quot;) data or testing (&quot;test&quot;)
data. Defaults to &quot;train&quot;.</p>
</td></tr>
<tr><td><code id="dataset_imdb_+3A_delete">delete</code></td>
<td>
<p>Logical, set <code>TRUE</code> to delete dataset.</p>
</td></tr>
<tr><td><code id="dataset_imdb_+3A_return_path">return_path</code></td>
<td>
<p>Logical, set <code>TRUE</code> to return the path of the dataset.</p>
</td></tr>
<tr><td><code id="dataset_imdb_+3A_clean">clean</code></td>
<td>
<p>Logical, set <code>TRUE</code> to remove intermediate files. This can
greatly reduce the size. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="dataset_imdb_+3A_manual_download">manual_download</code></td>
<td>
<p>Logical, set <code>TRUE</code> if you have manually
downloaded the file and placed it in the folder designated by running
this function with <code>return_path = TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the entire collection, no more than 30 reviews are allowed for any
given movie because reviews for the same movie tend to have correlated
ratings. Further, the train and test sets contain a disjoint set of
movies, so no significant performance is obtained by memorizing
movie-unique terms and their associated with observed labels. In the
labeled train/test sets, a negative review has a score &lt;= 4 out of 10,
and a positive review has a score &gt;= 7 out of 10. Thus reviews with
more neutral ratings are not included in the train/test sets. In the
unsupervised set, reviews of any rating are included and there are an
even number of reviews &gt; 5 and &lt;= 5.
</p>
<p>When using this dataset, please cite the ACL 2011 paper
</p>
<p>InProceedings{maas-EtAl:2011:ACL-HLT2011, <br />
author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher}, <br />
title     = {Learning Word Vectors for Sentiment Analysis}, <br />
booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies}, <br />
month     = {June}, <br />
year      = {2011}, <br />
address   = {Portland, Oregon, USA}, <br />
publisher = {Association for Computational Linguistics}, <br />
pages     = {142&ndash;150}, <br />
url       = {http://www.aclweb.org/anthology/P11-1015}
}
</p>


<h3>Value</h3>

<p>A tibble with 25,000 rows and 2 variables:
</p>

<dl>
<dt>Sentiment</dt><dd><p>Character, denoting the sentiment</p>
</dd>
<dt>text</dt><dd><p>Character, text of the review</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="http://ai.stanford.edu/~amaas/data/sentiment/">http://ai.stanford.edu/~amaas/data/sentiment/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dataset_imdb()

# Custom directory
dataset_imdb(dir = "data/")

# Deleting dataset
dataset_imdb(delete = TRUE)

# Returning filepath of data
dataset_imdb(return_path = TRUE)

# Access both training and testing dataset
train &lt;- dataset_imdb(split = "train")
test &lt;- dataset_imdb(split = "test")

## End(Not run)

</code></pre>

<hr>
<h2 id='dataset_sentence_polarity'>v1.0 sentence polarity dataset</h2><span id='topic+dataset_sentence_polarity'></span>

<h3>Description</h3>

<p>5331 positive and 5331 negative processed sentences / snippets.
Introduced in Pang/Lee ACL 2005. Released July 2005.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_sentence_polarity(
  dir = NULL,
  delete = FALSE,
  return_path = FALSE,
  clean = FALSE,
  manual_download = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_sentence_polarity_+3A_dir">dir</code></td>
<td>
<p>Character, path to directory where data will be stored. If
<code>NULL</code>, <a href="rappdirs.html#topic+user_cache_dir">user_cache_dir</a> will be used to determine path.</p>
</td></tr>
<tr><td><code id="dataset_sentence_polarity_+3A_delete">delete</code></td>
<td>
<p>Logical, set <code>TRUE</code> to delete dataset.</p>
</td></tr>
<tr><td><code id="dataset_sentence_polarity_+3A_return_path">return_path</code></td>
<td>
<p>Logical, set <code>TRUE</code> to return the path of the dataset.</p>
</td></tr>
<tr><td><code id="dataset_sentence_polarity_+3A_clean">clean</code></td>
<td>
<p>Logical, set <code>TRUE</code> to remove intermediate files. This can
greatly reduce the size. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="dataset_sentence_polarity_+3A_manual_download">manual_download</code></td>
<td>
<p>Logical, set <code>TRUE</code> if you have manually
downloaded the file and placed it in the folder designated by running
this function with <code>return_path = TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Citation info:
</p>
<p>This data was first used in Bo Pang and Lillian Lee,
&ldquo;Seeing stars: Exploiting class relationships for sentiment categorization
with respect to rating scales.&rdquo;, Proceedings of the ACL, 2005.
</p>
<p>InProceedings{pang05, <br />
author    = {Bo Pang and Lillian Lee}, <br />
title     = {Seeing stars: Exploiting class relationships for sentiment <br />
categorization with respect to rating scales}, <br />
booktitle = {Proceedings of the ACL}, <br />
year      = 2005 <br />
}
</p>


<h3>Value</h3>

<p>A tibble with 10,662 rows and 2 variables:
</p>

<dl>
<dt>text</dt><dd><p>Sentences or snippets</p>
</dd>
<dt>sentiment</dt><dd><p>Indicator for sentiment, &quot;neg&quot; for negative and &quot;pos&quot;
for positive</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="https://www.cs.cornell.edu/people/pabo/movie-review-data/">https://www.cs.cornell.edu/people/pabo/movie-review-data/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dataset_sentence_polarity()

# Custom directory
dataset_sentence_polarity(dir = "data/")

# Deleting dataset
dataset_sentence_polarity(delete = TRUE)

# Returning filepath of data
dataset_sentence_polarity(return_path = TRUE)

## End(Not run)

</code></pre>

<hr>
<h2 id='dataset_trec'>TREC dataset</h2><span id='topic+dataset_trec'></span>

<h3>Description</h3>

<p>The TREC dataset is dataset for question classification consisting of
open-domain, fact-based questions divided into broad semantic categories.
It has both a six-class (TREC-6) and a fifty-class (TREC-50) version. Both
have 5,452 training examples and 500 test examples, but TREC-50 has
finer-grained labels. Models are evaluated based on accuracy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_trec(
  dir = NULL,
  split = c("train", "test"),
  version = c("6", "50"),
  delete = FALSE,
  return_path = FALSE,
  clean = FALSE,
  manual_download = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_trec_+3A_dir">dir</code></td>
<td>
<p>Character, path to directory where data will be stored. If
<code>NULL</code>, <a href="rappdirs.html#topic+user_cache_dir">user_cache_dir</a> will be used to determine path.</p>
</td></tr>
<tr><td><code id="dataset_trec_+3A_split">split</code></td>
<td>
<p>Character. Return training (&quot;train&quot;) data or testing (&quot;test&quot;)
data. Defaults to &quot;train&quot;.</p>
</td></tr>
<tr><td><code id="dataset_trec_+3A_version">version</code></td>
<td>
<p>Character. Version 6(&quot;6&quot;) or version 50(&quot;50&quot;). Defaults to
&quot;6&quot;.</p>
</td></tr>
<tr><td><code id="dataset_trec_+3A_delete">delete</code></td>
<td>
<p>Logical, set <code>TRUE</code> to delete dataset.</p>
</td></tr>
<tr><td><code id="dataset_trec_+3A_return_path">return_path</code></td>
<td>
<p>Logical, set <code>TRUE</code> to return the path of the dataset.</p>
</td></tr>
<tr><td><code id="dataset_trec_+3A_clean">clean</code></td>
<td>
<p>Logical, set <code>TRUE</code> to remove intermediate files. This can
greatly reduce the size. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="dataset_trec_+3A_manual_download">manual_download</code></td>
<td>
<p>Logical, set <code>TRUE</code> if you have manually
downloaded the file and placed it in the folder designated by running
this function with <code>return_path = TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The classes in TREC-6 are
</p>

<ul>
<li><p> ABBR - Abbreviation
</p>
</li>
<li><p> DESC - Description and abstract concepts
</p>
</li>
<li><p> ENTY - Entities
</p>
</li>
<li><p> HUM - Human beings
</p>
</li>
<li><p> LOC - Locations
</p>
</li>
<li><p> NYM - Numeric values
</p>
</li></ul>

<p>the classes in TREC-50 can be found here
<a href="https://cogcomp.seas.upenn.edu/Data/QA/QC/definition.html">https://cogcomp.seas.upenn.edu/Data/QA/QC/definition.html</a>.
</p>


<h3>Value</h3>

<p>A tibble with 5,452 or 500 rows for &quot;train&quot; and &quot;test&quot;
respectively and 2 variables:
</p>

<dl>
<dt>class</dt><dd><p>Character, denoting the class</p>
</dd>
<dt>text</dt><dd><p>Character, question text</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="https://cogcomp.seas.upenn.edu/Data/QA/QC/">https://cogcomp.seas.upenn.edu/Data/QA/QC/</a>
</p>
<p><a href="https://trec.nist.gov/data/qa.html">https://trec.nist.gov/data/qa.html</a>
</p>


<h3>See Also</h3>

<p>Other topic: 
<code><a href="#topic+dataset_ag_news">dataset_ag_news</a>()</code>,
<code><a href="#topic+dataset_dbpedia">dataset_dbpedia</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dataset_trec()

# Custom directory
dataset_trec(dir = "data/")

# Deleting dataset
dataset_trec(delete = TRUE)

# Returning filepath of data
dataset_trec(return_path = TRUE)

# Access both training and testing dataset
train_6 &lt;- dataset_trec(split = "train")
test_6 &lt;- dataset_trec(split = "test")

train_50 &lt;- dataset_trec(split = "train", version = "50")
test_50 &lt;- dataset_trec(split = "test", version = "50")

## End(Not run)

</code></pre>

<hr>
<h2 id='embedding_glove'>Global Vectors for Word Representation</h2><span id='topic+embedding_glove'></span><span id='topic+embedding_glove6b'></span><span id='topic+embedding_glove27b'></span><span id='topic+embedding_glove42b'></span><span id='topic+embedding_glove840b'></span>

<h3>Description</h3>

<p>The GloVe pre-trained word vectors provide word embeddings created using
varying numbers of tokens.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>embedding_glove6b(
  dir = NULL,
  dimensions = c(50, 100, 200, 300),
  delete = FALSE,
  return_path = FALSE,
  clean = FALSE,
  manual_download = FALSE
)

embedding_glove27b(
  dir = NULL,
  dimensions = c(25, 50, 100, 200),
  delete = FALSE,
  return_path = FALSE,
  clean = FALSE,
  manual_download = FALSE
)

embedding_glove42b(
  dir = NULL,
  delete = FALSE,
  return_path = FALSE,
  clean = FALSE,
  manual_download = FALSE
)

embedding_glove840b(
  dir = NULL,
  delete = FALSE,
  return_path = FALSE,
  clean = FALSE,
  manual_download = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="embedding_glove_+3A_dir">dir</code></td>
<td>
<p>Character, path to directory where data will be stored. If
<code>NULL</code>, <a href="rappdirs.html#topic+user_cache_dir">user_cache_dir</a> will be used to determine path.</p>
</td></tr>
<tr><td><code id="embedding_glove_+3A_dimensions">dimensions</code></td>
<td>
<p>A number indicating the number of vectors to include. One
of 50, 100, 200, or 300 for glove6b, or one of 25, 50, 100, or 200 for
glove27b.</p>
</td></tr>
<tr><td><code id="embedding_glove_+3A_delete">delete</code></td>
<td>
<p>Logical, set <code>TRUE</code> to delete dataset.</p>
</td></tr>
<tr><td><code id="embedding_glove_+3A_return_path">return_path</code></td>
<td>
<p>Logical, set <code>TRUE</code> to return the path of the dataset.</p>
</td></tr>
<tr><td><code id="embedding_glove_+3A_clean">clean</code></td>
<td>
<p>Logical, set <code>TRUE</code> to remove intermediate files. This can
greatly reduce the size. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="embedding_glove_+3A_manual_download">manual_download</code></td>
<td>
<p>Logical, set <code>TRUE</code> if you have manually
downloaded the file and placed it in the folder designated by running
this function with <code>return_path = TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Citation info:
</p>
<p>InProceedings{pennington2014glove, <br />
author     = {Jeffrey Pennington and Richard Socher and Christopher D. <br />
Manning}, <br />
title      = {GloVe: Global Vectors for Word Representation}, <br />
booktitle  = {Empirical Methods in Natural Language Processing (EMNLP)}, <br />
year       = 2014 <br />
pages      = {1532-1543} <br />
url        = {http://www.aclweb.org/anthology/D14-1162} <br />
}
</p>


<h3>Value</h3>

<p>A tibble with 400k, 1.9m, 2.2m, or 1.2m rows (one row for each unique
token in the vocabulary) and the following variables:
</p>

<dl>
<dt>token</dt><dd><p>An individual token (usually a word)</p>
</dd>
<dt>d1, d2, etc</dt><dd><p>The embeddings for that token.</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a>
</p>


<h3>References</h3>

<p>Jeffrey Pennington, Richard Socher, and Christopher D. Manning.
2014. GloVe: Global Vectors for Word Representation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
embedding_glove6b(dimensions = 50)

# Custom directory
embedding_glove42b(dir = "data/")

# Deleting dataset
embedding_glove6b(delete = TRUE, dimensions = 300)

# Returning filepath of data
embedding_glove840b(return_path = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='lexicon_afinn'>AFINN-111 dataset</h2><span id='topic+lexicon_afinn'></span>

<h3>Description</h3>

<p>AFINN is a lexicon of English words rated for valence with an integer
between minus five (negative) and plus five (positive). The words have
been manually labeled by Finn Årup Nielsen in 2009-2011.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lexicon_afinn(
  dir = NULL,
  delete = FALSE,
  return_path = FALSE,
  clean = FALSE,
  manual_download = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lexicon_afinn_+3A_dir">dir</code></td>
<td>
<p>Character, path to directory where data will be stored. If
<code>NULL</code>, <a href="rappdirs.html#topic+user_cache_dir">user_cache_dir</a> will be used to determine path.</p>
</td></tr>
<tr><td><code id="lexicon_afinn_+3A_delete">delete</code></td>
<td>
<p>Logical, set <code>TRUE</code> to delete dataset.</p>
</td></tr>
<tr><td><code id="lexicon_afinn_+3A_return_path">return_path</code></td>
<td>
<p>Logical, set <code>TRUE</code> to return the path of the dataset.</p>
</td></tr>
<tr><td><code id="lexicon_afinn_+3A_clean">clean</code></td>
<td>
<p>Logical, set <code>TRUE</code> to remove intermediate files. This can
greatly reduce the size. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="lexicon_afinn_+3A_manual_download">manual_download</code></td>
<td>
<p>Logical, set <code>TRUE</code> if you have manually
downloaded the file and placed it in the folder designated by running
this function with <code>return_path = TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This dataset is the newest version with 2477 words and phrases.
</p>
<p>Citation info:
</p>
<p>This dataset was published in Finn Ärup Nielsen (2011),
&ldquo;A new ANEW: Evaluation of a word list for sentiment analysis in
microblogs&rdquo;, Proceedings of the ESWC2011 Workshop on
'Making Sense of Microposts': Big things come in small packages (2011) 93-98.
</p>
<p>article{nielsen11, <br />
author    = {Finn Äruprup Nielsen}, <br />
title     = {A new ANEW: Evaluation of a word list for sentiment analysis in microblogs}, <br />
journal   = {CoRR}, <br />
volume    = {abs/1103.2903}, <br />
year      = {2011}, <br />
url       = {http://arxiv.org/abs/1103.2903}, <br />
archivePrefix = {arXiv}, <br />
eprint    = {1103.2903}, <br />
biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1103-2903}, <br />
bibsource = {dblp computer science bibliography, https://dblp.org} <br />
}
</p>


<h3>Value</h3>

<p>A tibble with 2,477 rows and 2 variables:
</p>

<dl>
<dt>word</dt><dd><p>An English word</p>
</dd>
<dt>score</dt><dd><p>Indicator for sentiment: integer between -5 and +5</p>
</dd>
</dl>



<h3>See Also</h3>

<p>Other lexicon: 
<code><a href="#topic+lexicon_bing">lexicon_bing</a>()</code>,
<code><a href="#topic+lexicon_loughran">lexicon_loughran</a>()</code>,
<code><a href="#topic+lexicon_nrc_eil">lexicon_nrc_eil</a>()</code>,
<code><a href="#topic+lexicon_nrc_vad">lexicon_nrc_vad</a>()</code>,
<code><a href="#topic+lexicon_nrc">lexicon_nrc</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
lexicon_afinn()

# Custom directory
lexicon_afinn(dir = "data/")

# Deleting dataset
lexicon_afinn(delete = TRUE)

# Returning filepath of data
lexicon_afinn(return_path = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='lexicon_bing'>Bing sentiment lexicon</h2><span id='topic+lexicon_bing'></span>

<h3>Description</h3>

<p>General purpose English sentiment lexicon that categorizes words in a
binary fashion, either positive or negative
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lexicon_bing(
  dir = NULL,
  delete = FALSE,
  return_path = FALSE,
  clean = FALSE,
  manual_download = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lexicon_bing_+3A_dir">dir</code></td>
<td>
<p>Character, path to directory where data will be stored. If
<code>NULL</code>, <a href="rappdirs.html#topic+user_cache_dir">user_cache_dir</a> will be used to determine path.</p>
</td></tr>
<tr><td><code id="lexicon_bing_+3A_delete">delete</code></td>
<td>
<p>Logical, set <code>TRUE</code> to delete dataset.</p>
</td></tr>
<tr><td><code id="lexicon_bing_+3A_return_path">return_path</code></td>
<td>
<p>Logical, set <code>TRUE</code> to return the path of the dataset.</p>
</td></tr>
<tr><td><code id="lexicon_bing_+3A_clean">clean</code></td>
<td>
<p>Logical, set <code>TRUE</code> to remove intermediate files. This can
greatly reduce the size. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="lexicon_bing_+3A_manual_download">manual_download</code></td>
<td>
<p>Logical, set <code>TRUE</code> if you have manually
downloaded the file and placed it in the folder designated by running
this function with <code>return_path = TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Citation info:
</p>
<p>This dataset was first published in Minqing Hu and Bing Liu, &ldquo;Mining and
summarizing customer reviews.&rdquo;, Proceedings of the ACM SIGKDD International
Conference on Knowledge Discovery &amp; Data Mining (KDD-2004), 2004.
</p>
<p>inproceedings{Hu04, <br />
author    = {Hu, Minqing and Liu, Bing}, <br />
title     = {Mining and Summarizing Customer Reviews}, <br />
booktitle = {Proceedings of the Tenth ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining}, <br />
series    = {KDD '04}, <br />
year      = {2004}, <br />
isbn      = {1-58113-888-1}, <br />
location  = {Seattle, WA, USA}, <br />
pages     = {168&ndash;177}, <br />
numpages  = {10}, <br />
url       = {http://doi.acm.org/10.1145/1014052.1014073}, <br />
doi       = {10.1145/1014052.1014073}, <br />
acmid     = {1014073}, <br />
publisher = {ACM}, <br />
address   = {New York, NY, USA}, <br />
keywords  = {reviews, sentiment classification, summarization, text mining}, <br />
}
</p>


<h3>Value</h3>

<p>A tibble with 6,787 rows and 2 variables:
</p>

<dl>
<dt>word</dt><dd><p>An English word</p>
</dd>
<dt>sentiment</dt><dd><p>Indicator for sentiment: &quot;negative&quot; or &quot;positive&quot;</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html">https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html</a>
</p>


<h3>See Also</h3>

<p>Other lexicon: 
<code><a href="#topic+lexicon_afinn">lexicon_afinn</a>()</code>,
<code><a href="#topic+lexicon_loughran">lexicon_loughran</a>()</code>,
<code><a href="#topic+lexicon_nrc_eil">lexicon_nrc_eil</a>()</code>,
<code><a href="#topic+lexicon_nrc_vad">lexicon_nrc_vad</a>()</code>,
<code><a href="#topic+lexicon_nrc">lexicon_nrc</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
lexicon_bing()

# Custom directory
lexicon_bing(dir = "data/")

# Deleting dataset
lexicon_bing(delete = TRUE)

# Returning filepath of data
lexicon_bing(return_path = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='lexicon_loughran'>Loughran-McDonald sentiment lexicon</h2><span id='topic+lexicon_loughran'></span>

<h3>Description</h3>

<p>English sentiment lexicon created for use with financial documents. This
lexicon labels words with six possible sentiments important in financial
contexts: &quot;negative&quot;, &quot;positive&quot;, &quot;litigious&quot;, &quot;uncertainty&quot;, &quot;constraining&quot;,
or &quot;superfluous&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lexicon_loughran(
  dir = NULL,
  delete = FALSE,
  return_path = FALSE,
  clean = FALSE,
  manual_download = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lexicon_loughran_+3A_dir">dir</code></td>
<td>
<p>Character, path to directory where data will be stored. If
<code>NULL</code>, <a href="rappdirs.html#topic+user_cache_dir">user_cache_dir</a> will be used to determine path.</p>
</td></tr>
<tr><td><code id="lexicon_loughran_+3A_delete">delete</code></td>
<td>
<p>Logical, set <code>TRUE</code> to delete dataset.</p>
</td></tr>
<tr><td><code id="lexicon_loughran_+3A_return_path">return_path</code></td>
<td>
<p>Logical, set <code>TRUE</code> to return the path of the dataset.</p>
</td></tr>
<tr><td><code id="lexicon_loughran_+3A_clean">clean</code></td>
<td>
<p>Logical, set <code>TRUE</code> to remove intermediate files. This can
greatly reduce the size. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="lexicon_loughran_+3A_manual_download">manual_download</code></td>
<td>
<p>Logical, set <code>TRUE</code> if you have manually
downloaded the file and placed it in the folder designated by running
this function with <code>return_path = TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Citation info:
</p>
<p>This dataset was published in Loughran, T. and McDonald, B. (2011),
&ldquo;When Is a Liability Not a Liability? Textual Analysis, Dictionaries, and
10-Ks.&rdquo; The Journal of Finance, 66: 35-65.
</p>
<p>article{loughran11, <br />
author  = {Loughran, Tim and McDonald, Bill}, <br />
title   = {When Is a Liability Not a Liability? Textual Analysis, Dictionaries, and 10-Ks}, <br />
journal = {The Journal of Finance}, <br />
volume  = {66}, <br />
number  = {1}, <br />
pages   = {35-65}, <br />
doi     = {10.1111/j.1540-6261.2010.01625.x}, <br />
url     = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.2010.01625.x}, <br />
eprint  = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1540-6261.2010.01625.x}, <br />
year    = {2011} <br />
}
</p>


<h3>Value</h3>

<p>A tibble with 4,150 rows and 2 variables:
</p>

<dl>
<dt>word</dt><dd><p>An English word</p>
</dd>
<dt>sentiment</dt><dd><p>Indicator for sentiment: &quot;negative&quot;, &quot;positive&quot;,
&quot;litigious&quot;, &quot;uncertainty&quot;, &quot;constraining&quot;, or &quot;superfluous&quot;</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="https://sraf.nd.edu/loughranmcdonald-master-dictionary/">https://sraf.nd.edu/loughranmcdonald-master-dictionary/</a>
</p>


<h3>See Also</h3>

<p>Other lexicon: 
<code><a href="#topic+lexicon_afinn">lexicon_afinn</a>()</code>,
<code><a href="#topic+lexicon_bing">lexicon_bing</a>()</code>,
<code><a href="#topic+lexicon_nrc_eil">lexicon_nrc_eil</a>()</code>,
<code><a href="#topic+lexicon_nrc_vad">lexicon_nrc_vad</a>()</code>,
<code><a href="#topic+lexicon_nrc">lexicon_nrc</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
lexicon_loughran()

# Custom directory
lexicon_loughran(dir = "data/")

# Deleting dataset
lexicon_loughran(delete = TRUE)

# Returning filepath of data
lexicon_loughran(return_path = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='lexicon_nrc'>NRC word-emotion association lexicon</h2><span id='topic+lexicon_nrc'></span>

<h3>Description</h3>

<p>General purpose English sentiment/emotion lexicon. This lexicon labels words
with six possible sentiments or emotions: &quot;negative&quot;, &quot;positive&quot;, &quot;anger&quot;,
&quot;anticipation&quot;, &quot;disgust&quot;, &quot;fear&quot;, &quot;joy&quot;, &quot;sadness&quot;, &quot;surprise&quot;, or &quot;trust&quot;.
The annotations were manually done through Amazon's Mechanical Turk.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lexicon_nrc(
  dir = NULL,
  delete = FALSE,
  return_path = FALSE,
  clean = FALSE,
  manual_download = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lexicon_nrc_+3A_dir">dir</code></td>
<td>
<p>Character, path to directory where data will be stored. If
<code>NULL</code>, <a href="rappdirs.html#topic+user_cache_dir">user_cache_dir</a> will be used to determine path.</p>
</td></tr>
<tr><td><code id="lexicon_nrc_+3A_delete">delete</code></td>
<td>
<p>Logical, set <code>TRUE</code> to delete dataset.</p>
</td></tr>
<tr><td><code id="lexicon_nrc_+3A_return_path">return_path</code></td>
<td>
<p>Logical, set <code>TRUE</code> to return the path of the dataset.</p>
</td></tr>
<tr><td><code id="lexicon_nrc_+3A_clean">clean</code></td>
<td>
<p>Logical, set <code>TRUE</code> to remove intermediate files. This can
greatly reduce the size. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="lexicon_nrc_+3A_manual_download">manual_download</code></td>
<td>
<p>Logical, set <code>TRUE</code> if you have manually
downloaded the file and placed it in the folder designated by running
this function with <code>return_path = TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>License required for commercial use. Please contact Saif M. Mohammad
(saif.mohammad@nrc-cnrc.gc.ca).
</p>
<p>Citation info:
</p>
<p>This dataset was published in Saif Mohammad and Peter Turney. (2013),
&ldquo;Crowdsourcing a Word-Emotion Association Lexicon.&rdquo; Computational
Intelligence, 29(3): 436-465.
</p>
<p>article{mohammad13, <br />
author = {Mohammad, Saif M. and Turney, Peter D.}, <br />
title = {CROWDSOURCING A WORD–EMOTION ASSOCIATION LEXICON}, <br />
journal = {Computational Intelligence}, <br />
volume = {29}, <br />
number = {3}, <br />
pages = {436-465}, <br />
doi = {10.1111/j.1467-8640.2012.00460.x}, <br />
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8640.2012.00460.x}, <br />
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8640.2012.00460.x}, <br />
year = {2013} <br />
}
</p>


<h3>Value</h3>

<p>A tibble with 13,901 rows and 2 variables:
</p>

<dl>
<dt>word</dt><dd><p>An English word</p>
</dd>
<dt>sentiment</dt><dd><p>Indicator for sentiment or emotion: &quot;negative&quot;,
&quot;positive&quot;, &quot;anger&quot;, &quot;anticipation&quot;, &quot;disgust&quot;, &quot;fear&quot;, &quot;joy&quot;, &quot;sadness&quot;,
&quot;surprise&quot;, or &quot;trust&quot;</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="http://saifmohammad.com/WebPages/lexicons.html">http://saifmohammad.com/WebPages/lexicons.html</a>
</p>


<h3>See Also</h3>

<p>Other lexicon: 
<code><a href="#topic+lexicon_afinn">lexicon_afinn</a>()</code>,
<code><a href="#topic+lexicon_bing">lexicon_bing</a>()</code>,
<code><a href="#topic+lexicon_loughran">lexicon_loughran</a>()</code>,
<code><a href="#topic+lexicon_nrc_eil">lexicon_nrc_eil</a>()</code>,
<code><a href="#topic+lexicon_nrc_vad">lexicon_nrc_vad</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
lexicon_nrc()

# Custom directory
lexicon_nrc(dir = "data/")

# Deleting dataset
lexicon_nrc(delete = TRUE)

# Returning filepath of data
lexicon_nrc(return_path = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='lexicon_nrc_eil'>NRC Emotion Intensity Lexicon (aka Affect Intensity Lexicon) v0.5</h2><span id='topic+lexicon_nrc_eil'></span>

<h3>Description</h3>

<p>General purpose English sentiment/emotion lexicon. The NRC Affect Intensity
Lexicon is a list of English words and their associations with four basic
emotions (anger, fear, sadness, joy).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lexicon_nrc_eil(
  dir = NULL,
  delete = FALSE,
  return_path = FALSE,
  clean = FALSE,
  manual_download = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lexicon_nrc_eil_+3A_dir">dir</code></td>
<td>
<p>Character, path to directory where data will be stored. If
<code>NULL</code>, <a href="rappdirs.html#topic+user_cache_dir">user_cache_dir</a> will be used to determine path.</p>
</td></tr>
<tr><td><code id="lexicon_nrc_eil_+3A_delete">delete</code></td>
<td>
<p>Logical, set <code>TRUE</code> to delete dataset.</p>
</td></tr>
<tr><td><code id="lexicon_nrc_eil_+3A_return_path">return_path</code></td>
<td>
<p>Logical, set <code>TRUE</code> to return the path of the dataset.</p>
</td></tr>
<tr><td><code id="lexicon_nrc_eil_+3A_clean">clean</code></td>
<td>
<p>Logical, set <code>TRUE</code> to remove intermediate files. This can
greatly reduce the size. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="lexicon_nrc_eil_+3A_manual_download">manual_download</code></td>
<td>
<p>Logical, set <code>TRUE</code> if you have manually
downloaded the file and placed it in the folder designated by running
this function with <code>return_path = TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a given word and emotion X, the scores range from 0 to 1. A score of 1
means that the word conveys the highest amount of emotion X.  A score of 0
means that the word conveys the lowest amount of emotion X.
</p>
<p>License required for commercial use. Please contact Saif M. Mohammad
(saif.mohammad@nrc-cnrc.gc.ca).
</p>
<p>Citation info:
</p>
<p>Details of the lexicon are in this paper.
Word Affect Intensities. Saif M. Mohammad. In Proceedings of the 11th Edition
of the Language Resources and Evaluation Conference (LREC-2018), May 2018,
Miyazaki, Japan.
</p>
<p>inproceedings{LREC18-AIL, <br />
author = {Mohammad, Saif M.}, <br />
title = {Word Affect Intensities}, <br />
booktitle = {Proceedings of the 11th Edition of the Language Resources and Evaluation Conference (LREC-2018)}, <br />
year = {2018}, <br />
address={Miyazaki, Japan} <br />
} <br />
</p>


<h3>Value</h3>

<p>A tibble with 5.814 rows and 3 variables:
</p>

<dl>
<dt>term</dt><dd><p>An English word</p>
</dd>
<dt>score</dt><dd><p>Value between 0 and 1</p>
</dd>
<dt>AffectDimension</dt><dd><p>Indicator for sentiment or emotion: (&quot;anger&quot;,
&quot;fear&quot;, &quot;sadness&quot;, &quot;joy&quot;)</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="https://saifmohammad.com/WebPages/AffectIntensity.htm">https://saifmohammad.com/WebPages/AffectIntensity.htm</a>
</p>


<h3>See Also</h3>

<p>Other lexicon: 
<code><a href="#topic+lexicon_afinn">lexicon_afinn</a>()</code>,
<code><a href="#topic+lexicon_bing">lexicon_bing</a>()</code>,
<code><a href="#topic+lexicon_loughran">lexicon_loughran</a>()</code>,
<code><a href="#topic+lexicon_nrc_vad">lexicon_nrc_vad</a>()</code>,
<code><a href="#topic+lexicon_nrc">lexicon_nrc</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
lexicon_nrc_eil()

# Custom directory
lexicon_nrc_eil(dir = "data/")

# Deleting dataset
lexicon_nrc_eil(delete = TRUE)

# Returning filepath of data
lexicon_nrc_eil(return_path = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='lexicon_nrc_vad'>The NRC Valence, Arousal, and Dominance Lexicon</h2><span id='topic+lexicon_nrc_vad'></span>

<h3>Description</h3>

<p>The NRC Valence, Arousal, and Dominance (VAD) Lexicon includes a list of
more than 20,000 English words and their valence, arousal, and dominance
scores. For a given word and a dimension (V/A/D), the scores range from 0
(lowest V/A/D) to 1 (highest V/A/D). The lexicon with its fine-grained real-
valued scores was created by manual annotation using best&ndash;worst scaling.
The lexicon is markedly larger than any of the existing VAD lexicons. We also
show that the ratings obtained are substantially more reliable than those in
existing lexicons.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lexicon_nrc_vad(
  dir = NULL,
  delete = FALSE,
  return_path = FALSE,
  clean = FALSE,
  manual_download = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lexicon_nrc_vad_+3A_dir">dir</code></td>
<td>
<p>Character, path to directory where data will be stored. If
<code>NULL</code>, <a href="rappdirs.html#topic+user_cache_dir">user_cache_dir</a> will be used to determine path.</p>
</td></tr>
<tr><td><code id="lexicon_nrc_vad_+3A_delete">delete</code></td>
<td>
<p>Logical, set <code>TRUE</code> to delete dataset.</p>
</td></tr>
<tr><td><code id="lexicon_nrc_vad_+3A_return_path">return_path</code></td>
<td>
<p>Logical, set <code>TRUE</code> to return the path of the dataset.</p>
</td></tr>
<tr><td><code id="lexicon_nrc_vad_+3A_clean">clean</code></td>
<td>
<p>Logical, set <code>TRUE</code> to remove intermediate files. This can
greatly reduce the size. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="lexicon_nrc_vad_+3A_manual_download">manual_download</code></td>
<td>
<p>Logical, set <code>TRUE</code> if you have manually
downloaded the file and placed it in the folder designated by running
this function with <code>return_path = TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>License required for commercial use. Please contact Saif M. Mohammad
(saif.mohammad@nrc-cnrc.gc.ca).
</p>
<p>Citation info:
</p>
<p>Details of the NRC VAD Lexicon are available in this paper:
</p>
<p>Obtaining Reliable Human Ratings of Valence, Arousal, and Dominance for
20,000 English Words.  Saif M. Mohammad. In Proceedings of the 56th Annual
Meeting of the Association for Computational Linguistics, Melbourne,
Australia, July 2018.
</p>
<p>inproceedings{vad-acl2018, <br />
title={Obtaining Reliable Human Ratings of Valence, Arousal, and Dominance for 20,000 English Words}, <br />
author={Mohammad, Saif M.}, <br />
booktitle={Proceedings of The Annual Conference of the Association for Computational Linguistics (ACL)}, <br />
year={2018}, <br />
address={Melbourne, Australia} <br />
}
</p>


<h3>Value</h3>

<p>A tibble with 20.007 rows and 4 variables:
</p>

<dl>
<dt>word</dt><dd><p>An English word</p>
</dd>
<dt>Valence</dt><dd><p>valence score of the word</p>
</dd>
<dt>Arousal</dt><dd><p>arousal score of the word</p>
</dd>
<dt>Dominance</dt><dd><p>dominance score of the word</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="https://saifmohammad.com/WebPages/nrc-vad.html">https://saifmohammad.com/WebPages/nrc-vad.html</a>
</p>


<h3>See Also</h3>

<p>Other lexicon: 
<code><a href="#topic+lexicon_afinn">lexicon_afinn</a>()</code>,
<code><a href="#topic+lexicon_bing">lexicon_bing</a>()</code>,
<code><a href="#topic+lexicon_loughran">lexicon_loughran</a>()</code>,
<code><a href="#topic+lexicon_nrc_eil">lexicon_nrc_eil</a>()</code>,
<code><a href="#topic+lexicon_nrc">lexicon_nrc</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
lexicon_nrc_vad()

# Custom directory
lexicon_nrc_vad(dir = "data/")

# Deleting dataset
lexicon_nrc_vad(delete = TRUE)

# Returning filepath of data
lexicon_nrc_vad(return_path = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='load_dataset'>Internal Functions</h2><span id='topic+load_dataset'></span>

<h3>Description</h3>

<p>These are not to be used directly by the users.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_dataset(
  data_name,
  name,
  dir,
  delete,
  return_path,
  clean,
  clean_manual = NULL,
  manual_download
)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
