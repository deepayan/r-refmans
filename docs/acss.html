<!DOCTYPE html><html><head><title>Help for package acss</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {acss}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#acss'><p>ACSS complexity</p></a></li>
<li><a href='#entropy'><p>Standard measures of complexity for strings</p></a></li>
<li><a href='#exp1'><p>Data from Experiment 1 in Gauvrit, Singmann, Soler-Toscano &amp; Zenil</p></a></li>
<li><a href='#exp2'><p>Data from Experiment 2 in Gauvrit, Singmann, Soler-Toscano &amp; Zenil</p></a></li>
<li><a href='#matthews2013'><p>Data from Experiment 1 in Matthews (2013)</p></a></li>
<li><a href='#normalize_string'><p>Helper functions for calculating cognitive complexity.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Algorithmic Complexity for Short Strings</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2-5</td>
</tr>
<tr>
<td>Date:</td>
<td>2014-11-23</td>
</tr>
<tr>
<td>Imports:</td>
<td>zoo</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.15.0), acss.data</td>
</tr>
<tr>
<td>Suggests:</td>
<td>effects, lattice</td>
</tr>
<tr>
<td>Description:</td>
<td>Main functionality is to provide the algorithmic complexity for
    short strings, an approximation of the Kolmogorov Complexity of a short
    string using the coding theorem method (see ?acss). The database containing
    the complexity is provided in the data only package acss.data, this package
    provides functions accessing the data such as prob_random returning the
    posterior probability that a given string was produced by a random process.
    In addition, two traditional (but problematic) measures of complexity are
    also provided: entropy and change complexity.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://complexitycalculator.com/methodology.html">http://complexitycalculator.com/methodology.html</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2014-11-24 19:52:41 UTC; Henrik</td>
</tr>
<tr>
<td>Author:</td>
<td>Nicolas Gauvrit [aut],
  Henrik Singmann [aut, cre],
  Fernando Soler Toscano [ctb],
  Hector Zenil [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Henrik Singmann &lt;singmann+acss@gmail.com&gt;</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2014-11-25 08:45:18</td>
</tr>
</table>
<hr>
<h2 id='acss'>ACSS complexity</h2><span id='topic+acss'></span><span id='topic+likelihood_d'></span><span id='topic+likelihood_ratio'></span><span id='topic+local_complexity'></span><span id='topic+prob_random'></span>

<h3>Description</h3>

<p>Functions to obtain the algorithmic complexity for short strings, an approximation of the Kolmogorov Complexity of a short string using the coding theorem method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>acss(string, alphabet = 9)

local_complexity(string, alphabet = 9, span = 5)

likelihood_d(string, alphabet = 9)

likelihood_ratio(string, alphabet = 9)

prob_random(string, alphabet = 9, prior= 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="acss_+3A_string">string</code></td>
<td>
<p><code>character</code> vector containing the to be analyzed strings (can contain multiple strings).</p>
</td></tr>
<tr><td><code id="acss_+3A_alphabet">alphabet</code></td>
<td>
<p><code>numeric</code>, the number of possible symbols (not necessarily actually appearing in str). Must be one of <code>c(2, 4, 5, 6, 9)</code> (can also be <code>NULL</code> or contain multiple values for <code>acss()</code>). Default is 9.</p>
</td></tr>
<tr><td><code id="acss_+3A_prior">prior</code></td>
<td>
<p><code>numeric</code>,  the prior probability that the underlying process is random.</p>
</td></tr>
<tr><td><code id="acss_+3A_span">span</code></td>
<td>
<p>size of substrings to be created from <code>string</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithmic complexity is computed using the coding theorem method: For a given alphabet size (number of different symbols in a string), all possible or a large number of random samples of Turing machines (TM) with a given number of states (e.g., 5) and number of symbols corresponding to the alphabet size were simulated until they reached a halting state or failed to end.
The outputs of the TMs at the halting states produces a distribution of strings known as the algorithmic probability of the strings. The algorithmic coding theorem (Levin, 1974) establishes the connection between the complexity of a string <code class="reqn">K(s)</code> and its algorithmic probability <code class="reqn">D(s)</code> as:
</p>
<p style="text-align: center;"><code class="reqn">K(s)\approx -\log_{2}(D(s))</code>
</p>

<p>This package accesses a database containing data on 4.5 million strings from length 1 to 12 simulated on TMs with 2, 4, 5, 6, and 9 symbols.
</p>
<p>For a more detailed discussion see Gauvrit, Singmann, Soler-Toscano, and Zenil (2014), <a href="http://complexitycalculator.com/methodology.html">http://complexitycalculator.com/methodology.html</a>, or references below.
</p>


<h3>Value</h3>


<dl>
<dt>&quot;acss&quot;</dt><dd><p>A matrix in which the rows correspond to the strings entered and the columns to the algorithmic complexity K and the algorithmic probability D of the string (see <a href="http://complexitycalculator.com/methodology.html">http://complexitycalculator.com/methodology.html</a>).</p>
</dd>
<dt>&quot;local_complexity&quot;</dt><dd><p>A list with elements corresponding to the strings. Each list containes a named vector of algorithmic complexities (K) of all substrings in each string with length span.</p>
</dd>
<dt>&quot;likelihood_d&quot;</dt><dd><p>A named vector with the likelihoods for <code>string</code> given a detreministic process.</p>
</dd>
<dt>&quot;likelihood_ratio&quot;</dt><dd><p>A named vector with the likelihood ratios (or Bayes factors) for <code>string</code> given a random rather than detreministic process.</p>
</dd>
<dt>&quot;prob_random&quot;</dt><dd><p>A named vector with the posterior probabilities that for a random process given the strings and the provided prior for being produced by a random process (default is 0.5, which correspond to a prior of 1 - 0.5 = 0.5 for a detereministic process).</p>
</dd>
</dl>



<h3>Note</h3>

<p>The first time per session one of the functions described here is used, a relatively large dataset is loaded into memory which can take a considerable amount of time (&gt; 10 seconds).
</p>


<h3>References</h3>

<p>Delahaye, J.-P., &amp; Zenil, H. (2012). Numerical evaluation of algorithmic complexity for short strings: A glance into the innermost structure of randomness. <em>Applied Mathematics and Computation</em>, 219(1), 63-77. doi:10.1016/j.amc.2011.10.006
</p>
<p>Gauvrit, N., Singmann, H., Soler-Toscano, F., &amp; Zenil, H. (2014). Algorithmic complexity for psychology: A user-friendly implementation of the coding theorem method. arXiv:1409.4080 [cs, stat]. <a href="http://arxiv.org/abs/1409.4080">http://arxiv.org/abs/1409.4080</a>.
</p>
<p>Gauvrit, N., Zenil, H., Delahaye, J.-P., &amp; Soler-Toscano, F. (2014). Algorithmic complexity for short binary strings applied to psychology: a primer. <em>Behavior Research Methods</em>, 46(3), 732-744. doi:10.3758/s13428-013-0416-0
</p>
<p>Levin, L. A. (1974). Laws of information conservation (nongrowth) and aspects of the foundation of probability theory. <em>Problemy Peredachi Informatsii</em>, 10(3), 30-35.
</p>
<p>Soler-Toscano, F., Zenil, H., Delahaye, J.-P., &amp; Gauvrit, N. (2012). Calculating Kolmogorov Complexity from the Output Frequency Distributions of Small Turing Machines. <em>PLoS ONE</em>, 9(5): e96223.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# WARNING: The first call to one of the functions
# discussed on this page loads a large data set 
# and usually takes &gt; 10 seconds. Stay patient.

acss(c("HEHHEE", "GHHGGHGHH", "HSHSHHSHSS"))
##                 K.9          D.9
## HEHHEE     23.38852 9.106564e-08
## GHHGGHGHH  33.50168 8.222205e-11
## HSHSHHSHSS 35.15241 2.618613e-11

acss(c("HEHHEE", "GHHGGHGHH", "HSHSHHSHSS"))[,"K.9"]
## [1] 23.38852 33.50168 35.15241

acss(c("HEHHEE", "GHHGGHGHH", "HSHSHHSHSS"), alphabet = 2)
##                 K.2          D.2
## HEHHEE     14.96921 3.117581e-05
## GHHGGHGHH  25.60208 1.963387e-08
## HSHSHHSHSS 26.90906 7.935321e-09

acss(c("HEHHEE", "GHHGGHGHUE", "HSHSHHSHSS"), NULL)
##                 K.2      K.4      K.5      K.6      K.9
## HEHHEE     14.96921 18.55227 19.70361 20.75762 23.38852
## GHHGGHGHUE       NA 31.75832 33.00795 34.27457 37.78935
## HSHSHHSHSS 26.90906 29.37852 30.52566 31.76229 35.15241
##                     D.2          D.4          D.5          D.6
## HEHHEE     3.117581e-05 2.601421e-06 1.171176e-06 5.640722e-07
## GHHGGHGHUE           NA 2.752909e-10 1.157755e-10 4.812021e-11
## HSHSHHSHSS 7.935321e-09 1.432793e-09 6.469341e-10 2.745360e-10
##                     D.9
## HEHHEE     9.106564e-08
## GHHGGHGHUE 4.209915e-12
## HSHSHHSHSS 2.618613e-11

## Not run: 
likelihood_d(c("HTHTHTHT", "HTHHTHTT"), alphabet = 2)
##    HTHTHTHT    HTHHTHTT 
## 0.010366951 0.003102718 

likelihood_ratio(c("HTHTHTHT", "HTHHTHTT"), alphabet = 2)
##  HTHTHTHT  HTHHTHTT 
## 0.3767983 1.2589769 

prob_random(c("HTHTHTHT", "HTHHTHTT"), alphabet = 2)
##  HTHTHTHT  HTHHTHTT 
## 0.2736772 0.5573217

## End(Not run)

local_complexity(c("01011010111" ,"GHHGGHGHUE"), alphabet = 5, span=5)
## $`01011010111`
##    01011    10110    01101    11010    10101    01011    10111 
## 16.22469 16.24766 16.24766 16.22469 16.24322 16.22469 15.93927 
## 
## $GHHGGHGHUE
##    GHHGG    HHGGH    HGGHG    GGHGH    GHGHU    HGHUE 
## 16.44639 16.44639 16.24766 16.22469 16.58986 16.86449 


local_complexity(c("01011010111" ,"GHHGGHGHUE"), span=7)
## $`01011010111`
##  0101101  1011010  0110101  1101011  1010111 
## 26.52068 26.52068 26.47782 26.62371 26.29186 
## 
## $GHHGGHGHUE
##  GHHGGHG  HHGGHGH  HGGHGHU  GGHGHUE 
## 27.04623 26.86992 27.30871 27.84322 
</code></pre>

<hr>
<h2 id='entropy'>Standard measures of complexity for strings</h2><span id='topic+change_complexity'></span><span id='topic+entropy'></span><span id='topic+entropy2'></span>

<h3>Description</h3>

<p>Functions to compute different measures of complexity for strings: Entropy, Second-Order Entropy, and Change Complexity
</p>


<h3>Usage</h3>

<pre><code class='language-R'>entropy(string)

entropy2(string)

change_complexity(string)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="entropy_+3A_string">string</code></td>
<td>
<p><code>character</code> vector containing the to be analyzed strings (can contain multiple strings for the entropy measures).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For users who need advanced functions, a comprehensive package computing various versions of entropy estimators is available <span class="pkg">entropy</span>. For users who just need first and second-order entropy and which to apply them to short string, the <span class="pkg">acss</span> package provides two functions: <code>entropy</code> (first-order entropy) and <code>entropy2</code> second-order entropy.
</p>
<p>Change complexity (<code>change_complexity</code>) assesses cognitive complexity or the subjective perception of complexity of a binary string. It has been comprehensively defined by Aksentijevic and Gibson (2012). Although the algorithm will work with any number of symbols up to 10, the rationale of Change Complexity only applies to binary strings.
</p>


<h3>Value</h3>

<p><code>numeric</code>, the complexity of the string. For <code>entropy</code> and <code>entropy2</code> of the same length as <code>string</code>. <code>change_complexity</code> currently only works with inputs of length 1.
</p>


<h3>References</h3>

<p>Aksentijevic &amp; Gibson (2012). Complexity equals change. <em>Cognitive Systems Research</em>, 15-17, 1-16.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>strings1 &lt;- c("010011010001", "0010203928837", "0000000000")
strings2 &lt;- c("001011", "01213", "010101010101")

entropy(strings1)
entropy("XYXXYYXYXXXY") # "same" string as strings1[1]
entropy(c("HUHHEGGTE", "EGGHHU"))

entropy2(strings1)
entropy2("XYXXYYXYXXXY")

entropy2(strings2)

change_complexity(strings1)
change_complexity("XYXXYYXYXXXY")
</code></pre>

<hr>
<h2 id='exp1'>Data from Experiment 1 in Gauvrit, Singmann, Soler-Toscano &amp; Zenil</h2><span id='topic+exp1'></span>

<h3>Description</h3>

<p>34 participants were asked to produce at their own pace a series of 10 symbols among &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, and &quot;D&quot; that would &quot;look as random as possible, so that if someone else sees the sequence, she will believe it is a truly random one&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>exp1
</code></pre>


<h3>Format</h3>

<p>A data.frame with 34 rows and 2 variables.</p>


<h3>Source</h3>

<p>Gauvrit, Singmann, Soler-Toscano &amp; Zenil (submitted). Complexity for psychology. A user-friendly implementation of the coding theorem method.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load data
data(exp1)

# summary statistics
nrow(exp1)
summary(exp1$age)
mean(exp1$age)
sd(exp1$age)

## Not run: 
# this uses code from likelihood_d() to calculate the mean complexity K
# for all strings of length 10 with alphabet = 4:
tmp &lt;- acss_data[nchar(rownames(acss_data)) == 10, "K.4", drop = FALSE]
tmp &lt;- tmp[!is.na(tmp[,"K.4"]),,drop = FALSE]
tmp$count &lt;- count_class(rownames(tmp), alphabet = 4)
(mean_K &lt;- with(tmp, sum(K.4*count)/sum(count)))

t.test(acss(exp1$string, 4)[,"K.4"], mu = mean_K)

## End(Not run)

</code></pre>

<hr>
<h2 id='exp2'>Data from Experiment 2 in Gauvrit, Singmann, Soler-Toscano &amp; Zenil</h2><span id='topic+exp2'></span>

<h3>Description</h3>

<p>Responses of one participant (42 years old) to 200 randomly generated strings of length 10 from an alphabet of 6 symbols.
For each string, the participant was asked to indicate whther or not the string appears random or not.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>exp2
</code></pre>


<h3>Format</h3>

<p>A data.frame with 200 rows and 2 variables.</p>


<h3>Source</h3>

<p>Gauvrit, Singmann, Soler-Toscano &amp; Zenil (submitted). Complexity for psychology. A user-friendly implementation of the coding theorem method.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load data
data(exp2)

exp2$K &lt;- acss(exp2$string, 6)[,"K.6"]

m_log &lt;- glm(response ~ K, exp2, family = binomial)
summary(m_log)

# odds ratio of K:
exp(coef(m_log)[2])

# calculate threshold of 0.5
(threshold &lt;- -coef(m_log)[1]/coef(m_log)[2])

require(effects)
require(lattice)
plot(Effect("K", m_log), rescale.axis = FALSE, ylim = c(0, 1))
trellis.focus("panel", 1, 1)
panel.lines(rep(threshold, 2), c(0, 0.5), col = "black", lwd = 2.5, lty = 3)
panel.lines(c(33,threshold), c(0.5, 0.5), col = "black", lwd = 2.5, lty = 3)
trellis.unfocus()
</code></pre>

<hr>
<h2 id='matthews2013'>Data from Experiment 1 in Matthews (2013)</h2><span id='topic+matthews2013'></span>

<h3>Description</h3>

<p>Mean responses on a 6-point scale (&quot;definitely random&quot; to &quot;definitely not random&quot;) of participants to 216 strings of length 21.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matthews2013
</code></pre>


<h3>Format</h3>

<p>A data.frame with 216 rows and 3 variables.</p>


<h3>Source</h3>

<p>Matthews, W. (2013). Relatively random: Context effects on perceived randomness and predicted outcomes. <em>Journal of Experimental Psychology: Learning, Memory, and Cognition</em>, 39(5), 1642-1648.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
data(matthews2013)

spans &lt;- 3:11
# note, the next loop takes more than 5 minutes.
for (i in spans) {
  matthews2013[,paste0("K2_span", i)] &lt;- 
    sapply(local_complexity(matthews2013$string, alphabet=2, span = i), mean)
}

lm_list &lt;- vector("list", 8)
for (i in seq_along(spans)) {
  lm_list[[i]] &lt;- lm(as.formula(paste0("mean ~ K2_span", spans[i])), matthews2013)
}

plot(spans, sapply(lm_list, function(x) summary(x)$r.squared), type = "o")

# do more predictors increase fit?
require(MASS)
m_initial &lt;- lm(mean ~ 1, matthews2013)
m_step &lt;- stepAIC(m_initial, 
                  scope = as.formula(paste("~", paste(paste0("K2_span", spans), 
                  collapse = "+"))))
summary(m_step)

m_initial2 &lt;- lm(as.formula(paste("mean ~", paste(paste0("K2_span", spans), 
                  collapse = "+"))), matthews2013)
m_step2 &lt;- stepAIC(m_initial2)
summary(m_step2)


## End(Not run)
</code></pre>

<hr>
<h2 id='normalize_string'>Helper functions for calculating cognitive complexity.</h2><span id='topic+alternations'></span><span id='topic+count_class'></span><span id='topic+normalize_string'></span>

<h3>Description</h3>

<p><code>normalize_string</code> takes a character vector and normalizes its input using the symbols 0, 1, 2...9. <code>count_class</code> takes a character vector and an integer <code>alphabet</code> (with the restriction that the number of different symbols in the character vector doesn't exceed <code>alphabet</code>) and returns  the total number of strings that are equivalent to the input when normalized and considering <code>alphabet</code>. <code>alternations</code> returns the number of alternations of symbols in a string.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalize_string(string)

count_class(string, alphabet)

alternations(string, proportion = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="normalize_string_+3A_string">string</code></td>
<td>
<p><code>character</code> vector containing the to be analyzed strings (can contain multiple strings).</p>
</td></tr>
<tr><td><code id="normalize_string_+3A_alphabet">alphabet</code></td>
<td>
<p><code>numeric</code>, the number of possible symbols (not necessarily actually appearing in string).</p>
</td></tr>
<tr><td><code id="normalize_string_+3A_proportion">proportion</code></td>
<td>
<p><code>boolean</code>, indicating if the result from <code>alternation</code> should be given as a proportion (between 0 and 1) or the raw number of alternations (default is <code>FALSE</code> correpsonding to raw values).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>nothing yet.
</p>


<h3>Value</h3>


<dl>
<dt><code>normalize_string</code></dt><dd><p>A normalized vector of strings of the same length as <code>string</code>.</p>
</dd>
<dt><code>count_class</code></dt><dd><p>A vector of the same length as <code>string</code> with the number of possible equivalent strings when <code>string</code> is normalized and considering <code>alphabet</code>.</p>
</dd>
<dt><code>alternations</code></dt><dd><p>A vector with the number (or proprtion) of alternations of the same length as <code>string</code></p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>#normalize_string:
normalize_string(c("HUHHEGGTE", "EGGHHU"))

normalize_string("293948837163536")

# count_class
count_class("010011",2)

count_class("332120",4)

count_class(c("HUHHEGGTE", "EGGHHU"), 5)
count_class(c("HUHHEGGTE", "EGGHHU"), 6)

# alternations:
alternations("0010233")
alternations("0010233", proportion = TRUE)

alternations(c("HUHHEGGTE", "EGGHHU"))
alternations(c("HUHHEGGTE", "EGGHHU"), proportion = TRUE)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
