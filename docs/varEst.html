<!DOCTYPE html><html><head><title>Help for package varEst</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {varEst}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bsrcv'>
<p>Variance Estimation with Bootstrap-RCV</p></a></li>
<li><a href='#ensemble'><p>Variance Estimation with Ensemble method</p></a></li>
<li><a href='#krcv'>
<p>Variance Estimation with kfold-RCV</p></a></li>
<li><a href='#rcv'><p>Variance Estimation with Refitted Cross Validation(RCV)</p></a></li>
<li><a href='#varEst-package'>
<p>Variance Estimation</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Variance Estimation</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Author:</td>
<td>Sayanti Guha Majumdar, Anil Rai, Dwijesh Chandra Mishra</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Sayanti Guha Majumdar &lt;sayanti23gm@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Error variance estimation in ultrahigh dimensional datasets with four different methods, viz. Refitted cross validation, k-fold refitted cross validation, Bootstrap-refitted cross validation, Ensemble method.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>Imports:</td>
<td>SAM, caret, lm.beta, glmnet</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-09-17 09:47:18 UTC; user6</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-09-23 16:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='bsrcv'>
Variance Estimation with Bootstrap-RCV
</h2><span id='topic+bsrcv'></span>

<h3>Description</h3>

<p>Estimation of error variance using Bootstrap-refitted cross validation method in ultrahigh dimensional dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bsrcv(x,y,a,b,d,method=c("spam","lasso","lsr"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bsrcv_+3A_x">x</code></td>
<td>
<p>a matrix of markers or explanatory variables, each column contains one marker and each row represents an individual.</p>
</td></tr>
<tr><td><code id="bsrcv_+3A_y">y</code></td>
<td>
<p>a column vector of response variable.</p>
</td></tr>
<tr><td><code id="bsrcv_+3A_a">a</code></td>
<td>
<p>value of alpha, range is 0&lt;=a&lt;=1 where, a=1 is LASSO penalty and a=0 is Ridge penalty.If variable selection method is LASSO then providing value to a is compulsory. For other methods a should be NULL.</p>
</td></tr>
<tr><td><code id="bsrcv_+3A_b">b</code></td>
<td>
<p>number of bootstrap samples.</p>
</td></tr>
<tr><td><code id="bsrcv_+3A_d">d</code></td>
<td>
<p>number of variables to be selected from x.</p>
</td></tr>
<tr><td><code id="bsrcv_+3A_method">method</code></td>
<td>
<p>variable selection method, user can choose any method among &quot;spam&quot;, &quot;lasso&quot;, &quot;lsr&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In this method, bootstrap samples are taken from the original datasets and then RCV (Fan et al., 2012) method is applied to each of these bootstrap samples.
</p>


<h3>Value</h3>

<table>
<tr><td><code>Error variance</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Sayanti Guha Majumdar &lt;<a href="mailto:sayanti23gm@gmail.com">sayanti23gm@gmail.com</a>&gt;, Anil Rai, Dwijesh Chandra Mishra
</p>


<h3>References</h3>

<p>Fan, J., Guo, S., Hao, N. (2012).Variance estimation using refitted cross-validation in ultrahigh dimensional regression. <em>Journal of the Royal Statistical Society</em>, 74(1), 37-65<br /> Ravikumar, P., Lafferty, J., Liu, H. and Wasserman, L. (2009). Sparse additive models. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 71(5), 1009-1030<br /> Tibshirani, R. (1996). Regression shrinkage and selection via the Lasso. <em>Journal of Royal Statistical Society</em>, 58, 267-288
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## data simulation
marker &lt;- as.data.frame(matrix(NA, ncol =500, nrow = 200))
for(i in 1:500){
marker[i] &lt;- sample(1:3, 200, replace = TRUE, prob = c(1, 2, 1))
}
pheno &lt;- marker[,1]*1.41+marker[,2]*1.41+marker[,3]*1.41+marker[,4]*1.41+marker[,5]*1.41

pheno &lt;- as.matrix(pheno)
marker&lt;- as.matrix(marker)

## estimation of error variance
var &lt;- bsrcv(marker,pheno,1,10,5,"lasso")
</code></pre>

<hr>
<h2 id='ensemble'>Variance Estimation with Ensemble method</h2><span id='topic+ensemble'></span>

<h3>Description</h3>

<p>Estimation of error variance using ensemble method which combines bootstraping and sampling with srswor in ultrahigh dimensional dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ensemble(x,y,a,b,d,method=c("spam","lasso","lsr"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ensemble_+3A_x">x</code></td>
<td>
<p>a matrix of markers or explanatory variables, each column contains one marker and each row represents an individual.</p>
</td></tr>
<tr><td><code id="ensemble_+3A_y">y</code></td>
<td>
<p>a column vector of response variable.</p>
</td></tr>
<tr><td><code id="ensemble_+3A_a">a</code></td>
<td>
<p>value of alpha, range is 0&lt;=a&lt;=1 where, a=1 is LASSO penalty and a=0 is Ridge penalty.If variable selection method is LASSO then providing value to a is compulsory. For other methods a should be NULL.</p>
</td></tr>
<tr><td><code id="ensemble_+3A_b">b</code></td>
<td>
<p>number of bootstrap samples.</p>
</td></tr>
<tr><td><code id="ensemble_+3A_d">d</code></td>
<td>
<p>number of variables to be selected from x.</p>
</td></tr>
<tr><td><code id="ensemble_+3A_method">method</code></td>
<td>
<p>variable selection method, user can choose any method among &quot;spam&quot;, &quot;lasso&quot;, &quot;lsr&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In this method, both bootstrapping and simple random sampling without replacement are combined to estimate error variance. Variables are selected using Sparse Additive Models (SpAM) or LASSO or least squared regression (lsr) from the original datasets and all possible samples of a particular size are taken from the selected variables set with simple random sampling without replacement. With these selected samples error variance is estimated from bootstrap samples of the original datasets using least squared regression method. Finally the average of all the estimated variances is considered as the final estimate of the error variance.
</p>


<h3>Value</h3>

<table>
<tr><td><code>Error variance</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Sayanti Guha Majumdar &lt;<a href="mailto:sayanti23gm@gmail.com">sayanti23gm@gmail.com</a>&gt;, Anil Rai, Dwijesh Chandra Mishra
</p>


<h3>References</h3>

<p>Ravikumar, P., Lafferty, J., Liu, H. and Wasserman, L. (2009). Sparse additive models. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 71(5), 1009-1030
<br /> Tibshirani, R. (1996). Regression shrinkage and selection via the Lasso. <em>Journal of Royal Statistical Society</em>, 58, 267-288
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## data simulation
marker &lt;- as.data.frame(matrix(NA, ncol =500, nrow = 200))
for(i in 1:500){
marker[i] &lt;- sample(1:3, 200, replace = TRUE, prob = c(1, 2, 1))
}
pheno &lt;- marker[,1]*1.41+marker[,2]*1.41+marker[,3]*1.41+marker[,4]*1.41+marker[,5]*1.41

pheno &lt;- as.matrix(pheno)
marker&lt;- as.matrix(marker)

## estimation of error variance
var &lt;- ensemble(marker,pheno,1,10,10,"spam")
</code></pre>

<hr>
<h2 id='krcv'>
Variance Estimation with kfold-RCV
</h2><span id='topic+krcv'></span>

<h3>Description</h3>

<p>Estimation of error variance using k-fold refitted cross validation in ultrahigh dimensional dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>krcv(x,y,a,k,d,method=c("spam","lasso","lsr"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="krcv_+3A_x">x</code></td>
<td>
<p>a matrix of markers or explanatory variables, each column contains one marker and each row represents an individual.</p>
</td></tr>
<tr><td><code id="krcv_+3A_y">y</code></td>
<td>
<p>a column vector of response variable.</p>
</td></tr>
<tr><td><code id="krcv_+3A_a">a</code></td>
<td>
<p>value of alpha, range is 0&lt;=a&lt;=1 where, a=1 is LASSO penalty and a=0 is Ridge penalty.If variable selection method is LASSO then providing value to a is compulsory. For other methods a should be NULL.</p>
</td></tr>
<tr><td><code id="krcv_+3A_k">k</code></td>
<td>
<p>dataset is divided into this many numbers of sub-datasets.</p>
</td></tr>
<tr><td><code id="krcv_+3A_d">d</code></td>
<td>
<p>number of variables to be selected from x.</p>
</td></tr>
<tr><td><code id="krcv_+3A_method">method</code></td>
<td>
<p>variable selection method, user can choose any method among &quot;spam&quot;, &quot;lasso&quot;, &quot;lsr&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The error variance is estimated from a high dimensional datasets where number of parameters are more than number of individuals, i.e. p &gt; n.k-fold RCV is an extended version of original RCV method (Fan et al.,  2012). In this case the datasets are divided into k equal size groups instead of 2 groups. Variables are selected using Sparse Additive Models (SpAM) or LASSO or least squared regression (lsr) from one group and variance is estimated using selected variables with ordinary least squared estimation from rest of the k-1 groups. Likewise, all the groups are covered and in the end, average value of all the variances from each group is the final error variance.
</p>


<h3>Value</h3>

<table>
<tr><td><code>Error variance</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Sayanti Guha Majumdar &lt;<a href="mailto:sayanti23gm@gmail.com">sayanti23gm@gmail.com</a>&gt;, Anil Rai, Dwijesh Chandra Mishra
</p>


<h3>References</h3>

<p>Fan, J., Guo, S., Hao, N. (2012).Variance estimation using refitted cross-validation in ultrahigh dimensional regression. <em>Journal of the Royal Statistical Society</em>, 74(1), 37-65
<br /> Ravikumar, P., Lafferty, J., Liu, H. and Wasserman, L. (2009). Sparse additive models. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 71(5), 1009-1030
<br /> Tibshirani, R. (1996). Regression shrinkage and selection via the Lasso. <em>Journal of Royal Statistical Society</em>, 58, 267-288
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## data simulation
marker &lt;- as.data.frame(matrix(NA, ncol =500, nrow = 200))
for(i in 1:500){
marker[i] &lt;- sample(1:3, 200, replace = TRUE, prob = c(1, 2, 1))
}
pheno &lt;- marker[,1]*1.41+marker[,2]*1.41+marker[,3]*1.41+marker[,4]*1.41+marker[,5]*1.41

pheno &lt;- as.matrix(pheno)
marker&lt;- as.matrix(marker)

## estimation of error variance
var &lt;- krcv(marker,pheno,1,4,5,"spam")
</code></pre>

<hr>
<h2 id='rcv'>Variance Estimation with Refitted Cross Validation(RCV)</h2><span id='topic+rcv'></span>

<h3>Description</h3>

<p>Estimation of error variance using Refitted cross validation in ultrahigh dimensional dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rcv(x,y,a,d,method=c("spam","lasso","lsr"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rcv_+3A_x">x</code></td>
<td>
<p>a matrix of markers or explanatory variables, each column contains one marker and each row represents an individual.</p>
</td></tr>
<tr><td><code id="rcv_+3A_y">y</code></td>
<td>
<p>a column vector of response variable.</p>
</td></tr>
<tr><td><code id="rcv_+3A_a">a</code></td>
<td>
<p>value of alpha, range is 0&lt;=a&lt;=1 where, a=1 is LASSO penalty and a=0 is Ridge penalty. If variable selection method is LASSO then providing value to a is compulsory. For other methods a should be NULL.</p>
</td></tr>
<tr><td><code id="rcv_+3A_d">d</code></td>
<td>
<p>number of variables to be selected from x.</p>
</td></tr>
<tr><td><code id="rcv_+3A_method">method</code></td>
<td>
<p>variable selection method, user can choose any method among &quot;spam&quot;, &quot;lasso&quot;, &quot;lsr&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The error variance is estimated from a high dimensional datasets where number of parameters are more than number of individuals, i.e. p &gt; n. Refitted cross validation method (RCV) which is a two step method, is used to get the estimate of the error variance. In first step, dataset is divided into two sub-datasets and with the help of Sparse Additive Models (SpAM) or LASSO or least squared regression (lsr)  most significant markers(variables) are selected from the two sub-datasets. This results in two small sets of selected variables. Then using the set selected from 1st sub-dataset error variance is estimated from the 2nd sub-dataset with ordinary least square method and using the set selected from the 2nd sub-dataset error variance is estimated from the 1st sub-dataset with ordinary least square method. Finally the average of those two error variances are taken as the final estimator of error variance with RCV method.</p>


<h3>Value</h3>

<table>
<tr><td><code>Error variance</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Sayanti Guha Majumdar &lt;<a href="mailto:sayanti23gm@gmail.com">sayanti23gm@gmail.com</a>&gt;, Anil Rai, Dwijesh Chandra Mishra</p>


<h3>References</h3>

<p>Fan, J., Guo, S., Hao, N. (2012).Variance estimation using refitted cross-validation in ultrahigh dimensional regression. <em>Journal of the Royal Statistical Society</em>, 74(1), 37-65<br /> Ravikumar, P., Lafferty, J., Liu, H. and Wasserman, L. (2009). Sparse additive models. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 71(5), 1009-1030<br /> Tibshirani, R. (1996). Regression shrinkage and selection via the Lasso. <em>Journal of Royal Statistical Society</em>, 58, 267-288
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## data simulation
marker &lt;- as.data.frame(matrix(NA, ncol =500, nrow = 200))
for(i in 1:500){
marker[i] &lt;- sample(1:3, 200, replace = TRUE, prob = c(1, 2, 1))
}
pheno &lt;- marker[,1]*1.41+marker[,2]*1.41+marker[,3]*1.41+marker[,4]*1.41+marker[,5]*1.41

pheno &lt;- as.matrix(pheno)
marker&lt;- as.matrix(marker)

## estimation of error variance
var &lt;- rcv(marker,pheno,1,5,"spam")
</code></pre>

<hr>
<h2 id='varEst-package'>
Variance Estimation
</h2><span id='topic+varEst-package'></span><span id='topic+varEst'></span>

<h3>Description</h3>

<p>Error variance estimation in ultrahigh dimensional datasets with four different methods, viz. Refitted cross validation, k-fold refitted cross validation, Bootstrap-refitted cross validation, Ensemble method.
</p>


<h3>Details</h3>

<p>The DESCRIPTION file:
</p>

<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> varEst</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Title: </td><td style="text-align: left;"> Variance Estimation</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 0.1.0</td>
</tr>
<tr>
 <td style="text-align: left;">
Author: </td><td style="text-align: left;"> Sayanti Guha Majumdar, Anil Rai, Dwijesh Chandra Mishra</td>
</tr>
<tr>
 <td style="text-align: left;">
Maintainer: </td><td style="text-align: left;"> Sayanti Guha Majumdar &lt;sayanti23gm@gmail.com&gt;</td>
</tr>
<tr>
 <td style="text-align: left;">
Description: </td><td style="text-align: left;"> Error variance estimation in ultrahigh dimensional datasets with four different methods, viz. Refitted cross validation, k-fold refitted cross validation, Bootstrap-refitted cross validation, Ensemble method.</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-3</td>
</tr>
<tr>
 <td style="text-align: left;">
Encoding: </td><td style="text-align: left;"> UTF-8</td>
</tr>
<tr>
 <td style="text-align: left;">
LazyData: </td><td style="text-align: left;"> TRUE</td>
</tr>
<tr>
 <td style="text-align: left;">
Imports: </td><td style="text-align: left;"> SAM, caret, lm.beta, glmnet</td>
</tr>
<tr>
 <td style="text-align: left;">
RoxygenNote: </td><td style="text-align: left;"> 6.1.1</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>


<p>Index of help topics:
</p>
<pre>
bsrcv                   Variance Estimation with Bootstrap-RCV
ensemble                Variance Estimation with Ensemble method
krcv                    Variance Estimation with kfold-RCV
rcv                     Variance Estimation with Refitted Cross
                        Validation(RCV)
varEst-package          Variance Estimation
</pre>


<h3>Author(s)</h3>

<p>Sayanti Guha Majumdar, Anil Rai, Dwijesh Chandra Mishra
</p>
<p>Maintainer: Sayanti Guha Majumdar &lt;sayanti23gm@gmail.com&gt;
</p>


<h3>References</h3>

<p>Fan, J., Guo, S., Hao, N. (2012).Variance estimation using refitted cross-validation in ultrahigh dimensional regression. <em>Journal of the Royal Statistical Society</em>, 74(1), 37-65<br /> Ravikumar, P., Lafferty, J., Liu, H. and Wasserman, L. (2009). Sparse additive models. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 71(5), 1009-1030<br /> Tibshirani, R. (1996). Regression shrinkage and selection via the Lasso. <em>Journal of Royal Statistical Society</em>, 58, 267-288
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
