<!DOCTYPE html><html lang="en-US"><head><title>Help for package brulee</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {brulee}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#brulee-package'><p>brulee: High-Level Modeling Functions with 'torch'</p></a></li>
<li><a href='#brulee_activations'><p>Activation functions for neural networks in brulee</p></a></li>
<li><a href='#brulee_linear_reg'><p>Fit a linear regression model</p></a></li>
<li><a href='#brulee_logistic_reg'><p>Fit a logistic regression model</p></a></li>
<li><a href='#brulee_mlp'><p>Fit neural networks</p></a></li>
<li><a href='#brulee_multinomial_reg'><p>Fit a multinomial regression model</p></a></li>
<li><a href='#brulee-autoplot'><p>Plot model loss over epochs</p></a></li>
<li><a href='#brulee-coefs'><p>Extract Model Coefficients</p></a></li>
<li><a href='#matrix_to_dataset'><p>Convert data to torch format</p></a></li>
<li><a href='#predict.brulee_linear_reg'><p>Predict from a <code>brulee_linear_reg</code></p></a></li>
<li><a href='#predict.brulee_logistic_reg'><p>Predict from a <code>brulee_logistic_reg</code></p></a></li>
<li><a href='#predict.brulee_mlp'><p>Predict from a <code>brulee_mlp</code></p></a></li>
<li><a href='#predict.brulee_multinomial_reg'><p>Predict from a <code>brulee_multinomial_reg</code></p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#schedule_decay_time'><p>Change the learning rate over time</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>High-Level Modeling Functions with 'torch'</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides high-level modeling functions to define and train
    models using the 'torch' R package. Models include linear, logistic,
    and multinomial regression as well as multilayer perceptrons.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/tidymodels/brulee">https://github.com/tidymodels/brulee</a>,
<a href="https://brulee.tidymodels.org/">https://brulee.tidymodels.org/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/tidymodels/brulee/issues">https://github.com/tidymodels/brulee/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>cli, coro (&ge; 1.0.1), dplyr, generics, ggplot2, glue, hardhat,
rlang (&ge; 1.1.1), stats, tibble, torch (&ge; 0.13.0), utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, modeldata, purrr, recipes, spelling, testthat,
yardstick</td>
</tr>
<tr>
<td>Config/Needs/website:</td>
<td>tidyverse/tidytemplate</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-30 21:36:04 UTC; max</td>
</tr>
<tr>
<td>Author:</td>
<td>Max Kuhn <a href="https://orcid.org/0000-0003-2402-136X"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Daniel Falbel [aut],
  Posit Software, PBC [cph, fnd]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Max Kuhn &lt;max@posit.co&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-30 22:10:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='brulee-package'>brulee: High-Level Modeling Functions with 'torch'</h2><span id='topic+brulee'></span><span id='topic+brulee-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>Provides high-level modeling functions to define and train models using the 'torch' R package. Models include linear, logistic, and multinomial regression as well as multilayer perceptrons.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Max Kuhn <a href="mailto:max@posit.co">max@posit.co</a> (<a href="https://orcid.org/0000-0003-2402-136X">ORCID</a>)
</p>
<p>Authors:
</p>

<ul>
<li><p> Daniel Falbel <a href="mailto:daniel@posit.co">daniel@posit.co</a>
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Posit Software, PBC [copyright holder, funder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/tidymodels/brulee">https://github.com/tidymodels/brulee</a>
</p>
</li>
<li> <p><a href="https://brulee.tidymodels.org/">https://brulee.tidymodels.org/</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/tidymodels/brulee/issues">https://github.com/tidymodels/brulee/issues</a>
</p>
</li></ul>


<hr>
<h2 id='brulee_activations'>Activation functions for neural networks in brulee</h2><span id='topic+brulee_activations'></span>

<h3>Description</h3>

<p>Activation functions for neural networks in brulee
</p>


<h3>Usage</h3>

<pre><code class='language-R'>brulee_activations()
</code></pre>


<h3>Value</h3>

<p>A character vector of values.
</p>

<hr>
<h2 id='brulee_linear_reg'>Fit a linear regression model</h2><span id='topic+brulee_linear_reg'></span><span id='topic+brulee_linear_reg.default'></span><span id='topic+brulee_linear_reg.data.frame'></span><span id='topic+brulee_linear_reg.matrix'></span><span id='topic+brulee_linear_reg.formula'></span><span id='topic+brulee_linear_reg.recipe'></span>

<h3>Description</h3>

<p><code>brulee_linear_reg()</code> fits a linear regression model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>brulee_linear_reg(x, ...)

## Default S3 method:
brulee_linear_reg(x, ...)

## S3 method for class 'data.frame'
brulee_linear_reg(
  x,
  y,
  epochs = 20L,
  penalty = 0.001,
  mixture = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 1,
  momentum = 0,
  batch_size = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

## S3 method for class 'matrix'
brulee_linear_reg(
  x,
  y,
  epochs = 20L,
  penalty = 0.001,
  mixture = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 1,
  momentum = 0,
  batch_size = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

## S3 method for class 'formula'
brulee_linear_reg(
  formula,
  data,
  epochs = 20L,
  penalty = 0.001,
  mixture = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 1,
  momentum = 0,
  batch_size = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

## S3 method for class 'recipe'
brulee_linear_reg(
  x,
  data,
  epochs = 20L,
  penalty = 0.001,
  mixture = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 1,
  momentum = 0,
  batch_size = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="brulee_linear_reg_+3A_x">x</code></td>
<td>
<p>Depending on the context:
</p>

<ul>
<li><p> A <strong>data frame</strong> of predictors.
</p>
</li>
<li><p> A <strong>matrix</strong> of predictors.
</p>
</li>
<li><p> A <strong>recipe</strong> specifying a set of preprocessing steps
created from <code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code>.
</p>
</li></ul>

<p>The predictor data should be standardized (e.g. centered or scaled).</p>
</td></tr>
<tr><td><code id="brulee_linear_reg_+3A_...">...</code></td>
<td>
<p>Options to pass to the learning rate schedulers via
<code><a href="#topic+set_learn_rate">set_learn_rate()</a></code>. For example, the <code>reduction</code> or <code>steps</code> arguments to
<code><a href="#topic+schedule_step">schedule_step()</a></code> could be passed here.</p>
</td></tr>
<tr><td><code id="brulee_linear_reg_+3A_y">y</code></td>
<td>
<p>When <code>x</code> is a <strong>data frame</strong> or <strong>matrix</strong>, <code>y</code> is the outcome
specified as:
</p>

<ul>
<li><p> A <strong>data frame</strong> with 1 numeric column.
</p>
</li>
<li><p> A <strong>matrix</strong> with 1 numeric column.
</p>
</li>
<li><p> A numeric <strong>vector</strong>.
</p>
</li></ul>
</td></tr>
<tr><td><code id="brulee_linear_reg_+3A_epochs">epochs</code></td>
<td>
<p>An integer for the number of epochs of training.</p>
</td></tr>
<tr><td><code id="brulee_linear_reg_+3A_penalty">penalty</code></td>
<td>
<p>The amount of weight decay (i.e., L2 regularization).</p>
</td></tr>
<tr><td><code id="brulee_linear_reg_+3A_mixture">mixture</code></td>
<td>
<p>Proportion of Lasso Penalty (type: double, default: 0.0). A
value of mixture = 1 corresponds to a pure lasso model, while mixture = 0
indicates ridge regression (a.k.a weight decay).</p>
</td></tr>
<tr><td><code id="brulee_linear_reg_+3A_validation">validation</code></td>
<td>
<p>The proportion of the data randomly assigned to a
validation set.</p>
</td></tr>
<tr><td><code id="brulee_linear_reg_+3A_optimizer">optimizer</code></td>
<td>
<p>The method used in the optimization procedure. Possible choices
are 'LBFGS' and 'SGD'. Default is 'LBFGS'.</p>
</td></tr>
<tr><td><code id="brulee_linear_reg_+3A_learn_rate">learn_rate</code></td>
<td>
<p>A positive number that controls the initial rapidity that
the model moves along the descent path. Values around 0.1 or less are
typical.</p>
</td></tr>
<tr><td><code id="brulee_linear_reg_+3A_momentum">momentum</code></td>
<td>
<p>A positive number usually on <code style="white-space: pre;">&#8288;[0.50, 0.99]&#8288;</code> for the momentum
parameter in gradient descent.  (<code>optimizer = "SGD"</code> only)</p>
</td></tr>
<tr><td><code id="brulee_linear_reg_+3A_batch_size">batch_size</code></td>
<td>
<p>An integer for the number of training set points in each
batch. (<code>optimizer = "SGD"</code> only)</p>
</td></tr>
<tr><td><code id="brulee_linear_reg_+3A_stop_iter">stop_iter</code></td>
<td>
<p>A non-negative integer for how many iterations with no
improvement before stopping.</p>
</td></tr>
<tr><td><code id="brulee_linear_reg_+3A_verbose">verbose</code></td>
<td>
<p>A logical that prints out the iteration history.</p>
</td></tr>
<tr><td><code id="brulee_linear_reg_+3A_formula">formula</code></td>
<td>
<p>A formula specifying the outcome term(s) on the left-hand side,
and the predictor term(s) on the right-hand side.</p>
</td></tr>
<tr><td><code id="brulee_linear_reg_+3A_data">data</code></td>
<td>
<p>When a <strong>recipe</strong> or <strong>formula</strong> is used, <code>data</code> is specified as:
</p>

<ul>
<li><p> A <strong>data frame</strong> containing both the predictors and the outcome.
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>This function fits a linear combination of coefficients and predictors to
model the numeric outcome. The training process optimizes the
mean squared error loss function.
</p>
<p>The function internally standardizes the outcome data to have mean zero and
a standard deviation of one. The prediction function creates predictions on
the original scale.
</p>
<p>By default, training halts when the validation loss increases for at least
<code>step_iter</code> iterations. If <code>validation = 0</code> the training set loss is used.
</p>
<p>The <em>predictors</em> data should all be numeric and encoded in the same units (e.g.
standardized to the same range or distribution). If there are factor
predictors, use a recipe or formula to create indicator variables (or some
other method) to make them numeric. Predictors should be in the same units
before training.
</p>
<p>The model objects are saved for each epoch so that the number of epochs can
be efficiently tuned. Both the <code><a href="#topic+coef">coef()</a></code> and <code><a href="stats.html#topic+predict">predict()</a></code> methods for this
model have an <code>epoch</code> argument (which defaults to the epoch with the best
loss value).
</p>
<p>The use of the L1 penalty (a.k.a. the lasso penalty) does <em>not</em> force
parameters to be strictly zero (as it does in packages such as <span class="pkg">glmnet</span>).
The zeroing out of parameters is a specific feature the optimization method
used in those packages.
</p>


<h3>Value</h3>

<p>A <code>brulee_linear_reg</code> object with elements:
</p>

<ul>
<li> <p><code>models_obj</code>: a serialized raw vector for the torch module.
</p>
</li>
<li> <p><code>estimates</code>: a list of matrices with the model parameter estimates per
epoch.
</p>
</li>
<li> <p><code>best_epoch</code>: an integer for the epoch with the smallest loss.
</p>
</li>
<li> <p><code>loss</code>: A vector of loss values (MSE) at each epoch.
</p>
</li>
<li> <p><code>dim</code>: A list of data dimensions.
</p>
</li>
<li> <p><code>y_stats</code>: A list of summary statistics for numeric outcomes.
</p>
</li>
<li> <p><code>parameters</code>: A list of some tuning parameter values.
</p>
</li>
<li> <p><code>blueprint</code>: The <code>hardhat</code> blueprint data.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+predict.brulee_linear_reg">predict.brulee_linear_reg()</a></code>, <code><a href="#topic+coef.brulee_linear_reg">coef.brulee_linear_reg()</a></code>,
<code><a href="#topic+autoplot.brulee_linear_reg">autoplot.brulee_linear_reg()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (torch::torch_is_installed()  &amp; rlang::is_installed(c("recipes", "yardstick", "modeldata"))) {

 ## -----------------------------------------------------------------------------

 library(recipes)
 library(yardstick)

 data(ames, package = "modeldata")

 ames$Sale_Price &lt;- log10(ames$Sale_Price)

 set.seed(122)
 in_train &lt;- sample(1:nrow(ames), 2000)
 ames_train &lt;- ames[ in_train,]
 ames_test  &lt;- ames[-in_train,]


 # Using matrices
 set.seed(1)
 brulee_linear_reg(x = as.matrix(ames_train[, c("Longitude", "Latitude")]),
                    y = ames_train$Sale_Price,
                    penalty = 0.10, epochs = 1, batch_size = 64)

 # Using recipe
 library(recipes)

 ames_rec &lt;-
  recipe(Sale_Price ~ Bldg_Type + Neighborhood + Year_Built + Gr_Liv_Area +
         Full_Bath + Year_Sold + Lot_Area + Central_Air + Longitude + Latitude,
         data = ames_train) %&gt;%
    # Transform some highly skewed predictors
    step_BoxCox(Lot_Area, Gr_Liv_Area) %&gt;%
    # Lump some rarely occurring categories into "other"
    step_other(Neighborhood, threshold = 0.05)  %&gt;%
    # Encode categorical predictors as binary.
    step_dummy(all_nominal_predictors(), one_hot = TRUE) %&gt;%
    # Add an interaction effect:
    step_interact(~ starts_with("Central_Air"):Year_Built) %&gt;%
    step_zv(all_predictors()) %&gt;%
    step_normalize(all_numeric_predictors())

 set.seed(2)
 fit &lt;- brulee_linear_reg(ames_rec, data = ames_train,
                           epochs = 5, batch_size = 32)
 fit

 autoplot(fit)

 library(ggplot2)

 predict(fit, ames_test) %&gt;%
   bind_cols(ames_test) %&gt;%
   ggplot(aes(x = .pred, y = Sale_Price)) +
   geom_abline(col = "green") +
   geom_point(alpha = .3) +
   lims(x = c(4, 6), y = c(4, 6)) +
   coord_fixed(ratio = 1)

 library(yardstick)
 predict(fit, ames_test) %&gt;%
   bind_cols(ames_test) %&gt;%
   rmse(Sale_Price, .pred)

 }


</code></pre>

<hr>
<h2 id='brulee_logistic_reg'>Fit a logistic regression model</h2><span id='topic+brulee_logistic_reg'></span><span id='topic+brulee_logistic_reg.default'></span><span id='topic+brulee_logistic_reg.data.frame'></span><span id='topic+brulee_logistic_reg.matrix'></span><span id='topic+brulee_logistic_reg.formula'></span><span id='topic+brulee_logistic_reg.recipe'></span>

<h3>Description</h3>

<p><code>brulee_logistic_reg()</code> fits a model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>brulee_logistic_reg(x, ...)

## Default S3 method:
brulee_logistic_reg(x, ...)

## S3 method for class 'data.frame'
brulee_logistic_reg(
  x,
  y,
  epochs = 20L,
  penalty = 0.001,
  mixture = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 1,
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

## S3 method for class 'matrix'
brulee_logistic_reg(
  x,
  y,
  epochs = 20L,
  penalty = 0.001,
  mixture = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 1,
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

## S3 method for class 'formula'
brulee_logistic_reg(
  formula,
  data,
  epochs = 20L,
  penalty = 0.001,
  mixture = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 1,
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

## S3 method for class 'recipe'
brulee_logistic_reg(
  x,
  data,
  epochs = 20L,
  penalty = 0.001,
  mixture = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 1,
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="brulee_logistic_reg_+3A_x">x</code></td>
<td>
<p>Depending on the context:
</p>

<ul>
<li><p> A <strong>data frame</strong> of predictors.
</p>
</li>
<li><p> A <strong>matrix</strong> of predictors.
</p>
</li>
<li><p> A <strong>recipe</strong> specifying a set of preprocessing steps
created from <code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code>.
</p>
</li></ul>

<p>The predictor data should be standardized (e.g. centered or scaled).</p>
</td></tr>
<tr><td><code id="brulee_logistic_reg_+3A_...">...</code></td>
<td>
<p>Options to pass to the learning rate schedulers via
<code><a href="#topic+set_learn_rate">set_learn_rate()</a></code>. For example, the <code>reduction</code> or <code>steps</code> arguments to
<code><a href="#topic+schedule_step">schedule_step()</a></code> could be passed here.</p>
</td></tr>
<tr><td><code id="brulee_logistic_reg_+3A_y">y</code></td>
<td>
<p>When <code>x</code> is a <strong>data frame</strong> or <strong>matrix</strong>, <code>y</code> is the outcome
specified as:
</p>

<ul>
<li><p> A <strong>data frame</strong> with 1 factor column (with two levels).
</p>
</li>
<li><p> A <strong>matrix</strong> with 1 factor column (with two levels).
</p>
</li>
<li><p> A factor <strong>vector</strong> (with two levels).
</p>
</li></ul>
</td></tr>
<tr><td><code id="brulee_logistic_reg_+3A_epochs">epochs</code></td>
<td>
<p>An integer for the number of epochs of training.</p>
</td></tr>
<tr><td><code id="brulee_logistic_reg_+3A_penalty">penalty</code></td>
<td>
<p>The amount of weight decay (i.e., L2 regularization).</p>
</td></tr>
<tr><td><code id="brulee_logistic_reg_+3A_mixture">mixture</code></td>
<td>
<p>Proportion of Lasso Penalty (type: double, default: 0.0). A
value of mixture = 1 corresponds to a pure lasso model, while mixture = 0
indicates ridge regression (a.k.a weight decay).</p>
</td></tr>
<tr><td><code id="brulee_logistic_reg_+3A_validation">validation</code></td>
<td>
<p>The proportion of the data randomly assigned to a
validation set.</p>
</td></tr>
<tr><td><code id="brulee_logistic_reg_+3A_optimizer">optimizer</code></td>
<td>
<p>The method used in the optimization procedure. Possible choices
are 'LBFGS' and 'SGD'. Default is 'LBFGS'.</p>
</td></tr>
<tr><td><code id="brulee_logistic_reg_+3A_learn_rate">learn_rate</code></td>
<td>
<p>A positive number that controls the rapidity that the model
moves along the descent path. Values around 0.1 or less are typical.
(<code>optimizer = "SGD"</code> only)</p>
</td></tr>
<tr><td><code id="brulee_logistic_reg_+3A_momentum">momentum</code></td>
<td>
<p>A positive number usually on <code style="white-space: pre;">&#8288;[0.50, 0.99]&#8288;</code> for the momentum
parameter in gradient descent.  (<code>optimizer = "SGD"</code> only)</p>
</td></tr>
<tr><td><code id="brulee_logistic_reg_+3A_batch_size">batch_size</code></td>
<td>
<p>An integer for the number of training set points in each
batch. (<code>optimizer = "SGD"</code> only)</p>
</td></tr>
<tr><td><code id="brulee_logistic_reg_+3A_class_weights">class_weights</code></td>
<td>
<p>Numeric class weights (classification only). The value
can be:
</p>

<ul>
<li><p> A named numeric vector (in any order) where the names are the outcome
factor levels.
</p>
</li>
<li><p> An unnamed numeric vector assumed to be in the same order as the outcome
factor levels.
</p>
</li>
<li><p> A single numeric value for the least frequent class in the training data
and all other classes receive a weight of one.
</p>
</li></ul>
</td></tr>
<tr><td><code id="brulee_logistic_reg_+3A_stop_iter">stop_iter</code></td>
<td>
<p>A non-negative integer for how many iterations with no
improvement before stopping.</p>
</td></tr>
<tr><td><code id="brulee_logistic_reg_+3A_verbose">verbose</code></td>
<td>
<p>A logical that prints out the iteration history.</p>
</td></tr>
<tr><td><code id="brulee_logistic_reg_+3A_formula">formula</code></td>
<td>
<p>A formula specifying the outcome term(s) on the left-hand side,
and the predictor term(s) on the right-hand side.</p>
</td></tr>
<tr><td><code id="brulee_logistic_reg_+3A_data">data</code></td>
<td>
<p>When a <strong>recipe</strong> or <strong>formula</strong> is used, <code>data</code> is specified as:
</p>

<ul>
<li><p> A <strong>data frame</strong> containing both the predictors and the outcome.
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>This function fits a linear combination of coefficients and predictors to
model the log odds of the classes. The training process optimizes the
cross-entropy loss function (a.k.a Bernoulli loss).
</p>
<p>By default, training halts when the validation loss increases for at least
<code>step_iter</code> iterations. If <code>validation = 0</code> the training set loss is used.
</p>
<p>The <em>predictors</em> data should all be numeric and encoded in the same units (e.g.
standardized to the same range or distribution). If there are factor
predictors, use a recipe or formula to create indicator variables (or some
other method) to make them numeric. Predictors should be in the same units
before training.
</p>
<p>The model objects are saved for each epoch so that the number of epochs can
be efficiently tuned. Both the <code><a href="#topic+coef">coef()</a></code> and <code><a href="stats.html#topic+predict">predict()</a></code> methods for this
model have an <code>epoch</code> argument (which defaults to the epoch with the best
loss value).
</p>
<p>The use of the L1 penalty (a.k.a. the lasso penalty) does <em>not</em> force
parameters to be strictly zero (as it does in packages such as <span class="pkg">glmnet</span>).
The zeroing out of parameters is a specific feature the optimization method
used in those packages.
</p>


<h3>Value</h3>

<p>A <code>brulee_logistic_reg</code> object with elements:
</p>

<ul>
<li> <p><code>models_obj</code>: a serialized raw vector for the torch module.
</p>
</li>
<li> <p><code>estimates</code>: a list of matrices with the model parameter estimates per
epoch.
</p>
</li>
<li> <p><code>best_epoch</code>: an integer for the epoch with the smallest loss.
</p>
</li>
<li> <p><code>loss</code>: A vector of loss values (MSE for regression, negative log-
likelihood for classification) at each epoch.
</p>
</li>
<li> <p><code>dim</code>: A list of data dimensions.
</p>
</li>
<li> <p><code>parameters</code>: A list of some tuning parameter values.
</p>
</li>
<li> <p><code>blueprint</code>: The <code>hardhat</code> blueprint data.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+predict.brulee_logistic_reg">predict.brulee_logistic_reg()</a></code>, <code><a href="#topic+coef.brulee_logistic_reg">coef.brulee_logistic_reg()</a></code>,
<code><a href="#topic+autoplot.brulee_logistic_reg">autoplot.brulee_logistic_reg()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (torch::torch_is_installed() &amp; rlang::is_installed(c("recipes", "yardstick", "modeldata"))) {

 library(recipes)
 library(yardstick)

 ## -----------------------------------------------------------------------------
 # increase # epochs to get better results

 data(cells, package = "modeldata")

 cells$case &lt;- NULL

 set.seed(122)
 in_train &lt;- sample(1:nrow(cells), 1000)
 cells_train &lt;- cells[ in_train,]
 cells_test  &lt;- cells[-in_train,]

 # Using matrices
 set.seed(1)
 brulee_logistic_reg(x = as.matrix(cells_train[, c("fiber_width_ch_1", "width_ch_1")]),
                      y = cells_train$class,
                      penalty = 0.10, epochs = 3)

 # Using recipe
 library(recipes)

 cells_rec &lt;-
  recipe(class ~ ., data = cells_train) %&gt;%
  # Transform some highly skewed predictors
  step_YeoJohnson(all_numeric_predictors()) %&gt;%
  step_normalize(all_numeric_predictors()) %&gt;%
  step_pca(all_numeric_predictors(), num_comp = 10)

 set.seed(2)
 fit &lt;- brulee_logistic_reg(cells_rec, data = cells_train,
                             penalty = .01, epochs = 5)
 fit

 autoplot(fit)

 library(yardstick)
 predict(fit, cells_test, type = "prob") %&gt;%
  bind_cols(cells_test) %&gt;%
  roc_auc(class, .pred_PS)
}

</code></pre>

<hr>
<h2 id='brulee_mlp'>Fit neural networks</h2><span id='topic+brulee_mlp'></span><span id='topic+brulee_mlp.default'></span><span id='topic+brulee_mlp.data.frame'></span><span id='topic+brulee_mlp.matrix'></span><span id='topic+brulee_mlp.formula'></span><span id='topic+brulee_mlp.recipe'></span><span id='topic+brulee_mlp_two_layer'></span><span id='topic+brulee_mlp_two_layer.default'></span><span id='topic+brulee_mlp_two_layer.data.frame'></span><span id='topic+brulee_mlp_two_layer.matrix'></span><span id='topic+brulee_mlp_two_layer.formula'></span><span id='topic+brulee_mlp_two_layer.recipe'></span>

<h3>Description</h3>

<p><code>brulee_mlp()</code> fits neural network models. Multiple layers can be used. For
working with two-layer networks in tidymodels, <code>brulee_mlp_two_layer()</code> can
be helpful for specifying tuning parameters as scalars.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>brulee_mlp(x, ...)

## Default S3 method:
brulee_mlp(x, ...)

## S3 method for class 'data.frame'
brulee_mlp(
  x,
  y,
  epochs = 100L,
  hidden_units = 3L,
  activation = "relu",
  penalty = 0.001,
  mixture = 0,
  dropout = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 0.01,
  rate_schedule = "none",
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

## S3 method for class 'matrix'
brulee_mlp(
  x,
  y,
  epochs = 100L,
  hidden_units = 3L,
  activation = "relu",
  penalty = 0.001,
  mixture = 0,
  dropout = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 0.01,
  rate_schedule = "none",
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

## S3 method for class 'formula'
brulee_mlp(
  formula,
  data,
  epochs = 100L,
  hidden_units = 3L,
  activation = "relu",
  penalty = 0.001,
  mixture = 0,
  dropout = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 0.01,
  rate_schedule = "none",
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

## S3 method for class 'recipe'
brulee_mlp(
  x,
  data,
  epochs = 100L,
  hidden_units = 3L,
  activation = "relu",
  penalty = 0.001,
  mixture = 0,
  dropout = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 0.01,
  rate_schedule = "none",
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

brulee_mlp_two_layer(x, ...)

## Default S3 method:
brulee_mlp_two_layer(x, ...)

## S3 method for class 'data.frame'
brulee_mlp_two_layer(
  x,
  y,
  epochs = 100L,
  hidden_units = 3L,
  hidden_units_2 = 3L,
  activation = "relu",
  activation_2 = "relu",
  penalty = 0.001,
  mixture = 0,
  dropout = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 0.01,
  rate_schedule = "none",
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

## S3 method for class 'matrix'
brulee_mlp_two_layer(
  x,
  y,
  epochs = 100L,
  hidden_units = 3L,
  hidden_units_2 = 3L,
  activation = "relu",
  activation_2 = "relu",
  penalty = 0.001,
  mixture = 0,
  dropout = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 0.01,
  rate_schedule = "none",
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

## S3 method for class 'formula'
brulee_mlp_two_layer(
  formula,
  data,
  epochs = 100L,
  hidden_units = 3L,
  hidden_units_2 = 3L,
  activation = "relu",
  activation_2 = "relu",
  penalty = 0.001,
  mixture = 0,
  dropout = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 0.01,
  rate_schedule = "none",
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

## S3 method for class 'recipe'
brulee_mlp_two_layer(
  x,
  data,
  epochs = 100L,
  hidden_units = 3L,
  hidden_units_2 = 3L,
  activation = "relu",
  activation_2 = "relu",
  penalty = 0.001,
  mixture = 0,
  dropout = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 0.01,
  rate_schedule = "none",
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="brulee_mlp_+3A_x">x</code></td>
<td>
<p>Depending on the context:
</p>

<ul>
<li><p> A <strong>data frame</strong> of predictors.
</p>
</li>
<li><p> A <strong>matrix</strong> of predictors.
</p>
</li>
<li><p> A <strong>recipe</strong> specifying a set of preprocessing steps
created from <code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code>.
</p>
</li></ul>

<p>The predictor data should be standardized (e.g. centered or scaled).</p>
</td></tr>
<tr><td><code id="brulee_mlp_+3A_...">...</code></td>
<td>
<p>Options to pass to the learning rate schedulers via
<code><a href="#topic+set_learn_rate">set_learn_rate()</a></code>. For example, the <code>reduction</code> or <code>steps</code> arguments to
<code><a href="#topic+schedule_step">schedule_step()</a></code> could be passed here.</p>
</td></tr>
<tr><td><code id="brulee_mlp_+3A_y">y</code></td>
<td>
<p>When <code>x</code> is a <strong>data frame</strong> or <strong>matrix</strong>, <code>y</code> is the outcome
specified as:
</p>

<ul>
<li><p> A <strong>data frame</strong> with 1 column (numeric or factor).
</p>
</li>
<li><p> A <strong>matrix</strong> with numeric column  (numeric or factor).
</p>
</li>
<li><p> A  <strong>vector</strong>  (numeric or factor).
</p>
</li></ul>
</td></tr>
<tr><td><code id="brulee_mlp_+3A_epochs">epochs</code></td>
<td>
<p>An integer for the number of epochs of training.</p>
</td></tr>
<tr><td><code id="brulee_mlp_+3A_hidden_units">hidden_units</code></td>
<td>
<p>An integer for the number of hidden units, or a vector
of integers. If a vector of integers, the model will have <code>length(hidden_units)</code>
layers each with <code>hidden_units[i]</code> hidden units.</p>
</td></tr>
<tr><td><code id="brulee_mlp_+3A_activation">activation</code></td>
<td>
<p>A character vector for the activation function (such as
&quot;relu&quot;, &quot;tanh&quot;, &quot;sigmoid&quot;, and so on). See <code><a href="#topic+brulee_activations">brulee_activations()</a></code> for
a list of possible values. If <code>hidden_units</code> is a vector, <code>activation</code>
can be a character vector with length equals to <code>length(hidden_units)</code>
specifying the activation for each hidden layer.</p>
</td></tr>
<tr><td><code id="brulee_mlp_+3A_penalty">penalty</code></td>
<td>
<p>The amount of weight decay (i.e., L2 regularization).</p>
</td></tr>
<tr><td><code id="brulee_mlp_+3A_mixture">mixture</code></td>
<td>
<p>Proportion of Lasso Penalty (type: double, default: 0.0). A
value of mixture = 1 corresponds to a pure lasso model, while mixture = 0
indicates ridge regression (a.k.a weight decay).</p>
</td></tr>
<tr><td><code id="brulee_mlp_+3A_dropout">dropout</code></td>
<td>
<p>The proportion of parameters set to zero.</p>
</td></tr>
<tr><td><code id="brulee_mlp_+3A_validation">validation</code></td>
<td>
<p>The proportion of the data randomly assigned to a
validation set.</p>
</td></tr>
<tr><td><code id="brulee_mlp_+3A_optimizer">optimizer</code></td>
<td>
<p>The method used in the optimization procedure. Possible choices
are 'LBFGS' and 'SGD'. Default is 'LBFGS'.</p>
</td></tr>
<tr><td><code id="brulee_mlp_+3A_learn_rate">learn_rate</code></td>
<td>
<p>A positive number that controls the initial rapidity that
the model moves along the descent path. Values around 0.1 or less are
typical.</p>
</td></tr>
<tr><td><code id="brulee_mlp_+3A_rate_schedule">rate_schedule</code></td>
<td>
<p>A single character value for how the learning rate
should change as the optimization proceeds. Possible values are
<code>"none"</code> (the default), <code>"decay_time"</code>, <code>"decay_expo"</code>, <code>"cyclic"</code> and
<code>"step"</code>. See <code><a href="#topic+schedule_decay_time">schedule_decay_time()</a></code> for more details.</p>
</td></tr>
<tr><td><code id="brulee_mlp_+3A_momentum">momentum</code></td>
<td>
<p>A positive number usually on <code style="white-space: pre;">&#8288;[0.50, 0.99]&#8288;</code> for the momentum
parameter in gradient descent.  (<code>optimizer = "SGD"</code> only)</p>
</td></tr>
<tr><td><code id="brulee_mlp_+3A_batch_size">batch_size</code></td>
<td>
<p>An integer for the number of training set points in each
batch. (<code>optimizer = "SGD"</code> only)</p>
</td></tr>
<tr><td><code id="brulee_mlp_+3A_class_weights">class_weights</code></td>
<td>
<p>Numeric class weights (classification only). The value
can be:
</p>

<ul>
<li><p> A named numeric vector (in any order) where the names are the outcome
factor levels.
</p>
</li>
<li><p> An unnamed numeric vector assumed to be in the same order as the outcome
factor levels.
</p>
</li>
<li><p> A single numeric value for the least frequent class in the training data
and all other classes receive a weight of one.
</p>
</li></ul>
</td></tr>
<tr><td><code id="brulee_mlp_+3A_stop_iter">stop_iter</code></td>
<td>
<p>A non-negative integer for how many iterations with no
improvement before stopping.</p>
</td></tr>
<tr><td><code id="brulee_mlp_+3A_verbose">verbose</code></td>
<td>
<p>A logical that prints out the iteration history.</p>
</td></tr>
<tr><td><code id="brulee_mlp_+3A_formula">formula</code></td>
<td>
<p>A formula specifying the outcome term(s) on the left-hand side,
and the predictor term(s) on the right-hand side.</p>
</td></tr>
<tr><td><code id="brulee_mlp_+3A_data">data</code></td>
<td>
<p>When a <strong>recipe</strong> or <strong>formula</strong> is used, <code>data</code> is specified as:
</p>

<ul>
<li><p> A <strong>data frame</strong> containing both the predictors and the outcome.
</p>
</li></ul>
</td></tr>
<tr><td><code id="brulee_mlp_+3A_hidden_units_2">hidden_units_2</code></td>
<td>
<p>An integer for the number of hidden units for a second layer.</p>
</td></tr>
<tr><td><code id="brulee_mlp_+3A_activation_2">activation_2</code></td>
<td>
<p>A character vector for the activation function for a second layer.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function fits feed-forward neural network models for regression (when
the outcome is a number) or classification (a factor). For regression, the
mean squared error is optimized and cross-entropy is the loss function for
classification.
</p>
<p>When the outcome is a number, the function internally standardizes the
outcome data to have mean zero and a standard deviation of one. The prediction
function creates predictions on the original scale.
</p>
<p>By default, training halts when the validation loss increases for at least
<code>step_iter</code> iterations. If <code>validation = 0</code> the training set loss is used.
</p>
<p>The <em>predictors</em> data should all be numeric and encoded in the same units (e.g.
standardized to the same range or distribution). If there are factor
predictors, use a recipe or formula to create indicator variables (or some
other method) to make them numeric. Predictors should be in the same units
before training.
</p>
<p>The model objects are saved for each epoch so that the number of epochs can
be efficiently tuned. Both the <code><a href="#topic+coef">coef()</a></code> and <code><a href="stats.html#topic+predict">predict()</a></code> methods for this
model have an <code>epoch</code> argument (which defaults to the epoch with the best
loss value).
</p>
<p>The use of the L1 penalty (a.k.a. the lasso penalty) does <em>not</em> force
parameters to be strictly zero (as it does in packages such as <span class="pkg">glmnet</span>).
The zeroing out of parameters is a specific feature the optimization method
used in those packages.
</p>


<h4>Learning Rates</h4>

<p>The learning rate can be set to constant (the default) or dynamically set
via a learning rate scheduler (via the <code>rate_schedule</code>). Using
<code>rate_schedule = 'none'</code> uses the <code>learn_rate</code> argument. Otherwise, any
arguments to the schedulers can be passed via <code>...</code>.
</p>



<h3>Value</h3>

<p>A <code>brulee_mlp</code> object with elements:
</p>

<ul>
<li> <p><code>models_obj</code>: a serialized raw vector for the torch module.
</p>
</li>
<li> <p><code>estimates</code>: a list of matrices with the model parameter estimates per
epoch.
</p>
</li>
<li> <p><code>best_epoch</code>: an integer for the epoch with the smallest loss.
</p>
</li>
<li> <p><code>loss</code>: A vector of loss values (MSE for regression, negative log-
likelihood for classification) at each epoch.
</p>
</li>
<li> <p><code>dim</code>: A list of data dimensions.
</p>
</li>
<li> <p><code>y_stats</code>: A list of summary statistics for numeric outcomes.
</p>
</li>
<li> <p><code>parameters</code>: A list of some tuning parameter values.
</p>
</li>
<li> <p><code>blueprint</code>: The <code>hardhat</code> blueprint data.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+predict.brulee_mlp">predict.brulee_mlp()</a></code>, <code><a href="#topic+coef.brulee_mlp">coef.brulee_mlp()</a></code>, <code><a href="#topic+autoplot.brulee_mlp">autoplot.brulee_mlp()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (torch::torch_is_installed() &amp; rlang::is_installed(c("recipes", "yardstick", "modeldata"))) {

 ## -----------------------------------------------------------------------------
 # regression examples (increase # epochs to get better results)

 data(ames, package = "modeldata")

 ames$Sale_Price &lt;- log10(ames$Sale_Price)

 set.seed(122)
 in_train &lt;- sample(1:nrow(ames), 2000)
 ames_train &lt;- ames[ in_train,]
 ames_test  &lt;- ames[-in_train,]


 # Using matrices
 set.seed(1)
 fit &lt;-
   brulee_mlp(x = as.matrix(ames_train[, c("Longitude", "Latitude")]),
               y = ames_train$Sale_Price, penalty = 0.10)

 # Using recipe
 library(recipes)

 ames_rec &lt;-
  recipe(Sale_Price ~ Bldg_Type + Neighborhood + Year_Built + Gr_Liv_Area +
         Full_Bath + Year_Sold + Lot_Area + Central_Air + Longitude + Latitude,
         data = ames_train) %&gt;%
   # Transform some highly skewed predictors
   step_BoxCox(Lot_Area, Gr_Liv_Area) %&gt;%
   # Lump some rarely occurring categories into "other"
   step_other(Neighborhood, threshold = 0.05)  %&gt;%
   # Encode categorical predictors as binary.
   step_dummy(all_nominal_predictors(), one_hot = TRUE) %&gt;%
   # Add an interaction effect:
   step_interact(~ starts_with("Central_Air"):Year_Built) %&gt;%
   step_zv(all_predictors()) %&gt;%
   step_normalize(all_numeric_predictors())

 set.seed(2)
 fit &lt;- brulee_mlp(ames_rec, data = ames_train, hidden_units = 20,
                    dropout = 0.05, rate_schedule = "cyclic", step_size = 4)
 fit

 autoplot(fit)

 library(ggplot2)

 predict(fit, ames_test) %&gt;%
   bind_cols(ames_test) %&gt;%
   ggplot(aes(x = .pred, y = Sale_Price)) +
   geom_abline(col = "green") +
   geom_point(alpha = .3) +
   lims(x = c(4, 6), y = c(4, 6)) +
   coord_fixed(ratio = 1)

 library(yardstick)
 predict(fit, ames_test) %&gt;%
   bind_cols(ames_test) %&gt;%
   rmse(Sale_Price, .pred)

 # Using multiple hidden layers and activation functions
 set.seed(2)
 hidden_fit &lt;- brulee_mlp(ames_rec, data = ames_train,
                    hidden_units = c(15L, 17L), activation = c("relu", "elu"),
                    dropout = 0.05, rate_schedule = "cyclic", step_size = 4)

 predict(hidden_fit, ames_test) %&gt;%
   bind_cols(ames_test) %&gt;%
   rmse(Sale_Price, .pred)

 # ------------------------------------------------------------------------------
 # classification

 library(dplyr)
 library(ggplot2)

 data("parabolic", package = "modeldata")

 set.seed(1)
 in_train &lt;- sample(1:nrow(parabolic), 300)
 parabolic_tr &lt;- parabolic[ in_train,]
 parabolic_te &lt;- parabolic[-in_train,]

 set.seed(2)
 cls_fit &lt;- brulee_mlp(class ~ ., data = parabolic_tr, hidden_units = 2,
                        epochs = 200L, learn_rate = 0.1, activation = "elu",
                        penalty = 0.1, batch_size = 2^8, optimizer = "SGD")
 autoplot(cls_fit)

 grid_points &lt;- seq(-4, 4, length.out = 100)

 grid &lt;- expand.grid(X1 = grid_points, X2 = grid_points)

 predict(cls_fit, grid, type = "prob") %&gt;%
  bind_cols(grid) %&gt;%
  ggplot(aes(X1, X2)) +
  geom_contour(aes(z = .pred_Class1), breaks = 1/2, col = "black") +
  geom_point(data = parabolic_te, aes(col = class))

 }

</code></pre>

<hr>
<h2 id='brulee_multinomial_reg'>Fit a multinomial regression model</h2><span id='topic+brulee_multinomial_reg'></span><span id='topic+brulee_multinomial_reg.default'></span><span id='topic+brulee_multinomial_reg.data.frame'></span><span id='topic+brulee_multinomial_reg.matrix'></span><span id='topic+brulee_multinomial_reg.formula'></span><span id='topic+brulee_multinomial_reg.recipe'></span>

<h3>Description</h3>

<p><code>brulee_multinomial_reg()</code> fits a model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>brulee_multinomial_reg(x, ...)

## Default S3 method:
brulee_multinomial_reg(x, ...)

## S3 method for class 'data.frame'
brulee_multinomial_reg(
  x,
  y,
  epochs = 20L,
  penalty = 0.001,
  mixture = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 1,
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

## S3 method for class 'matrix'
brulee_multinomial_reg(
  x,
  y,
  epochs = 20L,
  penalty = 0.001,
  mixture = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 1,
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

## S3 method for class 'formula'
brulee_multinomial_reg(
  formula,
  data,
  epochs = 20L,
  penalty = 0.001,
  mixture = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 1,
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

## S3 method for class 'recipe'
brulee_multinomial_reg(
  x,
  data,
  epochs = 20L,
  penalty = 0.001,
  mixture = 0,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 1,
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="brulee_multinomial_reg_+3A_x">x</code></td>
<td>
<p>Depending on the context:
</p>

<ul>
<li><p> A <strong>data frame</strong> of predictors.
</p>
</li>
<li><p> A <strong>matrix</strong> of predictors.
</p>
</li>
<li><p> A <strong>recipe</strong> specifying a set of preprocessing steps
created from <code><a href="recipes.html#topic+recipe">recipes::recipe()</a></code>.
</p>
</li></ul>

<p>The predictor data should be standardized (e.g. centered or scaled).</p>
</td></tr>
<tr><td><code id="brulee_multinomial_reg_+3A_...">...</code></td>
<td>
<p>Options to pass to the learning rate schedulers via
<code><a href="#topic+set_learn_rate">set_learn_rate()</a></code>. For example, the <code>reduction</code> or <code>steps</code> arguments to
<code><a href="#topic+schedule_step">schedule_step()</a></code> could be passed here.</p>
</td></tr>
<tr><td><code id="brulee_multinomial_reg_+3A_y">y</code></td>
<td>
<p>When <code>x</code> is a <strong>data frame</strong> or <strong>matrix</strong>, <code>y</code> is the outcome
specified as:
</p>

<ul>
<li><p> A <strong>data frame</strong> with 1 factor column (with three or more levels).
</p>
</li>
<li><p> A <strong>matrix</strong> with 1 factor column (with three or more levels).
</p>
</li>
<li><p> A factor <strong>vector</strong> (with three or more levels).
</p>
</li></ul>
</td></tr>
<tr><td><code id="brulee_multinomial_reg_+3A_epochs">epochs</code></td>
<td>
<p>An integer for the number of epochs of training.</p>
</td></tr>
<tr><td><code id="brulee_multinomial_reg_+3A_penalty">penalty</code></td>
<td>
<p>The amount of weight decay (i.e., L2 regularization).</p>
</td></tr>
<tr><td><code id="brulee_multinomial_reg_+3A_mixture">mixture</code></td>
<td>
<p>Proportion of Lasso Penalty (type: double, default: 0.0). A
value of mixture = 1 corresponds to a pure lasso model, while mixture = 0
indicates ridge regression (a.k.a weight decay).</p>
</td></tr>
<tr><td><code id="brulee_multinomial_reg_+3A_validation">validation</code></td>
<td>
<p>The proportion of the data randomly assigned to a
validation set.</p>
</td></tr>
<tr><td><code id="brulee_multinomial_reg_+3A_optimizer">optimizer</code></td>
<td>
<p>The method used in the optimization procedure. Possible choices
are 'LBFGS' and 'SGD'. Default is 'LBFGS'.</p>
</td></tr>
<tr><td><code id="brulee_multinomial_reg_+3A_learn_rate">learn_rate</code></td>
<td>
<p>A positive number that controls the rapidity that the model
moves along the descent path. Values around 0.1 or less are typical.
(<code>optimizer = "SGD"</code> only)</p>
</td></tr>
<tr><td><code id="brulee_multinomial_reg_+3A_momentum">momentum</code></td>
<td>
<p>A positive number usually on <code style="white-space: pre;">&#8288;[0.50, 0.99]&#8288;</code> for the momentum
parameter in gradient descent.  (<code>optimizer = "SGD"</code> only)</p>
</td></tr>
<tr><td><code id="brulee_multinomial_reg_+3A_batch_size">batch_size</code></td>
<td>
<p>An integer for the number of training set points in each
batch. (<code>optimizer = "SGD"</code> only)</p>
</td></tr>
<tr><td><code id="brulee_multinomial_reg_+3A_class_weights">class_weights</code></td>
<td>
<p>Numeric class weights (classification only). The value
can be:
</p>

<ul>
<li><p> A named numeric vector (in any order) where the names are the outcome
factor levels.
</p>
</li>
<li><p> An unnamed numeric vector assumed to be in the same order as the outcome
factor levels.
</p>
</li>
<li><p> A single numeric value for the least frequent class in the training data
and all other classes receive a weight of one.
</p>
</li></ul>
</td></tr>
<tr><td><code id="brulee_multinomial_reg_+3A_stop_iter">stop_iter</code></td>
<td>
<p>A non-negative integer for how many iterations with no
improvement before stopping.</p>
</td></tr>
<tr><td><code id="brulee_multinomial_reg_+3A_verbose">verbose</code></td>
<td>
<p>A logical that prints out the iteration history.</p>
</td></tr>
<tr><td><code id="brulee_multinomial_reg_+3A_formula">formula</code></td>
<td>
<p>A formula specifying the outcome term(s) on the left-hand side,
and the predictor term(s) on the right-hand side.</p>
</td></tr>
<tr><td><code id="brulee_multinomial_reg_+3A_data">data</code></td>
<td>
<p>When a <strong>recipe</strong> or <strong>formula</strong> is used, <code>data</code> is specified as:
</p>

<ul>
<li><p> A <strong>data frame</strong> containing both the predictors and the outcome.
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>This function fits a linear combination of coefficients and predictors to
model the log of the class probabilities. The training process optimizes the
cross-entropy loss function.
</p>
<p>By default, training halts when the validation loss increases for at least
<code>step_iter</code> iterations. If <code>validation = 0</code> the training set loss is used.
</p>
<p>The <em>predictors</em> data should all be numeric and encoded in the same units (e.g.
standardized to the same range or distribution). If there are factor
predictors, use a recipe or formula to create indicator variables (or some
other method) to make them numeric. Predictors should be in the same units
before training.
</p>
<p>The model objects are saved for each epoch so that the number of epochs can
be efficiently tuned. Both the <code><a href="#topic+coef">coef()</a></code> and <code><a href="stats.html#topic+predict">predict()</a></code> methods for this
model have an <code>epoch</code> argument (which defaults to the epoch with the best
loss value).
</p>
<p>The use of the L1 penalty (a.k.a. the lasso penalty) does <em>not</em> force
parameters to be strictly zero (as it does in packages such as <span class="pkg">glmnet</span>).
The zeroing out of parameters is a specific feature the optimization method
used in those packages.
</p>


<h3>Value</h3>

<p>A <code>brulee_multinomial_reg</code> object with elements:
</p>

<ul>
<li> <p><code>models_obj</code>: a serialized raw vector for the torch module.
</p>
</li>
<li> <p><code>estimates</code>: a list of matrices with the model parameter estimates per
epoch.
</p>
</li>
<li> <p><code>best_epoch</code>: an integer for the epoch with the smallest loss.
</p>
</li>
<li> <p><code>loss</code>: A vector of loss values (MSE for regression, negative log-
likelihood for classification) at each epoch.
</p>
</li>
<li> <p><code>dim</code>: A list of data dimensions.
</p>
</li>
<li> <p><code>parameters</code>: A list of some tuning parameter values.
</p>
</li>
<li> <p><code>blueprint</code>: The <code>hardhat</code> blueprint data.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+predict.brulee_multinomial_reg">predict.brulee_multinomial_reg()</a></code>, <code><a href="#topic+coef.brulee_multinomial_reg">coef.brulee_multinomial_reg()</a></code>,
<code><a href="#topic+autoplot.brulee_multinomial_reg">autoplot.brulee_multinomial_reg()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (torch::torch_is_installed() &amp; rlang::is_installed(c("recipes", "yardstick", "modeldata"))) {

  library(recipes)
  library(yardstick)

  data(penguins, package = "modeldata")

  penguins &lt;- penguins %&gt;% na.omit()

  set.seed(122)
  in_train &lt;- sample(1:nrow(penguins), 200)
  penguins_train &lt;- penguins[ in_train,]
  penguins_test  &lt;- penguins[-in_train,]

  rec &lt;- recipe(island ~ ., data = penguins_train) %&gt;%
    step_dummy(species, sex) %&gt;%
    step_normalize(all_predictors())

  set.seed(3)
  fit &lt;- brulee_multinomial_reg(rec, data = penguins_train, epochs = 5)
  fit

  predict(fit, penguins_test) %&gt;%
    bind_cols(penguins_test) %&gt;%
    conf_mat(island, .pred_class)
}

</code></pre>

<hr>
<h2 id='brulee-autoplot'>Plot model loss over epochs</h2><span id='topic+brulee-autoplot'></span><span id='topic+autoplot.brulee_mlp'></span><span id='topic+autoplot.brulee_logistic_reg'></span><span id='topic+autoplot.brulee_multinomial_reg'></span><span id='topic+autoplot.brulee_linear_reg'></span>

<h3>Description</h3>

<p>Plot model loss over epochs
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'brulee_mlp'
autoplot(object, ...)

## S3 method for class 'brulee_logistic_reg'
autoplot(object, ...)

## S3 method for class 'brulee_multinomial_reg'
autoplot(object, ...)

## S3 method for class 'brulee_linear_reg'
autoplot(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="brulee-autoplot_+3A_object">object</code></td>
<td>
<p>A <code>brulee_mlp</code>, <code>brulee_logistic_reg</code>,
<code>brulee_multinomial_reg</code>, or <code>brulee_linear_reg</code> object.</p>
</td></tr>
<tr><td><code id="brulee-autoplot_+3A_...">...</code></td>
<td>
<p>Not currently used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function plots the loss function across the available epochs. A
vertical line shows the epoch with the best loss value.
</p>


<h3>Value</h3>

<p>A <code>ggplot</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (torch::torch_is_installed() &amp; rlang::is_installed(c("recipes", "yardstick", "modeldata"))) {
 library(ggplot2)
 library(recipes)
 theme_set(theme_bw())

 data(ames, package = "modeldata")

 ames$Sale_Price &lt;- log10(ames$Sale_Price)

 set.seed(1)
 in_train &lt;- sample(1:nrow(ames), 2000)
 ames_train &lt;- ames[ in_train,]
 ames_test  &lt;- ames[-in_train,]

 ames_rec &lt;-
  recipe(Sale_Price ~ Longitude + Latitude, data = ames_train) %&gt;%
    step_normalize(all_numeric_predictors())

 set.seed(2)
 fit &lt;- brulee_mlp(ames_rec, data = ames_train, epochs = 50, batch_size = 32)

 autoplot(fit)
}

</code></pre>

<hr>
<h2 id='brulee-coefs'>Extract Model Coefficients</h2><span id='topic+brulee-coefs'></span><span id='topic+coef.brulee_logistic_reg'></span><span id='topic+coef.brulee_linear_reg'></span><span id='topic+coef.brulee_mlp'></span><span id='topic+coef.brulee_multinomial_reg'></span>

<h3>Description</h3>

<p>Extract Model Coefficients
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'brulee_logistic_reg'
coef(object, epoch = NULL, ...)

## S3 method for class 'brulee_linear_reg'
coef(object, epoch = NULL, ...)

## S3 method for class 'brulee_mlp'
coef(object, epoch = NULL, ...)

## S3 method for class 'brulee_multinomial_reg'
coef(object, epoch = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="brulee-coefs_+3A_object">object</code></td>
<td>
<p>A model fit from <span class="pkg">brulee</span>.</p>
</td></tr>
<tr><td><code id="brulee-coefs_+3A_epoch">epoch</code></td>
<td>
<p>A single integer for the training iteration. If left <code>NULL</code>,
the estimates from the best model fit (via internal performance metrics).</p>
</td></tr>
<tr><td><code id="brulee-coefs_+3A_...">...</code></td>
<td>
<p>Not currently used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For logistic/linear regression, a named vector. For neural networks,
a list of arrays.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (torch::torch_is_installed() &amp; rlang::is_installed(c("recipes", "modeldata"))) {

 data(ames, package = "modeldata")

 ames$Sale_Price &lt;- log10(ames$Sale_Price)

 set.seed(1)
 in_train &lt;- sample(1:nrow(ames), 2000)
 ames_train &lt;- ames[ in_train,]
 ames_test  &lt;- ames[-in_train,]

 # Using recipe
 library(recipes)

 ames_rec &lt;-
  recipe(Sale_Price ~ Longitude + Latitude, data = ames_train) %&gt;%
    step_normalize(all_numeric_predictors())

 set.seed(2)
 fit &lt;- brulee_linear_reg(ames_rec, data = ames_train,
                           epochs = 50, batch_size = 32)

 coef(fit)
 coef(fit, epoch = 1)
}

</code></pre>

<hr>
<h2 id='matrix_to_dataset'>Convert data to torch format</h2><span id='topic+matrix_to_dataset'></span>

<h3>Description</h3>

<p>For an x/y interface, <code>matrix_to_dataset()</code> converts the data to proper
encodings then formats the results for consumption by <code>torch</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matrix_to_dataset(x, y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="matrix_to_dataset_+3A_x">x</code></td>
<td>
<p>A numeric matrix of predictors.</p>
</td></tr>
<tr><td><code id="matrix_to_dataset_+3A_y">y</code></td>
<td>
<p>A vector. If regression than <code>y</code> is numeric. For classification, it
is a factor.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Missing values should be removed before passing data to this function.
</p>


<h3>Value</h3>

<p>An R6 index sampler object with classes &quot;training_set&quot;,
&quot;dataset&quot;, and &quot;R6&quot;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (torch::torch_is_installed()) {
  matrix_to_dataset(as.matrix(mtcars[, -1]), mtcars$mpg)
}
</code></pre>

<hr>
<h2 id='predict.brulee_linear_reg'>Predict from a <code>brulee_linear_reg</code></h2><span id='topic+predict.brulee_linear_reg'></span>

<h3>Description</h3>

<p>Predict from a <code>brulee_linear_reg</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'brulee_linear_reg'
predict(object, new_data, type = NULL, epoch = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.brulee_linear_reg_+3A_object">object</code></td>
<td>
<p>A <code>brulee_linear_reg</code> object.</p>
</td></tr>
<tr><td><code id="predict.brulee_linear_reg_+3A_new_data">new_data</code></td>
<td>
<p>A data frame or matrix of new predictors.</p>
</td></tr>
<tr><td><code id="predict.brulee_linear_reg_+3A_type">type</code></td>
<td>
<p>A single character. The type of predictions to generate.
Valid options are:
</p>

<ul>
<li> <p><code>"numeric"</code> for numeric predictions.
</p>
</li></ul>
</td></tr>
<tr><td><code id="predict.brulee_linear_reg_+3A_epoch">epoch</code></td>
<td>
<p>An integer for the epoch to make predictions. If this value
is larger than the maximum number that was fit, a warning is issued and the
parameters from the last epoch are used. If left <code>NULL</code>, the epoch
associated with the smallest loss is used.</p>
</td></tr>
<tr><td><code id="predict.brulee_linear_reg_+3A_...">...</code></td>
<td>
<p>Not used, but required for extensibility.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble of predictions. The number of rows in the tibble is guaranteed
to be the same as the number of rows in <code>new_data</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (torch::torch_is_installed() &amp; rlang::is_installed("recipes")) {

 data(ames, package = "modeldata")

 ames$Sale_Price &lt;- log10(ames$Sale_Price)

 set.seed(1)
 in_train &lt;- sample(1:nrow(ames), 2000)
 ames_train &lt;- ames[ in_train,]
 ames_test  &lt;- ames[-in_train,]

 # Using recipe
 library(recipes)

 ames_rec &lt;-
  recipe(Sale_Price ~ Longitude + Latitude, data = ames_train) %&gt;%
    step_normalize(all_numeric_predictors())

 set.seed(2)
 fit &lt;- brulee_linear_reg(ames_rec, data = ames_train,
                           epochs = 50, batch_size = 32)

 predict(fit, ames_test)
}

</code></pre>

<hr>
<h2 id='predict.brulee_logistic_reg'>Predict from a <code>brulee_logistic_reg</code></h2><span id='topic+predict.brulee_logistic_reg'></span>

<h3>Description</h3>

<p>Predict from a <code>brulee_logistic_reg</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'brulee_logistic_reg'
predict(object, new_data, type = NULL, epoch = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.brulee_logistic_reg_+3A_object">object</code></td>
<td>
<p>A <code>brulee_logistic_reg</code> object.</p>
</td></tr>
<tr><td><code id="predict.brulee_logistic_reg_+3A_new_data">new_data</code></td>
<td>
<p>A data frame or matrix of new predictors.</p>
</td></tr>
<tr><td><code id="predict.brulee_logistic_reg_+3A_type">type</code></td>
<td>
<p>A single character. The type of predictions to generate.
Valid options are:
</p>

<ul>
<li> <p><code>"class"</code> for hard class predictions
</p>
</li>
<li> <p><code>"prob"</code> for soft class predictions (i.e., class probabilities)
</p>
</li></ul>
</td></tr>
<tr><td><code id="predict.brulee_logistic_reg_+3A_epoch">epoch</code></td>
<td>
<p>An integer for the epoch to make predictions. If this value
is larger than the maximum number that was fit, a warning is issued and the
parameters from the last epoch are used. If left <code>NULL</code>, the epoch
associated with the smallest loss is used.</p>
</td></tr>
<tr><td><code id="predict.brulee_logistic_reg_+3A_...">...</code></td>
<td>
<p>Not used, but required for extensibility.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble of predictions. The number of rows in the tibble is guaranteed
to be the same as the number of rows in <code>new_data</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (torch::torch_is_installed() &amp; rlang::is_installed(c("recipes", "yardstick", "modeldata"))) {

  library(recipes)
  library(yardstick)

  data(penguins, package = "modeldata")

  penguins &lt;- penguins %&gt;% na.omit()

  set.seed(122)
  in_train &lt;- sample(1:nrow(penguins), 200)
  penguins_train &lt;- penguins[ in_train,]
  penguins_test  &lt;- penguins[-in_train,]

  rec &lt;- recipe(sex ~ ., data = penguins_train) %&gt;%
    step_dummy(all_nominal_predictors()) %&gt;%
    step_normalize(all_numeric_predictors())

  set.seed(3)
  fit &lt;- brulee_logistic_reg(rec, data = penguins_train, epochs = 5)
  fit

  predict(fit, penguins_test)

  predict(fit, penguins_test, type = "prob") %&gt;%
    bind_cols(penguins_test) %&gt;%
    roc_curve(sex, .pred_female) %&gt;%
    autoplot()

}

</code></pre>

<hr>
<h2 id='predict.brulee_mlp'>Predict from a <code>brulee_mlp</code></h2><span id='topic+predict.brulee_mlp'></span>

<h3>Description</h3>

<p>Predict from a <code>brulee_mlp</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'brulee_mlp'
predict(object, new_data, type = NULL, epoch = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.brulee_mlp_+3A_object">object</code></td>
<td>
<p>A <code>brulee_mlp</code> object.</p>
</td></tr>
<tr><td><code id="predict.brulee_mlp_+3A_new_data">new_data</code></td>
<td>
<p>A data frame or matrix of new predictors.</p>
</td></tr>
<tr><td><code id="predict.brulee_mlp_+3A_type">type</code></td>
<td>
<p>A single character. The type of predictions to generate.
Valid options are:
</p>

<ul>
<li> <p><code>"numeric"</code> for numeric predictions.
</p>
</li>
<li> <p><code>"class"</code> for hard class predictions
</p>
</li>
<li> <p><code>"prob"</code> for soft class predictions (i.e., class probabilities)
</p>
</li></ul>
</td></tr>
<tr><td><code id="predict.brulee_mlp_+3A_epoch">epoch</code></td>
<td>
<p>An integer for the epoch to make predictions. If this value
is larger than the maximum number that was fit, a warning is issued and the
parameters from the last epoch are used. If left <code>NULL</code>, the epoch
associated with the smallest loss is used.</p>
</td></tr>
<tr><td><code id="predict.brulee_mlp_+3A_...">...</code></td>
<td>
<p>Not used, but required for extensibility.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble of predictions. The number of rows in the tibble is guaranteed
to be the same as the number of rows in <code>new_data</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (torch::torch_is_installed() &amp; rlang::is_installed(c("recipes", "modeldata"))) {
 # regression example:

 data(ames, package = "modeldata")

 ames$Sale_Price &lt;- log10(ames$Sale_Price)

 set.seed(1)
 in_train &lt;- sample(1:nrow(ames), 2000)
 ames_train &lt;- ames[ in_train,]
 ames_test  &lt;- ames[-in_train,]

 # Using recipe
 library(recipes)

 ames_rec &lt;-
  recipe(Sale_Price ~ Longitude + Latitude, data = ames_train) %&gt;%
    step_normalize(all_numeric_predictors())

 set.seed(2)
 fit &lt;- brulee_mlp(ames_rec, data = ames_train, epochs = 50, batch_size = 32)

 predict(fit, ames_test)
}

</code></pre>

<hr>
<h2 id='predict.brulee_multinomial_reg'>Predict from a <code>brulee_multinomial_reg</code></h2><span id='topic+predict.brulee_multinomial_reg'></span>

<h3>Description</h3>

<p>Predict from a <code>brulee_multinomial_reg</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'brulee_multinomial_reg'
predict(object, new_data, type = NULL, epoch = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.brulee_multinomial_reg_+3A_object">object</code></td>
<td>
<p>A <code>brulee_multinomial_reg</code> object.</p>
</td></tr>
<tr><td><code id="predict.brulee_multinomial_reg_+3A_new_data">new_data</code></td>
<td>
<p>A data frame or matrix of new predictors.</p>
</td></tr>
<tr><td><code id="predict.brulee_multinomial_reg_+3A_type">type</code></td>
<td>
<p>A single character. The type of predictions to generate.
Valid options are:
</p>

<ul>
<li> <p><code>"class"</code> for hard class predictions
</p>
</li>
<li> <p><code>"prob"</code> for soft class predictions (i.e., class probabilities)
</p>
</li></ul>
</td></tr>
<tr><td><code id="predict.brulee_multinomial_reg_+3A_epoch">epoch</code></td>
<td>
<p>An integer for the epoch to make predictions. If this value
is larger than the maximum number that was fit, a warning is issued and the
parameters from the last epoch are used. If left <code>NULL</code>, the epoch
associated with the smallest loss is used.</p>
</td></tr>
<tr><td><code id="predict.brulee_multinomial_reg_+3A_...">...</code></td>
<td>
<p>Not used, but required for extensibility.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble of predictions. The number of rows in the tibble is guaranteed
to be the same as the number of rows in <code>new_data</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (torch::torch_is_installed() &amp; rlang::is_installed(c("recipes", "yardstick", "modeldata"))) {

  library(recipes)
  library(yardstick)

  data(penguins, package = "modeldata")

  penguins &lt;- penguins %&gt;% na.omit()

  set.seed(122)
  in_train &lt;- sample(1:nrow(penguins), 200)
  penguins_train &lt;- penguins[ in_train,]
  penguins_test  &lt;- penguins[-in_train,]

  rec &lt;- recipe(island ~ ., data = penguins_train) %&gt;%
    step_dummy(species, sex) %&gt;%
    step_normalize(all_numeric_predictors())

  set.seed(3)
  fit &lt;- brulee_multinomial_reg(rec, data = penguins_train, epochs = 5)
  fit

  predict(fit, penguins_test) %&gt;%
    bind_cols(penguins_test) %&gt;%
    conf_mat(island, .pred_class)
}

</code></pre>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic++25+3E+25'></span><span id='topic+autoplot'></span><span id='topic+tunable'></span><span id='topic+coef'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>dplyr</dt><dd><p><code><a href="dplyr.html#topic+reexports">%&gt;%</a></code></p>
</dd>
<dt>generics</dt><dd><p><code><a href="generics.html#topic+tunable">tunable</a></code></p>
</dd>
<dt>ggplot2</dt><dd><p><code><a href="ggplot2.html#topic+autoplot">autoplot</a></code></p>
</dd>
<dt>stats</dt><dd><p><code><a href="stats.html#topic+coef">coef</a></code></p>
</dd>
</dl>

<hr>
<h2 id='schedule_decay_time'>Change the learning rate over time</h2><span id='topic+schedule_decay_time'></span><span id='topic+schedule_decay_expo'></span><span id='topic+schedule_step'></span><span id='topic+schedule_cyclic'></span><span id='topic+set_learn_rate'></span>

<h3>Description</h3>

<p>Learning rate schedulers alter the learning rate to adjust as training
proceeds. In most cases, the learning rate decreases as epochs increase.
The <code style="white-space: pre;">&#8288;schedule_*()&#8288;</code> functions are individual schedulers and
<code><a href="#topic+set_learn_rate">set_learn_rate()</a></code> is a general interface.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>schedule_decay_time(epoch, initial = 0.1, decay = 1)

schedule_decay_expo(epoch, initial = 0.1, decay = 1)

schedule_step(epoch, initial = 0.1, reduction = 1/2, steps = 5)

schedule_cyclic(epoch, initial = 0.001, largest = 0.1, step_size = 5)

set_learn_rate(epoch, learn_rate, type = "none", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="schedule_decay_time_+3A_epoch">epoch</code></td>
<td>
<p>An integer for the number of training epochs (zero being the
initial value),</p>
</td></tr>
<tr><td><code id="schedule_decay_time_+3A_initial">initial</code></td>
<td>
<p>A positive numeric value for the starting learning rate.</p>
</td></tr>
<tr><td><code id="schedule_decay_time_+3A_decay">decay</code></td>
<td>
<p>A positive numeric constant for decreasing the rate (see
Details below).</p>
</td></tr>
<tr><td><code id="schedule_decay_time_+3A_reduction">reduction</code></td>
<td>
<p>A positive numeric constant stating the proportional decrease
in the learning rate occurring at every <code>steps</code> epochs.</p>
</td></tr>
<tr><td><code id="schedule_decay_time_+3A_steps">steps</code></td>
<td>
<p>The number of epochs before the learning rate changes.</p>
</td></tr>
<tr><td><code id="schedule_decay_time_+3A_largest">largest</code></td>
<td>
<p>The maximum learning rate in the cycle.</p>
</td></tr>
<tr><td><code id="schedule_decay_time_+3A_step_size">step_size</code></td>
<td>
<p>The half-length of a cycle.</p>
</td></tr>
<tr><td><code id="schedule_decay_time_+3A_learn_rate">learn_rate</code></td>
<td>
<p>A constant learning rate (when no scheduler is used),</p>
</td></tr>
<tr><td><code id="schedule_decay_time_+3A_type">type</code></td>
<td>
<p>A single character value for the type of scheduler. Possible
values are: &quot;decay_time&quot;, &quot;decay_expo&quot;, &quot;none&quot;, &quot;cyclic&quot;, and &quot;step&quot;.</p>
</td></tr>
<tr><td><code id="schedule_decay_time_+3A_...">...</code></td>
<td>
<p>Arguments to pass to the individual scheduler functions (e.g.
<code>reduction</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The details for how the schedulers change the rates:
</p>

<ul>
<li> <p><code>schedule_decay_time()</code>: <code class="reqn">rate(epoch) = initial/(1 + decay \times epoch)</code>
</p>
</li>
<li> <p><code>schedule_decay_expo()</code>: <code class="reqn">rate(epoch) = initial\exp(-decay \times epoch)</code>
</p>
</li>
<li> <p><code>schedule_step()</code>: <code class="reqn">rate(epoch) = initial \times reduction^{floor(epoch / steps)}</code>
</p>
</li>
<li> <p><code>schedule_cyclic()</code>: <code class="reqn">cycle = floor( 1 + (epoch / 2 / step size) )</code>,
<code class="reqn">x = abs( ( epoch / step size ) - ( 2 * cycle) + 1 )</code>, and
<code class="reqn">rate(epoch) = initial + ( largest - initial ) * \max( 0, 1 - x)</code>
</p>
</li></ul>



<h3>Value</h3>

<p>A numeric value for the updated learning rate.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+brulee_mlp">brulee_mlp()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>if (rlang::is_installed("purrr")) {
 library(ggplot2)
 library(dplyr)
 library(purrr)

 iters &lt;- 0:50

 bind_rows(
  tibble(epoch = iters, rate = map_dbl(iters, schedule_decay_time), type = "decay_time"),
  tibble(epoch = iters, rate = map_dbl(iters, schedule_decay_expo), type = "decay_expo"),
  tibble(epoch = iters, rate = map_dbl(iters, schedule_step), type = "step"),
  tibble(epoch = iters, rate = map_dbl(iters, schedule_cyclic), type = "cyclic")
 ) %&gt;%
  ggplot(aes(epoch, rate)) +
  geom_line() +
  facet_wrap(~ type)

}

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
