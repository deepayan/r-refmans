<!DOCTYPE html><html><head><title>Help for package biopixR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {biopixR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#adaptiveInterpolation'><p>Connects LineEnd with the nearest labeled region</p></a></li>
<li><a href='#beads'><p>Image of microbeads</p></a></li>
<li><a href='#changePixelColor'><p>Change the color of pixels</p></a></li>
<li><a href='#droplet_beads'><p>Image of microbeads in luminescence channel</p></a></li>
<li><a href='#droplets'><p>Droplets containing microbeads</p></a></li>
<li><a href='#edgeDetection'><p>Canny edge detector</p></a></li>
<li><a href='#fillLineGaps'><p>Reconnecting discontinuous lines</p></a></li>
<li><a href='#imgPipe'><p>Image analysis pipeline</p></a></li>
<li><a href='#interactive_objectDetection'><p>Interactive object detection</p></a></li>
<li><a href='#interpolatePixels'><p>Pixel Interpolation</p></a></li>
<li><a href='#objectDetection'><p>Object detection</p></a></li>
<li><a href='#proximityFilter'><p>Proximity-based exclusion</p></a></li>
<li><a href='#resultAnalytics'><p>Image Summary</p></a></li>
<li><a href='#sizeFilter'><p>Size-based exclusion</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Extracting Insights from Biological Images</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.4</td>
</tr>
<tr>
<td>Description:</td>
<td>Combines the 'magick' and 'imager' packages to streamline image analysis, focusing on feature extraction and quantification from biological images, especially 
    microparticles. By providing high throughput pipelines and clustering capabilities, 'biopixR' facilitates efficient insight generation for researchers (Schneider J. et al. (2019) 
    &lt;<a href="https://doi.org/10.21037%2Fjlpm.2019.04.05">doi:10.21037/jlpm.2019.04.05</a>&gt;).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-3">LGPL (&ge; 3)</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>BuildVignettes:</td>
<td>true</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.2.0), data.table, imager, magick, tcltk, foreach</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, doParallel, kohonen</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/Brauckhoff/biopixR">https://github.com/Brauckhoff/biopixR</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/Brauckhoff/biopixR/issues">https://github.com/Brauckhoff/biopixR/issues</a></td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-24 08:25:58 UTC; brauctim</td>
</tr>
<tr>
<td>Author:</td>
<td>Tim Brauckhoff <a href="https://orcid.org/0009-0002-0142-7017"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Stefan Roediger <a href="https://orcid.org/0000-0002-1441-6512"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb],
  Coline Kieffer [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Tim Brauckhoff &lt;Tim.Brauckhoff@b-tu.de&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-25 19:10:09 UTC</td>
</tr>
</table>
<hr>
<h2 id='adaptiveInterpolation'>Connects LineEnd with the nearest labeled region</h2><span id='topic+adaptiveInterpolation'></span>

<h3>Description</h3>

<p>Function scans an increasing radius around a line end and connects it with
the nearest labeled region.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adaptiveInterpolation(
  end_points_df,
  diagonal_edges_df,
  clean_lab_df,
  lineends_cimg,
  radius = 5
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adaptiveInterpolation_+3A_end_points_df">end_points_df</code></td>
<td>
<p>data frame with the coordinates of all line ends. can
be obtained with <code><a href="magick.html#topic+image_morphology">image_morphology</a></code>.</p>
</td></tr>
<tr><td><code id="adaptiveInterpolation_+3A_diagonal_edges_df">diagonal_edges_df</code></td>
<td>
<p>data frame with coordinates of diagonal line ends.
can also be obtained by <code><a href="magick.html#topic+image_morphology">image_morphology</a></code>.</p>
</td></tr>
<tr><td><code id="adaptiveInterpolation_+3A_clean_lab_df">clean_lab_df</code></td>
<td>
<p>data of type 'data.frame', containing the x, y and value
information of every labeled region in an image. (only the edges should be
labeled)</p>
</td></tr>
<tr><td><code id="adaptiveInterpolation_+3A_lineends_cimg">lineends_cimg</code></td>
<td>
<p>image with dimensions of the image with discontinuous
edges. just for giving the dimensions of the output matrix.</p>
</td></tr>
<tr><td><code id="adaptiveInterpolation_+3A_radius">radius</code></td>
<td>
<p>maximal radius that should be scanned for another cluster</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is intended to be part of the fillLineGaps function, which
does the thresholding and line end detection preprocessing. The
adaptiveInterpolation creates a matrix in the dimensions of the original
image. At the beginning there are only background values (0) = black image.
The function then searches for LineEnds and looks for a given radius around
this line end for the nearest labeled region. The own cluster of the line
end is of course not considered as nearest neighbor.If another cluster is
found, the interpolatePixels function is used to connect the line end to the
found cluster. This means that specified pixels of the matrix are
transformed to a foreground value of (1). The diagonal line ends get a
special treatment, because for the labeling function,7 the diagonal pixels
are always treated as a separate cluster, which makes them difficult to
reconnect. To deal with this problem, diagonal line ends ignore not only
their cluster, but also the cluster of the direct neighbor. Thereafter,
the same procedure as before is repeated, where pixel values are changed
according to the interpolatePixel function.
</p>


<h3>Value</h3>

<p>binary matrix that can be applied as an overlay, for example with
<code><a href="imager.html#topic+imager.combine">imager.combine</a></code> to fill the gaps between line ends.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># creating an artificial binary image
mat &lt;- matrix(0, 8, 8)
mat[3, 1:2] &lt;- 1
mat[4, 3] &lt;- 1
mat[7:8, 3] &lt;- 1
mat[5, 6:8] &lt;- 1
mat_cimg &lt;- as.cimg(mat)

# preprocessing / LineEnd detection / labeling  (done in fillLineGaps)
mat_cimg_m &lt;- mirror(mat_cimg, axis = "x")
mat_magick &lt;- cimg2magick(mat_cimg)
lineends &lt;- image_morphology(mat_magick, "HitAndMiss", "LineEnds")
diagonalends &lt;- image_morphology(mat_magick, "HitAndMiss", "LineEnds:2&gt;")
lineends_cimg &lt;- magick2cimg(lineends)
diagonalends_cimg &lt;- magick2cimg(diagonalends)
end_points &lt;- which(lineends_cimg == TRUE, arr.ind = TRUE)
end_points_df &lt;- as.data.frame(end_points)
colnames(end_points_df) &lt;- c("x", "y", "dim3", "dim4")
diagonal_edges &lt;- which(diagonalends_cimg == TRUE, arr.ind = TRUE)
diagonal_edges_df &lt;- as.data.frame(diagonal_edges)
colnames(diagonal_edges_df) &lt;- c("x", "y", "dim3", "dim4")
lab &lt;- label(mat_cimg_m)
df_lab &lt;- as.data.frame(lab) |&gt; subset(value &gt; 0)
alt_x &lt;- list()
alt_y &lt;- list()
alt_value &lt;- list()
for (g in 1:nrow(df_lab)) {
  if (mat_cimg_m[df_lab$x[g], df_lab$y[g], 1, 1] == 1) {
    alt_x[g] &lt;- df_lab$x[g]
    alt_y[g] &lt;- df_lab$y[g]
    alt_value[g] &lt;- df_lab$value[g]
  }
}
clean_lab_df &lt;- data.frame(
  x = unlist(alt_x),
  y = unlist(alt_y),
  value = unlist(alt_value)
)

# actual function
overlay &lt;- adaptiveInterpolation(
  end_points_df,
  diagonal_edges_df,
  clean_lab_df,
  mat_cimg
)
parmax(list(mat_cimg_m, as.cimg(overlay$overlay))) |&gt; plot()
</code></pre>

<hr>
<h2 id='beads'>Image of microbeads</h2><span id='topic+beads'></span>

<h3>Description</h3>

<p>This fluorescence image, formatted as 'cimg' with dimensions of
117 x 138 pixels, shows microbeads. With a single color channel, the image
provides an ideal example for in-depth analysis of microbead structures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>beads
</code></pre>


<h3>Format</h3>

<p>The image was imported using imager and is therefore of class:
&quot;cimg&quot; &quot;imager_array&quot; &quot;numeric&quot;
</p>


<h3>Details</h3>

<p>Dimensions: width - 117; height - 138; depth - 1; channel - 1
</p>


<h3>References</h3>

<p>The image was provided by Coline Kieffer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(beads)
plot(beads)
</code></pre>

<hr>
<h2 id='changePixelColor'>Change the color of pixels</h2><span id='topic+changePixelColor'></span>

<h3>Description</h3>

<p>Can be used to change the color of specified pixels in an image. The coordinates
of the pixels are needed to colorize them.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>changePixelColor(img, coords, color = "purple", visualize = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="changePixelColor_+3A_img">img</code></td>
<td>
<p>image (import by <code><a href="imager.html#topic+load.image">load.image</a></code>)</p>
</td></tr>
<tr><td><code id="changePixelColor_+3A_coords">coords</code></td>
<td>
<p>coordinates specifying which pixels to be colored (should
be a X|Y Data frame (first column: X; second column: Y)).</p>
</td></tr>
<tr><td><code id="changePixelColor_+3A_color">color</code></td>
<td>
<p>color with which to replace specified pixels. can be either a
an RGB triplet or one of the colors listed by <code><a href="grDevices.html#topic+colors">colors</a></code>.</p>
</td></tr>
<tr><td><code id="changePixelColor_+3A_visualize">visualize</code></td>
<td>
<p>if TRUE the resulting image gets plotted</p>
</td></tr>
</table>


<h3>Value</h3>

<p>cimg with changed colors at desired positions and plot of the cimg
</p>


<h3>References</h3>

<p>https://CRAN.R-project.org/package=countcolors
</p>


<h3>Examples</h3>

<pre><code class='language-R'>coordinates &lt;- objectDetection(beads)
changePixelColor(beads, coordinates$coordinates)
</code></pre>

<hr>
<h2 id='droplet_beads'>Image of microbeads in luminescence channel</h2><span id='topic+droplet_beads'></span>

<h3>Description</h3>

<p>The image shows red fluorescence rhodamine microbeads measuring
151 x 112 pixels. The fluorescence channel was used to obtain the image,
resulting in identical dimensions and positions of the beads as in the
original image (droplets).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>droplet_beads
</code></pre>


<h3>Format</h3>

<p>The image was imported using imager and is therefore of class:
&quot;cimg&quot; &quot;imager_array&quot; &quot;numeric&quot;
</p>


<h3>Details</h3>

<p>Dimensions: width - 151; height - 112; depth - 1; channel - 3
</p>


<h3>References</h3>

<p>The image was provided by Coline Kieffer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(droplet_beads)
plot(droplet_beads)
</code></pre>

<hr>
<h2 id='droplets'>Droplets containing microbeads</h2><span id='topic+droplets'></span>

<h3>Description</h3>

<p>The image displays a water-oil emulsion with droplets observed through
brightfield microscopy. It is formatted as 'cimg' and sized at 151 Ã— 112
pixels. The droplets vary in size, and some contain microbeads, which adds
complexity. Brightfield microscopy enhances the contrast between water and
oil, revealing the droplet arrangement.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>droplets
</code></pre>


<h3>Format</h3>

<p>The image was imported using imager and is therefore of class:
&quot;cimg&quot; &quot;imager_array&quot; &quot;numeric&quot;
</p>


<h3>Details</h3>

<p>Dimensions: width - 151; height - 112; depth - 1; channel - 1
</p>


<h3>References</h3>

<p>The image was provided by Coline Kieffer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(droplets)
plot(droplets)
</code></pre>

<hr>
<h2 id='edgeDetection'>Canny edge detector</h2><span id='topic+edgeDetection'></span>

<h3>Description</h3>

<p>Adapted code from the imager <code><a href="imager.html#topic+cannyEdges">cannyEdges</a></code>) function
without the usage of dplyr and purrr. If the threshold parameters are
missing, they are determined automatically using a k-means heuristic. Use
the alpha parameter to adjust the automatic thresholds up or down. The
thresholds are returned as attributes. The edge detection is based on a
smoothed image gradient with a degree of smoothing set by the sigma
parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>edgeDetection(img, t1, t2, alpha = 1, sigma = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="edgeDetection_+3A_img">img</code></td>
<td>
<p>input image</p>
</td></tr>
<tr><td><code id="edgeDetection_+3A_t1">t1</code></td>
<td>
<p>threshold for weak edges (if missing, both thresholds are
determined automatically)</p>
</td></tr>
<tr><td><code id="edgeDetection_+3A_t2">t2</code></td>
<td>
<p>threshold for strong edges</p>
</td></tr>
<tr><td><code id="edgeDetection_+3A_alpha">alpha</code></td>
<td>
<p>threshold adjustment factor (default 1)</p>
</td></tr>
<tr><td><code id="edgeDetection_+3A_sigma">sigma</code></td>
<td>
<p>smoothing</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class 'cimg', displaying detected edges.
</p>


<h3>References</h3>

<p>https://CRAN.R-project.org/package=imager
</p>


<h3>Examples</h3>

<pre><code class='language-R'>edgeDetection(beads) |&gt; plot()
</code></pre>

<hr>
<h2 id='fillLineGaps'>Reconnecting discontinuous lines</h2><span id='topic+fillLineGaps'></span>

<h3>Description</h3>

<p>The function attempts to fill in edge discontinuities in order to enable
normal labeling and edge detection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fillLineGaps(
  droplet.img,
  bead.img = NULL,
  threshold = "13%",
  alpha = 0.75,
  sigma = 0.1,
  radius = 5,
  iterations = 2,
  visualize = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fillLineGaps_+3A_droplet.img">droplet.img</code></td>
<td>
<p>image that contains discontinuous lines like edges or
contours</p>
</td></tr>
<tr><td><code id="fillLineGaps_+3A_bead.img">bead.img</code></td>
<td>
<p>image that contains objects that should be removed before
before applying the fill algorithm</p>
</td></tr>
<tr><td><code id="fillLineGaps_+3A_threshold">threshold</code></td>
<td>
<p>&quot;in %&quot; (from <code><a href="imager.html#topic+threshold">threshold</a></code>)</p>
</td></tr>
<tr><td><code id="fillLineGaps_+3A_alpha">alpha</code></td>
<td>
<p>threshold adjustment factor for edge detection
(from <code><a href="imager.html#topic+cannyEdges">cannyEdges</a></code>)</p>
</td></tr>
<tr><td><code id="fillLineGaps_+3A_sigma">sigma</code></td>
<td>
<p>smoothing (from <code><a href="imager.html#topic+cannyEdges">cannyEdges</a></code>)</p>
</td></tr>
<tr><td><code id="fillLineGaps_+3A_radius">radius</code></td>
<td>
<p>maximal radius that should be scanned for another cluster</p>
</td></tr>
<tr><td><code id="fillLineGaps_+3A_iterations">iterations</code></td>
<td>
<p>how many times the algorithm should find line ends and
reconnect them to their closest neighbor</p>
</td></tr>
<tr><td><code id="fillLineGaps_+3A_visualize">visualize</code></td>
<td>
<p>if TRUE (default) a plot is displayed highlighting the
added pixels in the original image</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function pre-processes the image to enable the application of
adaptiveInterpolation. Pre-processing involves thresholding, optional
object removal, LineEnd and diagonal LineEnd detection, and labeling. The
threshold should be set to allow for some remaining &quot;bridge&quot; pixels between
gaps to facilitate reconnection. For more details about reconnection,
please consult adaptiveInterpolation.
Post-processing involves thinning the lines. When removing objects from an
image, their coordinates are collected using the objectDetection function.
Next, the pixels of the detected objects are nullified in the original
image, allowing the algorithm to proceed without the objects.
</p>


<h3>Value</h3>

<p>image with continuous edges (closed gaps)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fillLineGaps(droplets)
</code></pre>

<hr>
<h2 id='imgPipe'>Image analysis pipeline</h2><span id='topic+imgPipe'></span>

<h3>Description</h3>

<p>This function serves as a pipeline that integrates tools for complete
start-to-finish image analysis. It enables the handling of images from
different channels, including the analysis of dual-color microbeads. This
approach simplifies the workflow, providing a straightforward method to
analyze complex image data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>imgPipe(
  img1 = img,
  color1 = "color1",
  img2 = NULL,
  color2 = "color2",
  img3 = NULL,
  color3 = "color3",
  alpha = 1,
  sigma = 2,
  sizeFilter = TRUE,
  upperlimit = "auto",
  lowerlimit = "auto",
  proximityFilter = TRUE,
  radius = "auto",
  parallel = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="imgPipe_+3A_img1">img1</code></td>
<td>
<p>image (import by <code><a href="imager.html#topic+load.image">load.image</a></code>)</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_color1">color1</code></td>
<td>
<p>name of color in img1</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_img2">img2</code></td>
<td>
<p>image (import by <code><a href="imager.html#topic+load.image">load.image</a></code>)</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_color2">color2</code></td>
<td>
<p>name of color in img2</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_img3">img3</code></td>
<td>
<p>image (import by <code><a href="imager.html#topic+load.image">load.image</a></code>)</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_color3">color3</code></td>
<td>
<p>name of color in img3</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_alpha">alpha</code></td>
<td>
<p>threshold adjustment factor</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_sigma">sigma</code></td>
<td>
<p>smoothing</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_sizefilter">sizeFilter</code></td>
<td>
<p>applying sizeFilter function (default - TRUE)</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_upperlimit">upperlimit</code></td>
<td>
<p>highest accepted object size (only needed if
sizeFilter = TRUE)</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_lowerlimit">lowerlimit</code></td>
<td>
<p>smallest accepted object size (when 'auto' both limits are
calculated by using the mean and the standard deviation)</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_proximityfilter">proximityFilter</code></td>
<td>
<p>applying proximityFilter function (default - TRUE)</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_radius">radius</code></td>
<td>
<p>distance from one center in which no other centers
are allowed (in pixels)</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_parallel">parallel</code></td>
<td>
<p>if TRUE uses multiple cores (75 %) to process results</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of 3 to 4 objects:
</p>

<ol>
<li><p> summary of all the microbeads in the image
</p>
</li>
<li><p> detailed information about every single bead
</p>
</li>
<li><p> (optional) result for every individual color
</p>
</li>
<li><p> unfiltered coordinates of img1
</p>
</li></ol>



<h3>Examples</h3>

<pre><code class='language-R'>result &lt;- imgPipe(beads,
  alpha = 1, sigma = 2, upperlimit = 150,
  lowerlimit = 50
  )
plot(beads)
with(
  result$detailed,
  points(
    result$detailed$x,
    result$detailed$y,
    col = "darkgreen",
    pch = 19
    )
  )
</code></pre>

<hr>
<h2 id='interactive_objectDetection'>Interactive object detection</h2><span id='topic+interactive_objectDetection'></span>

<h3>Description</h3>

<p>This function uses the objectDetection function to visualize the detected
objects at varying threshold an smoothing parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>interactive_objectDetection(img, resolution = 0.1, return_param = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="interactive_objectDetection_+3A_img">img</code></td>
<td>
<p>image (preferred import: <code><a href="imager.html#topic+load.image">load.image</a></code>)</p>
</td></tr>
<tr><td><code id="interactive_objectDetection_+3A_resolution">resolution</code></td>
<td>
<p>resolution of slider</p>
</td></tr>
<tr><td><code id="interactive_objectDetection_+3A_return_param">return_param</code></td>
<td>
<p>used to define the final parameter values for alpha and
sigma printed in the console (TRUE or FALSE).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>values of alpha and sigma
</p>


<h3>References</h3>

<p>https://CRAN.R-project.org/package=magickGUI
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (interactive()) {
  interactive_objectDetection(beads)
}

</code></pre>

<hr>
<h2 id='interpolatePixels'>Pixel Interpolation</h2><span id='topic+interpolatePixels'></span>

<h3>Description</h3>

<p>Connects two points in a matrix, array, or an image.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>interpolatePixels(row1, col1, row2, col2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="interpolatePixels_+3A_row1">row1</code></td>
<td>
<p>first row: together with col1 coordinate for the first point</p>
</td></tr>
<tr><td><code id="interpolatePixels_+3A_col1">col1</code></td>
<td>
<p>first column: together with row1 coordinate for the first point</p>
</td></tr>
<tr><td><code id="interpolatePixels_+3A_row2">row2</code></td>
<td>
<p>second row: together with col2 coordinate for the second point</p>
</td></tr>
<tr><td><code id="interpolatePixels_+3A_col2">col2</code></td>
<td>
<p>second column: together with row2 coordinate for the second point</p>
</td></tr>
</table>


<h3>Value</h3>

<p>matrix containing the coordinates to connect the two input points
</p>


<h3>Examples</h3>

<pre><code class='language-R'>test &lt;- matrix(0, 4, 4)
test[1, 1] &lt;- 1
test[3, 4] &lt;- 1
link &lt;- interpolatePixels(1, 1, 3, 4)
test[link] &lt;- 1
</code></pre>

<hr>
<h2 id='objectDetection'>Object detection</h2><span id='topic+objectDetection'></span>

<h3>Description</h3>

<p>This function identifies objects in an image using edge detection and
labeling, gathering the coordinates and centers of the identified objects.
The edges of detected objects are then highlighted for easy recognition.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>objectDetection(img, alpha = 1, sigma = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="objectDetection_+3A_img">img</code></td>
<td>
<p>image (import by <code><a href="imager.html#topic+load.image">load.image</a></code>)</p>
</td></tr>
<tr><td><code id="objectDetection_+3A_alpha">alpha</code></td>
<td>
<p>threshold adjustment factor</p>
</td></tr>
<tr><td><code id="objectDetection_+3A_sigma">sigma</code></td>
<td>
<p>smoothing</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of 4 objects:
</p>

<ol>
<li><p> data frame of labeled region with the central coordinates
</p>
</li>
<li><p> all coordinates that are in labeled regions
</p>
</li>
<li><p> size of labeled objects
</p>
</li>
<li><p> image were object edges (purple) and detected centers (green) are colored
</p>
</li></ol>



<h3>Examples</h3>

<pre><code class='language-R'>res_objectDetection &lt;- objectDetection(beads, alpha = 1, sigma = 2)
res_objectDetection$marked_beads |&gt; plot()
</code></pre>

<hr>
<h2 id='proximityFilter'>Proximity-based exclusion</h2><span id='topic+proximityFilter'></span>

<h3>Description</h3>

<p>To detect objects within a defined range of one another, it is necessary to
calculate their centers to determine proximity. Pairs that are too close
will be discarded. (Input can be obtained by objectDetection function)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>proximityFilter(centers, coordinates, radius = "auto")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="proximityFilter_+3A_centers">centers</code></td>
<td>
<p>center coordinates of objects (mx|my|value data frame)</p>
</td></tr>
<tr><td><code id="proximityFilter_+3A_coordinates">coordinates</code></td>
<td>
<p>all coordinates of the objects (x|y|value data frame)</p>
</td></tr>
<tr><td><code id="proximityFilter_+3A_radius">radius</code></td>
<td>
<p>distance from one center in which no other centers
are allowed (in pixels)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of 3 objects:
</p>

<ol>
<li><p> center coordinates of remaining objects
</p>
</li>
<li><p> all coordinates of remaining objects
</p>
</li>
<li><p> size of remaining objects
</p>
</li></ol>



<h3>Examples</h3>

<pre><code class='language-R'>res_objectDetection &lt;- objectDetection(beads, alpha = 1, sigma = 2)
res_proximityFilter &lt;- proximityFilter(
  res_objectDetection$centers,
  res_objectDetection$coordinates,
  radius = "auto"
  )
changePixelColor(
  beads,
  res_proximityFilter$coordinates,
  color = "darkgreen",
  visualize = TRUE
  )
</code></pre>

<hr>
<h2 id='resultAnalytics'>Image Summary</h2><span id='topic+resultAnalytics'></span>

<h3>Description</h3>

<p>Extracts all important information of the remaining microbeads. This function
summarizes the data obtained by previous functions: objectDetection,
proximityFilter and sizeFilter. Provides information like amount, intensity,
size and density.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>resultAnalytics(unfiltered, coordinates, size, img, parallel = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="resultAnalytics_+3A_unfiltered">unfiltered</code></td>
<td>
<p>all coordinates from every object before applying filter functions</p>
</td></tr>
<tr><td><code id="resultAnalytics_+3A_coordinates">coordinates</code></td>
<td>
<p>all filtered coordinates of the objects (x|y|value data frame)</p>
</td></tr>
<tr><td><code id="resultAnalytics_+3A_size">size</code></td>
<td>
<p>size of the objects</p>
</td></tr>
<tr><td><code id="resultAnalytics_+3A_img">img</code></td>
<td>
<p>image (import by <code><a href="imager.html#topic+load.image">load.image</a></code>)</p>
</td></tr>
<tr><td><code id="resultAnalytics_+3A_parallel">parallel</code></td>
<td>
<p>if TRUE uses multiple cores (75 %) to process results</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of 2 objects:
</p>

<ol>
<li><p> summary of all the microbeads in the image
</p>
</li>
<li><p> detailed information about every single bead
</p>
</li></ol>



<h3>Examples</h3>

<pre><code class='language-R'>res_objectDetection &lt;- objectDetection(beads, alpha = 1, sigma = 2)
res_sizeFilter &lt;- sizeFilter(
  res_objectDetection$centers,
  res_objectDetection$coordinates,
  lowerlimit = 50, upperlimit = 150
  )
res_proximityFilter &lt;- proximityFilter(
  res_sizeFilter$centers,
  res_objectDetection$coordinates,
  radius = "auto"
  )
res_resultAnalytics &lt;- resultAnalytics(
  unfiltered = res_objectDetection$coordinates,
  coordinates = res_proximityFilter$coordinates,
  size = res_proximityFilter$size,
  img = beads
  )
plot(beads)
with(
  res_objectDetection$centers,
  points(
    res_objectDetection$centers$mx,
    res_objectDetection$centers$my,
    col = "red",
    pch = 19
    )
  )
with(
  res_resultAnalytics$detailed,
  points(
    res_resultAnalytics$detailed$x,
    res_resultAnalytics$detailed$y,
    col = "darkgreen",
    pch = 19
    )
  )
</code></pre>

<hr>
<h2 id='sizeFilter'>Size-based exclusion</h2><span id='topic+sizeFilter'></span>

<h3>Description</h3>

<p>Calculates the size of the objects in an image and discards objects based
on a lower and an upper size limit. (Input can be obtained by objectDetection function)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sizeFilter(centers, coordinates, lowerlimit = "auto", upperlimit = "auto")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sizeFilter_+3A_centers">centers</code></td>
<td>
<p>center coordinates of objects (needs to include 'value'
representing the center number)</p>
</td></tr>
<tr><td><code id="sizeFilter_+3A_coordinates">coordinates</code></td>
<td>
<p>all coordinates of the objects (x|y|value data frame)</p>
</td></tr>
<tr><td><code id="sizeFilter_+3A_lowerlimit">lowerlimit</code></td>
<td>
<p>smallest accepted object size (when 'auto' both limits are
calculated by using the mean and the standard deviation)</p>
</td></tr>
<tr><td><code id="sizeFilter_+3A_upperlimit">upperlimit</code></td>
<td>
<p>highest accepted object size</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of 3 objects:
</p>

<ol>
<li><p> remaining centers after discarding according to size
</p>
</li>
<li><p> remaining coordinates after discarding according to size
</p>
</li>
<li><p> size of remaining objects
</p>
</li></ol>



<h3>Examples</h3>

<pre><code class='language-R'>res_objectDetection &lt;- objectDetection(beads, alpha = 1, sigma = 2)
res_sizeFilter &lt;- sizeFilter(
  centers = res_objectDetection$centers,
  coordinates = res_objectDetection$coordinates,
  lowerlimit = 50, upperlimit = 150
  )
changePixelColor(
  beads,
  res_sizeFilter$coordinates,
  color = "darkgreen",
  visualize = TRUE
  )
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
