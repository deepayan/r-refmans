<!DOCTYPE html><html><head><title>Help for package biopixR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {biopixR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#adaptiveInterpolation'><p>Connects Line Ends with the nearest labeled region</p></a></li>
<li><a href='#beads'><p>Image of microbeads</p></a></li>
<li><a href='#beads_large1'><p>Image of microbeads</p></a></li>
<li><a href='#beads_large2'><p>Image of microbeads</p></a></li>
<li><a href='#changePixelColor'><p>Change the color of pixels</p></a></li>
<li><a href='#droplet_beads'><p>Image of microbeads in luminescence channel</p></a></li>
<li><a href='#droplets'><p>Droplets containing microbeads</p></a></li>
<li><a href='#edgeDetection'><p>Canny edge detector</p></a></li>
<li><a href='#fillLineGaps'><p>Reconnecting discontinuous lines</p></a></li>
<li><a href='#haralickCluster'><p>k-medoids clustering of images according to the Haralick features</p></a></li>
<li><a href='#imgPipe'><p>Image analysis pipeline</p></a></li>
<li><a href='#importImage'><p>Import an Image File</p></a></li>
<li><a href='#interactive_objectDetection'><p>Interactive object detection</p></a></li>
<li><a href='#interpolatePixels'><p>Pixel Interpolation</p></a></li>
<li><a href='#objectDetection'><p>Object detection</p></a></li>
<li><a href='#proximityFilter'><p>Proximity-based exclusion</p></a></li>
<li><a href='#resultAnalytics'><p>Result Calculation and Summary</p></a></li>
<li><a href='#scanDir'><p>Scan Directory for Image Analysis</p></a></li>
<li><a href='#shapeFeatures'><p>Extraction of Shape Features</p></a></li>
<li><a href='#sizeFilter'><p>Size-based exclusion</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Extracting Insights from Biological Images</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Combines the 'magick' and 'imager' packages to streamline image analysis, focusing on feature extraction and quantification from biological images, especially 
    microparticles. By providing high throughput pipelines and clustering capabilities, 'biopixR' facilitates efficient insight generation for researchers (Schneider J. et al. (2019) 
    &lt;<a href="https://doi.org/10.21037%2Fjlpm.2019.04.05">doi:10.21037/jlpm.2019.04.05</a>&gt;).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-3">LGPL (&ge; 3)</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>BuildVignettes:</td>
<td>true</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.2.0), imager, magick, tcltk</td>
</tr>
<tr>
<td>Imports:</td>
<td>data.table, cluster</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, doParallel, kohonen, imagerExtra, GPareto,
foreach</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/Brauckhoff/biopixR">https://github.com/Brauckhoff/biopixR</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/Brauckhoff/biopixR/issues">https://github.com/Brauckhoff/biopixR/issues</a></td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-06-24 16:45:52 UTC; brauctim</td>
</tr>
<tr>
<td>Author:</td>
<td>Tim Brauckhoff <a href="https://orcid.org/0009-0002-0142-7017"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Stefan Roediger <a href="https://orcid.org/0000-0002-1441-6512"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb],
  Coline Kieffer [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Tim Brauckhoff &lt;brauctile@disroot.org&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-06-24 17:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='adaptiveInterpolation'>Connects Line Ends with the nearest labeled region</h2><span id='topic+adaptiveInterpolation'></span>

<h3>Description</h3>

<p>The function scans an increasing radius around a line end and connects it
with the nearest labeled region.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adaptiveInterpolation(
  end_points_df,
  diagonal_edges_df,
  clean_lab_df,
  img,
  radius = 5
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adaptiveInterpolation_+3A_end_points_df">end_points_df</code></td>
<td>
<p><code>data.frame</code> with the coordinates of all line ends
(can be obtained by using <code><a href="magick.html#topic+image_morphology">image_morphology</a></code>)</p>
</td></tr>
<tr><td><code id="adaptiveInterpolation_+3A_diagonal_edges_df">diagonal_edges_df</code></td>
<td>
<p><code>data.frame</code> with coordinates of diagonal line ends
(can also be obtained by using <code><a href="magick.html#topic+image_morphology">image_morphology</a></code>)</p>
</td></tr>
<tr><td><code id="adaptiveInterpolation_+3A_clean_lab_df">clean_lab_df</code></td>
<td>
<p>data of type <code>data.frame</code>, containing the x, y and value
information of every labeled region in an image (only the edges should be
labeled)</p>
</td></tr>
<tr><td><code id="adaptiveInterpolation_+3A_img">img</code></td>
<td>
<p>image providing the dimensions of the output matrix
(import by <code><a href="#topic+importImage">importImage</a></code>)</p>
</td></tr>
<tr><td><code id="adaptiveInterpolation_+3A_radius">radius</code></td>
<td>
<p>maximal radius that should be scanned for another cluster</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is designed to be part of the
<code><a href="#topic+fillLineGaps">fillLineGaps</a></code> function, which performs the thresholding
and line end detection preprocessing. The
<code><a href="#topic+adaptiveInterpolation">adaptiveInterpolation</a></code> generates a matrix with
dimensions matching those of the original image. Initially, the matrix
contains only background values (0) corresponding to a black image. The
function then searches for line ends and identifies the nearest labeled
region within a given radius of the line end. It should be noted that the
cluster of the line end in question is not considered a nearest neighbor. In
the event that another cluster is identified, the
<code><a href="#topic+interpolatePixels">interpolatePixels</a></code> function is employed to connect the
line end to the aforementioned cluster. This entails transforming the
specified pixels of the matrix to a foreground value of (1).
It is important to highlight that diagonal line ends receive a special
treatment, as they are always treated as a separate cluster by the labeling
function. This makes it challenging to reconnect them. To address this issue,
diagonal line ends not only ignore their own cluster but also that of their
direct neighbor. Thereafter, the same procedure is repeated, with pixel
values being changed according to the
<code><a href="#topic+interpolatePixels">interpolatePixels</a></code> function.
</p>


<h3>Value</h3>

<p>Binary matrix that can be applied as an overlay, for example with
<code><a href="imager.html#topic+imager.combine">imager.combine</a></code> to fill the gaps between line ends.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Creating an artificial binary image
mat &lt;- matrix(0, 8, 8)
mat[3, 1:2] &lt;- 1
mat[4, 3] &lt;- 1
mat[7:8, 3] &lt;- 1
mat[5, 6:8] &lt;- 1
mat_cimg &lt;- as.cimg(mat)
plot(mat_cimg)

# Preprocessing / LineEnd detection / labeling (done in fillLineGaps())
mat_cimg_m &lt;- mirror(mat_cimg, axis = "x")
mat_magick &lt;- cimg2magick(mat_cimg)
lineends &lt;- image_morphology(mat_magick, "HitAndMiss", "LineEnds")
diagonalends &lt;- image_morphology(mat_magick, "HitAndMiss", "LineEnds:2&gt;")
lineends_cimg &lt;- magick2cimg(lineends)
diagonalends_cimg &lt;- magick2cimg(diagonalends)
end_points &lt;- which(lineends_cimg == TRUE, arr.ind = TRUE)
end_points_df &lt;- as.data.frame(end_points)
colnames(end_points_df) &lt;- c("x", "y", "dim3", "dim4")
diagonal_edges &lt;- which(diagonalends_cimg == TRUE, arr.ind = TRUE)
diagonal_edges_df &lt;- as.data.frame(diagonal_edges)
colnames(diagonal_edges_df) &lt;- c("x", "y", "dim3", "dim4")
lab &lt;- label(mat_cimg_m)
df_lab &lt;- as.data.frame(lab) |&gt; subset(value &gt; 0)
alt_x &lt;- list()
alt_y &lt;- list()
alt_value &lt;- list()
for (g in seq_len(nrow(df_lab))) {
  if (mat_cimg_m[df_lab$x[g], df_lab$y[g], 1, 1] == 1) {
    alt_x[g] &lt;- df_lab$x[g]
    alt_y[g] &lt;- df_lab$y[g]
    alt_value[g] &lt;- df_lab$value[g]
  }
}
clean_lab_df &lt;- data.frame(
  x = unlist(alt_x),
  y = unlist(alt_y),
  value = unlist(alt_value)
)

# Actual function
overlay &lt;- adaptiveInterpolation(
  end_points_df,
  diagonal_edges_df,
  clean_lab_df,
  mat_cimg
)
parmax(list(mat_cimg_m, as.cimg(overlay$overlay))) |&gt; plot()
</code></pre>

<hr>
<h2 id='beads'>Image of microbeads</h2><span id='topic+beads'></span>

<h3>Description</h3>

<p>This fluorescence image, formatted as 'cimg' with dimensions of
117 x 138 pixels, shows microbeads. With a single color channel, the image
provides an ideal example for in-depth analysis of microbead structures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>beads
</code></pre>


<h3>Format</h3>

<p>The image was imported using imager and is therefore of class:
&quot;cimg&quot; &quot;imager_array&quot; &quot;numeric&quot;
</p>


<h3>Details</h3>

<p>Dimensions: width - 117; height - 138; depth - 1; channel - 1
</p>


<h3>References</h3>

<p>The image was provided by Coline Kieffer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(beads)
plot(beads)
</code></pre>

<hr>
<h2 id='beads_large1'>Image of microbeads</h2><span id='topic+beads_large1'></span>

<h3>Description</h3>

<p>This fluorescence image, formatted as 'cimg' with dimensions of
492 x 376 pixels, shows microbeads. With a single color channel, the image
provides an ideal example for in-depth analysis of microbead structures.
The image's larger size encompasses a greater number of microbeads,
offering a broader range of experimental outcomes for examination.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>beads_large1
</code></pre>


<h3>Format</h3>

<p>The image was imported using imager and is therefore of class:
&quot;cimg&quot; &quot;imager_array&quot; &quot;numeric&quot;
</p>


<h3>Details</h3>

<p>Dimensions: width - 492; height - 376; depth - 1; channel - 1
</p>


<h3>References</h3>

<p>The image was provided by Coline Kieffer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(beads_large1)
plot(beads_large1)
</code></pre>

<hr>
<h2 id='beads_large2'>Image of microbeads</h2><span id='topic+beads_large2'></span>

<h3>Description</h3>

<p>This fluorescence image, formatted as 'cimg' with dimensions of
1384 x 1032 pixels, shows microbeads. With a single color channel, the image
provides an ideal example for in-depth analysis of microbead structures.
The image's larger size encompasses a greater number of microbeads,
offering a broader range of experimental outcomes for examination.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>beads_large2
</code></pre>


<h3>Format</h3>

<p>The image was imported using imager and is therefore of class:
&quot;cimg&quot; &quot;imager_array&quot; &quot;numeric&quot;
</p>


<h3>Details</h3>

<p>Dimensions: width - 1384; height - 1032; depth - 1; channel - 3
</p>


<h3>References</h3>

<p>The image was provided by Coline Kieffer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(beads_large2)
plot(beads_large2)
</code></pre>

<hr>
<h2 id='changePixelColor'>Change the color of pixels</h2><span id='topic+changePixelColor'></span>

<h3>Description</h3>

<p>The function allows the user to alter the color of a specified set of pixels
within an image. In order to achieve this, the coordinates of the pixels in
question must be provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>changePixelColor(img, coordinates, color = "purple", visualize = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="changePixelColor_+3A_img">img</code></td>
<td>
<p>image (import by <code><a href="#topic+importImage">importImage</a></code>)</p>
</td></tr>
<tr><td><code id="changePixelColor_+3A_coordinates">coordinates</code></td>
<td>
<p>specifying which pixels to be colored (should
be a x|y data frame).</p>
</td></tr>
<tr><td><code id="changePixelColor_+3A_color">color</code></td>
<td>
<p>color to be applied to specified pixels:
</p>

<ul>
<li><p> color from the list of colors defined by <code><a href="grDevices.html#topic+colors">colors</a></code>
</p>
</li>
<li><p> object of class factor
</p>
</li></ul>
</td></tr>
<tr><td><code id="changePixelColor_+3A_visualize">visualize</code></td>
<td>
<p>if TRUE the resulting image gets plotted</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class 'cimg' with changed colors at desired positions.
</p>


<h3>References</h3>

<p>https://CRAN.R-project.org/package=countcolors
</p>


<h3>Examples</h3>

<pre><code class='language-R'>coordinates &lt;-
  objectDetection(beads,
                  method = 'edge',
                  alpha = 1,
                  sigma = 0)
changePixelColor(
  beads,
  coordinates$coordinates,
  color = factor(coordinates$coordinates$value),
  visualize = TRUE
)
</code></pre>

<hr>
<h2 id='droplet_beads'>Image of microbeads in luminescence channel</h2><span id='topic+droplet_beads'></span>

<h3>Description</h3>

<p>The image shows red fluorescence rhodamine microbeads measuring
151 x 112 pixels. The fluorescence channel was used to obtain the image,
resulting in identical dimensions and positions of the beads as in the
original image (droplets).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>droplet_beads
</code></pre>


<h3>Format</h3>

<p>The image was imported using imager and is therefore of class:
&quot;cimg&quot; &quot;imager_array&quot; &quot;numeric&quot;
</p>


<h3>Details</h3>

<p>Dimensions: width - 151; height - 112; depth - 1; channel - 3
</p>


<h3>References</h3>

<p>The image was provided by Coline Kieffer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(droplet_beads)
plot(droplet_beads)
</code></pre>

<hr>
<h2 id='droplets'>Droplets containing microbeads</h2><span id='topic+droplets'></span>

<h3>Description</h3>

<p>The image displays a water-oil emulsion with droplets observed through
brightfield microscopy. It is formatted as 'cimg' and sized at 151 × 112
pixels. The droplets vary in size, and some contain microbeads, which adds
complexity. Brightfield microscopy enhances the contrast between water and
oil, revealing the droplet arrangement.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>droplets
</code></pre>


<h3>Format</h3>

<p>The image was imported using imager and is therefore of class:
&quot;cimg&quot; &quot;imager_array&quot; &quot;numeric&quot;
</p>


<h3>Details</h3>

<p>Dimensions: width - 151; height - 112; depth - 1; channel - 1
</p>


<h3>References</h3>

<p>The image was provided by Coline Kieffer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(droplets)
plot(droplets)
</code></pre>

<hr>
<h2 id='edgeDetection'>Canny edge detector</h2><span id='topic+edgeDetection'></span>

<h3>Description</h3>

<p>Adapted code from the 'imager' <code><a href="imager.html#topic+cannyEdges">cannyEdges</a></code> function
without the usage of 'dplyr' and 'purrr'. If the threshold parameters are
missing, they are determined automatically using a k-means heuristic. Use
the alpha parameter to adjust the automatic thresholds up or down. The
thresholds are returned as attributes. The edge detection is based on a
smoothed image gradient with a degree of smoothing set by the sigma
parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>edgeDetection(img, t1, t2, alpha = 1, sigma = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="edgeDetection_+3A_img">img</code></td>
<td>
<p>image (import by <code><a href="#topic+importImage">importImage</a></code>)</p>
</td></tr>
<tr><td><code id="edgeDetection_+3A_t1">t1</code></td>
<td>
<p>threshold for weak edges (if missing, both thresholds are
determined automatically)</p>
</td></tr>
<tr><td><code id="edgeDetection_+3A_t2">t2</code></td>
<td>
<p>threshold for strong edges</p>
</td></tr>
<tr><td><code id="edgeDetection_+3A_alpha">alpha</code></td>
<td>
<p>threshold adjustment factor (default 1)</p>
</td></tr>
<tr><td><code id="edgeDetection_+3A_sigma">sigma</code></td>
<td>
<p>smoothing (default 2)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class 'cimg', displaying detected edges.
</p>


<h3>References</h3>

<p>https://CRAN.R-project.org/package=imager
</p>


<h3>Examples</h3>

<pre><code class='language-R'>edgeDetection(beads, alpha = 0.5, sigma = 0.5) |&gt; plot()
</code></pre>

<hr>
<h2 id='fillLineGaps'>Reconnecting discontinuous lines</h2><span id='topic+fillLineGaps'></span>

<h3>Description</h3>

<p>The function attempts to fill in edge discontinuities in order to enable
normal labeling and edge detection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fillLineGaps(
  contours,
  objects = NULL,
  threshold = "13%",
  alpha = 1,
  sigma = 2,
  radius = 5,
  iterations = 2,
  visualize = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fillLineGaps_+3A_contours">contours</code></td>
<td>
<p>image that contains discontinuous lines like edges or
contours</p>
</td></tr>
<tr><td><code id="fillLineGaps_+3A_objects">objects</code></td>
<td>
<p>image that contains objects that should be removed before
applying the fill algorithm</p>
</td></tr>
<tr><td><code id="fillLineGaps_+3A_threshold">threshold</code></td>
<td>
<p>&quot;in %&quot; (from <code><a href="imager.html#topic+threshold">threshold</a></code>)</p>
</td></tr>
<tr><td><code id="fillLineGaps_+3A_alpha">alpha</code></td>
<td>
<p>threshold adjustment factor for edge detection
(from <code><a href="#topic+edgeDetection">edgeDetection</a></code>)</p>
</td></tr>
<tr><td><code id="fillLineGaps_+3A_sigma">sigma</code></td>
<td>
<p>smoothing (from <code><a href="#topic+edgeDetection">edgeDetection</a></code>)</p>
</td></tr>
<tr><td><code id="fillLineGaps_+3A_radius">radius</code></td>
<td>
<p>maximal radius that should be scanned for another cluster</p>
</td></tr>
<tr><td><code id="fillLineGaps_+3A_iterations">iterations</code></td>
<td>
<p>how many times the algorithm should find line ends and
reconnect them to their closest neighbor</p>
</td></tr>
<tr><td><code id="fillLineGaps_+3A_visualize">visualize</code></td>
<td>
<p>if TRUE (default) a plot is displayed highlighting the
added pixels in the original image</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function pre-processes the image in order to enable the implementation
of the <code><a href="#topic+adaptiveInterpolation">adaptiveInterpolation</a></code> function. The
pre-processing stage encompasses a number of operations, including
thresholding, the optional removal of objects, the detection of line ends
and diagonal line ends, and the labeling of pixels. The threshold should be
set to allow for the retention of some &quot;bridge&quot; pixels between gaps, thus
facilitating the subsequent process of reconnection. For further details
regarding the process of reconnection, please refer to the documentation on
<code><a href="#topic+adaptiveInterpolation">adaptiveInterpolation</a></code>. The subsequent post-processing
stage entails the reduction of line thickness in the image. With regard to
the possibility of object removal, the coordinates associated with these
objects are collected using the <code><a href="#topic+objectDetection">objectDetection</a></code>
function. Subsequently, the pixels of the detected objects are set to null
in the original image, thus allowing the algorithm to proceed without the
objects.
</p>


<h3>Value</h3>

<p>Image with continuous edges (closed gaps).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fillLineGaps(droplets)
</code></pre>

<hr>
<h2 id='haralickCluster'>k-medoids clustering of images according to the Haralick features</h2><span id='topic+haralickCluster'></span>

<h3>Description</h3>

<p>This function performs k-medoids clustering on images using Haralick
features, which describe texture. By evaluating contrast, correlation,
entropy, and homogeneity, it groups images into clusters with similar
textures. K-medoids is chosen for its outlier resilience, using actual
images as cluster centers. This approach simplifies texture-based image
analysis and classification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>haralickCluster(path)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="haralickCluster_+3A_path">path</code></td>
<td>
<p>directory path to folder with images to be analyzed</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>data.frame</code> containing file names, md5sums and cluster number.
</p>


<h3>References</h3>

<p>https://cran.r-project.org/package=radiomics
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  path2dir &lt;- system.file("images", package = 'biopixR')
  result &lt;- haralickCluster(path2dir)
  print(result)

</code></pre>

<hr>
<h2 id='imgPipe'>Image analysis pipeline</h2><span id='topic+imgPipe'></span>

<h3>Description</h3>

<p>This function serves as a pipeline that integrates tools for complete
start-to-finish image analysis. It enables the handling of images from
different channels, for example the analysis of dual-color micro particles.
This approach simplifies the workflow, providing a straightforward method to
analyze complex image data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>imgPipe(
  img1 = img,
  color1 = "color1",
  img2 = NULL,
  color2 = "color2",
  img3 = NULL,
  color3 = "color3",
  method = "edge",
  alpha = 1,
  sigma = 2,
  sizeFilter = FALSE,
  upperlimit = "auto",
  lowerlimit = "auto",
  proximityFilter = FALSE,
  radius = "auto"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="imgPipe_+3A_img1">img1</code></td>
<td>
<p>image (import by <code><a href="#topic+importImage">importImage</a></code>)</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_color1">color1</code></td>
<td>
<p>name of color in img1</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_img2">img2</code></td>
<td>
<p>image (import by <code><a href="#topic+importImage">importImage</a></code>)</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_color2">color2</code></td>
<td>
<p>name of color in img2</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_img3">img3</code></td>
<td>
<p>image (import by <code><a href="#topic+importImage">importImage</a></code>)</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_color3">color3</code></td>
<td>
<p>name of color in img3</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_method">method</code></td>
<td>
<p>choose method for object detection ('edge' / 'threshold')
(from <code><a href="#topic+objectDetection">objectDetection</a></code>)</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_alpha">alpha</code></td>
<td>
<p>threshold adjustment factor (numeric / 'static' / 'interactive' / 'gaussian')
(from <code><a href="#topic+objectDetection">objectDetection</a></code>)</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_sigma">sigma</code></td>
<td>
<p>smoothing (numeric / 'static' / 'interactive' / 'gaussian')
(from <code><a href="#topic+objectDetection">objectDetection</a></code>)</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_sizefilter">sizeFilter</code></td>
<td>
<p>applying <code><a href="#topic+sizeFilter">sizeFilter</a></code> function (default - FALSE)</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_upperlimit">upperlimit</code></td>
<td>
<p>highest accepted object size (numeric / 'auto')
(only needed if sizeFilter = TRUE)</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_lowerlimit">lowerlimit</code></td>
<td>
<p>smallest accepted object size (numeric / 'auto')
(only needed if sizeFilter = TRUE)</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_proximityfilter">proximityFilter</code></td>
<td>
<p>applying <code><a href="#topic+proximityFilter">proximityFilter</a></code> function (default - FALSE)</p>
</td></tr>
<tr><td><code id="imgPipe_+3A_radius">radius</code></td>
<td>
<p>distance from one object in which no other centers
are allowed (in pixels) (only needed if proximityFilter = TRUE)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of 2 to 3 objects:
</p>

<ul>
<li><p> Summary of all the objects in the image.
</p>
</li>
<li><p> Detailed information about every single object.
</p>
</li>
<li><p> (optional) Result for every individual color.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+objectDetection">objectDetection()</a></code>, <code><a href="#topic+sizeFilter">sizeFilter()</a></code>, <code><a href="#topic+proximityFilter">proximityFilter()</a></code>, <code><a href="#topic+resultAnalytics">resultAnalytics()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>result &lt;- imgPipe(
  beads,
  alpha = 1,
  sigma = 2,
  sizeFilter = TRUE,
  upperlimit = 150,
  lowerlimit = 50
  )

# Highlight remaining microparticles
plot(beads)
with(
  result$detailed,
  points(
    result$detailed$x,
    result$detailed$y,
    col = "darkgreen",
    pch = 19
    )
  )
</code></pre>

<hr>
<h2 id='importImage'>Import an Image File</h2><span id='topic+importImage'></span>

<h3>Description</h3>

<p>This function is a wrapper to the <code><a href="imager.html#topic+load.image">load.image</a></code> and
<code><a href="magick.html#topic+image_read">image_read</a></code> functions, and imports an image file and
returns the image as a 'cimg' object. The following file formats are
supported: TIFF, PNG, JPG/JPEG, and BMP. In the event that the image in
question contains an alpha channel, that channel is omitted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>importImage(path2file)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="importImage_+3A_path2file">path2file</code></td>
<td>
<p>path to file</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An image of class 'cimg'.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>path2img &lt;- system.file("images/beads_large1.bmp", package = 'biopixR')
img &lt;- importImage(path2img)
img |&gt; plot()

path2img &lt;- system.file("images/beads_large2.png", package = 'biopixR')
img &lt;- importImage(path2img)
img |&gt; plot()
</code></pre>

<hr>
<h2 id='interactive_objectDetection'>Interactive object detection</h2><span id='topic+interactive_objectDetection'></span>

<h3>Description</h3>

<p>This function uses the <code><a href="#topic+objectDetection">objectDetection</a></code> function to
visualize the detected objects at varying input parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>interactive_objectDetection(img, resolution = 0.1, return_param = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="interactive_objectDetection_+3A_img">img</code></td>
<td>
<p>image (import by <code><a href="#topic+importImage">importImage</a></code>)</p>
</td></tr>
<tr><td><code id="interactive_objectDetection_+3A_resolution">resolution</code></td>
<td>
<p>resolution of slider</p>
</td></tr>
<tr><td><code id="interactive_objectDetection_+3A_return_param">return_param</code></td>
<td>
<p>if TRUE the final parameter values for alpha and
sigma are printed to the console (TRUE | FALSE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function provides a graphical user interface (GUI) that allows users to
interactively adjust the parameters for object detection:
</p>

<ul>
<li> <p><strong>Alpha:</strong> Controls the threshold adjustment factor for edge detection.
</p>
</li>
<li> <p><strong>Sigma:</strong> Determines the amount of smoothing applied to the image.
</p>
</li>
<li> <p><strong>Scale:</strong> Adjusts the scale of the displayed image.
</p>
</li></ul>

<p>The GUI also includes a button to switch between two detection methods:
</p>

<ul>
<li> <p><strong>Edge Detection:</strong> Utilizes the <code><a href="#topic+edgeDetection">edgeDetection</a></code> function. The alpha parameter acts as a threshold adjustment factor, and sigma controls the smoothing.
</p>
</li>
<li> <p><strong>Threshold Detection:</strong> Applies a thresholding method, utilizing <code><a href="imagerExtra.html#topic+SPE">SPE</a></code> for background reduction and the <code><a href="imager.html#topic+threshold">threshold</a></code> function. (No dependency on alpha or sigma!)
</p>
</li></ul>



<h3>Value</h3>

<p>Values of alpha, sigma and the applied method.
</p>


<h3>References</h3>

<p>https://CRAN.R-project.org/package=magickGUI
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (interactive()) {
  interactive_objectDetection(beads)
  }

</code></pre>

<hr>
<h2 id='interpolatePixels'>Pixel Interpolation</h2><span id='topic+interpolatePixels'></span>

<h3>Description</h3>

<p>Connects two points in a matrix, array, or an image.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>interpolatePixels(row1, col1, row2, col2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="interpolatePixels_+3A_row1">row1</code></td>
<td>
<p>row index for the first point</p>
</td></tr>
<tr><td><code id="interpolatePixels_+3A_col1">col1</code></td>
<td>
<p>column index for the first point</p>
</td></tr>
<tr><td><code id="interpolatePixels_+3A_row2">row2</code></td>
<td>
<p>row index for the second point</p>
</td></tr>
<tr><td><code id="interpolatePixels_+3A_col2">col2</code></td>
<td>
<p>column index for the second point</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Matrix containing the coordinates to connect the two input points.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simulate two points in a matrix
test &lt;- matrix(0, 4, 4)
test[1, 1] &lt;- 1
test[3, 4] &lt;- 1
as.cimg(test) |&gt; plot()

# Connect them with each other
link &lt;- interpolatePixels(1, 1, 3, 4)
test[link] &lt;- 1
as.cimg(test) |&gt; plot()
</code></pre>

<hr>
<h2 id='objectDetection'>Object detection</h2><span id='topic+objectDetection'></span>

<h3>Description</h3>

<p>This function identifies objects in an image using either edge detection or
thresholding methods. It gathers the coordinates and centers of the
identified objects, highlighting the edges or overall coordinates for easy
recognition.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>objectDetection(img, method = "edge", alpha = 1, sigma = 2, vis = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="objectDetection_+3A_img">img</code></td>
<td>
<p>image (import by <code><a href="#topic+importImage">importImage</a></code>)</p>
</td></tr>
<tr><td><code id="objectDetection_+3A_method">method</code></td>
<td>
<p>choose method for object detection ('edge' / 'threshold')</p>
</td></tr>
<tr><td><code id="objectDetection_+3A_alpha">alpha</code></td>
<td>
<p>threshold adjustment factor (numeric / 'static' / 'interactive' / 'gaussian') (only needed for 'edge')</p>
</td></tr>
<tr><td><code id="objectDetection_+3A_sigma">sigma</code></td>
<td>
<p>smoothing (numeric / 'static' / 'interactive' / 'gaussian') (only needed for 'edge')</p>
</td></tr>
<tr><td><code id="objectDetection_+3A_vis">vis</code></td>
<td>
<p>creates image were object edges/coordinates (purple) and detected centers (green) are highlighted (TRUE | FALSE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code><a href="#topic+objectDetection">objectDetection</a></code> function provides several methods
for calculating the alpha and sigma parameters, which are critical for edge
detection:
</p>

<ol>
<li> <p><strong>Input of a Numeric Value:</strong>
</p>

<ul>
<li><p> Users can directly input numeric values for alpha and sigma, allowing for precise control over the edge detection parameters.
</p>
</li></ul>

</li>
<li> <p><strong>Static Scanning:</strong>
</p>

<ul>
<li><p> When both alpha and sigma are set to &quot;static&quot;, the function systematically tests all possible combinations of these parameters within the range (alpha: 0.1 - 1.5, sigma: 0 - 2). This exhaustive search helps identify the optimal parameter values for the given image. (Note: takes a lot of time)
</p>
</li></ul>

</li>
<li> <p><strong>Interactive Selection:</strong>
</p>

<ul>
<li><p> Setting the alpha and sigma values to &quot;interactive&quot; initiates a Tcl/Tk graphical user interface (GUI). This interface allows users to adjust the parameters interactively, based on visual feedback. To achieve optimal results, the user must input the necessary adjustments to align the parameters with the specific requirements of the image. The user can also switch between the methods through the interface.
</p>
</li></ul>

</li>
<li> <p><strong>Multi-Objective Optimization:</strong>
</p>

<ul>
<li><p> For advanced parameter optimization, the function <code><a href="GPareto.html#topic+easyGParetoptim">easyGParetoptim</a></code> will be utilized for multi-objective optimization using Gaussian process models. This method leverages the 'GPareto' package to perform the optimization. It involves building Gaussian Process models for each objective and running the optimization to find the best parameter values.
</p>
</li></ul>

</li></ol>



<h3>Value</h3>

<p>list of 3 objects:
</p>

<ul>
<li> <p><code>data.frame</code> of labeled regions with the central coordinates (including size information).
</p>
</li>
<li><p> All coordinates that are in labeled regions.
</p>
</li>
<li><p> Image where object edges/coordinates (purple) and detected centers (green) are colored.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>res_objectDetection &lt;- objectDetection(beads,
                                       method = 'edge',
                                       alpha = 1,
                                       sigma = 0)
res_objectDetection$marked_objects |&gt; plot()

res_objectDetection &lt;- objectDetection(beads,
                                       method = 'threshold')
res_objectDetection$marked_objects |&gt; plot()
</code></pre>

<hr>
<h2 id='proximityFilter'>Proximity-based exclusion</h2><span id='topic+proximityFilter'></span>

<h3>Description</h3>

<p>In order to identify objects within a specified proximity, it is essential to
calculate their respective centers, which serve to determine their proximity.
Pairs that are in close proximity will be discarded.
(Input can be obtained by <code><a href="#topic+objectDetection">objectDetection</a></code> function)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>proximityFilter(centers, coordinates, radius = "auto", elongation = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="proximityFilter_+3A_centers">centers</code></td>
<td>
<p>center coordinates of objects (mx|my|value data frame)</p>
</td></tr>
<tr><td><code id="proximityFilter_+3A_coordinates">coordinates</code></td>
<td>
<p>all coordinates of the objects (x|y|value data frame)</p>
</td></tr>
<tr><td><code id="proximityFilter_+3A_radius">radius</code></td>
<td>
<p>distance from one center in which no other centers
are allowed (in pixels) (numeric / 'auto')</p>
</td></tr>
<tr><td><code id="proximityFilter_+3A_elongation">elongation</code></td>
<td>
<p>factor by which the radius should be multiplied to create
the area of exclusion (default 2)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The automated radius calculation in the <code><a href="#topic+proximityFilter">proximityFilter</a></code>
function is based on the presumption of circular-shaped objects. The radius
is calculated using the following formula:
</p>
<p style="text-align: center;"><code class="reqn">\sqrt{\frac{A}{\pi}}</code>
</p>

<p>where A is the area of the detected objects. The function will exclude
objects that are too close by extending the calculated radius by one radius
length beyond the assumed circle, effectively doubling the radius to create
an exclusion zone. Therefore the elongation factor is set to 2 by default,
with one radius covering the object and an additional radius creating the
area of exclusion.
</p>


<h3>Value</h3>

<p>list of 2 objects:
</p>

<ul>
<li><p> Center coordinates of remaining objects.
</p>
</li>
<li><p> All coordinates of remaining objects.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>res_objectDetection &lt;- objectDetection(beads,
                                       alpha = 1,
                                       sigma = 0)
res_proximityFilter &lt;- proximityFilter(
  res_objectDetection$centers,
  res_objectDetection$coordinates,
  radius = "auto"
  )
changePixelColor(
  beads,
  res_proximityFilter$coordinates,
  color = "darkgreen",
  visualize = TRUE
  )
</code></pre>

<hr>
<h2 id='resultAnalytics'>Result Calculation and Summary</h2><span id='topic+resultAnalytics'></span>

<h3>Description</h3>

<p>This function summarizes the data obtained by previous functions:
<code><a href="#topic+objectDetection">objectDetection</a></code>, <code><a href="#topic+proximityFilter">proximityFilter</a></code>
or <code><a href="#topic+sizeFilter">sizeFilter</a></code>. Extracts information like amount,
intensity, size and density of the objects present in the image.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>resultAnalytics(img, coordinates, unfiltered = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="resultAnalytics_+3A_img">img</code></td>
<td>
<p>image (import by <code><a href="#topic+importImage">importImage</a></code>)</p>
</td></tr>
<tr><td><code id="resultAnalytics_+3A_coordinates">coordinates</code></td>
<td>
<p>all filtered coordinates of the objects (x|y|value data frame)</p>
</td></tr>
<tr><td><code id="resultAnalytics_+3A_unfiltered">unfiltered</code></td>
<td>
<p>all coordinates from every object before applying filter functions</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code><a href="#topic+resultAnalytics">resultAnalytics</a></code> function provides comprehensive
summary of objects detected in an image:
</p>

<ol>
<li> <p><strong>Summary</strong>
</p>

<ul>
<li><p> Generates a summary of all detected objects, including the total number of objects, their mean size, size standard deviation, mean intensity, intensity standard deviation, estimated rejected objects, and coverage.
</p>
</li></ul>

</li>
<li> <p><strong>Detailed Object Information</strong>
</p>

<ul>
<li><p> Provides detailed information for each object, including size, mean intensity, intensity standard deviation, and coordinates.
</p>
</li></ul>

</li></ol>



<h3>Value</h3>

<p>list of 2 objects:
</p>

<ul>
<li> <p><code>summary</code>: A summary of all the objects in the image.
</p>
</li>
<li> <p><code>detailed</code>: Detailed information about every single object.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+objectDetection">objectDetection()</a></code>, <code><a href="#topic+sizeFilter">sizeFilter()</a></code>, <code><a href="#topic+proximityFilter">proximityFilter()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>res_objectDetection &lt;- objectDetection(beads,
                                       alpha = 1,
                                       sigma = 0)
res_sizeFilter &lt;- sizeFilter(
  res_objectDetection$centers,
  res_objectDetection$coordinates,
  lowerlimit = 50, upperlimit = 150
  )
res_proximityFilter &lt;- proximityFilter(
  res_sizeFilter$centers,
  res_objectDetection$coordinates,
  radius = "auto"
  )
res_resultAnalytics &lt;- resultAnalytics(
  coordinates = res_proximityFilter$coordinates,
  unfiltered = res_objectDetection$coordinates,
  img = beads
  )
print(res_resultAnalytics$summary)
plot(beads)
with(
  res_objectDetection$centers,
  points(
    res_objectDetection$centers$mx,
    res_objectDetection$centers$my,
    col = "red",
    pch = 19
    )
  )
with(
  res_resultAnalytics$detailed,
  points(
    res_resultAnalytics$detailed$x,
    res_resultAnalytics$detailed$y,
    col = "darkgreen",
    pch = 19
    )
  )
</code></pre>

<hr>
<h2 id='scanDir'>Scan Directory for Image Analysis</h2><span id='topic+scanDir'></span>

<h3>Description</h3>

<p>This function scans a specified directory, imports images, and performs various analyses
including object detection, size filtering, and proximity filtering. Optionally, it can
perform these tasks in parallel and log the process.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scanDir(
  path,
  parallel = FALSE,
  backend = "PSOCK",
  cores = "auto",
  method = "edge",
  alpha = 1,
  sigma = 2,
  sizeFilter = FALSE,
  upperlimit = "auto",
  lowerlimit = "auto",
  proximityFilter = FALSE,
  radius = "auto",
  Rlog = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scanDir_+3A_path">path</code></td>
<td>
<p>directory path to folder with images to be analyzed</p>
</td></tr>
<tr><td><code id="scanDir_+3A_parallel">parallel</code></td>
<td>
<p>processing multiple images at the same time (default - FALSE)</p>
</td></tr>
<tr><td><code id="scanDir_+3A_backend">backend</code></td>
<td>
<p>'PSOCK' or 'FORK' (see <code><a href="parallel.html#topic+makeCluster">makeCluster</a></code>)</p>
</td></tr>
<tr><td><code id="scanDir_+3A_cores">cores</code></td>
<td>
<p>number of cores for parallel processing (numeric / 'auto') ('auto' uses 75% of the available cores)</p>
</td></tr>
<tr><td><code id="scanDir_+3A_method">method</code></td>
<td>
<p>choose method for object detection ('edge' / 'threshold')
(from <code><a href="#topic+objectDetection">objectDetection</a></code>)</p>
</td></tr>
<tr><td><code id="scanDir_+3A_alpha">alpha</code></td>
<td>
<p>threshold adjustment factor (numeric / 'static' / 'interactive' / 'gaussian')
(from <code><a href="#topic+objectDetection">objectDetection</a></code>)</p>
</td></tr>
<tr><td><code id="scanDir_+3A_sigma">sigma</code></td>
<td>
<p>smoothing (numeric / 'static' / 'interactive' / 'gaussian')
(from <code><a href="#topic+objectDetection">objectDetection</a></code>)</p>
</td></tr>
<tr><td><code id="scanDir_+3A_sizefilter">sizeFilter</code></td>
<td>
<p>applying <code><a href="#topic+sizeFilter">sizeFilter</a></code> function (default - FALSE)</p>
</td></tr>
<tr><td><code id="scanDir_+3A_upperlimit">upperlimit</code></td>
<td>
<p>highest accepted object size (only needed if sizeFilter = TRUE)</p>
</td></tr>
<tr><td><code id="scanDir_+3A_lowerlimit">lowerlimit</code></td>
<td>
<p>smallest accepted object size (numeric / 'auto')</p>
</td></tr>
<tr><td><code id="scanDir_+3A_proximityfilter">proximityFilter</code></td>
<td>
<p>applying <code><a href="#topic+proximityFilter">proximityFilter</a></code> function (default - FALSE)</p>
</td></tr>
<tr><td><code id="scanDir_+3A_radius">radius</code></td>
<td>
<p>distance from one center in which no other centers
are allowed (in pixels) (only needed if proximityFilter = TRUE)</p>
</td></tr>
<tr><td><code id="scanDir_+3A_rlog">Rlog</code></td>
<td>
<p>creates a log markdown document, summarizing the results (default - FALSE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function scans a specified directory for image files, imports them,
and performs analysis using designated methods. The function is capable of
parallel processing, utilizing multiple cores to accelerate computation.
Additionally, it is able to log the results into an R Markdown file.
Duplicate images are identified through the use of MD5 sums. In addition a
variety of filtering options are available to refine the analysis. If
logging is enabled, the results can be saved and rendered into a report.
When <code>Rlog = TRUE</code>, an R Markdown file and a CSV file are generated in the
current directory. More detailed information on individual results,
can be accessed through saved RDS files.
</p>


<h3>Value</h3>

<p><code>data.frame</code> summarizing each analyzed image, including details such as the number of objects, average size and intensity, estimated rejections, and coverage.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+imgPipe">imgPipe()</a></code>, <code><a href="#topic+objectDetection">objectDetection()</a></code>, <code><a href="#topic+sizeFilter">sizeFilter()</a></code>, <code><a href="#topic+proximityFilter">proximityFilter()</a></code>, <code><a href="#topic+resultAnalytics">resultAnalytics()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (interactive()) {
  path2dir &lt;- system.file("images", package = 'biopixR')
  results &lt;- scanDir(path2dir, alpha = 'interactive', sigma = 'interactive')
  print(results)
  }

</code></pre>

<hr>
<h2 id='shapeFeatures'>Extraction of Shape Features</h2><span id='topic+shapeFeatures'></span>

<h3>Description</h3>

<p>This function analyzes the objects detected in an image and calculates
distinct shape characteristics for each object, such as circularity,
eccentricity, radius, and perimeter. The resulting shape attributes can then
be grouped using a Self-Organizing Map (SOM) from the 'Kohonen' package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>shapeFeatures(
  img,
  alpha = 1,
  sigma = 2,
  xdim = 2,
  ydim = 1,
  SOM = FALSE,
  visualize = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shapeFeatures_+3A_img">img</code></td>
<td>
<p>image (import by <code><a href="imager.html#topic+load.image">load.image</a></code>)</p>
</td></tr>
<tr><td><code id="shapeFeatures_+3A_alpha">alpha</code></td>
<td>
<p>threshold adjustment factor (numeric / 'static' / 'interactive' / 'gaussian')
(from <code><a href="#topic+objectDetection">objectDetection</a></code>)</p>
</td></tr>
<tr><td><code id="shapeFeatures_+3A_sigma">sigma</code></td>
<td>
<p>smoothing (numeric / 'static' / 'interactive' / 'gaussian')
(from <code><a href="#topic+objectDetection">objectDetection</a></code>)</p>
</td></tr>
<tr><td><code id="shapeFeatures_+3A_xdim">xdim</code></td>
<td>
<p>x-dimension for the SOM-grid (grid = hexagonal)</p>
</td></tr>
<tr><td><code id="shapeFeatures_+3A_ydim">ydim</code></td>
<td>
<p>y-dimension for the SOM-grid (xdim * ydim = number of neurons)</p>
</td></tr>
<tr><td><code id="shapeFeatures_+3A_som">SOM</code></td>
<td>
<p>if TRUE runs SOM algorithm on extracted shape features, grouping
the detected objects</p>
</td></tr>
<tr><td><code id="shapeFeatures_+3A_visualize">visualize</code></td>
<td>
<p>visualizes the groups computed by SOM</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>data.frame</code> containing detailed information about every single object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+objectDetection">objectDetection()</a></code>, <code><a href="#topic+resultAnalytics">resultAnalytics()</a></code>, <code><a href="kohonen.html#topic+som">som</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>shapeFeatures(
  beads,
  alpha = 1,
  sigma = 0,
  SOM = TRUE,
  visualize = TRUE
)
</code></pre>

<hr>
<h2 id='sizeFilter'>Size-based exclusion</h2><span id='topic+sizeFilter'></span>

<h3>Description</h3>

<p>Takes the size of the objects in an image and discards objects based
on a lower and an upper size limit.
(Input can be obtained by <code><a href="#topic+objectDetection">objectDetection</a></code> function)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sizeFilter(centers, coordinates, lowerlimit = "auto", upperlimit = "auto")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sizeFilter_+3A_centers">centers</code></td>
<td>
<p>center coordinates of objects (value|mx|my|size data frame)</p>
</td></tr>
<tr><td><code id="sizeFilter_+3A_coordinates">coordinates</code></td>
<td>
<p>all coordinates of the objects (x|y|value data frame)</p>
</td></tr>
<tr><td><code id="sizeFilter_+3A_lowerlimit">lowerlimit</code></td>
<td>
<p>smallest accepted object size (numeric / 'auto' / 'interactive')</p>
</td></tr>
<tr><td><code id="sizeFilter_+3A_upperlimit">upperlimit</code></td>
<td>
<p>highest accepted object size (numeric / 'auto' / 'interactive')</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code><a href="#topic+sizeFilter">sizeFilter</a></code> function is designed to filter
detected objects based on their size, either through automated detection or
user-defined limits. The automated detection of size limits uses the 1.5*IQR
method to identify and remove outliers. This approach is most effective when
dealing with a large number of objects, (typically more than 50), and when
the sizes of the objects are relatively uniform. For smaller samples or when
the sizes of the objects vary significantly, the automated detection may not
be as accurate, and manual limit setting is recommended.
</p>


<h3>Value</h3>

<p>list of 2 objects:
</p>

<ul>
<li><p> Remaining centers after discarding according to size.
</p>
</li>
<li><p> Remaining coordinates after discarding according to size.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>res_objectDetection &lt;- objectDetection(
  beads,
  method = 'edge',
  alpha = 1,
  sigma = 0
  )
res_sizeFilter &lt;- sizeFilter(
  centers = res_objectDetection$centers,
  coordinates = res_objectDetection$coordinates,
  lowerlimit = 50, upperlimit = 150
  )
changePixelColor(
  beads,
  res_sizeFilter$coordinates,
  color = "darkgreen",
  visualize = TRUE
  )
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
