<!DOCTYPE html><html lang="en"><head><title>Help for package MuViCP</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {MuViCP}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#MuViCP-package'><p>MuViCP</p></a></li>
<li><a href='#ab.dist'><p>Distance Functions for nearest neighbours</p></a></li>
<li><a href='#basis_random'><p>Generate a random basis</p></a></li>
<li><a href='#bel.builder'><p>Building Belief Functions</p></a></li>
<li><a href='#bpa'><p>Basic Probability Assignment Objects</p></a></li>
<li><a href='#bpamat'><p>Matrices of Basic Probability Assignment Objects</p></a></li>
<li><a href='#cancer'><p>Wisconsin Breast Cancer Data from UCI website</p></a></li>
<li><a href='#combine.ds'><p>Combining Basic Probability Assignments</p></a></li>
<li><a href='#ensemble'><p>Ensemble Objects for Classification</p></a></li>
<li><a href='#get.NN'><p>Function to find the nearest neighbours</p></a></li>
<li><a href='#least.k'><p>Functions to find the few smallest elements in a vector.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>MultiClass Visualizable Classification using Combination of
Projections</td>
</tr>
<tr>
<td>Version:</td>
<td>1.3.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2016-02-22</td>
</tr>
<tr>
<td>Author:</td>
<td>Mohit Dayal</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Mohit Dayal &lt;mohitdayal2000@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>An ensemble classifier for multiclass classification. This is a novel classifier that natively works as an ensemble. It projects data on a large number of matrices, and uses very simple classifiers on each of these projections. The results are then combined, ideally via Dempster-Shafer Calculus.</td>
</tr>
<tr>
<td>Imports:</td>
<td>sm, MASS, gtools, parallel</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2016-02-22 18:07:24 UTC; mohit</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2016-02-22 22:43:49</td>
</tr>
</table>
<hr>
<h2 id='MuViCP-package'>MuViCP</h2><span id='topic+MuViCP-package'></span><span id='topic+MuViCP'></span>

<h3>Description</h3>

<p>MultiClass Visualizable Classification using Combination of Projections</p>


<h3>Details</h3>

<p>This is a novel classifier that natively works as an ensemble. It projects data on a large number of matrices, and uses very simple classifiers on each of these projections. The results are then combined, ideally via Dempster-Shafer Calculus.</p>


<h3>Author(s)</h3>

<p>Mohit Dayal</p>

<hr>
<h2 id='ab.dist'>Distance Functions for nearest neighbours</h2><span id='topic+ab.dist'></span><span id='topic+ab.dist.matY'></span><span id='topic+eu.dist'></span><span id='topic+eu.dist.matY'></span><span id='topic+mh.dist'></span><span id='topic+mh.dist.matY'></span>

<h3>Description</h3>

<p>These functions compute Absolute (<code>ab.dist</code>), Euclidean
(<code>eu.dist</code>) or Mahalanobis (<code>mh.dist</code>) distances between two
points. The variant functions (<code>*.matY</code>), accomplish the same task,
but between a point on the one hand, and every point specified as rows of a matrix on the other.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ab.dist(x, y)
eu.dist(x, y)
mh.dist(x, y, A)
ab.dist.matY(x, Y)
eu.dist.matY(x, Y)
mh.dist.matY(x, Y, A)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ab.dist_+3A_x">x</code></td>
<td>
<p>The vector (point) from which distance is sought.</p>
</td></tr>
<tr><td><code id="ab.dist_+3A_y">y</code></td>
<td>
<p>The vector (point) to which distance is sought.</p>
</td></tr>
<tr><td><code id="ab.dist_+3A_y">Y</code></td>
<td>
<p>A set of points, specified as rows of a matrix, to which distances are sought.</p>
</td></tr>
<tr><td><code id="ab.dist_+3A_a">A</code></td>
<td>
<p>The inverse matrix to use for the Mahalanobis distance.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions are used internally to decide how the nearest
neighbours shall be calculated; the user need not call any of these
functions directly. Rather, the choice of distance is specified as a
string ('euclidean' or 'absolute' or 'mahal').
</p>


<h3>Value</h3>

<p>Either a single number for the distance, or a vector of distances,
corresponding to each row of <code>Y</code>.
</p>


<h3>Author(s)</h3>

<p>Mohit Dayal</p>


<h3>See Also</h3>

<p>get.NN</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c(1,2)
y &lt;- c(0,3)
mu &lt;- c(1,3)
Sigma &lt;- rbind(c(1,0.2),c(0.2,1))
Y &lt;- MASS::mvrnorm(20, mu = mu, Sigma = Sigma)
ab.dist(x,y)
eu.dist(x,y)
mh.dist(x,y,Sigma)
ab.dist.matY(x,Y)
eu.dist.matY(x,Y)
mh.dist.matY(x,Y,Sigma)
</code></pre>

<hr>
<h2 id='basis_random'>Generate a random basis</h2><span id='topic+basis_random'></span>

<h3>Description</h3>

<p>Generate a random basis
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  basis_random(n, d = 2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="basis_random_+3A_n">n</code></td>
<td>
<p>dimensionality of data</p>
</td></tr>
<tr><td><code id="basis_random_+3A_d">d</code></td>
<td>
<p>dimensionality of target projection</p>
</td></tr>
</table>

<hr>
<h2 id='bel.builder'>Building Belief Functions</h2><span id='topic+kde_bel.builder'></span><span id='topic+knn_bel.builder'></span><span id='topic+jit_bel.builder'></span>

<h3>Description</h3>

<p>These are a set of functions that can be used to build belief
functions (hence the name <code>*.builder</code>). Each of these returns a
function that can be used to classify points in two dimensions.
</p>
<p>The algorithm used can be judged from the first three letters. Thus
the <code>kde_bel</code> function uses the kernel density estimate (kde), the
<code>knn_bel</code> function uses the kernel density estimate together with
information on the Nearest Neighbours, the <code>jit_bel</code> function
uses jittering of the point in the neighbourhood. Finally, the
<code>cor_bel</code> function uses the kde but includes a factor for
self-correction.
</p>
<p>These generated functions (return values) are meant to be passed to
the <code>ensemble</code> function to build an ensemble.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kde_bel.builder(labs, test, train, options = list(coef = 0.90))
knn_bel.builder(labs, test, train, options = list(k = 3, p = FALSE,
dist.type = c('euclidean', 'absolute', 'mahal'), out = c('var', 'cv'),
coef = 0.90))
jit_bel.builder(labs, test, train, options = list(k = 3, p = FALSE, s =
5, dist.type = c('euclidean', 'absolute', 'mahal'), out = c('var',
'cv'), coef = 0.90))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bel.builder_+3A_labs">labs</code></td>
<td>
<p>The possible labels for the points. Can be strings. Must
be of the same length as <code>train</code></p>
</td></tr>
<tr><td><code id="bel.builder_+3A_test">test</code></td>
<td>
<p>The indices of the test data in <code>P</code></p>
</td></tr>
<tr><td><code id="bel.builder_+3A_train">train</code></td>
<td>
<p>The indices of the training data in <code>P</code></p>
</td></tr>
<tr><td><code id="bel.builder_+3A_options">options</code></td>
<td>
<p>A list of arguments that determine the behaviour of the
constructed belief function.</p>
</td></tr>
<tr><td><code id="bel.builder_+3A_k">k</code></td>
<td>
<p>The number of nearest neighbours to consider, specified as a
definite integer</p>
</td></tr>
<tr><td><code id="bel.builder_+3A_p">p</code></td>
<td>
<p>The number of nearest neighbours to consider, specified as a
fraction of the test set</p>
</td></tr>
<tr><td><code id="bel.builder_+3A_s">s</code></td>
<td>
<p>For the jitter belief function : how many times should each
point be jittered in the neighbourhood? Usually, 2 or 3 works.</p>
</td></tr>
<tr><td><code id="bel.builder_+3A_dist.type">dist.type</code></td>
<td>
<p>The type of distance to use when computing nearest
neighbours. Can be one of &quot;euclidean&quot;, &quot;absolute&quot;, or &quot;mahal&quot;</p>
</td></tr>
<tr><td><code id="bel.builder_+3A_out">out</code></td>
<td>
<p>Should beliefs be built from the variance (<code>var</code>) or
the coefficient of variation(<code>cv</code>)? Also see the Details section below.</p>
</td></tr>
<tr><td><code id="bel.builder_+3A_coef">coef</code></td>
<td>
<p>The classifier only assigns the class labels that actually
occur, that is, ignorance is, by default not accounted for. This
argument specifies what amount of belief should be allocated to
ignorance; the beliefs to the other classes are correspondingly
adjusted. Note that for the 'corrected' classifier, the actual belief
assigned to ignorance may be higher than this for some
projections. See Details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each of these functions uses a different algorithm for classification.
</p>
<p>The <code>kde_bel.builder</code> returns a classifier that simply evaluates
the kernel density estimate of each class on each point, and
classifies that point to that class which has the maximum density on
it.
</p>
<p>The <code>knn_bel.builder</code> returns a classifier that tries to locate
<code>k</code> (or <code>p*length(train)</code>) nearest neighbours of each of the
points in the test set. It then evaluates the kernel density estimate
of each class in the training set on each of these nearest neighbours,
and at each of the testing points. With argument <code>var</code>, the
variance of the set of density values, centered at the density value
at the testing point itself, is taken as a measure of that point
belonging to this class. With argument <code>cv</code>, the coefficient of
variation is used instead, and for the mean, one uses the density
value on the point itself. Generally, the <code>var</code> classifier has
higher accuracy.
</p>
<p>The <code>jit_bel.builder</code> works very similar to the
<code>knn_bel.builder</code> classifier, but instead uses the nearest
neighbour information to determine a point &quot;neighbourhood&quot;. The test
points are then jittered in this neighbourhood, and on these fake
points the kernel density is evaluated. The <code>var</code> and <code>cv</code>
work here as they work in the <code>knn_bel.builder</code> classifier.  
</p>


<h3>Value</h3>

<p>A Classifier function that can be passed on to the
<code>ensemble</code> function.
</p>
<p>Alternately, 2-D projected data may directly be passed to the
classifier function returned, in which case, a matrix of dimensions
(Number of Classes) x (length(test)) is returned. Each column sums to
1, and represents the partial assignment of that point to each of the
classes. The rows are named after the class names, while the columns
are named after the test points. Ignorance is represented by the
special symbol 'Inf' and is the last class in the matrix.</p>


<h3>Author(s)</h3>

<p>Mohit Dayal</p>


<h3>Examples</h3>

<pre><code class='language-R'>##Setting Up
data(cancer)
table(cancer$V2)
colnames(cancer)[1:2] &lt;- c('id', 'type')
cancer.d &lt;- as.matrix(cancer[,3:32])
labs &lt;- cancer$type
test_size &lt;- floor(0.15*nrow(cancer.d))
train &lt;- sample(1:nrow(cancer.d), size = nrow(cancer.d) - test_size)
test &lt;- which(!(1:569 %in% train))
truelabs = labs[test]

projectron &lt;- function(A) cancer.d %*% A

seed &lt;- .Random.seed
F &lt;- projectron(basis_random(30))

##Simple Density Classification
kdebel &lt;- kde_bel.builder(labs = labs[train], test = test, train = train)
x1 &lt;- kdebel(F)
predicted1 &lt;- apply(x1, MARGIN = 2, FUN = function(x) names(which.max(x)))
table(truelabs, predicted1)

##Density Classification Using Nearest Neighbor Information
knnbel &lt;- knn_bel.builder(labs = labs[train], test = test, train =
train, options = list(k = 3, p = FALSE, dist.type = 'euclidean', out = 'var', coef
= 0.90))
x2 &lt;- knnbel(F)
predicted2 &lt;- apply(x2, MARGIN = 2, FUN = function(x) names(which.max(x)))
table(truelabs, predicted2)

##Same as above but now using the Coefficient of Variation for Classification
knnbel2 &lt;- knn_bel.builder(labs = labs[train], test = test, train =
train, options = list(k = 3, p = FALSE, dist.type = 'euclidean', out = 'cv', coef =
0.90))
x3 &lt;- knnbel2(F)
predicted3 &lt;- apply(x3, MARGIN = 2, FUN = function(x) names(which.max(x)))
table(truelabs, predicted3)

##Density Classification Using Jitter &amp; NN Information
jitbel &lt;- jit_bel.builder(labs = labs[train], test = test, train =
train, options = list(k = 3, s = 2, p = FALSE, dist.type = 'euclidean', out =
'var', coef = 0.90))
x4 &lt;- jitbel(F)
predicted4 &lt;- apply(x4, MARGIN = 2, FUN = function(x) names(which.max(x)))
table(truelabs, predicted4)

##Same as above but now using the Coefficient of Variation for Classification
jitbel2 &lt;- jit_bel.builder(labs = labs[train], test = test, train =
train, options = list(k = 3, p = FALSE, dist.type = 'euclidean', out =
'cv', s = 2, coef = 0.90))
x5 &lt;- jitbel2(F)
predicted5 &lt;- apply(x5, MARGIN = 2, FUN = function(x) names(which.max(x)))
table(truelabs, predicted5)

</code></pre>

<hr>
<h2 id='bpa'>Basic Probability Assignment Objects</h2><span id='topic+bpa'></span><span id='topic+print.bpa'></span>

<h3>Description</h3>

<p>These functions can be used to create, combine and print basic probability
assignment objects for classificiation. A Basic Probabilty Assignment is
a similar to a probabilty mass function, except that it has an
additional mass for the concept for &quot;ignorance&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bpa(n = 1, setlist = c(1:n, Inf), mlist = c(rep(0, n), 1))
## S3 method for class 'bpa'
print(x, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bpa_+3A_n">n</code></td>
<td>
<p>The number of distinct values that need to be
represented. Usually set to the number of classes in the data.</p>
</td></tr>
<tr><td><code id="bpa_+3A_setlist">setlist</code></td>
<td>
<p>A subset of 1:<code>n</code>, indicating those elements that have
positive mass. The special value 'Inf' is used to denote the whole
set, which is ignorance in dempster-shafer terms.</p>
</td></tr>
<tr><td><code id="bpa_+3A_mlist">mlist</code></td>
<td>
<p>The actual masses assigned to the elements in the <code>setlist</code>.</p>
</td></tr>
<tr><td><code id="bpa_+3A_x">x</code></td>
<td>
<p>The bpa object to be printed.</p>
</td></tr>
<tr><td><code id="bpa_+3A_verbose">verbose</code></td>
<td>
<p>If FALSE (default), simply prints out the basic
probability assignment. If TRUE, prints a list of all member functions
as well.</p>
</td></tr>
<tr><td><code id="bpa_+3A_...">...</code></td>
<td>
<p>Additional arguments to print method. Not Used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It should be noted that these functions are fairly simplistic, since
they were designed to be fast, and work with classification only. In
particular, if you have set-valued elements, the combination function
will likely give the wrong answer unless the sets are
non-intersecting. For the same reason, belief functions have not been
implemented either, since for atomic elements, bpa = belief function.
</p>


<h3>Value</h3>

<p>The <code>bpa</code> function returns a list of functions which can be used to
query and / or manipulate the create bpa object.
</p>
<table role = "presentation">
<tr><td><code>get.N</code></td>
<td>
<p>Get the number of distinct values represented in the
bpa. Usually set to the number of classes.</p>
</td></tr>
<tr><td><code>get.setlist</code></td>
<td>
<p>Get the sets represented in the bpa</p>
</td></tr>
<tr><td><code>get.full.m</code></td>
<td>
<p>Get the masses assigned to each of the elements.</p>
</td></tr>
<tr><td><code>get.focal.elements</code></td>
<td>
<p>Get only those elements that have a
positive mass attached to them. Such elements are called the focal
elements of the bpa.</p>
</td></tr>
<tr><td><code>get.m</code></td>
<td>
<p>Get the masses attached only to the focal elements, that
is the non-zero elements of the mlist.</p>
</td></tr>
<tr><td><code>get.mass</code></td>
<td>
<p>Get the masses attached to certain specified elements
of the bpa. The elements are specifed as a vector via the argument <code>s</code>.</p>
</td></tr>
<tr><td><code>assign.bpa</code></td>
<td>
<p>Can be used to re-assign mass to certain specified
elements of the bpa. The argument <code>s</code> is the same as the
setlist, and <code>m</code> is the same as mlist.</p>
</td></tr>
<tr><td><code>get.assigned.class</code></td>
<td>
<p>Returns a vector of all possible classes,
in decreasing order of assigned probabilty. (That is, the first
element is the most likely class, and the last element is the least
likely class.)</p>
</td></tr>
<tr><td><code>get.assigned.ratios</code></td>
<td>
<p>Returns a vector of length <code>N</code>-1,
each of whose elements is the ratio of the probability assigned to the
successive classes. That is, the first element is the ratio of the
probability assigned to the most likely class to that assigned to the
next most likely class, and so on.</p>
</td></tr>
<tr><td><code>set.name</code></td>
<td>
<p>Can be passed a string to name the <code>bpa</code> object. (Used
by the <code>bpamat</code> function; names are set to the row number.)</p>
</td></tr>
<tr><td><code>get.name</code></td>
<td>
<p>Returns the name of the <code>bpa</code> object. If not assigned,
returns <code>NULL</code>.</p>
</td></tr>
<tr><td><code>set.info</code></td>
<td>
<p>Can be used to store as auxiliary information. (Used
internally by the <code>bpamat</code> function to store the random seed, or
the special character <code>'C'</code> if the bpa object resulted from a combination.)</p>
</td></tr>
<tr><td><code>get.info</code></td>
<td>
<p>Returns whatever was stored as info. If empty,
<code>NULL</code> is returned.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Mohit Dayal</p>


<h3>References</h3>

<p>Gordon, J. and Shortliffe, E. H. (1984). The dempster-shafer theory of
evidence. Rule-Based Expert Systems: The MYCIN Experiments of the
Stanford Heuristic Programming Project, 3:832-838.
Shafer, G. (1986). The combination of evidence. International Journal of
Intelligent Systems, 1(3):155-179.
</p>


<h3>See Also</h3>

<p>combine.bpa.bs, combine.bpa.ds, combine.bpa.list.ds, combine.bpa.list.ds</p>


<h3>Examples</h3>

<pre><code class='language-R'>##Empty bpa - All mass is attached to ignorance
b1 &lt;- bpa(3)
b1

##Add a set to this bpa
b1$assign.bpa(s = c(1,2), m = c(0.3,0.4))
print(b1, verbose = TRUE)

##The same thing in a different way - classes can be named
##Note that the print method omits empty classes
b0 &lt;- bpa(3, c('A','B','C', Inf), c(0.3, 0.4, 0, 0.3))
b0

##Another bpa
##Again, class '2' has been omitted
b2 &lt;- bpa(3)
b2$assign.bpa(s = c(1,3), m = c(0.7,0.1))
b2

##Combine
b3 &lt;- combine.bpa.ds(b1,b2)
b3
combine.bpa.bs(b1,b2)

##As a list, should be same answer as above
b4 &lt;- combine.bpa.list.ds(list(b1,b2))
b4
combine.bpa.list.bs(list(b1,b2))
</code></pre>

<hr>
<h2 id='bpamat'>Matrices of Basic Probability Assignment Objects</h2><span id='topic+bpamat'></span><span id='topic+print.bpamat'></span>

<h3>Description</h3>

<p>These functions enhance the functionality provided via <code>bpa</code>
objects. They essentially provide for the storage of several bpa
objects at once as a matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bpamat(info = NULL, mat = NULL)
## S3 method for class 'bpamat'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bpamat_+3A_info">info</code></td>
<td>
<p>A piece of auxiliary information that you want stored
along with the matrix of bpa's. (Used internally to store the random seed, or
the special character <code>'C'</code> if the bpa object resulted from a combination.)</p>
</td></tr>
<tr><td><code id="bpamat_+3A_mat">mat</code></td>
<td>
<p>The matrix of bpa's. Each column represents a point, and
the rows represent classes. Should have column names set to the row
names of the points, and row names set to the names of the classes</p>
</td></tr>
<tr><td><code id="bpamat_+3A_x">x</code></td>
<td>
<p>The bpamat object to be printed.</p>
</td></tr>
<tr><td><code id="bpamat_+3A_...">...</code></td>
<td>
<p>Additional arguments to print method. Not Used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>ensemble</code> function returns objects of this type.</p>


<h3>Value</h3>

<p>The <code>bpamat</code> function returns a list of functions which can be used to
query and / or manipulate the create bpa object.
</p>
<table role = "presentation">
<tr><td><code>set.info</code></td>
<td>
<p>Takes a single argument which is set as the auxiliary
information you want stored with the matrix</p>
</td></tr>
<tr><td><code>get.info</code></td>
<td>
<p>Returns the auxiliary information stored with the matrix</p>
</td></tr>
<tr><td><code>assign.mat</code></td>
<td>
<p>Takes a single argument, which should be a matrix
of bpa's, to be stored inside the bpamat object.</p>
</td></tr>
<tr><td><code>get.classify</code></td>
<td>
<p>Returns a vector as long as the number of points
stored in the <code>bpamat</code> object. The elements are named after the
points, and are the current classification of the point, based on the
bpamat object.</p>
</td></tr>
<tr><td><code>get.point</code></td>
<td>
<p>Returns the bpa corresponding to a single point,
whose name is passed as the argument.</p>
</td></tr>
<tr><td><code>get.mat</code></td>
<td>
<p>Returns the matrix of bpa's.</p>
</td></tr>
<tr><td><code>get.setlist</code></td>
<td>
<p>Returns the class names that occur in the current matrix</p>
</td></tr>
<tr><td><code>get.pointlist</code></td>
<td>
<p>Returns the names of all the points whose bpa's
are stored in the current matrix.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Mohit Dayal</p>


<h3>See Also</h3>

<p>bpa, combine.bpamat.bs, combine.bpamat.ds,
combine.bpamat.list.bs, combine.bpamat.list.ds</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cancer)
table(cancer$V2)
colnames(cancer)[1:2] &lt;- c('id', 'type')

cancer.d &lt;- as.matrix(cancer[,3:32])
labs &lt;- cancer$type
test_size &lt;- floor(0.15*nrow(cancer.d))
train &lt;- sample(1:nrow(cancer.d), size = nrow(cancer.d) - test_size)
test &lt;- which(!(1:569 %in% train))
truelabs &lt;- labs[test]

projectron &lt;- function(A)
    cancer.d %*% A

kdebel &lt;- kde_bel.builder(labs = labs[train], test = test, train =
train)

##A projection
seed1 &lt;- .Random.seed
F1 &lt;- projectron(basis_random(30))
x1 &lt;- kdebel(F1)
y1 &lt;- bpamat(info = seed1, mat = x1)
y1
predicted1 &lt;- y1$get.classify()
table(truelabs, predicted1)

##Another projection
seed2 &lt;- .Random.seed
F2 &lt;- projectron(basis_random(30))
x2 &lt;- kdebel(F2)
y2 &lt;- bpamat(info = seed2, mat = x2)
y2
predicted2 &lt;- y2$get.classify()
table(truelabs, predicted2)

z1 &lt;- combine.bpamat.bs(y1, y2)
z2 &lt;- combine.bpamat.ds(y1, y2)
table(truelabs, z1$get.classify())
table(truelabs, z2$get.classify())

##Same result
w1 &lt;- combine.bpamat.list.bs(list(y1, y2))
w2 &lt;- combine.bpamat.list.ds(list(y1, y2))
</code></pre>

<hr>
<h2 id='cancer'>Wisconsin Breast Cancer Data from UCI website</h2><span id='topic+cancer'></span>

<h3>Description</h3>

<p>This is part of the Breast Cancer Data, publicly available from the UCI
Machine Learning Datasets webpage at https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Original%29
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(cancer)
</code></pre>


<h3>Details</h3>

<p>Ten real-valued features are computed for each cell nucleus:
</p>
<p>a) radius (mean of distances from center to points on the perimeter)
b) texture (standard deviation of gray-scale values)
c) perimeter
d) area
e) smoothness (local variation in radius lengths)
f) compactness (perimeter^2 / area - 1.0)
g) concavity (severity of concave portions of the contour)
h) concave points (number of concave portions of the contour)
i) symmetry 
j) fractal dimension (&quot;coastline approximation&quot; - 1)
</p>
<p>The mean, standard error, and &quot;worst&quot; or largest (mean of the three
largest values) of these features were computed for each image,
resulting in 30 features.  For instance, field 3 is Mean Radius, field
13 is Radius SE, field 23 is Worst Radius.
</p>
<p>All feature values are recoded with four significant digits.
</p>
<p>Missing attribute values: none
</p>
<p>Class distribution: 357 benign, 212 malignant	
</p>


<h3>Value</h3>

<p>A Data Frame with 569 rows and 32 columns. The first column is
some sort of serial number, and the second column is the class label
('M' for Malignant or 'B' for Benign). The rest of the columns are features.
</p>


<h3>Author(s)</h3>

<p>Mohit Dayal</p>


<h3>References</h3>

<p>1. O. L. Mangasarian and W. H. Wolberg: &quot;Cancer diagnosis via linear 
programming&quot;, SIAM News, Volume 23, Number 5, September 1990, pp 1 &amp; 18.
</p>
<p>2. William H. Wolberg and O.L. Mangasarian: &quot;Multisurface method of 
pattern separation for medical diagnosis applied to breast cytology&quot;, 
Proceedings of the National Academy of Sciences, U.S.A., Volume 87, 
December 1990, pp 9193-9196.
</p>
<p>3. O. L. Mangasarian, R. Setiono, and W.H. Wolberg: &quot;Pattern recognition 
via linear programming: Theory and application to medical diagnosis&quot;, 
in: &quot;Large-scale numerical optimization&quot;, Thomas F. Coleman and Yuying
Li, editors, SIAM Publications, Philadelphia 1990, pp 22-30.
</p>
<p>4. K. P. Bennett &amp; O. L. Mangasarian: &quot;Robust linear programming 
discrimination of two linearly inseparable sets&quot;, Optimization Methods
and Software 1, 1992, 23-34 (Gordon &amp; Breach Science Publishers).
</p>

<hr>
<h2 id='combine.ds'>Combining Basic Probability Assignments</h2><span id='topic+combine.bs'></span><span id='topic+combine.ds'></span><span id='topic+combine.bpa.bs'></span><span id='topic+combine.bpa.ds'></span><span id='topic+combine.bpa.list.bs'></span><span id='topic+combine.bpa.list.ds'></span><span id='topic+combine.bpamat.bs'></span><span id='topic+combine.bpamat.ds'></span><span id='topic+combine.bpamat.list.bs'></span><span id='topic+combine.bpamat.list.ds'></span>

<h3>Description</h3>

<p>These functions can be used to combine one or several basic
probability assignments (bpa). In the limited context that we support here, a
bpa is nothing but a discrete distribution, that may have an additional
mass for ignorance.
</p>
<p>The suffix tells how the combination will be done : <code>ds</code> denotes
that the Dempster-Shafer rules will be used, <code>bs</code> denotes that
Bayes' rule will be used. Thus the function <code>combine.ds</code> combines
two numeric vectors by Dempster-Shafer rules.
</p>
<p>The first middle denotes what kind of object a function operates
on. Thus <code>combine.bpa.ds</code> combines two <code>bpa</code> objects by
Dempster-Shafer rules, while <code>combine.bpamat.ds</code> does the same for
two bpamat objects.
</p>
<p>Finally, the second middle may be used - if set to <code>list</code>, it
combines lists of objects. Thus, the function <code>combine.bpa.list.ds</code>
combines lists of <code>bpa</code> objects by Dempster-Shafer rules.</p>


<h3>Usage</h3>

<pre><code class='language-R'>combine.bs(x, y)
combine.ds(x, y)
combine.bpa.bs(b1, b2)
combine.bpa.ds(b1, b2)
combine.bpa.list.bs(blist)
combine.bpa.list.ds(blist)
combine.bpamat.bs(bmat1, bmat2)
combine.bpamat.ds(bmat1, bmat2)
combine.bpamat.list.bs(bmatlist)
combine.bpamat.list.ds(bmatlist)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="combine.ds_+3A_x">x</code></td>
<td>
<p>A numeric vector representing a bpa.</p>
</td></tr>
<tr><td><code id="combine.ds_+3A_y">y</code></td>
<td>
<p>A numeric vector representing a bpa.</p>
</td></tr>  
<tr><td><code id="combine.ds_+3A_b1">b1</code></td>
<td>
<p>The first bpa object that needs to be combined.</p>
</td></tr>
<tr><td><code id="combine.ds_+3A_b2">b2</code></td>
<td>
<p>The second bpa object that needs to be combined.</p>
</td></tr>
<tr><td><code id="combine.ds_+3A_blist">blist</code></td>
<td>
<p>A list of bpa's to be be combined.</p>
</td></tr>  
<tr><td><code id="combine.ds_+3A_bmat1">bmat1</code></td>
<td>
<p>The first bpa matrix that needs to be combined.</p>
</td></tr>
<tr><td><code id="combine.ds_+3A_bmat2">bmat2</code></td>
<td>
<p>The second bpa matrix that needs to be combined.</p>
</td></tr>
<tr><td><code id="combine.ds_+3A_bmatlist">bmatlist</code></td>
<td>
<p>A list of bpa matrices to be be combined.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>combine.ds</code> functions returns a numeric vector representing
the new bpa.
</p>
<p>The <code>combine.bpamat.bs</code>, <code>combine.bpamat.ds</code>,
<code>combine.bpamat.list.bs</code> and <code>combine.bpamat.list.bs</code>
functions themselves returns a bpamat object.
</p>
<p>The <code>combine.bpa.bs</code>, <code>combine.bpa.ds</code>,
<code>combine.bpa.list.bs</code> and the <code>combine.bpa.list.ds</code>
functions themselves returns a bpa object.  
</p>


<h3>Author(s)</h3>

<p>Mohit Dayal</p>


<h3>References</h3>

<p>Gordon, J. and Shortliffe, E. H. (1984). The dempster-shafer theory of
evidence. Rule-Based Expert Systems: The MYCIN Experiments of the
Stanford Heuristic Programming Project, 3:832-838.
Shafer, G. (1986). The combination of evidence. International Journal of
Intelligent Systems, 1(3):155-179.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##Very Strong, Consistent Testimony
vstrong &lt;- c(0.85, 0.07, 0.08)
##Strong, Consistent Testimony
strong &lt;- c(0.7, 0.15, 0.15)
##Somewhat Ambiguous Testimony
amb &lt;- c(0.55, 0.40, 0.05)
##More Diffuse Testimony
amb2 &lt;- c(0.55, 0.20, 0.25)

fn_gen &lt;- function(par)
{
    x &lt;- gtools::rdirichlet(2, par)
    y &lt;- x
    y &lt;- t(apply(y, MARGIN = 1, FUN = function(x) x * 0.9))
    y &lt;- cbind(y, 0.1)
    return(y)
}

a1 &lt;- fn_gen(vstrong)
combine.bs(a1[1,], a1[2,])
combine.ds(a1[1,], a1[2,])

a2 &lt;- fn_gen(strong)
combine.bs(a2[1,], a2[2,])
combine.ds(a2[1,], a2[2,])

a3 &lt;- fn_gen(amb)
combine.bs(a3[1,], a3[2,])
combine.ds(a3[1,], a3[2,])

a4 &lt;- fn_gen(amb2)
combine.bs(a4[1,], a4[2,])
combine.ds(a4[1,], a4[2,])

##For bpa or bpamat examples, see the relevant help files
</code></pre>

<hr>
<h2 id='ensemble'>Ensemble Objects for Classification</h2><span id='topic+ensemble'></span>

<h3>Description</h3>

<p>This function provides for the creation, and storage of an ensemble of
simpler classifiers. Right now, only a single type of classifier is
available; this shall be fixed in the future.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ensemble(dat, train, test, labs, bel.type = c('kde', 'knn', 'jit'), bel_options)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ensemble_+3A_dat">dat</code></td>
<td>
<p>The full data matrix, including both test and train rows.</p>
</td></tr>
<tr><td><code id="ensemble_+3A_train">train</code></td>
<td>
<p>A vector containing the row numbers (or names) of the
training data in the matrix <code>dat</code>.</p>
</td></tr>
<tr><td><code id="ensemble_+3A_test">test</code></td>
<td>
<p>A vector containing the row numbers (or names) of the
test data in the matrix <code>dat</code>.</p>
</td></tr>
<tr><td><code id="ensemble_+3A_labs">labs</code></td>
<td>
<p>Labels for the training data; must be of the same length
as <code>train</code>.</p>
</td></tr>
<tr><td><code id="ensemble_+3A_bel.type">bel.type</code></td>
<td>
<p>The type of belief function to build the Ensemble. For
more details, see help on Belief.</p>
</td></tr>
<tr><td><code id="ensemble_+3A_bel_options">bel_options</code></td>
<td>
<p>The options list that should be passed to the
belief function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The simpler classifiers must work in 2-dimensions projections
of the data.</p>


<h3>Value</h3>

<p>Returns a list of functions which can be used to query and / or
manipulate the created <code>ensemble</code> object.
</p>
<table role = "presentation">
<tr><td><code>try.matrices</code></td>
<td>
<p>Takes a single argument <code>n</code> which is the
number of matrices you want to try.</p>
</td></tr>
<tr><td><code>get.bpamats</code></td>
<td>
<p>Returns a list of bpa.mat objects representing the
classifications recieved from each projection.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Mohit Dayal</p>


<h3>See Also</h3>

<p>belief</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cancer)
table(cancer$V2)
colnames(cancer)[1:2] &lt;- c('id', 'type')

cancer.d &lt;- as.matrix(cancer[,3:32])
labs &lt;- cancer$type
test_size &lt;- floor(0.15*nrow(cancer.d))
train &lt;- sample(1:nrow(cancer.d), size = nrow(cancer.d) - test_size)
test &lt;- which(!(1:569 %in% train))
truelabs = labs[test]

e &lt;- ensemble(dat = cancer.d, labs = labs[train], train = train, test =
test, bel.type = 'kde', bel_options = list(coef = 0.90))

##Try more matrices than that in real life!
##Also increase the mc.cores parameter if you have more cores!
e$try.matrices(n = 3, mc.cores = 1)

y &lt;- e$get.bpamats()
length(y)

##Can see results from each projection
##b.1  &lt;- bpamat(mat = y[[1]]$get.mat())
##b.2  &lt;- bpamat(mat = y[[2]]$get.mat())
##b.12b &lt;- combine.bpa.mat.bs(b.1, b.2)
##b.12d &lt;- combine.bpa.mat.ds(b.1, b.2)
##b.12b$get.classify()
##b.12d$get.classify()

##All the results
##b.n &lt;- lapply(y, function(x) x$get.classify())
##allb &lt;- combine.bpa.mat.list.bs(e$get.bpamats())
##alld &lt;- combine.bpa.mat.list.ds(e$get.bpamats())

##fn1 &lt;- function(x)
##    table(truelabs, x$get.classify())

##fn2 &lt;- function(x)
##    {
##        tmp &lt;- table(truelabs, x$get.classify())
##        100 *sum(diag(tmp)) / sum(tmp)
##    }

##fn1(allb)
##fn2(allb)
##fn1(alld)
##fn2(alld)
</code></pre>

<hr>
<h2 id='get.NN'>Function to find the nearest neighbours</h2><span id='topic+get.NN'></span>

<h3>Description</h3>

<p>This function locates the nearest neighbours of each point in the
test set in the training set. Both sets must of the same dimensions and
are passed as successive rows of the same matrix <code>P</code>.
</p>
<p>User can decide whether a specified number of neighbours should be
sought, or whether they should be sought as some fraction of the size of
the training set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get.NN(P, k = 2, p = !k, test, train, dist.type = c("euclidean",
"absolute", "mahal"), nn.type = c("which", "dist", "max"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get.NN_+3A_p">P</code></td>
<td>
<p>The matrix of data. Contains both the training and test sets.</p>
</td></tr>
<tr><td><code id="get.NN_+3A_k">k</code></td>
<td>
<p>The number of nearest neighbours sought.</p>
</td></tr>
<tr><td><code id="get.NN_+3A_p">p</code></td>
<td>
<p>The number of nearest neighbours sought, specified as a
fraction of the training set.</p>
</td></tr>
<tr><td><code id="get.NN_+3A_test">test</code></td>
<td>
<p>The rows of the matrix <code>P</code> that contain the test data.</p>
</td></tr>
<tr><td><code id="get.NN_+3A_train">train</code></td>
<td>
<p>The rows of the matrix <code>P</code> that contain the training data.</p>
</td></tr>
<tr><td><code id="get.NN_+3A_dist.type">dist.type</code></td>
<td>
<p>The type of distance to use when determining neighbours.</p>
</td></tr>
<tr><td><code id="get.NN_+3A_nn.type">nn.type</code></td>
<td>
<p>What should be returned? Either the actual distances
(<code>dist</code>) or their locations (rows) in <code>P</code> (<code>which</code>) or
the k-th maximum distances <code>max</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used internally to compute the nearest
neighbours; the user need not call any of these functions
directly.
</p>


<h3>Value</h3>

<p>Returns a matrix of dimensions (Number of Nearest Neighbours) x (Rows in
Test Set). Each column contains the nearest neighbours of the
corresponding row in the training set.
</p>


<h3>Author(s)</h3>

<p>Mohit Dayal</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(MASS)
mu &lt;- c(3,4)
Sigma &lt;- rbind(c(1,0.2),c(0.2,1))
Y &lt;- mvrnorm(20, mu = mu, Sigma = Sigma)
test &lt;- 1:4
train &lt;- 5:20
nn1a &lt;- get.NN(Y, k = 3, test = 1:4, train = 5:20, dist.type =
'euclidean', nn.type = 'which')
nn1b &lt;- get.NN(Y, k = 3, test = 1:4, train = 5:20, dist.type =
'euclidean', nn.type = 'dist')
nn1c &lt;- get.NN(Y, k = 3, test = 1:4, train = 5:20, dist.type =
'euclidean', nn.type = 'max')
nn2 &lt;- get.NN(Y, p = 0.3, test = 1:4, train = 5:20, dist.type =
'euclidean', nn.type = 'which')
</code></pre>

<hr>
<h2 id='least.k'>Functions to find the few smallest elements in a vector.</h2><span id='topic+least.k'></span><span id='topic+least.p'></span><span id='topic+which.least.k'></span><span id='topic+which.least.p'></span>

<h3>Description</h3>

<p>Given a numeric vector, these functions compute and return the few
smallest elements in it. The number of elements returned is specified as
either a definite number (<code>k</code>) or as a proportion of the vector
length (<code>p</code>). The variant functions (<code>which.*</code>), accomplish
the same task, but return instead the position of such elements in the vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>least.k(x, k)
least.p(x, p)
which.least.k(x, k)
which.least.p(x, p)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="least.k_+3A_x">x</code></td>
<td>
<p>The numeric vector.</p>
</td></tr>
<tr><td><code id="least.k_+3A_k">k</code></td>
<td>
<p>The number of smallest elements sought.</p>
</td></tr>
<tr><td><code id="least.k_+3A_p">p</code></td>
<td>
<p>The number of smallest elements sought, specified as proportion of the length of <code>x</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions are used internally in the determination of nearest neighbours; the user need not call any of these functions directly. Rather, the choice is specified via the arguments <code>k</code> and <code>p</code>.
</p>


<h3>Value</h3>

<p>Either the smallest values themselves or, for the which.* functions, their positions in the vector.
</p>


<h3>Author(s)</h3>

<p>Mohit Dayal</p>


<h3>See Also</h3>

<p>get.NN
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(10)
least.k(x, 3)
least.p(x, 0.3)
which.least.k(x, 3)
which.least.p(x, 0.3)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
