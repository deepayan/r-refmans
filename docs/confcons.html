<!DOCTYPE html><html lang="en"><head><title>Help for package confcons</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {confcons}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#confidence'><p>Confidence of the predictive distribution model</p></a></li>
<li><a href='#consistency'><p>Consistency of the predictive distribution model</p></a></li>
<li><a href='#measures'><p>Goodness-of-fit, confidence and consistency measures</p></a></li>
<li><a href='#thresholds'><p>Thresholds needed to create the extended confusion matrix</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Confidence and Consistency of Predictive Distribution Models</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-03-12</td>
</tr>
<tr>
<td>Description:</td>
<td>Calculate confidence and consistency that measure the
    goodness-of-fit and transferability of predictive/potential distribution
    models (including species distribution models) as described by Somodi &amp;
    Bede-Fazekas et al. (2024) &lt;<a href="https://doi.org/10.1016%2Fj.ecolmodel.2024.110667">doi:10.1016/j.ecolmodel.2024.110667</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/bfakos/confcons">https://github.com/bfakos/confcons</a>,
<a href="https://bfakos.github.io/confcons/">https://bfakos.github.io/confcons/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/bfakos/confcons/issues">https://github.com/bfakos/confcons/issues</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.0</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat (&ge; 3.0.0), mockery, vctrs, withr,
ROCR, covr, terra, sf, blockCV (&ge; 3.1-3), ggplot2, ranger</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-15 16:37:26 UTC; Ákos</td>
</tr>
<tr>
<td>Author:</td>
<td>Ákos Bede-Fazekas <a href="https://orcid.org/0000-0002-2905-338X"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Imelda Somodi <a href="https://orcid.org/0000-0002-6207-3796"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Zoltán Botta-Dukát
    <a href="https://orcid.org/0000-0002-9544-3474"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Ákos Bede-Fazekas &lt;bfakos@ecolres.hu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-17 09:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='confidence'>Confidence of the predictive distribution model</h2><span id='topic+confidence'></span>

<h3>Description</h3>

<p>Calculate the confidence in positive predictions within known presences (CPP,
<code>type = "positive"</code>) or confidence in predictions within known presences
(CP, <code>type = "neutral"</code>) based on the occurrence <code>observations</code>,
the <code>predictions</code> of the probability of occurrence, and the two
<code>thresholds</code> distinguishing certain negatives/positives from uncertain
predictions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>confidence(
  observations,
  predictions,
  thresholds = confcons::thresholds(observations = observations, predictions =
    predictions),
  type = "positive"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="confidence_+3A_observations">observations</code></td>
<td>
<p>Either an integer or logical vector containing the binary
observations where presences are encoded as <code>1</code>s/<code>TRUE</code>s and
absences as <code>0</code>s/<code>FALSE</code>s.</p>
</td></tr>
<tr><td><code id="confidence_+3A_predictions">predictions</code></td>
<td>
<p>A numeric vector containing the predicted probabilities of
occurrence typically within the <code>[0, 1]</code> interval.
<code>length(predictions)</code> should be equal to <code>length(observations)</code>
and the order of the elements should match.</p>
</td></tr>
<tr><td><code id="confidence_+3A_thresholds">thresholds</code></td>
<td>
<p>A numeric vector of length two, typically calculated by
<code><a href="#topic+thresholds">thresholds</a>()</code>. The first element distinguishes certain
negatives (certain absences) from uncertain predictions. The second element
distinguishes certain positives (certain presences) from uncertain
predictions. If missing, <code>confcons::thresholds(observations =
observations, predictions = predictions)</code> is called, but see section 'Note'
about why you should not use the default value.</p>
</td></tr>
<tr><td><code id="confidence_+3A_type">type</code></td>
<td>
<p>A character vector of length one containing the value &quot;positive&quot;
(for calculating <em>confidence in positive predictions</em> within known
presences (CPP)) or &quot;neutral&quot; (for calculating <em>confidence in
predictions</em> within known presences (CP)). Defaults to &quot;positive&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector of length one. It is either NA_real_ or a positive
number within the <code>[0, 1]</code> interval. Larger value indicates that the
model is more confident.
</p>


<h3>Note</h3>

<p>Technically, confidence can be calculated for the training subset, the
evaluation subset, or the whole dataset as well. Note, however, that there
is not so much sense to calculate confidence in the training subset, except
for using the result for <code><a href="#topic+consistency">consistency</a></code> calculation. If you need
only the confidence measure, calculate it on the evaluation subset using
<code><a href="#topic+thresholds">thresholds</a></code> previously determined on the whole dataset (i.e.,
do not use the default value of parameter <code>thresholds</code>). See the last
example below and the vignette.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+thresholds">thresholds</a></code> for calculating the two thresholds,
<code><a href="#topic+consistency">consistency</a></code> for calculating consistency
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(12345)

# Using logical observations, default 'thresholds' and 'type' parameter:
observations_1000_logical &lt;- c(rep(x = FALSE, times = 500),
                               rep(x = TRUE, times = 500))
predictions_1000 &lt;- c(runif(n = 500, min = 0, max = 0.7),
                      runif(n = 500, min = 0.3, max = 1))
confidence(observations = observations_1000_logical,
           predictions = predictions_1000) # 0.561

# Using integer observations, default 'thresholds' parameter,
# both 'positive' and 'neutral' confidence type:
observations_4000_integer &lt;- c(rep(x = 0L, times = 3000),
                               rep(x = 1L, times = 1000))
predictions_4000 &lt;- c(runif(n = 3000, min = 0, max = 0.8),
                      runif(n = 1000, min = 0.2, max = 0.9))
confidence(observations = observations_4000_integer,
           predictions = predictions_4000, type = "positive") # 0.691
confidence(observations = observations_4000_integer,
           predictions = predictions_4000, type = "neutral") # 0.778

# Using some previously selected thresholds:
strict_thresholds &lt;- c(0.1, 0.9)
permissive_thresholds &lt;- c(0.4, 0.5)
percentile_thresholds &lt;- quantile(x = predictions_4000[observations_4000_integer == 1],
                                  probs = c(0.1, 0.9)) # 10th and 90th percentile
confidence(observations = observations_4000_integer,
           predictions = predictions_4000,
           thresholds = strict_thresholds,
           type = "neutral") # 0
confidence(observations = observations_4000_integer,
           predictions = predictions_4000,
           thresholds = permissive_thresholds,
           type = "neutral") # 0.836
confidence(observations = observations_4000_integer,
           predictions = predictions_4000,
           thresholds = percentile_thresholds,
           type = "neutral") # 0.2

# Real-life case
# (thresholds calculated from the whole dataset, confidence from the evaluation subset):
dataset &lt;- data.frame(
  observations = observations_4000_integer,
  predictions = predictions_4000,
  evaluation_mask = c(rep(x = FALSE, times = 250),
                      rep(x = TRUE, times = 250),
                      rep(x = FALSE, times = 250),
                      rep(x = TRUE, times = 250))
)
thresholds_whole &lt;- thresholds(observations = dataset$observations,
                               predictions = dataset$predictions)
(confidence_evaluation &lt;- confidence(observations = dataset$observations[dataset$evaluation_mask],
                                     predictions = dataset$predictions[dataset$evaluation_mask],
                                     thresholds = thresholds_whole)) # 0.671

# Wrong parameterization:
try(confidence(observations = observations_1000_logical,
               predictions = predictions_1000,
               type = "pos")) # error
try(confidence(observations = observations_1000_logical,
               predictions = predictions_1000,
               thresholds = c(0.2, NA_real_))) # warning
try(confidence(observations = observations_1000_logical,
               predictions = predictions_1000,
               thresholds = c(-0.4, 0.85))) # warning
try(confidence(observations = observations_1000_logical,
               predictions = predictions_1000,
               thresholds = c(0.6, 0.3))) # warning
try(confidence(observations = observations_1000_logical,
               predictions = predictions_4000)) # error
set.seed(12345)
observations_4000_numeric &lt;- c(rep(x = 0, times = 3000),
                               rep(x = 1, times = 1000))
predictions_4000_strange &lt;- c(runif(n = 3000, min = -0.3, max = 0.4),
                              runif(n = 1000, min = 0.6, max = 1.5))
try(confidence(observations = observations_4000_numeric,
               predictions = predictions_4000_strange,
               thresholds = c(0.2, 0.7))) # multiple warnings
mask_of_normal_predictions &lt;- predictions_4000_strange &gt;= 0 &amp; predictions_4000_strange &lt;= 1
confidence(observations = as.integer(observations_4000_numeric)[mask_of_normal_predictions],
           predictions = predictions_4000_strange[mask_of_normal_predictions],
           thresholds = c(0.2, 0.7)) # OK
</code></pre>

<hr>
<h2 id='consistency'>Consistency of the predictive distribution model</h2><span id='topic+consistency'></span>

<h3>Description</h3>

<p>Calculate consistency (DCPP, DCP) of the model as the difference of the
confidence calculated on the evaluation and the confidence calculated on the
training subset. Consistency serves as a proxy for model's transferability.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>consistency(conf_train, conf_eval)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="consistency_+3A_conf_train">conf_train</code></td>
<td>
<p><strong>Conf</strong>idence calculated on the <strong>train</strong>ing
subset: a numeric vector of length one, containing a number within the
<code>[0, 1]</code> interval. Typically calculated by function
<code><a href="#topic+confidence">confidence</a>()</code> using the training subset.</p>
</td></tr>
<tr><td><code id="consistency_+3A_conf_eval">conf_eval</code></td>
<td>
<p><strong>Conf</strong>idence calculated on the <strong>eval</strong>uation
subset: a numeric vector of length one, containing a number within the
<code>[0, 1]</code> interval. Typically calculated by function
<code><a href="#topic+confidence">confidence</a>()</code> using the evaluation subset.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector of length one. It is either NA_real_ or a number
within the <code>[-1, 1]</code> interval. Typically, it falls within the
<code>[-1, 0]</code> interval. Greater value indicates more
consistent/transferable model. I.e, the closer the returned value is to -1,
the less consistence/transferable the model is. Value above 0 might be an
artifact or might indicate that the training and evaluation subsets were
accidentally swapped.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+thresholds">thresholds</a></code> for calculating the two thresholds,
<code><a href="#topic+confidence">confidence</a></code> for calculating confidence
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simple examples:
consistency(conf_train = 0.93,
            conf_eval = 0.21) # -0.72 - hardly consistent/transferable model
consistency(conf_train = 0.43,
            conf_eval = 0.35) # -0.08 - consistent/transferable model, although not so confident
consistency(conf_train = 0.87,
            conf_eval = 0.71) # -0.16 - a consistent/transferable model that is confident as well
consistency(conf_train = 0.67,
            conf_eval = 0.78) # 0.11 - positive value might be an artifact
consistency(conf_train = 0.67,
            conf_eval = NA_real_) # NA

# Real-life case:
set.seed(12345)
observations &lt;- c(rep(x = FALSE, times = 500),
                 rep(x = TRUE, times = 500))
predictions &lt;- c(runif(n = 500, min = 0, max = 0.7),
                 runif(n = 500, min = 0.3, max = 1))
dataset &lt;- data.frame(
	observations = observations,
	predictions = predictions,
	evaluation_mask = c(rep(x = FALSE, times = 250),
	                    rep(x = TRUE, times = 250),
	                    rep(x = FALSE, times = 250),
	                    rep(x = TRUE, times = 250))
)
thresholds_whole &lt;- thresholds(observations = dataset$observations,
                               predictions = dataset$predictions)
confidence_training &lt;- confidence(observations = dataset$observations[!dataset$evaluation_mask],
                                  predictions = dataset$predictions[!dataset$evaluation_mask],
                                  thresholds = thresholds_whole) # 0.602
confidence_evaluation &lt;- confidence(observations = dataset$observations[dataset$evaluation_mask],
                                    predictions = dataset$predictions[dataset$evaluation_mask],
                                    thresholds = thresholds_whole) # 0.520
consistency(conf_train = confidence_training,
            conf_eval = confidence_evaluation) # -0.083 - consistent/transferable model

# Wrong parameterization:
try(consistency(conf_train = 1.3,
                conf_eval = 0.5)) # warning
try(consistency(conf_train = 0.6,
                conf_eval = c(0.4, 0.5))) # warning
</code></pre>

<hr>
<h2 id='measures'>Goodness-of-fit, confidence and consistency measures</h2><span id='topic+measures'></span>

<h3>Description</h3>

<p>Wrapper function for calculating the predictive distribution model's
<code><a href="#topic+confidence">confidence</a></code>, <code><a href="#topic+consistency">consistency</a></code>, and optionally some
well-known goodness-of-fit measures as well. The calculated measures are as
follows: </p>
 <ul>
<li><p> confidence in predictions (CP) and confidence in
positive predictions (CPP) within known presences for the training and
evaluation subsets </p>
</li>
<li><p> consistency of predictions (difference of CPs; DCP)
and positive predictions (difference of CPPs; DCPP) </p>
</li>
<li><p> Area Under the ROC
Curve (AUC) - optional (see parameter <code>goodness</code>) </p>
</li>
<li><p> maximum of the
True Skill Statistic (maxTSS) - optional (see parameter <code>goodness</code>)</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>measures(
  observations,
  predictions,
  evaluation_mask,
  goodness = FALSE,
  df = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="measures_+3A_observations">observations</code></td>
<td>
<p>Either an integer or logical vector containing the binary
observations where presences are encoded as <code>1</code>s/<code>TRUE</code>s and
absences as <code>0</code>s/<code>FALSE</code>s.</p>
</td></tr>
<tr><td><code id="measures_+3A_predictions">predictions</code></td>
<td>
<p>A numeric vector containing the predicted probabilities of
occurrence typically within the <code>[0, 1]</code> interval.
<code>length(predictions)</code> should be equal to <code>length(observations)</code>
and the order of the elements should match.</p>
</td></tr>
<tr><td><code id="measures_+3A_evaluation_mask">evaluation_mask</code></td>
<td>
<p>A logical vector (mask) of the evaluation subset. Its
<code>i</code>th element indicates whether the  <code>i</code>th element of
<code>observations</code> was used for evaluation (<code>TRUE</code>) or for training
(<code>FALSE</code>). <code>length(evaluation_mask)</code> should be equal to
<code>length(observations)</code> and the order of the elements should match,
i.e. <code>observations[evaluation_mask]</code> were the evaluation subset and
<code>observations[!evaluation_mask]</code> were the training subset.</p>
</td></tr>
<tr><td><code id="measures_+3A_goodness">goodness</code></td>
<td>
<p>Logical vector of length one, defaults to <code>FALSE</code>.
Indicates, whether goodness-of-fit measures (AUC and maxTSS) should be
calculated. If set to <code>TRUE</code>, external package <span class="pkg">ROCR</span> (Sing et al.
2005) is needed for the calculation (see section 'Note').</p>
</td></tr>
<tr><td><code id="measures_+3A_df">df</code></td>
<td>
<p>Logical vector of length one, defaults to <code>FALSE</code>. Indicates,
whether the returned value should be a one-row <code>data.frame</code> that is
<code>rbind()</code>able if <code>measures()</code> is called on multiple models in a
<code>for</code> loop or a <code>lapply()</code>. See section 'Value' and 'Examples'
for details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named numeric vector (if <code>df</code> is <code>FALSE</code>; the default) or
a <code>data.frame</code> (if <code>df</code> is <code>TRUE</code>) of one row.
<code>length()</code> of the vector or <code>ncol()</code> of the <code>data.frame</code> is
6 (if <code>goodness</code> is <code>FALSE</code>; the default) or 8 (if
<code>goodness</code> is <code>TRUE</code>). The name of the elements/columns are as
follows: </p>
 <dl>
<dt>CP_train</dt><dd><p>confidence in predictions within known
presences (CP) for the training subset</p>
</dd> <dt>CP_eval</dt><dd><p>confidence in
predictions within known presences (CP) for the evaluation subset</p>
</dd>
<dt>DCP</dt><dd><p>consistency of predictions (difference of CPs)</p>
</dd> <dt>CPP_train</dt><dd><p>confidence in
positive predictions within known presences (CPP) for the training subset</p>
</dd>
<dt>CPP_eval</dt><dd><p>confidence in positive predictions within known presences
(CPP) for the evaluation subset</p>
</dd> <dt>DCPP</dt><dd><p>consistency of positive
predictions (difference of CPPs)</p>
</dd> <dt>AUC</dt><dd><p>Area Under the ROC Curve (Hanley and McNeil 1982;
calculated by <code><a href="ROCR.html#topic+performance">ROCR::performance()</a></code>). This
element/column is available only if parameter '<code>goodness</code>' is set to
<code>TRUE</code>. If package <span class="pkg">ROCR</span> is not available but parameter
'<code>goodness</code>' is set to <code>TRUE</code>, the value of AUC is
<code>NA_real_</code> and a warning is raised.</p>
</dd> <dt>maxTSS</dt><dd><p>Maximum of the True
Skill Statistic (Allouche et al. 2006; calculated by
<code><a href="ROCR.html#topic+performance">ROCR::performance()</a></code>). This element/column
is available only if parameter '<code>goodness</code>' is set to <code>TRUE</code>. If
package <span class="pkg">ROCR</span> is not available but parameter '<code>goodness</code>' is set
to <code>TRUE</code>, the value of maxTSS is <code>NA_real_</code> and a warning is
raised.</p>
</dd> </dl>



<h3>Note</h3>

<p>Since <span class="pkg">confcons</span> is a light-weight, stand-alone packages, it does
not import package <span class="pkg">ROCR</span> (Sing et al. 2005), i.e. installing
<span class="pkg">confcons</span> does not mean installing <span class="pkg">ROCR</span> automatically. If you
need AUC and maxTSS (i.e., parameter '<code>goodness</code>' is set to
<code>TRUE</code>), you should install <span class="pkg">ROCR</span> or install <span class="pkg">confcons</span> along
with its dependencies (i.e., <code>devtools::install_github(repo =
  "bfakos/confcons", dependencies = TRUE)</code>).
</p>


<h3>References</h3>

 <ul>
<li><p> Allouche O, Tsoar A, Kadmon R (2006): Assessing
the accuracy of species distribution models: prevalence, kappa and the true
skill statistic (TSS). Journal of Applied Ecology 43(6): 1223-1232.
<a href="https://doi.org/10.1111/j.1365-2664.2006.01214.x">doi:10.1111/j.1365-2664.2006.01214.x</a>. </p>
</li>
<li><p> Hanley JA, McNeil BJ (1982):
The meaning and use of the area under a receiver operating characteristic
(ROC) curve. Radiology 143(1): 29-36.
<a href="https://doi.org/10.1148/radiology.143.1.7063747">doi:10.1148/radiology.143.1.7063747</a>. </p>
</li>
<li><p> Sing T, Sander O, Beerenwinkel
N, Lengauer T. (2005): ROCR: visualizing classifier performance in R.
Bioinformatics 21(20): 3940-3941. <a href="https://doi.org/10.1093/bioinformatics/bti623">doi:10.1093/bioinformatics/bti623</a>. </p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+confidence">confidence</a></code> for calculating confidence,
<code><a href="#topic+consistency">consistency</a></code> for calculating consistency,
<code><a href="ROCR.html#topic+performance">ROCR::performance()</a></code> for calculating AUC and
TSS
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(12345)
dataset &lt;- data.frame(
	observations = c(rep(x = FALSE, times = 500),
                  rep(x = TRUE, times = 500)),
	predictions_model1 = c(runif(n = 250, min = 0, max = 0.6),
                        runif(n = 250, min = 0.1, max = 0.7),
                        runif(n = 250, min = 0.4, max = 1),
                        runif(n = 250, min = 0.3, max = 0.9)),
	predictions_model2 = c(runif(n = 250, min = 0.1, max = 0.55),
                        runif(n = 250, min = 0.15, max = 0.6),
                        runif(n = 250, min = 0.3, max = 0.9),
                        runif(n = 250, min = 0.25, max = 0.8)),
	evaluation_mask = c(rep(x = FALSE, times = 250),
	                    rep(x = TRUE, times = 250),
	                    rep(x = FALSE, times = 250),
	                    rep(x = TRUE, times = 250))
)

# Default parameterization, return a vector without AUC and maxTSS:
conf_and_cons &lt;- measures(observations = dataset$observations,
                          predictions = dataset$predictions_model1,
                          evaluation_mask = dataset$evaluation_mask)
print(conf_and_cons)
names(conf_and_cons)
conf_and_cons[c("CPP_eval", "DCPP")]

# Calculate AUC and maxTSS as well if package ROCR is installed:
if (requireNamespace(package = "ROCR", quietly = TRUE)) {
  conf_and_cons_and_goodness &lt;- measures(observations = dataset$observations,
                                         predictions = dataset$predictions_model1,
                                         evaluation_mask = dataset$evaluation_mask,
                                         goodness = TRUE)
}

# Calculate the measures for multiple models in a for loop:
model_IDs &lt;- as.character(1:2)
for (model_ID in model_IDs) {
  column_name &lt;- paste0("predictions_model", model_ID)
  conf_and_cons &lt;- measures(observations = dataset$observations,
                            predictions = dataset[, column_name, drop = TRUE],
                            evaluation_mask = dataset$evaluation_mask,
                            df = TRUE)
  if (model_ID == model_IDs[1]) {
    conf_and_cons_df &lt;- conf_and_cons
  } else {
    conf_and_cons_df &lt;- rbind(conf_and_cons_df, conf_and_cons)
  }
}
conf_and_cons_df

# Calculate the measures for multiple models in a lapply():
conf_and_cons_list &lt;- lapply(X = model_IDs,
                             FUN = function(model_ID) {
                               column_name &lt;- paste0("predictions_model", model_ID)
                               measures(observations = dataset$observations,
                                        predictions = dataset[, column_name, drop = TRUE],
                                        evaluation_mask = dataset$evaluation_mask,
                                        df = TRUE)
                             })
conf_and_cons_df &lt;- do.call(what = rbind,
                            args = conf_and_cons_list)
conf_and_cons_df
</code></pre>

<hr>
<h2 id='thresholds'>Thresholds needed to create the extended confusion matrix</h2><span id='topic+thresholds'></span>

<h3>Description</h3>

<p>Calculate the two thresholds distinguishing certain negatives/positives from
uncertain predictions. The thresholds are needed to create the extended
confusion matrix and are further used for confidence calculation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>thresholds(observations, predictions = NULL, type = "mean", range = 0.5)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="thresholds_+3A_observations">observations</code></td>
<td>
<p>Either an integer or logical vector containing the binary
observations where presences are encoded as <code>1</code>s/<code>TRUE</code>s and
absences as <code>0</code>s/<code>FALSE</code>s.</p>
</td></tr>
<tr><td><code id="thresholds_+3A_predictions">predictions</code></td>
<td>
<p>A numeric vector containing the predicted probabilities of
occurrence typically within the <code>[0, 1]</code> interval.
<code>length(predictions)</code> should be equal to <code>length(observations)</code>
and the order of the elements should match. <code>predictions</code> is optional:
needed and used only if <code>type</code> is 'mean' and ignored otherwise.</p>
</td></tr>
<tr><td><code id="thresholds_+3A_type">type</code></td>
<td>
<p>A character vector of length one containing the value 'mean' (for
calculating mean of the predictions within known presences and absences) or
'information' (for calculating thresholds based on relative information
gain) . Defaults to 'mean'.</p>
</td></tr>
<tr><td><code id="thresholds_+3A_range">range</code></td>
<td>
<p>A numeric vector of length one containing a value from the
<code>]0, 0.5]</code> interval. It is the parameter of the information-based
method and is used only if <code>type</code> is 'information'. The larger the
<code>range</code> is, the more predictions are treated as uncertain. Defaults to
0.5.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named numeric vector of length 2. The first element
('<code>threshold1</code>') is the mean of probabilities predicted to the absence
locations distinguishing certain negatives (certain absences) from
uncertain predictions. The second element ('<code>threshold2</code>') is the mean
of probabilities predicted to the presence locations distinguishing certain
positives (certain presences) from uncertain predictions. For a typical
model better than the random guess, the first element is smaller than the
second one. The returned value might contain <code>NaN</code>(s) if the number of
observed presences and/or absences is 0.
</p>


<h3>Note</h3>

<p><code>thresholds()</code> should be called using the whole dataset containing
both training and evaluation locations.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+confidence">confidence</a></code> for calculating confidence,
<code><a href="#topic+consistency">consistency</a></code> for calculating consistency
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(12345)

# Using logical observations:
observations_1000_logical &lt;- c(rep(x = FALSE, times = 500),
                               rep(x = TRUE, times = 500))
predictions_1000 &lt;- c(runif(n = 500, min = 0, max = 0.7),
                      runif(n = 500, min = 0.3, max = 1))
thresholds(observations = observations_1000_logical,
           predictions = predictions_1000) # 0.370 0.650

# Using integer observations:
observations_4000_integer &lt;- c(rep(x = 0L, times = 3000),
                               rep(x = 1L, times = 1000))
predictions_4000 &lt;- c(runif(n = 3000, min = 0, max = 0.8),
                      runif(n = 1000, min = 0.2, max = 0.9))
thresholds(observations = observations_4000_integer,
           predictions = predictions_4000) # 0.399 0.545

# Wrong parameterization:
try(thresholds(observations = observations_1000_logical,
               predictions = predictions_4000)) # error
set.seed(12345)
observations_4000_numeric &lt;- c(rep(x = 0, times = 3000),
                               rep(x = 1, times = 1000))
predictions_4000_strange &lt;- c(runif(n = 3000, min = -0.3, max = 0.4),
                              runif(n = 1000, min = 0.6, max = 1.5))
try(thresholds(observations = observations_4000_numeric,
               predictions = predictions_4000_strange)) # multiple warnings
mask_of_normal_predictions &lt;- predictions_4000_strange &gt;= 0 &amp; predictions_4000_strange &lt;= 1
thresholds(observations = as.integer(observations_4000_numeric)[mask_of_normal_predictions],
           predictions = predictions_4000_strange[mask_of_normal_predictions]) # OK
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
