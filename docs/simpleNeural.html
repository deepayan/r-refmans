<!DOCTYPE html><html><head><title>Help for package simpleNeural</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {simpleNeural}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#sN.MLPpredict'><p>Runs a multilayer perceptron</p></a></li>
<li><a href='#sN.MLPtrain'><p>Trains a multilayer perceptron with 1 hidden layer</p></a></li>
<li><a href='#sN.normalizeDF'><p>Normalize data</p></a></li>
<li><a href='#UCI.BCD.Wisconsin'><p>Breast Cancer Wisconsin (Diagnostic) Data Set</p></a></li>
<li><a href='#UCI.ISOLET.ABC'><p>ISOLET Data Set (ABC)</p></a></li>
<li><a href='#UCI.transfusion'><p>Blood Transfusion Service Center Data Set</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>0.1.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2020-02-18</td>
</tr>
<tr>
<td>Title:</td>
<td>An Easy to Use Multilayer Perceptron</td>
</tr>
<tr>
<td>Description:</td>
<td>Trains neural networks (multilayer perceptrons with one hidden layer) for bi- or multi-class classification.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>verification</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Contact:</td>
<td>mailto:me@daviddernoncourt.com</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-02-18 15:39:32 UTC; Admin</td>
</tr>
<tr>
<td>Author:</td>
<td>David Dernoncourt [aut, cre] (LUNAM Universite, Universite Angers,
    Laboratoire d'ergonomie et d'epidemiologie en sante au travail
    (LEEST), w/ support from the French National Research Program for
    Environmental and Occupational Health of Anses (2012/18))</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>David Dernoncourt &lt;me@daviddernoncourt.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-02-18 15:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='sN.MLPpredict'>Runs a multilayer perceptron</h2><span id='topic+sN.MLPpredict'></span>

<h3>Description</h3>

<p>Runs a multilayer perceptron
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sN.MLPpredict(nnModel, X, raw = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sN.MLPpredict_+3A_nnmodel">nnModel</code></td>
<td>
<p>A list containing the coefficients for the MLP (as produced with sN.MLPtrain())</p>
</td></tr>
<tr><td><code id="sN.MLPpredict_+3A_x">X</code></td>
<td>
<p>Matrix of predictors</p>
</td></tr>
<tr><td><code id="sN.MLPpredict_+3A_raw">raw</code></td>
<td>
<p>If true, returns score of each output option. If false, returns the output option with highest value.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The predicted values obtained by the MLP
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(UCI.transfusion);
X=as.matrix(sN.normalizeDF(as.data.frame(UCI.transfusion[,1:4])));
y=as.matrix(UCI.transfusion[,5]);
myMLP=sN.MLPtrain(X=X,y=y,hidden_layer_size=4,it=50,lambda=0.5,alpha=0.5);
myPrediction=sN.MLPpredict(nnModel=myMLP,X=X,raw=TRUE);
#library('verification');
#roc.area(y,myPrediction[,2]);
</code></pre>

<hr>
<h2 id='sN.MLPtrain'>Trains a multilayer perceptron with 1 hidden layer</h2><span id='topic+sN.MLPtrain'></span>

<h3>Description</h3>

<p>Trains a multilayer perceptron with 1 hidden layer and a sigmoid activation function,
using backpropagation and gradient descent.
Don't forget to normalize the data first - sN.normalizeDF(), provided in the package, can be used to do so.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sN.MLPtrain(X, y, hidden_layer_size = 5, it = 50, lambda = 0.5,
  alpha = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sN.MLPtrain_+3A_x">X</code></td>
<td>
<p>Matrix of predictors</p>
</td></tr>
<tr><td><code id="sN.MLPtrain_+3A_y">y</code></td>
<td>
<p>Vector of output (the ANN learns y=ANN(X)).
Classes should be assigned an integer number, starting at 0 for the first class.</p>
</td></tr>
<tr><td><code id="sN.MLPtrain_+3A_hidden_layer_size">hidden_layer_size</code></td>
<td>
<p>Number of units in the hidden layer</p>
</td></tr>
<tr><td><code id="sN.MLPtrain_+3A_it">it</code></td>
<td>
<p>Number of iterations for the gradient descent.
The default value of 50 may be a little low in some cases. 100 to 1000 are generally sensible values.</p>
</td></tr>
<tr><td><code id="sN.MLPtrain_+3A_lambda">lambda</code></td>
<td>
<p>Penalization for model coefficients (regularization parameter)</p>
</td></tr>
<tr><td><code id="sN.MLPtrain_+3A_alpha">alpha</code></td>
<td>
<p>Speed multiplier (learning rate) for gradient descent</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The coefficients of the MLP, in a list (Theta1 between input and hidden layers, Theta2 between hidden and output layers)
</p>


<h3>References</h3>

<p>M.W Gardner, S.R Dorling, Artificial neural networks (the multilayer perceptron)-
a review of applications in the atmospheric sciences, Atmospheric Environment, Volume 32,
Issues 14-15, 1 August 1998, Pages 2627-2636, ISSN 1352-2310, doi: 10.1016/S1352-2310(97)00447-0
[<a href="http://www.sciencedirect.com/science/article/pii/S1352231097004470">http://www.sciencedirect.com/science/article/pii/S1352231097004470</a>]
</p>
<p>Jain, A.K.; Jianchang Mao; Mohiuddin, K.M., &quot;Artificial neural networks: a tutorial,&quot;
Computer , vol.29, no.3, pp.31,44, Mar 1996. doi: 10.1109/2.485891
[<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=485891&amp;isnumber=10412">http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=485891&amp;isnumber=10412</a>]
</p>
<p>Rumelhart, David E., Geoffrey E. Hinton, and R. J. Williams.
&quot;Learning Internal Representations by Error Propagation&quot;. David E. Rumelhart, James L. McClelland, and
the PDP research group (editors).
Parallel distributed processing: Explorations in the microstructure of cognition, Volume 1: Foundations. MIT Press, 1986.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># NB: the provided examples are just here to help use the package's functions.
# In real use cases you should perform a proper validation (cross-validation,
# external validation data...)
data(UCI.BCD.Wisconsin);
X=as.matrix(sN.normalizeDF(as.data.frame(UCI.BCD.Wisconsin[,3:32])));
y=as.matrix(UCI.BCD.Wisconsin[,2]);
myMLP=sN.MLPtrain(X=X,y=y,hidden_layer_size=20,it=50,lambda=0.5,alpha=0.5);
myPrediction=sN.MLPpredict(nnModel=myMLP,X=X,raw=TRUE);
#library('verification');
#roc.area(y,myPrediction[,2]);
</code></pre>

<hr>
<h2 id='sN.normalizeDF'>Normalize data</h2><span id='topic+sN.normalizeDF'></span>

<h3>Description</h3>

<p>Normalize all columns of a dataframe so that all values are in [0;1] and for each column the maximum
value is 1 and the minimum 0.
</p>
<p>newx=(x-min(X))/(max(X)-min(X))
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sN.normalizeDF(dframe)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sN.normalizeDF_+3A_dframe">dframe</code></td>
<td>
<p>The dataframe to be normalized</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The normalized dataframe
</p>

<hr>
<h2 id='UCI.BCD.Wisconsin'>Breast Cancer Wisconsin (Diagnostic) Data Set</h2><span id='topic+UCI.BCD.Wisconsin'></span>

<h3>Description</h3>

<p>Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.
They describe characteristics of the cell nuclei present in the image.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(UCI.BCD.Wisconsin)
</code></pre>


<h3>Format</h3>

<p>A data frame with 569 rows and 32 variables</p>


<h3>Details</h3>

<p>Separating plane described above was obtained using Multisurface Method-Tree (MSM-T)
[K. P. Bennett, &quot;Decision Tree Construction Via Linear Programming.&quot; Proceedings of the 4th Midwest Artificial
Intelligence and Cognitive Science Society, pp. 97-101, 1992], a classification method which uses linear programming to
construct a decision tree. Relevant features were selected using an exhaustive search in the space of 1-4 features
and 1-3 separating planes.
</p>
<p>The actual linear program used to obtain the separating plane in the 3-dimensional space is that described in:
[K. P. Bennett and O. L. Mangasarian: &quot;Robust Linear Programming Discrimination of Two Linearly Inseparable Sets&quot;,
Optimization Methods and Software 1, 1992, 23-34].
</p>
<p>The variables are as follows:
</p>

<ul>
<li><p> ID number
</p>
</li>
<li><p> Diagnosis (1 = malignant, 0 = benign)
</p>
</li>
<li><p> Ten real-valued features are computed for each cell nucleus
</p>
</li></ul>



<h3>Source</h3>

<p>Dataset downloaded from the UCI Machine Learning Repository.
<a href="http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)">http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)</a>
</p>
<p>Creators:
</p>
<p>1. Dr. William H. Wolberg, General Surgery Dept.
University of Wisconsin, Clinical Sciences Center
Madison, WI 53792
wolberg 'at' eagle.surgery.wisc.edu
</p>
<p>2. W. Nick Street, Computer Sciences Dept.
University of Wisconsin, 1210 West Dayton St., Madison, WI 53706
street 'at' cs.wisc.edu 608-262-6619
</p>
<p>3. Olvi L. Mangasarian, Computer Sciences Dept.
University of Wisconsin, 1210 West Dayton St., Madison, WI 53706
olvi 'at' cs.wisc.edu
</p>
<p>Donor: Nick Street
</p>


<h3>References</h3>

<p>W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction for breast tumor diagnosis.
IS&amp;T/SPIE 1993 International Symposium on Electronic Imaging: Science and Technology, volume 1905, pages 861-870, San Jose, CA, 1993.
</p>
<p>Lichman, M. (2013). UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>].
Irvine, CA: University of California, School of Information and Computer Science.
</p>

<hr>
<h2 id='UCI.ISOLET.ABC'>ISOLET Data Set (ABC)</h2><span id='topic+UCI.ISOLET.ABC'></span>

<h3>Description</h3>

<p>This data set was generated as follows. 150 subjects spoke the name of each letter of the alphabet twice.
Hence, we have 52 training examples from each speaker.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(UCI.ISOLET.ABC)
</code></pre>


<h3>Format</h3>

<p>A data frame with 900 rows and 618 variables</p>


<h3>Details</h3>

<p>To reduce package size, only the 3 first letters are included here. The full dataset can be obtained
from <a href="http://archive.ics.uci.edu/ml/datasets/ISOLET">http://archive.ics.uci.edu/ml/datasets/ISOLET</a>.
</p>
<p>The features are described in the paper by Cole and Fanty cited below.
The features include spectral coefficients; contour features, sonorant features, pre-sonorant features,
and post-sonorant features. Exact order of appearance of the features is not known.
</p>


<h3>Source</h3>

<p>Dataset downloaded from the UCI Machine Learning Repository.
<a href="http://archive.ics.uci.edu/ml/datasets/ISOLET">http://archive.ics.uci.edu/ml/datasets/ISOLET</a>
</p>
<p>Creators:
</p>
<p>Ron Cole and Mark Fanty
Department of Computer Science and Engineering,
Oregon Graduate Institute, Beaverton, OR 97006.
cole 'at' cse.ogi.edu, fanty 'at' cse.ogi.edu
</p>
<p>Donor:
</p>
<p>Tom Dietterich
Department of Computer Science
Oregon State University, Corvallis, OR 97331
tgd 'at' cs.orst.edu
</p>


<h3>References</h3>

<p>Fanty, M., Cole, R. (1991). Spoken letter recognition. In Lippman, R. P., Moody, J.,
and Touretzky, D. S. (Eds). Advances in Neural Information Processing Systems 3. San Mateo, CA: Morgan Kaufmann.
[<a href="http://rexa.info/paper/bee6de062d0d168c5c5b5290b11cd6b12ca8472e">http://rexa.info/paper/bee6de062d0d168c5c5b5290b11cd6b12ca8472e</a>]
</p>


<h3>Examples</h3>

<pre><code class='language-R'># NB: 50 iterations isn't enough in this case,
# it was chosen so that the example runs fast enough on CRAN check farm
data(UCI.ISOLET.ABC);
X=as.matrix(sN.normalizeDF(as.data.frame(UCI.ISOLET.ABC[,1:617])));
y=as.matrix(UCI.ISOLET.ABC[,618]-1);
myMLP=sN.MLPtrain(X=X,y=y,hidden_layer_size=20,it=50,lambda=0.5,alpha=0.5);
myPrediction=sN.MLPpredict(nnModel=myMLP,X=X,raw=FALSE);
table(y,myPrediction);
</code></pre>

<hr>
<h2 id='UCI.transfusion'>Blood Transfusion Service Center Data Set</h2><span id='topic+UCI.transfusion'></span>

<h3>Description</h3>

<p>Data taken from the Blood Transfusion Service Center in Hsin-Chu City in Taiwan.
To demonstrate the RFMTC marketing model (a modified version of RFM), this study adopted the donor database
of Blood Transfusion Service Center in Hsin-Chu City in Taiwan.
The center passes their blood transfusion service bus to one university in Hsin-Chu City to gather blood
donated about every three months.
To build a FRMTC model, we selected 748 donors at random from the donor database.
These 748 donor data, each one included R (Recency - months since last donation),
F (Frequency - total number of donation), M (Monetary - total blood donated in c.c.),
T (Time - months since first donation), and a binary variable representing whether he/she donated blood in March 2007
(1 stand for donating blood; 0 stands for not donating blood).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(UCI.transfusion)
</code></pre>


<h3>Format</h3>

<p>A data frame with 748 rows and 5 variables</p>


<h3>Details</h3>

<p>The variables are as follows:
</p>

<ul>
<li><p> R. Recency - months since last donation
</p>
</li>
<li><p> F. Frequency - total number of donations
</p>
</li>
<li><p> M. Monetary - total blood donated in c.c. (mL)
</p>
</li>
<li><p> T. Time - months since first donation
</p>
</li>
<li><p> y. a binary variable representing whether he/she donated blood in March 2007 (1=yes; 0 =no)
</p>
</li></ul>



<h3>Source</h3>

<p>Dataset downloaded from the UCI Machine Learning Repository.
<a href="http://archive.ics.uci.edu/ml/datasets/Blood+Transfusion+Service+Center">http://archive.ics.uci.edu/ml/datasets/Blood+Transfusion+Service+Center</a>
</p>
<p>Original Owner and Donor:
Prof. I-Cheng Yeh
Department of Information Management
Chung-Hua University
Hsin Chu, Taiwan 30067, R.O.C.
e-mail: icyeh 'at' chu.edu.tw
</p>


<h3>References</h3>

<p>Yeh, I-Cheng, Yang, King-Jang, and Ting, Tao-Ming, &quot;Knowledge discovery on RFM model using Bernoulli sequence&quot;,
Expert Systems with Applications, 2008. DOI: 10.1016/j.eswa.2008.07.018
</p>
<p>Lichman, M. (2013). UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>].
Irvine, CA: University of California, School of Information and Computer Science.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
