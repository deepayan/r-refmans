<!DOCTYPE html><html><head><title>Help for package zipfR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {zipfR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Baayen2001'><p>Frequency Spectra from Baayen (2001) (zipfR)</p></a></li>
<li><a href='#beta_gamma'><p>Incomplete Beta and Gamma Functions (zipfR)</p></a></li>
<li><a href='#bootstrap.confint'><p>Estimate Confidence Intervals from Parametric Bootstrap Simulations (zipfR)</p></a></li>
<li><a href='#Brown'><p>Brown Corpus Frequency Data (zipfR)</p></a></li>
<li><a href='#BrownSubsets'><p>Brown Corpus Subset Frequency Data (zipfR)</p></a></li>
<li><a href='#confint.lnre'><p>Confidence Intervals for LNRE Model Parameters (zipfR)</p></a></li>
<li><a href='#Dickens'><p>Dickens' Frequency Data (zipfR)</p></a></li>
<li><a href='#estimate.model'><p>Estimate LNRE Model Parameters (zipfR)</p></a></li>
<li><a href='#EV-EVm'><p>Expected Frequency Spectrum (zipfR)</p></a></li>
<li><a href='#EV-EVm.spc'><p>Binomial Interpolation (zipfR)</p></a></li>
<li><a href='#EvertLuedeling2001'><p>Samples of German Word Formation Affixes (zipfR)</p></a></li>
<li><a href='#ItaPref'><p>Italian Ri- and Ultra- Prefix Frequency Data (zipfR)</p></a></li>
<li><a href='#lnre'><p>LNRE Models (zipfR)</p></a></li>
<li><a href='#LNRE'><p>Type and Probability Distributions of LNRE Models (zipfR)</p></a></li>
<li><a href='#LNRE_posterior'><p>Posterior Distribution of LNRE Model (zipfR)</p></a></li>
<li><a href='#lnre.bootstrap'><p>Parametric bootstrapping for LNRE models (zipfR)</p></a></li>
<li><a href='#lnre.details'><p>Technical Details of LNRE Model Objects (zipfR)</p></a></li>
<li><a href='#lnre.fzm'><p>The finite Zipf-Mandelbrot (fZM) LNRE Model (zipfR)</p></a></li>
<li><a href='#lnre.gigp'><p>The Generalized Inverse Gauss-Poisson (GIGP) LNRE Model (zipfR)</p></a></li>
<li><a href='#lnre.goodness.of.fit'><p>Goodness-of-fit Evaluation of LNRE Models (zipfR)</p></a></li>
<li><a href='#lnre.productivity.measures'><p>Measures of Productivity and Lexical Richness (zipfR)</p></a></li>
<li><a href='#lnre.spc'><p>Compute Expected Frequency Spectrum of LNRE Model (zipfR)</p></a></li>
<li><a href='#lnre.vgc'><p>Expected Vocabulary Growth Curves of LNRE Model (zipfR)</p></a></li>
<li><a href='#lnre.zm'><p>The Zipf-Mandelbrot (ZM) LNRE Model (zipfR)</p></a></li>
<li><a href='#merge.tfl'><p>Merging Type Frequency Lists (zipfR)</p></a></li>
<li><a href='#N-V-Vm'><p>Access Methods for Observed Frequency Data (zipfR)</p></a></li>
<li><a href='#N-V-Vm.spc'><p>Access Methods for Frequency Spectra (zipfR)</p></a></li>
<li><a href='#N-V-Vm.tfl'><p>Access Methods for Type Frequency Lists (zipfR)</p></a></li>
<li><a href='#N-V-Vm.vgc'><p>Access Methods for Vocabulary Growth Curves (zipfR)</p></a></li>
<li><a href='#plot.lnre'><p>Plot LNRE Population Distribution (zipfR)</p></a></li>
<li><a href='#plot.spc'><p>Plot Word Frequency Spectra (zipfR)</p></a></li>
<li><a href='#plot.tfl'><p>Plot Type-Frequency List / Zipf Ranking (zipfR)</p></a></li>
<li><a href='#plot.vgc'><p>Plot Vocabulary Growth Curves (zipfR)</p></a></li>
<li><a href='#print.lnre'><p>Printing LNRE Models (zipfR)</p></a></li>
<li><a href='#print.spc'><p>Printing Frequency Spectra (zipfR)</p></a></li>
<li><a href='#print.tfl'><p>Printing Type Frequency Lists (zipfR)</p></a></li>
<li><a href='#print.vgc'><p>Printing Vocabulary Growth Curves (zipfR)</p></a></li>
<li><a href='#productivity.measures'><p>Measures of Productivity and Lexical Richness (zipfR)</p></a></li>
<li><a href='#read.multiple.objects'><p>Reading Multiple Objects from Files (zipfR)</p></a></li>
<li><a href='#read.spc'><p>Loading and Saving Frequency Spectra (zipfR)</p></a></li>
<li><a href='#read.tfl'><p>Loading and Saving Type Frequency Lists (zipfR)</p></a></li>
<li><a href='#read.vgc'><p>Loading and Saving Vocabulary Growth Curves (zipfR)</p></a></li>
<li><a href='#sample.spc'><p>Incremental Samples from a Frequency Spectrum (zipfR)</p></a></li>
<li><a href='#sample.tfl'><p>Incremental Samples from a Type Frequency List (zipfR)</p></a></li>
<li><a href='#spc'><p>Frequency Spectra (zipfR)</p></a></li>
<li><a href='#spc.interp'><p>Expected Frequency Spectrum by Binomial Interpolation (zipfR)</p></a></li>
<li><a href='#spc.vector'><p>Create Vector of Spectrum Elements (zipfR)</p></a></li>
<li><a href='#spc2tfl'><p>Convert Between Frequency Spectra and Type Frequency Lists (zipfR)</p></a></li>
<li><a href='#tfl'><p>Type Frequency Lists (zipfR)</p></a></li>
<li><a href='#Tiger'><p>Tiger NP and PP expansions (zipfR)</p></a></li>
<li><a href='#vec2xxx'><p>Type-Token Statistics for Samples and Empirical Data (zipfR)</p></a></li>
<li><a href='#vgc'><p>Vocabulary Growth Curves (zipfR)</p></a></li>
<li><a href='#vgc.interp'><p>Expected Vocabulary Growth by Binomial Interpolation (zipfR)</p></a></li>
<li><a href='#VV-Vm'><p>Variances of the Expected Frequency Spectrum (zipfR)</p></a></li>
<li><a href='#zipfR-package'>
<p>zipfR: lexical statistics in R</p>
</p></a></li>
<li><a href='#zipfR.par'><p>Set or Query Graphics Parameters (zipfR)</p></a></li>
<li><a href='#zipfR.plotutils'><p>Plotting Utilities (zipfR)</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Statistical Models for Word Frequency Distributions</td>
</tr>
<tr>
<td>Version:</td>
<td>0.6-70</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, utils, stats, graphics, grDevices, parallel</td>
</tr>
<tr>
<td>Date:</td>
<td>2020-10-10</td>
</tr>
<tr>
<td>Author:</td>
<td>Stefan Evert &lt;stefan.evert@fau.de&gt;, Marco Baroni &lt;marco.baroni@unitn.it&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Stefan Evert &lt;stefan.evert@fau.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Statistical models and utilities for the analysis of word frequency distributions.
	The utilities include functions for loading, manipulating and visualizing word frequency
	data and vocabulary growth curves.  The package also implements several statistical 
	models for the distribution of word frequencies in a population.  (The name of this package 
	derives from the most famous word frequency distribution, Zipf's law.)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://zipfR.R-Forge.R-project.org/">https://zipfR.R-Forge.R-project.org/</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-10-11 18:25:21 UTC; evert</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-10-11 19:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='Baayen2001'>Frequency Spectra from Baayen (2001) (zipfR)</h2><span id='topic+Baayen2001'></span>

<h3>Description</h3>

<p>Frequency spectra included as examples in Baayen (2001).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
Baayen2001

</code></pre>


<h3>Format</h3>

<p>A list of 23 frequency spectra, i.e. objects of class <code><a href="#topic+spc">spc</a></code>.
List elements are named according to the original files, but without the extension <code>.spc</code>.
See Baayen (2001, pp. 249-277) for details.
</p>
<p>In particular, the following spectra are included:
</p>

<dl>
<dt><code>alice</code>:</dt><dd><p>Lewis Carroll, <em>Alice's Adventures in Wonderland</em></p>
</dd>
<dt><code>through</code>:</dt><dd><p>Lewis Carroll, <em>Through the Looking-Glass and What Alice Found There</em></p>
</dd>
<dt><code>war</code>:</dt><dd><p>H. G. Wells, <em>War of the Worlds</em></p>
</dd>
<dt><code>hound</code>:</dt><dd><p>Arthur Conan-Doyle, <em>Hound of the Baskervilles</em></p>
</dd>
<dt><code>havelaar</code>:</dt><dd><p>E. Douwes Dekker, <em>Max Havelaar</em></p>
</dd>
<dt><code>turkish</code>:</dt><dd><p>An archeology text (Turkish)</p>
</dd>
<dt><code>estonian</code>:</dt><dd><p>A. H. Tammsaare, <em>Truth and Justice</em> (Estonian)</p>
</dd>
<dt><code>bnc</code>:</dt><dd><p>The context-governed subcorpus of the British National Corpus (BNC)</p>
</dd>
<dt><code>in1</code>:</dt><dd><p>Sample of 1 million tokens from <em>The Independent</em></p>
</dd>
<dt><code>in8</code>:</dt><dd><p>Sample of 8 million tokens from <em>The Independent</em></p>
</dd>
<dt><code>heid</code>:</dt><dd><p>Nouns in <em>-heid</em> in the CELEX database (Dutch)</p>
</dd>
<dt><code>iteit</code>:</dt><dd><p>Nouns in <em>-iteit</em> in the CELEX database (Dutch)</p>
</dd>
<dt><code>ster</code>:</dt><dd><p>Nouns in <em>-ster</em> in the CELEX database (Dutch)</p>
</dd>
<dt><code>in</code>:</dt><dd><p>Nouns in <em>-in</em> in the CELEX database (Dutch)</p>
</dd>
<dt><code>nouns</code>:</dt><dd><p>Simplex nouns in the CELEX database (Dutch)</p>
</dd>
<dt><code>sing</code>:</dt><dd><p>Singular nouns in M. Innes, <em>The Bloody Wood</em></p>
</dd>
<dt><code>plur</code>:</dt><dd><p>Plural nouns in M. Innes, <em>The Bloody Wood</em></p>
</dd>
<dt><code>nessw</code>:</dt><dd><p>Nouns in <em>-ness</em> in the written subcorpus of the BNC</p>
</dd>
<dt><code>nesscg</code>:</dt><dd><p>Nouns in <em>-ness</em> in the context-governed subcorpus of the BNC</p>
</dd>
<dt><code>nessd</code>:</dt><dd><p>Nouns in <em>-ness</em> in the demographic subcorpus of the BNC</p>
</dd>
<dt><code>filarial</code>:</dt><dd><p>Counts of filarial worms in mites on rats</p>
</dd>
<dt><code>cv</code>:</dt><dd><p>Context-vowel patterns in the TIMIT speech database</p>
</dd>
<dt><code>pairs</code>:</dt><dd><p>Word pairs in E. Douwes Dekker, <em>Max Havelaar</em></p>
</dd>
</dl>



<h3>References</h3>

<p>Baayen, R. Harald (2001). <em>Word Frequency Distributions.</em> Kluwer,
Dordrecht.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
Baayen2001$alice

</code></pre>

<hr>
<h2 id='beta_gamma'>Incomplete Beta and Gamma Functions (zipfR)</h2><span id='topic+Cgamma'></span><span id='topic+Rgamma'></span><span id='topic+Rgamma.inv'></span><span id='topic+Igamma'></span><span id='topic+Igamma.inv'></span><span id='topic+Cbeta'></span><span id='topic+Rbeta'></span><span id='topic+Rbeta.inv'></span><span id='topic+Ibeta'></span><span id='topic+Ibeta.inv'></span>

<h3>Description</h3>

<p>The functions documented here compute incomplete and regularized
Beta and Gamma functions as well as their logarithms and the
corresponding inverse functions. These functions will be of interest
to developers, not users of the toolkit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  Cgamma(a, log=!missing(base), base=exp(1))
  Igamma(a, x, lower=TRUE, log=!missing(base), base=exp(1))
  Igamma.inv(a, y, lower=TRUE, log=!missing(base), base=exp(1))
  Rgamma(a, x, lower=TRUE, log=!missing(base), base=exp(1))
  Rgamma.inv(a, y, lower=TRUE, log=!missing(base), base=exp(1))

  Cbeta(a, b, log=!missing(base), base=exp(1))
  Ibeta(x, a, b, lower=TRUE, log=!missing(base), base=exp(1))
  Ibeta.inv(y, a, b, lower=TRUE, log=!missing(base), base=exp(1))
  Rbeta(x, a, b, lower=TRUE, log=!missing(base), base=exp(1))
  Rbeta.inv(y, a, b, lower=TRUE, log=!missing(base), base=exp(1))

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="beta_gamma_+3A_a">a</code>, <code id="beta_gamma_+3A_b">b</code></td>
<td>
<p>non-negative numeric vectors, the parameters of the Gamma
and Beta functions (<code>b</code> applies only to Beta functions)</p>
</td></tr>
<tr><td><code id="beta_gamma_+3A_x">x</code></td>
<td>
<p>a non-negative numeric vector, the point at which the
incomplete or regularized Gamma or Beta function is evaluated (for
the Beta functions, <code>x</code> must be in the range <code class="reqn">[0,1]</code></p>
</td></tr>
<tr><td><code id="beta_gamma_+3A_y">y</code></td>
<td>
<p>a non-negative numeric vector, the values of the Gamma or
Beta function on linear or logarithmic scale</p>
</td></tr>
<tr><td><code id="beta_gamma_+3A_lower">lower</code></td>
<td>
<p>whether to compute the lower (<code>TRUE</code>) or upper
(<code>FALSE</code>) incomplete or regularized Gamma or Beta function</p>
</td></tr>
<tr><td><code id="beta_gamma_+3A_log">log</code></td>
<td>
<p>if <code>TRUE</code>, return values of the Gamma and Beta
functions &ndash; as well as the <code>y</code> argument of the inverse
functions &ndash; are on logarithmic scale</p>
</td></tr>
<tr><td><code id="beta_gamma_+3A_base">base</code></td>
<td>
<p>a positive number, specifying the base of the logarithmic
scale for values of the Gamma and Beta functions (default: natural
logarithm).  Setting the <code>base</code> parameter implies
<code>log=TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>Cgamma</code> returns the (complete) Gamma function evaluated at
<code>a</code>, <code class="reqn">\Gamma(a)</code>.  <code>Igamma</code> returns the (lower or upper)
incomplete Gamma function with parameter <code>a</code> evaluated at point
<code>x</code>, <code class="reqn">\gamma(a,x)</code> (<code>lower=TRUE</code>) or <code class="reqn">\Gamma(a,x)</code>
(<code>lower=FALSE</code>).  <code>Rgamma</code> returns the corresponding
regularized Gamma function, <code class="reqn">P(a,x)</code> (<code>lower=TRUE</code>) or
<code class="reqn">Q(a,x)</code> (<code>lower=FALSE</code>).  If <code>log=TRUE</code>, the returned
values are on logarithmic scale as specified by the <code>base</code>
parameter.
</p>
<p><code>Igamma.inv</code> and <code>Rgamma.inv</code> compute the inverse of the
incomplete and regularized Gamma functions with respect to the
parameter <code>x</code>.  I.e., <code>Igamma.inv(a,y)</code> returns the point
<code>x</code> at which the (lower or upper) incomplete Gamma function with
parameter <code>a</code> evaluates to <code>y</code>, and <em>mutatis mutandis</em>
for <code>Rgamma.inv(a,y)</code>.  If <code>log=TRUE</code>, the parameter
<code>y</code> is taken to be on a logarithmic scale as specified by
<code>base</code>.
</p>
<p><code>Cbeta</code> returns the (complete) Beta function with arguments
<code>a</code> and <code>b</code>, <code class="reqn">B(a,b)</code>.  <code>Ibeta</code> returns the (lower
or upper) incomplete Beta function with parameters <code>a</code> and
<code>b</code>, evaluated at point <code>x</code>, <code class="reqn">B(x;a,b)</code>
(<code>lower=TRUE</code>) and <code class="reqn">B^*(x;a,b)</code>
(<code>lower=FALSE</code>).  Note that in contrast to the Gamma functions,
capital <code class="reqn">B</code> refers to the <em>lower</em> incomplete Beta function,
and there is no standardized notation for the upper incomplete Beta
function, so <code class="reqn">B^*</code> is used here as an ad-hoc symbol.
<code>Rbeta</code> returns the corresponding regularized Beta function,
<code class="reqn">I(x;a,b)</code> (<code>lower=TRUE</code>) or <code class="reqn">I^*(x;a,b)</code>
(<code>lower=FALSE</code>).  If <code>log=TRUE</code>, the returned values are on
logarithmic scale as specified by the <code>base</code> parameter.
</p>
<p><code>Ibeta.inv</code> and <code>Rbeta.inv</code> compute the inverse of the
incomplete and regularized Beta functions with respect to the
parameter <code>x</code>.  I.e., <code>Ibeta.inv(y,a,b)</code> returns the point
<code>x</code> at which the (lower or upper) incomplete Beta function with
parameters <code>a</code> and <code>b</code> evaluates to <code>y</code>, and
<em>mutatis mutandis</em> for <code>Rbeta.inv(y,a,b)</code>.  If
<code>log=TRUE</code>, the parameter <code>y</code> is taken to be on a
logarithmic scale as specified by <code>base</code>.
</p>
<p>All Gamma and Beta functions can be vectorized in the arguments
<code>x</code>, <code>y</code>, <code>a</code> and <code>b</code>, with the usual R value
recycling rules in the case of multiple vectorizations.
</p>


<h3>Mathematical Details</h3>

<p>The upper incomplete Gamma function is defined by the Gamma integral
</p>
<p style="text-align: center;"><code class="reqn">\Gamma(a,x) = \int_x^{\infty} t^{a-1} e^{-t}\,dt</code>
</p>

<p>The lower incomplete Gamma function is defined by the complementary
Gamma integral
</p>
<p style="text-align: center;"><code class="reqn">\gamma(a,x) = \int_0^x t^{a-1} e^{-t}\,dt</code>
</p>

<p>The complete Gamma function calculates the full Gamma integral,
i.e. <code class="reqn">\Gamma(a) = \gamma(a,0)</code>.  The regularized Gamma functions
scale the corresponding incomplete Gamma functions to the interval
<code class="reqn">[0,1]</code>, by dividing through <code class="reqn">\Gamma(a)</code>.  Thus, the lower
regularized Gamma function is given by
</p>
<p style="text-align: center;"><code class="reqn">P(a,x) = \frac{\gamma(a,x)}{\Gamma(a)}</code>
</p>

<p>and the upper regularized Gamma function is given by
</p>
<p style="text-align: center;"><code class="reqn">Q(a,x) = \frac{\Gamma(a,x)}{\Gamma(a)}</code>
</p>

<p>The lower incomplete Beta function is defined by the Beta integral
</p>
<p style="text-align: center;"><code class="reqn">B(x;a,b) = \int_0^x t^{a-1} (1-t)^{b-1}\,dt</code>
</p>

<p>and the upper incomplete Beta function is defined by the complementary
integral
</p>
<p style="text-align: center;"><code class="reqn">B^*(x;a,b) = \int_x^1 t^{a-1} (1-t)^{b-1}\,dt</code>
</p>

<p>The complete Beta function calculates the full Beta integral, i.e.
<code class="reqn">B(a,b) = B(1;a,b) = B^*(0;a,b)</code>.
The regularized Beta function scales the incomplete Beta function to
the interval <code class="reqn">[0,1]</code>, by dividing through <code class="reqn">B(a,b)</code>.  The lower
regularized Beta function is thus given by
</p>
<p style="text-align: center;"><code class="reqn">I(x;a,b) = \frac{B(x;a,b)}{B(a,b)}</code>
</p>

<p>and the upper regularized Beta function is given by 
</p>
<p style="text-align: center;"><code class="reqn">I^*(x;a,b) = \frac{B^*(x;a,b)}{B(a,b)}</code>
</p>



<h3>See Also</h3>

<p><code><a href="base.html#topic+gamma">gamma</a></code> and <code><a href="base.html#topic+lgamma">lgamma</a></code>, which are fully
equivalent to <code>Cgamma</code>.  <code><a href="base.html#topic+beta">beta</a></code> and
<code><a href="base.html#topic+lbeta">lbeta</a></code>, which are fully equivalent to <code>Cbeta</code>
</p>
<p>The implementations of the incomplete and regularized Gamma functions
are based on the Gamma distribution (see <code><a href="stats.html#topic+pgamma">pgamma</a></code>), and
those of the Beta functions are based on the Beta distribution (see
<code><a href="stats.html#topic+pbeta">pbeta</a></code>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
Cgamma(5 + 1) # = factorial(5)

## P(X &gt;= k) for Poisson distribution with mean alpha
alpha &lt;- 5
k &lt;- 10
Rgamma(k, alpha) # == ppois(k-1, alpha, lower=FALSE)

n &lt;- 49
k &lt;- 6
1 / ((n+1) * Cbeta(n-k+1, k+1)) # == choose(n, k)

## P(X &gt;= k) for binomial distribution with parameters n and p
n &lt;- 100
p &lt;- .1
k &lt;- 15
Rbeta(p, k, n-k+1) # == pbinom(k-1, n, p, lower=FALSE)

</code></pre>

<hr>
<h2 id='bootstrap.confint'>Estimate Confidence Intervals from Parametric Bootstrap Simulations (zipfR)</h2><span id='topic+bootstrap.confint'></span>

<h3>Description</h3>

<p>Estimate confidence intervals for empirical distributions obtained by parametric bootstrapping.
The input data must contain a sufficient number of bootstrap replicates for the desired confidence level.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootstrap.confint(x, level=0.95, 
                  method=c("normal", "mad", "empirical"),
                  data.frame=FALSE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bootstrap.confint_+3A_x">x</code></td>
<td>
<p>a numeric matrix, with rows corresponding to bootstrap replicates and columns corresponding to different statistics or coefficients. The matrix should have column labels, which will be preserved in the result.  A data frame with numeric columns is automatically converted to a matrix.</p>
</td></tr>
<tr><td><code id="bootstrap.confint_+3A_level">level</code></td>
<td>
<p>desired confidence level (two-sided)</p>
</td></tr>
<tr><td><code id="bootstrap.confint_+3A_method">method</code></td>
<td>
<p>type of confidence interval to be estimated (see &quot;Details&quot; below)</p>
</td></tr>
<tr><td><code id="bootstrap.confint_+3A_data.frame">data.frame</code></td>
<td>
<p>if <code>TRUE</code>, the return value is converted to a data frame</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can compute three different types of confidence intervals, selected by the <code>method</code> parameter.  In addition, corresponding estimates of central tendency (<code>center</code>) and spread (<code>spread</code>) of the distribution are returned.
</p>

<dl>
<dt><code>normal</code>:</dt><dd>
<p>Wald-type confidence interval based on normal approximation of the bootstrapped distribution (default).
Central tendency is given by the sample mean, spread by standard deviation.
</p>
<p>This method is unreliable if the bootstrapping produces outlier results and can report confidence limits
outside the feasible range of a parameter or coefficient (e.g. a negative population diversity <code class="reqn">S</code>).
For this reason, it is strongly recommended to use a more robust type of confidence interval.
</p>
</dd>
<dt><code>mad</code>:</dt><dd>
<p>Robust asymmetric confidence intervals around the median, using separate estimates for left and right median absolute deviation (MAD) as robust approximations of standard deviation.
Central tendency is given by the median, and spread by the average of left and right standard deviation (estimated via MAD).
</p>
<p>This method is applicable in most situations and requires fewer bootstrap replicates than empirical confidence intervals. Note that the values are different from those returned by <code><a href="stats.html#topic+mad">mad</a></code> because of the separate left and right estimators.
</p>
</dd>
<dt><code>empirical</code>:</dt><dd>
<p>The empirical inter-quantile range, e.g. 2.5% to 97.5% for default <code>conf.level=.95</code>.
Note that the actual range might be slightly different depending on the number of bootstrap replicates available.
Central tendency is given by the median, and spread by the inter-quartile range (IQR) re-scaled to be comparable to standard deviation (cf. <code><a href="stats.html#topic+IQR">IQR</a></code>).
</p>
<p>This is the only method guaranteed to stay within feasible range, but requires a large number of bootstrap replicates
for reliable confidence intervals (e.g. at least 120 replicates for the default 95% confidence level).
</p>
</dd>
</dl>



<h3>Value</h3>

<p>A numeric matrix with the same number of columns and column labels as <code>x</code>, and four rows:
</p>

<ol>
<li><p> the lower boundary of the confidence interval (labelled with the corresponding quantile, e.g. <code>2.5%</code>)
</p>
</li>
<li><p> the upper boundary of the confidence interval (labelled with the corresponding quantile, e.g. <code>97.5%</code>)
</p>
</li>
<li><p> an estimate of central tendency (labelled <code>center</code>)
</p>
</li>
<li><p> an estimate of spread on a scale comparable to standard deviaton (labelled <code>spread</code>)
</p>
</li></ol>

<p>If <code>data.frame=TRUE</code>, the matrix is converted to a data frame for convenient printing and access in interactive sessions.
</p>


<h3>See Also</h3>

<p><code>bootstrap.confint</code> is usually applied to the output of <code><a href="#topic+lnre.bootstrap">lnre.bootstrap</a></code> with <code>simplify=TRUE</code>.
In particual, <code><a href="#topic+confint.lnre">confint.lnre</a></code> uses this function to obtain bootstrapped confidence intervals for LNRE model parameters and other coefficients; <code><a href="#topic+lnre.productivity.measures">lnre.productivity.measures</a></code> (with <code>bootstrap=TRUE</code>) uses it to determine approximate sampling distributions of productivity measures for a LNRE population.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>M &lt;- cbind(alpha=rnorm(200, 10, 5),      # Gaussian distribution around mean = 10
           beta=rlnorm(200, log(10), 1)) # log-normal distribution around median = 10

summary(M) # overview of the distribution

bootstrap.confint(M, method="normal")    # normal approximation
bootstrap.confint(M, method="mad")       # robust asymmetric MAD estimates
bootstrap.confint(M, method="empirical") # empirical confidence interval

bootstrap.confint(M, method="normal", data.frame=TRUE) # as data frame
</code></pre>

<hr>
<h2 id='Brown'>Brown Corpus Frequency Data (zipfR)</h2><span id='topic+Brown'></span><span id='topic+Brown.tfl'></span><span id='topic+Brown.spc'></span><span id='topic+Brown.emp.vgc'></span>

<h3>Description</h3>

<p><code>Brown.tfl</code>, <code>Brown.spc</code> and <code>Brown.emp.vgc</code> are
<code><a href="#topic+zipfR">zipfR</a></code> objects of classes <code><a href="#topic+tfl">tfl</a></code>,
<code><a href="#topic+spc">spc</a></code> and <code><a href="#topic+vgc">vgc</a></code>, respectively.
</p>
<p>These data were extracted from the Brown corpus (see Kucera and
Francis 1967).
</p>


<h3>Details</h3>

<p><code>Brown.emp.vgc</code> is the <em>empirical</em> vocabulary growth
curve, reflecting the <code>V</code> and <code>V(1)</code> development in the
non-randomized corpus.
</p>
<p>We removed numbers and other forms of non-linguistic material before
collecting word counts from the Brown.
</p>


<h3>References</h3>

<p>Kucera, H. and Francis, W.N. (1967). <em>Computational analysis of
present-day American English</em>. Brown University Press, Providence.
</p>


<h3>See Also</h3>

<p>The datasets documented in <code><a href="#topic+BrownSubsets">BrownSubsets</a></code> pertain to
various subsets of the Brown (e.g., informative prose, adjectives
only, etc.)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  data(Brown.tfl)
  summary(Brown.tfl)

  data(Brown.spc)
  summary(Brown.spc)

  data(Brown.emp.vgc)
  summary(Brown.emp.vgc)

</code></pre>

<hr>
<h2 id='BrownSubsets'>Brown Corpus Subset Frequency Data (zipfR)</h2><span id='topic+BrownSubsets'></span><span id='topic+BrownAdj'></span><span id='topic+BrownAdj.spc'></span><span id='topic+BrownAdj.emp.vgc'></span><span id='topic+BrownNoun'></span><span id='topic+BrownNoun.spc'></span><span id='topic+BrownNoun.emp.vgc'></span><span id='topic+BrownVer'></span><span id='topic+BrownVer.spc'></span><span id='topic+BrownVer.emp.vgc'></span><span id='topic+BrownImag'></span><span id='topic+BrownImag.spc'></span><span id='topic+BrownImag.emp.vgc'></span><span id='topic+BrownInform'></span><span id='topic+BrownInform.spc'></span><span id='topic+BrownInform.emp.vgc'></span><span id='topic+Brown100k'></span><span id='topic+Brown100k.spc'></span>

<h3>Description</h3>

<p>Objects of classes <code><a href="#topic+spc">spc</a></code> and <code><a href="#topic+vgc">vgc</a></code> that
contain frequency data for various subsets of words from the Brown
corpus (see Kucera and Francis 1967).
</p>


<h3>Details</h3>

<p><code>BrownAdj.spc</code>, <code>BrownNoun.spc</code> and <code>BrownVer.spc</code>
are frequency spectra of all the Brown corpus words tagged as
adjectives, nouns and verbs, respectively. <code>BrownAdj.emp.vgc</code>,
<code>BrownNoun.emp.vgc</code> and <code>BrownVer.emp.vgc</code> are the
corresponding observed vocabulary growth curves (tracking the
development of <code>V</code> and <code>V(1)</code>, like all the files with
suffix <code>.emp.vgc</code> below).
</p>
<p><code>BrownImag.spc</code> and <code>BrownInform.spc</code> are frequency
spectra of the Brown corpus words subdivided into the two main
stylistic partitions of the corpus, i.e., imaginative and
informative prose, respectively. <code>BrownImag.emp.vgc</code> and
<code>BrownInform.emp.vgc</code> are the corresponding observed vocabulary
growth curves.
</p>
<p><code>Brown100k.spc</code> is the spectrum of the first 100,000 tokens in
the Brown (useful, e.g., for extrapolation experiments in which we
want to estimate a <code>lnre</code> model on a subset of the data
available). The corresponding observed growth curve can be easily
obtained from the one for the whole Brown (<code>Brown.emp.vgc</code>).
</p>
<p>Notice that we removed numbers and other forms of non-linguistic
material before collecting any data from the Brown.
</p>


<h3>References</h3>

<p>Kucera, H. and Francis, W.N. (1967). <em>Computational analysis of
present-day American English</em>. Brown University Press, Providence.
</p>


<h3>See Also</h3>

<p>The data described in <code><a href="#topic+Brown">Brown</a></code> pertain to the Brown as a
whole.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  data(BrownAdj.spc)
  summary(BrownAdj.spc)

  data(BrownAdj.emp.vgc)
  summary(BrownAdj.emp.vgc)

  data(BrownInform.spc)
  summary(BrownInform.spc)

  data(BrownInform.emp.vgc)
  summary(BrownInform.emp.vgc)

  data(Brown100k.spc)
  summary(Brown100k.spc)

</code></pre>

<hr>
<h2 id='confint.lnre'>Confidence Intervals for LNRE Model Parameters (zipfR)</h2><span id='topic+confint.lnre'></span>

<h3>Description</h3>

<p>Compute bootstrapped confidence intervals for LNRE model parameters.
The supplied model must contain a sufficient number of bootstrapping replicates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lnre'
confint(object, parm, level=0.95, method=c("mad", "normal", "empirical"), 
        plot=FALSE, breaks="Sturges", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="confint.lnre_+3A_object">object</code></td>
<td>
<p>an LNRE model (i.e. an object belonging to a subclass of <code>lnre</code>) with bootstrapping data</p>
</td></tr>
<tr><td><code id="confint.lnre_+3A_parm">parm</code></td>
<td>
<p>model parameter(s) for which confidence intervals are desired.
If unspecified, all parameters as well as population diversity <code class="reqn">S</code> and goodness-of-fit statistic <code class="reqn">X^2</code> are shown.</p>
</td></tr>
<tr><td><code id="confint.lnre_+3A_level">level</code></td>
<td>
<p>desired confidence level (two-sided)</p>
</td></tr>
<tr><td><code id="confint.lnre_+3A_method">method</code></td>
<td>
<p>type of confidence interval to be estimated (see <code><a href="#topic+bootstrap.confint">bootstrap.confint</a></code> for details). Note that this parameter defaults to the asymmetric and more robust <code>mad</code> method here.</p>
</td></tr>
<tr><td><code id="confint.lnre_+3A_plot">plot</code></td>
<td>
<p>if <code>TRUE</code>, plot bootstrapping histogram of the respective model parameter with density estimate and confidence interval</p>
</td></tr>
<tr><td><code id="confint.lnre_+3A_breaks">breaks</code></td>
<td>
<p>breakpoints for histogram shown with <code>plot=TRUE</code> (see <code><a href="graphics.html#topic+hist">hist</a></code> for details)</p>
</td></tr>
<tr><td><code id="confint.lnre_+3A_...">...</code></td>
<td>
<p>all other arguments are ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with one numeric column for each selected model parameter (labelled with the parameter name) and four rows:
</p>

<ol>
<li><p> the lower boundary of the confidence interval (labelled with the corresponding quantile, e.g. <code>2.5%</code>)
</p>
</li>
<li><p> the upper boundary of the confidence interval (labelled with the corresponding quantile, e.g. <code>97.5%</code>)
</p>
</li>
<li><p> an estimate of central tendency (labelled <code>center</code>)
</p>
</li>
<li><p> an estimate of spread on a scale comparable to standard deviaton (labelled <code>spread</code>)
</p>
</li></ol>



<h3>See Also</h3>

<p><code><a href="#topic+lnre">lnre</a></code> for estimating LNRE models with bootstrap replicates,
<code><a href="#topic+lnre.bootstrap">lnre.bootstrap</a></code> for the underlying parameteric bootstrapping code, and
<code><a href="#topic+bootstrap.confint">bootstrap.confint</a></code> for the different methods of estimating confidence intervals.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
model &lt;- lnre("fzm", spc=BrownAdj.spc, bootstrap=20)
confint(model, "alpha") # Zipf slope
confint(model, "S")     # population diversity
confint(model, "S", method="normal") # Gaussian approx works well in this case

confint(model) # overview
confint(model, "alpha", plot=TRUE) # visualize bootstrap distribution

</code></pre>

<hr>
<h2 id='Dickens'>Dickens' Frequency Data (zipfR)</h2><span id='topic+Dickens'></span><span id='topic+Dickens.spc'></span><span id='topic+Dickens.emp.vgc'></span><span id='topic+DickensOliverTwist'></span><span id='topic+DickensOliverTwist.spc'></span><span id='topic+DickensOliverTwist.emp.vgc'></span><span id='topic+DickensGreatExpectations'></span><span id='topic+DickensGreatExpectations.spc'></span><span id='topic+DickensGreatExpectations.emp.vgc'></span><span id='topic+DickensOurMutualFriend'></span><span id='topic+DickensOurMutualFriend.spc'></span><span id='topic+DickensOurMutualFriend.emp.vgc'></span>

<h3>Description</h3>

<p>Objects of classes <code><a href="#topic+spc">spc</a></code> and <code><a href="#topic+vgc">vgc</a></code> that
contain frequency data for a collection of Dickens's works from
Project Gutenberg, and for 3 novels (Oliver Twist, Great
Expectations and Our Mutual Friends).
</p>


<h3>Details</h3>

<p><code>Dickens.spc</code> has a frequency spectrum derived from a
collection of Dickens' works downloaded from the Gutenberg archive
(A Christmas Carol, David Copperfield, Dombey and Son, Great
Expectations, Hard Times, Master Humphrey's Clock, Nicholas
Nickleby, Oliver Twist, Our Mutual Friend, Sketches by BOZ, A Tale
of Two Cities, The Old Curiosity Shop, The Pickwick Papers, Three
Ghost Stories). <code>Dickens.emp.vgc</code> contains the corresponding
observed vocabulary growth (<code>V</code> and <code>V(1)</code>).
</p>
<p><code>DickensOliverTwist.spc</code> and <code>DickensOliverTwist.emp.vgc</code>
contain spectrum and observed growth curve (<code>V</code> and <code>V(1)</code>
of the early novel Oliver Twist (1837-1839).
</p>
<p><code>DickensGreatExpectations.spc</code> and
<code>DickensGreatExpectations.emp.vgc</code> contain spectrum and
observed growth curve (<code>V</code> and <code>V(1)</code>) of the late novel
Great Expectations (1860-1861).
</p>
<p><code>DickensOurMutualFriend.spc</code> and
<code>DickensOurMutualFriend.emp.vgc</code> contain spectrum and observed
growth curve (<code>V</code> and <code>V(1)</code>) of Our Mutual Friend, the
last novel completed by Dickens (1864-1865).
</p>
<p>Notice that we removed numbers and other forms of non-linguistic
material before collecting the frequency data.
</p>


<h3>References</h3>

<p>Project Gutenberg: <a href="https://www.gutenberg.org/">https://www.gutenberg.org/</a>
</p>
<p>Charles Dickens on Wikipedia: <a href="https://en.wikipedia.org/wiki/Charles_Dickens">https://en.wikipedia.org/wiki/Charles_Dickens</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  data(Dickens.spc)
  summary(Dickens.spc)

  data(Dickens.emp.vgc)
  summary(Dickens.emp.vgc)

  data(DickensOliverTwist.spc)
  summary(DickensOliverTwist.spc)

  data(DickensOliverTwist.emp.vgc)
  summary(DickensOliverTwist.emp.vgc)

</code></pre>

<hr>
<h2 id='estimate.model'>Estimate LNRE Model Parameters (zipfR)</h2><span id='topic+estimate.model'></span>

<h3>Description</h3>

<p><b>Internal function:</b> Generic method for estimation of LNRE model
parameters.  Based on the class of its first argument, the method
dispatches to a suitable implementation of the estimation procedure.
</p>
<p>Unless you are a developer working on the <code>zipfR</code> source code,
you are probably looking for the <code><a href="#topic+lnre">lnre</a></code> manpage.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  estimate.model(model, spc, param.names,
                 method, cost.function, m.max=15,
                 runs=3, debug=FALSE, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate.model_+3A_model">model</code></td>
<td>
<p>LNRE model object of the appropriate class (a subclass of
<code>lnre</code>).  All parameters of the LNRE model that are not listed
in <code>param.names</code> must have been initialized to their
prespecified values in the <code>model</code> object.</p>
</td></tr>
<tr><td><code id="estimate.model_+3A_spc">spc</code></td>
<td>
<p>an observed frequency spectrum, i.e. an object of class
<code>spc</code>.  The values of the missing parameters will be estimated
from this frequency spectrum.</p>
</td></tr>
<tr><td><code id="estimate.model_+3A_param.names">param.names</code></td>
<td>
<p>a character vector giving the names of parameters
for which values have to be estimated (&quot;missing&quot; parameters)</p>
</td></tr>
<tr><td><code id="estimate.model_+3A_method">method</code></td>
<td>
<p>name of the minimization algorithm used for parameter
estimation (see <code><a href="#topic+lnre">lnre</a></code> for details)</p>
</td></tr>
<tr><td><code id="estimate.model_+3A_cost.function">cost.function</code></td>
<td>
<p>cost function to be minimized (see
<code><a href="#topic+lnre">lnre</a></code> for details).  <b>NB:</b> this is a direct
reference to the function object rather than just the name of the
cost function.  Look-up of the appropriate cost function
implementation is performed in the <code>lnre</code> constructor.</p>
</td></tr>
<tr><td><code id="estimate.model_+3A_m.max">m.max</code></td>
<td>
<p>number of spectrum elements that will be used to compute
the cost function (passed on to <code>cost.function</code>)</p>
</td></tr>
<tr><td><code id="estimate.model_+3A_runs">runs</code></td>
<td>
<p>number of parameter optimization runs with random
initialization. Parameters from the run that achieves the smallest
value of the cost function will be selected. Some method implementations
may not support multiple optimization runs.</p>
</td></tr>
<tr><td><code id="estimate.model_+3A_debug">debug</code></td>
<td>
<p>if <code>TRUE</code>, some debugging and progress information
will be printed during the estimation procedure</p>
</td></tr>
<tr><td><code id="estimate.model_+3A_...">...</code></td>
<td>
<p>additional arguments are passed on and may be used by some
implementations</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default, <code>estimate.model</code> dispatches to a generic
implementation of the estimation procedure that can be used with all
types of LNRE models (<code>estimate.model.lnre</code>). 
</p>
<p>This generic implementation can be overridden for specific LNRE
models, e.g. to calculate better init values or improve the estimation
procedure in some other way.  To provide a custom implementation for
Zipf-Mandelbrot models (of class <code>lnre.zm</code>), for instance, it is
sufficient to define the corresponding method implementation
<code>estimate.model.lnre.zm</code>.  If no custom implementation is
provided but the user has selected the <code>Custom</code> method (which is
the default), <code>estimate.model</code> falls back on <code>Nelder-Mead</code>
for multi-dimensional minimization and <code>NLM</code> for one-dimensional
minimization (where Nelder-Mead is considered to be unreliable).
</p>
<p>Parmeter estimation is performed by minimization of the cost function
passed in the <code>cost.function</code> argument (see <code><a href="#topic+lnre">lnre</a></code>
for details).  Depending on the <code>method</code> argument, a range of
different minimization algorithms can be used (see <code><a href="#topic+lnre">lnre</a></code>
for a complete listing).  The minimization algorithm always operates
on <em>transformed</em> parameter values, making use of the
<code>transform</code> utility provided by LNRE models (see
<code><a href="#topic+lnre.details">lnre.details</a></code> for more information about utility
functions).  All parameters are initialized to 0 in the transformed
scale, which should translate to sensible starting points.
</p>
<p>Note that the <code>estimate.model</code> implementations <em>do not
perform any error checking</em>.  It is the responsibility of the caller
to make sure that the arguments are sensible and complete.  In
particular, all model parameters that will not be estimated (i.e. are
not listed in <code>param.names</code>) must have been initialized to
their prespecified values in the <code>model</code> passed to the function.
</p>


<h3>Value</h3>

<p>A modified version of <code>model</code>, where the missing parameters
listed in <code>param.names</code> have been estimated from the observed
frequency spectrum <code>spc</code>.  In addition, goodness-of-fit
information is added to the object.
</p>


<h3>See Also</h3>

<p>The user-level function for estimating LNRE models is
<code><a href="#topic+lnre">lnre</a></code>.  Its manpage also lists available cost functions
and minimization algorithms.
</p>
<p>The internal structure of <code>lnre</code> objects (representing LNRE
models) is described on the <code><a href="#topic+lnre.details">lnre.details</a></code> manpage, which
also outlines the necessary steps for implementing a new LNRE model.
</p>
<p>The minimization algorithms used are described in detail on the
<code><a href="stats.html#topic+nlm">nlm</a></code> and <code><a href="stats.html#topic+optim">optim</a></code> manpages from <span class="rlang"><b>R</b></span>'s standard
library.
</p>

<hr>
<h2 id='EV-EVm'>Expected Frequency Spectrum (zipfR)</h2><span id='topic+EV'></span><span id='topic+EVm'></span>

<h3>Description</h3>

<p><code>EV</code> and <code>EVm</code> are generic methods for computing the
expected vocabulary size <code class="reqn">E[V]</code> and frequency spectrum
<code class="reqn">E[V_m]</code> according to a LNRE model (i.e. an object belonging to a
subclass of <code>lnre</code>).
</p>
<p>When applied to a frequency spectrum (i.e. an object of class
<code>spc</code>), these methods perform binomial interpolation (see
<code><a href="#topic+EV.spc">EV.spc</a></code> for details), although <code><a href="#topic+spc.interp">spc.interp</a></code>
and <code><a href="#topic+vgc.interp">vgc.interp</a></code> might be more convenient binomial
interpolation functions for most purposes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  EV(obj, N, ...)
  EVm(obj, m, N, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EV-EVm_+3A_obj">obj</code></td>
<td>
<p>an LNRE model (i.e. an object belonging to a subclass of
<code>lnre</code>) or frequency spectrum (i.e. an object of class
<code>spc</code>)</p>
</td></tr>
<tr><td><code id="EV-EVm_+3A_m">m</code></td>
<td>
<p>positive integer value determining the frequency class
<code class="reqn">m</code> to be returned (or a vector of such values)</p>
</td></tr>
<tr><td><code id="EV-EVm_+3A_n">N</code></td>
<td>
<p>sample size <code class="reqn">N</code> for which the expected vocabulary size
and frequency spectrum are calculated (or a vector of sample sizes)</p>
</td></tr>
<tr><td><code id="EV-EVm_+3A_...">...</code></td>
<td>
<p>additional arguments passed on to the method implementation
(see respective manpages for details)</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>EV</code> returns the expected vocabulary size <code class="reqn">E[V(N)]</code> in a
sample of <code class="reqn">N</code> tokens, and <code>EVm</code> returns the expected spectrum
elements <code class="reqn">E[V_m(N)]</code>, according to the LNRE model given by
<code>obj</code> (or according to binomial interpolation).
</p>


<h3>See Also</h3>




<p>See <code><a href="#topic+lnre">lnre</a></code> for more information on LNRE models, a listing
of available models, and methods for parameter estimation.
</p>
<p>The variances of the random variables <code class="reqn">V(N)</code> and <code class="reqn">V_m(N)</code> can
be computed with the methods <code><a href="#topic+VV">VV</a></code> and <code><a href="#topic+VVm">VVm</a></code>.


</p>
<p>See <code><a href="#topic+EV.spc">EV.spc</a></code> and <code><a href="#topic+EVm.spc">EVm.spc</a></code> for more
information about the usage of these methods to perform binomial
interpolation (but consider using <code><a href="#topic+spc.interp">spc.interp</a></code> and
<code><a href="#topic+vgc.interp">vgc.interp</a></code> instead).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## see lnre() documentation for examples

</code></pre>

<hr>
<h2 id='EV-EVm.spc'>Binomial Interpolation (zipfR)</h2><span id='topic+EV.spc'></span><span id='topic+EVm.spc'></span>

<h3>Description</h3>

<p>Compute the expected vocabulary size <code class="reqn">E[V(N)]</code> (with function
<code>EV.spc</code>) or expected frequency spectrum <code class="reqn">E[V_m(N)]</code> (with
function <code>EVm.spc</code>) for a random sample of size <code class="reqn">N</code> from a
given frequency spectrum (i.e., an object of class <code>spc</code>).  The
expectations are calculated by binomial interpolation (following
Baayen 2001, pp. 64-69).
</p>
<p>Note that these functions are not user-visible.  They can be called
implicitly through the generic methods <code>EV</code> and <code>EVm</code>,
applied to an object of type <code>spc</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  ## S3 method for class 'spc'
EV(obj, N, allow.extrapolation=FALSE, ...)

  ## S3 method for class 'spc'
EVm(obj, m, N, allow.extrapolation=FALSE, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="EV-EVm.spc_+3A_obj">obj</code></td>
<td>
<p>an object of class <code>spc</code>, representing a frequency
spectrum</p>
</td></tr>
<tr><td><code id="EV-EVm.spc_+3A_m">m</code></td>
<td>
<p>positive integer value determining the frequency class
<code class="reqn">m</code> for which <code class="reqn">E[V_m(N)]</code> be returned (or a vector of such
values)</p>
</td></tr>
<tr><td><code id="EV-EVm.spc_+3A_n">N</code></td>
<td>
<p>sample size <code class="reqn">N</code> for which the expected vocabulary size
or frequency spectrum are calculated (or a vector of sample sizes)</p>
</td></tr>
<tr><td><code id="EV-EVm.spc_+3A_allow.extrapolation">allow.extrapolation</code></td>
<td>
<p>if <code>TRUE</code>, the requested sample size
<code class="reqn">N</code> may be larger than the sample size of the frequency spectrum
<code>obj</code>, for binomial <em>extrapolation</em>.  This obtion should
be used with great caution (see &quot;Details&quot; below).</p>
</td></tr>
<tr><td><code id="EV-EVm.spc_+3A_...">...</code></td>
<td>
<p>additional arguments passed on from generic methods will be
ignored</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions are naive implementations of binomial interpolation,
using Equations (2.41) and (2.43) from Baayen (2001).  No guarantees
are made concerning their numerical accuracy, especially for extreme
values of <code class="reqn">m</code> and <code class="reqn">N</code>.
</p>
<p>According to Baayen (2001), pp. 69-73., the same equations can also be
used for binomial <em>extrapolation</em> of a given frequency spectrum
to larger sample sizes.  However, they become numerically unstable in
this case and will typically break down when extrapolating to more
than twice the size of the observed sample (Baayen 2001, p. 75).
Therefore, extrapolation has to be enabled explicitly with the option
<code>allow.extrapolation=TRUE</code> and should be used with great caution.
</p>


<h3>Value</h3>

<p><code>EV</code> returns the expected vocabulary size <code class="reqn">E[V(N)]</code> for a
random sample of <code class="reqn">N</code> tokens from the frequency spectrum
<code>obj</code>, and <code>EVm</code> returns the expected spectrum elements
<code class="reqn">E[V_m(N)]</code> for a random sample of <code class="reqn">N</code> tokens from <code>obj</code>,
calculated by binomial interpolation.
</p>


<h3>References</h3>

<p>Baayen, R. Harald (2001). <em>Word Frequency Distributions.</em>
Kluwer, Dordrecht.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+EV">EV</a></code> and <code><a href="#topic+EVm">EVm</a></code> for the generic methods and
links to other implementations
</p>
<p><code><a href="#topic+spc.interp">spc.interp</a></code> and <code><a href="#topic+vgc.interp">vgc.interp</a></code> are convenience
functions that compute an expected frequency spectrum or vocabulary
growth curve by binomial interpolation
</p>

<hr>
<h2 id='EvertLuedeling2001'>Samples of German Word Formation Affixes (zipfR)</h2><span id='topic+EvertLuedeling2001'></span>

<h3>Description</h3>

<p>Corpus data for measuring the productivity of German word formation affixes
<em>-bar</em>, <em>-lich</em>, <em>-sam</em>, <em>-ös</em>, <em>-tum</em>,
<em>Klein-</em>, <em>-chen</em> and <em>-lein</em> (Evert &amp; Lüdeling 2001).
Data were extracted from two volumes of the German daily newspaper
<em>Stuttgarter Zeitung</em>, then manually cleaned and normalized.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
EvertLuedeling2001

</code></pre>


<h3>Format</h3>

<p>A list of 8 character vectors for the different affixes, with names
<code>klein</code> (<em>Klein-</em>), <code>bar</code> (<em>-bar</em>),
<code>chen</code> (<em>-chen</em>), <code>lein</code> (<em>-lein</em>),
<code>lich</code> (<em>-lich</em>), <code>oes</code> (<em>-ös</em>),
<code>sam</code> (<em>-sam</em>), <code>tum</code> (<em>-tum</em>).
</p>
<p>Each vector contains all relevant tokens from the corpus in their
original (chronological) ordering, so vocabulary growth curves can
be determined from the vectors in addition to type frequency lists
and frequency spectra.
</p>


<h3>References</h3>

<p>Evert, Stefan and Lüdeling, Anke (2001).
Measuring morphological productivity: Is automatic preprocessing sufficient?
In <em>Proceedings of the Corpus Linguistics 2001 Conference</em>, pages 167&ndash;175, Lancaster, UK.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
str(EvertLuedeling2001)

# tokens and type counts for the different affixes
sapply(EvertLuedeling2001, function (x) {
  y &lt;- vec2tfl(x)
  c(N=N(y), V=V(y))
})

</code></pre>

<hr>
<h2 id='ItaPref'>Italian Ri- and Ultra- Prefix Frequency Data (zipfR)</h2><span id='topic+ItaPref'></span><span id='topic+ItaRi'></span><span id='topic+ItaRi.spc'></span><span id='topic+ItaRi.emp.vgc'></span><span id='topic+ItaUltra'></span><span id='topic+ItaUltra.spc'></span><span id='topic+ItaUltra.emp.vgc'></span>

<h3>Description</h3>

<p><code>ItaRi.spc</code> and <code>ItaRi.emp.vgc</code> are <code><a href="#topic+zipfR">zipfR</a></code>
objects of classes <code><a href="#topic+tfl">tfl</a></code>, <code><a href="#topic+spc">spc</a></code> and
<code><a href="#topic+vgc">vgc</a></code>, respectively. They contain frequency data for all
verbal lemmas with the prefix ri- (similar to English re-) in the
Italian la Repubblica corpus.
</p>
<p><code>ItaUltra.spc</code> and <code>ItaUltra.emp.vgc</code> contain the same
kinds of data for the adjectival prefix ultra-.
</p>


<h3>Details</h3>

<p><code>ItaRi.emp.vgc</code> and <code>ItaUltra.emp.vgc</code> are
<em>empirical</em> vocabulary growth curves, reflecting the <code>V</code>
and <code>V(1)</code> development in the non-randomized corpus.
</p>
<p>The data were manually checked, as described for ri- in Baroni (to
appear).
</p>


<h3>References</h3>

<p>Baroni, M. (to appear) I sensi di ri-: Un'indagine
preliminare. In Maschi, R., Penello, N. and Rizzolatti,
P. (eds.), <em>Miscellanea di studi linguistici offerti a
Laura Vanelli</em>. Udine, Forum.
</p>
<p>la Repubblica corpus: <a href="http://sslmit.unibo.it/repubblica/">http://sslmit.unibo.it/repubblica/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  data(ItaRi.spc)
  summary(ItaRi.spc)

  data(ItaRi.emp.vgc)
  summary(ItaRi.emp.vgc)

  data(ItaUltra.spc)
  summary(ItaUltra.spc)

  data(ItaUltra.emp.vgc)
  summary(ItaUltra.emp.vgc)

</code></pre>

<hr>
<h2 id='lnre'>LNRE Models (zipfR)</h2><span id='topic+lnre'></span><span id='topic+lnre.object'></span>

<h3>Description</h3>

<p>LNRE model constructor, returns an object representing a LNRE model
with the specified parameters, or allows parameters to be estimated
automatically from an observed frequency spectrum.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  lnre(type=c("zm", "fzm", "gigp"),
       spc=NULL, debug=FALSE,
       cost=c("gof", "chisq", "linear", "smooth.linear", "mse", "exact"),
       m.max=15, runs=5,
       method=c("Nelder-Mead", "NLM", "BFGS", "SANN", "Custom"),
       exact=TRUE, sampling=c("Poisson", "multinomial"),
       bootstrap=0, verbose=TRUE, parallel=1L,
       ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lnre_+3A_type">type</code></td>
<td>
<p>class of LNRE model to use (see &quot;LNRE Models&quot; below)</p>
</td></tr>
<tr><td><code id="lnre_+3A_spc">spc</code></td>
<td>
<p>observed frequency spectrum used to estimate model
parameters</p>
</td></tr>
<tr><td><code id="lnre_+3A_debug">debug</code></td>
<td>
<p>if <code>TRUE</code>, detailed debugging information will be
printed during parameter estimation</p>
</td></tr>
<tr><td><code id="lnre_+3A_cost">cost</code></td>
<td>
<p>cost function for measuring the &quot;distance&quot; between
observed and expected vocabulary size and frequency spectrum.
Parameters are estimated by minimizing this cost function (see &quot;Cost
Functions&quot; below for a list of built-in cost functions and details
on user-defined cost functions).</p>
</td></tr>
<tr><td><code id="lnre_+3A_m.max">m.max</code></td>
<td>
<p>number of spectrum elements considered by the cost
function (see &quot;Cost Functions&quot; below for more information). If
unspecified, the default is automatically adjusted to avoid small
spectrum elements that may be mathematically unreliable.</p>
</td></tr>
<tr><td><code id="lnre_+3A_runs">runs</code></td>
<td>
<p>number of parameter optimization runs with random
initialization. Parameters from the run that achieves the smallest
value of the cost function will be selected. Currently not supported
for <code>method="Custom"</code>, please use <em>runs=1</em> in this case.</p>
</td></tr>
<tr><td><code id="lnre_+3A_method">method</code></td>
<td>
<p>algorithm used for parameter estimation, by minimizing
the value of the cost function (see &quot;Parameter Estimation&quot; below for
details, and &quot;Minimization Algorithms&quot; for descriptions of the
available algorithms)</p>
</td></tr>
<tr><td><code id="lnre_+3A_exact">exact</code></td>
<td>
<p>if <code>FALSE</code>, certain LNRE models will be allowed to
use approximations when calculating expected values and variances,
in order to improve performance and numerical stability.  However,
the computed values might be inaccurate or inconsistent in &quot;extreme&quot;
situations: in particular, <code class="reqn">E[V]</code> might be larger than <code class="reqn">N</code>
when <code class="reqn">N</code> is very small; <code class="reqn">\sum_m E[V_m]</code> can be larger than
<code class="reqn">E[V]</code> at the same <code class="reqn">N</code>; <code class="reqn">\sum_m m \cdot E[V_m]</code> can be larger than <code class="reqn">N</code></p>
</td></tr>
<tr><td><code id="lnre_+3A_sampling">sampling</code></td>
<td>
<p>type of random sampling model to use.  <code>Poisson</code>
sampling is mathematically simpler and allows fast and robust
calculations, while <code>multinomial</code> sampling is more accurate
especially for very small samples.  <code>Poisson</code> sampling is the
default and should be unproblematic for sample sizes <code class="reqn">N \ge
    10000</code>.  <b>NB:</b> The <code>multinomial</code> sampling option has not
been implemented yet.</p>
</td></tr>
<tr><td><code id="lnre_+3A_bootstrap">bootstrap</code></td>
<td>
<p>number of bootstrap samples used to estimate confidence
intervals for estimated model parameters.  Recommended values are
<code>bootstrap=100</code> or <code>bootstrap=200</code>.  Bootstrapping can be very
time-consuming and should not be used if the underlying sample size
is very large (roughly, more than 1 million tokens).  See
<code><a href="#topic+lnre.bootstrap">lnre.bootstrap</a></code> for further information and warnings.</p>
</td></tr>
<tr><td><code id="lnre_+3A_parallel">parallel</code></td>
<td>
<p>whether to use parallelisation for the bootstrapping procedure
(highly recommended). See <code><a href="#topic+lnre.bootstrap">lnre.bootstrap</a></code> for details.</p>
</td></tr>
<tr><td><code id="lnre_+3A_verbose">verbose</code></td>
<td>
<p>if <code>TRUE</code>, a progress bar will be shown in the R console
during the bootstrapping procedure</p>
</td></tr>
<tr><td><code id="lnre_+3A_...">...</code></td>
<td>
<p>all further named arguments are interpreted as parameter
values for the chosen LNRE model (see the respective manpages for
names and descriptions of the model parameters)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Currently, the following LNRE models are supported by the <code>zipfR</code>
package:
</p>
<p>The <b>Zipf-Mandelbrot (ZM)</b> LNRE model (see <code><a href="#topic+lnre.zm">lnre.zm</a></code>
for details).
</p>
<p>The <b>finite Zipf-Mandelbrot (fZM)</b> LNRE model (see
<code><a href="#topic+lnre.fzm">lnre.fzm</a></code> for details).
</p>
<p>The <b>Generalized Inverse Gauss-Poisson (GIGP)</b> LNRE model (see
<code><a href="#topic+lnre.gigp">lnre.gigp</a></code> for details).
</p>
<p>If explicit model parameters are specified in addition to an observed
frequency spectrum <code>spc</code>, these parameters are fixed to the given
values and are excluded from the estimation procedure.  This feature
can be useful if fully automatic parameter estimation leads to a poor
or counterintuitive fit.
</p>


<h3>Value</h3>

<p>An object of a suitable subclass of <code>lnre</code>, depending on the
<code>type</code> argument (e.g. <code>lnre.fzm</code> for <code>type="fzm"</code>).
This object represents a LNRE model of the selected type with the
specified parameter values, or with parameter values estimated from
the observed frequency spectrum <code>spc</code>.
</p>
<p>The internal structure of <code>lnre</code> objects is described on the
<code><a href="#topic+lnre.details">lnre.details</a></code> manpage (intended for developers).
</p>


<h3>Parameter Estimation</h3>

<p>Automatic parameter estimation for LNRE models is performed by
matching the expected vocabulary size and frequency spectrum of the
model against the observed data passed in the <code>spc</code> argument.
</p>
<p>For this purpose, a <em>cost function</em> has to be defined as a
measure of the &quot;distance&quot; between observed and expected frequency
spectrum.  Parameters are then estimated by applying a
<em>minimization algorithm</em> in order to find those parameter values
that lead to the smallest possible cost.
</p>
<p>Parameter estimation is a crucial and often also quite critical step
in the application of LNRE models.  Depending on the shape of the
observed frequency spectrum, the automatic estimation procedure may
result in a poor and counter-intuitive fit, or may fail altogether.
</p>
<p>Usually, multiple runs of the minimization are performed with different
random start values. An error will only be reported if all the estimation
runs fail. Such multiple runs have not been implemented for the <code>Custom</code>
minimization method yet; please specify <code>runs=1</code> in this case.
</p>
<p>Users can influence parameter estimation by choosing from a range of
predefined cost functions and from several minimization algorithms, as
described in the following sections.  Some experimentation with the
<code>cost</code>, <code>m.max</code> and <code>method</code> arguments will often help
to resolve estimation failures and may result in a considerably better
goodness-of-fit.
</p>


<h3>Cost Functions</h3>

<p>The following cost functions are available and can be selected with
the <code>cost</code> argument.  All functions are based on the differences
between observed and expected values for vocabulary size and the first
elements of the frequency spectrum (<code class="reqn">V_1, \ldots, V_m</code>, where
<code class="reqn">m</code> is given by the <code>m.max</code> argument):
</p>

<dl>
<dt><code>gof</code>:</dt><dd><p>the multivariate chi-squared statistic used for
goodness-of-fit testing (<code><a href="#topic+lnre.goodness.of.fit">lnre.goodness.of.fit</a></code>).
This cost function corresponds (almost) to maximum-likelihood
parameter estimation and is used by default.</p>
</dd>
<dt><code>chisq</code>:</dt><dd><p>cost function based on a simplified version of
the multivariate chi-squared test for goodness-of-fit (assuming
independence between the random variables <code class="reqn">V_m</code>).

</p>
</dd>
<dt><code>linear</code>:</dt><dd><p>linear cost function, which sums over the
absolute differences between observed and expected values.  This
cost function puts more weight on fitting the vocabulary size and
the first few elements of the frequency spectrum (where absolute
differences are much larger than for higher spectrum elements).</p>
</dd>
<dt><code>smooth.linear</code>:</dt><dd><p>modified version of the linear cost
function, which smoothes the kink of the absolute value function
for a difference of <code class="reqn">0</code> (since non-differentiable cost
functions might be problematic for gradient-base minimization
algorithms)</p>
</dd>
<dt><code>mse</code>:</dt><dd><p>mean squared error cost function, averaging over
the squares of differences between observed and expected values.
This cost function penalizes large absolute differences more
heavily than linear cost (and therefore puts even greater weight
on fitting vocabulary size and the first spectrum elements).</p>
</dd>
<dt><code>exact</code>:</dt><dd><p>this &quot;virtual&quot; cost function attempts to match
the observed vocabulary size and first spectrum elements exactly,
ignoring differences for all higher spectrum elements.  This is
achieved by adjusting the value of <code>m.max</code> automatically,
depending on the number of free parameters that are estimated (in
general, the number of constraints that can be satisfied by
estimating parameters is the same as the number of free
parameters).  Having adjusted <code>m.max</code>, the <code>mse</code> cost
function is used to determined parameter values, so that the
estimation procedure will not fail even if the constraints cannot
be matched exactly.</p>
</dd>
</dl>

<p>Alternatively a user-defined cost function can be passed as a function object
with signature 'cost(model, spc, m.max)', which compares the LNRE model 'model'
against the observed frequency spectrum 'spc' and returns a cost value
(i.e. lower cost indicates a better fit).
</p>


<h3>Minimization Algorithms</h3>

<p>Several different minimization algorithms can be used for parmeter
estimation and are selected with the <code>method</code> argument:
</p>

<dl>
<dt><code>Nelder-Mead</code>:</dt><dd><p>the Nelder-Mead algorithm, implemented by
the <code>optim</code> function, performs minimization without using
derivatives.  Parameter estimation is therefore very robust, while
almost as fast and accurate as the <code>NLM</code> method. 
<code>Nelder-Mead</code> is the default algorithm and is also used
internally by most custom minimization procedures (see below).</p>
</dd>
<dt><code>NLM</code>:</dt><dd><p>a standard Newton-type algorithm for nonlinear
minimization, implemented by the <code><a href="stats.html#topic+nlm">nlm</a></code> function, which
makes use of numerical derivatives of the cost function.
<code>NLM</code> minimization converges quickly and obtains very precise
parameter estimates (for a local minimum of the cost function),
but it is not very stable and may cause parameter estimation to
fail altogether.</p>
</dd>
<dt><code>SANN</code>:</dt><dd><p>minimization by simulated annealing, also provided by the
<code>optim</code> function.  Like <code>Nelder-Mead</code>, this algorithm is
very robust because it avoids numerical derivatives, but
convergence is extremely slow.  In some cases, <code>SANN</code> might
produce a better fit than <code>Nelder-Mead</code> (if the latter
converges to a suboptimal local minimum).</p>
</dd>
<dt><code>BFGS</code>:</dt><dd><p>a quasi-Newton method developed by Broyden, Fletcher,
Goldfarb and Shanno. This minimization algorithm is efficient, but
should be applied with care as it will often overshoot the valid range of
parameter values.</p>
</dd>
<dt><code>Custom</code>:</dt><dd><p>a custom estimation procedure provided 
for certain types of LNRE model, which may exploit special
mathematical properties of the model in order to calculate one or
more of the parameter values directly.  For example, one parameter
of the ZM and fZM models can easily be determined from the
constraint <code class="reqn">E[V] = V</code> (but note that this additional
constraint leads to a different fit than is obtained by plain
minimization of the cost function!).  Custom estimation might also
apply special configuration settings to improve convergence of the
minimization process, based on knowledge about the valid ranges
and &quot;behaviour&quot; of model parameters.  If no custom estimation
procedure has been implemented for the selected LNRE model,
<code>lnre</code> falls back on the <code>Nelder-Mead</code> or <code>NLM</code>
algorithm.</p>
</dd>
</dl>

<p>See the <code><a href="stats.html#topic+nlm">nlm</a></code> and <code><a href="stats.html#topic+optim">optim</a></code> manpages for more
information about the minimization algorithms used and key references.
</p>


<h3>See Also</h3>

<p>Detailed descriptions of the different LNRE models provided by
<code>zipfR</code> and their parameters can be found on the manpages
<code><a href="#topic+lnre.zm">lnre.zm</a></code>, <code><a href="#topic+lnre.fzm">lnre.fzm</a></code> and
<code><a href="#topic+lnre.gigp">lnre.gigp</a></code>.
</p>
<p>Useful methods for trained models are <code><a href="#topic+lnre.spc">lnre.spc</a></code>,
<code><a href="#topic+lnre.vgc">lnre.vgc</a></code>, <code><a href="#topic+EV">EV</a></code>, <code><a href="#topic+EVm">EVm</a></code>,
<code><a href="#topic+VV">VV</a></code>, <code><a href="#topic+VVm">VVm</a></code>.  Suitable implementations of the
<code><a href="base.html#topic+print">print</a></code> and <code><a href="base.html#topic+summary">summary</a></code> methods are also
provided (see <code><a href="#topic+print.lnre">print.lnre</a></code> for details), as well as for
plotting (see <code><a href="#topic+plot.lnre">plot.lnre</a></code>).  Note that the
methods <code><a href="#topic+N">N</a></code>, <code><a href="#topic+V">V</a></code> and <code><a href="#topic+Vm">Vm</a></code> can be
applied to LNRE models with estimated parameters and return
information about the observed frequency spectrum used for parameter
estimation.
</p>
<p>If bootstrapping samples have been generated (<code>bootstrap &gt; 0</code>), 
confidence intervals for the model parameters can be determined with
<code><a href="#topic+confint.lnre">confint.lnre</a></code>.  See <code><a href="#topic+lnre.bootstrap">lnre.bootstrap</a></code> for 
more information on the bootstrapping procedure and implementation.
</p>
<p>The <code><a href="#topic+lnre.details">lnre.details</a></code> manpage gives details about the
implementation of LNRE models and the internal structure of
<code>lnre</code> objects, while <code><a href="#topic+estimate.model">estimate.model</a></code> has more
information on the parameter estimation procedure (both manpages are
intended for developers).
</p>
<p>See <code><a href="#topic+lnre.goodness.of.fit">lnre.goodness.of.fit</a></code> for a complete description of
the goodness-of-fit test that is automatically performed after
parameter estimation (and which is reported in the <code>summary</code> of
the LNRE model).  This function can also be used to evaluate the
predictions of the LNRE model on a different data set than the one
used for parameter estimation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load Dickens dataset
data(Dickens.spc)

## estimate parameters of GIGP model and show summary
m &lt;- lnre("gigp", Dickens.spc)
m


## N, V and V1 of spectrum used to compute model
## (should be the same as for Dickens.spc)
N(m)
V(m)
Vm(m,1)


## expected V and V_m and their variances for arbitrary N 
EV(m,100e6)
VV(m,100e6)
EVm(m,1,100e6)
VVm(m,1,100e6)

## use only 10 instead of 15 spectrum elements to estimate model
## (note how fit improves for V and V1)
m.10 &lt;- lnre("gigp", Dickens.spc, m.max=10)
m.10

## experiment with different cost functions
m.mse &lt;- lnre("gigp", Dickens.spc, cost="mse")
m.mse
m.exact &lt;- lnre("gigp", Dickens.spc, cost="exact")
m.exact


## NLM minimization algorithm is faster but less robust
m.nlm &lt;- lnre("gigp", Dickens.spc, method="NLM")
m.nlm

## ZM and fZM LNRE models have special estimation algorithms
m.zm &lt;- lnre("zm", Dickens.spc)
m.zm
m.fzm &lt;- lnre("fzm", Dickens.spc)
m.fzm


## estimation is much faster if approximations are allowed
m.approx &lt;- lnre("fzm", Dickens.spc, exact=FALSE)
m.approx


## specify parameters of LNRE models directly
m &lt;- lnre("zm", alpha=.5, B=.01)
lnre.spc(m, N=1000, m.max=10)

m &lt;- lnre("fzm", alpha=.5, A=1e-6, B=.01)
lnre.spc(m, N=1000, m.max=10)

m &lt;- lnre("gigp", gamma=-.5, B=.01, C=.01)
lnre.spc(m, N=1000, m.max=10)

## bootstrapped confidence intervals for model parameters
## Not run: 
model &lt;- lnre("fzm", spc=BrownAdj.spc, bootstrap=40)
confint(model, "alpha") # Zipf slope
confint(model, "S")     # population diversity
confint(model, "S", method="normal") # Gaussian approx works well in this case

## speed up with parallelisation (see ?lnre.bootstrap for more information)
model &lt;- lnre("fzm", spc=BrownAdj.spc, bootstrap=40, 
              parallel=8) # on Linux / MacOS with 8 available cores
## End(Not run)
</code></pre>

<hr>
<h2 id='LNRE'>Type and Probability Distributions of LNRE Models (zipfR)</h2><span id='topic+LNRE'></span><span id='topic+rlnre'></span><span id='topic+dlnre'></span><span id='topic+plnre'></span><span id='topic+qlnre'></span><span id='topic+tdlnre'></span><span id='topic+tplnre'></span><span id='topic+tqlnre'></span><span id='topic+ldlnre'></span><span id='topic+ltdlnre'></span>

<h3>Description</h3>

<p>Type density <code class="reqn">g</code> (<code>tdlnre</code>), type distribution <code class="reqn">G</code>
(<code>tplnre</code>), type quantiles <code class="reqn">G^{-1}</code> (<code>tqlnre</code>),
probability density <code class="reqn">f</code> (<code>dlnre</code>), distribution function
<code class="reqn">F</code> (<code>plnre</code>), quantile function <code class="reqn">F^{-1}</code> (<code>qlnre</code>),
logarithmic type and probability densities (<code>ltdlnre</code> and
<code>ldlnre</code>), and random sample generation (<code>rlnre</code>) for LNRE
models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  tdlnre(model, x, ...)
  tplnre(model, q, lower.tail=FALSE, ...)
  tqlnre(model, p, lower.tail=FALSE, ...)

  dlnre(model, x, ...)
  plnre(model, q, lower.tail=TRUE, ...)
  qlnre(model, p, lower.tail=TRUE, ...)

  ltdlnre(model, x, base=10, log.x=FALSE, ...)
  ldlnre(model, x, base=10, log.x=FALSE, ...)

  rlnre(model, n, what=c("tokens", "tfl"), ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LNRE_+3A_model">model</code></td>
<td>
<p>an object belonging to a subclass of <code>lnre</code>,
representing a LNRE model</p>
</td></tr>
<tr><td><code id="LNRE_+3A_x">x</code></td>
<td>
<p>vector of type probabilities <code class="reqn">pi</code> for which the density
function is evaluated</p>
</td></tr>
<tr><td><code id="LNRE_+3A_q">q</code></td>
<td>
<p>vector of type probability quantiles, i.e. threshold values
<code class="reqn">\rho</code> on the type probability axis</p>
</td></tr>
<tr><td><code id="LNRE_+3A_p">p</code></td>
<td>
<p>vector of tail probabilities</p>
</td></tr>
<tr><td><code id="LNRE_+3A_lower.tail">lower.tail</code></td>
<td>
<p>if <code>TRUE</code>, lower tail probabilities or type
counts are returned / expected in the <code>p</code> argument.  Note that
the defaults differ for distribution function and type distribution,
and see &quot;Details&quot; below.</p>
</td></tr>
<tr><td><code id="LNRE_+3A_base">base</code></td>
<td>
<p>positive number, the base with respect to which the
log-transformation is peformed (see &quot;Details&quot; below)</p>
</td></tr>
<tr><td><code id="LNRE_+3A_log.x">log.x</code></td>
<td>
<p>if <code>TRUE</code>, the values passed in the argument
<code>x</code> are assumed to be logarithmic, i.e. <code class="reqn">\log_a \pi</code></p>
</td></tr>
<tr><td><code id="LNRE_+3A_n">n</code></td>
<td>
<p>size of random sample to generate.  If <code>length(n) &gt; 1</code>,
the length is taken to be the number required.</p>
</td></tr>
<tr><td><code id="LNRE_+3A_what">what</code></td>
<td>
<p>whether to return the sample as a vector of tokens or as
a type-frequency list (usually more efficient)</p>
</td></tr>
<tr><td><code id="LNRE_+3A_...">...</code></td>
<td>
<p>further arguments are passed through to the method
implementations (currently unused)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that the order in which arguments are specified differs from the
analogous functions for common statistical distributions in the <span class="rlang"><b>R</b></span>
standard library.  In particular, the LNRE model <code>model</code> always
has to be given as the first parameter so that <span class="rlang"><b>R</b></span> can dispatch the
function call to an appropriate method implementation for the chosen
LNRE model.
</p>
<p>Some of the functions may not be available for certain types of LNRE
models.  In particular, no analytical solutions are known for the
distribution and quantiles of GIGP models, so the functions
<code>tplnre</code>, <code>tqlnre</code>, <code>plnre</code>, <code>qlnre</code> and
<code>rlnre</code> (which depends on <code>qlnre</code> and <code>tplnre</code>) are not
implemented for objects of class <code>lnre.gigp</code>. 
</p>
<p>The default tails differ for the distribution function (<code>plnre</code>,
<code>qlnre</code>) and the type distribution (<code>tplnre</code>,
<code>tqlnre</code>), in order to match the definitions of <code class="reqn">F(\rho)</code> and
<code class="reqn">G(\rho)</code>.  While the distribution function defaults to lower
tails (<code>lower.tail=TRUE</code>, corresponding to <code class="reqn">F</code> and
<code class="reqn">F^{-1}</code>), the type distribution defaults to upper tails
(<code>lower.tail=FALSE</code>, corresponding to <code class="reqn">G</code> and <code class="reqn">G^{-1}</code>).
</p>
<p>Unlike for standard distriutions, logarithmic tail probabilities
(<code>log.p=TRUE</code>) are not provided for the LNRE models, since here
the focus is usually on the bulk of the distribution rather than on
the extreme tails.
</p>
<p>The log-transformed density functions <code class="reqn">f^*</code> and <code class="reqn">g^*</code>
returned by <code>ldlnre</code> and <code>ltdlnre</code>, respectively, can be
understood as probability and type densities for <code class="reqn">\log_a \pi</code>
instead of <code class="reqn">\pi</code>, and are useful for visualization of LNRE
populations (with a logarithmic scale for the parameter <code class="reqn">\pi</code> on
the x-axis).  For example,
</p>
<p style="text-align: center;"><code class="reqn">
    G(\log_a \rho) = \int_{\log_a \rho}^{0} g^*(t) \,dt
  </code>
</p>



<h3>Value</h3>

<p>For <code>rnlre</code>, either a factor of length <code>n</code> (<code>what="tokens"</code>,
the default) or a <code><a href="#topic+tfl">tfl</a></code> object (<code>what="tfl"</code>), representing
a random sample from the population described by the specified LNRE model.
Note that the type-frequency list is a sufficient statistic, i.e. it provides
all relevant information from the sample. For large <code>n</code>, type-frequency
lists are generated more efficiently and with less memory overhead.
</p>
<p>For all other functions, a vector of non-negative numbers of the same
length as the second argument (<code>x</code>, <code>p</code> or <code>q</code>).
</p>
<p><code>tdlnre</code> returns the type density <code class="reqn">g(\pi)</code> for the values of
<code class="reqn">\pi</code> specified in the vector <code>x</code>.  <code>tplnre</code> returns the
type distribution <code class="reqn">G(\rho)</code> (default) or its complement
<code class="reqn">1-G(\rho)</code> (if <code>lower.tail=TRUE</code>), for the values of
<code class="reqn">\rho</code> specified in the vector <code>q</code>.  <code>tqlnre</code> returns
type quantiles, i.e. the inverse <code class="reqn">G^{-1}(x)</code> (default) or
<code class="reqn">G^{-1}(S-x)</code> (if <code>lower.tail=TRUE</code>) of the type
distribution, for the type counts <code class="reqn">x</code> specified in the vector
<code>p</code>.
</p>
<p><code>dlnre</code> returns the probability density <code class="reqn">f(\pi)</code> for the
values of <code class="reqn">\pi</code> specified in the vector <code>x</code>.  <code>plnre</code>
returns the distribution function <code class="reqn">F(\rho)</code> (default) or its
complement <code class="reqn">1-F(\rho)</code> (if <code>lower.tail=FALSE</code>), for the
values of <code class="reqn">\rho</code> specified in the vector <code>q</code>.  <code>qlnre</code>
returns quantiles, i.e. the inverse <code class="reqn">F^{-1}(p)</code> (default) or
<code class="reqn">F^{-1}(1-p)</code> (if <code>lower.tail=FALSE</code>) of the distribution
function, for the probabilities <code class="reqn">p</code> specified in the vector
<code>p</code>.
</p>
<p><code>ldlnre</code> and <code>ltdlnre</code> compute logarithmically transformed
versions of the probability and type density functions, respectively,
taking logarithms with respect to the base <code class="reqn">a</code> specified in the
<code>base</code> argument (default: <code class="reqn">a=10</code>).  See &quot;Details&quot; above for
more information.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lnre">lnre</a></code> for more information about LNRE models and how to
initialize them.
</p>
<p>Random samples generated with <code>rnlre</code> can be further processed
with the functions <code><a href="#topic+vec2tfl">vec2tfl</a></code>, <code><a href="#topic+vec2spc">vec2spc</a></code> and
<code><a href="#topic+vec2vgc">vec2vgc</a></code> (for token vectors) and <code><a href="#topic+tfl2spc">tfl2spc</a></code> 
(for type-frequency lists).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## define ZM and fZM LNRE models 
ZM &lt;- lnre("zm", alpha=.8, B=1e-3)
FZM &lt;- lnre("fzm", alpha=.8, A=1e-5, B=.05)

## random samples from the two models
vec2tfl(rlnre(ZM, 10000))
vec2tfl(rlnre(FZM, 10000))
rlnre(FZM, 10000, what="tfl") # more efficient

## plot logarithmic type density functions
x &lt;- 10^seq(-6, 1, by=.01)  # pi = 10^(-6) .. 10^(-1)
y.zm &lt;- ltdlnre(ZM, x)
y.fzm &lt;- ltdlnre(FZM, x)

plot(x, y.zm, type="l", lwd=2, col="red", log="x", ylim=c(0,14000))
lines(x, y.fzm, lwd=2, col="blue")
legend("topright", legend=c("ZM", "fZM"), lwd=3, col=c("red", "blue"))

## probability pi_k of k-th type according to FZM model
k &lt;- 10
plnre(FZM, tqlnre(FZM, k-1)) - plnre(FZM, tqlnre(FZM, k))

## number of types with pi &gt;= 1e-6
tplnre(ZM, 1e-6)

## lower tail fails for infinite population size
## Not run: 
tplnre(ZM, 1e-3, lower=TRUE)
## End(Not run)

## total probability mass assigned to types with pi &lt;= 1e-6
plnre(ZM, 1e-6)
</code></pre>

<hr>
<h2 id='LNRE_posterior'>Posterior Distribution of LNRE Model (zipfR)</h2><span id='topic+LNRE_posterior'></span><span id='topic+postdlnre'></span><span id='topic+postldlnre'></span><span id='topic+postplnre'></span><span id='topic+postqlnre'></span>

<h3>Description</h3>

<p>Posterior distribution over the type probability space of a LNRE
model, given the observed frequency <code class="reqn">m</code> in a sample. Posterior
density (<code>postdlnre</code>) and log-transformed density
(<code>postldlnre</code>) can be computed for all LNRE models. The
distribution function (<code>postplnre</code>) and quantiles
(<code>postqlnre</code>) are only available for selected types of models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
postdlnre(model, x, m, N, ...)
postldlnre(model, x, m, N, base=10, log.x=FALSE, ...)
postplnre(model, q, m, N, lower.tail=FALSE, ...)
postqlnre(model, p, m, N, lower.tail=FALSE, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LNRE_posterior_+3A_model">model</code></td>
<td>
<p>an object belonging to a subclass of <code>lnre</code>,
representing an LNRE model</p>
</td></tr>
<tr><td><code id="LNRE_posterior_+3A_m">m</code></td>
<td>
<p>frequency <code class="reqn">m</code> of a type in the observed sample</p>
</td></tr>
<tr><td><code id="LNRE_posterior_+3A_n">N</code></td>
<td>
<p>sample size <code class="reqn">N</code></p>
</td></tr>
<tr><td><code id="LNRE_posterior_+3A_x">x</code></td>
<td>
<p>vector of type probabilities <code class="reqn">pi</code> for which the posterior
density function is evaluated</p>
</td></tr>
<tr><td><code id="LNRE_posterior_+3A_q">q</code></td>
<td>
<p>vector of type probability quantiles, i.e. threshold values
<code class="reqn">\rho</code> on the type probability axis</p>
</td></tr>
<tr><td><code id="LNRE_posterior_+3A_p">p</code></td>
<td>
<p>vector of tail probabilities</p>
</td></tr>
<tr><td><code id="LNRE_posterior_+3A_base">base</code></td>
<td>
<p>positive number, the base <code class="reqn">a</code> with respect to which the
log-transformation is peformed (see &quot;Details&quot; below)</p>
</td></tr>
<tr><td><code id="LNRE_posterior_+3A_log.x">log.x</code></td>
<td>
<p>if <code>TRUE</code>, the values passed in the argument
<code>x</code> are assumed to be logarithmic, i.e. <code class="reqn">\log_a \pi</code></p>
</td></tr>
<tr><td><code id="LNRE_posterior_+3A_lower.tail">lower.tail</code></td>
<td>
<p>if <code>TRUE</code>, lower tail probabilities or type
counts are returned / expected in the <code>p</code> argument.  Note that
the defaults differ for distribution function and type distribution,
and see &quot;Details&quot; below.</p>
</td></tr>
<tr><td><code id="LNRE_posterior_+3A_...">...</code></td>
<td>
<p>further arguments are passed through to the method
implementations (currently unused)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of non-negative numbers of the same length as the second
argument (<code>x</code>, <code>p</code> or <code>q</code>).
</p>
<p><code>postdlnre</code> returns the posterior type density <code class="reqn">P(\pi | f = m)</code>
for the values of <code class="reqn">\pi</code> specified in the vector <code>x</code>.
<code>postplnre</code> computes the posterior type distribution function
<code class="reqn">P(\pi \geq \rho | f = m)</code> (default) or its complement
<code class="reqn">P(\pi \leq \rho | f = m)</code>  (if <code>lower.tail=TRUE</code>).
These correspond to <code class="reqn">E[V_{m, &gt;\rho}]</code> and <code class="reqn">E[V_{m, \rho}]</code>, respectively (Evert 2004, p. 123).
<code>postqlnre</code> returns quantiles, i.e. the inverse of the posterior
type distribution function.
</p>
<p><code>postldlnre</code> computes a logarithmically transformed version of
the posterior type density, taking logarithms with respect to the
base <code class="reqn">a</code> specified in the <code>base</code> argument (default: <code class="reqn">a=10</code>).
Such log-transformed densities are useful for visualizing distributions,
see <code><a href="#topic+ldlnre">ldlnre</a></code> for more information.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lnre">lnre</a></code> for more information about LNRE models and how to
initialize them, <code><a href="#topic+LNRE">LNRE</a></code> for type density and distribution
functions (which represent the prior distribution).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## TODO
  
</code></pre>

<hr>
<h2 id='lnre.bootstrap'>Parametric bootstrapping for LNRE models (zipfR)</h2><span id='topic+lnre.bootstrap'></span>

<h3>Description</h3>

<p>This function implements parametric bootstrapping for LNRE models, i.e. it draws a specified number of random samples from the population described by a given <code>lnre</code> object.  For each sample, two callback functions are applied to perform transformations and/or extract statistics.  In an important application (bootstrapped confidence intervals for model parameters), the first callback estimates a new LNRE model and the second callback extracts the relevant parameters from this model.  See &lsquo;Use Cases&rsquo; and &lsquo;Examples&rsquo; below for other use cases.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lnre.bootstrap(model, N, ESTIMATOR, STATISTIC, 
               replicates=100, sample=c("spc", "tfl", "tokens"),
               simplify=TRUE, verbose=TRUE, parallel=1L, seed=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lnre.bootstrap_+3A_model">model</code></td>
<td>
<p>a trained LNRE model, i.e. an object belonging to a subclass of <code>lnre</code>.  The model must provide a <code><a href="#topic+rlnre">rlnre</a></code> method to generate random samples from the underlying frequency distribution.</p>
</td></tr>
<tr><td><code id="lnre.bootstrap_+3A_n">N</code></td>
<td>
<p>a single positive integer, specifying the size <code class="reqn">N</code> (i.e. token count) of the individual bootstrap samples</p>
</td></tr>
<tr><td><code id="lnre.bootstrap_+3A_estimator">ESTIMATOR</code></td>
<td>

<p>a callback function, normally used for estimating LNRE models in the bootstrap procedure.  It is called once for each bootstrap sample with the sample as first argument (in the form determined by <code>sample</code>).  Additional arguments (<code>...</code>) are passed on to the callback, so it is possible to use <code>ESTIMATOR=lnre</code> with appropriate settings.  If this step is not needed, set <code>ESTIMATOR=identity</code> to pass samples through to the <code>STATISTIC</code> callback.
</p>
</td></tr>
<tr><td><code id="lnre.bootstrap_+3A_statistic">STATISTIC</code></td>
<td>

<p>a callback function, normally used to extract model parameters and other relevant statistics from the bootstrapped LNRE models.  It is called once for each  bootstrap sample, with the value returned by <code>ESTIMATOR</code> as its single argument.  The return values are automatically aggregated across all bootstrap samples (see &lsquo;Value&rsquo; below).  If this step is not needed, set <code>STATISTIC=identity</code> in order to pass through the results of the <code>ESTIMATOR</code> callback.  Note that <code>STATISTIC</code> <b>must not</b> return <code>NULL</code>, which is used internally to signal errors.
</p>
</td></tr>
<tr><td><code id="lnre.bootstrap_+3A_replicates">replicates</code></td>
<td>
<p>a single positive integer, specifying the number of bootstrap samples to be generated</p>
</td></tr>
<tr><td><code id="lnre.bootstrap_+3A_sample">sample</code></td>
<td>

<p>the form in which each sample is passed to <code>ESTIMATOR</code>: as a frequency spectrum (<code>spc</code>, the default), as a type-frequency list (<code>tfl</code>) or as a factor vector representing the token sequence (<code>tokens</code>). <b>Warning:</b> The latter can be computationally expensive for large <code>N</code>.
</p>
<p>Alternatively, a callback function that will be invoked with arguments <code>model</code> and <code>replicates</code> and must return a random sample in the format expected by <code>ESTIMATOR</code>.  See &lsquo;Use Cases&rsquo; below for typical applications.
</p>
</td></tr>
<tr><td><code id="lnre.bootstrap_+3A_simplify">simplify</code></td>
<td>
<p>if <code>TRUE</code>, use <code>rbind()</code> to combine list of results into a single data structure. In this case, the estimator should return either a vector of fixed length or a single-row data frame or matrix.  No validation is carried out before attempting the simplification.</p>
</td></tr>
<tr><td><code id="lnre.bootstrap_+3A_verbose">verbose</code></td>
<td>
<p>if <code>TRUE</code>, show progress bar in R console during the bootstrapping process (which can take a long time). The progress bar may be updated quite infrequently if parallel processing is enabled.</p>
</td></tr>
<tr><td><code id="lnre.bootstrap_+3A_parallel">parallel</code></td>
<td>
<p>whether to enable parallel processing. Either an integer specifying the number of worker processes to be forked, or a pre-initialised <span class="pkg">snow</span> cluster created with <code><a href="parallel.html#topic+makeCluster">makeCluster</a></code>; see &lsquo;Details&rsquo; below.</p>
</td></tr>
<tr><td><code id="lnre.bootstrap_+3A_seed">seed</code></td>
<td>
<p>a single integer value used to initialize the RNG in order to generate reproducible results</p>
</td></tr>
<tr><td><code id="lnre.bootstrap_+3A_...">...</code></td>
<td>
<p>any further arguments are passed through to the <code>ESTIMATOR</code> callback function</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The parametric bootstrapping procedure works as follows:
</p>

<ol>
<li> <p><code>replicates</code> random samples of <code>N</code> tokens each are drawn from the population described by the LNRE model <code>model</code> (possibly using a callback function provided in argument <code>sample</code>)
</p>
</li>
<li><p> Each sample is passed to the callback function <code>ESTIMATOR</code> in the form determined by <code>sample</code> (a frequency spectrum, type-frequency list, or factor vector of tokens). If <code>ESTIMATOR</code> fails, it is re-run with a different sample, otherwise the return value is passed on to <code>STATISTIC</code>.  Use <code>ESTIMATOR=identity</code> to pass the original sample through to <code>STATISTIC</code>.
</p>
</li>
<li><p> The callback function <code>STATISTIC</code> is used to extract relevant information for each sample. If <code>STATISTIC</code> fails, the procedure is repeated from step 2 with a different sample.  The callback will typically return a vector of fixed length or a single-row data frame, and the results for all bootstrap samples are combined into a matrix or data frame if <code>simplify=TRUE</code>.
</p>
</li></ol>

<p><b>Warning:</b> Keep in mind that sampling a token vector can be slow and consume large amounts of memory for very large <code>N</code> (several million tokens). If possible, use <code>sample="spc"</code> or <code>sample="tfl"</code>, which can be generated more efficiently.
</p>
<p><b>Parallelisation</b>
</p>
<p>Since bootstrapping is a computationally expensive procedure, it is usually desirable to use parallel processing.  <code>lnre.bootstrap</code> supports two types of parallelisation, based on the <span class="pkg">parallel</span> package:
</p>

<ul>
<li><p> On Unix platforms, you can set <code>parallel</code> to an integer number in order to fork the specified number of worker processes, utilising multiple cores on the same machine.  The <code><a href="parallel.html#topic+detectCores">detectCores</a></code> function shows how many cores are available, but due to hyperthreading and memory contention, it is often better to set <code>parallel</code> to a smaller value.  Note that forking may be unstable especially in a GUI environment, as explained on the <code>mcfork</code> manpage.
</p>
</li>
<li><p> On all platforms, you can pass a pre-initialised <span class="pkg">snow</span> cluster in the <code>argument</code>, which consists of worker processes on the same machine or on different machines.  A suitable cluster can be created with <code><a href="parallel.html#topic+makeCluster">makeCluster</a></code>; see the <span class="pkg">parallel</span> package documentation for further information.  It is your responsibility to set up the cluster so that all required data sets, packages and custom functions are available on the worker processes; <code>lnre.bootstrap</code> will only ensure that the <span class="pkg">zipfR</span> package itself is loaded.
</p>
</li></ul>

<p>Note that parallel processing is not enabled by default and will only be used if <code>parallel</code> is set accordingly.
</p>


<h3>Value</h3>

<p>If <code>simplify=FALSE</code>, a list of length <code>replicates</code> containing the statistics obtained from each individual bootstrap sample.  In addition, the following attributes are set:
</p>

<ul>
<li> <p><code>N</code> = sample size of the bootstrap replicates
</p>
</li>
<li> <p><code>model</code> = the LNRE model from which samples were generated
</p>
</li>
<li> <p><code>errors</code> = number of samples for which either the <code>ESTIMATOR</code> or the <code>STATISTIC</code> callback produced an error
</p>
</li></ul>

<p>If <code>simplify=TRUE</code>, the statistics are combined with <code>rbind()</code>.  This is performed unconditionally, so make sure that <code>STATISTIC</code> returns a suitable value for all samples, typically vectors of the same length or single-row data frames with the same columns.
The return value is usually a matrix or data frame with <code>replicates</code> rows.  No additional attributes are set.
</p>


<h3>Use cases</h3>


<dl>
<dt>Bootstrapped <b>confidence intervals for model parameters</b>:</dt><dd>
<p>The <code><a href="#topic+confint.lnre">confint</a></code> method for LNRE models uses bootstrapping to estimate confidence intervals for the model parameters.
</p>
<p>For this application, <code>ESTIMATOR=lnre</code> re-estimates the LNRE model from each bootstrap sample. Configuration options such as the model type, cost function, etc. are passed as additional arguments in <code>...</code>, and the sample must be provided in the form of a frequency spectrum. The return values are successfully estimated LNRE models.
</p>
<p><code>STATISTIC</code> extracts the model parameters and other coefficients of interest (such as the population diversity <code>S</code>) from each model and returns them as a named vector or single-row data frame.  The results are combined with <code>simplify=TRUE</code>, then empirical confidence intervals are determined for each column.
</p>
</dd>
<dt>Empirical <b>sampling distribution of productivity measures</b>:</dt><dd>
<p>For some of the more complex measures of productivity and lexical richness (see <code><a href="#topic+productivity.measures">productivity.measures</a></code>), it is difficult to estimate the sampling distribution mathematically.  In these cases, an empirical approximation can be obtained by parametric bootstrapping.
</p>
<p>The most convenient approach is to set <code>ESTIMATOR=productivity.measures</code>, so the desired measures can be passed as an additional argument <code>measures=</code> to <code>lnre.bootstrap</code>. The default <code>sample="spc"</code> is appropriate for most measures and is efficient enough to carry out the procedure for multiple sample sizes.
</p>
<p>Since the estimator already returns the required statistics for each sample in a suitable format, set <code>STATISTIC=identity</code> and <code>simplify=TRUE</code>.
</p>
</dd>
<dt>Empirical <b>prediction intervals for vocabulary growth curves</b>:</dt><dd>
<p>Vocabulary growth curves can only be generated from token vectors, so set <code>sample="tokens"</code> and keep <code>N</code> reasonably small.
</p>
<p><code>ESTIMATOR=vec2vgc</code> compiles <code>vgc</code> objects for the samples. Pass <code>steps</code> or <code>stepsize</code> as desired and set <code>m.max</code> if growth curves for <code class="reqn">V_1, V_2, \ldots</code> are desired.
</p>
<p><em>Either</em> use <code>STATISTIC=identity</code> and <code>simplify=FALSE</code> to return a list of <code>vgc</code> objects, which can be plotted or processed further with <code>sapply()</code>. This strategy is particulary useful if one or more <code class="reqn">V_m</code> are desired in addition to <code class="reqn">V</code>.
</p>
<p><em>Or</em> use <code>STATISTIC=function (x) x$V</code> to extract y-coordinates for the growth curve and combine them into a matrix with <code>simplify=TRUE</code>, so that prediction intervals can be computed directly.  Note that the corresponding x-coordinates are not returned and have to be inferred from <code>N</code> and <code>stepsize</code>.
</p>
</dd>
<dt>Simulating <b>non-randomness</b> and <b>mixture distributions</b>:</dt><dd>
<p>More complex populations and non-random samples can be simulated by providing a user callback function in the <code>sample</code> argument.  This callback is invoked with parameters <code>model</code> and <code>n</code> and has to return a sample of size <code>n</code> in the format expected by <code>ESTIMATOR</code>.
</p>
<p>For simulating non-randomness, the callback will typically use <code>rlnre</code> to generate a random sample and then apply some transformation.
</p>
<p>For simulating mixture distributions, it will typically generate multiple samples from different populations and merge them; the proportion of tokens from each population should be determined by a multinomial random variable.  Individual populations might consist of LNRE models, or a finite number of &ldquo;lexicalised&rdquo; types.  Note that only a single LNRE model will be passed to the callback; any other parameters have to be injected as bound variables in a local function definition.
</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+lnre">lnre</a></code> for more information about LNRE models.  The high-level estimator function <code><a href="#topic+lnre">lnre</a></code> uses <code>lnre.bootstrap</code> to collect data for approximate confidence intervals; <code><a href="#topic+lnre.productivity.measures">lnre.productivity.measures</a></code> uses it to approximate the sampling distributions of productivity measures.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## parametric bootstrapping from realistic LNRE model
model &lt;- lnre("zm", spc=ItaRi.spc) # has quite a good fit

## estimate distribution of V, V1, V2 for sample size N=1000
res &lt;- lnre.bootstrap(model, N=1000, replicates=200,
                      ESTIMATOR=identity,
                      STATISTIC=function (x) c(V=V(x), V1=Vm(x,1), V2=Vm(x,2)))
bootstrap.confint(res, method="normal")
## compare with theoretical expectations (EV/EVm = center, VV/VVm = spread^2)
lnre.spc(model, 1000, m.max=2, variances=TRUE)

## lnre.bootstrap() also captures and ignores occasional failures
res &lt;- lnre.bootstrap(model, N=1000, replicates=200,
                      ESTIMATOR=function (x) if (runif(1) &lt; .2) stop() else x,
                      STATISTIC=function (x) c(V=V(x), V1=Vm(x,1), V2=Vm(x,2)))

## empirical confidence intervals for vocabulary growth curve
## (this may become expensive because token-level samples have to be generated)
res &lt;- lnre.bootstrap(model, N=1000, replicates=200, sample="tokens",
                      ESTIMATOR=vec2vgc, stepsize=100, # extra args passed to ESTIMATOR
                      STATISTIC=V) # extract vocabulary sizes at equidistant N
bootstrap.confint(res, method="normal")

## parallel processing is highly recommended for expensive bootstrapping
library(parallel)
## adjust number of processes according to available cores on your machine
cl &lt;- makeCluster(2) # PSOCK cluster, should work on all platforms
res &lt;- lnre.bootstrap(model, N=1e4, replicates=200, sample="tokens",
                      ESTIMATOR=vec2vgc, stepsize=1000, STATISTIC=V,
                      parallel=cl) # use cluster for parallelisation
bootstrap.confint(res, method="normal")
stopCluster(cl)

## on MacOS / Linux, simpler fork-based parallelisation also works well
## Not run: 
res &lt;- lnre.bootstrap(model, N=1e5, replicates=400, sample="tokens",
                      ESTIMATOR=vec2vgc, stepsize=1e4, STATISTIC=V,
                      parallel=8) # if you have enough cores ...
bootstrap.confint(res, method="normal")

## End(Not run)
</code></pre>

<hr>
<h2 id='lnre.details'>Technical Details of LNRE Model Objects (zipfR)</h2><span id='topic+lnre.details'></span><span id='topic+lnre.technical.details'></span>

<h3>Description</h3>

<p>This manpage describes technical details of LNRE models and parameter
estimation.  It is intended developers who want to implement new LNRE
models, improve the parameter estimation algorithms, or work directly
with the internals of <code>lnre</code> objects.  All information required
for standard applications of LNRE models can be found on the
<code><a href="#topic+lnre">lnre</a></code> manpage.
</p>


<h3>Details</h3>

<p>Most operations on LNRE models (in particular, computation of expected
values and variances, distribution function and type distribution,
random sampling, etc.) are realized as S3 methods, so they are
automatically dispatched to appropriate implementations for the
various types of LNRE models (e.g., <code>EV.lnre.zm</code>,
<code>EV.lnre.fzm</code> and <code>EV.lnre.gigp</code> for the <code>EV</code> method).
For some methods (e.g. estimated variances <code>VV</code> and <code>VVm</code>),
a single generic implementation can be used for all model types,
provided through the base class (<code>VV.lnre</code> and <code>VVm.lnre</code>
for variances).
</p>
<p>If you want to implement new LNRE models, have a look at &quot;Implementing
LNRE Models&quot; below.
</p>
<p><b>Important note:</b> LNRE model parameters can be passed as named
arguments to the <code>lnre</code> constructor function when they are not
estimated automatically from an observed frequency spectrum.  For this
reason, parameter names must be carefully chosen so that they do not
clash with other arguments of the <code>lnre</code> function.  Note that
because of <span class="rlang"><b>R</b></span>'s argument matching rules, any parameter name that is a
<em>prefix</em> of a standard argument name will lead to such a clash.
In particular, single-letter parameters (such as <code class="reqn">b</code> and <code class="reqn">c</code>
for the GIGP model) should always be written in uppercase (<code>B</code>
and <code>C</code> in <code>lnre.gigp</code>).
</p>


<h3>Value</h3>

<p>A LNRE model with estimated (or manually specified) parameter values
is represented by an object belonging to a suitable subclass of
<code>lnre</code>.  The specific class depends on the type of LNRE model, as
specified in the <code>type</code> argument to the <code>lnre</code> constructor
function (e.g. <code>lnre.fzm</code> for a fZM model selected with
<code>type="fzm"</code>).
</p>
<p>All subtypes of <code>lnre</code> object share the same data format, viz. a
list with the following components:
</p>
<table>
<tr><td><code>type</code></td>
<td>
<p>a character string specifying the class of LNRE model,
e.g. <code>"fzm"</code> for a finite Zipf-Mandelbrot model</p>
</td></tr>
<tr><td><code>name</code></td>
<td>
<p>a character string specifying a human-readable name for
the LNRE model, e.g. <code>"finite Zipf-Mandelbrot"</code></p>
</td></tr>
<tr><td><code>param</code></td>
<td>
<p>list of named model parameters, e.g. <code>(alpha=.8,
      B=.01)</code> for a ZM model</p>
</td></tr>
<tr><td><code>param2</code></td>
<td>
<p>a list of &quot;secondary&quot; parameters, i.e. constants that
can be determined from the model parameters but are frequently used
in the formulae for expected values, variances, etc.;
e.g. <code>(C=.5)</code> for the ZM model above</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>population size, i.e. number of types in the population
described by the LNRE model (may be <code>Inf</code>, e.g. for a ZM
model)</p>
</td></tr>
<tr><td><code>exact</code></td>
<td>
<p>whether approximations are allowed when calculating
expectations and variances (<code>FALSE</code>) or not (<code>TRUE</code>)</p>
</td></tr>
<tr><td><code>multinomial</code></td>
<td>
<p>whether to use equations for multionmial sampling
(<code>TRUE</code>) or independent Poisson sampling (<code>FALSE</code>)</p>
</td></tr>
<tr><td><code>spc</code></td>
<td>
<p>an object of class <code>spc</code>, the observed frequency
spectrum from which the model parameters have been estimated (only
if the LNRE model is based on empirical data)</p>
</td></tr>
<tr><td><code>gof</code></td>
<td>
<p>an object of class <code>lnre.gof</code> with goodness-of-fit
information for the estimated LNRE model (only if based on empirical
data, i.e. if the <code>spc</code> component is also present)</p>
</td></tr>
<tr><td><code>util</code></td>
<td>
<p>a set of utility functions, given as a list with the
following components:
</p>

<dl>
<dt><code>update</code>:</dt><dd><p>function with signature <code>(self, param,
	transformed=FALSE)</code>, which updates the parameters of the LNRE
model <code>self</code> with the values in <code>param</code>, checks that
their values are in the allowed range, and re-calculates
&quot;secondary&quot; parameters and lexicon size if necessary.  If
<code>transformed=TRUE</code>, the specified parameters are translated
back to normal scale before the update (see below).  Of course,
<code>self</code> should be the object from which the utility function
was called.  <code>update</code> returns a modified version of the
object <code>self</code>.</p>
</dd>
<dt><code>transform</code>:</dt><dd><p>function with signature <code>(param,
	inverse=FALSE)</code>, which transform model parameters (given as a
list in the argument <code>param</code>) to an unbounded range
centered at 0, and back (with option <code>inverse=TRUE</code>).  The
transformed model parameters are used for parameter estimation,
so that unconstrained minimization algorithms can be applied.
The link function for the transformation depends on the LNRE
model and the &quot;distribution&quot; of each parameter.  A felicitous
choice can be crucial for robust and quick parameter estimation,
especially with Newton-like gradient algorithms.  Note that
setting all transformed parameters to 0 should provide a
reasonable starting point for the parameter estimation.</p>
</dd>
<dt><code>print</code>:</dt><dd><p>partial print method for this subclass of
LNRE model, which displays the name of the model, its
parameters, and optionally some additional information (invoked
internally by <code>print.lnre</code> and <code>summary.lnre</code>)</p>
</dd>
<dt><code>label</code>:</dt><dd><p>returns a string with a short description
of the LNRE model, including its subclass and approximate values 
for its parameters (e.g. for use in legend text).</p>
</dd>
</dl>

</td></tr>    
</table>


<h3>Implementing LNRE Models</h3>

<p>In order to implement a new class of LNRE models, the following steps
are necessary (illustrated on the example of a lognormal type density
function, introducing the new LNRE class <code>lnre.lognormal</code>):
</p>

<ul>
<li><p> Provide a constructor function for LNRE models of this type
(here, <code>lnre.lognormal</code>), which must accept the parameters of
the LNRE model as named arguments with reasonable default values (or
alternatively as a list passed in the <code>param</code> argument).  The
constructor must return a partially initialized object of an
appropriate subclass of <code>lnre</code> (<code>lnre.lognormal</code> in our
example), and make sure that this object also inherits from the
<code>lnre</code> class.
</p>
</li>
<li><p> Provide the <code>update</code>, <code>transform</code>, <code>print</code> and <code>label</code>
utility functions for the LNRE model, which must be returned in the
<code>util</code> field of the LNRE model object (see &quot;Value&quot; above).
</p>
</li>
<li><p> Add the new type of LNRE model to the <code>type</code> argument of
the generic <code>lnre</code> constructor, and insert the new constructor
function (<code>lnre.lognormal</code>) in the <code>switch</code> call in the
body of <code>lnre</code>.
</p>
</li>
<li><p> As a minimum requirement, implementations of the <code>EV</code> and
<code>EVm</code> methods must be provided for the new LNRE model (in our
example, they will be named <code>EV.lnre.lognormal</code> and
<code>EVm.lnre.lognormal</code>).
</p>
</li>
<li><p> If possible, provide equations for the type density,
probability density, type distribution, distribution function 
and posterior distribution of
the new LNRE model, as implementations of the <code>tdlnre</code>,
<code>dlnre</code>, <code>tplnre</code>/<code>tqlnre</code>,
<code>plnre</code>/<code>qlnre</code> and <code>postplnre</code>/<code>postqlnre</code>
methods for the new LNRE model class.  If
all these functions are defined, log-scaled densities and random
number generation are automatically handled by generic
implementations.
</p>
</li>
<li><p> Optionally, provide a custom function for parameter estimation
of the new LNRE model, as an implementation of the
<code>estimate.model</code> method (here,
<code>estimate.model.lnre.lognormal</code>).  Custom parameter estimation
can considerably improve convergence and goodness-of-fit if it is
possible to obtain direct estimates for one or more of the
parameters, e.g. from the condition <code class="reqn">E[V] = V</code>.  However, the
default Nelder-Mead algorithm is robust and produces satisfactory
results, as long as the LNRE model defines an appropriate parameter
transformation mapping.  It is thus often more profitable to
optimize the <code>transform</code> utility than to spend a lot of time
implementing a complicated parameter estimation function.
</p>
</li></ul>

<p>The best way to get started is to take a look at one of the existing
implementations of LNRE models.  The GIGP model represents a &quot;minimum&quot;
implementation (without custom parameter estimation and distribution
functions), whereas ZM and fZM provide good examples of custom
parameter estimation functions.
</p>


<h3>See Also</h3>

<p>User-level information about LNRE models and parameter estimation can
be found on the <code><a href="#topic+lnre">lnre</a></code> manpage.
</p>
<p>Descriptions of the different LNRE models implemented in <code>zipfR</code>
and their parameters are given on separate manpages
<code><a href="#topic+lnre.zm">lnre.zm</a></code>, <code><a href="#topic+lnre.fzm">lnre.fzm</a></code> and
<code><a href="#topic+lnre.gigp">lnre.gigp</a></code>.  These descriptions are intended for
interested end users, but are not required for standard applications
of the models.
</p>
<p>The <code><a href="#topic+estimate.model">estimate.model</a></code> manpage explains details of the
parameter estimation procedure (intended for developers).
</p>
<p>See <code><a href="#topic+lnre.goodness.of.fit">lnre.goodness.of.fit</a></code> for a description of the
goodness-of-fit test performed after parameter estimation of an LNRE
model.  This function can also be used to evaluate the predictions of
the model on a different data set.
</p>

<hr>
<h2 id='lnre.fzm'>The finite Zipf-Mandelbrot (fZM) LNRE Model (zipfR)</h2><span id='topic+lnre.fzm'></span>

<h3>Description</h3>

<p>The finite Zipf-Mandelbrot (fZM) LNRE model of Evert (2004).
</p>
<p>The constructor function <code>lnre.fzm</code> is not user-visible.  It is
invoked implicitly when <code>lnre</code> is called with LNRE model type
<code>"fzm"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  lnre.fzm(alpha=.8, A=1e-9, B=.01, param=list())

  ## user call: lnre("fzm", spc=spc) or lnre("fzm", alpha=.8, A=1e-9, B=.01)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lnre.fzm_+3A_alpha">alpha</code></td>
<td>
<p>the <em>shape</em> parameter <code class="reqn">\alpha</code>, a number in the
range <code class="reqn">(0,1)</code></p>
</td></tr>
<tr><td><code id="lnre.fzm_+3A_a">A</code></td>
<td>
<p>the <em>lower cutoff</em> parameter <code class="reqn">A</code>, a positive number.
Note that a valid set of parameters must satisfy <code class="reqn">0 &lt; A &lt; B</code>.</p>
</td></tr>
<tr><td><code id="lnre.fzm_+3A_b">B</code></td>
<td>
<p>the <em>upper cutoff</em> parameter <code class="reqn">B</code>, a positive number
(<code class="reqn">B &gt; 1</code> is allowed although it is inconsistent with the
interpretation of <code class="reqn">B</code>)</p>
</td></tr>
<tr><td><code id="lnre.fzm_+3A_param">param</code></td>
<td>
<p>a list of parameters given as name-value pairs
(alternative method of parameter specification)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The parameters of the fZM model can either be specified as immediate
arguments:
</p>
<pre>
    lnre.fzm(alpha=.5, A=5e-12, B=.1)
  </pre>
<p>or as a list of name-value pairs:
</p>
<pre>
    lnre.fzm(param=list(alpha=.5, A=5e-12, B=.1))
  </pre>
<p>which is usually more convenient when the constructor is invoked by
another function (such as <code>lnre</code>).  If both immediate arguments
and the <code>param</code> list are given, the immediate arguments override
conflicting values in <code>param</code>.  For any parameters that are
neither specified as immediate arguments nor listed in <code>param</code>,
the defaults from the function prototype are inserted.
</p>
<p>The <code>lnre.fzm</code> constructor also checks the types and ranges of
parameter values and aborts with an error message if an invalid
parameter is detected.
</p>
<p><b>NB:</b> parameter estimation is faster and more robust for the
inexact fZM model, so you might consider passing the
<code>exact=FALSE</code> option to <code>lnre</code> unless you intend to make
predictions for small sample sizes <code class="reqn">N</code> and/or high spectrum elements
<code class="reqn">E[V_m(N)]</code> (<code class="reqn">m \gg 1</code>) with the model.
</p>


<h3>Value</h3>

<p>A partially initialized object of class <code>lnre.fzm</code>, which is
completed and passed back to the user by the <a href="#topic+lnre">lnre</a> function.
See <code><a href="#topic+lnre">lnre</a></code> for a detailed description of <code>lnre.fzm</code>
objects (as a subclass of <code>lnre</code>).
</p>


<h3>Mathematical Details</h3>

<p>Similar to ZM, the <b>fZM model</b> is a LNRE re-formulation of the
<b>Zipf-Mandelbrot</b> law for a population with a finite vocabulary
size <code class="reqn">S</code>, i.e.
</p>
<p style="text-align: center;"><code class="reqn">
    \pi_k = \frac{C}{(k + b) ^ a}
  </code>
</p>

<p>for <code class="reqn">k = 1, \ldots, S</code>.  The parameters of the Zipf-Mandelbrot law
are <code class="reqn">a &gt; 1</code>, <code class="reqn">b \ge 0</code> and <code class="reqn">S</code> (see also Baayen 2001,
101ff).  The fZM model is given by the <b>type density function</b>
</p>
<p style="text-align: center;"><code class="reqn">
    g(\pi) := C\cdot \pi^{-\alpha-1}
  </code>
</p>

<p>for <code class="reqn">A \le \pi \le B</code> (and <code class="reqn">\pi = 0</code> otherwise), and has three
<b>parameters</b> <code class="reqn">0 &lt; \alpha &lt; 1</code> and <code class="reqn">0 &lt; A &lt; B \le 1</code>.  The
normalizing constant is
</p>
<p style="text-align: center;"><code class="reqn">
    C = \frac{ 1 - \alpha }{ B^{1 - \alpha} - A^{1 - \alpha} }
  </code>
</p>

<p>and the population vocabulary size is
</p>
<p style="text-align: center;"><code class="reqn">
    S = \frac{1 - \alpha}{\alpha} \cdot
    \frac{ A^{-\alpha} - B^{-\alpha} }{ B^{1 - \alpha} - A^{1 - \alpha} }
  </code>
</p>

<p>See Evert (2004) and the <code><a href="#topic+lnre.zm">lnre.zm</a></code> manpage for further
details.
</p>


<h3>References</h3>

<p>Baayen, R. Harald (2001). <em>Word Frequency Distributions.</em> Kluwer,
Dordrecht.
</p>
<p>Evert, Stefan (2004). A simple LNRE model for random character
sequences. <em>Proceedings of JADT 2004</em>, 411-422.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lnre">lnre</a></code> for pointers to relevant methods and functions for
objects of class <code>lnre</code>, as well as a complete listing of LNRE
models implemented in the <code>zipfR</code> library.
</p>

<hr>
<h2 id='lnre.gigp'>The Generalized Inverse Gauss-Poisson (GIGP) LNRE Model (zipfR)</h2><span id='topic+lnre.gigp'></span>

<h3>Description</h3>

<p>The Generalized Inverse Gauss-Poisson (GIGP) LNRE model of Sichel
(1971).
</p>
<p>The constructor function <code>lnre.gigp</code> is not user-visible.  It is
invoked implicitly when <code>lnre</code> is called with LNRE model type
<code>"gigp"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  lnre.gigp(gamma=-.5, B=.01, C=.01, param=list())

  ## user call: lnre("gigp", spc=spc) or lnre("gigp", gamma=-.5, B=.01, C=.01)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lnre.gigp_+3A_gamma">gamma</code></td>
<td>
<p>the <em>shape</em> parameter <code class="reqn">\gamma</code>, a negative
number in the range <code class="reqn">(-1,0)</code>.  <code class="reqn">\gamma</code> corresponds to
<code class="reqn">-\alpha</code> in the Zipf-Mandelbrot notation.</p>
</td></tr>
<tr><td><code id="lnre.gigp_+3A_b">B</code></td>
<td>
<p>the <em>low-frequency decay</em> parameter <code class="reqn">b</code>, a
non-negative number.  This parameter determines how quickly the type
density function vanishes for <code class="reqn">\pi \to 0</code>, with larger
values corresponding to faster decay.</p>
</td></tr>
<tr><td><code id="lnre.gigp_+3A_c">C</code></td>
<td>
<p>the <em>high-frequency decay</em> parameter <code class="reqn">c</code>, a
non-negative number.  This parameter determines how quickly the type
density function vanishes for large values of <code class="reqn">\pi</code>, with
<em>smaller</em> values corresponding to faster decay.</p>
</td></tr>
<tr><td><code id="lnre.gigp_+3A_param">param</code></td>
<td>
<p>a list of parameters given as name-value pairs
(alternative method of parameter specification)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The parameters of the GIGP model can either be specified as immediate
arguments:
</p>
<pre>
    lnre.gigp(gamma=-.47, B=.001, C=.001)
  </pre>
<p>or as a list of name-value pairs:
</p>
<pre>
    lnre.gigp(param=list(gamma=-.47, B=.001, C=.001))
  </pre>
<p>which is usually more convenient when the constructor is invoked by
another function (such as <code>lnre</code>).  If both immediate arguments
and the <code>param</code> list are given, the immediate arguments override
conflicting values in <code>param</code>.  For any parameters that are
neither specified as immediate arguments nor listed in <code>param</code>,
the defaults from the function prototype are inserted.
</p>
<p>The <code>lnre.gigp</code> constructor also checks the types and ranges of
parameter values and aborts with an error message if an invalid
parameter is detected.
</p>
<p>Notice that the implementation of GIGP leads to numerical problems
when estimating the expected frequency of high spectrum elements
(you might start worrying if you need to go above <code class="reqn">m=150</code>).
</p>
<p>Note that the parameters <code class="reqn">b</code> and <code class="reqn">c</code> are normally written in
lowercase (e.g. Baayen 2001).  For the technical reasons, it was
necessary to use uppercase letters <code>B</code> and <code>C</code> in this
implementation.
</p>


<h3>Value</h3>

<p>A partially initialized object of class <code>lnre.gigp</code>, which is
completed and passed back to the user by the <a href="#topic+lnre">lnre</a> function.
See <code><a href="#topic+lnre">lnre</a></code> for a detailed description of <code>lnre.gigp</code>
objects (as a subclass of <code>lnre</code>).
</p>


<h3>Mathematical Details</h3>

<p>Despite its fance name, the <b>Generalized Inverse Gauss-Poisson</b>
or <b>GIGP model</b> belongs to the same class of LNRE models as ZM
and fZM.  This class of models is characterized by a power-law in the
type density function and derives from the <b>Zipf-Mandelbrot law</b>
(see <code><a href="#topic+lnre.zm">lnre.zm</a></code> for details on the relationship between
power-law LNRE models and the Zipf-Mandelbrot law).
</p>
<p>The GIGP model is given by the type density function
</p>
<p style="text-align: center;"><code class="reqn">
    g(\pi) := C\cdot \pi^{\gamma - 1} \cdot
    e^{- \frac{\pi}{c} - \frac{b^2 c}{4 \pi}}
  </code>
</p>

<p>with parameters <code class="reqn">-1 &lt; \gamma &lt; 0</code> and <code class="reqn">b, c \ge 0</code>.  The
normalizing constant is
</p>
<p style="text-align: center;"><code class="reqn">
    C = \frac{(2 / bc)^{\gamma+1}}{K_{\gamma+1}(b)}
  </code>
</p>

<p>and the population vocabulary size is
</p>
<p style="text-align: center;"><code class="reqn">
    S = \frac{2}{bc} \cdot \frac{K_{\gamma}(b)}{K_{\gamma+1}(b)}
  </code>
</p>

<p>Note that the &quot;shape&quot; parameter <code class="reqn">\gamma</code> corresponds to
<code class="reqn">-\alpha</code> in the ZM and fZM models.  The GIGP model was introduced
by Sichel (1971).  See Baayen (2001, 89-93) for further details.
</p>


<h3>References</h3>

<p>Baayen, R. Harald (2001). <em>Word Frequency Distributions.</em> Kluwer,
Dordrecht.
</p>
<p>Sichel, H. S. (1971). On a family of discrete distributions
particularly suited to represent long-tailed frequency
data. <em>Proceedings of the Third Symposium on Mathematical
Statistics</em>, 51-97.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lnre">lnre</a></code> for pointers to relevant methods and functions for
objects of class <code>lnre</code>, as well as a complete listing of LNRE
models implemented in the <code>zipfR</code> library.
</p>

<hr>
<h2 id='lnre.goodness.of.fit'>Goodness-of-fit Evaluation of LNRE Models (zipfR)</h2><span id='topic+lnre.goodness.of.fit'></span>

<h3>Description</h3>

<p>This function measures the goodness-of-fit of a LNRE model compared to
an observed frequency spectrum, using a multivariate chi-squared test
(Baayen 2001, p. 119ff).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  lnre.goodness.of.fit(model, spc, n.estimated=0, m.max=15)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lnre.goodness.of.fit_+3A_model">model</code></td>
<td>
<p>an LNRE model object, belonging to a suitable subclass of
<code>lnre</code>.</p>
</td></tr>
<tr><td><code id="lnre.goodness.of.fit_+3A_spc">spc</code></td>
<td>
<p>an observed frequency spectrum, i.e. an object of class
<code>spc</code>.  This can either be the spectrum on which the model
parameters have been estimated, or a different, independent spectrum.</p>
</td></tr>
<tr><td><code id="lnre.goodness.of.fit_+3A_n.estimated">n.estimated</code></td>
<td>
<p>number of parameters of the LNRE model that have
been estimated on <code>spc</code>.  This number is automatically
subtracted from the degrees of freedom of the resulting chi-squared
statistic.  When <code>spc</code> is an independent spectrum,
<code>n.estimated</code> should always be set to the default value of 0.</p>
</td></tr>
<tr><td><code id="lnre.goodness.of.fit_+3A_m.max">m.max</code></td>
<td>
<p>number of spectrum elements that will be used to compute
the chi-squared statistic.  The default value of 15 is also used by
Baayen (2001).  For small samples, it may be sensible to
use fewer spectrum elements, e.g. by setting <code>m.max=10</code> or
<code>m.max=5</code>.  Depending on how many degrees of freedom have to be
subtracted, <code>m.max</code> should not be chosen too low.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default, the number of spectrum elements included in the
calculation of the chi-squared statistic may be reduced automatically
in order to ensure that it is not dominated by the sampling error of
spectrum elements with very small expected frequencies (which are
scaled up due to the small variance of these random variables).  As an
ad-hoc rule of thumb, spectrum elements <code class="reqn">V_m</code> with variance less
than 5 are excluded, since the normal approximation to their discrete
distribution is likely to be inaccurate in this case.
</p>
<p>Automatic reduction is disabled when the parameter <code>m.max</code> is
specified explicitly (use <code>m.max=15</code> to disable automatic
reduction without changing the default value).
</p>


<h3>Value</h3>

<p>A data frame with one row and the following variables:
</p>
<table>
<tr><td><code>X2</code></td>
<td>
<p>value of the multivariate chi-squared statistic <code class="reqn">X^2</code></p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>number of degrees of freedom of <code class="reqn">X^2</code>, corrected for the
number of parameters that have been estimated on <code>spc</code></p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>p-value corresponding to <code class="reqn">X^2</code></p>
</td></tr>
</table>


<h3>References</h3>

<p>Baayen, R. Harald (2001). <em>Word Frequency Distributions.</em> Kluwer,
Dordrecht.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lnre">lnre</a></code> for more information about LNRE models
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load spectrum of first 100k Brown tokens
data(Brown100k.spc)

## use this spectrum to compute zm and gigp
## models
zm &lt;- lnre("zm",Brown100k.spc)
gigp &lt;- lnre("gigp",Brown100k.spc)

## lnre.goodness.of.fit with appropriate
## n.estimated value produces the same multivariate
## chi-squared test that is reported in a model
## summary

## compare:
zm
lnre.goodness.of.fit(zm,Brown100k.spc,n.estimated=2)

gigp
lnre.goodness.of.fit(gigp,Brown100k.spc,n.estimated=3)

## goodness of fit of the 100k models calculated on the
## whole Brown spectrum (although this is superset of
## 100k spectrum, let's pretend it is an independent
## spectrum, and set n.estimated to 0)

data(Brown.spc)

lnre.goodness.of.fit(zm,Brown.spc,n.estimated=0)
lnre.goodness.of.fit(gigp,Brown.spc,n.estimated=0)


</code></pre>

<hr>
<h2 id='lnre.productivity.measures'>Measures of Productivity and Lexical Richness (zipfR)</h2><span id='topic+lnre.productivity.measures'></span>

<h3>Description</h3>

<p>Compute expectations of various measures of productivity and lexical richness
for a LNRE population.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lnre.productivity.measures(model, N=NULL, measures, data.frame=TRUE, 
                           bootstrap=FALSE, method="normal", conf.level=.95, sample=NULL,
                           replicates=1000, parallel=1L, verbose=TRUE, seed=NULL)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lnre.productivity.measures_+3A_model">model</code></td>
<td>
<p>an object belonging to a subclass of <code>lnre</code>,
representing a LNRE model</p>
</td></tr>
<tr><td><code id="lnre.productivity.measures_+3A_measures">measures</code></td>
<td>
<p>character vector naming the productivity measures to
be computed (see <code><a href="#topic+productivity.measures">productivity.measures</a></code> for details).
Names may be abbreviated as long as they remain unique.
If unspecified or <code>NULL</code>, all supported measures are included.</p>
</td></tr>
<tr><td><code id="lnre.productivity.measures_+3A_n">N</code></td>
<td>
<p>an integer vector, specifying the sample size(s) <code class="reqn">N</code>
for which the productivity measures will be calculated.  If
<code>bootstrap=TRUE</code>, only a single sample size may be specified.
<code>N</code> defaults to the sample size used for estimating <code>model</code>
if unspecified or set to <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="lnre.productivity.measures_+3A_data.frame">data.frame</code></td>
<td>
<p>if <code>TRUE</code>, the return value is converted to a data frame
for convenience in interactive use (default).</p>
</td></tr>
<tr><td><code id="lnre.productivity.measures_+3A_bootstrap">bootstrap</code></td>
<td>
<p>if <code>TRUE</code>, use parametric bootstrapping to estimate 
expectations and confidence intervals for the productivity measures.
Otherwise, approximate expectations are obtained directly from the LNRE 
model (see &lsquo;Details&rsquo; below for the approximations and simplifications used).</p>
</td></tr>
<tr><td><code id="lnre.productivity.measures_+3A_method">method</code>, <code id="lnre.productivity.measures_+3A_conf.level">conf.level</code></td>
<td>
<p>type of confidence interval to be estimated by parametric
bootstrapping and the requested confidence level; 
see <code><a href="#topic+bootstrap.confint">bootstrap.confint</a></code> for details.</p>
</td></tr>
<tr><td><code id="lnre.productivity.measures_+3A_sample">sample</code></td>
<td>
<p>optional callback function to generate bootstrapping samples;
see <code><a href="#topic+lnre.bootstrap">lnre.bootstrap</a></code> for details and applications.</p>
</td></tr>
<tr><td><code id="lnre.productivity.measures_+3A_replicates">replicates</code>, <code id="lnre.productivity.measures_+3A_parallel">parallel</code>, <code id="lnre.productivity.measures_+3A_seed">seed</code>, <code id="lnre.productivity.measures_+3A_verbose">verbose</code></td>
<td>
<p>if <code>bootstrap=TRUE</code>, these parameters
are passed on to <code>lnre.bootstrap</code> to control the bootstrapping procedure; 
see <code><a href="#topic+lnre.bootstrap">lnre.bootstrap</a></code> for documentation.  In most cases, it is recommended
to set <code>parallel</code> in order to speed up the expensive bootstrapping process.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If  <code>bootstrap=FALSE</code>, expected values of the productivity measures are computed based on the following approximations:
</p>

<ul>
<li> <p><code>V</code>, <code>TTR</code>, <code>R</code> and <code>P</code> are linear transformations of <code class="reqn">V</code> or <code class="reqn">V_1</code>, so expectations can be obtained directly from the <code><a href="#topic+EV">EV</a></code> and <code><a href="#topic+EVm">EVm</a></code> methods.
</p>
</li>
<li> <p><code>C</code>, <code>k</code>, <code>U</code> and <code>W</code> are nonlinear transformations of <code class="reqn">V</code>. In this case, the transformation function is approximated by a linear function around <code class="reqn">E[V]</code>, which is reasonable under typical circumstances.
</p>
</li>
<li> <p><code>Hapax</code>, <code>S</code>, <code>alpha2</code> and <code>H</code> are based on ratios of two spectrum elements, in some cases with an additional nonlinear transformation. Expectations are based on normal approximations for <code class="reqn">V</code> and <code class="reqn">V_i</code> together with a generalisation of Díaz-Francés and Rubio's (2013: 313) result on the ratio of two independent normal distributions; for a nonlinear transformation the same linear approximation is made as above.
</p>
</li>
<li> <p><code>K</code> and <code>D</code> are (nearly) unbiased estimators of the population coefficient <code class="reqn">\delta = \sum_{i=1}^{\infty} \pi_i^2</code> (Simpson 1949: 688). 
</p>
</li></ul>

<p>Approximations used for expected values are explained in detail in Sec. 2.2 of the technical report <a href="https://zipfr.r-forge.r-project.org/materials/inside-zipfr.pdf">Inside <em>zipfR</em></a>.
</p>


<h3>Value</h3>

<p>If <code>bootstrap=FALSE</code>, a numeric matrix or data frame listing approximate expectations of the selected productivity measures,
with one row for each sample size <code>N</code> and one column for each <code>measure</code>.  Rows and columns are labelled.
</p>
<p>If <code>bootstrap=TRUE</code>, a numeric matrix or data frame with one column for each productivity <code>measure</code> and four rows
giving the lower and upper bound of the confidence interval, an estimate of central tendency, and an estimate of spread.
See <code><a href="#topic+bootstrap.confint">bootstrap.confint</a></code> for details.
</p>


<h3>Productivity Measures</h3>

<p>See <code><a href="#topic+productivity.measures">productivity.measures</a></code> for a list of supported measures with equations and references.
The measures <code>Entropy</code> and <code>eta</code> are only supported for <code>bootstrap=TRUE</code>.
</p>


<h3>References</h3>

<p>Díaz-Francés, Eloísa and Rubio, Francisco J. (2013).
On the existence of a normal approximation to the distribution of the ratio of two independent normal random variables.
<em>Statistical Papers</em>, <b>54</b>(2), 309&ndash;323.
</p>
<p>Simpson, E. H. (1949).
Measurement of diversity.
<em>Nature</em>, <b>163</b>, 688.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+productivity.measures">productivity.measures</a></code> computes productivity measures from observed data sets.
See <code><a href="#topic+lnre">lnre</a></code> for further information on LNRE models, and
<code><a href="#topic+lnre.bootstrap">lnre.bootstrap</a></code> and <code><a href="#topic+bootstrap.confint">bootstrap.confint</a></code> for details on the bootstrapping procedure.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## plausible model for an author's vocabulary
model &lt;- lnre("fzm", alpha=0.4, B=0.06, A=1e-12)

## approximate expectation for different sample sizes
lnre.productivity.measures(model, N=c(1000, 10000, 50000))

## estimate sampling distribution: 95% interval, mean, s.d.
## (using parametric bootstrapping, only one sample size at a time)
lnre.productivity.measures(model, N=1000, bootstrap=TRUE)

</code></pre>

<hr>
<h2 id='lnre.spc'>Compute Expected Frequency Spectrum of LNRE Model (zipfR)</h2><span id='topic+lnre.spc'></span>

<h3>Description</h3>

<p><code>lnre.spc</code> computes the expected frequency spectrum of a LNRE
model at specified sample size <code>N</code>, returning an object of class
<code>spc</code>.  Since almost all expected spectrum elements are non-zero,
only an incomplete spectrum can be generated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  lnre.spc(model, N=NULL, variances=FALSE, m.max=100)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lnre.spc_+3A_model">model</code></td>
<td>
<p>an object belonging to a subclass of <code>lnre</code>,
representing a LNRE model</p>
</td></tr>
<tr><td><code id="lnre.spc_+3A_n">N</code></td>
<td>
<p>a single positive integer, specifying the sample size <code class="reqn">N</code>
for which the expected frequency spectrum is calculated (defaults to
same sample size as used for estimating the model)</p>
</td></tr>
<tr><td><code id="lnre.spc_+3A_variances">variances</code></td>
<td>
<p>if <code>TRUE</code>, include variances for the spectrum
elements in the <code>spc</code> object</p>
</td></tr>
<tr><td><code id="lnre.spc_+3A_m.max">m.max</code></td>
<td>
<p>number of spectrum elements listed in the frequency
spectrum.  The default of 100 is chosen to avoid numerical
problems that certain LNRE models (in particular, GIGP) have for
higher <code class="reqn">m</code>.  If variance data is included, the default value
is automatically reduced to 50.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>~~ TODO, if any ~~
</p>


<h3>Value</h3>

<p>An object of class <code>spc</code>, representing the incomplete expected
frequency spectrum of the LNRE model <code>lnre</code> at sample size
<code>N</code>.  If <code>variances=TRUE</code>, the spectrum also includes
variance data.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+spc">spc</a></code> for more information about frequency spectra and
links to relevant functions; <code><a href="#topic+lnre">lnre</a></code> for more information
about LNRE models and how to initialize them
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load Dickens dataset and compute lnre models
data(Dickens.spc)

zm &lt;- lnre("zm",Dickens.spc)
fzm &lt;- lnre("fzm",Dickens.spc, exact=FALSE)
gigp &lt;- lnre("gigp",Dickens.spc)

## calculate the corresponding expected
## frequency spectra at the Dickens size
zm.spc &lt;- lnre.spc(zm,N(Dickens.spc))
fzm.spc &lt;- lnre.spc(fzm,N(Dickens.spc))
gigp.spc &lt;- lnre.spc(gigp,N(Dickens.spc))

## comparative plot
plot(Dickens.spc,zm.spc,fzm.spc,gigp.spc,m.max=10)

## expected spectra at N=100e+8
## and comparative plot
zm.spc &lt;- lnre.spc(zm,1e+8)
fzm.spc &lt;- lnre.spc(fzm,1e+8)
gigp.spc &lt;- lnre.spc(gigp,1e+8)

plot(zm.spc,fzm.spc,gigp.spc,m.max=10)

## with variances
zm.spc &lt;- lnre.spc(zm,1e+8,variances=TRUE)
head(zm.spc)

## asking for more than 50 spectrum elements
## (increasing m.max will eventually lead
## to error, at different threshold for
## the different models)
zm.spc &lt;- lnre.spc(zm,1e+8,m.max=1000)
fzm.spc &lt;- lnre.spc(fzm,1e+8,m.max=1000)
gigp.spc &lt;- lnre.spc(gigp,1e+8,m.max=100) ## gigp breaks first!


</code></pre>

<hr>
<h2 id='lnre.vgc'>Expected Vocabulary Growth Curves of LNRE Model (zipfR)</h2><span id='topic+lnre.vgc'></span>

<h3>Description</h3>

<p><code>lnre.vgc</code> computes expected vocabulary growth curves
<code class="reqn">E[V(N)]</code> according to a LNRE model, returning an object of class
<code>vgc</code>.  Data points are returned for the specified values of
<code class="reqn">N</code>, optionally including estimated variances and/or growth curves
for the spectrum elements <code class="reqn">E[V_m(N)]</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  lnre.vgc(model, N, m.max=0, variances=FALSE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lnre.vgc_+3A_model">model</code></td>
<td>
<p>an object belonging to a subclass of <code>lnre</code>,
representing a LNRE model</p>
</td></tr>
<tr><td><code id="lnre.vgc_+3A_n">N</code></td>
<td>
<p>an increasing sequence of non-negative integers, specifying
the sample sizes <code class="reqn">N</code> for which vocabulary growth data should be
calculated</p>
</td></tr>
<tr><td><code id="lnre.vgc_+3A_m.max">m.max</code></td>
<td>
<p>if specified, include vocabulary growth curves
<code class="reqn">E[V_m(N)]</code> for spectrum elements up to <code>m.max</code>.  Must be a
single integer in the range <code class="reqn">1 \ldots 9</code>.</p>
</td></tr>
<tr><td><code id="lnre.vgc_+3A_variances">variances</code></td>
<td>
<p>if <code>TRUE</code>, include variance estimates for the
vocabulary size (and the spectrum elements, if applicable)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>~~ TODO, if any ~~
</p>


<h3>Value</h3>

<p>An object of class <code>vgc</code>, representing the expected vocabulary
growth curve <code class="reqn">E[V(N)]</code> of the LNRE model <code>lnre</code>, with data
points at the sample sizes <code>N</code>.
</p>
<p>If <code>m.max</code> is specified, expected growth curves <code class="reqn">E[V_m(N)]</code>
for spectrum elements (<em>hapax legomena</em>, <em>dis legomena</em>,
etc.) up to <code>m.max</code> are also computed.
</p>
<p>If <code>variances=TRUE</code>, the <code>vgc</code> object includes variance data
for all growth curves.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vgc">vgc</a></code> for more information about vocabulary growth curves
and links to relevant functions; <code><a href="#topic+lnre">lnre</a></code> for more
information about LNRE models and how to initialize them
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load Dickens dataset and estimate lnre models
data(Dickens.spc)

zm &lt;- lnre("zm",Dickens.spc)
fzm &lt;- lnre("fzm",Dickens.spc,exact=FALSE)
gigp &lt;- lnre("gigp",Dickens.spc)

## compute expected V and V_1 growth up to 100 million tokens
## in 100 steps of 1 million tokens
zm.vgc &lt;- lnre.vgc(zm,(1:100)*1e6, m.max=1)
fzm.vgc &lt;- lnre.vgc(fzm,(1:100)*1e6, m.max=1)
gigp.vgc &lt;- lnre.vgc(gigp,(1:100)*1e6, m.max=1)

## compare
plot(zm.vgc,fzm.vgc,gigp.vgc,add.m=1,legend=c("ZM","fZM","GIGP"))

## load Italian ultra- prefix data
data(ItaUltra.spc)

## compute zm model
zm &lt;- lnre("zm",ItaUltra.spc)

## compute vgc up to about twice the sample size
## with variance of V
zm.vgc &lt;- lnre.vgc(zm,(1:100)*70, variances=TRUE)

## plot with confidence intervals derived from variance in
## vgc (with larger datasets, ci will typically be almost
## invisible)
plot(zm.vgc)

</code></pre>

<hr>
<h2 id='lnre.zm'>The Zipf-Mandelbrot (ZM) LNRE Model (zipfR)</h2><span id='topic+lnre.zm'></span>

<h3>Description</h3>

<p>The Zipf-Mandelbrot (ZM) LNRE model of Evert (2004).
</p>
<p>The constructor function <code>lnre.zm</code> is not user-visible.  It is
invoked implicitly when <code>lnre</code> is called with LNRE model type
<code>"zm"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  lnre.zm(alpha=.8, B=.01, param=list())

  ## user call: lnre("zm", spc=spc) or lnre("zm", alpha=.8, B=.1)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lnre.zm_+3A_alpha">alpha</code></td>
<td>
<p>the <em>shape</em> parameter <code class="reqn">\alpha</code>, a number in the
range <code class="reqn">(0,1)</code></p>
</td></tr>
<tr><td><code id="lnre.zm_+3A_b">B</code></td>
<td>
<p>the <em>upper cutoff</em> parameter <code class="reqn">B</code>, a positive number
(<code class="reqn">B &gt; 1</code> is allowed although it is inconsistent with the
interpretation of <code class="reqn">B</code>)</p>
</td></tr>
<tr><td><code id="lnre.zm_+3A_param">param</code></td>
<td>
<p>a list of parameters given as name-value pairs
(alternative method of parameter specification)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The parameters of the ZM model can either be specified as immediate
arguments:
</p>
<pre>
    lnre.zm(alpha=.5, B=.1)
  </pre>
<p>or as a list of name-value pairs:
</p>
<pre>
    lnre.zm(param=list(alpha=.5, B=.1))
  </pre>
<p>which is usually more convenient when the constructor is invoked by
another function (such as <code>lnre</code>).  If both immediate arguments
and the <code>param</code> list are given, the immediate arguments override
conflicting values in <code>param</code>.  For any parameters that are
neither specified as immediate arguments nor listed in <code>param</code>,
the defaults from the function prototype are inserted.
</p>
<p>The <code>lnre.zm</code> constructor also checks the types and ranges of
parameter values and aborts with an error message if an invalid
parameter is detected.
</p>


<h3>Value</h3>

<p>A partially initialized object of class <code>lnre.zm</code>, which is
completed and passed back to the user by the <a href="#topic+lnre">lnre</a> function.
See <code><a href="#topic+lnre">lnre</a></code> for a detailed description of <code>lnre.zm</code>
objects (as a subclass of <code>lnre</code>).
</p>


<h3>Mathematical Details</h3>

<p>The <b>ZM model</b> is a re-formulation of the <b>Zipf-Mandelbrot</b>
law
</p>
<p style="text-align: center;"><code class="reqn">
    \pi_k = \frac{C}{(k + b) ^ a}
  </code>
</p>

<p>with parameters <code class="reqn">a &gt; 1</code> and <code class="reqn">b \ge 0</code> (see also Baayen 2001,
101ff) as a LNRE model.  It is given by the <b>type density
function</b>
</p>
<p style="text-align: center;"><code class="reqn">
    g(\pi) := C\cdot \pi^{-\alpha-1}
  </code>
</p>

<p>for <code class="reqn">0 \le \pi \le B</code> (and <code class="reqn">\pi = 0</code> otherwise), with the
<b>parameters</b> <code class="reqn">0 &lt; \alpha &lt; 1</code> and <code class="reqn">0 &lt; B \le 1</code>.  The
normalizing constant is 
</p>
<p style="text-align: center;"><code class="reqn">
    C = \frac{ 1 - \alpha }{ B^{1 - \alpha} }
  </code>
</p>

<p>and the population vocabulary size is <code class="reqn">S = \infty</code>.  The
parameters of the ZM model are related to those of the original
Zipf-Mandelbrot law by <code class="reqn">a = 1/\alpha</code> and <code class="reqn">b = (1 -
  \alpha)/(B \cdot \alpha)</code>.  See Evert
(2004) for further details.
</p>


<h3>References</h3>

<p>Baayen, R. Harald (2001). <em>Word Frequency Distributions.</em> Kluwer,
Dordrecht.
</p>
<p>Evert, Stefan (2004). A simple LNRE model for random character
sequences. <em>Proceedings of JADT 2004</em>, 411-422.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lnre">lnre</a></code> for pointers to relevant methods and functions for
objects of class <code>lnre</code>, as well as a complete listing of LNRE
models implemented in the <code>zipfR</code> library.
</p>

<hr>
<h2 id='merge.tfl'>Merging Type Frequency Lists (zipfR)</h2><span id='topic+merge.tfl'></span>

<h3>Description</h3>

<p>Merge two or more type frequency lists. Types from the individual lists are pooled
and frequencies of types occurring in multiple lists are aggregated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S3 method for class 'tfl'
merge(x, y, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="merge.tfl_+3A_x">x</code>, <code id="merge.tfl_+3A_y">y</code></td>
<td>
<p>type frequency lists (i.e. objects of class <code><a href="#topic+tfl">tfl</a></code>)</p>
</td></tr>
<tr><td><code id="merge.tfl_+3A_...">...</code></td>
<td>
<p>optional further type frequency lists to be merged</p>
</td></tr>
</table>


<h3>Details</h3>

<p>All type frequency lists to be merged must contain type labels,
and none of them may be incomplete.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tfl">tfl</a></code> for more information about type frequency lists.
</p>

<hr>
<h2 id='N-V-Vm'>Access Methods for Observed Frequency Data (zipfR)</h2><span id='topic+N'></span><span id='topic+V'></span><span id='topic+Vm'></span>

<h3>Description</h3>

<p><code>N</code>, <code>V</code> and <code>Vm</code> are generic methods that can (and
should) be used to access observed frequency data for objects of class
<code>tfl</code>, <code>spc</code>, <code>vgc</code> and <code>lnre</code>.  The precise
behaviour of the functions depends on the class of the object, but in
general <code>N</code> returns the sample size, <code>V</code> the vocabulary
size, and <code>Vm</code> one or more selected elements of the frequency
spectrum.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  N(obj, ...)
  V(obj, ...)
  Vm(obj, m, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="N-V-Vm_+3A_obj">obj</code></td>
<td>
<p>an object of class <code>tfl</code> (type frequency list),
<code>spc</code> (frequency spectrum), <code>vgc</code> (vocabulary growth
curve) or <code>lnre</code> (LNRE model)</p>
</td></tr>
<tr><td><code id="N-V-Vm_+3A_m">m</code></td>
<td>
<p>positive integer value determining the frequency class
<code class="reqn">m</code> to be returned (or a vector of such values).</p>
</td></tr>
<tr><td><code id="N-V-Vm_+3A_...">...</code></td>
<td>
<p>additional arguments passed on to the method implementation
(see respective manpages for details)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>tfl</code> and <code>vgc</code> objects, the <code>Vm</code> method allows
only a single value <code>m</code> to be specified.
</p>


<h3>Value</h3>

<p>For a frequency spectrum (class <code>spc</code>), <code>N</code> returns the
sample size, <code>V</code> returns the vocabulary size, and <code>Vm</code>
returns individual spectrum elements.  
</p>
<p>For a type frequency list (class <code>tfl</code>), <code>N</code> returns the
sample size and <code>V</code> returns the vocabulary size corresponding to
the list.  <code>Vm</code> returns a single spectrum element from the
corresponding frequency spectrum, and may only be called with a single
value <code>m</code>.
</p>
<p>For a vocabulary growth curve (class <code>vgc</code>), <code>N</code> returns the
vector of sample sizes and <code>V</code> the vector of vocabulary sizes.
<code>Vm</code> may only be called with a single value <code>m</code> and returns
the corresponding vector from the <code>vgc</code> object (if present).
</p>
<p>For a LNRE model (class <code>lnre</code>) estimated from an observed
frequency spectrum, the methods <code>N</code>, <code>V</code> and <code>Vm</code>
return information about this frequency spectrum.
</p>


<h3>See Also</h3>

<p>For details on the implementations of these methods, see
<code><a href="#topic+N.tfl">N.tfl</a></code>, <code><a href="#topic+N.spc">N.spc</a></code>, <code><a href="#topic+N.vgc">N.vgc</a></code>, etc.
When applied to an LNRE model, the methods return information about
the observed frequency spectrum from which the model was estimated, so
the manpages for <code><a href="#topic+N.spc">N.spc</a></code> are relevant in this case.
</p>
<p>Expected vocabulary size and frequency spectrum for a sample of size
<code class="reqn">N</code> according to a LNRE model can be computed with the analogous
methods <code><a href="#topic+EV">EV</a></code> and <code><a href="#topic+EVm">EVm</a></code>.  The corresponding
variances are obtained with the <code><a href="#topic+VV">VV</a></code> and <code><a href="#topic+VVm">VVm</a></code>
methods, which can also be applied to expected or interpolated
frequency spectra and vocabulary growth curves.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load Brown spc and tfl
data(Brown.spc)
data(Brown.tfl)

## you can extract N, V and Vm (for a specific m)
## from either structure
N(Brown.spc)
N(Brown.tfl)

V(Brown.spc)
V(Brown.tfl)

Vm(Brown.spc,1)
Vm(Brown.tfl,1)

## you can extract the same info also from a lnre model estimated
## from these data (NB: these are the observed quantities; for the
## expected values predicted by the model use EV and EVm instead!)
model &lt;- lnre("gigp",Brown.spc)
N(model)
V(model)
Vm(model,1)

## Baayen's P:
Vm(Brown.spc,1)/N(Brown.spc)

## when input is a spectrum (and only then) you can specify a vector
## of m's; e.g., to obtain class sizes of first 5 spectrum elements
## you can write:
Vm(Brown.spc,1:5)

## the Brown vgc
data(Brown.emp.vgc)

## with a vgc as input, N, V and Vm return vectors of the respective
## values for each sample size listed in the vgc
Ns &lt;- N(Brown.emp.vgc)
Vs &lt;- V(Brown.emp.vgc)
V1s &lt;- Vm(Brown.emp.vgc,1)

head(Ns)
head(Vs)
head(V1s)

## since the last sample size in Brown.emp.vgc
## corresponds to the full Brown, the last elements
## of the Ns, Vs and V1s vectors are the same as
## the quantities extracted from the spectrum and
## tfl:
Ns[length(Ns)]
Vs[length(Vs)]
V1s[length(V1s)]

</code></pre>

<hr>
<h2 id='N-V-Vm.spc'>Access Methods for Frequency Spectra (zipfR)</h2><span id='topic+N.spc'></span><span id='topic+V.spc'></span><span id='topic+Vm.spc'></span><span id='topic+VV.spc'></span><span id='topic+VVm.spc'></span>

<h3>Description</h3>

<p>Return the sample size (<code>N.spc</code>), vocabulary size (<code>V.spc</code>)
and class sizes (<code>Vm.spc</code>) of the frequency spectrum represented
by a <code>spc</code> object.  For an expected spectrum with variance
information, <code>VV.spc</code> returns the variance of the expected
spectrum size and <code>VVm.spc</code> the variances of individual spectrum
elements.
</p>
<p>Note that these functions are not user-visible.  They can be called
implicitly through the generic methods <code>N</code>, <code>V</code>, <code>Vm</code>,
<code>VV</code> and <code>VVm</code>, applied to an object of type <code>spc</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  ## S3 method for class 'spc'
N(obj, ...)

  ## S3 method for class 'spc'
V(obj, ...)

  ## S3 method for class 'spc'
Vm(obj, m, ...)

  ## S3 method for class 'spc'
VV(obj, N=NA, ...)

  ## S3 method for class 'spc'
VVm(obj, m, N=NA, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="N-V-Vm.spc_+3A_obj">obj</code></td>
<td>
<p>an object of class <code>spc</code>, representing an observed or
expected frequency spectrum</p>
</td></tr>
<tr><td><code id="N-V-Vm.spc_+3A_m">m</code></td>
<td>
<p>positive integer value determining the frequency class
<code class="reqn">m</code> to be returned (or a vector of such values).</p>
</td></tr>
<tr><td><code id="N-V-Vm.spc_+3A_n">N</code></td>
<td>
<p>not applicable (this argument of the generic method is not
used by the implementation for <code>spc</code> objects and must not be
specified)</p>
</td></tr>
<tr><td><code id="N-V-Vm.spc_+3A_...">...</code></td>
<td>
<p>additional arguments passed on from generic method will be ignored</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>VV.spc</code> a <code>VVm.spc</code> will fail if the object <code>obj</code> is
not an expected frequency spectrum with variance data.
</p>
<p>For an incomplete frequency spectrum, <code>Vm.spc</code> (and
<code>VVm.spc</code>) will return <code>NA</code> for all spectrum elements that
are not listed in the object (i.e. for <code>m &gt; m.max</code>).
</p>


<h3>Value</h3>

<p><code>N.spc</code> returns the sample size <code class="reqn">N</code>, <code>V.spc</code> returns the
vocabulary size <code class="reqn">V</code> (or expected vocabulary size <code class="reqn">E[V]</code>), and
<code>Vm.spc</code> returns a vector of class sizes <code class="reqn">V_m</code> (ot the
expected spectrum elements <code class="reqn">E[V_m]</code>).
</p>
<p>For an expected spectrum with variances, <code>VV.spc</code> returns the
variance <code class="reqn">\mathop{Var}[V]</code> of the expected vocabulary
size, and <code>VVm.spc</code> returns variances
<code class="reqn">\mathop{Var}[V_m]</code> of the spectrum elements.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+N">N</a></code>, <code><a href="#topic+V">V</a></code>, <code><a href="#topic+Vm">Vm</a></code>, <code><a href="#topic+VV">VV</a></code>,
<code><a href="#topic+VVm">VVm</a></code> for the generic methods and links to other
implementations
</p>
<p><code><a href="#topic+spc">spc</a></code> for details on frequency spectrum objects and links
to other relevant functions
</p>

<hr>
<h2 id='N-V-Vm.tfl'>Access Methods for Type Frequency Lists (zipfR)</h2><span id='topic+N.tfl'></span><span id='topic+V.tfl'></span><span id='topic+Vm.tfl'></span>

<h3>Description</h3>

<p>Return the sample size (<code>N.tfl</code>) and vocabulary size
(<code>V.tfl</code>) of the type frequency list represented by a <code>tfl</code>
object, as well as class sizes (<code>Vm.tfl</code>) of the corresponding
frequency spectrum.
</p>
<p>Note that these functions are not user-visible.  They can be called
implicitly through the generic methods <code>N</code>, <code>V</code> and
<code>Vm</code>, applied to an object of type <code>tfl</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  ## S3 method for class 'tfl'
N(obj, ...)

  ## S3 method for class 'tfl'
V(obj, ...)

  ## S3 method for class 'tfl'
Vm(obj, m, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="N-V-Vm.tfl_+3A_obj">obj</code></td>
<td>
<p>an object of class <code>tfl</code>, representing an observed
type frequency list</p>
</td></tr>
<tr><td><code id="N-V-Vm.tfl_+3A_m">m</code></td>
<td>
<p>non-negative integer value determining the frequency class
<code class="reqn">m</code> to be returned</p>
</td></tr>
<tr><td><code id="N-V-Vm.tfl_+3A_...">...</code></td>
<td>
<p>additional arguments passed on from generic method will be
ignored</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Only a single value is allowed for <code class="reqn">m</code>, which may also be 0.
In order to obtain multiple class sizes <code class="reqn">V_m</code>, convert the type
frequency list to a frequency spectrum with <code>tfl2spc</code> first.
</p>
<p>For an incomplete type frequency list, <code>Vm.tfl</code> will return
<code>NA</code> if <code>m</code> is outside the range of listed frequencies
(i.e. for <code>m &lt; f.min</code> or <code>m &gt; f.max</code>).
</p>


<h3>Value</h3>

<p><code>N.tfl</code> returns the sample size <code class="reqn">N</code>, <code>V.tfl</code> returns the
vocabulary size <code class="reqn">V</code> (or expected vocabulary size <code class="reqn">E[V]</code>), and
<code>Vm.tfl</code> returns the number of types that occur exactly <code class="reqn">m</code>
times in the sample, i.e. the class size <code class="reqn">V_m</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+N">N</a></code>, <code><a href="#topic+V">V</a></code>, <code><a href="#topic+Vm">Vm</a></code> for the generic
methods and links to other implementations
</p>
<p><code><a href="#topic+tfl">tfl</a></code> for details on type frequency list objects and links
to other relevant functions
</p>

<hr>
<h2 id='N-V-Vm.vgc'>Access Methods for Vocabulary Growth Curves (zipfR)</h2><span id='topic+N.vgc'></span><span id='topic+V.vgc'></span><span id='topic+Vm.vgc'></span><span id='topic+VV.vgc'></span><span id='topic+VVm.vgc'></span>

<h3>Description</h3>

<p>Return the vector of sample sizes (<code>N.vgc</code>), vocabulary sizes
(<code>V.vgc</code>) or class sizes (<code>Vm.vgc</code>) from the vocabulary
growth curve (VGC) represented by a <code>vgc</code> object.  For an
expected or interpolated VGC with variance information, <code>VV.vgc</code>
returns the vector of variances of the vocabulary size and
<code>VVm.vgc</code> the variance vectors for individual spectrum elements.
</p>
<p>Note that these functions are not user-visible.  They can be called
implicitly through the generic methods <code>N</code>, <code>V</code>, <code>Vm</code>,
<code>VV</code> and <code>VVm</code>, applied to an object of type <code>vgc</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  ## S3 method for class 'vgc'
N(obj, ...)

  ## S3 method for class 'vgc'
V(obj, ...)

  ## S3 method for class 'vgc'
Vm(obj, m, ...)

  ## S3 method for class 'vgc'
VV(obj, N=NA, ...)

  ## S3 method for class 'vgc'
VVm(obj, m, N=NA, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="N-V-Vm.vgc_+3A_obj">obj</code></td>
<td>
<p>an object of class <code>vgc</code>, representing an observed,
interpolated or expected VGC</p>
</td></tr>
<tr><td><code id="N-V-Vm.vgc_+3A_m">m</code></td>
<td>
<p>positive integer value determining the frequency class
<code class="reqn">m</code> for which the vector of class sizes is returned</p>
</td></tr>
<tr><td><code id="N-V-Vm.vgc_+3A_n">N</code></td>
<td>
<p>not applicable (this argument of the generic method is not
used by the implementation for <code>vgc</code> objects and must not be
specified)</p>
</td></tr>
<tr><td><code id="N-V-Vm.vgc_+3A_...">...</code></td>
<td>
<p>additional arguments passed on from generic method will be ignored</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>VV.vgc</code> a <code>VVm.vgc</code> will fail if the object <code>obj</code> does
not include variance data.  <code>Vm.vgc</code> and <code>VVm.vgc</code> will fail
if the selected frequency class is not included in the VGC data.
</p>


<h3>Value</h3>

<p><code>N.vgc</code> returns the vector of sample sizes <code class="reqn">N</code>, <code>V.vgc</code>
returns the corresponding vocabulary sizes <code class="reqn">V(N)</code> (or expected
vocabulary sizes <code class="reqn">E[V(N)]</code>), and <code>Vm.vgc</code> returns the vector of
class sizes <code class="reqn">V_m(N)</code> (or the expected spectrum elements
<code class="reqn">E[V_m(N)]</code>) for the selected frequency class <code class="reqn">m</code>.
</p>
<p>For an expected or interpolated VGC with variance information,
<code>VV.vgc</code> returns the vector of variances
<code class="reqn">\mathop{Var}[V(N)]</code> of the expected vocabulary size,
and <code>VVm.vgc</code> returns vector of variances
<code class="reqn">\mathop{Var}[V_m(N)]</code> for the selected frequency
class <code class="reqn">m</code>.
</p>
<p>Except for <code>N.vgc</code>, the vector returned will be labelled with
corresponding sample sizes.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+N">N</a></code>, <code><a href="#topic+V">V</a></code>, <code><a href="#topic+Vm">Vm</a></code>, <code><a href="#topic+VV">VV</a></code>,
<code><a href="#topic+VVm">VVm</a></code> for the generic methods and links to other
implementations
</p>
<p><code><a href="#topic+vgc">vgc</a></code> for details on vocabulary growth curve objects and
links to other relevant functions
</p>

<hr>
<h2 id='plot.lnre'>Plot LNRE Population Distribution (zipfR)</h2><span id='topic+plot.lnre'></span>

<h3>Description</h3>

<p>Visualisation of LNRE population distribution, showing either the (log-transformed)
type or probability density function or the cumulative probability distribution function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lnre'
plot(x, y, ...,
     type=c("types", "probability", "cumulative"),
     xlim=c(1e-9, 1), ylim=NULL, steps=200,
     xlab=NULL, ylab=NULL, legend=NULL, grid=FALSE,
     main="LNRE Population Distribution",
     lty=NULL, lwd=NULL, col=NULL, bw=zipfR.par("bw"))

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.lnre_+3A_x">x</code>, <code id="plot.lnre_+3A_y">y</code>, <code id="plot.lnre_+3A_...">...</code></td>
<td>
<p>one or more objects of class <code>lnre</code>, containing trained
LNRE models describing the population(s) to be plotted. Alternatively, all
models can be passed as a list in the <code>x</code> argument if the method is
called explicitly (see &lsquo;Examples&rsquo;).</p>
</td></tr>
<tr><td><code id="plot.lnre_+3A_type">type</code></td>
<td>
<p>what type of plot should be drawn, <code>"types"</code> for the log-transformed
type density function, <code>"probability"</code> for the log-transformed probability 
density function, and <code>"cumulative"</code> for the cumulative probability distribution.</p>
</td></tr>
<tr><td><code id="plot.lnre_+3A_xlim">xlim</code>, <code id="plot.lnre_+3A_ylim">ylim</code></td>
<td>
<p>visible range on x- and y-axis.  The default <code>ylim</code> is <code class="reqn">[0, 1]</code>
for <code>type="cumulative"</code> and automatically chosen to fit the selected density curves  
for <code>type="density"</code>.  Note that the x-axis is always logarithmic and <code>xlim</code>
should be chosen accordingly.</p>
</td></tr>
<tr><td><code id="plot.lnre_+3A_steps">steps</code></td>
<td>
<p>number of steps for drawing curves (increase for extra smoothness)</p>
</td></tr>
<tr><td><code id="plot.lnre_+3A_xlab">xlab</code>, <code id="plot.lnre_+3A_ylab">ylab</code></td>
<td>
<p>labels for the x-axis and y-axis (with suitable defaults depending on <code>type</code>)</p>
</td></tr>
<tr><td><code id="plot.lnre_+3A_legend">legend</code></td>
<td>
<p>optional vector of character strings or expressions
specifying labels for a legend box, which will be drawn in the upper
right-hand or left-hand corner of the screen.  If <code>legend=TRUE</code>,
labels showing model type and parameters are automatically generated.</p>
</td></tr>
<tr><td><code id="plot.lnre_+3A_grid">grid</code></td>
<td>
<p>whether to display a suitable grid in the background of the plot</p>
</td></tr>
<tr><td><code id="plot.lnre_+3A_main">main</code></td>
<td>
<p>a character string or expression specifying a main title for the plot</p>
</td></tr>
<tr><td><code id="plot.lnre_+3A_lty">lty</code>, <code id="plot.lnre_+3A_lwd">lwd</code>, <code id="plot.lnre_+3A_col">col</code></td>
<td>
<p>style vectors that can be used to
override the global styles defined by <code><a href="#topic+zipfR.par">zipfR.par</a></code>.  If
these vectors are specified, they must contain at least as many
elements as the number of populations shown in the plot:
the values are <em>not</em> automatically recycled.</p>
</td></tr>
<tr><td><code id="plot.lnre_+3A_bw">bw</code></td>
<td>
<p>if <code>TRUE</code>, draw plot in B/W style (default is the
global <code><a href="#topic+zipfR.par">zipfR.par</a></code> setting)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are two useful ways of visualising a LNRE population distribution, selected with the 
<code>type</code> argument:
</p>

<dl>
<dt><code>types</code></dt><dd>
<p>A plot of the type density function <code class="reqn">g(\pi)</code> over the type probability <code class="reqn">\pi</code>
on a log-transformed scale (so that the number of types corresponds to an integral over
<code class="reqn">\log_{10} \pi</code>, see <code><a href="#topic+ltdlnre">ltdlnre</a></code>). 
The log transformation is essential so that the density function
remains in a reasonable range; a logarithmic y-axis would be very counter-intuitive.
Note that density values correspond to the number of types per order of magnitude
on the x-axis.
</p>
</dd>
<dt><code>probability</code></dt><dd>
<p>A plot of the probability density function <code class="reqn">\pi g(\pi)</code> over the type probability <code class="reqn">\pi</code>
on a log-transformed scale (so that probability mass corresponds to an integral over
<code class="reqn">\log_{10} \pi</code>, see <code><a href="#topic+ldlnre">ldlnre</a></code>). 
Note that density values correspond to the total probability mass of types across one
order of magnitude on the x-axis.
</p>
</dd>
<dt><code>cumulative</code></dt><dd>
<p>A plot of the cumulative probability distribution, i.e. the distribution function 
<code class="reqn">F(\rho) = P(\pi \le \rho)</code> showing the total probability mass of types with 
type probability <code class="reqn">\pi \le \rho</code>.  The x-axis shows <code class="reqn">\rho</code> on a logarithmic scale
(but is labelled more intuitively with <code class="reqn">\pi</code> by default).  No special transformations
are required because <code class="reqn">0 \le F(\rho) \le 1</code>.
</p>
</dd>
</dl>

<p>Line styles are defined globally through <code>zipfR.par</code>,
but can be overridden with the optional parameters
<code>lty</code>, <code>lwd</code> and <code>col</code>.  In most cases, it is more advisable to
change the global settings temporarily for a sequence of plots, though.
</p>
<p>The <code>bw</code> parameter is used to switch between B/W and colour
modes.  It can also be set globally with <code>zipfR.par</code>.
</p>
<p>Other standard graphics parameters (such as <code>cex</code> or <code>mar</code>) cannot
be passed to the plot function an need to be set up with <code><a href="graphics.html#topic+par">par</a></code>
in advance.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lnre">lnre</a></code>, <code><a href="#topic+ltdlnre">ltdlnre</a></code>, <code><a href="#topic+plnre">plnre</a></code>
<code><a href="#topic+zipfR.par">zipfR.par</a></code>, <code><a href="#topic+zipfR.plotutils">zipfR.plotutils</a></code>
</p>
<p><code><a href="#topic+plot.tfl">plot.tfl</a></code> offers a different visualisation of the LNRE population distribution,
in the form of a Zipf-Mandelbrot law rather than type density.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## visualise three LNRE models trained on same data
m1 &lt;- lnre("zm", Dickens.spc)
m2 &lt;- lnre("fzm", Dickens.spc)
m3 &lt;- lnre("gigp", Dickens.spc)
plot(m1, m2, m3, type="types",
     xlim=c(1e-8, 1e-2), ylim=c(0, 7.5e4), legend=TRUE)
plot(m1, m2, m3, type="probability", 
     xlim=c(1e-8, 1e-2), grid=TRUE, legend=TRUE)

## cumulative probability distribution is not available for GIGP
plot(m1, m2, type="cumulative", grid=TRUE,
     xlim=c(1e-8, 1e-2), legend=c("ZM", "fZM"))

## first argument can also be a list of models with explicit call
models &lt;- lapply(seq(.1, .9, .2), 
                 function (x) lnre("zm", alpha=x, B=.1))
plot.lnre(models, type="cum", grid=TRUE, legend=TRUE)
plot.lnre(models, type="prob", grid=TRUE, legend=TRUE)
</code></pre>

<hr>
<h2 id='plot.spc'>Plot Word Frequency Spectra (zipfR)</h2><span id='topic+plot.spc'></span>

<h3>Description</h3>

<p>Plot a word frequency spectrum, or a comparison of several word
frequency spectra, either as a side-by-side barplot or as points and
lines on various logarithmic scales.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'spc'
plot(x, y, ...,
     m.max=if (log=="") 15 else 50, 
     log="", conf.level=.95,
     bw=zipfR.par("bw"), points=TRUE,
     xlim=NULL, ylim=NULL,
     xlab="m", ylab="V_m", legend=NULL,
     main="Frequency Spectrum",
     barcol=NULL, pch=NULL, lty=NULL, lwd=NULL, col=NULL)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.spc_+3A_x">x</code>, <code id="plot.spc_+3A_y">y</code>, <code id="plot.spc_+3A_...">...</code></td>
<td>
<p>one or more objects of class <code>spc</code>, representing
observed or expected frequency spectra to be plotted. Alternatively, all
spectra can be passed as a list in the <code>x</code> argument if the method is
called explicitly (see &lsquo;Examples&rsquo;.</p>
</td></tr>
<tr><td><code id="plot.spc_+3A_m.max">m.max</code></td>
<td>
<p>number of frequency classes that will be shown in plot.
The default is 15 on linear scale and 50 when using any type of
logarithmic scale.</p>
</td></tr>
<tr><td><code id="plot.spc_+3A_log">log</code></td>
<td>
<p>a character string specifying the axis or axes for which
logarithmic scale is to be used (<code>"x"</code>, <code>"y"</code>, or
<code>"xy"</code>), similar to the <code>log</code> argument of
<code><a href="graphics.html#topic+plot.default">plot.default</a></code>.  By default, a barplot on linear scale
is displayed.  Use <code>log=""</code> to show non-logarithmic
points-and-lines plot (also see &quot;Details&quot; below).</p>
</td></tr>
<tr><td><code id="plot.spc_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level for confidence intervals in
logarithmic plots (see &quot;Details&quot; below).  The default value of
<code class="reqn">.95</code> produces 95%-confidence intervals.  Set to <code>NA</code>
in order to suppress confidence interval markers.</p>
</td></tr>
<tr><td><code id="plot.spc_+3A_bw">bw</code></td>
<td>
<p>if <code>TRUE</code>, draw plot in B/W style (default is the
global <code><a href="#topic+zipfR.par">zipfR.par</a></code> setting)</p>
</td></tr>
<tr><td><code id="plot.spc_+3A_points">points</code></td>
<td>
<p>if <code>TRUE</code>, spectrum plots on any type of
logarithmic scale are drawn as overplotted lines and points
(default).  Otherwise, they are drawn as lines with different
styles.</p>
</td></tr>
<tr><td><code id="plot.spc_+3A_xlim">xlim</code>, <code id="plot.spc_+3A_ylim">ylim</code></td>
<td>
<p>visible range on x- and y-axis.  The default values
are automatically determined to fit the selected data in the plot.</p>
</td></tr>
<tr><td><code id="plot.spc_+3A_xlab">xlab</code>, <code id="plot.spc_+3A_ylab">ylab</code></td>
<td>
<p>labels for the x-axis and y-axis.  The default
values nicely typeset mathematical expressions.  The y-axis label
also distinguishes between observed and expected frequency spectra.</p>
</td></tr>
<tr><td><code id="plot.spc_+3A_main">main</code></td>
<td>
<p>a character string or expression specifying a main title
for the plot</p>
</td></tr>
<tr><td><code id="plot.spc_+3A_legend">legend</code></td>
<td>
<p>optional vector of character strings or expressions,
specifying labels for a legend box, which will be drawn in the upper
right-hand corner of the screen.  If <code>legend</code> is given, its
length must correspond to the number of frequency spectra in the
plot.</p>
</td></tr>
<tr><td><code id="plot.spc_+3A_barcol">barcol</code>, <code id="plot.spc_+3A_pch">pch</code>, <code id="plot.spc_+3A_lty">lty</code>, <code id="plot.spc_+3A_lwd">lwd</code>, <code id="plot.spc_+3A_col">col</code></td>
<td>
<p>style vectors that can be used to
override the global styles defined by <code><a href="#topic+zipfR.par">zipfR.par</a></code>.  If
these vectors are specified, they must contain at least as many
elements as there are frequency spectra in the plot: the values are
<em>not</em> automatically recycled.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default, the frequency spectrum or spectra are represented as a
barplot, with both axes using linear scale.  If the <code>log</code>
parameter is given, the spectra are shown either as lines in different
styles (<code>points=FALSE</code>) or as overplotted points and lines
(<code>point=TRUE</code>).  The value of <code>log</code> specifies which axes
should use logarithmic scale (specify <code>log=""</code> for a
points-and-lines plot on linear scale).
</p>
<p>In y-logarithmic plots, frequency classes with <code class="reqn">V_m = 0</code> are drawn
outside the plot region (below the bottom margin) rather than skipped.
</p>
<p>In all logarithmic plots, confidence intervals are indicated for
expected frequency spectra with variance data (by vertical lines with
T-shaped hooks at both ends).  The size of the confidence intervals is
controlled by the <code>conf.level</code> parameter (default: 95%).  Set
<code>conf.level=NA</code> in order to suppress the confidence interval
indicators.
</p>
<p>Line and point styles, as well as bar colours in the barplot, can be
defined globally with <code>zipfR.par</code>.  They can be overridden
locally with the optional parameters <code>barcol</code>, <code>pch</code>,
<code>lty</code>, <code>lwd</code> and <code>col</code>, but this should only be used
when absolutely necessary.  In most cases, it is more advisable to
change the global settings temporarily for a sequence of plots.
</p>
<p>The <code>bw</code> parameter is used to switch between B/W and colour
modes.  It can also be set globally with <code>zipfR.par</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+spc">spc</a></code>, <code><a href="#topic+lnre">lnre</a></code>, <code><a href="#topic+lnre.spc">lnre.spc</a></code>,
<code><a href="#topic+plot.tfl">plot.tfl</a></code>, <code><a href="#topic+plot.vgc">plot.vgc</a></code>, <code><a href="#topic+zipfR.par">zipfR.par</a></code>,
<code><a href="#topic+zipfR.plotutils">zipfR.plotutils</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load Italian ultra- prefix data
data(ItaUltra.spc)

## plot spectrum
plot(ItaUltra.spc)

## logarithmic scale for m (more elements are plotted)
plot(ItaUltra.spc, log="x")

## just lines
plot(ItaUltra.spc, log="x", points=FALSE)

## just the first five elements, then the first 100
plot(ItaUltra.spc, m.max=5)
plot(ItaUltra.spc, m.max=100, log="x")

## compute zm model and expeccted spectrum
zm &lt;- lnre("zm", ItaUltra.spc)
zm.spc &lt;- lnre.spc(zm, N(ItaUltra.spc))

## compare observed and expected spectra (also
## in black and white to print on papers)
plot(ItaUltra.spc, zm.spc, legend=c("observed", "expected"))
plot(ItaUltra.spc, zm.spc, legend=c("observed", "expected"), bw=TRUE)
plot(ItaUltra.spc, zm.spc, legend=c("observed", "expected"), log="x")
plot(ItaUltra.spc, zm.spc, legend=c("observed", "expected"), log="x", bw=TRUE)

## re-generate expected spectrum with variances
zm.spc &lt;- lnre.spc(zm, N(ItaUltra.spc), variances=TRUE)

## now 95% ci is shown in log plot
plot(zm.spc, log="x")

## different title and labels
plot(zm.spc, log="x", main="Expected Spectrum with Confidence Interval",
     xlab="spectrum elements", ylab="expected type counts")

## can pass list of spectra in first argument with explicit call
plot.spc(Baayen2001[1:7], m.max=6, legend=names(Baayen2001)[1:7])
</code></pre>

<hr>
<h2 id='plot.tfl'>Plot Type-Frequency List / Zipf Ranking (zipfR)</h2><span id='topic+plot.tfl'></span>

<h3>Description</h3>

<p>Zipf ranking plot of a type-frequency list, or comparison of several Zipf rankings,
on linear or logarithmic scale.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tfl'
plot(x, y, ...,
     min.rank=1, max.rank=NULL, log="", 
     type=c("p", "l", "b", "o", "s"),
     xlim=NULL, ylim=NULL, freq=TRUE,
     xlab="rank", ylab="frequency", legend=NULL, grid=FALSE,
     main="Type-Frequency List (Zipf ranking)",
     bw=zipfR.par("bw"), cex=1, steps=200,
     pch=NULL, lty=NULL, lwd=NULL, col=NULL)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.tfl_+3A_x">x</code>, <code id="plot.tfl_+3A_y">y</code>, <code id="plot.tfl_+3A_...">...</code></td>
<td>
<p>one or more objects of class <code>tfl</code>, containing the
type frequency list(s) to be plotted.  LNRE models of class <code>lnre</code>
can also be specified to display the corresponding population Zipf rankings
(see &lsquo;Details&rsquo; for more information).  It is also possible to pass
all objects as a list in argument <code>x</code>, but the method needs to be called
explicitly in this case (see &lsquo;Examples&rsquo;).</p>
</td></tr>
<tr><td><code id="plot.tfl_+3A_min.rank">min.rank</code>, <code id="plot.tfl_+3A_max.rank">max.rank</code></td>
<td>
<p>range of Zipf ranks to be plotted for each type-frequency list.
By default, all ranks are shown.</p>
</td></tr>
<tr><td><code id="plot.tfl_+3A_log">log</code></td>
<td>
<p>a character string specifying the axis or axes for which
logarithmic scale is to be used (<code>"x"</code>, <code>"y"</code>, or
<code>"xy"</code>), similar to the <code>log</code> argument of
<code><a href="graphics.html#topic+plot.default">plot.default</a></code>.</p>
</td></tr>
<tr><td><code id="plot.tfl_+3A_type">type</code></td>
<td>
<p>what type of plot should be drawn. Types <code>p</code> (points),
<code>l</code> (lines), <code>b</code> (both) and <code>o</code> (points over lines) are the
same as in <code><a href="graphics.html#topic+plot.default">plot.default</a></code>. Type <code>s</code> plots a step function 
with type frequencies corresponding to right upper corners (i.e.
type <code>S</code> in <code><a href="graphics.html#topic+plot">plot</a></code>); it is recommended for plotting full
type-frequency lists and can be much more efficient than the other types.
See &lsquo;Details&rsquo; below.</p>
</td></tr>
<tr><td><code id="plot.tfl_+3A_xlim">xlim</code>, <code id="plot.tfl_+3A_ylim">ylim</code></td>
<td>
<p>visible range on x- and y-axis.  The default values
are automatically determined to fit the selected data in the plot.</p>
</td></tr>
<tr><td><code id="plot.tfl_+3A_freq">freq</code></td>
<td>
<p>if <code>freq=FALSE</code>, plot relative frequency (per million words) instead of
absolute frequency on the y-axis.  This is useful for comparing type-frequency lists
with different sample size and is required for plotting LNRE populations.</p>
</td></tr>
<tr><td><code id="plot.tfl_+3A_xlab">xlab</code>, <code id="plot.tfl_+3A_ylab">ylab</code></td>
<td>
<p>labels for the x-axis and y-axis.</p>
</td></tr>
<tr><td><code id="plot.tfl_+3A_legend">legend</code></td>
<td>
<p>optional vector of character strings or expressions,
specifying labels for a legend box, which will be drawn in the upper
right-hand corner of the screen.  If <code>legend</code> is given, its
length must correspond to the number of type-frequeny lists in the
plot.</p>
</td></tr>
<tr><td><code id="plot.tfl_+3A_grid">grid</code></td>
<td>
<p>whether to display a suitable grid in the background of the plot 
(only for logarithmic axis)</p>
</td></tr>
<tr><td><code id="plot.tfl_+3A_main">main</code></td>
<td>
<p>a character string or expression specifying a main title for the plot</p>
</td></tr>
<tr><td><code id="plot.tfl_+3A_bw">bw</code></td>
<td>
<p>if <code>TRUE</code>, draw plot in B/W style (default is the
global <code><a href="#topic+zipfR.par">zipfR.par</a></code> setting)</p>
</td></tr>
<tr><td><code id="plot.tfl_+3A_cex">cex</code></td>
<td>
<p>scaling factor for plot symbols (types <code>"p"</code>, <code>"b"</code> and <code>"o"</code>).
This scaling factor is <em>not</em> applied to other text elements in the plot;
use <code><a href="graphics.html#topic+par">par</a></code> for this purpose.</p>
</td></tr>
<tr><td><code id="plot.tfl_+3A_steps">steps</code></td>
<td>
<p>number of steps for drawing population Zipf rankings of LNRE models. These
are always drawn as lines (regardless of <code>type</code>) and are not aligned with integer
type ranks (because the LNRE models are actually continuous approximations).</p>
</td></tr>
<tr><td><code id="plot.tfl_+3A_pch">pch</code>, <code id="plot.tfl_+3A_lty">lty</code>, <code id="plot.tfl_+3A_lwd">lwd</code>, <code id="plot.tfl_+3A_col">col</code></td>
<td>
<p>style vectors that can be used to
override the global styles defined by <code><a href="#topic+zipfR.par">zipfR.par</a></code>.  If
these vectors are specified, they must contain at least as many
elements as the number of type-frequency lists shown in the plot:
the values are <em>not</em> automatically recycled.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The type-frequency lists are shown as Zipf plots, i.e. scatterplots of
the Zipf-ranked frequencies on a linear or logarithmic scale.  Only a 
sensible subset of the default plotting styles described in <code><a href="graphics.html#topic+plot">plot</a></code>
are supported: <code>p</code> (points), <code>l</code> (lines), <code>b</code> (both, with a margin around points), 
<code>o</code> (both overplotted) and <code>s</code> (stair steps, but actually of type <code>S</code>).
</p>
<p>For plotting complete type-frequency lists from larger samples, type <code>s</code> is
strongly recommended.  It aggregates all types with the same frequency and is thus
much more efficient than the other plot types.  Note that the points shown by the 
other plot types coincide with the the right upper corners of the stair steps.
</p>
<p>Trained LNRE models can also be included in the plot, but only with <code>freq=FALSE</code>.
In this case, the corresponding
population Zipf rankings are displayed as lines (i.e. always type <code>l</code>, regardless
of the <code>type</code> parameter). The lines are intended to be smooth and are not aligned
with integer type ranks in order to highlight the fact that LNRE models are continuous
approximations of the discrete population. 
</p>
<p>Line and point styles are defined globally through <code>zipfR.par</code>,
but can be overridden with the optional parameters <code>pch</code>,
<code>lty</code>, <code>lwd</code> and <code>col</code>.  In most cases, it is more advisable to
change the global settings temporarily for a sequence of plots, though.
</p>
<p>The <code>bw</code> parameter is used to switch between B/W and colour
modes.  It can also be set globally with <code>zipfR.par</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tfl">tfl</a></code>, <code><a href="#topic+vec2tfl">vec2tfl</a></code>, <code><a href="#topic+rlnre">rlnre</a></code>, <code><a href="#topic+spc2tfl">spc2tfl</a></code>, 
<code><a href="#topic+plot.spc">plot.spc</a></code>, <code><a href="#topic+plot.vgc">plot.vgc</a></code>, <code><a href="#topic+plot.lnre">plot.lnre</a></code>,
<code><a href="#topic+zipfR.par">zipfR.par</a></code>, <code><a href="#topic+zipfR.plotutils">zipfR.plotutils</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## plot tiny type-frequency lists (N = 100) for illustration
tfl1 &lt;- vec2tfl(EvertLuedeling2001$bar[1:100])
tfl2 &lt;- vec2tfl(EvertLuedeling2001$lein[1:100])
plot(tfl1, type="b")
plot(tfl1, type="b", log="xy")
plot(tfl1, tfl2, legend=c("bar", "lein"))

## realistic type-frequency lists (type="s" recommended for efficiency)
tfl1 &lt;- spc2tfl(BrownImag.spc)
tfl2 &lt;- spc2tfl(BrownInform.spc)
plot(tfl1, tfl2, log="xy", type="s", 
     legend=c("fiction", "non-fiction"), grid=TRUE)
## always use freq=FALSE to compare samples of different size
plot(tfl1, tfl2, log="xy", type="s", freq=FALSE,
     legend=c("fiction", "non-fiction"), grid=TRUE)

## show Zipf-Mandelbrot law fitted to low end of frequency spectrum
m1 &lt;- lnre("zm", BrownInform.spc)
m2 &lt;- lnre("fzm", BrownInform.spc)
plot(tfl1, tfl2, m1, m2, log="xy", type="s", freq=FALSE, grid=TRUE,
     legend=c("fiction", "non-fiction", "ZM", "fZM"))

## call plot.tfl explicitly if only LNRE populations are displayed
plot.tfl(m1, m2, max.rank=1e5, freq=FALSE, log="xy")

## first argument can then also be a list of TFLs and/or LNRE models
plot.tfl(lapply(EvertLuedeling2001, vec2tfl), log="xy", type="s", freq=FALSE, 
         legend=names(EvertLuedeling2001))
</code></pre>

<hr>
<h2 id='plot.vgc'>Plot Vocabulary Growth Curves (zipfR)</h2><span id='topic+plot.vgc'></span>

<h3>Description</h3>

<p>Plot a vocabulary growth curve (i.e., <code class="reqn">V(N)</code> or <code class="reqn">V_m(N)</code>
against <code class="reqn">N</code>), or a comparison of several vocabulary growth curves.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'vgc'
plot(x, y, ...,
     m=NULL, add.m=NULL, N0=NULL,
     conf.level=.95, conf.style=c("ticks", "lines"),
     log=c("", "x", "y", "xy"),
     bw=zipfR.par("bw"), 
     xlim=NULL, ylim=NULL,
     xlab="N", ylab="V(N)", legend=NULL,
     main="Vocabulary Growth",
     lty=NULL, lwd=NULL, col=NULL)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.vgc_+3A_x">x</code>, <code id="plot.vgc_+3A_y">y</code>, <code id="plot.vgc_+3A_...">...</code></td>
<td>
<p>one or more objects of class <code>vgc</code>, representing
observed or expected vocabulary growth curves to be plotted. Alternatively, 
all VGCs can be passed as a list in the <code>x</code> argument if the method is
called explicitly (see &lsquo;Examples&rsquo;.</p>
</td></tr>
<tr><td><code id="plot.vgc_+3A_m">m</code></td>
<td>
<p>a single integer <code class="reqn">m</code> in the range <code class="reqn">1 \ldots 9</code>.  If
specified, graphs will be plotted for <code class="reqn">V_m(N)</code> instead of
<code class="reqn">V(N)</code> (the default).  Note that all <code>vgc</code> objects to be
plotted must contain the necessary data in this case.</p>
</td></tr>
<tr><td><code id="plot.vgc_+3A_add.m">add.m</code></td>
<td>
<p>a vector of integers in the range <code class="reqn">1 \ldots 9</code>.  If
specified, graphs for <code class="reqn">V_m(N)</code> will be added as thin lines to
the default <code class="reqn">V(N)</code> curve, for all specified frequency classes
<code class="reqn">m</code>.  This option cannot be combined with the <code>m</code> option
above.  See &quot;Details&quot; below.</p>
</td></tr>
<tr><td><code id="plot.vgc_+3A_n0">N0</code></td>
<td>
<p>if specified, draw a dashed vertical line at <code class="reqn">N=N_0</code>,
indicating the sample size where a LNRE model has been estimated
(this is never done automatically)</p>
</td></tr>
<tr><td><code id="plot.vgc_+3A_log">log</code></td>
<td>
<p>a character string specifying the axis or axes for which
logarithmic scale is to be used (<code>"x"</code>, <code>"y"</code>, or
<code>"xy"</code>), similar to the <code>log</code> argument of
<code><a href="graphics.html#topic+plot.default">plot.default</a></code>.  By default, both axes use linear scale
(also see &quot;Details&quot; below).</p>
</td></tr>
<tr><td><code id="plot.vgc_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level for confidence intervals around
expected vocabulary growth curves (see &quot;Details&quot; below).  The
default value of <code class="reqn">.95</code> produces 95%-confidence intervals.
Set to <code>NA</code> in order to suppress confidence interval
markers.</p>
</td></tr>
<tr><td><code id="plot.vgc_+3A_conf.style">conf.style</code></td>
<td>
<p>if <code>"ticks"</code>, confidence intervals are
indicated by vertical lines at each data point in the <code>vgc</code>
object (default).  If <code>"lines"</code>, confidence intervals are
indicated by thin curves above and below the VGC (which may be
difficult to see when plotting multiple VGCs).  Notice that
confidence intervals might be so narrow as to be invisible in
plots (one way to visualize them in such case might be to set an
extremely conservative confidence level, such as <code class="reqn">.9999</code>).</p>
</td></tr>
<tr><td><code id="plot.vgc_+3A_bw">bw</code></td>
<td>
<p>if <code>TRUE</code>, draw plot in B/W style (default is the
global <code><a href="#topic+zipfR.par">zipfR.par</a></code> setting)</p>
</td></tr>
<tr><td><code id="plot.vgc_+3A_xlim">xlim</code>, <code id="plot.vgc_+3A_ylim">ylim</code></td>
<td>
<p>visible range on x- and y-axis.  The default values
are automatically determined to fit the selected data in the plot.</p>
</td></tr>
<tr><td><code id="plot.vgc_+3A_xlab">xlab</code>, <code id="plot.vgc_+3A_ylab">ylab</code></td>
<td>
<p>labels for the x-axis and y-axis.  The default
values nicely typeset mathematical expressions.  The y-axis label
also distinguishes between observed and expected vocabulary growth
curves, as well as between <code class="reqn">V(N)</code> and <code class="reqn">V_m(N)</code>.</p>
</td></tr>
<tr><td><code id="plot.vgc_+3A_main">main</code></td>
<td>
<p>a character string or expression specifying a main title
for the plot</p>
</td></tr>
<tr><td><code id="plot.vgc_+3A_legend">legend</code></td>
<td>
<p>optional vector of character strings or expressions,
specifying labels for a legend box, which will be drawn in the lower
right-hand corner of the screen.  If <code>legend</code> is given, its
length must correspond to the number of VGCs in the plot.</p>
</td></tr>
<tr><td><code id="plot.vgc_+3A_lty">lty</code>, <code id="plot.vgc_+3A_lwd">lwd</code>, <code id="plot.vgc_+3A_col">col</code></td>
<td>
<p>style vectors that can be used to override the
global styles defined by <code><a href="#topic+zipfR.par">zipfR.par</a></code>.  If these vectors
are specified, they must contain at least as many elements as there
are VGCs in the plot: the values are <em>not</em> automatically
recycled.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default, standard vocabulary growth curves are plotted for all
specified <code>vgc</code> objects, i.e. graphs of <code class="reqn">V(N)</code> against
<code class="reqn">N</code>.  If <code>m</code> is specified, growth curves for hapax legomena
or other frequency classes are shown instead, i.e. graphs of
<code class="reqn">V_m(N)</code> against <code class="reqn">N</code>.  In this case, all <code>vgc</code> objects
must contain the necessary data for <code class="reqn">V_m(N)</code>.
</p>
<p>Alternatively, the option <code>add.m</code> can be used to display growth
curves for one or more spectrum elements <em>in addition</em> to the
standard VGCs.  These growth curves are plotted as thinner lines,
otherwise matching the styles of the main curves.  Since such plots
can become fairly confusing and there is no finer control over the
styles of the additional curves, it is generally not recommended to
make use of the <code>add.m</code> option.
</p>
<p>Confidence intervals are indicated for expected vocabulary growth
curves with variance data, either by short vertical lines
(<code>conf.style="ticks"</code>, the default) or by thin curves above and
below the main growth curve (<code>conf.style="lines"</code>). The size of
the confidence intervals is controlled by the <code>conf.level</code>
parameter (default: 95%).  Set <code>conf.level=NA</code> in order to
suppress the confidence interval indicators.
</p>
<p>In y-logarithmic plots, data points with <code class="reqn">V(N) = 0</code> or <code class="reqn">V_m(N)
  = 0</code> are drawn outside the plot region (below the bottom margin)
rather than skipped.
</p>
<p>Line and point styles can be defined globally with <code>zipfR.par</code>.
They can be overridden locally with the optional parameters
<code>lty</code>, <code>lwd</code> and <code>col</code>, but this should only be used
when absolutely necessary.  In most cases, it is more advisable to
change the global settings temporarily for a sequence of plots.
</p>
<p>The <code>bw</code> parameter is used to switch between B/W and color
modes.  It can also be set globally with <code>zipfR.par</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vgc">vgc</a></code>, <code><a href="#topic+lnre">lnre</a></code>, <code><a href="#topic+lnre.vgc">lnre.vgc</a></code>,
<code><a href="#topic+plot.tfl">plot.tfl</a></code>, <code><a href="#topic+plot.spc">plot.spc</a></code>, <code><a href="#topic+zipfR.par">zipfR.par</a></code>,
<code><a href="#topic+zipfR.plotutils">zipfR.plotutils</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load Our Mutual Friend spectrum and empirical vgc
data(DickensOurMutualFriend.emp.vgc)
data(DickensOurMutualFriend.spc)

## plot empirical V and V1 growth
plot(DickensOurMutualFriend.emp.vgc,add.m=1)

## use log scale for y-axis
plot(DickensOurMutualFriend.emp.vgc,add.m=1,log="y")

## binomially interpolated vgc at same points as
## empirical vgc
omf.bin.vgc &lt;- vgc.interp(DickensOurMutualFriend.spc,N(DickensOurMutualFriend.emp.vgc))

## compare empirical and interpolated vgc, also with
## thinner lines, and in black and white
plot(DickensOurMutualFriend.emp.vgc,omf.bin.vgc,legend=c("observed","interpolated"))
plot(DickensOurMutualFriend.emp.vgc,omf.bin.vgc,legend=c("observed","interpolated"),lwd=c(1,1)) 
plot(DickensOurMutualFriend.emp.vgc,omf.bin.vgc,legend=c("observed","interpolated"),bw=TRUE)


## load Great Expectations spectrum and use it to
## compute ZM model
data(DickensGreatExpectations.spc)
ge.zm &lt;- lnre("zm",DickensGreatExpectations.spc)

## expected V of Great Expectations at sample
## sizes of OMF's interpolated vgc
ge.zm.vgc &lt;- lnre.vgc(ge.zm,N(omf.bin.vgc))

## compare interpolated OMF Vs and inter/extra-polated
## GE Vs, with a vertical line at sample size
## used to compute GE model
plot(omf.bin.vgc,ge.zm.vgc,N0=N(ge.zm),legend=c("OMF","GE"))


## load Italian ultra- prefix data and compute zm model
data(ItaUltra.spc)
ultra.zm &lt;- lnre("zm",ItaUltra.spc)

## compute vgc up to about twice the sample size
## with variance of V
ultra.zm.vgc &lt;- lnre.vgc(ultra.zm,(1:100)*70, variances=TRUE)

## plot with confidence intervals derived from variance in
## vgc (with larger datasets, ci will typically be almost
## invisible)
plot(ultra.zm.vgc)

## use more conservative confidence level, and plot 
## the intervals as lines
plot(ultra.zm.vgc,conf.level=.99,conf.style="lines")

## suppress ci plotting, and insert different title and labels
plot(ultra.zm.vgc,conf.level=NA,main="ultra-",xlab="sample sizes",ylab="types")

## load Brown adjective spectrum
## (about 80k tokens) 
data(BrownAdj.spc)

## binomially interpolated curve of V and V_1 to V_5
BrownAdj.bin.vgc &lt;- vgc.interp(BrownAdj.spc,(1:100)*800,m.max=5)

## plot with V and 5 spectrum elements
plot(BrownAdj.bin.vgc,add.m=c(1:5))

## can pass list of VGCs in first argument with explicit call
plot.vgc(lapply(EvertLuedeling2001, vec2vgc), 
         xlim=c(0, 30000), ylim=c(0, 1200),
         legend=names(EvertLuedeling2001))
</code></pre>

<hr>
<h2 id='print.lnre'>Printing LNRE Models (zipfR)</h2><span id='topic+print.lnre'></span><span id='topic+summary.lnre'></span>

<h3>Description</h3>

<p>Implementations of the <code><a href="base.html#topic+print">print</a></code> and <code><a href="base.html#topic+summary">summary</a></code>
methods for LNRE models (subclasses of <code>lnre</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  ## S3 method for class 'lnre'
print(x, ...)

  ## S3 method for class 'lnre'
summary(object, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.lnre_+3A_x">x</code>, <code id="print.lnre_+3A_object">object</code></td>
<td>
<p>an object of class <code>lnre</code> or one of its
subclasses, representing a LNRE model</p>
</td></tr>
<tr><td><code id="print.lnre_+3A_...">...</code></td>
<td>
<p>other arguments passed on from generic method will be ignored</p>
</td></tr>
</table>


<h3>Details</h3>

<p><b>NB:</b> implementation details and format of the summary are
subject to change in future releases
</p>
<p>In the current implementation, <code>print</code> and <code>summary</code> produce
the same output for LNRE models.
</p>
<p>This summary comprises the type of LNRE model, its parameter values,
derived parameters such as normalization constants, and the population
size <code class="reqn">S</code>.
</p>
<p>If the model parameters have been estimated from an observed frequency
spectrum, a comparison of the observed and expected frequency spectrum
is shown, including goodness-of-fit statistics.
</p>


<h3>Value</h3>

<p><code>NULL</code>
</p>
<p>Unlike other implementations of the <code>summary</code> method,
<code>summary.lnre</code> only prints a summary on screen and does not return
a special &quot;summary&quot; object.
</p>


<h3>See Also</h3>

<p>See the <code><a href="#topic+lnre">lnre</a></code> manpage for more information on LNRE
models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# load Brown verbs dataset and estimate lnre models
data(BrownVer.spc)
zm &lt;- lnre("zm",BrownVer.spc)
fzm &lt;- lnre("fzm",BrownVer.spc,exact=FALSE)
gigp &lt;- lnre("gigp",BrownVer.spc)

# look at summaries with either summary or print
summary(zm)
print(zm)

summary(fzm)
print(fzm)

summary(gigp)
print(gigp)

</code></pre>

<hr>
<h2 id='print.spc'>Printing Frequency Spectra (zipfR)</h2><span id='topic+print.spc'></span><span id='topic+summary.spc'></span>

<h3>Description</h3>

<p>Implementations of the <code><a href="base.html#topic+print">print</a></code> and <code><a href="base.html#topic+summary">summary</a></code>
methods for frequency spectrum objects (of class <code>spc</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  ## S3 method for class 'spc'
print(x, all=FALSE, ...)

  ## S3 method for class 'spc'
summary(object, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.spc_+3A_x">x</code>, <code id="print.spc_+3A_object">object</code></td>
<td>
<p>an object of class <code>spc</code>, representing a
frequency spectrum</p>
</td></tr>
<tr><td><code id="print.spc_+3A_all">all</code></td>
<td>
<p>if <code>FALSE</code>, only the first ten non-empty frequency
classes will be shown (default)</p>
</td></tr>
<tr><td><code id="print.spc_+3A_...">...</code></td>
<td>
<p>other arguments passed on from generic method will be ignored</p>
</td></tr>
</table>


<h3>Details</h3>

<p><b>NB:</b> implementation details and format of the summary are subject to
change in future releases
</p>
<p><code>print.spc</code> works similar to the standard <code>print</code> method for
data frames, but provides additional information about <code class="reqn">N</code> and
<code class="reqn">V</code>.  Unless <code>all</code> is set to <code>TRUE</code>, only the first ten
non-zero spectrum elements will be shown.
</p>
<p><code>summary.spc</code> gives a concise summary of the most important
information about the frequency spectrum.  In addition to <code class="reqn">N</code>
<code class="reqn">V</code>, the first spectrum elements are shown.  The summary will also
indicate whether the spectrum is incomplete, an expected spectrum, or
has variances (but does not show the variances).
</p>


<h3>Value</h3>

<p><code>NULL</code>
</p>
<p>Unlike other implementations of the <code>summary</code> method,
<code>summary.spc</code> only prints a summary on screen and does not return
a special &quot;summary&quot; object.
</p>


<h3>See Also</h3>

<p>See the <code><a href="#topic+spc">spc</a></code> manpage for details on <code>spc</code> objects.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load Brown verbs dataset
data(BrownVer.spc)

## look at summary and print BrownVer.spc
summary(BrownVer.spc)
print(BrownVer.spc)

## print all non-zero spectrum elements
print(BrownVer.spc,all=TRUE)

## estimate zm model and construct expected spectrum with
## variances
zm &lt;- lnre("zm",BrownVer.spc)
zm.spc &lt;- lnre.spc(zm,N(zm),variances=TRUE)

## summary and print for the expected spectrum
summary(zm.spc)
print(zm.spc)

</code></pre>

<hr>
<h2 id='print.tfl'>Printing Type Frequency Lists (zipfR)</h2><span id='topic+print.tfl'></span><span id='topic+summary.tfl'></span>

<h3>Description</h3>

<p>Implementations of the <code><a href="base.html#topic+print">print</a></code> and <code><a href="base.html#topic+summary">summary</a></code>
methods for type frequency list objects (of class <code>tfl</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  ## S3 method for class 'tfl'
print(x, all=FALSE, ...)

  ## S3 method for class 'tfl'
summary(object, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.tfl_+3A_x">x</code>, <code id="print.tfl_+3A_object">object</code></td>
<td>
<p>an object of class <code>tfl</code>, representing a type
frequency list</p>
</td></tr>
<tr><td><code id="print.tfl_+3A_all">all</code></td>
<td>
<p>if <code>FALSE</code>, only the twenty most frequent types will
be shown (default)</p>
</td></tr>
<tr><td><code id="print.tfl_+3A_...">...</code></td>
<td>
<p>other arguments passed on from generic method will be ignored</p>
</td></tr>
</table>


<h3>Details</h3>

<p><b>NB:</b> implementation details and format of the summary are subject to
change in future releases
</p>
<p><code>print.tfl</code> works similar to the standard <code>print</code> method for
data frames, but provides additional information about <code class="reqn">N</code> and
<code class="reqn">V</code>.  Unless <code>all</code> is set to <code>TRUE</code>, only the twenty
most frequent types will be shown.
</p>
<p><code>summary.tfl</code> gives a concise summary of the most important
information about the type frequency list.  In addition to showing
<code class="reqn">N</code> <code class="reqn">V</code>, the summary also indicates whether the list is
incomplete and shows examples of type representations (if present).
</p>


<h3>Value</h3>

<p><code>NULL</code>
</p>
<p>Unlike other implementations of the <code>summary</code> method,
<code>summary.tfl</code> only prints a summary on screen and does not return
a special &quot;summary&quot; object.
</p>


<h3>See Also</h3>

<p>See the <code><a href="#topic+tfl">tfl</a></code> manpage for details on <code>tfl</code> objects.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load Brown tfl
data(Brown.tfl)

## summary and print most frequent types
summary(Brown.tfl)
print(Brown.tfl)

## the whole type list (don't try this unless you have some time to spare)
## Not run: 
print(Brown.tfl,all=TRUE)
## End(Not run)
</code></pre>

<hr>
<h2 id='print.vgc'>Printing Vocabulary Growth Curves (zipfR)</h2><span id='topic+print.vgc'></span><span id='topic+summary.vgc'></span>

<h3>Description</h3>

<p>Implementations of the <code><a href="base.html#topic+print">print</a></code> and <code><a href="base.html#topic+summary">summary</a></code>
methods for vocabulary growth curve objects (of class <code>vgc</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  ## S3 method for class 'vgc'
print(x, all=FALSE, ...)

  ## S3 method for class 'vgc'
summary(object, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.vgc_+3A_x">x</code>, <code id="print.vgc_+3A_object">object</code></td>
<td>
<p>an object of class <code>vgc</code>, representing a vocabulary
growth curve</p>
</td></tr>
<tr><td><code id="print.vgc_+3A_all">all</code></td>
<td>
<p>if <code>FALSE</code>, vocabulary growth data are shown for at
most 25 sample sizes (default)</p>
</td></tr>
<tr><td><code id="print.vgc_+3A_...">...</code></td>
<td>
<p>other arguments passed on from generic method will be ignored</p>
</td></tr>
</table>


<h3>Details</h3>

<p><b>NB:</b> implementation details and format of the summary are subject to
change in future releases
</p>
<p><code>print.vgc</code> calls the standard <code>print</code> method for
data frames internally, but reduces the data set randomly to
show at most 25 sample sizes (unless <code>all=TRUE</code>).
</p>
<p><code>summary.vgc</code> gives a concise summary of the available vocabulary
growth data in the <code>vgc</code> object, including the number and range
of sample sizes, whether spectrum elements are included, and whether
variances are included.
</p>


<h3>Value</h3>

<p><code>NULL</code>
</p>
<p>Unlike other implementations of the <code>summary</code> method,
<code>summary.vgc</code> only prints a summary on screen and does not return
a special &quot;summary&quot; object.
</p>


<h3>See Also</h3>

<p>See the <code><a href="#topic+vgc">vgc</a></code> manpage for details on <code>vgc</code> objects.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load Brown "informative" prose empirical vgc
data(BrownInform.emp.vgc)

## summary, print (random subset) and print all
summary(BrownInform.emp.vgc)
print(BrownInform.emp.vgc)
print(BrownInform.emp.vgc,all=TRUE)

## load Brown informative prose spectrum
## and get estimate a fzm model
data(BrownInform.spc)
fzm &lt;- lnre("fzm",BrownInform.spc,exact=FALSE)

## obtain expected vgc up to 2M tokens
## with spectrum elements up to V_3
## and variances
fzm.vgc &lt;- lnre.vgc(fzm,(1:100)*2e+4,m.max=3,variances=TRUE)

## summary and print
summary(fzm.vgc)
print(fzm.vgc)
print(fzm.vgc,all=TRUE)

</code></pre>

<hr>
<h2 id='productivity.measures'>Measures of Productivity and Lexical Richness (zipfR)</h2><span id='topic+productivity.measures'></span><span id='topic+productivity.measures.tfl'></span><span id='topic+productivity.measures.spc'></span><span id='topic+productivity.measures.vgc'></span><span id='topic+productivity.measures.default'></span>

<h3>Description</h3>

<p>Compute various measures of productivity and lexical richness from
an observed frequency spectrum or type-frequency list, from an
observed vocabulary growth curve, or from a vector of tokens.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>productivity.measures(obj, measures, data.frame=TRUE, ...)

## S3 method for class 'tfl'
productivity.measures(obj, measures, data.frame=TRUE, ...)
## S3 method for class 'spc'
productivity.measures(obj, measures, data.frame=TRUE, ...)
## S3 method for class 'vgc'
productivity.measures(obj, measures, data.frame=TRUE, ...)

## Default S3 method:
productivity.measures(obj, measures, data.frame=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="productivity.measures_+3A_obj">obj</code></td>
<td>
<p>a suitable data object from which productivity measures
can be computed. Currently either a frequency spectrum
(of class <code>spc</code>), a type-frequency list (of class <code>tfl</code>),
a vocabulary growth curve (of class <code>vgc</code>), or a token vector.</p>
</td></tr>
<tr><td><code id="productivity.measures_+3A_measures">measures</code></td>
<td>
<p>character vector naming the productivity measures to
be computed (see &quot;Productivity Measures&quot; below).
Names may be abbreviated as long as they remain unique.
If unspecified, all supported measures are computed.</p>
</td></tr>
<tr><td><code id="productivity.measures_+3A_data.frame">data.frame</code></td>
<td>
<p>if <code>TRUE</code>, the return value is converted to a data frame
for convenience in interactive use (default).</p>
</td></tr>
<tr><td><code id="productivity.measures_+3A_...">...</code></td>
<td>
<p>additional arguments passed on to the method implementations
(currently, no further arguments are recognized)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes productivity measures based on an observed frequency spectrum, type-frequency list or vocabulary growth curve.
If an <em>expected</em> spectrum or VGC is passed, the expectations <code class="reqn">E[V]</code>, <code class="reqn">E[V_m]</code> will simply be substituted for the sample values <code class="reqn">V</code>, <code class="reqn">V_m</code> in the equations. In most cases, this does <em>not</em> yield the expected value of the productivity measure!
</p>
<p>Some measures can only be computed from a complete frequency spectrum.  They will return <code>NA</code> if <code>obj</code> is an incomplete spectrum or type-frequency list, an expected spectrum or a vocabulary growth curve is passed.
</p>
<p>Some other measures can only be computed is a sufficient number of spectrum elements is included in a vocabulary growth curve (usually at least
<code class="reqn">V_1</code> and <code class="reqn">V_2</code>), and will return <code>NA</code> otherwise.
</p>
<p>Such limitations are indicated in the list of measures below (unless spectrum elements <code class="reqn">V_1</code> and <code class="reqn">V_2</code> are sufficient).
</p>


<h3>Value</h3>

<p>If <code>obj</code> is a frequency spectrum, type-frequency list or token vector:
A numeric vector of the same length as <code>measures</code> with the corresponding observed values of the productivity measures.
If <code>data.frame=TRUE</code> (the default), a single-row data frame is returned.
</p>
<p>If <code>obj</code> is a vocabulary growth curve:
A numeric matrix with columns corresponding to the selected productivity measures and rows corresponding to the sample sizes of the vocabulary growth curve.
If <code>data.frame=TRUE</code> (the default), the matrix is converted to a data frame.
</p>


<h3>Productivity Measures</h3>

<p>The following productivity measures are currently supported:
</p>

<dl>
<dt><code>V</code>:</dt><dd>
<p>the total number of types <code class="reqn">V</code> 
</p>
</dd>
<dt><code>TTR</code>:</dt><dd>
<p>the type-token ratio TTR = <code class="reqn">V / N</code>
</p>
</dd>
<dt><code>R</code>:</dt><dd> 
<p>Guiraud's (1954) <code class="reqn">R = V / \sqrt{N}</code>. An equivalent measure is Carroll's (1964) <code class="reqn">CTTR = R / \sqrt{2}</code>.
</p>
</dd>
<dt><code>C</code>:</dt><dd>
<p>Herdan's (1964) <code class="reqn">C = \frac{ \log V }{ \log N }</code> 
</p>
</dd>
<dt><code>k</code>:</dt><dd>
<p>Dugast's (1979) <code class="reqn">k = \frac{ \log V }{ \log \log N}</code>
</p>
</dd>
<dt><code>U</code>:</dt><dd>
<p>Dugast's (1978, 1979) <code class="reqn">U = \frac{ (\log N)^2 }{ \log N - \log V}</code>.
Maas (1972) proposed an equivalent measure <code class="reqn">a^2 = 1 / U</code>.
</p>
</dd>
<dt><code>W</code>:</dt><dd>
<p>Brunet's (1978) <code class="reqn">W = N ^ {V ^ {-a}}</code> with <code class="reqn">a = 0.172</code>. 
</p>
</dd>
<dt><code>P</code>:</dt><dd>
<p>Baayen's (1991) productivity index <code class="reqn">P = \frac{V_1}{N}</code>, which corresponds to the slope of the vocabulary growth curve (under random sampling assumptions) 
</p>
</dd>
<dt><code>Hapax</code>:</dt><dd>
<p>the proportion of <em>hapax legomena</em> <code class="reqn">\frac{V_1}{V}</code> is a direct estimate for the parameter <code class="reqn">\alpha = 1 / a</code> of a population following the Zipf-Mandelbrot law (Evert 2004b: 130).
</p>
</dd>
<dt><code>H</code>:</dt><dd>
<p>Honoré's (1979) <code class="reqn">H = 100 \frac{ \log N }{ 1 - V_1 / V }</code>, a transformation of the proportion of <em>hapax legomena</em> adjusted for sample size
</p>
</dd>
<dt><code>S</code>:</dt><dd>
<p>Sichel's (1975) <code class="reqn">S = V_2 / V</code>, i.e. the proportion of <em>dis legomena</em>. Michéa's (1969, 1971) <code class="reqn">M = 1 / S</code> is an equivalent measure.
</p>
</dd>
<dt><code>alpha2</code>:</dt><dd>
<p>Evert's <code class="reqn">\alpha_2 = 1 - 2 \frac{V_2}{V_1}</code> is another direct estimate for the parameter <code class="reqn">\alpha = 1 / a</code> of a Zipf-Mandelbrot population (Evert 2004b: 127).
</p>
</dd>
<dt><code>K</code>:</dt><dd>
<p>Yule's (1944) <code class="reqn">K = 10^4 \cdot \frac{ \sum_m m^2 V_m - N}{ N^2 }</code> <br />
(only for complete frequency spectrum or type-frequency list). Herdan (1955) proposes an almost equivalent measure <code class="reqn">v_m \approx \sqrt{K}</code> based on a different derviation. Both measures converge for large <code class="reqn">N</code> and <code class="reqn">V</code>.
Yule's <code class="reqn">K</code> is almost identical to Simpson's <code class="reqn">D</code> and is an unbiased estimator for the same population coefficient <code class="reqn">\delta</code> under an independent Poisson sampling scheme.
A measure of <em>lexical poverty</em>, i.e. smaller values correpond to higher productivity.
</p>
</dd>
<dt><code>D</code>:</dt><dd>
<p>Simpson's (1949) <code class="reqn">D = \sum_m V_m \frac{m}{N}\cdot \frac{m-1}{N-1}</code> <br />
(only for complete frequency spectrum or type-frequency list) is a slightly modified version of Yule's <code class="reqn">K</code>.
This measure is an unbiased estimator for a population coefficient <code class="reqn">\delta</code>, representing the probability of picking the same type twice in two consecutive draws from the population. 
A measure of <em>lexical poverty</em>, i.e. smaller values correpond to higher productivity.
</p>
</dd>
<dt><code>Entropy</code>:</dt><dd>
<p>Entropy of the sample frequency distribution <code class="reqn">-\sum_m V_m \frac{m}{N} \log_2 \frac{m}{N}</code> <br />
(only for complete frequency spectrum or type-frequency list). This is not a reliable estimator of population entropy. It is therefore not recommended as a productivity measure and has only been included for evaluation studies.
A measure of <em>lexical poverty</em>, i.e. smaller values correpond to higher productivity.
</p>
</dd>
<dt><code>eta</code>:</dt><dd>
<p>Normalised entropy or <em>evenness</em> <code class="reqn">\eta = \textrm{Entropy} / \log_2 V</code><br />
(only for complete frequency spectrum or type-frequency list) where <code class="reqn">\log_2 V</code> is the largest possible value for a sample with the observed vocabulary size (obtained for a uniform distribution).  Therefore, <code class="reqn">0 \le \eta \le 1</code>.
Not recommended as a productivity measure because it is expected to produce erratic and counterintuitive results.
</p>
</dd>

</dl>

<p>See Sec. 2.1 of the technical report <a href="https://zipfr.r-forge.r-project.org/materials/inside-zipfr.pdf">Inside <em>zipfR</em></a> for further details and references.
</p>


<h3>References</h3>

<p>Evert, Stefan (2004b). <em>The Statistics of Word Cooccurrences: Word
Pairs and Collocations.</em> PhD Thesis, IMS, University of Stuttgart.
URN urn:nbn:de:bsz:93-opus-23714
<a href="http://dx.doi.org/10.18419/opus-2556">http://dx.doi.org/10.18419/opus-2556</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lnre.productivity.measures">lnre.productivity.measures</a></code> for parametric bootstrapping and approximate expectations 
of productivity measures in random samples from a LNRE population.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>rbind(
  AllTexts=productivity.measures(Brown.spc),
  Fiction=productivity.measures(BrownImag.spc),
  NonFiction=productivity.measures(BrownInform.spc))

## can be applied to token vector, type-frequency list, or frequency spectrum
bar.vec &lt;- EvertLuedeling2001$bar
bar1 &lt;- productivity.measures(bar.vec)          # token vector
bar2 &lt;- productivity.measures(vec2tfl(bar.vec)) # type-frequency list
bar3 &lt;- productivity.measures(vec2spc(bar.vec)) # frequency spectrum
print(rbind(tokens=bar1, tfl=bar2, spc=bar3))

## sample-size dependency of productivity measures in Brown corpus
## (note that only a subset of the measures can be computed)
n &lt;- c(10e3, 50e3, 100e3, 200e3, 500e3, 1e6)
idx &lt;- N(Brown.emp.vgc) %in% n
my.vgc &lt;- vgc(N=N(Brown.emp.vgc)[idx],
              V=V(Brown.emp.vgc)[idx],
              Vm=list(Vm(Brown.emp.vgc, 1)[idx]))
print(my.vgc) # since we don't have a subset method for VGCs yet
productivity.measures(my.vgc)

productivity.measures(my.vgc, measures=c("TTR", "P")) # selected measures

## parametric bootstrapping to obtain sampling distribution of measures
## (much easier with ?lnre.productivity.measures)
model &lt;- lnre("zm", spc=ItaRi.spc) # realistic LNRE model
res &lt;- lnre.bootstrap(model, 1e6, ESTIMATOR=identity,
                      STATISTIC=productivity.measures)
bootstrap.confint(res, method="normal")
</code></pre>

<hr>
<h2 id='read.multiple.objects'>Reading Multiple Objects from Files (zipfR)</h2><span id='topic+read.multiple.objects'></span>

<h3>Description</h3>

<p><code>read.multiple.objects</code> constructs a list of <code><a href="#topic+spc">spc</a></code>,
<code><a href="#topic+vgc">vgc</a></code> or <code><a href="#topic+tfl">tfl</a></code> objects from a set of input
text files in the specified directory
</p>
<p><b>NB</b>: This function is intended for users that want to run
advanced experiments (e.g., handling hundreds of spectra generated in
multiple randomizations experiments). For the standard
one-object-at-a-time reading functionality, look at the documentation
of <code><a href="#topic+read.spc">read.spc</a></code>, <code><a href="#topic+read.vgc">read.vgc</a></code> and
<code><a href="#topic+read.tfl">read.tfl</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  read.multiple.objects(directory, prefix, class=c("spc", "vgc", "tfl"))

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read.multiple.objects_+3A_directory">directory</code></td>
<td>
<p>character string specifying the directory where the
target input files reside (absolute path, or path relative to
current working directory)</p>
</td></tr>
<tr><td><code id="read.multiple.objects_+3A_prefix">prefix</code></td>
<td>
<p>character string specifying prefix that must be shared
by all target input file names</p>
</td></tr>
<tr><td><code id="read.multiple.objects_+3A_class">class</code></td>
<td>
<p>one of <code>spc</code>, <code>vgc</code> or <code>tfl</code> as character
string, specifying the class of object we are importing (see the
manpages of <code><a href="#topic+spc">spc</a></code>, <code><a href="#topic+vgc">vgc</a></code> and <code><a href="#topic+tfl">tfl</a></code>
for details)</p>
</td></tr>
</table>


<h3>Format</h3>

<p><code>read.multiple.objects</code> reads in all files matching the pattern
<code>prefix.id.class</code> from the specified directory, where the
<code>prefix</code> and <code>class</code> strings are passed as arguments, and
<code>id</code> is an arbitrary string that is used as index of the
corresponding object in the output list
</p>
<p><code>read.multiple.objects</code> calls the <code>read</code> function
corresponding to the <code>class</code> argument. Thus, the input files must
respect the formatting conventions of the relevant reading functions
(see documentation of <code><a href="#topic+read.spc">read.spc</a></code>, <code><a href="#topic+read.vgc">read.vgc</a></code>
and <code><a href="#topic+read.tfl">read.tfl</a></code>)
</p>


<h3>Value</h3>

<p><code>read.multiple.objects</code> returns a list of objects of the
specified class; each object is indexed with the id extracted from the
corresponding file name (see section &quot;Format&quot;)
</p>


<h3>See Also</h3>

<p>See the <code><a href="#topic+spc">spc</a></code>, <code><a href="#topic+vgc">vgc</a></code> and <code><a href="#topic+tfl">tfl</a></code>
manpages for details on the corresponding objects;
<code><a href="#topic+read.spc">read.spc</a></code>, <code><a href="#topic+read.vgc">read.vgc</a></code> and
<code><a href="#topic+read.tfl">read.tfl</a></code> for the single-file reading functions and input
format details
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## These are just illustrative examples. Users should fill in their
## own files instead of the dummy names used here.

## suppose that the current working directory contains
## 100 spc files named: rand.1.spc, rand.2.spc, ...,
## rand.100.spc

## read the files in:
spc.list &lt;- read.multiple.objects(".","rand","spc")

## you can access each spc using the id extracted from
## the file name, e.g.:
summary(spc.list[["1"]])

## more usefully, you might want to iterate over the
## whole list, e.g., to calculate mean V:
mean(sapply(spc.list,V))

## notice that ids are arbitrary strings
## e.g., suppose that directory /home/me/animals
## contains sounds.dog.vgc and sounds.elephant.vgc

## we read the vgcs in:
vgc.list &lt;- read.multiple.objects("/home/me/animals","sounds","vgc")

## accessing the elephant vgc:
V(vgc.list[["elephant"]])

## of course, tfl-reading works in the same way (assuming
## that the animals directory also contains some tfl files):
tfl.list &lt;- read.multiple.objects("/home/me/animals","sounds","tfl")

## End(Not run)
</code></pre>

<hr>
<h2 id='read.spc'>Loading and Saving Frequency Spectra (zipfR)</h2><span id='topic+read.spc'></span><span id='topic+write.spc'></span>

<h3>Description</h3>

<p><code>read.spc</code> loads frequency spectrum from <code>.spc</code> file
</p>
<p><code>write.spc</code> saves frequency spectrum object in <code>.spc</code>
file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  read.spc(file)

  write.spc(spc, file)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read.spc_+3A_file">file</code></td>
<td>
<p>character string specifying the pathname of a disk file.
Files with extension <code>.gz</code> will automatically be compressed/decompressed.
See section &quot;Format&quot; for a description of the required file format</p>
</td></tr>
<tr><td><code id="read.spc_+3A_spc">spc</code></td>
<td>
<p>a frequency spectrum, i.e.\ an object of class
<code>spc</code></p>
</td></tr>
</table>


<h3>Format</h3>

<p>A TAB-delimited text file with column headers but no row names
(suitable for reading with <code>read.delim</code>).  The file must contain
at least the following two columns:
</p>

<dl>
<dt><code>m</code></dt><dd><p>frequency class <code class="reqn">m</code> </p>
</dd>
<dt><code>Vm</code></dt><dd><p>number <code class="reqn">V_m</code> of types in frequency class
<code class="reqn">m</code> (or expected class size <code class="reqn">E[V_m]</code>)</p>
</dd>
</dl>

<p>An optional column labelled <code>VVm</code> can be used to specify
variances of expected class sizes (for a frequency spectrum derived
from a LNRE model or by binomial interpolation).  
</p>
<p>These columns may appear in any order in the text file.  All other
columns will be silently ignored.
</p>


<h3>Details</h3>

<p>If the filename <code>file</code> ends in the extension <code>.gz</code>, <code>.bz2</code> or <code>.xz</code>,
the disk file will automatically be decompressed (<code>read.spc</code>) or compressed (<code>write.spc</code>).
</p>
<p>The <code>.spc</code> file format does not store the values of <code>N</code>,
<code>V</code> and <code>VV</code> explicitly.  Therefore, incomplete frequency
spectra and expected spectra with variances cannot be fully
reconstructed from disk files.  Saving such frequency spectra (or
loading a spectrum with variance data) will trigger corresponding
warnings.
</p>


<h3>Value</h3>

<p><code>read.spc</code> returns an object of class <code>spc</code> (see the
<code><a href="#topic+spc">spc</a></code> manpage for details)
</p>


<h3>See Also</h3>

<p>See the <code><a href="#topic+spc">spc</a></code> manpage for details on <code>spc</code>
objects. See <code><a href="#topic+read.tfl">read.tfl</a></code> and <code><a href="#topic+read.vgc">read.vgc</a></code> for
import/export of other data structures.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## save Italian ultra- frequency spectru to external text file
fname &lt;- tempfile(fileext=".spc")
write.spc(ItaUltra.spc, fname)
## now &lt;fname&gt; is a TAB-delimited text file with columns m and Vm

## we ready it back in
New.spc &lt;- read.spc(fname)

## same spectrum as ItaUltra.spc, compare:
summary(New.spc)
summary(ItaUltra.spc)

stopifnot(isTRUE(all.equal(New.spc, ItaUltra.spc))) # should be identical

## Not run: 
## DON'T do the following, incomplete spectrum will not be restored properly !!!
zm &lt;- lnre("zm", ItaUltra.spc) # estimate model
zm.spc &lt;- lnre.spc(zm,N(zm))   # incomplete spectrum from model
write.spc(zm.spc, fname)       # WARNINGS
bad.spc &lt;- read.spc(fname)     # but this function cannot know something is wrong

summary(zm.spc)
summary(bad.spc) # note that N and V are completely wrong !!!

## End(Not run)
</code></pre>

<hr>
<h2 id='read.tfl'>Loading and Saving Type Frequency Lists (zipfR)</h2><span id='topic+read.tfl'></span><span id='topic+write.tfl'></span>

<h3>Description</h3>

<p><code>read.tfl</code> loads type frequency list from <code>.tfl</code> file
</p>
<p><code>write.tfl</code> saves type frequency list object in <code>.tfl</code>
file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  read.tfl(file, encoding=getOption("encoding"))

  write.tfl(tfl, file, encoding=getOption("encoding"))

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read.tfl_+3A_file">file</code></td>
<td>
<p>character string specifying the pathname of a disk file.
Files with extension <code>.gz</code> will automatically be compressed/decompressed.
See section &quot;Format&quot; for a description of the required file format</p>
</td></tr>
<tr><td><code id="read.tfl_+3A_tfl">tfl</code></td>
<td>
<p>a type frequency list, i.e.\ an object of class <code>tfl</code></p>
</td></tr>
<tr><td><code id="read.tfl_+3A_encoding">encoding</code></td>
<td>
<p>specifies the character encoding of the disk
file to be read or written to.  See <code><a href="base.html#topic+file">file</a></code> for details.</p>
</td></tr>
</table>


<h3>Format</h3>

<p>A TAB-delimited text file with column headers but no row names
(suitable for reading with <code>read.delim</code>), containing the
following columns:
</p>

<dl>
<dt><code>f</code></dt><dd><p>type frequencies <code class="reqn">f_k</code> </p>
</dd>
<dt><code>k</code></dt><dd><p>optional: the corresponding type IDs <code class="reqn">k</code>.  If
missing, increasing non-negative integers are automatically
assigned as IDs.</p>
</dd>
<dt><code>type</code></dt><dd><p>optional: type representations (such as word
forms or lemmas)</p>
</dd>
</dl>

<p>These columns may appear in any order in the text file.  Only the
<code>f</code> column is mandatory and all unrecognized columns will be
silently ignored.
</p>


<h3>Details</h3>

<p>If the filename <code>file</code> ends in the extension <code>.gz</code>, <code>.bz2</code> pr <code>.xz</code>,
the disk file will automatically be decompressed (<code>read.tfl</code>) and compressed (<code>write.tfl</code>).
</p>
<p>The <code>.tfl</code> file format stores neither the values of <code>N</code> and
<code>V</code> nor the range of type frequencies explicitly.  Therefore,
incomplete type frequency lists cannot be fully reconstructed from
disk files (and will not even be recognized as such).  An attempt to
save such a list will trigger a corresponding warning.
</p>


<h3>Value</h3>

<p><code>read.tfl</code> returns an object of class <code>tfl</code> (see the
<code><a href="#topic+tfl">tfl</a></code> manpage for details)
</p>


<h3>See Also</h3>

<p>See the <code><a href="#topic+tfl">tfl</a></code> manpage for details on <code>tfl</code>
objects. See <code><a href="#topic+read.spc">read.spc</a></code> and <code><a href="#topic+read.vgc">read.vgc</a></code> for
import/export of other data structures.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## save type-frequency list for Brown corpus to external file
fname &lt;- tempfile(fileext=".tfl.gz") # automatically compresses file
write.tfl(Brown.tfl, fname)
## file &lt;fname&gt; contains a compressed TAB-delimited table with fields
##   k    ... type ID (usually Zipf rank)
##   f    ... frequency of type
##   type ... the type itself (here a word form)

## read it back in
New.tfl &lt;- read.tfl(fname)

## same as Brown.tfl
summary(New.tfl)
summary(Brown.tfl)
print(New.tfl)
print(Brown.tfl)
head(New.tfl)
head(Brown.tfl)
stopifnot(isTRUE(all.equal(New.tfl, Brown.tfl))) # should by identical

## Not run: 
## suppose you have a text file with a frequency list, one f per line, e.g.:
##   f
##   14
##   12
##   31
##   ...

## you can import this with read.tfl
MyData.tfl &lt;- read.tfl("mylist.txt")
summary(MyData.tfl)
print(MyData.tfl) # ids in column k added by zipfR

## from this you can generate a spectrum with tfl2spc
MyData.spc &lt;- tfl2spc(MyData.tfl)
summary(MyData.spc)

## End(Not run)
</code></pre>

<hr>
<h2 id='read.vgc'>Loading and Saving Vocabulary Growth Curves (zipfR)</h2><span id='topic+read.vgc'></span><span id='topic+write.vgc'></span>

<h3>Description</h3>

<p><code>read.vgc</code> loads vocabulary growth data from <code>.vgc</code> file
</p>
<p><code>write.vgc</code> saves vocabulary growth data in <code>.vgc</code> file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  read.vgc(file)

  write.vgc(vgc, file)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read.vgc_+3A_file">file</code></td>
<td>
<p>character string specifying the pathname of a disk file.
Files with extension <code>.gz</code> will automatically be compressed/decompressed.
See section &quot;Format&quot; for a description of the required file format</p>
</td></tr>
<tr><td><code id="read.vgc_+3A_vgc">vgc</code></td>
<td>
<p>a vocabulary growth curve, i.e.\ an object of class
<code>vgc</code></p>
</td></tr>
</table>


<h3>Format</h3>

<p>A TAB-delimited text file with column headers but no row names
(suitable for reading with <code>read.delim</code>).  The file must contain
at least the following two columns:
</p>

<dl>
<dt><code>N</code></dt><dd><p>increasing integer vector of sample sizes <code class="reqn">N</code> </p>
</dd>
<dt><code>V</code></dt><dd><p>corresponding observed vocabulary sizes <code class="reqn">V(N)</code>
or expected vocabulary sizes <code class="reqn">E[V(N)]</code> </p>
</dd>
</dl>

<p>Optionally, columns <code>V1</code>, ..., <code>V9</code> can be added to
specify the number of hapaxes (<code class="reqn">V_1(N)</code>), dis legomena
(<code class="reqn">V_2(N)</code>), and further spectrum elements up to <code class="reqn">V_9(N)</code>.
</p>
<p>It is not necessary to include all 9 columns, but for any <code class="reqn">V_m(N)</code>
in the data set, all &quot;lower&quot; spectrum elements <code class="reqn">V_{m'}(N)</code> (for
<code class="reqn">m' &lt; m</code>) must also be present.  For example, it is valid to have
columns <code>V1 V2 V3</code>, but not <code>V1 V3 V5</code> or <code>V2 V3 V4</code>.
</p>
<p>Variances for expected vocabulary sizes and spectrum elements can be
given in further columns <code>VV</code> (for
<code class="reqn">\mathop{Var}[V(N)]</code>), and <code>VV1</code>, ...,
<code>VV9</code> (for <code class="reqn">\mathop{Var}[V_m(N)]</code>).  <code>VV</code>
is mandatory in this case, and columns <code>VVm</code> must be specified
for exactly the same frequency classes <code>m</code> as the <code>Vm</code>
above.
</p>
<p>These columns may appear in any order in the text file.  All other
columns will be silently ignored.
</p>


<h3>Details</h3>

<p>If the filename <code>file</code> ends in the extension <code>.gz</code>, <code>.bz2</code> or <code>.xz</code>, 
the disk file will automatically be decompressed (<code>read.vgc</code>) or compressed (<code>write.vgc</code>).
</p>


<h3>Value</h3>

<p><code>read.vgc</code> returns an object of class <code>vgc</code> (see the
<code><a href="#topic+vgc">vgc</a></code> manpage for details)
</p>


<h3>See Also</h3>

<p>See the <code><a href="#topic+vgc">vgc</a></code> manpage for details on <code>vgc</code> objects.
See <code><a href="#topic+read.tfl">read.tfl</a></code> and <code><a href="#topic+read.spc">read.spc</a></code> for
import/export of other data structures.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## save Italian ultra- prefix VGC to external text file
fname &lt;- tempfile(fileext=".vgc")
write.vgc(ItaUltra.emp.vgc, fname)
## now &lt;fname&gt; is a TAB-delimited text file with columns N, V and V1

## we ready it back in
New.vgc &lt;- read.vgc(fname)

## same vgc as ItaUltra.emp.vgc, compare:
summary(New.vgc)
summary(ItaUltra.emp.vgc)
head(New.vgc)
head(ItaUltra.emp.vgc)

stopifnot(isTRUE(all.equal(New.vgc, ItaUltra.emp.vgc))) # should be identical
</code></pre>

<hr>
<h2 id='sample.spc'>Incremental Samples from a Frequency Spectrum (zipfR)</h2><span id='topic+sample.spc'></span>

<h3>Description</h3>

<p>Compute incremental random samples from a frequency spectrum (an object
of class <code>spc</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  sample.spc(obj, N, force.list=FALSE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample.spc_+3A_obj">obj</code></td>
<td>
<p>an object of class <code>spc</code>, representing a frequency
spectrum</p>
</td></tr>
<tr><td><code id="sample.spc_+3A_n">N</code></td>
<td>
<p>a vector of non-negative integers in increasing order, the
sample sizes for which incremental samples will be generated</p>
</td></tr>
<tr><td><code id="sample.spc_+3A_force.list">force.list</code></td>
<td>
<p>if <code>TRUE</code>, the return value will always be a
list of <code>spc</code> objects, even if <code>N</code> is just a single
integer</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is currently implemented as a wrapper around
<code>sample.tfl</code>, using <code>spc2tfl</code> and <code>tfl2spc</code> to convert
between frequency spectra and type frequency lists.  A direct
implementation might be slightly more efficient, but would very likely
not make a substantial difference.
</p>


<h3>Value</h3>

<p>If <code>N</code> is a single integer (and the <code>force.list</code> flag is not
set), a <code>spc</code> object representing the frequency spectrum of a
random sample of size <code class="reqn">N</code> from <code>obj</code>.
</p>
<p>If <code>N</code> is a vector of length greater one, or if
<code>force.list=TRUE</code>, a list of <code>spc</code> objects representing the
frequency spectra of incremental random samples of the specified sizes
<code class="reqn">N</code>.  <em>Incremental</em> means that each sample is a superset of
the preceding sample.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+spc">spc</a></code> for more information about frequency spectra
</p>
<p><code><a href="#topic+sample.tfl">sample.tfl</a></code> is an analogous function for type frequency
lists (objects of class <code>tfl</code>)
</p>
<p><code><a href="#topic+sample.spc">sample.spc</a></code> takes a single <em>concrete</em> random
subsample from a spectrum and returns the spectrum of the subsample,
unlike <code>spc.interp</code>, that computes the <em>expected</em>
frequency spectrum for random subsamples of size <code>N</code> by
binomial interpolation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## read Brown spectrum
data(Brown.spc)
summary(Brown.spc)

## sample a spectrum of 100k tokens
MiniBrown.spc &lt;- sample.spc(Brown.spc,1e+5)
summary(MiniBrown.spc)

## if we repat, we get a different sample
MiniBrown.spc &lt;- sample.spc(Brown.spc,1e+5)
summary(MiniBrown.spc)

</code></pre>

<hr>
<h2 id='sample.tfl'>Incremental Samples from a Type Frequency List (zipfR)</h2><span id='topic+sample.tfl'></span>

<h3>Description</h3>

<p>Compute incremental random samples from a type frequency list (an
object of class <code>tfl</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  sample.tfl(obj, N, force.list=FALSE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample.tfl_+3A_obj">obj</code></td>
<td>
<p>an object of class <code>tfl</code>, representing a type
frequency list</p>
</td></tr>
<tr><td><code id="sample.tfl_+3A_n">N</code></td>
<td>
<p>a vector of non-negative integers in increasing order, the
sample sizes for which incremental samples will be generated</p>
</td></tr>
<tr><td><code id="sample.tfl_+3A_force.list">force.list</code></td>
<td>
<p>if <code>TRUE</code>, the return value will always be a
list of <code>tfl</code> objects, even if <code>N</code> is just a single
integer</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The current implementation is reasonably efficient, but will be rather
slow when applied to very large type frequency lists.
</p>


<h3>Value</h3>

<p>If <code>N</code> is a single integer (and the <code>force.list</code> flag is not
set), a <code>tfl</code> object representing a random sample of size <code class="reqn">N</code>
from the type frequency list <code>obj</code>.
</p>
<p>If <code>N</code> is a vector of length greater one, or if
<code>force.list=TRUE</code>, a list of <code>tfl</code> objects representing
incremental random samples of the specified sizes <code class="reqn">N</code>.
<em>Incremental</em> means that each sample is a superset of the
preceding sample.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tfl">tfl</a></code> for more information about type frequency lists
</p>
<p><code><a href="#topic+sample.spc">sample.spc</a></code> is an analogous function for frequency
spectra (objects of class <code>spc</code>)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load Brown tfl
data(Brown.tfl)
summary(Brown.tfl)

## sample a tfl of 100k tokens
MiniBrown.tfl &lt;- sample.tfl(Brown.tfl,1e+5)
summary(MiniBrown.tfl)

## if we repat, we get a different sample
MiniBrown.tfl &lt;- sample.tfl(Brown.tfl,1e+5)
summary(MiniBrown.tfl)

</code></pre>

<hr>
<h2 id='spc'>Frequency Spectra (zipfR)</h2><span id='topic+spc'></span><span id='topic+spc.object'></span>

<h3>Description</h3>

<p>In the <code>zipfR</code> library, <code>spc</code> objects are used to represent
a word frequency spectrum (either an observed spectrum or the expected
spectrum of a LNRE model at a given sample size).
</p>
<p>With the <code>spc</code> constructor function, an object can be initialized
directly from the specified data vectors.  It is more common to read
an observed spectrum from a disk file with <code><a href="#topic+read.spc">read.spc</a></code> or
compute an expected spectrum with <code><a href="#topic+lnre.spc">lnre.spc</a></code>, though.
</p>
<p><code>spc</code> objects should always be treated as read-only.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  spc(Vm, m=1:length(Vm), VVm=NULL, N=NA, V=NA, VV=NA,
      m.max=0, expected=!missing(VVm))

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spc_+3A_m">m</code></td>
<td>
<p>integer vector of frequency classes <code class="reqn">m</code> (if omitted,
<code>Vm</code> is assumed to list the first <code class="reqn">k</code> frequency classes
<code class="reqn">V_1, \ldots, V_k</code>)</p>
</td></tr>
<tr><td><code id="spc_+3A_vm">Vm</code></td>
<td>
<p>vector of corresponding class sizes <code class="reqn">V_m</code> (may be
fractional for expected frequency spectrum <code class="reqn">E[V_m]</code>)</p>
</td></tr>
<tr><td><code id="spc_+3A_vvm">VVm</code></td>
<td>
<p>optional vector of estimated variances
<code class="reqn">\mathop{Var}[V_m]</code> (for expected frequency spectrum
only)</p>
</td></tr>
<tr><td><code id="spc_+3A_n">N</code>, <code id="spc_+3A_v">V</code></td>
<td>
<p>total sample size <code class="reqn">N</code> and vocabulary size <code class="reqn">V</code> of
frequency spectrum.  While these values are usually determined
automatically from <code>m</code> and <code>Vm</code>, they are required for an
incomplete frequency spectrum that does not list all non-empty
frequency classes.</p>
</td></tr>
<tr><td><code id="spc_+3A_vv">VV</code></td>
<td>
<p>variance <code class="reqn">\mathop{Var}[V]</code> of expected
vocabulary size.  If <code>VVm</code> is specified, <code>VV</code> should
also be given.</p>
</td></tr>
<tr><td><code id="spc_+3A_m.max">m.max</code></td>
<td>
<p>highest frequency class <code class="reqn">m</code> listed in incomplete
spectrum.  If <code>m.max</code> is set, <code>N</code> and <code>V</code> also have
to be specified, and all non-zero frequency classes up to
<code>m.max</code> have to be included in the input vectors.  Frequency
classes above <code>m.max</code> in the input will automatically be
deleted.</p>
</td></tr>
<tr><td><code id="spc_+3A_expected">expected</code></td>
<td>
<p>set to <code>TRUE</code> if the frequency spectrum
represents expected values <code class="reqn">E[V_m]</code> of the class sizes according
to some LNRE model (this is automatically triggered when the
<code>VVm</code> argument is specified).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A <code>spc</code> object is a data frame with the following variables:
</p>

<dl>
<dt><code>m</code></dt><dd><p>frequency class <code class="reqn">m</code>, an integer vector</p>
</dd>
<dt><code>Vm</code></dt><dd><p>class size, i.e. number <code class="reqn">V_m</code> of types in
frequency class <code class="reqn">m</code> (either observed class size from a sample
or expected class size <code class="reqn">E[V_m]</code> based on a LNRE model)</p>
</dd>
<dt><code>VVm</code></dt><dd><p>optional: estimated variance <code class="reqn">V[V_m]</code> of
expected class size (only meaningful for expected spectrum derived
from LNRE model)</p>
</dd>
</dl>

<p>The following attributes are used to store additional information
about the frequency spectrum:
</p>

<dl>
<dt><code>m.max</code></dt><dd><p>if non-zero, the frequency spectrum is
incomplete and lists only frequency classes up to <code>m.max</code></p>
</dd>
<dt><code>N, V</code></dt><dd><p>sample size <code class="reqn">N</code> and vocabulary size <code class="reqn">V</code>
of the frequency spectrum.  For a complete frequency spectrum,
these values could easily be determined from <code>m</code> and
<code>Vm</code>, but they are essential for an incomplete spectrum.</p>
</dd>
<dt><code>VV</code></dt><dd><p>variance of expected vocabulary size; only present
if <code>hasVariances</code> is <code>TRUE</code>.  Note that <code>VV</code> may
have the value <code>NA</code> is the user failed to specify it.</p>
</dd>
<dt><code>expected</code></dt><dd><p>if <code>TRUE</code>, frequency spectrum lists
expected class sizes <code class="reqn">E[V_m]</code> (rather than observed
sizes <code class="reqn">V_m</code>).  Note that the <code>VVm</code> variable is only
allowed for an expected frequency spectrum.</p>
</dd>
<dt><code>hasVariances</code></dt><dd><p>indicates whether or not the <code>VVm</code>
variable is present</p>
</dd>
</dl>



<h3>Value</h3>

<p>An object of class <code>spc</code> representing the specified frequency
spectrum.  This object should be treated as read-only (although such
behaviour cannot be enforced in <span class="rlang"><b>R</b></span>).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+read.spc">read.spc</a></code>, <code><a href="#topic+write.spc">write.spc</a></code>,
<code><a href="#topic+spc.vector">spc.vector</a></code>, <code><a href="#topic+sample.spc">sample.spc</a></code>,
<code><a href="#topic+spc2tfl">spc2tfl</a></code>, <code><a href="#topic+tfl2spc">tfl2spc</a></code>,
<code><a href="#topic+lnre.spc">lnre.spc</a></code>, <code><a href="#topic+plot.spc">plot.spc</a></code>
</p>
<p>Generic methods supported by <code>spc</code> objects are
<code><a href="base.html#topic+print">print</a></code>, <code><a href="base.html#topic+summary">summary</a></code>, <code><a href="#topic+N">N</a></code>,
<code><a href="#topic+V">V</a></code>, <code><a href="#topic+Vm">Vm</a></code>, <code><a href="#topic+VV">VV</a></code>, and
<code><a href="#topic+VVm">VVm</a></code>.
</p>
<p>Implementation details and non-standard arguments for these methods
can be found on the manpages <code><a href="#topic+print.spc">print.spc</a></code>,
<code><a href="#topic+summary.spc">summary.spc</a></code>, <code><a href="#topic+N.spc">N.spc</a></code>, <code><a href="#topic+V.spc">V.spc</a></code>,
etc.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load Brown imaginative prose spectrum and inspect it
data(BrownImag.spc)

summary(BrownImag.spc)
print(BrownImag.spc)

plot(BrownImag.spc)

N(BrownImag.spc)
V(BrownImag.spc)
Vm(BrownImag.spc,1)
Vm(BrownImag.spc,1:5)

## compute ZM model, and generate PARTIAL expected spectrum
## with variances for a sample of 10 million tokens
zm &lt;- lnre("zm",BrownImag.spc)
zm.spc &lt;- lnre.spc(zm,1e+7,variances=TRUE)

## inspect extrapolated spectrum
summary(zm.spc)
print(zm.spc)

plot(zm.spc,log="x")

N(zm.spc)
V(zm.spc)
VV(zm.spc)
Vm(zm.spc,1)
VVm(zm.spc,1)

## generate an artificial Zipfian-looking spectrum
## and take a look at it
zipf.spc &lt;- spc(round(1000/(1:1000)^2))

summary(zipf.spc)
plot(zipf.spc)

## see manpages of lnre, and the various *.spc mapages
## for more examples of spc usage

</code></pre>

<hr>
<h2 id='spc.interp'>Expected Frequency Spectrum by Binomial Interpolation (zipfR)</h2><span id='topic+spc.interp'></span>

<h3>Description</h3>

<p><code>spc.interp</code> computes the expected frequency spectrum for a
random sample of specified size <code class="reqn">N</code>, taken from a data set
described by the frequency spectrum object <code>obj</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  spc.interp(obj, N, m.max=max(obj$m), allow.extrapolation=FALSE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spc.interp_+3A_obj">obj</code></td>
<td>
<p>an object of class <code>spc</code>, representing the frequency
spectrum of the data set from which samples are taken</p>
</td></tr>
<tr><td><code id="spc.interp_+3A_n">N</code></td>
<td>
<p>a single non-negative integer specifying the sample size for
which the expected frequency spectrum is calculated</p>
</td></tr>
<tr><td><code id="spc.interp_+3A_m.max">m.max</code></td>
<td>
<p>number of spectrum elements listed in the expected
frequency spectrum.  By default, as many spectrum elements are
included as the spectrum <code>obj</code> contains, since the expectations
of higher spectrum elements will always be 0 in the binomial
interpolation.  See note in section &quot;Details&quot; below.</p>
</td></tr> 
<tr><td><code id="spc.interp_+3A_allow.extrapolation">allow.extrapolation</code></td>
<td>
<p>if <code>TRUE</code>, the requested sample size
<code class="reqn">N</code> may be larger than the sample size of the frequency spectrum
<code>obj</code>, for binomial <em>extrapolation</em>.  This obtion should
be used with great caution (see <code><a href="#topic+EVm.spc">EVm.spc</a></code> for details).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See the <code><a href="#topic+EVm.spc">EVm.spc</a></code> manpage for more information, especially
concerning binomial <em>extrapolation</em>.
</p>
<p>For large frequency spectra, the default value of <code>m.max</code> may
lead to very long computation times.  It is therefore recommended to
specify <code>m.max</code> explicitly and calculate only as many spectrum
elements as are actually required.
</p>


<h3>Value</h3>

<p>An object of class <code>spc</code>, representing the expected frequency
spectrum for a random sample of size <code>N</code> taken from the data set
that is described by <code>obj</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+spc">spc</a></code> for more information about frequency spectra and
links to relevant functions
</p>
<p>The implementation of <code>spc.interp</code> is based on the functions
<code><a href="#topic+EV.spc">EV.spc</a></code> and <code><a href="#topic+EVm.spc">EVm.spc</a></code>.  See the respective
manpages for technical details.
</p>
<p><code><a href="#topic+vgc.interp">vgc.interp</a></code> computes expected vocabulary growth curves by
binomial interpolation from a frequency spectrum
</p>
<p><code><a href="#topic+sample.spc">sample.spc</a></code> takes a single <em>concrete</em> random
subsample from a spectrum and returns the spectrum of the subsample,
unlike <code>spc.interp</code>, that computes the <em>expected</em>
frequency spectrum for random subsamples of size <code>N</code> by
binomial interpolation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load the Tiger NP expansion spectrum
## (sample size: about 109k tokens) 
data(TigerNP.spc)

## interpolated expected frequency subspectrum of 50k tokens
TigerNP.sub.spc &lt;- spc.interp(TigerNP.spc,5e+4)
summary(TigerNP.sub.spc)

## previous is slow since it calculates all expected  spectrum
## elements; suppose we only need the first 10 expected
## spectrum element frequencies; then we can do:
TigerNP.sub.spc &lt;- spc.interp(TigerNP.spc,5e+4,m.max=10) # much faster!
summary(TigerNP.sub.spc)

</code></pre>

<hr>
<h2 id='spc.vector'>Create Vector of Spectrum Elements (zipfR)</h2><span id='topic+spc.vector'></span>

<h3>Description</h3>

<p><code>spc.vector</code> returns a selected range of elements from a
frequency spectrum as a plain numeric vector (which may contain
entries with <code class="reqn">V_m = 0</code>, unlike the <code>spc</code> object
itself).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  spc.vector(obj, m.min=1, m.max=15, all=FALSE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spc.vector_+3A_obj">obj</code></td>
<td>
<p>an object of class <code>spc</code>, representing an observed or
expected frequency spectrum</p>
</td></tr>
<tr><td><code id="spc.vector_+3A_m.min">m.min</code>, <code id="spc.vector_+3A_m.max">m.max</code></td>
<td>
<p>determine the range of frequency classes to be returned
(defaulting to 1 ... 15)</p>
</td></tr>
<tr><td><code id="spc.vector_+3A_all">all</code></td>
<td>
<p>if <code>TRUE</code>, a vector containing the entire frequency
spectrum is returned, i.e. <code>m.max</code> is set to <code>max(obj$m)</code>.
Note that the value of <code>m.min</code> can still be overridden manually
to return only part of the spectrum.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>spc.vector(obj, a, b)</code> is fully equivalent to <code>Vm(obj,
  a:b)</code> (and is implemented in this way).
</p>


<h3>Value</h3>

<p>A numeric vector with the selected elements of the frequency spectrum.
In this vector, empty frequency classes (<code class="reqn">V_m = 0</code>) are
represented by 0 entries (unlike the <code>spc</code> object, which omits
all empty classes).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+spc">spc</a></code> for more information about <code>spc</code> objects and
links to relevant functions
</p>
<p><code><a href="#topic+Vm.spc">Vm.spc</a></code> for an alternative way of extracting spectrum
vectors from a <code>.spc</code> object, and <code><a href="#topic+N.spc">N.spc</a></code>,
<code><a href="#topic+V.spc">V.spc</a></code>, <code><a href="#topic+VV.spc">VV.spc</a></code>, <code><a href="#topic+VVm.spc">VVm.spc</a></code> for
extracting related information
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Brown Noun spectrum
data(BrownNoun.spc)

## by default, extract first 15 elements
spc.vector(BrownNoun.spc)

## first five elements
spc.vector(BrownNoun.spc,1,5)

## just frequencies of spc elements 4 and 5 
spc.vector(BrownNoun.spc,4,5)
## same as
Vm(BrownNoun.spc,4:5)

</code></pre>

<hr>
<h2 id='spc2tfl'>Convert Between Frequency Spectra and Type Frequency Lists (zipfR)</h2><span id='topic+spc2tfl'></span><span id='topic+tfl2spc'></span>

<h3>Description</h3>

<p><code>tfl2spc</code> computes an observed frequency spectrum from a type
frequency list, while <code>spc2tfl</code> reconstructs the type frequency
list underlying a frequency spectrum (but without type
representations).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  tfl2spc(tfl)

  spc2tfl(spc)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spc2tfl_+3A_tfl">tfl</code></td>
<td>
<p>an object of class <code>tfl</code>, representing a type
frequency list</p>
</td></tr>
<tr><td><code id="spc2tfl_+3A_spc">spc</code></td>
<td>
<p>an object of class <code>spc</code>, representing a frequency
spectrum</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The current implementation of these functions does not support
incomplete type frequency lists and frequency spectra.
</p>
<p><code>spc2tfl</code> can only convert frequency spectra where all class
sizes are integers.  For this reason, expected frequency spectra
(including all spectra with variance data) are not supported.
</p>


<h3>Value</h3>

<p>For <code>tfl2spc</code>, an object of class <code>spc</code> representing the
frequency spectrum corresponding to the type frequency list <code>tfl</code>.
</p>
<p>For <code>spc2tfl</code>, an object of class <code>tfl</code> representing type
frequency list underlying the observed frequency spectrum <code>tfl</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+spc">spc</a></code> for more information about <code>spc</code> objects and
links to relevant functions; <code><a href="#topic+tfl">tfl</a></code> for more information
about <code>tfl</code> objects and links to relevant functions
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Brown tfl and spc
data(Brown.tfl)
data(Brown.spc)


## a spectrum from a tfl
Brown.spc2 &lt;- tfl2spc(Brown.tfl)

## identical to Brown.spc:
summary(Brown.spc)
summary(Brown.spc2)

tail(Brown.spc)
tail(Brown.spc2)


## a tfl from a spectrum
Brown.tfl2 &lt;- spc2tfl(Brown.spc)

## same frequency information as Brown.tfl
## but with different ids and no type labels
summary(Brown.tfl)
summary(Brown.tfl2)

print(Brown.tfl2)
print(Brown.tfl)

</code></pre>

<hr>
<h2 id='tfl'>Type Frequency Lists (zipfR)</h2><span id='topic+tfl'></span><span id='topic+tfl.object'></span>

<h3>Description</h3>

<p>In the <code>zipfR</code> library, <code>tfl</code> objects are used to represent
a type frequency list, which specifies the observed frequency of each
type in a corpus.  For mathematical reasons, expected type frequencies
are rarely considered.
</p>
<p>With the <code>tfl</code> constructor function, an object can be initialized
directly from the specified data vectors.  It is more common to read
a type frequency list from a disk file with <code><a href="#topic+read.tfl">read.tfl</a></code> or,
in some cases, derive it from an observed frequency spectrum with
<code><a href="#topic+spc2tfl">spc2tfl</a></code>. 
</p>
<p><code>tfl</code> objects should always be treated as read-only.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  tfl(f, k=seq_along(f), type=NULL, f.min=min(f), f.max=max(f),
      incomplete=!(missing(f.min) &amp;&amp; missing(f.max)), N=NA, V=NA,
      delete.zeros=FALSE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfl_+3A_k">k</code></td>
<td>
<p>integer vector of type IDs <code class="reqn">k</code> (if omitted, natural
numbers <code class="reqn">1,2,\ldots</code> are assigned automatically)</p>
</td></tr>
<tr><td><code id="tfl_+3A_f">f</code></td>
<td>
<p>vector of corresponding type frequencies <code class="reqn">f_k</code></p>
</td></tr>
<tr><td><code id="tfl_+3A_type">type</code></td>
<td>
<p>optional character vector of type representations
(e.g. word forms or lemmata), used for informational and
printing purposes only</p>
</td></tr>
<tr><td><code id="tfl_+3A_incomplete">incomplete</code></td>
<td>
<p>indicates that the type frequency list is
incomplete, i.e. only contains types in a certain frequency range
(typically, the lowest-frequency types may be excluded).  Incomplete
type frequency lists are rarely useful.</p>
</td></tr>
<tr><td><code id="tfl_+3A_n">N</code>, <code id="tfl_+3A_v">V</code></td>
<td>
<p>sample size and vocabulary size corresponding to the type
frequency list have to be specified explicitly for incomplete lists</p>
</td></tr>
<tr><td><code id="tfl_+3A_f.min">f.min</code>, <code id="tfl_+3A_f.max">f.max</code></td>
<td>
<p>frequency range represented in an incomplete type
frequency list (see details below)</p>
</td></tr>
<tr><td><code id="tfl_+3A_delete.zeros">delete.zeros</code></td>
<td>
<p>if <code>TRUE</code>, delete types with <code class="reqn">f=0</code> from
the type frequency list, <em>after</em> assigning type IDs.  This
operation does <em>not</em> make the resulting <code>tfl</code> object
incomplete.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>f.min</code> and <code>f.max</code> are not specified, but the list is
marked as incomplete (with <code>incomplete=TRUE</code>), they are
automatically determined from the frequency vector <code>f</code> (making
the assumption that all types in this frequency range are listed).
Explicit specification of either <code>f.min</code> or <code>f.max</code> implies
an incomplete list.  In this case, all types outside the specified
range will be deleted from the list.  If <code>incomplete=FALSE</code> is
explicitly given, <code>N</code> and <code>V</code> will be determined
automatically from the input data (which is assumed to be complete),
but the resulting type frequency list will still be incomplete.
</p>
<p>If you just want to remove types with <code class="reqn">f=0</code> without marking the
type frequency list as incomplete, use the option
<code>delete.zeros=TRUE</code>.
</p>
<p>A <code>tfl</code> object is a data frame with the following variables:
</p>

<dl>
<dt><code>k</code></dt><dd><p>integer type ID <code class="reqn">k</code> </p>
</dd>
<dt><code>f</code></dt><dd><p>corresponding type frequency <code class="reqn">f_k</code> </p>
</dd>
<dt><code>type</code></dt><dd><p>optional: character vector with type
representations used for printing</p>
</dd>
</dl>

<p>The data frame always has to be sorted with respect to the <code>k</code>
column (ascending order).  If a <code>type</code> column is present, 
rownames are set to the types and can be used for character indexing.
</p>
<p>The following attributes are used to store additional information
about the frequency spectrum:
</p>

<dl>
<dt><code>N, V</code></dt><dd><p>sample size <code class="reqn">N</code> and vocabulary size <code class="reqn">V</code>
corresponding to the type frequency list.  For a complete list,
these values could easily be determined from the <code>f</code>
variable, but they are essential for an incomplete list.</p>
</dd>
<dt><code>incomplete</code></dt><dd><p>if <code>TRUE</code>, the type frequency list is
incomplete, i.e. it lists only types in the frequency range given
by <code>f.min</code> and <code>f.max</code></p>
</dd>
<dt><code>f.min</code>, <code>f.max</code></dt><dd><p>range of type frequencies
represented in the list (should be ignored unless the
<code>incomplete</code> flag is set)</p>
</dd>
<dt><code>hasTypes</code></dt><dd><p>indicates whether or not the <code>type</code>
variable is present</p>
</dd>
</dl>



<h3>Value</h3>

<p>An object of class <code>tfl</code> representing the specified type
frequency list.  This object should be treated as read-only (although
such behaviour cannot be enforced in <span class="rlang"><b>R</b></span>).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+read.tfl">read.tfl</a></code>, <code><a href="#topic+write.tfl">write.tfl</a></code>, <code><a href="#topic+plot.tfl">plot.tfl</a></code>,
<code><a href="#topic+sample.tfl">sample.tfl</a></code>, <code><a href="#topic+spc2tfl">spc2tfl</a></code>, <code><a href="#topic+tfl2spc">tfl2spc</a></code>
</p>
<p>Generic methods supported by <code>tfl</code> objects are
<code><a href="base.html#topic+print">print</a></code>, <code><a href="base.html#topic+summary">summary</a></code>, <code><a href="#topic+N">N</a></code>,
<code><a href="#topic+V">V</a></code> and <code><a href="#topic+Vm">Vm</a></code>.
</p>
<p>Implementation details and non-standard arguments for these methods
can be found on the manpages <code><a href="#topic+print.tfl">print.tfl</a></code>,
<code><a href="#topic+summary.tfl">summary.tfl</a></code>, <code><a href="#topic+N.tfl">N.tfl</a></code>, <code><a href="#topic+V.tfl">V.tfl</a></code>,
etc.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## typically, you will read a tfl from a file
## (see examples in the read.tfl manpage)

## or you can load a ready-made tfl
data(Brown.tfl)
summary(Brown.tfl)
print(Brown.tfl)

## or create it from a spectrum (with different ids and
## no type labels)
data(Brown.spc)

Brown.tfl2 &lt;- spc2tfl(Brown.spc)

## same frequency information as Brown.tfl
## but with different ids and no type labels
summary(Brown.tfl2)
print(Brown.tfl2)

## how to display draw a Zipf's rank/frequency plot
## by extracting frequencies from a tfl
plot(sort(Brown.tfl$f,decreasing=TRUE),log="y",xlab="rank",ylab="frequency")

## simulating a tfl
Zipfian.tfl &lt;- tfl(1000/(1:1000))
plot(Zipfian.tfl$f,log="y")

</code></pre>

<hr>
<h2 id='Tiger'>Tiger NP and PP expansions (zipfR)</h2><span id='topic+Tiger'></span><span id='topic+TigerNP'></span><span id='topic+TigerNP.tfl'></span><span id='topic+TigerNP.spc'></span><span id='topic+TigerNP.emp.vgc'></span><span id='topic+TigerPP'></span><span id='topic+TigerPP.tfl'></span><span id='topic+TigerPP.spc'></span><span id='topic+TigerPP.emp.vgc'></span>

<h3>Description</h3>

<p>Objects of classes <code><a href="#topic+tfl">tfl</a></code>, <code><a href="#topic+spc">spc</a></code> and
<code><a href="#topic+vgc">vgc</a></code> that contain frequency data for the syntactic
expansions of Noun Phrases (NP) and Prepositional Phrases (PP) in
the Tiger German treebank.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
TigerNP.tfl
TigerNP.spc
TigerNP.emp.vgc

TigerPP.tfl
TigerPP.spc
TigerPP.emp.vgc

</code></pre>


<h3>Details</h3>

<p>In this dataset, types are not words, but syntactic expansions,
i.e., sequences of syntactic categories that form NPs (in
<code>TigerNP</code>) or PPs (in <code>TigerPP</code>), according to the Tiger
annotation scheme for German. Thus, for example, among the expansion
types in the <code>TigerNP</code> dataset, we find <code>ART_NN</code> and
<code>ART_ADJA_NN</code>, whereas among the PP expansions in
<code>TigerPP</code> we find <code>APPR_ART_NN</code> and <code>APPR_NN</code>
(<code>APPR</code> is the tag for prepositions in the Tiger tagset).
</p>
<p>The Tiger treebank contains about 900,000 tokens (50,000 sentences)
of German newspaper text from the Frankfurter Rundschau. The token
frequencies of the expansion types are taken from this corpus.
</p>
<p><code>TigerNP.tfl</code> and <code>TigerPP.tfl</code> are the type frequency
lists.  <code>TigerNP.spc</code> and <code>TigerPP.spc</code> are frequency
spectra. <code>TigerNP.emp.vgc</code> and <code>TigerPP.emp.vgc</code> are the
corresponding observed vocabulary growth curves (tracking the
development of <code>V</code> and <code>V(1)</code> in the original order of
occurrence of the expansion tokens in the source corpus).
</p>


<h3>References</h3>

<p>Tiger Project:
<a href="https://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/tiger/">https://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/tiger/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
TigerNP.tfl
summary(TigerNP.spc)
summary(TigerNP.emp.vgc)

TigerPP.tfl
summary(TigerPP.spc)
summary(TigerPP.emp.vgc)

</code></pre>

<hr>
<h2 id='vec2xxx'>Type-Token Statistics for Samples and Empirical Data (zipfR)</h2><span id='topic+vec2tfl'></span><span id='topic+vec2spc'></span><span id='topic+vec2vgc'></span>

<h3>Description</h3>

<p>Compute type-frequency list, frequency spectrum and vocabulary growth
curve from a token vector representing a random sample or an observed
sequence of tokens.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  vec2tfl(x)

  vec2spc(x)

  vec2vgc(x, steps=200, stepsize=NA, m.max=0)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vec2xxx_+3A_x">x</code></td>
<td>
<p>a vector of length <code class="reqn">N_0</code>, representing a random sample or
other observed data set of <code class="reqn">N_0</code> tokens.  For each token, the
corresponding element of <code>x</code> specifies the <em>type</em> that the
token belongs to.  Usually, <code>x</code> is a character vector, but it
might also specify integer IDs in some cases.</p>
</td></tr>
<tr><td><code id="vec2xxx_+3A_steps">steps</code></td>
<td>
<p>number of steps for which vocabulary growth data
<code class="reqn">V(N)</code> is calculated.  The values of <code class="reqn">N</code> will be evenly
spaced (up to rounding differences) from <code class="reqn">N=1</code> to <code class="reqn">N=N_0</code>.</p>
</td></tr>
<tr><td><code id="vec2xxx_+3A_stepsize">stepsize</code></td>
<td>
<p>alternative way of specifying the steps of the
vocabulary growth curve.  In this case, vocabulary growth data will
be calculated every <code>stepsize</code> tokens.  The first step is
chosen such that the last step corresponds to the full sample
(<code class="reqn">N=N_0</code>).  Only one of the parameters <code>steps</code> and
<code>stepsize</code> may be specified.</p>
</td></tr>
<tr><td><code id="vec2xxx_+3A_m.max">m.max</code></td>
<td>
<p>an integer in the range $1 ... 9$, specifying how many
spectrum elements <code class="reqn">V_m(N)</code> to include in the vocabulary growth
curve.  By default only vocabulary size <code class="reqn">V(N)</code> is calculated,
i.e. <code>m.max=0</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are two main applications for the <code>vec2xxx</code> functions:
</p>

<dl>
<dt>a)</dt><dd><p>They can be used to calculate type-token statistics and
vocabulary growth curves for random samples generated from a LNRE
model (with the <code><a href="#topic+rlnre">rlnre</a></code> function).</p>
</dd>
<dt>b)</dt><dd><p>They provide an easy way to process a user's own data
without having to rely on external scripts to compute frequency
spectra and vocabulary growth curves.  All that is needed is a
text file in one-token-per-line formt (i.e. where each token is
given on a separate line).  See &quot;Examples&quot; below for further
hints.</p>
</dd>
</dl>

<p>Both applications work well for samples of up to approx. 1 million
tokens.  For considerably larger data sets, specialized external
software should be used, such as the Perl scripts provided on the
<code>zipfR</code> homepage.
</p>


<h3>Value</h3>

<p>An object of class <code>tfl</code>, <code>spc</code> or <code>vgc</code>, representing
the type frequency list, frequency spectrum or vocabulary growth curve
of the token vector <code>x</code>, respectively.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tfl">tfl</a></code>, <code><a href="#topic+spc">spc</a></code> and <code><a href="#topic+vgc">vgc</a></code> for more
information about type frequency lists, frequency spectra and
vocabulary growth curves
</p>
<p><code><a href="#topic+rlnre">rlnre</a></code> for generating random samples (in the form of the
required token vectors) from a LNRE model
</p>
<p><code><a href="base.html#topic+readLines">readLines</a></code> and <code><a href="base.html#topic+scan">scan</a></code> for loading token
vectors from disk files
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## type-token statistics for random samples from a LNRE distribution

model &lt;- lnre("fzm", alpha=.5, A=1e-6, B=.05)
x &lt;- rlnre(model, 100000)

vec2tfl(x)
vec2spc(x)  # same as tfl2spc(vec2tfl(x))
vec2vgc(x)

sample.spc &lt;- vec2spc(x)
exp.spc &lt;- lnre.spc(model, 100000)
plot(exp.spc, sample.spc)

sample.vgc &lt;- vec2vgc(x, m.max=1, steps=500)
exp.vgc &lt;- lnre.vgc(model, N=N(sample.vgc), m.max=1)
plot(exp.vgc, sample.vgc, add.m=1)

## Not run: 
## load token vector from a file in one-token-per-line format
x &lt;- readLines(filename)
x &lt;- readLines(file.choose()) # with file selection dialog 

## you can also perform whitespace tokenization and filter the data
brown &lt;- scan("brown.pos", what=character(0), quote="")
nouns &lt;- grep("/NNS?$", brown, value=TRUE)
plot(vec2spc(nouns))
plot(vec2vgc(nouns, m.max=1), add.m=1)

## End(Not run)
</code></pre>

<hr>
<h2 id='vgc'>Vocabulary Growth Curves (zipfR)</h2><span id='topic+vgc'></span><span id='topic+vgc.object'></span>

<h3>Description</h3>

<p>In the <code>zipfR</code> library, <code>vgc</code> objects are used to represent
a vocabulary growth curve (VGC).  This can be an observed VGC from an
incremental set of sample (such as a corpus), a randomized VGC
obtained by binomial interpolation, or the expected VGC according to a
LNRE model.
</p>
<p>With the <code>vgc</code> constructor function, an object can be initialized
directly from the specified data vectors.  It is more common to read
an observed VGC from a disk file with <code><a href="#topic+read.vgc">read.vgc</a></code>, generate
a randomized VGC with <code><a href="#topic+vgc.interp">vgc.interp</a></code> or compute an expected
VGC with <code><a href="#topic+lnre.vgc">lnre.vgc</a></code>, though.
</p>
<p><code>vgc</code> objects should always be treated as read-only.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  vgc(N, V, Vm=NULL, VV=NULL, VVm=NULL, expected=FALSE, check=TRUE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vgc_+3A_n">N</code></td>
<td>
<p>integer vector of sample sizes <code class="reqn">N</code> for which vocabulary
growth data is available</p>
</td></tr>
<tr><td><code id="vgc_+3A_v">V</code></td>
<td>
<p>vector of corresponding vocabulary sizes <code class="reqn">V(N)</code>, or
expected vocabulary sizes <code class="reqn">E[V(N)]</code> for an interpolated or
expected VGC.</p>
</td></tr>
<tr><td><code id="vgc_+3A_vm">Vm</code></td>
<td>
<p>optional list of growth vectors for hapaxes <code class="reqn">V_1(N)</code>,
dis legomena <code class="reqn">V_2(N)</code>, etc.  Up to 9 growth vectors are accepted
(i.e.\ <code class="reqn">V_m(N)</code> for <code class="reqn">m \le 9</code>).  For an interpolated or
expected VGC, the vectors represent expected class sizes
<code class="reqn">E[V_m(N)]</code>.</p>
</td></tr>
<tr><td><code id="vgc_+3A_vv">VV</code></td>
<td>
<p>optional vector of variances
<code class="reqn">\mathop{Var}[V(N)]</code> for an interpolated or expected
VGC</p>
</td></tr>
<tr><td><code id="vgc_+3A_vvm">VVm</code></td>
<td>
<p>optional list of variance vectors
<code class="reqn">\mathop{Var}[V_m(N)]</code> for an expected VGC.  If
present, these vectors must be defined for exactly the same frequency
classes <code class="reqn">m</code> as the vectors in <code>Vm</code>.</p>
</td></tr>
<tr><td><code id="vgc_+3A_expected">expected</code></td>
<td>
<p>if <code>TRUE</code>, the object represents an interpolated
or expected VGC (for informational purposes only)</p>
</td></tr>
<tr><td><code id="vgc_+3A_check">check</code></td>
<td>
<p>by default, various sanity checks are performed on the
data supplied to the <code>spc</code> constructor.  Specify
<code>check=FALSE</code> to skip these sanity test, e.g. when
automatically processing data from external programs that may be
numerically unstable.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If variances (<code>VV</code> or <code>VVm</code>) are specified for an expected
VGC, all relevant vectors must be given.  In other words, <code>VV</code>
always has to be present in this case, and <code>VVm</code> has to be
present whenever <code>Vm</code> is specified, and must contain vectors for
exactly the same frequency classes.
</p>
<p><code>V</code> and <code>VVm</code> are integer vectors for an observed VGC, but
will usually be fractional for an interpolated or expected VGC.
</p>
<p>A <code>vgc</code> object is a data frame with the following variables:
</p>

<dl>
<dt><code>N</code></dt><dd><p>sample size <code class="reqn">N</code> </p>
</dd>
<dt><code>V</code></dt><dd><p>corresponding vocabulary size (either observed
vocabulary size <code class="reqn">V(N)</code> or expected vocabulary size
<code class="reqn">E[V(N)]</code>)</p>
</dd>
<dt><code>V1</code> ... <code>V9</code></dt><dd><p>optional: observed or expected
spectrum elements (<code class="reqn">V_m(N)</code> or <code class="reqn">E[V_m(N)]</code>).  Not all of
these variables have to be present, but there must not be any
&quot;gaps&quot; in the spectrum.</p>
</dd>
<dt><code>VV</code></dt><dd><p>optional: variance of expected vocabulary size,
<code class="reqn">\mathop{Var}[V(N)]</code> </p>
</dd>
<dt><code>VV1</code> ... <code>VV9</code></dt><dd><p>optional: variances of expected
spectrum elements, <code class="reqn">\mathop{Var}[V_m(N)]</code>.  If
variances are present, they must be available for exactly the same
frequency classes as the corresponding expected values.</p>
</dd>
</dl>

<p>The following attributes are used to store additional information
about the vocabulary growth curve:
</p>

<dl>
<dt><code>m.max</code></dt><dd><p>if non-zero, the VGC includes spectrum elements
<code class="reqn">V_m(N)</code> for <code class="reqn">m</code> up to <code>m.max</code>.  For <code>m.max=0</code>,
no spectrum elements are present.</p>
</dd>
<dt><code>expected</code></dt><dd><p>if <code>TRUE</code>, the object represents an
interpolated or expected VGC, with expected vocabulary size and
spectrum elements.  Otherwise, the object represents an observed
VGC.</p>
</dd>
<dt><code>hasVariances</code></dt><dd><p>indicates whether or not the <code>VV</code>
variable is present (as well as <code>VV1</code>, <code>VV2</code>, etc., if
appropriate)</p>
</dd>
</dl>



<h3>Value</h3>

<p>An object of class <code>vgc</code> representing the specified vocabulary
growth curve.  This object should be treated as read-only (although
such behaviour cannot be enforced in <span class="rlang"><b>R</b></span>).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+read.vgc">read.vgc</a></code>, <code><a href="#topic+write.vgc">write.vgc</a></code>, <code><a href="#topic+plot.vgc">plot.vgc</a></code>,
<code><a href="#topic+vgc.interp">vgc.interp</a></code>, <code><a href="#topic+lnre.vgc">lnre.vgc</a></code> 
</p>
<p>Generic methods supported by <code>vgc</code> objects are
<code><a href="base.html#topic+print">print</a></code>, <code><a href="base.html#topic+summary">summary</a></code>, <code><a href="#topic+N">N</a></code>,
<code><a href="#topic+V">V</a></code>, <code><a href="#topic+Vm">Vm</a></code>, <code><a href="#topic+VV">VV</a></code>, and
<code><a href="#topic+VVm">VVm</a></code>.
</p>
<p>Implementation details and non-standard arguments for these methods
can be found on the manpages <code><a href="#topic+print.vgc">print.vgc</a></code>,
<code><a href="#topic+summary.vgc">summary.vgc</a></code>, <code><a href="#topic+N.vgc">N.vgc</a></code>, <code><a href="#topic+V.vgc">V.vgc</a></code>,
etc.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load Dickens' work empirical vgc and take a look at it

data(Dickens.emp.vgc)
summary(Dickens.emp.vgc)
print(Dickens.emp.vgc)

plot(Dickens.emp.vgc,add.m=1)

## vectors of sample sizes in the vgc, and the
## corresponding V and V_1 vectors
Ns &lt;- N(Dickens.emp.vgc)
Vs &lt;- V(Dickens.emp.vgc)
Vm &lt;- V(Dickens.emp.vgc,1)

## binomially interpolated V and V_1 at the same sample sizes
## as the empirical curve
data(Dickens.spc)
Dickens.bin.vgc &lt;- vgc.interp(Dickens.spc,N(Dickens.emp.vgc),m.max=1)

## compare observed and interpolated
plot(Dickens.emp.vgc,Dickens.bin.vgc,add.m=1,legend=c("observed","interpolated"))


## load Italian ultra- prefix data
data(ItaUltra.spc)

## compute zm model
zm &lt;- lnre("zm",ItaUltra.spc)

## compute vgc up to about twice the sample size
## with variance of V
zm.vgc &lt;- lnre.vgc(zm,(1:100)*70, variances=TRUE)

summary(zm.vgc)
print(zm.vgc)

## plot with confidence intervals derived from variance in
## vgc (with larger datasets, ci will typically be almost
## invisible)
plot(zm.vgc)

## for more examples of vgc usages, see manpages of lnre.vgc,
## plot.vgc, print.vgc  and vgc.interp


</code></pre>

<hr>
<h2 id='vgc.interp'>Expected Vocabulary Growth by Binomial Interpolation (zipfR)</h2><span id='topic+vgc.interp'></span>

<h3>Description</h3>

<p><code>vgc.interp</code> computes the expected vocabulary growth curve for
random sample taken from a data set described by the frequency
spectrum object <code>obj</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  vgc.interp(obj, N, m.max=0, allow.extrapolation=FALSE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vgc.interp_+3A_obj">obj</code></td>
<td>
<p>an object of class <code>spc</code>, representing the frequency
spectrum of the data set from which samples are taken</p>
</td></tr>
<tr><td><code id="vgc.interp_+3A_n">N</code></td>
<td>
<p>a vector of increasing non-negative integers specifying the
sample sizes for the expected vocabulary size is calculated (as well
as expected spectrum elements if requested)</p>
</td></tr>
<tr><td><code id="vgc.interp_+3A_m.max">m.max</code></td>
<td>
<p>an integer in the range <code class="reqn">1 \ldots 9</code>, specifying the
number of spectrum elements to be included in the vocabulary growth
curve (default: none)</p>
</td></tr>
<tr><td><code id="vgc.interp_+3A_allow.extrapolation">allow.extrapolation</code></td>
<td>
<p>if <code>TRUE</code>, the requested sample sizes
<code class="reqn">N</code> may be larger than the sample size of the frequency spectrum
<code>obj</code>, so that binomial <em>extrapolation</em> is performed.
This obtion should be used with great caution (see
<code><a href="#topic+EV.spc">EV.spc</a></code> for details).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See the <code><a href="#topic+EV.spc">EV.spc</a></code> manpage for more information, especially
concerning binomial <em>extrapolation</em>.
</p>
<p>Note that the <em>result</em> of <code>vgc.interp</code> is an object of class
<code>vgc</code> (a vocabulary growth curve), but its <em>input</em> is an
object of class <code>spc</code> (a frequency spectrum).
</p>


<h3>Value</h3>

<p>An object of class <code>vgc</code>, representing the expected vocabulary
growth curves for random samples taken from the data set described by
<code>obj</code>.  Data points will be generated for the specified sample
sizes <code>N</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vgc">vgc</a></code> for more information about vocabulary growth curves
and links to relevant functions; <code><a href="#topic+spc">spc</a></code> for more
information about frequency spectra
</p>
<p>The implementation of <code>vgc.interp</code> is based on the functions
<code><a href="#topic+EV.spc">EV.spc</a></code> and <code><a href="#topic+EVm.spc">EVm.spc</a></code>.  See the respective
manpages for technical details.
</p>
<p><code><a href="#topic+spc.interp">spc.interp</a></code> computes the expected frequency spectrum for
a random sample by binomial interpolation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## load the Tiger PP expansion spectrum
## (sample size: about 91k tokens) 
data(TigerPP.spc)

## binomially interpolated curve
TigerPP.bin.vgc &lt;- vgc.interp(TigerPP.spc,(1:100)*910)
summary(TigerPP.bin.vgc)

## let's also add growth of V_1 to V_5 and plot
TigerPP.bin.vgc &lt;- vgc.interp(TigerPP.spc,(1:100)*910,m.max=5)
plot(TigerPP.bin.vgc,add.m=c(1:5))


</code></pre>

<hr>
<h2 id='VV-Vm'>Variances of the Expected Frequency Spectrum (zipfR)</h2><span id='topic+VV'></span><span id='topic+VVm'></span>

<h3>Description</h3>

<p><code>VV</code> and <code>VVm</code> are generic methods that can (and should) be
used to compute the variance of the vocabulary size and the variances
of spectrum elements according to an LNRE model (i.e. an object of
class <code>lnre</code>).  These methods are also used to access variance
information stored in some objects of class <code>spc</code> and <code>vgc</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  VV(obj, N=NA, ...)
  VVm(obj, m, N=NA, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="VV-Vm_+3A_obj">obj</code></td>
<td>
<p>an object of class <code>lnre</code> (LNRE model), <code>spc</code>
(frequency spectrum) or <code>vgc</code> (vocabulary growth curve).</p>
</td></tr>
<tr><td><code id="VV-Vm_+3A_m">m</code></td>
<td>
<p>positive integer value determining the frequency class
<code class="reqn">m</code> for which variances are returned (or a vector of such values).</p>
</td></tr>
<tr><td><code id="VV-Vm_+3A_n">N</code></td>
<td>
<p>sample size <code class="reqn">N</code> for which variances are calculated
(<code>lnre</code> objects only)</p>
</td></tr>
<tr><td><code id="VV-Vm_+3A_...">...</code></td>
<td>
<p>additional arguments passed on to the method implementation
(see respective manpages for details)</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>spc</code> and <code>vgc</code> objects must represent an expected or
interpolated frequency spectrum or VGC, and must include variance
data.
</p>
<p>For <code>vgc</code> objects, the <code>VVm</code> method allows only a single
value <code>m</code> to be specified.
</p>
<p>The argument <code>N</code> is only allowed for LNRE models and will trigger
an error message otherwise.
</p>


<h3>Value</h3>

<p>For a LNRE model (class <code>lnre</code>), <code>VV</code> computes the variance
of the random variable <code class="reqn">V(N)</code> (vocabulary size),  and <code>VVm</code>
computes the variance of the random variables <code class="reqn">V_m(N)</code> (spectrum
elements), for a sample of specified size <code class="reqn">N</code>.
</p>
<p>For an observed or interpolated frequency spectrum (class <code>spc</code>),
<code>VV</code> returns the variance of the expected vocabulary size, and
<code>VVm</code> returns variances of the spectrum elements.  These methods
are only applicable if the <code>spc</code> object includes variance
information.
</p>
<p>For an expected or interpolated vocabulary growth curve (class
<code>vgc</code>), <code>VV</code> returns the variance vector of the expected
vocabulary sizes <code class="reqn">V</code>, and <code>VVm</code> the corresponding vector for
<code class="reqn">V_m</code>.  These methods are only applicable if the <code>vgc</code> object
includes variance information.
</p>


<h3>See Also</h3>

<p>For details on the implementations of these methods, see <code><a href="#topic+VV.spc">VV.spc</a></code>, <code><a href="#topic+VV.vgc">VV.vgc</a></code>, etc.
</p>
<p>Expected vocabulary size and frequency spectrum for a sample of size
<code class="reqn">N</code> according to a LNRE model can be computed with the analogous
methods <code><a href="#topic+EV">EV</a></code> and <code><a href="#topic+EVm">EVm</a></code>.  For <code>spc</code> and
<code>vgc</code> objects, <code class="reqn">V</code> and <code class="reqn">V_m</code> are always accessed with the
methods <code><a href="#topic+V">V</a></code> and <code><a href="#topic+Vm">Vm</a></code>, even if they represent
expected or interpolated values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## see lnre documentation for examples

</code></pre>

<hr>
<h2 id='zipfR-package'>
zipfR: lexical statistics in R
</h2><span id='topic+zipfR-package'></span><span id='topic+zipfR'></span>

<h3>Description</h3>

<p>The zipfR package performs Large-Number-of-Rare-Events (LNRE) modeling
of (linguistic) type frequency distributions (Baayen 2001) and
provides utilities to run various forms of lexical statistics analysis
in R.
</p>


<h3>Details</h3>

<p>The best way to get started with zipfR is to read the tutorial, which
you can find as a package vignettte via the HTML documentation;
you can also download it from
<a href="https://zipfr.r-forge.r-project.org/#start">https://zipfr.r-forge.r-project.org/#start</a>
</p>
<p>zipfR is released under the GNU General Public License
(<a href="http://www.gnu.org/copyleft/gpl.html">http://www.gnu.org/copyleft/gpl.html</a>)
</p>


<h3>Author(s)</h3>

<p> Stefan Evert &lt;<a href="mailto:stefan.evert@fau.de">stefan.evert@fau.de</a>&gt; and Marco Baroni
&lt;<a href="mailto:marco.baroni@unitn.it">marco.baroni@unitn.it</a>&gt;
</p>
<p>Maintainer: Stefan Evert &lt;<a href="mailto:stefan.evert@fau.de">stefan.evert@fau.de</a>&gt;
</p>


<h3>References</h3>

<p>zipfR Website: <a href="https://zipfR.r-forge.r-project.org/">https://zipfR.r-forge.r-project.org/</a>
</p>
<p>Baayen, R. Harald (2001). <em>Word Frequency Distributions.</em> Kluwer,
Dordrecht.
</p>
<p>Baroni, Marco (2008). Distributions in text. In: A. Lüdeling
and M. Kytö (eds.), <em>Corpus Linguistics. An
International Handbook</em>, article 37. Mouton de Gruyter, Berlin.
</p>
<p>Evert, Stefan (2004). A simple LNRE model for random character
sequences. <em>Proceedings of JADT 2004</em>, 411-422.
</p>
<p>Evert, Stefan (2004b). <em>The Statistics of Word Cooccurrences: Word
Pairs and Collocations.</em> PhD Thesis, IMS, University of Stuttgart.
URN urn:nbn:de:bsz:93-opus-23714
<a href="http://dx.doi.org/10.18419/opus-2556">http://dx.doi.org/10.18419/opus-2556</a>
</p>
<p>Evert, Stefan and Baroni, Marco (2006). Testing the extrapolation
quality of word frequency models. <em>Proceedings of Corpus
Linguistics 2005</em>.
</p>
<p>Evert, Stefan and Baroni, Marco (2006). The zipfR library: Words and
other rare events in R. <em>useR! 2006: The second R user
conference</em>.
</p>


<h3>See Also</h3>

<p>The zipfR tutorial: available as a package vignette and online from
<a href="https://zipfr.r-forge.r-project.org/#start">https://zipfr.r-forge.r-project.org/#start</a>.
</p>
<p>Some good entry points into the zipfR documentation are
be <code><a href="#topic+spc">spc</a></code>, <code><a href="#topic+vgc">vgc</a></code>, <code><a href="#topic+tfl">tfl</a></code>,
<code><a href="#topic+read.spc">read.spc</a></code>, <code><a href="#topic+read.tfl">read.tfl</a></code>,
<code><a href="#topic+read.vgc">read.vgc</a></code>, <code><a href="#topic+lnre">lnre</a></code>,
<code><a href="#topic+lnre.vgc">lnre.vgc</a></code>, <code><a href="#topic+plot.spc">plot.spc</a></code>,
<code><a href="#topic+plot.vgc">plot.vgc</a></code>
</p>
<p>Harald Baayen's LEXSTATS tools, which implement a wider range of LNRE models:
<a href="https://www.springer.com/de/book/9780792370178">https://www.springer.com/de/book/9780792370178</a>
</p>
<p>Stefan Evert's UCS tools for collocation analysis, which include
functions that have been integrated into <b>zipfR</b>:
<a href="http://www.collocations.de/software.html">http://www.collocations.de/software.html</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## load Oliver Twist and Great Expectations frequency spectra
data(DickensOliverTwist.spc)
data(DickensGreatExpectations.spc)

## check sample size and vocabulary and hapax counts
N(DickensOliverTwist.spc)
V(DickensOliverTwist.spc)
Vm(DickensOliverTwist.spc,1)
N(DickensGreatExpectations.spc)
V(DickensGreatExpectations.spc)
Vm(DickensGreatExpectations.spc,1)

## compute binomially interpolated growth curves
ot.vgc &lt;- vgc.interp(DickensOliverTwist.spc,(1:100)*1570)
ge.vgc &lt;- vgc.interp(DickensGreatExpectations.spc,(1:100)*1865)

## plot them
plot(ot.vgc,ge.vgc,legend=c("Oliver Twist","Great Expectations"))

## load Dickens' works frequency spectrum
data(Dickens.spc)

## compute Zipf-Mandelbrot model from Dickens data
## and look at model summary
zm &lt;- lnre("zm",Dickens.spc)
zm

## plot observed and expected spectrum
zm.spc &lt;- lnre.spc(zm,N(Dickens.spc))
plot(Dickens.spc,zm.spc)

## obtain expected V and V1 values at arbitrary sample sizes
EV(zm,1e+8)
EVm(zm,1,1e+8)

## generate expected V and V1 growth curves up to a sample size
## of 10 million tokens and plot them, with vertical line at 
## estimation size
ext.vgc &lt;- lnre.vgc(zm,(1:100)*1e+5,m.max=1)
plot(ext.vgc,N0=N(zm),add.m=1)


</code></pre>

<hr>
<h2 id='zipfR.par'>Set or Query Graphics Parameters (zipfR)</h2><span id='topic+zipfR.par'></span>

<h3>Description</h3>

<p>Set default graphics parameters for <code>zipfR</code> high-level plots and
plot utilities, similar to <code>par</code> for general graphics parameters.
The current parameter values are queried by giving their names as
character strings.  The values can be set by specifying them as
arguments in <code>name=value</code> form, or by passing a single list of
named values.
</p>
<p><b>NB:</b> This is an advanced function to fine-tune zipfR
plots. For basic plotting options (that are likely to be sufficient
for most purposes) see <code><a href="#topic+plot.spc">plot.spc</a></code> and
<code><a href="#topic+plot.vgc">plot.vgc</a></code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  zipfR.par(..., bw.mode=FALSE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="zipfR.par_+3A_...">...</code></td>
<td>
<p>either character strings (or vectors) specifying the names
of parameters to be queried, or parameters to be set in
<code>name=value</code> form, or a single list of named values.  A listing
of valid parameter names is given below.</p>
</td></tr>
<tr><td><code id="zipfR.par_+3A_bw.mode">bw.mode</code></td>
<td>
<p>if <code>TRUE</code> and parameter values are queried, then
return the corresponding parameters for B/W mode if possible (e.g.,
<code>zipfR.par("col",bw.mode=TRUE)</code> returns the value of the
<code>col.bw</code> parameter).  Note that <code>bw.mode</code> cannot be
abbreviated in the function call!</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Parameters are set by specifying their names and the new values as
<code>name=value</code> pairs.  Such a list can also be passed as a single
argument to <code>zipfR.par</code>, which is typically used to restore previous
parameter values (that have been saved in a list variable).
</p>
<p>Most of the default values can be manually overridden in the
high-level plots.
</p>
<p><code>zipfR.par()</code> shows all parameters with their current values, and
<code>names(zipfR.par())</code> produces a listing of valid parameter names.
</p>


<h3>Value</h3>

<p>When parameters are set, their former values are returned in an
invisible named list.  Such a list can be passed as a single argument
to <code>zipfR.par</code> to restore the parameter values.
</p>
<p>When a single parameter is queried, its value is returned directly.
When two or more parameters are queried, the result is a named list.
</p>
<p>Note the inconsistency, which is the same as for <code>par</code>: setting
one parameter returns a list, but querying one parameter returns a
vector (or a scalar, i.e. a vector of length 1).
</p>


<h3>zipfR Graphics Parameters</h3>


<dl>
<dt><code>col</code></dt><dd><p>a character or integer vector specifying up to 10
line colours (see the <code><a href="graphics.html#topic+par">par</a></code> manpage for
details).  Values of shorter vectors are recycled as necessary.</p>
</dd>
<dt><code>lty</code></dt><dd><p>a character or integer vector specifying up to 10
line styles (see the <code><a href="graphics.html#topic+par">par</a></code> manpage for
details).  Values of shorter vectors are recycled as necessary.</p>
</dd>
<dt><code>lwd</code></dt><dd><p>a numeric vector specifying up to 10 line widths
(see the <code>par</code> manpage for details).  Values of shorter
vectors are recycled as necessary.</p>
</dd>
<dt><code>pch</code></dt><dd><p>a character or integer vector specifying up to 10
plot symbols.  Values of shorter vectors are recycled as
necessary.</p>
</dd>
<dt><code>barcol</code></dt><dd><p>a character or integer vector specifying up to
10 colours for the bars in non-logarithmic spectrum plots.  Values
of shorter vectors are recycled as necessary.</p>
</dd>
<dt><code>col.bw</code></dt><dd><p>the line colours used in B/W mode
(<code>bw=TRUE</code>)</p>
</dd>
<dt><code>lty.bw</code></dt><dd><p>the line styles used in B/W mode
(<code>bw=TRUE</code>)</p>
</dd>
<dt><code>lwd.bw</code></dt><dd><p>the line widths used in B/W mode
(<code>bw=TRUE</code>)</p>
</dd>
<dt><code>pch.bw</code></dt><dd><p>the plot symbols used in B/W mode
(<code>bw=TRUE</code>)</p>
</dd>
<dt><code>barcol.bw</code></dt><dd><p>the bar colours used in B/W mode
(<code>bw=TRUE</code>)</p>
</dd>
<dt><code>bw</code></dt><dd><p>if <code>TRUE</code>, plots are drawn in B/W mode unless
specified otherwise (default: <code>FALSE</code>, i.e. colour mode</p>
</dd>
<dt><code>device</code></dt><dd><p>plot device used by the <code>zipfR</code> plotutils
(see <code><a href="#topic+zipfR.begin.plot">zipfR.begin.plot</a></code> for details).  Currently
supported devices are <code>x11</code> (default on most platforms), <code>eps</code>,
<code>pdf</code>, as well as <code>png</code> and <code>quartz</code> where
available (default on Mac OS X).</p>
</dd>
<dt><code>init.par</code></dt><dd><p>list of named graphics parameters passed to
the <code>par</code> function whenever a new viewport is created with
<code><a href="#topic+zipfR.begin.plot">zipfR.begin.plot</a></code></p>
</dd>
<dt><code>width</code>, <code>height</code></dt><dd><p>default width and height of the
plotting window opened by <code>zipfR.begin.plot</code></p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+plot.spc">plot.spc</a></code>, <code><a href="#topic+plot.vgc">plot.vgc</a></code>,
<code><a href="#topic+zipfR.begin.plot">zipfR.begin.plot</a></code>, <code><a href="#topic+zipfR.end.plot">zipfR.end.plot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
print(names(zipfR.par()))         # list available parameters

zipfR.par("col", "lty", "lwd")    # the default line styles
zipfR.par(c("col", "lty", "lwd")) # works as well

## temporary changes to graphics paramters:
par.save &lt;- zipfR.par(bw=TRUE, lwd.bw=2)
## plots use the modified parameters here
zipfR.par(par.save)		  # restore previous values

</code></pre>

<hr>
<h2 id='zipfR.plotutils'>Plotting Utilities (zipfR)</h2><span id='topic+zipfR.plotutils'></span><span id='topic+zipfR.begin.plot'></span><span id='topic+zipfR.end.plot'></span><span id='topic+zipfR.pick.device'></span>

<h3>Description</h3>

<p><b>These functions are deprecated and should not be used in new code.</b>
</p>
<p>Conveniently create plots with different layout and in different
output formats (both on-screen and various graphics file formats).
</p>
<p>Each plot is wrapped in a pair of <code>zipfR.begin.plot</code> and
<code>zipfR.end.plot</code> commands, which make sure that a suitable
plotting window / image file is opened and closed as required.  Format
and dimensions of the plots are controlled by global settings made
with <code>zipfR.par</code>, but can be overridden in the
<code>zipfR.begin.plot</code> call.
</p>
<p><code>zipfR.pick.device</code> automatically selects a default device by
scanning the specified vector for strings of the form <code>--pdf</code>,
<code>--eps</code>, etc.
</p>
<p><b>NB:</b> These are advanced functions intended to make it easier
to produce plots in different formats. Most users will only need
the basic plotting functionalities provided by <code><a href="#topic+plot.tfl">plot.tfl</a></code>, 
<code><a href="#topic+plot.spc">plot.spc</a></code> and <code><a href="#topic+plot.vgc">plot.vgc</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
zipfR.pick.device(args=commandArgs())

zipfR.begin.plot(device=zipfR.par("device"), filename="",
                 width=zipfR.par("width"), height=zipfR.par("height"),
                 bg=zipfR.par("bg"), pointsize=zipfR.par("pointsize"))

## plotting commands go here

zipfR.end.plot(shutdown=FALSE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="zipfR.plotutils_+3A_args">args</code></td>
<td>
<p>a character vector, which will be scanned for strings of
the form <code>--pdf</code>, <code>--eps</code>, etc.  If <code>args</code> is not
specified, the command-line arguments supplied to <span class="rlang"><b>R</b></span> will be
examined.</p>
</td></tr>
<tr><td><code id="zipfR.plotutils_+3A_device">device</code></td>
<td>
<p>name of plotting device to be used (see &quot;Devices&quot;
below)</p>
</td></tr>
<tr><td><code id="zipfR.plotutils_+3A_filename">filename</code></td>
<td>
<p>for graphics file devices, <em>basename</em> of the
output file.  A suitable extension for the selected file format will
be added automatically to <code>filename</code>.  This parameter is
ignored for screen devices.</p>
</td></tr>
<tr><td><code id="zipfR.plotutils_+3A_width">width</code>, <code id="zipfR.plotutils_+3A_height">height</code></td>
<td>
<p>width and height of the plotting window or image,
in inches</p>
</td></tr>
<tr><td><code id="zipfR.plotutils_+3A_bg">bg</code></td>
<td>
<p>background colour of the plotting window or image (use
<code>"transparent"</code> for images with transparent background)</p>
</td></tr>
<tr><td><code id="zipfR.plotutils_+3A_pointsize">pointsize</code></td>
<td>
<p>default point size for text in the plot</p>
</td></tr>
<tr><td><code id="zipfR.plotutils_+3A_shutdown">shutdown</code></td>
<td>
<p>if set to FALSE (the default), on-screen plot devices
will be kept open for re-use in the next plot.  Specify <code>shutdown=TRUE</code>
to ensure that the screen device is closed after a series of related plots.</p>
</td></tr> 
</table>


<h3>Details</h3>

<p><code>zipfR.begin.plot</code> opens a new plotting window or image file of
the specified dimensions (<code>width</code>, <code>height</code>), using the
selected graphics device (<code>device</code>).  Background colour
(<code>bg</code>) and default point size (<code>pointsize</code>) are set as
requested.  Then, any global graphics parameter settings (defined with
the <code>init.par</code> option of <code>zipfR.par</code>) are applied.
See the <code><a href="#topic+zipfR.par">zipfR.par</a></code> manpage for the &quot;factory default&quot;
settings of these options.
</p>
<p><code>zipfR.end.plot</code> finalizes the current plot.  For image file
devices, the device will be closed, writing the generated file to
disk.  For screen devices, the plotting window remains visible until a
new plot is started (which will close and re-open the plotting
window).
</p>
<p>The main purpose of the <code>zipfR</code> plotting utilities is to make it
easier to draw plots that are both shown on screen (for interactive
work) and saved to image files in various formats.  If an <span class="rlang"><b>R</b></span> script
specifies <code>filename</code>s in all <code>zipfR.begin.plot</code> commands, a
single global parameter setting at the start of the script is
sufficient to switch from screen graphics to EPS files, or any other
supported file format.
</p>
<p>On-screen plotting devices are platform-dependent, and there may be
different devices available depending on which version of R is used.
For this reason, <code>zipfR.begin.plot</code> no longer allows users to
pick an on-screen device explicitly, but rather opens a default device
with <code><a href="grDevices.html#topic+dev.new">dev.new</a></code>. Note that this default device may write
output to a graphics file, but is usually set to a suitable on-screen
device in an interactive R session. In any case, users can change the
default by setting <code>options(device=...)</code>. For backwards-compatibility,
the device name <code>x11</code> (and <code>quartz</code> on macOS is accepted
for the default graphics device.
</p>
<p>The <code>png</code> bitmap device may not be available on all platforms,
and may also require access to an X server.  Since the width and
height of a PNG device have to be specified in pixels rather than
inches, <code>zipfR.begin.plot</code> translates the <code>width</code> and
<code>height</code> settings, assuming a resolution of 150 dpi.  Use of
the <code>png</code> device is strongly discouraged.  A better way of
producing high-quality bitmaps is to generate EPS image (with the
<code>eps</code> device) and convert them to PNG or JPEG format with the
external <code>pstoimg</code> program (part of the <code>latex2html</code>
distribution).
</p>
<p><code>zipfR.pick.device</code> will issue a warning if multiple flags
matching supported graphics devices are found.  However, it is not an
error to find no matching flag, and all unrecognized strings are
silently ignored.
</p>


<h3>Value</h3>

<p><code>zipfR.begin.plot</code> invisibly returns the ID of the active plot device.
</p>


<h3>Devices</h3>

<p>Currently, the following devices are supported (and can be used in the
<code>device</code> argument).
</p>
<p><em>Screen devices:</em>
</p>

<dl>
<dt><code>x11</code></dt><dd><p>opens the default graphic device set by
<code><a href="base.html#topic+getOption">getOption</a>("device")</code>. In an interactive R sessions,
this will usually be a suitable on-screen device.</p>
</dd>
<dt><code>quartz</code></dt><dd><p>accepted as an alias for <code>x11</code> on
macOS platforms</p>
</dd>
</dl>

<p><em>Graphics file devices:</em>
</p>

<dl>
<dt><code>eps</code></dt><dd><p>Encapsulated PostScript (EPS) output (using
<code>postscript</code> device with appropriate settings)</p>
</dd>
<dt><code>pdf</code></dt><dd><p>PDF output</p>
</dd>
<dt><code>png</code></dt><dd><p>PNG bitmap file (may not be available on all
platforms)</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+zipfR.par">zipfR.par</a></code>, <code><a href="graphics.html#topic+par">par</a></code>
</p>
<p><a href="grDevices.html#topic+Devices">Devices</a>, <code><a href="grDevices.html#topic+dev.new">dev.new</a></code>, <code><a href="grDevices.html#topic+postscript">postscript</a></code>,
<code><a href="grDevices.html#topic+pdf">pdf</a></code> and <code>png</code> for more information about the
supported graphics devices
</p>
<p><code>zipfR</code>-specific plotting commands are <code><a href="#topic+plot.spc">plot.spc</a></code>, 
<code><a href="#topic+plot.spc">plot.spc</a></code> and <code><a href="#topic+plot.vgc">plot.vgc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
## these graphics parameters will be set for every new plot
zipfR.par(init.par=list(bg="lightblue", cex=1.3))
zipfR.par(width=12, height=9)

## will be shown on screen or saved to specified file, depending on
## selected device (eps -&gt; "myplot.eps", pdf -&gt; "myplot.pdf", etc.)

zipfR.begin.plot(filename="myplot")
plot.spc(Brown100k.spc)
zipfR.end.plot()

## By starting an R script "myplots.R" with this command, you can
## display plots on screen when stepping through the script in an
## interactive session, or save them to disk files in various
## graphics formats with "R --no-save --args --pdf &lt; myplots.R" etc.
zipfR.pick.device()

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
