<!DOCTYPE html><html><head><title>Help for package label.switching</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {label.switching}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#label.switching-package'>
<p>Algorithms for solving the label switching problem</p></a></li>
<li><a href='#aic'>
<p>Artificial Identifiability Constraints</p></a></li>
<li><a href='#compare.clust'>
<p>Make all estimated clusters agree with a pivot allocation</p></a></li>
<li><a href='#data_list'><p>Simulated MCMC sample and related information</p></a></li>
<li><a href='#dataBased'>
<p>Data-based labelling</p></a></li>
<li><a href='#ecr'>
<p>ECR algorithm (default version)</p></a></li>
<li><a href='#ecr.iterative.1'>
<p>ECR algorithm (iterative version 1)</p></a></li>
<li><a href='#ecr.iterative.2'>
<p>ECR algorithm (iterative version 2)</p></a></li>
<li><a href='#label.switching'>
<p>Main calling function</p></a></li>
<li><a href='#lamb'><p>Fetal lamb dataset</p></a></li>
<li><a href='#permute.mcmc'>
<p>Reorder MCMC samples</p></a></li>
<li><a href='#pra'>
<p>PRA algorithm</p></a></li>
<li><a href='#sjw'>
<p>Probabilistic relabelling algorithm</p></a></li>
<li><a href='#stephens'>
<p>Stephens' algorithm</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Relabelling MCMC Outputs of Mixture Models</td>
</tr>
<tr>
<td>Version:</td>
<td>1.8</td>
</tr>
<tr>
<td>Date:</td>
<td>2019-07-01</td>
</tr>
<tr>
<td>Author:</td>
<td>Panagiotis Papastamoulis</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Panagiotis Papastamoulis &lt;papapast@yahoo.gr&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>The Bayesian estimation of mixture models (and more general hidden Markov models) suffers from the label switching phenomenon, making the MCMC output non-identifiable. This package can be used in order to deal with this problem using various relabelling algorithms.</td>
</tr>
<tr>
<td>Imports:</td>
<td>combinat, lpSolve</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-07-01 10:23:21 UTC; panagiotis</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-07-01 10:50:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='label.switching-package'>
Algorithms for solving the label switching problem
</h2><span id='topic+label.switching-package'></span>

<h3>Description</h3>

<p>This package can be used to reorder MCMC outputs of parameters of mixture models (or more general ones, like hidden Markov). The label switching phenomenon is a fundamental problem to MCMC estimation of the parameters of such models. This package contains eight label switching solving algorithms: the default and iterative versions of ECR algorithm (Papastamoulis and Iliopoulos, 2010, 2013, Rodriguez and Walker, 2014, Papastamoulis, 2014), the data-based algorithm (Rodriguez and Walker, 2014), the Kullback-Leibler based algorithm of Stephens (2000), the probabilistic relabelling algorithm of Sperrin et al (2010), the artificial identifiability constraints method and the PRA algorithm (Marin et al, 2005, Marin and Robert, 2007). The user input depends on each method. Each algorithm returns a list of permutations. For comparison purposes, the user can also provide his/hers own set of permutations.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> label.switching</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.8</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2019-07-01</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-2</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>This is NOT a package to simulate MCMC samples from the posterior distribution of mixture models. MCMC output and related information serves as input to the available methods. There are eight functions that can be used to post-process the MCMC output:
</p>

<table>
<tr>
 <td style="text-align: left;">
Function </td><td style="text-align: left;"> Method </td><td style="text-align: left;"> Input </td>
</tr>
<tr>
 <td style="text-align: left;">
---------------------------- </td><td style="text-align: left;"> ---------------------------- </td><td style="text-align: left;"> ----------------------------------- </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>aic</code> </td><td style="text-align: left;"> ordering constraints </td><td style="text-align: left;"> <code>mcmc,constraint</code></td>
</tr>
<tr>
 <td style="text-align: left;">
<code>dataBased</code> </td><td style="text-align: left;"> data based </td><td style="text-align: left;"> <code>x,K,z</code></td>
</tr>
<tr>
 <td style="text-align: left;">
<code>ecr</code> </td><td style="text-align: left;"> ECR (default) </td><td style="text-align: left;"> <code>zpivot, z, K</code></td>
</tr>
<tr>
 <td style="text-align: left;">
<code>ecr.iterative.1</code> </td><td style="text-align: left;"> ECR (iterative vs. 1)</td><td style="text-align: left;"> <code>z, K</code></td>
</tr>
<tr>
 <td style="text-align: left;">
<code>ecr.iterative.2</code> </td><td style="text-align: left;"> ECR (iterative vs. 2)</td><td style="text-align: left;"><code>z, K, p</code></td>
</tr>
<tr>
 <td style="text-align: left;">
<code>pra</code> </td><td style="text-align: left;"> PRA </td><td style="text-align: left;"> <code>mcmc, pivot</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>stephens</code> </td><td style="text-align: left;"> Stephens</td><td style="text-align: left;"> <code>p</code></td>
</tr>
<tr>
 <td style="text-align: left;">
<code>sjw</code> </td><td style="text-align: left;"> Probabilistic</td><td style="text-align: left;"> <code>mcmc, z, complete, x</code>
</td>
</tr>

</table>

<p>Each function returns an <code class="reqn">m\times K</code> array of permutations, where <code class="reqn">m</code> and <code class="reqn">K</code> denote the MCMC sample size and number of mixture components, respectively. Next, these permutations can be applied to reorder the MCMC sample by applying the function <code>permute.mcmc</code>. The useR can call any of the above functions simultaneously using the main function of the package: <code><a href="#topic+label.switching">label.switching</a></code>. 
</p>


<h3>Note</h3>

<p>The most common method is to impose an identifiability constraint <code>aic</code>, however this approach has been widely criticized in the literature. The methods <code>ecr, ecr.iterative.1,ecr.iterative.2</code>, <code>stephens, dataBased</code> are solving the label switching problem using the function <code>lpAssign</code> of the package <code>lpSolve</code>. This is an integer programming algorithm for the solution of the assignment problem. Hence, these functions are computationally efficient even in cases where the number of components is quite large. On the other hand, methods <code>pra</code> and <code>sjw</code> are not designed in this way, so they are not suggested for large <code class="reqn">K</code>. 
</p>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>
<p>Maintainer: &lt;papapast@yahoo.gr&gt;
</p>


<h3>References</h3>

<p>Marin, J.M., Mengersen, K. and Robert, C.P. (2005). Bayesian modelling and inference on mixtures of distributions. Handbook of Statistics (25), D. Dey and C.R. Rao (eds). Elsevier-Sciences.
</p>
<p>Marin, J.M. and Robert, C.P. (2007). Bayesian Core: A Practical Approach to Computational Bayesian Statistics, Springer-Verlag, New York.
</p>
<p>Papastamoulis P. and Iliopoulos G. (2010). An artificial allocations based solution to the label switching problem in Bayesian analysis of mixtures of distributions. Journal of Computational and Graphical Statistics, 19: 313-331.
</p>
<p>Papastamoulis P. and Iliopoulos G. (2013). On the convergence rate of Random Permutation Sampler and ECR algorithm in missing data models. Methodology and Computing in Applied Probability, 15(2): 293-304.
</p>
<p>Papastamoulis P. (2014). Handling the label switching problem in latent class models via the ECR algorithm. Communications in Statistics, Simulation and Computation, 43(4): 913-927.
</p>
<p>Papastamoulis P. (2016). label.switching: An R Package for Dealing with the Label Switching Problem in MCMC Outputs. Journal of Statistical Software, Code Snippets, 69(1): 1-24.
</p>
<p>Rodriguez C.E. and Walker S. (2014). Label Switching in Bayesian Mixture Models: Deterministic relabeling strategies. Journal of Computational and Graphical Statistics. 23:1, 25-45
</p>
<p>Sperrin M, Jaki T and Wit E (2010). Probabilistic relabelling strategies for the label switching problem in Bayesian mixture models. Statistics and Computing, 20(3), 357-366.
</p>
<p>Stephens, M. (2000). Dealing with label Switching in mixture models. Journal of the Royal Statistical Society Series B, 62, 795-809.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+label.switching">label.switching</a></code>
</p>

<hr>
<h2 id='aic'>
Artificial Identifiability Constraints
</h2><span id='topic+aic'></span>

<h3>Description</h3>

<p>This function relabels the MCMC output by simply ordering a specific parameter. Let <code class="reqn">m</code>, <code class="reqn">K</code> and <code class="reqn">J</code> denote the number of simulated MCMC samples, number of mixture components and different parameter types, respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aic(mcmc.pars, constraint)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aic_+3A_mcmc.pars">mcmc.pars</code></td>
<td>

<p><code class="reqn">m\times K\times J</code> array of simulated MCMC parameters.
</p>
</td></tr>
<tr><td><code id="aic_+3A_constraint">constraint</code></td>
<td>

<p>An integer between 1 and J corresponding to the parameter that will be used to apply the Identifiabiality Constraint. In this case, the MCMC output is reordered according to the constraint </p>
<p style="text-align: center;"><code class="reqn">mcmc.pars[i,1,constraint] &lt; \ldots &lt; mcmc.pars[i,K,constraint],</code>
</p>
<p> for all <code class="reqn">i=1,\ldots,m</code>. If <code>constraint = "ALL"</code>, all <code class="reqn">J</code> Identifiability Constraints are applied. 
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>permutations</code></td>
<td>
<p>an <code class="reqn">m\times K</code> array of permutations.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>See Also</h3>

<p><code><a href="#topic+permute.mcmc">permute.mcmc</a></code>, <code><a href="#topic+label.switching">label.switching</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#load a toy example: MCMC output consists of the random beta model
# applied to a normal mixture of \code{K=2} components. The number of
# observations is equal to \code{n=5}. The number of MCMC samples is
# equal to \code{m=300}. The 1000 generated MCMC samples are stored 
#to array mcmc.pars. 
data("mcmc_output")
mcmc.pars&lt;-data_list$"mcmc.pars"

# mcmc parameters are stored to array \code{mcmc.pars}
# mcmc.pars[,,1]: simulated means of the two components
# mcmc.pars[,,2]: simulated variances of the two components
# mcmc.pars[,,3]: simulated weights of the two components
# We will apply AIC by ordering the means
# which corresponds to value \code{constraint=1}
run&lt;-aic(mcmc = mcmc.pars,constraint=1)
# apply the permutations returned by typing:
reordered.mcmc&lt;-permute.mcmc(mcmc.pars,run$permutations)
# reordered.mcmc[,,1]: reordered means of the two components
# reordered.mcmc[,,2]: reordered variances of the components
# reordered.mcmc[,,3]: reordered weights 
</code></pre>

<hr>
<h2 id='compare.clust'>
Make all estimated clusters agree with a pivot allocation
</h2><span id='topic+compare.clust'></span>

<h3>Description</h3>

<p>Given a pivot allocation vector, a set of simulated allocations and a set of permutations from different relabelling algorithms, this function relabels the permutations so that all methods maximize their similarity with the pivot. This is helpful when comparing different different label switching algorithms. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compare.clust(pivot.clust,perms,z,K)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compare.clust_+3A_pivot.clust">pivot.clust</code></td>
<td>

<p>a pivot allocation vector of the <code class="reqn">n</code> observations among the <code class="reqn">K</code> clusters.
</p>
</td></tr>
<tr><td><code id="compare.clust_+3A_perms">perms</code></td>
<td>

<p>a list containing <code class="reqn">f</code> permutation arrays, as returned by <code><a href="#topic+label.switching">label.switching</a></code> function.
</p>
</td></tr>
<tr><td><code id="compare.clust_+3A_z">z</code></td>
<td>

<p>a set of simulated allocation arrays.
</p>
</td></tr>
<tr><td><code id="compare.clust_+3A_k">K</code></td>
<td>

<p>number of mixture components
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>similarity</code></td>
<td>
<p><code class="reqn">(f+1) K\times (f+1)</code> matrix containing the similarity coefficient of the resulting clusters.</p>
</td></tr>
<tr><td><code>clusters</code></td>
<td>
<p><code class="reqn">f\times n</code> array of single best clusterings, relabelled in order to maximize their similarity with <code>pivot.clust</code>.</p>
</td></tr>
<tr><td><code>permutations</code></td>
<td>
<p>releaballed permutations.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>See Also</h3>

<p><code><a href="#topic+label.switching">label.switching</a></code> 
</p>

<hr>
<h2 id='data_list'>Simulated MCMC sample and related information</h2><span id='topic+data_list'></span>

<h3>Description</h3>

<p>This is a (very) small MCMC sample corresponding to data of 5 observations from a mixture 2 normal distributions. The MCMC sample consists of 300 iterations. It is stored to <code>data_list$mcmc.pars</code>. <code>data_list$mcmc.pars[,,1]</code> corresponds to means, <code>data_list$mcmc.pars[,,2]</code> corresponds to variances and <code>data_list$mcmc.pars[,,3]</code> corresponds to weights.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_list</code></pre>


<h3>Format</h3>

<p>A list containing simulated MCMC sample and all information required for the relabelling algorithms.</p>

<hr>
<h2 id='dataBased'>
Data-based labelling
</h2><span id='topic+dataBased'></span>

<h3>Description</h3>

<p>This function reorders the MCMC output according the data-based relabelling algorithm of Rodriguez and Walker (2014). The idea is to define a loss function which resembles a k-means type diveging measure of cluster centers. After the cluster centers have been estimated, the algorithm finds the optimal permutations that switch every simulated MCMC sample to them.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataBased(x, K, z)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataBased_+3A_x">x</code></td>
<td>

<p><code class="reqn">n</code>-dimensional data vector/array. 
</p>
</td></tr>
<tr><td><code id="dataBased_+3A_k">K</code></td>
<td>

<p>the number of mixture components.
</p>
</td></tr>
<tr><td><code id="dataBased_+3A_z">z</code></td>
<td>

<p><code class="reqn">m\times n</code> integer array of the latent allocation vectors generated from an MCMC algorithm. 
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>permutations</code></td>
<td>
<p><code class="reqn">m\times K</code> dimensional array of permutations</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>References</h3>

<p>Rodriguez C.E. and Walker S. (2014). Label Switching in Bayesian Mixture Models: Deterministic relabeling strategies. Journal of Computational and Graphical Statistics. 23:1, 25-45
</p>


<h3>See Also</h3>

<p><code><a href="#topic+permute.mcmc">permute.mcmc</a></code>, <code><a href="#topic+label.switching">label.switching</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#load a toy example: MCMC output consists of the random beta model
# applied to a normal mixture of \code{K=2} components. The number of
# observations is equal to \code{n=5}. The number of MCMC samples is
# equal to \code{m=300}. The 1000 generated MCMC samples are stored 
#to array mcmc.pars. 
data("mcmc_output")
z&lt;-data_list$"z"
K&lt;-data_list$"K"
x&lt;-data_list$"x"
mcmc.pars&lt;-data_list$"mcmc.pars"
# mcmc parameters are stored to array \code{mcmc.pars}
# mcmc.pars[,,1]: simulated means of the two components
# mcmc.pars[,,2]: simulated variances of the two components
# mcmc.pars[,,3]: simulated weights of the two components
# Apply dataBased relabelling
run&lt;-dataBased(x = x, K = K, z = z)
# apply the permutations returned by typing:
reordered.mcmc&lt;-permute.mcmc(mcmc.pars,run$permutations)
# reordered.mcmc[,,1]: reordered means of the two components
# reordered.mcmc[,,2]: reordered variances of the components
# reordered.mcmc[,,3]: reordered weights 
</code></pre>

<hr>
<h2 id='ecr'>
ECR algorithm (default version)
</h2><span id='topic+ecr'></span>

<h3>Description</h3>

<p>This function applies the standard version of Equivalence Classes Representatives (ECR) algorithm (Papastamoulis and Iliopoulos, 2010). The set of all allocation variables is partitioned into equivalence classes and exactly one representative is chosen from each class. The practical implementation of this idea is to reorder the output so that all simulated allocation vectors (<code>z</code>) are as similar as possible with a pivot allocation vector (<code>zpivot</code>).  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ecr(zpivot, z, K)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ecr_+3A_zpivot">zpivot</code></td>
<td>
<p><code class="reqn">n</code>-dimensional integer vector <code class="reqn">(z_1,\ldots,z_n)</code> with <code class="reqn">z_i\in\{1,\ldots,K\}</code>, <code class="reqn">i=1,\ldots,n</code>. 
</p>
</td></tr>
<tr><td><code id="ecr_+3A_z">z</code></td>
<td>

<p><code class="reqn">m\times n</code> integer array of the latent allocation vectors generated from an MCMC algorithm.
</p>
</td></tr>
<tr><td><code id="ecr_+3A_k">K</code></td>
<td>

<p>the number of mixture components (at least equal to 2).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>zpivot</code> should be chosen as an allocation vector that corresponds to a high-posterior density area, or in general as an allocation that is considered as a good allocation of the observations among the <code class="reqn">K</code> components. The useR has to specify this pivot allocation vector as a good allocation of the observations among the mixture components. Some typical choices are the allocations that correspond to the complete or non-complete MAP/ML estimates.
</p>


<h3>Value</h3>

<table>
<tr><td><code>permutations</code></td>
<td>
<p><code class="reqn">m\times K</code> dimensional array of permutations</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>References</h3>

<p>Papastamoulis P. and Iliopoulos G. (2010). An artificial allocations based solution to the label switching problem in Bayesian analysis of mixtures of distributions. Journal of Computational and Graphical Statistics, 19: 313-331.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+permute.mcmc">permute.mcmc</a></code>, <code><a href="#topic+label.switching">label.switching</a></code>, <code><a href="#topic+ecr.iterative.1">ecr.iterative.1</a></code>, <code><a href="#topic+ecr.iterative.2">ecr.iterative.2</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#load a toy example: MCMC output consists of the random beta model
#	applied to a normal mixture of \code{K=2} components. The
# 	number of observations is equal to \code{n=5}. The number
#	of MCMC samples is equal to \code{m=300}. The 300 
#	simulated allocations are stored to array \code{z}. The 
#	complete MAP estimate corresponds to iteration \code{mapindex}.
data("mcmc_output")
z&lt;-data_list$"z"
K&lt;-data_list$"K"
mapindex&lt;-data_list$"mapindex"

# mcmc parameters are stored to array \code{mcmc.pars}
mcmc.pars&lt;-data_list$"mcmc.pars"
# mcmc.pars[,,1]: simulated means of the two components
# mcmc.pars[,,2]: simulated variances 
# mcmc.pars[,,3]: simulated weights
run&lt;-ecr(zpivot = z[mapindex,],z = z, K = K)
# apply the permutations returned by typing:
reordered.mcmc&lt;-permute.mcmc(mcmc.pars,run$permutations)
# reordered.mcmc[,,1]: reordered means of the two components
# reordered.mcmc[,,2]: reordered variances
# reordered.mcmc[,,3]: reordered weights
</code></pre>

<hr>
<h2 id='ecr.iterative.1'>
ECR algorithm (iterative version 1)
</h2><span id='topic+ecr.iterative.1'></span>

<h3>Description</h3>

<p>This function applies the first iterative version of Equivalence Classes Representatives (ECR) algorithm (Papastamoulis and Iliopoulos, 2010, Rodriguez and Walker, 2012). The set of all allocation variables is partitioned into equivalence classes and exactly one representative is chosen from each class. The difference with the default version of ECR algorithm is that no pivot is required and the method is iterative, until a fixed pivot has been found.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ecr.iterative.1(z, K, opt_init, threshold, maxiter)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ecr.iterative.1_+3A_z">z</code></td>
<td>
<p><code class="reqn">m\times n</code> integer array of the latent allocation vectors generated from an MCMC algorithm.
</p>
</td></tr>
<tr><td><code id="ecr.iterative.1_+3A_k">K</code></td>
<td>
<p>the number of mixture components (at least equal to 2).
</p>
</td></tr>
<tr><td><code id="ecr.iterative.1_+3A_opt_init">opt_init</code></td>
<td>

<p>An (optional) <code class="reqn">m\times K</code> array of permutations to initialize the algorithm. The identity permutation is used if it is not specified.
</p>
</td></tr>
<tr><td><code id="ecr.iterative.1_+3A_threshold">threshold</code></td>
<td>

<p>An (optional) positive number controlling the convergence criterion. Default value: 1e-6.
</p>
</td></tr>
<tr><td><code id="ecr.iterative.1_+3A_maxiter">maxiter</code></td>
<td>

<p>An (optional) integer controlling the max number of iterations. Default value: 100.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>permutations</code></td>
<td>
<p><code class="reqn">m\times K</code> dimensional array of permutations</p>
</td></tr>
<tr><td><code>iterations</code></td>
<td>
<p>integer denoting the number of iterations until convergence</p>
</td></tr>
<tr><td><code>status</code></td>
<td>
<p>returns the exit status</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>References</h3>

<p>Papastamoulis P. and Iliopoulos G. (2010). An artificial allocations based solution to the label switching problem in Bayesian analysis of mixtures of distributions. Journal of Computational and Graphical Statistics, 19: 313-331.
</p>
<p>Rodriguez C.E. and Walker S. (2014). Label Switching in Bayesian Mixture Models: Deterministic relabeling strategies. Journal of Computational and Graphical Statistics. 23:1, 25-45
</p>


<h3>See Also</h3>

<p><code><a href="#topic+permute.mcmc">permute.mcmc</a></code>, <code><a href="#topic+label.switching">label.switching</a></code>, <code><a href="#topic+ecr">ecr</a></code>, <code><a href="#topic+ecr.iterative.2">ecr.iterative.2</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#load a toy example: MCMC output consists of the random beta model
# applied to a normal mixture of \code{K=2} components. The number of
# observations is equal to \code{n=5}. The number of MCMC samples is
# equal to \code{m=1000}. The 300 simulated allocations are stored to
# array \code{z}. 
data("mcmc_output")
# mcmc parameters are stored to array \code{mcmc.pars}
mcmc.pars&lt;-data_list$"mcmc.pars"
z&lt;-data_list$"z"
K&lt;-data_list$"K"
# mcmc.pars[,,1]: simulated means of the two components
# mcmc.pars[,,2]: simulated variances 
# mcmc.pars[,,3]: simulated weights
# the relabelling algorithm will run with the default initialization
# (no opt_init is specified)
run&lt;-ecr.iterative.1(z = z, K = K)
# apply the permutations returned by typing:
reordered.mcmc&lt;-permute.mcmc(mcmc.pars,run$permutations)
# reordered.mcmc[,,1]: reordered means of the two components
# reordered.mcmc[,,2]: reordered variances
# reordered.mcmc[,,3]: reordered weights
</code></pre>

<hr>
<h2 id='ecr.iterative.2'>
ECR algorithm (iterative version 2)
</h2><span id='topic+ecr.iterative.2'></span>

<h3>Description</h3>

<p>This function applies the second iterative version of Equivalence Classes Representatives (ECR) algorithm (Papastamoulis and Iliopoulos, 2010, Rodriguez and Walker, 2012). The set of all allocation variables is partitioned into equivalence classes and exactly one representative is chosen from each class. In this version the <code class="reqn">m\times n \times K</code> of allocation probabilities should be given as input as well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ecr.iterative.2(z, K, p, threshold, maxiter)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ecr.iterative.2_+3A_z">z</code></td>
<td>
<p><code class="reqn">m\times n</code> integer array of the latent allocation vectors generated from an MCMC algorithm.
</p>
</td></tr>
<tr><td><code id="ecr.iterative.2_+3A_k">K</code></td>
<td>
<p>the number of mixture components (at least equal to 2).
</p>
</td></tr>
<tr><td><code id="ecr.iterative.2_+3A_p">p</code></td>
<td>
<p><code class="reqn">m\times n \times K</code> dimensional array of allocation probabilities of the <code class="reqn">n</code> observations among the <code class="reqn">K</code> mixture components, for each iteration <code class="reqn">t = 1,\ldots,m</code> of the MCMC algorithm.
</p>
</td></tr>
<tr><td><code id="ecr.iterative.2_+3A_threshold">threshold</code></td>
<td>

<p>An (optional) positive number controlling the convergence criterion. Default value: 1e-6.
</p>
</td></tr>
<tr><td><code id="ecr.iterative.2_+3A_maxiter">maxiter</code></td>
<td>

<p>An (optional) integer controlling the max number of iterations. Default value: 100.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a given MCMC iteration <code class="reqn">t=1,\ldots,m</code>, let <code class="reqn">w_k^{(t)}</code> and <code class="reqn">\theta_k^{(t)}</code>, <code class="reqn">k=1,\ldots,K</code> denote the simulated mixture weights and component specific parameters respectively. Then, the <code class="reqn">(t,i,k)</code> element of <code>p</code> corresponds to the conditional probability that observation <code class="reqn">i=1,\ldots,n</code> belongs to component <code class="reqn">k</code> and is proportional to <code class="reqn">p_{tik} \propto w_k^{(t)} f(x_i|\theta_k^{(t)}), k=1,\ldots,K</code>, where <code class="reqn">f(x_i|\theta_k)</code> denotes the density of component <code class="reqn">k</code>. This means that:
</p>
<p style="text-align: center;"><code class="reqn">p_{tik} = \frac{w_k^{(t)} f(x_i|\theta_k^{(t)})}{w_1^{(t)} f(x_i|\theta_1^{(t)})+\ldots + w_K^{(t)} f(x_i|\theta_K^{(t)})}.</code>
</p>

<p>In case of hidden Markov models, the probabilities <code class="reqn">w_k</code> should be replaced with the proper left (normalized) eigenvector of the state-transition matrix.
</p>


<h3>Value</h3>

<table>
<tr><td><code>permutations</code></td>
<td>
<p><code class="reqn">m\times K</code> dimensional array of permutations</p>
</td></tr>
<tr><td><code>iterations</code></td>
<td>
<p>integer denoting the number of iterations until convergence</p>
</td></tr>
<tr><td><code>status</code></td>
<td>
<p>returns the exit status</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>References</h3>

<p>Papastamoulis P. and Iliopoulos G. (2010). An artificial allocations based solution to the label switching problem in Bayesian analysis of mixtures of distributions. Journal of Computational and Graphical Statistics, 19: 313-331.
</p>
<p>Rodriguez C.E. and Walker S. (2014). Label Switching in Bayesian Mixture Models: Deterministic relabeling strategies. Journal of Computational and Graphical Statistics. 23:1, 25-45
</p>


<h3>See Also</h3>

<p><code><a href="#topic+permute.mcmc">permute.mcmc</a></code>, <code><a href="#topic+label.switching">label.switching</a></code>, <code><a href="#topic+ecr">ecr</a></code>, <code><a href="#topic+ecr.iterative.1">ecr.iterative.1</a></code>, <code><a href="#topic+stephens">stephens</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#load a toy example: MCMC output consists of the random beta model
# applied to a normal mixture of \code{K=2} components. The number of
# observations is equal to \code{n=5}. The number of MCMC samples is
# equal to \code{m=1000}. The 300 simulated allocations are stored to
# array \code{z}. The matrix of allocation probabilities is stored to
# array \code{p}. 
data("mcmc_output")
z&lt;-data_list$"z"
K&lt;-data_list$"K"
p&lt;-data_list$"p"
# mcmc parameters are stored to array \code{mcmc.pars}
mcmc.pars&lt;-data_list$"mcmc.pars"
# mcmc.pars[,,1]: simulated means of the two components
# mcmc.pars[,,2]: simulated variances 
# mcmc.pars[,,3]: simulated weights
# the relabelling algorithm will run with the default initialization
# (no opt_init is specified)
run&lt;-ecr.iterative.2(z = z, K = 2, p = p)
# apply the permutations returned by typing:
reordered.mcmc&lt;-permute.mcmc(mcmc.pars,run$permutations)
# reordered.mcmc[,,1]: reordered means of the two mixture components
# reordered.mcmc[,,2]: reordered variances of the two components
# reordered.mcmc[,,3]: reordered weights of the two components
</code></pre>

<hr>
<h2 id='label.switching'>
Main calling function
</h2><span id='topic+label.switching'></span>

<h3>Description</h3>

<p>This is the main function of the package. It is used to reorder a simulated MCMC sample of the parameters of a mixture (or more general a hidden Markov model) according to eight label switching solving methods: ECR algorithm (default version), ECR algorithm (two iterative versions), PRA algorithm, Stephens' algorithm, Artificial Identifiability Constraint (AIC), Data-Based relabelling and a probabilistic relabelling algorithm (SJW). The input depends on the type of the label switching method. The output contains a list with the permutation returned by each method, the corresponding single best clusterings and the CPU time demanded for each method. In what follows: <code class="reqn">m</code> denotes the number of MCMC iterations, <code class="reqn">n</code> denotes the sample size of the observed data, <code class="reqn">K</code> denotes the number of mixture components and <code class="reqn">J</code> the number of different types of parameters of the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>label.switching(method, zpivot, z, K, prapivot, p, complete, 
			mcmc, sjwinit, data, constraint, 
			groundTruth, thrECR, thrSTE, thrSJW, 
			maxECR, maxSTE, maxSJW, userPerm)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="label.switching_+3A_method">method</code></td>
<td>

<p>any non-empty subset of c(&quot;ECR&quot;,&quot;ECR-ITERATIVE-1&quot;,&quot;PRA&quot;,&quot;ECR-ITERATIVE-2&quot;,&quot;STEPHENS&quot;,&quot;SJW&quot;,&quot;AIC&quot;,&quot;DATA-BASED&quot;) indicating the desired label-switching solving method. Also available is the option &quot;USER-PERM&quot; which corresponds to a user-defined set of permutations <code>userPerm</code>.
</p>
</td></tr>
<tr><td><code id="label.switching_+3A_zpivot">zpivot</code></td>
<td>

<p><code class="reqn">d\times n</code>-dimensional array of pivot allocation vectors, where <code class="reqn">d</code> denotes the number of pivots. This is demanded by the <code><a href="#topic+ecr">ecr</a></code> method. The method will be applied <code class="reqn">d</code> times.
</p>
</td></tr>
<tr><td><code id="label.switching_+3A_z">z</code></td>
<td>

<p><code class="reqn">m\times n</code> integer array of the latent allocation vectors generated from an MCMC algorithm. 
</p>
</td></tr>
<tr><td><code id="label.switching_+3A_k">K</code></td>
<td>

<p>the number of mixture components. This is demanded by the <code><a href="#topic+ecr">ecr</a></code>, <code><a href="#topic+ecr.iterative.1">ecr.iterative.1</a></code> and <code><a href="#topic+ecr.iterative.2">ecr.iterative.2</a></code> methods.
</p>
</td></tr>
<tr><td><code id="label.switching_+3A_prapivot">prapivot</code></td>
<td>

<p><code class="reqn">K\times J</code> array containing the parameter that will be used as a pivot by the <code><a href="#topic+pra">pra</a></code> method.
</p>
</td></tr>
<tr><td><code id="label.switching_+3A_p">p</code></td>
<td>

<p><code class="reqn">m\times n \times K</code> dimensional array of allocation probabilities of the <code class="reqn">n</code> observations among the <code class="reqn">K</code> mixture components, for each iteration <code class="reqn">t = 1,\ldots,m</code> of the MCMC algorithm. This is demanded by the <code><a href="#topic+ecr.iterative.2">ecr.iterative.2</a></code> and <code><a href="#topic+stephens">stephens</a></code> methods.
</p>
</td></tr>
<tr><td><code id="label.switching_+3A_complete">complete</code></td>
<td>

<p>function that returns the complete log-likelihood of the mixture model. Demanded by <code><a href="#topic+sjw">sjw</a></code> method.
</p>
</td></tr>
<tr><td><code id="label.switching_+3A_mcmc">mcmc</code></td>
<td>

<p><code class="reqn">m\times K\times J</code> array of simulated MCMC parameters. Needed by <code><a href="#topic+sjw">sjw</a></code> and <code><a href="#topic+pra">pra</a></code> methods.
</p>
</td></tr>
<tr><td><code id="label.switching_+3A_sjwinit">sjwinit</code></td>
<td>

<p>An index pointing at the MCMC iteration whose parameters will initialize the <code><a href="#topic+sjw">sjw</a></code> algorithm (optional). 
</p>
</td></tr>
<tr><td><code id="label.switching_+3A_data">data</code></td>
<td>

<p><code class="reqn">n</code>-dimensional data vector/array. Needed by the <code><a href="#topic+sjw">sjw</a></code> and <code><a href="#topic+dataBased">dataBased</a></code> algorithms.
</p>
</td></tr>
<tr><td><code id="label.switching_+3A_constraint">constraint</code></td>
<td>

<p>An (optional) integer between 1 and J corresponding to the parameter that will be used to apply the Identifiabiality Constraint. In this casethe mcmc output is reordered according to the constraint <code class="reqn">mcmc[i,1,constraint] &lt; \ldots &lt; mcmc[i,K,constraint]</code>. If <code>constraint = "ALL"</code>, all <code class="reqn">J</code> Identifiability Constraints are applied. Default value: 1.
</p>
</td></tr>
<tr><td><code id="label.switching_+3A_groundtruth">groundTruth</code></td>
<td>

<p>Optional integer vector of <code class="reqn">n</code> allocations, which are considered as the 'ground truth' allocations of the <code class="reqn">n</code> observations among the <code class="reqn">K</code> mixture components. The output of all methods will be relabelled in a way that the resulting single best clusterings maximize their similarity with the ground truth. This option is very useful in simulation studies or in any other case that the cluster labels are known in order to perform comparisons between methods. 
</p>
</td></tr>
<tr><td><code id="label.switching_+3A_threcr">thrECR</code></td>
<td>

<p>An (optional) positive number controlling the convergence criterion for <code>ecr.iterative.1</code> and <code>ecr.iterative.2</code>. Default value: 1e-6.
</p>
</td></tr>
<tr><td><code id="label.switching_+3A_thrste">thrSTE</code></td>
<td>

<p>An (optional) positive number controlling the convergence criterion for <code>stephens</code>. Default value: 1e-6.
</p>
</td></tr>
<tr><td><code id="label.switching_+3A_thrsjw">thrSJW</code></td>
<td>

<p>An (optional) positive number controlling the convergence criterion for <code>sjw</code>. Default value: 1e-6.
</p>
</td></tr>
<tr><td><code id="label.switching_+3A_maxecr">maxECR</code></td>
<td>

<p>An (optional) integer controlling the max number of iterations for <code>ecr.iterative.1</code> and <code>ecr.iterative.2</code>. Default value: 100.
</p>
</td></tr>
<tr><td><code id="label.switching_+3A_maxste">maxSTE</code></td>
<td>

<p>An (optional) integer controlling the max number of iterations for <code>stephens</code>. Default value: 100.
</p>
</td></tr>
<tr><td><code id="label.switching_+3A_maxsjw">maxSJW</code></td>
<td>

<p>An (optional) integer controlling the max number of iterations for <code>sjw</code>. Default value: 100.
</p>
</td></tr>
<tr><td><code id="label.switching_+3A_userperm">userPerm</code></td>
<td>

<p>An (optional) list with user-defined permutations. It is required only if &quot;USER-PERM&quot; has been chosen in <code>method</code>. In this case, <code>userPerm[[i]]</code> is an <code class="reqn">m\times K</code> array of permutations for all <code class="reqn">i = 1,\ldots,S</code>, where <code class="reqn">S</code> denotes the number of permutation arrays. This is useful in case that the user wants to compare his/hers own relabelling method with the available ones. 
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>permutations</code></td>
<td>
<p>an <code class="reqn">m\times K</code> array of permutations per method.</p>
</td></tr>
<tr><td><code>clusters</code></td>
<td>
<p>an <code class="reqn">n</code> dimensional vector of best clustering of the the observations for each method.</p>
</td></tr>
<tr><td><code>timings</code></td>
<td>
<p>CPU time needed for each relabelling method.</p>
</td></tr>
<tr><td><code>similarity</code></td>
<td>
<p>correlation matrix between the label switching solving methods in terms of their matching best-clustering allocations.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>If the ground truth is not given, all methods are reordered using the estimated single best clustering of the first provided method. The methods <code><a href="#topic+sjw">sjw</a></code> and <code><a href="#topic+pra">pra</a></code> are not suggested for large number of components. Also note that <code><a href="#topic+sjw">sjw</a></code> might be quite slow even for small number of components. In this case try adjusting <code>thrSJW</code> or <code>maxSJW</code> to smaller values the default ones.
</p>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>References</h3>

<p>Papastamoulis P. (2016). label.switching: An R Package for Dealing with the Label Switching Problem in MCMC Outputs. Journal of Statistical Software, Code Snippets, 69(1): 1-24.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ecr">ecr</a></code>, <code><a href="#topic+ecr.iterative.1">ecr.iterative.1</a></code>, <code><a href="#topic+ecr.iterative.2">ecr.iterative.2</a></code>, <code><a href="#topic+stephens">stephens</a></code>, <code><a href="#topic+pra">pra</a></code>, <code><a href="#topic+sjw">sjw</a></code>, <code><a href="#topic+dataBased">dataBased</a></code>, <code><a href="#topic+aic">aic</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># We will apply the following methods:
# ECR, ECR-ITERATIVE-1, PRA, AIC and DATA-BASED.
# default ECR will use two different pivots.

#load a toy example: MCMC output consists of the random beta model
# applied to a normal mixture of \code{K=2} components. The number of
# observations is equal to \code{n=5}. The number of MCMC samples is
# equal to \code{m=300}. simulated allocations are stored to array \code{z}. 
data("mcmc_output")
mcmc.pars&lt;-data_list$"mcmc.pars"
# mcmc parameters are stored to array \code{mcmc.pars}
# mcmc.pars[,,1]: simulated means of the two components
# mcmc.pars[,,2]: simulated variances 
# mcmc.pars[,,3]: simulated weights 
# We will use two pivots for default ECR algorithm:
# the first one corresponds to iteration \code{mapindex} (complete MAP)
# the second one corresponds to iteration \code{mapindex.non} (observed MAP)

z&lt;-data_list$"z"
K&lt;-data_list$"K"
x&lt;-data_list$"x"
mapindex&lt;-data_list$"mapindex"
mapindex.non&lt;-data_list$"mapindex.non"
# The PRA method will use as pivot the iteration that corresponds to
# the observed MAP estimate (mapindex). 

#Apply (a subset of the available) methods by typing:

ls&lt;-label.switching(method=c("ECR","ECR-ITERATIVE-1","PRA", "AIC","DATA-BASED"),
zpivot=z[c(mapindex,mapindex.non),],z = z,K = K, data = x,
prapivot = mcmc.pars[mapindex,,],mcmc = mcmc.pars)

#plot the raw and reordered means of the K=2 normal mixture components for each method
par(mfrow = c(2,4))
#raw MCMC output for the means (with label switching)
matplot(mcmc.pars[,,1],type="l",
xlab="iteration",main="Raw MCMC output",ylab = "means")
# Reordered outputs
matplot(permute.mcmc(mcmc.pars,ls$permutations$"ECR-1")$output[,,1],type="l",
xlab="iteration",main="ECR (1st pivot)",ylab = "means")
matplot(permute.mcmc(mcmc.pars,ls$permutations$"ECR-2")$output[,,1],type="l",
xlab="iteration",main="ECR (2nd pivot)",ylab = "means")
matplot(permute.mcmc(mcmc.pars,ls$permutations$"ECR-ITERATIVE-1")$output[,,1],
type="l",xlab="iteration",main="ECR-iterative-1",ylab = "means")
matplot(permute.mcmc(mcmc.pars,ls$permutations$"PRA")$output[,,1],type="l",
xlab="iteration",main="PRA",ylab = "means")
matplot(permute.mcmc(mcmc.pars,ls$permutations$"AIC")$output[,,1],type="l",
xlab="iteration",main="AIC",ylab = "means")
matplot(permute.mcmc(mcmc.pars,ls$permutations$"DATA-BASED")$output[,,1],type="l",
xlab="iteration",main="DATA-BASED",ylab = "means")

#######################################################
# if the useR wants to apply the STEPHENS and SJW algorithm as well:
# The STEPHENS method requires the classification probabilities
p&lt;-data_list$"p"

# The SJW method needs to define the complete log-likelihood of the
# model. For the univariate normal mixture, this is done as follows:

complete.normal.loglikelihood&lt;-function(x,z,pars){
	#x: denotes the n data points
	#z: denotes an allocation vector (size=n)
	#pars: K\times 3 vector of means,variance, weights
	# pars[k,1]: corresponds to the mean of component k
	# pars[k,2]: corresponds to the variance of component k
	# pars[k,3]: corresponds to the weight of component k
	g &lt;- dim(pars)[1]
	n &lt;- length(x)
	logl&lt;- rep(0, n)
 	logpi &lt;- log(pars[,3])
	mean &lt;- pars[,1]
	sigma &lt;- sqrt(pars[,2])
	logl&lt;-logpi[z] + dnorm(x,mean = mean[z],sd = sigma[z],log = T)
	return(sum(logl))
}

# and then run (after removing all #):
#ls&lt;-label.switching(method=c("ECR","ECR-ITERATIVE-1","ECR-ITERATIVE-2",
#"PRA","STEPHENS","SJW","AIC","DATA-BASED"),
#zpivot=z[c(mapindex,mapindex.non),],z = z,
#K = K,prapivot = mcmc.pars[mapindex,,],p=p,
#complete = complete.normal.loglikelihood,mcmc.pars,
#data = x)

</code></pre>

<hr>
<h2 id='lamb'>Fetal lamb dataset</h2><span id='topic+lamb'></span>

<h3>Description</h3>

<p>240 body movement measurements of a fetal lamb at consecutive 5 second intervals.</p>


<h3>Usage</h3>

<pre><code class='language-R'>lamb</code></pre>


<h3>Format</h3>

<p>Count data.</p>


<h3>References</h3>


<p>Leroux B, Putterman M (1992). Maximum Penalized Likelihood estimation for independent and Markov-dependent Mixture models.Biometrics, 48, 545&ndash;558.
</p>

<hr>
<h2 id='permute.mcmc'>
Reorder MCMC samples
</h2><span id='topic+permute.mcmc'></span>

<h3>Description</h3>

<p>This function applies the permutation returned by any relabelling algorithm to a simulated MCMC output.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>permute.mcmc(mcmc, permutations)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="permute.mcmc_+3A_mcmc">mcmc</code></td>
<td>

<p><code class="reqn">m\times K\times J</code> array of simulated MCMC parameters.
</p>
</td></tr>
<tr><td><code id="permute.mcmc_+3A_permutations">permutations</code></td>
<td>

<p><code class="reqn">m\times K</code> dimensional array of permutations.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>output</code></td>
<td>
<p><code class="reqn">m\times K\times J</code> array of reordered MCMC parameters.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>See Also</h3>

<p><code><a href="#topic+label.switching">label.switching</a></code>, <code><a href="#topic+ecr">ecr</a></code>, <code><a href="#topic+ecr.iterative.1">ecr.iterative.1</a></code>, <code><a href="#topic+ecr.iterative.2">ecr.iterative.2</a></code>,<code><a href="#topic+stephens">stephens</a></code>,<code><a href="#topic+pra">pra</a></code>, <code><a href="#topic+sjw">sjw</a></code>, <code><a href="#topic+aic">aic</a></code>, <code><a href="#topic+dataBased">dataBased</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#load MCMC simulated data
data("mcmc_output")
mcmc.pars&lt;-data_list$"mcmc.pars"
z&lt;-data_list$"z"
K&lt;-data_list$"K"

#apply \code{ecr.iterative.1} algorithm
run&lt;-ecr.iterative.1(z = z, K = 2)
#reorder the MCMC output according to this method:
reordered.mcmc&lt;-permute.mcmc(mcmc.pars,run$permutations)
# reordered.mcmc[,,1]: reordered means of the two components
# reordered.mcmc[,,2]: reordered variances of the components
# reordered.mcmc[,,3]: reordered weights of the two components
</code></pre>

<hr>
<h2 id='pra'>
PRA algorithm
</h2><span id='topic+pra'></span>

<h3>Description</h3>

<p>This function reorders the MCMC output using the geometrically-based Pivotal Reordering Algorithm (PRA) (Marin et al, 2005, Marin and Robert, 2007). The method requires as input the generated MCMC sample and a pivot parameter vector. The user should be careful in order the pivot elements have the same parameters with the generated MCMC output. The simulated MCMC sample should be provided by the useR as a <code class="reqn">m\times K\times J</code> dimensional array, where <code class="reqn">m</code> denotes the number of MCMC samples, <code class="reqn">K</code> denotes the number of mixture components and <code class="reqn">J</code> corresponds to the number of different parameter types of the model. The pivot should correspond to a high-posterior density point.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pra(mcmc.pars, pivot)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pra_+3A_mcmc.pars">mcmc.pars</code></td>
<td>

<p><code class="reqn">m\times K\times J</code> array of simulated MCMC parameters.
</p>
</td></tr>
<tr><td><code id="pra_+3A_pivot">pivot</code></td>
<td>

<p><code class="reqn">K\times J</code> array containing the parameter that will be used as a pivot.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The positive integer <code class="reqn">J</code> denotes the number of different parameter types of the model. For example, in a univariate normal mixture model there are <code class="reqn">J = 3</code> different types: means, variances and weights. In a Poisson mixture there are <code class="reqn">J=2</code> types: means and weights.
</p>


<h3>Value</h3>

<table>
<tr><td><code>permutations</code></td>
<td>
<p><code class="reqn">m\times K</code> dimensional array of permutations</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>References</h3>

<p>Marin, J.M., Mengersen, K. and Robert, C.P. (2005). Bayesian modelling and inference on mixtures of distributions. Handbook of Statistics (25), D. Dey and C.R. Rao (eds). Elsevier-Sciences.
</p>
<p>Marin, J.M. and Robert, C.P. (2007). Bayesian Core: A Practical Approach to Computational Bayesian Statistics, Springer-Verlag, New York.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+permute.mcmc">permute.mcmc</a></code>, <code><a href="#topic+label.switching">label.switching</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#load a toy example: MCMC output consists of the random beta model
# applied to a normal mixture of \code{K=2} components. The number of
# observations is equal to \code{n=5}. The number of MCMC samples is
# equal to \code{m=300}. The 1000 generated MCMC samples are stored 
#to array mcmc.pars. 
data("mcmc_output")
mcmc.pars&lt;-data_list$"mcmc.pars"
mapindex&lt;-data_list$"mapindex"

# mcmc parameters are stored to array \code{mcmc.pars}
# mcmc.pars[,,1]: simulated means of the two components
# mcmc.pars[,,2]: simulated variances of the two components
# mcmc.pars[,,3]: simulated weights of the two components
# We will apply PRA using as pivot the complete MAP estimate
# which corresponds to \code{mcmc.pars[mapindex,,]}
run&lt;-pra(mcmc = mcmc.pars, pivot = mcmc.pars[mapindex,,])
# apply the permutations returned by typing:
reordered.mcmc&lt;-permute.mcmc(mcmc.pars,run$permutations)
# reordered.mcmc[,,1]: reordered means of the two components
# reordered.mcmc[,,2]: reordered variances of the components
# reordered.mcmc[,,3]: reordered weights 
</code></pre>

<hr>
<h2 id='sjw'>
Probabilistic relabelling algorithm
</h2><span id='topic+sjw'></span>

<h3>Description</h3>

<p>Function to apply the probabilistic relabelling strategy of Sperrin et al (2010). The concept here is to treat the MCMC output as observed data, while the unknown permutations need to be applied to each mcmc data point is treated as unobserved data with associated uncertainty. Then, an EM-type algorithm estimates the weights for each permutation per MCMC data point.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sjw(mcmc.pars, z, complete, x, init, threshold, maxiter)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sjw_+3A_mcmc.pars">mcmc.pars</code></td>
<td>

<p><code class="reqn">m\times K\times J</code> array of simulated MCMC parameters.
</p>
</td></tr>
<tr><td><code id="sjw_+3A_z">z</code></td>
<td>

<p><code class="reqn">m\times n</code> integer array of the latent allocation vectors generated from an MCMC algorithm.
</p>
</td></tr>
<tr><td><code id="sjw_+3A_complete">complete</code></td>
<td>

<p>function that returns the complete log-likelihood of the mixture model.  
</p>
</td></tr>
<tr><td><code id="sjw_+3A_x">x</code></td>
<td>

<p><code class="reqn">n</code>-dimensional data vector/array
</p>
</td></tr>
<tr><td><code id="sjw_+3A_init">init</code></td>
<td>

<p>An (optional) index pointing at the MCMC iteration whose parameters will initialize the algorithm. If it is less or equal to zero, the overall MCMC mean will be used for initialization. 
</p>
</td></tr>
<tr><td><code id="sjw_+3A_threshold">threshold</code></td>
<td>

<p>An (optional) positive number controlling the convergence criterion. Default value: 1e-6.
</p>
</td></tr>
<tr><td><code id="sjw_+3A_maxiter">maxiter</code></td>
<td>

<p>An (optional) integer controlling the max number of iterations. Default value: 100.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let <code class="reqn">x=(x_1,\ldots,x_n)</code> denote the observed data and <code class="reqn">\boldsymbol{w},\boldsymbol{\theta}</code> denote the mixture weights and component specific parameters, respectively. Assume that <code class="reqn">K</code> is the the number of components. Then,
</p>
<p style="text-align: center;"><code class="reqn">L(\boldsymbol{w},\boldsymbol{\theta}|\boldsymbol{x})=\prod_{i=1}^{n}\sum_{k=1}^{K}w_k f_k(x_i|\theta_k),</code>
</p>
 
<p><code class="reqn">i=1,\ldots,n</code> is the observed likelihood of the mixture model. Given the latent allocation variables <code class="reqn">\boldsymbol{z}=(z_1,\ldots,z_n)</code>, the complete likelihood of the model is defined as: 
</p>
<p style="text-align: center;"><code class="reqn">L_c(\boldsymbol{w},\boldsymbol{\theta}|\boldsymbol{x},\boldsymbol{z})=\prod_{i=1}^{n}w_{z_{i}}f_{z_{i}}(x_i|\theta_{z_{i}}).</code>
</p>

<p>Then, <code>complete</code> corresponds to the log of <code class="reqn">L_c</code> and should take as input the following: a vector of <code class="reqn">n</code> allocations, the observed data and the parameters of the model as a <code class="reqn">K\times J</code> array where <code class="reqn">J</code> corresponds to the different parameter types of the model. See the example for an implementation at a univariate normal mixture.
</p>


<h3>Value</h3>

<table>
<tr><td><code>permutations</code></td>
<td>
<p><code class="reqn">m\times K</code> dimensional array of permutations</p>
</td></tr>
<tr><td><code>iterations</code></td>
<td>
<p>integer denoting the number of iterations until convergence</p>
</td></tr>
<tr><td><code>status</code></td>
<td>
<p>returns the exit status</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This algorithm is not suggested for large number of components due to the computational overload: <code class="reqn">K!</code> permutation probabilities are computed at each MCMC iteration. Moreover, the useR should carefully provide the complete log-likelihood function of the model as input to the algorithm and this makes its use quite complicated.
</p>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>References</h3>

<p>Sperrin M, Jaki T and Wit E (2010). Probabilistic relabelling strategies for the label switching problem in Bayesian mixture models.  Statistics and Computing, 20(3), 357-366.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+permute.mcmc">permute.mcmc</a></code>, <code><a href="#topic+label.switching">label.switching</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#load a toy example: MCMC output consists of the random beta model
# applied to a normal mixture of \code{K=2} components. The number of
# observations is equal to \code{n=5}. The number of MCMC samples is
# equal to \code{m=300}.  
data("mcmc_output")
mcmc.pars&lt;-data_list$"mcmc.pars"
z&lt;-data_list$"z"
K&lt;-data_list$"K"
x&lt;-data_list$"x"

# mcmc parameters are stored to array \code{mcmc.pars}
# mcmc.pars[,,1]: simulated means of the two components
# mcmc.pars[,,2]: simulated variances
# mcmc.pars[,,3]: simulated weights 
# The number of different parameters for the univariate
# normal mixture is equal to J = 3: means, variances 
# and weights. The generated allocations variables are 
# stored to \code{z}. The observed data is stored to \code{x}.  
# The complete data log-likelihood is defined as follows:
complete.normal.loglikelihood&lt;-function(x,z,pars){
#	x: data (size = n)
#	z: allocation vector (size = n)
#	pars: K\times J vector of normal mixture parameters:
#		pars[k,1] = mean of the k-normal component
#		pars[k,2] = variance of the k-normal component
#		pars[k,3] = weight of the k-normal component
#			k = 1,...,K
	g &lt;- dim(pars)[1] #K (number of mixture components)
	n &lt;- length(x)	#this denotes the sample size
	logl&lt;- rep(0, n)	
 	logpi &lt;- log(pars[,3])
	mean &lt;- pars[,1]
	sigma &lt;- sqrt(pars[,2])
	logl&lt;-logpi[z] + dnorm(x,mean = mean[z],sd = sigma[z],log = TRUE)
	return(sum(logl))
}

#run the algorithm:
run&lt;-sjw(mcmc = mcmc.pars,z = z, 
complete = complete.normal.loglikelihood,x = x, init=0,threshold = 1e-4)
# apply the permutations returned by typing:
reordered.mcmc&lt;-permute.mcmc(mcmc.pars,run$permutations)
# reordered.mcmc[,,1]: reordered means of the two components
# reordered.mcmc[,,2]: reordered variances 
# reordered.mcmc[,,3]: reordered weights
</code></pre>

<hr>
<h2 id='stephens'>
Stephens' algorithm
</h2><span id='topic+stephens'></span>

<h3>Description</h3>

<p>Stephens (2000) developed a relabelling algorithm that makes the permuted sample points to agree as much as possible on the <code class="reqn">n\times K</code> matrix of classification probabilities, using the Kullback-Leibler divergence. The algorithm's input is the matrix of allocation probabilities for each MCMC iteration.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stephens(p, threshold, maxiter)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stephens_+3A_p">p</code></td>
<td>

<p><code class="reqn">m\times n \times K</code> dimensional array of allocation probabilities of the <code class="reqn">n</code> observations among the <code class="reqn">K</code> mixture components, for each iteration <code class="reqn">t = 1,\ldots,m</code> of the MCMC algorithm.
</p>
</td></tr>
<tr><td><code id="stephens_+3A_threshold">threshold</code></td>
<td>

<p>An (optional) positive number controlling the convergence criterion. Default value: 1e-6.
</p>
</td></tr>
<tr><td><code id="stephens_+3A_maxiter">maxiter</code></td>
<td>

<p>An (optional) integer controlling the max number of iterations. Default value: 100.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For a given MCMC iteration <code class="reqn">t=1,\ldots,m</code>, let <code class="reqn">w_k^{(t)}</code> and <code class="reqn">\theta_k^{(t)}</code>, <code class="reqn">k=1,\ldots,K</code> denote the simulated mixture weights and component specific parameters respectively. Then, the <code class="reqn">(t,i,k)</code> element of <code>p</code> corresponds to the conditional probability that observation <code class="reqn">i=1,\ldots,n</code> belongs to component <code class="reqn">k</code> and is proportional to <code class="reqn">p_{tik} \propto w_k^{(t)} f(x_i|\theta_k^{(t)}), k=1,\ldots,K</code>, where <code class="reqn">f(x_i|\theta_k)</code> denotes the density of component <code class="reqn">k</code>. This means that:
</p>
<p style="text-align: center;"><code class="reqn">p_{tik} = \frac{w_k^{(t)} f(x_i|\theta_k^{(t)})}{w_1^{(t)} f(x_i|\theta_1^{(t)})+\ldots + w_K^{(t)} f(x_i|\theta_K^{(t)})}.</code>
</p>

<p>In case of hidden Markov models, the probabilities <code class="reqn">w_k</code> should be replaced with the proper left (normalized) eigenvector of the state-transition matrix.
</p>


<h3>Value</h3>

<table>
<tr><td><code>permutations</code></td>
<td>
<p><code class="reqn">m\times K</code> dimensional array of permutations</p>
</td></tr>
<tr><td><code>iterations</code></td>
<td>
<p>integer denoting the number of iterations until convergence</p>
</td></tr>
<tr><td><code>status</code></td>
<td>
<p>returns the exit status</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Panagiotis Papastamoulis
</p>


<h3>References</h3>

<p>Stephens, M. (2000). Dealing with label Switching in mixture models. Journal of the Royal Statistical Society Series B, 62, 795-809.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+permute.mcmc">permute.mcmc</a></code>, <code><a href="#topic+label.switching">label.switching</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#load a toy example: MCMC output consists of the random beta model
# applied to a normal mixture of \code{K=2} components. The number 
# of observations is equal to \code{n=5}. The number of MCMC samples
# is equal to \code{m=300}. The matrix of allocation probabilities 
# is stored to matrix \code{p}. 
data("mcmc_output")
# mcmc parameters are stored to array \code{mcmc.pars}
mcmc.pars&lt;-data_list$"mcmc.pars"
# mcmc.pars[,,1]: simulated means of the two components
# mcmc.pars[,,2]: simulated variances 
# mcmc.pars[,,3]: simulated weights 
# the computed allocation matrix is p
p&lt;-data_list$"p"
run&lt;-stephens(p)
# apply the permutations returned by typing:
reordered.mcmc&lt;-permute.mcmc(mcmc.pars,run$permutations)
# reordered.mcmc[,,1]: reordered means of the components
# reordered.mcmc[,,2]: reordered variances
# reordered.mcmc[,,3]: reordered weights
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
