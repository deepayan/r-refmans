<!DOCTYPE html><html lang="en"><head><title>Help for package psvmSDR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {psvmSDR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#npsdr'><p>A unified Principal sufficient dimension reduction method via kernel trick</p></a></li>
<li><a href='#npsdr_x'><p>Reconstruct the estimated sufficient predictors for a given data matrix</p></a></li>
<li><a href='#plot.npsdr'><p>Scatter plot with sufficient predictors from npsdr() function</p></a></li>
<li><a href='#plot.psdr'><p>Scatter plot with sufficient predictors from psdr() function</p></a></li>
<li><a href='#psdr'><p>Unified linear principal sufficient dimension reduction methods</p></a></li>
<li><a href='#psdr_bic'><p>Order estimation via BIC-type criterion</p></a></li>
<li><a href='#rtpsdr'><p>Real time sufficient dimension reduction through principal least squares SVM</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Unified Principal Sufficient Dimension Reduction Package</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.2</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jungmin Shin &lt;jungminshin@korea.ac.kr&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>A unified and user-friendly framework for applying the principal sufficient dimension reduction methods for both linear and nonlinear cases. The package has an extendable power by varying loss functions for the support vector machine, even for an user-defined arbitrary function, unless those are convex and differentiable everywhere over the support (Li et al. (2011) &lt;<a href="https://doi.org/10.1214%2F11-AOS932">doi:10.1214/11-AOS932</a>&gt;). Also, it provides a real-time sufficient dimension reduction update procedure using the principal least squares support vector machine (Artemiou et al. (2021) &lt;<a href="https://doi.org/10.1016%2Fj.patcog.2020.107768">doi:10.1016/j.patcog.2020.107768</a>&gt;).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, graphics</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-09-02 04:10:45 UTC; shin</td>
</tr>
<tr>
<td>Author:</td>
<td>Jungmin Shin [aut, cre],
  Seung Jun Shin [aut],
  Andreas Artemiou [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-09-09 12:10:04 UTC</td>
</tr>
</table>
<hr>
<h2 id='npsdr'>A unified Principal sufficient dimension reduction method via kernel trick</h2><span id='topic+npsdr'></span>

<h3>Description</h3>

<p>Principal Sufficient Dimension Reduction method
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npsdr(
  x,
  y,
  loss = "svm",
  h = 10,
  lambda = 1,
  b = floor(length(y)/3),
  eps = 1e-05,
  max.iter = 100,
  eta = 0.1,
  mtype,
  plot = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="npsdr_+3A_x">x</code></td>
<td>
<p>data matrix</p>
</td></tr>
<tr><td><code id="npsdr_+3A_y">y</code></td>
<td>
<p>either continuous or (+1,-1) typed binary response vector</p>
</td></tr>
<tr><td><code id="npsdr_+3A_loss">loss</code></td>
<td>
<p>pre-specified loss functions belongs to <code>"svm", "logit", "l2svm", "wsvm", "qr", "asls", "wlogit", "wl2svm", "lssvm", "wlssvm"</code>, and user-defined loss function object also can be used formed by inside double (or single) quotation mark. Default is 'svm'.</p>
</td></tr>
<tr><td><code id="npsdr_+3A_h">h</code></td>
<td>
<p>the number of slices. default value is 10</p>
</td></tr>
<tr><td><code id="npsdr_+3A_lambda">lambda</code></td>
<td>
<p>hyperparameter for the loss function. default value is 1</p>
</td></tr>
<tr><td><code id="npsdr_+3A_b">b</code></td>
<td>
<p>number of basis functions for a kernel trick, floor(length(y)/3) is default</p>
</td></tr>
<tr><td><code id="npsdr_+3A_eps">eps</code></td>
<td>
<p>threshold for stopping iteration with respect to the magnitude of derivative, default value is 1.0e-4</p>
</td></tr>
<tr><td><code id="npsdr_+3A_max.iter">max.iter</code></td>
<td>
<p>maximum iteration number for the optimization process. default value is 30</p>
</td></tr>
<tr><td><code id="npsdr_+3A_eta">eta</code></td>
<td>
<p>learning rate for gradient descent method. default value is 0.1</p>
</td></tr>
<tr><td><code id="npsdr_+3A_mtype">mtype</code></td>
<td>
<p>type of margin, either &quot;m&quot; or &quot;r&quot; refer margin and residual, respectively (See, Table 1 in the pacakge manuscript). When one use user-defined loss function this argument should be specified. Default is &quot;m&quot;.</p>
</td></tr>
<tr><td><code id="npsdr_+3A_plot">plot</code></td>
<td>
<p>If <code>TRUE</code> then it produces scatter plots of <code class="reqn">Y</code> versus the first sufficient predictor. The default is FALSE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object with S3 class &quot;npsdr&quot;. Details are listed below.
</p>
<table role = "presentation">
<tr><td><code>evalues</code></td>
<td>
<p>Eigenvalues of the estimated working matrix M.</p>
</td></tr>
<tr><td><code>evectors</code></td>
<td>
<p>Eigenvectors of the estimated working matrix M, the first d leading eigenvectors consists
the basis of the central subspace.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jungmin Shin, <a href="mailto:jungminshin@korea.ac.kr">jungminshin@korea.ac.kr</a>, Seung Jun Shin, <a href="mailto:sjshin@korea.ac.kr">sjshin@korea.ac.kr</a>, Andreas Artemiou <a href="mailto:artemiou@uol.ac.cy">artemiou@uol.ac.cy</a>
</p>


<h3>References</h3>

<p>Artemiou, A. and Dong, Y. (2016)
<em>Sufficient dimension reduction via principal lq support vector machine,
Electronic Journal of Statistics 10: 783–805</em>.<br />
Artemiou, A., Dong, Y. and Shin, S. J. (2021)
<em>Real-time sufficient dimension reduction through principal least
squares support vector machines, Pattern Recognition 112: 107768</em>.<br />
Kim, B. and Shin, S. J. (2019)
<em>Principal weighted logistic regression for sufficient dimension
reduction in binary classification, Journal of the Korean Statistical Society 48(2): 194–206</em>.<br />
Li, B., Artemiou, A. and Li, L. (2011)
<em>Principal support vector machines for linear and
nonlinear sufficient dimension reduction, Annals of Statistics 39(6): 3182–3210</em>.<br />
Soale, A.-N. and Dong, Y. (2022)
<em>On sufficient dimension reduction via principal asymmetric
least squares, Journal of Nonparametric Statistics 34(1): 77–94</em>.<br />
Wang, C., Shin, S. J. and Wu, Y. (2018)
<em>Principal quantile regression for sufficient dimension
reduction with heteroscedasticity, Electronic Journal of Statistics 12(2): 2114–2140</em>.<br />
Shin, S. J., Wu, Y., Zhang, H. H. and Liu, Y. (2017)
<em>Principal weighted support vector machines for sufficient dimension reduction in
binary classification, Biometrika 104(1): 67–81</em>. <br />
Li, L. (2007)
<em>Sparse sufficient dimension reduction, Biometrika 94(3): 603–613</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npsdr_x">npsdr_x</a></code>, <code><a href="#topic+psdr">psdr</a></code>, <code><a href="#topic+rtpsdr">rtpsdr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1)
n &lt;- 200;
p &lt;- 5;
x &lt;- matrix(rnorm(n*p, 0, 2), n, p)
y &lt;- 0.5*sqrt((x[,1]^2+x[,2]^2))*(log(x[,1]^2+x[,2]^2))+ 0.2*rnorm(n)
obj_kernel &lt;- npsdr(x, y, plot=FALSE)
print(obj_kernel)
plot(obj_kernel)

</code></pre>

<hr>
<h2 id='npsdr_x'>Reconstruct the estimated sufficient predictors for a given data matrix</h2><span id='topic+npsdr_x'></span>

<h3>Description</h3>

<p>Returning the estimated sufficient predictors <code class="reqn">\hat{\phi}(\mathbf{x})</code> for a given <code class="reqn">\mathbf{x}</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npsdr_x(object, newdata, d = 2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="npsdr_x_+3A_object">object</code></td>
<td>
<p>The object from function <code>npsdr</code></p>
</td></tr>
<tr><td><code id="npsdr_x_+3A_newdata">newdata</code></td>
<td>
<p>new data <code class="reqn">\mathbf{X}</code></p>
</td></tr>
<tr><td><code id="npsdr_x_+3A_d">d</code></td>
<td>
<p>structural dimensionality. d=2 is default.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the value of the estimated nonlinear mapping <code class="reqn">\phi(\cdot)</code> is applied to
newdata <code class="reqn">X</code> with dimension d is returned.
</p>


<h3>Author(s)</h3>

<p>Jungmin Shin, <a href="mailto:jungminshin@korea.ac.kr">jungminshin@korea.ac.kr</a>, Seung Jun Shin, <a href="mailto:sjshin@korea.ac.kr">sjshin@korea.ac.kr</a>, Andreas Artemiou <a href="mailto:artemiou@uol.ac.cy">artemiou@uol.ac.cy</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npsdr">npsdr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1)
n &lt;- 200; n.new &lt;- 300
p &lt;- 5;
h &lt;- 20;
x &lt;- matrix(rnorm(n*p, 0, 2), n, p)
y &lt;- 0.5*sqrt((x[,1]^2+x[,2]^2))*(log(x[,1]^2+x[,2]^2))+ 0.2*rnorm(n)
new.x &lt;- matrix(rnorm(n.new*p, 0, 2), n.new, p)
obj_kernel &lt;- npsdr(x, y)
npsdr_x(object=obj_kernel, newdata=new.x)

</code></pre>

<hr>
<h2 id='plot.npsdr'>Scatter plot with sufficient predictors from npsdr() function</h2><span id='topic+plot.npsdr'></span>

<h3>Description</h3>

<p>Scatter plot with sufficient predictors from npsdr() function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'npsdr'
plot(x, ..., d = 1, lowess = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.npsdr_+3A_x">x</code></td>
<td>
<p>object from the function <code>npsdr()</code></p>
</td></tr>
<tr><td><code id="plot.npsdr_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to generic <code>plot</code> function.</p>
</td></tr>
<tr><td><code id="plot.npsdr_+3A_d">d</code></td>
<td>
<p>number of sufficient predictors. Default is 1.</p>
</td></tr>
<tr><td><code id="plot.npsdr_+3A_lowess">lowess</code></td>
<td>
<p>draw a lowess curve. Default is TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A scatter plot with sufficient predictors.
</p>


<h3>Author(s)</h3>

<p>Jungmin Shin, <a href="mailto:jungminshin@korea.ac.kr">jungminshin@korea.ac.kr</a>, Seung Jun Shin, <a href="mailto:sjshin@korea.ac.kr">sjshin@korea.ac.kr</a>, Andreas Artemiou <a href="mailto:artemiou@uol.ac.cy">artemiou@uol.ac.cy</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npsdr_x">npsdr_x</a></code>, <code><a href="#topic+npsdr">npsdr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1)
n &lt;- 200;
p &lt;- 5;
x &lt;- matrix(rnorm(n*p, 0, 2), n, p)
y &lt;-  x[,1]/(0.5 + (x[,2] + 1)^2) + 0.2*rnorm(n)
obj_kernel &lt;- npsdr(x, y, plot=FALSE)
plot(obj_kernel)

</code></pre>

<hr>
<h2 id='plot.psdr'>Scatter plot with sufficient predictors from psdr() function</h2><span id='topic+plot.psdr'></span>

<h3>Description</h3>

<p>Scatter plot with sufficient predictors from psdr() function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'psdr'
plot(x, ..., d = 1, lowess = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.psdr_+3A_x">x</code></td>
<td>
<p>object from the function <code>psdr()</code></p>
</td></tr>
<tr><td><code id="plot.psdr_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to generic <code>plot</code> function.</p>
</td></tr>
<tr><td><code id="plot.psdr_+3A_d">d</code></td>
<td>
<p>number of sufficient predictors. Default is 1.</p>
</td></tr>
<tr><td><code id="plot.psdr_+3A_lowess">lowess</code></td>
<td>
<p>draw a locally weighted scatterplot smoothing curve. Default is TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A scatter plot with sufficient predictors.
</p>


<h3>Author(s)</h3>

<p>Jungmin Shin, <a href="mailto:jungminshin@korea.ac.kr">jungminshin@korea.ac.kr</a>, Seung Jun Shin, <a href="mailto:sjshin@korea.ac.kr">sjshin@korea.ac.kr</a>, Andreas Artemiou <a href="mailto:artemiou@uol.ac.cy">artemiou@uol.ac.cy</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+psdr_bic">psdr_bic</a></code>, <code><a href="#topic+psdr">psdr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1)
n &lt;- 200; p &lt;- 5;
x &lt;- matrix(rnorm(n*p, 0, 2), n, p)
y &lt;-  x[,1]/(0.5 + (x[,2] + 1)^2) + 0.2*rnorm(n)
obj &lt;- psdr(x, y)
plot(obj, d=2, lowess=TRUE)

</code></pre>

<hr>
<h2 id='psdr'>Unified linear principal sufficient dimension reduction methods</h2><span id='topic+psdr'></span>

<h3>Description</h3>

<p>A function for a linear principal sufficient dimension reduction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>psdr(
  x,
  y,
  loss = "svm",
  h = 10,
  lambda = 1,
  eps = 1e-05,
  max.iter = 100,
  eta = 0.1,
  mtype = "m",
  plot = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="psdr_+3A_x">x</code></td>
<td>
<p>input matrix, of dimension <code>nobs</code> x <code>nvars</code>; each row is an observation vector. Requirement: <code>nvars</code>&gt;1; in other words, <code>x</code> should have 2 or more columns.</p>
</td></tr>
<tr><td><code id="psdr_+3A_y">y</code></td>
<td>
<p>response variable, either can be continuous variable or (+1,-1) coded binary response vector.</p>
</td></tr>
<tr><td><code id="psdr_+3A_loss">loss</code></td>
<td>
<p>pre-specified loss functions belongs to <code>"svm", "logit", "l2svm", "wsvm", "qr", "asls", "wlogit", "wl2svm", "lssvm", "wlssvm"</code>, and user-defined loss function object also can be used formed by inside double (or single) quotation mark. Default is 'svm'.</p>
</td></tr>
<tr><td><code id="psdr_+3A_h">h</code></td>
<td>
<p>the number of slices and probabilities equally spaced in <code class="reqn">(0,1)</code>. Default value is 10.</p>
</td></tr>
<tr><td><code id="psdr_+3A_lambda">lambda</code></td>
<td>
<p>the cost parameter for the svm loss function. The default value is 1.</p>
</td></tr>
<tr><td><code id="psdr_+3A_eps">eps</code></td>
<td>
<p>the threshold for stopping iteration with respect to the magnitude of the change of the derivative. The default value is 1.0e-5.</p>
</td></tr>
<tr><td><code id="psdr_+3A_max.iter">max.iter</code></td>
<td>
<p>maximum iteration number for the optimization process. default value is 100.</p>
</td></tr>
<tr><td><code id="psdr_+3A_eta">eta</code></td>
<td>
<p>learning rate for the gradient descent algorithm. The default value is 0.1.</p>
</td></tr>
<tr><td><code id="psdr_+3A_mtype">mtype</code></td>
<td>
<p>a margin type, which is either margin (&quot;m&quot;) or residual (&quot;r&quot;) (See, Table 1 in the manuscript). Only need when user-defined loss is used. Default is &quot;m&quot;.</p>
</td></tr>
<tr><td><code id="psdr_+3A_plot">plot</code></td>
<td>
<p>If <code>TRUE</code> then it produces scatter plots of <code class="reqn">Y</code> versus <code class="reqn">\hat{B^{\top}}_{j}\mathbf{X}</code>. <code class="reqn">j</code> can be specified by the user with <code class="reqn">j=1</code> as a default. The default is FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Two examples of the usage of user-defined losses are presented below (<code>u</code> represents a margin):
</p>
<p><code>mylogit &lt;- function(u, ...) log(1+exp(-u))</code>,
</p>
<p><code>myls &lt;- function(u ...) u^2</code>.
</p>
<p>Argument <code>u</code> is a function variable  (any character is possible) and the argument <code>mtype</code> for <code>psdr()</code> determines a type of a margin, either (<code>type="m"</code>) or (<code>type="r"</code>) method. <code>type="m"</code> is a default.
Users have to change <code>type="r"</code>, when applying residual type loss.
Any additional parameters of the loss can be specified via <code>...</code> argument.
</p>


<h3>Value</h3>

<p>An object with S3 class &quot;psdr&quot;. Details are listed below.
</p>
<table role = "presentation">
<tr><td><code>Mn</code></td>
<td>
<p>The estimated working matrix, which is obtained by the cumulative
outer product of the estimated parameters over the slices. It will not print out, unless it is called manually.</p>
</td></tr>
<tr><td><code>evalues</code></td>
<td>
<p>Eigenvalues of the working matrix <code class="reqn">Mn</code></p>
</td></tr>
<tr><td><code>evectors</code></td>
<td>
<p>Eigenvectors of the <code class="reqn">Mn</code>, the first leading <code class="reqn">d</code> eigenvectors consists
the basis of the central subspace</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jungmin Shin, <a href="mailto:jungminshin@korea.ac.kr">jungminshin@korea.ac.kr</a>, Seung Jun Shin, <a href="mailto:sjshin@korea.ac.kr">sjshin@korea.ac.kr</a>, Andreas Artemiou <a href="mailto:artemiou@uol.ac.cy">artemiou@uol.ac.cy</a>
</p>


<h3>References</h3>

<p>Artemiou, A. and Dong, Y. (2016)
<em>Sufficient dimension reduction via principal lq support vector machine,
Electronic Journal of Statistics 10: 783–805</em>.<br />
Artemiou, A., Dong, Y. and Shin, S. J. (2021)
<em>Real-time sufficient dimension reduction through principal least
squares support vector machines, Pattern Recognition 112: 107768</em>.<br />
Kim, B. and Shin, S. J. (2019)
<em>Principal weighted logistic regression for sufficient dimension
reduction in binary classification, Journal of the Korean Statistical Society 48(2): 194–206</em>.<br />
Li, B., Artemiou, A. and Li, L. (2011)
<em>Principal support vector machines for linear and
nonlinear sufficient dimension reduction, Annals of Statistics 39(6): 3182–3210</em>.<br />
Soale, A.-N. and Dong, Y. (2022)
<em>On sufficient dimension reduction via principal asymmetric
least squares, Journal of Nonparametric Statistics 34(1): 77–94</em>.<br />
Wang, C., Shin, S. J. and Wu, Y. (2018)
<em>Principal quantile regression for sufficient dimension
reduction with heteroscedasticity, Electronic Journal of Statistics 12(2): 2114–2140</em>.<br />
Shin, S. J., Wu, Y., Zhang, H. H. and Liu, Y. (2017)
<em>Principal weighted support vector machines for sufficient dimension reduction in
binary classification, Biometrika 104(1): 67–81</em>. <br />
Li, L. (2007)
<em>Sparse sufficient dimension reduction, Biometrika 94(3): 603–613</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+psdr_bic">psdr_bic</a></code>, <code><a href="#topic+rtpsdr">rtpsdr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## ----------------------------
## Linear PM
## ----------------------------
set.seed(1)
n &lt;- 200; p &lt;- 5;
x &lt;- matrix(rnorm(n*p, 0, 2), n, p)
y &lt;-  x[,1]/(0.5 + (x[,2] + 1)^2) + 0.2*rnorm(n)
y.tilde &lt;- sign(y)
obj &lt;- psdr(x, y)
print(obj)
plot(obj, d=2)

## ----------------------------
## Kernel PM
## ----------------------------
obj_wsvm &lt;- psdr(x, y.tilde, loss="wsvm")
plot(obj_wsvm)

## ----------------------------
## User-defined loss function
## ----------------------------
mylogistic &lt;- function(u) log(1+exp(-u))
psdr(x, y, loss="mylogistic")

</code></pre>

<hr>
<h2 id='psdr_bic'>Order estimation via BIC-type criterion</h2><span id='topic+psdr_bic'></span>

<h3>Description</h3>

<p>Estimation of a structural dimensionality. Choose the k which maximizes a BIC (Bayesian information criterion) value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>psdr_bic(obj, rho = 0.01, plot = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="psdr_bic_+3A_obj">obj</code></td>
<td>
<p>The psdr object</p>
</td></tr>
<tr><td><code id="psdr_bic_+3A_rho">rho</code></td>
<td>
<p>Parameter for BIC criterion. Default is 0.01.</p>
</td></tr>
<tr><td><code id="psdr_bic_+3A_plot">plot</code></td>
<td>
<p>Boolean. If TRUE, the plot of BIC values are depicted.</p>
</td></tr>
<tr><td><code id="psdr_bic_+3A_...">...</code></td>
<td>
<p>Additional arguments to be passed to generic <code>plot</code> function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Estimated BIC scores for determining the optimal structural dimension will be returned with plot.
</p>


<h3>Author(s)</h3>

<p>Jungmin Shin, <a href="mailto:jungminshin@korea.ac.kr">jungminshin@korea.ac.kr</a>, Seung Jun Shin, <a href="mailto:sjshin@korea.ac.kr">sjshin@korea.ac.kr</a>, Andreas Artemiou <a href="mailto:artemiou@uol.ac.cy">artemiou@uol.ac.cy</a>
</p>


<h3>References</h3>

<p>Li, B., Artemiou, A. and Li, L. (2011)
<em>Principal support vector machines for linear and
nonlinear sufficient dimension reduction, Annals of Statistics 39(6): 3182–3210</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+psdr">psdr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1234)
n &lt;- 200; p &lt;- 10;
x &lt;- matrix(rnorm(n*p, 0, 1), n, p)
y &lt;-  x[,1]/(0.5 + (x[,2] + 1)^2) + rnorm(n, 0, .2)
obj &lt;- psdr(x, y, loss="svm")
d.hat &lt;- psdr_bic(obj)
print(d.hat)


</code></pre>

<hr>
<h2 id='rtpsdr'>Real time sufficient dimension reduction through principal least squares SVM</h2><span id='topic+rtpsdr'></span>

<h3>Description</h3>

<p>In stream data, where we need to constantly update the estimation as new data are collected,
the use of all available data can create computational challenges even for computationally efficient algorithms.
Therefore it is important to develop real time SDR algorithms that work efficiently in the case that there are data streams.
After getting an initial estimator with the currently available data,
the basic idea of real-time method is to update the estimator efficiently as new data are collected.
This function realizes real time least squares SVM SDR method for a both regression and classification problem
It is efficient algorithms for either adding new data or removing old data are provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rtpsdr(x, y, obj = NULL, h = 10, lambda = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rtpsdr_+3A_x">x</code></td>
<td>
<p>x in new data</p>
</td></tr>
<tr><td><code id="rtpsdr_+3A_y">y</code></td>
<td>
<p>y in new data, y is continuous</p>
</td></tr>
<tr><td><code id="rtpsdr_+3A_obj">obj</code></td>
<td>
<p>the latest output object from the <code>rtpsdr</code></p>
</td></tr>
<tr><td><code id="rtpsdr_+3A_h">h</code></td>
<td>
<p>a number of slices. default is set to 10.</p>
</td></tr>
<tr><td><code id="rtpsdr_+3A_lambda">lambda</code></td>
<td>
<p>hyperparameter for the loss function. default is set to 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object with S3 class &quot;rtpsdr&quot;. Details are listed below.
</p>
<table role = "presentation">
<tr><td><code>x</code></td>
<td>
<p>input data matrix</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>iniput response vector</p>
</td></tr>
<tr><td><code>Mn</code></td>
<td>
<p>The estimated working matrix, which is obtained by the cumulative
outer product of the estimated parameters over H</p>
</td></tr>
<tr><td><code>evalues</code></td>
<td>
<p>Eigenvalues of the Mn</p>
</td></tr>
<tr><td><code>evectors</code></td>
<td>
<p>Eigenvectors of the Mn, the first d leading eigenvectors consists
the basis of the central subspace</p>
</td></tr>
<tr><td><code>N</code></td>
<td>
<p>total number of observation <code class="reqn">n_1 + n_2</code></p>
</td></tr>
<tr><td><code>Xbar</code></td>
<td>
<p>mean of total <code class="reqn">\mathbf{x}</code></p>
</td></tr>
<tr><td><code>r</code></td>
<td>
<p>updated estimated coefficients matrix</p>
</td></tr>
<tr><td><code>A</code></td>
<td>
<p>new A part for update. See Artemiou et. al., (2021)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jungmin Shin, <a href="mailto:jungminshin@korea.ac.kr">jungminshin@korea.ac.kr</a>, Seung Jun Shin, <a href="mailto:sjshin@korea.ac.kr">sjshin@korea.ac.kr</a>, Andreas Artemiou <a href="mailto:artemiou@uol.ac.cy">artemiou@uol.ac.cy</a>
</p>


<h3>References</h3>

<p>Artemiou, A. and Dong, Y. (2016)
<em>Sufficient dimension reduction via principal lq support vector machine,
Electronic Journal of Statistics 10: 783–805</em>.<br />
Artemiou, A., Dong, Y. and Shin, S. J. (2021)
<em>Real-time sufficient dimension reduction through principal least
squares support vector machines, Pattern Recognition 112: 107768</em>.<br />
Kim, B. and Shin, S. J. (2019)
<em>Principal weighted logistic regression for sufficient dimension
reduction in binary classification, Journal of the Korean Statistical Society 48(2): 194–206</em>.<br />
Li, B., Artemiou, A. and Li, L. (2011)
<em>Principal support vector machines for linear and
nonlinear sufficient dimension reduction, Annals of Statistics 39(6): 3182–3210</em>.<br />
Soale, A.-N. and Dong, Y. (2022)
<em>On sufficient dimension reduction via principal asymmetric
least squares, Journal of Nonparametric Statistics 34(1): 77–94</em>.<br />
Wang, C., Shin, S. J. and Wu, Y. (2018)
<em>Principal quantile regression for sufficient dimension
reduction with heteroscedasticity, Electronic Journal of Statistics 12(2): 2114–2140</em>.<br />
Shin, S. J., Wu, Y., Zhang, H. H. and Liu, Y. (2017)
<em>Principal weighted support vector machines for sufficient dimension reduction in
binary classification, Biometrika 104(1): 67–81</em>. <br />
Li, L. (2007)
<em>Sparse sufficient dimension reduction, Biometrika 94(3): 603–613</em>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+psdr">psdr</a></code>, <code><a href="#topic+npsdr">npsdr</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
p &lt;- 5
m &lt;- 500 # batch size
N &lt;- 10  # number of batches
obj &lt;- NULL
for (iter in 1:N){
 set.seed(iter)
 x &lt;- matrix(rnorm(m*p), m, p)
 y &lt;-  x[,1]/(0.5 + (x[,2] + 1)^2) + 0.2 * rnorm(m)
 obj &lt;- rtpsdr(x = x, y = y, obj=obj)
}
print(obj)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
