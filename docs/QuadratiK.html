<!DOCTYPE html><html lang="en-US"><head><title>Help for package QuadratiK</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {QuadratiK}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#QuadratiK-package'><p>Collection of Methods Constructed using the Kernel-Based Quadratic</p>
Distances</a></li>
<li><a href='#breast_cancer'><p>Breast Cancer Wisconsin (Diagnostic)</p></a></li>
<li><a href='#compare_qq'><p>QQ-plot of given two samples using ggplot2</p></a></li>
<li><a href='#compute_CV'><p>Compute the critical value for two-sample KBQD tests</p></a></li>
<li><a href='#compute_stats'><p>Compute and display some descriptive statistics for the two sample tests</p></a></li>
<li><a href='#cv_ksample'><p>Compute the critical value for the KBQD k-sample tests</p></a></li>
<li><a href='#DOF'><p>Degrees of freedom (DOF) for the Poisson kernel</p></a></li>
<li><a href='#DOF_norm'><p>Degrees of freedom (DOF) for the Normal kernel</p></a></li>
<li><a href='#dpkb'><p>The Poisson kernel-based Distribution (PKBD)</p></a></li>
<li><a href='#generate_SN'><p>Generate two samples data from skew-normal distributions</p></a></li>
<li><a href='#kb.test'><p>Kernel-based quadratic distance (KBQD) Goodness-of-Fit tests</p></a></li>
<li><a href='#kb.test-class'><p>An S4 class for kernel-based distance tests with normal kernel</p></a></li>
<li><a href='#normal_CV'><p>Compute the critical value for the KBQD tests for multivariate Normality</p></a></li>
<li><a href='#pk.test'><p>Poisson kernel-based quadratic distance test of Uniformity on the sphere</p></a></li>
<li><a href='#pk.test-class'><p>An S4 class for Poisson kernel-based quadratic distance tests.</p></a></li>
<li><a href='#pkbc'><p>Poisson kernel-based clustering on the sphere</p></a></li>
<li><a href='#pkbc_validation'><p>Validation of Poisson kernel-based clustering results</p></a></li>
<li><a href='#pkbc-class'><p>A S4 class for the clustering algorithm on the sphere based on</p>
Poisson kernel-based distributions.</a></li>
<li><a href='#plot.pkbc'><p>Plotting method for Poisson kernel-based clustering</p></a></li>
<li><a href='#poisson_CV'><p>Compute the critical value for the Poisson KBQD tests for Uniformity</p></a></li>
<li><a href='#predict.pkbc'><p>Cluster spherical observations using a mixture of Poisson kernel-based</p>
densities</a></li>
<li><a href='#sample_hypersphere'><p>Generate random sample from the hypersphere</p></a></li>
<li><a href='#select_h'><p>Select the value of the kernel tuning parameter</p></a></li>
<li><a href='#stats_clusters'><p>Descriptive statistics for the clusters identified by the Poisson</p>
kernel-based clustering.</a></li>
<li><a href='#summary.kb.test'><p>Summarizing kernel-based quadratic distance results</p></a></li>
<li><a href='#summary.pk.test'><p>Summarizing kernel-based quadratic distance results</p></a></li>
<li><a href='#summary.pkbc'><p>Summarizing PKBD mixture Fits</p></a></li>
<li><a href='#var_k'><p>Exact variance of k-sample test</p></a></li>
<li><a href='#var_norm'><p>Exact variance of normality test</p></a></li>
<li><a href='#var_two'><p>Exact variance of two-sample test</p></a></li>
<li><a href='#wine'><p>Wine data set</p></a></li>
<li><a href='#wireless'><p>Wireless Indoor Localization</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Collection of Methods Constructed using Kernel-Based Quadratic
Distances</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.3</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Giovanni Saraceno &lt;giovanni.saraceno@unipd.it&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>It includes test for multivariate normality, test for uniformity on the d-dimensional 
    Sphere, non-parametric two- and k-sample tests, random generation of points from the Poisson 
    kernel-based density and clustering algorithm for spherical data. For more information see
    Saraceno G., Markatou M., Mukhopadhyay R. and Golzy M. (2024)
    &lt;<a href="https://doi.org/10.48550%2FarXiv.2402.02290">doi:10.48550/arXiv.2402.02290</a>&gt;
    Markatou, M. and Saraceno, G. (2024) &lt;<a href="https://doi.org/10.48550%2FarXiv.2407.16374">doi:10.48550/arXiv.2407.16374</a>&gt;, 
    Ding, Y., Markatou, M. and Saraceno, G. (2023) &lt;<a href="https://doi.org/10.5705%2Fss.202022.0347">doi:10.5705/ss.202022.0347</a>&gt;, 
    and Golzy, M. and Markatou, M. (2020) &lt;<a href="https://doi.org/10.1080%2F10618600.2020.1740713">doi:10.1080/10618600.2020.1740713</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://CRAN.R-project.org/package=QuadratiK">https://CRAN.R-project.org/package=QuadratiK</a>,
<a href="https://github.com/ropensci/QuadratiK/">https://github.com/ropensci/QuadratiK/</a>,
<a href="https://docs.ropensci.org/QuadratiK/">https://docs.ropensci.org/QuadratiK/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ropensci/QuadratiK/issues">https://github.com/ropensci/QuadratiK/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>parallel, doParallel, foreach, ggplot2, ggpubr, methods,
moments, mvtnorm, Rcpp, RcppEigen, rlecuyer, sn, stats, rrcov,
scatterplot3d</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, roxygen2, testthat (&ge; 3.0.0), rgl,
sphunif, circular, cluster, clusterRepro, mclust, Tinflex,
movMF</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppEigen</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-02-04 14:23:35 UTC; sarac</td>
</tr>
<tr>
<td>Author:</td>
<td>Giovanni Saraceno [aut, cre] (ORCID 000-0002-1753-2367),
  Marianthi Markatou [aut],
  Raktim Mukhopadhyay [aut],
  Mojgan Golzy [aut],
  Hingee Kassel [rev],
  Emi Tanaka [rev]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-02-04 21:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='QuadratiK-package'>Collection of Methods Constructed using the Kernel-Based Quadratic
Distances</h2><span id='topic+QuadratiK'></span><span id='topic+QuadratiK-package'></span>

<h3>Description</h3>

<p>Collection of Methods Constructed using the Kernel-Based Quadratic Distances
</p>
<p><code>QuadratiK</code> provides the first implementation, in R and Python, of a
comprehensive set of goodness-of-fit tests and a clustering technique for
<code class="reqn">d</code>-dimensional spherical data <code class="reqn">d \ge 2</code> using kernel-based quadratic
distances. It includes:
</p>

<ul>
<li> <p><strong>Goodness-of-Fit Tests</strong>: The software implements one, two, and
<em>k</em>-sample tests for goodness of fit, offering an efficient and
mathematically sound way to assess the fit of probability distributions.
Our tests are
particularly useful for large, high dimensional data sets where the
assessment of fit of probability models is of interest. Specifically, we
offer tests for normality, as well as two- and <em>k</em>-sample tests, where
testing equality of two or more distributions is of interest, that is
<code class="reqn">H_0: F_1 = F_2</code> and <code class="reqn">H_0: F_1 = \ldots = F_k</code> respectively.
The proposed tests perform well in terms of level and power for contiguous
alternatives, heavy tailed distributions and in higher dimensions. <br />
Expanded capabilities include supporting tests for uniformity on the
<em>d</em>-dimensional Sphere based on the Poisson kernel, exhibiting excellent
results especially in the case of multimodal distributions.
</p>
</li>
<li> <p><strong>Poisson kernel-based distribution (PKBD)</strong>: the package offers
functions for computing the density value and for generating random samples
from a PKBD. The Poisson kernel-based densities are based on the normalized
Poisson kernel and are defined on the <code class="reqn">d</code>-dimensional unit sphere.
Given a vector <code class="reqn">\mu \in \mathcal{S}^{d-1}</code>, and a parameter <code class="reqn">\rho</code>
such that <code class="reqn">0 &lt; \rho &lt; 1</code>, the probability density function of a
<code class="reqn">d</code>-variate Poisson kernel-based density is defined by:
</p>
<p style="text-align: center;"><code class="reqn">f(\mathbf{x}|\rho, \mathbf{\mu}) = \frac{1-\rho^2}{\omega_d 
||\mathbf{x} - \rho \mathbf{\mu}||^d},</code>
</p>

<p>where <code class="reqn">\mu</code> is a vector orienting the center of the distribution,
<code class="reqn">\rho</code> is a parameter to control the concentration of the distribution
around the vector <code class="reqn">\mu</code> and it is related to the variance of the
distribution. Furthermore, <code class="reqn">\omega_d = 2\pi^{d/2} [\Gamma(d/2)]^{-1}</code>
is the surface area of the unit sphere in <code class="reqn">\mathbb{R}^d</code>
(see Golzy and Markatou, 2020).
</p>
</li>
<li> <p><strong>Clustering Algorithm for Spherical Data</strong>: the package incorporates a
unique clustering algorithm specifically tailored for <code class="reqn">d</code>-dimensional
spherical data and it is especially useful in the presence of noise in the
data and the presence of non-negligible overlap between clusters. This
algorithm leverages a mixture of Poisson kernel-based densities on the
Sphere, enabling effective clustering of spherical data or data that has
been spherically transformed.
</p>
</li>
<li> <p><strong>Additional Features</strong>: Alongside these functionalities, the software
includes additional graphical functions, aiding users in validating and
representing the cluster results as well as enhancing the interpretability
and usability of the analysis.
</p>
</li></ul>

<p>For an introduction to <code>QuadratiK</code> see the vignette
<a href="../doc/Introduction.html">Introduction to the QuadratiK Package</a>.
</p>


<h3>Details</h3>

<p>The work has been supported by Kaleida Health Foundation and the
National Science Foundation.
</p>


<h3>Note</h3>

<p>The <code>QuadratiK</code> package is also available in Python on PyPI
<a href="https://pypi.org/project/QuadratiK/">https://pypi.org/project/QuadratiK/</a> and also as a Dashboard application.
Usage instruction for the Dashboard can be found at
&lt;https://quadratik.readthedocs.io/en/latest/user_guide/
dashboard_application_usage.html&gt;.
</p>


<h3>Author(s)</h3>

<p>Giovanni Saraceno, Marianthi Markatou, Raktim Mukhopadhyay, Mojgan Golzy
</p>
<p>Maintainer: Giovanni Saraceno <a href="mailto:giovanni.saracen@unipd.it">giovanni.saracen@unipd.it</a>
</p>


<h3>References</h3>

<p>Saraceno, G., Markatou, M., Mukhopadhyay, R. and Golzy, M.
(2024). Goodness-of-Fit and Clustering of Spherical Data: the QuadratiK
package in R and Python. arXiv preprint arXiv:2402.02290.
</p>
<p>Ding, Y., Markatou, M. and Saraceno, G. (2023). “Poisson
Kernel-Based Tests for Uniformity on the d-Dimensional Sphere.”
Statistica Sinica. doi: doi:10.5705/ss.202022.0347.
</p>
<p>Golzy, M. and Markatou, M. (2020) Poisson Kernel-Based Clustering on
the Sphere: Convergence Properties, Identifiability, and a Method of
Sampling, Journal of Computational and Graphical Statistics, 29:4, 758-770,
DOI: 10.1080/10618600.2020.1740713.
</p>
<p>Markatou, M. and Saraceno, G. (2024). “A Unified Framework for
Multivariate Two- and k-Sample Kernel-based Quadratic Distance
Goodness-of-Fit Tests.” <br />
https://doi.org/10.48550/arXiv.2407.16374
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://CRAN.R-project.org/package=QuadratiK">https://CRAN.R-project.org/package=QuadratiK</a>
</p>
</li>
<li> <p><a href="https://github.com/ropensci/QuadratiK/">https://github.com/ropensci/QuadratiK/</a>
</p>
</li>
<li> <p><a href="https://docs.ropensci.org/QuadratiK/">https://docs.ropensci.org/QuadratiK/</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/ropensci/QuadratiK/issues">https://github.com/ropensci/QuadratiK/issues</a>
</p>
</li></ul>


<hr>
<h2 id='breast_cancer'>Breast Cancer Wisconsin (Diagnostic)</h2><span id='topic+breast_cancer'></span>

<h3>Description</h3>

<p>The <code>breast_cancer</code> Wisconsin data has 569 rows and 31 columns. The
first 30 variables report the features that are computed from a digitized
image of a fine needle aspirate (FNA) of a breast mass. They describe
characteristics of the cell nuclei present in the image. The last column
indicates the class labels (Benign = 0 or Malignant = 1).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>breast_cancer
</code></pre>


<h3>Format</h3>

<p>A data frame of 569 observations and 31 variables.
</p>


<h3>Source</h3>

<p>Wolberg, W., Mangasarian, O., Street, N., &amp; Street, W. (1993).
Breast Cancer Wisconsin (Diagnostic).
UCI Machine Learning Repository.
https://doi.org/10.24432/C5DW2B.
</p>


<h3>References</h3>

<p>Street, W. N., Wolberg, W. H., &amp; Mangasarian, O. L. (1993, July). Nuclear
feature extraction for breast tumor diagnosis. In Biomedical image processing
and biomedical visualization (Vol. 1905, pp. 861-870). SPIE.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(breast_cancer)
summary(breast_cancer)

</code></pre>

<hr>
<h2 id='compare_qq'>QQ-plot of given two samples using ggplot2</h2><span id='topic+compare_qq'></span>

<h3>Description</h3>

<p>QQ-plot of given two samples using ggplot2
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compare_qq(sample1, sample2, main_title)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="compare_qq_+3A_sample1">sample1</code></td>
<td>
<p>matrix of observations in sample 1</p>
</td></tr>
<tr><td><code id="compare_qq_+3A_sample2">sample2</code></td>
<td>
<p>matrix of observations in sample 2</p>
</td></tr>
<tr><td><code id="compare_qq_+3A_main_title">main_title</code></td>
<td>
<p>title of the generated plot</p>
</td></tr>
</table>


<h3>Value</h3>

<p>QQ-plot of given samples
</p>

<hr>
<h2 id='compute_CV'>Compute the critical value for two-sample KBQD tests</h2><span id='topic+compute_CV'></span>

<h3>Description</h3>

<p>This function computes the critical value for two-sample kernel tests with
centered Gaussian kernel
using one of three methods: bootstrap, permutation, or subsampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_CV(
  B,
  Quantile,
  data_pool,
  size_x,
  size_y,
  h,
  method,
  b = 1,
  compute_variance
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="compute_CV_+3A_b">B</code></td>
<td>
<p>the number of bootstrap/permutation/subsampling samples to generate.</p>
</td></tr>
<tr><td><code id="compute_CV_+3A_quantile">Quantile</code></td>
<td>
<p>the quantile of the bootstrap/permutation/subsampling
distribution to use as the critical value.</p>
</td></tr>
<tr><td><code id="compute_CV_+3A_data_pool">data_pool</code></td>
<td>
<p>a matrix containing the data to be used in the test.</p>
</td></tr>
<tr><td><code id="compute_CV_+3A_size_x">size_x</code></td>
<td>
<p>the number of rows in the <code>data_pool</code> matrix
corresponding to group X.</p>
</td></tr>
<tr><td><code id="compute_CV_+3A_size_y">size_y</code></td>
<td>
<p>the number of rows in the <code>data_pool</code> matrix
corresponding to group Y.</p>
</td></tr>
<tr><td><code id="compute_CV_+3A_h">h</code></td>
<td>
<p>the tuning parameter for the kernel test.</p>
</td></tr>
<tr><td><code id="compute_CV_+3A_method">method</code></td>
<td>
<p>the method to use for computing the critical value
(one of &quot;bootstrap&quot;, &quot;permutation&quot;, or &quot;subsampling&quot;).</p>
</td></tr>
<tr><td><code id="compute_CV_+3A_b">b</code></td>
<td>
<p>the subsampling block size (only used if <code>method</code> is
&quot;subsampling&quot;).</p>
</td></tr>
<tr><td><code id="compute_CV_+3A_compute_variance">compute_variance</code></td>
<td>
<p>indicates if the nonparametric variance is computed.
Default is TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the critical value for the specified method and significance level.
</p>


<h3>References</h3>

<p>Markatou Marianthi &amp; Saraceno Giovanni (2024). “A Unified Framework for
Multivariate Two- and k-Sample Kernel-based Quadratic Distance
Goodness-of-Fit Tests.”
https://doi.org/10.48550/arXiv.2407.16374
</p>

<hr>
<h2 id='compute_stats'>Compute and display some descriptive statistics for the two sample tests</h2><span id='topic+compute_stats'></span>

<h3>Description</h3>

<p>Compute and display some descriptive statistics for the two sample tests
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_stats(var1, var2, var_name, eps = 3)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="compute_stats_+3A_var1">var1</code></td>
<td>
<p>vector of observations of a given variable from sample 1</p>
</td></tr>
<tr><td><code id="compute_stats_+3A_var2">var2</code></td>
<td>
<p>vector of observations of a given variable from sample 2</p>
</td></tr>
<tr><td><code id="compute_stats_+3A_var_name">var_name</code></td>
<td>
<p>Name of the variable displayed</p>
</td></tr>
<tr><td><code id="compute_stats_+3A_eps">eps</code></td>
<td>
<p>precision of displayed statistics</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Computed statistics with a plot
</p>

<hr>
<h2 id='cv_ksample'>Compute the critical value for the KBQD k-sample tests</h2><span id='topic+cv_ksample'></span>

<h3>Description</h3>

<p>This function computes the empirical critical value for the k-sample KBQD
tests using the centered Gaussian kernel, with bootstrap, permutation, or
subsampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv_ksample(
  x,
  y,
  h,
  B = 150,
  b = 0.9,
  Quantile = 0.95,
  method = "subsampling",
  compute_variance = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cv_ksample_+3A_x">x</code></td>
<td>
<p>matrix containing the observations to be used in the k-sample test</p>
</td></tr>
<tr><td><code id="cv_ksample_+3A_y">y</code></td>
<td>
<p>vector indicating the sample for each observation</p>
</td></tr>
<tr><td><code id="cv_ksample_+3A_h">h</code></td>
<td>
<p>the tuning parameter for the test using the Gaussian kernel</p>
</td></tr>
<tr><td><code id="cv_ksample_+3A_b">B</code></td>
<td>
<p>the number of bootstrap/permutation/subsampling samples to generate</p>
</td></tr>
<tr><td><code id="cv_ksample_+3A_b">b</code></td>
<td>
<p>the subsampling block size (only used if <code>method</code> is
&quot;subsampling&quot;)</p>
</td></tr>
<tr><td><code id="cv_ksample_+3A_quantile">Quantile</code></td>
<td>
<p>the quantile of the bootstrap/permutation/subsampling
distribution to use as the critical value</p>
</td></tr>
<tr><td><code id="cv_ksample_+3A_method">method</code></td>
<td>
<p>the method to use for computing the critical value
(one of &quot;bootstrap&quot;, &quot;permutation&quot;)</p>
</td></tr>
<tr><td><code id="cv_ksample_+3A_compute_variance">compute_variance</code></td>
<td>
<p>indicates if the nonparametric variance is computed.
Default is TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of two critical values corresponding to different
formulation of the k-sample test statistics.
</p>

<hr>
<h2 id='DOF'>Degrees of freedom (DOF) for the Poisson kernel</h2><span id='topic+DOF'></span>

<h3>Description</h3>

<p>Compute the Degrees of Freedom (DOF) of the Poisson Kernel given the
dimension d and concentration parameter rho
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DOF(d, rho)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="DOF_+3A_d">d</code></td>
<td>
<p>the number of dimensions</p>
</td></tr>
<tr><td><code id="DOF_+3A_rho">rho</code></td>
<td>
<p>concentration parameter</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing the DOF and the coefficient c of the asymptotic
distribution
</p>

<hr>
<h2 id='DOF_norm'>Degrees of freedom (DOF) for the Normal kernel</h2><span id='topic+DOF_norm'></span>

<h3>Description</h3>

<p>Compute the Degrees of Freedom (DOF) of the normal Kernel centered with
respect to the standard normal distribution, given the dimension d and the
bandwidth parameter h.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DOF_norm(Sigma_h, V)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="DOF_norm_+3A_sigma_h">Sigma_h</code></td>
<td>
<p>covariance matrix of the gaussian kernel</p>
</td></tr>
<tr><td><code id="DOF_norm_+3A_v">V</code></td>
<td>
<p>Covariance matrix of the tested distribution G</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing the DOF and the coefficient c of the asymptotic
distribution
</p>

<hr>
<h2 id='dpkb'>The Poisson kernel-based Distribution (PKBD)</h2><span id='topic+dpkb'></span><span id='topic+rpkb'></span>

<h3>Description</h3>

<p>The Poisson kernel-based densities are based on the normalized Poisson kernel
and are defined on the <code class="reqn">(d-1)</code>-dimensional unit sphere. Given a vector
<code class="reqn">\mathbf{\mu} \in \mathcal{S}^{d-1}</code>, where <code class="reqn">\mathcal{S}^{d-1}= 
\{x \in \mathbb{R}^d : ||x|| = 1\}</code>, and a parameter <code class="reqn">\rho</code> such that
<code class="reqn">0 &lt; \rho &lt; 1</code>, the probability density function of a <code class="reqn">d</code>-variate
Poisson kernel-based density is defined by:
</p>
<p style="text-align: center;"><code class="reqn">f(\mathbf{x}|\rho, \mathbf{\mu}) = \frac{1-\rho^2}{\omega_d 
||\mathbf{x} - \rho \mathbf{\mu}||^d},</code>
</p>

<p>where <code class="reqn">\mu</code> is a vector orienting the center of the distribution,
<code class="reqn">\rho</code> is a parameter to control the concentration of the distribution
around the vector <code class="reqn">\mu</code> and it is related to the variance of the
distribution. Recall that, for <code class="reqn">x = (x_1, \ldots, x_d) \in \mathbb{R}^d</code>,
<code class="reqn">||x|| = \sqrt{x_1^2 + \ldots + x_d^2}</code>. Furthermore, <code class="reqn">\omega_d =
2\pi^{d/2} [\Gamma(d/2)]^{-1}</code> is the surface area of the unit sphere in
<code class="reqn">\mathbb{R}^d</code> (see Golzy and Markatou, 2020). When <code class="reqn">\rho \to 0</code>,
the Poisson kernel-based density tends to the uniform density on the sphere.
Connections of the PKBDs to other distributions are discussed in detail in
Golzy and Markatou (2020). Here we note that when <code class="reqn">d=2</code>, PKBDs reduce to
the wrapped Cauchy distribution. Additionally, with precise choice of the
parameters <code class="reqn">\rho</code> and <code class="reqn">\mu</code> the two-dimensional PKBD becomes a
two-dimensional projected normal distribution. However, the connection with
the <code class="reqn">d</code>-dimensional projected normal distributions does not carry beyond
<code class="reqn">d=2</code>.
Golzy and Markatou (2020) proposed an acceptance-rejection method for
simulating data from a PKBD using von Mises-Fisher envelopes (<code>rejvmf</code>
method). Furthermore Sablica, Hornik and Leydold (2023) proposed new ways for
simulating from the PKBD, using angular central Gaussian envelopes
(<code>rejacg</code>) or using the projected Saw distributions (<code>rejpsaw</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dpkb(x, mu, rho, logdens = FALSE)

rpkb(
  n,
  mu,
  rho,
  method = "rejacg",
  tol.eps = .Machine$double.eps^0.25,
  max.iter = 1000
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dpkb_+3A_x">x</code></td>
<td>
<p><code class="reqn">n \times d</code>-matrix (or data.frame) of <code class="reqn">n</code> data point on the
sphere <code class="reqn">\mathcal{S}^{d-1}</code>, with <code class="reqn">d \ge 2</code>.</p>
</td></tr>
<tr><td><code id="dpkb_+3A_mu">mu</code></td>
<td>
<p>location vector parameter with length indicating the dimension of
generated points.</p>
</td></tr>
<tr><td><code id="dpkb_+3A_rho">rho</code></td>
<td>
<p>Concentration parameter, with <code class="reqn">0 \le</code> <code>rho</code> <code class="reqn">&lt; 1</code>.</p>
</td></tr>
<tr><td><code id="dpkb_+3A_logdens">logdens</code></td>
<td>
<p>Logical; if 'TRUE', densities are returned in logarithmic
scale.</p>
</td></tr>
<tr><td><code id="dpkb_+3A_n">n</code></td>
<td>
<p>number of observations.</p>
</td></tr>
<tr><td><code id="dpkb_+3A_method">method</code></td>
<td>
<p>string that indicates the method used for sampling
observations. The available methods are
</p>

<ul>
<li> <p><code>'rejvmf'</code> acceptance-rejection algorithm using
von Mises-Fisher envelopes (Algorithm in Table 2 of
Golzy and Markatou 2020);
</p>
</li>
<li> <p><code>'rejacg'</code> using angular central Gaussian envelopes
(Algorithm in Table 1 of Sablica et al. 2023);
</p>
</li>
<li> <p><code>'rejpsaw'</code> using projected Saw distributions
(Algorithm in Table 2 of Sablica et al. 2023).
</p>
</li></ul>
</td></tr>
<tr><td><code id="dpkb_+3A_tol.eps">tol.eps</code></td>
<td>
<p>the desired accuracy of convergence tolerance
(for 'rejacg' method).</p>
</td></tr>
<tr><td><code id="dpkb_+3A_max.iter">max.iter</code></td>
<td>
<p>the maximum number of iterations (for 'rejacg' method).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function <code>dpkb()</code> computes the density value for a given point
<code>x</code> from the Poisson kernel-based distribution with mean direction
vector <code>mu</code> and concentration parameter <code>rho</code>.
</p>
<p>The number of observations generated is determined by <code>n</code> for
<code>rpkb()</code>. This function returns the <code class="reqn">(n \times d)</code>-matrix of
generated <code class="reqn">n</code> observations on <code class="reqn">\mathcal{S}^{(d-1)}</code>.
</p>
<p>A limitation of the <code>rejvmf</code> is that the method does not ensure the
computational feasibility of the sampler for <code class="reqn">\rho</code> approaching 1.
</p>
<p>If the chosen method is 'rejacg', the function <code>uniroot</code>, from the
<code>stat</code> package, is used to estimate the beta parameter. In this case,
the complete results are provided as output.
</p>


<h3>Value</h3>

<p><code>dpkb</code> gives the density value;
<code>rpkb</code> generates random observations from the PKBD.
</p>


<h3>Note</h3>

<p>If the required packages (<code>movMF</code> for <code>rejvmf</code> method, and
<code>Tinflex</code> for <code>rejpsaw</code>) are not installed, the function will display a
message asking the user to install the missing package(s).
</p>


<h3>References</h3>

<p>Golzy, M. and Markatou, M. (2020) Poisson Kernel-Based Clustering on the
Sphere: Convergence Properties, Identifiability, and a Method of Sampling,
Journal of Computational and Graphical Statistics, 29:4, 758-770,
DOI: 10.1080/10618600.2020.1740713.
</p>
<p>Sablica L., Hornik K. and Leydold J. (2023) &quot;Efficient sampling from the PKBD
distribution&quot;, Electronic Journal of Statistics, 17(2), 2180-2209.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate some data from pkbd density
pkbd_dat &lt;- rpkb(10, c(0.5, 0), 0.5)

# Calculate the PKBD density values
dens_val &lt;- dpkb(pkbd_dat, c(0.5, 0.5), 0.5)

</code></pre>

<hr>
<h2 id='generate_SN'>Generate two samples data from skew-normal distributions</h2><span id='topic+generate_SN'></span>

<h3>Description</h3>

<p>This function generates data from skew-normal distributions with the
specified parameters of means and covariance matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_SN(d, size_x, size_y, mu_x, mu_y, sigma_x, sigma_y, skewness_y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="generate_SN_+3A_d">d</code></td>
<td>
<p>number of dimensions.</p>
</td></tr>
<tr><td><code id="generate_SN_+3A_size_x">size_x</code></td>
<td>
<p>the number of observations for sample X</p>
</td></tr>
<tr><td><code id="generate_SN_+3A_size_y">size_y</code></td>
<td>
<p>the number of observations for sample Y</p>
</td></tr>
<tr><td><code id="generate_SN_+3A_mu_x">mu_x</code></td>
<td>
<p>the mean of X</p>
</td></tr>
<tr><td><code id="generate_SN_+3A_mu_y">mu_y</code></td>
<td>
<p>the mean of Y</p>
</td></tr>
<tr><td><code id="generate_SN_+3A_sigma_x">sigma_x</code></td>
<td>
<p>the standard deviation of X</p>
</td></tr>
<tr><td><code id="generate_SN_+3A_sigma_y">sigma_y</code></td>
<td>
<p>the standard deviation of Y</p>
</td></tr>
<tr><td><code id="generate_SN_+3A_skewness_y">skewness_y</code></td>
<td>
<p>the skewness of Y (the skewness of X is set to zero).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list containing the generated X and Y data sets.
</p>

<hr>
<h2 id='kb.test'>Kernel-based quadratic distance (KBQD) Goodness-of-Fit tests</h2><span id='topic+kb.test'></span><span id='topic+kb.test+2CANY-method'></span><span id='topic+show+2Ckb.test-method'></span>

<h3>Description</h3>

<p>This function performs the kernel-based quadratic distance goodness-of-fit
tests. It includes tests for multivariate normality, two-sample tests and
<code class="reqn">k</code>-sample tests.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kb.test(
  x,
  y = NULL,
  h = NULL,
  method = "subsampling",
  B = 150,
  b = NULL,
  Quantile = 0.95,
  mu_hat = NULL,
  Sigma_hat = NULL,
  centeringType = "Nonparam",
  K_threshold = 10,
  alternative = "skewness"
)

## S4 method for signature 'ANY'
kb.test(
  x,
  y = NULL,
  h = NULL,
  method = "subsampling",
  B = 150,
  b = 0.9,
  Quantile = 0.95,
  mu_hat = NULL,
  Sigma_hat = NULL,
  centeringType = "Nonparam",
  K_threshold = 10,
  alternative = "skewness"
)

## S4 method for signature 'kb.test'
show(object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kb.test_+3A_x">x</code></td>
<td>
<p>Numeric matrix or vector of data values.</p>
</td></tr>
<tr><td><code id="kb.test_+3A_y">y</code></td>
<td>
<p>Numeric matrix or vector of data values. Depending on the input
<code>y</code>, the corresponding test is performed.
</p>

<ul>
<li><p> if <code>y</code> = NULL, the function performs the tests for normality on
<code>x</code>
</p>
</li>
<li><p> if <code>y</code> is a data matrix, with same dimensions of <code>x</code>, the
function performs the two-sample test between <code>x</code> and <code>y</code>.
</p>
</li>
<li><p> if <code>y</code> is a numeric or factor vector, indicating the group
memberships for each observation, the function performs the k-sample
test.
</p>
</li></ul>
</td></tr>
<tr><td><code id="kb.test_+3A_h">h</code></td>
<td>
<p>Bandwidth for the kernel function. If a value is not provided, the
algorithm for the selection of an optimal h is performed
automatically. See the function <code><a href="#topic+select_h">select_h</a></code> for more
details.</p>
</td></tr>
<tr><td><code id="kb.test_+3A_method">method</code></td>
<td>
<p>The method used for critical value estimation (&quot;subsampling&quot;,
&quot;bootstrap&quot;, or &quot;permutation&quot;)(default: &quot;subsampling&quot;).</p>
</td></tr>
<tr><td><code id="kb.test_+3A_b">B</code></td>
<td>
<p>The number of iterations to use for critical value estimation
(default: 150).</p>
</td></tr>
<tr><td><code id="kb.test_+3A_b">b</code></td>
<td>
<p>The size of the subsamples used in the subsampling algorithm
(default: 0.8).</p>
</td></tr>
<tr><td><code id="kb.test_+3A_quantile">Quantile</code></td>
<td>
<p>The quantile to use for critical value estimation, 0.95 is
the default value.</p>
</td></tr>
<tr><td><code id="kb.test_+3A_mu_hat">mu_hat</code></td>
<td>
<p>Mean vector for the reference distribution.</p>
</td></tr>
<tr><td><code id="kb.test_+3A_sigma_hat">Sigma_hat</code></td>
<td>
<p>Covariance matrix of the reference distribution.</p>
</td></tr>
<tr><td><code id="kb.test_+3A_centeringtype">centeringType</code></td>
<td>
<p>String indicating the method used for centering the
normal kernel ('Param' or 'Nonparam').</p>
</td></tr>
<tr><td><code id="kb.test_+3A_k_threshold">K_threshold</code></td>
<td>
<p>maximum number of groups allowed. Default is 10. It is a
control parameter. Change in case of more than 10 samples.</p>
</td></tr>
<tr><td><code id="kb.test_+3A_alternative">alternative</code></td>
<td>
<p>Family of alternative chosen for selecting h, between
&quot;location&quot;, &quot;scale&quot; and &quot;skewness&quot; (only if <code>h</code>
is not provided).</p>
</td></tr>
<tr><td><code id="kb.test_+3A_object">object</code></td>
<td>
<p>Object of class <code>kb.test</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>kb.test</code> performs the kernel-based quadratic
distance tests using the Gaussian kernel with bandwidth parameter <code>h</code>.
Depending on the shape of the input <code>y</code> the function performs the tests
of multivariate normality, the non-parametric two-sample tests or the
k-sample tests.
</p>
<p>The quadratic distance between two probability distributions <code class="reqn">F</code> and
<code class="reqn">G</code> is
defined as </p>
<p style="text-align: center;"><code class="reqn">d_{K}(F,G)=\iint K(x,y)d(F-G)(x)d(F-G)(y),</code>
</p>

<p>where <code class="reqn">G</code> is a distribution whose goodness of fit we wish to assess and
<code class="reqn">K</code> denotes the Normal kernel defined as
</p>
<p style="text-align: center;"><code class="reqn"> K_{{h}}(\mathbf{s}, \mathbf{t}) = (2 \pi)^{-d/2} 
\left(\det{\mathbf{\Sigma}_h}\right)^{-\frac{1}{2}}  
\exp\left\{-\frac{1}{2}(\mathbf{s} - \mathbf{t})^\top 
\mathbf{\Sigma}_h^{-1}(\mathbf{s} - \mathbf{t})\right\},</code>
</p>

<p>for every <code class="reqn">\mathbf{s}, \mathbf{t} \in \mathbb{R}^d \times 
\mathbb{R}^d</code>, with covariance matrix <code class="reqn">\mathbf{\Sigma}_h=h^2 I</code> and
tuning parameter <code class="reqn">h</code>. <br />
</p>

<ul>
<li> <p><strong>Test for Normality</strong>: <br />
Let <code class="reqn">x_1, x_2, ..., x_n</code> be a random sample with empirical
distribution function <code class="reqn">\hat F</code>. We test the null hypothesis of
normality, i.e. <code class="reqn">H_0:F=G=\mathcal{N}_d(\mu, \Sigma)</code>.
</p>
<p>We consider the U-statistic estimate of the sample KBQD
</p>
<p style="text-align: center;"><code class="reqn">U_{n}=\frac{1}{n(n-1)}\sum_{i=2}^{n}\sum_{j=1}^{i-1}
   K_{cen}(\mathbf{x}_{i}, \mathbf{x}_{j}),</code>
</p>

<p>then the first test statistics is
</p>
<p style="text-align: center;"><code class="reqn">T_{n}=\frac{U_{n}}{\sqrt{Var(U_{n})}},</code>
</p>

<p>with <code class="reqn">Var(U_n)</code> computed exactly following Lindsay et al.(2014),
and the V-statistic estimate </p>
<p style="text-align: center;"><code class="reqn">V_{n} = \frac{1}{n}\sum_{i=1}^{n}
   \sum_{j=1}^{n}K_{cen}(\mathbf{x}_{i}, \mathbf{x}_{j}),</code>
</p>

<p>where <code class="reqn">K_{cen}</code> denotes the Normal kernel <code class="reqn">K_h</code> with parametric
centering with respect to the considered normal distribution
<code class="reqn">G = \mathcal{N}_d(\mu, \Sigma)</code>.
</p>
<p>The asymptotic distribution of the V-statistic is an infinite combination
of weighted independent chi-squared random variables with one degree of
freedom. The cutoff value is obtained using the Satterthwaite
approximation <code class="reqn">c \cdot \chi_{DOF}^2</code>, where <code class="reqn">c</code> and <code class="reqn">DOF</code>
are computed exactly following the formulas in Lindsay et al.(2014).
</p>
<p>For the <code class="reqn">U</code>-statistic the cutoff is determined empirically:
</p>

<ul>
<li><p> Generate data from the considered normal distribution ;
</p>
</li>
<li><p> Compute the test statistics for <code>B</code> Monte Carlo(MC) replications;
</p>
</li>
<li><p> Compute the 95th quantile of the empirical distribution of the test
statistic.
</p>
</li></ul>

</li>
<li> <p><strong>k-sample test</strong>: <br />
Consider <code class="reqn">k</code> random samples of i.i.d. observations
<code class="reqn">\mathbf{x}^{(i)}_1,
   \mathbf{x}^{(i)}_{2},\ldots, \mathbf{x}^{(i)}_{n_i} \sim F_i</code>,
<code class="reqn">i = 1, \ldots, k</code>.
We test if the samples are generated from the same <em>unknown</em> distribution,
that is <code class="reqn">H_0: F_1 = F_2 = \ldots = F_k</code> versus
<code class="reqn">H_1: F_i \not = F_j</code>, for some <code class="reqn">1 \le i \not = j \le k</code>. <br />
We construct a matrix distance <code class="reqn">\hat{\mathbf{D}}</code>, with
off-diagonal elements
</p>
<p style="text-align: center;"><code class="reqn">\hat{D}_{ij} = \frac{1}{n_i n_j} \sum_{\ell=1}^{n_i}
   \sum_{r=1}^{n_j}K_{\bar{F}}(\mathbf{x}^{(i)}_\ell,\mathbf{x}^{(j)}_r), 
   \qquad \mbox{ for }i \not= j</code>
</p>

<p>and in the diagonal
</p>
<p style="text-align: center;"><code class="reqn">\hat{D}_{ii} = \frac{1}{n_i (n_i -1)} \sum_{\ell=1}^{n_i}
   \sum_{r\not= \ell}^{n_i} K_{\bar{F}}(\mathbf{x}^{(i)}_\ell,
   \mathbf{x}^{(i)}_r), \qquad \mbox{ for }i = j,</code>
</p>

<p>where <code class="reqn">K_{\bar{F}}</code> denotes the Normal kernel <code class="reqn">K_h</code>
centered non-parametrically with respect to
</p>
<p style="text-align: center;"><code class="reqn">\bar{F} = \frac{n_1 \hat{F}_1 + \ldots + n_k \hat{F}_k}{n}, 
   \quad \mbox{ with } n=\sum_{i=1}^k n_i.</code>
</p>

<p>We compute the trace statistic
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{trace}(\hat{\mathbf{D}}_n) =  \sum_{i=1}^{k}\hat{D}_{ii}</code>
</p>

<p>and <code class="reqn">D_n</code>, derived considering all the possible pairwise comparisons
in the <em>k</em>-sample null hypothesis, given as
</p>
<p style="text-align: center;"><code class="reqn">D_n = (k-1) \mathrm{trace}(\hat{\mathbf{D}}_n) 
   - 2 \sum_{i=1}^{k}\sum_{j&gt; i}^{k}\hat{D}_{ij}.</code>
</p>

<p>We compute the empirical critical value by employing numerical techniques
such as the bootstrap, permutation and subsampling algorithms:
</p>

<ul>
<li><p> Generate k-tuples, of total size <code class="reqn">n_B</code>, from the pooled sample
following one of the sampling methods;
</p>
</li>
<li><p> Compute the k-sample test statistic;
</p>
</li>
<li><p> Repeat <code>B</code> times;
</p>
</li>
<li><p> Select the <code class="reqn">95^{th}</code> quantile of the obtained values.
</p>
</li></ul>

</li>
<li> <p><strong>Two-sample test</strong>: <br />
Let <code class="reqn">x_1, x_2, ..., x_{n_1} \sim F</code> and
<code class="reqn">y_1, y_2, ..., y_{n_2} \sim G</code> be
random samples from the distributions <code class="reqn">F</code> and <code class="reqn">G</code>, respectively.
We test the null hypothesis that the two samples are generated from
the same <em>unknown</em> distribution, that is <code class="reqn">H_0: F=G</code> vs
<code class="reqn">H_1:F\not=G</code>. The test statistics coincide with the <code class="reqn">k</code>-sample
test statistics when <code class="reqn">k=2</code>.
</p>
</li></ul>



<h4>Kernel centering</h4>

<p>The arguments <code>mu_hat</code> and <code>Sigma_hat</code> indicate the normal model
considered for the normality test, that is <code class="reqn">H_0: F = N(</code><code>mu_hat</code>,
<code>Sigma_hat</code>).
For the two-sample and <code class="reqn">k</code>-sample tests, <code>mu_hat</code> and
<code>Sigma_hat</code> can
be used for the parametric centering of the kernel, in the case we want to
specify the reference distribution, with <code>centeringType = "Param"</code>.
This is the default method when the test for normality is performed.
The normal kernel centered with respect to
<code class="reqn">G \sim N_d(\mathbf{\mu}, \mathbf{V})</code> can be computed as
</p>
<p style="text-align: center;"><code class="reqn">K_{cen(G)}(\mathbf{s}, \mathbf{t}) = 
 K_{\mathbf{\Sigma_h}}(\mathbf{s}, \mathbf{t}) - 
 K_{\mathbf{\Sigma_h} + \mathbf{V}}(\mathbf{\mu}, \mathbf{t}) 
-  K_{\mathbf{\Sigma_h} + \mathbf{V}}(\mathbf{s}, \mathbf{\mu}) +
 K_{\mathbf{\Sigma_h} + 2\mathbf{V}}(\mathbf{\mu}, \mathbf{\mu}).</code>
</p>

<p>We consider the non-parametric centering of the kernel with respect to
<code class="reqn">\bar{F}=(n_1 F_1 + \ldots n_k F_k)/n</code> where <code class="reqn">n=\sum_{i=1}^k n_i</code>,
with <code>centeringType = "Nonparam"</code>, for the two- and <code class="reqn">k</code>-sample
tests.
Let <code class="reqn">\mathbf{z}_1,\ldots, \mathbf{z}_n</code> denote the pooled sample. For any
<code class="reqn">s,t \in \{\mathbf{z}_1,\ldots, \mathbf{z}_n\}</code>, it is given by
</p>
<p style="text-align: center;"><code class="reqn">K_{cen(\bar{F})}(\mathbf{s},\mathbf{t}) =    K(\mathbf{s},\mathbf{t}) -
 \frac{1}{n}\sum_{i=1}^{n} K(\mathbf{s},\mathbf{z}_i) - 
 \frac{1}{n}\sum_{i=1}^{n} K(\mathbf{z}_i,\mathbf{t}) + 
 \frac{1}{n(n-1)}\sum_{i=1}^{n} \sum_{j \not=i}^{n} 
 K(\mathbf{z}_i,\mathbf{z}_j).</code>
</p>




<h3>Value</h3>

<p>An S4 object of class <code>kb.test</code> containing the results of the
kernel-based quadratic distance tests, based on the normal kernel. The object
contains the following slots:
</p>

<ul>
<li> <p><code>method</code>: Description of the kernel-based quadratic
distance test performed.
</p>
</li>
<li> <p><code>x</code> Data list of samples X (and Y).
</p>
</li>
<li> <p><code>Un</code> The value of the U-statistic.
</p>
</li>
<li> <p><code>H0_Un</code> A logical value indicating whether or not the null
hypothesis is rejected according to Un.
</p>
</li>
<li> <p><code>CV_Un</code> The critical value computed for the test Un.
</p>
</li>
<li> <p><code>Vn</code> The value of the V-statistic (if available).
</p>
</li>
<li> <p><code>H0_Vn</code> A logical value indicating whether or not the null
hypothesis is rejected according to Vn (if available).
</p>
</li>
<li> <p><code>CV_Vn</code> The critical value computed for the test Vn
(if available).
</p>
</li>
<li> <p><code>h</code> List with the value of bandwidth parameter used for the
normal kernel function. If <code>select_h</code> is used, the matrix of computed
power values and the corresponding power plot are also provided.
</p>
</li>
<li> <p><code>B</code> Number of bootstrap/permutation/subsampling replications.
</p>
</li>
<li> <p><code>var_Un</code> exact variance of the kernel-based U-statistic.
</p>
</li>
<li> <p><code>cv_method</code> The method used to estimate the critical value
(one of &quot;subsampling&quot;, &quot;permutation&quot; or &quot;bootstrap&quot;).
</p>
</li></ul>



<h3>Note</h3>

<p>For the two- and <code class="reqn">k</code>-sample tests, the slots <code>Vn</code>, <code>H0_Vn</code> and
<code>CV_Vn</code> are empty, while the computed statistics are both reported in
slots <code>Un</code>, <code>H0_Un</code> and <code>CV_Un</code>.
</p>
<p>A U-statistic is a type of statistic that is used to estimate a population
parameter. It is based on the idea of averaging over all possible <em>distinct</em>
combinations of a fixed size from a sample.
A V-statistic considers all possible tuples of a certain size, not just
distinct combinations and can be used in contexts where unbiasedness is not
required.
</p>


<h3>References</h3>

<p>Markatou, M. and Saraceno, G. (2024). “A Unified Framework for
Multivariate Two- and k-Sample Kernel-based Quadratic Distance
Goodness-of-Fit Tests.” <br />
https://doi.org/10.48550/arXiv.2407.16374
</p>
<p>Lindsay, B.G., Markatou, M. and Ray, S. (2014) &quot;Kernels, Degrees of Freedom,
and Power Properties of Quadratic Distance Goodness-of-Fit Tests&quot;, Journal
of the American Statistical Association, 109:505, 395-410,
DOI: 10.1080/01621459.2013.836972
</p>


<h3>See Also</h3>

<p><a href="#topic+kb.test-class">kb.test</a> for the class definition.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create a kb.test object
x &lt;- matrix(rnorm(100), ncol = 2)
y &lt;- matrix(rnorm(100), ncol = 2)

# Normality test
my_test &lt;- kb.test(x, h=0.5)
my_test

# Two-sample test
my_test &lt;- kb.test(x, y, h = 0.5, method = "subsampling", b = 0.9,
                   centeringType = "Nonparam")
my_test

# k-sample test
z &lt;- matrix(rnorm(100, 2), ncol = 2)
dat &lt;- rbind(x, y, z)
group &lt;- rep(c(1, 2, 3), each = 50)
my_test &lt;- kb.test(x = dat, y = group, h = 0.5, method = "subsampling", b = 0.9)
my_test

</code></pre>

<hr>
<h2 id='kb.test-class'>An S4 class for kernel-based distance tests with normal kernel</h2><span id='topic+kb.test-class'></span>

<h3>Description</h3>

<p>A class to represent the results of Gaussian kernel-based
quadratic distance tests. This includes the normality test, the two-sample
test statistics and the k-sample tests.
</p>


<h3>Slots</h3>


<dl>
<dt><code>method</code></dt><dd><p>String indicating the kernel-based quadratic distance
test performed.</p>
</dd>
<dt><code>Un</code></dt><dd><p>The value of the test U-statistic.</p>
</dd>
<dt><code>Vn</code></dt><dd><p>The value of the test V-statistic.</p>
</dd>
<dt><code>H0_Un</code></dt><dd><p>A logical value indicating whether or not the null hypothesis is
rejected according to U-statistic.</p>
</dd>
<dt><code>H0_Vn</code></dt><dd><p>A logical value indicating whether or not the null hypothesis is
rejected according to Vn.</p>
</dd>
<dt><code>data</code></dt><dd><p>List of samples X (and Y).</p>
</dd>
<dt><code>CV_Un</code></dt><dd><p>The critical value computed for the test Un.</p>
</dd>
<dt><code>CV_Vn</code></dt><dd><p>The critical value computed for the test Vn.</p>
</dd>
<dt><code>cv_method</code></dt><dd><p>The method used to estimate the critical value (one of
&quot;subsampling&quot;, &quot;permutation&quot; or &quot;bootstrap&quot;).</p>
</dd>
<dt><code>h</code></dt><dd><p>A list with the value of bandwidth parameter used for the Gaussian
kernel. If the function <code>select_h</code> is used, then also the matrix
of computed power values and the resulting power plot are provided.</p>
</dd>
<dt><code>B</code></dt><dd><p>Number of bootstrap/permutation/subsampling replications.</p>
</dd>
<dt><code>var_Un</code></dt><dd><p>Exact variance of the kernel-based U-statistic.</p>
</dd>
</dl>


<h3>See Also</h3>

<p><code><a href="#topic+kb.test">kb.test()</a></code> for the function that generates this class.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create a kb.test object
x &lt;- matrix(rnorm(100), ncol = 2)
y &lt;- matrix(rnorm(100), ncol = 2)
# Normality test
kb.test(x, h = 0.5)

# Two-sample test
kb.test(x, y, h=0.5, method = "subsampling", b = 0.9)

</code></pre>

<hr>
<h2 id='normal_CV'>Compute the critical value for the KBQD tests for multivariate Normality</h2><span id='topic+normal_CV'></span>

<h3>Description</h3>

<p>This function computes the empirical critical value for the Normality test
based on the KBQD tests using the centered Gaussian kernel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normal_CV(d, size, h, mu_hat, Sigma_hat, B = 150, Quantile = 0.95)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="normal_CV_+3A_d">d</code></td>
<td>
<p>the dimension of generated samples.</p>
</td></tr>
<tr><td><code id="normal_CV_+3A_size">size</code></td>
<td>
<p>the number of observations to be generated.</p>
</td></tr>
<tr><td><code id="normal_CV_+3A_h">h</code></td>
<td>
<p>the concentration parameter for the Gaussian kernel.</p>
</td></tr>
<tr><td><code id="normal_CV_+3A_mu_hat">mu_hat</code></td>
<td>
<p>Mean vector for the reference distribution.</p>
</td></tr>
<tr><td><code id="normal_CV_+3A_sigma_hat">Sigma_hat</code></td>
<td>
<p>Covariance matrix of the reference distribution.</p>
</td></tr>
<tr><td><code id="normal_CV_+3A_b">B</code></td>
<td>
<p>the number of replications.</p>
</td></tr>
<tr><td><code id="normal_CV_+3A_quantile">Quantile</code></td>
<td>
<p>the quantile of the distribution use to select the critical
value</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each replication, a sample from the d-dimensional Normal distribution
with mean vector <code>mu_hat</code> and covariance matrix <code>Sigma_hat</code> is
generated and the KBQD test U-statistic for Normality is computed.
After B iterations, the critical value is selected as the <code>Quantile</code>
of the empirical distribution of the computed test statistics.
</p>


<h3>Value</h3>

<p>the critical value for the specified dimension, size and level.
</p>

<hr>
<h2 id='pk.test'>Poisson kernel-based quadratic distance test of Uniformity on the sphere</h2><span id='topic+pk.test'></span><span id='topic+pk.test+2CANY-method'></span><span id='topic+show+2Cpk.test-method'></span>

<h3>Description</h3>

<p>This function performs the kernel-based quadratic distance goodness-of-fit
tests for Uniformity for multivariate spherical data <code>x</code> on
<code class="reqn">\mathcal{S}^{d-1}</code> using the Poisson kernel
with concentration parameter <code>rho</code>. <br />
The Poisson kernel-based test for uniformity exhibits excellent results
especially in the case of multimodal distributions, as shown in the example
of the <a href="../doc/uniformity.html">Uniformity test on the Sphere vignette</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pk.test(x, rho, B = 300, Quantile = 0.95)

## S4 method for signature 'ANY'
pk.test(x, rho, B = 300, Quantile = 0.95)

## S4 method for signature 'pk.test'
show(object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pk.test_+3A_x">x</code></td>
<td>
<p>A numeric <code class="reqn">(n \times d)</code>-matrix of <code class="reqn">n</code> data points on the
Sphere <code class="reqn">\mathcal{S}^(d-1)</code> as rows.</p>
</td></tr>
<tr><td><code id="pk.test_+3A_rho">rho</code></td>
<td>
<p>Concentration parameter of the Poisson kernel function.</p>
</td></tr>
<tr><td><code id="pk.test_+3A_b">B</code></td>
<td>
<p>Number of Monte Carlo iterations for critical value estimation of Un
(default: 300).</p>
</td></tr>
<tr><td><code id="pk.test_+3A_quantile">Quantile</code></td>
<td>
<p>The quantile to use for critical value estimation,
0.95 is the default value.</p>
</td></tr>
<tr><td><code id="pk.test_+3A_object">object</code></td>
<td>
<p>Object of class <code>pk.test</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let <code class="reqn">x_1, x_2, ..., x_n</code> be a random sample with empirical distribution
function <code class="reqn">\hat F</code>. We test the null hypothesis of uniformity on the
<code class="reqn">(d-1)</code>-dimensional sphere, i.e. <code class="reqn">H_0:F=G</code>, where <code class="reqn">G</code> is the
uniform distribution on the <code class="reqn">(d-1)</code>-dimensional sphere
<code class="reqn">\mathcal{S}^{d-1}</code>.
We compute the U-statistic estimate of the sample KBQD (Kernel-Based
Quadratic Distance)
</p>
<p style="text-align: center;"><code class="reqn">U_{n}=\frac{1}{n(n-1)}\sum_{i=2}^{n}\sum_{j=1}^{i-1}K_{cen}
(\mathbf{x}_{i}, \mathbf{x}_{j}),</code>
</p>

<p>then the first test statistic is given as
</p>
<p style="text-align: center;"><code class="reqn">T_{n}=\frac{U_{n}}{\sqrt{Var(U_{n})}},</code>
</p>

<p>with
</p>
<p style="text-align: center;"><code class="reqn">Var(U_{n})= \frac{2}{n(n-1)}
\left[\frac{1+\rho^{2}}{(1-\rho^{2})^{d-1}}-1\right],</code>
</p>

<p>and the V-statistic estimate of the KBQD
</p>
<p style="text-align: center;"><code class="reqn">V_{n} = \frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{n}K_{cen}
(\mathbf{x}_{i}, \mathbf{x}_{j}),</code>
</p>

<p>where <code class="reqn">K_{cen}</code> denotes the Poisson kernel <code class="reqn">K_\rho</code> centered with
respect to the uniform distribution on the <code class="reqn">(d-1)</code>-dimensional sphere,
that is
</p>
<p style="text-align: center;"><code class="reqn">K_{cen}(\mathbf{u}, \mathbf{v}) = K_\rho(\mathbf{u}, \mathbf{v}) -1</code>
</p>

<p>and
</p>
<p style="text-align: center;"><code class="reqn">K_\rho(\mathbf{u}, \mathbf{v}) = \frac{1-\rho^{2}}{\left(1+\rho^{2}-
2\rho (\mathbf{u}\cdot \mathbf{v})\right)^{d/2}},</code>
</p>

<p>for every <code class="reqn">\mathbf{u}, \mathbf{v} \in \mathcal{S}^{d-1} 
\times \mathcal{S}^{d-1}</code>.
</p>
<p>The asymptotic distribution of the V-statistic is an infinite combination
of weighted independent chi-squared random variables with one degree of
freedom. The cutoff value is obtained using the Satterthwaite approximation
<code class="reqn">c \cdot \chi_{DOF}^2</code>, where </p>
<p style="text-align: center;"><code class="reqn">c=\frac{(1+\rho^{2})-
(1-\rho^{2})^{d-1}}{(1+\rho)^{d}-(1-\rho^{2})^{d-1}}</code>
</p>
<p> and </p>
<p style="text-align: center;"><code class="reqn">DOF(K_{cen}
)=\left(\frac{1+\rho}{1-\rho} \right)^{d-1}\left\{ 
\frac{\left(1+\rho-(1-\rho)^{d-1} \right )^{2}}
{1+\rho^{2}-(1-\rho^{2})^{d-1}}\right \}.</code>
</p>
<p>.
For the <code class="reqn">U</code>-statistic the cutoff is determined empirically:
</p>

<ul>
<li><p> Generate data from a Uniform distribution on the d-dimensional sphere;
</p>
</li>
<li><p> Compute the test statistics for <code>B</code> Monte Carlo(MC) replications;
</p>
</li>
<li><p> Compute the 95th quantile of the empirical distribution of the test
statistic.
</p>
</li></ul>



<h3>Value</h3>

<p>An S4 object of class <code>pk.test</code> containing the results of the
Poisson kernel-based tests. The object contains the following slots:
</p>

<ul>
<li> <p><code>method</code>: Description of the test performed.
</p>
</li>
<li> <p><code>x</code> Data matrix.
</p>
</li>
<li> <p><code>Un</code> The value of the U-statistic.
</p>
</li>
<li> <p><code>CV_Un</code> The empirical critical value for Un.
</p>
</li>
<li> <p><code>H0_Vn</code> A logical value indicating whether or not the null
hypothesis is rejected according to Un.
</p>
</li>
<li> <p><code>Vn</code> The value of the V-statistic Vn.
</p>
</li>
<li> <p><code>CV_Vn</code> The critical value for Vn computed following the
asymptotic distribution.
</p>
</li>
<li> <p><code>H0_Vn</code> A logical value indicating whether or not the null
hypothesis is rejected according to Vn.
</p>
</li>
<li> <p><code>rho</code> The value of concentration parameter used for the Poisson
kernel function.
</p>
</li>
<li> <p><code>B</code> Number of replications for the critical value of the
U-statistic Un.
</p>
</li></ul>



<h3>Note</h3>

<p>A U-statistic is a type of statistic that is used to estimate a population
parameter. It is based on the idea of averaging over all possible <em>distinct</em>
combinations of a fixed size from a sample.
A V-statistic considers all possible tuples of a certain size, not just
distinct combinations and can be used in contexts where unbiasedness is not
required.
</p>


<h3>References</h3>

<p>Ding, Y., Markatou, M. and Saraceno, G. (2023). “Poisson Kernel-Based Tests
for Uniformity on the d-Dimensional Sphere.” Statistica Sinica.
doi:10.5705/ss.202022.0347
</p>


<h3>See Also</h3>

<p><a href="#topic+pk.test-class">pk.test</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create a pk.test object
x_sp &lt;- sample_hypersphere(3, n_points = 100)
unif_test &lt;- pk.test(x_sp, rho = 0.8)
unif_test

</code></pre>

<hr>
<h2 id='pk.test-class'>An S4 class for Poisson kernel-based quadratic distance tests.</h2><span id='topic+pk.test-class'></span>

<h3>Description</h3>

<p>A class to represent the results of Poisson kernel-based
quadratic distance tests for Uniformity on the sphere.
</p>


<h3>Slots</h3>


<dl>
<dt><code>method</code></dt><dd><p>Description of the test.</p>
</dd>
<dt><code>x</code></dt><dd><p>Matrix of data</p>
</dd>
<dt><code>Un</code></dt><dd><p>The value of the U-statistic.</p>
</dd>
<dt><code>CV_Un</code></dt><dd><p>The critical value for Un computed through replications.</p>
</dd>
<dt><code>H0_Un</code></dt><dd><p>A logical value indicating whether or not the null hypothesis is
rejected according to Un.</p>
</dd>
<dt><code>Vn</code></dt><dd><p>The value of the V-statistic.</p>
</dd>
<dt><code>CV_Vn</code></dt><dd><p>The critical value for Vn computed following the asymptotic
distribution.</p>
</dd>
<dt><code>H0_Vn</code></dt><dd><p>A logical value indicating whether or not the null hypothesis is
rejected according to Vn.</p>
</dd>
<dt><code>rho</code></dt><dd><p>The concentration parameter of the Poisson kernel.</p>
</dd>
<dt><code>B</code></dt><dd><p>Number of replications.</p>
</dd>
<dt><code>var_Un</code></dt><dd><p>exact variance of the kernel-based U-statistic.</p>
</dd>
</dl>


<h3>See Also</h3>

<p><code><a href="#topic+pk.test">pk.test()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create a pk.test object
d=3
size=100
x_sp &lt;- sample_hypersphere(d, n_points=size)
pk.test(x_sp,rho=0.8)

</code></pre>

<hr>
<h2 id='pkbc'>Poisson kernel-based clustering on the sphere</h2><span id='topic+pkbc'></span><span id='topic+pkbc+2CANY-method'></span><span id='topic+show+2Cpkbc-method'></span>

<h3>Description</h3>

<p>The function <code>pkbc()</code> performs the Poisson kernel-based clustering
algorithm on the sphere proposed by Golzy and Markatou (2020).
The proposed algorithm is based on a mixture, with <code class="reqn">M</code> components, of
Poisson kernel-based densities on the hypersphere <code class="reqn">\mathcal{S}^{d-1}</code>
given by
</p>
<p style="text-align: center;"><code class="reqn">f(x|\Theta) = \sum_{j=1}^M \alpha_j f_j(x|\rho_j, \mu_j)</code>
</p>

<p>where <code class="reqn">\alpha_j</code>'s are the mixing proportions and <code class="reqn">f_j(x|\rho_j, 
\mu_j)</code>'s denote the probability density function of a <code class="reqn">d</code>-variate
Poisson kernel-based density given as
</p>
<p style="text-align: center;"><code class="reqn">f(\mathbf{x}|\rho, \mathbf{\mu}) = \frac{1-\rho^2}{\omega_d 
||\mathbf{x} - \rho \mathbf{\mu}||^d}.</code>
</p>

<p>The parameters <code class="reqn">\alpha_j, \mu_j, \rho_j</code> are estimated through a
iterative reweighted EM algorithm. <br />
The proposed clustering algorithm exhibits excellent results when
(1) the clusters are not well separated; (2) the data points
are fairly well concentrated around the vectors <code class="reqn">\mu_j</code> of each cluster;
(3) the percentage of noise in the data increases.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pkbc(
  dat,
  nClust,
  maxIter = 300,
  stoppingRule = "loglik",
  initMethod = "sampleData",
  numInit = 10
)

## S4 method for signature 'ANY'
pkbc(
  dat,
  nClust,
  maxIter = 300,
  stoppingRule = "loglik",
  initMethod = "sampleData",
  numInit = 10
)

## S4 method for signature 'pkbc'
show(object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pkbc_+3A_dat">dat</code></td>
<td>
<p><code class="reqn">(n \times d)</code>-data matrix or data.frame of data points on the
sphere to be clustered. The observations in <code>dat</code> are
normalized by dividing with the length of the vector to ensure
that they lie on the <code class="reqn">d</code>-dimensional sphere. Note that
<code class="reqn">d &gt; 1</code>.</p>
</td></tr>
<tr><td><code id="pkbc_+3A_nclust">nClust</code></td>
<td>
<p>Number of clusters. It can be a single value or a numeric
vector.</p>
</td></tr>
<tr><td><code id="pkbc_+3A_maxiter">maxIter</code></td>
<td>
<p>The maximum number of iterations before a run is terminated.</p>
</td></tr>
<tr><td><code id="pkbc_+3A_stoppingrule">stoppingRule</code></td>
<td>
<p>String describing the stopping rule to be used within
each run. Currently must be either <code>'max'</code>,
<code>'membership'</code>, or <code>'loglik'</code>.</p>
</td></tr>
<tr><td><code id="pkbc_+3A_initmethod">initMethod</code></td>
<td>
<p>String describing the initialization method to be used.
Currently must be <code>'sampleData'</code>.</p>
</td></tr>
<tr><td><code id="pkbc_+3A_numinit">numInit</code></td>
<td>
<p>Number of initialization.</p>
</td></tr>
<tr><td><code id="pkbc_+3A_object">object</code></td>
<td>
<p>Object of class <code>pkbc</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>We set all concentration parameters equal to 0.5 and all mixing proportions
to be equal. <br />
The initialization method <code>'sampleData'</code> indicates that observation
points are randomly chosen as initializers of the centroids <code class="reqn">\mu_j</code>.
This random starts strategy has a chance of not obtaining initial
representatives from the underlying clusters, then the clustering is
performed <code>numInit</code> times and the random start with the highest
likelihood is chosen as the final estimate of the parameters.
</p>
<p>The possible <code>stoppingRule</code> for each iteration are: <br />
</p>

<ul>
<li> <p><code>'loglik'</code> run the algorithm until the change in log-likelihood from
one iteration to the next is less than a given threshold (1e-7) <br />
</p>
</li>
<li> <p><code>'membership'</code> run the algorithm until the membership is unchanged for
all points from one iteration to the next <br />
</p>
</li>
<li> <p><code>'max'</code> reach a maximum number of iterations <code>maxIter</code>
</p>
</li></ul>

<p>The obtained estimates are used for assigning final memberships, identifying
the <code>nClust</code> clusters, according to the following rule
</p>
<p style="text-align: center;"><code class="reqn">P(x_i, \Theta) = \arg\max_{j \in \{1, \ldots, k\}} \{ \frac{\alpha_j 
f_j(x_i|\mu_j, \rho_j)}{f(x_i, \Theta)}\}.</code>
</p>

<p>The number of clusters <code>nClust</code> must be provided as input to the
clustering algorithm.
</p>


<h3>Value</h3>

<p>An S4 object of class <code>pkbc</code> containing the results of the
clustering procedure based on Poisson kernel-based distributions. The object
contains the following slots:
</p>
<p><code>res_k</code>: List of results of the Poisson kernel-based clustering
algorithm for each value of number of clusters specified in <code>nClust</code>.
Each object in the list contains:
</p>

<ul>
<li> <p><code>postProbs</code> Posterior probabilities of each observation for
the indicated clusters.
</p>
</li>
<li> <p><code>LogLik</code> Maximum value of log-likelihood function
</p>
</li>
<li> <p><code>wcss</code> Values of within-cluster sum of squares computed with
Euclidean distance and cosine similarity, respectively.
</p>
</li>
<li> <p><code>params</code> List of estimated parameters of the mixture model
</p>

<ul>
<li> <p><code>mu</code> estimated centroids
</p>
</li>
<li> <p><code>rho</code> estimated concentration parameters rho
</p>
</li>
<li> <p><code>alpha</code> estimated mixing proportions
</p>
</li></ul>

</li>
<li> <p><code>finalMemb</code> Vector of final memberships
</p>
</li>
<li> <p><code>runInfo</code> List of information of the EM algorithm iterations
</p>

<ul>
<li> <p><code>lokLikVec</code> vector of log-likelihood values
</p>
</li>
<li> <p><code>numIterPerRun</code> number of E-M iterations per run
</p>
</li></ul>

</li></ul>

<p><code>input</code>: List of input information.
</p>


<h3>Note</h3>

<p>The clustering algorithm is tailored for data points on the sphere
<code class="reqn">\mathcal{S}^{d-1}</code>, but it can also be performed on spherically
transformed observations, i.e. data points on the Euclidean space
<code class="reqn">\mathbb{R}^d</code> that are normalized such that they lie on the
corresponding <code class="reqn">(d-1)</code>-dimensional sphere <code class="reqn">\mathcal{S}^{d-1}</code>.
</p>


<h3>References</h3>

<p>Golzy, M. and Markatou, M. (2020) Poisson Kernel-Based Clustering on the
Sphere: Convergence Properties, Identifiability, and a Method of Sampling,
Journal of Computational and Graphical Statistics, 29:4, 758-770,
DOI: 10.1080/10618600.2020.1740713.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dpkb">dpkb()</a></code> and <code><a href="#topic+rpkb">rpkb()</a></code> for more information on the Poisson
kernel-based distribution. <br />
<a href="#topic+pkbc-class">pkbc</a> for the class definition.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># We generate three samples of 100 observations from 3-dimensional
# Poisson kernel-based densities with rho=0.8 and different mean directions
size &lt;- 100
groups &lt;- c(rep(1, size), rep(2, size), rep(3, size))
rho &lt;- 0.8
set.seed(081423)
data1 &lt;- rpkb(size, c(1, 0, 0), rho)
data2 &lt;- rpkb(size, c(0, 1, 0), rho)
data3 &lt;- rpkb(size, c(0, 0, 1), rho)
dat &lt;- rbind(data1, data2, data3)

# Perform the clustering algorithm with number of clusters k=3.
pkbd &lt;- pkbc(dat = dat, nClust = 3)
show(pkbd)

</code></pre>

<hr>
<h2 id='pkbc_validation'>Validation of Poisson kernel-based clustering results</h2><span id='topic+pkbc_validation'></span>

<h3>Description</h3>

<p>Method for objects of class <code>pkbc</code> which computes evaluation measures
for clustering results.
The following evaluation measures are computed:
In-Group Proportion (Kapp and Tibshirani (2007)). If true label are
provided, ARI, Average Silhouette Width (Rousseeuw (1987)), Macro-Precision
and Macro-Recall are computed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pkbc_validation(object, true_label = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pkbc_validation_+3A_object">object</code></td>
<td>
<p>Object of class <code>pkbc</code></p>
</td></tr>
<tr><td><code id="pkbc_validation_+3A_true_label">true_label</code></td>
<td>
<p>factor or vector of true membership to clusters (if
available). It must have the same length of final
memberships.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The IGP is a statistical measure that quantifies the proportion of
observations within a group that belong to the same predefined category or
class. It is often used to assess the homogeneity of a group by evaluating
how many of its members share the same label. A higher IGP indicates that the
group is more cohesive, while a lower proportion suggests greater diversity
or misclassification within the group (Kapp and Tibshirani 2007).
</p>
<p>The Adjusted Rand Index (ARI) is a statistical measure used in data
clustering analysis. It quantifies the similarity between two partitions of
a dataset by comparing the assignments of data points to clusters. The ARI
value ranges from 0 to 1, where a value of 1 indicates a perfect match
between the partitions and a value close to 0 indicates a random assignment
of data points to clusters.
</p>
<p>The average silhouette width quantifies the quality of clustering by
measuring how well each object fits within its assigned cluster. It is the
mean of silhouette values, which compare the tightness of an object within
its cluster to its separation from other clusters. Higher values indicate
well-separated, cohesive clusters, making it useful for selecting the
<em>appropriate</em> number of clusters (Rousseeuw 1987).
</p>
<p>Macro Precision is a metric used in multi-class classification that
calculates the precision for each class independently and then takes the
average of these values. Precision for a class is defined as the proportion
of true positive predictions out of all predictions made for that class.
</p>
<p>Macro Recall is similar to Macro Precision but focuses on recall. Recall for
a class is the proportion of true positive predictions out of all actual
instances of that class. Macro Recall is the average of the recall values
computed for each class.
</p>


<h3>Value</h3>

<p>List with the following components:
</p>

<ul>
<li> <p><code>metrics</code> Table of computed evaluation measures for each value
of number of clusters in the <code>pkbc</code> object. The
number of cluster is indicated as column name.
</p>
</li>
<li> <p><code>IGP</code> List of in-group proportions for each value of number of
clusters specified.
</p>
</li></ul>



<h3>Note</h3>

<p>Note that Macro Precision and Macro Recall depend on the assigned labels,
while the ARI measures the similarity between partition up to label
switching.
</p>
<p>If the required packages (<code>mclust</code> for ARI, <code>clusterRepro</code> for IGP, and
<code>cluster</code> for ASW) are not installed, the function will display a message
asking the user to install the missing package(s).
</p>


<h3>References</h3>

<p>Kapp, A.V. and Tibshirani, R. (2007) &quot;Are clusters found in one dataset
present in another dataset?&quot;, Biostatistics, 8(1), 9–31,
https://doi.org/10.1093/biostatistics/kxj029
</p>
<p>Rousseeuw, P.J. (1987) Silhouettes: A graphical aid to the interpretation and
validation of cluster analysis. Journal of Computational and Applied
Mathematics, 20, 53–65.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pkbc">pkbc()</a></code> for the clustering algorithm <br />
<a href="#topic+pkbc-class">pkbc</a> for the class object definition.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#We generate three samples of 100 observations from 3-dimensional
#Poisson kernel-based densities with rho=0.8 and different mean directions

size &lt;- 20
groups &lt;- c(rep(1, size), rep(2, size), rep(3, size))
rho &lt;- 0.8
set.seed(081423)
data1 &lt;- rpkb(size, c(1,0,0), rho, method = 'rejvmf')
data2 &lt;- rpkb(size, c(0,1,0), rho, method = 'rejvmf')
data3 &lt;- rpkb(size, c(1,0,0), rho, method = 'rejvmf')
data &lt;- rbind(data1, data2, data3)

#Perform the clustering algorithm
pkbc_res &lt;- pkbc(data, 3)
pkbc_validation(pkbc_res)


</code></pre>

<hr>
<h2 id='pkbc-class'>A S4 class for the clustering algorithm on the sphere based on
Poisson kernel-based distributions.</h2><span id='topic+pkbc-class'></span>

<h3>Description</h3>

<p>A class to represent the results of Poisson kernel-based
clustering procedure for spherical observations.
</p>


<h3>Slots</h3>


<dl>
<dt><code>res_k</code></dt><dd><p>List of objects with the results of the clustering algorithm for
each value of possible number of clusters considered.</p>
</dd>
<dt><code>input</code></dt><dd><p>List of input data</p>
</dd>
</dl>


<h3>See Also</h3>

<p><code><a href="#topic+pkbc">pkbc()</a></code> for more details.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("wireless")
res &lt;- pkbc(as.matrix(wireless[,-8]),4)

</code></pre>

<hr>
<h2 id='plot.pkbc'>Plotting method for Poisson kernel-based clustering</h2><span id='topic+plot.pkbc'></span><span id='topic+plot+2Cpkbc+2CANY-method'></span>

<h3>Description</h3>

<p>Plots for a pkbc object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'pkbc,ANY'
plot(x, k = NULL, true_label = NULL, pca_res = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.pkbc_+3A_x">x</code></td>
<td>
<p>Object of class <code>pkbc</code></p>
</td></tr>
<tr><td><code id="plot.pkbc_+3A_k">k</code></td>
<td>
<p>number of considered clusters. If it is not provided the scatter
plot is displayed for each value of number of clusters present in
the <code>x</code> object</p>
</td></tr>
<tr><td><code id="plot.pkbc_+3A_true_label">true_label</code></td>
<td>
<p>factor or vector of true membership to clusters (if
available). It must have the same length of final
memberships.</p>
</td></tr>
<tr><td><code id="plot.pkbc_+3A_pca_res">pca_res</code></td>
<td>
<p>Logical. If TRUE the results from PCALocantore are also
reported (when dimension is greater than 3).</p>
</td></tr>
<tr><td><code id="plot.pkbc_+3A_...">...</code></td>
<td>
<p>Additional arguments that can be passed to the plot function</p>
</td></tr>
</table>


<h3>Details</h3>


<ul>
<li><p> scatterplot: If dimension is equal to 2 or 3, points are displayed on the
circle and sphere, respectively. If dimension if greater than 3, the
spherical Principal Component procedure proposed by Locantore et al. (1999),
is applied for dimensionality reduction and the first three principal
components are normalized and displayed on the sphere. For d &gt; 3, the
complete results from the <code>PcaLocantore</code> function (package <code>rrcov</code>)
are returned if <code>pca_res=TRUE</code>.
</p>
</li>
<li><p> elbow plot: the within cluster sum of squares (wcss) is computed using the
Euclidean distance (left) and the cosine similarity (right).
</p>
</li></ul>



<h3>Value</h3>

<p>The scatter-plot(s) and the elbow plot.
</p>


<h3>Note</h3>

<p>The elbow plot is commonly used as a graphical method for choosing the
<em>appropriate</em> number of clusters. Specifically, plotting the wcss versus the
number of clusters, the suggested number of clusters correspond to the point
in which the plotted line has the greatest change in slope, showing
an elbow.
</p>


<h3>References</h3>

<p>Locantore, N., Marron, J.S., Simpson, D.G. et al. (1999) &quot;Robust principal
component analysis for functional data.&quot; Test 8, 1–73.
https://doi.org/10.1007/BF02595862
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pkbc">pkbc()</a></code> for the clustering algorithm <br />
<a href="#topic+pkbc-class">pkbc</a> for the class object definition.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- matrix(rnorm(300), ncol = 3)
pkbc_res &lt;- pkbc(dat, 3)
plot(pkbc_res, 3)

</code></pre>

<hr>
<h2 id='poisson_CV'>Compute the critical value for the Poisson KBQD tests for Uniformity</h2><span id='topic+poisson_CV'></span>

<h3>Description</h3>

<p>This function computes the empirical critical value for the U-statistics for
testing uniformity on the sphere based on the centered poisson kernel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>poisson_CV(d, size, rho, B, Quantile)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="poisson_CV_+3A_d">d</code></td>
<td>
<p>the dimension of generated samples.</p>
</td></tr>
<tr><td><code id="poisson_CV_+3A_size">size</code></td>
<td>
<p>the number of observations to be generated.</p>
</td></tr>
<tr><td><code id="poisson_CV_+3A_rho">rho</code></td>
<td>
<p>the concentration parameter for the Poisson kernel.</p>
</td></tr>
<tr><td><code id="poisson_CV_+3A_b">B</code></td>
<td>
<p>the number of replications.</p>
</td></tr>
<tr><td><code id="poisson_CV_+3A_quantile">Quantile</code></td>
<td>
<p>the quantile of the distribution use to select the critical
value.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each replication, a sample of d-dimensional observations from the uniform
distribution on the Sphere are generated and the Poisson kernel-based
U-statistic is computed. After B iterations, the critical value is selected
as the <code>Quantile</code> of the empirical distribution of the computed test
statistics.
</p>


<h3>Value</h3>

<p>the critical value for the specified dimension, size and level.
</p>


<h3>References</h3>

<p>Ding Yuxin, Markatou Marianthi, Saraceno Giovanni (2023). “Poisson
Kernel-Based Tests for Uniformity on the d-Dimensional Sphere.”
Statistica Sinica. doi: doi:10.5705/ss.202022.0347
</p>

<hr>
<h2 id='predict.pkbc'>Cluster spherical observations using a mixture of Poisson kernel-based
densities</h2><span id='topic+predict.pkbc'></span><span id='topic+predict+2Cpkbc-method'></span>

<h3>Description</h3>

<p>Obtain predictions of membership for spherical observations based on a
mixture of Poisson kernel-based densities estimated by <code>pkbc</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'pkbc'
predict(object, k, newdata = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.pkbc_+3A_object">object</code></td>
<td>
<p>Object of class <code>pkbc</code></p>
</td></tr>
<tr><td><code id="predict.pkbc_+3A_k">k</code></td>
<td>
<p>Number of clusters to be used.</p>
</td></tr>
<tr><td><code id="predict.pkbc_+3A_newdata">newdata</code></td>
<td>
<p>a data.frame or a matrix of the data. If missing the
clustering data obtained from the <code>pkbc</code> object are
classified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with the following components
</p>

<ul>
<li><p> Memb: vector of predicted memberships of <code>newdata</code>
</p>
</li>
<li><p> Probs: matrix where entry (i,j) denotes the probability that
observation i belongs to the k-th cluster.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+pkbc">pkbc()</a></code> for the clustering algorithm <br />
<a href="#topic+pkbc-class">pkbc</a> for the class object definition.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># generate data
dat &lt;- rbind(matrix(rnorm(100), ncol = 2), matrix(rnorm(100, 5), ncol = 2))
res &lt;- pkbc(dat, 2)

# extract membership of dat
predict(res, k = 2)
# predict membership of new data
newdat &lt;- rbind(matrix(rnorm(10), ncol = 2), matrix(rnorm(10, 5), ncol = 2))
predict(res, k = 2, newdat)
 
</code></pre>

<hr>
<h2 id='sample_hypersphere'>Generate random sample from the hypersphere</h2><span id='topic+sample_hypersphere'></span>

<h3>Description</h3>

<p>Generate a random sample from the uniform distribution on the hypersphere.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_hypersphere(d, n_points = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sample_hypersphere_+3A_d">d</code></td>
<td>
<p>Number of dimensions.</p>
</td></tr>
<tr><td><code id="sample_hypersphere_+3A_n_points">n_points</code></td>
<td>
<p>Number of sampled observations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data matrix with the sampled observations.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x_sp &lt;- sample_hypersphere(3,100)

</code></pre>

<hr>
<h2 id='select_h'>Select the value of the kernel tuning parameter</h2><span id='topic+select_h'></span>

<h3>Description</h3>

<p>This function computes the kernel bandwidth of the Gaussian kernel for the
normality, two-sample and k-sample kernel-based quadratic distance (KBQD)
tests.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>select_h(
  x,
  y = NULL,
  alternative = NULL,
  method = "subsampling",
  b = 0.8,
  B = 100,
  delta_dim = 1,
  delta = NULL,
  h_values = NULL,
  Nrep = 50,
  n_cores = 2,
  Quantile = 0.95,
  power.plot = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="select_h_+3A_x">x</code></td>
<td>
<p>Data set of observations from X.</p>
</td></tr>
<tr><td><code id="select_h_+3A_y">y</code></td>
<td>
<p>Numeric matrix or vector of data values. Depending on the input
<code>y</code>, the selection of h is performed for the corresponding
test.
</p>

<ul>
<li><p> if <code>y</code> = NULL, the function performs the tests for normality on
<code>x</code>.
</p>
</li>
<li><p> if <code>y</code> is a data matrix, with same dimensions of <code>x</code>, the
function performs the two-sample test between <code>x</code> and <code>y</code>.
</p>
</li>
<li><p> if <code>y</code> is a numeric or factor vector, indicating the group
memberships for each observation, the function performs the k-sample
test.
</p>
</li></ul>
</td></tr>
<tr><td><code id="select_h_+3A_alternative">alternative</code></td>
<td>
<p>Family of alternative chosen for selecting h, between
&quot;location&quot;, &quot;scale&quot; and &quot;skewness&quot;.</p>
</td></tr>
<tr><td><code id="select_h_+3A_method">method</code></td>
<td>
<p>The method used for critical value estimation
(&quot;subsampling&quot;, &quot;bootstrap&quot;, or &quot;permutation&quot;).</p>
</td></tr>
<tr><td><code id="select_h_+3A_b">b</code></td>
<td>
<p>The size of the subsamples used in the subsampling algorithm .</p>
</td></tr>
<tr><td><code id="select_h_+3A_b">B</code></td>
<td>
<p>The number of iterations to use for critical value estimation,
B = 150 as default.</p>
</td></tr>
<tr><td><code id="select_h_+3A_delta_dim">delta_dim</code></td>
<td>
<p>Vector of coefficient of alternative with respect to each
dimension</p>
</td></tr>
<tr><td><code id="select_h_+3A_delta">delta</code></td>
<td>
<p>Vector of parameter values indicating chosen alternatives</p>
</td></tr>
<tr><td><code id="select_h_+3A_h_values">h_values</code></td>
<td>
<p>Values of the tuning parameter used for the selection</p>
</td></tr>
<tr><td><code id="select_h_+3A_nrep">Nrep</code></td>
<td>
<p>Number of bootstrap/permutation/subsampling replications.</p>
</td></tr>
<tr><td><code id="select_h_+3A_n_cores">n_cores</code></td>
<td>
<p>Number of cores used to parallel the h selection algorithm.
If this is not provided, the function will detect the
available cores.</p>
</td></tr>
<tr><td><code id="select_h_+3A_quantile">Quantile</code></td>
<td>
<p>The quantile to use for critical value estimation, 0.95 is
the default value.</p>
</td></tr>
<tr><td><code id="select_h_+3A_power.plot">power.plot</code></td>
<td>
<p>Logical. If TRUE, it is displayed the plot of power for
values in h_values and delta.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function performs the selection of the optimal value for the tuning
parameter <code class="reqn">h</code> of the normal kernel function, for normality test, the
two-sample and k-sample KBQD tests. It performs a small simulation study,
generating samples according to the family of <code>alternative</code> specified,
for the chosen values of <code>h_values</code> and <code>delta</code>.
</p>
<p>We consider target alternatives <code class="reqn">F_\delta(\hat{\mathbf{\mu}},
\hat{\mathbf{\Sigma}}, \hat{\mathbf{\lambda}})</code>, where
<code class="reqn">\hat{\mathbf{\mu}}, \hat{\mathbf{\Sigma}}</code> and
<code class="reqn">\hat{\mathbf{\lambda}}</code> indicate the location,
covariance and skewness parameter estimates from the pooled sample.
</p>

<ul>
<li><p> Compute the estimates of the mean <code class="reqn">\hat{\mu}</code>, covariance matrix
<code class="reqn">\hat{\Sigma}</code> and skewness <code class="reqn">\hat{\lambda}</code> from the pooled sample.
</p>
</li>
<li><p> Choose the family of alternatives <code class="reqn">F_\delta = F_\delta(\hat{\mu}
,\hat{\Sigma}, \hat{\lambda})</code>. <br /> <br />
<em>For each value of <code class="reqn">\delta</code> and <code class="reqn">h</code>:</em>
</p>
</li>
<li><p> Generate <code class="reqn">\mathbf{X}_1,\ldots,\mathbf{X}_{k-1}  \sim F_0</code>, for
<code class="reqn">\delta=0</code>;
</p>
</li>
<li><p> Generate <code class="reqn">\mathbf{X}_k \sim F_\delta</code>;
</p>
</li>
<li><p> Compute the <code class="reqn">k</code>-sample test statistic between <code class="reqn">\mathbf{X}_1, 
\mathbf{X}_2, \ldots, \mathbf{X}_k</code> with kernel parameter <code class="reqn">h</code>;
</p>
</li>
<li><p> Compute the power of the test. If it is greater than 0.5,
select <code class="reqn">h</code> as optimal value.
</p>
</li>
<li><p> If an optimal value has not been selected, choose the <code class="reqn">h</code> which
corresponds to maximum power.
</p>
</li></ul>

<p>The available <code>alternative</code> are <br />
<em>location</em> alternatives, <code class="reqn">F_\delta = 
SN_d(\hat{\mu} + \delta,\hat{\Sigma}, \hat{\lambda})</code>,with
<code class="reqn">\delta = 0.2, 0.3, 0.4</code>; <br />
<em>scale</em> alternatives,
<code class="reqn">F_\delta = SN_d(\hat{\mu} ,\hat{\Sigma}*\delta, \hat{\lambda})</code>,
<code class="reqn">\delta = 0.1, 0.3, 0.5</code>; <br />
<em>skewness</em> alternatives,
<code class="reqn">F_\delta = SN_d(\hat{\mu} ,\hat{\Sigma}, \hat{\lambda} + \delta)</code>,
with <code class="reqn">\delta = 0.2, 0.3, 0.6</code>. <br />
The values of <code class="reqn">h = 0.6, 1, 1.4, 1.8, 2.2</code> and <code class="reqn">N=50</code> are set as
default values. <br />
The function <code>select_h()</code> allows the user to
set the values of <code class="reqn">\delta</code> and <code class="reqn">h</code> for a more extensive grid search.
We suggest to set a more extensive grid search when computational resources
permit.
</p>


<h3>Value</h3>

<p>A list with the following attributes:
</p>

<ul>
<li> <p><code>h_sel</code> the selected value of tuning parameter h;
</p>
</li>
<li> <p><code>power</code> matrix of power values computed for the considered
values of <code>delta</code> and <code>h_values</code>;
</p>
</li>
<li> <p><code>power.plot</code> power plots (if <code>power.plot</code> is <code>TRUE</code>).
</p>
</li></ul>



<h3>Note</h3>

<p>Please be aware that the <code>select_h()</code> function may take a significant
amount of time to run, especially with larger datasets or when using an
larger number of parameters in <code>h_values</code> and <code>delta</code>. Consider
this when applying the function to large or complex data.
</p>


<h3>References</h3>

<p>Markatou, M. and Saraceno, G. (2024). “A Unified Framework for
Multivariate Two- and k-Sample Kernel-based Quadratic Distance
Goodness-of-Fit Tests.” <br />
https://doi.org/10.48550/arXiv.2407.16374
</p>
<p>Saraceno, G., Markatou, M., Mukhopadhyay, R. and Golzy, M. (2024).
Goodness-of-Fit and Clustering of Spherical Data: the QuadratiK package
in R and Python. <br />
https://arxiv.org/abs/2402.02290.
</p>


<h3>See Also</h3>

<p>The function <code>select_h</code> is used in the <code><a href="#topic+kb.test">kb.test()</a></code> function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Select the value of h using the mid-power algorithm

x &lt;- matrix(rnorm(100), ncol = 2)
y &lt;- matrix(rnorm(100), ncol = 2)
h_sel &lt;- select_h(x, y, "skewness")
h_sel


</code></pre>

<hr>
<h2 id='stats_clusters'>Descriptive statistics for the clusters identified by the Poisson
kernel-based clustering.</h2><span id='topic+stats_clusters'></span><span id='topic+stats_clusters+2Cpkbc-method'></span>

<h3>Description</h3>

<p>Method for objects of class <code>pkbc</code> which computes some
descriptive for each variable with respect to the detected groups.
</p>
<p>Method for objects of class <code>pkbc</code> which computes descriptive
statistics for each variable with respect to the detected groups.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stats_clusters(object, ...)

## S4 method for signature 'pkbc'
stats_clusters(object, k)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stats_clusters_+3A_object">object</code></td>
<td>
<p>Object of class <code>pkbc</code>.</p>
</td></tr>
<tr><td><code id="stats_clusters_+3A_...">...</code></td>
<td>
<p>possible additional inputs</p>
</td></tr>
<tr><td><code id="stats_clusters_+3A_k">k</code></td>
<td>
<p>Number of clusters to be used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function computes mean, standard deviation, median,
inter-quantile range, minimum and maximum for each variable in the data set
given the final membership assigned by the clustering algorithm.
</p>


<h3>Value</h3>

<p>List with computed descriptive statistics for each dimension.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pkbc">pkbc()</a></code> for the clustering algorithm <br />
<a href="#topic+pkbc-class">pkbc</a> for the class object definition.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#We generate three samples of 100 observations from 3-dimensional
#Poisson kernel-based densities with rho=0.8 and different mean directions
dat&lt;-matrix(rnorm(300),ncol=3)

#Perform the clustering algorithm
pkbc_res&lt;- pkbc(dat, 3)
stats_clusters(pkbc_res, 3)


</code></pre>

<hr>
<h2 id='summary.kb.test'>Summarizing kernel-based quadratic distance results</h2><span id='topic+summary.kb.test'></span><span id='topic+summary+2Ckb.test-method'></span>

<h3>Description</h3>

<p><code>summary</code> method for the class <code>kb.test</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'kb.test'
summary(object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.kb.test_+3A_object">object</code></td>
<td>
<p>Object of class <code>kb.test</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with the following components:
</p>

<ul>
<li> <p><code>summary_tables</code> Table of computed descriptive statistics per
variable (and per group if available).
</p>
</li>
<li> <p><code>test_results</code> Data frame with the results of the performed
kernel-based quadratic distance test.
</p>
</li>
<li> <p><code>qqplots</code> Figure with qq-plots for each variable.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+kb.test">kb.test()</a></code> and <a href="#topic+kb.test-class">kb.test</a> for more details.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create a kb.test object
x &lt;- matrix(rnorm(100),ncol=2)
# Normality test
my_test &lt;- kb.test(x, h=0.5)
summary(my_test)

</code></pre>

<hr>
<h2 id='summary.pk.test'>Summarizing kernel-based quadratic distance results</h2><span id='topic+summary.pk.test'></span><span id='topic+summary+2Cpk.test-method'></span>

<h3>Description</h3>

<p><code>summary</code> method for the class <code>pk.test</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'pk.test'
summary(object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.pk.test_+3A_object">object</code></td>
<td>
<p>Object of class <code>pk.test</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with the following components:
</p>

<ul>
<li> <p><code>summary_tables</code> Table of computed descriptive statistics per
variable.
</p>
</li>
<li> <p><code>test_results</code> Data frame with the results of the performed
Poisson kernel-based test.
</p>
</li>
<li> <p><code>qqplots</code> Figure with qq-plots for each variable against the
uniform distribution.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+pk.test">pk.test()</a></code> and <a href="#topic+pk.test-class">pk.test</a> for additional details.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create a pk.test object
x_sp &lt;- sample_hypersphere(3, n_points=100)
unif_test &lt;- pk.test(x_sp,rho=0.8)
summary(unif_test)

</code></pre>

<hr>
<h2 id='summary.pkbc'>Summarizing PKBD mixture Fits</h2><span id='topic+summary.pkbc'></span><span id='topic+summary+2Cpkbc-method'></span>

<h3>Description</h3>

<p>Summary method for class &quot;pkbc&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'pkbc'
summary(object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.pkbc_+3A_object">object</code></td>
<td>
<p>Object of class <code>pkbc</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Display the logLikelihood values and within cluster sum of squares
(wcss) for all the values of number of clusters provided. For each of
these values the estimated mixing proportions are showed together
with a table with the assigned memberships.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pkbc">pkbc()</a></code> for the clustering algorithm <br />
<a href="#topic+pkbc-class">pkbc</a> for the class object definition.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- rbind(matrix(rnorm(100), 2), matrix(rnorm(100, 5), 2))
res &lt;- pkbc(dat, 2:4)
summary(res)

</code></pre>

<hr>
<h2 id='var_k'>Exact variance of k-sample test</h2><span id='topic+var_k'></span>

<h3>Description</h3>

<p>Compute the exact variance of kernel test for the k-sample problem under
the null hypothesis that F1=...=Fk.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var_k(Kcen, sizes, cum_size)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="var_k_+3A_kcen">Kcen</code></td>
<td>
<p>the matrix with centered kernel values</p>
</td></tr>
<tr><td><code id="var_k_+3A_sizes">sizes</code></td>
<td>
<p>vector indicating sample's size.</p>
</td></tr>
<tr><td><code id="var_k_+3A_cum_size">cum_size</code></td>
<td>
<p>vector indicating sample's cumulative sizes.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the value of computed variance.
</p>

<hr>
<h2 id='var_norm'>Exact variance of normality test</h2><span id='topic+var_norm'></span>

<h3>Description</h3>

<p>Compute the exact variance of kernel test for normality under the null
hypothesis that G=N(0,I).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var_norm(Sigma_h, V, n)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="var_norm_+3A_sigma_h">Sigma_h</code></td>
<td>
<p>covariance matrix of the gaussian kernel</p>
</td></tr>
<tr><td><code id="var_norm_+3A_v">V</code></td>
<td>
<p>Covariance matrix of the tested distribution G</p>
</td></tr>
<tr><td><code id="var_norm_+3A_n">n</code></td>
<td>
<p>sample size</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the value of computed variance
</p>

<hr>
<h2 id='var_two'>Exact variance of two-sample test</h2><span id='topic+var_two'></span>

<h3>Description</h3>

<p>Compute the exact variance of kernel test for the two-sample problem under
the null hypothesis that F=G.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var_two(Kcen, nsamples)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="var_two_+3A_kcen">Kcen</code></td>
<td>
<p>the matrix with centered kernel values</p>
</td></tr>
<tr><td><code id="var_two_+3A_nsamples">nsamples</code></td>
<td>
<p>vector indicating sample's membership.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the value of computed variance.
</p>

<hr>
<h2 id='wine'>Wine data set</h2><span id='topic+wine'></span>

<h3>Description</h3>

<p>The <code>wine</code> data frame has 178 rows and 14 columns. The first 13
variables report 13 constituents found in each of the three types of wines.
The last column indicates the class labels (1,2 or 3).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wine
</code></pre>


<h3>Format</h3>

<p>A data frame containing the following columns:
</p>

<ul>
<li> <p><code>Alcohol</code>
</p>
</li>
<li> <p><code>Malic acid</code>
</p>
</li>
<li> <p><code>Ash</code>
</p>
</li>
<li> <p><code>Alcalinity of ash</code>
</p>
</li>
<li> <p><code>Magnesium</code>
</p>
</li>
<li> <p><code>Total phenols</code>
</p>
</li>
<li> <p><code>Flavanoids</code>
</p>
</li>
<li> <p><code>Nonflavanoid phenols</code>
</p>
</li>
<li> <p><code>Proanthocyanins</code>
</p>
</li>
<li> <p><code>Color intensity</code>
</p>
</li>
<li> <p><code>Hue</code>
</p>
</li>
<li> <p><code>OD280/OD315 of diluted wines</code>
</p>
</li>
<li> <p><code>Proline</code>
</p>
</li>
<li> <p><code>y</code>: class membership
</p>
</li></ul>



<h3>Details</h3>

<p>These data are the results of a chemical analysis of wines grown in the same
region in Italy but derived from three different cultivars. The analysis
determined the quantities of 13 constituents found in each of the three types
of wines.
</p>


<h3>Source</h3>

<p>Aeberhard, S. and Forina, M. (1991). Wine.
UCI Machine Learning Repository.
https://doi.org/10.24432/C5PC7J.
</p>


<h3>References</h3>

<p>Aeberhard, S., Coomans, D. and De Vel, O. (1994). Comparative analysis of
statistical pattern recognition methods in high dimensional settings.
Pattern Recognition, 27(8), 1065-1077.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wine)
summary(wine)

</code></pre>

<hr>
<h2 id='wireless'>Wireless Indoor Localization</h2><span id='topic+wireless'></span>

<h3>Description</h3>

<p>The <code>wireless</code> data frame has 2000 rows and 8 columns. The first 7
variables report the measurements of the Wi-Fi signal strength received from
7 Wi-Fi routers in an office location in Pittsburgh (USA). The last column
indicates the class labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wireless
</code></pre>


<h3>Format</h3>

<p>A data frame containing the following columns:
</p>

<ul>
<li> <p><code>V1</code> Signal strength from router 1.
</p>
</li>
<li> <p><code>V2</code> Signal strength from router 2.
</p>
</li>
<li> <p><code>V3</code> Signal strength from router 3.
</p>
</li>
<li> <p><code>V4</code> Signal strength from router 4.
</p>
</li>
<li> <p><code>V5</code> Signal strength from router 5.
</p>
</li>
<li> <p><code>V6</code> Signal strength from router 6.
</p>
</li>
<li> <p><code>V7</code> Signal strength from router 7.
</p>
</li>
<li> <p><code>V8</code> Group memberships, from 1 to 4.
</p>
</li></ul>



<h3>Details</h3>

<p>The Wi-Fi signal strength is measured in dBm, decibel milliwatts, which is
expressed as a negative value ranging from -100 to 0.
The labels correspond to 4 different rooms.
In total, we have 4 groups with 500 observations each.
</p>


<h3>Source</h3>

<p>Bhatt, R. (2017). Wireless Indoor Localization.
UCI Machine Learning Repository. <br />
https://doi.org/10.24432/C51880.
</p>


<h3>References</h3>

<p>Rohra, J.G., Perumal, B., Narayanan, S.J., Thakur, P. and Bhatt, R.B. (2017).
&quot;User Localization in an Indoor Environment Using Fuzzy Hybrid of Particle
Swarm Optimization &amp; Gravitational Search Algorithm with Neural Networks&quot;.
In: Deep, K., et al. Proceedings of Sixth International Conference on Soft
Computing for Problem Solving. Advances in Intelligent Systems and Computing,
vol 546. Springer, Singapore. https://doi.org/10.1007/978-981-10-3322-3_27
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(wireless)
summary(wireless)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
