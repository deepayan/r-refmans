<!DOCTYPE html><html><head><title>Help for package DiceKriging</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {DiceKriging}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#branin'><p> 2D test function</p></a></li>
<li><a href='#camelback'><p> 2D test function</p></a></li>
<li><a href='#checkNames'><p>Consistency test between the column names of two matrices</p></a></li>
<li><a href='#coef'><p> Get coefficients values</p></a></li>
<li><a href='#computeAuxVariables'><p> Auxiliary variables for kriging</p></a></li>
<li><a href='#covIso-class'><p> Class of tensor-product spatial covariances with isotropic range</p></a></li>
<li><a href='#covKernel-class'><p>Class &quot;covKernel&quot;</p></a></li>
<li><a href='#covMat1Mat2'><p> Cross covariance matrix</p></a></li>
<li><a href='#covMatrix'><p> Covariance matrix</p></a></li>
<li><a href='#covMatrixDerivative'><p> Covariance matrix derivatives</p></a></li>
<li><a href='#covparam2vect'><p> Auxiliary function</p></a></li>
<li><a href='#covParametersBounds'><p> Boundaries for covariance parameters</p></a></li>
<li><a href='#covScaling-class'><p>Class &quot;covScaling&quot;</p></a></li>
<li><a href='#covStruct.create'><p> Spatial covariance - Class constructor</p></a></li>
<li><a href='#covTensorProduct-class'><p> Class of tensor-product spatial covariances</p></a></li>
<li><a href='#covUser-class'><p>Class &quot;covUser&quot;</p></a></li>
<li><a href='#covVector.dx'><p> Spatial covariance - Derivatives</p></a></li>
<li><a href='#cv'><p>Multiple fold cross validation for a km object</p></a></li>
<li><a href='#DiceKriging-package'><p> Kriging Methods for Computer Experiments</p></a></li>
<li><a href='#drop.response'><p> Trend model formula operation</p></a></li>
<li><a href='#goldsteinPrice'><p> 2D test function</p></a></li>
<li><a href='#hartman3'><p> 3D test function</p></a></li>
<li><a href='#hartman6'><p> 6D test function</p></a></li>
<li><a href='#inputnames'><p>Get the input variables names</p></a></li>
<li><a href='#kernelname'><p>Get the kernel name</p></a></li>
<li><a href='#km'><p> Fit and/or create kriging models</p></a></li>
<li><a href='#km-class'><p> Kriging models class</p></a></li>
<li><a href='#km1Nugget.init'><p> Fitting Kriging Models</p></a></li>
<li><a href='#kmData'><p> Fit and/or create kriging models</p></a></li>
<li><a href='#kmEstimate'><p> Fitting Kriging Models</p></a></li>
<li><a href='#kmNoNugget.init'><p> Fitting Kriging Models</p></a></li>
<li><a href='#kmNuggets.init'><p> Fitting Kriging Models</p></a></li>
<li><a href='#leaveOneOut.km'><p> Leave-one-out for a km object</p></a></li>
<li><a href='#leaveOneOutFun'>
<p>Leave-one-out least square criterion of a km object</p></a></li>
<li><a href='#leaveOneOutGrad'>
<p>Leave-one-out least square criterion - Analytical gradient</p></a></li>
<li><a href='#logLik'><p> log-likelihood of a km object</p></a></li>
<li><a href='#logLikFun'><p> Concentrated log-likelihood of a km object</p></a></li>
<li><a href='#logLikGrad'><p> Concentrated log-Likelihood of a km object - Analytical gradient</p></a></li>
<li><a href='#ninput'><p>Get the spatial dimension</p></a></li>
<li><a href='#nuggetflag'><p>Get the nugget flag</p></a></li>
<li><a href='#nuggetvalue'><p>Get or set the nugget value</p></a></li>
<li><a href='#plot'><p>Diagnostic plot for the validation of a km object</p></a></li>
<li><a href='#predict'><p> Predict values and confidence intervals at newdata for a km object</p></a></li>
<li><a href='#SCAD'><p> Penalty function</p></a></li>
<li><a href='#SCAD.derivative'><p> Penalty function derivative</p></a></li>
<li><a href='#scalingFun'>
<p>Scaling function</p></a></li>
<li><a href='#scalingFun1d'>
<p>Scaling 1-dimensional function</p></a></li>
<li><a href='#scalingGrad'>
<p>Gradient of the dimensional Scaling function</p></a></li>
<li><a href='#show'><p> Print values of a km object</p></a></li>
<li><a href='#simulate'><p> Simulate GP values at any given set of points for a  km object</p></a></li>
<li><a href='#trend.deltax'>
<p>Trend derivatives</p></a></li>
<li><a href='#trendMatrix.update'><p> Trend model matrix operation</p></a></li>
<li><a href='#update'><p>Update of a kriging model</p></a></li>
<li><a href='#vect2covparam'><p> Auxiliary function</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Kriging Methods for Computer Experiments</td>
</tr>
<tr>
<td>Version:</td>
<td>1.6.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-02-23</td>
</tr>
<tr>
<td>Author:</td>
<td>Olivier Roustant, David Ginsbourger, Yves Deville. Contributors: Clement Chevalier, Yann Richet.</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Olivier Roustant &lt;roustant@insa-toulouse.fr&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Estimation, validation and prediction of kriging models. Important functions : km, print.km, plot.km, predict.km.</td>
</tr>
<tr>
<td>Depends:</td>
<td>methods</td>
</tr>
<tr>
<td>Suggests:</td>
<td>rgenoud (&ge; 5.8-2.0), foreach, doParallel, testthat, numDeriv</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://dicekrigingclub.github.io/www/">https://dicekrigingclub.github.io/www/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-02-23 17:02:18 UTC; yves</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-02-23 17:30:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='branin'> 2D test function</h2><span id='topic+branin'></span>

<h3>Description</h3>

<p>Branin-Hoo 2-dimensional test function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>branin(x)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="branin_+3A_x">x</code></td>
<td>
<p> a 2-dimensional vector specifying the location where the function is to be evaluated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Branin-Hoo function is defined here over [0,1] x [0,1], instead of [-5,0] x [10,15] as usual. 
It has 3 global minima : 
x1 = c(0.9616520, 0.15); x2 = c(0.1238946, 0.8166644); x3 = c(0.5427730, 0.15)
</p>


<h3>Value</h3>

<p>A real number equal to the Branin-Hoo function values at <code>x</code>
</p>


<h3>Author(s)</h3>

<p> D. Ginsbourger, Ecole des Mines de St-Etienne. </p>


<h3>Examples</h3>

<pre><code class='language-R'> 
n.grid &lt;- 20
x.grid &lt;- y.grid &lt;- seq(0,1,length=n.grid)
design.grid &lt;- expand.grid(x.grid, y.grid)
response.grid &lt;- apply(design.grid, 1, branin)
z.grid &lt;- matrix(response.grid, n.grid, n.grid)
contour(x.grid,y.grid,z.grid,40)
x1 = c(0.9616520, 0.15); x2 = c(0.1238946, 0.8166644); x3 = c(0.5427730, 0.15)
points(rbind(t(x1), t(x2), t(x3)), pch=19, col="red")
title("Fonction de Branin")
</code></pre>

<hr>
<h2 id='camelback'> 2D test function</h2><span id='topic+camelback'></span>

<h3>Description</h3>

<p>Camelback 2-dimensional test function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>camelback(x)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="camelback_+3A_x">x</code></td>
<td>
<p> a 2-dimensional vector specifying the location where the function is to be evaluated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Camelback function is usually defined over the domain [-3,-2] x [3, 2]. 
Here, the function is adapted to the domain [0,1] x [0,1]. It has 2 global minima : 
x1 = c(0.5149730,0.3218374); x2 = c(0.4850263,0.6781641)
</p>


<h3>Value</h3>

<p>A real number equal to the Camelback function values at <code>x</code>
</p>


<h3>Author(s)</h3>

<p> D. Ginsbourger, Ecole des Mines de St-Etienne. </p>


<h3>Examples</h3>

<pre><code class='language-R'> 
n.grid &lt;- 20
x.grid &lt;- y.grid &lt;- seq(0,1,length=n.grid)
design.grid &lt;- expand.grid(x.grid, y.grid)
response.grid &lt;- apply(design.grid, 1, camelback)
z.grid &lt;- matrix(response.grid, n.grid, n.grid)
contour(x.grid,y.grid,z.grid,20)
x1 = c(0.5149730,0.3218374); x2 = c(0.4850263,0.6781641)
points(rbind(t(x1), t(x2)), pch=19, col="red")
title("Fonction Camelback")
</code></pre>

<hr>
<h2 id='checkNames'>Consistency test between the column names of two matrices
</h2><span id='topic+checkNames'></span><span id='topic+checkNamesList'></span>

<h3>Description</h3>

<p>Tests if the names of a second matrix are equal to a given matrix up to a permutation, and permute its columns accordingly. When the second one has no column names, the names of the first one are used in the same order.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>checkNames(X1, X2, X1.name = "X1", X2.name = "X2")
checkNamesList(X1, l2, X1.name = "X1", l2.name = "l2")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="checkNames_+3A_x1">X1</code></td>
<td>
<p>a matrix containing column names.</p>
</td></tr>
<tr><td><code id="checkNames_+3A_x2">X2</code></td>
<td>
<p>a matrix containing the same number of columns.</p>
</td></tr>
<tr><td><code id="checkNames_+3A_l2">l2</code></td>
<td>
<p>a list with length <code>ncol(X1)</code>.</p>
</td></tr>
<tr><td><code id="checkNames_+3A_x1.name">X1.name</code></td>
<td>
<p>, </p>
</td></tr>
<tr><td><code id="checkNames_+3A_x2.name">X2.name</code></td>
<td>
<p>optional names for the matrix <code>X1</code> and <code>X2</code> theirselves (useful for error messages).</p>
</td></tr>
<tr><td><code id="checkNames_+3A_l2.name">l2.name</code></td>
<td>
<p>optional names for <code>l2</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>X2</code> does not contain variable names, then the names of <code>X1</code> are used in the same order, and <code>X2</code> is returned with these names. Otherwise, if the column names of <code>X1</code> and <code>X2</code> are equal up to a permutation, the column of <code>X2</code> are permuted according to the order of <code>X1</code>' names.
</p>


<h3>Value</h3>

<p>The matrix <code>X2</code>, with columns possibly permuted. See details.
</p>


<h3>Author(s)</h3>

<p>O. Roustant</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict+2Ckm-method">predict,km-method</a></code>, <code><a href="#topic+simulate+2Ckm-method">simulate,km-method</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X1 &lt;- matrix(1, 2, 3)
X2 &lt;- matrix(1:6, 2, 3)

colnames(X1) &lt;- c("x1", "x2", "x3")
checkNames(X1, X2)
# attributes the same names for X2, and returns X2

colnames(X2) &lt;- c("x1", "x2", "x5")
## Not run: checkNames(X1, X2)
# returns an error since the names of X1 and X2 are different

colnames(X2) &lt;- c("x2", "x1", "x3")
checkNames(X1, X2)
# returns the matrix X2, but with permuted columns

l2 &lt;- list(x3 = 1, x2 = c(2, 3), x1 = -6)
checkNamesList(X1, l2)

</code></pre>

<hr>
<h2 id='coef'> Get coefficients values </h2><span id='topic+coef'></span>

<h3>Description</h3>

<p> Get or set coefficients values.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  coef(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef_+3A_object">object</code></td>
<td>
<p>an object specifying a covariance structure or a km object.</p>
</td></tr>
<tr><td><code id="coef_+3A_...">...</code></td>
<td>
<p>other arguments (undocumented at this stage).</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The replacement method <code>coef&lt;-</code> is not available.</p>


<h3>Author(s)</h3>

<p>Y. Deville, O. Roustant</p>

<hr>
<h2 id='computeAuxVariables'> Auxiliary variables for kriging </h2><span id='topic+computeAuxVariables'></span>

<h3>Description</h3>

<p>Computes or updates some auxiliary variables used for kriging (see below). This is useful in several situations : when all parameters are known (as for one basic step in Bayesian analysis), or when some new data is added but one does not want to re-estimate the model coefficients. On the other hand, <code>computeAuxVariables</code> is not used during the estimation of covariance parameters, since this function requires to compute the trend coefficients at each optimization step; the alternative given by (Park, Baek, 2001) is preferred.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>computeAuxVariables(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="computeAuxVariables_+3A_model">model</code></td>
<td>
<p> an object of class <code>km</code> with missing (or non updated) items. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>An updated <code>km</code> objet, where the changes concern the following items:
</p>
<table>
<tr><td><code>T</code></td>
<td>
<p> a matrix equal to the upper triangular factor of the Choleski decomposition of <code>C</code>, such that <code>t(T)*T = C</code> (where C is the covariance matrix). </p>
</td></tr>
<tr><td><code>z</code></td>
<td>
<p> a vector equal to <code>inv(t(T))*(y - F*beta)</code>, with <code>y</code>, <code>F</code>, <code>beta</code> are respectively the response, the experimental matrix and the trend coefficients specified in <code>model@trend.coef</code>. If <code>model@trend.coef</code> is empty, <code>z</code> is not computed.</p>
</td></tr>
<tr><td><code>M</code></td>
<td>
<p> a matrix equal to <code>inv(t(T))*F</code>. </p>
</td></tr>
</table>


<h3>Note</h3>

<p><code>T</code> is computed with the base function <code>chol</code>. <code>z</code> and <code>M</code> are computed by solving triangular linear systems with <code>backsolve</code>. <code>z</code> is not computed if <code>model@trend.coef</code> is empty.
</p>


<h3>Author(s)</h3>

<p>O. Roustant, D. Ginsbourger, Ecole des Mines de St-Etienne</p>


<h3>References</h3>

 
<p>J.-S. Park and J. Baek (2001), Efficient computation of maximum likelihood estimators in a spatial linear model with power exponential covariogram, <em>Computer Geosciences</em>, <b>27</b> no. 1, 1-7. 
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+covMatrix">covMatrix</a></code>, <code><a href="base.html#topic+chol">chol</a></code>, <code><a href="base.html#topic+backsolve">backsolve</a></code>. </p>

<hr>
<h2 id='covIso-class'> Class of tensor-product spatial covariances with isotropic range</h2><span id='topic+covIso-class'></span><span id='topic+coef+2CcovIso-method'></span><span id='topic+covMat1Mat2+2CcovIso-method'></span><span id='topic+covMatrix+2CcovIso-method'></span><span id='topic+covMatrixDerivative+2CcovIso-method'></span><span id='topic+covParametersBounds+2CcovIso-method'></span><span id='topic+covparam2vect+2CcovIso-method'></span><span id='topic+vect2covparam+2CcovIso-method'></span><span id='topic+covVector.dx+2CcovIso-method'></span><span id='topic+inputnames+2CcovIso-method'></span><span id='topic+kernelname+2CcovIso-method'></span><span id='topic+ninput+2CcovIso-method'></span><span id='topic+nuggetflag+2CcovIso-method'></span><span id='topic+nuggetvalue+2CcovIso-method'></span><span id='topic+nuggetvalue+3C-+2CcovIso+2Cnumeric-method'></span><span id='topic+show+2CcovIso-method'></span><span id='topic+summary+2CcovIso-method'></span>

<h3>Description</h3>

<p> S4 class of isotropic spatial covariance kerlnes based upon the covTensorProduct class</p>


<h3>Objects from the Class</h3>

<p>In 1-dimension, the covariance kernels are parameterized as in (Rasmussen, Williams, 2006). Denote by <code>theta</code> the range parameter, <code>p</code> the exponent parameter (for power-exponential covariance), <code>s</code> the standard deviation, and <code>h=||x-y||</code>. Then we have <code>C(x,y) = s^2 * k(x,y)</code>, with:
</p>

<table>
<tr>
 <td style="text-align: left;">
Gauss </td><td style="text-align: left;"> <code> k(x,y) = exp(-1/2*(h/theta)^2) </code> </td>
</tr>
<tr>
 <td style="text-align: left;">
Exponential </td><td style="text-align: left;"> <code> k(x,y) = exp(-h/theta) </code> </td>
</tr>
<tr>
 <td style="text-align: left;">
Matern(3/2) </td><td style="text-align: left;"> <code> k(x,y) = (1+sqrt(3)*h/theta)*exp(-sqrt(3)*h/theta) </code> </td>
</tr>
<tr>
 <td style="text-align: left;">
Matern(5/2) </td><td style="text-align: left;"> <code> k(x,y) = (1+sqrt(5)*h/theta+(1/3)*5*(h/theta)^2)</code> </td>
</tr>
<tr>
 <td style="text-align: left;"> 
</td><td style="text-align: left;"> <code>          *exp(-sqrt(5)*h/theta)</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
Power-exponential </td><td style="text-align: left;"> <code> k(x,y) = exp(-(h/theta)^p) </code> </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Slots</h3>


<dl>
<dt><code>d</code>:</dt><dd><p>Object of class <code>"integer"</code>. The spatial dimension. </p>
</dd>
<dt><code>name</code>:</dt><dd><p>Object of class <code>"character"</code>. The covariance function name. To be chosen between <code>"gauss", "matern5_2", "matern3_2", "exp"</code>, and <code>"powexp"</code> </p>
</dd>
<dt><code>paramset.n</code>:</dt><dd><p>Object of class <code>"integer"</code>. 1 for covariance depending only on the ranges parameters, 2 for &quot;powexp&quot; which also depends on exponent parameters. </p>
</dd>
<dt><code>var.names</code>:</dt><dd><p>Object of class <code>"character"</code>. The variable names. </p>
</dd>
<dt><code>sd2</code>:</dt><dd><p>Object of class <code>"numeric"</code>. The variance of the stationary part of the process. </p>
</dd>
<dt><code>known.covparam</code>:</dt><dd><p>Object of class <code>"character"</code>. Internal use. One of: &quot;None&quot;, &quot;All&quot;. </p>
</dd>
<dt><code>nugget.flag</code>:</dt><dd><p>Object of class <code>"logical"</code>. Is there a nugget effect? </p>
</dd>
<dt><code>nugget.estim</code>:</dt><dd><p>Object of class <code>"logical"</code>. Is the nugget effect estimated or known? </p>
</dd>
<dt><code>nugget</code>:</dt><dd><p>Object of class <code>"numeric"</code>. If there is a nugget effect, its value (homogeneous to a variance). </p>
</dd>
<dt><code>param.n</code>:</dt><dd><p>Object of class <code>"integer"</code>. The total number of parameters. </p>
</dd>
<dt><code>range.names</code>:</dt><dd><p>Object of class <code>"character"</code>. Names of range parameters, for printing purpose. Default is &quot;theta&quot;. </p>
</dd>
<dt><code>range.val</code>:</dt><dd><p>Object of class <code>"numeric"</code>. Values of range parameters. </p>
</dd>
</dl>



<h3>Extends</h3>

<p>Class <code>"<a href="#topic+covKernel-class">covKernel</a>"</code>, directly.
</p>


<h3>Methods</h3>


<dl>
<dt>coef</dt><dd><p><code>signature(object = "covIso")</code>: ... </p>
</dd>
<dt>covMat1Mat2</dt><dd><p><code>signature(object = "covIso")</code>: ... </p>
</dd>
<dt>covMatrix</dt><dd><p><code>signature(object = "covIso")</code>: ... </p>
</dd>
<dt>covMatrixDerivative</dt><dd><p><code>signature(object = "covIso")</code>: ... </p>
</dd>
<dt>covParametersBounds</dt><dd><p><code>signature(object = "covIso")</code>: ... </p>
</dd>
<dt>covparam2vect</dt><dd><p><code>signature(object = "covIso")</code>: ... </p>
</dd>  
<dt>vect2covparam</dt><dd><p><code>signature(object = "covIso")</code>: ... </p>
</dd>  
<dt>covVector.dx</dt><dd><p><code>signature(object = "covIso")</code>: ... </p>
</dd>
<dt>inputnames</dt><dd><p><code>signature(x = "covIso")</code>: ... </p>
</dd>
<dt>kernelname</dt><dd><p><code>signature(x = "covIso")</code>: ... </p>
</dd>
<dt>ninput</dt><dd><p><code>signature(x = "covIso")</code>: ... </p>
</dd>
<dt>nuggetflag</dt><dd><p><code>signature(x = "covIso")</code>: ... </p>
</dd>
<dt>nuggetvalue</dt><dd><p><code>signature(x = "covIso")</code>: ... </p>
</dd>
<dt>show</dt><dd><p><code>signature(object = "covIso")</code>: ... </p>
</dd>
<dt>summary</dt><dd><p><code>signature(object = "covIso")</code>: ... </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> O. Roustant, D. Ginsbourger</p>


<h3>References</h3>

<p>N.A.C. Cressie (1993), <em>Statistics for spatial data</em>, Wiley series in probability and mathematical statistics.
</p>
<p>C.E. Rasmussen and C.K.I. Williams (2006), <em>Gaussian Processes for Machine Learning</em>, the MIT Press, <a href="http://www.gaussianprocess.org/gpml/">http://www.gaussianprocess.org/gpml/</a>
</p>
<p>M.L. Stein (1999), <em>Interpolation of spatial data, some theory for kriging</em>, Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+km">km</a></code>
<code><a href="#topic+covTensorProduct-class">covTensorProduct</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("covIso")
</code></pre>

<hr>
<h2 id='covKernel-class'>Class &quot;covKernel&quot; </h2><span id='topic+covKernel-class'></span>

<h3>Description</h3>

<p> Union of classes including &quot;covTensorProduct&quot;, 
&quot;covIso&quot;, &quot;covScaling&quot; and &quot;covUser&quot;</p>


<h3>Objects from the Class</h3>

<p>A virtual Class: No objects may be created from it.</p>


<h3>Methods</h3>

<p>No methods defined with class &quot;covKernel&quot; in the signature.
</p>


<h3>Author(s)</h3>

<p> Olivier Roustant, David Ginsbourger, Yves Deville </p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("covKernel")
</code></pre>

<hr>
<h2 id='covMat1Mat2'> Cross covariance matrix </h2><span id='topic+covMat1Mat2'></span>

<h3>Description</h3>

<p>Computes the cross covariance matrix between two sets of locations for a spatial random process with a given covariance structure. Typically the two sets are a learning set and a test set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>covMat1Mat2(object, X1, X2, nugget.flag=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="covMat1Mat2_+3A_object">object</code></td>
<td>
<p>an object specifying the covariance structure.</p>
</td></tr>
<tr><td><code id="covMat1Mat2_+3A_x1">X1</code></td>
<td>
<p> a matrix whose rows represent the locations of a first set (for instance a set of learning points).</p>
</td></tr>
<tr><td><code id="covMat1Mat2_+3A_x2">X2</code></td>
<td>
<p> a matrix whose rows represent the locations of a second set (for instance a set of test points).</p>
</td></tr>
<tr><td><code id="covMat1Mat2_+3A_nugget.flag">nugget.flag</code></td>
<td>
<p>an optional boolean. If <code>TRUE</code>, the covariance between 2 equal locations takes into account the nugget effect (if any). Locations are considered equal if their euclidian distance is inferior to <code>1e-15</code>. Default is <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix of size <code>(nb of rows of X1 * nb of rows of X2)</code> whose element <code>(i1,i2)</code> is equal to the covariance between the locations specified by row <code>i1</code> of <code>X1</code> and row <code>i2</code> of <code>X2</code>.
</p>


<h3>Author(s)</h3>

<p> Olivier Roustant, David Ginsbourger, Ecole des Mines de St-Etienne. </p>


<h3>See Also</h3>

  <p><code><a href="#topic+covMatrix">covMatrix</a></code> </p>

<hr>
<h2 id='covMatrix'> Covariance matrix</h2><span id='topic+covMatrix'></span>

<h3>Description</h3>

<p>Computes the covariance matrix at a set of locations for a spatial random process with a given covariance structure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>covMatrix(object, X, noise.var = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="covMatrix_+3A_object">object</code></td>
<td>
<p>an object specifying the covariance structure.</p>
</td></tr>
<tr><td><code id="covMatrix_+3A_x">X</code></td>
<td>
<p> a matrix whose columns represent locations. </p>
</td></tr>
<tr><td><code id="covMatrix_+3A_noise.var">noise.var</code></td>
<td>
<p>for noisy observations : an optional vector containing the noise variance at each observation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with the following items :
</p>
<table>
<tr><td><code>C</code></td>
<td>
<p> a matrix representing the covariance matrix for the locations specified in the X argument, including a possible nugget effect or observation noise.</p>
</td></tr>
<tr><td><code>vn</code></td>
<td>
<p> a vector of length <code>n</code> (<code>X</code> size) containing a replication of the nugget effet or the observation noise (so that <code>C-diag(vn)</code> contains the covariance matrix when there is no nugget effect nor observation noise)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Olivier Roustant, David Ginsbourger, Ecole des Mines de St-Etienne. </p>


<h3>See Also</h3>

  <p><code><a href="#topic+covMatrixDerivative">covMatrixDerivative</a></code> </p>

<hr>
<h2 id='covMatrixDerivative'> Covariance matrix derivatives </h2><span id='topic+covMatrixDerivative'></span>

<h3>Description</h3>

<p>Computes a partial derivative of the covariance matrix <code>C</code> in function <code><a href="#topic+covMatrix">covMatrix</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>covMatrixDerivative(object, X, C0, k, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="covMatrixDerivative_+3A_object">object</code></td>
<td>
<p>an object specifying the covariance structure. </p>
</td></tr>
<tr><td><code id="covMatrixDerivative_+3A_x">X</code></td>
<td>
<p>a matrix whose columns represent locations. </p>
</td></tr>
<tr><td><code id="covMatrixDerivative_+3A_c0">C0</code></td>
<td>
<p>a matrix corresponding to the covariance matrix for the locations specified in the X argument, when there is no nugget effet nor observation noise. </p>
</td></tr>
<tr><td><code id="covMatrixDerivative_+3A_k">k</code></td>
<td>
<p>an integer representing the partial derivative index. </p>
</td></tr>
<tr><td><code id="covMatrixDerivative_+3A_...">...</code></td>
<td>
<p>additional parameters, typically an environment used for storage</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix representing the partial derivative of <code>C</code>
</p>


<h3>Author(s)</h3>

<p> Olivier Roustant, David Ginsbourger, Ecole des Mines de St-Etienne. </p>


<h3>See Also</h3>

  <p><code><a href="#topic+covMatrix">covMatrix</a></code> </p>

<hr>
<h2 id='covparam2vect'> Auxiliary function </h2><span id='topic+covparam2vect'></span>

<h3>Description</h3>

<p>  Gather the covariance parameters in a single vector. This is useful in the estimation step. Not for direct use.</p>


<h3>Usage</h3>

<pre><code class='language-R'>covparam2vect(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="covparam2vect_+3A_object">object</code></td>
<td>
<p>an object specifying the covariance structure.</p>
</td></tr>
</table>


<h3>Value</h3>

<p> A vector containing the covariance parameters. </p>


<h3>Author(s)</h3>

<p> O. Roustant, D. Ginsbourger</p>


<h3>See Also</h3>

 <p><code><a href="#topic+vect2covparam">vect2covparam</a></code> </p>

<hr>
<h2 id='covParametersBounds'> Boundaries for covariance parameters</h2><span id='topic+covParametersBounds'></span>

<h3>Description</h3>

<p>Default boundaries for covariance parameters. </p>


<h3>Usage</h3>

<pre><code class='language-R'>covParametersBounds(object, X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="covParametersBounds_+3A_object">object</code></td>
<td>
<p>an object specifying the covariance structure.</p>
</td></tr>
<tr><td><code id="covParametersBounds_+3A_x">X</code></td>
<td>
<p>a matrix representing the design of experiments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The default values are chosen as follows : 
</p>

<table>
<tr>
 <td style="text-align: left;">
Range parameters (all covariances) </td><td style="text-align: left;"> <code>lower=1e-10</code>, <code>upper</code>=2 times the difference between </td>
</tr>
<tr>
 <td style="text-align: left;">
</td><td style="text-align: left;"> the max. and min. values of <code>X</code> for each coordinate </td>
</tr>
<tr>
 <td style="text-align: left;">
Shape parameters (<code>powexp</code> covariance) </td><td style="text-align: left;">  <code>lower=1e-10</code>, <code>upper=2</code> for each coordinate
</td>
</tr>

</table>



<h3>Value</h3>

<p>a list with items <code>lower, upper</code> containing default boundaries for the covariance parameters.
</p>


<h3>Author(s)</h3>

<p> Olivier Roustant, David Ginsbourger, Ecole des Mines de St-Etienne. </p>


<h3>See Also</h3>

  <p><code><a href="#topic+km">km</a></code> </p>

<hr>
<h2 id='covScaling-class'>Class &quot;covScaling&quot; </h2><span id='topic+covScaling-class'></span><span id='topic+coef+2CcovScaling-method'></span><span id='topic+covMat1Mat2+2CcovScaling-method'></span><span id='topic+covMatrix+2CcovScaling-method'></span><span id='topic+covMatrixDerivative+2CcovScaling-method'></span><span id='topic+covParametersBounds+2CcovScaling-method'></span><span id='topic+covparam2vect+2CcovScaling-method'></span><span id='topic+vect2covparam+2CcovScaling-method'></span><span id='topic+covVector.dx+2CcovScaling-method'></span><span id='topic+show+2CcovScaling-method'></span><span id='topic+inputnames+2CcovScaling-method'></span><span id='topic+kernelname+2CcovScaling-method'></span><span id='topic+ninput+2CcovScaling-method'></span><span id='topic+nuggetflag+2CcovScaling-method'></span><span id='topic+nuggetvalue+2CcovScaling-method'></span><span id='topic+nuggetvalue+3C-+2CcovScaling+2Cnumeric-method'></span><span id='topic+summary+2CcovScaling-method'></span>

<h3>Description</h3>

<p> Composition of isotropic kernels with coordinatewise 
non-linear scaling obtained by integrating piecewise affine functions </p>


<h3>Objects from the Class</h3>

<p>In 1-dimension, the covariance kernels are parameterized as in (Rasmussen, Williams, 2006). Denote by <code>theta</code> the range parameter, <code>p</code> the exponent parameter (for power-exponential covariance), <code>s</code> the standard deviation, and <code>h=|x-y|</code>. Then we have <code>C(x,y) = s^2 * k(x,y)</code>, with:
</p>

<table>
<tr>
 <td style="text-align: left;">
Gauss </td><td style="text-align: left;"> <code> k(x,y) = exp(-1/2*(h/theta)^2) </code> </td>
</tr>
<tr>
 <td style="text-align: left;">
Exponential </td><td style="text-align: left;"> <code> k(x,y) = exp(-h/theta) </code> </td>
</tr>
<tr>
 <td style="text-align: left;">
Matern(3/2) </td><td style="text-align: left;"> <code> k(x,y) = (1+sqrt(3)*h/theta)*exp(-sqrt(3)*h/theta) </code> </td>
</tr>
<tr>
 <td style="text-align: left;">
Matern(5/2) </td><td style="text-align: left;"> <code> k(x,y) = (1+sqrt(5)*h/theta+(1/3)*5*(h/theta)^2)</code> </td>
</tr>
<tr>
 <td style="text-align: left;"> 
</td><td style="text-align: left;"> <code>          *exp(-sqrt(5)*h/theta)</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
Power-exponential </td><td style="text-align: left;"> <code> k(x,y) = exp(-(h/theta)^p) </code> </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Here, in every dimension, the corresponding one-dimensional stationary kernel <code>k(x,y)</code> is replaced by <code>k(f(x),f(y))</code>, where <code>f</code> is a continuous monotonic function indexed by a finite number of  parameters (see the references for more detail). 
</p>


<h3>Slots</h3>


<dl>
<dt><code>d</code>:</dt><dd><p>Object of class <code>"integer"</code>. The spatial dimension. </p>
</dd>
<dt><code>knots</code>:</dt><dd><p>Object of class <code>"list"</code>. The j-th element is a vector containing the knots for dimension j. </p>
</dd>
<dt><code>eta</code>:</dt><dd><p>Object of class <code>"list"</code>. In correspondance with knots, the j-th element is a vector containing the scaling coefficients (i.e. the derivatives of the scaling function at the knots) for dimension j.</p>
</dd> 
<dt><code>name</code>:</dt><dd><p>Object of class <code>"character"</code>. The covariance function name. To be chosen between <code>"gauss", "matern5_2", "matern3_2", "exp"</code>, and <code>"powexp"</code> </p>
</dd>
<dt><code>paramset.n</code>:</dt><dd><p>Object of class <code>"integer"</code>. 1 for covariance depending only on the ranges parameters, 2 for &quot;powexp&quot; which also depends on exponent parameters. </p>
</dd>
<dt><code>var.names</code>:</dt><dd><p>Object of class <code>"character"</code>. The variable names. </p>
</dd>
<dt><code>sd2</code>:</dt><dd><p>Object of class <code>"numeric"</code>. The variance of the stationary part of the process. </p>
</dd>
<dt><code>known.covparam</code>:</dt><dd><p>Object of class <code>"character"</code>. Internal use. One of: &quot;None&quot;, &quot;All&quot;. </p>
</dd>
<dt><code>nugget.flag</code>:</dt><dd><p>Object of class <code>"logical"</code>. Is there a nugget effect? </p>
</dd>
<dt><code>nugget.estim</code>:</dt><dd><p>Object of class <code>"logical"</code>. Is the nugget effect estimated or known? </p>
</dd>
<dt><code>nugget</code>:</dt><dd><p>Object of class <code>"numeric"</code>. If there is a nugget effect, its value (homogeneous to a variance). </p>
</dd>
<dt><code>param.n</code>:</dt><dd><p>Object of class <code>"integer"</code>. The total number of parameters. </p>
</dd>
</dl>



<h3>Extends</h3>

<p>Class <code>"<a href="#topic+covKernel-class">covKernel</a>"</code>, directly.
</p>


<h3>Methods</h3>


<dl>
<dt>coef</dt><dd><p><code>signature(object = "covScaling")</code>: ... </p>
</dd>
<dt>covMat1Mat2</dt><dd><p><code>signature(object = "covScaling")</code>: ... </p>
</dd>
<dt>covMatrix</dt><dd><p><code>signature(object = "covScaling")</code>: ... </p>
</dd>
<dt>covMatrixDerivative</dt><dd><p><code>signature(object = "covScaling")</code>: ... </p>
</dd>
<dt>covParametersBounds</dt><dd><p><code>signature(object = "covScaling")</code>: ... </p>
</dd>
<dt>covparam2vect</dt><dd><p><code>signature(object = "covScaling")</code>: ... </p>
</dd>
<dt>vect2covparam</dt><dd><p><code>signature(object = "covScaling")</code>: ... </p>
</dd>
<dt>show</dt><dd><p><code>signature(object = "covScaling")</code>: ... </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> Olivier Roustant, David Ginsbourger, Yves Deville </p>


<h3>References</h3>

<p>Y. Xiong, W. Chen, D. Apley, and X. Ding (2007), <em>Int. J. Numer. Meth. Engng</em>, A non-stationary covariance-based Kriging method for metamodelling in engineering design. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+km-class">km</a></code>
<code><a href="#topic+covTensorProduct-class">covTensorProduct</a></code>
<code><a href="#topic+covIso-class">covIso</a></code>
<code><a href="#topic+covKernel-class">covKernel</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("covScaling")
</code></pre>

<hr>
<h2 id='covStruct.create'> Spatial covariance - Class constructor </h2><span id='topic+covStruct.create'></span>

<h3>Description</h3>

<p>  Creates a covariance structure.</p>


<h3>Usage</h3>

<pre><code class='language-R'>covStruct.create(covtype, d, known.covparam, var.names, coef.cov = NULL, coef.var = NULL, 
     nugget = NULL, nugget.estim = FALSE, nugget.flag = FALSE,
     iso = FALSE, scaling = FALSE, knots=NULL, kernel=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="covStruct.create_+3A_covtype">covtype</code></td>
<td>
<p> a character string specifying the covariance structure.</p>
</td></tr>
<tr><td><code id="covStruct.create_+3A_d">d</code></td>
<td>
<p> an integer containing the spatial dimension.</p>
</td></tr>
<tr><td><code id="covStruct.create_+3A_known.covparam">known.covparam</code></td>
<td>
<p> a character (&quot;None&quot; or &quot;All&quot;) indicating whether covariance parameters are known or must be estimated.</p>
</td></tr>
<tr><td><code id="covStruct.create_+3A_var.names">var.names</code></td>
<td>
<p> a vector of character strings containing the variable names.</p>
</td></tr>  
<tr><td><code id="covStruct.create_+3A_coef.cov">coef.cov</code></td>
<td>
<p> an optional vector containing the values for covariance parameters.</p>
</td></tr>
<tr><td><code id="covStruct.create_+3A_coef.var">coef.var</code></td>
<td>
<p> an optional number containing the variance value.</p>
</td></tr>
<tr><td><code id="covStruct.create_+3A_nugget">nugget</code></td>
<td>
<p> an optional variance value standing for the homogenous nugget effect. Default is NULL.</p>
</td></tr>
<tr><td><code id="covStruct.create_+3A_nugget.estim">nugget.estim</code></td>
<td>
<p> is the nugget effect estimated or known?</p>
</td></tr>
<tr><td><code id="covStruct.create_+3A_nugget.flag">nugget.flag</code></td>
<td>
<p> is there a nugget effect?</p>
</td></tr>
<tr><td><code id="covStruct.create_+3A_iso">iso</code></td>
<td>
<p> an optional boolean that can be used to force a tensor-product covariance structure to have a range parameter common to all dimensions.</p>
</td></tr>
<tr><td><code id="covStruct.create_+3A_scaling">scaling</code></td>
<td>
<p> an optional boolean indicating whether a scaling on the covariance structure should be used.</p>
</td></tr>
<tr><td><code id="covStruct.create_+3A_knots">knots</code></td>
<td>
<p> an optional list of knots (used if <code>scaling = TRUE</code>)</p>
</td></tr>
<tr><td><code id="covStruct.create_+3A_kernel">kernel</code></td>
<td>
<p> an optional function containing a new covariance structure</p>
</td></tr>
</table>


<h3>Value</h3>

<p> A formal S4 class of type <code><a href="#topic+covTensorProduct-class">covTensorProduct-class</a></code>, <code><a href="#topic+covIso-class">covIso-class</a></code> (if <code>iso</code> is <code>TRUE</code>) (if <code>scaling</code> is <code>TRUE</code>),
or <code><a href="#topic+covUser-class">covUser-class</a></code> (if <code>kernel</code> is <code>TRUE</code>).</p>


<h3>Author(s)</h3>

<p> O. Roustant, D. Ginsbourger</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+km">km</a></code>
</p>

<hr>
<h2 id='covTensorProduct-class'> Class of tensor-product spatial covariances </h2><span id='topic+covTensorProduct-class'></span><span id='topic+show+2CcovTensorProduct-method'></span><span id='topic+coef+2CcovTensorProduct-method'></span><span id='topic+covMat1Mat2+2CcovTensorProduct-method'></span><span id='topic+covMatrix+2CcovTensorProduct-method'></span><span id='topic+covMatrixDerivative+2CcovTensorProduct-method'></span><span id='topic+covParametersBounds+2CcovTensorProduct-method'></span><span id='topic+covparam2vect+2CcovTensorProduct-method'></span><span id='topic+vect2covparam+2CcovTensorProduct-method'></span><span id='topic+covVector.dx+2CcovTensorProduct-method'></span><span id='topic+inputnames+2CcovTensorProduct-method'></span><span id='topic+kernelname+2CcovTensorProduct-method'></span><span id='topic+ninput+2CcovTensorProduct-method'></span><span id='topic+nuggetflag+2CcovTensorProduct-method'></span><span id='topic+nuggetvalue+2CcovTensorProduct-method'></span><span id='topic+nuggetvalue+3C-+2CcovTensorProduct+2Cnumeric-method'></span><span id='topic+summary+2CcovTensorProduct-method'></span>

<h3>Description</h3>

<p>S4 class of tensor-product (or separable) covariances.</p>


<h3>Value</h3>

<table>
<tr><td><code>covTensorProduct</code></td>
<td>
<p>separable covariances depending on 1 set of
parameters, such as Gaussian, exponential, Matern with fixed nu... or
on 2 sets of parameters, such as power-exponential.</p>
</td></tr>
</table>


<h3>Objects from the Class</h3>

<p>A d-dimensional tensor product (or separable) covariance kernel <code>C(x,y)</code> is the tensor product of 1-dimensional covariance kernels : <code>C(x,y) = C(x1,y1)C(x2,y2)...C(xd,yd)</code>.
</p>
<p>In 1-dimension, the covariance kernels are parameterized as in (Rasmussen, Williams, 2006). Denote by <code>theta</code> the range parameter, <code>p</code> the exponent parameter (for power-exponential covariance), <code>s</code> the standard deviation, and <code>h=|x-y|</code>. Then we have <code>C(x,y) = s^2 * k(x,y)</code>, with:
</p>

<table>
<tr>
 <td style="text-align: left;">
Gauss </td><td style="text-align: left;"> <code> k(x,y) = exp(-1/2*(h/theta)^2) </code> </td>
</tr>
<tr>
 <td style="text-align: left;">
Exponential </td><td style="text-align: left;"> <code> k(x,y) = exp(-h/theta) </code> </td>
</tr>
<tr>
 <td style="text-align: left;">
Matern(3/2) </td><td style="text-align: left;"> <code> k(x,y) = (1+sqrt(3)*h/theta)*exp(-sqrt(3)*h/theta) </code> </td>
</tr>
<tr>
 <td style="text-align: left;">
Matern(5/2) </td><td style="text-align: left;"> <code> k(x,y) = (1+sqrt(5)*h/theta+(1/3)*5*(h/theta)^2)</code> </td>
</tr>
<tr>
 <td style="text-align: left;"> 
</td><td style="text-align: left;"> <code>          *exp(-sqrt(5)*h/theta)</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
Power-exponential </td><td style="text-align: left;"> <code> k(x,y) = exp(-(h/theta)^p) </code> </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Slots</h3>


<dl>
<dt><code>d</code>:</dt><dd><p>Object of class <code>"integer"</code>. The spatial dimension. </p>
</dd>
<dt><code>name</code>:</dt><dd><p>Object of class <code>"character"</code>. The covariance function name. To be chosen between <code>"gauss", "matern5_2", "matern3_2", "exp"</code>, and <code>"powexp"</code> </p>
</dd>
<dt><code>paramset.n</code>:</dt><dd><p>Object of class <code>"integer"</code>. 1 for covariance depending only on the ranges parameters, 2 for &quot;powexp&quot; which also depends on exponent parameters. </p>
</dd>
<dt><code>var.names</code>:</dt><dd><p>Object of class <code>"character"</code>. The variable names. </p>
</dd>
<dt><code>sd2</code>:</dt><dd><p>Object of class <code>"numeric"</code>. The variance of the stationary part of the process. </p>
</dd>
<dt><code>known.covparam</code>:</dt><dd><p>Object of class <code>"character"</code>. Internal use. One of: &quot;None&quot;, &quot;All&quot;. </p>
</dd>
<dt><code>nugget.flag</code>:</dt><dd><p>Object of class <code>"logical"</code>. Is there a nugget effect? </p>
</dd>
<dt><code>nugget.estim</code>:</dt><dd><p>Object of class <code>"logical"</code>. Is the nugget effect estimated or known? </p>
</dd>
<dt><code>nugget</code>:</dt><dd><p>Object of class <code>"numeric"</code>. If there is a nugget effect, its value (homogeneous to a variance). </p>
</dd>
<dt><code>param.n</code>:</dt><dd><p>Object of class <code>"integer"</code>. The total number of parameters. </p>
</dd>
<dt><code>range.n</code>:</dt><dd><p>Object of class <code>"integer"</code>. The number of range parameters. </p>
</dd>
<dt><code>range.names</code>:</dt><dd><p>Object of class <code>"character"</code>. Names of range parameters, for printing purpose. Default is &quot;theta&quot;. </p>
</dd>
<dt><code>range.val</code>:</dt><dd><p>Object of class <code>"numeric"</code>. Values of range parameters. </p>
</dd>
<dt><code>shape.n</code>:</dt><dd><p>Object of class <code>"integer"</code>. The number of shape parameters (exponent parameters in &quot;powexp&quot;). </p>
</dd>
<dt><code>shape.names</code>:</dt><dd><p>Object of class <code>"character"</code>. Names of shape parameters, for printing purpose. Default is &quot;p&quot;. </p>
</dd>
<dt><code>shape.val</code>:</dt><dd><p>Object of class <code>"numeric"</code>. Values of shape parameters. </p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>show</dt><dd><p><code>signature(x = "covTensorProduct")</code> Print covariance function. See <code><a href="#topic+show+2Ckm-method">show,km-method</a></code>. </p>
</dd>
<dt>coef</dt><dd><p><code>signature(x = "covTensorProduct")</code> Get the coefficients of the covariance function. </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> O. Roustant, D. Ginsbourger</p>


<h3>References</h3>

<p>N.A.C. Cressie (1993), <em>Statistics for spatial data</em>, Wiley series in probability and mathematical statistics.
</p>
<p>C.E. Rasmussen and C.K.I. Williams (2006), <em>Gaussian Processes for Machine Learning</em>, the MIT Press, <a href="http://www.gaussianprocess.org/gpml/">http://www.gaussianprocess.org/gpml/</a>
</p>
<p>M.L. Stein (1999), <em>Interpolation of spatial data, some theory for kriging</em>, Springer.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+covStruct.create">covStruct.create</a></code> to construct a covariance structure.</p>

<hr>
<h2 id='covUser-class'>Class &quot;covUser&quot; </h2><span id='topic+covUser-class'></span><span id='topic+coef+2CcovUser-method'></span><span id='topic+covMat1Mat2+2CcovUser-method'></span><span id='topic+covMatrix+2CcovUser-method'></span><span id='topic+show+2CcovUser-method'></span><span id='topic+nuggetflag+2CcovUser-method'></span><span id='topic+nuggetvalue+2CcovUser-method'></span><span id='topic+nuggetvalue+3C-+2CcovUser+2Cnumeric-method'></span>

<h3>Description</h3>

<p> An arbitrary covariance kernel provided by the user </p>


<h3>Objects from the Class</h3>

<p>Any valid covariance kernel, provided as a 2-dimensional function (x,y) -&gt; k(x,y). At this stage, no test is done to check that k is positive definite.
</p>


<h3>Slots</h3>


<dl>
<dt><code>kernel</code>:</dt><dd><p>Object of class <code>"function"</code>. The new covariance kernel. </p>
</dd>
<dt><code>nugget.flag</code>:</dt><dd><p>Object of class <code>"logical"</code>. Is there a nugget effect? </p>
</dd>
<dt><code>nugget</code>:</dt><dd><p>Object of class <code>"numeric"</code>. If there is a nugget effect, its value (homogeneous to a variance). </p>
</dd>
</dl>



<h3>Extends</h3>

<p>Class <code>"<a href="#topic+covKernel-class">covKernel</a>"</code>, directly.
</p>


<h3>Methods</h3>


<dl>
<dt>coef</dt><dd><p><code>signature(object = "covUser")</code>: ... </p>
</dd>
<dt>covMat1Mat2</dt><dd><p><code>signature(object = "covScaling")</code>: ... </p>
</dd>
<dt>covMatrix</dt><dd><p><code>signature(object = "covScaling")</code>: ... </p>
</dd>
<dt>show</dt><dd><p><code>signature(object = "covScaling")</code>: ... </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> Olivier Roustant, David Ginsbourger, Yves Deville </p>


<h3>See Also</h3>

<p><code><a href="#topic+km-class">km</a></code>
<code><a href="#topic+covTensorProduct-class">covTensorProduct</a></code>
<code><a href="#topic+covIso-class">covIso</a></code>
<code><a href="#topic+covKernel-class">covKernel</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>showClass("covUser")
</code></pre>

<hr>
<h2 id='covVector.dx'> Spatial covariance - Derivatives</h2><span id='topic+covVector.dx'></span>

<h3>Description</h3>

<p>Computes the gradient of the covariance vector <code>c(x)</code> computed by <code><a href="#topic+covMat1Mat2">covMat1Mat2</a></code> with respect to <code>x</code>, for a given covariance structure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>covVector.dx(object, x, X, c)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="covVector.dx_+3A_object">object</code></td>
<td>
<p>an object specifying the covariance structure. </p>
</td></tr>
<tr><td><code id="covVector.dx_+3A_x">x</code></td>
<td>
<p>a vector representing the specific location. </p>
</td></tr>
<tr><td><code id="covVector.dx_+3A_x">X</code></td>
<td>
<p>a matrix whose columns represent locations. </p>
</td></tr>
<tr><td><code id="covVector.dx_+3A_c">c</code></td>
<td>
<p>a vector containing the covariances between the location <code>x</code> and the set of locations specified in <code>X</code> (see <code><a href="#topic+covMat1Mat2">covMat1Mat2</a></code>). </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of real numbers equal to the gradient of <code>c</code> as a function of <code>x</code>.
</p>


<h3>Author(s)</h3>

<p> Olivier Roustant, David Ginsbourger, Ecole des Mines de St-Etienne. </p>


<h3>See Also</h3>

  <p><code><a href="#topic+covMat1Mat2">covMat1Mat2</a></code> </p>

<hr>
<h2 id='cv'>Multiple fold cross validation for a km object </h2><span id='topic+cv'></span>

<h3>Description</h3>

<p>Multiple fold cross validation for a <code>km</code> object without noisy observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv(model, folds, type="UK", trend.reestim=TRUE, fast=TRUE, light=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv_+3A_model">model</code></td>
<td>
<p> an object of class &quot;km&quot; without noisy observations.</p>
</td></tr>
<tr><td><code id="cv_+3A_folds">folds</code></td>
<td>
<p> a list of index subsets without index redundancy within each fold.</p>
</td></tr>
<tr><td><code id="cv_+3A_type">type</code></td>
<td>
<p> a character string corresponding to the kriging family, to be chosen between simple kriging (&quot;SK&quot;), or universal kriging (&quot;UK&quot;).</p>
</td></tr>
<tr><td><code id="cv_+3A_trend.reestim">trend.reestim</code></td>
<td>
<p> should the trend be reestimated when removing an observation? Default to FALSE.</p>
</td></tr>
<tr><td><code id="cv_+3A_fast">fast</code></td>
<td>
<p> binary option to use analytical multiple fold cross validation formulae when applicable.</p>
</td></tr>
<tr><td><code id="cv_+3A_light">light</code></td>
<td>
<p> binary option to force not calculating cross validation residual covariances between different folds (relevant, e.g., when performing speed comparisons across baseline versus fast settings).</p>
</td></tr>
</table>


<h3>Value</h3>

<p> A list composed of
</p>
<table>
<tr><td><code>mean</code></td>
<td>
<p> a list of cross validation mean predictions with same number of elements and respective dimensions than in folds. The ith element is equal to the kriging mean (including the trend) at the ith fold number when it is left out of the design, </p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p> a vector of actual responses,</p>
</td></tr>
<tr><td><code>cvcov.list</code></td>
<td>
<p> a list of cross validation conditional covariance matrices with same number of elements than in folds and dimensions set accordingly. The ith element is equal to the kriging covariance matrix corresponding to the ith fold number when it is left out of the design,</p>
</td></tr>
<tr><td><code>cvcov.mat</code></td>
<td>
<p> a ntot*ntot matrix containing all covariances between cross-validation errors (stacked with respect to orders between and within folds),</p>
</td></tr>
</table>
<p>where ntot is the total number of points in the folds list (with possible point redundancies as some points may belong to several folds). 
</p>


<h3>Warning</h3>

<p>Kriging parameters are not re-estimated when removing observations. With few points in the learning set, the re-estimated values can be far from those obtained with the entire learning set. One option is to reestimate the trend coefficients, by setting <code>trend.reestim=TRUE</code>.
</p>


<h3>Author(s)</h3>

<p> D. Ginsbourger, University of Bern. </p>


<h3>References</h3>

 
<p>F. Bachoc (2013), Cross Validation and Maximum Likelihood estimations of hyper-parameters of Gaussian processes with model misspecification. <em>Computational Statistics and Data Analysis</em>, <b>66</b>, 55-69. 
</p>
<p>N.A.C. Cressie (1993), <em>Statistics for spatial data</em>, Wiley series in probability and mathematical statistics.
</p>
<p>O. Dubrule (1983), Cross validation of Kriging in a unique neighborhood. <em>Mathematical Geology</em>, <b>15</b>, 687-699.
</p>
<p>J. Gallier. The schur complement and symmetric positive semidefinite (and definite) matrices. Retrieved at <a href="https://www.cis.upenn.edu/~jean/schur-comp.pdf">https://www.cis.upenn.edu/~jean/schur-comp.pdf</a>.
</p>
<p>D. Ginsbourger and C. Schaerer (2021). Fast calculation of Gaussian Process multiple-fold cross-validation residuals and their covariances. 
arXiv:2101.03108 [stat.ME].
</p>
<p>J.D. Martin and T.W. Simpson (2005), Use of kriging models to approximate deterministic computer models, <em>AIAA Journal</em>, <b>43</b> no. 4, 853-863.
</p>
<p>M. Schonlau (1997), <em>Computer experiments and global optimization</em>, Ph.D. thesis, University of Waterloo.	
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+predict+2Ckm-method">predict,km-method</a></code>,  <code><a href="#topic+leaveOneOut.km">leaveOneOut.km</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'># -------------------------------------------------
# A 1D example illustrating leave-one-out residuals 
# and their correlation 
# -------------------------------------------------

# Test function (From Xiong et al. 2007; See scalingFun's doc) 
myfun &lt;- function(x){ 
  sin(30 * (x - 0.9)^4) * cos(2 * (x - 0.9)) + (x - 0.9) / 2
}
t &lt;- seq(from = 0, to = 1, by = 0.005)
allresp &lt;- myfun(t)
par(mfrow = c(1, 1), mar = c(4, 4, 2, 2))
plot(t, allresp, type = "l")

# Design points and associated responses 
nn &lt;- 10
design &lt;- seq(0, 1, length.out = nn)
y &lt;- myfun(design)
points(design, y, pch = 19)

# Model definition and GP prediction (Kriging)
set.seed(1)
model1 &lt;- km(design = data.frame(design = design), 
             response = data.frame(y = y), nugget = 1e-5,
             multistart = 10, control = list(trace = FALSE))
pred1 &lt;- predict(model1, newdata = data.frame(design = t), type = "UK") 
lines(t, pred1$mean, type = "l", col = "blue", lty = 2, lwd = 2)

# Plotting the prediction error versus the GP standard deviation 
par(mfrow = c(2,1))
pred_abserrors &lt;- abs(allresp - pred1$mean)
plot(t, pred_abserrors, type = "l", ylab = "abs pred error")
plot(t, pred1$sd, type = "l", ylab = "GP prediction sd")

# Leave-one-out cross-validation with the cv function 
loofolds &lt;- as.list(seq(1, length(design)))
loo1 &lt;- cv(model = model1, folds = loofolds, type = "UK", 
              trend.reestim = TRUE, fast = TRUE, light = FALSE) 

# y axis limits need to be taken care of 
plotCVmean &lt;- function(cvObj){
  cvObjMean &lt;- unlist(cvObj$mean)
  plot(t, allresp, type = "l", ylim = range(cvObjMean, allresp))
  points(design, y, pch = 19)
  lines(t, pred1$mean, type = "l", col = "blue", lty = 2, lwd = 2)
  points(design, cvObjMean, col = "red", pch = 22, lwd = 2)
}
plotCVsd &lt;- function(cvObj, ylim){
  cv_abserrors &lt;- abs(y - unlist(cvObj$mean))
  plot(t, pred_abserrors, type = "l", ylab = "abs pred error", 
       ylim = ylim)
  points(design, cv_abserrors, col = "red", pch = 22, lwd = 2)
  lines(t, pred1$sd, ylab = "GP prediction sd", col = "blue", 
      lty = 2, lwd = 2)
}

loo1Mean &lt;- unlist(loo1$mean)
loo_abserrors &lt;- abs(y - loo1Mean)
ylim &lt;- c(0, max(loo_abserrors, pred_abserrors))

plotCVmean(loo1)
plotCVsd(loo1, ylim = ylim)

# Calculation of uncorrelated CV residuals and corresponding qqplot 
T &lt;- model1@T
B &lt;- diag(as.numeric(diag(loo1$cvcov.mat))^(-1))
res &lt;- y - loo1Mean
stand &lt;- T %*% B %*% res
opar &lt;- par(mfrow = c(1, 2))
qqnorm(stand, 
       main = "Normal Q-Q Plot of uncorrelated LOO Residuals")
abline(a = 0, b = 1)

# Comparison to "usual" standardized LOO residuals
usual_stand &lt;- diag(as.numeric(diag(loo1$cvcov.mat))^(-1/2)) %*% res
qqnorm(usual_stand, 
       main = "Normal Q-Q Plot of Standardized LOO Residuals") 
abline(a = 0, b = 1)
par(opar)

# Calculation and plot of correlations between most left 
# and other cross-validation residuals 
cvcov.mat &lt;- loo1$cvcov.mat
coco &lt;- cov2cor(cvcov.mat)
par(mfrow = c(1, 1))
plot(coco[1, ], type = "h", ylim = c(-1, 1), lwd = 2,
     main = "Correlation between first and other LOO residuals", 
     ylab = "Correlation")
points(coco[1, ])
abline(h = 0, lty = "dotted")
     
par(mfrow = c(1, 1), mar = c(5.1, 4.1, 4.1, 2.1))

# ------------------------------------------------
# Same example with multiple-fold cross validation 
# under various settings
# ------------------------------------------------

# First with successive two-element folds 
myfolds &lt;- list(c(1, 2), c(3, 4), c(5, 6), c(7, 8), c(9, 10))
cv_2fold &lt;- cv(model = model1, folds = myfolds, type = "SK", 
               trend.reestim = FALSE, fast = TRUE, light = FALSE)  
cv_2fold

opar &lt;- par(mfrow = c(2,1))
plotCVmean(cv_2fold)
plotCVsd(cv_2fold, ylim = ylim)


# With overlapping two-element folds 
myfolds &lt;- list(c(1, 3), c(2, 4), c(3, 5), c(4, 6), 
                c(5, 7), c(6, 8), c(7, 9), c(8, 10))
cv_2fold_overlap &lt;- cv(model = model1, folds = myfolds, type = "UK", 
                       trend.reestim = TRUE, fast = TRUE, light = FALSE)
cv_2fold_overlap


# With a three-fold partition 
myfolds &lt;- list(c(1, 2, 3), c(4, 5, 6, 7), c(8, 9, 10))
cv_3fold &lt;- cv(model = model1, folds = myfolds, type = "UK", 
           trend.reestim = TRUE, fast = TRUE, light = FALSE)
cv_3fold

plotCVmean(cv_3fold)
plotCVsd(cv_3fold, ylim = ylim)
par(opar)

</code></pre>

<hr>
<h2 id='DiceKriging-package'> Kriging Methods for Computer Experiments</h2><span id='topic+DiceKriging'></span>

<h3>Description</h3>

<p>Estimation, validation and prediction of kriging models.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> DiceKriging</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.6.0</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2021-02-23</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-2 | GPL-3 </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Note</h3>

<p>A previous version of this package was conducted within the frame of the DICE (Deep Inside Computer Experiments) Consortium between ARMINES, Renault, EDF, IRSN, ONERA and TOTAL S.A. (http://dice.emse.fr/).
</p>
<p>The authors wish to thank Laurent Carraro, Delphine Dupuy and Celine Helbert for fruitful discussions about the structure of the code, and Francois Bachoc for his participation in validation and estimation by leave-one-out. They also thank Gregory Six and Gilles Pujol for their advices on practical implementation issues, as well as the DICE members for useful feedbacks.
</p>
<p>Package <code>rgenoud</code> &gt;= 5.8-2.0 is recommended.
</p>
<p>Important functions or methods:
</p>

<table>
<tr>
 <td style="text-align: left;">
<code>km</code> </td><td style="text-align: left;"> Estimation (or definition) of a kriging model with unknown (known) parameters </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>predict</code> </td><td style="text-align: left;"> Prediction of the objective function at new points using a kriging model (Simple and </td>
</tr>
<tr>
 <td style="text-align: left;"> 
</td><td style="text-align: left;"> Universal Kriging) </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>plot</code> </td><td style="text-align: left;"> Plot diagnostic for a kriging model (leave-one-out) </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>simulate</code> </td><td style="text-align: left;"> Simulation of kriging models </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Olivier Roustant, David Ginsbourger, Yves Deville. Contributors: C. Chevalier, Y. Richet.
</p>
<p>(maintainer: Olivier Roustant <a href="mailto:roustant@insa-toulouse.fr">roustant@insa-toulouse.fr</a>)
</p>


<h3>References</h3>

<p>F. Bachoc (2013), Cross Validation and Maximum Likelihood estimations of hyper-parameters of Gaussian processes with model misspecification. <em>Computational Statistics and Data Analysis</em>, <b>66</b>, 55-69. <a href="http://www.lpma.math.upmc.fr/pageperso/bachoc/publications.html">http://www.lpma.math.upmc.fr/pageperso/bachoc/publications.html</a>
</p>
<p>N.A.C. Cressie (1993), <em>Statistics for spatial data</em>, Wiley series in probability and mathematical statistics.
</p>
<p>O. Dubrule (1983), Cross validation of Kriging in a unique neighborhood. <em>Mathematical Geology</em>, <b>15</b>, 687-699.
</p>
<p>D. Ginsbourger (2009), <em>Multiples metamodeles pour l'approximation et l'optimisation
de fonctions numeriques multivariables</em>, Ph.D. thesis, Ecole Nationale Superieure des
Mines de Saint-Etienne, 2009.
</p>
<p>D. Ginsbourger, D. Dupuy, A. Badea, O. Roustant, and L. Carraro (2009), A note on the choice and the estimation of kriging models for the analysis of deterministic computer experiments, <em>Applied Stochastic Models for Business and Industry</em>, <b>25</b> no. 2, 115-131.
</p>
<p>A.G. Journel and C.J. Huijbregts (1978), <em>Mining Geostatistics</em>, Academic Press, London.
</p>
<p>A.G. Journel and M.E. Rossi (1989), When do we need a trend model in kriging ?, <em>Mathematical Geology</em>, <b>21</b> no. 7, 715-739.
</p>
<p>D.G. Krige (1951), A statistical approach to some basic mine valuation problems on the witwatersrand, <em>J. of the Chem., Metal. and Mining Soc. of South Africa</em>, <b>52</b> no. 6, 119-139.
</p>
<p>R. Li and A. Sudjianto (2005), Analysis of Computer Experiments Using Penalized Likelihood in Gaussian Kriging Models, <em>Technometrics</em>, <b>47</b> no. 2, 111-120.
</p>
<p>K.V. Mardia and R.J. Marshall (1984), Maximum likelihood estimation of models for residual covariance in spatial regression, <em>Biometrika</em>, <b>71</b>, 135-146.
</p>
<p>J.D. Martin and T.W. Simpson (2005), Use of kriging models to approximate deterministic computer models, <em>AIAA Journal</em>, <b>43</b> no. 4, 853-863.
</p>
<p>G. Matheron (1963), Principles of geostatistics, <em>Economic Geology</em>, <b>58</b>,
1246-1266.
</p>
<p>G. Matheron (1969), Le krigeage universel, <em>Les Cahiers du Centre de Morphologie Mathematique de Fontainebleau</em>, <b>1</b>.
</p>
<p>W.R. Mebane, Jr., J.S. Sekhon (2011). Genetic Optimization Using Derivatives: The rgenoud Package for R. <em>Journal of Statistical Software</em>, <b>42</b>(11), 1-26. <a href="https://www.jstatsoft.org/v42/i11/">https://www.jstatsoft.org/v42/i11/</a>
</p>
<p>J.-S. Park and J. Baek (2001), Efficient computation of maximum likelihood estimators in a spatial linear model with power exponential covariogram, <em>Computer Geosciences</em>, <b>27</b> no. 1, 1-7.
</p>
<p>C.E. Rasmussen and C.K.I. Williams (2006), <em>Gaussian Processes for Machine Learning</em>, the MIT Press, <a href="http://www.gaussianprocess.org/gpml/">http://www.gaussianprocess.org/gpml/</a>
</p>
<p>B.D. Ripley (1987), <em>Stochastic Simulation</em>, Wiley.
</p>
<p>O. Roustant, D. Ginsbourger and Yves Deville (2012), DiceKriging, DiceOptim: Two R Packages for the Analysis of Computer Experiments by Kriging-Based Metamodeling and Optimization, <em>Journal of Statistical Software</em>, <b>51</b>(1), 1-55, <a href="https://www.jstatsoft.org/v51/i01/">https://www.jstatsoft.org/v51/i01/</a>.
</p>
<p>J. Sacks, W.J. Welch, T.J. Mitchell, and H.P. Wynn (1989), Design and analysis of computer experiments, <em>Statistical Science</em>, <b>4</b>, 409-435.
</p>
<p>M. Schonlau (1997), <em>Computer experiments and global optimization</em>, Ph.D. thesis, University of Waterloo.
</p>
<p>M.L. Stein (1999), <em>Interpolation of spatial data, some theory for kriging</em>, Springer.
</p>
<p>Y. Xiong, W. Chen, D. Apley, and X. Ding (2007), <em>Int. J. Numer. Meth. Engng</em>, A non-stationary covariance-based Kriging method for metamodelling in engineering design.
</p>

<hr>
<h2 id='drop.response'> Trend model formula operation </h2><span id='topic+drop.response'></span>

<h3>Description</h3>

<p>  Drop the response in the formula specifying the linear trend. </p>


<h3>Usage</h3>

<pre><code class='language-R'>drop.response(formula, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="drop.response_+3A_formula">formula</code></td>
<td>
<p>  an object of class formula. </p>
</td></tr>
<tr><td><code id="drop.response_+3A_data">data</code></td>
<td>
<p> a data frame corresponding to formula.</p>
</td></tr>
</table>


<h3>Value</h3>

<p> An object of class formula. </p>


<h3>Author(s)</h3>

<p> O. Roustant, D. Ginsbourger </p>


<h3>See Also</h3>

 <p><code><a href="#topic+trendMatrix.update">trendMatrix.update</a></code> </p>

<hr>
<h2 id='goldsteinPrice'> 2D test function</h2><span id='topic+goldsteinPrice'></span>

<h3>Description</h3>

<p>Goldstein price 2-dimensional test function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>goldsteinPrice(x)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="goldsteinPrice_+3A_x">x</code></td>
<td>
<p> a 2-dimensional vector specifying the location where the function is to be evaluated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Goldstein price function is usually defined over the domain [-2,-2] x [2, 2]. 
Here, the function is adapted to the domain [0,1] x [0,1]. It has 1 global minimum : 
x1 = c(0.5, 0.25)
</p>


<h3>Value</h3>

<p>A real number equal to the Goldstein price function values at <code>x</code>
</p>


<h3>Author(s)</h3>

<p> D. Ginsbourger, Ecole des Mines de St-Etienne. </p>


<h3>Examples</h3>

<pre><code class='language-R'> 
n.grid &lt;- 20
x.grid &lt;- y.grid &lt;- seq(0,1,length=n.grid)
design.grid &lt;- expand.grid(x.grid, y.grid)
response.grid &lt;- apply(design.grid, 1, goldsteinPrice)
z.grid &lt;- matrix(response.grid, n.grid, n.grid)
contour(x.grid, y.grid, z.grid, 40)
x1 = c(0.5, 0.25)
points(t(x1), pch=19, col="red")
title("Fonction de Goldstein price")
</code></pre>

<hr>
<h2 id='hartman3'> 3D test function</h2><span id='topic+hartman3'></span>

<h3>Description</h3>

<p>Hartman 3-dimensional test function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hartman3(x)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hartman3_+3A_x">x</code></td>
<td>
<p> a 3-dimensional vector specifying the location where the function is to be evaluated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The hartman3 function is defined over the domain <code>[0,1]^3</code>. It has 1 global minimum : 
x1 = c(0.1, 0.55592003, 0.85218259)
</p>


<h3>Value</h3>

<p>A real number equal to the hartman3 function values at <code>x</code>
</p>


<h3>Author(s)</h3>

<p> D. Ginsbourger, Ecole des Mines de St-Etienne. </p>


<h3>Examples</h3>

<pre><code class='language-R'> 
design &lt;- matrix(runif(300), 100, 3)
response &lt;- apply(design, 1, hartman3)
</code></pre>

<hr>
<h2 id='hartman6'> 6D test function</h2><span id='topic+hartman6'></span>

<h3>Description</h3>

<p>Hartman 6-dimensional test function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hartman6(x)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hartman6_+3A_x">x</code></td>
<td>
<p> a 6-dimensional vector specifying the location where the function is to be evaluated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The hartman6 function is defined over the domain <code>[0,1]^6</code>. It has 1 global minimum : 
x1 = c(0.20168952, 0.15001069, 0.47687398, 0.27533243, 0.31165162, 0.65730054)
</p>


<h3>Value</h3>

<p>A real number equal to the hartman6 function values at <code>x</code>
</p>


<h3>Author(s)</h3>

<p> D. Ginsbourger, Ecole des Mines de St-Etienne. </p>


<h3>Examples</h3>

<pre><code class='language-R'> 
design &lt;- matrix(runif(600), 100, 6)
response &lt;- apply(design, 1, hartman6)
</code></pre>

<hr>
<h2 id='inputnames'>Get the input variables names</h2><span id='topic+inputnames'></span>

<h3>Description</h3>

<p>Get the names of the input variables.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  inputnames(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="inputnames_+3A_x">x</code></td>
<td>
<p> an object containing the covariance structure.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of character strings containing the names of the input variables.
</p>

<hr>
<h2 id='kernelname'>Get the kernel name</h2><span id='topic+kernelname'></span>

<h3>Description</h3>

<p>Get the name of the underlying tensor-product covariance structure.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  kernelname(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kernelname_+3A_x">x</code></td>
<td>
<p> an object containing the covariance structure.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character string.
</p>

<hr>
<h2 id='km'> Fit and/or create kriging models </h2><span id='topic+km'></span>

<h3>Description</h3>

<p><code>km</code> is used to fit kriging models when parameters are unknown, or to create <code>km</code> objects otherwise. In both cases, the result is a <code>km</code> object. If parameters are unknown, they are estimated by Maximum Likelihood. As a beta version, Penalized Maximum Likelihood Estimation is also possible if some penalty is given, or Leave-One-Out for noise-free observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>km(formula=~1, design, response, covtype="matern5_2",
   coef.trend = NULL, coef.cov = NULL, coef.var = NULL,
   nugget = NULL, nugget.estim=FALSE, noise.var=NULL, estim.method="MLE",
   penalty = NULL, optim.method = "BFGS", lower = NULL, upper = NULL, 
   parinit = NULL, multistart = 1, control = NULL, gr = TRUE, 
   iso=FALSE, scaling=FALSE, knots=NULL, kernel=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="km_+3A_formula">formula</code></td>
<td>
<p> an optional object of class &quot;formula&quot; specifying the linear trend of the kriging model (see <code><a href="stats.html#topic+lm">lm</a></code>). This formula should concern only the input variables, and not the output (response). If there is any, it is automatically dropped. In particular, no response transformation is available yet. The default is <code>~1</code>, which defines a constant trend.</p>
</td></tr>
<tr><td><code id="km_+3A_design">design</code></td>
<td>
<p> a data frame representing the design of experiments. The ith row contains the values of the d input variables corresponding to the ith evaluation</p>
</td></tr>
<tr><td><code id="km_+3A_response">response</code></td>
<td>
<p> a vector (or 1-column matrix or data frame) containing the values of the 1-dimensional output given by the objective function at the <code>design</code> points. </p>
</td></tr>
<tr><td><code id="km_+3A_covtype">covtype</code></td>
<td>
<p> an optional character string specifying the covariance structure to be used, to be chosen between <code>"gauss"</code>, <code>"matern5_2"</code>, <code>"matern3_2"</code>, <code>"exp"</code> or <code>"powexp"</code>. See a full description of available covariance kernels in <code><a href="#topic+covTensorProduct-class">covTensorProduct-class</a></code>. Default is <code>"matern5_2"</code>. See also the argument <code>kernel</code> that allows the user to build its own covariance structure.</p>
</td></tr>
<tr><td><code id="km_+3A_coef.trend">coef.trend</code></td>
<td>
<p> (see below)</p>
</td></tr>
<tr><td><code id="km_+3A_coef.cov">coef.cov</code></td>
<td>
<p> (see below)</p>
</td></tr>
<tr><td><code id="km_+3A_coef.var">coef.var</code></td>
<td>
<p> optional vectors containing the values for the trend, covariance and variance parameters. For estimation, 4 cases are implemented: 1. (All unknown) If all are missing, all are estimated. 2. (All known) If all are provided, no estimation is performed; 3. (Known trend) If <code>coef.trend</code> is provided but at least one of <code>coef.cov</code> or <code>coef.var</code> is missing, then BOTH <code>coef.cov</code> and <code>coef.var</code> are estimated; 4. (Unknown trend) If <code>coef.cov</code> and <code>coef.var</code> are provided but <code>coef.trend</code> is missing, then <code>coef.trend</code> is estimated (GLS formula).</p>
</td></tr>
<tr><td><code id="km_+3A_nugget">nugget</code></td>
<td>
<p> an optional variance value standing for the homogeneous nugget effect.</p>
</td></tr>
<tr><td><code id="km_+3A_nugget.estim">nugget.estim</code></td>
<td>
<p> an optional boolean indicating whether the nugget effect should be estimated. Note that this option does not concern the case of heterogeneous noisy observations (see <code>noise.var</code> below). If <code>nugget</code> is given, it is used as an initial value. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="km_+3A_noise.var">noise.var</code></td>
<td>
<p> for noisy observations : an optional vector containing the noise variance at each observation. This is useful for stochastic simulators. Default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="km_+3A_estim.method">estim.method</code></td>
<td>
<p>  a character string specifying the method by which unknown parameters are estimated. Default is <code>"MLE"</code> (Maximum Likelihood). At this stage, a beta version of leave-One-Out estimation (<code>estim.method="LOO"</code>) is also implemented for noise-free observations.</p>
</td></tr>
<tr><td><code id="km_+3A_penalty">penalty</code></td>
<td>
<p> (beta version) an optional list suitable for Penalized Maximum Likelihood Estimation. The list must contain the item <code>fun</code> indicating the penalty function, and the item <code>value</code> equal to the value of the penalty parameter. At this stage the only available <code>fun</code> is <code>"SCAD"</code>, and <code>covtype</code> must be <code>"gauss"</code>. Default is <code>NULL</code>, corresponding to (un-penalized) Maximum Likelihood Estimation.</p>
</td></tr>
<tr><td><code id="km_+3A_optim.method">optim.method</code></td>
<td>
<p> an optional character string indicating which optimization method is chosen for the likelihood maximization. <code>"BFGS"</code> is the <code>optim</code> quasi-Newton procedure of package <code>stats</code>, with the method &quot;L-BFGS-B&quot;. <code>"gen"</code> is the <code>genoud</code> genetic algorithm (using derivatives) from package <code>rgenoud</code> (&gt;= 5.3.3). </p>
</td></tr>
<tr><td><code id="km_+3A_lower">lower</code></td>
<td>
<p> (see below) </p>
</td></tr>
<tr><td><code id="km_+3A_upper">upper</code></td>
<td>
<p> optional vectors containing the bounds of the correlation parameters for optimization. The default values are given by <code><a href="#topic+covParametersBounds">covParametersBounds</a></code>. </p>
</td></tr>
<tr><td><code id="km_+3A_parinit">parinit</code></td>
<td>
<p> an optional vector containing the initial values for the variables to be optimized over. If no vector is given, an initial point is generated as follows. For method <code>"gen"</code>, the initial point is generated uniformly inside the hyper-rectangle domain defined by <code>lower</code> and <code>upper</code>. For method <code>"BFGS"</code>, some points (see <code>control</code> below) are generated uniformly in the domain. Then the best point with respect to the likelihood (or penalized likelihood, see <code>penalty</code>) criterion is chosen. </p>
</td></tr>
<tr><td><code id="km_+3A_multistart">multistart</code></td>
<td>
<p> an optional integer indicating the number of initial points from which running the BFGS optimizer. These points will be selected as the best <code>multistart</code> one(s) among those evaluated (see above <code>parinit</code>). The multiple optimizations will be performed in parallel provided that a parallel backend is registered (see package <code>foreach</code>).</p>
</td></tr> 
<tr><td><code id="km_+3A_control">control</code></td>
<td>
<p> an optional list of control parameters for optimization. See details below.</p>
</td></tr>
<tr><td><code id="km_+3A_gr">gr</code></td>
<td>
<p> an optional boolean indicating whether the analytical gradient should be used. Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="km_+3A_iso">iso</code></td>
<td>
<p> an optional boolean that can be used to force a tensor-product covariance structure (see <code><a href="#topic+covTensorProduct-class">covTensorProduct-class</a></code>) to have a range parameter common to all dimensions. Default is <code>FALSE</code>. Not used (at this stage) for the power-exponential type.</p>
</td></tr>
<tr><td><code id="km_+3A_scaling">scaling</code></td>
<td>
<p> an optional boolean indicating whether a scaling on the covariance structure should be used.</p>
</td></tr>
<tr><td><code id="km_+3A_knots">knots</code></td>
<td>
<p> an optional list of knots for scaling. The j-th element is a vector containing the knots for dimension j. If <code>scaling=TRUE</code> and knots are not specified, than knots are fixed to 0 and 1 in each dimension (which corresponds to affine scaling for the domain [0,1]^d).</p>
</td></tr>
<tr><td><code id="km_+3A_kernel">kernel</code></td>
<td>
<p> an optional function containing a new covariance structure. At this stage, the parameters must be provided as well, and are not estimated. See an example below.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The optimisers are tunable by the user by the argument <code>control</code>. 
Most of the control parameters proposed by <code>BFGS</code> and <code>genoud</code> can be passed to <code>control</code> except the ones that must be forced [for the purpose of optimization setting], as indicated in the table below. See <code><a href="stats.html#topic+optim">optim</a></code> and  <code><a href="rgenoud.html#topic+genoud">genoud</a></code> to get more details about them.
</p>

<table>
<tr>
 <td style="text-align: left;">
BFGS </td><td style="text-align: left;"> <code>trace</code>, <code>parscale</code>, <code>ndeps</code>, <code>maxit</code>, <code>abstol</code>, <code>reltol</code>, <code>REPORT</code>, <code>lnm</code>, <code>factr</code>, <code>pgtol</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
genoud </td><td style="text-align: left;"> all parameters EXCEPT: <code>fn, nvars, max, starting.values, Domains, gr, gradient.check, boundary.enforcement, hessian</code> and <code>optim.method</code>. </td>
</tr>
<tr>
 <td style="text-align: left;"> 
</td>
</tr>

</table>

<p>Notice that the right places to specify the optional starting values and boundaries are in <code>parinit</code> and <code>lower, upper</code>, as explained above. Some additional possibilities and initial values are indicated in the table below:
</p>

<table>
<tr>
 <td style="text-align: left;">
<code>trace</code> </td><td style="text-align: left;"> Turn it to <code>FALSE</code> to avoid printing during optimization progress.</td>
</tr>
<tr>
 <td style="text-align: left;">
<code>pop.size</code> </td><td style="text-align: left;"> For method <code>"BFGS"</code>, it is the number of candidate initial points generated before optimization starts (see <code>parinit</code> above). Default is 20. For method <code>"gen"</code>, <code>"pop.size"</code> is the population size, set by default at min(20, 4+3*log(nb of variables) </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>max.generations</code> </td><td style="text-align: left;"> Default is 5 </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>wait.generations</code> </td><td style="text-align: left;"> Default is 2 </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>BFGSburnin</code> </td><td style="text-align: left;"> Default is 0 </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Value</h3>

<p>An object of class <code>km</code> (see <code><a href="#topic+km-class">km-class</a></code>).
</p>


<h3>Author(s)</h3>

<p> O. Roustant, D. Ginsbourger, Ecole des Mines de St-Etienne. </p>


<h3>References</h3>

 
<p>N.A.C. Cressie (1993), <em>Statistics for spatial data</em>, Wiley series in probability and mathematical statistics.
</p>
<p>D. Ginsbourger (2009), <em>Multiples metamodeles pour l'approximation et l'optimisation
de fonctions numeriques multivariables</em>, Ph.D. thesis, Ecole Nationale Superieure des
Mines de Saint-Etienne, 2009.
</p>
<p>D. Ginsbourger, D. Dupuy, A. Badea, O. Roustant, and L. Carraro (2009), A note on the choice and the estimation of kriging models for the analysis of deterministic computer experiments, <em>Applied Stochastic Models for Business and Industry</em>, <b>25</b> no. 2, 115-131.
</p>
<p>A.G. Journel and M.E. Rossi (1989), When do we need a trend model in kriging ?, <em>Mathematical Geology</em>, <b>21</b> no. 7, 715-739.
</p>
<p>D.G. Krige (1951), A statistical approach to some basic mine valuation problems on the witwatersrand, <em>J. of the Chem., Metal. and Mining Soc. of South Africa</em>, <b>52</b> no. 6, 119-139.
</p>
<p>R. Li and A. Sudjianto (2005), Analysis of Computer Experiments Using Penalized Likelihood in Gaussian Kriging Models, <em>Technometrics</em>, <b>47</b> no. 2, 111-120.
</p>
<p>K.V. Mardia and R.J. Marshall (1984), Maximum likelihood estimation of models for residual covariance in spatial regression, <em>Biometrika</em>, <b>71</b>, 135-146.
</p>
<p>J.D. Martin and T.W. Simpson (2005), Use of kriging models to approximate deterministic computer models, <em>AIAA Journal</em>, <b>43</b> no. 4, 853-863.
</p>
<p>G. Matheron (1969), Le krigeage universel, <em>Les Cahiers du Centre de Morphologie Mathematique de Fontainebleau</em>, <b>1</b>.
</p>
<p>W.R. Jr. Mebane and J.S. Sekhon, in press (2009), Genetic optimization using derivatives: The rgenoud package for R, <em>Journal of Statistical Software</em>.
</p>
<p>J.-S. Park and J. Baek (2001), Efficient computation of maximum likelihood estimators in a spatial linear model with power exponential covariogram, <em>Computer Geosciences</em>, <b>27</b> no. 1, 1-7.
</p>
<p>C.E. Rasmussen and C.K.I. Williams (2006), <em>Gaussian Processes for Machine Learning</em>, the MIT Press, <a href="http://www.gaussianprocess.org/gpml/">http://www.gaussianprocess.org/gpml/</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+kmData">kmData</a></code> for another interface with the data,
<code><a href="#topic+show+2Ckm-method">show,km-method</a></code>,
<code><a href="#topic+predict+2Ckm-method">predict,km-method</a></code>,
<code><a href="#topic+plot+2Ckm-method">plot,km-method</a></code>.
Some programming details and initialization choices can be found in <code><a href="#topic+kmEstimate">kmEstimate</a></code>, <code><a href="#topic+kmNoNugget.init">kmNoNugget.init</a></code>,
<code><a href="#topic+km1Nugget.init">km1Nugget.init</a></code> and <code><a href="#topic+kmNuggets.init">kmNuggets.init</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>
# ----------------------------------
# A 2D example - Branin-Hoo function
# ----------------------------------

# a 16-points factorial design, and the corresponding response
d &lt;- 2; n &lt;- 16
design.fact &lt;- expand.grid(x1=seq(0,1,length=4), x2=seq(0,1,length=4))
y &lt;- apply(design.fact, 1, branin) 

# kriging model 1 : matern5_2 covariance structure, no trend, no nugget effect
m1 &lt;- km(design=design.fact, response=y)

# kriging model 2 : matern5_2 covariance structure, 
#                   linear trend + interactions, no nugget effect
m2 &lt;- km(~.^2, design=design.fact, response=y)

# graphics 
n.grid &lt;- 50
x.grid &lt;- y.grid &lt;- seq(0,1,length=n.grid)
design.grid &lt;- expand.grid(x1=x.grid, x2=y.grid)
response.grid &lt;- apply(design.grid, 1, branin)
predicted.values.model1 &lt;- predict(m1, design.grid, "UK")$mean
predicted.values.model2 &lt;- predict(m2, design.grid, "UK")$mean
par(mfrow=c(3,1))
contour(x.grid, y.grid, matrix(response.grid, n.grid, n.grid), 50, main="Branin")
points(design.fact[,1], design.fact[,2], pch=17, cex=1.5, col="blue")
contour(x.grid, y.grid, matrix(predicted.values.model1, n.grid, n.grid), 50, 
        main="Ordinary Kriging")
points(design.fact[,1], design.fact[,2], pch=17, cex=1.5, col="blue")
contour(x.grid, y.grid, matrix(predicted.values.model2, n.grid, n.grid), 50, 
        main="Universal Kriging")
points(design.fact[,1], design.fact[,2], pch=17, cex=1.5, col="blue")
par(mfrow=c(1,1))


# (same example) how to use the multistart argument
# -------------------------------------------------
require(foreach)

# below an example for a computer with 2 cores, but also work with 1 core

nCores &lt;- 2
require(doParallel)
cl &lt;-  makeCluster(nCores) 
registerDoParallel(cl)

# kriging model 1, with 4 starting points
m1_4 &lt;- km(design=design.fact, response=y, multistart=4)

stopCluster(cl)

# -------------------------------
# A 1D example with penalized MLE
# -------------------------------

# from Fang K.-T., Li R. and Sudjianto A. (2006), "Design and Modeling for 
# Computer Experiments", Chapman &amp; Hall, pages 145-152

n &lt;- 6; d &lt;- 1
x &lt;- seq(from=0, to=10, length=n)
y &lt;- sin(x)
t &lt;- seq(0,10, length=100)

# one should add a small nugget effect, to avoid numerical problems
epsilon &lt;- 1e-3
model &lt;- km(formula&lt;- ~1, design=data.frame(x=x), response=data.frame(y=y), 
            covtype="gauss", penalty=list(fun="SCAD", value=3), nugget=epsilon)

p &lt;- predict(model, data.frame(x=t), "UK")

plot(t, p$mean, type="l", xlab="x", ylab="y", 
                     main="Prediction via Penalized Kriging")
points(x, y, col="red", pch=19)
lines(t, sin(t), lty=2, col="blue")
legend(0, -0.5, legend=c("Sine Curve", "Sample", "Fitted Curve"), 
       pch=c(-1,19,-1), lty=c(2,-1,1), col=c("blue","red","black"))


# ------------------------------------------------------------------------
# A 1D example with known trend and known or unknown covariance parameters
# ------------------------------------------------------------------------

x &lt;- c(0, 0.4, 0.6, 0.8, 1);
y &lt;- c(-0.3, 0, -0.8, 0.5, 0.9)

theta &lt;- 0.01; sigma &lt;- 3; trend &lt;- c(-1,2)

model &lt;- km(~x, design=data.frame(x=x), response=data.frame(y=y), 
            covtype="matern5_2", coef.trend=trend, coef.cov=theta, 
            coef.var=sigma^2)

# below: if you want to specify trend only, and estimate both theta and sigma:
# model &lt;- km(~x, design=data.frame(x=x), response=data.frame(y=y), 
#             covtype="matern5_2", coef.trend=trend, lower=0.2)
# Remark: a lower bound or penalty function is useful here,
#         due to the very small number of design points...

# kriging with gaussian covariance C(x,y)=sigma^2 * exp(-[(x-y)/theta]^2), 
#         and linear trend t(x) = -1 + 2x

t &lt;- seq(from=0, to=1, by=0.005)
p &lt;- predict(model, newdata=data.frame(x=t), type="SK")
# beware that type = "SK" for known parameters (default is "UK")

plot(t, p$mean, type="l", ylim=c(-7,7), xlab="x", ylab="y")
lines(t, p$lower95, col="black", lty=2)
lines(t, p$upper95, col="black", lty=2)
points(x, y, col="red", pch=19)
abline(h=0)


# --------------------------------------------------------------
# Kriging with noisy observations (heterogeneous noise variance)
# --------------------------------------------------------------

fundet &lt;- function(x){
return((sin(10*x)/(1+x)+2*cos(5*x)*x^3+0.841)/1.6)
}

level &lt;- 0.5; epsilon &lt;- 0.1
theta &lt;- 1/sqrt(30); p &lt;- 2; n &lt;- 10
x &lt;- seq(0,1, length=n)

# Heteregeneous noise variances: number of Monte Carlo evaluation among 
#                                a total budget of 1000 stochastic simulations
MC_numbers &lt;- c(10,50,50,290,25,75,300,10,40,150)
noise.var &lt;- 3/MC_numbers

# Making noisy observations from 'fundet' function (defined above)
y &lt;- fundet(x) + noise.var*rnorm(length(x))

# kriging model definition (no estimation here)
model &lt;- km(y~1, design=data.frame(x=x), response=data.frame(y=y), 
            covtype="gauss", coef.trend=0, coef.cov=theta, coef.var=1, 
            noise.var=noise.var)

# prediction
t &lt;- seq(0, 1, by=0.01)
p &lt;- predict.km(model, newdata=data.frame(x=t), type="SK")
lower &lt;- p$lower95; upper &lt;- p$upper95

# graphics
par(mfrow=c(1,1))
plot(t, p$mean, type="l", ylim=c(1.1*min(c(lower,y)) , 1.1*max(c(upper,y))), 
                xlab="x", ylab="y",col="blue", lwd=1.5)
polygon(c(t,rev(t)), c(lower, rev(upper)), col=gray(0.9), border = gray(0.9))
lines(t, p$mean, type="l", ylim=c(min(lower) ,max(upper)), xlab="x", ylab="y",
                 col="blue", lwd=1)
lines(t, lower, col="blue", lty=4, lwd=1.7)
lines(t, upper, col="blue", lty=4, lwd=1.7)
lines(t, fundet(t), col="black", lwd=2)
points(x, y, pch=8,col="blue")
text(x, y, labels=MC_numbers, pos=3)


# -----------------------------
# Checking parameter estimation 
# -----------------------------

d &lt;- 3       	# problem dimension
n &lt;- 40			# size of the experimental design
design &lt;- matrix(runif(n*d), n, d)

covtype &lt;- "matern5_2"		
theta &lt;- c(0.3, 0.5, 1)		# the parameters to be found by estimation
sigma &lt;- 2
nugget &lt;- NULL  # choose a numeric value if you want to estimate nugget 
nugget.estim &lt;- FALSE # choose TRUE if you want to estimate it

n.simu &lt;- 30		# number of simulations
sigma2.estimate &lt;- nugget.estimate &lt;- mu.estimate &lt;- matrix(0, n.simu, 1)
coef.estimate &lt;- matrix(0, n.simu, length(theta))

model &lt;- km(~1, design=data.frame(design), response=rep(0,n), covtype=covtype, 
            coef.trend=0, coef.cov=theta, coef.var=sigma^2, nugget=nugget)
y &lt;- simulate(model, nsim=n.simu)

for (i in 1:n.simu) {
	# parameter estimation: tune the optimizer by changing optim.method, control
	model.estimate &lt;- km(~1, design=data.frame(design), response=data.frame(y=y[i,]), 
	covtype=covtype, optim.method="BFGS", control=list(pop.size=50, trace=FALSE), 
        nugget.estim=nugget.estim) 
	
	# store results
	coef.estimate[i,] &lt;- covparam2vect(model.estimate@covariance)
	sigma2.estimate[i] &lt;- model.estimate@covariance@sd2
	mu.estimate[i] &lt;- model.estimate@trend.coef
	if (nugget.estim) nugget.estimate[i] &lt;- model.estimate@covariance@nugget
}

# comparison true values / estimation
cat("\nResults with ", n, "design points, 
    obtained with ", n.simu, "simulations\n\n",
    "Median of covar. coef. estimates: ", apply(coef.estimate, 2, median), "\n",
    "Median of trend  coef. estimates: ", median(mu.estimate), "\n", 
    "Mean of the var. coef. estimates: ", mean(sigma2.estimate))
if (nugget.estim) cat("\nMean of the nugget effect estimates: ", 
                      mean(nugget.estimate))

# one figure for this specific example - to be adapted
split.screen(c(2,1))        # split display into two screens
split.screen(c(1,2), screen = 2) # now split the bottom half into 3

screen(1)
boxplot(coef.estimate[,1], coef.estimate[,2], coef.estimate[,3], 
        names=c("theta1", "theta2", "theta3"))
abline(h=theta, col="red")
fig.title &lt;- paste("Empirical law of the parameter estimates 
                    (n=", n , ", n.simu=", n.simu, ")", sep="")
title(fig.title)

screen(3)
boxplot(mu.estimate, xlab="mu")
abline(h=0, col="red")

screen(4)
boxplot(sigma2.estimate, xlab="sigma2")
abline(h=sigma^2, col="red")

close.screen(all = TRUE)  

# ----------------------------------------------------------
# Kriging with non-linear scaling on Xiong et al.'s function
# ----------------------------------------------------------

f11_xiong &lt;- function(x){ 
return( sin(30 * (x - 0.9)^4) * cos(2 * (x - 0.9)) + (x - 0.9) / 2)
}

t &lt;- seq(0, 1, , 300)
f &lt;- f11_xiong(t)

plot(t, f, type = "l", ylim = c(-1,0.6), lwd = 2)

doe &lt;- data.frame(x = seq(0, 1, , 20))
resp &lt;- f11_xiong(doe)

knots &lt;- list(x = c(0, 0.5, 1)) 
eta &lt;- list(c(15, 2, 0.5))
m &lt;- km(design = doe, response = resp, scaling = TRUE, gr = TRUE, 
knots = knots, covtype = "matern5_2",  coef.var = 1, coef.trend = 0)

p &lt;- predict(m, data.frame(x = t), "UK")

plot(t, f, type = "l", ylim = c(-1, 0.6), lwd = 2)

lines(t, p$mean, col = "blue", lty = 2, lwd = 2)
lines(t, p$mean + 2 * p$sd, col = "blue")
lines(t, p$mean - 2 * p$sd, col = "blue")

abline(v = knots[[1]], lty = 2, col = "green")


# -----------------------------------------------------
# Kriging with a symmetric kernel: example with covUser
# -----------------------------------------------------

x &lt;- c(0, 0.15, 0.3, 0.4, 0.5)
y &lt;- c(0.3, -0.2, 0, 0.5, 0.2)

k &lt;- function(x,y) {
  theta &lt;- 0.15
  0.5*exp(-((x-y)/theta)^2) + 0.5*exp(-((1-x-y)/theta)^2)    
}

muser &lt;- km(design=data.frame(x=x), response=data.frame(y=y), 
            coef.trend=0, kernel=k)

u &lt;- seq(from=0, to=1, by=0.01)
puser &lt;- predict(muser, newdata=data.frame(x=u), type="SK")

set.seed(0)
nsim &lt;- 5
zuser &lt;- simulate(muser, nsim=nsim, newdata=data.frame(x=u), cond=TRUE, nugget.sim=1e-8)
par(mfrow=c(1,1))
matplot(u, t(zuser), type="l", lty=rep("solid", nsim), col=1:5, lwd=1)
polygon(c(u, rev(u)), c(puser$upper, rev(puser$lower)), col="lightgrey", border=NA)
lines(u, puser$mean, lwd=5, col="blue", lty="dotted")
matlines(u, t(zuser), type="l", lty=rep("solid", nsim), col=1:5, lwd=1)
points(x, y, pch=19, cex=1.5)

</code></pre>

<hr>
<h2 id='km-class'> Kriging models class </h2><span id='topic+km-class'></span><span id='topic+coef+2Ckm-method'></span>

<h3>Description</h3>

<p>	S4 class for kriging models.</p>


<h3>Objects from the Class</h3>

<p>To create a <code>km</code> object, use <code><a href="#topic+km">km</a></code>. See also this function for more details.
</p>


<h3>Slots</h3>


<dl>
<dt><code>d</code>:</dt><dd><p>Object of class <code>"integer"</code>. The spatial dimension. </p>
</dd>
<dt><code>n</code>:</dt><dd><p>Object of class <code>"integer"</code>. The number of observations. </p>
</dd>
<dt><code>X</code>:</dt><dd><p>Object of class <code>"matrix"</code>. The design of experiments. </p>
</dd>
<dt><code>y</code>:</dt><dd><p>Object of class <code>"matrix"</code>. The vector of response values at design points. </p>
</dd>
<dt><code>p</code>:</dt><dd><p>Object of class <code>"integer"</code>. The number of basis functions of the linear trend. </p>
</dd>
<dt><code>F</code>:</dt><dd><p>Object of class <code>"matrix"</code>. The experimental matrix corresponding to the evaluation of the linear trend basis functions at the design of experiments.</p>
</dd>
<dt><code>trend.formula</code>:</dt><dd><p>Object of class <code>"formula"</code>. A formula specifying the trend as a linear model (no response needed). </p>
</dd>
<dt><code>trend.coef</code>:</dt><dd><p>Object of class <code>"numeric"</code>. Trend coefficients. </p>
</dd>
<dt><code>covariance</code>:</dt><dd><p>Object of class <code>"covTensorProduct"</code>. See <code><a href="#topic+covTensorProduct-class">covTensorProduct-class</a></code>. </p>
</dd>
<dt><code>noise.flag</code>:</dt><dd><p>Object of class <code>"logical"</code>. Are the observations noisy? </p>
</dd>
<dt><code>noise.var</code>:</dt><dd><p>Object of class <code>"numeric"</code>. If the observations are noisy, the vector of noise variances. </p>
</dd>
<dt><code>known.param</code>:</dt><dd><p>Object of class <code>"character"</code>. Internal use. One of: <code>"None", "All", "CovAndVar"</code> or <code>"Trend"</code>. </p>
</dd>
<dt><code>case</code>:</dt><dd><p>Object of class <code>"character"</code>. Indicates the likelihood to use in estimation (Internal use). One of: <code>"LLconcentration_beta", "LLconcentration_beta_sigma2", "LLconcentration_beta_v_alpha"</code>. </p>
</dd>
<dt><code>param.estim</code>:</dt><dd><p>Object of class <code>"logical"</code>. <code>TRUE</code> if at least one parameter is estimated, <code>FALSE</code> otherwise. </p>
</dd>
<dt><code>method</code>:</dt><dd><p>Object of class <code>"character"</code>. <code>"MLE"</code> or <code>"PMLE"</code> depending on <code>penalty</code>. </p>
</dd>
<dt><code>penalty</code>:</dt><dd><p>Object of class <code>"list"</code>. For penalized ML estimation. </p>
</dd>
<dt><code>optim.method</code>:</dt><dd><p>Object of class <code>"character"</code>. To be chosen between <code>"BFGS"</code> and <code>"gen"</code>.</p>
</dd>
<dt><code>lower</code>:</dt><dd><p>Object of class <code>"numeric"</code>. Lower bounds for covariance parameters estimation. </p>
</dd>
<dt><code>upper</code>:</dt><dd><p>Object of class <code>"numeric"</code>. Upper bounds for covariance parameters estimation. </p>
</dd>
<dt><code>control</code>:</dt><dd><p>Object of class <code>"list"</code>. Additional control parameters for covariance parameters estimation. </p>
</dd>
<dt><code>gr</code>:</dt><dd><p>Object of class <code>"logical"</code>. Do you want analytical gradient to be used ? </p>
</dd>
<dt><code>call</code>:</dt><dd><p>Object of class <code>"language"</code>. User call reminder. </p>
</dd>
<dt><code>parinit</code>:</dt><dd><p>Object of class <code>"numeric"</code>. Initial values for covariance parameters estimation. </p>
</dd>
<dt><code>logLik</code>:</dt><dd><p>Object of class <code>"numeric"</code>. Value of the concentrated log-Likelihood at its optimum. </p>
</dd>
<dt><code>T</code>:</dt><dd><p>Object of class <code>"matrix"</code>. Triangular matrix delivered by the Choleski decomposition of the covariance matrix. </p>
</dd>
<dt><code>z</code>:</dt><dd><p>Object of class <code>"numeric"</code>. Auxiliary variable: see <code><a href="#topic+computeAuxVariables">computeAuxVariables</a></code>. </p>
</dd>
<dt><code>M</code>:</dt><dd><p>Object of class <code>"matrix"</code>. Auxiliary variable: see <code><a href="#topic+computeAuxVariables">computeAuxVariables</a></code>. </p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>coef</dt><dd><p><code>signature(x = "km")</code> Get the coefficients of the <code>km</code> object. </p>
</dd>
<dt>plot</dt><dd><p><code>signature(x = "km")</code>: see <code><a href="#topic+plot+2Ckm-method">plot,km-method</a></code>. </p>
</dd>
<dt>predict</dt><dd><p><code>signature(object = "km")</code>: see <code><a href="#topic+predict+2Ckm-method">predict,km-method</a></code>. </p>
</dd>
<dt>show</dt><dd><p><code>signature(object = "km")</code>: see <code><a href="#topic+show+2Ckm-method">show,km-method</a></code>. </p>
</dd>
<dt>simulate</dt><dd><p><code>signature(object = "km")</code>: see <code><a href="#topic+simulate+2Ckm-method">simulate,km-method</a></code>. </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p> O. Roustant, D. Ginsbourger</p>


<h3>See Also</h3>

 <p><code><a href="#topic+km">km</a></code> for more details about slots and to create a <code>km</code> object, <code><a href="#topic+covStruct.create">covStruct.create</a></code> to construct a covariance structure, and <code><a href="#topic+covTensorProduct-class">covTensorProduct-class</a></code> for the S4 covariance class defined in this package.</p>

<hr>
<h2 id='km1Nugget.init'> Fitting Kriging Models</h2><span id='topic+km1Nugget.init'></span>

<h3>Description</h3>

<p><code>km1Nugget.init</code> is used to give good initial values to fit kriging models when there is an unknown nugget effect to be estimated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>km1Nugget.init(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="km1Nugget.init_+3A_model">model</code></td>
<td>
<p> an object of class <code>km</code>. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The procedure can be summarized in 4 stages : 
</p>

<table>
<tr>
 <td style="text-align: left;">
  1) </td><td style="text-align: left;"> Compute the variogram and deduce a first estimation of the total variance. If an initial value is provided for <code>nugget</code>, check its compatibility with the estimated variance. If not, use again the variogram to give a first estimation of the nugget effect. </td>
</tr>
<tr>
 <td style="text-align: left;">
  2) </td><td style="text-align: left;"> Simulate several values for the nugget effect and the process variance, around the estimations obtained at stage 1). The number of simulations is the one given in <code>model@control$pop.size</code>. </td>
</tr>
<tr>
 <td style="text-align: left;">
  3) </td><td style="text-align: left;"> If no initial value is provided for the other covariance parameters, simulate them uniformly inside the domain delimited by <code>model@lower</code> and <code>model@upper</code>. The number of simulations is the same as in stage 2). </td>
</tr>
<tr>
 <td style="text-align: left;">
  4) </td><td style="text-align: left;"> Compute the likelihood at each simulated "point" (variance + nugget effect + other covariance parameters), and take the best(s) one(s). This(these) point(s) gives the first initial value(s). The number of values considered can be set by the argument <code>multistart</code> in <code><a href="#topic+km">km</a></code>.
   </td>
</tr>

</table>



<h3>Value</h3>

<table>
<tr><td><code>par</code></td>
<td>
<p> a matrix whose rows contain initial vectors of parameters.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p> a vector containing the function values corresponding to <code>par</code>.</p>
</td></tr>
<tr><td><code>cov</code></td>
<td>
<p> a list containing the covariance objects corresponding to <code>par</code>.</p>
</td></tr>
<tr><td><code>lower</code></td>
<td>
<p> , </p>
</td></tr>
<tr><td><code>upper</code></td>
<td>
<p> vectors containing lower and upper bounds for parameters.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> O. Roustant, David Ginsbourger, Ecole des Mines de St-Etienne. </p>


<h3>See Also</h3>

 <p><code><a href="#topic+km">km</a></code>, <code><a href="#topic+kmEstimate">kmEstimate</a></code> </p>

<hr>
<h2 id='kmData'> Fit and/or create kriging models </h2><span id='topic+kmData'></span>

<h3>Description</h3>

 <p><code>kmData</code> is equivalent to <code>km</code>, except for the interface with the data. In <code>kmData</code>, the user must supply both the design and the response within a single data.frame <code>data</code>. To supply them separately, use <code>km</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kmData(formula, data, inputnames = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kmData_+3A_formula">formula</code></td>
<td>
<p> an object of class &quot;formula&quot; specifying the linear trend of the kriging model (see <code><a href="stats.html#topic+lm">lm</a></code>). At this stage, transformations of the response are not taken into account.</p>
</td></tr>
<tr><td><code id="kmData_+3A_data">data</code></td>
<td>
<p> a data.frame containing both the design (input variables) and the response (1-dimensional output given by the objective function at the design points).</p>
</td></tr>
<tr><td><code id="kmData_+3A_inputnames">inputnames</code></td>
<td>
<p> an optional vector of character containing the names of variables in <code>data</code> to be considered as input variables. By default, all variables but the response are input variables.</p>
</td></tr>
<tr><td><code id="kmData_+3A_...">...</code></td>
<td>
<p> other arguments for creating or fitting Kriging models, to be taken among the arguments of <code>km</code> function apart from <code>design</code> and <code>response</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>km</code> (see <code><a href="#topic+km-class">km-class</a></code>).
</p>


<h3>Author(s)</h3>

<p>O. Roustant
</p>


<h3>See Also</h3>

<p><code><a href="#topic+km">km</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># a 16-points factorial design, and the corresponding response
d &lt;- 2; n &lt;- 16
design.fact &lt;- expand.grid(x1=seq(0,1,length=4), x2=seq(0,1,length=4))
y &lt;- apply(design.fact, 1, branin)
data &lt;- cbind(design.fact, y=y)

# kriging model 1 : matern5_2 covariance structure, no trend, no nugget effect
m1 &lt;- kmData(y~1, data=data)
# this is equivalent to: m1 &lt;- km(design=design.fact, response=y)

# now, add a second response to data:
data2 &lt;- cbind(data, y2=-y)
# the previous model is now obtained with:
m1_2 &lt;- kmData(y~1, data=data2, inputnames=c("x1", "x2"))

</code></pre>

<hr>
<h2 id='kmEstimate'> Fitting Kriging Models </h2><span id='topic+kmEstimate'></span>

<h3>Description</h3>

<p><code>kmEstimate</code> is used to fit kriging models. This function should not be called directly, due to the environments defined in <code>km</code> to avoid computing twice <code>nxn</code> matrices. Call <code>km</code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kmEstimate(model, envir)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kmEstimate_+3A_model">model</code></td>
<td>
<p>  an object of class <code>km</code>. </p>
</td></tr>
<tr><td><code id="kmEstimate_+3A_envir">envir</code></td>
<td>
<p>  an environment specifying where to assign intermediate values for future gradient calculations. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>km</code>.
</p>


<h3>Author(s)</h3>

<p> O. Roustant, D. Ginsbourger, Ecole des Mines de St-Etienne. </p>


<h3>References</h3>

<p> Park J-S, Baek J. (2001), Efficient computation of maximum likelihood estimators
in a spatial linear model with power exponential covariogram, <em>Computer Geosciences</em>,
<b>27</b>, 1-7. </p>


<h3>See Also</h3>

 <p><code><a href="#topic+kmNoNugget.init">kmNoNugget.init</a></code>, <code><a href="#topic+km">km</a></code> </p>

<hr>
<h2 id='kmNoNugget.init'> Fitting Kriging Models</h2><span id='topic+kmNoNugget.init'></span>

<h3>Description</h3>

<p><code>kmNoNugget.init</code> is used to give initial values to fit kriging models when there is no nugget effect nor noisy observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kmNoNugget.init(model, fn, fnscale)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kmNoNugget.init_+3A_model">model</code></td>
<td>
<p> an object of class <code>km</code>. </p>
</td></tr>
<tr><td><code id="kmNoNugget.init_+3A_fn">fn</code></td>
<td>
<p> the function considered: <code><a href="#topic+logLikFun">logLikFun</a></code> or  <code><a href="#topic+leaveOneOutFun">leaveOneOutFun</a></code>.</p>
</td></tr>
<tr><td><code id="kmNoNugget.init_+3A_fnscale">fnscale</code></td>
<td>
<p> a real number which sign determines the direction for optimization: &lt;0 for <code>logLikFun</code>, &gt;0 for <code>leaveOneOutFun</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The procedure can be summarized in 2 stages:
</p>

<table>
<tr>
 <td style="text-align: left;"> 
   1) </td><td style="text-align: left;"> If no initial value is provided by the user for the covariance parameters, simulate them uniformly inside the domain delimited by <code>model@lower</code> and <code>model@upper</code>. The number of simulations is the one given in <code>model@control$pop.size</code>. </td>
</tr>
<tr>
 <td style="text-align: left;">
   2) </td><td style="text-align: left;"> Compute the likelihood for each parameters set, and select the one(s) that gives the highest value(s). The number of values considered can be set by the argument <code>multistart</code> in <code><a href="#topic+km">km</a></code>.
   	</td>
</tr>

</table>



<h3>Value</h3>

<table>
<tr><td><code>par</code></td>
<td>
<p> a matrix whose rows contain initial vectors of parameters.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p> a vector containing the function values corresponding to <code>par</code>.</p>
</td></tr>
<tr><td><code>cov</code></td>
<td>
<p> a list containing the covariance objects corresponding to <code>par</code>.</p>
</td></tr>
<tr><td><code>lower</code></td>
<td>
<p> , </p>
</td></tr>
<tr><td><code>upper</code></td>
<td>
<p> vectors containing lower and upper bounds for parameters.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> O. Roustant, David Ginsbourger, Ecole des Mines de St-Etienne. </p>


<h3>See Also</h3>

 <p><code><a href="#topic+km">km</a></code>, <code><a href="#topic+kmEstimate">kmEstimate</a></code> </p>

<hr>
<h2 id='kmNuggets.init'> Fitting Kriging Models</h2><span id='topic+kmNuggets.init'></span>

<h3>Description</h3>

<p><code>kmNuggets.init</code> is used to give initial values to fit kriging models, in presence of noisy observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kmNuggets.init(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kmNuggets.init_+3A_model">model</code></td>
<td>
<p> an object of class <code>km</code>. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The procedure can be summarized in 4 stages:
</p>

<table>
<tr>
 <td style="text-align: left;"> 
  1) </td><td style="text-align: left;"> Compute the variogram and give a first estimation of the process variance, as well as lower and upper bounds. </td>
</tr>
<tr>
 <td style="text-align: left;">
  2) </td><td style="text-align: left;"> Simulate several values for the process variance, around the estimation obtained at stage 1). The number of simulations is the one given in <code>model@control$pop.size</code>. </td>
</tr>
<tr>
 <td style="text-align: left;">
  3) </td><td style="text-align: left;"> If no initial value is provided for the other covariance parameters, simulate them uniformly inside the domain delimited by <code>model@lower</code> and <code>model@upper</code>. The number of simulations is the same as in stage 2). </td>
</tr>
<tr>
 <td style="text-align: left;">
  4) </td><td style="text-align: left;"> Compute the likelihood at each simulated "point" (variance + other covariance parameters), and take the best one(s). This(these) point(s) gives the first initial value(s). The number of values considered can be set by the argument <code>multistart</code> in <code><a href="#topic+km">km</a></code>.
  </td>
</tr>

</table>



<h3>Value</h3>

<table>
<tr><td><code>par</code></td>
<td>
<p> a matrix whose rows contain initial vectors of parameters.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p> a vector containing the function values corresponding to <code>par</code>.</p>
</td></tr>
<tr><td><code>cov</code></td>
<td>
<p> a list containing the covariance objects corresponding to <code>par</code>.</p>
</td></tr>
<tr><td><code>lower</code></td>
<td>
<p> , </p>
</td></tr>
<tr><td><code>upper</code></td>
<td>
<p> vectors containing lower and upper bounds for parameters.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> O. Roustant, David Ginsbourger, Ecole des Mines de St-Etienne. </p>


<h3>See Also</h3>

 <p><code><a href="#topic+km">km</a></code>, <code><a href="#topic+kmEstimate">kmEstimate</a></code> </p>

<hr>
<h2 id='leaveOneOut.km'> Leave-one-out for a km object </h2><span id='topic+leaveOneOut.km'></span>

<h3>Description</h3>

<p>Cross validation by leave-one-out for a <code>km</code> object without noisy observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>leaveOneOut.km(model, type, trend.reestim=FALSE) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="leaveOneOut.km_+3A_model">model</code></td>
<td>
<p> an object of class &quot;km&quot; without noisy observations.</p>
</td></tr>
<tr><td><code id="leaveOneOut.km_+3A_type">type</code></td>
<td>
<p> a character string corresponding to the kriging family, to be chosen between simple kriging (&quot;SK&quot;), or universal kriging (&quot;UK&quot;).</p>
</td></tr>
<tr><td><code id="leaveOneOut.km_+3A_trend.reestim">trend.reestim</code></td>
<td>
<p> should the trend be reestimated when removing an observation? Default to FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Leave-one-out (LOO) consists of computing the prediction at a design point when the corresponding observation is removed from the learning set (and this, for all design points). A quick version of LOO based on Dubrule formula is also implemented; It is limited to 2 cases: <code>type=="SK" &amp; (!trend.reestim)</code> and <code>type=="UK" &amp; trend.reestim</code>. Leave-one-out is not implemented yet for noisy observations.</p>


<h3>Value</h3>

<p> A list composed of
</p>
<table>
<tr><td><code>mean</code></td>
<td>
<p> a vector of length <em>n</em>. The ith coordinate is equal to the kriging mean (including the trend) at the ith observation number when removing it from the learning set, </p>
</td></tr>
<tr><td><code>sd</code></td>
<td>
<p> a vector of length <em>n</em>. The ith coordinate is equal to the kriging standard deviation at the ith observation number when removing it from the learning set,</p>
</td></tr>
</table>
<p>where <em>n</em> is the total number of observations.
</p>


<h3>Warning</h3>

<p>Kriging parameters are not re-estimated when removing one observation. With few points, the re-estimated values can be far from those obtained with the entire learning set. One option is to reestimate the trend coefficients, by setting <code>trend.reestim=TRUE</code>.
</p>


<h3>Author(s)</h3>

<p> O. Roustant, D. Ginsbourger, Ecole des Mines de St-Etienne. </p>


<h3>References</h3>

 
<p>F. Bachoc (2013), Cross Validation and Maximum Likelihood estimations of hyper-parameters of Gaussian processes with model misspecification. <em>Computational Statistics and Data Analysis</em>, <b>66</b>, 55-69. <a href="http://www.lpma.math.upmc.fr/pageperso/bachoc/publications.html">http://www.lpma.math.upmc.fr/pageperso/bachoc/publications.html</a>
</p>
<p>N.A.C. Cressie (1993), <em>Statistics for spatial data</em>, Wiley series in probability and mathematical statistics.
</p>
<p>O. Dubrule (1983), Cross validation of Kriging in a unique neighborhood. <em>Mathematical Geology</em>, <b>15</b>, 687-699.
</p>
<p>J.D. Martin and T.W. Simpson (2005), Use of kriging models to approximate deterministic computer models, <em>AIAA Journal</em>, <b>43</b> no. 4, 853-863.
</p>
<p>M. Schonlau (1997), <em>Computer experiments and global optimization</em>, Ph.D. thesis, University of Waterloo.	
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+predict+2Ckm-method">predict,km-method</a></code>,  <code><a href="#topic+plot+2Ckm-method">plot,km-method</a></code>,
<code><a href="#topic+cv">cv</a></code></p>

<hr>
<h2 id='leaveOneOutFun'>
Leave-one-out least square criterion of a km object
</h2><span id='topic+leaveOneOutFun'></span>

<h3>Description</h3>

<p>Returns the mean of the squared leave-one-out errors, computed with Dubrule's formula.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>leaveOneOutFun(param, model, envir = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="leaveOneOutFun_+3A_param">param</code></td>
<td>
<p> a vector containing the optimization variables.</p>
</td></tr>
<tr><td><code id="leaveOneOutFun_+3A_model">model</code></td>
<td>
<p> an object of class <code>km</code>.</p>
</td></tr>
<tr><td><code id="leaveOneOutFun_+3A_envir">envir</code></td>
<td>
<p> an optional environment specifying where to assign intermediate values for future gradient calculations. Default is NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The mean of the squared leave-one-out errors.
</p>


<h3>Note</h3>

<p>At this stage, only the standard case has been implemented: no nugget effect, no observation noise.
</p>


<h3>Author(s)</h3>

<p>O. Roustant, Ecole des Mines de St-Etienne
</p>


<h3>References</h3>

<p>F. Bachoc (2013), Cross Validation and Maximum Likelihood estimations of hyper-parameters of Gaussian processes with model misspecification. <em>Computational Statistics and Data Analysis</em>, <b>66</b>, 55-69. <a href="http://www.lpma.math.upmc.fr/pageperso/bachoc/publications.html">http://www.lpma.math.upmc.fr/pageperso/bachoc/publications.html</a>
</p>
<p>O. Dubrule (1983), Cross validation of Kriging in a unique neighborhood. <em>Mathematical Geology</em>, <b>15</b>, 687-699.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+leaveOneOut.km">leaveOneOut.km</a></code>, <code><a href="#topic+leaveOneOutGrad">leaveOneOutGrad</a></code>
</p>

<hr>
<h2 id='leaveOneOutGrad'>
Leave-one-out least square criterion - Analytical gradient
</h2><span id='topic+leaveOneOutGrad'></span>

<h3>Description</h3>

<p>Returns the analytical gradient of <code><a href="#topic+leaveOneOutFun">leaveOneOutFun</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>leaveOneOutGrad(param, model, envir)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="leaveOneOutGrad_+3A_param">param</code></td>
<td>
<p> a vector containing the optimization variables.</p>
</td></tr>
<tr><td><code id="leaveOneOutGrad_+3A_model">model</code></td>
<td>
<p> an object of class <code>km</code>.</p>
</td></tr>
<tr><td><code id="leaveOneOutGrad_+3A_envir">envir</code></td>
<td>
<p> an environment specifying where to get intermediate values calculated in <code>leaveOneOutFun</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the gradient of <code>leaveOneOutFun</code> at <code>param</code>.
</p>


<h3>Author(s)</h3>

<p>O. Roustant, Ecole des Mines de St-Etienne
</p>


<h3>References</h3>

<p>F. Bachoc (2013), Cross Validation and Maximum Likelihood estimations of hyper-parameters of Gaussian processes with model misspecification. <em>Computational Statistics and Data Analysis</em>, <b>66</b>, 55-69. <a href="http://www.lpma.math.upmc.fr/pageperso/bachoc/publications.html">http://www.lpma.math.upmc.fr/pageperso/bachoc/publications.html</a>
</p>
<p>O. Dubrule (1983), Cross validation of Kriging in a unique neighborhood. <em>Mathematical Geology</em>, <b>15</b>, 687-699.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+leaveOneOutFun">leaveOneOutFun</a></code>
</p>

<hr>
<h2 id='logLik'> log-likelihood of a km object </h2><span id='topic+logLik'></span><span id='topic+logLik.km'></span><span id='topic+logLik+2Ckm-method'></span>

<h3>Description</h3>

<p>Returns the log-likelihood value of a km object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'km'
logLik(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logLik_+3A_object">object</code></td>
<td>
<p> an object of class <code>km</code> containing the trend and covariance structures. </p>
</td></tr>
<tr><td><code id="logLik_+3A_...">...</code></td>
<td>
<p> no other argument for this method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The log likelihood value.
</p>


<h3>Author(s)</h3>

<p> O. Roustant, D. Ginsbourger, Ecole des Mines de St-Etienne</p>


<h3>References</h3>

<p>N.A.C. Cressie (1993), <em>Statistics for spatial data</em>, Wiley series in probability and mathematical statistics.
</p>
<p>D. Ginsbourger, D. Dupuy, A. Badea, O. Roustant, and L. Carraro (2009), A note on the choice and the estimation of kriging models for the analysis of deterministic computer experiments, <em>Applied Stochastic Models for Business and Industry</em>, <b>25</b> no. 2, 115-131.
</p>
<p>R. Li and A. Sudjianto (2005), Analysis of Computer Experiments Using Penalized Likelihood in Gaussian Kriging Models, <em>Technometrics</em>, <b>47</b> no. 2, 111-120.
</p>
<p>K.V. Mardia and R.J. Marshall (1984), Maximum likelihood estimation of models for residual covariance in spatial regression, <em>Biometrika</em>, <b>71</b>, 135-146.
</p>
<p>J.D. Martin and T.W. Simpson (2005), Use of kriging models to approximate deterministic computer models, <em>AIAA Journal</em>, <b>43</b> no. 4, 853-863.
</p>
<p>J.-S. Park and J. Baek (2001), Efficient computation of maximum likelihood estimators in a spatial linear model with power exponential covariogram, <em>Computer Geosciences</em>, <b>27</b> no. 1, 1-7.
</p>
<p>C.E. Rasmussen and C.K.I. Williams (2006), <em>Gaussian Processes for Machine Learning</em>, the MIT Press, <a href="http://www.gaussianprocess.org/gpml/">http://www.gaussianprocess.org/gpml/</a>
</p>
<p>J. Sacks, W.J. Welch, T.J. Mitchell, and H.P. Wynn (1989), Design and analysis of computer experiments, <em>Statistical Science</em>, <b>4</b>, 409-435.
</p>
<p>M.L. Stein (1999), <em>Interpolation of spatial data, some theory for kriging</em>, Springer.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+km">km</a></code>, <code><a href="#topic+logLikFun">logLikFun</a></code></p>

<hr>
<h2 id='logLikFun'> Concentrated log-likelihood of a km object </h2><span id='topic+logLikFun'></span>

<h3>Description</h3>

<p>Returns the concentrated log-likelihood, obtained from the likelihood by plugging in the estimators of the parameters that can be expressed in function of the other ones.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logLikFun(param, model, envir=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logLikFun_+3A_param">param</code></td>
<td>
<p> a vector containing the optimization variables. </p>
</td></tr>
<tr><td><code id="logLikFun_+3A_model">model</code></td>
<td>
<p> an object of class <code>km</code>. </p>
</td></tr>
<tr><td><code id="logLikFun_+3A_envir">envir</code></td>
<td>
<p> an optional environment specifying where to assign intermediate values for future gradient calculations. Default is NULL.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When there is no nugget effect nor observation noise, the concentrated log-likelihood is obtained by plugging in the variance and the trend MLE. Maximizing the likelihood is then equivalent to maximizing the concentrated log-likelihood with respect to the covariance parameters. In the other cases, the maximization of the concentrated log-likelihood also involves other parameters (the variance explained by the stationary part of the process for noisy observations, and this variance divided by the total variance if there is an unknown homogeneous nugget effect).
</p>


<h3>Value</h3>

<p>The concentrated log-likelihood value.
</p>


<h3>Author(s)</h3>

<p> O. Roustant, D. Ginsbourger, Ecole des Mines de St-Etienne</p>


<h3>References</h3>

 
<p>J.-S. Park and J. Baek (2001), Efficient computation of maximum likelihood estimators in a spatial linear model with power exponential covariogram, <em>Computer Geosciences</em>, <b>27</b> no. 1, 1-7.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+logLik+2Ckm-method">logLik,km-method</a></code>, <code><a href="#topic+km">km</a></code>, <code><a href="#topic+logLikGrad">logLikGrad</a></code> </p>

<hr>
<h2 id='logLikGrad'> Concentrated log-Likelihood of a km object - Analytical gradient </h2><span id='topic+logLikGrad'></span>

<h3>Description</h3>

<p>Returns the exact gradient vector of the concentrated log-likelihood.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logLikGrad(param, model, envir)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logLikGrad_+3A_param">param</code></td>
<td>
<p> a vector containing the optimization variables. </p>
</td></tr>
<tr><td><code id="logLikGrad_+3A_model">model</code></td>
<td>
<p> an object of class <code>km</code>. </p>
</td></tr>
<tr><td><code id="logLikGrad_+3A_envir">envir</code></td>
<td>
<p> an environment specifying where to get intermediate values calculated in <code>logLikFun</code>. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>the gradient of the concentrated log-likelihood.
</p>


<h3>Author(s)</h3>

<p> O. Roustant, D. Ginsbourger, Ecole des Mines de St-Etienne</p>


<h3>References</h3>

<p>J.-S. Park and J. Baek (2001), Efficient computation of maximum likelihood estimators in a spatial linear model with power exponential covariogram, <em>Computer Geosciences</em>, <b>27</b> no. 1, 1-7.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+km">km</a></code>, <code><a href="#topic+logLikFun">logLikFun</a></code> </p>

<hr>
<h2 id='ninput'>Get the spatial dimension</h2><span id='topic+ninput'></span>

<h3>Description</h3>

<p>Get the spatial dimension (number of input variables).</p>


<h3>Usage</h3>

<pre><code class='language-R'>  ninput(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ninput_+3A_x">x</code></td>
<td>
<p> an object containing the covariance structure.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An integer equal to the spatial dimension.
</p>

<hr>
<h2 id='nuggetflag'>Get the nugget flag</h2><span id='topic+nuggetflag'></span>

<h3>Description</h3>

<p>Get a boolean indicating whether there is a nugget effect.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  nuggetflag(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nuggetflag_+3A_x">x</code></td>
<td>
<p> an object containing the covariance structure.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A boolean.
</p>

<hr>
<h2 id='nuggetvalue'>Get or set the nugget value</h2><span id='topic+nuggetvalue'></span><span id='topic+nuggetvalue+3C-'></span>

<h3>Description</h3>

<p>Get or set the nugget value.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  nuggetvalue(x)
  nuggetvalue(x) &lt;- value 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nuggetvalue_+3A_x">x</code></td>
<td>
<p> an object containing the covariance structure.</p>
</td></tr>
<tr><td><code id="nuggetvalue_+3A_value">value</code></td>
<td>
<p> an optional variance value standing for the homogeneous nugget effect.</p>
</td></tr>
</table>

<hr>
<h2 id='plot'>Diagnostic plot for the validation of a km object</h2><span id='topic+plot'></span><span id='topic+plot.km'></span><span id='topic+plot+2Ckm-method'></span>

<h3>Description</h3>

<p>Three plots are currently available, based on the <code>leaveOneOut.km</code> results: one plot of fitted values against response values, one plot of standardized residuals, and one qqplot of standardized residuals.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'km'
plot(x, y, kriging.type = "UK", trend.reestim = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_+3A_x">x</code></td>
<td>
<p> an object of class &quot;km&quot; without noisy observations.</p>
</td></tr>
<tr><td><code id="plot_+3A_y">y</code></td>
<td>
<p> not used.  </p>
</td></tr>
<tr><td><code id="plot_+3A_kriging.type">kriging.type</code></td>
<td>
<p> an optional character string corresponding to the kriging family, to be chosen between simple kriging (&quot;SK&quot;) or universal kriging (&quot;UK&quot;).</p>
</td></tr>
<tr><td><code id="plot_+3A_trend.reestim">trend.reestim</code></td>
<td>
<p> should the trend be reestimated when removing an observation? Default to FALSE.</p>
</td></tr>
<tr><td><code id="plot_+3A_...">...</code></td>
<td>
<p> no other argument for this method.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The diagnostic plot has not been implemented yet for noisy observations. The standardized residuals are defined by <code>( y(xi) - yhat_{-i}(xi) ) / sigmahat_{-i}(xi)</code>, where <code>y(xi)</code> is the response at the point <code>xi</code>, <code>yhat_{-i}(xi)</code> is the fitted value when removing the observation <code>xi</code> (see <code><a href="#topic+leaveOneOut.km">leaveOneOut.km</a></code>), and <code>sigmahat_{-i}(xi)</code> is the corresponding kriging standard deviation. </p>


<h3>Value</h3>

<p> A list composed of:
</p>
<table>
<tr><td><code>mean</code></td>
<td>
<p> a vector of length <em>n</em>. The ith coordinate is equal to the kriging mean (including the trend) at the ith observation number when removing it from the learning set, </p>
</td></tr>
<tr><td><code>sd</code></td>
<td>
<p> a vector of length <em>n</em>. The ith coordinate is equal to the kriging standard deviation at the ith observation number when removing it from the learning set,</p>
</td></tr>
</table>
<p>where <em>n</em> is the total number of observations.
</p>


<h3>Warning</h3>

<p>Kriging parameters are not re-estimated when removing one observation. With few points, the re-estimated values can be far from those obtained with the entire learning set. One option is to reestimate the trend coefficients, by setting <code>trend.reestim=TRUE</code>.
</p>


<h3>Author(s)</h3>

<p> O. Roustant, D. Ginsbourger, Ecole des Mines de St-Etienne. </p>


<h3>References</h3>

 
<p>N.A.C. Cressie (1993), <em>Statistics for spatial data</em>, Wiley series in probability and mathematical statistics.
</p>
<p>J.D. Martin and T.W. Simpson (2005), Use of kriging models to approximate deterministic computer models, <em>AIAA Journal</em>, <b>43</b> no. 4, 853-863.
</p>
<p>M. Schonlau (1997), <em>Computer experiments and global optimization</em>, Ph.D. thesis, University of Waterloo.
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+predict+2Ckm-method">predict,km-method</a></code>,  <code><a href="#topic+leaveOneOut.km">leaveOneOut.km</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'># A 2D example - Branin-Hoo function

# a 16-points factorial design, and the corresponding response
d &lt;- 2; n &lt;- 16
fact.design &lt;- expand.grid(seq(0,1,length=4), seq(0,1,length=4))
fact.design &lt;- data.frame(fact.design); names(fact.design)&lt;-c("x1", "x2")
branin.resp &lt;- data.frame(branin(fact.design)); names(branin.resp) &lt;- "y" 

# kriging model 1 : gaussian covariance structure, no trend, 
#                   no nugget effect
m1 &lt;- km(~.^2, design=fact.design, response=branin.resp, covtype="gauss")
plot(m1)  # LOO without parameter reestimation
plot(m1, trend.reestim=TRUE)  # LOO with trend parameters reestimation 
                              # (gives nearly the same result here)
</code></pre>

<hr>
<h2 id='predict'> Predict values and confidence intervals at newdata for a km object </h2><span id='topic+predict'></span><span id='topic+predict.km'></span><span id='topic+predict+2Ckm-method'></span>

<h3>Description</h3>

<p>Predicted values and (marginal of joint) conditional variances based on a <code>km</code> model.  95 % confidence intervals are given, based on strong assumptions: Gaussian process assumption, specific prior distribution on the trend parameters, known covariance parameters. This might be abusive in particular in the case where estimated covariance parameters are plugged in.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'km'
predict(object, newdata, type, se.compute = TRUE, 
         cov.compute = FALSE, light.return = FALSE,
         bias.correct = FALSE, checkNames = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_+3A_object">object</code></td>
<td>
<p> an object of class <code>km</code>. </p>
</td></tr>
<tr><td><code id="predict_+3A_newdata">newdata</code></td>
<td>
<p> a vector, matrix or data frame containing the points where to perform predictions. </p>
</td></tr>
<tr><td><code id="predict_+3A_type">type</code></td>
<td>
<p> a character string corresponding to the kriging family, to be chosen between simple kriging (&quot;SK&quot;), or universal kriging (&quot;UK&quot;).</p>
</td></tr>
<tr><td><code id="predict_+3A_se.compute">se.compute</code></td>
<td>
<p> an optional boolean. If <code>FALSE</code>, only the kriging mean is computed. If <code>TRUE</code>, the kriging variance (actually, the corresponding standard deviation) and confidence intervals are computed too.</p>
</td></tr>
<tr><td><code id="predict_+3A_cov.compute">cov.compute</code></td>
<td>
<p> an optional boolean. If <code>TRUE</code>, the conditional covariance matrix is computed.</p>
</td></tr>
<tr><td><code id="predict_+3A_light.return">light.return</code></td>
<td>
<p> an optional boolean. If <code>TRUE</code>, <code>c</code> and <code>Tinv.c</code> are not returned. This should be reserved to expert users who want to save memory and know that they will not miss these values.</p>
</td></tr>
<tr><td><code id="predict_+3A_bias.correct">bias.correct</code></td>
<td>
<p> an optional boolean to correct bias in the UK variance and covariances. Default is <code>FALSE</code>. See Section Warning below.</p>
</td></tr>
<tr><td><code id="predict_+3A_checknames">checkNames</code></td>
<td>
<p> an optional boolean. If <code>TRUE</code> (default), a consistency test is performed between the names of <code>newdata</code> and the names of the experimental design (contained in <code>object@X</code>), see Section Warning below.</p>
</td></tr>
<tr><td><code id="predict_+3A_...">...</code></td>
<td>
<p> no other argument for this method.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>mean</code></td>
<td>
<p> kriging mean (including the trend) computed at <code>newdata</code>. </p>
</td></tr>
<tr><td><code>sd</code></td>
<td>
<p> kriging standard deviation computed at <code>newdata</code>. Not computed if 
<code>se.compute=FALSE</code>. </p>
</td></tr>
<tr><td><code>trend</code></td>
<td>
<p> the trend computed at <code>newdata</code>.</p>
</td></tr>
<tr><td><code>cov</code></td>
<td>
<p> kriging conditional covariance matrix. Not computed if <code>cov.compute=FALSE</code> (default).</p>
</td></tr>
<tr><td><code>lower95</code></td>
<td>
 </td></tr>
<tr><td><code>upper95</code></td>
<td>
<p> bounds of the 95 % confidence interval computed at <code>newdata</code> (to be interpreted with special care when parameters are estimated, see description above). Not computed if <code>se.compute=FALSE</code>.</p>
</td></tr>
<tr><td><code>c</code></td>
<td>
<p> an auxiliary matrix, containing all the covariances between newdata and the initial design points. Not returned if <code>light.return=TRUE</code>.</p>
</td></tr>
<tr><td><code>Tinv.c</code></td>
<td>
<p> an auxiliary vector, equal to <code>inv(t(T))*c</code>. Not returned if <code>light.return=TRUE</code>.</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>1. Contrarily to DiceKriging&lt;=1.3.2, the estimated (UK) variance and covariances are NOT multiplied by <code>n/(n-p)</code> by default (<code>n</code> and <code>p</code> denoting the number of rows and columns of the design matrix F). Recall that this correction would contribute to limit bias: it would totally remove it if the correlation parameters were known (which is not the case here). However, this correction is often useless in the context of computer experiments, especially in adaptive strategies. It can be activated by turning <code>bias.correct</code> to <code>TRUE</code>, when <code>type="UK"</code>.
</p>
<p>2. The columns of <code>newdata</code> should correspond to the input variables, and only the input variables (nor the response is not admitted, neither external variables). If <code>newdata</code> contains variable names, and if <code>checkNames</code> is <code>TRUE</code> (default), then <code><a href="#topic+checkNames">checkNames</a></code> performs a complete consistency test with the names of the experimental design. Otherwise, it is assumed that its columns correspond to the same variables than the experimental design and in the same order.
</p>


<h3>Author(s)</h3>

<p> O. Roustant, D. Ginsbourger, Ecole des Mines de St-Etienne. </p>


<h3>References</h3>

<p>N.A.C. Cressie (1993), <em>Statistics for spatial data</em>, Wiley series in probability and mathematical statistics.
</p>
<p>A.G. Journel and C.J. Huijbregts (1978), <em>Mining Geostatistics</em>, Academic Press, London.
</p>
<p>D.G. Krige (1951), A statistical approach to some basic mine valuation problems on the witwatersrand, <em>J. of the Chem., Metal. and Mining Soc. of South Africa</em>, <b>52</b> no. 6, 119-139.
</p>
<p>J.D. Martin and T.W. Simpson (2005), Use of kriging models to approximate deterministic computer models, <em>AIAA Journal</em>, <b>43</b> no. 4, 853-863.
</p>
<p>G. Matheron (1963), Principles of geostatistics, <em>Economic Geology</em>, <b>58</b>,
1246-1266.
</p>
<p>G. Matheron (1969), Le krigeage universel, <em>Les Cahiers du Centre de Morphologie Mathematique de Fontainebleau</em>, <b>1</b>.
</p>
<p>J.-S. Park and J. Baek (2001), Efficient computation of maximum likelihood estimators in a spatial linear model with power exponential covariogram, <em>Computer Geosciences</em>, <b>27</b> no. 1, 1-7.
</p>
<p>C.E. Rasmussen and C.K.I. Williams (2006), <em>Gaussian Processes for Machine Learning</em>, the MIT Press, <a href="http://www.gaussianprocess.org/gpml/">http://www.gaussianprocess.org/gpml/</a>
</p>
<p>J. Sacks, W.J. Welch, T.J. Mitchell, and H.P. Wynn (1989), Design and analysis of computer experiments, <em>Statistical Science</em>, <b>4</b>, 409-435.
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+km">km</a></code>,  <code><a href="#topic+plot+2Ckm-method">plot,km-method</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'># ------------
# a 1D example
# ------------

x &lt;- c(0, 0.4, 0.6, 0.8, 1)
y &lt;- c(-0.3, 0, 0, 0.5, 0.9)

formula &lt;- y~x   # try also   y~1  and  y~x+I(x^2)

model &lt;- km(formula=formula, design=data.frame(x=x), response=data.frame(y=y), 
            covtype="matern5_2")

tmin &lt;- -0.5; tmax &lt;- 2.5
t &lt;- seq(from=tmin, to=tmax, by=0.005)
color &lt;- list(SK="black", UK="blue")

# Results with Universal Kriging formulae (mean and and 95% intervals)
p.UK &lt;- predict(model, newdata=data.frame(x=t), type="UK")
plot(t, p.UK$mean, type="l", ylim=c(min(p.UK$lower95),max(p.UK$upper95)), 
                xlab="x", ylab="y")
lines(t, p.UK$trend, col="violet", lty=2)
lines(t, p.UK$lower95, col=color$UK, lty=2)
lines(t, p.UK$upper95, col=color$UK, lty=2)
points(x, y, col="red", pch=19)
abline(h=0)

# Results with Simple Kriging (SK) formula. The difference between the width of
# SK and UK intervals are due to the estimation error of the trend parameters 
# (but not to the range parameters, not taken into account in the UK formulae).
p.SK &lt;- predict(model, newdata=data.frame(x=t), type="SK")
lines(t, p.SK$mean, type="l", ylim=c(-7,7), xlab="x", ylab="y")
lines(t, p.SK$lower95, col=color$SK, lty=2)
lines(t, p.SK$upper95, col=color$SK, lty=2)
points(x, y, col="red", pch=19)
abline(h=0)

legend.text &lt;- c("Universal Kriging (UK)", "Simple Kriging (SK)")
legend(x=tmin, y=max(p.UK$upper), legend=legend.text, 
       text.col=c(color$UK, color$SK), col=c(color$UK, color$SK), 
       lty=3, bg="white")


# ---------------------------------------------------------------------------------
# a 1D example (following)- COMPARISON with the PREDICTION INTERVALS for REGRESSION
# ---------------------------------------------------------------------------------
# There are two interesting cases: 
# *  When the range parameter is near 0 ; Then the intervals should be nearly 
#    the same for universal kriging (when bias.correct=TRUE, see above) as for regression. 
#    This is because the uncertainty around the range parameter is not taken into account 
#    in the Universal Kriging formula.
# *  Where the predicted sites are "far" (relatively to the spatial correlation) 
#    from the design points ; in this case, the kriging intervals are not equal 
#    but nearly proportional to the regression ones, since the variance estimate 
#    for regression is not the same than for kriging (that depends on the 
#    range estimate)

x &lt;- c(0, 0.4, 0.6, 0.8, 1)
y &lt;- c(-0.3, 0, 0, 0.5, 0.9)

formula &lt;- y~x   # try also   y~1  and  y~x+I(x^2)
upper &lt;- 0.05    # this is to get something near to the regression case. 
                 # Try also upper=1 (or larger) to get usual results.

model &lt;- km(formula=formula, design=data.frame(x=x), response=data.frame(y=y), 
               covtype="matern5_2", upper=upper)

tmin &lt;- -0.5; tmax &lt;- 2.5
t &lt;- seq(from=tmin, to=tmax, by=0.005)
color &lt;- list(SK="black", UK="blue", REG="red")

# Results with Universal Kriging formulae (mean and and 95% intervals)
p.UK &lt;- predict(model, newdata=data.frame(x=t), type="UK", bias.correct=TRUE)
plot(t, p.UK$mean, type="l", ylim=c(min(p.UK$lower95),max(p.UK$upper95)), 
                   xlab="x", ylab="y")
lines(t, p.UK$trend, col="violet", lty=2)
lines(t, p.UK$lower95, col=color$UK, lty=2)
lines(t, p.UK$upper95, col=color$UK, lty=2)
points(x, y, col="red", pch=19)
abline(h=0)

# Results with Simple Kriging (SK) formula. The difference between the width of
# SK and UK intervals are due to the estimation error of the trend parameters 
# (but not to the range parameters, not taken into account in the UK formulae).
p.SK &lt;- predict(model, newdata=data.frame(x=t), type="SK")
lines(t, p.SK$mean, type="l", ylim=c(-7,7), xlab="x", ylab="y")
lines(t, p.SK$lower95, col=color$SK, lty=2)
lines(t, p.SK$upper95, col=color$SK, lty=2)
points(x, y, col="red", pch=19)
abline(h=0)

# results with regression given by lm (package stats)
m.REG &lt;- lm(formula)
p.REG &lt;- predict(m.REG, newdata=data.frame(x=t), interval="prediction")
lines(t, p.REG[,1], col=color$REG)
lines(t, p.REG[,2], col=color$REG, lty=2)
lines(t, p.REG[,3], col=color$REG, lty=2)

legend.text &lt;- c("UK with bias.correct=TRUE", "SK", "Regression")
legend(x=tmin, y=max(p.UK$upper), legend=legend.text, 
       text.col=c(color$UK, color$SK, color$REG), 
       col=c(color$UK, color$SK, color$REG), lty=3, bg="white")


# ----------------------------------
# A 2D example - Branin-Hoo function
# ----------------------------------

# a 16-points factorial design, and the corresponding response
d &lt;- 2; n &lt;- 16
fact.design &lt;- expand.grid(x1=seq(0,1,length=4), x2=seq(0,1,length=4))
branin.resp &lt;- apply(fact.design, 1, branin)

# kriging model 1 : gaussian covariance structure, no trend, 
#                   no nugget effect
m1 &lt;- km(~1, design=fact.design, response=branin.resp, covtype="gauss")

# predicting at testdata points
testdata &lt;- expand.grid(x1=s &lt;- seq(0,1, length=15), x2=s)
predicted.values.model1 &lt;- predict(m1, testdata, "UK")
</code></pre>

<hr>
<h2 id='SCAD'> Penalty function</h2><span id='topic+SCAD'></span>

<h3>Description</h3>

<p>Smoothly Clipped Absolute Deviation function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SCAD(x, lambda)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SCAD_+3A_x">x</code></td>
<td>
<p> a vector where the function is to be evaluated.</p>
</td></tr>
<tr><td><code id="SCAD_+3A_lambda">lambda</code></td>
<td>
<p> a number representing a tuning parameter.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>SCAD  is an even continuous function equal to 0 at <code>x=0</code>, and defined piecewise with derivative <code>lambda</code> in <code>[0, lambda]</code>, <code>(a*lambda - x)/(a-1)</code> in <code>[lambda, a*lambda]</code>, and <code>0</code> for <code>x</code> larger than  <code>a*lambda</code>. As suggested by (Li, Sudjianto, 2005), we set <code>a=3.7</code>.
</p>


<h3>Value</h3>

<p>A vector containing the SCAD values at <code>x</code>.
</p>


<h3>Note</h3>

<p>In MLE problems, the penalty value <code>lambda</code> should tend to 0 when the sample size tends to infinity to insure that the asymptotic properties of Penalized-MLE and MLE are the same (see Li, Sudjianto, 2005).
</p>


<h3>Author(s)</h3>

<p> O. Roustant, D. Ginsbourger, Ecole des Mines de St-Etienne. </p>


<h3>References</h3>

 
<p>R. Li and A. Sudjianto (2005), Analysis of Computer Experiments Using Penalized Likelihood in Gaussian Kriging Models, <em>Technometrics</em>, <b>47</b> no. 2, 111-120.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+SCAD.derivative">SCAD.derivative</a></code> and <code><a href="#topic+km">km</a></code> for a famous example </p>


<h3>Examples</h3>

<pre><code class='language-R'> 
x &lt;- seq(-8,8, length=200)
a &lt;- 3.7

lambda &lt;- 1.5
y &lt;- SCAD(x, lambda)
plot(x, y, type="l", ylim=c(0,6))
x.knots &lt;- c(-a*lambda, -lambda, 0, lambda, a*lambda)
points(x.knots, SCAD(x.knots, lambda), pch=19, cex=0.5)
text(6, SCAD(6, lambda)+0.3, paste("lambda =", lambda))

for (i in 1:2) {
   lambda &lt;- lambda - 0.5
   y &lt;- SCAD(x, lambda)
   lines(x, y, type="l")
   x.knots &lt;- c(-a*lambda, -lambda, 0, lambda, a*lambda)
   points(x.knots, SCAD(x.knots, lambda), pch=19, cex=0.5)
   text(6, SCAD(6, lambda)+0.3, paste("lambda =", lambda))
}

abline(v=0, h=0, lty="dotted")
title("SCAD function")
</code></pre>

<hr>
<h2 id='SCAD.derivative'> Penalty function derivative</h2><span id='topic+SCAD.derivative'></span>

<h3>Description</h3>

<p>Derivative of SCAD function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SCAD.derivative(x, lambda)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SCAD.derivative_+3A_x">x</code></td>
<td>
<p> a vector where the function is to be evaluated.</p>
</td></tr>
<tr><td><code id="SCAD.derivative_+3A_lambda">lambda</code></td>
<td>
<p> a number representing a tuning parameter.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector containing the SCAD derivative values at <code>x</code>.
</p>


<h3>Author(s)</h3>

<p> O. Roustant, D. Ginsbourger, Ecole des Mines de St-Etienne. </p>


<h3>References</h3>

 
<p>R. Li and A. Sudjianto (2005), Analysis of Computer Experiments Using Penalized Likelihood in Gaussian Kriging Models, <em>Technometrics</em>, <b>47</b> no. 2, 111-120.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+SCAD">SCAD</a></code>, <code><a href="#topic+km">km</a></code> </p>

<hr>
<h2 id='scalingFun'>
Scaling function 
</h2><span id='topic+scalingFun'></span>

<h3>Description</h3>

<p>Parametric transformation of the input space variables. The transformation is obtained coordinatewise by integrating piecewise affine marginal &quot;densities&quot; parametrized by a vector of knots and a matrix of density values at the knots. See references for more detail.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scalingFun(X, knots, eta, plot=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scalingFun_+3A_x">X</code></td>
<td>
<p> an n*d matrix standing for a design of n experiments in d-dimensional space </p>
</td></tr>
<tr><td><code id="scalingFun_+3A_knots">knots</code></td>
<td>
<p> a list of knots parametrizing the transformation. </p>
</td></tr>
<tr><td><code id="scalingFun_+3A_eta">eta</code></td>
<td>
<p> a list of coefficients parametrizing the d marginal transformations. Each element stands for a set of marginal density values at the knots defined above.</p>
</td></tr>
<tr><td><code id="scalingFun_+3A_plot">plot</code></td>
<td>
<p> if TRUE plots the image of the columns of X according to the corresponding marginal transformations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The image of X by a scaling transformation of parameters knots and eta
</p>


<h3>References</h3>

<p>Y. Xiong, W. Chen, D. Apley, and X. Ding (2007), <em>Int. J. Numer. Meth. Engng</em>, A non-stationary covariance-based Kriging method for metamodelling in engineering design. 
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+scalingGrad">scalingGrad</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## 1D Transform of Xiong et al.
knots &lt;- c(0, 0.3, 0.8, 1); eta &lt;- c(2, 0.4, 1.4, 1.1)
nk &lt;- length(knots)
t &lt;- seq(from = 0, to = 1, length = 200)
f &lt;- scalingFun(X = matrix(t), knots = list(knots), eta = list(eta))

## for text positions only
itext &lt;- round(length(t) * 0.7)
xtext &lt;- t[itext]; ftext &lt;- f[itext] / 2; etamax &lt;- max(eta)

## plot the transform function
opar &lt;- par(mfrow = c(2, 1))
par(mar = c(0, 4, 5, 4))
plot(x = t, y = f, type = "l", lwd = 2, col = "orangered",
     main = "scaling transform f(x) and density g(x)",
     xlab = "", ylab = "", xaxt = "n", yaxt = "n") 
axis(side = 4)
abline(v = knots, lty = "dotted"); abline(h = 0)
text(x = xtext, y = ftext, cex = 1.4,
     labels = expression(f(x) == integral(g(t)*dt, 0, x)))

## plot the density function, which is piecewise linear
scalingDens1d &lt;- approxfun(x = knots, y = eta)
g &lt;- scalingDens1d(t)
gtext &lt;- 0.5 * g[itext] + 0.6 * etamax
par(mar = c(5, 4, 0, 4))
plot(t, g, type = "l", lwd = 2, ylim = c(0, etamax * 1.2),
     col = "SpringGreen4", xlab = expression(x), ylab ="")
abline(v = knots, lty = "dotted")
lines(x = knots, y = eta, lty = 1, lwd = 2, type = "h", col = "SpringGreen4")
abline(h = 0)
text(x = 0.7, y = gtext, cex = 1.4, labels = expression(g(x)))

## show knots with math symbols eta, zeta
for (i in 1:nk) {
  text(x = knots[i], y = eta[i] + 0.12 * etamax, cex = 1.4,
       labels = substitute(eta[i], list(i = i)))
  mtext(side = 1, cex = 1.4, at = knots[i], line = 2.4,
        text = substitute(zeta[i], list(i = i)))
}
polygon(x = c(knots, knots[nk], knots[1]),  y = c(eta, 0, 0),
        density = 15, angle = 45, col = "SpringGreen", border = NA)
par(opar)
</code></pre>

<hr>
<h2 id='scalingFun1d'>
Scaling 1-dimensional function
</h2><span id='topic+scalingFun1d'></span>

<h3>Description</h3>

<p>Parametric transformation of the input space variable. The transformation is obtained coordinatewise by integrating piecewise affine marginal &quot;density&quot; parametrized by a vector of knots and a matrix of density values at the knots. See references for more detail.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scalingFun1d(x, knots, eta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scalingFun1d_+3A_x">x</code></td>
<td>
<p> an n matrix standing for a design of n experiments </p>
</td></tr>
<tr><td><code id="scalingFun1d_+3A_knots">knots</code></td>
<td>
<p> a list of knots parametrizing the transformation. </p>
</td></tr>
<tr><td><code id="scalingFun1d_+3A_eta">eta</code></td>
<td>
<p> a list of coefficients parametrizing the marginal transformation. Each element stands for a set of marginal density values at the knots defined above.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The image of x by a scaling transformation of parameters knots and eta
</p>


<h3>References</h3>

<p>Y. Xiong, W. Chen, D. Apley, and X. Ding (2007), <em>Int. J. Numer. Meth. Engng</em>, A non-stationary covariance-based Kriging method for metamodelling in engineering design.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+scalingFun">scalingFun</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## 1D Transform of Xiong et al.
knots &lt;- c(0, 0.3, 0.8, 1); eta &lt;- c(2, 0.4, 1.4, 1.1)
nk &lt;- length(knots)
t &lt;- seq(from = 0, to = 1, length = 200)
f &lt;- scalingFun1d(x = matrix(t), knots = knots, eta = eta)

## for text positions only
itext &lt;- round(length(t) * 0.7)
xtext &lt;- t[itext]; ftext &lt;- f[itext] / 2; etamax &lt;- max(eta)

## plot the transform function
opar &lt;- par(mfrow = c(2, 1))
par(mar = c(0, 4, 5, 4))
plot(x = t, y = f, type = "l", lwd = 2, col = "orangered",
     main = "scaling transform f(x) and density g(x)",
     xlab = "", ylab = "", xaxt = "n", yaxt = "n")
axis(side = 4)
abline(v = knots, lty = "dotted"); abline(h = 0)
text(x = xtext, y = ftext, cex = 1.4,
     labels = expression(f(x) == integral(g(t)*dt, 0, x)))

## plot the density function, which is piecewise linear
scalingDens1d &lt;- approxfun(x = knots, y = eta)
g &lt;- scalingDens1d(t)
gtext &lt;- 0.5 * g[itext] + 0.6 * etamax
par(mar = c(5, 4, 0, 4))
plot(t, g, type = "l", lwd = 2, ylim = c(0, etamax * 1.2),
     col = "SpringGreen4", xlab = expression(x), ylab ="")
abline(v = knots, lty = "dotted")
lines(x = knots, y = eta, lty = 1, lwd = 2, type = "h", col = "SpringGreen4")
abline(h = 0)
text(x = 0.7, y = gtext, cex = 1.4, labels = expression(g(x)))

## show knots with math symbols eta, zeta
for (i in 1:nk) {
  text(x = knots[i], y = eta[i] + 0.12 * etamax, cex = 1.4,
       labels = substitute(eta[i], list(i = i)))
  mtext(side = 1, cex = 1.4, at = knots[i], line = 2.4,
        text = substitute(zeta[i], list(i = i)))
}
polygon(x = c(knots, knots[nk], knots[1]),  y = c(eta, 0, 0),
        density = 15, angle = 45, col = "SpringGreen", border = NA)
par(opar)
</code></pre>

<hr>
<h2 id='scalingGrad'>
Gradient of the dimensional Scaling function
</h2><span id='topic+scalingGrad'></span>

<h3>Description</h3>

<p>Gradient of the Scaling function (marginal in dimension k) of Xiong et al. with respect to eta
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scalingGrad(X, knots, k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scalingGrad_+3A_x">X</code></td>
<td>
<p> an n*d matrix standing for a design of n experiments in d-dimensional space. </p>
</td></tr>
<tr><td><code id="scalingGrad_+3A_knots">knots</code></td>
<td>
<p> a list of knots parametrizing the transformation.</p>
</td></tr>
<tr><td><code id="scalingGrad_+3A_k">k</code></td>
<td>
<p> dimension of the input variables for which the gradient is calculated. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>Gradient of the Scaling function of Xiong et al. with respect to eta
</p>


<h3>References</h3>

<p>Y. Xiong, W. Chen, D. Apley, and X. Ding (2007), <em>Int. J. Numer. Meth. Engng</em>, A non-stationary covariance-based Kriging method for metamodelling in engineering design. 
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+scalingFun">scalingFun</a></code> </p>

<hr>
<h2 id='show'> Print values of a km object</h2><span id='topic+show+2Ckm-method'></span>

<h3>Description</h3>

<p>Show method for <code>km</code> object. Printing the main features of a kriging model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   ## S4 method for signature 'km'
show(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="show_+3A_object">object</code></td>
<td>
<p> an object of class <code>km</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> O. Roustant, D. Ginsbourger, Ecole des Mines de St-Etienne. </p>


<h3>See Also</h3>

 <p><code><a href="#topic+km">km</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'> 
# A 2D example - Branin-Hoo function

# a 16-points factorial design, and the corresponding response
d &lt;- 2; n &lt;- 16
fact.design &lt;- expand.grid(seq(0,1,length=4), seq(0,1,length=4))
fact.design &lt;- data.frame(fact.design); names(fact.design)&lt;-c("x1", "x2")
branin.resp &lt;- data.frame(branin(fact.design)); names(branin.resp) &lt;- "y" 

# kriging model 1 : power-exponential covariance structure, no trend, 
#                   no nugget effect
m1 &lt;- km(y~1, design=fact.design, response=branin.resp, covtype="powexp")
m1    # equivalently : show(m1)
</code></pre>

<hr>
<h2 id='simulate'> Simulate GP values at any given set of points for a  km object </h2><span id='topic+simulate'></span><span id='topic+simulate+2Ckm-method'></span>

<h3>Description</h3>

<p><code>simulate</code> is used to simulate Gaussian process values at any given set of points for a specified km object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'km'
simulate(object, nsim=1, seed=NULL, newdata=NULL, 
                            cond=FALSE, nugget.sim=0, checkNames=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulate_+3A_object">object</code></td>
<td>
<p> an object of class <code>km</code>. </p>
</td></tr>
<tr><td><code id="simulate_+3A_nsim">nsim</code></td>
<td>
<p> an optional number specifying the number of response vectors to simulate. Default is 1.</p>
</td></tr>
<tr><td><code id="simulate_+3A_seed">seed</code></td>
<td>
<p> usual <code>seed</code> argument of method simulate. Not used yet in <code>simulated.km</code>.</p>
</td></tr>
<tr><td><code id="simulate_+3A_newdata">newdata</code></td>
<td>
<p> an optional vector, matrix or data frame containing the points where to perform predictions. Default is NULL: simulation is performed at design points specified in <code>object</code>.</p>
</td></tr>
<tr><td><code id="simulate_+3A_cond">cond</code></td>
<td>
<p> an optional boolean indicating the type of simulations. If <code>TRUE</code>, the simulations are performed conditionally to the response vector defined by using <code>km</code>, and contained in <code>model</code> (slot y: <code>model@y</code>). If <code>FALSE</code>, the simulations are non conditional. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="simulate_+3A_nugget.sim">nugget.sim</code></td>
<td>
<p> an optional number corresponding to a numerical nugget effect, which may be useful in presence of numerical instabilities. If specified, it is added to the diagonal terms of the covariance matrix (that is: <code>newdata</code> if <code>cond=TRUE</code>, or of <code>(newdata, model@y)</code> either) to ensure that it is positive definite. In any case, this parameter does not modify <code>model</code>. It has no effect if <code>newdata=NULL</code>. Default is 0.</p>
</td></tr>
<tr><td><code id="simulate_+3A_checknames">checkNames</code></td>
<td>
<p> an optional boolean. If <code>TRUE</code> (default), a consistency test is performed between the names of <code>newdata</code> and the names of the experimental design (contained in <code>object@X</code>), see section Warning below.</p>
</td></tr>
<tr><td><code id="simulate_+3A_...">...</code></td>
<td>
<p> no other argument for this method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix containing the simulated response vectors at the newdata points, with one sample in each row. 
</p>


<h3>Warning</h3>

<p>The columns of <code>newdata</code> should correspond to the input variables, and only the input variables (nor the response is not admitted, neither external variables). If <code>newdata</code> contains variable names, and if <code>checkNames</code> is <code>TRUE</code> (default), then <code><a href="#topic+checkNames">checkNames</a></code> performs a complete consistency test with the names of the experimental design. Otherwise, it is assumed that its columns correspond to the same variables than the experimental design and in the same order.
</p>


<h3>Note</h3>

 

<table>
<tr>
 <td style="text-align: left;">
 </td><td style="text-align: left;"> 1. When constructing a <code>km</code> object with known parameters, note that the argument <code>y</code> (the output) is required in <code>km</code> even if it will not be used for simulation. </td>
</tr>
<tr>
 <td style="text-align: left;">
 </td><td style="text-align: left;"> 2. Sometimes, a small nugget effect is necessary to avoid numerical instabilities (see the ex. below). </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p> O. Roustant, D. Ginsbourger, Ecole des Mines de St-Etienne. </p>


<h3>References</h3>

<p>N.A.C. Cressie (1993), <em>Statistics for spatial data</em>, Wiley series in probability and mathematical statistics.
</p>
<p>A.G. Journel and C.J. Huijbregts (1978), <em>Mining Geostatistics</em>, Academic Press, London.
</p>
<p>B.D. Ripley (1987), <em>Stochastic Simulation</em>, Wiley.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+km">km</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>

# ----------------
# some simulations 
# ----------------

n &lt;- 200
x &lt;- seq(from=0, to=1, length=n)

covtype &lt;- "matern3_2"
coef.cov &lt;- c(theta &lt;- 0.3/sqrt(3))
sigma &lt;- 1.5
trend &lt;- c(intercept &lt;- -1, beta1 &lt;- 2, beta2 &lt;- 3)
nugget &lt;- 0   # may be sometimes a little more than zero in some cases, 
              # due to numerical instabilities

formula &lt;- ~x+I(x^2)    # quadratic trend (beware to the usual I operator)

ytrend &lt;- intercept + beta1*x + beta2*x^2
plot(x, ytrend, type="l", col="black", ylab="y", lty="dashed",
     ylim=c(min(ytrend)-2*sigma, max(ytrend) + 2*sigma))

model &lt;- km(formula, design=data.frame(x=x), response=rep(0,n), 
            covtype=covtype, coef.trend=trend, coef.cov=coef.cov, 
            coef.var=sigma^2, nugget=nugget)
y &lt;- simulate(model, nsim=5, newdata=NULL)

for (i in 1:5) {
  lines(x, y[i,], col=i)
}


# --------------------------------------------------------------------
# conditional simulations and consistancy with Simple Kriging formulas
# --------------------------------------------------------------------

n &lt;- 6
m &lt;- 101
x &lt;- seq(from=0, to=1, length=n)
response &lt;- c(0.5, 0, 1.5, 2, 3, 2.5)

covtype &lt;- "matern5_2"
coef.cov &lt;- 0.1
sigma &lt;- 1.5

trend &lt;- c(intercept &lt;- 5, beta &lt;- -4)
model &lt;- km(formula=~cos(x), design=data.frame(x=x), response=response, 
            covtype=covtype, coef.trend=trend, coef.cov=coef.cov, 
            coef.var=sigma^2)

t &lt;- seq(from=0, to=1, length=m)
nsim &lt;- 1000
y &lt;- simulate(model, nsim=nsim, newdata=data.frame(x=t), cond=TRUE, nugget.sim=1e-5)

## graphics

plot(x, intercept + beta*cos(x), type="l", col="black", 
     ylim=c(-4, 7), ylab="y", lty="dashed")
for (i in 1:nsim) {
	lines(t, y[i,], col=i)
}

p &lt;- predict(model, newdata=data.frame(x=t), type="SK")
lines(t, p$lower95, lwd=3)
lines(t, p$upper95, lwd=3)

points(x, response, pch=19, cex=1.5, col="red")

# compare theoretical kriging mean and sd with the mean and sd of
# simulated sample functions
mean.theoretical &lt;- p$mean
sd.theoretical &lt;- p$sd
mean.simulated &lt;- apply(y, 2, mean) 
sd.simulated &lt;- apply(y, 2, sd)
par(mfrow=c(1,2))
plot(t, mean.theoretical, type="l")
lines(t, mean.simulated, col="blue", lty="dotted")
points(x, response, pch=19, col="red")
plot(t, sd.theoretical, type="l")
lines(t, sd.simulated, col="blue", lty="dotted")
points(x, rep(0, n), pch=19, col="red")
par(mfrow=c(1,1))

# estimate the confidence level at each point
level &lt;- rep(0, m)
for (j in 1:m) {
	level[j] &lt;- sum((y[,j]&gt;=p$lower95[j]) &amp; (y[,j]&lt;=p$upper95[j]))/nsim
}
level    # level computed this way may be completely wrong at interpolation 
         # points, due to the numerical errors in the calculation of the 
         # kriging mean


# ---------------------------------------------------------------------
# covariance kernel + simulations for "exp", "matern 3/2", "matern 5/2" 
#                                 and "exp" covariances
# ---------------------------------------------------------------------

covtype &lt;- c("exp", "matern3_2", "matern5_2", "gauss")

d &lt;- 1
n &lt;- 500
x &lt;- seq(from=0, to=3, length=n)

par(mfrow=c(1,2))
plot(x, rep(0,n), type="l", ylim=c(0,1), xlab="distance", ylab="covariance")

param &lt;- 1
sigma2 &lt;- 1

for (i in 1:length(covtype)) {
	covStruct &lt;- covStruct.create(covtype=covtype[i], d=d, known.covparam="All", 
                      var.names="x", coef.cov=param, coef.var=sigma2)
	y &lt;- covMat1Mat2(covStruct, X1=as.matrix(x), X2=as.matrix(0))
	lines(x, y, col=i, lty=i)
	}
legend(x=1.3, y=1, legend=covtype, col=1:length(covtype), 
       lty=1:length(covtype), cex=0.8)

plot(x, rep(0,n), type="l", ylim=c(-2.2, 2.2), xlab="input, x", 
     ylab="output, f(x)")
for (i in 1:length(covtype)) {
	model &lt;- km(~1, design=data.frame(x=x), response=rep(0,n), covtype=covtype[i], 
		    coef.trend=0, coef.cov=param, coef.var=sigma2, nugget=1e-4)
	y &lt;- simulate(model)
	lines(x, y, col=i, lty=i)
}
par(mfrow=c(1,1))

# -------------------------------------------------------
# covariance kernel + simulations for "powexp" covariance
# -------------------------------------------------------

covtype &lt;- "powexp"

d &lt;- 1
n &lt;- 500
x &lt;- seq(from=0, to=3, length=n)

par(mfrow=c(1,2))
plot(x, rep(0,n), type="l", ylim=c(0,1), xlab="distance", ylab="covariance")

param &lt;- c(1, 1.5, 2)
sigma2 &lt;- 1

for (i in 1:length(param)) {
	covStruct &lt;- covStruct.create(covtype=covtype, d=d, known.covparam="All",
                      var.names="x", coef.cov=c(1, param[i]), coef.var=sigma2)
	y &lt;- covMat1Mat2(covStruct, X1=as.matrix(x), X2=as.matrix(0))
	lines(x, y, col=i, lty=i)
	}
legend(x=1.4, y=1, legend=paste("p=", param), col=1:3, lty=1:3)

plot(x, rep(0,n), type="l", ylim=c(-2.2, 2.2), xlab="input, x", 
     ylab="output, f(x)")
for (i in 1:length(param)) {
	model &lt;- km(~1, design=data.frame(x=x), response=rep(0,n), covtype=covtype, 
        coef.trend=0, coef.cov=c(1, param[i]), coef.var=sigma2, nugget=1e-4)
	y &lt;- simulate(model)
	lines(x, y, col=i)
}
par(mfrow=c(1,1))

</code></pre>

<hr>
<h2 id='trend.deltax'>
Trend derivatives
</h2><span id='topic+trend.deltax'></span>

<h3>Description</h3>

<p>Computes the gradient of the vector of trend basis functions f(x)=(f1(x);...;fp(x))
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trend.deltax(x, model, h = sqrt(.Machine$double.eps))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trend.deltax_+3A_x">x</code></td>
<td>
<p> a vector representing the specific location.</p>
</td></tr>
<tr><td><code id="trend.deltax_+3A_model">model</code></td>
<td>
<p> an object of class km.</p>
</td></tr> 
<tr><td><code id="trend.deltax_+3A_h">h</code></td>
<td>
<p> the precision for numerical derivatives.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>pxd</code> matrix where the <code>p</code> rows contain the gradient of the trend basis functions.
</p>


<h3>Note</h3>

<p>The gradient is computed analytically in 4 common practical situations: <code>formula=~1</code> (constant trend), <code>formula=~.</code> (first-order polynomial), <code>formula=~.^2</code> (first-order polynomial + second-order interactions), first-order polynomial + (pure) quadratic terms. In the other cases, the gradient is approximated by a finite difference of the form <code>(g(x+h)-g(x-h))/2h</code>, where <code>h</code> is tunable. 
</p>


<h3>Author(s)</h3>

<p>O. Roustant, Ecole des Mines de St-Etienne.
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+covVector.dx">covVector.dx</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>X &lt;- expand.grid(x1=seq(0,1,length=4), x2=seq(0,1,length=4), x3=seq(0,1,length=4))
fun &lt;- function(x){
  (x[1]+2*x[2]+3*x[3])^2
}
y &lt;- apply(X, 1, fun) 

x &lt;- c(0.2, 0.4, 0.6)
coef.cov=c(0.5, 0.9, 1.3); coef.var=3

m &lt;- km(~.^2, design=X, response=y, coef.cov=coef.cov, coef.var=coef.var)
grad.trend &lt;- trend.deltax(x, m)
print(grad.trend)

m &lt;- km(~. + I(x1^2) + I(x2^2) + I(x3^2), 
        design=X, response=y, coef.cov=coef.cov, coef.var=coef.var)
grad.trend &lt;- trend.deltax(x, m)
print(grad.trend)
</code></pre>

<hr>
<h2 id='trendMatrix.update'> Trend model matrix operation </h2><span id='topic+trendMatrix.update'></span>

<h3>Description</h3>

<p>  Updates the trend linear model matrix when new data are given. </p>


<h3>Usage</h3>

<pre><code class='language-R'>trendMatrix.update(model, Xnew)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trendMatrix.update_+3A_model">model</code></td>
<td>
<p>  an object of class <code>km</code> corresponding to the non-updated kriging model. </p>
</td></tr>
<tr><td><code id="trendMatrix.update_+3A_xnew">Xnew</code></td>
<td>
<p> a data frame containing the new data points.   </p>
</td></tr>
</table>


<h3>Value</h3>

<p> The model matrix corresponding to known design points and new data points. </p>


<h3>Note</h3>

<p> The model design and model response are not updated. This should be done before using <code>trendMatrix.update</code>.</p>


<h3>Author(s)</h3>

<p> O. Roustant, D. Ginsbourger </p>

<hr>
<h2 id='update'>Update of a kriging model</h2><span id='topic+update'></span><span id='topic+update.km'></span><span id='topic+update+2Ckm-method'></span>

<h3>Description</h3>

<p>Update a <code><a href="#topic+km">km</a></code> object when one or many new
observations are added. Many,  but not all, fields of the
<code><a href="#topic+km">km</a></code> object  need to be recalculated when new observations are added. 
It is also possible to modify the k last (existing) observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'km'
update(object, newX, newy, newX.alreadyExist = FALSE,
          cov.reestim = TRUE, trend.reestim = TRUE, nugget.reestim = FALSE, 
          newnoise.var = NULL, kmcontrol = NULL, newF = NULL,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="update_+3A_object">object</code></td>
<td>
<p>Kriging model of <code><a href="#topic+km">km</a></code> class.</p>
</td></tr>
<tr><td><code id="update_+3A_newx">newX</code></td>
<td>
<p>Matrix with <code>object@d</code> columns and r rows
corresponding to the r locations of  the observations to be
updated. These locations can be new  locations or existing ones.</p>
</td></tr>
<tr><td><code id="update_+3A_newy">newy</code></td>
<td>
<p>Matrix with one column and r rows corresponding to the r
responses at the r  locations <code>newX</code>.</p>
</td></tr>
<tr><td><code id="update_+3A_newx.alreadyexist">newX.alreadyExist</code></td>
<td>
<p>Boolean: indicate whether the locations <code>newX</code> are all news or not.</p>
</td></tr>
<tr><td><code id="update_+3A_cov.reestim">cov.reestim</code></td>
<td>
<p>Should the covariance parameters
of the <code><a href="#topic+km">km</a></code> object be re-estimated?</p>
</td></tr>
<tr><td><code id="update_+3A_trend.reestim">trend.reestim</code></td>
<td>
<p>Should the trend parameters be re-estimated?</p>
</td></tr>
<tr><td><code id="update_+3A_nugget.reestim">nugget.reestim</code></td>
<td>
<p>Should the nugget effect be re-estimated?</p>
</td></tr>
<tr><td><code id="update_+3A_newnoise.var">newnoise.var</code></td>
<td>
<p>Vector containing the noise variance at each new observations.</p>
</td></tr>
<tr><td><code id="update_+3A_kmcontrol">kmcontrol</code></td>
<td>
<p>Optional list representing the control variables for
the re-estimation  of the kriging model once new points are
sampled. The items are the  same as in <code><a href="#topic+km">km</a></code></p>
</td></tr>
<tr><td><code id="update_+3A_newf">newF</code></td>
<td>
<p>Optional matrix containing the value of the trend at the
new locations.  Setting this argument avoids a call to an expensive function.</p>
</td></tr>
<tr><td><code id="update_+3A_...">...</code></td>
<td>
<p>Further arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Updated km object
</p>


<h3>Author(s)</h3>

 
<p>Clement Chevalier (IMSV, Switzerland, and IRSN, France)
</p>


<h3>References</h3>

<p>Bect J., Ginsbourger D., Li L., Picheny V., Vazquez E. (2010),
<em>Sequential design of  computer experiments for the estimation of
a probability of failure</em>,  Statistics and Computing, pp.1-21, 2011, <a href="https://arxiv.org/abs/1009.5177">https://arxiv.org/abs/1009.5177</a>
</p>
<p>Chevalier C., Bect J., Ginsbourger D., Vazquez E., Picheny V., Richet
Y. (2011),  <em>Fast parallel kriging-based stepwise uncertainty
reduction with  application to the identification of an excursion
set</em>,  <a href="https://hal.archives-ouvertes.fr/hal-00641108/">https://hal.archives-ouvertes.fr/hal-00641108/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+km">km</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(8)
N &lt;- 9 # number of observations
testfun &lt;- branin

# a 9 points initial design 
design &lt;- expand.grid(x1=seq(0,1,length=3), x2=seq(0,1,length=3))
response &lt;- testfun(design)

# km object with matern3_2 covariance
# params estimated by ML from the observations
model &lt;- km(formula = ~., design = design, 
	response = response, covtype = "matern3_2")
model@covariance

newX &lt;- matrix(c(0.4,0.5), ncol = 2) #the point that we are going to add in the km object
newy &lt;- testfun(newX)
newmodel &lt;- update(object = model, newX = newX, newy = newy, cov.reestim = TRUE)
newmodel@covariance
</code></pre>

<hr>
<h2 id='vect2covparam'> Auxiliary function </h2><span id='topic+vect2covparam'></span>

<h3>Description</h3>

<p>  Extract the covariance parameters from a single vector. Not for direct use.</p>


<h3>Usage</h3>

<pre><code class='language-R'>vect2covparam(object, param)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vect2covparam_+3A_object">object</code></td>
<td>
<p>an object specifying the covariance structure.</p>
</td></tr>
<tr><td><code id="vect2covparam_+3A_param">param</code></td>
<td>
<p>a vector containing the covariance parameters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p> The updated object.</p>


<h3>Author(s)</h3>

<p> O. Roustant, D. Ginsbourger</p>


<h3>See Also</h3>

 <p><code><a href="#topic+covparam2vect">covparam2vect</a></code> </p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
