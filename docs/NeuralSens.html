<!DOCTYPE html><html lang="en"><head><title>Help for package NeuralSens</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {NeuralSens}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#NeuralSens'><p>NeuralSens: Sensitivity Analysis of Neural Networks</p></a></li>
<li><a href='#ActFunc'><p>Activation function of neuron</p></a></li>
<li><a href='#AlphaSensAnalysis'><p>Sensitivity alpha-curve associated to MLP function</p></a></li>
<li><a href='#AlphaSensCurve'><p>Sensitivity alpha-curve associated to MLP function of an input variable</p></a></li>
<li><a href='#ChangeBootAlpha'><p>Change significance of boot SensMLP Class</p></a></li>
<li><a href='#CombineSens'><p>Sensitivity analysis plot over time of the data</p></a></li>
<li><a href='#ComputeHessMeasures'><p>Plot sensitivities of a neural network model</p></a></li>
<li><a href='#ComputeSensMeasures'><p>Plot sensitivities of a neural network model</p></a></li>
<li><a href='#DAILY_DEMAND_TR'><p>Data frame with 4 variables</p></a></li>
<li><a href='#DAILY_DEMAND_TV'><p>Data frame with 3 variables</p></a></li>
<li><a href='#Der2ActFunc'><p>Second derivative of activation function of neuron</p></a></li>
<li><a href='#Der3ActFunc'><p>Third derivative of activation function of neuron</p></a></li>
<li><a href='#DerActFunc'><p>Derivative of activation function of neuron</p></a></li>
<li><a href='#diag3Darray'><p>Define function to create a 'diagonal' array or get the diagonal of an array</p></a></li>
<li><a href='#diag3Darray+26lt+3B-'><p>Define function to change the diagonal of array</p></a></li>
<li><a href='#diag4Darray'><p>Define function to create a 'diagonal' array or get the diagonal of an array</p></a></li>
<li><a href='#diag4Darray+26lt+3B-'><p>Define function to change the diagonal of array</p></a></li>
<li><a href='#find_critical_value'><p>Find Critical Value</p></a></li>
<li><a href='#HessDotPlot'><p>Second derivatives 3D scatter or surface plot against input values</p></a></li>
<li><a href='#HessFeaturePlot'><p>Feature sensitivity plot</p></a></li>
<li><a href='#HessianMLP'><p>Sensitivity of MLP models</p></a></li>
<li><a href='#HessMLP'><p>Constructor of the HessMLP Class</p></a></li>
<li><a href='#HessToSensMLP'><p>Convert a HessMLP to a SensMLP object</p></a></li>
<li><a href='#is.HessMLP'><p>Check if object is of class <code>HessMLP</code></p></a></li>
<li><a href='#is.SensMLP'><p>Check if object is of class <code>SensMLP</code></p></a></li>
<li><a href='#kStepMAlgorithm'><p>k-StepM Algorithm for Hypothesis Testing</p></a></li>
<li><a href='#plot.HessMLP'><p>Plot method for the HessMLP Class</p></a></li>
<li><a href='#plot.SensMLP'><p>Plot method for the SensMLP Class</p></a></li>
<li><a href='#PlotSensMLP'><p>Neural network structure sensitivity plot</p></a></li>
<li><a href='#print.HessMLP'><p>Print method for the HessMLP Class</p></a></li>
<li><a href='#print.SensMLP'><p>Print method for the SensMLP Class</p></a></li>
<li><a href='#print.summary.HessMLP'><p>Print method of the summary HessMLP Class</p></a></li>
<li><a href='#print.summary.SensMLP'><p>Print method of the summary SensMLP Class</p></a></li>
<li><a href='#SensAnalysisMLP'><p>Sensitivity of MLP models</p></a></li>
<li><a href='#SensDotPlot'><p>Sensitivity scatter plot against input values</p></a></li>
<li><a href='#SensFeaturePlot'><p>Feature sensitivity plot</p></a></li>
<li><a href='#SensitivityPlots'><p>Plot sensitivities of a neural network model</p></a></li>
<li><a href='#SensMatPlot'><p>Plot sensitivities of a neural network model</p></a></li>
<li><a href='#SensMLP'><p>Constructor of the SensMLP Class</p></a></li>
<li><a href='#SensTimePlot'><p>Sensitivity analysis plot over time of the data</p></a></li>
<li><a href='#simdata'><p>Simulated data to test the package functionalities</p></a></li>
<li><a href='#summary.HessMLP'><p>Summary Method for the HessMLP Class</p></a></li>
<li><a href='#summary.SensMLP'><p>Summary Method for the SensMLP Class</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Version:</td>
<td>1.1.3</td>
</tr>
<tr>
<td>Title:</td>
<td>Sensitivity Analysis of Neural Networks</td>
</tr>
<tr>
<td>Description:</td>
<td>Analysis functions to quantify inputs importance in neural network models.
  Functions are available for calculating and plotting the inputs importance and obtaining
  the activation function of each neuron layer and its derivatives. The importance of a given
  input is defined as the distribution of the derivatives of the output with respect to that
  input in each training data point &lt;<a href="https://doi.org/10.18637%2Fjss.v102.i07">doi:10.18637/jss.v102.i07</a>&gt;.</td>
</tr>
<tr>
<td>Author:</td>
<td>José Portela González [aut],
  Antonio Muñoz San Roque [aut],
  Jaime Pizarroso Gonzalo [aut, ctb, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jaime Pizarroso Gonzalo &lt;jpizarroso@comillas.edu&gt;</td>
</tr>
<tr>
<td>Imports:</td>
<td>ggplot2, gridExtra, NeuralNetTools, reshape2, caret,
fastDummies, stringr, Hmisc, ggforce, scales, ggnewscale,
magrittr, ggrepel, ggbreak, dplyr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>h2o, RSNNS, nnet, neuralnet, plotly, e1071</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/JaiPizGon/NeuralSens">https://github.com/JaiPizGon/NeuralSens</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/JaiPizGon/NeuralSens/issues">https://github.com/JaiPizGon/NeuralSens/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-05-11 19:26:10 UTC; jpizarroso</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-05-11 19:43:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='NeuralSens'>NeuralSens: Sensitivity Analysis of Neural Networks</h2><span id='topic+NeuralSens-package'></span><span id='topic+NeuralSens'></span>

<h3>Description</h3>

<p>Visualization and analysis tools to aid in the interpretation of
neural network models.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Jaime Pizarroso Gonzalo <a href="mailto:jpizarroso@comillas.edu">jpizarroso@comillas.edu</a> [contributor]
</p>
<p>Authors:
</p>

<ul>
<li><p> José Portela González <a href="mailto:Jose.Portela@iit.comillas.edu">Jose.Portela@iit.comillas.edu</a>
</p>
</li>
<li><p> Antonio Muñoz San Roque <a href="mailto:antonio.munoz@iit.comillas.edu">antonio.munoz@iit.comillas.edu</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/JaiPizGon/NeuralSens">https://github.com/JaiPizGon/NeuralSens</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/JaiPizGon/NeuralSens/issues">https://github.com/JaiPizGon/NeuralSens/issues</a>
</p>
</li></ul>


<hr>
<h2 id='ActFunc'>Activation function of neuron</h2><span id='topic+ActFunc'></span>

<h3>Description</h3>

<p>Evaluate activation function of a neuron
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ActFunc(type = "sigmoid", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ActFunc_+3A_type">type</code></td>
<td>
<p><code>character</code> name of the activation function</p>
</td></tr>
<tr><td><code id="ActFunc_+3A_...">...</code></td>
<td>
<p>extra arguments needed to calculate the functions</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>numeric</code> output of the neuron
</p>


<h3>References</h3>

<p>Pizarroso J, Portela J, Muñoz A (2022). NeuralSens: Sensitivity Analysis of
Neural Networks. Journal of Statistical Software, 102(7), 1-36.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Return the sigmoid activation function of a neuron
ActivationFunction &lt;- ActFunc("sigmoid")
# Return the tanh activation function of a neuron
ActivationFunction &lt;- ActFunc("tanh")
# Return the activation function of several layers of neurons
actfuncs &lt;- c("linear","sigmoid","linear")
ActivationFunctions &lt;- sapply(actfuncs, ActFunc)
</code></pre>

<hr>
<h2 id='AlphaSensAnalysis'>Sensitivity alpha-curve associated to MLP function</h2><span id='topic+AlphaSensAnalysis'></span>

<h3>Description</h3>

<p>Obtain sensitivity alpha-curves associated to MLP function obtained from
the sensitivities returned by <code><a href="#topic+SensAnalysisMLP">SensAnalysisMLP</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AlphaSensAnalysis(
  sens,
  tol = NULL,
  max_alpha = 15,
  curve_equal_origin = FALSE,
  inp_var = NULL,
  line_width = 1,
  title = "Alpha curve of Lp norm values",
  alpha_bar = 1,
  kind = "line"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="AlphaSensAnalysis_+3A_sens">sens</code></td>
<td>
<p>sensitivity object returned by <code><a href="#topic+SensAnalysisMLP">SensAnalysisMLP</a></code></p>
</td></tr>
<tr><td><code id="AlphaSensAnalysis_+3A_tol">tol</code></td>
<td>
<p>difference between M_alpha and maximum sensitivity of the sensitivity of each input variable</p>
</td></tr>
<tr><td><code id="AlphaSensAnalysis_+3A_max_alpha">max_alpha</code></td>
<td>
<p>maximum alpha value to analyze</p>
</td></tr>
<tr><td><code id="AlphaSensAnalysis_+3A_curve_equal_origin">curve_equal_origin</code></td>
<td>
<p>make all the curves begin at (1,0)</p>
</td></tr>
<tr><td><code id="AlphaSensAnalysis_+3A_inp_var">inp_var</code></td>
<td>
<p><code>character</code> indicating which input variable to show in density plot. Only useful when
choosing plot_type='raw' to show the density plot of one input variable. If <code>NULL</code>, all variables
are plotted in density plot. By default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="AlphaSensAnalysis_+3A_line_width">line_width</code></td>
<td>
<p><code>int</code> width of the line in the plot.</p>
</td></tr>
<tr><td><code id="AlphaSensAnalysis_+3A_title">title</code></td>
<td>
<p><code>char</code> title of the alpha-curves plot</p>
</td></tr>
<tr><td><code id="AlphaSensAnalysis_+3A_alpha_bar">alpha_bar</code></td>
<td>
<p><code>int</code> alpha value to show as column plot.</p>
</td></tr>
<tr><td><code id="AlphaSensAnalysis_+3A_kind">kind</code></td>
<td>
<p><code>char</code> select the type of plot: &quot;line&quot; or &quot;bar&quot;</p>
</td></tr>
</table>


<h3>Value</h3>

<p>alpha-curves of the MLP function
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
mod &lt;- RSNNS::mlp(simdata[, c("X1", "X2", "X3")], simdata[, "Y"],
                 maxit = 1000, size = 15, linOut = TRUE)

sens &lt;- SensAnalysisMLP(mod, trData = simdata,
                        output_name = "Y", plot = FALSE)

AlphaSensAnalysis(sens)

</code></pre>

<hr>
<h2 id='AlphaSensCurve'>Sensitivity alpha-curve associated to MLP function of an input variable</h2><span id='topic+AlphaSensCurve'></span>

<h3>Description</h3>

<p>Obtain sensitivity alpha-curve associated to MLP function obtained from
the sensitivities returned by <code><a href="#topic+SensAnalysisMLP">SensAnalysisMLP</a></code> of an input variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AlphaSensCurve(sens, tol = NULL, max_alpha = 100)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="AlphaSensCurve_+3A_sens">sens</code></td>
<td>
<p>raw sensitivities of the MLP output with respect to input variable.</p>
</td></tr>
<tr><td><code id="AlphaSensCurve_+3A_tol">tol</code></td>
<td>
<p>difference between M_alpha and maximum sensitivity of the sensitivity of each input variable</p>
</td></tr>
<tr><td><code id="AlphaSensCurve_+3A_max_alpha">max_alpha</code></td>
<td>
<p>maximum alpha value to analyze</p>
</td></tr>
</table>


<h3>Value</h3>

<p>alpha-curve of the MLP function
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
mod &lt;- RSNNS::mlp(simdata[, c("X1", "X2", "X3")], simdata[, "Y"],
                 maxit = 1000, size = 15, linOut = TRUE)

sens &lt;- SensAnalysisMLP(mod, trData = simdata,
                        output_name = "Y", plot = FALSE)

AlphaSensCurve(sens$raw_sens[[1]][,1])

</code></pre>

<hr>
<h2 id='ChangeBootAlpha'>Change significance of boot SensMLP Class</h2><span id='topic+ChangeBootAlpha'></span>

<h3>Description</h3>

<p>For a SensMLP Class object, change the significance level of the statistical tests
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ChangeBootAlpha(x, boot.alpha)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ChangeBootAlpha_+3A_x">x</code></td>
<td>
<p><code>SensMLP</code> object created by <code><a href="#topic+SensAnalysisMLP">SensAnalysisMLP</a></code></p>
</td></tr>
<tr><td><code id="ChangeBootAlpha_+3A_boot.alpha">boot.alpha</code></td>
<td>
<p><code>float</code> significance level</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>SensMLP</code> object with changed significance level. All boot related
metrics are changed
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Load data -------------------------------------------------------------------
data("DAILY_DEMAND_TR")
fdata &lt;- DAILY_DEMAND_TR

## Parameters of the NNET ------------------------------------------------------
hidden_neurons &lt;- 5
iters &lt;- 250
decay &lt;- 0.1

################################################################################
#########################  REGRESSION NNET #####################################
################################################################################
## Regression dataframe --------------------------------------------------------
# Scale the data
fdata.Reg.tr &lt;- fdata[,2:ncol(fdata)]
fdata.Reg.tr[,3] &lt;- fdata.Reg.tr[,3]/10
fdata.Reg.tr[,1] &lt;- fdata.Reg.tr[,1]/1000


## TRAIN nnet NNET --------------------------------------------------------

set.seed(150)
nnetmod &lt;- caret::train(DEM ~ .,
                 data = fdata.Reg.tr,
                 method = "nnet",
                 tuneGrid = expand.grid(size = c(1), decay = c(0.01)),
                 trControl = caret::trainControl(method="none"),
                 preProcess = c('center', 'scale'),
                 linout = FALSE,
                 trace = FALSE,
                 maxit = 300)
# Try SensAnalysisMLP
sens &lt;- NeuralSens::SensAnalysisMLP(nnetmod, trData = fdata.Reg.tr,
                                    plot = FALSE, boot.R=2, output_name='DEM')
NeuralSens::ChangeBootAlpha(sens, boot.alpha=0.1)

</code></pre>

<hr>
<h2 id='CombineSens'>Sensitivity analysis plot over time of the data</h2><span id='topic+CombineSens'></span>

<h3>Description</h3>

<p>Plot of sensitivity of the neural network output respect
to the inputs over the time variable from the data provided
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CombineSens(object, comb_type = "mean")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="CombineSens_+3A_object">object</code></td>
<td>
<p><code>SensMLP</code> object generated by <code><a href="#topic+SensAnalysisMLP">SensAnalysisMLP</a></code>
with several outputs (classification MLP)</p>
</td></tr>
<tr><td><code id="CombineSens_+3A_comb_type">comb_type</code></td>
<td>
<p>Function to combine the matrixes of the <code>raw_sens</code> component of <code>object</code>.
It can be &quot;mean&quot;, &quot;median&quot; or &quot;sqmean&quot;. It can also be a function to combine the rows of the matrixes</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>SensMLP</code> object with the sensitivities combined
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
fdata &lt;- iris
## Parameters of the NNET ------------------------------------------------------
hidden_neurons &lt;- 5
iters &lt;- 250
decay &lt;- 0.1

#' ## TRAIN nnet NNET --------------------------------------------------------
# Create a formula to train NNET
form &lt;- paste(names(fdata)[1:ncol(fdata)-1], collapse = " + ")
form &lt;- formula(paste(names(fdata)[5], form, sep = " ~ "))

set.seed(150)
mod &lt;- nnet::nnet(form,
                  data = fdata,
                  linear.output = TRUE,
                  size = hidden_neurons,
                  decay = decay,
                  maxit = iters)
# mod should be a neural network classification model
sens &lt;- SensAnalysisMLP(mod, trData = fdata, output_name = 'Species')
combinesens &lt;- CombineSens(sens, "sqmean")

</code></pre>

<hr>
<h2 id='ComputeHessMeasures'>Plot sensitivities of a neural network model</h2><span id='topic+ComputeHessMeasures'></span>

<h3>Description</h3>

<p>Function to plot the sensitivities created by <code><a href="#topic+SensAnalysisMLP">SensAnalysisMLP</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ComputeHessMeasures(sens)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ComputeHessMeasures_+3A_sens">sens</code></td>
<td>
<p><code>SensAnalysisMLP</code> object created by <code><a href="#topic+SensAnalysisMLP">SensAnalysisMLP</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>SensAnalysisMLP</code> object with the sensitivities calculated
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load data -------------------------------------------------------------------
data("DAILY_DEMAND_TR")
fdata &lt;- DAILY_DEMAND_TR

## Parameters of the NNET ------------------------------------------------------
hidden_neurons &lt;- 5
iters &lt;- 250
decay &lt;- 0.1

################################################################################
#########################  REGRESSION NNET #####################################
################################################################################
## Regression dataframe --------------------------------------------------------
# Scale the data
fdata.Reg.tr &lt;- fdata[,2:ncol(fdata)]
fdata.Reg.tr[,3] &lt;- fdata.Reg.tr[,3]/10
fdata.Reg.tr[,1] &lt;- fdata.Reg.tr[,1]/1000

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.tr, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.tr)

#' ## TRAIN nnet NNET --------------------------------------------------------
# Create a formula to train NNET
form &lt;- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = " + ")
form &lt;- formula(paste(names(fdata.Reg.tr)[1], form, sep = " ~ "))

set.seed(150)
nnetmod &lt;- nnet::nnet(form,
                           data = nntrData,
                           linear.output = TRUE,
                           size = hidden_neurons,
                           decay = decay,
                           maxit = iters)
# Try SensAnalysisMLP
sens &lt;- NeuralSens::SensAnalysisMLP(nnetmod, trData = nntrData, plot = FALSE)
</code></pre>

<hr>
<h2 id='ComputeSensMeasures'>Plot sensitivities of a neural network model</h2><span id='topic+ComputeSensMeasures'></span>

<h3>Description</h3>

<p>Function to plot the sensitivities created by <code><a href="#topic+SensAnalysisMLP">SensAnalysisMLP</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ComputeSensMeasures(sens)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ComputeSensMeasures_+3A_sens">sens</code></td>
<td>
<p><code>SensAnalysisMLP</code> object created by <code><a href="#topic+SensAnalysisMLP">SensAnalysisMLP</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>SensAnalysisMLP</code> object with the sensitivities calculated
</p>


<h3>References</h3>

<p>Pizarroso J, Portela J, Muñoz A (2022). NeuralSens: Sensitivity Analysis of
Neural Networks. Journal of Statistical Software, 102(7), 1-36.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load data -------------------------------------------------------------------
data("DAILY_DEMAND_TR")
fdata &lt;- DAILY_DEMAND_TR

## Parameters of the NNET ------------------------------------------------------
hidden_neurons &lt;- 5
iters &lt;- 250
decay &lt;- 0.1

################################################################################
#########################  REGRESSION NNET #####################################
################################################################################
## Regression dataframe --------------------------------------------------------
# Scale the data
fdata.Reg.tr &lt;- fdata[,2:ncol(fdata)]
fdata.Reg.tr[,3] &lt;- fdata.Reg.tr[,3]/10
fdata.Reg.tr[,1] &lt;- fdata.Reg.tr[,1]/1000

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.tr, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.tr)

#' ## TRAIN nnet NNET --------------------------------------------------------
# Create a formula to train NNET
form &lt;- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = " + ")
form &lt;- formula(paste(names(fdata.Reg.tr)[1], form, sep = " ~ "))

set.seed(150)
nnetmod &lt;- nnet::nnet(form,
                           data = nntrData,
                           linear.output = TRUE,
                           size = hidden_neurons,
                           decay = decay,
                           maxit = iters)
# Try SensAnalysisMLP
sens &lt;- NeuralSens::SensAnalysisMLP(nnetmod, trData = nntrData, plot = FALSE)
</code></pre>

<hr>
<h2 id='DAILY_DEMAND_TR'>Data frame with 4 variables</h2><span id='topic+DAILY_DEMAND_TR'></span>

<h3>Description</h3>

<p>Training dataset with values of temperature and working day to predict electrical demand
</p>


<h3>Format</h3>

<p>A data frame with 1980 rows and 4 variables:
</p>

<dl>
<dt>DATE</dt><dd><p>date of the measure</p>
</dd>
<dt>DEM</dt><dd><p>electrical demand</p>
</dd>
<dt>WD</dt><dd><p>Working Day: index which express how much work is made that day</p>
</dd>
<dt>TEMP</dt><dd><p>weather temperature</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Jose Portela Gonzalez
</p>


<h3>References</h3>

<p>Pizarroso J, Portela J, Muñoz A (2022). NeuralSens: Sensitivity Analysis of
Neural Networks. Journal of Statistical Software, 102(7), 1-36.
</p>

<hr>
<h2 id='DAILY_DEMAND_TV'>Data frame with 3 variables</h2><span id='topic+DAILY_DEMAND_TV'></span>

<h3>Description</h3>

<p>Validation dataset with values of temperature and working day to predict electrical demand
</p>


<h3>Format</h3>

<p>A data frame with 7 rows and 3 variables:
</p>

<dl>
<dt>DATE</dt><dd><p>date of the measure</p>
</dd>
<dt>WD</dt><dd><p>Working Day: index which express how much work is made that day</p>
</dd>
<dt>TEMP</dt><dd><p>weather temperature</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Jose Portela Gonzalez
</p>


<h3>References</h3>

<p>Pizarroso J, Portela J, Muñoz A (2022). NeuralSens: Sensitivity Analysis of
Neural Networks. Journal of Statistical Software, 102(7), 1-36.
</p>

<hr>
<h2 id='Der2ActFunc'>Second derivative of activation function of neuron</h2><span id='topic+Der2ActFunc'></span>

<h3>Description</h3>

<p>Evaluate second derivative of activation function of a neuron
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Der2ActFunc(type = "sigmoid", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Der2ActFunc_+3A_type">type</code></td>
<td>
<p><code>character</code> name of the activation function</p>
</td></tr>
<tr><td><code id="Der2ActFunc_+3A_...">...</code></td>
<td>
<p>extra arguments needed to calculate the functions</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>numeric</code> output of the neuron
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Return derivative of the sigmoid activation function of a neuron
ActivationFunction &lt;- Der2ActFunc("sigmoid")
# Return derivative of the tanh activation function of a neuron
ActivationFunction &lt;- Der2ActFunc("tanh")
# Return derivative of the activation function of several layers of neurons
actfuncs &lt;- c("linear","sigmoid","linear")
ActivationFunctions &lt;- sapply(actfuncs, Der2ActFunc)
</code></pre>

<hr>
<h2 id='Der3ActFunc'>Third derivative of activation function of neuron</h2><span id='topic+Der3ActFunc'></span>

<h3>Description</h3>

<p>Evaluate third derivative of activation function of a neuron
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Der3ActFunc(type = "sigmoid", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Der3ActFunc_+3A_type">type</code></td>
<td>
<p><code>character</code> name of the activation function</p>
</td></tr>
<tr><td><code id="Der3ActFunc_+3A_...">...</code></td>
<td>
<p>extra arguments needed to calculate the functions</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>numeric</code> output of the neuron
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Return derivative of the sigmoid activation function of a neuron
ActivationFunction &lt;- Der3ActFunc("sigmoid")
# Return derivative of the tanh activation function of a neuron
ActivationFunction &lt;- Der3ActFunc("tanh")
# Return derivative of the activation function of several layers of neurons
actfuncs &lt;- c("linear","sigmoid","linear")
ActivationFunctions &lt;- sapply(actfuncs, Der3ActFunc)
</code></pre>

<hr>
<h2 id='DerActFunc'>Derivative of activation function of neuron</h2><span id='topic+DerActFunc'></span>

<h3>Description</h3>

<p>Evaluate derivative of activation function of a neuron
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DerActFunc(type = "sigmoid", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="DerActFunc_+3A_type">type</code></td>
<td>
<p><code>character</code> name of the activation function</p>
</td></tr>
<tr><td><code id="DerActFunc_+3A_...">...</code></td>
<td>
<p>extra arguments needed to calculate the functions</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>numeric</code> output of the neuron
</p>


<h3>References</h3>

<p>Pizarroso J, Portela J, Muñoz A (2022). NeuralSens: Sensitivity Analysis of
Neural Networks. Journal of Statistical Software, 102(7), 1-36.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Return derivative of the sigmoid activation function of a neuron
ActivationFunction &lt;- DerActFunc("sigmoid")
# Return derivative of the tanh activation function of a neuron
ActivationFunction &lt;- DerActFunc("tanh")
# Return derivative of the activation function of several layers of neurons
actfuncs &lt;- c("linear","sigmoid","linear")
ActivationFunctions &lt;- sapply(actfuncs, DerActFunc)
</code></pre>

<hr>
<h2 id='diag3Darray'>Define function to create a 'diagonal' array or get the diagonal of an array</h2><span id='topic+diag3Darray'></span>

<h3>Description</h3>

<p>Define function to create a 'diagonal' array or get the diagonal of an array
</p>


<h3>Usage</h3>

<pre><code class='language-R'>diag3Darray(x = 1, dim = length(x), out = "vector")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="diag3Darray_+3A_x">x</code></td>
<td>
<p><code>number</code> or <code>vector</code> defining the value of the diagonal of <code>3D array</code></p>
</td></tr>
<tr><td><code id="diag3Darray_+3A_dim">dim</code></td>
<td>
<p><code>integer</code> defining the length of the diagonal. Default is <code>length(x)</code>.
If <code>length(x) != 1</code>, <code>dim</code> must be equal to <code>length(x)</code>.</p>
</td></tr>
<tr><td><code id="diag3Darray_+3A_out">out</code></td>
<td>
<p><code>character</code> specifying which type of diagonal to return (<code>"vector"</code>
or <code>"matrix"</code>). See <code>Details</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The diagonal of a 3D array has been defined as those elements in positions c(int,int,int),
i.e., the three digits are the same.
</p>
<p>If the diagonal should be returned, <code>out</code> specifies if it should return a <code>"vector"</code> with
the elements of position c(int,int,int), or <code>"matrix"</code> with the elements of position c(int,dim,int),
i.e., <code>dim = 2</code> -&gt; elements (1,1,1),(2,1,2),(3,1,3),(1,2,1),(2,2,2),(3,2,3),(3,1,3),(3,2,3),(3,3,3).
</p>


<h3>Value</h3>

<p><code>array</code> with all elements zero except the diagonal, with dimensions c(dim,dim,dim)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- diag3Darray(c(1,4,6), dim = 3)
x
# , , 1
#
# [,1] [,2] [,3]
# [1,]    1    0    0
# [2,]    0    0    0
# [3,]    0    0    0
#
# , , 2
#
# [,1] [,2] [,3]
# [1,]    0    0    0
# [2,]    0    4    0
# [3,]    0    0    0
#
# , , 3
#
# [,1] [,2] [,3]
# [1,]    0    0    0
# [2,]    0    0    0
# [3,]    0    0    6
diag3Darray(x)
# 1, 4, 6
</code></pre>

<hr>
<h2 id='diag3Darray+26lt+3B-'>Define function to change the diagonal of array</h2><span id='topic+diag3Darray+3C-'></span>

<h3>Description</h3>

<p>Define function to change the diagonal of array
</p>


<h3>Usage</h3>

<pre><code class='language-R'>diag3Darray(x) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="diag3Darray+2B26lt+2B3B-_+3A_x">x</code></td>
<td>
<p><code>3D array</code> whose diagonal must be c hanged</p>
</td></tr>
<tr><td><code id="diag3Darray+2B26lt+2B3B-_+3A_value">value</code></td>
<td>
<p><code>vector</code> defining the new values of diagonal.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The diagonal of a 3D array has been defined as those elements in positions c(int,int,int),
i.e., the three digits are the same.
</p>


<h3>Value</h3>

<p><code>array</code> with all elements zero except the diagonal, with dimensions c(dim,dim,dim)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- array(1, dim = c(3,3,3))
diag3Darray(x) &lt;- c(2,2,2)
x
#  , , 1
#
#  [,1] [,2] [,3]
#  [1,]    2    1    1
#  [2,]    1    1    1
#  [3,]    1    1    1
#
#  , , 2
#
#  [,1] [,2] [,3]
#  [1,]    1    1    1
#  [2,]    1    2    1
#  [3,]    1    1    1
#
#  , , 3
#
#  [,1] [,2] [,3]
#  [1,]    1    1    1
#  [2,]    1    1    1
#  [3,]    1    1    2
</code></pre>

<hr>
<h2 id='diag4Darray'>Define function to create a 'diagonal' array or get the diagonal of an array</h2><span id='topic+diag4Darray'></span>

<h3>Description</h3>

<p>Define function to create a 'diagonal' array or get the diagonal of an array
</p>


<h3>Usage</h3>

<pre><code class='language-R'>diag4Darray(x = 1, dim = length(x))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="diag4Darray_+3A_x">x</code></td>
<td>
<p><code>number</code> or <code>vector</code> defining the value of the diagonal of <code>4D array</code></p>
</td></tr>
<tr><td><code id="diag4Darray_+3A_dim">dim</code></td>
<td>
<p><code>integer</code> defining the length of the diagonal. Default is <code>length(x)</code>.
If <code>length(x) != 1</code>, <code>dim</code> must be equal to <code>length(x)</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The diagonal of a 4D array has been defined as those elements in positions c(int,int,int,int),
i.e., the four digits are the same.
</p>


<h3>Value</h3>

<p><code>array</code> with all elements zero except the diagonal, with dimensions c(dim,dim,dim)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- diag4Darray(c(1,3,6,2), dim = 4)
x
# , , 1, 1
#
#      [,1] [,2] [,3] [,4]
# [1,]    1    0    0    0
# [2,]    0    0    0    0
# [3,]    0    0    0    0
# [4,]    0    0    0    0
#
# , , 2, 1
#
#      [,1] [,2] [,3] [,4]
# [1,]    0    0    0    0
# [2,]    0    0    0    0
# [3,]    0    0    0    0
# [4,]    0    0    0    0
#
# , , 3, 1
#
#      [,1] [,2] [,3] [,4]
# [1,]    0    0    0    0
# [2,]    0    0    0    0
# [3,]    0    0    0    0
# [4,]    0    0    0    0
#
# , , 4, 1
#
#      [,1] [,2] [,3] [,4]
# [1,]    0    0    0    0
# [2,]    0    0    0    0
# [3,]    0    0    0    0
# [4,]    0    0    0    0
#
# , , 1, 2
#
#      [,1] [,2] [,3] [,4]
# [1,]    0    0    0    0
# [2,]    0    0    0    0
# [3,]    0    0    0    0
# [4,]    0    0    0    0
#
# , , 2, 2
#
#      [,1] [,2] [,3] [,4]
# [1,]    0    0    0    0
# [2,]    0    3    0    0
# [3,]    0    0    0    0
# [4,]    0    0    0    0
#
# , , 3, 2
#
#      [,1] [,2] [,3] [,4]
# [1,]    0    0    0    0
# [2,]    0    0    0    0
# [3,]    0    0    0    0
# [4,]    0    0    0    0
#
# , , 4, 2
#
#      [,1] [,2] [,3] [,4]
# [1,]    0    0    0    0
# [2,]    0    0    0    0
# [3,]    0    0    0    0
# [4,]    0    0    0    0
#
# , , 1, 3
#
#      [,1] [,2] [,3] [,4]
# [1,]    0    0    0    0
# [2,]    0    0    0    0
# [3,]    0    0    0    0
# [4,]    0    0    0    0
#
# , , 2, 3
#
#      [,1] [,2] [,3] [,4]
# [1,]    0    0    0    0
# [2,]    0    0    0    0
# [3,]    0    0    0    0
# [4,]    0    0    0    0
#
# , , 3, 3
#
#      [,1] [,2] [,3] [,4]
# [1,]    0    0    0    0
# [2,]    0    0    0    0
# [3,]    0    0    6    0
# [4,]    0    0    0    0
#
# , , 4, 3
#
#      [,1] [,2] [,3] [,4]
# [1,]    0    0    0    0
# [2,]    0    0    0    0
# [3,]    0    0    0    0
# [4,]    0    0    0    0
#
# , , 1, 4
#
#      [,1] [,2] [,3] [,4]
# [1,]    0    0    0    0
# [2,]    0    0    0    0
# [3,]    0    0    0    0
# [4,]    0    0    0    0
#
# , , 2, 4
#
#      [,1] [,2] [,3] [,4]
# [1,]    0    0    0    0
# [2,]    0    0    0    0
# [3,]    0    0    0    0
# [4,]    0    0    0    0
#
# , , 3, 4
#
#      [,1] [,2] [,3] [,4]
# [1,]    0    0    0    0
# [2,]    0    0    0    0
# [3,]    0    0    0    0
# [4,]    0    0    0    0
#
# , , 4, 4
#
#      [,1] [,2] [,3] [,4]
# [1,]    0    0    0    0
# [2,]    0    0    0    0
# [3,]    0    0    0    0
# [4,]    0    0    0    2
diag4Darray(x)
# 1, 3, 6, 2
</code></pre>

<hr>
<h2 id='diag4Darray+26lt+3B-'>Define function to change the diagonal of array</h2><span id='topic+diag4Darray+3C-'></span>

<h3>Description</h3>

<p>Define function to change the diagonal of array
</p>


<h3>Usage</h3>

<pre><code class='language-R'>diag4Darray(x) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="diag4Darray+2B26lt+2B3B-_+3A_x">x</code></td>
<td>
<p><code>3D array</code> whose diagonal must be c hanged</p>
</td></tr>
<tr><td><code id="diag4Darray+2B26lt+2B3B-_+3A_value">value</code></td>
<td>
<p><code>vector</code> defining the new values of diagonal.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The diagonal of a 3D array has been defined as those elements in positions c(int,int,int),
i.e., the three digits are the same.
</p>


<h3>Value</h3>

<p><code>array</code> with all elements zero except the diagonal, with dimensions c(dim,dim,dim)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- array(1, dim = c(4,4,4,4))
diag4Darray(x) &lt;- c(2,2,2,2)
x
# , , 1, 1
#
#      [,1] [,2] [,3] [,4]
# [1,]    2    1    1    1
# [2,]    1    1    1    1
# [3,]    1    1    1    1
# [4,]    1    1    1    1
#
# , , 2, 1
#
#      [,1] [,2] [,3] [,4]
# [1,]    1    1    1    1
# [2,]    1    1    1    1
# [3,]    1    1    1    1
# [4,]    1    1    1    1
#
# , , 3, 1
#
#      [,1] [,2] [,3] [,4]
# [1,]    1    1    1    1
# [2,]    1    1    1    1
# [3,]    1    1    1    1
# [4,]    1    1    1    1
#
# , , 4, 1
#
#      [,1] [,2] [,3] [,4]
# [1,]    1    1    1    1
# [2,]    1    1    1    1
# [3,]    1    1    1    1
# [4,]    1    1    1    1
#
# , , 1, 2
#
#      [,1] [,2] [,3] [,4]
# [1,]    1    1    1    1
# [2,]    1    1    1    1
# [3,]    1    1    1    1
# [4,]    1    1    1    1
#
# , , 2, 2
#
#      [,1] [,2] [,3] [,4]
# [1,]    1    1    1    1
# [2,]    1    2    1    1
# [3,]    1    1    1    1
# [4,]    1    1    1    1
#
# , , 3, 2
#
#      [,1] [,2] [,3] [,4]
# [1,]    1    1    1    1
# [2,]    1    1    1    1
# [3,]    1    1    1    1
# [4,]    1    1    1    1
#
# , , 4, 2
#
#      [,1] [,2] [,3] [,4]
# [1,]    1    1    1    1
# [2,]    1    1    1    1
# [3,]    1    1    1    1
# [4,]    1    1    1    1
#
# , , 1, 3
#
#      [,1] [,2] [,3] [,4]
# [1,]    1    1    1    1
# [2,]    1    1    1    1
# [3,]    1    1    1    1
# [4,]    1    1    1    1
#
# , , 2, 3
#
#      [,1] [,2] [,3] [,4]
# [1,]    1    1    1    1
# [2,]    1    1    1    1
# [3,]    1    1    1    1
# [4,]    1    1    1    1
#
# , , 3, 3
#
#      [,1] [,2] [,3] [,4]
# [1,]    1    1    1    1
# [2,]    1    1    1    1
# [3,]    1    1    2    1
# [4,]    1    1    1    1
#
# , , 4, 3
#
#      [,1] [,2] [,3] [,4]
# [1,]    1    1    1    1
# [2,]    1    1    1    1
# [3,]    1    1    1    1
# [4,]    1    1    1    1
#
# , , 1, 4
#
#      [,1] [,2] [,3] [,4]
# [1,]    1    1    1    1
# [2,]    1    1    1    1
# [3,]    1    1    1    1
# [4,]    1    1    1    1
#
# , , 2, 4
#
#      [,1] [,2] [,3] [,4]
# [1,]    1    1    1    1
# [2,]    1    1    1    1
# [3,]    1    1    1    1
# [4,]    1    1    1    1
#
# , , 3, 4
#
#      [,1] [,2] [,3] [,4]
# [1,]    1    1    1    1
# [2,]    1    1    1    1
# [3,]    1    1    1    1
# [4,]    1    1    1    1
#
# , , 4, 4
#
#      [,1] [,2] [,3] [,4]
# [1,]    1    1    1    1
# [2,]    1    1    1    1
# [3,]    1    1    1    1
# [4,]    1    1    1    2
</code></pre>

<hr>
<h2 id='find_critical_value'>Find Critical Value</h2><span id='topic+find_critical_value'></span>

<h3>Description</h3>

<p>This function finds the smallest x such that the probability of a random variable
being less than or equal to x is greater than or equal to 1 - alpha.
It uses the uniroot function to find where the empirical cumulative distribution function (ECDF)
crosses 1 - alpha.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_critical_value(ecdf_func, alpha)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="find_critical_value_+3A_ecdf_func">ecdf_func</code></td>
<td>
<p>An ECDF function representing the distribution of a random variable.</p>
</td></tr>
<tr><td><code id="find_critical_value_+3A_alpha">alpha</code></td>
<td>
<p>A numeric value specifying the significance level.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The smallest x such that P(X &lt;= x) &gt;= 1 - alpha.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- rnorm(100)
ecdf_data &lt;- ecdf(data)
critical_val &lt;- find_critical_value(ecdf_data, 0.05)
</code></pre>

<hr>
<h2 id='HessDotPlot'>Second derivatives 3D scatter or surface plot against input values</h2><span id='topic+HessDotPlot'></span>

<h3>Description</h3>

<p>3D Plot of second derivatives of the neural network output respect
to the inputs. This function use <code>plotly</code> instead of <code>ggplot2</code> to
achieve better visualization
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HessDotPlot(
  object,
  fdata = NULL,
  input_vars = "all",
  input_vars2 = "all",
  output_vars = "all",
  surface = FALSE,
  grid = FALSE,
  color = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="HessDotPlot_+3A_object">object</code></td>
<td>
<p>fitted neural network model or <code>array</code> containing the raw
second derivatives from the function <code><a href="#topic+HessianMLP">HessianMLP</a></code></p>
</td></tr>
<tr><td><code id="HessDotPlot_+3A_fdata">fdata</code></td>
<td>
<p><code>data.frame</code> containing the data to evaluate the second derivatives of the model.</p>
</td></tr>
<tr><td><code id="HessDotPlot_+3A_input_vars">input_vars</code></td>
<td>
<p><code>character vector</code> with the variables to create the scatter plot in x-axis. If <code>"all"</code>,
then scatter plots are created for all the input variables in <code>fdata</code>.</p>
</td></tr>
<tr><td><code id="HessDotPlot_+3A_input_vars2">input_vars2</code></td>
<td>
<p><code>character vector</code> with the variables to create the scatter plot in y-axis. If <code>"all"</code>,
then scatter plots are created for all the input variables in <code>fdata</code>.</p>
</td></tr>
<tr><td><code id="HessDotPlot_+3A_output_vars">output_vars</code></td>
<td>
<p><code>character vector</code> with the variables to create the scatter plot. If <code>"all"</code>,
then scatter plots are created for all the output variables in <code>fdata</code>.</p>
</td></tr>
<tr><td><code id="HessDotPlot_+3A_surface">surface</code></td>
<td>
<p><code>logical</code> if <code>TRUE</code>, a 3D surface is created instead of 3D scatter plot
(only for combinations of different inputs)</p>
</td></tr>
<tr><td><code id="HessDotPlot_+3A_grid">grid</code></td>
<td>
<p><code>logical</code>. If <code>TRUE</code>, plots created are show together using <code><a href="gridExtra.html#topic+arrangeGrob">arrangeGrob</a></code>.
It does not work on Windows platforms due to bugs in <code>plotly</code> library.</p>
</td></tr>
<tr><td><code id="HessDotPlot_+3A_color">color</code></td>
<td>
<p><code>character</code> specifying the name of a <code>numeric</code> variable of <code>fdata</code> to color the 3D scatter plot.</p>
</td></tr>
<tr><td><code id="HessDotPlot_+3A_...">...</code></td>
<td>
<p>further arguments that should be passed to  <code><a href="#topic+HessianMLP">HessianMLP</a></code> function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of 3D <code>geom_point</code> plots for the inputs variables representing the
sensitivity of each output respect to the inputs
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load data -------------------------------------------------------------------
data("DAILY_DEMAND_TR")
fdata &lt;- DAILY_DEMAND_TR
## Parameters of the NNET ------------------------------------------------------
hidden_neurons &lt;- 5
iters &lt;- 250
decay &lt;- 0.1

################################################################################
#########################  REGRESSION NNET #####################################
################################################################################
## Regression dataframe --------------------------------------------------------
# Scale the data
fdata.Reg.tr &lt;- fdata[,2:ncol(fdata)]
fdata.Reg.tr[,3] &lt;- fdata.Reg.tr[,3]/10
fdata.Reg.tr[,1] &lt;- fdata.Reg.tr[,1]/1000

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.tr, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.tr)

#' ## TRAIN nnet NNET --------------------------------------------------------
# Create a formula to train NNET
form &lt;- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = " + ")
form &lt;- formula(paste(names(fdata.Reg.tr)[1], form, sep = " ~ "))

set.seed(150)
nnetmod &lt;- nnet::nnet(form,
                      data = nntrData,
                      linear.output = TRUE,
                      size = hidden_neurons,
                      decay = decay,
                      maxit = iters)
# Try HessDotPlot
NeuralSens::HessDotPlot(nnetmod, fdata = nntrData, surface = TRUE, color = "WD")
</code></pre>

<hr>
<h2 id='HessFeaturePlot'>Feature sensitivity plot</h2><span id='topic+HessFeaturePlot'></span>

<h3>Description</h3>

<p>Show the distribution of the sensitivities of the output
in <code>geom_sina()</code> plot which color depends on the input values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HessFeaturePlot(object, fdata = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="HessFeaturePlot_+3A_object">object</code></td>
<td>
<p>fitted neural network model or <code>array</code> containing the raw
sensitivities from the function <code><a href="#topic+SensAnalysisMLP">SensAnalysisMLP</a></code></p>
</td></tr>
<tr><td><code id="HessFeaturePlot_+3A_fdata">fdata</code></td>
<td>
<p><code>data.frame</code> containing the data to evaluate the sensitivity of the model.
Not needed if the raw sensitivities has been passed as <code>object</code></p>
</td></tr>
<tr><td><code id="HessFeaturePlot_+3A_...">...</code></td>
<td>
<p>further arguments that should be passed to  <code><a href="#topic+SensAnalysisMLP">SensAnalysisMLP</a></code> function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of Feature sensitivity plot as described in
<a href="https://www.r-bloggers.com/2019/03/a-gentle-introduction-to-shap-values-in-r/">https://www.r-bloggers.com/2019/03/a-gentle-introduction-to-shap-values-in-r/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load data -------------------------------------------------------------------
data("DAILY_DEMAND_TR")
fdata &lt;- DAILY_DEMAND_TR

## Parameters of the NNET ------------------------------------------------------
hidden_neurons &lt;- 5
iters &lt;- 250
decay &lt;- 0.1

################################################################################
#########################  REGRESSION NNET #####################################
################################################################################
## Regression dataframe --------------------------------------------------------
# Scale the data
fdata.Reg.tr &lt;- fdata[,2:ncol(fdata)]
fdata.Reg.tr[,3] &lt;- fdata.Reg.tr[,3]/10
fdata.Reg.tr[,1] &lt;- fdata.Reg.tr[,1]/1000

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.tr, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.tr)

#' ## TRAIN nnet NNET --------------------------------------------------------
# Create a formula to train NNET
form &lt;- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = " + ")
form &lt;- formula(paste(names(fdata.Reg.tr)[1], form, sep = " ~ "))

set.seed(150)
nnetmod &lt;- nnet::nnet(form,
                           data = nntrData,
                           linear.output = TRUE,
                           size = hidden_neurons,
                           decay = decay,
                           maxit = iters)
# Try SensAnalysisMLP
hess &lt;- NeuralSens::HessianMLP(nnetmod, trData = nntrData, plot = FALSE)
NeuralSens::HessFeaturePlot(hess)
</code></pre>

<hr>
<h2 id='HessianMLP'>Sensitivity of MLP models</h2><span id='topic+HessianMLP'></span><span id='topic+HessianMLP.default'></span><span id='topic+HessianMLP.train'></span><span id='topic+HessianMLP.H2OMultinomialModel'></span><span id='topic+HessianMLP.H2ORegressionModel'></span><span id='topic+HessianMLP.list'></span><span id='topic+HessianMLP.mlp'></span><span id='topic+HessianMLP.nn'></span><span id='topic+HessianMLP.nnet'></span><span id='topic+HessianMLP.nnetar'></span><span id='topic+HessianMLP.numeric'></span>

<h3>Description</h3>

<p>Function for evaluating the sensitivities of the inputs
variables in a mlp model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HessianMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  ...
)

## Default S3 method:
HessianMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  trData,
  actfunc = NULL,
  deractfunc = NULL,
  der2actfunc = NULL,
  preProc = NULL,
  terms = NULL,
  output_name = NULL,
  ...
)

## S3 method for class 'train'
HessianMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  ...
)

## S3 method for class 'H2OMultinomialModel'
HessianMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  ...
)

## S3 method for class 'H2ORegressionModel'
HessianMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  ...
)

## S3 method for class 'list'
HessianMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  trData,
  actfunc,
  ...
)

## S3 method for class 'mlp'
HessianMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  trData,
  preProc = NULL,
  terms = NULL,
  ...
)

## S3 method for class 'nn'
HessianMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  preProc = NULL,
  terms = NULL,
  ...
)

## S3 method for class 'nnet'
HessianMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  trData,
  preProc = NULL,
  terms = NULL,
  ...
)

## S3 method for class 'nnetar'
HessianMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  ...
)

## S3 method for class 'numeric'
HessianMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  trData,
  actfunc = NULL,
  preProc = NULL,
  terms = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="HessianMLP_+3A_mlp.fit">MLP.fit</code></td>
<td>
<p>fitted neural network model</p>
</td></tr>
<tr><td><code id="HessianMLP_+3A_.returnsens">.returnSens</code></td>
<td>
<p>DEPRECATED</p>
</td></tr>
<tr><td><code id="HessianMLP_+3A_plot">plot</code></td>
<td>
<p><code>logical</code> whether or not to plot the analysis. By default is
<code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="HessianMLP_+3A_.rawsens">.rawSens</code></td>
<td>
<p>DEPRECATED</p>
</td></tr>
<tr><td><code id="HessianMLP_+3A_sens_origin_layer">sens_origin_layer</code></td>
<td>
<p><code>numeric</code> specifies the layer of neurons with
respect to which the derivative must be calculated. The input layer is
specified by 1 (default).</p>
</td></tr>
<tr><td><code id="HessianMLP_+3A_sens_end_layer">sens_end_layer</code></td>
<td>
<p><code>numeric</code> specifies the layer of neurons of which
the derivative is calculated. It may also be 'last' to specify the output
layer (default).</p>
</td></tr>
<tr><td><code id="HessianMLP_+3A_sens_origin_input">sens_origin_input</code></td>
<td>
<p><code>logical</code> specifies if the derivative must be
calculated with respect to the inputs (<code>TRUE</code>) or output
(<code>FALSE</code>) of the <code>sens_origin_layer</code> layer of the model. By
default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="HessianMLP_+3A_sens_end_input">sens_end_input</code></td>
<td>
<p><code>logical</code> specifies if the derivative calculated
is of the output (<code>FALSE</code>) or from the input (<code>TRUE</code>) of the
<code>sens_end_layer</code> layer of the model. By default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="HessianMLP_+3A_...">...</code></td>
<td>
<p>additional arguments passed to or from other methods</p>
</td></tr>
<tr><td><code id="HessianMLP_+3A_trdata">trData</code></td>
<td>
<p><code>data.frame</code> containing the data to evaluate the sensitivity of the model</p>
</td></tr>
<tr><td><code id="HessianMLP_+3A_actfunc">actfunc</code></td>
<td>
<p><code>character</code> vector indicating the activation function of each
neurons layer.</p>
</td></tr>
<tr><td><code id="HessianMLP_+3A_deractfunc">deractfunc</code></td>
<td>
<p><code>character</code> vector indicating the derivative of the activation
function of each neurons layer.</p>
</td></tr>
<tr><td><code id="HessianMLP_+3A_der2actfunc">der2actfunc</code></td>
<td>
<p><code>character</code> vector indicating the second derivative of the activation
function of each neurons layer.</p>
</td></tr>
<tr><td><code id="HessianMLP_+3A_preproc">preProc</code></td>
<td>
<p>preProcess structure applied to the training data. See also
<code><a href="caret.html#topic+preProcess">preProcess</a></code></p>
</td></tr>
<tr><td><code id="HessianMLP_+3A_terms">terms</code></td>
<td>
<p>function applied to the training data to create factors. See
also <code><a href="caret.html#topic+train">train</a></code></p>
</td></tr>
<tr><td><code id="HessianMLP_+3A_output_name">output_name</code></td>
<td>
<p><code>character</code> name of the output variable in order to
avoid changing the name of the output variable in <code>trData</code> to
'.outcome'</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In case of using an input of class <code>factor</code> and a package which
need to enter the input data as matrix, the dummies must be created before
training the neural network.
</p>
<p>After that, the training data must be given to the function using the
<code>trData</code> argument.
</p>


<h3>Value</h3>

<p><code>SensMLP</code> object with the sensitivity metrics and sensitivities of
the MLP model passed to the function.
</p>


<h3>Plots</h3>

 <ul>
<li><p> Plot 1: colorful plot with the classification
of the classes in a 2D map </p>
</li>
<li><p> Plot 2: b/w plot with probability of the
chosen class in a 2D map </p>
</li>
<li><p> Plot 3: plot with the stats::predictions of
the data provided </p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Load data -------------------------------------------------------------------
data("DAILY_DEMAND_TR")
fdata &lt;- DAILY_DEMAND_TR
## Parameters of the NNET ------------------------------------------------------
hidden_neurons &lt;- 5
iters &lt;- 100
decay &lt;- 0.1

################################################################################
#########################  REGRESSION NNET #####################################
################################################################################
## Regression dataframe --------------------------------------------------------
# Scale the data
fdata.Reg.tr &lt;- fdata[,2:ncol(fdata)]
fdata.Reg.tr[,3] &lt;- fdata.Reg.tr[,3]/10
fdata.Reg.tr[,1] &lt;- fdata.Reg.tr[,1]/1000

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.tr, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.tr)

#' ## TRAIN nnet NNET --------------------------------------------------------
# Create a formula to train NNET
form &lt;- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = " + ")
form &lt;- formula(paste(names(fdata.Reg.tr)[1], form, sep = " ~ "))

set.seed(150)
nnetmod &lt;- nnet::nnet(form,
                      data = nntrData,
                      linear.output = TRUE,
                      size = hidden_neurons,
                      decay = decay,
                      maxit = iters)
# Try HessianMLP
NeuralSens::HessianMLP(nnetmod, trData = nntrData, plot = FALSE)

# Try HessianMLP to calculate sensitivities with respect to output of hidden neurones
NeuralSens::HessianMLP(nnetmod, trData = nntrData,
                             sens_origin_layer = 2,
                             sens_end_layer = "last",
                             sens_origin_input = FALSE,
                             sens_end_input = FALSE)
## Train caret NNET ------------------------------------------------------------
# Create trainControl
ctrl_tune &lt;- caret::trainControl(method = "boot",
                                 savePredictions = FALSE,
                                 summaryFunction = caret::defaultSummary)
set.seed(150) #For replication
caretmod &lt;- caret::train(form = DEM~.,
                              data = fdata.Reg.tr,
                              method = "nnet",
                              linout = TRUE,
                              tuneGrid = data.frame(size = 3,
                                                    decay = decay),
                              maxit = iters,
                              preProcess = c("center","scale"),
                              trControl = ctrl_tune,
                              metric = "RMSE")

# Try HessianMLP
NeuralSens::HessianMLP(caretmod)

## Train h2o NNET --------------------------------------------------------------
# Create a cluster with 4 available cores
h2o::h2o.init(ip = "localhost",
              nthreads = 4)

# Reset the cluster
h2o::h2o.removeAll()
fdata_h2o &lt;- h2o::as.h2o(x = fdata.Reg.tr, destination_frame = "fdata_h2o")

set.seed(150)
h2omod &lt;-h2o:: h2o.deeplearning(x = names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)],
                                     y = names(fdata.Reg.tr)[1],
                                     distribution = "AUTO",
                                     training_frame = fdata_h2o,
                                     standardize = TRUE,
                                     activation = "Tanh",
                                     hidden = c(hidden_neurons),
                                     stopping_rounds = 0,
                                     epochs = iters,
                                     seed = 150,
                                     model_id = "nnet_h2o",
                                     adaptive_rate = FALSE,
                                     rate_decay = decay,
                                     export_weights_and_biases = TRUE)

# Try HessianMLP
NeuralSens::HessianMLP(h2omod)

# Turn off the cluster
h2o::h2o.shutdown(prompt = FALSE)
rm(fdata_h2o)

## Train RSNNS NNET ------------------------------------------------------------
# Normalize data using RSNNS algorithms
trData &lt;- as.data.frame(RSNNS::normalizeData(fdata.Reg.tr))
names(trData) &lt;- names(fdata.Reg.tr)
set.seed(150)
RSNNSmod &lt;-RSNNS::mlp(x = trData[,2:ncol(trData)],
                           y = trData[,1],
                           size = hidden_neurons,
                           linOut = TRUE,
                           learnFuncParams=c(decay),
                           maxit=iters)

# Try HessianMLP
NeuralSens::HessianMLP(RSNNSmod, trData = trData, output_name = "DEM")

## USE DEFAULT METHOD ----------------------------------------------------------
NeuralSens::HessianMLP(caretmod$finalModel$wts,
                            trData = fdata.Reg.tr,
                            mlpstr = caretmod$finalModel$n,
                            coefnames = caretmod$coefnames,
                            actfun = c("linear","sigmoid","linear"),
                            output_name = "DEM")

################################################################################
#########################  CLASSIFICATION NNET #################################
################################################################################
## Regression dataframe --------------------------------------------------------
# Scale the data
fdata.Reg.cl &lt;- fdata[,2:ncol(fdata)]
fdata.Reg.cl[,2:3] &lt;- fdata.Reg.cl[,2:3]/10
fdata.Reg.cl[,1] &lt;- fdata.Reg.cl[,1]/1000

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.cl, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.cl)

# Factorize the output
fdata.Reg.cl$DEM &lt;- factor(round(fdata.Reg.cl$DEM, digits = 1))

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.cl, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.cl)

## Train caret NNET ------------------------------------------------------------
# Create trainControl
ctrl_tune &lt;- caret::trainControl(method = "boot",
                                 savePredictions = FALSE,
                                 summaryFunction = caret::defaultSummary)
set.seed(150) #For replication
caretmod &lt;- caret::train(form = DEM~.,
                                data = fdata.Reg.cl,
                                method = "nnet",
                                linout = FALSE,
                                tuneGrid = data.frame(size = hidden_neurons,
                                                      decay = decay),
                                maxit = iters,
                                preProcess = c("center","scale"),
                                trControl = ctrl_tune,
                                metric = "Accuracy")

# Try HessianMLP
NeuralSens::HessianMLP(caretmod)

## Train h2o NNET --------------------------------------------------------------
# Create local cluster with 4 available cores
h2o::h2o.init(ip = "localhost",
              nthreads = 4)

# Reset the cluster
h2o::h2o.removeAll()
fdata_h2o &lt;- h2o::as.h2o(x = fdata.Reg.cl, destination_frame = "fdata_h2o")

set.seed(150)
h2omod &lt;- h2o::h2o.deeplearning(x = names(fdata.Reg.cl)[2:ncol(fdata.Reg.cl)],
                                       y = names(fdata.Reg.cl)[1],
                                       distribution = "AUTO",
                                       training_frame = fdata_h2o,
                                       standardize = TRUE,
                                       activation = "Tanh",
                                       hidden = c(hidden_neurons),
                                       stopping_rounds = 0,
                                       epochs = iters,
                                       seed = 150,
                                       model_id = "nnet_h2o",
                                       adaptive_rate = FALSE,
                                       rate_decay = decay,
                                       export_weights_and_biases = TRUE)

# Try HessianMLP
NeuralSens::HessianMLP(h2omod)

# Apaga el cluster
h2o::h2o.shutdown(prompt = FALSE)
rm(fdata_h2o)

## TRAIN nnet NNET --------------------------------------------------------
# Create a formula to train NNET
form &lt;- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = " + ")
form &lt;- formula(paste(names(fdata.Reg.tr)[1], form, sep = " ~ "))

set.seed(150)
nnetmod &lt;- nnet::nnet(form,
                      data = nntrData,
                      linear.output = TRUE,
                      size = hidden_neurons,
                      decay = decay,
                      maxit = iters)
# Try HessianMLP
NeuralSens::HessianMLP(nnetmod, trData = nntrData)

</code></pre>

<hr>
<h2 id='HessMLP'>Constructor of the HessMLP Class</h2><span id='topic+HessMLP'></span>

<h3>Description</h3>

<p>Create an object of HessMLP class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HessMLP(
  sens = list(),
  raw_sens = list(),
  mlp_struct = numeric(),
  trData = data.frame(),
  coefnames = character(),
  output_name = character()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="HessMLP_+3A_sens">sens</code></td>
<td>
<p><code>list</code> of sensitivity measures, one <code>list</code> per output neuron</p>
</td></tr>
<tr><td><code id="HessMLP_+3A_raw_sens">raw_sens</code></td>
<td>
<p><code>list</code> of sensitivities, one <code>array</code> per output neuron</p>
</td></tr>
<tr><td><code id="HessMLP_+3A_mlp_struct">mlp_struct</code></td>
<td>
<p><code>numeric</code> vector describing the structur of the MLP model</p>
</td></tr>
<tr><td><code id="HessMLP_+3A_trdata">trData</code></td>
<td>
<p><code>data.frame</code> with the data used to calculate the sensitivities</p>
</td></tr>
<tr><td><code id="HessMLP_+3A_coefnames">coefnames</code></td>
<td>
<p><code>character</code> vector with the name of the predictor(s)</p>
</td></tr>
<tr><td><code id="HessMLP_+3A_output_name">output_name</code></td>
<td>
<p><code>character</code> vector with the name of the output(s)</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>HessMLP</code> object
</p>

<hr>
<h2 id='HessToSensMLP'>Convert a HessMLP to a SensMLP object</h2><span id='topic+HessToSensMLP'></span>

<h3>Description</h3>

<p>Auxiliary function to turn a HessMLP object to a SensMLP object in order to use the plot-related functions
associated with SensMLP
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HessToSensMLP(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="HessToSensMLP_+3A_x">x</code></td>
<td>
<p><code>HessMLP</code> object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>SensMLP</code> object
</p>

<hr>
<h2 id='is.HessMLP'>Check if object is of class <code>HessMLP</code></h2><span id='topic+is.HessMLP'></span>

<h3>Description</h3>

<p>Check if object is of class <code>HessMLP</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.HessMLP(object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.HessMLP_+3A_object">object</code></td>
<td>
<p><code>HessMLP</code> object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if <code>object</code> is a <code>HessMLP</code> object
</p>

<hr>
<h2 id='is.SensMLP'>Check if object is of class <code>SensMLP</code></h2><span id='topic+is.SensMLP'></span>

<h3>Description</h3>

<p>Check if object is of class <code>SensMLP</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.SensMLP(object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.SensMLP_+3A_object">object</code></td>
<td>
<p><code>SensMLP</code> object</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if <code>object</code> is a <code>SensMLP</code> object
</p>


<h3>References</h3>

<p>Pizarroso J, Portela J, Muñoz A (2022). NeuralSens: Sensitivity Analysis of
Neural Networks. Journal of Statistical Software, 102(7), 1-36.
</p>

<hr>
<h2 id='kStepMAlgorithm'>k-StepM Algorithm for Hypothesis Testing</h2><span id='topic+kStepMAlgorithm'></span>

<h3>Description</h3>

<p>This function implements the k-stepM algorithm for multiple hypothesis testing.
It tests each hypothesis using the critical value calculated from the ECDF
of the k-max differences, updating the critical value, and iterating until all hypotheses
are tested.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kStepMAlgorithm(original_stats, bootstrap_stats, num_hypotheses, alpha, k)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kStepMAlgorithm_+3A_original_stats">original_stats</code></td>
<td>
<p>A numeric vector of original test statistics for each hypothesis.</p>
</td></tr>
<tr><td><code id="kStepMAlgorithm_+3A_bootstrap_stats">bootstrap_stats</code></td>
<td>
<p>A numeric matrix of bootstrap test statistics, with rows representing
bootstrap samples and columns representing hypotheses.</p>
</td></tr>
<tr><td><code id="kStepMAlgorithm_+3A_num_hypotheses">num_hypotheses</code></td>
<td>
<p>An integer specifying the total number of hypotheses.</p>
</td></tr>
<tr><td><code id="kStepMAlgorithm_+3A_alpha">alpha</code></td>
<td>
<p>A numeric value specifying the significance level.</p>
</td></tr>
<tr><td><code id="kStepMAlgorithm_+3A_k">k</code></td>
<td>
<p>An integer specifying the threshold number for controlling the k-familywise error rate.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing two elements: 'signif', a logical vector indicating which hypotheses
are rejected, and 'cv', a numeric vector of critical values used for each hypothesis.
</p>


<h3>References</h3>

<p>Romano, Joseph P., Azeem M. Shaikh, and Michael Wolf. &quot;Formalized data snooping
based on generalized error rates.&quot; Econometric Theory 24.2 (2008): 404-447.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>original_stats &lt;- rnorm(10)
bootstrap_stats &lt;- matrix(rnorm(1000), ncol = 10)
result &lt;- kStepMAlgorithm(original_stats, bootstrap_stats, 10, 0.05, 1)
</code></pre>

<hr>
<h2 id='plot.HessMLP'>Plot method for the HessMLP Class</h2><span id='topic+plot.HessMLP'></span>

<h3>Description</h3>

<p>Plot the sensitivities and sensitivity metrics of a <code>HessMLP</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'HessMLP'
plot(
  x,
  plotType = c("sensitivities", "time", "features", "matrix", "interactions"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.HessMLP_+3A_x">x</code></td>
<td>
<p><code>HessMLP</code> object created by <code><a href="#topic+HessianMLP">HessianMLP</a></code></p>
</td></tr>
<tr><td><code id="plot.HessMLP_+3A_plottype">plotType</code></td>
<td>
<p><code>character</code> specifying which type of plot should be created. It can be:
</p>

<ul>
<li><p> &quot;sensitivities&quot; (default): use <code><a href="#topic+HessianMLP">HessianMLP</a></code> function
</p>
</li>
<li><p> &quot;time&quot;: use <code><a href="#topic+SensTimePlot">SensTimePlot</a></code> function
</p>
</li>
<li><p> &quot;features&quot;: use  <code><a href="#topic+HessFeaturePlot">HessFeaturePlot</a></code> function
</p>
</li>
<li><p> &quot;matrix&quot;: use <code><a href="#topic+SensMatPlot">SensMatPlot</a></code> function to show the values
of second partial derivatives
</p>
</li>
<li><p> &quot;interactions&quot;: use <code><a href="#topic+SensMatPlot">SensMatPlot</a></code> function to show the
values of second partial derivatives and the first partial derivatives in the diagonal
</p>
</li></ul>
</td></tr>
<tr><td><code id="plot.HessMLP_+3A_...">...</code></td>
<td>
<p>additional parameters passed to plot function of the <code>NeuralSens</code> package</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of graphic objects created by <code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#' ## Load data -------------------------------------------------------------------
data("DAILY_DEMAND_TR")
fdata &lt;- DAILY_DEMAND_TR

## Parameters of the NNET ------------------------------------------------------
hidden_neurons &lt;- 5
iters &lt;- 250
decay &lt;- 0.1

################################################################################
#########################  REGRESSION NNET #####################################
################################################################################
## Regression dataframe --------------------------------------------------------
# Scale the data
fdata.Reg.tr &lt;- fdata[,2:ncol(fdata)]
fdata.Reg.tr[,3] &lt;- fdata.Reg.tr[,3]/10
fdata.Reg.tr[,1] &lt;- fdata.Reg.tr[,1]/1000

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.tr, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.tr)

#' ## TRAIN nnet NNET --------------------------------------------------------
# Create a formula to train NNET
form &lt;- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = " + ")
form &lt;- formula(paste(names(fdata.Reg.tr)[1], form, sep = " ~ "))

set.seed(150)
nnetmod &lt;- nnet::nnet(form,
                           data = nntrData,
                           linear.output = TRUE,
                           size = hidden_neurons,
                           decay = decay,
                           maxit = iters)
# Try HessianMLP
sens &lt;- NeuralSens::HessianMLP(nnetmod, trData = nntrData, plot = FALSE)

plot(sens)
plot(sens,"time")

</code></pre>

<hr>
<h2 id='plot.SensMLP'>Plot method for the SensMLP Class</h2><span id='topic+plot.SensMLP'></span>

<h3>Description</h3>

<p>Plot the sensitivities and sensitivity metrics of a <code>SensMLP</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SensMLP'
plot(x, plotType = c("sensitivities", "time", "features"), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.SensMLP_+3A_x">x</code></td>
<td>
<p><code>SensMLP</code> object created by <code><a href="#topic+SensAnalysisMLP">SensAnalysisMLP</a></code></p>
</td></tr>
<tr><td><code id="plot.SensMLP_+3A_plottype">plotType</code></td>
<td>
<p><code>character</code> specifying which type of plot should be created. It can be:
</p>

<ul>
<li><p> &quot;sensitivities&quot; (default): use <code><a href="#topic+SensAnalysisMLP">SensAnalysisMLP</a></code> function
</p>
</li>
<li><p> &quot;time&quot;: use <code><a href="#topic+SensTimePlot">SensTimePlot</a></code> function
</p>
</li>
<li><p> &quot;features&quot;: use  <code><a href="#topic+SensFeaturePlot">SensFeaturePlot</a></code> function
</p>
</li></ul>
</td></tr>
<tr><td><code id="plot.SensMLP_+3A_...">...</code></td>
<td>
<p>additional parameters passed to plot function of the <code>NeuralSens</code> package</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of graphic objects created by <code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>
</p>


<h3>References</h3>

<p>Pizarroso J, Portela J, Muñoz A (2022). NeuralSens: Sensitivity Analysis of
Neural Networks. Journal of Statistical Software, 102(7), 1-36.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#' ## Load data -------------------------------------------------------------------
data("DAILY_DEMAND_TR")
fdata &lt;- DAILY_DEMAND_TR

## Parameters of the NNET ------------------------------------------------------
hidden_neurons &lt;- 5
iters &lt;- 250
decay &lt;- 0.1

################################################################################
#########################  REGRESSION NNET #####################################
################################################################################
## Regression dataframe --------------------------------------------------------
# Scale the data
fdata.Reg.tr &lt;- fdata[,2:ncol(fdata)]
fdata.Reg.tr[,3] &lt;- fdata.Reg.tr[,3]/10
fdata.Reg.tr[,1] &lt;- fdata.Reg.tr[,1]/1000

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.tr, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.tr)

#' ## TRAIN nnet NNET --------------------------------------------------------
# Create a formula to train NNET
form &lt;- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = " + ")
form &lt;- formula(paste(names(fdata.Reg.tr)[1], form, sep = " ~ "))

set.seed(150)
nnetmod &lt;- nnet::nnet(form,
                           data = nntrData,
                           linear.output = TRUE,
                           size = hidden_neurons,
                           decay = decay,
                           maxit = iters)
# Try SensAnalysisMLP
sens &lt;- NeuralSens::SensAnalysisMLP(nnetmod, trData = nntrData, plot = FALSE)

plot(sens)
plot(sens,"time")
plot(sens,"features")

</code></pre>

<hr>
<h2 id='PlotSensMLP'>Neural network structure sensitivity plot</h2><span id='topic+PlotSensMLP'></span>

<h3>Description</h3>

<p>Plot a neural interpretation diagram colored by sensitivities
of the model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PlotSensMLP(
  MLP.fit,
  metric = "mean",
  sens_neg_col = "red",
  sens_pos_col = "blue",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="PlotSensMLP_+3A_mlp.fit">MLP.fit</code></td>
<td>
<p>fitted neural network model</p>
</td></tr>
<tr><td><code id="PlotSensMLP_+3A_metric">metric</code></td>
<td>
<p>metric to plot in the NID. It can be &quot;mean&quot; (default), &quot;median or &quot;sqmean&quot;.
It can be any metric to combine the raw sensitivities</p>
</td></tr>
<tr><td><code id="PlotSensMLP_+3A_sens_neg_col">sens_neg_col</code></td>
<td>
<p><code>character</code> string indicating color of negative sensitivity
measure, default 'red'. The same is passed to argument <code>neg_col</code> of
<a href="NeuralNetTools.html#topic+plotnet">plotnet</a></p>
</td></tr>
<tr><td><code id="PlotSensMLP_+3A_sens_pos_col">sens_pos_col</code></td>
<td>
<p><code>character</code> string indicating color of positive sensitivity
measure, default 'blue'. The same is passed to argument <code>pos_col</code> of
<a href="NeuralNetTools.html#topic+plotnet">plotnet</a></p>
</td></tr>
<tr><td><code id="PlotSensMLP_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <a href="NeuralNetTools.html#topic+plotnet">plotnet</a> and/or
<a href="#topic+SensAnalysisMLP">SensAnalysisMLP</a></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A graphics object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load data -------------------------------------------------------------------
data("DAILY_DEMAND_TR")
fdata &lt;- DAILY_DEMAND_TR
## Parameters of the NNET ------------------------------------------------------
hidden_neurons &lt;- 5
iters &lt;- 100
decay &lt;- 0.1

################################################################################
#########################  REGRESSION NNET #####################################
################################################################################
## Regression dataframe --------------------------------------------------------
# Scale the data
fdata.Reg.tr &lt;- fdata[,2:ncol(fdata)]
fdata.Reg.tr[,3] &lt;- fdata.Reg.tr[,3]/10
fdata.Reg.tr[,1] &lt;- fdata.Reg.tr[,1]/1000

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.tr, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.tr)

#' ## TRAIN nnet NNET --------------------------------------------------------
# Create a formula to train NNET
form &lt;- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = " + ")
form &lt;- formula(paste(names(fdata.Reg.tr)[1], form, sep = " ~ "))

set.seed(150)
nnetmod &lt;- nnet::nnet(form,
                      data = nntrData,
                      linear.output = TRUE,
                      size = hidden_neurons,
                      decay = decay,
                      maxit = iters)
# Try SensAnalysisMLP
NeuralSens::PlotSensMLP(nnetmod, trData = nntrData)
</code></pre>

<hr>
<h2 id='print.HessMLP'>Print method for the HessMLP Class</h2><span id='topic+print.HessMLP'></span>

<h3>Description</h3>

<p>Print the sensitivities of a <code>HessMLP</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'HessMLP'
print(x, n = 5, round_digits = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.HessMLP_+3A_x">x</code></td>
<td>
<p><code>HessMLP</code> object created by <code><a href="#topic+HessianMLP">HessianMLP</a></code></p>
</td></tr>
<tr><td><code id="print.HessMLP_+3A_n">n</code></td>
<td>
<p><code>integer</code> specifying number of sensitivities to print per each output</p>
</td></tr>
<tr><td><code id="print.HessMLP_+3A_round_digits">round_digits</code></td>
<td>
<p><code>integer</code> number of decimal places, default <code>NULL</code></p>
</td></tr>
<tr><td><code id="print.HessMLP_+3A_...">...</code></td>
<td>
<p>additional parameters</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Load data -------------------------------------------------------------------
data("DAILY_DEMAND_TR")
fdata &lt;- DAILY_DEMAND_TR

## Parameters of the NNET ------------------------------------------------------
hidden_neurons &lt;- 5
iters &lt;- 250
decay &lt;- 0.1

################################################################################
#########################  REGRESSION NNET #####################################
################################################################################
## Regression dataframe --------------------------------------------------------
# Scale the data
fdata.Reg.tr &lt;- fdata[,2:ncol(fdata)]
fdata.Reg.tr[,3] &lt;- fdata.Reg.tr[,3]/10
fdata.Reg.tr[,1] &lt;- fdata.Reg.tr[,1]/1000

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.tr, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.tr)

#' ## TRAIN nnet NNET --------------------------------------------------------
# Create a formula to train NNET
form &lt;- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = " + ")
form &lt;- formula(paste(names(fdata.Reg.tr)[1], form, sep = " ~ "))

set.seed(150)
nnetmod &lt;- nnet::nnet(form,
                           data = nntrData,
                           linear.output = TRUE,
                           size = hidden_neurons,
                           decay = decay,
                           maxit = iters)
# Try HessianMLP
sens &lt;- NeuralSens::HessianMLP(nnetmod, trData = nntrData, plot = FALSE)
sens
</code></pre>

<hr>
<h2 id='print.SensMLP'>Print method for the SensMLP Class</h2><span id='topic+print.SensMLP'></span>

<h3>Description</h3>

<p>Print the sensitivities of a <code>SensMLP</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SensMLP'
print(x, n = 5, round_digits = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.SensMLP_+3A_x">x</code></td>
<td>
<p><code>SensMLP</code> object created by <code><a href="#topic+SensAnalysisMLP">SensAnalysisMLP</a></code></p>
</td></tr>
<tr><td><code id="print.SensMLP_+3A_n">n</code></td>
<td>
<p><code>integer</code> specifying number of sensitivities to print per each output</p>
</td></tr>
<tr><td><code id="print.SensMLP_+3A_round_digits">round_digits</code></td>
<td>
<p><code>integer</code> number of decimal places, default <code>NULL</code></p>
</td></tr>
<tr><td><code id="print.SensMLP_+3A_...">...</code></td>
<td>
<p>additional parameters</p>
</td></tr>
</table>


<h3>References</h3>

<p>Pizarroso J, Portela J, Muñoz A (2022). NeuralSens: Sensitivity Analysis of
Neural Networks. Journal of Statistical Software, 102(7), 1-36.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load data -------------------------------------------------------------------
data("DAILY_DEMAND_TR")
fdata &lt;- DAILY_DEMAND_TR

## Parameters of the NNET ------------------------------------------------------
hidden_neurons &lt;- 5
iters &lt;- 250
decay &lt;- 0.1

################################################################################
#########################  REGRESSION NNET #####################################
################################################################################
## Regression dataframe --------------------------------------------------------
# Scale the data
fdata.Reg.tr &lt;- fdata[,2:ncol(fdata)]
fdata.Reg.tr[,3] &lt;- fdata.Reg.tr[,3]/10
fdata.Reg.tr[,1] &lt;- fdata.Reg.tr[,1]/1000

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.tr, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.tr)

#' ## TRAIN nnet NNET --------------------------------------------------------
# Create a formula to train NNET
form &lt;- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = " + ")
form &lt;- formula(paste(names(fdata.Reg.tr)[1], form, sep = " ~ "))

set.seed(150)
nnetmod &lt;- nnet::nnet(form,
                           data = nntrData,
                           linear.output = TRUE,
                           size = hidden_neurons,
                           decay = decay,
                           maxit = iters)
# Try SensAnalysisMLP
sens &lt;- NeuralSens::SensAnalysisMLP(nnetmod, trData = nntrData, plot = FALSE)
sens
</code></pre>

<hr>
<h2 id='print.summary.HessMLP'>Print method of the summary HessMLP Class</h2><span id='topic+print.summary.HessMLP'></span>

<h3>Description</h3>

<p>Print the sensitivity metrics of a <code>HessMLP</code> object.
This metrics are the mean sensitivity, the standard deviation
of sensitivities and the mean of sensitivities square
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'summary.HessMLP'
print(x, round_digits = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.summary.HessMLP_+3A_x">x</code></td>
<td>
<p><code>summary.HessMLP</code> object created by summary method of <code>HessMLP</code> object</p>
</td></tr>
<tr><td><code id="print.summary.HessMLP_+3A_round_digits">round_digits</code></td>
<td>
<p><code>integer</code> number of decimal places, default <code>NULL</code></p>
</td></tr>
<tr><td><code id="print.summary.HessMLP_+3A_...">...</code></td>
<td>
<p>additional parameters</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Load data -------------------------------------------------------------------
data("DAILY_DEMAND_TR")
fdata &lt;- DAILY_DEMAND_TR

## Parameters of the NNET ------------------------------------------------------
hidden_neurons &lt;- 5
iters &lt;- 250
decay &lt;- 0.1

################################################################################
#########################  REGRESSION NNET #####################################
################################################################################
## Regression dataframe --------------------------------------------------------
# Scale the data
fdata.Reg.tr &lt;- fdata[,2:ncol(fdata)]
fdata.Reg.tr[,3] &lt;- fdata.Reg.tr[,3]/10
fdata.Reg.tr[,1] &lt;- fdata.Reg.tr[,1]/1000

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.tr, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.tr)

#' ## TRAIN nnet NNET --------------------------------------------------------
# Create a formula to train NNET
form &lt;- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = " + ")
form &lt;- formula(paste(names(fdata.Reg.tr)[1], form, sep = " ~ "))

set.seed(150)
nnetmod &lt;- nnet::nnet(form,
                           data = nntrData,
                           linear.output = TRUE,
                           size = hidden_neurons,
                           decay = decay,
                           maxit = iters)
# Try HessianMLP
sens &lt;- NeuralSens::HessianMLP(nnetmod, trData = nntrData, plot = FALSE)
print(summary(sens))
</code></pre>

<hr>
<h2 id='print.summary.SensMLP'>Print method of the summary SensMLP Class</h2><span id='topic+print.summary.SensMLP'></span>

<h3>Description</h3>

<p>Print the sensitivity metrics of a <code>SensMLP</code> object.
This metrics are the mean sensitivity, the standard deviation
of sensitivities and the mean of sensitivities square
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'summary.SensMLP'
print(x, round_digits = NULL, boot.alpha = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.summary.SensMLP_+3A_x">x</code></td>
<td>
<p><code>summary.SensMLP</code> object created by summary method of <code>SensMLP</code> object</p>
</td></tr>
<tr><td><code id="print.summary.SensMLP_+3A_round_digits">round_digits</code></td>
<td>
<p><code>integer</code> number of decimal places, default <code>NULL</code></p>
</td></tr>
<tr><td><code id="print.summary.SensMLP_+3A_boot.alpha">boot.alpha</code></td>
<td>
<p><code>float</code> significance level to show statistical metrics. If <code>NULL</code>,
boot.alpha inherits from <code>x</code> is used. Defaults to <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="print.summary.SensMLP_+3A_...">...</code></td>
<td>
<p>additional parameters</p>
</td></tr>
</table>


<h3>References</h3>

<p>Pizarroso J, Portela J, Muñoz A (2022). NeuralSens: Sensitivity Analysis of
Neural Networks. Journal of Statistical Software, 102(7), 1-36.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load data -------------------------------------------------------------------
data("DAILY_DEMAND_TR")
fdata &lt;- DAILY_DEMAND_TR

## Parameters of the NNET ------------------------------------------------------
hidden_neurons &lt;- 5
iters &lt;- 250
decay &lt;- 0.1

################################################################################
#########################  REGRESSION NNET #####################################
################################################################################
## Regression dataframe --------------------------------------------------------
# Scale the data
fdata.Reg.tr &lt;- fdata[,2:ncol(fdata)]
fdata.Reg.tr[,3] &lt;- fdata.Reg.tr[,3]/10
fdata.Reg.tr[,1] &lt;- fdata.Reg.tr[,1]/1000

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.tr, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.tr)

#' ## TRAIN nnet NNET --------------------------------------------------------
# Create a formula to train NNET
form &lt;- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = " + ")
form &lt;- formula(paste(names(fdata.Reg.tr)[1], form, sep = " ~ "))

set.seed(150)
nnetmod &lt;- nnet::nnet(form,
                           data = nntrData,
                           linear.output = TRUE,
                           size = hidden_neurons,
                           decay = decay,
                           maxit = iters)
# Try SensAnalysisMLP
sens &lt;- NeuralSens::SensAnalysisMLP(nnetmod, trData = nntrData, plot = FALSE)
print(summary(sens))
</code></pre>

<hr>
<h2 id='SensAnalysisMLP'>Sensitivity of MLP models</h2><span id='topic+SensAnalysisMLP'></span><span id='topic+SensAnalysisMLP.default'></span><span id='topic+SensAnalysisMLP.train'></span><span id='topic+SensAnalysisMLP.H2OMultinomialModel'></span><span id='topic+SensAnalysisMLP.H2ORegressionModel'></span><span id='topic+SensAnalysisMLP.list'></span><span id='topic+SensAnalysisMLP.mlp'></span><span id='topic+SensAnalysisMLP.nn'></span><span id='topic+SensAnalysisMLP.nnet'></span><span id='topic+SensAnalysisMLP.nnetar'></span><span id='topic+SensAnalysisMLP.numeric'></span>

<h3>Description</h3>

<p>Function for evaluating the sensitivities of the inputs
variables in a mlp model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SensAnalysisMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  ...
)

## Default S3 method:
SensAnalysisMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  trData,
  actfunc = NULL,
  deractfunc = NULL,
  preProc = NULL,
  terms = NULL,
  output_name = NULL,
  ...
)

## S3 method for class 'train'
SensAnalysisMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  boot.R = NULL,
  boot.seed = 1,
  boot.alpha = 0.05,
  ...
)

## S3 method for class 'H2OMultinomialModel'
SensAnalysisMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  ...
)

## S3 method for class 'H2ORegressionModel'
SensAnalysisMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  ...
)

## S3 method for class 'list'
SensAnalysisMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  trData,
  actfunc,
  ...
)

## S3 method for class 'mlp'
SensAnalysisMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  trData,
  preProc = NULL,
  terms = NULL,
  ...
)

## S3 method for class 'nn'
SensAnalysisMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  preProc = NULL,
  terms = NULL,
  ...
)

## S3 method for class 'nnet'
SensAnalysisMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  trData,
  preProc = NULL,
  terms = NULL,
  ...
)

## S3 method for class 'nnetar'
SensAnalysisMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  ...
)

## S3 method for class 'numeric'
SensAnalysisMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  trData,
  actfunc = NULL,
  preProc = NULL,
  terms = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SensAnalysisMLP_+3A_mlp.fit">MLP.fit</code></td>
<td>
<p>fitted neural network model</p>
</td></tr>
<tr><td><code id="SensAnalysisMLP_+3A_.returnsens">.returnSens</code></td>
<td>
<p>DEPRECATED</p>
</td></tr>
<tr><td><code id="SensAnalysisMLP_+3A_plot">plot</code></td>
<td>
<p><code>logical</code> whether or not to plot the analysis. By default is
<code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="SensAnalysisMLP_+3A_.rawsens">.rawSens</code></td>
<td>
<p>DEPRECATED</p>
</td></tr>
<tr><td><code id="SensAnalysisMLP_+3A_sens_origin_layer">sens_origin_layer</code></td>
<td>
<p><code>numeric</code> specifies the layer of neurons with
respect to which the derivative must be calculated. The input layer is
specified by 1 (default).</p>
</td></tr>
<tr><td><code id="SensAnalysisMLP_+3A_sens_end_layer">sens_end_layer</code></td>
<td>
<p><code>numeric</code> specifies the layer of neurons of which
the derivative is calculated. It may also be 'last' to specify the output
layer (default).</p>
</td></tr>
<tr><td><code id="SensAnalysisMLP_+3A_sens_origin_input">sens_origin_input</code></td>
<td>
<p><code>logical</code> specifies if the derivative must be
calculated with respect to the inputs (<code>TRUE</code>) or output
(<code>FALSE</code>) of the <code>sens_origin_layer</code> layer of the model. By
default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="SensAnalysisMLP_+3A_sens_end_input">sens_end_input</code></td>
<td>
<p><code>logical</code> specifies if the derivative calculated
is of the output (<code>FALSE</code>) or from the input (<code>TRUE</code>) of the
<code>sens_end_layer</code> layer of the model. By default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="SensAnalysisMLP_+3A_...">...</code></td>
<td>
<p>additional arguments passed to or from other methods</p>
</td></tr>
<tr><td><code id="SensAnalysisMLP_+3A_trdata">trData</code></td>
<td>
<p><code>data.frame</code> containing the data to evaluate the sensitivity of the model</p>
</td></tr>
<tr><td><code id="SensAnalysisMLP_+3A_actfunc">actfunc</code></td>
<td>
<p><code>character</code> vector indicating the activation function of each
neurons layer.</p>
</td></tr>
<tr><td><code id="SensAnalysisMLP_+3A_deractfunc">deractfunc</code></td>
<td>
<p><code>character</code> vector indicating the derivative of the activation
function of each neurons layer.</p>
</td></tr>
<tr><td><code id="SensAnalysisMLP_+3A_preproc">preProc</code></td>
<td>
<p>preProcess structure applied to the training data. See also
<code><a href="caret.html#topic+preProcess">preProcess</a></code></p>
</td></tr>
<tr><td><code id="SensAnalysisMLP_+3A_terms">terms</code></td>
<td>
<p>function applied to the training data to create factors. See
also <code><a href="caret.html#topic+train">train</a></code></p>
</td></tr>
<tr><td><code id="SensAnalysisMLP_+3A_output_name">output_name</code></td>
<td>
<p><code>character</code> name of the output variable in order to
avoid changing the name of the output variable in <code>trData</code> to
'.outcome'</p>
</td></tr>
<tr><td><code id="SensAnalysisMLP_+3A_boot.r">boot.R</code></td>
<td>
<p><code>int</code> Number of bootstrap samples to calculate. Used
to detect significant inputs and significant non-linearities. Only
available for <code>train</code> objects. Defaults to <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="SensAnalysisMLP_+3A_boot.seed">boot.seed</code></td>
<td>
<p><code>int</code> Seed of bootstrap evaluations.</p>
</td></tr>
<tr><td><code id="SensAnalysisMLP_+3A_boot.alpha">boot.alpha</code></td>
<td>
<p><code>float</code> Significance level of statistical test.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In case of using an input of class <code>factor</code> and a package which
need to enter the input data as matrix, the dummies must be created before
training the neural network.
</p>
<p>After that, the training data must be given to the function using the
<code>trData</code> argument.
</p>


<h3>Value</h3>

<p><code>SensMLP</code> object with the sensitivity metrics and sensitivities of
the MLP model passed to the function.
</p>


<h3>Plots</h3>

 <ul>
<li><p> Plot 1: colorful plot with the classification
of the classes in a 2D map </p>
</li>
<li><p> Plot 2: b/w plot with probability of the
chosen class in a 2D map </p>
</li>
<li><p> Plot 3: plot with the stats::predictions of
the data provided </p>
</li></ul>



<h3>References</h3>

<p>Pizarroso J, Portela J, Muñoz A (2022). NeuralSens: Sensitivity Analysis of
Neural Networks. Journal of Statistical Software, 102(7), 1-36.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load data -------------------------------------------------------------------
data("DAILY_DEMAND_TR")
fdata &lt;- DAILY_DEMAND_TR
## Parameters of the NNET ------------------------------------------------------
hidden_neurons &lt;- 5
iters &lt;- 100
decay &lt;- 0.1

################################################################################
#########################  REGRESSION NNET #####################################
################################################################################
## Regression dataframe --------------------------------------------------------
# Scale the data
fdata.Reg.tr &lt;- fdata[,2:ncol(fdata)]
fdata.Reg.tr[,3] &lt;- fdata.Reg.tr[,3]/10
fdata.Reg.tr[,1] &lt;- fdata.Reg.tr[,1]/1000

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.tr, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.tr)

#' ## TRAIN nnet NNET --------------------------------------------------------
# Create a formula to train NNET
form &lt;- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = " + ")
form &lt;- formula(paste(names(fdata.Reg.tr)[1], form, sep = " ~ "))

set.seed(150)
nnetmod &lt;- nnet::nnet(form,
                      data = nntrData,
                      linear.output = TRUE,
                      size = hidden_neurons,
                      decay = decay,
                      maxit = iters)
# Try SensAnalysisMLP
NeuralSens::SensAnalysisMLP(nnetmod, trData = nntrData)

# Try SensAnalysisMLP to calculate sensitivities with respect to output of hidden neurones
NeuralSens::SensAnalysisMLP(nnetmod, trData = nntrData,
                             sens_origin_layer = 2,
                             sens_end_layer = "last",
                             sens_origin_input = FALSE,
                             sens_end_input = FALSE)
## Train caret NNET ------------------------------------------------------------
# Create trainControl
ctrl_tune &lt;- caret::trainControl(method = "boot",
                                 savePredictions = FALSE,
                                 summaryFunction = caret::defaultSummary)
set.seed(150) #For replication
caretmod &lt;- caret::train(form = DEM~.,
                              data = fdata.Reg.tr,
                              method = "nnet",
                              linout = TRUE,
                              tuneGrid = data.frame(size = 3,
                                                    decay = decay),
                              maxit = iters,
                              preProcess = c("center","scale"),
                              trControl = ctrl_tune,
                              metric = "RMSE")

# Try SensAnalysisMLP
NeuralSens::SensAnalysisMLP(caretmod)

## Train h2o NNET --------------------------------------------------------------
# Create a cluster with 4 available cores
h2o::h2o.init(ip = "localhost",
              nthreads = 4)

# Reset the cluster
h2o::h2o.removeAll()
fdata_h2o &lt;- h2o::as.h2o(x = fdata.Reg.tr, destination_frame = "fdata_h2o")

set.seed(150)
h2omod &lt;-h2o:: h2o.deeplearning(x = names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)],
                                     y = names(fdata.Reg.tr)[1],
                                     distribution = "AUTO",
                                     training_frame = fdata_h2o,
                                     standardize = TRUE,
                                     activation = "Tanh",
                                     hidden = c(hidden_neurons),
                                     stopping_rounds = 0,
                                     epochs = iters,
                                     seed = 150,
                                     model_id = "nnet_h2o",
                                     adaptive_rate = FALSE,
                                     rate_decay = decay,
                                     export_weights_and_biases = TRUE)

# Try SensAnalysisMLP
NeuralSens::SensAnalysisMLP(h2omod)

# Turn off the cluster
h2o::h2o.shutdown(prompt = FALSE)
rm(fdata_h2o)

## Train RSNNS NNET ------------------------------------------------------------
# Normalize data using RSNNS algorithms
trData &lt;- as.data.frame(RSNNS::normalizeData(fdata.Reg.tr))
names(trData) &lt;- names(fdata.Reg.tr)
set.seed(150)
RSNNSmod &lt;-RSNNS::mlp(x = trData[,2:ncol(trData)],
                           y = trData[,1],
                           size = hidden_neurons,
                           linOut = TRUE,
                           learnFuncParams=c(decay),
                           maxit=iters)

# Try SensAnalysisMLP
NeuralSens::SensAnalysisMLP(RSNNSmod, trData = trData, output_name = "DEM")

## USE DEFAULT METHOD ----------------------------------------------------------
NeuralSens::SensAnalysisMLP(caretmod$finalModel$wts,
                            trData = fdata.Reg.tr,
                            mlpstr = caretmod$finalModel$n,
                            coefnames = caretmod$coefnames,
                            actfun = c("linear","sigmoid","linear"),
                            output_name = "DEM")

################################################################################
#########################  CLASSIFICATION NNET #################################
################################################################################
## Regression dataframe --------------------------------------------------------
# Scale the data
fdata.Reg.cl &lt;- fdata[,2:ncol(fdata)]
fdata.Reg.cl[,2:3] &lt;- fdata.Reg.cl[,2:3]/10
fdata.Reg.cl[,1] &lt;- fdata.Reg.cl[,1]/1000

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.cl, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.cl)

# Factorize the output
fdata.Reg.cl$DEM &lt;- factor(round(fdata.Reg.cl$DEM, digits = 1))

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.cl, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.cl)

## Train caret NNET ------------------------------------------------------------
# Create trainControl
ctrl_tune &lt;- caret::trainControl(method = "boot",
                                 savePredictions = FALSE,
                                 summaryFunction = caret::defaultSummary)
set.seed(150) #For replication
caretmod &lt;- caret::train(form = DEM~.,
                                data = fdata.Reg.cl,
                                method = "nnet",
                                linout = FALSE,
                                tuneGrid = data.frame(size = hidden_neurons,
                                                      decay = decay),
                                maxit = iters,
                                preProcess = c("center","scale"),
                                trControl = ctrl_tune,
                                metric = "Accuracy")

# Try SensAnalysisMLP
NeuralSens::SensAnalysisMLP(caretmod)

## Train h2o NNET --------------------------------------------------------------
# Create local cluster with 4 available cores
h2o::h2o.init(ip = "localhost",
              nthreads = 4)

# Reset the cluster
h2o::h2o.removeAll()
fdata_h2o &lt;- h2o::as.h2o(x = fdata.Reg.cl, destination_frame = "fdata_h2o")

set.seed(150)
h2omod &lt;- h2o::h2o.deeplearning(x = names(fdata.Reg.cl)[2:ncol(fdata.Reg.cl)],
                                       y = names(fdata.Reg.cl)[1],
                                       distribution = "AUTO",
                                       training_frame = fdata_h2o,
                                       standardize = TRUE,
                                       activation = "Tanh",
                                       hidden = c(hidden_neurons),
                                       stopping_rounds = 0,
                                       epochs = iters,
                                       seed = 150,
                                       model_id = "nnet_h2o",
                                       adaptive_rate = FALSE,
                                       rate_decay = decay,
                                       export_weights_and_biases = TRUE)

# Try SensAnalysisMLP
NeuralSens::SensAnalysisMLP(h2omod)

# Apaga el cluster
h2o::h2o.shutdown(prompt = FALSE)
rm(fdata_h2o)

## TRAIN nnet NNET --------------------------------------------------------
# Create a formula to train NNET
form &lt;- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = " + ")
form &lt;- formula(paste(names(fdata.Reg.tr)[1], form, sep = " ~ "))

set.seed(150)
nnetmod &lt;- nnet::nnet(form,
                      data = nntrData,
                      linear.output = TRUE,
                      size = hidden_neurons,
                      decay = decay,
                      maxit = iters)
# Try SensAnalysisMLP
NeuralSens::SensAnalysisMLP(nnetmod, trData = nntrData)

</code></pre>

<hr>
<h2 id='SensDotPlot'>Sensitivity scatter plot against input values</h2><span id='topic+SensDotPlot'></span>

<h3>Description</h3>

<p>Plot of sensitivities of the neural network output respect
to the inputs
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SensDotPlot(
  object,
  fdata = NULL,
  input_vars = "all",
  output_vars = "all",
  smooth = FALSE,
  nspline = NULL,
  color = NULL,
  grid = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SensDotPlot_+3A_object">object</code></td>
<td>
<p>fitted neural network model or <code>array</code> containing the raw
sensitivities from the function <code><a href="#topic+SensAnalysisMLP">SensAnalysisMLP</a></code></p>
</td></tr>
<tr><td><code id="SensDotPlot_+3A_fdata">fdata</code></td>
<td>
<p><code>data.frame</code> containing the data to evaluate the sensitivity of the model.</p>
</td></tr>
<tr><td><code id="SensDotPlot_+3A_input_vars">input_vars</code></td>
<td>
<p><code>character vector</code> with the variables to create the scatter plot. If <code>"all"</code>,
then scatter plots are created for all the input variables in <code>fdata</code>.</p>
</td></tr>
<tr><td><code id="SensDotPlot_+3A_output_vars">output_vars</code></td>
<td>
<p><code>character vector</code> with the variables to create the scatter plot. If <code>"all"</code>,
then scatter plots are created for all the output variables in <code>fdata</code>.</p>
</td></tr>
<tr><td><code id="SensDotPlot_+3A_smooth">smooth</code></td>
<td>
<p><code>logical</code> if <code>TRUE</code>, <code>geom_smooth</code> plots are added to each variable plot</p>
</td></tr>
<tr><td><code id="SensDotPlot_+3A_nspline">nspline</code></td>
<td>
<p><code>integer</code> if <code>smooth</code> is TRUE, this determine the degree of the spline used
to perform <code>geom_smooth</code>. If <code>nspline</code> is NULL, the square root of the length of the data
is used as degrees of the spline.</p>
</td></tr>
<tr><td><code id="SensDotPlot_+3A_color">color</code></td>
<td>
<p><code>character</code> specifying the name of a <code>numeric</code> variable of <code>fdata</code> to color the scatter plot.</p>
</td></tr>
<tr><td><code id="SensDotPlot_+3A_grid">grid</code></td>
<td>
<p><code>logical</code>. If <code>TRUE</code>, plots created are show together using <code><a href="gridExtra.html#topic+arrangeGrob">arrangeGrob</a></code></p>
</td></tr>
<tr><td><code id="SensDotPlot_+3A_...">...</code></td>
<td>
<p>further arguments that should be passed to  <code><a href="#topic+SensAnalysisMLP">SensAnalysisMLP</a></code> function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of <code>geom_point</code> plots for the inputs variables representing the
sensitivity of each output respect to the inputs
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load data -------------------------------------------------------------------
data("DAILY_DEMAND_TR")
fdata &lt;- DAILY_DEMAND_TR
## Parameters of the NNET ------------------------------------------------------
hidden_neurons &lt;- 5
iters &lt;- 250
decay &lt;- 0.1

################################################################################
#########################  REGRESSION NNET #####################################
################################################################################
## Regression dataframe --------------------------------------------------------
# Scale the data
fdata.Reg.tr &lt;- fdata[,2:ncol(fdata)]
fdata.Reg.tr[,3] &lt;- fdata.Reg.tr[,3]/10
fdata.Reg.tr[,1] &lt;- fdata.Reg.tr[,1]/1000

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.tr, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.tr)

#' ## TRAIN nnet NNET --------------------------------------------------------
# Create a formula to train NNET
form &lt;- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = " + ")
form &lt;- formula(paste(names(fdata.Reg.tr)[1], form, sep = " ~ "))

set.seed(150)
nnetmod &lt;- nnet::nnet(form,
                      data = nntrData,
                      linear.output = TRUE,
                      size = hidden_neurons,
                      decay = decay,
                      maxit = iters)
# Try SensDotPlot
NeuralSens::SensDotPlot(nnetmod, fdata = nntrData)
</code></pre>

<hr>
<h2 id='SensFeaturePlot'>Feature sensitivity plot</h2><span id='topic+SensFeaturePlot'></span>

<h3>Description</h3>

<p>Show the distribution of the sensitivities of the output
in <code>geom_sina()</code> plot which color depends on the input values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SensFeaturePlot(object, fdata = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SensFeaturePlot_+3A_object">object</code></td>
<td>
<p>fitted neural network model or <code>array</code> containing the raw
sensitivities from the function <code><a href="#topic+SensAnalysisMLP">SensAnalysisMLP</a></code></p>
</td></tr>
<tr><td><code id="SensFeaturePlot_+3A_fdata">fdata</code></td>
<td>
<p><code>data.frame</code> containing the data to evaluate the sensitivity of the model.
Not needed if the raw sensitivities has been passed as <code>object</code></p>
</td></tr>
<tr><td><code id="SensFeaturePlot_+3A_...">...</code></td>
<td>
<p>further arguments that should be passed to  <code><a href="#topic+SensAnalysisMLP">SensAnalysisMLP</a></code> function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of Feature sensitivity plot as described in
<a href="https://www.r-bloggers.com/2019/03/a-gentle-introduction-to-shap-values-in-r/">https://www.r-bloggers.com/2019/03/a-gentle-introduction-to-shap-values-in-r/</a>
</p>


<h3>References</h3>

<p>Pizarroso J, Portela J, Muñoz A (2022). NeuralSens: Sensitivity Analysis of
Neural Networks. Journal of Statistical Software, 102(7), 1-36.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load data -------------------------------------------------------------------
data("DAILY_DEMAND_TR")
fdata &lt;- DAILY_DEMAND_TR

## Parameters of the NNET ------------------------------------------------------
hidden_neurons &lt;- 5
iters &lt;- 250
decay &lt;- 0.1

################################################################################
#########################  REGRESSION NNET #####################################
################################################################################
## Regression dataframe --------------------------------------------------------
# Scale the data
fdata.Reg.tr &lt;- fdata[,2:ncol(fdata)]
fdata.Reg.tr[,3] &lt;- fdata.Reg.tr[,3]/10
fdata.Reg.tr[,1] &lt;- fdata.Reg.tr[,1]/1000

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.tr, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.tr)

#' ## TRAIN nnet NNET --------------------------------------------------------
# Create a formula to train NNET
form &lt;- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = " + ")
form &lt;- formula(paste(names(fdata.Reg.tr)[1], form, sep = " ~ "))

set.seed(150)
nnetmod &lt;- nnet::nnet(form,
                           data = nntrData,
                           linear.output = TRUE,
                           size = hidden_neurons,
                           decay = decay,
                           maxit = iters)
# Try SensAnalysisMLP
sens &lt;- NeuralSens::SensAnalysisMLP(nnetmod, trData = nntrData, plot = FALSE)
NeuralSens::SensFeaturePlot(sens)
</code></pre>

<hr>
<h2 id='SensitivityPlots'>Plot sensitivities of a neural network model</h2><span id='topic+SensitivityPlots'></span>

<h3>Description</h3>

<p>Function to plot the sensitivities created by <code><a href="#topic+SensAnalysisMLP">SensAnalysisMLP</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SensitivityPlots(
  sens = NULL,
  der = TRUE,
  zoom = TRUE,
  quit.legend = FALSE,
  output = 1,
  plot_type = NULL,
  inp_var = NULL,
  title = "Sensitivity Plots",
  dodge_var = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SensitivityPlots_+3A_sens">sens</code></td>
<td>
<p><code>SensAnalysisMLP</code> object created by <code><a href="#topic+SensAnalysisMLP">SensAnalysisMLP</a></code> or <code>HessMLP</code> object
created by <code><a href="#topic+HessianMLP">HessianMLP</a></code>.</p>
</td></tr>
<tr><td><code id="SensitivityPlots_+3A_der">der</code></td>
<td>
<p><code>logical</code> indicating if density plots should be created. By default is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="SensitivityPlots_+3A_zoom">zoom</code></td>
<td>
<p><code>logical</code> indicating if the distributions should be zoomed when there is any of them which is too tiny to be appreciated in the third plot.
<code><a href="ggforce.html#topic+facet_zoom">facet_zoom</a></code> function from <code>ggforce</code> package is required.</p>
</td></tr>
<tr><td><code id="SensitivityPlots_+3A_quit.legend">quit.legend</code></td>
<td>
<p><code>logical</code> indicating if legend of the third plot should be removed. By default is <code>FALSE</code></p>
</td></tr>
<tr><td><code id="SensitivityPlots_+3A_output">output</code></td>
<td>
<p><code>numeric</code> or <code>character</code> specifying the output neuron or output name to be plotted.
By default is the first output (<code>output = 1</code>).</p>
</td></tr>
<tr><td><code id="SensitivityPlots_+3A_plot_type">plot_type</code></td>
<td>
<p><code>character</code> indicating which of the 3 plots to show. Useful when several variables are analyzed.
Acceptable values are 'mean_sd', 'square', 'raw' corresponding to first, second and third plot respectively. If <code>NULL</code>,
all plots are shown at the same time. By default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="SensitivityPlots_+3A_inp_var">inp_var</code></td>
<td>
<p><code>character</code> indicating which input variable to show in density plot. Only useful when
choosing plot_type='raw' to show the density plot of one input variable. If <code>NULL</code>, all variables
are plotted in density plot. By default is <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="SensitivityPlots_+3A_title">title</code></td>
<td>
<p><code>character</code> title of the sensitivity plots</p>
</td></tr>
<tr><td><code id="SensitivityPlots_+3A_dodge_var">dodge_var</code></td>
<td>
<p><code>bool</code> Flag to indicate that x ticks in meanSensSQ plot must dodge between them. Useful with
too long input names.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with the following plot for each output: </p>
 <ul>
<li><p> Plot 1: colorful plot with the
classification of the classes in a 2D map </p>
</li>
<li><p> Plot 2: b/w plot with
probability of the chosen class in a 2D map </p>
</li>
<li><p> Plot 3: plot with the
stats::predictions of the data provided if param <code>der</code> is <code>FALSE</code></p>
</li></ul>



<h3>References</h3>

<p>Pizarroso J, Portela J, Muñoz A (2022). NeuralSens: Sensitivity Analysis of
Neural Networks. Journal of Statistical Software, 102(7), 1-36.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load data -------------------------------------------------------------------
data("DAILY_DEMAND_TR")
fdata &lt;- DAILY_DEMAND_TR

## Parameters of the NNET ------------------------------------------------------
hidden_neurons &lt;- 5
iters &lt;- 250
decay &lt;- 0.1

################################################################################
#########################  REGRESSION NNET #####################################
################################################################################
## Regression dataframe --------------------------------------------------------
# Scale the data
fdata.Reg.tr &lt;- fdata[,2:ncol(fdata)]
fdata.Reg.tr[,3] &lt;- fdata.Reg.tr[,3]/10
fdata.Reg.tr[,1] &lt;- fdata.Reg.tr[,1]/1000

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.tr, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.tr)

#' ## TRAIN nnet NNET --------------------------------------------------------
# Create a formula to train NNET
form &lt;- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = " + ")
form &lt;- formula(paste(names(fdata.Reg.tr)[1], form, sep = " ~ "))

set.seed(150)
nnetmod &lt;- nnet::nnet(form,
                           data = nntrData,
                           linear.output = TRUE,
                           size = hidden_neurons,
                           decay = decay,
                           maxit = iters)
# Try SensAnalysisMLP
sens &lt;- NeuralSens::SensAnalysisMLP(nnetmod, trData = nntrData, plot = FALSE)
NeuralSens::SensitivityPlots(sens)
</code></pre>

<hr>
<h2 id='SensMatPlot'>Plot sensitivities of a neural network model</h2><span id='topic+SensMatPlot'></span>

<h3>Description</h3>

<p>Function to plot the sensitivities created by <code><a href="#topic+HessianMLP">HessianMLP</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SensMatPlot(
  hess,
  sens = NULL,
  output = 1,
  metric = c("mean", "std", "meanSensSQ"),
  senstype = c("matrix", "interactions"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SensMatPlot_+3A_hess">hess</code></td>
<td>
<p><code>HessMLP</code> object created by <code><a href="#topic+HessianMLP">HessianMLP</a></code>.</p>
</td></tr>
<tr><td><code id="SensMatPlot_+3A_sens">sens</code></td>
<td>
<p><code>SensMLP</code> object created by <code><a href="#topic+SensAnalysisMLP">SensAnalysisMLP</a></code>.</p>
</td></tr>
<tr><td><code id="SensMatPlot_+3A_output">output</code></td>
<td>
<p><code>numeric</code> or <code>character</code> specifying the output neuron or output name to be plotted.
By default is the first output (<code>output = 1</code>).</p>
</td></tr>
<tr><td><code id="SensMatPlot_+3A_metric">metric</code></td>
<td>
<p><code>character</code> specifying the metric to be plotted. It can be &quot;mean&quot;,
&quot;std&quot; or &quot;meanSensSQ&quot;.</p>
</td></tr>
<tr><td><code id="SensMatPlot_+3A_senstype">senstype</code></td>
<td>
<p><code>character</code> specifying the type of plot to be plotted. It can be &quot;matrix&quot; or
&quot;interactions&quot;. If type = &quot;matrix&quot;, only the second derivatives are plotted. If type = &quot;interactions&quot;
the main diagonal are the first derivatives respect each input variable.</p>
</td></tr>
<tr><td><code id="SensMatPlot_+3A_...">...</code></td>
<td>
<p>further argument passed similar to <code>ggcorrplot</code> arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Most of the code of this function is based on
<code>ggcorrplot()</code> function from package <code>ggcorrplot</code>. However, due to the
inhability of changing the limits of the color scale, it keeps giving a warning
if that function is used and the color scale overwritten.
</p>


<h3>Value</h3>

<p>a list of <code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>s, one for each output neuron.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load data -------------------------------------------------------------------
data("DAILY_DEMAND_TR")
fdata &lt;- DAILY_DEMAND_TR
## Parameters of the NNET ------------------------------------------------------
hidden_neurons &lt;- 5
iters &lt;- 100
decay &lt;- 0.1

################################################################################
#########################  REGRESSION NNET #####################################
################################################################################
## Regression dataframe --------------------------------------------------------
# Scale the data
fdata.Reg.tr &lt;- fdata[,2:ncol(fdata)]
fdata.Reg.tr[,3] &lt;- fdata.Reg.tr[,3]/10
fdata.Reg.tr[,1] &lt;- fdata.Reg.tr[,1]/1000

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.tr, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.tr)

#' ## TRAIN nnet NNET --------------------------------------------------------
# Create a formula to train NNET
form &lt;- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = " + ")
form &lt;- formula(paste(names(fdata.Reg.tr)[1], form, sep = " ~ "))

set.seed(150)
nnetmod &lt;- nnet::nnet(form,
                      data = nntrData,
                      linear.output = TRUE,
                      size = hidden_neurons,
                      decay = decay,
                      maxit = iters)
# Try HessianMLP
H &lt;- NeuralSens::HessianMLP(nnetmod, trData = nntrData, plot = FALSE)
NeuralSens::SensMatPlot(H)
S &lt;- NeuralSens::SensAnalysisMLP(nnetmod, trData = nntrData, plot = FALSE)
NeuralSens::SensMatPlot(H, S, senstype = "interactions")
</code></pre>

<hr>
<h2 id='SensMLP'>Constructor of the SensMLP Class</h2><span id='topic+SensMLP'></span>

<h3>Description</h3>

<p>Create an object of SensMLP class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SensMLP(
  sens = list(),
  raw_sens = list(),
  mlp_struct = numeric(),
  trData = data.frame(),
  coefnames = character(),
  output_name = character(),
  cv = NULL,
  boot = NULL,
  boot.alpha = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SensMLP_+3A_sens">sens</code></td>
<td>
<p><code>list</code> of sensitivity measures, one <code>data.frame</code> per output neuron</p>
</td></tr>
<tr><td><code id="SensMLP_+3A_raw_sens">raw_sens</code></td>
<td>
<p><code>list</code> of sensitivities, one <code>matrix</code> per output neuron</p>
</td></tr>
<tr><td><code id="SensMLP_+3A_mlp_struct">mlp_struct</code></td>
<td>
<p><code>numeric</code> vector describing the structur of the MLP model</p>
</td></tr>
<tr><td><code id="SensMLP_+3A_trdata">trData</code></td>
<td>
<p><code>data.frame</code> with the data used to calculate the sensitivities</p>
</td></tr>
<tr><td><code id="SensMLP_+3A_coefnames">coefnames</code></td>
<td>
<p><code>character</code> vector with the name of the predictor(s)</p>
</td></tr>
<tr><td><code id="SensMLP_+3A_output_name">output_name</code></td>
<td>
<p><code>character</code> vector with the name of the output(s)</p>
</td></tr>
<tr><td><code id="SensMLP_+3A_cv">cv</code></td>
<td>
<p><code>list</code> list with critical values of significance for std and mean square.</p>
</td></tr>
<tr><td><code id="SensMLP_+3A_boot">boot</code></td>
<td>
<p><code>array</code> bootstrapped sensitivity measures.</p>
</td></tr>
<tr><td><code id="SensMLP_+3A_boot.alpha">boot.alpha</code></td>
<td>
<p><code>array</code> significance level.
Defaults to <code>NULL</code>. Only available for analyzed <code>caret::train</code> models.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>SensMLP</code> object
</p>


<h3>References</h3>

<p>Pizarroso J, Portela J, Muñoz A (2022). NeuralSens: Sensitivity Analysis of
Neural Networks. Journal of Statistical Software, 102(7), 1-36.
</p>

<hr>
<h2 id='SensTimePlot'>Sensitivity analysis plot over time of the data</h2><span id='topic+SensTimePlot'></span>

<h3>Description</h3>

<p>Plot of sensitivity of the neural network output respect
to the inputs over the time variable from the data provided
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SensTimePlot(
  object,
  fdata = NULL,
  date.var = NULL,
  facet = FALSE,
  smooth = FALSE,
  nspline = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SensTimePlot_+3A_object">object</code></td>
<td>
<p>fitted neural network model or <code>array</code> containing the raw
sensitivities from the function <code><a href="#topic+SensAnalysisMLP">SensAnalysisMLP</a></code></p>
</td></tr>
<tr><td><code id="SensTimePlot_+3A_fdata">fdata</code></td>
<td>
<p><code>data.frame</code> containing the data to evaluate the sensitivity of the model.
Not needed if the raw sensitivities has been passed as <code>object</code></p>
</td></tr>
<tr><td><code id="SensTimePlot_+3A_date.var">date.var</code></td>
<td>
<p><code>Posixct vector</code> with the date of each sample of <code>fdata</code>
If <code>NULL</code>, the first variable with Posixct format of <code>fdata</code> is used as dates</p>
</td></tr>
<tr><td><code id="SensTimePlot_+3A_facet">facet</code></td>
<td>
<p><code>logical</code> if <code>TRUE</code>, function <code>facet_grid</code> from <code>ggplot2</code> is used</p>
</td></tr>
<tr><td><code id="SensTimePlot_+3A_smooth">smooth</code></td>
<td>
<p><code>logical</code> if <code>TRUE</code>, <code>geom_smooth</code> plots are added to each variable plot</p>
</td></tr>
<tr><td><code id="SensTimePlot_+3A_nspline">nspline</code></td>
<td>
<p><code>integer</code> if <code>smooth</code> is TRUE, this determine the degree of the spline used
to perform <code>geom_smooth</code>. If <code>nspline</code> is NULL, the square root of the length of the timeseries
is used as degrees of the spline.</p>
</td></tr>
<tr><td><code id="SensTimePlot_+3A_...">...</code></td>
<td>
<p>further arguments that should be passed to  <code><a href="#topic+SensAnalysisMLP">SensAnalysisMLP</a></code> function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list of <code>geom_line</code> plots for the inputs variables representing the
sensitivity of each output respect to the inputs over time
</p>


<h3>References</h3>

<p>Pizarroso J, Portela J, Muñoz A (2022). NeuralSens: Sensitivity Analysis of
Neural Networks. Journal of Statistical Software, 102(7), 1-36.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load data -------------------------------------------------------------------
data("DAILY_DEMAND_TR")
fdata &lt;- DAILY_DEMAND_TR
fdata[,3] &lt;- ifelse(as.data.frame(fdata)[,3] %in% c("SUN","SAT"), 0, 1)
## Parameters of the NNET ------------------------------------------------------
hidden_neurons &lt;- 5
iters &lt;- 250
decay &lt;- 0.1

################################################################################
#########################  REGRESSION NNET #####################################
################################################################################
## Regression dataframe --------------------------------------------------------
# Scale the data
fdata.Reg.tr &lt;- fdata[,2:ncol(fdata)]
fdata.Reg.tr[,3] &lt;- fdata.Reg.tr[,3]/10
fdata.Reg.tr[,1] &lt;- fdata.Reg.tr[,1]/1000

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.tr, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.tr)

#' ## TRAIN nnet NNET --------------------------------------------------------
# Create a formula to train NNET
form &lt;- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = " + ")
form &lt;- formula(paste(names(fdata.Reg.tr)[1], form, sep = " ~ "))

set.seed(150)
nnetmod &lt;- nnet::nnet(form,
                      data = nntrData,
                      linear.output = TRUE,
                      size = hidden_neurons,
                      decay = decay,
                      maxit = iters)
# Try SensTimePlot
NeuralSens::SensTimePlot(nnetmod, fdata = nntrData, date.var = NULL)
</code></pre>

<hr>
<h2 id='simdata'>Simulated data to test the package functionalities</h2><span id='topic+simdata'></span>

<h3>Description</h3>

<p><code>data.frame</code> with 2000 rows of 4 columns with 3
input variables <code>X1, X2, X3</code> and one output variable <code>Y</code>.
The data is already scaled, and has been generated using the following code:
</p>
<p><code>set.seed(150)</code>
</p>
<p><code>simdata &lt;- data.frame(</code>
<code>   "X1" = rnorm(2000, 0, 1),</code>
<code>   "X2" = rnorm(2000, 0, 1),</code>
<code>   "X3" = rnorm(2000, 0, 1)</code>
<code> )</code>
</p>
<p><code> simdata$Y &lt;- simdata$X1^2 + 0.5*simdata$X2 + 0.1*rnorm(2000, 0, 1)</code>
</p>


<h3>Format</h3>

<p>A data frame with 2000 rows and 4 variables:
</p>

<dl>
<dt>X1</dt><dd><p>Random input 1</p>
</dd>
<dt>X2</dt><dd><p>Random input 2</p>
</dd>
<dt>X3</dt><dd><p>Random input 3</p>
</dd>
<dt>Y</dt><dd><p>Output</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Jaime Pizarroso Gonzalo
</p>


<h3>References</h3>

<p>Pizarroso J, Portela J, Muñoz A (2022). NeuralSens: Sensitivity Analysis of
Neural Networks. Journal of Statistical Software, 102(7), 1-36.
</p>

<hr>
<h2 id='summary.HessMLP'>Summary Method for the HessMLP Class</h2><span id='topic+summary.HessMLP'></span>

<h3>Description</h3>

<p>Print the sensitivity metrics of a <code>HessMLP</code> object.
This metrics are the mean sensitivity, the standard deviation
of sensitivities and the mean of sensitivities square
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'HessMLP'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.HessMLP_+3A_object">object</code></td>
<td>
<p><code>HessMLP</code> object created by <code><a href="#topic+HessianMLP">HessianMLP</a></code></p>
</td></tr>
<tr><td><code id="summary.HessMLP_+3A_...">...</code></td>
<td>
<p>additional parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>summary object of the <code>HessMLP</code> object passed
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load data -------------------------------------------------------------------
data("DAILY_DEMAND_TR")
fdata &lt;- DAILY_DEMAND_TR

## Parameters of the NNET ------------------------------------------------------
hidden_neurons &lt;- 5
iters &lt;- 250
decay &lt;- 0.1

################################################################################
#########################  REGRESSION NNET #####################################
################################################################################
## Regression dataframe --------------------------------------------------------
# Scale the data
fdata.Reg.tr &lt;- fdata[,2:ncol(fdata)]
fdata.Reg.tr[,3] &lt;- fdata.Reg.tr[,3]/10
fdata.Reg.tr[,1] &lt;- fdata.Reg.tr[,1]/1000

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.tr, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.tr)

#' ## TRAIN nnet NNET --------------------------------------------------------
# Create a formula to train NNET
form &lt;- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = " + ")
form &lt;- formula(paste(names(fdata.Reg.tr)[1], form, sep = " ~ "))

set.seed(150)
nnetmod &lt;- nnet::nnet(form,
                           data = nntrData,
                           linear.output = TRUE,
                           size = hidden_neurons,
                           decay = decay,
                           maxit = iters)
# Try HessianMLP
sens &lt;- NeuralSens::HessianMLP(nnetmod, trData = nntrData, plot = FALSE)
summary(sens)
</code></pre>

<hr>
<h2 id='summary.SensMLP'>Summary Method for the SensMLP Class</h2><span id='topic+summary.SensMLP'></span>

<h3>Description</h3>

<p>Print the sensitivity metrics of a <code>SensMLP</code> object.
This metrics are the mean sensitivity, the standard deviation
of sensitivities and the mean of sensitivities square
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SensMLP'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.SensMLP_+3A_object">object</code></td>
<td>
<p><code>SensMLP</code> object created by <code><a href="#topic+SensAnalysisMLP">SensAnalysisMLP</a></code></p>
</td></tr>
<tr><td><code id="summary.SensMLP_+3A_...">...</code></td>
<td>
<p>additional parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>summary object of the <code>SensMLP</code> object passed
</p>


<h3>References</h3>

<p>Pizarroso J, Portela J, Muñoz A (2022). NeuralSens: Sensitivity Analysis of
Neural Networks. Journal of Statistical Software, 102(7), 1-36.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Load data -------------------------------------------------------------------
data("DAILY_DEMAND_TR")
fdata &lt;- DAILY_DEMAND_TR

## Parameters of the NNET ------------------------------------------------------
hidden_neurons &lt;- 5
iters &lt;- 250
decay &lt;- 0.1

################################################################################
#########################  REGRESSION NNET #####################################
################################################################################
## Regression dataframe --------------------------------------------------------
# Scale the data
fdata.Reg.tr &lt;- fdata[,2:ncol(fdata)]
fdata.Reg.tr[,3] &lt;- fdata.Reg.tr[,3]/10
fdata.Reg.tr[,1] &lt;- fdata.Reg.tr[,1]/1000

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.tr, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.tr)

#' ## TRAIN nnet NNET --------------------------------------------------------
# Create a formula to train NNET
form &lt;- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = " + ")
form &lt;- formula(paste(names(fdata.Reg.tr)[1], form, sep = " ~ "))

set.seed(150)
nnetmod &lt;- nnet::nnet(form,
                           data = nntrData,
                           linear.output = TRUE,
                           size = hidden_neurons,
                           decay = decay,
                           maxit = iters)
# Try SensAnalysisMLP
sens &lt;- NeuralSens::SensAnalysisMLP(nnetmod, trData = nntrData, plot = FALSE)
summary(sens)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
