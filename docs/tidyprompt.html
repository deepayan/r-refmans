<!DOCTYPE html><html lang="en"><head><title>Help for package tidyprompt</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tidyprompt}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#tidyprompt-package'><p>tidyprompt: Prompt Large Language Models and Enhance Their Functionality</p></a></li>
<li><a href='#add_msg_to_chat_history'><p>Add a message to a chat history</p></a></li>
<li><a href='#add_text'><p>Add text to a tidyprompt</p></a></li>
<li><a href='#answer_as_boolean'><p>Make LLM answer as a boolean (TRUE or FALSE)</p></a></li>
<li><a href='#answer_as_integer'><p>Make LLM answer as an integer (between min and max)</p></a></li>
<li><a href='#answer_as_json'><p>Make LLM answer as JSON (with optional schema)</p></a></li>
<li><a href='#answer_as_key_value'><p>Make LLM answer as a list of key-value pairs</p></a></li>
<li><a href='#answer_as_list'><p>Make LLM answer as a list of items</p></a></li>
<li><a href='#answer_as_named_list'><p>Make LLM answer as a named list</p></a></li>
<li><a href='#answer_as_regex_match'><p>Make LLM answer match a specific regex</p></a></li>
<li><a href='#answer_as_text'><p>Make LLM answer as a constrained text response</p></a></li>
<li><a href='#answer_by_chain_of_thought'><p>Set chain of thought mode for a prompt</p></a></li>
<li><a href='#answer_by_react'><p>Set ReAct mode for a prompt</p></a></li>
<li><a href='#answer_using_r'><p>Enable LLM to draft and execute R code</p></a></li>
<li><a href='#answer_using_sql'><p>Enable LLM to draft and execute SQL queries on a database</p></a></li>
<li><a href='#answer_using_tools'><p>Enable LLM to call R functions</p></a></li>
<li><a href='#chat_history'><p>Create or validate <code>chat_history</code> object</p></a></li>
<li><a href='#chat_history.character'><p>Method for <code>chat_history()</code> when the input is a single string</p></a></li>
<li><a href='#chat_history.data.frame'><p>Method for <code>chat_history()</code> when the input is a <code>data.frame</code></p></a></li>
<li><a href='#chat_history.default'><p>Default method for <code>chat_history()</code></p></a></li>
<li><a href='#construct_prompt_text'><p>Construct prompt text from a tidyprompt object</p></a></li>
<li><a href='#df_to_string'><p>Convert a dataframe to a string representation</p></a></li>
<li><a href='#extract_from_return_list'><p>Function to extract a specific element from a list</p></a></li>
<li><a href='#get_chat_history'><p>Get the chat history of a tidyprompt object</p></a></li>
<li><a href='#get_prompt_wraps'><p>Get prompt wraps from a tidyprompt object</p></a></li>
<li><a href='#is_tidyprompt'><p>Check if object is a tidyprompt object</p></a></li>
<li><a href='#llm_break'><p>Create an <code>llm_break</code> object</p></a></li>
<li><a href='#llm_feedback'><p>Create an <code>llm_feedback</code> object</p></a></li>
<li><a href='#llm_provider_google_gemini'><p>Create a new Google Gemini LLM provider</p></a></li>
<li><a href='#llm_provider_groq'><p>Create a new Groq LLM provider</p></a></li>
<li><a href='#llm_provider_mistral'><p>Create a new Mistral LLM provider</p></a></li>
<li><a href='#llm_provider_ollama'><p>Create a new Ollama LLM provider</p></a></li>
<li><a href='#llm_provider_openai'><p>Create a new OpenAI LLM provider</p></a></li>
<li><a href='#llm_provider_openrouter'><p>Create a new OpenRouter LLM provider</p></a></li>
<li><a href='#llm_provider_xai'><p>Create a new XAI (Grok) LLM provider</p></a></li>
<li><a href='#llm_provider-class'><p>LlmProvider R6 Class</p></a></li>
<li><a href='#llm_verify'><p>Have LLM check the result of a prompt (LLM-in-the-loop)</p></a></li>
<li><a href='#persistent_chat-class'><p>PersistentChat R6 class</p></a></li>
<li><a href='#prompt_wrap'><p>Wrap a prompt with functions for modification and handling the LLM response</p></a></li>
<li><a href='#quit_if'><p>Make evaluation of a prompt stop if LLM gives a specific response</p></a></li>
<li><a href='#r_json_schema_to_example'><p>Generate an example object from a JSON schema</p></a></li>
<li><a href='#send_prompt'><p>Send a prompt to a LLM provider</p></a></li>
<li><a href='#set_chat_history'><p>Set the chat history of a tidyprompt object</p></a></li>
<li><a href='#set_system_prompt'><p>Set system prompt of a tidyprompt object</p></a></li>
<li><a href='#skim_with_labels_and_levels'><p>Skim a dataframe and include labels and levels</p></a></li>
<li><a href='#tidyprompt'><p>Create a tidyprompt object</p></a></li>
<li><a href='#tidyprompt-class'><p>Tidyprompt R6 Class</p></a></li>
<li><a href='#tools_add_docs'><p>Add tidyprompt function documentation to a function</p></a></li>
<li><a href='#tools_get_docs'><p>Extract documentation from a function</p></a></li>
<li><a href='#user_verify'><p>Have user check the result of a prompt (human-in-the-loop)</p></a></li>
<li><a href='#vector_list_to_string'><p>Convert a named or unnamed list/vector to a string representation</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Prompt Large Language Models and Enhance Their Functionality</td>
</tr>
<tr>
<td>Version:</td>
<td>0.0.1</td>
</tr>
<tr>
<td>Description:</td>
<td>
  Easily construct prompts and associated logic for interacting with 
  large language models (LLMs). 
  'tidyprompt' introduces the concept of prompt wraps, which are building
  blocks that you can use to quickly turn a simple prompt into a complex
  one. Prompt wraps do not just modify the prompt text, but also add
  extraction and validation functions that will be applied to the response
  of the LLM. This ensures that the user gets the desired output.
  'tidyprompt' can add various features to prompts and their evaluation
  by LLMs, such as structured output, automatic feedback, retries, reasoning
  modes, autonomous R function calling, and R code generation and evaluation.
  It is designed to be compatible with any LLM provider that offers
  chat completion.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a> | file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/tjarkvandemerwe/tidyprompt">https://github.com/tjarkvandemerwe/tidyprompt</a>,
<a href="https://tjarkvandemerwe.github.io/tidyprompt/">https://tjarkvandemerwe.github.io/tidyprompt/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/tjarkvandemerwe/tidyprompt/issues">https://github.com/tjarkvandemerwe/tidyprompt/issues</a></td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat (&ge; 3.0.0), here, callr, skimr,
jsonvalidate, tidyjson, DBI</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Imports:</td>
<td>dplyr, glue, httr2, jsonlite, stringr, utils, cli</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-07 21:45:11 UTC; dhrko</td>
</tr>
<tr>
<td>Author:</td>
<td>Luka Koning [aut, cre, cph],
  Tjark Van de Merwe [aut, cph],
  Kennispunt Twente [fnd]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Luka Koning &lt;l.koning@kennispunttwente.nl&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-08 15:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='tidyprompt-package'>tidyprompt: Prompt Large Language Models and Enhance Their Functionality</h2><span id='topic+tidyprompt-package'></span>

<h3>Description</h3>

<p>Easily construct prompts and associated logic for interacting with large language models (LLMs). 'tidyprompt' introduces the concept of prompt wraps, which are building blocks that you can use to quickly turn a simple prompt into a complex one. Prompt wraps do not just modify the prompt text, but also add extraction and validation functions that will be applied to the response of the LLM. This ensures that the user gets the desired output. 'tidyprompt' can add various features to prompts and their evaluation by LLMs, such as structured output, automatic feedback, retries, reasoning modes, autonomous R function calling, and R code generation and evaluation. It is designed to be compatible with any LLM provider that offers chat completion.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Luka Koning <a href="mailto:l.koning@kennispunttwente.nl">l.koning@kennispunttwente.nl</a> [copyright holder]
</p>
<p>Authors:
</p>

<ul>
<li><p> Tjark Van de Merwe <a href="mailto:t.vandemerwe@kennispunttwente.nl">t.vandemerwe@kennispunttwente.nl</a> [copyright holder]
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Kennispunt Twente <a href="mailto:info@kennispunttwente.nl">info@kennispunttwente.nl</a> [funder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/tjarkvandemerwe/tidyprompt">https://github.com/tjarkvandemerwe/tidyprompt</a>
</p>
</li>
<li> <p><a href="https://tjarkvandemerwe.github.io/tidyprompt/">https://tjarkvandemerwe.github.io/tidyprompt/</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/tjarkvandemerwe/tidyprompt/issues">https://github.com/tjarkvandemerwe/tidyprompt/issues</a>
</p>
</li></ul>


<hr>
<h2 id='add_msg_to_chat_history'>Add a message to a chat history</h2><span id='topic+add_msg_to_chat_history'></span>

<h3>Description</h3>

<p>This function appends a message to a <code><a href="#topic+chat_history">chat_history()</a></code> object.
The function can automatically determine the role of the message to be
added based on the last message in the chat history. The role can also be
manually specified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_msg_to_chat_history(
  chat_history,
  message,
  role = c("auto", "user", "assistant", "system", "tool"),
  tool_result = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="add_msg_to_chat_history_+3A_chat_history">chat_history</code></td>
<td>
<p>A single string, a data.frame which is a valid chat history
(see <code style="white-space: pre;">&#8288;[chat_history()]&#8288;</code>), a list containing a valid chat history under key
'$chat_history', a <a href="#topic+tidyprompt-class">tidyprompt-class</a> object, or NULL
</p>
<p>A <code><a href="#topic+chat_history">chat_history()</a></code> object</p>
</td></tr>
<tr><td><code id="add_msg_to_chat_history_+3A_message">message</code></td>
<td>
<p>A character string representing the message to add</p>
</td></tr>
<tr><td><code id="add_msg_to_chat_history_+3A_role">role</code></td>
<td>
<p>A character string representing the role of the message sender.
One of: </p>

<ul>
<li><p> &quot;auto&quot;: The function automatically determines the role. If the last message
was from the user, the role will be &quot;assistant&quot;. If the last message was from anything
else, the role will be &quot;user&quot;
</p>
</li>
<li><p> &quot;user&quot;: The message is from the user
</p>
</li>
<li><p> &quot;assistant&quot;: The message is from the assistant
</p>
</li>
<li><p> &quot;system&quot;: The message is from the system
</p>
</li>
<li><p> &quot;tool&quot;: The message is from a tool (e.g., indicating the result of a function call)
</p>
</li></ul>
</td></tr>
<tr><td><code id="add_msg_to_chat_history_+3A_tool_result">tool_result</code></td>
<td>
<p>A logical indicating whether the message is a tool result
(e.g., the result of a function call)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The chat_history object may be of different types:
</p>

<ul>
<li><p> A single string: The function will create a new chat history object
with the string as the first message; the role of that first message will be
&quot;user&quot;
</p>
</li>
<li><p> A data.frame: The function will append the message to the data.frame.
The data.frame must be a valid chat history; see <code><a href="#topic+chat_history">chat_history()</a></code>
</p>
</li>
<li><p> A list: The function will extract the chat history from the list.
The list must contain a valid chat history under the key 'chat_history'.
This may typically be the result from <code><a href="#topic+send_prompt">send_prompt()</a></code> when using
'return_mode = &quot;full&quot;'
</p>
</li>
<li><p> A Tidyprompt object (<a href="#topic+tidyprompt-class">tidyprompt</a>): The function will extract the chat history
from the object. This will be done by concatenating the 'system_prompt',
'chat_history', and 'base_prompt' into a chat history data.frame. Note
that the other properties of the <a href="#topic+tidyprompt-class">tidyprompt</a> object will be lost
</p>
</li>
<li><p> NULL: The function will create a new chat history object
with no messages; the message will be the first message
</p>
</li></ul>



<h3>Value</h3>

<p>A <code><a href="#topic+chat_history">chat_history()</a></code> object with the message added as the last row
</p>


<h3>Examples</h3>

<pre><code class='language-R'>chat &lt;- "Hi there!" |&gt;
  chat_history()
chat

chat_from_df &lt;- data.frame(
  role = c("user", "assistant"),
  content = c("Hi there!", "Hello! How can I help you today?")
) |&gt;
  chat_history()
chat_from_df

# `add_msg_to_chat_history()` may be used to add messages to a chat history
chat_from_df &lt;- chat_from_df |&gt;
  add_msg_to_chat_history("Calculate 2+2 for me, please!")
chat_from_df

# You can also continue conversations which originate from `send_prompt()`:
## Not run: 
  result &lt;- "Hi there!" |&gt;
    send_prompt(return_mode = "full")
  # --- Sending request to LLM provider (llama3.1:8b): ---
  # Hi there!
  # --- Receiving response from LLM provider: ---
  # It's nice to meet you. Is there something I can help you with, or would you
  # like to chat?

  # Access the chat history from the result:
  chat_from_send_prompt &lt;- result$chat_history

  # Add a message to the chat history:
  chat_history_with_new_message &lt;- chat_from_send_prompt |&gt;
    add_msg_to_chat_history("Let's chat!")

  # The new chat history can be input for a new tidyprompt:
  prompt &lt;- tidyprompt(chat_history_with_new_message)

  # You can also take an existing tidyprompt and add the new chat history to it;
  #   this way, you can continue a conversation using the same prompt wraps
  prompt$set_chat_history(chat_history_with_new_message)

  # send_prompt() also accepts a chat history as input:
  new_result &lt;- chat_history_with_new_message |&gt;
    send_prompt(return_mode = "full")

  # You can also create a persistent chat history object from
  #   a chat history data frame; see ?`persistent_chat-class`
  chat &lt;- `persistent_chat-class`$new(llm_provider_ollama(), chat_from_send_prompt)
  chat$chat("Let's chat!")

## End(Not run)
</code></pre>

<hr>
<h2 id='add_text'>Add text to a tidyprompt</h2><span id='topic+add_text'></span>

<h3>Description</h3>

<p>Add text to a prompt by adding a <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> which will append the text to
the before or after the current prompt text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_text(prompt, text, position = c("after", "before"), sep = "\n\n")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="add_text_+3A_prompt">prompt</code></td>
<td>
<p>A single string or a <code><a href="#topic+tidyprompt">tidyprompt()</a></code> object</p>
</td></tr>
<tr><td><code id="add_text_+3A_text">text</code></td>
<td>
<p>Text to be added to the current prompt text</p>
</td></tr>
<tr><td><code id="add_text_+3A_position">position</code></td>
<td>
<p>Where to add the text; either &quot;after&quot; or &quot;before&quot;.</p>
</td></tr>
<tr><td><code id="add_text_+3A_sep">sep</code></td>
<td>
<p>Separator to be used between the current prompt text and the text to be added</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="#topic+tidyprompt">tidyprompt()</a></code> with an added <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> which
will append the text to the end of the current prompt text
</p>


<h3>See Also</h3>

<p>Other pre_built_prompt_wraps: 
<code><a href="#topic+answer_as_boolean">answer_as_boolean</a>()</code>,
<code><a href="#topic+answer_as_integer">answer_as_integer</a>()</code>,
<code><a href="#topic+answer_as_json">answer_as_json</a>()</code>,
<code><a href="#topic+answer_as_list">answer_as_list</a>()</code>,
<code><a href="#topic+answer_as_named_list">answer_as_named_list</a>()</code>,
<code><a href="#topic+answer_as_regex_match">answer_as_regex_match</a>()</code>,
<code><a href="#topic+answer_as_text">answer_as_text</a>()</code>,
<code><a href="#topic+answer_by_chain_of_thought">answer_by_chain_of_thought</a>()</code>,
<code><a href="#topic+answer_by_react">answer_by_react</a>()</code>,
<code><a href="#topic+answer_using_r">answer_using_r</a>()</code>,
<code><a href="#topic+answer_using_sql">answer_using_sql</a>()</code>,
<code><a href="#topic+answer_using_tools">answer_using_tools</a>()</code>,
<code><a href="#topic+prompt_wrap">prompt_wrap</a>()</code>,
<code><a href="#topic+quit_if">quit_if</a>()</code>,
<code><a href="#topic+set_system_prompt">set_system_prompt</a>()</code>
</p>
<p>Other miscellaneous_prompt_wraps: 
<code><a href="#topic+quit_if">quit_if</a>()</code>,
<code><a href="#topic+set_system_prompt">set_system_prompt</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prompt &lt;- "Hi there!" |&gt;
  add_text("How is your day?")
prompt
prompt |&gt;
  construct_prompt_text()
</code></pre>

<hr>
<h2 id='answer_as_boolean'>Make LLM answer as a boolean (TRUE or FALSE)</h2><span id='topic+answer_as_boolean'></span>

<h3>Description</h3>

<p>Make LLM answer as a boolean (TRUE or FALSE)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>answer_as_boolean(
  prompt,
  true_definition = NULL,
  false_definition = NULL,
  add_instruction_to_prompt = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="answer_as_boolean_+3A_prompt">prompt</code></td>
<td>
<p>A single string or a <code><a href="#topic+tidyprompt">tidyprompt()</a></code> object</p>
</td></tr>
<tr><td><code id="answer_as_boolean_+3A_true_definition">true_definition</code></td>
<td>
<p>(optional) Definition of what would constitute TRUE.
This will be included in the instruction to the LLM. Should be a single string</p>
</td></tr>
<tr><td><code id="answer_as_boolean_+3A_false_definition">false_definition</code></td>
<td>
<p>(optional) Definition of what would constitute FALSE.
This will be included in the instruction to the LLM. Should be a single string</p>
</td></tr>
<tr><td><code id="answer_as_boolean_+3A_add_instruction_to_prompt">add_instruction_to_prompt</code></td>
<td>
<p>(optional) Add instruction for replying
as a boolean to the prompt text. Set to FALSE for debugging if extractions/validations
are working as expected (without instruction the answer should fail the
validation function, initiating a retry)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="#topic+tidyprompt">tidyprompt()</a></code> with an added <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> which
will ensure that the LLM response is a boolean
</p>


<h3>See Also</h3>

<p>Other pre_built_prompt_wraps: 
<code><a href="#topic+add_text">add_text</a>()</code>,
<code><a href="#topic+answer_as_integer">answer_as_integer</a>()</code>,
<code><a href="#topic+answer_as_json">answer_as_json</a>()</code>,
<code><a href="#topic+answer_as_list">answer_as_list</a>()</code>,
<code><a href="#topic+answer_as_named_list">answer_as_named_list</a>()</code>,
<code><a href="#topic+answer_as_regex_match">answer_as_regex_match</a>()</code>,
<code><a href="#topic+answer_as_text">answer_as_text</a>()</code>,
<code><a href="#topic+answer_by_chain_of_thought">answer_by_chain_of_thought</a>()</code>,
<code><a href="#topic+answer_by_react">answer_by_react</a>()</code>,
<code><a href="#topic+answer_using_r">answer_using_r</a>()</code>,
<code><a href="#topic+answer_using_sql">answer_using_sql</a>()</code>,
<code><a href="#topic+answer_using_tools">answer_using_tools</a>()</code>,
<code><a href="#topic+prompt_wrap">prompt_wrap</a>()</code>,
<code><a href="#topic+quit_if">quit_if</a>()</code>,
<code><a href="#topic+set_system_prompt">set_system_prompt</a>()</code>
</p>
<p>Other answer_as_prompt_wraps: 
<code><a href="#topic+answer_as_integer">answer_as_integer</a>()</code>,
<code><a href="#topic+answer_as_json">answer_as_json</a>()</code>,
<code><a href="#topic+answer_as_list">answer_as_list</a>()</code>,
<code><a href="#topic+answer_as_named_list">answer_as_named_list</a>()</code>,
<code><a href="#topic+answer_as_regex_match">answer_as_regex_match</a>()</code>,
<code><a href="#topic+answer_as_text">answer_as_text</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  "Are you a large language model?" |&gt;
    answer_as_boolean() |&gt;
    send_prompt()
  # --- Sending request to LLM provider (llama3.1:8b): ---
  #   Are you a large language model?
  #
  #   You must answer with only TRUE or FALSE (use no other characters).
  # --- Receiving response from LLM provider: ---
  #   TRUE
  # [1] TRUE

## End(Not run)
</code></pre>

<hr>
<h2 id='answer_as_integer'>Make LLM answer as an integer (between min and max)</h2><span id='topic+answer_as_integer'></span>

<h3>Description</h3>

<p>Make LLM answer as an integer (between min and max)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>answer_as_integer(
  prompt,
  min = NULL,
  max = NULL,
  add_instruction_to_prompt = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="answer_as_integer_+3A_prompt">prompt</code></td>
<td>
<p>A single string or a <code><a href="#topic+tidyprompt">tidyprompt()</a></code> object</p>
</td></tr>
<tr><td><code id="answer_as_integer_+3A_min">min</code></td>
<td>
<p>(optional) Minimum value for the integer</p>
</td></tr>
<tr><td><code id="answer_as_integer_+3A_max">max</code></td>
<td>
<p>(optional) Maximum value for the integer</p>
</td></tr>
<tr><td><code id="answer_as_integer_+3A_add_instruction_to_prompt">add_instruction_to_prompt</code></td>
<td>
<p>(optional) Add instruction for replying
as an integer to the prompt text. Set to FALSE for debugging if extractions/validations
are working as expected (without instruction the answer should fail the
validation function, initiating a retry)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="#topic+tidyprompt">tidyprompt()</a></code> with an added <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> which
will ensure that the LLM response is an integer.
</p>


<h3>See Also</h3>

<p>Other pre_built_prompt_wraps: 
<code><a href="#topic+add_text">add_text</a>()</code>,
<code><a href="#topic+answer_as_boolean">answer_as_boolean</a>()</code>,
<code><a href="#topic+answer_as_json">answer_as_json</a>()</code>,
<code><a href="#topic+answer_as_list">answer_as_list</a>()</code>,
<code><a href="#topic+answer_as_named_list">answer_as_named_list</a>()</code>,
<code><a href="#topic+answer_as_regex_match">answer_as_regex_match</a>()</code>,
<code><a href="#topic+answer_as_text">answer_as_text</a>()</code>,
<code><a href="#topic+answer_by_chain_of_thought">answer_by_chain_of_thought</a>()</code>,
<code><a href="#topic+answer_by_react">answer_by_react</a>()</code>,
<code><a href="#topic+answer_using_r">answer_using_r</a>()</code>,
<code><a href="#topic+answer_using_sql">answer_using_sql</a>()</code>,
<code><a href="#topic+answer_using_tools">answer_using_tools</a>()</code>,
<code><a href="#topic+prompt_wrap">prompt_wrap</a>()</code>,
<code><a href="#topic+quit_if">quit_if</a>()</code>,
<code><a href="#topic+set_system_prompt">set_system_prompt</a>()</code>
</p>
<p>Other answer_as_prompt_wraps: 
<code><a href="#topic+answer_as_boolean">answer_as_boolean</a>()</code>,
<code><a href="#topic+answer_as_json">answer_as_json</a>()</code>,
<code><a href="#topic+answer_as_list">answer_as_list</a>()</code>,
<code><a href="#topic+answer_as_named_list">answer_as_named_list</a>()</code>,
<code><a href="#topic+answer_as_regex_match">answer_as_regex_match</a>()</code>,
<code><a href="#topic+answer_as_text">answer_as_text</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  "What is 5 + 5?" |&gt;
    answer_as_integer() |&gt;
    send_prompt()
  # --- Sending request to LLM provider (llama3.1:8b): ---
  #   What is 5 + 5?
  #
  #   You must answer with only an integer (use no other characters).
  # --- Receiving response from LLM provider: ---
  #   10
  # [1] 10

## End(Not run)
</code></pre>

<hr>
<h2 id='answer_as_json'>Make LLM answer as JSON (with optional schema)</h2><span id='topic+answer_as_json'></span>

<h3>Description</h3>

<p>This functions wraps a prompt with settings that ensure the LLM response
is a valid JSON object, optionally matching a given JSON schema.
</p>
<p>The function can work with all models and providers through text-based
handling, but also supports native settings for the OpenAI and Ollama
API types. (See argument 'type'.) This means that it is possible to easily
switch between providers with different levels of JSON support,
while ensuring the results will be in the correct format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>answer_as_json(
  prompt,
  schema = NULL,
  schema_strict = FALSE,
  schema_in_prompt_as = c("example", "schema"),
  type = c("text-based", "auto", "openai", "ollama", "openai_oo", "ollama_oo")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="answer_as_json_+3A_prompt">prompt</code></td>
<td>
<p>A single string or a <code><a href="#topic+tidyprompt">tidyprompt()</a></code> object</p>
</td></tr>
<tr><td><code id="answer_as_json_+3A_schema">schema</code></td>
<td>
<p>A list which represents
a JSON schema that the response should match.See example and your API's
documentation for more information on defining JSON schemas. Note that the schema should be a
list (R object) representing a JSON schema, not a JSON string
(use <code><a href="jsonlite.html#topic+fromJSON">jsonlite::fromJSON()</a></code> and <code><a href="jsonlite.html#topic+fromJSON">jsonlite::toJSON()</a></code> to convert between the two)</p>
</td></tr>
<tr><td><code id="answer_as_json_+3A_schema_strict">schema_strict</code></td>
<td>
<p>If TRUE, the provided schema will be strictly enforced.
This option is passed as part of the schema when using type  type
&quot;openai&quot; or &quot;ollama&quot;, and when using the other types it is passed to
<code><a href="jsonvalidate.html#topic+json_validate">jsonvalidate::json_validate()</a></code></p>
</td></tr>
<tr><td><code id="answer_as_json_+3A_schema_in_prompt_as">schema_in_prompt_as</code></td>
<td>
<p>If providing a schema and
when using type &quot;text-based&quot;, &quot;openai_oo&quot;, or &quot;ollama_oo&quot;, this argument specifies
how the schema should be included in the prompt:
</p>

<ul>
<li><p> &quot;example&quot; (default): The schema will be included as an example JSON object
(tends to work best). <code><a href="#topic+r_json_schema_to_example">r_json_schema_to_example()</a></code> is used to generate the example object
from the schema
</p>
</li>
<li><p> &quot;schema&quot;: The schema will be included as a JSON schema
</p>
</li></ul>
</td></tr>
<tr><td><code id="answer_as_json_+3A_type">type</code></td>
<td>
<p>The way that JSON response should be enforced:
</p>

<ul>
<li><p> &quot;text-based&quot;: Instruction will be added to the prompt
asking for JSON; when a schema is provided, this will also be included
in the prompt (see argument 'schema_in_prompt_as'). JSON will be parsed
from the LLM response and, when a schema is provided, it will be validated
against the schema with <code><a href="jsonvalidate.html#topic+json_validate">jsonvalidate::json_validate()</a></code>. Feedback is sent to the
LLM when the response is not valid. This option always works, but may in some
cases may be less powerful than the other native JSON options
</p>
</li>
<li><p> &quot;auto&quot;: Automatically determine the type based on 'llm_provider$api_type'.
This does not consider model compatibility and could lead to errors; set 'type'
manually if errors occur; use 'text-based' if unsure
</p>
</li>
<li><p> &quot;openai&quot; and &quot;ollama&quot;: The response format will be set via the relevant API parameters,
making the API enforce a valid JSON response. If a schema is provided,
it will also be included in the API parameters and also be enforced by the API.
When no schema is provided, a request for JSON is added to the prompt (as required
by the APIs). Note that these JSON options may not be available for all models
of your provider; consult their documentation for more information.
If you are unsure or encounter errors, use &quot;text-based&quot;
</p>
</li>
<li><p> &quot;openai_oo&quot; and &quot;ollama_oo&quot;: Similar to &quot;openai&quot; and &quot;ollama&quot;, but if a
schema is provided it is not included in the API parameters. Schema validation
will be done in R with <code><a href="jsonvalidate.html#topic+json_validate">jsonvalidate::json_validate()</a></code>. This can be useful if
you want to use the API's JSON support, but their schema support is limited
</p>
</li></ul>

<p>Note that the &quot;openai&quot; and &quot;ollama&quot; types may also work for other APIs with a similar structure</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="#topic+tidyprompt">tidyprompt()</a></code> with an added <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> which will ensure
that the LLM response is a valid JSON object
</p>


<h3>See Also</h3>

<p>Other pre_built_prompt_wraps: 
<code><a href="#topic+add_text">add_text</a>()</code>,
<code><a href="#topic+answer_as_boolean">answer_as_boolean</a>()</code>,
<code><a href="#topic+answer_as_integer">answer_as_integer</a>()</code>,
<code><a href="#topic+answer_as_list">answer_as_list</a>()</code>,
<code><a href="#topic+answer_as_named_list">answer_as_named_list</a>()</code>,
<code><a href="#topic+answer_as_regex_match">answer_as_regex_match</a>()</code>,
<code><a href="#topic+answer_as_text">answer_as_text</a>()</code>,
<code><a href="#topic+answer_by_chain_of_thought">answer_by_chain_of_thought</a>()</code>,
<code><a href="#topic+answer_by_react">answer_by_react</a>()</code>,
<code><a href="#topic+answer_using_r">answer_using_r</a>()</code>,
<code><a href="#topic+answer_using_sql">answer_using_sql</a>()</code>,
<code><a href="#topic+answer_using_tools">answer_using_tools</a>()</code>,
<code><a href="#topic+prompt_wrap">prompt_wrap</a>()</code>,
<code><a href="#topic+quit_if">quit_if</a>()</code>,
<code><a href="#topic+set_system_prompt">set_system_prompt</a>()</code>
</p>
<p>Other answer_as_prompt_wraps: 
<code><a href="#topic+answer_as_boolean">answer_as_boolean</a>()</code>,
<code><a href="#topic+answer_as_integer">answer_as_integer</a>()</code>,
<code><a href="#topic+answer_as_list">answer_as_list</a>()</code>,
<code><a href="#topic+answer_as_named_list">answer_as_named_list</a>()</code>,
<code><a href="#topic+answer_as_regex_match">answer_as_regex_match</a>()</code>,
<code><a href="#topic+answer_as_text">answer_as_text</a>()</code>
</p>
<p>Other json: 
<code><a href="#topic+r_json_schema_to_example">r_json_schema_to_example</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>base_prompt &lt;- "How can I solve 8x + 7 = -23?"

# This example will show how to enforce JSON format in the response,
#   with and without a schema, using the 'answer_as_json()' prompt wrap.
# If you use type = 'auto', the function will automatically detect the
#   best way to enforce JSON based on the LLM provider you are using.
# Note that the default type is 'text-based', which will work for any provider/model

#### Enforcing JSON without a schema: ####

## Not run: 
  ## Text-based (works for any provider/model):
  #   Adds request to prompt for a JSON object
  #   Extracts JSON from textual response (feedback for retry if no JSON received)
  #   Parses JSON to R object
  json_1 &lt;- base_prompt |&gt;
    answer_as_json() |&gt;
    send_prompt(llm_provider_ollama())
  # --- Sending request to LLM provider (llama3.1:8b): ---
  # How can I solve 8x + 7 = -23?
  #
  # Your must format your response as a JSON object.
  # --- Receiving response from LLM provider: ---
  # Here is the solution to the equation formatted as a JSON object:
  #
  # ```
  # {
  #   "equation": "8x + 7 = -23",
  #   "steps": [
  #     {
  #       "step": "Subtract 7 from both sides of the equation",
  #       "expression": "-23 - 7"
  #     },
  #     {
  #       "step": "Simplify the expression on the left side",
  #       "result": "-30"
  #     },
  #     {
  #       "step": "Divide both sides by -8 to solve for x",
  #       "expression": "-30 / -8"
  #     },
  #     {
  #       "step": "Simplify the expression on the right side",
  #       "result": "3.75"
  #     }
  #   ],
  #   "solution": {
  #     "x": 3.75
  #   }
  # }
  # ```


  ## Ollama:
  #   - Sets 'format' parameter to 'json', enforcing JSON
  #   - Adds request to prompt for a JSON object, as is recommended by the docs
  #   - Parses JSON to R object
  json_2 &lt;- base_prompt |&gt;
    answer_as_json(type = "auto") |&gt;
    send_prompt(llm_provider_ollama())
  # --- Sending request to LLM provider (llama3.1:8b): ---
  # How can I solve 8x + 7 = -23?
  #
  # Your must format your response as a JSON object.
  # --- Receiving response from LLM provider: ---
  # {"steps": [
  #   "Subtract 7 from both sides to get 8x = -30",
  #   "Simplify the right side of the equation to get 8x = -30",
  #   "Divide both sides by 8 to solve for x, resulting in x = -30/8",
  #   "Simplify the fraction to find the value of x"
  # ],
  # "value_of_x": "-3.75"}


  ## OpenAI-type API without schema:
  #   - Sets 'response_format' parameter to 'json_object', enforcing JSON
  #   - Adds request to prompt for a JSON object, as is required by the API
  #   - Parses JSON to R object
  json_3 &lt;- base_prompt |&gt;
    answer_as_json(type = "auto") |&gt;
    send_prompt(llm_provider_openai())
  # --- Sending request to LLM provider (gpt-4o-mini): ---
  # How can I solve 8x + 7 = -23?
  #
  # Your must format your response as a JSON object.
  # --- Receiving response from LLM provider: ---
  # {
  #   "solution_steps": [
  #     {
  #       "step": 1,
  #       "operation": "Subtract 7 from both sides",
  #       "equation": "8x + 7 - 7 = -23 - 7",
  #       "result": "8x = -30"
  #     },
  #     {
  #       "step": 2,
  #       "operation": "Divide both sides by 8",
  #       "equation": "8x / 8 = -30 / 8",
  #       "result": "x = -3.75"
  #     }
  #   ],
  #   "solution": {
  #     "x": -3.75
  #   }
  # }

## End(Not run)



#### Enforcing JSON with a schema: ####

# Make a list representing a JSON schema,
#   which the LLM response must adhere to:
json_schema &lt;- list(
  name = "steps_to_solve", # Required for OpenAI API
  description = NULL, # Optional for OpenAI API
  schema = list(
    type = "object",
    properties = list(
      steps = list(
        type = "array",
        items = list(
          type = "object",
          properties = list(
            explanation = list(type = "string"),
            output = list(type = "string")
          ),
          required = c("explanation", "output"),
          additionalProperties = FALSE
        )
      ),
      final_answer = list(type = "string")
    ),
    required = c("steps", "final_answer"),
    additionalProperties = FALSE
  )
  # 'strict' parameter is set as argument 'answer_as_json()'
)
# Note: when you are not using an OpenAI API, you can also pass just the
#   internal 'schema' list object to 'answer_as_json()' instead of the full
#   'json_schema' list object

# Generate example R object based on schema:
r_json_schema_to_example(json_schema)

## Not run: 
  ## Text-based with schema (works for any provider/model):
  #   - Adds request to prompt for a JSON object
  #   - Adds schema to prompt
  #   - Extracts JSON from textual response (feedback for retry if no JSON received)
  #   - Validates JSON against schema with 'jsonvalidate' package (feedback for retry if invalid)
  #   - Parses JSON to R object
  json_4 &lt;- base_prompt |&gt;
    answer_as_json(schema = json_schema) |&gt;
    send_prompt(llm_provider_ollama())
  # --- Sending request to LLM provider (llama3.1:8b): ---
  # How can I solve 8x + 7 = -23?
  #
  # Your must format your response as a JSON object.
  #
  # Your JSON object should match this example JSON object:
  #   {
  #     "steps": [
  #       {
  #         "explanation": "...",
  #         "output": "..."
  #       }
  #     ],
  #     "final_answer": "..."
  #   }
  # --- Receiving response from LLM provider: ---
  # Here is the solution to the equation:
  #
  # ```
  # {
  #   "steps": [
  #     {
  #       "explanation": "First, we want to isolate the term with 'x' by
  #       subtracting 7 from both sides of the equation.",
  #       "output": "8x + 7 - 7 = -23 - 7"
  #     },
  #     {
  #       "explanation": "This simplifies to: 8x = -30",
  #       "output": "8x = -30"
  #     },
  #     {
  #       "explanation": "Next, we want to get rid of the coefficient '8' by
  #       dividing both sides of the equation by 8.",
  #       "output": "(8x) / 8 = (-30) / 8"
  #     },
  #     {
  #       "explanation": "This simplifies to: x = -3.75",
  #       "output": "x = -3.75"
  #     }
  #   ],
  #   "final_answer": "-3.75"
  # }
  # ```


  ## Ollama with schema:
  #   - Sets 'format' parameter to 'json', enforcing JSON
  #   - Adds request to prompt for a JSON object, as is recommended by the docs
  #   - Adds schema to prompt
  #   - Validates JSON against schema with 'jsonvalidate' package (feedback for retry if invalid)
  json_5 &lt;- base_prompt |&gt;
    answer_as_json(json_schema, type = "auto") |&gt;
    send_prompt(llm_provider_ollama())
  # --- Sending request to LLM provider (llama3.1:8b): ---
  # How can I solve 8x + 7 = -23?
  #
  # Your must format your response as a JSON object.
  #
  # Your JSON object should match this example JSON object:
  # {
  #   "steps": [
  #     {
  #       "explanation": "...",
  #       "output": "..."
  #     }
  #   ],
  #   "final_answer": "..."
  # }
  # --- Receiving response from LLM provider: ---
  # {
  #   "steps": [
  #     {
  #       "explanation": "First, subtract 7 from both sides of the equation to
  #       isolate the term with x.",
  #       "output": "8x = -23 - 7"
  #     },
  #     {
  #       "explanation": "Simplify the right-hand side of the equation.",
  #       "output": "8x = -30"
  #     },
  #     {
  #       "explanation": "Next, divide both sides of the equation by 8 to solve for x.",
  #       "output": "x = -30 / 8"
  #     },
  #     {
  #       "explanation": "Simplify the right-hand side of the equation.",
  #       "output": "x = -3.75"
  #     }
  #   ],
  #   "final_answer": "-3.75"
  # }

  ## OpenAI with schema:
  #   - Sets 'response_format' parameter to 'json_object', enforcing JSON
  #   - Adds json_schema to the API request, API enforces JSON adhering schema
  #   - Parses JSON to R object
  json_6 &lt;- base_prompt |&gt;
    answer_as_json(json_schema, type = "auto") |&gt;
    send_prompt(llm_provider_openai())
  # --- Sending request to LLM provider (gpt-4o-mini): ---
  # How can I solve 8x + 7 = -23?
  # --- Receiving response from LLM provider: ---
  # {"steps":[
  # {"explanation":"Start with the original equation.",
  # "output":"8x + 7 = -23"},
  # {"explanation":"Subtract 7 from both sides to isolate the term with x.",
  # "output":"8x + 7 - 7 = -23 - 7"},
  # {"explanation":"Simplify the left side and the right side of the equation.",
  # "output":"8x = -30"},
  # {"explanation":"Now, divide both sides by 8 to solve for x.",
  # "output":"x = -30 / 8"},
  # {"explanation":"Simplify the fraction by dividing both the numerator and the
  # denominator by 2.",
  # "output":"x = -15 / 4"}
  # ], "final_answer":"x = -15/4"}

## End(Not run)
</code></pre>

<hr>
<h2 id='answer_as_key_value'>Make LLM answer as a list of key-value pairs</h2><span id='topic+answer_as_key_value'></span>

<h3>Description</h3>

<p>This function is similar to <code>answer_as_list()</code> but instead of returning
a list of items, it instructs the LLM to return a list of key-value pairs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>answer_as_key_value(
  prompt,
  key_name = "key",
  value_name = "value",
  pair_explanation = NULL,
  n_unique_items = NULL,
  list_mode = c("bullet", "comma")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="answer_as_key_value_+3A_prompt">prompt</code></td>
<td>
<p>A single string or a <code><a href="#topic+tidyprompt">tidyprompt()</a></code> object</p>
</td></tr>
<tr><td><code id="answer_as_key_value_+3A_key_name">key_name</code></td>
<td>
<p>(optional) A name or placeholder describing the &quot;key&quot; part
of each pair</p>
</td></tr>
<tr><td><code id="answer_as_key_value_+3A_value_name">value_name</code></td>
<td>
<p>(optional) A name or placeholder describing the &quot;value&quot; part
of each pair</p>
</td></tr>
<tr><td><code id="answer_as_key_value_+3A_pair_explanation">pair_explanation</code></td>
<td>
<p>(optional) Additional explanation of what a pair should
be. It should be a single string. It will be appended after the list instruction.</p>
</td></tr>
<tr><td><code id="answer_as_key_value_+3A_n_unique_items">n_unique_items</code></td>
<td>
<p>(optional) Number of unique key-value pairs required in the list</p>
</td></tr>
<tr><td><code id="answer_as_key_value_+3A_list_mode">list_mode</code></td>
<td>
<p>(optional) Mode of the list: &quot;bullet&quot; or &quot;comma&quot;.
</p>

<ul>
<li><p> &quot;bullet&quot; mode expects pairs like:
</p>
<div class="sourceCode"><pre>-- key1: value1
-- key2: value2
</pre></div>
</li>
<li><p> &quot;comma&quot; mode expects pairs like:
</p>
<div class="sourceCode"><pre>1. key: value, 2. key: value, etc.
</pre></div>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="#topic+tidyprompt">tidyprompt()</a></code> with an added <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> which
will ensure that the LLM response is a list of key-value pairs.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  "What are a few capital cities around the world?" |&gt;
    answer_as_key_value(
      key_name = "country",
      value_name = "capital"
    ) |&gt;
    send_prompt()
  # --- Sending request to LLM provider (llama3.1:8b): ---
  # What are a few capital cities around the world?
  #
  # Respond with a list of key-value pairs, like so:
  #   -- &lt;&lt;country 1&gt;&gt;: &lt;&lt;capital 1&gt;&gt;
  #   -- &lt;&lt;country 2&gt;&gt;: &lt;&lt;capital 2&gt;&gt;
  #   etc.
  # --- Receiving response from LLM provider: ---
  # Here are a few:
  #   -- Australia: Canberra
  #   -- France: Paris
  #   -- United States: Washington D.C.
  #   -- Japan: Tokyo
  #   -- China: Beijing
  # $Australia
  # [1] "Canberra"
  #
  # $France
  # [1] "Paris"
  #
  # $`United States`
  # [1] "Washington D.C."
  #
  # $Japan
  # [1] "Tokyo"
  #
  # $China
  # [1] "Beijing"

## End(Not run)
</code></pre>

<hr>
<h2 id='answer_as_list'>Make LLM answer as a list of items</h2><span id='topic+answer_as_list'></span>

<h3>Description</h3>

<p>Make LLM answer as a list of items
</p>


<h3>Usage</h3>

<pre><code class='language-R'>answer_as_list(
  prompt,
  item_name = "item",
  item_explanation = NULL,
  n_unique_items = NULL,
  list_mode = c("bullet", "comma")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="answer_as_list_+3A_prompt">prompt</code></td>
<td>
<p>A single string or a <code><a href="#topic+tidyprompt">tidyprompt()</a></code> object</p>
</td></tr>
<tr><td><code id="answer_as_list_+3A_item_name">item_name</code></td>
<td>
<p>(optional) Name of the items in the list</p>
</td></tr>
<tr><td><code id="answer_as_list_+3A_item_explanation">item_explanation</code></td>
<td>
<p>(optional) Additional explanation of what an item
should be. Item explanation should be a single string. It will be
appended after the list instruction</p>
</td></tr>
<tr><td><code id="answer_as_list_+3A_n_unique_items">n_unique_items</code></td>
<td>
<p>(optional) Number of unique items required in the list</p>
</td></tr>
<tr><td><code id="answer_as_list_+3A_list_mode">list_mode</code></td>
<td>
<p>(optional) Mode of the list. Either &quot;bullet&quot; or &quot;comma&quot;.
&quot;bullet mode expects items to be listed with &quot;&ndash;&quot; before each item, with a
new line for each item (e.g., &quot;&ndash; item1\n&ndash; item2\n&ndash; item3&quot;).
&quot;comma&quot; mode expects items to be listed with a number and a period before
(e.g., &quot;1. item1, 2. item2, 3. item3&quot;). &quot;comma&quot; mode may be easier for
smaller LLMs to use</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="#topic+tidyprompt">tidyprompt()</a></code> with an added <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> which
will ensure that the LLM response is a list of items
</p>


<h3>See Also</h3>

<p><code><a href="#topic+answer_as_named_list">answer_as_named_list()</a></code>
</p>
<p>Other pre_built_prompt_wraps: 
<code><a href="#topic+add_text">add_text</a>()</code>,
<code><a href="#topic+answer_as_boolean">answer_as_boolean</a>()</code>,
<code><a href="#topic+answer_as_integer">answer_as_integer</a>()</code>,
<code><a href="#topic+answer_as_json">answer_as_json</a>()</code>,
<code><a href="#topic+answer_as_named_list">answer_as_named_list</a>()</code>,
<code><a href="#topic+answer_as_regex_match">answer_as_regex_match</a>()</code>,
<code><a href="#topic+answer_as_text">answer_as_text</a>()</code>,
<code><a href="#topic+answer_by_chain_of_thought">answer_by_chain_of_thought</a>()</code>,
<code><a href="#topic+answer_by_react">answer_by_react</a>()</code>,
<code><a href="#topic+answer_using_r">answer_using_r</a>()</code>,
<code><a href="#topic+answer_using_sql">answer_using_sql</a>()</code>,
<code><a href="#topic+answer_using_tools">answer_using_tools</a>()</code>,
<code><a href="#topic+prompt_wrap">prompt_wrap</a>()</code>,
<code><a href="#topic+quit_if">quit_if</a>()</code>,
<code><a href="#topic+set_system_prompt">set_system_prompt</a>()</code>
</p>
<p>Other answer_as_prompt_wraps: 
<code><a href="#topic+answer_as_boolean">answer_as_boolean</a>()</code>,
<code><a href="#topic+answer_as_integer">answer_as_integer</a>()</code>,
<code><a href="#topic+answer_as_json">answer_as_json</a>()</code>,
<code><a href="#topic+answer_as_named_list">answer_as_named_list</a>()</code>,
<code><a href="#topic+answer_as_regex_match">answer_as_regex_match</a>()</code>,
<code><a href="#topic+answer_as_text">answer_as_text</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  "What are some delicious fruits?" |&gt;
    answer_as_list(item_name = "fruit", n_unique_items = 5) |&gt;
    send_prompt()
  # --- Sending request to LLM provider (llama3.1:8b): ---
  # What are some delicious fruits?
  #
  # Respond with a list, like so:
  #   -- &lt;&lt;fruit 1&gt;&gt;
  #   -- &lt;&lt;fruit 2&gt;&gt;
  #   etc.
  # The list should contain 5 unique items.
  # --- Receiving response from LLM provider: ---
  # Here's the list of delicious fruits:
  #   -- Strawberries
  #   -- Pineapples
  #   -- Mangoes
  #   -- Blueberries
  #   -- Pomegranates
  # [[1]]
  # [1] "Strawberries"
  #
  # [[2]]
  # [1] "Pineapples"
  #
  # [[3]]
  # [1] "Mangoes"
  #
  # [[4]]
  # [1] "Blueberries"
  #
  # [[5]]
  # [1] "Pomegranates"

## End(Not run)

</code></pre>

<hr>
<h2 id='answer_as_named_list'>Make LLM answer as a named list</h2><span id='topic+answer_as_named_list'></span>

<h3>Description</h3>

<p>Get a named list from LLM response with optional item instructions and validations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>answer_as_named_list(
  prompt,
  item_names,
  item_instructions = NULL,
  item_validations = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="answer_as_named_list_+3A_prompt">prompt</code></td>
<td>
<p>A single string or a <code><a href="#topic+tidyprompt">tidyprompt()</a></code> object</p>
</td></tr>
<tr><td><code id="answer_as_named_list_+3A_item_names">item_names</code></td>
<td>
<p>A character vector specifying the expected item names</p>
</td></tr>
<tr><td><code id="answer_as_named_list_+3A_item_instructions">item_instructions</code></td>
<td>
<p>An optional named list of additional instructions for each item</p>
</td></tr>
<tr><td><code id="answer_as_named_list_+3A_item_validations">item_validations</code></td>
<td>
<p>An optional named list of validation functions for each item.
Like validation functions for a <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code>, these functions should return
<code><a href="#topic+llm_feedback">llm_feedback()</a></code> if the validation fails. If the validation
is successful, the function should return TRUE</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="#topic+tidyprompt">tidyprompt()</a></code> with an added <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> that ensures
the LLM response is a named list with the specified item names, optional
instructions, and validations.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+answer_as_list">answer_as_list()</a></code> <code><a href="#topic+llm_feedback">llm_feedback()</a></code>
</p>
<p>Other pre_built_prompt_wraps: 
<code><a href="#topic+add_text">add_text</a>()</code>,
<code><a href="#topic+answer_as_boolean">answer_as_boolean</a>()</code>,
<code><a href="#topic+answer_as_integer">answer_as_integer</a>()</code>,
<code><a href="#topic+answer_as_json">answer_as_json</a>()</code>,
<code><a href="#topic+answer_as_list">answer_as_list</a>()</code>,
<code><a href="#topic+answer_as_regex_match">answer_as_regex_match</a>()</code>,
<code><a href="#topic+answer_as_text">answer_as_text</a>()</code>,
<code><a href="#topic+answer_by_chain_of_thought">answer_by_chain_of_thought</a>()</code>,
<code><a href="#topic+answer_by_react">answer_by_react</a>()</code>,
<code><a href="#topic+answer_using_r">answer_using_r</a>()</code>,
<code><a href="#topic+answer_using_sql">answer_using_sql</a>()</code>,
<code><a href="#topic+answer_using_tools">answer_using_tools</a>()</code>,
<code><a href="#topic+prompt_wrap">prompt_wrap</a>()</code>,
<code><a href="#topic+quit_if">quit_if</a>()</code>,
<code><a href="#topic+set_system_prompt">set_system_prompt</a>()</code>
</p>
<p>Other answer_as_prompt_wraps: 
<code><a href="#topic+answer_as_boolean">answer_as_boolean</a>()</code>,
<code><a href="#topic+answer_as_integer">answer_as_integer</a>()</code>,
<code><a href="#topic+answer_as_json">answer_as_json</a>()</code>,
<code><a href="#topic+answer_as_list">answer_as_list</a>()</code>,
<code><a href="#topic+answer_as_regex_match">answer_as_regex_match</a>()</code>,
<code><a href="#topic+answer_as_text">answer_as_text</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  persona &lt;- "Create a persona for me, please." |&gt;
    answer_as_named_list(
      item_names = c("name", "age", "occupation"),
      item_instructions = list(
        name = "The name of the persona",
        age = "The age of the persona",
        occupation = "The occupation of the persona"
      )
    ) |&gt; send_prompt(llm_provider_ollama())
  # --- Sending request to LLM provider (llama3.1:8b): ---
  #   Create a persona for me, please.
  #
  #   Respond with a named list like so:
  #     -- name: &lt;&lt;value&gt;&gt; (The name of the persona)
  #     -- age: &lt;&lt;value&gt;&gt; (The age of the persona)
  #     -- occupation: &lt;&lt;value&gt;&gt; (The occupation of the persona)
  #   Each name must correspond to: name, age, occupation
  # --- Receiving response from LLM provider: ---
  #   Here is your persona:
  #
  #   -- name: Astrid Welles
  #   -- age: 32
  #   -- occupation: Museum Curator
  persona$name
  # [1] "Astrid Welles"
  persona$age
  # [1] "32"
  persona$occupation
  # [1] "Museum Curator"

## End(Not run)
</code></pre>

<hr>
<h2 id='answer_as_regex_match'>Make LLM answer match a specific regex</h2><span id='topic+answer_as_regex_match'></span>

<h3>Description</h3>

<p>Make LLM answer match a specific regex
</p>


<h3>Usage</h3>

<pre><code class='language-R'>answer_as_regex_match(prompt, regex, mode = c("full_match", "extract_matches"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="answer_as_regex_match_+3A_prompt">prompt</code></td>
<td>
<p>A single string or a <code><a href="#topic+tidyprompt">tidyprompt()</a></code> object</p>
</td></tr>
<tr><td><code id="answer_as_regex_match_+3A_regex">regex</code></td>
<td>
<p>A character string specifying the regular expression the response must match</p>
</td></tr>
<tr><td><code id="answer_as_regex_match_+3A_mode">mode</code></td>
<td>
<p>A character string specifying the mode of the regex match. Options are
&quot;exact_match&quot; (default) and &quot;extract_all_matches&quot;.
Under &quot;full_match&quot;, the full LLM response must match the regex. If
it does not, the LLM will be sent feedback to retry. The full LLM response
will be returned if the regex is matched.
Under &quot;extract_matches&quot;, all matches of the regex in the LLM response will be returned
(if present). If the regex is not matched at all, the LLM will be sent feedback to retry.
If there is at least one match, the matches will be returned as a character vector</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="#topic+tidyprompt">tidyprompt()</a></code> with an added <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> which will ensure
that the LLM response matches the specified regex
</p>


<h3>See Also</h3>

<p>Other pre_built_prompt_wraps: 
<code><a href="#topic+add_text">add_text</a>()</code>,
<code><a href="#topic+answer_as_boolean">answer_as_boolean</a>()</code>,
<code><a href="#topic+answer_as_integer">answer_as_integer</a>()</code>,
<code><a href="#topic+answer_as_json">answer_as_json</a>()</code>,
<code><a href="#topic+answer_as_list">answer_as_list</a>()</code>,
<code><a href="#topic+answer_as_named_list">answer_as_named_list</a>()</code>,
<code><a href="#topic+answer_as_text">answer_as_text</a>()</code>,
<code><a href="#topic+answer_by_chain_of_thought">answer_by_chain_of_thought</a>()</code>,
<code><a href="#topic+answer_by_react">answer_by_react</a>()</code>,
<code><a href="#topic+answer_using_r">answer_using_r</a>()</code>,
<code><a href="#topic+answer_using_sql">answer_using_sql</a>()</code>,
<code><a href="#topic+answer_using_tools">answer_using_tools</a>()</code>,
<code><a href="#topic+prompt_wrap">prompt_wrap</a>()</code>,
<code><a href="#topic+quit_if">quit_if</a>()</code>,
<code><a href="#topic+set_system_prompt">set_system_prompt</a>()</code>
</p>
<p>Other answer_as_prompt_wraps: 
<code><a href="#topic+answer_as_boolean">answer_as_boolean</a>()</code>,
<code><a href="#topic+answer_as_integer">answer_as_integer</a>()</code>,
<code><a href="#topic+answer_as_json">answer_as_json</a>()</code>,
<code><a href="#topic+answer_as_list">answer_as_list</a>()</code>,
<code><a href="#topic+answer_as_named_list">answer_as_named_list</a>()</code>,
<code><a href="#topic+answer_as_text">answer_as_text</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  "What would be a suitable e-mail address for cupcake company?" |&gt;
    answer_as_regex_match("^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$") |&gt;
    send_prompt(llm_provider_ollama())
  # --- Sending request to LLM provider (llama3.1:8b): ---
  #   What would be a suitable e-mail address for cupcake company?
  #
  #   You must answer with a response that matches this regex format:
  #     ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$
  #     (use no other characters)
  # --- Receiving response from LLM provider: ---
  #   sweet.treats.cupcakes@gmail.com
  # [1] "sweet.treats.cupcakes@gmail.com"

  "What would be a suitable e-mail address for cupcake company?" |&gt;
    add_text("Give three ideas.") |&gt;
    answer_as_regex_match(
      "[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}",
      mode = "extract_matches"
    ) |&gt;
    send_prompt(llm_provider_ollama())
  # --- Sending request to LLM provider (llama3.1:8b): ---
  #   What would be a suitable e-mail address for cupcake company?
  #
  #   Give three ideas.
  #
  #   You must answer with a response that matches this regex format:
  #     [a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}
  # --- Receiving response from LLM provider: ---
  #   Here are three potential email addresses for a cupcake company:
  #
  #   1. sweettreats.cupcakes@yummail.com
  #   2. cupcakes.and.love@flourpower.net
  #   3. thecupcakery@gmail.com
  # [1] "sweettreats.cupcakes@yummail.com" "cupcakes.and.love@flourpower.net"
  # "thecupcakery@gmail.com"

## End(Not run)
</code></pre>

<hr>
<h2 id='answer_as_text'>Make LLM answer as a constrained text response</h2><span id='topic+answer_as_text'></span>

<h3>Description</h3>

<p>Make LLM answer as a constrained text response
</p>


<h3>Usage</h3>

<pre><code class='language-R'>answer_as_text(
  prompt,
  max_words = NULL,
  max_characters = NULL,
  add_instruction_to_prompt = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="answer_as_text_+3A_prompt">prompt</code></td>
<td>
<p>A single string or a <code><a href="#topic+tidyprompt">tidyprompt()</a></code> object</p>
</td></tr>
<tr><td><code id="answer_as_text_+3A_max_words">max_words</code></td>
<td>
<p>(optional) Maximum number of words allowed in the response.
If specified, responses exceeding this limit will fail validation</p>
</td></tr>
<tr><td><code id="answer_as_text_+3A_max_characters">max_characters</code></td>
<td>
<p>(optional) Maximum number of characters allowed in the response.
If specified, responses exceeding this limit will fail validation</p>
</td></tr>
<tr><td><code id="answer_as_text_+3A_add_instruction_to_prompt">add_instruction_to_prompt</code></td>
<td>
<p>(optional) Add instruction for replying
within the constraints to the prompt text. Set to FALSE for debugging if
extractions/validations are working as expected (without instruction the
answer should fail the validation function, initiating a retry)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="#topic+tidyprompt">tidyprompt()</a></code> with an added <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> which
will ensure that the LLM response conforms to the specified constraints
</p>


<h3>See Also</h3>

<p>Other pre_built_prompt_wraps: 
<code><a href="#topic+add_text">add_text</a>()</code>,
<code><a href="#topic+answer_as_boolean">answer_as_boolean</a>()</code>,
<code><a href="#topic+answer_as_integer">answer_as_integer</a>()</code>,
<code><a href="#topic+answer_as_json">answer_as_json</a>()</code>,
<code><a href="#topic+answer_as_list">answer_as_list</a>()</code>,
<code><a href="#topic+answer_as_named_list">answer_as_named_list</a>()</code>,
<code><a href="#topic+answer_as_regex_match">answer_as_regex_match</a>()</code>,
<code><a href="#topic+answer_by_chain_of_thought">answer_by_chain_of_thought</a>()</code>,
<code><a href="#topic+answer_by_react">answer_by_react</a>()</code>,
<code><a href="#topic+answer_using_r">answer_using_r</a>()</code>,
<code><a href="#topic+answer_using_sql">answer_using_sql</a>()</code>,
<code><a href="#topic+answer_using_tools">answer_using_tools</a>()</code>,
<code><a href="#topic+prompt_wrap">prompt_wrap</a>()</code>,
<code><a href="#topic+quit_if">quit_if</a>()</code>,
<code><a href="#topic+set_system_prompt">set_system_prompt</a>()</code>
</p>
<p>Other answer_as_prompt_wraps: 
<code><a href="#topic+answer_as_boolean">answer_as_boolean</a>()</code>,
<code><a href="#topic+answer_as_integer">answer_as_integer</a>()</code>,
<code><a href="#topic+answer_as_json">answer_as_json</a>()</code>,
<code><a href="#topic+answer_as_list">answer_as_list</a>()</code>,
<code><a href="#topic+answer_as_named_list">answer_as_named_list</a>()</code>,
<code><a href="#topic+answer_as_regex_match">answer_as_regex_match</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  "What is a large language model?" |&gt;
    answer_as_text(max_words = 10) |&gt;
    send_prompt()
  # --- Sending request to LLM provider (llama3.1:8b): ---
  # What is a large language model?
  #
  # You must provide a text response. The response must be at most 10 words.
  # --- Receiving response from LLM provider: ---
  # A type of AI that processes and generates human-like text.
  # [1] "A type of AI that processes and generates human-like text."

## End(Not run)
</code></pre>

<hr>
<h2 id='answer_by_chain_of_thought'>Set chain of thought mode for a prompt</h2><span id='topic+answer_by_chain_of_thought'></span>

<h3>Description</h3>

<p>This function enables chain of thought mode for evaluation of a prompt
or a <code><a href="#topic+tidyprompt">tidyprompt()</a></code>. In chain of thought mode, the large language model (LLM)
In chain of thought mode, the large language model (LLM) is asked to think
step by step to arrive at a final answer. It is hypothesized that this may
increase LLM performance at solving complex tasks. Chain of thought mode
is inspired by the method described in Wei et al. (2022).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>answer_by_chain_of_thought(
  prompt,
  extract_from_finish_brackets = TRUE,
  extraction_lenience = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="answer_by_chain_of_thought_+3A_prompt">prompt</code></td>
<td>
<p>A single string or a <code><a href="#topic+tidyprompt">tidyprompt()</a></code> object</p>
</td></tr>
<tr><td><code id="answer_by_chain_of_thought_+3A_extract_from_finish_brackets">extract_from_finish_brackets</code></td>
<td>
<p>A logical indicating whether the final answer
should be extracted from the text inside the &quot;FINISH<a href="base.html#topic+...">...</a>&quot; brackets.</p>
</td></tr>
<tr><td><code id="answer_by_chain_of_thought_+3A_extraction_lenience">extraction_lenience</code></td>
<td>
<p>A logical indcating whether the extraction function should be lenient.
If TRUE, the extraction function will attempt to extract the final answer
even if it cannot be extracted from within the brackets, by extracting
everything after the final occurence of 'FINISH' (if present). This may
be useful for smaller LLMs which may not follow the output format
as strictly</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="#topic+tidyprompt">tidyprompt()</a></code> with an added <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> which will ensure
that the LLM follows the chain of thought mode in answering the prompt
</p>


<h3>References</h3>

<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., &amp; Zhou, D. (2022).
Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.
<a href="doi:10.48550/arXiv.2201.11903">doi:10.48550/arXiv.2201.11903</a>
</p>


<h3>See Also</h3>

<p>Other pre_built_prompt_wraps: 
<code><a href="#topic+add_text">add_text</a>()</code>,
<code><a href="#topic+answer_as_boolean">answer_as_boolean</a>()</code>,
<code><a href="#topic+answer_as_integer">answer_as_integer</a>()</code>,
<code><a href="#topic+answer_as_json">answer_as_json</a>()</code>,
<code><a href="#topic+answer_as_list">answer_as_list</a>()</code>,
<code><a href="#topic+answer_as_named_list">answer_as_named_list</a>()</code>,
<code><a href="#topic+answer_as_regex_match">answer_as_regex_match</a>()</code>,
<code><a href="#topic+answer_as_text">answer_as_text</a>()</code>,
<code><a href="#topic+answer_by_react">answer_by_react</a>()</code>,
<code><a href="#topic+answer_using_r">answer_using_r</a>()</code>,
<code><a href="#topic+answer_using_sql">answer_using_sql</a>()</code>,
<code><a href="#topic+answer_using_tools">answer_using_tools</a>()</code>,
<code><a href="#topic+prompt_wrap">prompt_wrap</a>()</code>,
<code><a href="#topic+quit_if">quit_if</a>()</code>,
<code><a href="#topic+set_system_prompt">set_system_prompt</a>()</code>
</p>
<p>Other answer_by_prompt_wraps: 
<code><a href="#topic+answer_by_react">answer_by_react</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  "What is 5 + 5?" |&gt;
    answer_by_chain_of_thought() |&gt;
    answer_as_integer() |&gt;
    send_prompt()
  # --- Sending request to LLM provider (llama3.1:8b): ---
  # You are given a user's prompt.
  # To answer the user's prompt, you need to think step by step to arrive at a final answer.
  #
  # ----- START OF USER'S PROMPT -----
  # What is 5 + 5?
  #
  # You must answer with only an integer (use no other characters).
  # ----- END OF USER'S PROMPT -----
  #
  # What are the steps you would take to answer the user's prompt?
  # Describe your thought process in the following format:
  #   &gt;&gt; step 1: &lt;step 1 description&gt;
  #   &gt;&gt; step 2: &lt;step 2 description&gt;
  #   (etc.)
  #
  # When you are done, you must type:
  #   FINISH[&lt;put here your final answer to the user's prompt&gt;]
  #
  # Make sure your final answer follows the logical conclusion of your thought process.
  # --- Receiving response from LLM provider: ---
  # &gt;&gt; step 1: Recognize that the user's prompt is a simple arithmetic question asking for
  # the sum of two numbers.
  #
  # &gt;&gt; step 2: Identify the specific numbers mentioned in the prompt, which are 5 and 5.
  #
  # &gt;&gt; step 3: Recall basic addition facts from memory or knowledge.
  # In this case, knowing that 5 + 0 = 5, 5 + 1 = 6, 5 + 2 = 7, and so on up to 5 + 4 = 9,
  # it is clear that the next step in adding 5 to itself would be to keep the same sum.
  #
  # &gt;&gt; step 4: Apply the addition principle by considering the original number (5) as being
  # added to itself once more. Since 5 + 1 = 6 and the process of adding 1 increases the total
  # sum, then 5 + 2 must equal 7, thus indicating a direct progression in the sum as the additive
  # amount increases.
  #
  # &gt;&gt; step 5: From steps 3 and 4, conclude that since 5 added to itself (once) would be 10,
  # then when it is added again (to itself), it would continue from the last result which was
  # achieved by adding one more than the original number. Therefore, it must follow the pattern
  # established in step 3 where each additional amount increases the total sum.
  #
  # &gt;&gt; step 6: Now knowing that the operation of adding a second 5 to the first one results in
  # increasing the sum to the next integer after 10 (based on steps 3 and 4), determine what the
  # next integer is after 10. Knowing that integers are whole numbers without fractions, it's
  # clear that if you add another unit to 10, it becomes 11.
  #
  # FINISH[10]
  # [1] 10

## End(Not run)
</code></pre>

<hr>
<h2 id='answer_by_react'>Set ReAct mode for a prompt</h2><span id='topic+answer_by_react'></span>

<h3>Description</h3>

<p>This function enables ReAct mode for the evaluation of a prompt
or a <code><a href="#topic+tidyprompt">tidyprompt()</a></code>. In ReAct mode, the large language model (LLM) is asked to think step by step,
each time detailing a thought, action, and observation, to eventually arrive at
a final answer. It is hypothesized that this may increase LLM performance
at solving complex tasks. ReAct mode is inspired by the method described in
Yao et al. (2022).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>answer_by_react(
  prompt,
  extract_from_finish_brackets = TRUE,
  extraction_lenience = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="answer_by_react_+3A_prompt">prompt</code></td>
<td>
<p>A single string or a <code><a href="#topic+tidyprompt">tidyprompt()</a></code> object</p>
</td></tr>
<tr><td><code id="answer_by_react_+3A_extract_from_finish_brackets">extract_from_finish_brackets</code></td>
<td>
<p>A logical indicating whether the final answer
should be extracted from the text inside the &quot;FINISH<a href="base.html#topic+...">...</a>&quot; brackets</p>
</td></tr>
<tr><td><code id="answer_by_react_+3A_extraction_lenience">extraction_lenience</code></td>
<td>
<p>A logical indcating whether the extraction function should be lenient.
If TRUE, the extraction function will attempt to extract the final answer
even if it cannot be extracted from within the brackets, by extracting
everything after the final occurence of 'FINISH' (if present). This may
be useful for smaller LLMs which may not follow the output format
as strictly</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Please note that ReAct mode may be most useful if in combination
with tools that the LLM can use. See, for example, 'add_tools()' for
enabling R function calling, or, for example, 'answer_as_code()' with
'output_as_tool = TRUE' for enabling R code evaluation as a tool.
</p>


<h3>Value</h3>

<p>A <code><a href="#topic+tidyprompt">tidyprompt()</a></code> with an added <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> which will ensure
that the LLM follows the ReAct mode in answering the prompt
</p>


<h3>References</h3>

<p>Yao, S., Wu, Y., Cheung, W., Wang, Z., Narasimhan, K., &amp; Kong, L. (2022).
ReAct: Synergizing Reasoning and Acting in Language Models.
<a href="doi:10.48550/arXiv.2210.03629">doi:10.48550/arXiv.2210.03629</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+answer_using_tools">answer_using_tools()</a></code> <code><a href="#topic+answer_using_r">answer_using_r()</a></code>
</p>
<p>Other pre_built_prompt_wraps: 
<code><a href="#topic+add_text">add_text</a>()</code>,
<code><a href="#topic+answer_as_boolean">answer_as_boolean</a>()</code>,
<code><a href="#topic+answer_as_integer">answer_as_integer</a>()</code>,
<code><a href="#topic+answer_as_json">answer_as_json</a>()</code>,
<code><a href="#topic+answer_as_list">answer_as_list</a>()</code>,
<code><a href="#topic+answer_as_named_list">answer_as_named_list</a>()</code>,
<code><a href="#topic+answer_as_regex_match">answer_as_regex_match</a>()</code>,
<code><a href="#topic+answer_as_text">answer_as_text</a>()</code>,
<code><a href="#topic+answer_by_chain_of_thought">answer_by_chain_of_thought</a>()</code>,
<code><a href="#topic+answer_using_r">answer_using_r</a>()</code>,
<code><a href="#topic+answer_using_sql">answer_using_sql</a>()</code>,
<code><a href="#topic+answer_using_tools">answer_using_tools</a>()</code>,
<code><a href="#topic+prompt_wrap">prompt_wrap</a>()</code>,
<code><a href="#topic+quit_if">quit_if</a>()</code>,
<code><a href="#topic+set_system_prompt">set_system_prompt</a>()</code>
</p>
<p>Other answer_by_prompt_wraps: 
<code><a href="#topic+answer_by_chain_of_thought">answer_by_chain_of_thought</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  "What is 5 + 5?" |&gt;
    answer_by_react() |&gt;
    answer_as_integer() |&gt;
    send_prompt()
  # --- Sending request to LLM provider (llama3.1:8b): ---
  # You are given a user's prompt.
  # To answer the user's prompt, you need to think step by step,
  # take an action if needed, and then return a final answer.
  #
  # ----- START OF USER'S PROMPT -----
  # What is 5 + 5?
  #
  # You must answer with only an integer (use no other characters).
  # ----- END OF USER'S PROMPT -----
  #
  # Use the following structure:
  #   Thought: &lt;describe your thought process&gt;
  #   Action: &lt;if needed, describe the action you take (e.g., look up information)&gt;
  #   Observation: &lt;describe the result or observation from the action&gt;
  # (Repeat Thought -&gt; Action -&gt; Observation as necessary)
  #
  # When you are done, you must type:
  #   FINISH[&lt;put here your final answer to the user's prompt&gt;]
  #
  # Ensure your final answer aligns with your reasoning and observations.
  # --- Receiving response from LLM provider: ---
  # Thought: The problem is asking for the sum of two numbers, 5 and 5.
  #
  # Action: None needed, as this is a simple arithmetic operation that can be performed mentally.
  #
  # Observation: I can easily add these two numbers together in my mind to get the result.
  #
  # Thought: To find the sum, I will simply add the two numbers together: 5 + 5 = ?
  #
  # Action: Perform the addition.
  #
  # Observation: The result of adding 5 and 5 is 10.
  #
  # FINISH[10]
  # [1] 10

## End(Not run)
</code></pre>

<hr>
<h2 id='answer_using_r'>Enable LLM to draft and execute R code</h2><span id='topic+answer_using_r'></span>

<h3>Description</h3>

<p>This function adds a prompt wrap to a <code><a href="#topic+tidyprompt">tidyprompt()</a></code> that instructs the
LLM to answer the prompt with R code. There are various options to customize
the behavior of this prompt wrap, concerning the evaluation of the R code,
the packages that may be used, the objects that already exist in the R
session, and if the console output that should be sent back to the LLM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>answer_using_r(
  prompt,
  add_text = "You must code in the programming language 'R' to answer this prompt.",
  pkgs_to_use = c(),
  objects_to_use = list(),
  list_packages = TRUE,
  list_objects = TRUE,
  skim_dataframes = TRUE,
  evaluate_code = FALSE,
  r_session_options = list(),
  output_as_tool = FALSE,
  return_mode = c("full", "code", "console", "object", "formatted_output", "llm_answer")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="answer_using_r_+3A_prompt">prompt</code></td>
<td>
<p>A single string or a <code><a href="#topic+tidyprompt">tidyprompt()</a></code> object</p>
</td></tr>
<tr><td><code id="answer_using_r_+3A_add_text">add_text</code></td>
<td>
<p>Single string which will be added to the prompt text,
informing the LLM that they must code in R to answer the prompt</p>
</td></tr>
<tr><td><code id="answer_using_r_+3A_pkgs_to_use">pkgs_to_use</code></td>
<td>
<p>A character vector of package names that may be used
in the R code that the LLM will generate. If evaluating the R code, these
packages will be pre-loaded in the R session</p>
</td></tr>
<tr><td><code id="answer_using_r_+3A_objects_to_use">objects_to_use</code></td>
<td>
<p>A named list of objects that may be used in the R code
that the LLM will generate. If evaluating the R code, these objects will be pre-loaded
in the R session. The names of the list will be used as the object names in the
R session</p>
</td></tr>
<tr><td><code id="answer_using_r_+3A_list_packages">list_packages</code></td>
<td>
<p>Logical indicating whether the LLM should be informed
about the packages that may be used in their R code (if TRUE, a list of the
loaded packages will be shown in the initial prompt)</p>
</td></tr>
<tr><td><code id="answer_using_r_+3A_list_objects">list_objects</code></td>
<td>
<p>Logical indicating whether the LLM should be informed
about the existence of 'objects_to_use' (if TRUE, a list of the objects
plus their types will be shown in the initial prompt)</p>
</td></tr>
<tr><td><code id="answer_using_r_+3A_skim_dataframes">skim_dataframes</code></td>
<td>
<p>Logical indicating whether the LLM should be informed
about the structure of dataframes present in 'objects_to_use' (if TRUE,
a skim summary of each <code>data.frame</code> type object will be shown in the initial prompt).
This uses the function <code><a href="#topic+skim_with_labels_and_levels">skim_with_labels_and_levels()</a></code></p>
</td></tr>
<tr><td><code id="answer_using_r_+3A_evaluate_code">evaluate_code</code></td>
<td>
<p>Logical indicating whether the R code should be
evaluated. If TRUE, the R code will be evaluated in a separate R session
(using 'callr' to create an isolated R session via <a href="callr.html#topic+r_session">r_session</a>).
Note that setting this to 'TRUE' means that code generated by the LLM will
run on your system; use this setting with caution</p>
</td></tr>
<tr><td><code id="answer_using_r_+3A_r_session_options">r_session_options</code></td>
<td>
<p>A list of options to pass to the <a href="callr.html#topic+r_session">r_session</a>.
This can be used to customize the R session. See <a href="callr.html#topic+r_session_options">r_session_options</a>
for the available options. If no options are provided, the default options
will be used but with 'system_profile' and 'user_profile' set to FALSE</p>
</td></tr>
<tr><td><code id="answer_using_r_+3A_output_as_tool">output_as_tool</code></td>
<td>
<p>Logical indicating whether the console output of the
evaluated R code should be sent back to the LLM, meaning the LLM will use
R code as a tool to formulate a final answer to the prompt. If TRUE, the LLM
can decide if they can answer the prompt with the output, or if they need to modify
their R code. Once the LLM does not provide new R code (i.e., the prompt is being answered)
this prompt wrap will end (it will continue for as long as the LLM provides R code).
When this option is enabled, the resulting <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> will be of type 'tool'.
If TRUE, the return mode will also always be set to 'llm_answer'</p>
</td></tr>
<tr><td><code id="answer_using_r_+3A_return_mode">return_mode</code></td>
<td>
<p>Single string indicating the return mode. One of:
</p>

<ul>
<li><p> 'full': Return a list with the final LLM answer, the extracted R code,
and (if argument 'evaluate_code' is TRUE) the output of the R code
</p>
</li>
<li><p> 'code': Return only the extracted R code
</p>
</li>
<li><p> 'console': Return only the console output of the evaluated R code
</p>
</li>
<li><p> 'object': Return only the object produced by the evaluated R code
</p>
</li>
<li><p> 'formatted_output': Return a formatted string with the extracted R code
and its console output, and a print of the last object (this is identical to how it would
be presented to the LLM if 'output_as_tool' is TRUE)
</p>
</li>
<li><p> 'llm_answer': Return only the final LLM answer
</p>
</li></ul>

<p>When choosing 'console' or 'object', an additional instruction will be added to
the prompt text to inform the LLM about the expected output of the R code.
If 'output_as_tool' is TRUE, the return mode will always be set to 'llm_answer'
(as the LLM will be using the R code as a tool to answer the prompt)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the evaluation of the R code, the 'callr' package is required.
Please note: automatic evaluation of generated R code may be dangerous to your
system; you must use this function with caution.
</p>


<h3>Value</h3>

<p>A <code><a href="#topic+tidyprompt">tidyprompt()</a></code> object with the <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> added to it, which
will handle R code generation and possibly evaluation
</p>


<h3>See Also</h3>

<p><code><a href="#topic+answer_using_tools">answer_using_tools()</a></code>
</p>
<p>Other pre_built_prompt_wraps: 
<code><a href="#topic+add_text">add_text</a>()</code>,
<code><a href="#topic+answer_as_boolean">answer_as_boolean</a>()</code>,
<code><a href="#topic+answer_as_integer">answer_as_integer</a>()</code>,
<code><a href="#topic+answer_as_json">answer_as_json</a>()</code>,
<code><a href="#topic+answer_as_list">answer_as_list</a>()</code>,
<code><a href="#topic+answer_as_named_list">answer_as_named_list</a>()</code>,
<code><a href="#topic+answer_as_regex_match">answer_as_regex_match</a>()</code>,
<code><a href="#topic+answer_as_text">answer_as_text</a>()</code>,
<code><a href="#topic+answer_by_chain_of_thought">answer_by_chain_of_thought</a>()</code>,
<code><a href="#topic+answer_by_react">answer_by_react</a>()</code>,
<code><a href="#topic+answer_using_sql">answer_using_sql</a>()</code>,
<code><a href="#topic+answer_using_tools">answer_using_tools</a>()</code>,
<code><a href="#topic+prompt_wrap">prompt_wrap</a>()</code>,
<code><a href="#topic+quit_if">quit_if</a>()</code>,
<code><a href="#topic+set_system_prompt">set_system_prompt</a>()</code>
</p>
<p>Other answer_using_prompt_wraps: 
<code><a href="#topic+answer_using_sql">answer_using_sql</a>()</code>,
<code><a href="#topic+answer_using_tools">answer_using_tools</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  # Prompt to value calculated with R
  avg_miles_per_gallon &lt;- paste0(
    "Using my data,",
    " calculate the average miles per gallon (mpg) for cars with 6 cylinders."
  ) |&gt;
    answer_as_integer() |&gt;
    answer_using_r(
      pkgs_to_use = c("dplyr"),
      objects_to_use = list(mtcars = mtcars),
      evaluate_code = TRUE,
      output_as_tool = TRUE
    ) |&gt;
    send_prompt()
  avg_miles_per_gallon

  # Prompt to linear model object in R
  model &lt;- paste0(
    "Using my data, create a statistical model",
    " investigating the relationship between two variables."
  ) |&gt;
    answer_using_r(
      objects_to_use = list(data = mtcars),
      evaluate_code = TRUE,
      return_mode = "object"
    ) |&gt;
    prompt_wrap(
      validation_fn = function(x) {
        if (!inherits(x, "lm"))
          return(llm_feedback("The output should be a linear model object."))
        return(x)
      }
    ) |&gt;
    send_prompt()
  summary(model)

  # Prompt to plot object in R
  plot &lt;- paste0(
    "Create a scatter plot of miles per gallon (mpg) versus",
    " horsepower (hp) for the cars in my data.",
    " Use different colors to represent the number of cylinders (cyl).",
    " Be very creative and make the plot look nice but also a little crazy!"
  ) |&gt;
    answer_using_r(
      pkgs_to_use = c("ggplot2"),
      objects_to_use = list(mtcars = mtcars),
      evaluate_code = TRUE,
      return_mode = "object"
    ) |&gt;
    send_prompt()
  plot

## End(Not run)
</code></pre>

<hr>
<h2 id='answer_using_sql'>Enable LLM to draft and execute SQL queries on a database</h2><span id='topic+answer_using_sql'></span>

<h3>Description</h3>

<p>Enable LLM to draft and execute SQL queries on a database
</p>


<h3>Usage</h3>

<pre><code class='language-R'>answer_using_sql(
  prompt,
  add_text = paste0("You must code in SQL to answer this prompt.",
    " You must provide all SQL code between ```sql and ```.", "\n\n",
    "Never make assumptions about the possible values in the tables.\n",
    "Instead, execute SQL queries to retrieve information you need."),
  conn,
  list_tables = TRUE,
  describe_tables = TRUE,
  evaluate_code = FALSE,
  output_as_tool = FALSE,
  return_mode = c("full", "code", "object", "formatted_output", "llm_answer")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="answer_using_sql_+3A_prompt">prompt</code></td>
<td>
<p>A single string or a <code><a href="#topic+tidyprompt">tidyprompt()</a></code> object</p>
</td></tr>
<tr><td><code id="answer_using_sql_+3A_add_text">add_text</code></td>
<td>
<p>Single string which will be added to the prompt text,
informing the LLM that they must use SQL to answer the prompt</p>
</td></tr>
<tr><td><code id="answer_using_sql_+3A_conn">conn</code></td>
<td>
<p>A DBIConnection object to the SQL database</p>
</td></tr>
<tr><td><code id="answer_using_sql_+3A_list_tables">list_tables</code></td>
<td>
<p>Logical indicating whether to list tables available in the database
in the prompt text</p>
</td></tr>
<tr><td><code id="answer_using_sql_+3A_describe_tables">describe_tables</code></td>
<td>
<p>Logical indicating whether to describe the tables available in the database
in the prompt text. If TRUE, the columns of each table will be listed</p>
</td></tr>
<tr><td><code id="answer_using_sql_+3A_evaluate_code">evaluate_code</code></td>
<td>
<p>Logical indicating whether to evaluate the SQL code.
If TRUE, the SQL code will be executed on the database and the results will be returned.
Use with caution, as this allows the LLM to execute arbitrary SQL code</p>
</td></tr>
<tr><td><code id="answer_using_sql_+3A_output_as_tool">output_as_tool</code></td>
<td>
<p>Logical indicating whether to return the output as a tool result.
If TRUE, the output of the SQL query will be sent back to the LLM as a tool result.
The LLM can then provide a final answer or try another query. This can
continue until the LLM provides a final answer without any SQL code</p>
</td></tr>
<tr><td><code id="answer_using_sql_+3A_return_mode">return_mode</code></td>
<td>
<p>Character string indicating the return mode. Options are:
</p>

<ul>
<li><p> &quot;full&quot;: Return a list containing the SQL code, output, and formatted output
</p>
</li>
<li><p> &quot;code&quot;: Return only the SQL code
</p>
</li>
<li><p> &quot;object&quot;: Return only the query result object
</p>
</li>
<li><p> &quot;formatted_output&quot;: Return the formatted output: a string detailing the SQL code
and query result object.This is identical to how the LLM would see the output when
output_as_tool is TRUE
</p>
</li>
<li><p> &quot;llm_answer&quot;: Return the LLM answer. If output as tool is TRUE,
the return mode will always be &quot;llm_answer&quot; (since the LLM uses SQL to
provide a final answer)
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="#topic+tidyprompt">tidyprompt()</a></code> with an added <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> which will ensure
that the LLM will use SQL to answer the prompt
</p>


<h3>See Also</h3>

<p>Other pre_built_prompt_wraps: 
<code><a href="#topic+add_text">add_text</a>()</code>,
<code><a href="#topic+answer_as_boolean">answer_as_boolean</a>()</code>,
<code><a href="#topic+answer_as_integer">answer_as_integer</a>()</code>,
<code><a href="#topic+answer_as_json">answer_as_json</a>()</code>,
<code><a href="#topic+answer_as_list">answer_as_list</a>()</code>,
<code><a href="#topic+answer_as_named_list">answer_as_named_list</a>()</code>,
<code><a href="#topic+answer_as_regex_match">answer_as_regex_match</a>()</code>,
<code><a href="#topic+answer_as_text">answer_as_text</a>()</code>,
<code><a href="#topic+answer_by_chain_of_thought">answer_by_chain_of_thought</a>()</code>,
<code><a href="#topic+answer_by_react">answer_by_react</a>()</code>,
<code><a href="#topic+answer_using_r">answer_using_r</a>()</code>,
<code><a href="#topic+answer_using_tools">answer_using_tools</a>()</code>,
<code><a href="#topic+prompt_wrap">prompt_wrap</a>()</code>,
<code><a href="#topic+quit_if">quit_if</a>()</code>,
<code><a href="#topic+set_system_prompt">set_system_prompt</a>()</code>
</p>
<p>Other answer_using_prompt_wraps: 
<code><a href="#topic+answer_using_r">answer_using_r</a>()</code>,
<code><a href="#topic+answer_using_tools">answer_using_tools</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  # Create an in-memory SQLite database
  conn &lt;- DBI::dbConnect(RSQLite::SQLite(), ":memory:")

  # Create a sample table of customers
  DBI::dbExecute(conn, "
  CREATE TABLE
    customers (
      id INTEGER PRIMARY KEY,
      name TEXT,
      email TEXT,
      country TEXT
    );
  ")

  # Insert some sample customer data
  DBI::dbExecute(conn, "
  INSERT INTO
    customers (name, email, country)
  VALUES
    ('Alice', 'alice@example.com', 'USA'),
    ('Bob', 'bob@example.com', 'Canada'),
    ('Charlie', 'charlie@example.com', 'UK'),
    ('Diana', 'diana@example.com', 'USA');
  ")

  # Create another sample table for orders
  DBI::dbExecute(conn, "
  CREATE TABLE orders (
    order_id INTEGER PRIMARY KEY,
    customer_id INTEGER,
    product TEXT,
    amount REAL,
    order_date TEXT,
    FOREIGN KEY(customer_id) REFERENCES customers(id)
  );
  ")

  # Insert some sample orders
  DBI::dbExecute(conn, "
  INSERT INTO
    orders (customer_id, product, amount, order_date)
  VALUES
    (1, 'Widget', 19.99, '2024-01-15'),
    (1, 'Gadget', 29.99, '2024-01-17'),
    (2, 'Widget', 19.99, '2024-02-10'),
    (3, 'SuperWidget', 49.99, '2024-03-05'),
    (4, 'Gadget', 29.99, '2024-04-01'),
    (1, 'Thingamajig', 9.99, '2024-04-02');
  ")

  # Ask LLM a question which it will answer using the SQL database:
  "Where are my customers from?" |&gt;
    answer_using_sql(
      conn = conn,
      evaluate_code = TRUE,
      output_as_tool = TRUE
    ) |&gt;
    send_prompt(llm_provider_openai())
  # --- Sending request to LLM provider (gpt-4o-mini): ---
  # Where are my customers from?
  #
  # You must code in SQL to answer this prompt. You must provide all SQL code
  # between ```sql and ```.
  #
  # Never make assumptions about the possible values in the tables.
  # Instead, execute SQL queries to retrieve information you need.
  #
  # These tables are available in the database:
  #   customers, orders
  #
  # Table descriptions:
  #   - customers
  # Columns: id, name, email, country
  #
  # - orders
  # Columns: order_id, customer_id, product, amount, order_date
  #
  # Your SQL query will be executed on the database. The results will be sent back
  # to you. After seeing the results, you can either provide a final answer or try
  # another SQL query. When you provide your final answer, do not include any SQL code.
  # --- Receiving response from LLM provider: ---
  # ```sql
  # SELECT DISTINCT country FROM customers;
  # ```
  # --- Sending request to LLM provider (gpt-4o-mini): ---
  # --- SQL code: ---
  # SELECT DISTINCT country FROM customers;
  #
  # --- Query results: ---
  #   country
  # 1     USA
  # 2  Canada
  # 3      UK
  # --- Receiving response from LLM provider: ---
  # Based on the query results, your customers are from the following countries:
  # USA, Canada, and UK.
  # [1] "Based on the query results, your customers are from the following countries:
  # USA, Canada, and UK."

## End(Not run)
</code></pre>

<hr>
<h2 id='answer_using_tools'>Enable LLM to call R functions</h2><span id='topic+answer_using_tools'></span>

<h3>Description</h3>

<p>This function adds the ability for the a LLM to call R functions.
Users can specify a list of functions that the LLM can call, and the
prompt will be modified to include information, as well as an
accompanying extraction function to call the functions (handled by
<code><a href="#topic+send_prompt">send_prompt()</a></code>). Documentation for the functions is extracted from
the help file (if available), or from documentation added by
<code><a href="#topic+tools_add_docs">tools_add_docs()</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>answer_using_tools(
  prompt,
  tools = list(),
  type = c("text-based", "auto", "openai", "ollama")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="answer_using_tools_+3A_prompt">prompt</code></td>
<td>
<p>A single string or a <code><a href="#topic+tidyprompt">tidyprompt()</a></code> object</p>
</td></tr>
<tr><td><code id="answer_using_tools_+3A_tools">tools</code></td>
<td>
<p>An R function or a list of R functions that the LLM can call.
If the function has been documented in a help file (e.g., because it is part of a
package), the documentation will be parsed from the help file. If it is a custom
function, documentation should be added with <code><a href="#topic+tools_add_docs">tools_add_docs()</a></code></p>
</td></tr>
<tr><td><code id="answer_using_tools_+3A_type">type</code></td>
<td>
<p>(optional) The way that tool calling should be enabled.
'auto' will automatically determine the type based on 'llm_provider$api_type'
(note that this does not consider model compatibility, and could lead to errors;
set 'type' manually if errors occur). 'openai' and 'ollama' will set the
relevant API parameters. 'text-based' will provide function definitions
in the prompt, extract function calls from the LLM response, and call the
functions, providing the results back via <code><a href="#topic+llm_feedback">llm_feedback()</a></code>. 'text-based'
always works, but may be inefficient for APIs that support tool calling
natively. However, 'text-based' may be more reliable and flexible, especially
when combining with other prompt wraps. 'openai' and 'ollama' may not allow
for retries if the function call did not provide the expected result. Note that
when using 'openai' or 'ollama', tool calls are not counted as interactions and
may continue indefinitely (use with caution)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that this method of function calling is purely text-based.
This makes it suitable for any LLM and any LLM provider. However,
'native' function calling (where the LLM model provider restricts the
model to special tokens that can be used to call functions) may perform
better in terms of accuracy and efficiency. 'tidyprompt' may support
'native' function calling in the future
</p>


<h3>Value</h3>

<p>A <code><a href="#topic+tidyprompt">tidyprompt()</a></code> with an added <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> which
will allow the LLM to call R functions
</p>


<h3>See Also</h3>

<p><code><a href="#topic+answer_using_r">answer_using_r()</a></code> <code><a href="#topic+tools_get_docs">tools_get_docs()</a></code>
</p>
<p>Other pre_built_prompt_wraps: 
<code><a href="#topic+add_text">add_text</a>()</code>,
<code><a href="#topic+answer_as_boolean">answer_as_boolean</a>()</code>,
<code><a href="#topic+answer_as_integer">answer_as_integer</a>()</code>,
<code><a href="#topic+answer_as_json">answer_as_json</a>()</code>,
<code><a href="#topic+answer_as_list">answer_as_list</a>()</code>,
<code><a href="#topic+answer_as_named_list">answer_as_named_list</a>()</code>,
<code><a href="#topic+answer_as_regex_match">answer_as_regex_match</a>()</code>,
<code><a href="#topic+answer_as_text">answer_as_text</a>()</code>,
<code><a href="#topic+answer_by_chain_of_thought">answer_by_chain_of_thought</a>()</code>,
<code><a href="#topic+answer_by_react">answer_by_react</a>()</code>,
<code><a href="#topic+answer_using_r">answer_using_r</a>()</code>,
<code><a href="#topic+answer_using_sql">answer_using_sql</a>()</code>,
<code><a href="#topic+prompt_wrap">prompt_wrap</a>()</code>,
<code><a href="#topic+quit_if">quit_if</a>()</code>,
<code><a href="#topic+set_system_prompt">set_system_prompt</a>()</code>
</p>
<p>Other answer_using_prompt_wraps: 
<code><a href="#topic+answer_using_r">answer_using_r</a>()</code>,
<code><a href="#topic+answer_using_sql">answer_using_sql</a>()</code>
</p>
<p>Other tools: 
<code><a href="#topic+tools_add_docs">tools_add_docs</a>()</code>,
<code><a href="#topic+tools_get_docs">tools_get_docs</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  # When using functions from base R or R packages,
  #   documentation is automatically extracted from help files:
  "What are the files in my current directory?" |&gt;
    answer_using_tools(dir) |&gt; # 'dir' function is from base R
    send_prompt()

## End(Not run)

# Custom functions may also be provided;
#   in this case, some documentation is extracted from the function's formals;
#   descriptions may be added manually. See below

# Example fake weather function to add to the prompt:
temperature_in_location &lt;- function(
    location = c("Amsterdam", "Utrecht", "Enschede"),
    unit = c("Celcius", "Fahrenheit")
) {
  location &lt;- match.arg(location)
  unit &lt;- match.arg(unit)

  temperature_celcius &lt;- switch(
    location,
    "Amsterdam" = 32.5,
    "Utrecht" = 19.8,
    "Enschede" = 22.7
  )

  if (unit == "Celcius") {
    return(temperature_celcius)
  } else {
    return(temperature_celcius * 9/5 + 32)
  }
}

# Generate documentation for a function
#   (based on formals, &amp; help file if available)
docs &lt;- tools_get_docs(temperature_in_location)

# The types get inferred from the function's formals
# However, descriptions are still missing as the function is not from a package
# We can modify the documentation object to add descriptions:
docs$description &lt;- "Get the temperature in a location"
docs$arguments$unit$description &lt;- "Unit in which to return the temperature"
docs$arguments$location$description &lt;- "Location for which to return the temperature"
docs$return$description &lt;- "The temperature in the specified location and unit"
# (See `?tools_add_docs` for more details on the structure of the documentation)

# When we are satisfied with the documentation, we can add it to the function:
temperature_in_location &lt;- tools_add_docs(temperature_in_location, docs)

## Not run: 
  # Now the LLM can use the function:
  "Hi, what is the weather in Enschede? Give me Celcius degrees" |&gt;
    answer_using_tools(temperature_in_location) |&gt;
    send_prompt()

## End(Not run)
</code></pre>

<hr>
<h2 id='chat_history'>Create or validate <code>chat_history</code> object</h2><span id='topic+chat_history'></span>

<h3>Description</h3>

<p>This function creates and validates a <code>chat_history</code> object, ensuring that it matches
the expected format with 'role' and 'content' columns. It has separate methods
for <code>data.frame</code> and <code>character</code> inputs and includes a helper function to add a
system prompt to the chat history.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat_history(chat_history)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chat_history_+3A_chat_history">chat_history</code></td>
<td>
<p>A single string, a <code>data.frame</code> with 'role' and 'content' columns,
or NULL. If a <code>data.frame</code> is provided, it should contain 'role' and 'content' columns,
where 'role' is either 'user', 'assistant', or 'system', and 'content' is a character string
representing a chat message</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A valid chat history <code>data.frame</code> (of class <code>chat_history</code>)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>chat &lt;- "Hi there!" |&gt;
  chat_history()
chat

chat_from_df &lt;- data.frame(
  role = c("user", "assistant"),
  content = c("Hi there!", "Hello! How can I help you today?")
) |&gt;
  chat_history()
chat_from_df

# `add_msg_to_chat_history()` may be used to add messages to a chat history
chat_from_df &lt;- chat_from_df |&gt;
  add_msg_to_chat_history("Calculate 2+2 for me, please!")
chat_from_df

# You can also continue conversations which originate from `send_prompt()`:
## Not run: 
  result &lt;- "Hi there!" |&gt;
    send_prompt(return_mode = "full")
  # --- Sending request to LLM provider (llama3.1:8b): ---
  # Hi there!
  # --- Receiving response from LLM provider: ---
  # It's nice to meet you. Is there something I can help you with, or would you
  # like to chat?

  # Access the chat history from the result:
  chat_from_send_prompt &lt;- result$chat_history

  # Add a message to the chat history:
  chat_history_with_new_message &lt;- chat_from_send_prompt |&gt;
    add_msg_to_chat_history("Let's chat!")

  # The new chat history can be input for a new tidyprompt:
  prompt &lt;- tidyprompt(chat_history_with_new_message)

  # You can also take an existing tidyprompt and add the new chat history to it;
  #   this way, you can continue a conversation using the same prompt wraps
  prompt$set_chat_history(chat_history_with_new_message)

  # send_prompt() also accepts a chat history as input:
  new_result &lt;- chat_history_with_new_message |&gt;
    send_prompt(return_mode = "full")

  # You can also create a persistent chat history object from
  #   a chat history data frame; see ?`persistent_chat-class`
  chat &lt;- `persistent_chat-class`$new(llm_provider_ollama(), chat_from_send_prompt)
  chat$chat("Let's chat!")

## End(Not run)
</code></pre>

<hr>
<h2 id='chat_history.character'>Method for <code>chat_history()</code> when the input is a single string</h2><span id='topic+chat_history.character'></span>

<h3>Description</h3>

<p>Creates a <code>chat_history</code> object from a single string.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'character'
chat_history(chat_history)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chat_history.character_+3A_chat_history">chat_history</code></td>
<td>
<p>A single string</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A valid chat history <code>data.frame</code> (of class <code>chat_history</code>),
with the 'role' set to 'user' and the 'content' set to the input string
</p>

<hr>
<h2 id='chat_history.data.frame'>Method for <code>chat_history()</code> when the input is a <code>data.frame</code></h2><span id='topic+chat_history.data.frame'></span>

<h3>Description</h3>

<p>Creates a <code>chat_history</code> object from a data frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'data.frame'
chat_history(chat_history)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chat_history.data.frame_+3A_chat_history">chat_history</code></td>
<td>
<p>A data frame with 'role' and 'content' columns,
where 'role' is either 'user', 'assistant', or 'system', and 'content' is a character string
representing a chat message</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A valid chat history <code>data.frame</code> (of class <code>chat_history</code>),
with the 'role' and 'content' columns as specified in the input data frame
</p>

<hr>
<h2 id='chat_history.default'>Default method for <code>chat_history()</code></h2><span id='topic+chat_history.default'></span>

<h3>Description</h3>

<p>Calls error which indicates that the input was not a <code>character</code> or <code>data.frame</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
chat_history(chat_history)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="chat_history.default_+3A_chat_history">chat_history</code></td>
<td>
<p>Object which is not <code>character</code> or <code>data.frame</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>When input is a <code>character</code> or <code>data.frame</code>, the appropriate method will be called
(see '<code><a href="#topic+chat_history.character">chat_history.character()</a></code> and <code><a href="#topic+chat_history.data.frame">chat_history.data.frame()</a></code>).
</p>


<h3>Value</h3>

<p>No return value; an error is thrown
</p>

<hr>
<h2 id='construct_prompt_text'>Construct prompt text from a <a href="#topic+tidyprompt-class">tidyprompt</a> object</h2><span id='topic+construct_prompt_text'></span>

<h3>Description</h3>

<p>Construct prompt text from a <a href="#topic+tidyprompt-class">tidyprompt</a> object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>construct_prompt_text(x, llm_provider = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="construct_prompt_text_+3A_x">x</code></td>
<td>
<p>A <a href="#topic+tidyprompt-class">tidyprompt</a> object</p>
</td></tr>
<tr><td><code id="construct_prompt_text_+3A_llm_provider">llm_provider</code></td>
<td>
<p>An optional <a href="#topic+llm_provider-class">llm_provider</a> object.
This may sometimes affect the prompt text construction</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The constructed prompt text
</p>


<h3>See Also</h3>

<p>Other tidyprompt: 
<code><a href="#topic+get_chat_history">get_chat_history</a>()</code>,
<code><a href="#topic+get_prompt_wraps">get_prompt_wraps</a>()</code>,
<code><a href="#topic+is_tidyprompt">is_tidyprompt</a>()</code>,
<code><a href="#topic+set_chat_history">set_chat_history</a>()</code>,
<code><a href="#topic+tidyprompt">tidyprompt</a>()</code>,
<code><a href="#topic+tidyprompt-class">tidyprompt-class</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prompt &lt;- tidyprompt("Hi!")
print(prompt)

# Add to a tidyprompt using a prompt wrap:
prompt &lt;- tidyprompt("Hi!") |&gt;
  add_text("How are you?")
print(prompt)

# Strings can be input for prompt wraps; therefore,
#   a call to tidyprompt() is not necessary:
prompt &lt;- "Hi" |&gt;
  add_text("How are you?")

# Example of adding extraction &amp; validation with a prompt_wrap():
prompt &lt;- "Hi" |&gt;
  add_text("What is 5 + 5?") |&gt;
  answer_as_integer()

## Not run: 
  # tidyprompt objects are evaluated by send_prompt(), which will
  #   handle construct the prompt text, send it to the LLM provider,
  #   and apply the extraction and validation functions from the tidyprompt object
  prompt |&gt;
    send_prompt(llm_provider_ollama())
  # --- Sending request to LLM provider (llama3.1:8b): ---
  #   Hi
  #
  #   What is 5 + 5?
  #
  #   You must answer with only an integer (use no other characters).
  # --- Receiving response from LLM provider: ---
  #   10
  # [1] 10

  # See prompt_wrap() and send_prompt() for more details

## End(Not run)

# `tidyprompt` objects may be validated with these helpers:
is_tidyprompt(prompt) # Returns TRUE if input is a valid tidyprompt object

# Get base prompt text
base_prompt &lt;- prompt$base_prompt

# Get all prompt wraps
prompt_wraps &lt;- prompt$get_prompt_wraps()
# Alternative:
prompt_wraps &lt;- get_prompt_wraps(prompt)

# Construct prompt text
prompt_text &lt;- prompt$construct_prompt_text()
# Alternative:
prompt_text &lt;- construct_prompt_text(prompt)

# Set chat history (affecting also the base prompt)
chat_history &lt;- data.frame(
  role = c("user", "assistant", "user"),
  content = c("What is 5 + 5?", "10", "And what is 5 + 6?")
)
prompt$set_chat_history(chat_history)

# Get chat history
chat_history &lt;- prompt$get_chat_history()
</code></pre>

<hr>
<h2 id='df_to_string'>Convert a dataframe to a string representation</h2><span id='topic+df_to_string'></span>

<h3>Description</h3>

<p>Converts a data frame to a string format, intended for sending it to a LLM
(or for display or logging).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>df_to_string(df, how = c("wide", "long"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="df_to_string_+3A_df">df</code></td>
<td>
<p>A <code>data.frame</code> object to be converted to a string</p>
</td></tr>
<tr><td><code id="df_to_string_+3A_how">how</code></td>
<td>
<p>In what way the df should be converted to a string;
either &quot;wide&quot; or &quot;long&quot;. &quot;wide&quot; presents column names on the first row,
followed by the row values on each new row. &quot;long&quot; presents the values
of each row together with the column names, repeating for every row
after two lines of whitespace</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A single string representing the df
</p>


<h3>See Also</h3>

<p>Other text_helpers: 
<code><a href="#topic+skim_with_labels_and_levels">skim_with_labels_and_levels</a>()</code>,
<code><a href="#topic+vector_list_to_string">vector_list_to_string</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cars |&gt;
  head(5) |&gt;
  df_to_string(how = "wide")

cars |&gt;
  head(5) |&gt;
  df_to_string(how = "long")
</code></pre>

<hr>
<h2 id='extract_from_return_list'>Function to extract a specific element from a list</h2><span id='topic+extract_from_return_list'></span>

<h3>Description</h3>

<p>This function is intended as a helper function for piping with output from
<code><a href="#topic+send_prompt">send_prompt()</a></code> when using <code>return_mode = "full"</code>. It allows to
extract a specific element from the list returned by <code><a href="#topic+send_prompt">send_prompt()</a></code>, which
can be useful for further piping.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_from_return_list(list, name_of_element = "response")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="extract_from_return_list_+3A_list">list</code></td>
<td>
<p>A list, typically the output from <code><a href="#topic+send_prompt">send_prompt()</a></code> with <code>return_mode = "full"</code></p>
</td></tr>
<tr><td><code id="extract_from_return_list_+3A_name_of_element">name_of_element</code></td>
<td>
<p>A character string with the name of the element to extract from the list</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The extracted element from the list
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  response &lt;- "Hi!" |&gt;
    send_prompt(llm_provider_ollama(), return_mode = "full") |&gt;
    extract_from_return_list("response")
  response
  # [1] "It's nice to meet you. Is there something I can help you with,
  # or would you like to chat?"

## End(Not run)
</code></pre>

<hr>
<h2 id='get_chat_history'>Get the chat history of a <a href="#topic+tidyprompt-class">tidyprompt</a> object</h2><span id='topic+get_chat_history'></span>

<h3>Description</h3>

<p>This function gets the chat history of the <a href="#topic+tidyprompt-class">tidyprompt</a> object.
The chat history is constructed from the base prompt, system prompt, and chat
history field. The returned object will be the chat history
with the system prompt as the first message with role 'system' and the
the base prompt as the last message with role 'user'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_chat_history(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_chat_history_+3A_x">x</code></td>
<td>
<p>A <a href="#topic+tidyprompt-class">tidyprompt</a> object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataframe containing the chat history
</p>


<h3>See Also</h3>

<p><code><a href="#topic+chat_history">chat_history()</a></code>
</p>
<p>Other tidyprompt: 
<code><a href="#topic+construct_prompt_text">construct_prompt_text</a>()</code>,
<code><a href="#topic+get_prompt_wraps">get_prompt_wraps</a>()</code>,
<code><a href="#topic+is_tidyprompt">is_tidyprompt</a>()</code>,
<code><a href="#topic+set_chat_history">set_chat_history</a>()</code>,
<code><a href="#topic+tidyprompt">tidyprompt</a>()</code>,
<code><a href="#topic+tidyprompt-class">tidyprompt-class</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prompt &lt;- tidyprompt("Hi!")
print(prompt)

# Add to a tidyprompt using a prompt wrap:
prompt &lt;- tidyprompt("Hi!") |&gt;
  add_text("How are you?")
print(prompt)

# Strings can be input for prompt wraps; therefore,
#   a call to tidyprompt() is not necessary:
prompt &lt;- "Hi" |&gt;
  add_text("How are you?")

# Example of adding extraction &amp; validation with a prompt_wrap():
prompt &lt;- "Hi" |&gt;
  add_text("What is 5 + 5?") |&gt;
  answer_as_integer()

## Not run: 
  # tidyprompt objects are evaluated by send_prompt(), which will
  #   handle construct the prompt text, send it to the LLM provider,
  #   and apply the extraction and validation functions from the tidyprompt object
  prompt |&gt;
    send_prompt(llm_provider_ollama())
  # --- Sending request to LLM provider (llama3.1:8b): ---
  #   Hi
  #
  #   What is 5 + 5?
  #
  #   You must answer with only an integer (use no other characters).
  # --- Receiving response from LLM provider: ---
  #   10
  # [1] 10

  # See prompt_wrap() and send_prompt() for more details

## End(Not run)

# `tidyprompt` objects may be validated with these helpers:
is_tidyprompt(prompt) # Returns TRUE if input is a valid tidyprompt object

# Get base prompt text
base_prompt &lt;- prompt$base_prompt

# Get all prompt wraps
prompt_wraps &lt;- prompt$get_prompt_wraps()
# Alternative:
prompt_wraps &lt;- get_prompt_wraps(prompt)

# Construct prompt text
prompt_text &lt;- prompt$construct_prompt_text()
# Alternative:
prompt_text &lt;- construct_prompt_text(prompt)

# Set chat history (affecting also the base prompt)
chat_history &lt;- data.frame(
  role = c("user", "assistant", "user"),
  content = c("What is 5 + 5?", "10", "And what is 5 + 6?")
)
prompt$set_chat_history(chat_history)

# Get chat history
chat_history &lt;- prompt$get_chat_history()
</code></pre>

<hr>
<h2 id='get_prompt_wraps'>Get prompt wraps from a <a href="#topic+tidyprompt-class">tidyprompt</a> object</h2><span id='topic+get_prompt_wraps'></span>

<h3>Description</h3>

<p>Get prompt wraps from a <a href="#topic+tidyprompt-class">tidyprompt</a> object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_prompt_wraps(x, order = c("default", "modification", "evaluation"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_prompt_wraps_+3A_x">x</code></td>
<td>
<p>A <a href="#topic+tidyprompt-class">tidyprompt</a> object</p>
</td></tr>
<tr><td><code id="get_prompt_wraps_+3A_order">order</code></td>
<td>
<p>The order to return the wraps. Options are:
</p>

<ul>
<li><p> &quot;default&quot;: as originally added to the object
</p>
</li>
<li><p> &quot;modification&quot;: as ordered for modification of the base prompt;
ordered by type: check, unspecified, mode, tool, break. This is the order
in which prompt wraps are applied during <code><a href="#topic+construct_prompt_text">construct_prompt_text()</a></code>
</p>
</li>
<li><p> &quot;evaluation&quot;: ordered for evaluation of the LLM response;
ordered by type: tool, mode, break, unspecified, check. This is the order
in which wraps are applied to the LLM output during <code><a href="#topic+send_prompt">send_prompt()</a></code>
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of prompt wrap objects (see <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code>)
</p>


<h3>See Also</h3>

<p>Other tidyprompt: 
<code><a href="#topic+construct_prompt_text">construct_prompt_text</a>()</code>,
<code><a href="#topic+get_chat_history">get_chat_history</a>()</code>,
<code><a href="#topic+is_tidyprompt">is_tidyprompt</a>()</code>,
<code><a href="#topic+set_chat_history">set_chat_history</a>()</code>,
<code><a href="#topic+tidyprompt">tidyprompt</a>()</code>,
<code><a href="#topic+tidyprompt-class">tidyprompt-class</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prompt &lt;- tidyprompt("Hi!")
print(prompt)

# Add to a tidyprompt using a prompt wrap:
prompt &lt;- tidyprompt("Hi!") |&gt;
  add_text("How are you?")
print(prompt)

# Strings can be input for prompt wraps; therefore,
#   a call to tidyprompt() is not necessary:
prompt &lt;- "Hi" |&gt;
  add_text("How are you?")

# Example of adding extraction &amp; validation with a prompt_wrap():
prompt &lt;- "Hi" |&gt;
  add_text("What is 5 + 5?") |&gt;
  answer_as_integer()

## Not run: 
  # tidyprompt objects are evaluated by send_prompt(), which will
  #   handle construct the prompt text, send it to the LLM provider,
  #   and apply the extraction and validation functions from the tidyprompt object
  prompt |&gt;
    send_prompt(llm_provider_ollama())
  # --- Sending request to LLM provider (llama3.1:8b): ---
  #   Hi
  #
  #   What is 5 + 5?
  #
  #   You must answer with only an integer (use no other characters).
  # --- Receiving response from LLM provider: ---
  #   10
  # [1] 10

  # See prompt_wrap() and send_prompt() for more details

## End(Not run)

# `tidyprompt` objects may be validated with these helpers:
is_tidyprompt(prompt) # Returns TRUE if input is a valid tidyprompt object

# Get base prompt text
base_prompt &lt;- prompt$base_prompt

# Get all prompt wraps
prompt_wraps &lt;- prompt$get_prompt_wraps()
# Alternative:
prompt_wraps &lt;- get_prompt_wraps(prompt)

# Construct prompt text
prompt_text &lt;- prompt$construct_prompt_text()
# Alternative:
prompt_text &lt;- construct_prompt_text(prompt)

# Set chat history (affecting also the base prompt)
chat_history &lt;- data.frame(
  role = c("user", "assistant", "user"),
  content = c("What is 5 + 5?", "10", "And what is 5 + 6?")
)
prompt$set_chat_history(chat_history)

# Get chat history
chat_history &lt;- prompt$get_chat_history()
</code></pre>

<hr>
<h2 id='is_tidyprompt'>Check if object is a <a href="#topic+tidyprompt-class">tidyprompt</a> object</h2><span id='topic+is_tidyprompt'></span>

<h3>Description</h3>

<p>Check if object is a <a href="#topic+tidyprompt-class">tidyprompt</a> object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_tidyprompt(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is_tidyprompt_+3A_x">x</code></td>
<td>
<p>An object to check</p>
</td></tr>
</table>


<h3>Value</h3>

<p>TRUE if the object is a valid <a href="#topic+tidyprompt-class">tidyprompt</a> object, otherwise FALSE
</p>


<h3>See Also</h3>

<p>Other tidyprompt: 
<code><a href="#topic+construct_prompt_text">construct_prompt_text</a>()</code>,
<code><a href="#topic+get_chat_history">get_chat_history</a>()</code>,
<code><a href="#topic+get_prompt_wraps">get_prompt_wraps</a>()</code>,
<code><a href="#topic+set_chat_history">set_chat_history</a>()</code>,
<code><a href="#topic+tidyprompt">tidyprompt</a>()</code>,
<code><a href="#topic+tidyprompt-class">tidyprompt-class</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prompt &lt;- tidyprompt("Hi!")
print(prompt)

# Add to a tidyprompt using a prompt wrap:
prompt &lt;- tidyprompt("Hi!") |&gt;
  add_text("How are you?")
print(prompt)

# Strings can be input for prompt wraps; therefore,
#   a call to tidyprompt() is not necessary:
prompt &lt;- "Hi" |&gt;
  add_text("How are you?")

# Example of adding extraction &amp; validation with a prompt_wrap():
prompt &lt;- "Hi" |&gt;
  add_text("What is 5 + 5?") |&gt;
  answer_as_integer()

## Not run: 
  # tidyprompt objects are evaluated by send_prompt(), which will
  #   handle construct the prompt text, send it to the LLM provider,
  #   and apply the extraction and validation functions from the tidyprompt object
  prompt |&gt;
    send_prompt(llm_provider_ollama())
  # --- Sending request to LLM provider (llama3.1:8b): ---
  #   Hi
  #
  #   What is 5 + 5?
  #
  #   You must answer with only an integer (use no other characters).
  # --- Receiving response from LLM provider: ---
  #   10
  # [1] 10

  # See prompt_wrap() and send_prompt() for more details

## End(Not run)

# `tidyprompt` objects may be validated with these helpers:
is_tidyprompt(prompt) # Returns TRUE if input is a valid tidyprompt object

# Get base prompt text
base_prompt &lt;- prompt$base_prompt

# Get all prompt wraps
prompt_wraps &lt;- prompt$get_prompt_wraps()
# Alternative:
prompt_wraps &lt;- get_prompt_wraps(prompt)

# Construct prompt text
prompt_text &lt;- prompt$construct_prompt_text()
# Alternative:
prompt_text &lt;- construct_prompt_text(prompt)

# Set chat history (affecting also the base prompt)
chat_history &lt;- data.frame(
  role = c("user", "assistant", "user"),
  content = c("What is 5 + 5?", "10", "And what is 5 + 6?")
)
prompt$set_chat_history(chat_history)

# Get chat history
chat_history &lt;- prompt$get_chat_history()
</code></pre>

<hr>
<h2 id='llm_break'>Create an <code>llm_break</code> object</h2><span id='topic+llm_break'></span>

<h3>Description</h3>

<p>This object is used to break a extraction and validation loop defined in a <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code>
as evaluated by <code><a href="#topic+send_prompt">send_prompt()</a></code>. When an extraction or validation function returns
this object, the loop will be broken and no further extraction or validation
functions are applied; instead, <code><a href="#topic+send_prompt">send_prompt()</a></code> will be able to return
the result at that point. This may be useful in scenarios where
it is determined the LLM is unable to provide a response to a prompt.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>llm_break(object_to_return = NULL, success = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="llm_break_+3A_object_to_return">object_to_return</code></td>
<td>
<p>The object to return as the response result
from <code><a href="#topic+send_prompt">send_prompt()</a></code> when this object is returned from an extraction or
validation function.</p>
</td></tr>
<tr><td><code id="llm_break_+3A_success">success</code></td>
<td>
<p>A logical indicating whether the <code><a href="#topic+send_prompt">send_prompt()</a></code> loop break
should nonetheless be considered as a successful completion of the
extraction and validation process.
If <code>FALSE</code>, the <code>object_to_return</code> must
be <code>NULL</code> (as the response result of <code><a href="#topic+send_prompt">send_prompt()</a></code> will always be 'NULL'
when the evaluation was unsuccessful); if <code>FALSE</code>, <code><a href="#topic+send_prompt">send_prompt()</a></code> will also
print a warning about the unsuccessful evaluation.
If <code>TRUE</code>, the <code>object_to_return</code>
will be returned as the response result of <code><a href="#topic+send_prompt">send_prompt()</a></code> (and <code><a href="#topic+send_prompt">send_prompt()</a></code>)
will print no warning about unsuccessful evaluation).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An list of class &quot;llm_break&quot; containing the object to return and
a logical indicating whether the evaluation was successful
</p>


<h3>See Also</h3>

<p><code><a href="#topic+llm_feedback">llm_feedback()</a></code>
</p>
<p>Other prompt_wrap: 
<code><a href="#topic+llm_feedback">llm_feedback</a>()</code>,
<code><a href="#topic+prompt_wrap">prompt_wrap</a>()</code>
</p>
<p>Other prompt_evaluation: 
<code><a href="#topic+llm_feedback">llm_feedback</a>()</code>,
<code><a href="#topic+send_prompt">send_prompt</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example usage within an extraction function similar to the one in 'quit_if()':
extraction_fn &lt;- function(x) {
  quit_detect_regex &lt;- "NO ANSWER"

  if (grepl(quit_detect_regex, x)) {
      return(llm_break(
        object_to_return = NULL,
        success = TRUE
      ))
  }

  return(x)
}

## Not run: 
  result &lt;- "How many months old is the cat of my uncle?" |&gt;
    answer_as_integer() |&gt;
    prompt_wrap(
      modify_fn = function(prompt) {
        paste0(
          prompt, "\n\n",
          "Type only 'NO ANSWER' if you do not know."
        )
      },
      extraction_fn = extraction_fn,
      type = "break"
    ) |&gt;
    send_prompt()
  result
  # NULL

## End(Not run)
</code></pre>

<hr>
<h2 id='llm_feedback'>Create an <code>llm_feedback</code> object</h2><span id='topic+llm_feedback'></span>

<h3>Description</h3>

<p>This object is used to send feedback to a LLM when a LLM reply
does not succesfully pass an extraction or validation function
(as handled by <code><a href="#topic+send_prompt">send_prompt()</a></code> and defined using <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code>).
The feedback text is sent back to the LLM. The extraction or validation function
should then return this object with the feedback text that should be sent to the LLM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>llm_feedback(text, tool_result = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="llm_feedback_+3A_text">text</code></td>
<td>
<p>A character string containing the feedback text. This will
be sent back to the LLM after not passing an extractor or validator function</p>
</td></tr>
<tr><td><code id="llm_feedback_+3A_tool_result">tool_result</code></td>
<td>
<p>A logical indicating whether the feedback is a tool result.
If TRUE, <code><a href="#topic+send_prompt">send_prompt()</a></code> will not remove it from the chat history when
cleaning the context window during repeated interactions</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class &quot;llm_feedback&quot; (or &quot;llm_feedback_tool_result&quot;)
containing the feedback text to send back to the LLM
</p>


<h3>See Also</h3>

<p><code><a href="#topic+llm_break">llm_break()</a></code>
</p>
<p>Other prompt_wrap: 
<code><a href="#topic+llm_break">llm_break</a>()</code>,
<code><a href="#topic+prompt_wrap">prompt_wrap</a>()</code>
</p>
<p>Other prompt_evaluation: 
<code><a href="#topic+llm_break">llm_break</a>()</code>,
<code><a href="#topic+send_prompt">send_prompt</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example usage within a validation function similar to the one in 'answer_as_integer()':
validation_fn &lt;- function(x, min = 0, max = 100) {
  if (x != floor(x)) { # Not a whole number
    return(llm_feedback(
      "You must answer with only an integer (use no other characters)."
    ))
  }
  if (!is.null(min) &amp;&amp; x &lt; min) {
    return(llm_feedback(glue::glue(
      "The number should be greater than or equal to {min}."
    )))
  }
  if (!is.null(max) &amp;&amp; x &gt; max) {
    return(llm_feedback(glue::glue(
      "The number should be less than or equal to {max}."
    )))
  }
  return(TRUE)
}

# This validation_fn would be part of a prompt_wrap();
#   see the `answer_as_integer()` function for an example of how to use it
</code></pre>

<hr>
<h2 id='llm_provider_google_gemini'>Create a new Google Gemini LLM provider</h2><span id='topic+llm_provider_google_gemini'></span>

<h3>Description</h3>

<p>This function creates a new <a href="#topic+llm_provider-class">llm_provider</a> object that interacts with the Google Gemini API.
Streaming is not yet supported in this implementation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>llm_provider_google_gemini(
  parameters = list(model = "gemini-1.5-flash"),
  verbose = getOption("tidyprompt.verbose", TRUE),
  url = "https://generativelanguage.googleapis.com/v1beta/models/",
  api_key = Sys.getenv("GOOGLE_AI_STUDIO_API_KEY")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="llm_provider_google_gemini_+3A_parameters">parameters</code></td>
<td>
<p>A named list of parameters. Currently the following parameters are required:
</p>

<ul>
<li><p> model: The name of the model to use (see: https://ai.google.dev/gemini-api/docs/models/gemini)
</p>
</li></ul>

<p>Additional parameters are appended to the request body; see the Google AI Studio API
documentation for more information: https://ai.google.dev/gemini-api/docs/text-generation
and https://github.com/google/generative-ai-docs/blob/main/site/en/gemini-api/docs/get-started/rest.ipynb</p>
</td></tr>
<tr><td><code id="llm_provider_google_gemini_+3A_verbose">verbose</code></td>
<td>
<p>A logical indicating whether the interaction with the LLM provider
should be printed to the console</p>
</td></tr>
<tr><td><code id="llm_provider_google_gemini_+3A_url">url</code></td>
<td>
<p>The URL to the Google Gemini API endpoint for chat completion</p>
</td></tr>
<tr><td><code id="llm_provider_google_gemini_+3A_api_key">api_key</code></td>
<td>
<p>The API key to use for authentication with the Google Gemini API
(see: https://aistudio.google.com/app/apikey)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new <a href="#topic+llm_provider-class">llm_provider</a> object for use of the Google Gemini API
</p>


<h3>See Also</h3>

<p>Other llm_provider: 
<code><a href="#topic+llm_provider-class">llm_provider-class</a></code>,
<code><a href="#topic+llm_provider_groq">llm_provider_groq</a>()</code>,
<code><a href="#topic+llm_provider_mistral">llm_provider_mistral</a>()</code>,
<code><a href="#topic+llm_provider_ollama">llm_provider_ollama</a>()</code>,
<code><a href="#topic+llm_provider_openai">llm_provider_openai</a>()</code>,
<code><a href="#topic+llm_provider_openrouter">llm_provider_openrouter</a>()</code>,
<code><a href="#topic+llm_provider_xai">llm_provider_xai</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Various providers:
ollama &lt;- llm_provider_ollama()
openai &lt;- llm_provider_openai()
openrouter &lt;- llm_provider_openrouter()
mistral &lt;- llm_provider_mistral()
groq &lt;- llm_provider_groq()
xai &lt;- llm_provider_xai()
gemini &lt;- llm_provider_google_gemini()

# Initialize with settings:
ollama &lt;- llm_provider_ollama(
  parameters = list(
    model = "llama3.2:3b",
    stream = TRUE
  ),
  verbose = TRUE,
  url = "http://localhost:11434/api/chat"
)

# Change settings:
ollama$verbose &lt;- FALSE
ollama$parameters$stream &lt;- FALSE
ollama$parameters$model &lt;- "llama3.1:8b"

## Not run: 
  # Try a simple chat message with '$complete_chat()':
  response &lt;- ollama$complete_chat("Hi!")
  response
  # $role
  # [1] "assistant"
  #
  # $content
  # [1] "How's it going? Is there something I can help you with or would you like
  # to chat?"
  #
  # $http
  # Response [http://localhost:11434/api/chat]
  # Date: 2024-11-18 14:21
  # Status: 200
  # Content-Type: application/json; charset=utf-8
  # Size: 375 B

  # Use with send_prompt():
  "Hi" |&gt;
    send_prompt(ollama)
  # [1] "How's your day going so far? Is there something I can help you with or
  # would you like to chat?"

## End(Not run)
</code></pre>

<hr>
<h2 id='llm_provider_groq'>Create a new Groq LLM provider</h2><span id='topic+llm_provider_groq'></span>

<h3>Description</h3>

<p>This function creates a new <a href="#topic+llm_provider-class">llm_provider</a> object that interacts with the Groq API.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>llm_provider_groq(
  parameters = list(model = "llama-3.1-8b-instant", stream = TRUE),
  verbose = getOption("tidyprompt.verbose", TRUE),
  url = "https://api.groq.com/openai/v1/chat/completions",
  api_key = Sys.getenv("GROQ_API_KEY")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="llm_provider_groq_+3A_parameters">parameters</code></td>
<td>
<p>A named list of parameters. Currently the following parameters are required:
</p>

<ul>
<li><p> model: The name of the model to use
</p>
</li>
<li><p> stream: A logical indicating whether the API should stream responses
</p>
</li></ul>

<p>Additional parameters are appended to the request body; see the Groq API
documentation for more information: https://console.groq.com/docs/api-reference#chat-create</p>
</td></tr>
<tr><td><code id="llm_provider_groq_+3A_verbose">verbose</code></td>
<td>
<p>A logical indicating whether the interaction with the LLM provider
should be printed to the console</p>
</td></tr>
<tr><td><code id="llm_provider_groq_+3A_url">url</code></td>
<td>
<p>The URL to the Groq API endpoint for chat completion</p>
</td></tr>
<tr><td><code id="llm_provider_groq_+3A_api_key">api_key</code></td>
<td>
<p>The API key to use for authentication with the Groq API</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new <a href="#topic+llm_provider-class">llm_provider</a> object for use of the Groq API
</p>


<h3>See Also</h3>

<p>Other llm_provider: 
<code><a href="#topic+llm_provider-class">llm_provider-class</a></code>,
<code><a href="#topic+llm_provider_google_gemini">llm_provider_google_gemini</a>()</code>,
<code><a href="#topic+llm_provider_mistral">llm_provider_mistral</a>()</code>,
<code><a href="#topic+llm_provider_ollama">llm_provider_ollama</a>()</code>,
<code><a href="#topic+llm_provider_openai">llm_provider_openai</a>()</code>,
<code><a href="#topic+llm_provider_openrouter">llm_provider_openrouter</a>()</code>,
<code><a href="#topic+llm_provider_xai">llm_provider_xai</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Various providers:
ollama &lt;- llm_provider_ollama()
openai &lt;- llm_provider_openai()
openrouter &lt;- llm_provider_openrouter()
mistral &lt;- llm_provider_mistral()
groq &lt;- llm_provider_groq()
xai &lt;- llm_provider_xai()
gemini &lt;- llm_provider_google_gemini()

# Initialize with settings:
ollama &lt;- llm_provider_ollama(
  parameters = list(
    model = "llama3.2:3b",
    stream = TRUE
  ),
  verbose = TRUE,
  url = "http://localhost:11434/api/chat"
)

# Change settings:
ollama$verbose &lt;- FALSE
ollama$parameters$stream &lt;- FALSE
ollama$parameters$model &lt;- "llama3.1:8b"

## Not run: 
  # Try a simple chat message with '$complete_chat()':
  response &lt;- ollama$complete_chat("Hi!")
  response
  # $role
  # [1] "assistant"
  #
  # $content
  # [1] "How's it going? Is there something I can help you with or would you like
  # to chat?"
  #
  # $http
  # Response [http://localhost:11434/api/chat]
  # Date: 2024-11-18 14:21
  # Status: 200
  # Content-Type: application/json; charset=utf-8
  # Size: 375 B

  # Use with send_prompt():
  "Hi" |&gt;
    send_prompt(ollama)
  # [1] "How's your day going so far? Is there something I can help you with or
  # would you like to chat?"

## End(Not run)
</code></pre>

<hr>
<h2 id='llm_provider_mistral'>Create a new Mistral LLM provider</h2><span id='topic+llm_provider_mistral'></span>

<h3>Description</h3>

<p>This function creates a new <a href="#topic+llm_provider-class">llm_provider</a> object that interacts with the Mistral API.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>llm_provider_mistral(
  parameters = list(model = "ministral-3b-latest", stream =
    getOption("tidyprompt.stream", TRUE)),
  verbose = getOption("tidyprompt.verbose", TRUE),
  url = "https://api.mistral.ai/v1/chat/completions",
  api_key = Sys.getenv("MISTRAL_API_KEY")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="llm_provider_mistral_+3A_parameters">parameters</code></td>
<td>
<p>A named list of parameters. Currently the following parameters are required:
</p>

<ul>
<li><p> model: The name of the model to use
</p>
</li>
<li><p> stream: A logical indicating whether the API should stream responses
</p>
</li></ul>

<p>Additional parameters are appended to the request body; see the Mistral API
documentation for more information: https://docs.mistral.ai/api/#tag/chat</p>
</td></tr>
<tr><td><code id="llm_provider_mistral_+3A_verbose">verbose</code></td>
<td>
<p>A logical indicating whether the interaction with the LLM provider
should be printed to the consol</p>
</td></tr>
<tr><td><code id="llm_provider_mistral_+3A_url">url</code></td>
<td>
<p>The URL to the Mistral API endpoint for chat completion</p>
</td></tr>
<tr><td><code id="llm_provider_mistral_+3A_api_key">api_key</code></td>
<td>
<p>The API key to use for authentication with the Mistral API</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new <a href="#topic+llm_provider-class">llm_provider</a> object for use of the Mistral API
</p>


<h3>See Also</h3>

<p>Other llm_provider: 
<code><a href="#topic+llm_provider-class">llm_provider-class</a></code>,
<code><a href="#topic+llm_provider_google_gemini">llm_provider_google_gemini</a>()</code>,
<code><a href="#topic+llm_provider_groq">llm_provider_groq</a>()</code>,
<code><a href="#topic+llm_provider_ollama">llm_provider_ollama</a>()</code>,
<code><a href="#topic+llm_provider_openai">llm_provider_openai</a>()</code>,
<code><a href="#topic+llm_provider_openrouter">llm_provider_openrouter</a>()</code>,
<code><a href="#topic+llm_provider_xai">llm_provider_xai</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Various providers:
ollama &lt;- llm_provider_ollama()
openai &lt;- llm_provider_openai()
openrouter &lt;- llm_provider_openrouter()
mistral &lt;- llm_provider_mistral()
groq &lt;- llm_provider_groq()
xai &lt;- llm_provider_xai()
gemini &lt;- llm_provider_google_gemini()

# Initialize with settings:
ollama &lt;- llm_provider_ollama(
  parameters = list(
    model = "llama3.2:3b",
    stream = TRUE
  ),
  verbose = TRUE,
  url = "http://localhost:11434/api/chat"
)

# Change settings:
ollama$verbose &lt;- FALSE
ollama$parameters$stream &lt;- FALSE
ollama$parameters$model &lt;- "llama3.1:8b"

## Not run: 
  # Try a simple chat message with '$complete_chat()':
  response &lt;- ollama$complete_chat("Hi!")
  response
  # $role
  # [1] "assistant"
  #
  # $content
  # [1] "How's it going? Is there something I can help you with or would you like
  # to chat?"
  #
  # $http
  # Response [http://localhost:11434/api/chat]
  # Date: 2024-11-18 14:21
  # Status: 200
  # Content-Type: application/json; charset=utf-8
  # Size: 375 B

  # Use with send_prompt():
  "Hi" |&gt;
    send_prompt(ollama)
  # [1] "How's your day going so far? Is there something I can help you with or
  # would you like to chat?"

## End(Not run)
</code></pre>

<hr>
<h2 id='llm_provider_ollama'>Create a new Ollama LLM provider</h2><span id='topic+llm_provider_ollama'></span>

<h3>Description</h3>

<p>This function creates a new <a href="#topic+llm_provider-class">llm_provider</a> object that interacts with the Ollama API.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>llm_provider_ollama(
  parameters = list(model = "llama3.1:8b", stream = getOption("tidyprompt.stream", TRUE)),
  verbose = getOption("tidyprompt.verbose", TRUE),
  url = "http://localhost:11434/api/chat"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="llm_provider_ollama_+3A_parameters">parameters</code></td>
<td>
<p>A named list of parameters. Currently the following parameters are required:
</p>

<ul>
<li><p> model: The name of the model to use
</p>
</li>
<li><p> stream: A logical indicating whether the API should stream responses
</p>
</li></ul>

<p>Additional parameters may be passed by adding them to the parameters list;
these parameters will be passed to the Ollama API via the body of the POST request.
Note that various Ollama options need to be set in a list named 'options' within
the parameters list (e.g., context window size is represented in $parameters$options$num_ctx)</p>
</td></tr>
<tr><td><code id="llm_provider_ollama_+3A_verbose">verbose</code></td>
<td>
<p>A logical indicating whether the interaction with the LLM provider
should be printed to the console</p>
</td></tr>
<tr><td><code id="llm_provider_ollama_+3A_url">url</code></td>
<td>
<p>The URL to the Ollama API endpoint for chat completion
(typically: &quot;http://localhost:11434/api/chat&quot;)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new <a href="#topic+llm_provider-class">llm_provider</a> object for use of the Ollama API
</p>


<h3>See Also</h3>

<p>Other llm_provider: 
<code><a href="#topic+llm_provider-class">llm_provider-class</a></code>,
<code><a href="#topic+llm_provider_google_gemini">llm_provider_google_gemini</a>()</code>,
<code><a href="#topic+llm_provider_groq">llm_provider_groq</a>()</code>,
<code><a href="#topic+llm_provider_mistral">llm_provider_mistral</a>()</code>,
<code><a href="#topic+llm_provider_openai">llm_provider_openai</a>()</code>,
<code><a href="#topic+llm_provider_openrouter">llm_provider_openrouter</a>()</code>,
<code><a href="#topic+llm_provider_xai">llm_provider_xai</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Various providers:
ollama &lt;- llm_provider_ollama()
openai &lt;- llm_provider_openai()
openrouter &lt;- llm_provider_openrouter()
mistral &lt;- llm_provider_mistral()
groq &lt;- llm_provider_groq()
xai &lt;- llm_provider_xai()
gemini &lt;- llm_provider_google_gemini()

# Initialize with settings:
ollama &lt;- llm_provider_ollama(
  parameters = list(
    model = "llama3.2:3b",
    stream = TRUE
  ),
  verbose = TRUE,
  url = "http://localhost:11434/api/chat"
)

# Change settings:
ollama$verbose &lt;- FALSE
ollama$parameters$stream &lt;- FALSE
ollama$parameters$model &lt;- "llama3.1:8b"

## Not run: 
  # Try a simple chat message with '$complete_chat()':
  response &lt;- ollama$complete_chat("Hi!")
  response
  # $role
  # [1] "assistant"
  #
  # $content
  # [1] "How's it going? Is there something I can help you with or would you like
  # to chat?"
  #
  # $http
  # Response [http://localhost:11434/api/chat]
  # Date: 2024-11-18 14:21
  # Status: 200
  # Content-Type: application/json; charset=utf-8
  # Size: 375 B

  # Use with send_prompt():
  "Hi" |&gt;
    send_prompt(ollama)
  # [1] "How's your day going so far? Is there something I can help you with or
  # would you like to chat?"

## End(Not run)
</code></pre>

<hr>
<h2 id='llm_provider_openai'>Create a new OpenAI LLM provider</h2><span id='topic+llm_provider_openai'></span>

<h3>Description</h3>

<p>This function creates a new <a href="#topic+llm_provider-class">llm_provider</a> object that interacts with the Open AI API.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>llm_provider_openai(
  parameters = list(model = "gpt-4o-mini", stream = getOption("tidyprompt.stream", TRUE)),
  verbose = getOption("tidyprompt.verbose", TRUE),
  url = "https://api.openai.com/v1/chat/completions",
  api_key = Sys.getenv("OPENAI_API_KEY")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="llm_provider_openai_+3A_parameters">parameters</code></td>
<td>
<p>A named list of parameters. Currently the following parameters are required:
</p>

<ul>
<li><p> model: The name of the model to use
</p>
</li>
<li><p> api_key: The API key to use for authentication with the OpenAI API. This
should be a project API key (not a user API key)
</p>
</li>
<li><p> url: The URL to the OpenAI API (may also be an alternative endpoint
that provides a similar API.)
</p>
</li>
<li><p> stream: A logical indicating whether the API should stream responses
</p>
</li></ul>

<p>Additional parameters are appended to the request body; see the OpenAI API
documentation for more information: https://platform.openai.com/docs/api-reference/chat</p>
</td></tr>
<tr><td><code id="llm_provider_openai_+3A_verbose">verbose</code></td>
<td>
<p>A logical indicating whether the interaction with the LLM provider
should be printed to the console. Default is TRUE.</p>
</td></tr>
<tr><td><code id="llm_provider_openai_+3A_url">url</code></td>
<td>
<p>The URL to the OpenAI API endpoint for chat completion
(typically: &quot;https://api.openai.com/v1/chat/completions&quot;)</p>
</td></tr>
<tr><td><code id="llm_provider_openai_+3A_api_key">api_key</code></td>
<td>
<p>The API key to use for authentication with the OpenAI API</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new <a href="#topic+llm_provider-class">llm_provider</a> object for use of the OpenAI API
</p>


<h3>See Also</h3>

<p>Other llm_provider: 
<code><a href="#topic+llm_provider-class">llm_provider-class</a></code>,
<code><a href="#topic+llm_provider_google_gemini">llm_provider_google_gemini</a>()</code>,
<code><a href="#topic+llm_provider_groq">llm_provider_groq</a>()</code>,
<code><a href="#topic+llm_provider_mistral">llm_provider_mistral</a>()</code>,
<code><a href="#topic+llm_provider_ollama">llm_provider_ollama</a>()</code>,
<code><a href="#topic+llm_provider_openrouter">llm_provider_openrouter</a>()</code>,
<code><a href="#topic+llm_provider_xai">llm_provider_xai</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Various providers:
ollama &lt;- llm_provider_ollama()
openai &lt;- llm_provider_openai()
openrouter &lt;- llm_provider_openrouter()
mistral &lt;- llm_provider_mistral()
groq &lt;- llm_provider_groq()
xai &lt;- llm_provider_xai()
gemini &lt;- llm_provider_google_gemini()

# Initialize with settings:
ollama &lt;- llm_provider_ollama(
  parameters = list(
    model = "llama3.2:3b",
    stream = TRUE
  ),
  verbose = TRUE,
  url = "http://localhost:11434/api/chat"
)

# Change settings:
ollama$verbose &lt;- FALSE
ollama$parameters$stream &lt;- FALSE
ollama$parameters$model &lt;- "llama3.1:8b"

## Not run: 
  # Try a simple chat message with '$complete_chat()':
  response &lt;- ollama$complete_chat("Hi!")
  response
  # $role
  # [1] "assistant"
  #
  # $content
  # [1] "How's it going? Is there something I can help you with or would you like
  # to chat?"
  #
  # $http
  # Response [http://localhost:11434/api/chat]
  # Date: 2024-11-18 14:21
  # Status: 200
  # Content-Type: application/json; charset=utf-8
  # Size: 375 B

  # Use with send_prompt():
  "Hi" |&gt;
    send_prompt(ollama)
  # [1] "How's your day going so far? Is there something I can help you with or
  # would you like to chat?"

## End(Not run)
</code></pre>

<hr>
<h2 id='llm_provider_openrouter'>Create a new OpenRouter LLM provider</h2><span id='topic+llm_provider_openrouter'></span>

<h3>Description</h3>

<p>This function creates a new <a href="#topic+llm_provider-class">llm_provider</a> object that interacts with the OpenRouter API.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>llm_provider_openrouter(
  parameters = list(model = "qwen/qwen-2.5-7b-instruct", stream =
    getOption("tidyprompt.stream", TRUE)),
  verbose = getOption("tidyprompt.verbose", TRUE),
  url = "https://openrouter.ai/api/v1/chat/completions",
  api_key = Sys.getenv("OPENROUTER_API_KEY")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="llm_provider_openrouter_+3A_parameters">parameters</code></td>
<td>
<p>A named list of parameters. Currently the following parameters are required:
</p>

<ul>
<li><p> model: The name of the model to use
</p>
</li>
<li><p> stream: A logical indicating whether the API should stream responses
</p>
</li></ul>

<p>Additional parameters are appended to the request body; see the OpenRouter API
documentation for more information: https://openrouter.ai/docs/parameters</p>
</td></tr>
<tr><td><code id="llm_provider_openrouter_+3A_verbose">verbose</code></td>
<td>
<p>A logical indicating whether the interaction with the LLM provider
should be printed to the console.</p>
</td></tr>
<tr><td><code id="llm_provider_openrouter_+3A_url">url</code></td>
<td>
<p>The URL to the OpenRouter API endpoint for chat completion</p>
</td></tr>
<tr><td><code id="llm_provider_openrouter_+3A_api_key">api_key</code></td>
<td>
<p>The API key to use for authentication with the OpenRouter API</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new <a href="#topic+llm_provider-class">llm_provider</a> object for use of the OpenRouter API
</p>


<h3>See Also</h3>

<p>Other llm_provider: 
<code><a href="#topic+llm_provider-class">llm_provider-class</a></code>,
<code><a href="#topic+llm_provider_google_gemini">llm_provider_google_gemini</a>()</code>,
<code><a href="#topic+llm_provider_groq">llm_provider_groq</a>()</code>,
<code><a href="#topic+llm_provider_mistral">llm_provider_mistral</a>()</code>,
<code><a href="#topic+llm_provider_ollama">llm_provider_ollama</a>()</code>,
<code><a href="#topic+llm_provider_openai">llm_provider_openai</a>()</code>,
<code><a href="#topic+llm_provider_xai">llm_provider_xai</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Various providers:
ollama &lt;- llm_provider_ollama()
openai &lt;- llm_provider_openai()
openrouter &lt;- llm_provider_openrouter()
mistral &lt;- llm_provider_mistral()
groq &lt;- llm_provider_groq()
xai &lt;- llm_provider_xai()
gemini &lt;- llm_provider_google_gemini()

# Initialize with settings:
ollama &lt;- llm_provider_ollama(
  parameters = list(
    model = "llama3.2:3b",
    stream = TRUE
  ),
  verbose = TRUE,
  url = "http://localhost:11434/api/chat"
)

# Change settings:
ollama$verbose &lt;- FALSE
ollama$parameters$stream &lt;- FALSE
ollama$parameters$model &lt;- "llama3.1:8b"

## Not run: 
  # Try a simple chat message with '$complete_chat()':
  response &lt;- ollama$complete_chat("Hi!")
  response
  # $role
  # [1] "assistant"
  #
  # $content
  # [1] "How's it going? Is there something I can help you with or would you like
  # to chat?"
  #
  # $http
  # Response [http://localhost:11434/api/chat]
  # Date: 2024-11-18 14:21
  # Status: 200
  # Content-Type: application/json; charset=utf-8
  # Size: 375 B

  # Use with send_prompt():
  "Hi" |&gt;
    send_prompt(ollama)
  # [1] "How's your day going so far? Is there something I can help you with or
  # would you like to chat?"

## End(Not run)
</code></pre>

<hr>
<h2 id='llm_provider_xai'>Create a new XAI (Grok) LLM provider</h2><span id='topic+llm_provider_xai'></span>

<h3>Description</h3>

<p>This function creates a new <a href="#topic+llm_provider-class">llm_provider</a> object that interacts with the XAI API.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>llm_provider_xai(
  parameters = list(model = "grok-beta", stream = getOption("tidyprompt.stream", TRUE)),
  verbose = getOption("tidyprompt.verbose", TRUE),
  url = "https://api.x.ai/v1/chat/completions",
  api_key = Sys.getenv("XAI_API_KEY")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="llm_provider_xai_+3A_parameters">parameters</code></td>
<td>
<p>A named list of parameters. Currently the following parameters are required:
</p>

<ul>
<li><p> model: The name of the model to use
</p>
</li>
<li><p> stream: A logical indicating whether the API should stream responses
</p>
</li></ul>

<p>Additional parameters are appended to the request body; see the XAI API
documentation for more information: https://docs.x.ai/api/endpoints#chat-completions</p>
</td></tr>
<tr><td><code id="llm_provider_xai_+3A_verbose">verbose</code></td>
<td>
<p>A logical indicating whether the interaction with the LLM provider
should be printed to the console. Default is TRUE.</p>
</td></tr>
<tr><td><code id="llm_provider_xai_+3A_url">url</code></td>
<td>
<p>The URL to the XAI API endpoint for chat completion</p>
</td></tr>
<tr><td><code id="llm_provider_xai_+3A_api_key">api_key</code></td>
<td>
<p>The API key to use for authentication with the XAI API</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A new <a href="#topic+llm_provider-class">llm_provider</a> object for use of the XAI API
</p>


<h3>See Also</h3>

<p>Other llm_provider: 
<code><a href="#topic+llm_provider-class">llm_provider-class</a></code>,
<code><a href="#topic+llm_provider_google_gemini">llm_provider_google_gemini</a>()</code>,
<code><a href="#topic+llm_provider_groq">llm_provider_groq</a>()</code>,
<code><a href="#topic+llm_provider_mistral">llm_provider_mistral</a>()</code>,
<code><a href="#topic+llm_provider_ollama">llm_provider_ollama</a>()</code>,
<code><a href="#topic+llm_provider_openai">llm_provider_openai</a>()</code>,
<code><a href="#topic+llm_provider_openrouter">llm_provider_openrouter</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Various providers:
ollama &lt;- llm_provider_ollama()
openai &lt;- llm_provider_openai()
openrouter &lt;- llm_provider_openrouter()
mistral &lt;- llm_provider_mistral()
groq &lt;- llm_provider_groq()
xai &lt;- llm_provider_xai()
gemini &lt;- llm_provider_google_gemini()

# Initialize with settings:
ollama &lt;- llm_provider_ollama(
  parameters = list(
    model = "llama3.2:3b",
    stream = TRUE
  ),
  verbose = TRUE,
  url = "http://localhost:11434/api/chat"
)

# Change settings:
ollama$verbose &lt;- FALSE
ollama$parameters$stream &lt;- FALSE
ollama$parameters$model &lt;- "llama3.1:8b"

## Not run: 
  # Try a simple chat message with '$complete_chat()':
  response &lt;- ollama$complete_chat("Hi!")
  response
  # $role
  # [1] "assistant"
  #
  # $content
  # [1] "How's it going? Is there something I can help you with or would you like
  # to chat?"
  #
  # $http
  # Response [http://localhost:11434/api/chat]
  # Date: 2024-11-18 14:21
  # Status: 200
  # Content-Type: application/json; charset=utf-8
  # Size: 375 B

  # Use with send_prompt():
  "Hi" |&gt;
    send_prompt(ollama)
  # [1] "How's your day going so far? Is there something I can help you with or
  # would you like to chat?"

## End(Not run)
</code></pre>

<hr>
<h2 id='llm_provider-class'>LlmProvider R6 Class</h2><span id='topic+llm_provider-class'></span>

<h3>Description</h3>

<p>This class provides a structure for creating <a href="#topic+llm_provider-class">llm_provider</a>
objects with different implementations of <code style="white-space: pre;">&#8288;$complete_chat()&#8288;</code>.
Using this class, you can create an <a href="#topic+llm_provider-class">llm_provider</a> object that interacts
with different LLM providers, such Ollama, OpenAI, or other custom providers.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>parameters</code></dt><dd><p>A named list of parameters to configure the <a href="#topic+llm_provider-class">llm_provider</a>.
Parameters may be appended to the request body when interacting with the
LLM provider API</p>
</dd>
<dt><code>verbose</code></dt><dd><p>A logical indicating whether interaction with the LLM provider should be
printed to the console</p>
</dd>
<dt><code>url</code></dt><dd><p>The URL to the LLM provider API endpoint for chat completion</p>
</dd>
<dt><code>api_key</code></dt><dd><p>The API key to use for authentication with the LLM provider API</p>
</dd>
<dt><code>api_type</code></dt><dd><p>The type of API to use (e.g., &quot;openai&quot;, &quot;ollama&quot;).
This is used to determine certain specific behaviors for different APIs,
for instance, as is done in the <code><a href="#topic+answer_as_json">answer_as_json()</a></code> function</p>
</dd>
<dt><code>handler_fns</code></dt><dd><p>A list of functions that will be called after the completion of a chat.
See <code style="white-space: pre;">&#8288;$add_handler_fn()&#8288;</code></p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-LlmProvider-new"><code>llm_provider-class$new()</code></a>
</p>
</li>
<li> <p><a href="#method-LlmProvider-set_parameters"><code>llm_provider-class$set_parameters()</code></a>
</p>
</li>
<li> <p><a href="#method-LlmProvider-complete_chat"><code>llm_provider-class$complete_chat()</code></a>
</p>
</li>
<li> <p><a href="#method-LlmProvider-add_handler_fn"><code>llm_provider-class$add_handler_fn()</code></a>
</p>
</li>
<li> <p><a href="#method-LlmProvider-set_handler_fns"><code>llm_provider-class$set_handler_fns()</code></a>
</p>
</li>
<li> <p><a href="#method-LlmProvider-clone"><code>llm_provider-class$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-LlmProvider-new"></a>



<h4>Method <code>new()</code></h4>

<p>Create a new <a href="#topic+llm_provider-class">llm_provider</a> object
</p>


<h5>Usage</h5>

<div class="r"><pre>llm_provider-class$new(
  complete_chat_function,
  parameters = list(),
  verbose = TRUE,
  url = NULL,
  api_key = NULL,
  api_type = "unspecified"
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>complete_chat_function</code></dt><dd><p>Function that will be called by the <a href="#topic+llm_provider-class">llm_provider</a> to complete a chat.
This function should take a list containing at least '$chat_history'
(a data frame with 'role' and 'content' columns) and return a response
object, which contains:
</p>

<ul>
<li><p> 'completed': A dataframe with 'role' and 'content' columns,
containing the completed chat history
</p>
</li>
<li><p> 'http': A list containing a list 'requests' and a list 'responses',
containing the HTTP requests and responses made during the chat completion
</p>
</li></ul>
</dd>
<dt><code>parameters</code></dt><dd><p>A named list of parameters to configure the <a href="#topic+llm_provider-class">llm_provider</a>.
These parameters may be appended to the request body when interacting with
the LLM provider.
For example, the <code>model</code> parameter may often be required.
The 'stream' parameter may be used to indicate that the API should stream.
Parameters should not include the chat_history, or 'api_key' or 'url', which
are handled separately by the <a href="#topic+llm_provider-class">llm_provider</a> and '$complete_chat()'.
Parameters should also not be set when they are handled by prompt wraps</p>
</dd>
<dt><code>verbose</code></dt><dd><p>A logical indicating whether interaction with the LLM
provider should be printed to the console</p>
</dd>
<dt><code>url</code></dt><dd><p>The URL to the LLM provider API endpoint for chat completion
(typically required, but may be left NULL in some cases, for instance
when creating a fake LLM provider)</p>
</dd>
<dt><code>api_key</code></dt><dd><p>The API key to use for authentication with the LLM
provider API (optional, not required for, for instance, Ollama)</p>
</dd>
<dt><code>api_type</code></dt><dd><p>The type of API to use (e.g., &quot;openai&quot;, &quot;ollama&quot;).
This is used to determine certain specific behaviors for different APIs
(see for example the <code><a href="#topic+answer_as_json">answer_as_json()</a></code> function)</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>A new <a href="#topic+llm_provider-class">llm_provider</a> R6 object
</p>


<hr>
<a id="method-LlmProvider-set_parameters"></a>



<h4>Method <code>set_parameters()</code></h4>

<p>Helper function to set the parameters of the <a href="#topic+llm_provider-class">llm_provider</a>
object.
This function appends new parameters to the existing parameters
list.
</p>


<h5>Usage</h5>

<div class="r"><pre>llm_provider-class$set_parameters(new_parameters)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>new_parameters</code></dt><dd><p>A named list of new parameters to append to the
existing parameters list</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>The modified <a href="#topic+llm_provider-class">llm_provider</a> object
</p>


<hr>
<a id="method-LlmProvider-complete_chat"></a>



<h4>Method <code>complete_chat()</code></h4>

<p>Sends a chat history (see <code><a href="#topic+chat_history">chat_history()</a></code>
for details) to the LLM provider using the configured <code style="white-space: pre;">&#8288;$complete_chat()&#8288;</code>.
This function is typically called by <code><a href="#topic+send_prompt">send_prompt()</a></code> to interact with the LLM
provider, but it can also be called directly.
</p>


<h5>Usage</h5>

<div class="r"><pre>llm_provider-class$complete_chat(input)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>input</code></dt><dd><p>A string, a data frame which is a valid chat history
(see <code><a href="#topic+chat_history">chat_history()</a></code>), or a list containing a valid chat history under key
'$chat_history'</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>The response from the LLM provider
</p>


<hr>
<a id="method-LlmProvider-add_handler_fn"></a>



<h4>Method <code>add_handler_fn()</code></h4>

<p>Helper function to add a handler function to the
<a href="#topic+llm_provider-class">llm_provider</a> object.
Handler functions are called after the completion of a chat and can be
used to modify the response before it is returned by the <a href="#topic+llm_provider-class">llm_provider</a>.
Each handler function should take the response object
as input (first argument) as well as 'self' (the <a href="#topic+llm_provider-class">llm_provider</a>
object) and return a modified response object.
The functions will be called in the order they are added to the list.
</p>


<h5>Usage</h5>

<div class="r"><pre>llm_provider-class$add_handler_fn(handler_fn)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>handler_fn</code></dt><dd><p>A function that takes the response object plus
'self' (the <a href="#topic+llm_provider-class">llm_provider</a> object) as input and
returns a modified response object</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>If a handler function returns a list with a 'break' field set to <code>TRUE</code>,
the chat completion will be interrupted and the response will be returned
at that point.
If a handler function returns a list with a 'done' field set to <code>FALSE</code>,
the handler functions will continue to be called in a loop until the 'done'
field is not set to <code>FALSE</code>.
</p>


<hr>
<a id="method-LlmProvider-set_handler_fns"></a>



<h4>Method <code>set_handler_fns()</code></h4>

<p>Helper function to set the handler functions of the
<a href="#topic+llm_provider-class">llm_provider</a> object.
This function replaces the existing
handler functions list with a new list of handler functions.
See <code style="white-space: pre;">&#8288;$add_handler_fn()&#8288;</code> for more information
</p>


<h5>Usage</h5>

<div class="r"><pre>llm_provider-class$set_handler_fns(handler_fns)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>handler_fns</code></dt><dd><p>A list of handler functions to set</p>
</dd>
</dl>

</div>


<hr>
<a id="method-LlmProvider-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>llm_provider-class$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other llm_provider: 
<code><a href="#topic+llm_provider_google_gemini">llm_provider_google_gemini</a>()</code>,
<code><a href="#topic+llm_provider_groq">llm_provider_groq</a>()</code>,
<code><a href="#topic+llm_provider_mistral">llm_provider_mistral</a>()</code>,
<code><a href="#topic+llm_provider_ollama">llm_provider_ollama</a>()</code>,
<code><a href="#topic+llm_provider_openai">llm_provider_openai</a>()</code>,
<code><a href="#topic+llm_provider_openrouter">llm_provider_openrouter</a>()</code>,
<code><a href="#topic+llm_provider_xai">llm_provider_xai</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example creation of a llm_provider-class object:
llm_provider_openai &lt;- function(
    parameters = list(
      model = "gpt-4o-mini",
      stream = getOption("tidyprompt.stream", TRUE)
    ),
    verbose = getOption("tidyprompt.verbose", TRUE),
    url = "https://api.openai.com/v1/chat/completions",
    api_key = Sys.getenv("OPENAI_API_KEY")
) {
  complete_chat &lt;- function(chat_history) {
    headers &lt;- c(
      "Content-Type" = "application/json",
      "Authorization" = paste("Bearer", self$api_key)
    )

    body &lt;- list(
      messages = lapply(seq_len(nrow(chat_history)), function(i) {
        list(role = chat_history$role[i], content = chat_history$content[i])
      })
    )

    for (name in names(self$parameters))
      body[[name]] &lt;- self$parameters[[name]]

    request &lt;- httr2::request(self$url) |&gt;
      httr2::req_body_json(body) |&gt;
      httr2::req_headers(!!!headers)

    request_llm_provider(
      chat_history,
      request,
      stream = self$parameters$stream,
      verbose = self$verbose,
      api_type = self$api_type
    )
  }

  return(`llm_provider-class`$new(
    complete_chat_function = complete_chat,
    parameters = parameters,
    verbose = verbose,
    url = url,
    api_key = api_key,
    api_type = "openai"
  ))
}

llm_provider &lt;- llm_provider_openai()

## Not run: 
  llm_provider$complete_chat("Hi!")
  # --- Sending request to LLM provider (gpt-4o-mini): ---
  # Hi!
  # --- Receiving response from LLM provider: ---
  # Hello! How can I assist you today?

## End(Not run)
</code></pre>

<hr>
<h2 id='llm_verify'>Have LLM check the result of a prompt (LLM-in-the-loop)</h2><span id='topic+llm_verify'></span>

<h3>Description</h3>

<p>This function will wrap a prompt with a check for a LLM to accept or decline
the result of the prompt, providing feedback if the result is declined.
The evaluating LLM will be presented with the original prompt and the result
of the prompt, and will be asked to verify if the answer is satisfactory
(using chain of thought reasoning to arrive at a boolean decision). If
the result is declined, the chain of thought responsible for the decision
will be summarized and sent back to the original LLM that was asked to
evaluate the prompt, so that it may retry the prompt.
</p>
<p>Note that this function is experimental and, because it relies on chain of
thought reasoning by a LLM about the answer of another LLM, it may not
always provide accurate results and can increase the token cost of
evaluating a prompt.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>llm_verify(
  prompt,
  question = "Is the answer satisfactory?",
  llm_provider = NULL,
  max_words_feedback = 50
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="llm_verify_+3A_prompt">prompt</code></td>
<td>
<p>A single string or a <a href="#topic+tidyprompt-class">tidyprompt</a> object</p>
</td></tr>
<tr><td><code id="llm_verify_+3A_question">question</code></td>
<td>
<p>The question to ask the LLM to verify the result of the prompt.
The LLM will be presented the original prompt, its result, and this question.
The LLM will be asked to provide a boolean answer to this question. If TRUE,
the result of the prompt will be accepted; if FALSE, the result will be declined</p>
</td></tr>
<tr><td><code id="llm_verify_+3A_llm_provider">llm_provider</code></td>
<td>
<p>A <a href="#topic+llm_provider-class">llm_provider</a> object
which will be used to verify the evaluation led to a satisfactory result.
If not provided, the same LLM provider as the prompt was originally
evaluated with will be used</p>
</td></tr>
<tr><td><code id="llm_verify_+3A_max_words_feedback">max_words_feedback</code></td>
<td>
<p>The maximum number of words allowed in the summary of
why the result was declined.
This summary is sent back to the LLM originally asked to evaluate the prompt</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The original prompt text shown to the LLM is built from
the base prompt as well as all prompt wraps that have a modify function
but do not have an extraction or validation function. This is to ensure
that no redundant validation is performed by the evaluating LLM on
instructions which have already been validated by functions in those
prompt wraps.
</p>


<h3>Value</h3>

<p>A <a href="#topic+tidyprompt-class">tidyprompt</a> with an added <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> which
will add a check for a LLM to accept or decline the result of the prompt,
providing feedback if the result is declined
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  "What is 'Enschede'?!" |&gt;
    answer_as_text(max_words = 50) |&gt;
    llm_verify() |&gt;
    send_prompt()
#   --- Sending request to LLM provider (gpt-4o-mini): ---
#   What is 'Enschede'?!
#
#   You must provide a text response. The response must be at most 50 words.
#   --- Receiving response from LLM provider: ---
#   Enschede is a city in the Netherlands, located in the eastern part near the German border.
#   It is known for its vibrant culture, history, and universities, particularly the
#   University of Twente, as well as its textiles industry and beautiful parks.
#   --- Sending request to LLM provider (gpt-4o-mini): ---
#   You are given a user's prompt.
#   To answer the user's prompt, you need to think step by step to arrive at a final answer.
#
#   ----- START OF USER'S PROMPT -----
#   &gt;&gt;&gt; An assistant was asked:
#
#   What is 'Enschede'?!
#
#   &gt;&gt;&gt; The assistant answered:
#
#   [1] "Enschede is a city in the Netherlands, located in the eastern part near
#   the German border. It is known for its vibrant culture, history, and universities,
#   particularly the University of Twente, as well as its textiles industry and
#   beautiful parks."
#
#   &gt;&gt;&gt; Is the answer satisfactory?
#
#   You must answer with only TRUE or FALSE (use no other characters).
#   ----- END OF USER'S PROMPT -----
#
#   What are the steps you would take to answer the user's prompt?
#   Describe your thought process in the following format:
#     &gt;&gt; step 1: &lt;step 1 description&gt;
#     &gt;&gt; step 2: &lt;step 2 description&gt;
#     (etc.)
#
#   When you are done, you must type:
#     FINISH[&lt;put here your final answer to the user's prompt&gt;]
#
#   Make sure your final answer follows the logical conclusion of your thought process.
#   --- Receiving response from LLM provider: ---
#   &gt;&gt; step 1: Read the user's prompt carefully to understand what is being asked
#   regarding the assistant's answer about Enschede.
#   &gt;&gt; step 2: Analyze the assistant's provided answer for accuracy and completeness,
#   ensuring it conveys relevant information about Enschede.
#   &gt;&gt; step 3: Assess whether the answer includes key aspects, such as the city's
#   location, cultural significance, educational institutions, and any notable
#   historical references or industries tied to the city.
#   &gt;&gt; step 4: Determine if the assistant's answer sufficiently addresses the
#   question "What is Enschede?" and provides a clear and informative overview of the city.
#   &gt;&gt; step 5: Conclude whether the assistant's answer meets the standards for being
#   satisfactory in terms of detail and correctness.
#
#   FINISH[TRUE]
#   [1] "Enschede is a city in the Netherlands, located in the eastern part near the
#   German border. It is known for its vibrant culture, history, and universities,
#   particularly the University of Twente, as well as its textiles industry and beautiful parks."

## End(Not run)
</code></pre>

<hr>
<h2 id='persistent_chat-class'>PersistentChat R6 class</h2><span id='topic+persistent_chat-class'></span>

<h3>Description</h3>

<p>A class for managing a persistent chat with a large language model (LLM).
</p>
<p>While 'tidyprompt' is primariy focused on automatic interactions with
LLMs through <code><a href="#topic+send_prompt">send_prompt()</a></code> using a <a href="#topic+tidyprompt-class">tidyprompt</a> object with
<code><a href="#topic+prompt_wrap">prompt_wrap()</a></code>, this class may be useful for having a manual conversation
with an LLM. (It may specifically be used to continue a chat history which was
returned by <code><a href="#topic+send_prompt">send_prompt()</a></code> with <code>return_mode = "full"</code>.)
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>chat_history</code></dt><dd><p>A <code><a href="#topic+chat_history">chat_history()</a></code> object</p>
</dd>
<dt><code>llm_provider</code></dt><dd><p>A <a href="#topic+llm_provider-class">llm_provider</a> object</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-PersistentChat-new"><code>persistent_chat-class$new()</code></a>
</p>
</li>
<li> <p><a href="#method-PersistentChat-chat"><code>persistent_chat-class$chat()</code></a>
</p>
</li>
<li> <p><a href="#method-PersistentChat-reset_chat_history"><code>persistent_chat-class$reset_chat_history()</code></a>
</p>
</li>
<li> <p><a href="#method-PersistentChat-clone"><code>persistent_chat-class$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-PersistentChat-new"></a>



<h4>Method <code>new()</code></h4>

<p>Initialize the PersistentChat object
</p>


<h5>Usage</h5>

<div class="r"><pre>persistent_chat-class$new(llm_provider, chat_history = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>llm_provider</code></dt><dd><p>A <a href="#topic+llm_provider-class">llm_provider</a> object</p>
</dd>
<dt><code>chat_history</code></dt><dd><p>(optional) A <code><a href="#topic+chat_history">chat_history()</a></code> object</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>The initialized PersistentChat object
</p>


<hr>
<a id="method-PersistentChat-chat"></a>



<h4>Method <code>chat()</code></h4>

<p>Add a message to the chat history and get a response from the LLM
</p>


<h5>Usage</h5>

<div class="r"><pre>persistent_chat-class$chat(msg, role = "user", verbose = TRUE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>msg</code></dt><dd><p>Message to add to the chat history</p>
</dd>
<dt><code>role</code></dt><dd><p>Role of the message</p>
</dd>
<dt><code>verbose</code></dt><dd><p>Whether to print the interaction to the console</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>The response from the LLM
</p>


<hr>
<a id="method-PersistentChat-reset_chat_history"></a>



<h4>Method <code>reset_chat_history()</code></h4>

<p>Reset the chat history
</p>


<h5>Usage</h5>

<div class="r"><pre>persistent_chat-class$reset_chat_history()</pre></div>



<h5>Returns</h5>

<p>NULL
</p>


<hr>
<a id="method-PersistentChat-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>persistent_chat-class$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p><a href="#topic+llm_provider-class">llm_provider</a> <code><a href="#topic+chat_history">chat_history()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create a persistent chat with any LLM provider
chat &lt;- `persistent_chat-class`$new(llm_provider_ollama())

## Not run: 
  chat$chat("Hi! Tell me about Twente, in a short sentence?")
  # --- Sending request to LLM provider (llama3.1:8b): ---
  # Hi! Tell me about Twente, in a short sentence?
  # --- Receiving response from LLM provider: ---
  # Twente is a charming region in the Netherlands known for its picturesque
  # countryside and vibrant culture!

  chat$chat("How many people live there?")
  # --- Sending request to LLM provider (llama3.1:8b): ---
  # How many people live there?
  # --- Receiving response from LLM provider: ---
  # The population of Twente is approximately 650,000 inhabitants, making it one of
  # the largest regions in the Netherlands.

  # Access the chat history:
  chat$chat_history

  # Reset the chat history:
  chat$reset_chat_history()

  # Continue a chat from the result of `send_prompt()`:
  result &lt;- "Hi there!" |&gt;
    answer_as_integer() |&gt;
    send_prompt(return_mode = "full")
  # --- Sending request to LLM provider (llama3.1:8b): ---
  # Hi there!
  #
  # You must answer with only an integer (use no other characters).
  # --- Receiving response from LLM provider: ---
  # 42
  chat &lt;- `persistent_chat-class`$new(llm_provider_ollama(), result$chat_history)
  chat$chat("Why did you choose that number?")
  # --- Sending request to LLM provider (llama3.1:8b): ---
  # Why did you choose that number?
  # --- Receiving response from LLM provider: ---
  # I chose the number 42 because it's a reference to Douglas Adams' science fiction
  # series "The Hitchhiker's Guide to the Galaxy," in which a supercomputer named
  # Deep Thought is said to have calculated the "Answer to the Ultimate Question of
  # Life, the Universe, and Everything" as 42.

## End(Not run)

</code></pre>

<hr>
<h2 id='prompt_wrap'>Wrap a prompt with functions for modification and handling the LLM response</h2><span id='topic+prompt_wrap'></span>

<h3>Description</h3>

<p>This function takes a single string or a <a href="#topic+tidyprompt-class">tidyprompt</a> object and
adds a new prompt wrap to it.
</p>
<p>A prompt wrap is a set of functions that modify the prompt text,
extract a value from the LLM response, and validate the extracted value.
</p>
<p>The functions are used to ensure that the prompt and LLM response are in the
correct format and meet the specified criteria; they may also be used to
provide the LLM with feedback or additional information,
like the result of a tool call or some evaluated code.
</p>
<p>Advanced prompt wraps may also include functions that directly handle
the response from a LLM API or configure API parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prompt_wrap(
  prompt,
  modify_fn = NULL,
  extraction_fn = NULL,
  validation_fn = NULL,
  handler_fn = NULL,
  parameter_fn = NULL,
  type = c("unspecified", "mode", "tool", "break", "check"),
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prompt_wrap_+3A_prompt">prompt</code></td>
<td>
<p>A string or a <a href="#topic+tidyprompt-class">tidyprompt</a> object</p>
</td></tr>
<tr><td><code id="prompt_wrap_+3A_modify_fn">modify_fn</code></td>
<td>
<p>A function that takes the previous prompt text (as
first argument) and returns the new prompt text</p>
</td></tr>
<tr><td><code id="prompt_wrap_+3A_extraction_fn">extraction_fn</code></td>
<td>
<p>A function that takes the LLM response (as first argument)
and attempts to extract a value from it.
Upon succesful extraction, the function should return the extracted value.
If the extraction fails, the function should return a <code><a href="#topic+llm_feedback">llm_feedback()</a></code> message
to initiate a retry.
A <code><a href="#topic+llm_break">llm_break()</a></code> can be returned to break the extraction and validation loop,
ending <code><a href="#topic+send_prompt">send_prompt()</a></code></p>
</td></tr>
<tr><td><code id="prompt_wrap_+3A_validation_fn">validation_fn</code></td>
<td>
<p>A function that takes the (extracted) LLM response
(as first argument) and attempts to validate it.
Upon succesful validation, the function should return TRUE. If the validation
fails, the function should return a <code><a href="#topic+llm_feedback">llm_feedback()</a></code> message to initiate a retry.
A <code><a href="#topic+llm_break">llm_break()</a></code> can be returned to break the extraction and validation loop,
ending <code><a href="#topic+send_prompt">send_prompt()</a></code></p>
</td></tr>
<tr><td><code id="prompt_wrap_+3A_handler_fn">handler_fn</code></td>
<td>
<p>A function that takes a 'completion' object (a result
of a request to a LLM, as returned by <code style="white-space: pre;">&#8288;$complete_chat()&#8288;</code> of a <a href="#topic+llm_provider-class">llm_provider</a>
object) as first argument and the <a href="#topic+llm_provider-class">llm_provider</a> object as second argument.
The function should return a (modified or identical) completion object.
This can be used for advanced side effects, like logging, or native tool calling,
or keeping track of token usage. See <a href="#topic+llm_provider-class">llm_provider</a> for more information;
handler_fn is attached to the <a href="#topic+llm_provider-class">llm_provider</a> object that is being used.
For example usage, see source code of <code><a href="#topic+answer_using_tools">answer_using_tools()</a></code></p>
</td></tr>
<tr><td><code id="prompt_wrap_+3A_parameter_fn">parameter_fn</code></td>
<td>
<p>A function that takes the <a href="#topic+llm_provider-class">llm_provider</a> object
which is being used with <code><a href="#topic+send_prompt">send_prompt()</a></code> and returns a named list of parameters
to be set in the <a href="#topic+llm_provider-class">llm_provider</a> object via its <code style="white-space: pre;">&#8288;$set_parameters()&#8288;</code> method.
This can be used to configure specific parameters of the <a href="#topic+llm_provider-class">llm_provider</a>
object when evaluating the prompt.
For example, <code><a href="#topic+answer_as_json">answer_as_json()</a></code> may set different parameters for different APIs
related to JSON output.
This function is typically only used with advanced prompt wraps that require
specific settings in the <a href="#topic+llm_provider-class">llm_provider</a> object</p>
</td></tr>
<tr><td><code id="prompt_wrap_+3A_type">type</code></td>
<td>
<p>The type of prompt wrap. Must be one of:
</p>

<ul>
<li><p> &quot;unspecified&quot;: The default type, typically used for prompt wraps
which request a specific format of the LLM response, like <code><a href="#topic+answer_as_integer">answer_as_integer()</a></code>
</p>
</li>
<li><p> &quot;mode&quot;: For prompt wraps that change how the LLM should answer the prompt,
like <code><a href="#topic+answer_by_chain_of_thought">answer_by_chain_of_thought()</a></code> or <code><a href="#topic+answer_by_react">answer_by_react()</a></code>
</p>
</li>
<li><p> &quot;tool&quot;: For prompt wraps that enable the LLM to use tools, like <code><a href="#topic+answer_using_tools">answer_using_tools()</a></code>
or <code><a href="#topic+answer_using_r">answer_using_r()</a></code> when 'output_as_tool' = TRUE
</p>
</li>
<li><p> &quot;break&quot;: For prompt wraps that may break the extraction and validation loop,
like <code><a href="#topic+quit_if">quit_if()</a></code>. These are applied before type &quot;unspecified&quot; as they may
instruct the LLM to not answer the prompt in the manner specified by those
prompt wraps
</p>
</li>
<li><p> &quot;check&quot;: For prompt wraps that apply a last check to the final answer,
after all other prompt wraps have been evaluated.
These prompt wraps may only contain a validation function, and are applied
after all other prompt wraps have been evaluated. These prompt wraps are
even applied after an earlier prompt wrap has broken the extraction and validation loop
with <code><a href="#topic+llm_break">llm_break()</a></code>
</p>
</li></ul>

<p>Types are used to determine the order in which prompt wraps are applied.
When constructing the prompt text, prompt wraps are applied to the base prompt
in the following order: 'check', 'unspecified', 'break', 'mode', 'tool'.
When evaluating the LLM response and applying extraction and validation functions,
prompt wraps are applied in the reverse order: 'tool', 'mode', 'break',
'unspecified', 'check'.
Order among the same type is preserved in the order they were added to the prompt.</p>
</td></tr>
<tr><td><code id="prompt_wrap_+3A_name">name</code></td>
<td>
<p>An optional name for the prompt wrap.
This can be used to identify the prompt wrap in the <a href="#topic+tidyprompt-class">tidyprompt</a> object</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For advanced use, modify_fn, extraction_fn, and validation_fn
may take the <a href="#topic+llm_provider-class">llm_provider</a> object (as used with <code><a href="#topic+send_prompt">send_prompt()</a></code>) as
second argument, and the 'http_list' (a list of all HTTP requests
and responses made during <code><a href="#topic+send_prompt">send_prompt()</a></code>) as third argument.
Use of these arguments is not required, but can be useful for more complex
prompt wraps which require additional information about the LLM provider
or requests made so far.
The functions (including parameter_fn) also have access to
the object <code>self</code> (not a function argument; it is attached to the environment
of the function) which contains the <a href="#topic+tidyprompt-class">tidyprompt</a> object that the prompt wrap
is a part of. This can be used to access other prompt wraps, or to access the
prompt text or other information about the prompt. For instance,
other prompt wraps can be accessed through <code>self$get_prompt_wraps()</code>.
</p>


<h3>Value</h3>

<p>A <a href="#topic+tidyprompt-class">tidyprompt</a> object with the <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> appended to it
</p>


<h3>See Also</h3>

<p><a href="#topic+tidyprompt-class">tidyprompt</a> <code><a href="#topic+send_prompt">send_prompt()</a></code>
</p>
<p>Other prompt_wrap: 
<code><a href="#topic+llm_break">llm_break</a>()</code>,
<code><a href="#topic+llm_feedback">llm_feedback</a>()</code>
</p>
<p>Other pre_built_prompt_wraps: 
<code><a href="#topic+add_text">add_text</a>()</code>,
<code><a href="#topic+answer_as_boolean">answer_as_boolean</a>()</code>,
<code><a href="#topic+answer_as_integer">answer_as_integer</a>()</code>,
<code><a href="#topic+answer_as_json">answer_as_json</a>()</code>,
<code><a href="#topic+answer_as_list">answer_as_list</a>()</code>,
<code><a href="#topic+answer_as_named_list">answer_as_named_list</a>()</code>,
<code><a href="#topic+answer_as_regex_match">answer_as_regex_match</a>()</code>,
<code><a href="#topic+answer_as_text">answer_as_text</a>()</code>,
<code><a href="#topic+answer_by_chain_of_thought">answer_by_chain_of_thought</a>()</code>,
<code><a href="#topic+answer_by_react">answer_by_react</a>()</code>,
<code><a href="#topic+answer_using_r">answer_using_r</a>()</code>,
<code><a href="#topic+answer_using_sql">answer_using_sql</a>()</code>,
<code><a href="#topic+answer_using_tools">answer_using_tools</a>()</code>,
<code><a href="#topic+quit_if">quit_if</a>()</code>,
<code><a href="#topic+set_system_prompt">set_system_prompt</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># A custom prompt_wrap may be created during piping
prompt &lt;- "Hi there!" |&gt;
  prompt_wrap(
    modify_fn = function(base_prompt) {
      paste(base_prompt, "How are you?", sep = "\n\n")
    }
  )
prompt

# (Shorter notation of the above:)
prompt &lt;- "Hi there!" |&gt;
  prompt_wrap(\(x) paste(x, "How are you?", sep = "\n\n"))

# It may often be preferred to make a function which takes a prompt and
#   returns a wrapped prompt:
my_prompt_wrap &lt;- function(prompt) {
  modify_fn &lt;- function(base_prompt) {
    paste(base_prompt, "How are you?", sep = "\n\n")
  }

  prompt_wrap(prompt, modify_fn)
}
prompt &lt;- "Hi there!" |&gt;
  my_prompt_wrap()

# For more advanced examples, take a look at the source code of the
#   pre-built prompt wraps in the tidyprompt package, like
#   answer_as_boolean, answer_as_integer, add_tools, answer_as_code, etc.
# Below is the source code for the 'answer_as_integer' prompt wrap function:

#' Make LLM answer as an integer (between min and max)
#'
#' @param prompt A single string or a [tidyprompt()] object
#' @param min (optional) Minimum value for the integer
#' @param max (optional) Maximum value for the integer
#' @param add_instruction_to_prompt (optional) Add instruction for replying
#' as an integer to the prompt text. Set to FALSE for debugging if extractions/validations
#' are working as expected (without instruction the answer should fail the
#' validation function, initiating a retry)
#'
#' @return A [tidyprompt()] with an added [prompt_wrap()] which
#' will ensure that the LLM response is an integer.
#'
#' @export
#'
#' @example inst/examples/answer_as_integer.R
#'
#' @family pre_built_prompt_wraps
#' @family answer_as_prompt_wraps
answer_as_integer &lt;- function(
    prompt,
    min = NULL,
    max = NULL,
    add_instruction_to_prompt = TRUE
) {
  instruction &lt;- "You must answer with only an integer (use no other characters)."

  if (!is.null(min) &amp;&amp; !is.null(max)) {
    instruction &lt;- paste(instruction, glue::glue(
      "Enter an integer between {min} and {max}."
    ))
  } else if (!is.null(min)) {
    instruction &lt;- paste(instruction, glue::glue(
      "Enter an integer greater than or equal to {min}."
    ))
  } else if (!is.null(max)) {
    instruction &lt;- paste(instruction, glue::glue(
      "Enter an integer less than or equal to {max}."
    ))
  }

  modify_fn &lt;- function(original_prompt_text) {
    if (!add_instruction_to_prompt) {
      return(original_prompt_text)
    }

    glue::glue("{original_prompt_text}\n\n{instruction}")
  }

  extraction_fn &lt;- function(x) {
    extracted &lt;- suppressWarnings(as.numeric(x))
    if (is.na(extracted)) {
      return(llm_feedback(instruction))
    }
    return(extracted)
  }

  validation_fn &lt;- function(x) {
    if (x != floor(x)) { # Not a whole number
      return(llm_feedback(instruction))
    }

    if (!is.null(min) &amp;&amp; x &lt; min) {
      return(llm_feedback(glue::glue(
        "The number should be greater than or equal to {min}."
      )))
    }
    if (!is.null(max) &amp;&amp; x &gt; max) {
      return(llm_feedback(glue::glue(
        "The number should be less than or equal to {max}."
      )))
    }
    return(TRUE)
  }

  prompt_wrap(
    prompt,
    modify_fn, extraction_fn, validation_fn,
    name = "answer_as_integer"
  )
}
</code></pre>

<hr>
<h2 id='quit_if'>Make evaluation of a prompt stop if LLM gives a specific response</h2><span id='topic+quit_if'></span>

<h3>Description</h3>

<p>This function is used to wrap a <code><a href="#topic+tidyprompt">tidyprompt()</a></code> object and ensure that the
evaluation will stop if the LLM says it cannot answer the prompt. This is
useful in scenarios where it is determined the LLM is unable to provide a
response to a prompt.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quit_if(
  prompt,
  quit_detect_regex = "NO ANSWER",
  instruction =
    paste0("If you think that you cannot provide a valid answer, you must type:\n",
    "'NO ANSWER' (use no other characters)"),
  success = TRUE,
  response_result = c("null", "llm_response", "regex_match")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="quit_if_+3A_prompt">prompt</code></td>
<td>
<p>A single string or a <code><a href="#topic+tidyprompt">tidyprompt()</a></code> object</p>
</td></tr>
<tr><td><code id="quit_if_+3A_quit_detect_regex">quit_detect_regex</code></td>
<td>
<p>A regular expression to detect in the LLM's
response which will cause the evaluation to stop. The default
will detect the string &quot;NO ANSWER&quot; in the response</p>
</td></tr>
<tr><td><code id="quit_if_+3A_instruction">instruction</code></td>
<td>
<p>A string to be added to the prompt to instruct the LLM
how to respond if it cannot answer the prompt. The default is
&quot;If you think that you cannot provide a valid answer, you must type: 'NO ANSWER' (use no other characters)&quot;.
This parameter can be set to <code>NULL</code> if no instruction is needed in the prompt</p>
</td></tr>
<tr><td><code id="quit_if_+3A_success">success</code></td>
<td>
<p>A logical indicating whether the <code><a href="#topic+send_prompt">send_prompt()</a></code> loop break
should nonetheless be considered as a successful completion of the
extraction and validation process. If <code>FALSE</code>, the <code>object_to_return</code> must
will always be set to NULL and thus parameter 'response_result' must also
be set to 'null'; if <code>FALSE</code>, <code><a href="#topic+send_prompt">send_prompt()</a></code> will also print a warning
about the unsuccessful evaluation. If <code>TRUE</code>, the <code>object_to_return</code> will be
returned as the response result of <code><a href="#topic+send_prompt">send_prompt()</a></code> (and <code><a href="#topic+send_prompt">send_prompt()</a></code>
will print no warning about unsuccessful evaluation); parameter 'response_result'
will then determine what is returned as the response result of <code><a href="#topic+send_prompt">send_prompt()</a></code>.</p>
</td></tr>
<tr><td><code id="quit_if_+3A_response_result">response_result</code></td>
<td>
<p>A character string indicating what should be returned
when the quit_detect_regex is detected in the LLM's response. The default is
'null', which will return NULL as the response result o f <code><a href="#topic+send_prompt">send_prompt()</a></code>.
Under 'llm_response', the full LLM response will be returned as the
response result of <code><a href="#topic+send_prompt">send_prompt()</a></code>.
Under 'regex_match', the part of the LLM response that matches the
quit_detect_regex will be returned as the response result of <code><a href="#topic+send_prompt">send_prompt()</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="#topic+tidyprompt">tidyprompt()</a></code> with an added <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> which will ensure
that the evaluation will stop upon detection of the quit_detect_regex in the
LLM's response
</p>


<h3>See Also</h3>

<p>Other pre_built_prompt_wraps: 
<code><a href="#topic+add_text">add_text</a>()</code>,
<code><a href="#topic+answer_as_boolean">answer_as_boolean</a>()</code>,
<code><a href="#topic+answer_as_integer">answer_as_integer</a>()</code>,
<code><a href="#topic+answer_as_json">answer_as_json</a>()</code>,
<code><a href="#topic+answer_as_list">answer_as_list</a>()</code>,
<code><a href="#topic+answer_as_named_list">answer_as_named_list</a>()</code>,
<code><a href="#topic+answer_as_regex_match">answer_as_regex_match</a>()</code>,
<code><a href="#topic+answer_as_text">answer_as_text</a>()</code>,
<code><a href="#topic+answer_by_chain_of_thought">answer_by_chain_of_thought</a>()</code>,
<code><a href="#topic+answer_by_react">answer_by_react</a>()</code>,
<code><a href="#topic+answer_using_r">answer_using_r</a>()</code>,
<code><a href="#topic+answer_using_sql">answer_using_sql</a>()</code>,
<code><a href="#topic+answer_using_tools">answer_using_tools</a>()</code>,
<code><a href="#topic+prompt_wrap">prompt_wrap</a>()</code>,
<code><a href="#topic+set_system_prompt">set_system_prompt</a>()</code>
</p>
<p>Other miscellaneous_prompt_wraps: 
<code><a href="#topic+add_text">add_text</a>()</code>,
<code><a href="#topic+set_system_prompt">set_system_prompt</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  "What the favourite food of my cat on Thursday mornings?" |&gt;
    quit_if() |&gt;
    send_prompt(llm_provider_ollama())
  # --- Sending request to LLM provider (llama3.1:8b): ---
  #   What the favourite food of my cat on Thursday mornings?
  #
  #   If you think that you cannot provide a valid answer, you must type:
  #   'NO ANSWER' (use no other characters)
  # --- Receiving response from LLM provider: ---
  #   NO ANSWER
  # NULL

## End(Not run)
</code></pre>

<hr>
<h2 id='r_json_schema_to_example'>Generate an example object from a JSON schema</h2><span id='topic+r_json_schema_to_example'></span>

<h3>Description</h3>

<p>This function generates an example JSON object from a JSON schema.
This is used when enforcing a JSON schema through text-based handling
(requiring an example to be added to the prompt text).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r_json_schema_to_example(schema)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="r_json_schema_to_example_+3A_schema">schema</code></td>
<td>
<p>A list (R object) representing a JSON schema</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list (R object) which matches the JSON schema definition
</p>


<h3>See Also</h3>

<p>Other json: 
<code><a href="#topic+answer_as_json">answer_as_json</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>base_prompt &lt;- "How can I solve 8x + 7 = -23?"

# This example will show how to enforce JSON format in the response,
#   with and without a schema, using the 'answer_as_json()' prompt wrap.
# If you use type = 'auto', the function will automatically detect the
#   best way to enforce JSON based on the LLM provider you are using.
# Note that the default type is 'text-based', which will work for any provider/model

#### Enforcing JSON without a schema: ####

## Not run: 
  ## Text-based (works for any provider/model):
  #   Adds request to prompt for a JSON object
  #   Extracts JSON from textual response (feedback for retry if no JSON received)
  #   Parses JSON to R object
  json_1 &lt;- base_prompt |&gt;
    answer_as_json() |&gt;
    send_prompt(llm_provider_ollama())
  # --- Sending request to LLM provider (llama3.1:8b): ---
  # How can I solve 8x + 7 = -23?
  #
  # Your must format your response as a JSON object.
  # --- Receiving response from LLM provider: ---
  # Here is the solution to the equation formatted as a JSON object:
  #
  # ```
  # {
  #   "equation": "8x + 7 = -23",
  #   "steps": [
  #     {
  #       "step": "Subtract 7 from both sides of the equation",
  #       "expression": "-23 - 7"
  #     },
  #     {
  #       "step": "Simplify the expression on the left side",
  #       "result": "-30"
  #     },
  #     {
  #       "step": "Divide both sides by -8 to solve for x",
  #       "expression": "-30 / -8"
  #     },
  #     {
  #       "step": "Simplify the expression on the right side",
  #       "result": "3.75"
  #     }
  #   ],
  #   "solution": {
  #     "x": 3.75
  #   }
  # }
  # ```


  ## Ollama:
  #   - Sets 'format' parameter to 'json', enforcing JSON
  #   - Adds request to prompt for a JSON object, as is recommended by the docs
  #   - Parses JSON to R object
  json_2 &lt;- base_prompt |&gt;
    answer_as_json(type = "auto") |&gt;
    send_prompt(llm_provider_ollama())
  # --- Sending request to LLM provider (llama3.1:8b): ---
  # How can I solve 8x + 7 = -23?
  #
  # Your must format your response as a JSON object.
  # --- Receiving response from LLM provider: ---
  # {"steps": [
  #   "Subtract 7 from both sides to get 8x = -30",
  #   "Simplify the right side of the equation to get 8x = -30",
  #   "Divide both sides by 8 to solve for x, resulting in x = -30/8",
  #   "Simplify the fraction to find the value of x"
  # ],
  # "value_of_x": "-3.75"}


  ## OpenAI-type API without schema:
  #   - Sets 'response_format' parameter to 'json_object', enforcing JSON
  #   - Adds request to prompt for a JSON object, as is required by the API
  #   - Parses JSON to R object
  json_3 &lt;- base_prompt |&gt;
    answer_as_json(type = "auto") |&gt;
    send_prompt(llm_provider_openai())
  # --- Sending request to LLM provider (gpt-4o-mini): ---
  # How can I solve 8x + 7 = -23?
  #
  # Your must format your response as a JSON object.
  # --- Receiving response from LLM provider: ---
  # {
  #   "solution_steps": [
  #     {
  #       "step": 1,
  #       "operation": "Subtract 7 from both sides",
  #       "equation": "8x + 7 - 7 = -23 - 7",
  #       "result": "8x = -30"
  #     },
  #     {
  #       "step": 2,
  #       "operation": "Divide both sides by 8",
  #       "equation": "8x / 8 = -30 / 8",
  #       "result": "x = -3.75"
  #     }
  #   ],
  #   "solution": {
  #     "x": -3.75
  #   }
  # }

## End(Not run)



#### Enforcing JSON with a schema: ####

# Make a list representing a JSON schema,
#   which the LLM response must adhere to:
json_schema &lt;- list(
  name = "steps_to_solve", # Required for OpenAI API
  description = NULL, # Optional for OpenAI API
  schema = list(
    type = "object",
    properties = list(
      steps = list(
        type = "array",
        items = list(
          type = "object",
          properties = list(
            explanation = list(type = "string"),
            output = list(type = "string")
          ),
          required = c("explanation", "output"),
          additionalProperties = FALSE
        )
      ),
      final_answer = list(type = "string")
    ),
    required = c("steps", "final_answer"),
    additionalProperties = FALSE
  )
  # 'strict' parameter is set as argument 'answer_as_json()'
)
# Note: when you are not using an OpenAI API, you can also pass just the
#   internal 'schema' list object to 'answer_as_json()' instead of the full
#   'json_schema' list object

# Generate example R object based on schema:
r_json_schema_to_example(json_schema)

## Not run: 
  ## Text-based with schema (works for any provider/model):
  #   - Adds request to prompt for a JSON object
  #   - Adds schema to prompt
  #   - Extracts JSON from textual response (feedback for retry if no JSON received)
  #   - Validates JSON against schema with 'jsonvalidate' package (feedback for retry if invalid)
  #   - Parses JSON to R object
  json_4 &lt;- base_prompt |&gt;
    answer_as_json(schema = json_schema) |&gt;
    send_prompt(llm_provider_ollama())
  # --- Sending request to LLM provider (llama3.1:8b): ---
  # How can I solve 8x + 7 = -23?
  #
  # Your must format your response as a JSON object.
  #
  # Your JSON object should match this example JSON object:
  #   {
  #     "steps": [
  #       {
  #         "explanation": "...",
  #         "output": "..."
  #       }
  #     ],
  #     "final_answer": "..."
  #   }
  # --- Receiving response from LLM provider: ---
  # Here is the solution to the equation:
  #
  # ```
  # {
  #   "steps": [
  #     {
  #       "explanation": "First, we want to isolate the term with 'x' by
  #       subtracting 7 from both sides of the equation.",
  #       "output": "8x + 7 - 7 = -23 - 7"
  #     },
  #     {
  #       "explanation": "This simplifies to: 8x = -30",
  #       "output": "8x = -30"
  #     },
  #     {
  #       "explanation": "Next, we want to get rid of the coefficient '8' by
  #       dividing both sides of the equation by 8.",
  #       "output": "(8x) / 8 = (-30) / 8"
  #     },
  #     {
  #       "explanation": "This simplifies to: x = -3.75",
  #       "output": "x = -3.75"
  #     }
  #   ],
  #   "final_answer": "-3.75"
  # }
  # ```


  ## Ollama with schema:
  #   - Sets 'format' parameter to 'json', enforcing JSON
  #   - Adds request to prompt for a JSON object, as is recommended by the docs
  #   - Adds schema to prompt
  #   - Validates JSON against schema with 'jsonvalidate' package (feedback for retry if invalid)
  json_5 &lt;- base_prompt |&gt;
    answer_as_json(json_schema, type = "auto") |&gt;
    send_prompt(llm_provider_ollama())
  # --- Sending request to LLM provider (llama3.1:8b): ---
  # How can I solve 8x + 7 = -23?
  #
  # Your must format your response as a JSON object.
  #
  # Your JSON object should match this example JSON object:
  # {
  #   "steps": [
  #     {
  #       "explanation": "...",
  #       "output": "..."
  #     }
  #   ],
  #   "final_answer": "..."
  # }
  # --- Receiving response from LLM provider: ---
  # {
  #   "steps": [
  #     {
  #       "explanation": "First, subtract 7 from both sides of the equation to
  #       isolate the term with x.",
  #       "output": "8x = -23 - 7"
  #     },
  #     {
  #       "explanation": "Simplify the right-hand side of the equation.",
  #       "output": "8x = -30"
  #     },
  #     {
  #       "explanation": "Next, divide both sides of the equation by 8 to solve for x.",
  #       "output": "x = -30 / 8"
  #     },
  #     {
  #       "explanation": "Simplify the right-hand side of the equation.",
  #       "output": "x = -3.75"
  #     }
  #   ],
  #   "final_answer": "-3.75"
  # }

  ## OpenAI with schema:
  #   - Sets 'response_format' parameter to 'json_object', enforcing JSON
  #   - Adds json_schema to the API request, API enforces JSON adhering schema
  #   - Parses JSON to R object
  json_6 &lt;- base_prompt |&gt;
    answer_as_json(json_schema, type = "auto") |&gt;
    send_prompt(llm_provider_openai())
  # --- Sending request to LLM provider (gpt-4o-mini): ---
  # How can I solve 8x + 7 = -23?
  # --- Receiving response from LLM provider: ---
  # {"steps":[
  # {"explanation":"Start with the original equation.",
  # "output":"8x + 7 = -23"},
  # {"explanation":"Subtract 7 from both sides to isolate the term with x.",
  # "output":"8x + 7 - 7 = -23 - 7"},
  # {"explanation":"Simplify the left side and the right side of the equation.",
  # "output":"8x = -30"},
  # {"explanation":"Now, divide both sides by 8 to solve for x.",
  # "output":"x = -30 / 8"},
  # {"explanation":"Simplify the fraction by dividing both the numerator and the
  # denominator by 2.",
  # "output":"x = -15 / 4"}
  # ], "final_answer":"x = -15/4"}

## End(Not run)
</code></pre>

<hr>
<h2 id='send_prompt'>Send a prompt to a LLM provider</h2><span id='topic+send_prompt'></span>

<h3>Description</h3>

<p>This function is responsible for sending prompts to a LLM provider for evaluation.
The function will interact with the LLM provider until a successful response
is received or the maximum number of interactions is reached. The function will
apply extraction and validation functions to the LLM response, as specified
in the prompt wraps (see <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code>). If the maximum number of interactions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>send_prompt(
  prompt,
  llm_provider = llm_provider_ollama(),
  max_interactions = 10,
  clean_chat_history = TRUE,
  verbose = NULL,
  stream = NULL,
  return_mode = c("only_response", "full")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="send_prompt_+3A_prompt">prompt</code></td>
<td>
<p>A string or a <a href="#topic+tidyprompt-class">tidyprompt</a> object</p>
</td></tr>
<tr><td><code id="send_prompt_+3A_llm_provider">llm_provider</code></td>
<td>
<p><a href="#topic+llm_provider-class">llm_provider</a> object
(default is <code><a href="#topic+llm_provider_ollama">llm_provider_ollama()</a></code>).
This object and its settings will be used to evaluate the prompt.
Note that the 'verbose' and 'stream' settings in the LLM provider will be
overruled by the 'verbose' and 'stream' arguments in this function
when those are not NULL.
Furthermore, advanced <a href="#topic+tidyprompt-class">tidyprompt</a> objects may carry '$parameter_fn'
functions which can set parameters in the llm_provider object
(see <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> and <a href="#topic+llm_provider-class">llm_provider</a> for more ).</p>
</td></tr>
<tr><td><code id="send_prompt_+3A_max_interactions">max_interactions</code></td>
<td>
<p>Maximum number of interactions allowed with the
LLM provider. Default is 10. If the maximum number of interactions is reached
without a successful response, 'NULL' is returned as the response (see return
value). The first interaction is the initial chat completion</p>
</td></tr>
<tr><td><code id="send_prompt_+3A_clean_chat_history">clean_chat_history</code></td>
<td>
<p>If the chat history should be cleaned after each
interaction. Cleaning the chat history means that only the
first and last message from the user, the last message from the assistant,
all messages from the system, and all tool results are kept in a 'clean'
chat history. This clean chat history is used when requesting a new chat completion.
(i.e., if a LLM repeatedly fails to provide a correct response, only its last failed response
will included in the context window). This may increase the LLM performance
on the next interaction</p>
</td></tr>
<tr><td><code id="send_prompt_+3A_verbose">verbose</code></td>
<td>
<p>If the interaction with the LLM provider should be printed
to the console. This will overrule the 'verbose' setting in the LLM provider</p>
</td></tr>
<tr><td><code id="send_prompt_+3A_stream">stream</code></td>
<td>
<p>If the interaction with the LLM provider should be streamed.
This setting will only be used if the LLM provider already has a
'stream' parameter (which indicates there is support for streaming). Note
that when 'verbose' is set to FALSE, the 'stream' setting will be ignored</p>
</td></tr>
<tr><td><code id="send_prompt_+3A_return_mode">return_mode</code></td>
<td>
<p>One of 'full' or 'only_response'. See return value</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p> If return mode 'only_response', the function will return only the LLM response
after extraction and validation functions have been applied (NULL is returned
when unsuccessful after the maximum number of interactions).
</p>
</li>
<li><p> If return mode 'full', the function will return a list with the following elements:
</p>

<ul>
<li><p> 'response' (the LLM response after extraction and validation functions have been applied;
NULL is returned when unsuccessful after the maximum number of interactions),
</p>
</li>
<li><p> 'interactions' (the number of interactions with the LLM provider),
</p>
</li>
<li><p> 'chat_history' (a dataframe with the full chat history which led to the final response),
</p>
</li>
<li><p> 'chat_history_clean' (a dataframe with the cleaned chat history which led to
the final response; here, only the first and last message from the user, the
last message from the assistant, and all messages from the system are kept),
</p>
</li>
<li><p> 'start_time' (the time when the function was called),
</p>
</li>
<li><p> 'end_time' (the time when the function ended),
</p>
</li>
<li><p> 'duration_seconds' (the duration of the function in seconds), and
</p>
</li>
<li><p> 'http_list' (a list with all HTTP responses made during the interactions;
as returned by <code>llm_provider$complete_chat()</code>).
</p>
</li></ul>

</li></ul>



<h3>See Also</h3>

<p><a href="#topic+tidyprompt-class">tidyprompt</a>, <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code>, <a href="#topic+llm_provider-class">llm_provider</a>, <code><a href="#topic+llm_provider_ollama">llm_provider_ollama()</a></code>,
<code><a href="#topic+llm_provider_openai">llm_provider_openai()</a></code>
</p>
<p>Other prompt_evaluation: 
<code><a href="#topic+llm_break">llm_break</a>()</code>,
<code><a href="#topic+llm_feedback">llm_feedback</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  "Hi!" |&gt;
    send_prompt(llm_provider_ollama())
  # --- Sending request to LLM provider (llama3.1:8b): ---
  #   Hi!
  # --- Receiving response from LLM provider: ---
  #   It's nice to meet you. Is there something I can help you with, or would you like to chat?
  # [1] "It's nice to meet you. Is there something I can help you with, or would you like to chat?"

  "Hi!" |&gt;
    send_prompt(llm_provider_ollama(), return_mode = "full")
  # --- Sending request to LLM provider (llama3.1:8b): ---
  #   Hi!
  # --- Receiving response from LLM provider: ---
  #   It's nice to meet you. Is there something I can help you with, or would you like to chat?
  # $response
  # [1] "It's nice to meet you. Is there something I can help you with, or would you like to chat?"
  #
  # $chat_history
  # ...
  #
  # $chat_history_clean
  # ...
  #
  # $start_time
  # [1] "2024-11-18 15:43:12 CET"
  #
  # $end_time
  # [1] "2024-11-18 15:43:13 CET"
  #
  # $duration_seconds
  # [1] 1.13276
  #
  # $http_list
  # $http_list[[1]]
  # Response [http://localhost:11434/api/chat]
  #   Date: 2024-11-18 14:43
  #   Status: 200
  #   Content-Type: application/x-ndjson
  # &lt;EMPTY BODY&gt;

  "Hi!" |&gt;
    add_text("What is 5 + 5?") |&gt;
    answer_as_integer() |&gt;
    send_prompt(llm_provider_ollama(), verbose = FALSE)
  # [1] 10

## End(Not run)
</code></pre>

<hr>
<h2 id='set_chat_history'>Set the chat history of a <a href="#topic+tidyprompt-class">tidyprompt</a> object</h2><span id='topic+set_chat_history'></span>

<h3>Description</h3>

<p>This function sets the chat history for a <a href="#topic+tidyprompt-class">tidyprompt</a> object.
The chat history will also set the base prompt and system prompt
(the last message of the chat history should be of role 'user' and
will be used as the base prompt; the first message of the chat history
may be of the role 'system' and will then be used as the system prompt).
</p>
<p>This may be useful when one wants to change the base prompt, system prompt,
and chat history of a <a href="#topic+tidyprompt-class">tidyprompt</a> object while retaining other fields like
the list of prompt wraps.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_chat_history(x, chat_history)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="set_chat_history_+3A_x">x</code></td>
<td>
<p>A <a href="#topic+tidyprompt-class">tidyprompt</a> object</p>
</td></tr>
<tr><td><code id="set_chat_history_+3A_chat_history">chat_history</code></td>
<td>
<p>A valid chat history (see <code><a href="#topic+chat_history">chat_history()</a></code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The updated <a href="#topic+tidyprompt-class">tidyprompt</a> object
</p>


<h3>See Also</h3>

<p><code><a href="#topic+chat_history">chat_history()</a></code>
</p>
<p>Other tidyprompt: 
<code><a href="#topic+construct_prompt_text">construct_prompt_text</a>()</code>,
<code><a href="#topic+get_chat_history">get_chat_history</a>()</code>,
<code><a href="#topic+get_prompt_wraps">get_prompt_wraps</a>()</code>,
<code><a href="#topic+is_tidyprompt">is_tidyprompt</a>()</code>,
<code><a href="#topic+tidyprompt">tidyprompt</a>()</code>,
<code><a href="#topic+tidyprompt-class">tidyprompt-class</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prompt &lt;- tidyprompt("Hi!")
print(prompt)

# Add to a tidyprompt using a prompt wrap:
prompt &lt;- tidyprompt("Hi!") |&gt;
  add_text("How are you?")
print(prompt)

# Strings can be input for prompt wraps; therefore,
#   a call to tidyprompt() is not necessary:
prompt &lt;- "Hi" |&gt;
  add_text("How are you?")

# Example of adding extraction &amp; validation with a prompt_wrap():
prompt &lt;- "Hi" |&gt;
  add_text("What is 5 + 5?") |&gt;
  answer_as_integer()

## Not run: 
  # tidyprompt objects are evaluated by send_prompt(), which will
  #   handle construct the prompt text, send it to the LLM provider,
  #   and apply the extraction and validation functions from the tidyprompt object
  prompt |&gt;
    send_prompt(llm_provider_ollama())
  # --- Sending request to LLM provider (llama3.1:8b): ---
  #   Hi
  #
  #   What is 5 + 5?
  #
  #   You must answer with only an integer (use no other characters).
  # --- Receiving response from LLM provider: ---
  #   10
  # [1] 10

  # See prompt_wrap() and send_prompt() for more details

## End(Not run)

# `tidyprompt` objects may be validated with these helpers:
is_tidyprompt(prompt) # Returns TRUE if input is a valid tidyprompt object

# Get base prompt text
base_prompt &lt;- prompt$base_prompt

# Get all prompt wraps
prompt_wraps &lt;- prompt$get_prompt_wraps()
# Alternative:
prompt_wraps &lt;- get_prompt_wraps(prompt)

# Construct prompt text
prompt_text &lt;- prompt$construct_prompt_text()
# Alternative:
prompt_text &lt;- construct_prompt_text(prompt)

# Set chat history (affecting also the base prompt)
chat_history &lt;- data.frame(
  role = c("user", "assistant", "user"),
  content = c("What is 5 + 5?", "10", "And what is 5 + 6?")
)
prompt$set_chat_history(chat_history)

# Get chat history
chat_history &lt;- prompt$get_chat_history()
</code></pre>

<hr>
<h2 id='set_system_prompt'>Set system prompt of a <a href="#topic+tidyprompt-class">tidyprompt</a> object</h2><span id='topic+set_system_prompt'></span>

<h3>Description</h3>

<p>Set the system prompt for a prompt. The system prompt will be added
as a message with role 'system' at the start of the chat history when
this prompt is evaluated by <code><a href="#topic+send_prompt">send_prompt()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_system_prompt(prompt, system_prompt)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="set_system_prompt_+3A_prompt">prompt</code></td>
<td>
<p>A single string or a <code><a href="#topic+tidyprompt">tidyprompt()</a></code> object</p>
</td></tr>
<tr><td><code id="set_system_prompt_+3A_system_prompt">system_prompt</code></td>
<td>
<p>A single character string representing the system prompt</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The system prompt will be stored in the <code><a href="#topic+tidyprompt">tidyprompt()</a></code> object
as '$system_prompt'.
</p>


<h3>Value</h3>

<p>A <code><a href="#topic+tidyprompt">tidyprompt()</a></code> with the system prompt set
</p>


<h3>See Also</h3>

<p>Other pre_built_prompt_wraps: 
<code><a href="#topic+add_text">add_text</a>()</code>,
<code><a href="#topic+answer_as_boolean">answer_as_boolean</a>()</code>,
<code><a href="#topic+answer_as_integer">answer_as_integer</a>()</code>,
<code><a href="#topic+answer_as_json">answer_as_json</a>()</code>,
<code><a href="#topic+answer_as_list">answer_as_list</a>()</code>,
<code><a href="#topic+answer_as_named_list">answer_as_named_list</a>()</code>,
<code><a href="#topic+answer_as_regex_match">answer_as_regex_match</a>()</code>,
<code><a href="#topic+answer_as_text">answer_as_text</a>()</code>,
<code><a href="#topic+answer_by_chain_of_thought">answer_by_chain_of_thought</a>()</code>,
<code><a href="#topic+answer_by_react">answer_by_react</a>()</code>,
<code><a href="#topic+answer_using_r">answer_using_r</a>()</code>,
<code><a href="#topic+answer_using_sql">answer_using_sql</a>()</code>,
<code><a href="#topic+answer_using_tools">answer_using_tools</a>()</code>,
<code><a href="#topic+prompt_wrap">prompt_wrap</a>()</code>,
<code><a href="#topic+quit_if">quit_if</a>()</code>
</p>
<p>Other miscellaneous_prompt_wraps: 
<code><a href="#topic+add_text">add_text</a>()</code>,
<code><a href="#topic+quit_if">quit_if</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prompt &lt;- "Hi there!" |&gt;
  set_system_prompt("You are an assistant who always answers in very short poems.")
prompt$system_prompt

## Not run: 
  prompt |&gt;
    send_prompt(llm_provider_ollama())
  # --- Sending request to LLM provider (llama3.1:8b): ---
  #   Hi there!
  # --- Receiving response from LLM provider: ---
  #   Hello to you, I say,
  #   Welcome here, come what may!
  #   How can I assist today?
  # [1] "Hello to you, I say,\nWelcome here, come what may!\nHow can I assist today?"

## End(Not run)
</code></pre>

<hr>
<h2 id='skim_with_labels_and_levels'>Skim a dataframe and include labels and levels</h2><span id='topic+skim_with_labels_and_levels'></span>

<h3>Description</h3>

<p>This function takes a <code>data.frame</code> and returns a skim summary with variable names,
labels, and levels for categorical variables. It is a wrapper around the <code><a href="skimr.html#topic+skim">skimr::skim()</a></code>
function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>skim_with_labels_and_levels(data)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="skim_with_labels_and_levels_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> to be skimmed</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>data.frame</code> with variable names, labels, levels, and a skim summary
</p>


<h3>See Also</h3>

<p>Other text_helpers: 
<code><a href="#topic+df_to_string">df_to_string</a>()</code>,
<code><a href="#topic+vector_list_to_string">vector_list_to_string</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># First add some labels to 'mtcars':
mtcars$car &lt;- rownames(mtcars)
mtcars$car &lt;- factor(mtcars$car, levels = rownames(mtcars))
attr(mtcars$car, "label") &lt;- "Name of the car"

# Then skim the data:
mtcars |&gt;
  skim_with_labels_and_levels()
</code></pre>

<hr>
<h2 id='tidyprompt'>Create a <a href="#topic+tidyprompt-class">tidyprompt</a> object</h2><span id='topic+tidyprompt'></span>

<h3>Description</h3>

<p>This is a wrapper around the <a href="#topic+tidyprompt-class">tidyprompt</a> constructor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tidyprompt(input)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tidyprompt_+3A_input">input</code></td>
<td>
<p>A string, a chat history, a list containing
a chat history under key '$chat_history', or a <a href="#topic+tidyprompt-class">tidyprompt</a> object</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Different types of input are accepted for initialization of
a <a href="#topic+tidyprompt-class">tidyprompt</a> object:
</p>

<ul>
<li><p> A single character string. This will be used as the base prompt
</p>
</li>
<li><p> A dataframe which is a valid chat history (see <code><a href="#topic+chat_history">chat_history()</a></code>)
</p>
</li>
<li><p> A list containing a valid chat history under '$chat_history'
(e.g., a result from <code><a href="#topic+send_prompt">send_prompt()</a></code> when using 'return_mode' = &quot;full&quot;)
</p>
</li>
<li><p> A <a href="#topic+tidyprompt-class">tidyprompt</a> object. This will be checked for validity and, if valid,
the fields are copied to the object which is returned from this method
</p>
</li></ul>

<p>When passing a dataframe or list with a chat history, the last row of the
chat history must have role 'user'; this row will be used as the base prompt.
If the first row of the chat history has role 'system', it will be used
as the system prompt.
</p>


<h3>Value</h3>

<p>A <a href="#topic+tidyprompt-class">tidyprompt</a> object
</p>


<h3>See Also</h3>

<p>Other tidyprompt: 
<code><a href="#topic+construct_prompt_text">construct_prompt_text</a>()</code>,
<code><a href="#topic+get_chat_history">get_chat_history</a>()</code>,
<code><a href="#topic+get_prompt_wraps">get_prompt_wraps</a>()</code>,
<code><a href="#topic+is_tidyprompt">is_tidyprompt</a>()</code>,
<code><a href="#topic+set_chat_history">set_chat_history</a>()</code>,
<code><a href="#topic+tidyprompt-class">tidyprompt-class</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prompt &lt;- tidyprompt("Hi!")
print(prompt)

# Add to a tidyprompt using a prompt wrap:
prompt &lt;- tidyprompt("Hi!") |&gt;
  add_text("How are you?")
print(prompt)

# Strings can be input for prompt wraps; therefore,
#   a call to tidyprompt() is not necessary:
prompt &lt;- "Hi" |&gt;
  add_text("How are you?")

# Example of adding extraction &amp; validation with a prompt_wrap():
prompt &lt;- "Hi" |&gt;
  add_text("What is 5 + 5?") |&gt;
  answer_as_integer()

## Not run: 
  # tidyprompt objects are evaluated by send_prompt(), which will
  #   handle construct the prompt text, send it to the LLM provider,
  #   and apply the extraction and validation functions from the tidyprompt object
  prompt |&gt;
    send_prompt(llm_provider_ollama())
  # --- Sending request to LLM provider (llama3.1:8b): ---
  #   Hi
  #
  #   What is 5 + 5?
  #
  #   You must answer with only an integer (use no other characters).
  # --- Receiving response from LLM provider: ---
  #   10
  # [1] 10

  # See prompt_wrap() and send_prompt() for more details

## End(Not run)

# `tidyprompt` objects may be validated with these helpers:
is_tidyprompt(prompt) # Returns TRUE if input is a valid tidyprompt object

# Get base prompt text
base_prompt &lt;- prompt$base_prompt

# Get all prompt wraps
prompt_wraps &lt;- prompt$get_prompt_wraps()
# Alternative:
prompt_wraps &lt;- get_prompt_wraps(prompt)

# Construct prompt text
prompt_text &lt;- prompt$construct_prompt_text()
# Alternative:
prompt_text &lt;- construct_prompt_text(prompt)

# Set chat history (affecting also the base prompt)
chat_history &lt;- data.frame(
  role = c("user", "assistant", "user"),
  content = c("What is 5 + 5?", "10", "And what is 5 + 6?")
)
prompt$set_chat_history(chat_history)

# Get chat history
chat_history &lt;- prompt$get_chat_history()
</code></pre>

<hr>
<h2 id='tidyprompt-class'>Tidyprompt R6 Class</h2><span id='topic+tidyprompt-class'></span>

<h3>Description</h3>

<p>A <a href="#topic+tidyprompt-class">tidyprompt</a> object contains a base prompt and a list
of <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> objects. It provides structured methods to modify the prompt
while simultaneously adding logic to extract from and validate the LLM response.
Besides a base prompt, a <a href="#topic+tidyprompt-class">tidyprompt</a> object may contain a system prompt
and a chat history which precede the base prompt.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>base_prompt</code></dt><dd><p>The base prompt string.
The base prompt be modified by prompt wraps during <code><a href="#topic+construct_prompt_text">construct_prompt_text()</a></code>;
the modified prompt text will be used as the final message of role 'user'
during <code><a href="#topic+send_prompt">send_prompt()</a></code></p>
</dd>
<dt><code>system_prompt</code></dt><dd><p>A system prompt string.
This will be added at the start of the chat history as role 'system'
during <code><a href="#topic+send_prompt">send_prompt()</a></code></p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-Tidyprompt-new"><code>tidyprompt-class$new()</code></a>
</p>
</li>
<li> <p><a href="#method-Tidyprompt-is_valid"><code>tidyprompt-class$is_valid()</code></a>
</p>
</li>
<li> <p><a href="#method-Tidyprompt-add_prompt_wrap"><code>tidyprompt-class$add_prompt_wrap()</code></a>
</p>
</li>
<li> <p><a href="#method-Tidyprompt-get_prompt_wraps"><code>tidyprompt-class$get_prompt_wraps()</code></a>
</p>
</li>
<li> <p><a href="#method-Tidyprompt-construct_prompt_text"><code>tidyprompt-class$construct_prompt_text()</code></a>
</p>
</li>
<li> <p><a href="#method-Tidyprompt-set_chat_history"><code>tidyprompt-class$set_chat_history()</code></a>
</p>
</li>
<li> <p><a href="#method-Tidyprompt-get_chat_history"><code>tidyprompt-class$get_chat_history()</code></a>
</p>
</li>
<li> <p><a href="#method-Tidyprompt-clone"><code>tidyprompt-class$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-Tidyprompt-new"></a>



<h4>Method <code>new()</code></h4>

<p>Initialize a <a href="#topic+tidyprompt-class">tidyprompt</a> object
</p>


<h5>Usage</h5>

<div class="r"><pre>tidyprompt-class$new(input)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>input</code></dt><dd><p>A string, a chat history, a list containing
a chat history under key '$chat_history', or a <a href="#topic+tidyprompt-class">tidyprompt</a> object</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>Different types of input are accepted for initialization of
a <a href="#topic+tidyprompt-class">tidyprompt</a> object:
</p>

<ul>
<li><p> A single character string. This will be used as the base prompt
</p>
</li>
<li><p> A dataframe which is a valid chat history (see <code><a href="#topic+chat_history">chat_history()</a></code>)
</p>
</li>
<li><p> A list containing a valid chat history under '$chat_history'
(e.g., a result from <code><a href="#topic+send_prompt">send_prompt()</a></code> when using 'return_mode' = &quot;full&quot;)
</p>
</li>
<li><p> A <a href="#topic+tidyprompt-class">tidyprompt</a> object. This will be checked for validity and, if valid,
the fields are copied to the object which is returned from this method
</p>
</li></ul>

<p>When passing a dataframe or list with a chat history, the last row of the
chat history must have role 'user'; this row will be used as the base prompt.
If the first row of the chat history has role 'system', it will be used
as the system prompt.
</p>



<h5>Returns</h5>

<p>A <a href="#topic+tidyprompt-class">tidyprompt</a> object
</p>


<hr>
<a id="method-Tidyprompt-is_valid"></a>



<h4>Method <code>is_valid()</code></h4>

<p>Check if the tidyprompt object is valid.
</p>


<h5>Usage</h5>

<div class="r"><pre>tidyprompt-class$is_valid()</pre></div>



<h5>Returns</h5>

<p><code>TRUE</code> if valid, otherwise <code>FALSE</code>
</p>


<hr>
<a id="method-Tidyprompt-add_prompt_wrap"></a>



<h4>Method <code>add_prompt_wrap()</code></h4>

<p>Add a <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> to the <a href="#topic+tidyprompt-class">tidyprompt</a> object.
</p>


<h5>Usage</h5>

<div class="r"><pre>tidyprompt-class$add_prompt_wrap(prompt_wrap)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>prompt_wrap</code></dt><dd><p>A <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> object</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>The updated <a href="#topic+tidyprompt-class">tidyprompt</a> object
</p>


<hr>
<a id="method-Tidyprompt-get_prompt_wraps"></a>



<h4>Method <code>get_prompt_wraps()</code></h4>

<p>Get list of <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> objects from the <a href="#topic+tidyprompt-class">tidyprompt</a> object.
</p>


<h5>Usage</h5>

<div class="r"><pre>tidyprompt-class$get_prompt_wraps(
  order = c("default", "modification", "evaluation")
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>order</code></dt><dd><p>The order to return the wraps. Options are:
</p>

<ul>
<li><p> &quot;default&quot;: as originally added to the object
</p>
</li>
<li><p> &quot;modification&quot;: as ordered for modification of the base prompt;
ordered by type: check, unspecified, mode, tool, break. This is the order
in which prompt wraps are applied during <code><a href="#topic+construct_prompt_text">construct_prompt_text()</a></code>
</p>
</li>
<li><p> &quot;evaluation&quot;: ordered for evaluation of the LLM response;
ordered by type: tool, mode, break, unspecified, check. This is the order
in which wraps are applied to the LLM output during <code><a href="#topic+send_prompt">send_prompt()</a></code>
</p>
</li></ul>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>A list of <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> objects.
</p>


<hr>
<a id="method-Tidyprompt-construct_prompt_text"></a>



<h4>Method <code>construct_prompt_text()</code></h4>

<p>Construct the complete prompt text.
</p>


<h5>Usage</h5>

<div class="r"><pre>tidyprompt-class$construct_prompt_text(llm_provider = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>llm_provider</code></dt><dd><p>Optional <a href="#topic+llm_provider-class">llm_provider</a> object.
This may sometimes affect the prompt text construction</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>A string representing the constructed prompt text
</p>


<hr>
<a id="method-Tidyprompt-set_chat_history"></a>



<h4>Method <code>set_chat_history()</code></h4>

<p>This function sets the chat history for the tidyprompt object.
The chat history will also set the base prompt and system prompt
(the last message of the chat history should be of role 'user' and
will be used as the base prompt; the first message of the chat history
may be of the role 'system' and will then be used as the system prompt).
This may be useful when one wants to change the base prompt, system prompt,
and chat history of a <a href="#topic+tidyprompt-class">tidyprompt</a> object while retaining other fields like
the prompt wraps.
</p>


<h5>Usage</h5>

<div class="r"><pre>tidyprompt-class$set_chat_history(chat_history)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>chat_history</code></dt><dd><p>A valid chat history (see <code><a href="#topic+chat_history">chat_history()</a></code>)</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>The updated <a href="#topic+tidyprompt-class">tidyprompt</a> object
</p>


<hr>
<a id="method-Tidyprompt-get_chat_history"></a>



<h4>Method <code>get_chat_history()</code></h4>

<p>This function gets the chat history of the <a href="#topic+tidyprompt-class">tidyprompt</a> object.
The chat history is constructed from the base prompt, system prompt, and chat
history field. The returned object will be the chat history
with the system prompt as the first message with role 'system' and the
the base prompt as the last message with role 'user'.
</p>


<h5>Usage</h5>

<div class="r"><pre>tidyprompt-class$get_chat_history()</pre></div>



<h5>Returns</h5>

<p>A dataframe containing the chat history
</p>


<hr>
<a id="method-Tidyprompt-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>tidyprompt-class$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other tidyprompt: 
<code><a href="#topic+construct_prompt_text">construct_prompt_text</a>()</code>,
<code><a href="#topic+get_chat_history">get_chat_history</a>()</code>,
<code><a href="#topic+get_prompt_wraps">get_prompt_wraps</a>()</code>,
<code><a href="#topic+is_tidyprompt">is_tidyprompt</a>()</code>,
<code><a href="#topic+set_chat_history">set_chat_history</a>()</code>,
<code><a href="#topic+tidyprompt">tidyprompt</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prompt &lt;- tidyprompt("Hi!")
print(prompt)

# Add to a tidyprompt using a prompt wrap:
prompt &lt;- tidyprompt("Hi!") |&gt;
  add_text("How are you?")
print(prompt)

# Strings can be input for prompt wraps; therefore,
#   a call to tidyprompt() is not necessary:
prompt &lt;- "Hi" |&gt;
  add_text("How are you?")

# Example of adding extraction &amp; validation with a prompt_wrap():
prompt &lt;- "Hi" |&gt;
  add_text("What is 5 + 5?") |&gt;
  answer_as_integer()

## Not run: 
  # tidyprompt objects are evaluated by send_prompt(), which will
  #   handle construct the prompt text, send it to the LLM provider,
  #   and apply the extraction and validation functions from the tidyprompt object
  prompt |&gt;
    send_prompt(llm_provider_ollama())
  # --- Sending request to LLM provider (llama3.1:8b): ---
  #   Hi
  #
  #   What is 5 + 5?
  #
  #   You must answer with only an integer (use no other characters).
  # --- Receiving response from LLM provider: ---
  #   10
  # [1] 10

  # See prompt_wrap() and send_prompt() for more details

## End(Not run)

# `tidyprompt` objects may be validated with these helpers:
is_tidyprompt(prompt) # Returns TRUE if input is a valid tidyprompt object

# Get base prompt text
base_prompt &lt;- prompt$base_prompt

# Get all prompt wraps
prompt_wraps &lt;- prompt$get_prompt_wraps()
# Alternative:
prompt_wraps &lt;- get_prompt_wraps(prompt)

# Construct prompt text
prompt_text &lt;- prompt$construct_prompt_text()
# Alternative:
prompt_text &lt;- construct_prompt_text(prompt)

# Set chat history (affecting also the base prompt)
chat_history &lt;- data.frame(
  role = c("user", "assistant", "user"),
  content = c("What is 5 + 5?", "10", "And what is 5 + 6?")
)
prompt$set_chat_history(chat_history)

# Get chat history
chat_history &lt;- prompt$get_chat_history()
</code></pre>

<hr>
<h2 id='tools_add_docs'>Add tidyprompt function documentation to a function</h2><span id='topic+tools_add_docs'></span>

<h3>Description</h3>

<p>This function adds documentation to a custom function. This documentation
is used to extract information about the function's name, description, arguments,
and return value. This information is used to provide an LLM with information
about the functions, so that the LLM can call R functions. The intended
use of this function is to add documentation to custom functions that do not
have help files; <code><a href="#topic+tools_get_docs">tools_get_docs()</a></code> may generate documentation from a
help file when the function is part of base R or a package.
</p>
<p>If a function already has documentation, the documentation added by this
function may overwrite it. If you wish to modify existing documentation,
you may make a call to <code><a href="#topic+tools_get_docs">tools_get_docs()</a></code> to extract the existing documentation,
modify it, and then call <code><a href="#topic+tools_add_docs">tools_add_docs()</a></code> to add the modified documentation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tools_add_docs(func, docs)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tools_add_docs_+3A_func">func</code></td>
<td>
<p>A function object</p>
</td></tr>
<tr><td><code id="tools_add_docs_+3A_docs">docs</code></td>
<td>
<p>A list with the following elements:
</p>

<ul>
<li><p> 'name': (optional) The name of the function. If not provided, the function
name will be extracted from the function object. Use this parameter to override
the function name if necessary
</p>
</li>
<li><p> 'description': A description of the function and its purpose
</p>
</li>
<li><p> 'arguments': A named list of arguments with descriptions. Each argument is a list
which may contain: </p>

<ul>
<li><p> 'description': A description of the argument and its purpose. Not
required or used for native function calling (e.g., with OpenAI), but recommended
for text-based function calling
</p>
</li>
<li><p> 'type': The type of the argument. This should be one of:
'integer', 'numeric', 'logical', 'string', 'match.arg',
'vector integer', 'vector numeric', 'vector logical', 'vector string'.
For arguments which are named lists, 'type' should be a named list
which contains the types of the elements. For type 'match.arg', the
possible values should be passed as a vector under 'default_value'.
'type' is required for native function calling (with, e.g., OpenAI) but may
also be useful to provide for text-based function calling, in which it will
be added to the prompt introducing the function
</p>
</li>
<li><p> 'default_value': The default value of the argument. This is only required
when 'type' is set to 'match.arg'. It should then be a vector of possible values
for the argument. In other cases, it is not required; for native function calling,
it is not used in other cases; for text-based function calling, it may be useful
to provide the default value, which will be added to the prompt introducing the
function
</p>
</li></ul>

</li>
<li><p> 'return': A list with the following elements:
</p>

<ul>
<li><p> 'description': A description of the return value or the side effects of the function
</p>
</li></ul>

</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>The function object with the documentation added as an attribute
('tidyprompt_tool_docs')
</p>


<h3>See Also</h3>

<p>Other tools: 
<code><a href="#topic+answer_using_tools">answer_using_tools</a>()</code>,
<code><a href="#topic+tools_get_docs">tools_get_docs</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  # When using functions from base R or R packages,
  #   documentation is automatically extracted from help files:
  "What are the files in my current directory?" |&gt;
    answer_using_tools(dir) |&gt; # 'dir' function is from base R
    send_prompt()

## End(Not run)

# Custom functions may also be provided;
#   in this case, some documentation is extracted from the function's formals;
#   descriptions may be added manually. See below

# Example fake weather function to add to the prompt:
temperature_in_location &lt;- function(
    location = c("Amsterdam", "Utrecht", "Enschede"),
    unit = c("Celcius", "Fahrenheit")
) {
  location &lt;- match.arg(location)
  unit &lt;- match.arg(unit)

  temperature_celcius &lt;- switch(
    location,
    "Amsterdam" = 32.5,
    "Utrecht" = 19.8,
    "Enschede" = 22.7
  )

  if (unit == "Celcius") {
    return(temperature_celcius)
  } else {
    return(temperature_celcius * 9/5 + 32)
  }
}

# Generate documentation for a function
#   (based on formals, &amp; help file if available)
docs &lt;- tools_get_docs(temperature_in_location)

# The types get inferred from the function's formals
# However, descriptions are still missing as the function is not from a package
# We can modify the documentation object to add descriptions:
docs$description &lt;- "Get the temperature in a location"
docs$arguments$unit$description &lt;- "Unit in which to return the temperature"
docs$arguments$location$description &lt;- "Location for which to return the temperature"
docs$return$description &lt;- "The temperature in the specified location and unit"
# (See `?tools_add_docs` for more details on the structure of the documentation)

# When we are satisfied with the documentation, we can add it to the function:
temperature_in_location &lt;- tools_add_docs(temperature_in_location, docs)

## Not run: 
  # Now the LLM can use the function:
  "Hi, what is the weather in Enschede? Give me Celcius degrees" |&gt;
    answer_using_tools(temperature_in_location) |&gt;
    send_prompt()

## End(Not run)
</code></pre>

<hr>
<h2 id='tools_get_docs'>Extract documentation from a function</h2><span id='topic+tools_get_docs'></span>

<h3>Description</h3>

<p>This function extracts documentation from a help file (if available,
i.e., when the function is part of a package) or from documentation added
by <code><a href="#topic+tools_add_docs">tools_add_docs()</a></code>. The extracted documentation includes
the function's name, description, arguments, and return value.
This information is used to provide an LLM with information about the functions,
so that the LLM can call R functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tools_get_docs(func, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tools_get_docs_+3A_func">func</code></td>
<td>
<p>A function object. The function should belong to a package
and have documentation available in a help file, or it should
have documentation added by <code><a href="#topic+tools_add_docs">tools_add_docs()</a></code></p>
</td></tr>
<tr><td><code id="tools_get_docs_+3A_name">name</code></td>
<td>
<p>The name of the function if already known (optional).
If not provided it will be extracted from the documentation or the
function object's name</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function will prioritize documentation added by
<code><a href="#topic+tools_add_docs">tools_add_docs()</a></code> over documentation from a help file.
Thus, it is possible to override the help file documentation by adding
custom documentation
</p>


<h3>Value</h3>

<p>A list with documentation for the function. See <code><a href="#topic+tools_add_docs">tools_add_docs()</a></code>
for more information on the contents
</p>


<h3>See Also</h3>

<p>Other tools: 
<code><a href="#topic+answer_using_tools">answer_using_tools</a>()</code>,
<code><a href="#topic+tools_add_docs">tools_add_docs</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  # When using functions from base R or R packages,
  #   documentation is automatically extracted from help files:
  "What are the files in my current directory?" |&gt;
    answer_using_tools(dir) |&gt; # 'dir' function is from base R
    send_prompt()

## End(Not run)

# Custom functions may also be provided;
#   in this case, some documentation is extracted from the function's formals;
#   descriptions may be added manually. See below

# Example fake weather function to add to the prompt:
temperature_in_location &lt;- function(
    location = c("Amsterdam", "Utrecht", "Enschede"),
    unit = c("Celcius", "Fahrenheit")
) {
  location &lt;- match.arg(location)
  unit &lt;- match.arg(unit)

  temperature_celcius &lt;- switch(
    location,
    "Amsterdam" = 32.5,
    "Utrecht" = 19.8,
    "Enschede" = 22.7
  )

  if (unit == "Celcius") {
    return(temperature_celcius)
  } else {
    return(temperature_celcius * 9/5 + 32)
  }
}

# Generate documentation for a function
#   (based on formals, &amp; help file if available)
docs &lt;- tools_get_docs(temperature_in_location)

# The types get inferred from the function's formals
# However, descriptions are still missing as the function is not from a package
# We can modify the documentation object to add descriptions:
docs$description &lt;- "Get the temperature in a location"
docs$arguments$unit$description &lt;- "Unit in which to return the temperature"
docs$arguments$location$description &lt;- "Location for which to return the temperature"
docs$return$description &lt;- "The temperature in the specified location and unit"
# (See `?tools_add_docs` for more details on the structure of the documentation)

# When we are satisfied with the documentation, we can add it to the function:
temperature_in_location &lt;- tools_add_docs(temperature_in_location, docs)

## Not run: 
  # Now the LLM can use the function:
  "Hi, what is the weather in Enschede? Give me Celcius degrees" |&gt;
    answer_using_tools(temperature_in_location) |&gt;
    send_prompt()

## End(Not run)
</code></pre>

<hr>
<h2 id='user_verify'>Have user check the result of a prompt (human-in-the-loop)</h2><span id='topic+user_verify'></span>

<h3>Description</h3>

<p>This function is used to have a user check the result of a prompt.
After evaluation of the prompt and applying prompt wraps,
the user is presented with the result and asked to accept or decline.
If the user declines, they are asked to provide feedback to the large
language model (LLM) so that the LLM can retry the prompt.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>user_verify(prompt)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="user_verify_+3A_prompt">prompt</code></td>
<td>
<p>A single string or a <a href="#topic+tidyprompt-class">tidyprompt</a> object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+tidyprompt-class">tidyprompt</a> with an added <code><a href="#topic+prompt_wrap">prompt_wrap()</a></code> which
will add a check for the user to accept or decline the result of the prompt,
providing feedback if the result is declined
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  "Tell me a fun fact about yourself!" |&gt;
    user_verify() |&gt;
    send_prompt()
  # --- Sending request to LLM provider (gpt-4o-mini): ---
  # Tell me a fun fact about yourself!
  # --- Receiving response from LLM provider: ---
  # I don't have personal experiences or feelings, but a fun fact about me is that
  # I can generate text in multiple languages! From English to Spanish, French, and
  # more, I'm here to help with diverse linguistic needs.
  #
  # --- Evaluation of tidyprompt resulted in:
  # [1] "I don't have personal experiences or feelings, but a fun fact about me is
  # that I can generate text in multiple languages! From English to Spanish, French,
  # and more, I'm here to help with diverse linguistic needs."
  #
  # --- Accept or decline
  # * If satisfied, type nothing
  # * If not satisfied, type feedback to the LLM
  # Type: Needs to be funnier!
  # --- Sending request to LLM provider (gpt-4o-mini): ---
  # Needs to be funnier!
  # --- Receiving response from LLM provider: ---
  # Alright, how about this: I once tried to tell a joke, but my punchline got lost
  # in translation! Now, I just stick to delivering 'byte-sized' humor!
  #
  # --- Evaluation of tidyprompt resulted in:
  # [1] "Alright, how about this: I once tried to tell a joke, but my punchline got
  # lost in translation! Now, I just stick to delivering 'byte-sized' humor!"
  #
  # --- Accept or decline
  # * If satisfied, type nothing
  # * If not satisfied, type feedback to the LLM
  # Type:
  # * Result accepted
  # [1] "Alright, how about this: I once tried to tell a joke, but my punchline got
  # lost in translation! Now, I just stick to delivering 'byte-sized' humor!"

## End(Not run)

</code></pre>

<hr>
<h2 id='vector_list_to_string'>Convert a named or unnamed list/vector to a string representation</h2><span id='topic+vector_list_to_string'></span>

<h3>Description</h3>

<p>Converts a named or unnamed list/vector to a string format, intended for
sending it to an LLM (or for display or logging).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vector_list_to_string(obj, how = c("inline", "expanded"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vector_list_to_string_+3A_obj">obj</code></td>
<td>
<p>A list or vector (named or unnamed) to be converted to a string.</p>
</td></tr>
<tr><td><code id="vector_list_to_string_+3A_how">how</code></td>
<td>
<p>In what way the object should be converted to a string;
either &quot;inline&quot; or &quot;expanded&quot;. &quot;inline&quot; presents all key-value pairs
or values as a single line. &quot;expanded&quot; presents each key-value pair or value
on a separate line.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A single string representing the list/vector.
</p>


<h3>See Also</h3>

<p>Other text_helpers: 
<code><a href="#topic+df_to_string">df_to_string</a>()</code>,
<code><a href="#topic+skim_with_labels_and_levels">skim_with_labels_and_levels</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>named_vector &lt;- c(x = 10, y = 20, z = 30)

vector_list_to_string(named_vector, how = "inline")

vector_list_to_string(named_vector, how = "expanded")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
