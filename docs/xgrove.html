<!DOCTYPE html><html lang="en"><head><title>Help for package xgrove</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {xgrove}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#plot.sgtree'><p>Plot surrogate tree statistics</p></a></li>
<li><a href='#plot.xgrove'><p>Plot surrogate grove statistics</p></a></li>
<li><a href='#sgtree'><p>Surrogate trees</p></a></li>
<li><a href='#upsilon'><p>Explainability</p></a></li>
<li><a href='#xgrove'><p>Explanation groves</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Explanation Groves</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1-15</td>
</tr>
<tr>
<td>Date:</td>
<td>2025-01-04</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Gero Szepannek &lt;gero.szepannek@web.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Compute surrogate explanation groves for predictive machine learning models and analyze complexity vs. explanatory power of an explanation according to Szepannek, G. and von Holt, B. (2023) &lt;<a href="https://doi.org/10.1007%2Fs41237-023-00205-2">doi:10.1007/s41237-023-00205-2</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>gbm, dplyr, rpart, rpart.plot, ggplot2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>pdp, randomForest</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-01-09 15:53:45 UTC; szepannek</td>
</tr>
<tr>
<td>Author:</td>
<td>Gero Szepannek <a href="https://orcid.org/0000-0001-8456-1283"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-01-10 09:00:01 UTC</td>
</tr>
</table>
<hr>
<h2 id='plot.sgtree'>Plot surrogate tree statistics</h2><span id='topic+plot.sgtree'></span>

<h3>Description</h3>

<p>Plot statistics of surrogate trees to analyze complexity vs. explanatory power.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sgtree'
plot(x, abs = "rules", ord = "upsilon", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.sgtree_+3A_x">x</code></td>
<td>
<p>An object of class <code>sgtree</code>.</p>
</td></tr>
<tr><td><code id="plot.sgtree_+3A_abs">abs</code></td>
<td>
<p>Name of the measure to be plotted on the x-axis, either <code>"trees"</code>, <code>"rules"</code>, <code>"upsilon"</code> or <code>"cor"</code>.</p>
</td></tr>
<tr><td><code id="plot.sgtree_+3A_ord">ord</code></td>
<td>
<p>Name of the measure to be plotted on the y-axis, either <code>"trees"</code>, <code>"rules"</code>, <code>"upsilon"</code> or <code>"cor"</code>.</p>
</td></tr>
<tr><td><code id="plot.sgtree_+3A_...">...</code></td>
<td>
<p>Further arguments passed to <code>plot</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value.
</p>


<h3>Author(s)</h3>

<p><a href="mailto:gero.szepannek@web.de">gero.szepannek@web.de</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(randomForest)
library(pdp)
data(boston)
set.seed(42)
rf &lt;- randomForest(cmedv ~ ., data = boston)
data &lt;- boston[,-3] # remove target variable
ntrees &lt;- c(4,8,16,32,64,128)
xg &lt;- xgrove(rf, data, ntrees)
xg
plot(xg)

</code></pre>

<hr>
<h2 id='plot.xgrove'>Plot surrogate grove statistics</h2><span id='topic+plot.xgrove'></span>

<h3>Description</h3>

<p>Plot statistics of surrogate groves to analyze complexity vs. explanatory power.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'xgrove'
plot(x, n.trees = NULL, abs = "rules", ord = "upsilon", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.xgrove_+3A_x">x</code></td>
<td>
<p>An object of class <code>xgrove</code>.</p>
</td></tr>
<tr><td><code id="plot.xgrove_+3A_n.trees">n.trees</code></td>
<td>
<p>Number of trees in case the effects of a grove should be visualized and <code>abs</code> and <code>ord</code> are ignored. If <code>NULL</code> a screeplot of complexity vs explanation is shown for <code>abs</code> vs. <code>ord</code>.</p>
</td></tr>
<tr><td><code id="plot.xgrove_+3A_abs">abs</code></td>
<td>
<p>Name of the measure to be plotted on the x-axis, either <code>"trees"</code>, <code>"rules"</code>, <code>"upsilon"</code> or <code>"cor"</code>.</p>
</td></tr>
<tr><td><code id="plot.xgrove_+3A_ord">ord</code></td>
<td>
<p>Name of the measure to be plotted on the y-axis, either <code>"trees"</code>, <code>"rules"</code>, <code>"upsilon"</code> or <code>"cor"</code>.</p>
</td></tr>
<tr><td><code id="plot.xgrove_+3A_...">...</code></td>
<td>
<p>Further arguments passed to <code>plot</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value.
</p>


<h3>Author(s)</h3>

<p><a href="mailto:gero.szepannek@web.de">gero.szepannek@web.de</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(randomForest)
library(pdp)
data(boston)
set.seed(42)
rf &lt;- randomForest(cmedv ~ ., data = boston)
data &lt;- boston[,-3] # remove target variable
ntrees &lt;- c(4,8,16,32,64,128)
xg &lt;- xgrove(rf, data, ntrees)
xg
plot(xg)

# alternatively, visualize weights for the grove of size 8:
plot(xg, n.trees = 8)

</code></pre>

<hr>
<h2 id='sgtree'>Surrogate trees</h2><span id='topic+sgtree'></span>

<h3>Description</h3>

<p>Compute surrogate trees of different depth to explain predictive machine learning model and analyze complexity vs. explanatory power.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sgtree(model, data, maxdeps = 1:8, cparam = 0, pfun = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sgtree_+3A_model">model</code></td>
<td>
<p>A model with corresponding predict function that returns numeric values.</p>
</td></tr>
<tr><td><code id="sgtree_+3A_data">data</code></td>
<td>
<p>Data that must not (!) contain the target variable.</p>
</td></tr>
<tr><td><code id="sgtree_+3A_maxdeps">maxdeps</code></td>
<td>
<p>Sequence of integers: Maximum depth of the trees.</p>
</td></tr>
<tr><td><code id="sgtree_+3A_cparam">cparam</code></td>
<td>
<p>Complexity parameter for growing the trees.</p>
</td></tr>
<tr><td><code id="sgtree_+3A_pfun">pfun</code></td>
<td>
<p>Optional predict function <code>function(model, data)</code> returning a real number. Default is the <code>predict()</code> method of the <code>model</code>.</p>
</td></tr>
<tr><td><code id="sgtree_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to <code><a href="rpart.html#topic+rpart.control">rpart.control</a></code> or the <code>predict()</code> method of the <code>model</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A surrogate grove is trained via gradient boosting using <code><a href="rpart.html#topic+rpart">rpart</a></code> on <code>data</code> with the predictions of using of the <code>model</code> as target variable.
Note that <code>data</code> must not contain the original target variable!
</p>


<h3>Value</h3>

<p>List of the results:
</p>
<table role = "presentation">
<tr><td><code>explanation</code></td>
<td>
<p>Matrix containing tree sizes, rules, explainability <code class="reqn">{\Upsilon}</code> and the correlation between the predictions of the explanation and the true model.</p>
</td></tr>
<tr><td><code>rules</code></td>
<td>
<p>List of rules for each tree.</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>List of the <code>rpart</code> models.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p><a href="mailto:gero.szepannek@web.de">gero.szepannek@web.de</a>
</p>


<h3>References</h3>


<ul>
<li> <p>Szepannek, G. and Laabs, B.H. (2023): Canâ€™t see the forest for the trees &ndash; analyzing groves to explain random forests,
Behaviormetrika, submitted.
</p>
</li>
<li> <p>Szepannek, G. and Luebke, K.(2023): How much do we see? On the explainability of partial dependence plots for credit risk scoring,
Argumenta Oeconomica 50, DOI: 10.15611/aoe.2023.1.07.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(randomForest)
library(pdp)
data(boston)
set.seed(42)
rf    &lt;- randomForest(cmedv ~ ., data = boston)
data  &lt;- boston[,-3] # remove target variable
maxds &lt;- 1:7
st    &lt;- sgtree(rf, data, maxds)
st
# rules for tree of depth 3
st$rules[["3"]]
# plot tree of depth 3
rpart.plot::rpart.plot(st$model[["3"]])

</code></pre>

<hr>
<h2 id='upsilon'>Explainability</h2><span id='topic+upsilon'></span>

<h3>Description</h3>

<p>Compute explainability given predicted data of the model and an explainer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>upsilon(porig, pexp)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="upsilon_+3A_porig">porig</code></td>
<td>
<p>An object of class <code>xgrove</code>.</p>
</td></tr>
<tr><td><code id="upsilon_+3A_pexp">pexp</code></td>
<td>
<p>Name of the measure to be plotted on the x-axis, either <code>"trees"</code>, <code>"rules"</code>, <code>"upsilon"</code> or <code>"cor"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric explainability upsilon.
</p>


<h3>Author(s)</h3>

<p><a href="mailto:gero.szepannek@web.de">gero.szepannek@web.de</a>
</p>


<h3>References</h3>


<ul>
<li> <p>Szepannek, G. and Luebke, K.(2023): How much do we see? On the explainability of partial dependence plots for credit risk scoring,
Argumenta Oeconomica 50, DOI: 10.15611/aoe.2023.1.07.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(randomForest)
library(pdp)
data(boston)
set.seed(42)
# Compute original model
rf &lt;- randomForest(cmedv ~ ., data = boston)
data &lt;- boston[,-3] # remove target variable
# Compute predictions
porig &lt;- predict(rf, data)

# Compute surrogate grove
xg &lt;- xgrove(rf, data)
pexp &lt;- predict(xg$model, data, n.trees = 16)
upsilon(porig, pexp)

</code></pre>

<hr>
<h2 id='xgrove'>Explanation groves</h2><span id='topic+xgrove'></span>

<h3>Description</h3>

<p>Compute surrogate groves to explain predictive machine learning model and analyze complexity vs. explanatory power.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>xgrove(
  model,
  data,
  ntrees = c(4, 8, 16, 32, 64, 128),
  pfun = NULL,
  remove.target = T,
  shrink = 1,
  b.frac = 1,
  seed = 42,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="xgrove_+3A_model">model</code></td>
<td>
<p>A model with corresponding predict function that returns numeric values.</p>
</td></tr>
<tr><td><code id="xgrove_+3A_data">data</code></td>
<td>
<p>Training data.</p>
</td></tr>
<tr><td><code id="xgrove_+3A_ntrees">ntrees</code></td>
<td>
<p>Sequence of integers: number of boosting trees for rule extraction.</p>
</td></tr>
<tr><td><code id="xgrove_+3A_pfun">pfun</code></td>
<td>
<p>Optional predict function <code>function(model, data)</code> returning a real number. Default is the <code>predict()</code> method of the <code>model</code>.</p>
</td></tr>
<tr><td><code id="xgrove_+3A_remove.target">remove.target</code></td>
<td>
<p>Logical. If <code>TRUE</code> the name of the target variable is identified from <code>terms(model)</code> and automatically removed if this variable is still in <code>data</code>.</p>
</td></tr>
<tr><td><code id="xgrove_+3A_shrink">shrink</code></td>
<td>
<p>Sets the <code>shrinkage</code> argument for the internal call of <code><a href="gbm.html#topic+gbm">gbm</a></code>. As the <code>model</code> usually has a deterministic response 
the default is 1 different to the default of <code><a href="gbm.html#topic+gbm">gbm</a></code> applied train a model based on data.</p>
</td></tr>
<tr><td><code id="xgrove_+3A_b.frac">b.frac</code></td>
<td>
<p>Sets the <code>bag.fraction</code> argument for the internal call of <code><a href="gbm.html#topic+gbm">gbm</a></code>. As the <code>model</code> usually has a deterministic response 
the default is 1 different to the default of <code><a href="gbm.html#topic+gbm">gbm</a></code> applied train a model based on data.</p>
</td></tr>
<tr><td><code id="xgrove_+3A_seed">seed</code></td>
<td>
<p>Seed for the random number generator to ensure reproducible results (e.g. for the default <code>bag.fraction</code> &lt; 1 in boosting).</p>
</td></tr>
<tr><td><code id="xgrove_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to <code>gbm</code> or the <code>predict()</code> method of the <code>model</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A surrogate grove is trained via gradient boosting using <code><a href="gbm.html#topic+gbm">gbm</a></code> on <code>data</code> with the predictions of using of the <code>model</code> as target variable.
Note that <code>data</code> must not contain the original target variable! The boosting model is trained using stumps of depth 1.
The resulting interpretation is extracted from <code><a href="gbm.html#topic+pretty.gbm.tree">pretty.gbm.tree</a></code>. 
The column <code>upper_bound_left</code> of the <code>rules</code> and the <code>groves</code> value of the output object contains 
the split point for numeric variables denoting the uppoer bound of the left branch. Correspondingly, the 
<code>levels_left</code> column contains the levels of factor variables assigned to the left branch. 
The rule weights of the branches are given in the rightmost columns. The prediction of the grove is 
obtained as the sum of the assigned weights over all rows.       
Note that the training data must not contain the target variable. It can be either removed manually or will be removed automatically from <code>data</code> 
if the argument <code>remove.target == TRUE</code>.
</p>


<h3>Value</h3>

<p>List of the results:
</p>
<table role = "presentation">
<tr><td><code>explanation</code></td>
<td>
<p>Matrix containing tree sizes, rules, explainability <code class="reqn">{\Upsilon}</code> and the correlation between the predictions of the explanation and the true model.</p>
</td></tr>
<tr><td><code>rules</code></td>
<td>
<p>Summary of the explanation grove: Rules with identical splits are aggegated. For numeric variables any splits are merged if they lead to identical parititions of the training data.</p>
</td></tr>
<tr><td><code>groves</code></td>
<td>
<p>Rules of the explanation grove.</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p><code>gbm</code> model.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p><a href="mailto:gero.szepannek@web.de">gero.szepannek@web.de</a>
</p>


<h3>References</h3>


<ul>
<li> <p>Szepannek, G. and von Holt, B.H. (2023): Canâ€™t see the forest for the trees &ndash; analyzing groves to explain random forests,
Behaviormetrika, DOI: 10.1007/s41237-023-00205-2.
</p>
</li>
<li> <p>Szepannek, G. and Luebke, K.(2023): How much do we see? On the explainability of partial dependence plots for credit risk scoring,
Argumenta Oeconomica 50, DOI: 10.15611/aoe.2023.1.07.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(randomForest)
library(pdp)
data(boston)
set.seed(42)
rf &lt;- randomForest(cmedv ~ ., data = boston)
data &lt;- boston[,-3] # remove target variable
ntrees &lt;- c(4,8,16,32,64,128)
xg &lt;- xgrove(rf, data, ntrees)
xg
plot(xg)

# Example of a classification problem using the iris data.
# A predict function has to be defined, here for the posterior probabilities of the class Virginica.  
data(iris)
set.seed(42)
rf    &lt;- randomForest(Species ~ ., data = iris)
data  &lt;- iris[,-5] # remove target variable

pf &lt;- function(model, data){
  predict(model, data, type = "prob")[,3]
  }
  
xgrove(rf, data, pfun = pf)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
