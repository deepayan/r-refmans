<!DOCTYPE html><html><head><title>Help for package msaenet</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {msaenet}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#msaenet-package'><p>msaenet: Multi-Step Adaptive Estimation Methods for Sparse Regressions</p></a></li>
<li><a href='#aenet'><p>Adaptive Elastic-Net</p></a></li>
<li><a href='#amnet'><p>Adaptive MCP-Net</p></a></li>
<li><a href='#asnet'><p>Adaptive SCAD-Net</p></a></li>
<li><a href='#coef.msaenet'><p>Extract Model Coefficients</p></a></li>
<li><a href='#msaenet'><p>Multi-Step Adaptive Elastic-Net</p></a></li>
<li><a href='#msaenet.fn'><p>Get the Number of False Negative Selections</p></a></li>
<li><a href='#msaenet.fp'><p>Get the Number of False Positive Selections</p></a></li>
<li><a href='#msaenet.mae'><p>Mean Absolute Error (MAE)</p></a></li>
<li><a href='#msaenet.mse'><p>Mean Squared Error (MSE)</p></a></li>
<li><a href='#msaenet.nzv'><p>Get Indices of Non-Zero Variables</p></a></li>
<li><a href='#msaenet.nzv.all'><p>Get Indices of Non-Zero Variables in All Steps</p></a></li>
<li><a href='#msaenet.rmse'><p>Root Mean Squared Error (RMSE)</p></a></li>
<li><a href='#msaenet.rmsle'><p>Root Mean Squared Logarithmic Error (RMSLE)</p></a></li>
<li><a href='#msaenet.sim.binomial'><p>Generate Simulation Data for Benchmarking Sparse Regressions</p>
(Binomial Response)</a></li>
<li><a href='#msaenet.sim.cox'><p>Generate Simulation Data for Benchmarking Sparse Regressions (Cox Model)</p></a></li>
<li><a href='#msaenet.sim.gaussian'><p>Generate Simulation Data for Benchmarking Sparse Regressions</p>
(Gaussian Response)</a></li>
<li><a href='#msaenet.sim.poisson'><p>Generate Simulation Data for Benchmarking Sparse Regressions</p>
(Poisson Response)</a></li>
<li><a href='#msaenet.tp'><p>Get the Number of True Positive Selections</p></a></li>
<li><a href='#msaenet.tune.glmnet'><p>Automatic (parallel) parameter tuning for glmnet models</p></a></li>
<li><a href='#msaenet.tune.ncvreg'><p>Automatic (parallel) parameter tuning for ncvreg models</p></a></li>
<li><a href='#msaenet.tune.nsteps.glmnet'><p>Select the number of adaptive estimation steps</p></a></li>
<li><a href='#msaenet.tune.nsteps.ncvreg'><p>Select the number of adaptive estimation steps</p></a></li>
<li><a href='#msamnet'><p>Multi-Step Adaptive MCP-Net</p></a></li>
<li><a href='#msasnet'><p>Multi-Step Adaptive SCAD-Net</p></a></li>
<li><a href='#plot.msaenet'><p>Plot msaenet Model Objects</p></a></li>
<li><a href='#predict.msaenet'><p>Make Predictions from an msaenet Model</p></a></li>
<li><a href='#print.msaenet'><p>Print msaenet Model Information</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Multi-Step Adaptive Estimation Methods for Sparse Regressions</td>
</tr>
<tr>
<td>Version:</td>
<td>3.1.1</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Nan Xiao &lt;me@nanx.me&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Multi-step adaptive elastic-net (MSAENet) algorithm for
    feature selection in high-dimensional regressions proposed in
    Xiao and Xu (2015) &lt;<a href="https://doi.org/10.1080%2F00949655.2015.1016944">doi:10.1080/00949655.2015.1016944</a>&gt;,
    with support for multi-step adaptive MCP-net (MSAMNet) and
    multi-step adaptive SCAD-net (MSASNet) methods.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://nanx.me/msaenet/">https://nanx.me/msaenet/</a>, <a href="https://github.com/nanxstats/msaenet">https://github.com/nanxstats/msaenet</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/nanxstats/msaenet/issues">https://github.com/nanxstats/msaenet/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.2)</td>
</tr>
<tr>
<td>Imports:</td>
<td>glmnet, ncvreg (&ge; 3.8-0), foreach, mvtnorm, survival, Matrix</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, doParallel</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-04 02:05:54 UTC; nanx</td>
</tr>
<tr>
<td>Author:</td>
<td>Nan Xiao <a href="https://orcid.org/0000-0002-0250-5673"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Qing-Song Xu [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-04 04:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='msaenet-package'>msaenet: Multi-Step Adaptive Estimation Methods for Sparse Regressions</h2><span id='topic+msaenet-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>Multi-step adaptive elastic-net (MSAENet) algorithm for feature selection in high-dimensional regressions proposed in Xiao and Xu (2015) <a href="https://doi.org/10.1080/00949655.2015.1016944">doi:10.1080/00949655.2015.1016944</a>, with support for multi-step adaptive MCP-net (MSAMNet) and multi-step adaptive SCAD-net (MSASNet) methods.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Nan Xiao <a href="mailto:me@nanx.me">me@nanx.me</a> (<a href="https://orcid.org/0000-0002-0250-5673">ORCID</a>)
</p>
<p>Authors:
</p>

<ul>
<li><p> Qing-Song Xu <a href="mailto:qsxu@csu.edu.cn">qsxu@csu.edu.cn</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://nanx.me/msaenet/">https://nanx.me/msaenet/</a>
</p>
</li>
<li> <p><a href="https://github.com/nanxstats/msaenet">https://github.com/nanxstats/msaenet</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/nanxstats/msaenet/issues">https://github.com/nanxstats/msaenet/issues</a>
</p>
</li></ul>


<hr>
<h2 id='aenet'>Adaptive Elastic-Net</h2><span id='topic+aenet'></span>

<h3>Description</h3>

<p>Adaptive Elastic-Net
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aenet(
  x,
  y,
  family = c("gaussian", "binomial", "poisson", "cox"),
  init = c("enet", "ridge"),
  alphas = seq(0.05, 0.95, 0.05),
  tune = c("cv", "ebic", "bic", "aic"),
  nfolds = 5L,
  rule = c("lambda.min", "lambda.1se"),
  ebic.gamma = 1,
  scale = 1,
  lower.limits = -Inf,
  upper.limits = Inf,
  penalty.factor.init = rep(1, ncol(x)),
  seed = 1001,
  parallel = FALSE,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aenet_+3A_x">x</code></td>
<td>
<p>Data matrix.</p>
</td></tr>
<tr><td><code id="aenet_+3A_y">y</code></td>
<td>
<p>Response vector if <code>family</code> is <code>"gaussian"</code>,
<code>"binomial"</code>, or <code>"poisson"</code>. If <code>family</code> is
<code>"cox"</code>, a response matrix created by <code><a href="survival.html#topic+Surv">Surv</a></code>.</p>
</td></tr>
<tr><td><code id="aenet_+3A_family">family</code></td>
<td>
<p>Model family, can be <code>"gaussian"</code>,
<code>"binomial"</code>, <code>"poisson"</code>, or <code>"cox"</code>.</p>
</td></tr>
<tr><td><code id="aenet_+3A_init">init</code></td>
<td>
<p>Type of the penalty used in the initial
estimation step. Can be <code>"enet"</code> or <code>"ridge"</code>.</p>
</td></tr>
<tr><td><code id="aenet_+3A_alphas">alphas</code></td>
<td>
<p>Vector of candidate <code>alpha</code>s to use in
<code><a href="glmnet.html#topic+cv.glmnet">cv.glmnet</a></code>.</p>
</td></tr>
<tr><td><code id="aenet_+3A_tune">tune</code></td>
<td>
<p>Parameter tuning method for each estimation step.
Possible options are <code>"cv"</code>, <code>"ebic"</code>, <code>"bic"</code>,
and <code>"aic"</code>. Default is <code>"cv"</code>.</p>
</td></tr>
<tr><td><code id="aenet_+3A_nfolds">nfolds</code></td>
<td>
<p>Fold numbers of cross-validation when <code>tune = "cv"</code>.</p>
</td></tr>
<tr><td><code id="aenet_+3A_rule">rule</code></td>
<td>
<p>Lambda selection criterion when <code>tune = "cv"</code>,
can be <code>"lambda.min"</code> or <code>"lambda.1se"</code>.
See <code><a href="glmnet.html#topic+cv.glmnet">cv.glmnet</a></code> for details.</p>
</td></tr>
<tr><td><code id="aenet_+3A_ebic.gamma">ebic.gamma</code></td>
<td>
<p>Parameter for Extended BIC penalizing
size of the model space when <code>tune = "ebic"</code>,
default is <code>1</code>. For details, see Chen and Chen (2008).</p>
</td></tr>
<tr><td><code id="aenet_+3A_scale">scale</code></td>
<td>
<p>Scaling factor for adaptive weights:
<code>weights = coefficients^(-scale)</code>.</p>
</td></tr>
<tr><td><code id="aenet_+3A_lower.limits">lower.limits</code></td>
<td>
<p>Lower limits for coefficients.
Default is <code>-Inf</code>. For details, see <code><a href="glmnet.html#topic+glmnet">glmnet</a></code>.</p>
</td></tr>
<tr><td><code id="aenet_+3A_upper.limits">upper.limits</code></td>
<td>
<p>Upper limits for coefficients.
Default is <code>Inf</code>. For details, see <code><a href="glmnet.html#topic+glmnet">glmnet</a></code>.</p>
</td></tr>
<tr><td><code id="aenet_+3A_penalty.factor.init">penalty.factor.init</code></td>
<td>
<p>The multiplicative factor for the penalty
applied to each coefficient in the initial estimation step. This is
useful for incorporating prior information about variable weights,
for example, emphasizing specific clinical variables. To make certain
variables more likely to be selected, assign a smaller value.
Default is <code>rep(1, ncol(x))</code>.</p>
</td></tr>
<tr><td><code id="aenet_+3A_seed">seed</code></td>
<td>
<p>Random seed for cross-validation fold division.</p>
</td></tr>
<tr><td><code id="aenet_+3A_parallel">parallel</code></td>
<td>
<p>Logical. Enable parallel parameter tuning or not,
default is <code>FALSE</code>. To enable parallel tuning, load the
<code>doParallel</code> package and run <code>registerDoParallel()</code>
with the number of CPU cores before calling this function.</p>
</td></tr>
<tr><td><code id="aenet_+3A_verbose">verbose</code></td>
<td>
<p>Should we print out the estimation progress?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of model coefficients, <code>glmnet</code> model object,
and the optimal parameter set.
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>


<h3>References</h3>

<p>Zou, Hui, and Hao Helen Zhang. (2009).
On the adaptive elastic-net with a diverging number of parameters.
<em>The Annals of Statistics</em> 37(4), 1733&ndash;1751.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- msaenet.sim.gaussian(
  n = 150, p = 500, rho = 0.6,
  coef = rep(1, 5), snr = 2, p.train = 0.7,
  seed = 1001
)

aenet.fit &lt;- aenet(
  dat$x.tr, dat$y.tr,
  alphas = seq(0.2, 0.8, 0.2), seed = 1002
)

print(aenet.fit)
msaenet.nzv(aenet.fit)
msaenet.fp(aenet.fit, 1:5)
msaenet.tp(aenet.fit, 1:5)
aenet.pred &lt;- predict(aenet.fit, dat$x.te)
msaenet.rmse(dat$y.te, aenet.pred)
plot(aenet.fit)
</code></pre>

<hr>
<h2 id='amnet'>Adaptive MCP-Net</h2><span id='topic+amnet'></span>

<h3>Description</h3>

<p>Adaptive MCP-Net
</p>


<h3>Usage</h3>

<pre><code class='language-R'>amnet(
  x,
  y,
  family = c("gaussian", "binomial", "poisson", "cox"),
  init = c("mnet", "ridge"),
  gammas = 3,
  alphas = seq(0.05, 0.95, 0.05),
  tune = c("cv", "ebic", "bic", "aic"),
  nfolds = 5L,
  ebic.gamma = 1,
  scale = 1,
  eps = 1e-04,
  max.iter = 10000L,
  penalty.factor.init = rep(1, ncol(x)),
  seed = 1001,
  parallel = FALSE,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="amnet_+3A_x">x</code></td>
<td>
<p>Data matrix.</p>
</td></tr>
<tr><td><code id="amnet_+3A_y">y</code></td>
<td>
<p>Response vector if <code>family</code> is <code>"gaussian"</code>,
<code>"binomial"</code>, or <code>"poisson"</code>. If <code>family</code> is
<code>"cox"</code>, a response matrix created by <code><a href="survival.html#topic+Surv">Surv</a></code>.</p>
</td></tr>
<tr><td><code id="amnet_+3A_family">family</code></td>
<td>
<p>Model family, can be <code>"gaussian"</code>,
<code>"binomial"</code>, <code>"poisson"</code>, or <code>"cox"</code>.</p>
</td></tr>
<tr><td><code id="amnet_+3A_init">init</code></td>
<td>
<p>Type of the penalty used in the initial
estimation step. Can be <code>"mnet"</code> or <code>"ridge"</code>.</p>
</td></tr>
<tr><td><code id="amnet_+3A_gammas">gammas</code></td>
<td>
<p>Vector of candidate <code>gamma</code>s (the concavity parameter)
to use in MCP-Net. Default is <code>3</code>.</p>
</td></tr>
<tr><td><code id="amnet_+3A_alphas">alphas</code></td>
<td>
<p>Vector of candidate <code>alpha</code>s to use in MCP-Net.</p>
</td></tr>
<tr><td><code id="amnet_+3A_tune">tune</code></td>
<td>
<p>Parameter tuning method for each estimation step.
Possible options are <code>"cv"</code>, <code>"ebic"</code>, <code>"bic"</code>,
and <code>"aic"</code>. Default is <code>"cv"</code>.</p>
</td></tr>
<tr><td><code id="amnet_+3A_nfolds">nfolds</code></td>
<td>
<p>Fold numbers of cross-validation when <code>tune = "cv"</code>.</p>
</td></tr>
<tr><td><code id="amnet_+3A_ebic.gamma">ebic.gamma</code></td>
<td>
<p>Parameter for Extended BIC penalizing
size of the model space when <code>tune = "ebic"</code>,
default is <code>1</code>. For details, see Chen and Chen (2008).</p>
</td></tr>
<tr><td><code id="amnet_+3A_scale">scale</code></td>
<td>
<p>Scaling factor for adaptive weights:
<code>weights = coefficients^(-scale)</code>.</p>
</td></tr>
<tr><td><code id="amnet_+3A_eps">eps</code></td>
<td>
<p>Convergence threshhold to use in MCP-net.</p>
</td></tr>
<tr><td><code id="amnet_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations to use in MCP-net.</p>
</td></tr>
<tr><td><code id="amnet_+3A_penalty.factor.init">penalty.factor.init</code></td>
<td>
<p>The multiplicative factor for the penalty
applied to each coefficient in the initial estimation step. This is
useful for incorporating prior information about variable weights,
for example, emphasizing specific clinical variables. To make certain
variables more likely to be selected, assign a smaller value.
Default is <code>rep(1, ncol(x))</code>.</p>
</td></tr>
<tr><td><code id="amnet_+3A_seed">seed</code></td>
<td>
<p>Random seed for cross-validation fold division.</p>
</td></tr>
<tr><td><code id="amnet_+3A_parallel">parallel</code></td>
<td>
<p>Logical. Enable parallel parameter tuning or not,
default is <code>FALSE</code>. To enable parallel tuning, load the
<code>doParallel</code> package and run <code>registerDoParallel()</code>
with the number of CPU cores before calling this function.</p>
</td></tr>
<tr><td><code id="amnet_+3A_verbose">verbose</code></td>
<td>
<p>Should we print out the estimation progress?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of model coefficients, <code>ncvreg</code> model object,
and the optimal parameter set.
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- msaenet.sim.gaussian(
  n = 150, p = 500, rho = 0.6,
  coef = rep(1, 5), snr = 2, p.train = 0.7,
  seed = 1001
)

amnet.fit &lt;- amnet(
  dat$x.tr, dat$y.tr,
  alphas = seq(0.2, 0.8, 0.2), seed = 1002
)

print(amnet.fit)
msaenet.nzv(amnet.fit)
msaenet.fp(amnet.fit, 1:5)
msaenet.tp(amnet.fit, 1:5)
amnet.pred &lt;- predict(amnet.fit, dat$x.te)
msaenet.rmse(dat$y.te, amnet.pred)
plot(amnet.fit)
</code></pre>

<hr>
<h2 id='asnet'>Adaptive SCAD-Net</h2><span id='topic+asnet'></span>

<h3>Description</h3>

<p>Adaptive SCAD-Net
</p>


<h3>Usage</h3>

<pre><code class='language-R'>asnet(
  x,
  y,
  family = c("gaussian", "binomial", "poisson", "cox"),
  init = c("snet", "ridge"),
  gammas = 3.7,
  alphas = seq(0.05, 0.95, 0.05),
  tune = c("cv", "ebic", "bic", "aic"),
  nfolds = 5L,
  ebic.gamma = 1,
  scale = 1,
  eps = 1e-04,
  max.iter = 10000L,
  penalty.factor.init = rep(1, ncol(x)),
  seed = 1001,
  parallel = FALSE,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="asnet_+3A_x">x</code></td>
<td>
<p>Data matrix.</p>
</td></tr>
<tr><td><code id="asnet_+3A_y">y</code></td>
<td>
<p>Response vector if <code>family</code> is <code>"gaussian"</code>,
<code>"binomial"</code>, or <code>"poisson"</code>. If <code>family</code> is
<code>"cox"</code>, a response matrix created by <code><a href="survival.html#topic+Surv">Surv</a></code>.</p>
</td></tr>
<tr><td><code id="asnet_+3A_family">family</code></td>
<td>
<p>Model family, can be <code>"gaussian"</code>,
<code>"binomial"</code>, <code>"poisson"</code>, or <code>"cox"</code>.</p>
</td></tr>
<tr><td><code id="asnet_+3A_init">init</code></td>
<td>
<p>Type of the penalty used in the initial
estimation step. Can be <code>"snet"</code> or <code>"ridge"</code>.</p>
</td></tr>
<tr><td><code id="asnet_+3A_gammas">gammas</code></td>
<td>
<p>Vector of candidate <code>gamma</code>s (the concavity parameter)
to use in SCAD-Net. Default is <code>3.7</code>.</p>
</td></tr>
<tr><td><code id="asnet_+3A_alphas">alphas</code></td>
<td>
<p>Vector of candidate <code>alpha</code>s to use in SCAD-Net.</p>
</td></tr>
<tr><td><code id="asnet_+3A_tune">tune</code></td>
<td>
<p>Parameter tuning method for each estimation step.
Possible options are <code>"cv"</code>, <code>"ebic"</code>, <code>"bic"</code>,
and <code>"aic"</code>. Default is <code>"cv"</code>.</p>
</td></tr>
<tr><td><code id="asnet_+3A_nfolds">nfolds</code></td>
<td>
<p>Fold numbers of cross-validation when <code>tune = "cv"</code>.</p>
</td></tr>
<tr><td><code id="asnet_+3A_ebic.gamma">ebic.gamma</code></td>
<td>
<p>Parameter for Extended BIC penalizing
size of the model space when <code>tune = "ebic"</code>,
default is <code>1</code>. For details, see Chen and Chen (2008).</p>
</td></tr>
<tr><td><code id="asnet_+3A_scale">scale</code></td>
<td>
<p>Scaling factor for adaptive weights:
<code>weights = coefficients^(-scale)</code>.</p>
</td></tr>
<tr><td><code id="asnet_+3A_eps">eps</code></td>
<td>
<p>Convergence threshhold to use in SCAD-net.</p>
</td></tr>
<tr><td><code id="asnet_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations to use in SCAD-net.</p>
</td></tr>
<tr><td><code id="asnet_+3A_penalty.factor.init">penalty.factor.init</code></td>
<td>
<p>The multiplicative factor for the penalty
applied to each coefficient in the initial estimation step. This is
useful for incorporating prior information about variable weights,
for example, emphasizing specific clinical variables. To make certain
variables more likely to be selected, assign a smaller value.
Default is <code>rep(1, ncol(x))</code>.</p>
</td></tr>
<tr><td><code id="asnet_+3A_seed">seed</code></td>
<td>
<p>Random seed for cross-validation fold division.</p>
</td></tr>
<tr><td><code id="asnet_+3A_parallel">parallel</code></td>
<td>
<p>Logical. Enable parallel parameter tuning or not,
default is <code>FALSE</code>. To enable parallel tuning, load the
<code>doParallel</code> package and run <code>registerDoParallel()</code>
with the number of CPU cores before calling this function.</p>
</td></tr>
<tr><td><code id="asnet_+3A_verbose">verbose</code></td>
<td>
<p>Should we print out the estimation progress?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of model coefficients, <code>ncvreg</code> model object,
and the optimal parameter set.
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- msaenet.sim.gaussian(
  n = 150, p = 500, rho = 0.6,
  coef = rep(1, 5), snr = 2, p.train = 0.7,
  seed = 1001
)

asnet.fit &lt;- asnet(
  dat$x.tr, dat$y.tr,
  alphas = seq(0.2, 0.8, 0.2), seed = 1002
)

print(asnet.fit)
msaenet.nzv(asnet.fit)
msaenet.fp(asnet.fit, 1:5)
msaenet.tp(asnet.fit, 1:5)
asnet.pred &lt;- predict(asnet.fit, dat$x.te)
msaenet.rmse(dat$y.te, asnet.pred)
plot(asnet.fit)
</code></pre>

<hr>
<h2 id='coef.msaenet'>Extract Model Coefficients</h2><span id='topic+coef.msaenet'></span>

<h3>Description</h3>

<p>Extract model coefficients from the final model in msaenet model objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'msaenet'
coef(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.msaenet_+3A_object">object</code></td>
<td>
<p>An object of class <code>msaenet</code> produced
by <code><a href="#topic+aenet">aenet</a></code>, <code>amnet</code>, <code>asnet</code>,
<code><a href="#topic+msaenet">msaenet</a></code>, <code><a href="#topic+msamnet">msamnet</a></code>, or <code><a href="#topic+msasnet">msasnet</a></code>.</p>
</td></tr>
<tr><td><code id="coef.msaenet_+3A_...">...</code></td>
<td>
<p>Additional parameters for <code><a href="stats.html#topic+coef">coef</a></code> (not used).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numerical vector of model coefficients.
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- msaenet.sim.gaussian(
  n = 150, p = 500, rho = 0.6,
  coef = rep(1, 5), snr = 2, p.train = 0.7,
  seed = 1001
)

msaenet.fit &lt;- msaenet(
  dat$x.tr, dat$y.tr,
  alphas = seq(0.2, 0.8, 0.2),
  nsteps = 3L, seed = 1003
)

coef(msaenet.fit)
</code></pre>

<hr>
<h2 id='msaenet'>Multi-Step Adaptive Elastic-Net</h2><span id='topic+msaenet'></span>

<h3>Description</h3>

<p>Multi-Step Adaptive Elastic-Net
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msaenet(
  x,
  y,
  family = c("gaussian", "binomial", "poisson", "cox"),
  init = c("enet", "ridge"),
  alphas = seq(0.05, 0.95, 0.05),
  tune = c("cv", "ebic", "bic", "aic"),
  nfolds = 5L,
  rule = c("lambda.min", "lambda.1se"),
  ebic.gamma = 1,
  nsteps = 2L,
  tune.nsteps = c("max", "ebic", "bic", "aic"),
  ebic.gamma.nsteps = 1,
  scale = 1,
  lower.limits = -Inf,
  upper.limits = Inf,
  penalty.factor.init = rep(1, ncol(x)),
  seed = 1001,
  parallel = FALSE,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="msaenet_+3A_x">x</code></td>
<td>
<p>Data matrix.</p>
</td></tr>
<tr><td><code id="msaenet_+3A_y">y</code></td>
<td>
<p>Response vector if <code>family</code> is <code>"gaussian"</code>,
<code>"binomial"</code>, or <code>"poisson"</code>. If <code>family</code> is
<code>"cox"</code>, a response matrix created by <code><a href="survival.html#topic+Surv">Surv</a></code>.</p>
</td></tr>
<tr><td><code id="msaenet_+3A_family">family</code></td>
<td>
<p>Model family, can be <code>"gaussian"</code>,
<code>"binomial"</code>, <code>"poisson"</code>, or <code>"cox"</code>.</p>
</td></tr>
<tr><td><code id="msaenet_+3A_init">init</code></td>
<td>
<p>Type of the penalty used in the initial
estimation step. Can be <code>"enet"</code> or <code>"ridge"</code>.
See <code><a href="glmnet.html#topic+glmnet">glmnet</a></code> for details.</p>
</td></tr>
<tr><td><code id="msaenet_+3A_alphas">alphas</code></td>
<td>
<p>Vector of candidate <code>alpha</code>s to use in
<code><a href="glmnet.html#topic+cv.glmnet">cv.glmnet</a></code>.</p>
</td></tr>
<tr><td><code id="msaenet_+3A_tune">tune</code></td>
<td>
<p>Parameter tuning method for each estimation step.
Possible options are <code>"cv"</code>, <code>"ebic"</code>, <code>"bic"</code>,
and <code>"aic"</code>. Default is <code>"cv"</code>.</p>
</td></tr>
<tr><td><code id="msaenet_+3A_nfolds">nfolds</code></td>
<td>
<p>Fold numbers of cross-validation when <code>tune = "cv"</code>.</p>
</td></tr>
<tr><td><code id="msaenet_+3A_rule">rule</code></td>
<td>
<p>Lambda selection criterion when <code>tune = "cv"</code>,
can be <code>"lambda.min"</code> or <code>"lambda.1se"</code>.
See <code><a href="glmnet.html#topic+cv.glmnet">cv.glmnet</a></code> for details.</p>
</td></tr>
<tr><td><code id="msaenet_+3A_ebic.gamma">ebic.gamma</code></td>
<td>
<p>Parameter for Extended BIC penalizing
size of the model space when <code>tune = "ebic"</code>,
default is <code>1</code>. For details, see Chen and Chen (2008).</p>
</td></tr>
<tr><td><code id="msaenet_+3A_nsteps">nsteps</code></td>
<td>
<p>Maximum number of adaptive estimation steps.
At least <code>2</code>, assuming adaptive elastic-net has only
one adaptive estimation step.</p>
</td></tr>
<tr><td><code id="msaenet_+3A_tune.nsteps">tune.nsteps</code></td>
<td>
<p>Optimal step number selection method
(aggregate the optimal model from the each step and compare).
Options include <code>"max"</code> (select the final-step model directly),
or compare these models using <code>"ebic"</code>, <code>"bic"</code>, or <code>"aic"</code>.
Default is <code>"max"</code>.</p>
</td></tr>
<tr><td><code id="msaenet_+3A_ebic.gamma.nsteps">ebic.gamma.nsteps</code></td>
<td>
<p>Parameter for Extended BIC penalizing
size of the model space when <code>tune.nsteps = "ebic"</code>,
default is <code>1</code>.</p>
</td></tr>
<tr><td><code id="msaenet_+3A_scale">scale</code></td>
<td>
<p>Scaling factor for adaptive weights:
<code>weights = coefficients^(-scale)</code>.</p>
</td></tr>
<tr><td><code id="msaenet_+3A_lower.limits">lower.limits</code></td>
<td>
<p>Lower limits for coefficients.
Default is <code>-Inf</code>. For details, see <code><a href="glmnet.html#topic+glmnet">glmnet</a></code>.</p>
</td></tr>
<tr><td><code id="msaenet_+3A_upper.limits">upper.limits</code></td>
<td>
<p>Upper limits for coefficients.
Default is <code>Inf</code>. For details, see <code><a href="glmnet.html#topic+glmnet">glmnet</a></code>.</p>
</td></tr>
<tr><td><code id="msaenet_+3A_penalty.factor.init">penalty.factor.init</code></td>
<td>
<p>The multiplicative factor for the penalty
applied to each coefficient in the initial estimation step. This is
useful for incorporating prior information about variable weights,
for example, emphasizing specific clinical variables. To make certain
variables more likely to be selected, assign a smaller value.
Default is <code>rep(1, ncol(x))</code>.</p>
</td></tr>
<tr><td><code id="msaenet_+3A_seed">seed</code></td>
<td>
<p>Random seed for cross-validation fold division.</p>
</td></tr>
<tr><td><code id="msaenet_+3A_parallel">parallel</code></td>
<td>
<p>Logical. Enable parallel parameter tuning or not,
default is <code>FALSE</code>. To enable parallel tuning, load the
<code>doParallel</code> package and run <code>registerDoParallel()</code>
with the number of CPU cores before calling this function.</p>
</td></tr>
<tr><td><code id="msaenet_+3A_verbose">verbose</code></td>
<td>
<p>Should we print out the estimation progress?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of model coefficients, <code>glmnet</code> model object,
and the optimal parameter set.
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>


<h3>References</h3>

<p>Nan Xiao and Qing-Song Xu. (2015). Multi-step adaptive elastic-net:
reducing false positives in high-dimensional variable selection.
<em>Journal of Statistical Computation and Simulation</em> 85(18), 3755&ndash;3765.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- msaenet.sim.gaussian(
  n = 150, p = 500, rho = 0.6,
  coef = rep(1, 5), snr = 2, p.train = 0.7,
  seed = 1001
)

msaenet.fit &lt;- msaenet(
  dat$x.tr, dat$y.tr,
  alphas = seq(0.2, 0.8, 0.2),
  nsteps = 3L, seed = 1003
)

print(msaenet.fit)
msaenet.nzv(msaenet.fit)
msaenet.fp(msaenet.fit, 1:5)
msaenet.tp(msaenet.fit, 1:5)
msaenet.pred &lt;- predict(msaenet.fit, dat$x.te)
msaenet.rmse(dat$y.te, msaenet.pred)
plot(msaenet.fit)
</code></pre>

<hr>
<h2 id='msaenet.fn'>Get the Number of False Negative Selections</h2><span id='topic+msaenet.fn'></span>

<h3>Description</h3>

<p>Get the number of false negative selections from msaenet model objects,
given the indices of true variables (if known).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msaenet.fn(object, true.idx)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="msaenet.fn_+3A_object">object</code></td>
<td>
<p>An object of class <code>msaenet</code> produced
by <code><a href="#topic+aenet">aenet</a></code>, <code>amnet</code>, <code>asnet</code>,
<code><a href="#topic+msaenet">msaenet</a></code>, <code><a href="#topic+msamnet">msamnet</a></code>, or <code><a href="#topic+msasnet">msasnet</a></code>.</p>
</td></tr>
<tr><td><code id="msaenet.fn_+3A_true.idx">true.idx</code></td>
<td>
<p>Vector. Indices of true variables.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Number of false negative variables in the model.
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- msaenet.sim.gaussian(
  n = 150, p = 500, rho = 0.6,
  coef = rep(1, 5), snr = 2, p.train = 0.7,
  seed = 1001
)

msaenet.fit &lt;- msaenet(
  dat$x.tr, dat$y.tr,
  alphas = seq(0.2, 0.8, 0.2),
  nsteps = 3L, seed = 1003
)

msaenet.fn(msaenet.fit, 1:5)
</code></pre>

<hr>
<h2 id='msaenet.fp'>Get the Number of False Positive Selections</h2><span id='topic+msaenet.fp'></span>

<h3>Description</h3>

<p>Get the number of false positive selections from msaenet model objects,
given the indices of true variables (if known).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msaenet.fp(object, true.idx)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="msaenet.fp_+3A_object">object</code></td>
<td>
<p>An object of class <code>msaenet</code> produced
by <code><a href="#topic+aenet">aenet</a></code>, <code>amnet</code>, <code>asnet</code>,
<code><a href="#topic+msaenet">msaenet</a></code>, <code><a href="#topic+msamnet">msamnet</a></code>, or <code><a href="#topic+msasnet">msasnet</a></code>.</p>
</td></tr>
<tr><td><code id="msaenet.fp_+3A_true.idx">true.idx</code></td>
<td>
<p>Vector. Indices of true variables.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Number of false positive variables in the model.
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- msaenet.sim.gaussian(
  n = 150, p = 500, rho = 0.6,
  coef = rep(1, 5), snr = 2, p.train = 0.7,
  seed = 1001
)

msaenet.fit &lt;- msaenet(
  dat$x.tr, dat$y.tr,
  alphas = seq(0.2, 0.8, 0.2),
  nsteps = 3L, seed = 1003
)

msaenet.fp(msaenet.fit, 1:5)
</code></pre>

<hr>
<h2 id='msaenet.mae'>Mean Absolute Error (MAE)</h2><span id='topic+msaenet.mae'></span>

<h3>Description</h3>

<p>Compute mean absolute error (MAE).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msaenet.mae(yreal, ypred)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="msaenet.mae_+3A_yreal">yreal</code></td>
<td>
<p>Vector. True response.</p>
</td></tr>
<tr><td><code id="msaenet.mae_+3A_ypred">ypred</code></td>
<td>
<p>Vector. Predicted response.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>MAE
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>

<hr>
<h2 id='msaenet.mse'>Mean Squared Error (MSE)</h2><span id='topic+msaenet.mse'></span>

<h3>Description</h3>

<p>Compute mean squared error (MSE).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msaenet.mse(yreal, ypred)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="msaenet.mse_+3A_yreal">yreal</code></td>
<td>
<p>Vector. True response.</p>
</td></tr>
<tr><td><code id="msaenet.mse_+3A_ypred">ypred</code></td>
<td>
<p>Vector. Predicted response.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>MSE
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>

<hr>
<h2 id='msaenet.nzv'>Get Indices of Non-Zero Variables</h2><span id='topic+msaenet.nzv'></span>

<h3>Description</h3>

<p>Get the indices of non-zero variables from msaenet model objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msaenet.nzv(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="msaenet.nzv_+3A_object">object</code></td>
<td>
<p>An object of class <code>msaenet</code> produced
by <code><a href="#topic+aenet">aenet</a></code>, <code>amnet</code>, <code>asnet</code>,
<code><a href="#topic+msaenet">msaenet</a></code>, <code><a href="#topic+msamnet">msamnet</a></code>, or <code><a href="#topic+msasnet">msasnet</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Indices vector of non-zero variables in the model.
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- msaenet.sim.gaussian(
  n = 150, p = 500, rho = 0.6,
  coef = rep(1, 5), snr = 2, p.train = 0.7,
  seed = 1001
)

msaenet.fit &lt;- msaenet(
  dat$x.tr, dat$y.tr,
  alphas = seq(0.2, 0.8, 0.2),
  nsteps = 3L, seed = 1003
)

msaenet.nzv(msaenet.fit)

# coefficients of non-zero variables
coef(msaenet.fit)[msaenet.nzv(msaenet.fit)]
</code></pre>

<hr>
<h2 id='msaenet.nzv.all'>Get Indices of Non-Zero Variables in All Steps</h2><span id='topic+msaenet.nzv.all'></span>

<h3>Description</h3>

<p>Get the indices of non-zero variables in all steps from msaenet model objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msaenet.nzv.all(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="msaenet.nzv.all_+3A_object">object</code></td>
<td>
<p>An object of class <code>msaenet</code> produced
by <code><a href="#topic+aenet">aenet</a></code>, <code>amnet</code>, <code>asnet</code>,
<code><a href="#topic+msaenet">msaenet</a></code>, <code><a href="#topic+msamnet">msamnet</a></code>, or <code><a href="#topic+msasnet">msasnet</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing indices vectors of non-zero variables in all steps.
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- msaenet.sim.gaussian(
  n = 150, p = 500, rho = 0.6,
  coef = rep(1, 5), snr = 2, p.train = 0.7,
  seed = 1001
)

msaenet.fit &lt;- msaenet(
  dat$x.tr, dat$y.tr,
  alphas = seq(0.2, 0.8, 0.2),
  nsteps = 3L, seed = 1003
)

msaenet.nzv.all(msaenet.fit)
</code></pre>

<hr>
<h2 id='msaenet.rmse'>Root Mean Squared Error (RMSE)</h2><span id='topic+msaenet.rmse'></span>

<h3>Description</h3>

<p>Compute root mean squared error (RMSE).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msaenet.rmse(yreal, ypred)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="msaenet.rmse_+3A_yreal">yreal</code></td>
<td>
<p>Vector. True response.</p>
</td></tr>
<tr><td><code id="msaenet.rmse_+3A_ypred">ypred</code></td>
<td>
<p>Vector. Predicted response.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>RMSE
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>

<hr>
<h2 id='msaenet.rmsle'>Root Mean Squared Logarithmic Error (RMSLE)</h2><span id='topic+msaenet.rmsle'></span>

<h3>Description</h3>

<p>Compute root mean squared logarithmic error (RMSLE).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msaenet.rmsle(yreal, ypred)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="msaenet.rmsle_+3A_yreal">yreal</code></td>
<td>
<p>Vector. True response.</p>
</td></tr>
<tr><td><code id="msaenet.rmsle_+3A_ypred">ypred</code></td>
<td>
<p>Vector. Predicted response.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>RMSLE
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>

<hr>
<h2 id='msaenet.sim.binomial'>Generate Simulation Data for Benchmarking Sparse Regressions
(Binomial Response)</h2><span id='topic+msaenet.sim.binomial'></span>

<h3>Description</h3>

<p>Generate simulation data for benchmarking sparse logistic regression models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msaenet.sim.binomial(
  n = 300,
  p = 500,
  rho = 0.5,
  coef = rep(0.2, 50),
  snr = 1,
  p.train = 0.7,
  seed = 1001
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="msaenet.sim.binomial_+3A_n">n</code></td>
<td>
<p>Number of observations.</p>
</td></tr>
<tr><td><code id="msaenet.sim.binomial_+3A_p">p</code></td>
<td>
<p>Number of variables.</p>
</td></tr>
<tr><td><code id="msaenet.sim.binomial_+3A_rho">rho</code></td>
<td>
<p>Correlation base for generating correlated variables.</p>
</td></tr>
<tr><td><code id="msaenet.sim.binomial_+3A_coef">coef</code></td>
<td>
<p>Vector of non-zero coefficients.</p>
</td></tr>
<tr><td><code id="msaenet.sim.binomial_+3A_snr">snr</code></td>
<td>
<p>Signal-to-noise ratio (SNR).</p>
</td></tr>
<tr><td><code id="msaenet.sim.binomial_+3A_p.train">p.train</code></td>
<td>
<p>Percentage of training set.</p>
</td></tr>
<tr><td><code id="msaenet.sim.binomial_+3A_seed">seed</code></td>
<td>
<p>Random seed for reproducibility.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of <code>x.tr</code>, <code>x.te</code>, <code>y.tr</code>, and <code>y.te</code>.
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- msaenet.sim.binomial(
  n = 300, p = 500, rho = 0.6,
  coef = rep(1, 10), snr = 3, p.train = 0.7,
  seed = 1001
)

dim(dat$x.tr)
dim(dat$x.te)
table(dat$y.tr)
table(dat$y.te)
</code></pre>

<hr>
<h2 id='msaenet.sim.cox'>Generate Simulation Data for Benchmarking Sparse Regressions (Cox Model)</h2><span id='topic+msaenet.sim.cox'></span>

<h3>Description</h3>

<p>Generate simulation data for benchmarking sparse Cox regression models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msaenet.sim.cox(
  n = 300,
  p = 500,
  rho = 0.5,
  coef = rep(0.2, 50),
  snr = 1,
  p.train = 0.7,
  seed = 1001
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="msaenet.sim.cox_+3A_n">n</code></td>
<td>
<p>Number of observations.</p>
</td></tr>
<tr><td><code id="msaenet.sim.cox_+3A_p">p</code></td>
<td>
<p>Number of variables.</p>
</td></tr>
<tr><td><code id="msaenet.sim.cox_+3A_rho">rho</code></td>
<td>
<p>Correlation base for generating correlated variables.</p>
</td></tr>
<tr><td><code id="msaenet.sim.cox_+3A_coef">coef</code></td>
<td>
<p>Vector of non-zero coefficients.</p>
</td></tr>
<tr><td><code id="msaenet.sim.cox_+3A_snr">snr</code></td>
<td>
<p>Signal-to-noise ratio (SNR).</p>
</td></tr>
<tr><td><code id="msaenet.sim.cox_+3A_p.train">p.train</code></td>
<td>
<p>Percentage of training set.</p>
</td></tr>
<tr><td><code id="msaenet.sim.cox_+3A_seed">seed</code></td>
<td>
<p>Random seed for reproducibility.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of <code>x.tr</code>, <code>x.te</code>, <code>y.tr</code>, and <code>y.te</code>.
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>


<h3>References</h3>

<p>Simon, N., Friedman, J., Hastie, T., &amp; Tibshirani, R. (2011).
Regularization Paths for Cox's Proportional Hazards Model via
Coordinate Descent. <em>Journal of Statistical Software</em>, 39(5), 1&ndash;13.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- msaenet.sim.cox(
  n = 300, p = 500, rho = 0.6,
  coef = rep(1, 10), snr = 3, p.train = 0.7,
  seed = 1001
)

dim(dat$x.tr)
dim(dat$x.te)
dim(dat$y.tr)
dim(dat$y.te)
</code></pre>

<hr>
<h2 id='msaenet.sim.gaussian'>Generate Simulation Data for Benchmarking Sparse Regressions
(Gaussian Response)</h2><span id='topic+msaenet.sim.gaussian'></span>

<h3>Description</h3>

<p>Generate simulation data (Gaussian case) following the
settings in Xiao and Xu (2015).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msaenet.sim.gaussian(
  n = 300,
  p = 500,
  rho = 0.5,
  coef = rep(0.2, 50),
  snr = 1,
  p.train = 0.7,
  seed = 1001
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="msaenet.sim.gaussian_+3A_n">n</code></td>
<td>
<p>Number of observations.</p>
</td></tr>
<tr><td><code id="msaenet.sim.gaussian_+3A_p">p</code></td>
<td>
<p>Number of variables.</p>
</td></tr>
<tr><td><code id="msaenet.sim.gaussian_+3A_rho">rho</code></td>
<td>
<p>Correlation base for generating correlated variables.</p>
</td></tr>
<tr><td><code id="msaenet.sim.gaussian_+3A_coef">coef</code></td>
<td>
<p>Vector of non-zero coefficients.</p>
</td></tr>
<tr><td><code id="msaenet.sim.gaussian_+3A_snr">snr</code></td>
<td>
<p>Signal-to-noise ratio (SNR).
SNR is defined as
</p>
<p style="text-align: center;"><code class="reqn">
\frac{Var(E(y | X))}{Var(Y - E(y | X))} =
\frac{Var(f(X))}{Var(\varepsilon)} =
\frac{Var(X^T \beta)}{Var(\varepsilon)} =
\frac{Var(\beta^T \Sigma \beta)}{\sigma^2}.
</code>
</p>
</td></tr>
<tr><td><code id="msaenet.sim.gaussian_+3A_p.train">p.train</code></td>
<td>
<p>Percentage of training set.</p>
</td></tr>
<tr><td><code id="msaenet.sim.gaussian_+3A_seed">seed</code></td>
<td>
<p>Random seed for reproducibility.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of <code>x.tr</code>, <code>x.te</code>, <code>y.tr</code>, and <code>y.te</code>.
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>


<h3>References</h3>

<p>Nan Xiao and Qing-Song Xu. (2015). Multi-step adaptive elastic-net:
reducing false positives in high-dimensional variable selection.
<em>Journal of Statistical Computation and Simulation</em> 85(18), 3755&ndash;3765.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- msaenet.sim.gaussian(
  n = 300, p = 500, rho = 0.6,
  coef = rep(1, 10), snr = 3, p.train = 0.7,
  seed = 1001
)

dim(dat$x.tr)
dim(dat$x.te)
</code></pre>

<hr>
<h2 id='msaenet.sim.poisson'>Generate Simulation Data for Benchmarking Sparse Regressions
(Poisson Response)</h2><span id='topic+msaenet.sim.poisson'></span>

<h3>Description</h3>

<p>Generate simulation data for benchmarking sparse Poisson regression models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msaenet.sim.poisson(
  n = 300,
  p = 500,
  rho = 0.5,
  coef = rep(0.2, 50),
  snr = 1,
  p.train = 0.7,
  seed = 1001
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="msaenet.sim.poisson_+3A_n">n</code></td>
<td>
<p>Number of observations.</p>
</td></tr>
<tr><td><code id="msaenet.sim.poisson_+3A_p">p</code></td>
<td>
<p>Number of variables.</p>
</td></tr>
<tr><td><code id="msaenet.sim.poisson_+3A_rho">rho</code></td>
<td>
<p>Correlation base for generating correlated variables.</p>
</td></tr>
<tr><td><code id="msaenet.sim.poisson_+3A_coef">coef</code></td>
<td>
<p>Vector of non-zero coefficients.</p>
</td></tr>
<tr><td><code id="msaenet.sim.poisson_+3A_snr">snr</code></td>
<td>
<p>Signal-to-noise ratio (SNR).</p>
</td></tr>
<tr><td><code id="msaenet.sim.poisson_+3A_p.train">p.train</code></td>
<td>
<p>Percentage of training set.</p>
</td></tr>
<tr><td><code id="msaenet.sim.poisson_+3A_seed">seed</code></td>
<td>
<p>Random seed for reproducibility.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of <code>x.tr</code>, <code>x.te</code>, <code>y.tr</code>, and <code>y.te</code>.
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- msaenet.sim.poisson(
  n = 300, p = 500, rho = 0.6,
  coef = rep(1, 10), snr = 3, p.train = 0.7,
  seed = 1001
)

dim(dat$x.tr)
dim(dat$x.te)
</code></pre>

<hr>
<h2 id='msaenet.tp'>Get the Number of True Positive Selections</h2><span id='topic+msaenet.tp'></span>

<h3>Description</h3>

<p>Get the number of true positive selections from msaenet model objects,
given the indices of true variables (if known).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msaenet.tp(object, true.idx)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="msaenet.tp_+3A_object">object</code></td>
<td>
<p>An object of class <code>msaenet</code> produced
by <code><a href="#topic+aenet">aenet</a></code>, <code>amnet</code>, <code>asnet</code>,
<code><a href="#topic+msaenet">msaenet</a></code>, <code><a href="#topic+msamnet">msamnet</a></code>, or <code><a href="#topic+msasnet">msasnet</a></code>.</p>
</td></tr>
<tr><td><code id="msaenet.tp_+3A_true.idx">true.idx</code></td>
<td>
<p>Vector. Indices of true variables.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Number of true positive variables in the model.
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- msaenet.sim.gaussian(
  n = 150, p = 500, rho = 0.6,
  coef = rep(1, 5), snr = 2, p.train = 0.7,
  seed = 1001
)

msaenet.fit &lt;- msaenet(
  dat$x.tr, dat$y.tr,
  alphas = seq(0.2, 0.8, 0.2),
  nsteps = 3L, seed = 1003
)

msaenet.tp(msaenet.fit, 1:5)
</code></pre>

<hr>
<h2 id='msaenet.tune.glmnet'>Automatic (parallel) parameter tuning for glmnet models</h2><span id='topic+msaenet.tune.glmnet'></span>

<h3>Description</h3>

<p>Automatic (parallel) parameter tuning for glmnet models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msaenet.tune.glmnet(
  x,
  y,
  family,
  alphas,
  tune,
  nfolds,
  rule,
  ebic.gamma,
  lower.limits,
  upper.limits,
  seed,
  parallel,
  ...
)
</code></pre>


<h3>Value</h3>

<p>Optimal model object, parameter set, and criterion value
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>


<h3>References</h3>

<p>Chen, Jiahua, and Zehua Chen. (2008).
Extended Bayesian information criteria for model selection with
large model spaces. <em>Biometrika</em> 95(3), 759&ndash;771.
</p>

<hr>
<h2 id='msaenet.tune.ncvreg'>Automatic (parallel) parameter tuning for ncvreg models</h2><span id='topic+msaenet.tune.ncvreg'></span>

<h3>Description</h3>

<p>Automatic (parallel) parameter tuning for ncvreg models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msaenet.tune.ncvreg(
  x,
  y,
  family,
  penalty,
  gammas,
  alphas,
  tune,
  nfolds,
  ebic.gamma,
  eps,
  max.iter,
  seed,
  parallel,
  ...
)
</code></pre>


<h3>Value</h3>

<p>Optimal model object, parameter set, and criterion value
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>


<h3>References</h3>

<p>Chen, Jiahua, and Zehua Chen. (2008).
Extended Bayesian information criteria for model selection with
large model spaces. <em>Biometrika</em> 95(3), 759&ndash;771.
</p>

<hr>
<h2 id='msaenet.tune.nsteps.glmnet'>Select the number of adaptive estimation steps</h2><span id='topic+msaenet.tune.nsteps.glmnet'></span>

<h3>Description</h3>

<p>Select the number of adaptive estimation steps
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msaenet.tune.nsteps.glmnet(model.list, tune.nsteps, ebic.gamma.nsteps)
</code></pre>


<h3>Value</h3>

<p>optimal step number
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>

<hr>
<h2 id='msaenet.tune.nsteps.ncvreg'>Select the number of adaptive estimation steps</h2><span id='topic+msaenet.tune.nsteps.ncvreg'></span>

<h3>Description</h3>

<p>Select the number of adaptive estimation steps
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msaenet.tune.nsteps.ncvreg(model.list, tune.nsteps, ebic.gamma.nsteps)
</code></pre>


<h3>Value</h3>

<p>optimal step number
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>

<hr>
<h2 id='msamnet'>Multi-Step Adaptive MCP-Net</h2><span id='topic+msamnet'></span>

<h3>Description</h3>

<p>Multi-Step Adaptive MCP-Net
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msamnet(
  x,
  y,
  family = c("gaussian", "binomial", "poisson", "cox"),
  init = c("mnet", "ridge"),
  gammas = 3,
  alphas = seq(0.05, 0.95, 0.05),
  tune = c("cv", "ebic", "bic", "aic"),
  nfolds = 5L,
  ebic.gamma = 1,
  nsteps = 2L,
  tune.nsteps = c("max", "ebic", "bic", "aic"),
  ebic.gamma.nsteps = 1,
  scale = 1,
  eps = 1e-04,
  max.iter = 10000L,
  penalty.factor.init = rep(1, ncol(x)),
  seed = 1001,
  parallel = FALSE,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="msamnet_+3A_x">x</code></td>
<td>
<p>Data matrix.</p>
</td></tr>
<tr><td><code id="msamnet_+3A_y">y</code></td>
<td>
<p>Response vector if <code>family</code> is <code>"gaussian"</code>,
<code>"binomial"</code>, or <code>"poisson"</code>. If <code>family</code> is
<code>"cox"</code>, a response matrix created by <code><a href="survival.html#topic+Surv">Surv</a></code>.</p>
</td></tr>
<tr><td><code id="msamnet_+3A_family">family</code></td>
<td>
<p>Model family, can be <code>"gaussian"</code>,
<code>"binomial"</code>, <code>"poisson"</code>, or <code>"cox"</code>.</p>
</td></tr>
<tr><td><code id="msamnet_+3A_init">init</code></td>
<td>
<p>Type of the penalty used in the initial
estimation step. Can be <code>"mnet"</code> or <code>"ridge"</code>.</p>
</td></tr>
<tr><td><code id="msamnet_+3A_gammas">gammas</code></td>
<td>
<p>Vector of candidate <code>gamma</code>s (the concavity parameter)
to use in MCP-Net. Default is <code>3</code>.</p>
</td></tr>
<tr><td><code id="msamnet_+3A_alphas">alphas</code></td>
<td>
<p>Vector of candidate <code>alpha</code>s to use in MCP-Net.</p>
</td></tr>
<tr><td><code id="msamnet_+3A_tune">tune</code></td>
<td>
<p>Parameter tuning method for each estimation step.
Possible options are <code>"cv"</code>, <code>"ebic"</code>, <code>"bic"</code>,
and <code>"aic"</code>. Default is <code>"cv"</code>.</p>
</td></tr>
<tr><td><code id="msamnet_+3A_nfolds">nfolds</code></td>
<td>
<p>Fold numbers of cross-validation when <code>tune = "cv"</code>.</p>
</td></tr>
<tr><td><code id="msamnet_+3A_ebic.gamma">ebic.gamma</code></td>
<td>
<p>Parameter for Extended BIC penalizing
size of the model space when <code>tune = "ebic"</code>,
default is <code>1</code>. For details, see Chen and Chen (2008).</p>
</td></tr>
<tr><td><code id="msamnet_+3A_nsteps">nsteps</code></td>
<td>
<p>Maximum number of adaptive estimation steps.
At least <code>2</code>, assuming adaptive MCP-net has only
one adaptive estimation step.</p>
</td></tr>
<tr><td><code id="msamnet_+3A_tune.nsteps">tune.nsteps</code></td>
<td>
<p>Optimal step number selection method
(aggregate the optimal model from the each step and compare).
Options include <code>"max"</code> (select the final-step model directly),
or compare these models using <code>"ebic"</code>, <code>"bic"</code>, or <code>"aic"</code>.
Default is <code>"max"</code>.</p>
</td></tr>
<tr><td><code id="msamnet_+3A_ebic.gamma.nsteps">ebic.gamma.nsteps</code></td>
<td>
<p>Parameter for Extended BIC penalizing
size of the model space when <code>tune.nsteps = "ebic"</code>,
default is <code>1</code>.</p>
</td></tr>
<tr><td><code id="msamnet_+3A_scale">scale</code></td>
<td>
<p>Scaling factor for adaptive weights:
<code>weights = coefficients^(-scale)</code>.</p>
</td></tr>
<tr><td><code id="msamnet_+3A_eps">eps</code></td>
<td>
<p>Convergence threshhold to use in MCP-net.</p>
</td></tr>
<tr><td><code id="msamnet_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations to use in MCP-net.</p>
</td></tr>
<tr><td><code id="msamnet_+3A_penalty.factor.init">penalty.factor.init</code></td>
<td>
<p>The multiplicative factor for the penalty
applied to each coefficient in the initial estimation step. This is
useful for incorporating prior information about variable weights,
for example, emphasizing specific clinical variables. To make certain
variables more likely to be selected, assign a smaller value.
Default is <code>rep(1, ncol(x))</code>.</p>
</td></tr>
<tr><td><code id="msamnet_+3A_seed">seed</code></td>
<td>
<p>Random seed for cross-validation fold division.</p>
</td></tr>
<tr><td><code id="msamnet_+3A_parallel">parallel</code></td>
<td>
<p>Logical. Enable parallel parameter tuning or not,
default is <code>FALSE</code>. To enable parallel tuning, load the
<code>doParallel</code> package and run <code>registerDoParallel()</code>
with the number of CPU cores before calling this function.</p>
</td></tr>
<tr><td><code id="msamnet_+3A_verbose">verbose</code></td>
<td>
<p>Should we print out the estimation progress?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of model coefficients, <code>ncvreg</code> model object,
and the optimal parameter set.
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- msaenet.sim.gaussian(
  n = 150, p = 500, rho = 0.6,
  coef = rep(1, 5), snr = 2, p.train = 0.7,
  seed = 1001
)

msamnet.fit &lt;- msamnet(
  dat$x.tr, dat$y.tr,
  alphas = seq(0.3, 0.9, 0.3),
  nsteps = 3L, seed = 1003
)

print(msamnet.fit)
msaenet.nzv(msamnet.fit)
msaenet.fp(msamnet.fit, 1:5)
msaenet.tp(msamnet.fit, 1:5)
msamnet.pred &lt;- predict(msamnet.fit, dat$x.te)
msaenet.rmse(dat$y.te, msamnet.pred)
plot(msamnet.fit)
</code></pre>

<hr>
<h2 id='msasnet'>Multi-Step Adaptive SCAD-Net</h2><span id='topic+msasnet'></span>

<h3>Description</h3>

<p>Multi-Step Adaptive SCAD-Net
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msasnet(
  x,
  y,
  family = c("gaussian", "binomial", "poisson", "cox"),
  init = c("snet", "ridge"),
  gammas = 3.7,
  alphas = seq(0.05, 0.95, 0.05),
  tune = c("cv", "ebic", "bic", "aic"),
  nfolds = 5L,
  ebic.gamma = 1,
  nsteps = 2L,
  tune.nsteps = c("max", "ebic", "bic", "aic"),
  ebic.gamma.nsteps = 1,
  scale = 1,
  eps = 1e-04,
  max.iter = 10000L,
  penalty.factor.init = rep(1, ncol(x)),
  seed = 1001,
  parallel = FALSE,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="msasnet_+3A_x">x</code></td>
<td>
<p>Data matrix.</p>
</td></tr>
<tr><td><code id="msasnet_+3A_y">y</code></td>
<td>
<p>Response vector if <code>family</code> is <code>"gaussian"</code>,
<code>"binomial"</code>, or <code>"poisson"</code>. If <code>family</code> is
<code>"cox"</code>, a response matrix created by <code><a href="survival.html#topic+Surv">Surv</a></code>.</p>
</td></tr>
<tr><td><code id="msasnet_+3A_family">family</code></td>
<td>
<p>Model family, can be <code>"gaussian"</code>,
<code>"binomial"</code>, <code>"poisson"</code>, or <code>"cox"</code>.</p>
</td></tr>
<tr><td><code id="msasnet_+3A_init">init</code></td>
<td>
<p>Type of the penalty used in the initial
estimation step. Can be <code>"snet"</code> or <code>"ridge"</code>.</p>
</td></tr>
<tr><td><code id="msasnet_+3A_gammas">gammas</code></td>
<td>
<p>Vector of candidate <code>gamma</code>s (the concavity parameter)
to use in SCAD-Net. Default is <code>3.7</code>.</p>
</td></tr>
<tr><td><code id="msasnet_+3A_alphas">alphas</code></td>
<td>
<p>Vector of candidate <code>alpha</code>s to use in SCAD-Net.</p>
</td></tr>
<tr><td><code id="msasnet_+3A_tune">tune</code></td>
<td>
<p>Parameter tuning method for each estimation step.
Possible options are <code>"cv"</code>, <code>"ebic"</code>, <code>"bic"</code>,
and <code>"aic"</code>. Default is <code>"cv"</code>.</p>
</td></tr>
<tr><td><code id="msasnet_+3A_nfolds">nfolds</code></td>
<td>
<p>Fold numbers of cross-validation when <code>tune = "cv"</code>.</p>
</td></tr>
<tr><td><code id="msasnet_+3A_ebic.gamma">ebic.gamma</code></td>
<td>
<p>Parameter for Extended BIC penalizing
size of the model space when <code>tune = "ebic"</code>,
default is <code>1</code>. For details, see Chen and Chen (2008).</p>
</td></tr>
<tr><td><code id="msasnet_+3A_nsteps">nsteps</code></td>
<td>
<p>Maximum number of adaptive estimation steps.
At least <code>2</code>, assuming adaptive SCAD-net has only
one adaptive estimation step.</p>
</td></tr>
<tr><td><code id="msasnet_+3A_tune.nsteps">tune.nsteps</code></td>
<td>
<p>Optimal step number selection method
(aggregate the optimal model from the each step and compare).
Options include <code>"max"</code> (select the final-step model directly),
or compare these models using <code>"ebic"</code>, <code>"bic"</code>, or <code>"aic"</code>.
Default is <code>"max"</code>.</p>
</td></tr>
<tr><td><code id="msasnet_+3A_ebic.gamma.nsteps">ebic.gamma.nsteps</code></td>
<td>
<p>Parameter for Extended BIC penalizing
size of the model space when <code>tune.nsteps = "ebic"</code>,
default is <code>1</code>.</p>
</td></tr>
<tr><td><code id="msasnet_+3A_scale">scale</code></td>
<td>
<p>Scaling factor for adaptive weights:
<code>weights = coefficients^(-scale)</code>.</p>
</td></tr>
<tr><td><code id="msasnet_+3A_eps">eps</code></td>
<td>
<p>Convergence threshhold to use in SCAD-net.</p>
</td></tr>
<tr><td><code id="msasnet_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations to use in SCAD-net.</p>
</td></tr>
<tr><td><code id="msasnet_+3A_penalty.factor.init">penalty.factor.init</code></td>
<td>
<p>The multiplicative factor for the penalty
applied to each coefficient in the initial estimation step. This is
useful for incorporating prior information about variable weights,
for example, emphasizing specific clinical variables. To make certain
variables more likely to be selected, assign a smaller value.
Default is <code>rep(1, ncol(x))</code>.</p>
</td></tr>
<tr><td><code id="msasnet_+3A_seed">seed</code></td>
<td>
<p>Random seed for cross-validation fold division.</p>
</td></tr>
<tr><td><code id="msasnet_+3A_parallel">parallel</code></td>
<td>
<p>Logical. Enable parallel parameter tuning or not,
default is <code>FALSE</code>. To enable parallel tuning, load the
<code>doParallel</code> package and run <code>registerDoParallel()</code>
with the number of CPU cores before calling this function.</p>
</td></tr>
<tr><td><code id="msasnet_+3A_verbose">verbose</code></td>
<td>
<p>Should we print out the estimation progress?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of model coefficients, <code>ncvreg</code> model object,
and the optimal parameter set.
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- msaenet.sim.gaussian(
  n = 150, p = 500, rho = 0.6,
  coef = rep(1, 5), snr = 2, p.train = 0.7,
  seed = 1001
)

msasnet.fit &lt;- msasnet(
  dat$x.tr, dat$y.tr,
  alphas = seq(0.3, 0.9, 0.3),
  nsteps = 3L, seed = 1003
)

print(msasnet.fit)
msaenet.nzv(msasnet.fit)
msaenet.fp(msasnet.fit, 1:5)
msaenet.tp(msasnet.fit, 1:5)
msasnet.pred &lt;- predict(msasnet.fit, dat$x.te)
msaenet.rmse(dat$y.te, msasnet.pred)
plot(msasnet.fit)
</code></pre>

<hr>
<h2 id='plot.msaenet'>Plot msaenet Model Objects</h2><span id='topic+plot.msaenet'></span>

<h3>Description</h3>

<p>Plot msaenet model objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'msaenet'
plot(
  x,
  type = c("coef", "criterion", "dotplot"),
  nsteps = NULL,
  highlight = TRUE,
  col = NULL,
  label = FALSE,
  label.vars = NULL,
  label.pos = 2,
  label.offset = 0.3,
  label.cex = 0.7,
  label.srt = 90,
  xlab = NULL,
  ylab = NULL,
  abs = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.msaenet_+3A_x">x</code></td>
<td>
<p>An object of class <code>msaenet</code> produced
by <code><a href="#topic+aenet">aenet</a></code>, <code>amnet</code>, <code>asnet</code>,
<code><a href="#topic+msaenet">msaenet</a></code>, <code><a href="#topic+msamnet">msamnet</a></code>, or <code><a href="#topic+msasnet">msasnet</a></code>.</p>
</td></tr>
<tr><td><code id="plot.msaenet_+3A_type">type</code></td>
<td>
<p>Plot type, <code>"coef"</code> for a coefficient path plot
across all estimation steps; <code>"criterion"</code> for a scree plot of
the model evaluation criterion used (CV error, AIC, BIC, or EBIC);
<code>"dotplot"</code> for a Cleveland dot plot of the coefficients
estimated by the model at the optimal step.</p>
</td></tr>
<tr><td><code id="plot.msaenet_+3A_nsteps">nsteps</code></td>
<td>
<p>Maximum number of estimation steps to plot.
Default is to plot all steps.</p>
</td></tr>
<tr><td><code id="plot.msaenet_+3A_highlight">highlight</code></td>
<td>
<p>Should we highlight the &quot;optimal&quot; step
according to the criterion? Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="plot.msaenet_+3A_col">col</code></td>
<td>
<p>Color palette to use for the coefficient paths.
If it is <code>NULL</code>, a default color palette will be assigned.</p>
</td></tr>
<tr><td><code id="plot.msaenet_+3A_label">label</code></td>
<td>
<p>Should we label all the non-zero variables of the
optimal step in the coefficient plot or the dot plot?
Default is <code>FALSE</code>. If <code>TRUE</code> and <code>label.vars = NULL</code>,
the index of the non-zero variables will be used as labels.</p>
</td></tr>
<tr><td><code id="plot.msaenet_+3A_label.vars">label.vars</code></td>
<td>
<p>Labels to use for all the variables
if <code>label = "TRUE"</code>.</p>
</td></tr>
<tr><td><code id="plot.msaenet_+3A_label.pos">label.pos</code></td>
<td>
<p>Position of the labels. See argument
<code>pos</code> in <code><a href="graphics.html#topic+text">text</a></code> for details.</p>
</td></tr>
<tr><td><code id="plot.msaenet_+3A_label.offset">label.offset</code></td>
<td>
<p>Offset of the labels. See argument
<code>offset</code> in <code><a href="graphics.html#topic+text">text</a></code> for details.</p>
</td></tr>
<tr><td><code id="plot.msaenet_+3A_label.cex">label.cex</code></td>
<td>
<p>Character expansion factor of the labels.
See argument <code>cex</code> in <code><a href="graphics.html#topic+text">text</a></code> for details.</p>
</td></tr>
<tr><td><code id="plot.msaenet_+3A_label.srt">label.srt</code></td>
<td>
<p>Label rotation in degrees for the Cleveland dot plot.
Default is <code>90</code>. See argument <code>srt</code> in
<code><a href="graphics.html#topic+par">par</a></code> for details.</p>
</td></tr>
<tr><td><code id="plot.msaenet_+3A_xlab">xlab</code></td>
<td>
<p>Title for x axis. If is <code>NULL</code>, will use the default title.</p>
</td></tr>
<tr><td><code id="plot.msaenet_+3A_ylab">ylab</code></td>
<td>
<p>Title for y axis. If is <code>NULL</code>, will use the default title.</p>
</td></tr>
<tr><td><code id="plot.msaenet_+3A_abs">abs</code></td>
<td>
<p>Should we plot the absolute values of the coefficients
instead of the raw coefficients in the Cleveland dot plot?
Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="plot.msaenet_+3A_...">...</code></td>
<td>
<p>Other parameters (not used).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
dat &lt;- msaenet.sim.gaussian(
  n = 150, p = 500, rho = 0.6,
  coef = rep(1, 5), snr = 2, p.train = 0.7,
  seed = 1001
)

fit &lt;- msaenet(
  dat$x.tr, dat$y.tr,
  alphas = seq(0.2, 0.8, 0.2),
  nsteps = 5L, tune.nsteps = "bic",
  seed = 1002
)

plot(fit)
plot(fit, label = TRUE)
plot(fit, label = TRUE, nsteps = 5)
plot(fit, type = "criterion")
plot(fit, type = "criterion", nsteps = 5)
plot(fit, type = "dotplot", label = TRUE)
plot(fit, type = "dotplot", label = TRUE, abs = TRUE)

</code></pre>

<hr>
<h2 id='predict.msaenet'>Make Predictions from an msaenet Model</h2><span id='topic+predict.msaenet'></span>

<h3>Description</h3>

<p>Make predictions on new data by a msaenet model object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'msaenet'
predict(object, newx, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.msaenet_+3A_object">object</code></td>
<td>
<p>An object of class <code>msaenet</code> produced
by <code><a href="#topic+aenet">aenet</a></code>, <code>amnet</code>, <code>asnet</code>,
<code><a href="#topic+msaenet">msaenet</a></code>, <code><a href="#topic+msamnet">msamnet</a></code>, or <code><a href="#topic+msasnet">msasnet</a></code>.</p>
</td></tr>
<tr><td><code id="predict.msaenet_+3A_newx">newx</code></td>
<td>
<p>New data to predict with.</p>
</td></tr>
<tr><td><code id="predict.msaenet_+3A_...">...</code></td>
<td>
<p>Additional parameters, particularly prediction <code>type</code> in
<code><a href="glmnet.html#topic+predict.glmnet">predict.glmnet</a></code>, <code><a href="ncvreg.html#topic+predict.ncvreg">predict.ncvreg</a></code>,
or <code><a href="ncvreg.html#topic+predict.ncvsurv">predict.ncvsurv</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric matrix of the predicted values.
</p>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- msaenet.sim.gaussian(
  n = 150, p = 500, rho = 0.6,
  coef = rep(1, 5), snr = 2, p.train = 0.7,
  seed = 1001
)

msaenet.fit &lt;- msaenet(
  dat$x.tr, dat$y.tr,
  alphas = seq(0.2, 0.8, 0.2),
  nsteps = 3L, seed = 1003
)

msaenet.pred &lt;- predict(msaenet.fit, dat$x.te)
msaenet.rmse(dat$y.te, msaenet.pred)
</code></pre>

<hr>
<h2 id='print.msaenet'>Print msaenet Model Information</h2><span id='topic+print.msaenet'></span>

<h3>Description</h3>

<p>Print msaenet model objects (currently, only
printing the model information of the final step).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'msaenet'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.msaenet_+3A_x">x</code></td>
<td>
<p>An object of class <code>msaenet</code>.</p>
</td></tr>
<tr><td><code id="print.msaenet_+3A_...">...</code></td>
<td>
<p>Additional parameters for <code><a href="base.html#topic+print">print</a></code> (not used).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Nan Xiao &lt;<a href="https://nanx.me">https://nanx.me</a>&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- msaenet.sim.gaussian(
  n = 150, p = 500, rho = 0.6,
  coef = rep(1, 5), snr = 2, p.train = 0.7,
  seed = 1001
)

msaenet.fit &lt;- msaenet(
  dat$x.tr, dat$y.tr,
  alphas = seq(0.2, 0.8, 0.2),
  nsteps = 3L, seed = 1003
)

print(msaenet.fit)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
