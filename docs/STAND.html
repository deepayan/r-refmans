<!DOCTYPE html><html lang="en"><head><title>Help for package STAND</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {STAND}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#About-STAND'><p>Statistical Analysis of Non-detect Data</p></a></li>
<li><a href='#aihand'><p>Industrial Hygiene Air Monitoring Data</p></a></li>
<li><a href='#beTWA'><p>TWA Beryllium Exposure Data</p></a></li>
<li><a href='#cansdata'><p>Container Data Used To Evaluate Beryllium Surface Contamination</p></a></li>
<li><a href='#efclnp'><p>Nonparametric Confidence Limits for the Exceedance Fraction</p></a></li>
<li><a href='#efraction.exact'><p>Exceedance Fraction and Exact Confidence Limits</p></a></li>
<li><a href='#efraction.ml'><p> Calculate ML Estimate of Exceedance Fraction and Confidence Limits</p></a></li>
<li><a href='#filmbadge'><p>Quarterly Film Badge Data</p></a></li>
<li><a href='#icfit'><p>Calculates the Self-Consistent Estimate of Survival from</p>
Interval Censored Data</a></li>
<li><a href='#icplot'><p>Plots Survival Functions</p></a></li>
<li><a href='#ictest'><p>Performs Tests for Interval Censored Data</p></a></li>
<li><a href='#IH.summary'><p>Summary Statistic for Samples With Non-detects</p></a></li>
<li><a href='#kmms'><p>Kaplan-Meier (KM) Mean and Standard Error</p></a></li>
<li><a href='#lnorm.ml'><p> ML Estimation for Lognormal Data with Non-detects</p></a></li>
<li><a href='#npower.lnorm'><p>Sample Size and Power For Lognormal Distribution</p></a></li>
<li><a href='#nptl'><p>Nonparametric Upper Tolerance Limit</p></a></li>
<li><a href='#percentile.exact'><p>Estimate of Xp and Exact Confidence Limits for Normal/Lognormal</p></a></li>
<li><a href='#percentile.ml'><p>Calculate ML Estimate of Xp and Confidence Limits</p></a></li>
<li><a href='#percentile.ple'><p>Calculate Nonparametric Estimate of Xp and Confidence Limits</p></a></li>
<li><a href='#ple.plot'><p> Plot PLE With Confidence Limits</p></a></li>
<li><a href='#pleicf'><p>Product Limit Estimate for Interval Censored Data</p></a></li>
<li><a href='#plekm'><p>Product Limit Estimate for Non-detects Using Kaplan-Meier</p></a></li>
<li><a href='#plend'><p> Compute Product Limit Estimate for Non-detects</p></a></li>
<li><a href='#qq.lnorm'><p> Quantile-Quantile Plot for Censored Lognormal Data</p></a></li>
<li><a href='#readss'><p>Read Analyze Data From ASCII  File</p></a></li>
<li><a href='#SESdata'><p>Samples from Elevated Surfaces of a Smelter</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Statistical Analysis of Non-Detects</td>
</tr>
<tr>
<td>Version:</td>
<td>2.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2015-09-10</td>
</tr>
<tr>
<td>Author:</td>
<td>E. L. Frome &lt;fromeEL@ornl.gov&gt; and D. P. Frome</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>E. P. Adams &lt;eric.adams@orau.org&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides functions for the analysis of
        occupational and environmental data with non-detects. Maximum
        likelihood (ML) methods for censored log-normal data and
        non-parametric methods based on the product limit estimate (PLE)
        for left censored data are used to calculate all of the
        statistics recommended by the American Industrial Hygiene
        Association (AIHA) for the complete data case. Functions for
        the analysis of complete samples using exact methods are also
        provided for the lognormal model. Revised from 2007-11-05
        'survfit~1'.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://www.csm.ornl.gov/esh/statoed/">http://www.csm.ornl.gov/esh/statoed/</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>survival</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2015-09-25 14:20:17 UTC; AdminEA</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2015-09-25 17:03:51</td>
</tr>
</table>
<hr>
<h2 id='About-STAND'>Statistical Analysis of Non-detect Data</h2><span id='topic+About-STAND'></span><span id='topic+STAND'></span>

<h3>Description</h3>

<p> Environmental exposure measurements are, in general,
positive and may be subject to left censoring; i.e., the measured
value is less than a &quot;detection limit&quot;, and is referred to as a
non-detect or &quot;less than&quot; value.  This package calculates the censored
data equivalent of a number of statistics that are used in the
analysis of environmental data that do not contain non-detects,
i.e. the usual complete data case.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> stand</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 2.0</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2015-09-10</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL version 2 or newer</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>In occupational monitoring, strategies for
assessing workplace exposures typically focus on the mean exposure
level or the probability that any measurement exceeds a limit.
Parametric methods used to determine acceptable levels of exposure are
based on a two parameter lognormal distribution. The mean exposure
level, an upper percentile, the exceedance fraction, and confidence
limits for each of these statistics are calculated.  Statistical
methods for random samples (without non-detects) from the lognormal
distribution are well known for each of these situations&ndash;see
e.g., Lyles and Kupper (1996).  In this package the maximum likelihood
method for randomly left censored lognormal data is used to calculate
these statistics, and graphical methods are provided to evaluate the
lognormal assumption.  Nonparametric methods based on the product
limit estimate for left censored data are used to estimate the mean
exposure level, and the upper confidence limit on an upper percentile
(i.e., the upper tolerance limit) is obtained using a nonparametric
approach.
</p>
<p>The American Industrial Hygiene Association (AIHA) has published a
consensus standard with two basic strategies for evaluating an
exposure profile&mdash;see Mulhausen and Damiano(1998), Ignacio and
Bullock (2006).  Most of the AIHA methods are based on the assumptions
that the exposure data does not contain non-detects, and that a
lognormal distribution can be used to describe the data. Exposure
monitoring results from a compliant workplace tend to contain a high
percentage of non-detected results when the detection limit is close
to the exposure limit, and in some situations, the lognormal
assumption may not be reasonable.  The function
<code><a href="#topic+IH.summary">IH.summary</a></code> calculates most of the statistics proposed by
AIHA for exposure data with non-detects. All of the methods are
described in the report Frome and Wambach (2005).  </p>


<h3>Acknowledgements</h3>

<p>This work was supported in part by the Office of
Health, Safety, and Security of the U. S. Department of Energy and was
performed in the Computer Science and Mathematics Division (CSMD) at
the Oak Ridge National Laboratory, which is managed by UT-Battelle,
LLC under Contract No. DE-AC05-00OR22725.  Additional funding and
oversight have been provided through the Occupational Exposure and
Worker Studies Group, Oak Ridge Institute for Science and Education,
which is managed by Oak Ridge Associated Universities under Contract
No. DE-AC05-060R23100.
</p>
<p>This report was prepared as an account of work sponsored by an agency
of the United States Government. Neither the United States government
nor any agency thereof, nor any of their employees, makes any
warranty, express or implied, or assumes any legal liability or
responsibility for the accuracy, completeness, or usefulness of any
information, apparatus, product, or process disclosed, or represents
that its use would not infringe privately owned rights. Reference
herein to any specific commercial product, process, or service by
trade name, trademark, manufacturer, or otherwise, does not
necessarily constitute or imply its endorsement, recommendation, or
favoring by the United States Government or any agency thereof. The
views and opinions of authors expressed herein do not necessarily
state or reflect those of the United States Government or any agency
thereof.
</p>
<p>The work has been authored by a contractor of the U.S. Government.
Accordingly, the U.S. Government retains a nonexclusive, royalty-free
license to publish or reproduce the published form of this work, or to
allow others to do so for U. S. Government purposes.
</p>


<h3>Note</h3>

<p>Throughout this document and the online help files the greek letter <code class="reqn">\gamma</code> is used to represent the confidence level for a one-sided confidence limit (default value 0.95). This is represented by <code>gam</code> or <code>gamma</code> in the argument list and value of functions that compute confidence limits.</p>


<h3>References</h3>

<p>Aitchison, J. and J. A. C. Brown (1969), <em>The Lognormal Distribution</em>,
Cambridge, U.K., Cambridge University Press.
</p>
<p>Akritas, M. G., T. F. Ruscitti, and G. S. Patil (1994), &quot;Statistical
Analysis of Censored Environmental Data,&quot; <em>Handbook of Statistics</em>,
Vol. 12, G. P. Patil and C. R. Rao (eds), 221-242, Elsevier Science, New
York.
</p>
<p>American Conference of Governmental Industrial Hygienists (ACGIH) (2004),
&quot;Notice of Intended Change In: 2004 TLVs and BEIs,&quot; <em>ACGIH</em>, p. 60,
Cincinnati, OH.
</p>
<p>Burrows, G. L. (1963), &quot;Statistical Tolerance Limits - What are They,&quot;
<em>Applied Statistics</em>, 12, 133-144.
</p>
<p>Armstrong, B. G. (1992), &quot;Confidence Intervals for Arithmetic Means of
Lognormally Distributed Exposures,&quot; <em>American Industrial Hygiene
Association Journal</em>, 53(8), 481-485.
</p>
<p>Chambers, J. M., W. S. Cleveland, B. Kleiner, and P. A. Tukey (1983),
<em>Graphical Methods for Data Analysis</em>, Duxbury Press, Boston.
</p>
<p>Clopper, C. J. and Pearson, E. S. (1934), &quot;The Use of Confidence or
Fiducial Limits Illustrated in the Case of the Binomial,&quot; <em>Biometrika</em>,
26, 404-413.
</p>
<p>Cohen, A. C. (1991), <em>Truncated and Censored Samples</em>, Marcel Dekker,
Inc., New York.
</p>
<p>Crow, E. L. and K. Shimizu (1988), <em>Lognormal Distribution</em>, Marcel
Decker, New York.
</p>
<p>Cox, D. R. and D. V. Hinkley (1979), <em>Theoretical Statistics</em>, Chapman and
Hall, New York.
</p>
<p>Cox, D. R. and D. Oakes (1984), <em>Analysis of Survival Data</em>, Chapman and
Hall, New York.
</p>
<p>Department of Energy (December, 1999), &quot;Chronic Beryllium Disease
Prevention Program, Federal Register,&quot; <em>10 CFR Part 850</em>, volume 64, number 235, 68854-68914.
</p>
<p>Fowlkes, E. B. (1979), &quot;Some Methods for Studying the Mixture of Two
Normal (Lognormal) Distributions,&quot; <em>Journal of the American Statistical
Association</em>, 74, 561-575.
</p>
<p>Frome, E. L. and Wambach, P. F. (2005), &quot;Statistical Methods and Software for
the Analysis of Occupational Exposure Data with Non-Detectable Values,&quot;
<em>ORNL/TM-2005/52,Oak Ridge National Laboratory</em>, Oak Ridge, TN 37830.
Available at: <a href="http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf">http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf</a>
</p>
<p>Hahn, G. J. and W. Q. Meeker (1991), <em>Statistical Intervals</em>, John Wiley
and Sons, New York.
</p>
<p>Hewett, P. and G. H. Ganser, (1997), &quot;Simple Procedures for Calculating
Confidence Intervals Around the Sample Mean and Exceedance Fraction
Derived from Lognormally Distributed Data,&quot; <em>Applied Occupational and
Environmental Hygiene</em>, 12(2), 132-147.
</p>
<p>Helsel, D. (1990), &quot;Less Than Obvious: Statistical Treatment of Date
Below the Detection Limit,&quot; <em>Environmental Science and Technology</em>,
24(12), 1767-1774.
</p>
<p>Hesel, D. R. and T. A. Cohn (1988), &quot;Estimation of Descriptive Statistics
for Multiply Censored Water Quality Data,&quot; <em>Water Resources Research</em>,
24, 1997-2004.
</p>
<p>Johnson, N. L. and B. L. Welch (1940), &quot;Application of the Non-Central
t-Distribution,&quot; <em>Biometrika</em>, 31(3/4), 362-389.
</p>
<p>Ignacio, J. S. and W. H. Bullock (2006), <em>A Strategy for Assessing
and Managing Occupational Exposures</em>, Third Edition, AIHA Press,
Fairfax, VA.
</p>
<p>Kalbfleisch, J. D. and R. L. Prentice (1980), <em>The Statistical Analysis
of Failure Time Data</em>, John Wiley and Sons, New York.
</p>
<p>Kaplan, E. L. and Meir, P. (1958), &quot;Nonparametric Estimation from
Incomplete Observations,&quot; <em>Journal of the American Statistical
Association</em>, 457-481.
</p>
<p>Land, C. E. (1972), &quot;An Evaluation of Approximate Confidence Interval
Estimation Methods for Lognormal Means,&quot; <em>Technometrics</em>, 14(1),
145-158.
</p>
<p>Lyles R. H. and L. L. Kupper (1996), &quot;On Strategies for Comparing Occupational
Exposure Data to Limits,&quot; <em>American Industrial Hygiene Association
Journal</em>, 57:6-15.
</p>
<p>Meeker, W. Q. and L. A. Escobar (1998), <em>Statistical Methods for
Reliability Data</em>, John Wiley and Sons, New York.
</p>
<p>Moulton, L. H. and N. A. Halsey (1995), &quot;A Mixture Model with Detection
Limits for Regression Analysis of Antibody Response on Vaccine,&quot;
<em>Biometrics</em>, 51, 1570-1578.
</p>
<p>Mulhausen, J. R. and J. Damiano (1998), <em> A Strategy for Assessing
and Managing Occupational Exposures</em>, Second Edition, AIHA Press, Fairfax, VA.
</p>
<p>Neuman, M. C., P. M. Dixon, B. B. Looney, and J. E. Pinder (1989),
&quot;Estimating Mean and Variance for Environmental Samples with Below
Detection Limit Observations,&quot; <em>Water Resources Bulletin</em>, 25, 905-916.
</p>
<p>Ng, M. P. (2002), &quot;A Modification of Peto's Nonparametric Estimation of
Survival Curves for Interval-Censored Data,&quot; <em>Biometrics</em>, volume 58,
number 2, pp. 439-442.
</p>
<p>Odeh, R. E. and D. B. Owen (1980), <em>Tables for Normal Tolerance Limits,
Sampling Plans, and Screening</em>, Marcel Deker, New York.
</p>
<p>Peto, R. (1973), &quot;Experimental Survival Curves for Interval-censored
Data,&quot; <em>Applied Statistics</em>, volume 22, number 1, pp. 86-91.
</p>
<p>Schmee, J., D. Gladstein, and W. Nelson (1985), &quot;Confidence Limits for
Parameters of a Normal Distribution From Singly Censored Samples,
Using Maximum Likelihood,&quot; <em>Technometrics</em>, 27, 119-128.
</p>
<p>Schmoyer, R. L., J. J. Beauchamp, C. C. Brandt and F. O. Hoffman, Jr.
(1996), &quot;Difficulties with the Lognormal Model in Mean Estimation and
Testing,&quot; <em>Environmental and Ecological Statistics</em>, 3, 81-97.
</p>
<p>Sommerville, P. N. (1958), &quot;Tables for Obtaining Non-Parametric
Confidence Limits,&quot; <em>Annals of Mathematical Statistics</em>, 29, 599-601.
</p>
<p>Taylor, D. J., L. L. Kupper, S. M. Rappaport, and R. H. Lyles (2001), &quot;A Mixture Model for Occupational Exposure Mean Testing with a
Limit of Detection,&quot; <em>Biometrics</em>, 57, 681-688.
</p>
<p>Tuggle, R. M. (1982), &quot;Assessment of Occupational Exposure Using
One-Sided Tolerance Limits,&quot; <em>American Industrial Hygiene Association
Journal</em>, 43, 338-346.
</p>
<p>Turnbull, B. W. (1976), &quot;The Empirical Distribution Function with
Arbitrarily Grouped, Censored and Truncated Data,&quot; <em>Journal of the
Royal Statistical Society</em>, Series B (Methodological), 38(3), 290-295.
</p>
<p>Venables, W. N. and B. D. Ripley (2002), <em>Modern Applied Statistics
with S</em>, 4th edition. Springer-Verlag, New York.
</p>
<p>Verrill, S. and R. A. Johnson (1998), &quot;Tables and Large-Sample
Distribution Theory for Censored-Data Correlation Statistics for
Testing Normality,&quot; <em>Journal of the American Statistical Association</em>,
83(404), 1192-1197.
</p>
<p>Waller, L. A., and B. W. Turnbull, (1992), &quot;Probability Plotting with
Censored Data,&quot; <em>The American Statistician</em>, 46(1), 5-12.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example 1 from Frome and Wambach (2005) ORNL/TM-2005/52
# NOTE THAT FUNCTIONS NAMES AND DETAILS HAVE BEEN REVISED IN THIS PACKAGE
# the results are the same. For example lnorm.ml() replaces mlndln().
data(SESdata)
mle&lt;-lnorm.ml(SESdata)
unlist(mle[1:4])             # ML estimates mu sigma E(X) and sigma^2
unlist(mle[5:8])            # ML Estimates of standard errors
unlist(mle[9:13])            # additional  output from ORNL/TM-2005/52
IH.summary(SESdata,L=0.2)    #  All sumarry statistics for SESdata
#  lognormal q-q plot for SESdata Figure in ORNL/TM-2005/52
qq.lnorm(plend(SESdata),mle$mu,mle$sigma)
title("SESdata: Smelter-Elevated Surfaces")
</code></pre>

<hr>
<h2 id='aihand'>Industrial Hygiene Air Monitoring Data</h2><span id='topic+aihand'></span>

<h3>Description</h3>

<p>Data from Mulhausen and Damiano Appendix V is used to
illustrate data with &quot;less-than&quot; values (non-detects).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(aihand)</code></pre>


<h3>Format</h3>

<p>A data frame with 15 observations on the following 3 variables
</p>

<dl>
<dt>xnd</dt><dd><p>air monitoring data with 3 non-detects</p>
</dd>
<dt>det</dt><dd><p> 0 if non-detect, 1 if detect</p>
</dd>
<dt>x</dt><dd><p> air monitoring data <code class="reqn">(mg/m^3)</code></p>
</dd>
</dl>



<h3>Details</h3>

<p>The data in column 1 was obtained from the data in
column 3 by assuming a limit of detection of 1.9.  The original
data in column 3 is used as an example in Appendices V, VI,
and VII ( see Tables V.1, V.5, V.6, VI.2 ) to illustrate methods of
analysis when there are no non-detects.
</p>


<h3>Source</h3>

<p>Table V.2 page 244 in Mulhausen and Damiano (1998) and
Table IV.3 page 349 in Ignacio and Bullock (2006)
</p>


<h3>References</h3>

<p>Ignacio, J. S. and W. H. Bullock (2006), <em>A Strategy for Assessing
and Managing Occupational Exposures</em>, Third Edition, AIHA Press,
Fairfax, VA.
</p>
<p>Mulhausen, J. R. and J. Damiano (1998), <em>A Strategy for Assessing
and Managing Occupational Exposures</em>, Second Edition, AIHA Press, Fairfax, VA.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(aihand) # Table V.2 Mulhausen and Damiano (1998)

# calculate summary statistics for non-detect data
ndt &lt;- IH.summary(aihand,L=5)
x &lt;- aihand$x ;  det &lt;- rep(1,length(x))
aiha&lt;-data.frame(x,det) #  complete data
# calculate summary statistics for complete data
cdt &lt;- IH.summary(aiha,L=5)
# output results in a table
round(cbind(cdt,ndt),3)

#  rm(aiha,aihand,cdt,det,ndt)
# add results for exact method for complete data case
# for Xp and the exceedance fraction
</code></pre>

<hr>
<h2 id='beTWA'>TWA Beryllium Exposure Data</h2><span id='topic+beTWA'></span>

<h3>Description</h3>

<p> As part of a chronic disease prevention program the DOE
adopted an 8-hour time-weighted average (TWA) occupational exposure
limit (OEL) value of 0.2 <code class="reqn">\mu g/m^3</code> proposed by the American
Conference of Government Industrial Hygienists (<em>DOE 10 CRF Part
850</em> and <em>ACGIH</em> 2004).  </p>


<h3>Usage</h3>

<pre><code class='language-R'>data(beTWA)</code></pre>


<h3>Format</h3>

<p>A data frame with 280 observations on the following 2 variables:
</p>

<dl>
<dt>twa</dt><dd><p> 8-hour TWA Beryllium exposure <code class="reqn">\mu g/m^3</code></p>
</dd>
<dt>det</dt><dd><p> 0 if non-detect; 1 if detect </p>
</dd>
</dl>



<h3>Details</h3>

<p>The beTWA data set is the results of 280 personal 8-hour TWA
beryllium exposure readings at a DOE facility.  This data contains 175
non-detects that range in value from 0.005 to 0.1 <code class="reqn">\mu
g/m^3</code>. A detailed description and analysis of this data is given
as Example 2 in Section 4 of Frome and Wambach (2005).  </p>


<h3>References</h3>

<p>Frome, E. L. and Wambach, P. F. (2005), &quot;Statistical Methods and Software for
the Analysis of Occupational Exposure Data with Non-Detectable Values,&quot;
<em>ORNL/TM-2005/52,Oak Ridge National Laboratory</em>, Oak Ridge , TN 37830
Available at: <a href="http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf">http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(beTWA)
##  calculate all of the summary statistics described in
##  Example 2 in Section 4 of Frome and Wambach (2005)
round( IH.summary(beTWA,L=0.2), 3)
</code></pre>

<hr>
<h2 id='cansdata'>Container Data Used To Evaluate Beryllium Surface Contamination</h2><span id='topic+cansdata'></span>

<h3>Description</h3>

<p>Surface wipe samples obtained from containers that are used to
ship beryllium components.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(cansdata)</code></pre>


<h3>Format</h3>

<p>A data frame with 120 observations on the following 4 variables:
</p>
    
<dl>
<dt>x</dt><dd><p> Surface wipe sample <code class="reqn">\mu g/100cm^2</code> </p>
</dd>
<dt>det</dt><dd><p> 1 if detect, 0 if non-detect</p>
</dd>
<dt>strata</dt><dd><p> a factor with levels <code>A</code> and <code>B</code> </p>
</dd>
<dt>sample</dt><dd><p> a factor with levels <code>1</code> and <code>2</code> </p>
</dd>
</dl>



<h3>Details</h3>

<p> In a scoping survey, the investigator
decides to divide the survey unit into two strata: <code>A</code>, used recently,
and <code>B</code>, not used for several years.  The specified limit that is
used to determine if the survey unit is contaminated is
<code class="reqn">L = 0.2\mu g/100cm^2</code>. 
An initial sample of n = 30 was obtained from each stratum (sample = 1).
The initial survey produced discrepant results that were
hard to interpret.  A second sample of n = 30 surface wipe samples was
obtained from strata <code>A</code> and <code>B</code>.  Results below the limit of
quantification are reported as non-detects. 
</p>


<h3>References</h3>

<p>Frome, E. L. and Wambach, P. F. (2005), &quot; Statistical Methods and
Software for the Analysis of Occupational Exposure Data with
Non-Detectable Values,&quot; <em>ORNL/TM-2005/52,Oak Ridge National
Laboratory</em>, Oak Ridge, TN 37830.
Available at: <a href="http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf">http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cansdata)
#     subset container data into stratum A and stratum B
sa60 &lt;- cansdata[ cansdata$st=="A",] ; Ia &lt;- "Be Samples From  Stratum A"
sb60 &lt;- cansdata[ cansdata$st=="B",] ; Ib &lt;- "Be Samples From  Stratum B"
mle.sa60 &lt;- unlist(lnorm.ml(sa60))  # MLEs for stratum A
mle.sb60 &lt;- unlist(lnorm.ml(sb60)) 
#     print MLE for stratum A and B
round( data.frame(mle.sa60,mle.sb60),3)
#
# Q-Q plot for each stratum
par( mfcol=c(1,2) )
qq.lnorm(plend(sa60),mle.sa60[1,2],ylim=c(0.01,1.2),xlim=c(-0.5,2.5),main=Ia )
qq.lnorm(plend(sb60),mle.sb60[1,2],ylim=c(0.01,1.2),xlim=c(-0.5,2.5),main=Ib )
#   list all summary statistics by Strata
round(IH.summary(cansdata,L=0.2,bcol=3),4)
</code></pre>

<hr>
<h2 id='efclnp'>Nonparametric Confidence Limits for the Exceedance Fraction </h2><span id='topic+efclnp'></span>

<h3>Description</h3>

<p>When the distribution function for the X's is not specified a nonparametric approach
can be used to estimate the exceedance fraction <code class="reqn">FL = Pr [X &gt; L]</code> the
proportion of measurements that exceed the limit L. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>efclnp(dd,gam = 0.95,L)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="efclnp_+3A_dd">dd</code></td>
<td>
<p> An n by 2 matrix or data frame with <br />
x (exposure) variable in column 1, and <br />
det = 0 for non-detect or 1 for detect in column 2 </p>
</td></tr>
<tr><td><code id="efclnp_+3A_gam">gam</code></td>
<td>
<p>one-sided confidence level <code class="reqn">\gamma</code>. Default is 0.95</p>
</td></tr>
<tr><td><code id="efclnp_+3A_l">L</code></td>
<td>
<p>L is specified limit for the exceedance fraction; e.g., the occupational exposure limit</p>
</td></tr>
</table>


<h3>Details</h3>

<p> Given a random
sample of size n the number y of nonconforming observations (i.e., y =
number of X's that exceed the limit L) is described using the binomial
distribution.  The point estimate of FL is <code class="reqn">fnp = y / n</code> and confidence
limits are obtained using the method of Clopper and Pearson (1934)
(Hahn and Meeker, 1991) and the R documentation for base R
function  <code><a href="stats.html#topic+binom.test">binom.test</a></code>.
</p>


<h3>Value</h3>

<p>A LIST with components:
</p>
<table role = "presentation">
<tr><td><code>fnp</code></td>
<td>
<p> nonparametric estimate of exceedance fraction (as percent) </p>
</td></tr>
<tr><td><code>fnp.LCL</code></td>
<td>
<p> is the 100*<code class="reqn">\gamma</code>% lower confidence limit for <code>fnp</code></p>
</td></tr>
<tr><td><code>fnp.UCL</code></td>
<td>
<p> is the 100*<code class="reqn">\gamma</code>% upper confidence limit for <code>fnp</code></p>
</td></tr>
<tr><td><code>L</code></td>
<td>
<p>is specified limit for the exceedance fraction( e.g. OEL)</p>
</td></tr>
<tr><td><code>gam</code></td>
<td>
<p>one-sided confidence level <code class="reqn">\gamma</code>. Default is 0.95</p>
</td></tr>
</table>


<h3>Assumptions </h3>

<p>All non-detects &lt; L</p>


<h3>Note</h3>

<p>The estimates of the exceedance fraction and CL's are in percentage units</p>


<h3>Author(s)</h3>

<p>E. L. Frome </p>


<h3>References</h3>

<p>Clopper, C. J. and E. S. Pearson (1934), &quot;The Use of Confidence or
Fiducial Limits Illustrated in the Case of the Binomial,&quot; <em>Biometrika</em>, 26, 404-413.
</p>
<p>Hahn, G. J. and W. Q. Meeker (1991), <em>Statistical Intervals</em>, John Wiley and Sons, New York.
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+efraction.ml">efraction.ml</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>#   calculate nonparametric estimate 
#   for Example 2 in ORNLTM2005 
data(beTWA)
unlist(efclnp(beTWA,L=0.2))
# calculate ML estimates of exceedance fraction and CLs 
unlist(efraction.ml(beTWA,L=0.2))
</code></pre>

<hr>
<h2 id='efraction.exact'>Exceedance Fraction and Exact Confidence Limits  </h2><span id='topic+efraction.exact'></span>

<h3>Description</h3>

<p>Calculate estimate of the exceedance fraction <code class="reqn">FL = Pr [X &gt; L]</code>
and exact confidence limits for random sample from normal/lognormal distribution.
This function should only be used for complete samples. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>efraction.exact(x, gam = 0.95, L=NA ,logx=TRUE,wpnt=FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="efraction.exact_+3A_x">x</code></td>
<td>
<p>vector of data values</p>
</td></tr>
<tr><td><code id="efraction.exact_+3A_gam">gam</code></td>
<td>
<p>one-sided confidence level <code class="reqn">\gamma</code>. Default is 0.95</p>
</td></tr>
<tr><td><code id="efraction.exact_+3A_l">L</code></td>
<td>
<p> L is specified limit for the exceedance fraction; e.g., the occupational exposure limit</p>
</td></tr>
<tr><td><code id="efraction.exact_+3A_logx">logx</code></td>
<td>
<p>If <code>TRUE</code>, sample is from lognormal, else normal. Default is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="efraction.exact_+3A_wpnt">wpnt</code></td>
<td>
<p>if <code>TRUE</code>, show warning from pnt. Default is <code class="reqn">FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p> The exceedance fraction represent the proportion of the X's
that exceed a given limit Lp. The null hypothesis of interest is
<code class="reqn">Ho: F \ge Fo = 1-p</code>; i.e., Fo is the maximum proportion of the
population that can exceed the limit Lp. The null hypothesis is
rejected if the <code class="reqn">100 \gamma\%</code> UCL for FL is less than Fo ,
indicating that the exposure profile is acceptable. The type I error rate
for this test is less than or equal to <code class="reqn">\alpha = 1 - \gamma</code>.  </p>


<h3>Value</h3>

<p>A LIST with components:
</p>
<table role = "presentation">
<tr><td><code>f</code></td>
<td>
<p>estimate of exceedance fraction for lognormal distribution as %</p>
</td></tr>
<tr><td><code>fe.LCL</code></td>
<td>
<p> 100*<code class="reqn">\gamma</code>% exact lower confidence limit % units</p>
</td></tr>
<tr><td><code>fe.UCL</code></td>
<td>
<p> 100*<code class="reqn">\gamma</code>% exact upper confidence limit % units</p>
</td></tr>
<tr><td><code>L</code></td>
<td>
<p>L is specified limit for the exceedance fraction, e.g. the occupational exposure limit</p>
</td></tr>
<tr><td><code>gam</code></td>
<td>
<p>one-sided confidence level <code class="reqn">\gamma</code>. Default is 0.95</p>
</td></tr>
<tr><td><code>Logx</code></td>
<td>
<p>If <code>TRUE</code>, sample is from lognormal, else normal. Default is <code>TRUE</code></p>
</td></tr>
</table>


<h3>Note</h3>

<p>(fe.LCL, fe.UCL) is an approximate <code class="reqn">100(2\gamma -1)</code> percent
confidence interval for F. The R function <code>uniroot</code> is used to find the
noncentrality parameter of noncentral t distribution to calculate CL's
for <code class="reqn">U = (L - \mu) / \sigma</code> where F = pnorm(U).  In some versions of R this
may cause a warning message.  See R bug report RP 9171 full precision
was not achieved in 'pnt'.  This warning message may occur in <code>uniroot</code>
calls to <code>pt</code> and does not effect the precision of the final result</p>


<h3>Author(s)</h3>

<p> E. L. Frome</p>


<h3>References</h3>

<p>Johnson, N. L. and B. L. Welch (1940), &quot;Application of the Non-Central
t-Distribution,&quot; <em>Biometrika</em>, 31(3/4), 362-389.
</p>
<p>Lyles, R. H. and L. L. Kupper (1996), &quot;On strategies for comparing occupational
exposure data to limits,&quot; <em>American Industrial Hygiene Association
Journal</em>. 57:6-15.
</p>
<p>Frome, E. L. and Wambach, P. F. (2005), &quot;Statistical Methods and Software for 
the Analysis of Occupational Exposure Data with Non-Detectable Values,&quot; 
<em>ORNL/TM-2005/52,Oak Ridge National Laboratory</em>, Oak Ridge, TN 37830.
Available at: <a href="http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf">http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf</a>
</p>
<p>Ignacio, J. S. and W. H. Bullock (2006), <em>A Strategy for Assesing
and Managing Occupational Exposures</em>, Third Edition, AIHA Press,
Fairfax, VA.
</p>
<p>Mulhausen, J. R. and J. Damiano (1998), <em> A Strategy for Assessing
and Managing Occupational Exposures</em>, Second Edition, AIHA Press, Fairfax, VA.
</p>


<h3>See Also</h3>

<p>Help files for <code><a href="#topic+efraction.ml">efraction.ml</a></code>,<code><a href="#topic+efclnp">efclnp</a></code>,
<code><a href="#topic+percentile.exact">percentile.exact</a></code>, <code><a href="#topic+efraction.exact">efraction.exact</a></code>, 
<code><a href="#topic+aihand">aihand</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># calculate exceedance fraction and exact CLs for Example data
# Appendix  Mulhausen and Damiano(1998) Ignacion and Bullock (2006
data(aihand)
x &lt;- aihand$x ;  det &lt;- rep(1,length(x))
aiha&lt;-data.frame(x,det) #  complete data
unlist(efraction.exact(x,gam=0.95,L=5) ) #  exact CLs
unlist(efraction.ml(aiha,gam=0.95,L=5))  #  ML CLs
unlist(efclnp(aiha,L=5))                 #  nonparametric CLs 
</code></pre>

<hr>
<h2 id='efraction.ml'> Calculate ML Estimate of Exceedance Fraction and Confidence Limits  </h2><span id='topic+efraction.ml'></span>

<h3>Description</h3>

<p>Calculate the ML estimate of the exceedance fraction <code class="reqn">F = Pr [X &gt; L]</code>
and &quot;large sample&quot; confidence limits for lognormal data with non-detects. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>efraction.ml(dd, gam = 0.95, L = 5, dat = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="efraction.ml_+3A_dd">dd</code></td>
<td>
<p> if <code>dat</code> is <code>TRUE</code> <code>dd</code> is an n by 2 matrix or data frame with x in column 1 det in column 2</p>
</td></tr>
<tr><td><code id="efraction.ml_+3A_gam">gam</code></td>
<td>
<p> one-sided confidence level <code class="reqn">\gamma</code>. Default is 0.95</p>
</td></tr>
<tr><td><code id="efraction.ml_+3A_l">L</code></td>
<td>
<p> L is specified limit for the exceedance fraction; e.g., the occupational exposure limit</p>
</td></tr>
<tr><td><code id="efraction.ml_+3A_dat">dat</code></td>
<td>
<p> if <code>dat</code> is <code>FALSE</code>, then <code>dd</code> is a list from
<code><a href="#topic+lnorm.ml">lnorm.ml</a></code>. Default is <code>TRUE</code> </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The exceedance fraction FL represent the proportion of the X's that exceed a
given limit Lp. The null hypothesis of interest is <code class="reqn">Ho: FL \ge Fo=
1-p</code>; i.e., Fo is the maximum proportion of the population that can exceed
the limit Lp. The ML point estimate of FL is <code class="reqn">f = 1 - N(v)</code> where
<code class="reqn">v = [log(L)-\mu ] /\sigma</code> , and  N(v) is the standard normal distribution 
function. The large sample <code class="reqn">100\gamma\%</code> LCL for <code class="reqn">V = [log(L) - \mu 
]/\sigma</code> is LCLv <code class="reqn">= v - t(\gamma , m-1) var(v)^{1/2}</code>, where
</p>
<p style="text-align: center;"><code class="reqn">var(v)= p1^2 var(\mu )+ p2^2 var(\sigma)+ 2p1p2 cov( \mu, \sigma)</code>
</p>
<p>,
and p1 and p2 are partial derivatives of <code class="reqn">v</code> with respect to <code class="reqn">\mu</code> and <code class="reqn">\sigma</code>.
The <code class="reqn">100\gamma\%</code> UCL for FL is <code class="reqn">UF( L, \gamma) = 1 - N(LCLv)</code>.
The <code class="reqn">100\gamma\%</code> LCL for FL is <code class="reqn">LF( L, \gamma) = 1 - N(UCLv)</code>, where
<code class="reqn">UCLv = u + t(\gamma, m-1) var(v)^{1/2}</code>. The null hypothesis <code class="reqn">Ho: FL = 1 - p</code>
is rejected if the <code class="reqn">100\gamma\%</code> UCL for FL is less
than Fo, indicating that the exposure profile is acceptable. The large
sample ML estimates of the exceedance fraction and <code class="reqn">100\gamma\%</code>
confidence limits for lognormal data are calculated using the
output from <code><a href="#topic+lnorm.ml">lnorm.ml</a></code>. 
</p>


<h3>Value</h3>

<p>A LIST with components:
</p>
<table role = "presentation">
<tr><td><code>f</code></td>
<td>
<p>is the ML estimate of exceedance fraction for lognormal distribution</p>
</td></tr>
<tr><td><code>f.LCL</code></td>
<td>
<p> is the 100*<code class="reqn">\gamma</code>% lower confidence limit for <code>f</code></p>
</td></tr>
<tr><td><code>f.UCL</code></td>
<td>
<p> is the 100*<code class="reqn">\gamma</code>% upper confidence limit for <code>f</code></p>
</td></tr>
<tr><td><code>L</code></td>
<td>
<p>L is specified limit for the exceedance fraction; e.g., the occupational exposure limit</p>
</td></tr>
<tr><td><code>gam</code></td>
<td>
<p>one-sided confidence level <code class="reqn">\gamma</code>. Default is 0.95</p>
</td></tr>
</table>


<h3>Note</h3>

<p> (f.LCL, f.UCL) is an 100<code class="reqn">(2\gamma -1)</code> percent confidence interval 
for F</p>


<h3>Author(s)</h3>

<p> E. L. Frome</p>


<h3>References</h3>

<p>Frome, E. L. and Wambach, P. F. (2005), &quot;Statistical Methods and Software for 
the Analysis of Occupational Exposure Data with Non-Detectable Values,&quot; 
<em>ORNL/TM-2005/52,Oak Ridge National Laboratory</em>, Oak Ridge, TN 37830.
Available at: <a href="http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf">http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf</a>
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+lnorm.ml">lnorm.ml</a></code>,<code><a href="#topic+percentile.ml">percentile.ml</a></code>  </p>


<h3>Examples</h3>

<pre><code class='language-R'># calculate ML estimate of exceedance fraction and CLs for Example 2 in ORNLTM2005-52 
data(beTWA)
unlist(efraction.ml(beTWA,L=0.2))
#  calculate nonparametric CLs 
unlist(efclnp(beTWA,L=0.2))
</code></pre>

<hr>
<h2 id='filmbadge'>Quarterly Film Badge Data</h2><span id='topic+filmbadge'></span>

<h3>Description</h3>

<p>Example of quarterly film badge data with non-detects</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(filmbadge)</code></pre>


<h3>Format</h3>

<p>A data frame with 28 observations on the following 6 variables.
</p>

<dl>
<dt>dlow</dt><dd><p>lower end of annual dose</p>
</dd>
<dt>dhigh</dt><dd><p>upper end of annual</p>
</dd>
<dt>Q1</dt><dd><p>dose for quarter 1</p>
</dd>
<dt>Q2</dt><dd><p>dose for quarter 2</p>
</dd>
<dt>Q3</dt><dd><p>dose for quarter 3</p>
</dd>
<dt>Q4</dt><dd><p>dose for quarter 4</p>
</dd>
</dl>



<h3>Details</h3>

<p>The product limit estimate (PLE) of the distribution function
F(x) was first proposed by Kaplan and Meier (1958) for right-censored
data, and Schmoyer et. al. (1996) defined the PLE for situations in
which left censored data occurs.  Both left censoring and right
censoring are special cases of the general PLE (Peto,1973; Turnbull,
1976).  A non-detect or left censored dose occurs when the dose is
less than a detection limit. For a non-detect it is only known that
the dose does not exceed the limit of detection(LOD).  To obtain an
estimate of the annual dose distribution F(x) from quarterly doses the
general PLE is required since the annual doses will be &quot;interval
censored&quot; if at least two of the quarterly doses are non-detects.
Consider, for example, a worker with quarterly dose of 0, 50, 0, and
100 mrem.  The quarterly interval doses are (0,30), (50,50), (0,30),
and (100,100) assuming an LOD of 30 mrem.  The annual dose is obtained
by adding the lower and upper bounds of the quarterly doses and is
equal to (150,210) for the example, i.e., it is only known that the
dose is between 150 and 210.  </p>


<h3>Note</h3>

<p> The dose unit is mrem (100 mrem= 10mSv). The LOD is 30 mrem.  This
is a representative sample of quarterly external dose records for 28
workers at the Y-12 facility in Oak Ridge (see <em>ORAUT-OTIB-0044,
Rev. 01-A</em> and <em>ORAUT-OTIB-0064</em>) and is used here to illustrate
the calculation of the PLE for interval censored data.  The R function
<code>survreg</code> can be used to obtain ML estimates of the parameters
for the lognormal model and the covariance matrix that is needed for
CLs for the exceedance fraction and 95th percentile.
</p>


<h3>References</h3>

<p>Kaplan, E. L. and P. Meir (1958), &quot;Nonparametric Estimation from Incomplete Observations,&quot; <em>Journal of the American Statistical Association</em>, 457-481. 
</p>
<p>Ng, M. P. (2002), &quot;A Modification of Peto's Nonparametric Estimation of Survival Curves for Interval-Censored Data,&quot; <em>Biometrics</em>, volume 58, number 2, pp. 439-442.
</p>
<p>ORAUT (Oak Ridge Associated Universities Team), 2005c, &quot;Historical Evaluation of the Film Badge Dosimetry Program at the Y-12 Facility in Oak Ridge, Tennessee:  Part 1 - Gamma Radiation&quot;, <em>ORAUT-OTIB-0044, Rev. 01-A (was ORAUT-RPRT-0032, Rev. 00)</em>, Oak Ridge, Tennessee.
</p>
<p>ORAUT (Oak Ridge Associated Universities Team), 2007, &quot;External Coworker Dosimetry Data for the Y-12 National Security Complex&quot;. <em>ORAUT-OTIB-0064</em> (Under Revision).
</p>
<p>Peto, R. (1973), &quot;Experimental Survival Curves for Interval-censored Data,&quot; <em>Applied Statistics</em>, volume 22, number 1, pp. 86-91. 
</p>
<p>Schmoyer, R. L., J. J. Beauchamp, C. C. Brandt and F. O. Hoffman, Jr. (1996), &quot;Difficulties with the Lognormal Model in Mean Estimation and Testing,&quot; <em>Environmental and Ecological Statistics</em>, 3, 81-97.
</p>
<p>Turnbull, B. W. (1976), &quot;The Empirical Distribution Function with Arbitrarily Grouped, Censored and Truncated Data,&quot; <em>Journal of the Royal Statistical Society</em>, Series B (Methodological), 38(3), 290-295.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(filmbadge)
head(filmbadge)  #  LOOK AT FIRST FIVE RECORDS
#   USE icfit() TO CALCULATE PLE FOR INTERVAL CENSORED DATA
par( mfrow=c(1,2) )
out &lt;- icfit(filmbadge$dlow,filmbadge$dhigh)
#   PLOT EXCEEDANCE S(x) vs x USING icplot()
tp &lt;- "PLE of Exceedance for Filmbadge Data" 
icplot(out$surv, out$time,XLAB="Dose",YLAB="Exceedance Probability",main=tp,cex.main=.8)
#   USE pleicf() TO CALCULATE PLE FOR filmbadge DATA
ple.fb &lt;- pleicf(filmbadge[,1:2],FALSE)
#   USE qq.lnorm  FOR LOGNORMAL Q-Q PLOT FOR INTERVAL CENSORED DATA
tmp &lt;- qq.lnorm(ple.fb)  
GM &lt;-round(exp(tmp$par[1])); GSD &lt;- round(exp(tmp$par[2]),2)
tp&lt;-paste("Lognormal Q-Q plot for Filmbadge\n  Data GM= ",GM,"GSD= ",GSD)
title(tp,cex.main=0.8) # title for q-q plot with graphical parameter estimates
#  RESULTS FROM  pleicf()
round(ple.fb,3)
#

</code></pre>

<hr>
<h2 id='icfit'>Calculates the Self-Consistent Estimate of Survival from
Interval Censored Data </h2><span id='topic+icfit'></span>

<h3>Description</h3>

<p>This function calculates the self-consistent estimate of survival
for interval censored data.
(i.e., the nonparametric maximum likelihood estimate that generalizes
the Kaplan-Meier estimate to interval censored data).
The censoring is such that if the i<em>th</em> observation fails at <code class="reqn">x</code>,
we only observe that <code class="reqn">L[i] &lt; x \le R[i]</code>. Data may be entered with
&quot;exact&quot; values, i.e., <code class="reqn">L[i] = x = R[i]</code>. In that case the <code class="reqn">L[i]</code> is
changed internally to <code class="reqn">L[i]*</code> which is the next lower of any of the
observed endpoints (unless <code class="reqn">R[i] = 0</code> then an error results).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>icfit(L, R, initp = NA, minerror = 1e-06, maxcount = 1000)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="icfit_+3A_l">L</code></td>
<td>
<p> a vector of the left endpoints of the interval </p>
</td></tr>
<tr><td><code id="icfit_+3A_r">R</code></td>
<td>
<p> a vector of the right endpoints of the interval </p>
</td></tr>
<tr><td><code id="icfit_+3A_initp">initp</code></td>
<td>
<p> a vector with an initial estimate of the density
function. This vector should sum to 1 and have a
length equal to the number of unique values of <code>L</code>
and <code>R</code> combined. If <code>initp</code> = NA (default) then an initial
value is estimated from the data.</p>
</td></tr>
<tr><td><code id="icfit_+3A_minerror">minerror</code></td>
<td>
<p>The minimum error for convergence purposes. The
EM algorithm stops when <code>error</code> &lt; <code>minerror</code>, where
error is the maximum of the reduced gradients (see Gentleman
and Geyer, 1994). Default = 1e-06.</p>
</td></tr>
<tr><td><code id="icfit_+3A_maxcount">maxcount</code></td>
<td>
<p> the maximum number of iterations. Default is 10000.  </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm is basically an EM-algorithm applied to
interval censored data (see Turnbull, 1976); however,
first there is a primary reduction (See Aragon and
Eberly, 1992). Convergence is defined when the maximum
reduced gradient is less than minerror, and the
Kuhn-Tucker conditions are approximately met,
otherwise a warning will result.  (see Gentleman and
Geyer, 1994). There may be other faster algorithms,
but they are more complicated (see Aragon and Eberly,
1992). The output for time is <code>sort(unique(c(0,L,R,Inf)))</code>
without the Inf. The output for <code>p</code> keeps the value
related to Inf so that <code>p</code> may be inserted into <code>initp</code>
for another run. The outputs for <code>p</code> and <code>surv</code> act as if
the jumps in the survival curve happen at the largest
of the possible times (see Gentleman and Geyer, 1994,
Table 2, for a more accurate way to present <code>p</code>).
</p>


<h3>Value</h3>

<p>Returns a list with the following elements:
</p>
<table role = "presentation">
<tr><td><code>u</code></td>
<td>
<p>a vector of Lagrange multipliers. If there are any
negative values of <code>u</code> the Kuhn-Tucker conditions for
convergence are not met. If this happens a warning
will result.</p>
</td></tr>
<tr><td><code>error</code></td>
<td>
<p>this is the maximum of the reduced gradients. If
convergence is correct then <code>error</code> &lt; <code>minerror</code>
and all values of <code>u</code> are nonnegative,
otherwise a warning results.</p>
</td></tr>
<tr><td><code>count</code></td>
<td>
<p>number of iterations of the self-consistent algorithm
(i.e., EM-algorithm)</p>
</td></tr>
<tr><td><code>time</code></td>
<td>
<p>a vector of times (see details)</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>a vector of probabilities, all except the last
values are associated with the time
vector above, i.e., <code class="reqn">p[i] = Prob (X = time[i])</code>.
The last value is associated with time==Inf.
(see details).</p>
</td></tr>
<tr><td><code>surv</code></td>
<td>
<p>a vector of survival values associated with
the time vector above, i.e.,
<code class="reqn">surv[i] = Prob (X &gt; time[i] )</code></p>
</td></tr>
</table>


<h3>Note</h3>

<p> The functions <code><a href="#topic+icfit">icfit</a></code>, <code><a href="#topic+icplot">icplot</a></code>,
and  <code><a href="#topic+ictest">ictest</a></code> and documentation for these functions are from  Michael P. Fay.
You are free to distribute these functions to whomever is
interested. They come with no warranty however.
</p>


<h3>Author(s)</h3>

<p>Michael P. Fay</p>


<h3>References</h3>

<p>Aragon, J. and Eberly, D. (1992), &quot;On Convergence of Convex
Minorant Algorithms for Distribution Estimation with
Interval-Censored Data,&quot; <em>Journal of Computational and Graphical
Statistics.</em> 1: 129-140.
</p>
<p>Gentleman, R. and Geyer, C. J. (1994), &quot;Maximum Likelihood
for Interval Censored Data: Consistency and Computation,&quot;
<em>Biometrika</em>, 81, 618-623.
</p>
<p>Turnbull, B. W. (1976), &quot;The Empirical Distribution Function
with Arbitrarily Grouped, Censored and Truncated Data,&quot;
<em>Journal of the Royal Statistical Society</em>, Series B,(Methodological), 38(3), 290-295.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+plekm">plekm</a></code> , <code><a href="#topic+qq.lnorm">qq.lnorm</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'># Calculate and plot a Kaplan-Meier type curve for interval censored data.
# This is S(x) = 1 - F(x) and is the sample estimate of the probability
# of exceeding x.  The filmbadge data is used as an example.
data(filmbadge)
out &lt;- icfit(filmbadge$dlow,filmbadge$dhigh)
icplot(out$surv, out$time,XLAB="Dose",YLAB="Exceedance Probability")
</code></pre>

<hr>
<h2 id='icplot'>Plots Survival Functions</h2><span id='topic+icplot'></span>

<h3>Description</h3>

<p>This function takes a vector of survival values and a
vector of time values and either plots a survival
function or adds the survival lines to an existing plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>icplot(surv, time = as.numeric(names(surv)), xrange = NA, lines.only = FALSE,
XLAB = "Time", YLAB = "Probability", LTY = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="icplot_+3A_surv">surv</code></td>
<td>
<p> a vector of survival values. </p>
</td></tr>
<tr><td><code id="icplot_+3A_time">time</code></td>
<td>
<p> a vector of times. These are related to the
vector of survival by
<code class="reqn">Prob(X &gt; time[i]) = surv[i]</code>. If this vector
is not given then the default is to use
<code>as.numeric(names(surv))</code>. </p>
</td></tr>
<tr><td><code id="icplot_+3A_xrange">xrange</code></td>
<td>
<p> the range of the x values. The default is
<code>c(0,max(time[time&lt;Inf]))</code>. This value may need
to be supplied when plotting more than one
survival curve on a single plot
(see lines.only). </p>
</td></tr>
<tr><td><code id="icplot_+3A_lines.only">lines.only</code></td>
<td>
<p> a logical value; default = <code>FALSE</code>. If lines.only = <code>FALSE</code>
the function draws a new set of axes. If
lines.only = <code>TRUE</code> the function adds lines to
an existing plot.  To print 2 lines on one
plot, call the function twice, the first time
with lines.only = <code>FALSE</code> and <code>xrange = range(c(t1,t2))</code>
where t1 and t2 are the (finite) times for the
two survival curves, the second time with
lines.only = <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="icplot_+3A_xlab">XLAB</code></td>
<td>
<p> a character string denoting the x label.
Default = &quot;Time&quot;.</p>
</td></tr>
<tr><td><code id="icplot_+3A_ylab">YLAB</code></td>
<td>
<p> a character string denoting the y label.
Default = &quot;Probability&quot;. </p>
</td></tr>
<tr><td><code id="icplot_+3A_lty">LTY</code></td>
<td>
<p> an integer denoting the line type (lty value). </p>
</td></tr>
<tr><td><code id="icplot_+3A_...">...</code></td>
<td>
<p> additional plotting parameters (except xlab and ylab). </p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code><a href="#topic+icplot">icplot</a></code> details. This may not be the most accurate
way to present the data. See Betensky, Lindsey, Ryan,
and Wand (1999, p. 238) for an alternative method.
</p>


<h3>Value</h3>

<p>Returns a plot or adds a line to an existing plot.
</p>


<h3>Note</h3>

<p> The functions <code><a href="#topic+icfit">icfit</a></code>, <code><a href="#topic+icplot">icplot</a></code>,
and  <code><a href="#topic+ictest">ictest</a></code> and documentation for these functions are from  Michael P. Fay.
You are free to distribute these functions to whomever is
interested. They come with no warrantee however.
</p>


<h3>Author(s)</h3>

<p>Michael P. Fay</p>


<h3>References</h3>

<p>Betensky, R. A., Lindsey, J. C., Ryan, L. M., and Wand, M. P. (1999),
&quot;Local EM Estimation of the Hazard Function for
Interval-Censored Data,&quot; <em>Biometrics</em>, 55: 238-245.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+icfit">icfit</a></code>, <code><a href="#topic+ictest">ictest</a></code>  </p>


<h3>Examples</h3>

<pre><code class='language-R'># Plot two Survival curves on one plot.
#  need data set for this example
#   s1&lt;- icfit(left[treatment==1],right[treatment==1])
#   s2&lt;- icfit(left[treatment==2],right[treatment==2])
#   icplot(s1$surv,s1$time,xrange=range(c(s1$time,s2$time)))
#   icplot(s2$surv,s2$time,lines.only=TRUE,LTY=2)
</code></pre>

<hr>
<h2 id='ictest'>Performs Tests for Interval Censored Data</h2><span id='topic+ictest'></span>

<h3>Description</h3>

<p>This function performs several different tests for
interval censored data. It has 3 different models
that generalize either the Wilcoxon rank sum test
(model = &quot;PO&quot;) or the logrank test (model = &quot;GPH&quot; or
model = &quot;Sun&quot;). Each model may be one of 2 types,
either an asymptotic permutation test or a
score test.
</p>
<p>The censoring is such that if the i<em>th</em> observation fails at <code class="reqn">x</code>,
we only observe that <code class="reqn">L[i] &lt; x \le R[i]</code>. Data may be entered with
&quot;exact&quot; values, i.e., <code class="reqn">L[i] = x = R[i]</code>. In that case the <code class="reqn">L[i]</code> is
changed internally to <code class="reqn">L*[i]</code> which is the next lower of any of the
observed endpoints (unless <code class="reqn">R[i] = 0</code> then an error results).
</p>
<p>The function requires a previously calculated survival
curve (see <code><a href="#topic+icfit">icfit</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ictest(L, R, S, group, model, type = "permutation", fuzz , output.scores)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ictest_+3A_l">L</code></td>
<td>
<p> a vector of left endpoints of the interval.
We assume each <code class="reqn">L[i] \ge 0</code>.</p>
</td></tr>
<tr><td><code id="ictest_+3A_r">R</code></td>
<td>
<p> a vector of right endpoints of the interval. Exact
values may be entered as L[i] == R[i] they are changed
internally.
</p>
</td></tr>
<tr><td><code id="ictest_+3A_s">S</code></td>
<td>
<p> a vector of survival values calculated from
all the data (i.e., ignoring group membership)
(see time below), typically output from icfit.</p>
</td></tr>
<tr><td><code id="ictest_+3A_group">group</code></td>
<td>
<p> a vector denoting the group for which the test
is desired. If group is a factor or character
then a k-sample test is performed, where k is
the number of unique values of group. If group
is numeric then a &quot;correlation&quot; type test is
performed. If there are only two groups, both
methods give the same results. </p>
</td></tr>
<tr><td><code id="ictest_+3A_model">model</code></td>
<td>
<p> a character vector with three possible values
describing the model:
model = &quot;GPH&quot;  (default) gives the grouped proportional
hazards model. This generalizes a logrank test.
model = &quot;Sun&quot;  gives a Logistic model. This
generalizes another form of the logrank test.
model = &quot;PO&quot; gives a proportional odds model.
This generalizes the Wilcoxon rank sum test.
(see details).</p>
</td></tr>
<tr><td><code id="ictest_+3A_type">type</code></td>
<td>
<p> a character vector with two possible values,
&quot;permutation&quot; or &quot;score&quot; (see details) </p>
</td></tr>
<tr><td><code id="ictest_+3A_fuzz">fuzz</code></td>
<td>
<p> a small numeric value. Because
we need to determine places in the survival
curve where there are no changes, and the machine
may have rounding error, we use this. (Default = 1e-12)</p>
</td></tr>
<tr><td><code id="ictest_+3A_output.scores">output.scores</code></td>
<td>
<p> a logical value. <code>output.scores</code> = <code>TRUE</code>
outputs the scores in the output list. Default is <code>output.scores</code> = <code>FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The 3 models are compared in depth in Fay (1999).
For censored data two common likelihoods are the
marginal likelihood or the ranks and the
likelihood with nuisance parameters for the
baseline survival. Here we use the latter
likelihood (as in Finkelstein, 1986, Fay, 1996,
and Sun, 1996).  It is difficult to create proper
score tests for this likelihood because the
number of nuisance parameters typically grows with
the sample size and often many of
the parameters are equal at the nonparametric
MLE, i.e., they are on the boundary of the
parameter space. One way around this is to
perform a permutation test on the scores
(Fay, 1996). A second way around (the boundary
problem at least) it is to redefine
the interval points so that  no boundary
problems exist (see Fay, 1996). These are the
two methods used here.
</p>
<p>We present two generalizations of the logrank
test. The method of Sun (1996) is more difficult to
calculate and has no theoretical advantages
of which I am aware. The grouped proportional
hazards model of Finkelstein (1996) is recommended.
Note that when <code>icfit</code> and <code>ictest</code> are used on right-censored
data, because the method of estimating
variance is different, even Sun's method does not
produce exactly the standard logrank test results.
</p>
<p>There are some typos in Appendix II of Fay (1999).
See the S code for the corrections.
</p>


<h3>Value</h3>

<p>Returns a list with the following elements:
</p>
<table role = "presentation">
<tr><td><code>scores</code></td>
<td>
<p>only returned if output.scores = T. This is a vector
the same length as L and R, containing the scores
used in the permutation test.</p>
</td></tr>
<tr><td><code>U</code></td>
<td>
<p>The efficient score vector. When group is a factor
or character vector then each element of U has the
interpretation as the weighted sum of &quot;observed&quot; minus
&quot;expected&quot; deaths for the group element defined by the
label of U. Thus negative values indicate better than average
survival (see Fay, 1999).
</p>
</td></tr>
<tr><td><code>chisq.value</code></td>
<td>
<p>Chi-square value of the test</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>Degrees of freedom for the chi-square test.</p>
</td></tr>
<tr><td><code>pvalue</code></td>
<td>
<p>p-value from the chi-square value.</p>
</td></tr>
<tr><td><code>test</code></td>
<td>
<p>a character vector of length one, either
&quot;2-sample&quot;,&quot;correlation&quot; or &quot;k-sample&quot;
where k in the number of unique group values.</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>same as model input, either &quot;GPH&quot;,&quot;Sun&quot; or &quot;PO&quot;</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>same as type input, either &quot;permutation&quot; or
&quot;score&quot;</p>
</td></tr>
</table>


<h3>Note</h3>

<p> The functions <code><a href="#topic+icfit">icfit</a></code>, <code><a href="#topic+icplot">icplot</a></code>,
and  <code><a href="#topic+ictest">ictest</a></code> and documentation for these functions are from  Michael P. Fay.
You are free to distribute these functions to whomever is
interested. They come with no warranty however.</p>


<h3>Author(s)</h3>

<p>Michael P. Fay</p>


<h3>References</h3>

<p>Fay, M. P. (1996), &quot;Rank Invariant Tests for Interval
Censored Data Under the Grouped Continuous Model,&quot;
<em>Biometrics</em>, 52: 811-822.
</p>
<p>Fay, M. P. (1999), &quot;Comparing Several Score Tests for
Interval Censored Data,&quot; <em>Statistics in Medicine</em>,
18: 273-285.
</p>
<p>Finkelstein, D. M. (1986), &quot;A Proportional Hazards
Model for Interval Censored Failure Time Data,&quot;
<em>Biometrics</em>, 42: 845-854.
</p>
<p>Sun, J. (1996), &quot;A Non-parametric Test for Interval
Censored Failure Time Data With Applications to
AIDS Studies,&quot; <em>Statistics in Medicine</em>, 15: 1387-1395.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+icfit">icfit</a></code>,<code><a href="#topic+icplot">icplot</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## Perform a logrank-type test using the observed information variance.
## need data set for this example
#   out&lt;-icfit(left,right)
#  ictest(left,right,out$surv,group,out$time,model = "GPH",type = "score")
#
## Perform a Wilcoxon rank sum-type test using asymptotic permutation variance.
#
# ictest(left,right,out$surv,group,out$time, model = "PO",type = "permutation")
</code></pre>

<hr>
<h2 id='IH.summary'>Summary Statistic for Samples With Non-detects </h2><span id='topic+IH.summary'></span>

<h3>Description</h3>

<p>Summary statistic described by The American Industrial
Hygiene Association (AIHA) for occupational exposure data are
calculated for samples with non-detects (aka left censored data).
Parametric estimates are based on a lognormal model using maximum
likelihood (ML).  Nonparametric methods are based on the product limit
estimate (PLE) for left censored data.  </p>


<h3>Usage</h3>

<pre><code class='language-R'>IH.summary(dd,L, p = 0.95, gam = 0.95,bcol=NA)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="IH.summary_+3A_dd">dd</code></td>
<td>
<p> An n by 2 matrix or data frame with <br />
x (exposure) variable in column 1, and <br />
det = 0 for non-detect or 1 for detect in column 2 </p>
</td></tr>
<tr><td><code id="IH.summary_+3A_l">L</code></td>
<td>
<p> L is specified limit for the exceedance fraction; e.g., the occupational exposure limit</p>
</td></tr>
<tr><td><code id="IH.summary_+3A_p">p</code></td>
<td>
<p> p is probability for Xp the 100p<em>th</em> percentile. Default is 0.95</p>
</td></tr>
<tr><td><code id="IH.summary_+3A_gam">gam</code></td>
<td>
<p> one-sided confidence level <code class="reqn">\gamma</code>. Default is 0.95</p>
</td></tr>
<tr><td><code id="IH.summary_+3A_bcol">bcol</code></td>
<td>
<p> Column number that contains a BY variable. This column
must contain a factor and the value of each of the summary statistics is calculated
for each level of the factor. Default NA</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Regulatory and advisory criteria for evaluating the adequacy of
occupational exposure controls are generally expressed as limits that
are not to be exceeded in a work shift or shorter time-period if the
agent is acutely hazardous.  Exposure monitoring results above the
limit require minimal interpretation and should trigger immediate
corrective action. Demonstrating compliance with a limit is more
difficult.  AIHA has published a consensus standard with two basic
strategies for evaluating an exposure profile&mdash;see Mulhausen and
Damiano(1998), Ignacio and Bullock (2006). The first approach is based
on the mean of the exposure distribution, and the second approach
considers the &quot;upper tail&quot; of the exposure profile.  Statistical
methods for estimating the mean, an upper percentile of the
distribution, the exceedance fraction, and the uncertainty in each of
these parameters are provided by this package.  Most of the AIHA
methods are based on the assumptions that the exposure data does not
contain non-detects, and that a lognormal distribution can be used to
describe the data. Exposure monitoring results from a compliant
workplace tend to contain a high percentage of non-detected results
when the detection limit is close to the exposure limit, and in some
situations, the lognormal assumption may not be reasonable.  All of
these methods are described in a companion report by Frome and Wambach
(2005).  </p>


<h3>Value</h3>

<p>A data.frame with column names based on levels of the BY variable and row names:
</p>
<table role = "presentation">
<tr><td><code>mu</code></td>
<td>
<p>ML estimate of mean of y=log(x)</p>
</td></tr>
<tr><td><code>se.mu</code></td>
<td>
<p>Estimate of standard error of mu</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>ML estimate of sigma</p>
</td></tr>
<tr><td><code>se.sigma</code></td>
<td>
<p>Estimate of standard error of sigma</p>
</td></tr>
<tr><td><code>GM</code></td>
<td>
<p>MLE of geometric mean</p>
</td></tr>
<tr><td><code>GSD</code></td>
<td>
<p>MLE of geometric standard deviation</p>
</td></tr>
<tr><td><code>EX</code></td>
<td>
<p>MLE of E(X) the (arithmetic) mean</p>
</td></tr>
<tr><td><code>EX-LCL</code></td>
<td>
<p>Lower Confidence Limit for E(X)</p>
</td></tr>
<tr><td><code>EX-UCL</code></td>
<td>
<p>Upper Confidence Limit for E(X)</p>
</td></tr>
<tr><td><code>KM-mean</code></td>
<td>
<p>Kaplan-Meier(KM) Estimate of E(X)</p>
</td></tr>
<tr><td><code>KM-LCL</code></td>
<td>
<p>KM Lower Confidence Limit for E(X)</p>
</td></tr>
<tr><td><code>KM-UCL</code></td>
<td>
<p>KM Upper Confidence Limit for E(X)</p>
</td></tr>
<tr><td><code>KM-se</code></td>
<td>
<p>Standard Error of KM-mean</p>
</td></tr>
<tr><td><code>obs.Xp</code></td>
<td>
<p>Estimate of Xp from PLE</p>
</td></tr>
<tr><td><code>Xp</code></td>
<td>
<p>ML estimate of Xp the pth percentile</p>
</td></tr>
<tr><td><code>Xp.LCL</code></td>
<td>
<p>MLE of LX(p,gam) the LCL for Xp</p>
</td></tr>
<tr><td><code>Xp.UCL</code></td>
<td>
<p>MLE of UX(p,gam) the UCL for Xp</p>
</td></tr>
<tr><td><code>zL</code></td>
<td>
<p>MLE of the Z value for limit L</p>
</td></tr>
<tr><td><code>NpUTL</code></td>
<td>
<p>Nonparametric estimate of the UTL <code class="reqn">p-\gamma</code></p>
</td></tr>
<tr><td><code>Maximum</code></td>
<td>
<p>Largest value in the data set</p>
</td></tr>
<tr><td><code>NonDet</code></td>
<td>
<p>percent of X's that are left censored, i.e., non-detects</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>number of observations in the data set</p>
</td></tr>
<tr><td><code>Rsq</code></td>
<td>
<p>Square of correlation for the quantile-quantile (q-q) plot</p>
</td></tr>
<tr><td><code>m</code></td>
<td>
<p>number X's greater than the LOD</p>
</td></tr>
<tr><td><code>f</code></td>
<td>
<p>MLE of exceedance fraction F for limit L</p>
</td></tr>
<tr><td><code>f.LCL</code></td>
<td>
<p>LCf(L,gam) MLE of LCL for F</p>
</td></tr>
<tr><td><code>F.UCL</code></td>
<td>
<p>UCf(L,gam) MLE of UCL for F </p>
</td></tr>
<tr><td><code>fnp</code></td>
<td>
<p>Nonparametric estimate of F for limit L</p>
</td></tr>
<tr><td><code>fnp.LCL</code></td>
<td>
<p>Nonparametric estimate of LCL for  F</p>
</td></tr>
<tr><td><code>fnp.UCL</code></td>
<td>
<p>Nonparametric estimate of UCL for  F </p>
</td></tr>
<tr><td><code>m2log(L)</code></td>
<td>
<p> -2 times the log-likelihood function</p>
</td></tr>
<tr><td><code>L</code></td>
<td>
<p>L is specified limit for the exceedance fraction; e.g., the occupational exposure limit</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>percentile for UTL <code>p</code>-<code class="reqn">\gamma</code> </p>
</td></tr>
<tr><td><code>gam</code></td>
<td>
<p>one-sided confidence level <code class="reqn">\gamma</code>. Default is 0.95</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>E. L. Frome </p>


<h3>References</h3>

<p>Frome, E. L. and Wambach, P. F. (2005), &quot;Statistical Methods and Software for
the Analysis of Occupational Exposure Data with Non-Detectable Values,&quot;
<em>ORNL/TM-2005/52,Oak Ridge National Laboratory</em>, Oak Ridge, TN 37830.
Available at: <a href="http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf">http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf</a>
</p>
<p>Ignacio, J. S. and W. H. Bullock (2006), <em>A Strategy for Assesing
and Managing Occupational Exposures</em>, Third Edition, AIHA Press,
Fairfax, VA.
</p>
<p>Mulhausen, J. R. and J. Damiano (1998), <em> A Strategy for Assesing
and Managing Occupational Exposures</em>, Second Edition, AIHA Press, Fairfax, VA.
</p>
<p>See complete list of references at <code><a href="#topic+About-STAND">About-STAND</a></code>
</p>


<h3>See Also</h3>

<p>See Also <code><a href="#topic+lnorm.ml">lnorm.ml</a></code>, <code><a href="#topic+efraction.ml">efraction.ml</a></code>,
<code><a href="#topic+percentile.ml">percentile.ml</a></code>, <code><a href="#topic+kmms">kmms</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Analysis for cansdata  Example 1 from ORNLTM2005-52
data(cansdata)
Allcans&lt;- round(IH.summary(cansdata,L=0.2,bcol=NA),3)
# Example using cansdata with By variable
cansout &lt;- round(IH.summary(cansdata,L=0.2,bcol=3),3)
#  combine out from both analysis
cbind(Allcans,cansout)
</code></pre>

<hr>
<h2 id='kmms'>Kaplan-Meier (KM) Mean and Standard Error</h2><span id='topic+kmms'></span>

<h3>Description</h3>

<p>Kaplan- Meier Estimate of Mean and Standard Error of
the Mean for Left Censored Data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kmms(dd, gam = 0.95)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kmms_+3A_dd">dd</code></td>
<td>
<p> An n by 2 matrix or data frame with <br />
x (exposure) variable in column 1, and <br />
det= 0 for non-detect or 1 for detect in column 2 </p>
</td></tr>
<tr><td><code id="kmms_+3A_gam">gam</code></td>
<td>
<p> one-sided confidence level <code class="reqn">\gamma</code>. Default is 0.95 </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The product limit estimate (PLE) of the cumulative distribution function
was first proposed by Kaplan and Meier (1958) for right censored data.
Turnbull (1976) provides a more general treatment of nonparametric
estimation of the distribution function for arbitrary censoring. For
randomly left censored data, the PLE is defined by Schmoyer et al.
(1996)&ndash;see <code><a href="#topic+plend">plend</a></code>.
</p>
<p>The mean of the PLE is a censoring-adjusted
point estimate of E(X) the mean of X.  An approximate standard error
of the PLE mean can be obtained using the method of Kaplan and Meier
(1958), and the <code class="reqn">100\gamma\%</code> UCL is <code class="reqn">KM.mean + t(\gamma -1,
m-1) sp</code>, where <code>sp</code> is the Kaplan-Meier standard error of the mean
adjusted by the factor <code class="reqn">m/(m-1)</code>, where <code>m</code> is the number of detects in the
sample.  When there is no censoring this reduces to the second
approximate method described by Land (1972).
</p>


<h3>Value</h3>

<p>A LIST with components:
</p>
<table role = "presentation">
<tr><td><code>KM.mean</code></td>
<td>
<p>Kaplan- Meier(KM) estimate of mean E(X) </p>
</td></tr>
<tr><td><code>KM.LCL</code></td>
<td>
<p>KM estimate of lower confidence limit </p>
</td></tr>
<tr><td><code>KM.UCL</code></td>
<td>
<p>KM estimate of upper confidence limit</p>
</td></tr>
<tr><td><code>KM.se</code></td>
<td>
<p>estimate of standard error of KM-mean</p>
</td></tr>
<tr><td><code>gamma</code></td>
<td>
<p>one-sided confidence level <code class="reqn">\gamma</code>. Default 0.95</p>
</td></tr>
</table>


<h3>Note</h3>

<p> Error in KM.se corrected on 12 June 2007. KM standard error is
adjusted by multiplying by sqrt(m/(m-1)) where m is number of detected
values.  Error occurred if there were ties in detected values by
calculating the number of unique detected values. For example, for
beTWA sqrt(m/(m-1)) is 1.004796 .  Due to error 1.008032 was used. The
sqrt(m/(m-1)) will always be smaller after correction, depending on
value of m and the number of ties. See the example.   
</p>


<h3>Author(s)</h3>

<p>E. L. Frome</p>


<h3>References</h3>

<p>Kaplan, E. L. and Meier, P. (1958), &quot;Nonparametric Estimation from Incomplete Observations,&quot;
<em>Journal of the American Statistical Association</em>, 457-481. 
</p>
<p>Schmoyer, R. L., J. J. Beauchamp, C. C. Brandt and F. O. Hoffman, Jr.
(1996), &quot;Difficulties with the Lognormal Model in Mean Estimation and
Testing,&quot; <em>Environmental and Ecological Statistics</em>, 3, 81-97.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+plend">plend</a></code>, <code><a href="#topic+plekm">plekm</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'># results for beTWA data using kmms in stand Ver 1.1 with error
#    KM.mean      KM.LCL      KM.UCL       KM.se       gamma 
# 0.018626709 0.014085780 0.023167637 0.002720092 0.950000000
#
data(beTWA) # Use data from Example 2 in ORNLTM2002-51
unlist(kmms(beTWA))
</code></pre>

<hr>
<h2 id='lnorm.ml'> ML Estimation for Lognormal Data with Non-detects </h2><span id='topic+lnorm.ml'></span>

<h3>Description</h3>

 
<p>When an exposure measurement may be less than a detection limit closed
form and exact methods have not been developed for the lognormal
model.  The maximum likelihood (ML) principle is used to develop an
algorithm for parameter estimation, and to obtain large sample
equivalents of confidence limits for the mean exposure level, the
100p<em>th</em> percentile, and the exceedance fraction.  For a detailed
discussion of assumptions, properties, and computational issues
related to ML estimation see Cox and Hinkley (1979) and Cohen (1991).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lnorm.ml(dd)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lnorm.ml_+3A_dd">dd</code></td>
<td>
<p> An n by 2 matrix or data frame with <br />
x (exposure) variable in column 1, and <br />
det= 0 for non-detect or 1 for detect in Column 2  </p>
</td></tr>
</table>


<h3>Details</h3>

<p>For notational convenience the m detected values <code class="reqn">x[i]</code> are listed first 
followed by the <code class="reqn">nx[i]</code> indicating non-detects, so that the data are
<code class="reqn">x[i], i = 1, \ldots , m, nx[i] i = m + 1, \ldots ,n</code>. If <code class="reqn">nx[i]</code> is the same for each
non-detect, this is referred to as a left singly censored sample (Type I
censoring) and <code class="reqn">nx</code> is the limit of detection(LOD). If the <code class="reqn">nx[i]</code> are different,
this is known as randomly (or progressively) left-censored data[see
Cohen(1991) and Schmoyer et al (1996)]. In some situations a value of 0 is
recorded when the exposure measurement is less than the LOD. In this
situation, the value of <code class="reqn">nx[i]</code> is the LOD indicating that <code class="reqn">x</code> is in the interval
<code class="reqn">(0, nx[i])</code>. The probability density function for lognormal distribution is
</p>
<p style="text-align: center;"><code class="reqn">g(x;\mu,\sigma)= exp[-(log(x) - \mu)^2/(2\sigma^2)] /[\sigma x \sqrt(2\Pi )]</code>
</p>

<p>where <code class="reqn">y = log(x)</code> is normally distributed with mean <code class="reqn">\mu</code> and standard
deviation <code class="reqn">\sigma</code> [Atkinson and Brown (1969)]. The geometric mean of X is
<code class="reqn">GM = exp(\mu)</code> and the geometric standard deviation is <code class="reqn">GSD = exp(\sigma)</code>. 
Strom and Stansberry (2000) provide a summary of these and other
relationships for lognormal parameters. Assuming the data are a random
sample from a lognormal distribution, the log of the likelihood function for
the unknown parameters <code class="reqn">\mu</code>  and <code class="reqn">\sigma</code> given the data is
</p>
<p style="text-align: center;"><code class="reqn">L (\mu, \sigma )=\sum log[g(x; \mu, \sigma )] + \sum log[G (nx; \mu, \sigma )],</code>
</p>

<p>where <code class="reqn">G(x; \mu , \sigma)</code> is the lognormal distribution function, i.e., <code class="reqn">G(nx; \mu , \sigma)</code> is the probability that <code class="reqn">x \le nx</code>. 
The first summation is over <code class="reqn">i = 1, \ldots , m</code>, and the second is over <code class="reqn">i = m + 
1, \ldots ,n</code>.
</p>
<p>To test that the mean of <code class="reqn">X &gt; L</code>, <code class="reqn">Ho: E(X) &gt; L</code> at the
<code class="reqn">\alpha = 1- \gamma</code> significance level a one-sided upper <code class="reqn">100\gamma\%</code>
confidence limit can be used. One method for calculating this UCL is to use the
censored data equivalent of Cox's direct method; i.e., calculate the ML
estimate of <code class="reqn">\phi =\mu + [1/2] \sigma ^2</code>, and <code class="reqn">var(\phi) = var(\mu + [1/2] \sigma ^2)</code> where
</p>
<p style="text-align: center;"><code class="reqn">var(\phi )= var(\mu ) + [1/4] var(\sigma^2)+cov(\mu ,\sigma^2).</code>
</p>

<p>The ML estimator of E(X) is <code class="reqn">exp(\phi)</code>, the <code class="reqn">100\gamma {\%}</code> LCL for E(X)
is exp[<code class="reqn">\phi - t var(\phi )</code>], and the <code class="reqn">100\gamma\%</code> UCL for
E(x) is <code class="reqn">exp[\phi + t var(\phi )</code>], where <code class="reqn">t = t(\gamma , m-1)</code>. The
resulting confidence interval (LCL, UCL) has confidence level <code class="reqn">100(2\gamma
-1)\%</code>. An equivalent procedure is to estimate <code class="reqn">\phi = \mu + [1/2] \sigma^2</code>
and its standard error directly, i.e., by maximizing the log-likelihood with
parameters <code class="reqn">\mu + [1/2]\sigma^2</code> and <code class="reqn">\sigma^2</code>. ML estimates of <code class="reqn">\mu , \sigma , \phi , \sigma^2</code>,
estimates of their standard errors, and covariance terms are calculated.
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table role = "presentation">
<tr><td><code>mu</code></td>
<td>
<p> ML estimate of <code class="reqn">\mu</code></p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p> ML estimate of <code class="reqn">\sigma</code></p>
</td></tr> 
<tr><td><code>logEX</code></td>
<td>
<p> ML estimate of log of E(X)</p>
</td></tr>
<tr><td><code>SigmaSq</code></td>
<td>
<p> ML estimate of <code class="reqn">\sigma^2</code></p>
</td></tr>
<tr><td><code>se.mu</code></td>
<td>
<p> ML estimate of standard error of <code class="reqn">\mu</code></p>
</td></tr>
<tr><td><code>se.sigma</code></td>
<td>
<p> ML estimate of standard error of <code class="reqn">\sigma</code></p>
</td></tr> 
<tr><td><code>se.logEX</code></td>
<td>
<p> ML estimate of standard error of log of E(X)</p>
</td></tr>
<tr><td><code>se.Sigmasq</code></td>
<td>
<p> ML estimate of standard error of <code class="reqn">\sigma^2</code></p>
</td></tr>
<tr><td><code>cov.musig</code></td>
<td>
<p> ML estimate of cov(<code class="reqn">\mu</code>,<code class="reqn">\sigma)</code></p>
</td></tr>  
<tr><td><code>m</code></td>
<td>
<p>number of detects</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>number of observations in the data set</p>
</td></tr>
<tr><td><code>m2log(L)</code></td>
<td>
<p> -2 times the log-likelihood function</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p> convergence indicator from <code>optim</code> </p>
</td></tr>  
</table>


<h3>Note</h3>

<p> Local function <code>ndln</code> is called by <code>optim</code> for <code>mu</code> and <code>sigma</code>
and local function <code>ndln2</code> is called by <code>optim</code> for <code>logEX</code> and <code>Sigmasq</code>.
</p>


<h3>Author(s)</h3>

<p>E. L. Frome</p>


<h3>References</h3>

<p>Cohen, A. C. (1991), <em>Truncated and Censored Samples</em>, Marcel Decker, New York
</p>
<p>Cox, D. R. and D. V. Hinkley (1979), <em>Theoretical Statistics</em>, Chapman and Hall, New York.
</p>
<p>Frome, E. L. and Wambach, P. F. (2005), &quot;Statistical Methods and Software for 
the Analysis of Occupational Exposure Data with Non-Detectable Values,&quot; 
<em>ORNL/TM-2005/52,Oak Ridge National Laboratory</em>, Oak Ridge, TN 37830.
Available at: <a href="http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf">http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf</a>
</p>


<h3>See Also</h3>

 <p><code><a href="stats.html#topic+optim">optim</a></code>, <code><a href="#topic+efraction.ml">efraction.ml</a></code>, <code><a href="#topic+percentile.ml">percentile.ml</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'># Calculate MLE for Example 2 in ORNLTM2005-52
data(beTWA)
mle.TWA&lt;- unlist(lnorm.ml(beTWA)) # ML for Be monitoring data
mle.TWA[1:4]  #  ML estimates of parameters
mle.TWA[5:8]  #  Standard errors of ML estimates
mle.TWA[9:13] #  additional results from lnorm.ml
</code></pre>

<hr>
<h2 id='npower.lnorm'>Sample Size and Power For Lognormal Distribution </h2><span id='topic+npower.lnorm'></span>

<h3>Description</h3>

<p>Find either the sample size or power for complete sample from lognormal distribution</p>


<h3>Usage</h3>

<pre><code class='language-R'>npower.lnorm(n=NA,power=NA,fstar=1,p=0.95,gamma=0.95)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="npower.lnorm_+3A_n">n</code></td>
<td>
<p>sample size</p>
</td></tr>
<tr><td><code id="npower.lnorm_+3A_power">power</code></td>
<td>
<p>power of the test = 1 - <code class="reqn">\beta</code></p>
</td></tr>
<tr><td><code id="npower.lnorm_+3A_fstar">fstar</code></td>
<td>
<p> true percent of X's <code class="reqn">\ge</code> limit L</p>
</td></tr>
<tr><td><code id="npower.lnorm_+3A_p">p</code></td>
<td>
<p> probability for Xp the 100p<em>th</em> percentile. Default is 0.95</p>
</td></tr>
<tr><td><code id="npower.lnorm_+3A_gamma">gamma</code></td>
<td>
<p>one-sided confidence level <code class="reqn">\gamma</code>. Default is 0.95</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Find either the sample size <code>n</code> or the <code>power</code> of the test for specified
values of <code>fstar</code>,  <code>p</code>, and <code>gamma</code>. Either <code>n</code> is missing
or <code>power</code> is missing.
</p>
<p>The null hypothesis of interest is
<code class="reqn">Ho: F \ge Fo = 1-p</code>; i.e., Fo is the maximum proportion of the
population that can exceed the limit Lp. The null hypothesis is
rejected if the <code class="reqn">100 \gamma\%</code> UCL for F is less than Fo ,
indicating that the exposure profile is acceptable. For the complete
data case this is equivalent to testing the null hypothesis 
<code class="reqn">Ho: Xp \ge Lp</code> at the <code class="reqn">\alpha = (1- \gamma )</code> significance level.
See <code><a href="#topic+efraction.exact">efraction.exact</a></code>, <code><a href="#topic+percentile.exact">percentile.exact</a></code> and
Section 2.3 of Frome and Wambach(2005) for further details.
</p>


<h3>Value</h3>

<p>A vector with components:
</p>
<table role = "presentation">
<tr><td><code>n</code></td>
<td>
<p>sample size</p>
</td></tr>
<tr><td><code>power</code></td>
<td>
<p>power of the test = 1 -<code class="reqn">\beta</code></p>
</td></tr>
<tr><td><code>fstar</code></td>
<td>
<p>true percent of X's <code class="reqn">\ge</code> limit L</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>probability for Xp the 100p<em>th</em> percentile. Default is 0.95</p>
</td></tr>
<tr><td><code>gamma</code></td>
<td>
<p>one-sided confidence level <code class="reqn">\gamma</code>. Default is 0.95</p>
</td></tr>
</table>


<h3>Note</h3>

<p> The R function <code>uniroot</code> is used to find a parameter of the
non-central t distribution.  In some versions of R this
may cause a warning message.  See R bug report RP 9171 full precision
was not achieved in 'pnt'.  This warning message may occur in <code>uniroot</code>
calls to <code>pt</code> and does not effect the precision of the final result</p>


<h3>Author(s)</h3>

<p> E.L. Frome</p>


<h3>References</h3>

<p>Johnson, N. L. and B. L. Welch (1940), &quot;Application of the Non-Central
t-Distribution,&quot; <em>Biometrika</em>, 31(3/4), 362-389.
</p>
<p>Lyles R. H. and L. L. Kupper (1996), &quot;On strategies for comparing occupational
exposure data to limits,&quot;  <em>American Industrial Hygiene Association
Journal</em>, 57:6-15.
</p>
<p>Frome, E. L. and Wambach, P. F. (2005), &quot;Statistical Methods and Software for 
the Analysis of Occupational Exposure Data with Non-Detectable Values,&quot; 
<em>ORNL/TM-2005/52,Oak Ridge National Laboratory</em>, Oak Ridge, TN 37830.
Available at: <a href="http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf">http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf</a>
</p>
<p>Ignacio, J. S. and W. H. Bullock (2006), <em>A Strategy for Assesing
and Managing Occupational Exposures</em>, Third Edition, AIHA Press,
Fairfax, VA.
</p>
<p>Mulhausen, J. R. and J. Damiano (1998), <em> A Strategy for Assessing
and Managing Occupational Exposures</em>, Second Edition, AIHA Press, Fairfax, VA.
</p>


<h3>See Also</h3>

<p>Help files for <code><a href="#topic+efraction.ml">efraction.ml</a></code>,<code><a href="#topic+percentile.ml">percentile.ml</a></code>,
<code><a href="#topic+efclnp">efclnp</a></code>,<code><a href="#topic+aihand">aihand</a></code>   </p>


<h3>Examples</h3>

<pre><code class='language-R'>#                              EXAMPLE 1
#    Table VII.1 Mulhausen and Damiano (1998) adapted from
#    Table II in Lyles and Kupper (1996) JAIHA vol 57 6-15 Table II
#    Sample Size Needed When Using UTL(95,95) to Show 95% Confidence
#    that the 95th Percentile is below the OEL (Power = 0.8)
rx&lt;-c(1.5,2,2.5,3)
sdx&lt;- sqrt(c(0.5,1,1.5,2,2.5,3))
tabn&lt;-matrix(0,4,6)
for ( i in 1:4) {
  for (j in 1:6) {
fstar&lt;- 100*(1 -pnorm( log(rx[i])/sdx[j] + qnorm(0.95) ))
tabn[i,j]&lt;- npower.lnorm(NA,0.8,fstar,p=0.95,gamma=0.95)[1] 
}
}
cn&lt;- paste("GSD = ",round(exp(sdx),2),sep="" )
dimnames(tabn)&lt;-list( round(1/rx,2),cn)
rm(cn,rx,sdx)
tabn
#                              EXAMPLE 2
top&lt;-"Power For Sample Size n = 20 for p=0.95 gamma=0.95"
fstar &lt;- seq(0.2,4.8,0.1)
pow &lt;- rep(1,length(fstar))
for (i in 1 : length(fstar)) {
pow[i]&lt;-npower.lnorm(20,NA,fstar[i],p=0.95,gamma=0.95)[2]
}
plot(fstar,pow,xlim=c(0,5),ylim=c(0,1),main=top,
xlab="fstar = True Percent of Xs &gt; L(Specified Limit )",ylab="Power")
</code></pre>

<hr>
<h2 id='nptl'>Nonparametric Upper Tolerance Limit</h2><span id='topic+nptl'></span>

<h3>Description</h3>

<p> Given a random sample of size <code>n</code> from a continuous
distribution, then, with a confidence level of at least <code class="reqn">\gamma</code>,
at least 100p percent of the population will be below the k<em>th</em> largest value in the
sample. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nptl(n , p = 0.95, gam = 0.95)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nptl_+3A_n">n</code></td>
<td>
<p>the sample size</p>
</td></tr>
<tr><td><code id="nptl_+3A_p">p</code></td>
<td>
<p>probability for Xp the 100p<em>th</em> percentile. Default is 0.95</p>
</td></tr>
<tr><td><code id="nptl_+3A_gam">gam</code></td>
<td>
<p>one-sided confidence level <code class="reqn">\gamma</code>. Default is 0.95 </p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>k</code></td>
<td>
<p>index of the order statistic</p>
</td></tr>
</table>


<h3>Note</h3>

<p> The maximum non-detect must be less than the k<em>th</em> largest
value. 
</p>


<h3>Author(s)</h3>

<p> E. L. Frome  </p>


<h3>References</h3>

<p>Sommerville, P. N. (1958), &quot;Tables for Obtaining Non-Parametric
Confidence Limits,&quot; <em>Annals of Mathematical Statistics</em>, 29, 599-601.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(beTWA)
k&lt;- nptl(length(beTWA[,1]))
rev(sort(beTWA[,1]))[k]
</code></pre>

<hr>
<h2 id='percentile.exact'>Estimate of Xp and Exact Confidence Limits for Normal/Lognormal </h2><span id='topic+percentile.exact'></span>

<h3>Description</h3>

<p>Calculate estimate of Xp the 100*p percentile of the
normal/lognormal distribution, and the lower and upper 100*<code class="reqn">\gamma</code>% exact
confidence limits. The resulting interval (Xp.LCL,Xp.UCL) is an
approximate <code class="reqn">100*(2\gamma - 1)</code> percent confidence interval for
Xp the 100*p percentile. This function should only be used for complete samples.</p>


<h3>Usage</h3>

<pre><code class='language-R'>percentile.exact(x, p = 0.95, gam = 0.95,logx=TRUE,wpnt=FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="percentile.exact_+3A_x">x</code></td>
<td>
<p>vector of positive data values</p>
</td></tr>
<tr><td><code id="percentile.exact_+3A_p">p</code></td>
<td>
<p>probability for Xp the 100p<em>th</em> percentile. Default is 0.95</p>
</td></tr>
<tr><td><code id="percentile.exact_+3A_gam">gam</code></td>
<td>
<p>one-sided confidence level <code class="reqn">\gamma</code>. Default 0.95</p>
</td></tr>
<tr><td><code id="percentile.exact_+3A_logx">logx</code></td>
<td>
<p>If <code>TRUE</code>, sample is from lognormal, else normal. Default is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="percentile.exact_+3A_wpnt">wpnt</code></td>
<td>
<p>if <code>TRUE</code>, show warning from pnt. Default is <code class="reqn">FALSE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>A point estimate of Xp, the 100p<em>th</em> percentile of a normal/lognormal
distribution is calculated.  Exact confidence limits for Xp are
calculated using the quantile function of the non-central t
distribution.  The exact UCL is <code class="reqn">m + K*s</code>, where <code class="reqn">m</code> is the sample mean, <code class="reqn">s</code>
is the sample standard deviation, and the <code class="reqn">K factor</code> depends on <code class="reqn">n, p,</code> and
<code class="reqn">\gamma</code>.  The exact LCL is <code class="reqn">m + K'*s</code>.  The local function
<code>kf</code> calculates <code class="reqn">K</code> and <code class="reqn">K'</code> using the quantile
function of the non-central t distribution <code>qt</code>.
</p>
<p>The null hypothesis <code class="reqn">Ho: Xp \ge Lp</code> is rejected at the <code class="reqn">\alpha = (1- \gamma )</code> 
significance level if the <code class="reqn">100\gamma\%</code> UCL for Xp
is less than the specified limit Lp (indicating the exposure profile is acceptable).
</p>


<h3>Value</h3>

<p>A LIST with components:
</p>
<table role = "presentation">
<tr><td><code>Xp</code></td>
<td>
<p> estimate of the p<em>th</em> percentile of the distribution</p>
</td></tr>
<tr><td><code>Xpe.LCL</code></td>
<td>
 <p><code class="reqn">100*\gamma</code>% exact lower confidence limit for Xp</p>
</td></tr>
<tr><td><code>Xpe.UCL</code></td>
<td>
 <p><code class="reqn">100*\gamma</code>% exact upper confidence limit for Xp</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>probability for Xp the 100p<em>th</em> percentile. Default 0.95</p>
</td></tr>
<tr><td><code>gam</code></td>
<td>
<p>one-sided confidence level <code class="reqn">\gamma</code>. Default is 0.95</p>
</td></tr>
<tr><td><code>Logx</code></td>
<td>
<p>If <code>TRUE</code>, sample is from lognormal, else normal. Default is TRUE</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>sample size</p>
</td></tr>
<tr><td><code>Ku</code></td>
<td>
<p>the K factor used to calculate the exact UCL</p>
</td></tr>
<tr><td><code>Kl</code></td>
<td>
<p>the K' factor used to calculate the exact LCL</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The UCL is also referred to as an upper tolerance limit,
i.e., if <code>p</code> = 0.95 and <code class="reqn">\gamma</code> = 0.99 then Xpe.UCL is the exact UTL 95% - 99%.
</p>


<h3>Author(s)</h3>

<p>E. L. Frome </p>


<h3>References</h3>

<p>Burrows, G. L. (1963), &quot;Statistical Tolerance Limits - What are They,&quot;
<em>Applied Statistics</em>, 12, 133-144.
</p>
<p>Johnson, N. L. and B. L. Welch (1940), &quot;Application of the Non-Central
t-Distribution,&quot; <em>Biometrika</em>, 31(3/4), 362-389.
</p>
<p>Lyles R. H. and L. L. Kupper (1996), &quot;On Strategies for Comparing Occupational
Exposure Data to Limits,&quot;  <em>American Industrial Hygiene Association
Journal</em>, 57:6-15.
</p>
<p>Tuggle, R. M. (1982), &quot;Assessment of Occupational Exposure Using
One-Sided Tolerance Limits,&quot; <em>American Industrial Hygiene Association
Journal</em>, 43, 338-346.
</p>
<p>Frome, E. L. and Wambach, P. F. (2005), &quot;Statistical Methods and Software for 
the Analysis of Occupational Exposure Data with Non-Detectable Values,&quot; 
<em>ORNL/TM-2005/52,Oak Ridge National Laboratory</em>, Oak Ridge, TN 37830.
Available at: <a href="http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf">http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf</a>
</p>
<p>Ignacio, J. S. and W. H. Bullock (2006), <em>A Strategy for Assesing
and Managing Occupational Exposures</em>, Third Edition, AIHA Press,
Fairfax, VA.
</p>
<p>Mulhausen, J. R. and J. Damiano (1998), <em> A Strategy for Assessing
and Managing Occupational Exposures</em>, Second Edition, AIHA Press, Fairfax, VA.
</p>


<h3>See Also</h3>

<p> Help files for <code><a href="#topic+percentile.ml">percentile.ml</a></code>,
<code><a href="#topic+efraction.exact">efraction.exact</a></code>, <code><a href="#topic+aihand">aihand</a></code>  </p>


<h3>Examples</h3>

<pre><code class='language-R'>#                      EXAMPLE 1
# calculate 95th percentile and exact CLs for Example data
# Appendix  Mulhausen and Damiano (1998)
data(aihand)
x &lt;- aihand$x ;  det &lt;- rep(1,length(x))
aiha &lt;- data.frame(x,det) #  complete data
unlist(percentile.exact(x,gam=0.95,p=0.95) )[1:5]  #  exact CLs
unlist(percentile.ml(aiha,gam=0.95,p=0.95))   #  ML CLs
#                      EXAMPLE 2
#  Ignacio and Bullock (2006) Mulhausen and Damiano (1998)
#  Calculate TABLE VII.3 (page 272) Factor for One-Sided Tolerance
#  Limits for Normal Distribution (Abridged Version)
#  Same as Table III Burrows(1963) Panel 3 Page 138
nn &lt;- c(seq(3,25),seq(30,50,5))
pv &lt;-c(0.75,0.9,0.95,0.99,0.999)
tab &lt;- matrix(0,length(nn),length(pv))
  for( k in (1:length(nn) ) ){
  xx &lt;- seq(1,nn[k])
  for(j in (1:length(pv))) {
  tab[k,j ]&lt;- percentile.exact(xx,pv[j],gam=0.95,FALSE)$Ku
}}
dimnames(tab)&lt;-(list(nn,pv)) ; rm(nn,pv,xx)
round(tab,3)
#
#                      EXAMPLE 3
#  Calculate TABLE I One Sided Tolerance Factor K'
#  Tuggle(1982) Page 339 (Abridged Version)
nn &lt;- c(seq(3,20),50,50000000)
pv &lt;-c(0.9,0.95,0.99)
tab &lt;- matrix(0,length(nn),length(pv))
  for( k in (1:length(nn) ) ){
  xx &lt;- seq(1,nn[k])
  for(j in (1:length(pv))) {
  tab[k,j ]&lt;- percentile.exact(xx,pv[j],gam=0.95,FALSE)$Kl
}}
dimnames(tab)&lt;-(list(nn,pv)) ; rm(nn,pv,xx)
round(tab,3)
</code></pre>

<hr>
<h2 id='percentile.ml'>Calculate ML Estimate of Xp and Confidence Limits </h2><span id='topic+percentile.ml'></span>

<h3>Description</h3>

<p>Calculate the ML estimate of Xp the 100p<em>th</em> percentile
of the lognormal distribution, and the lower and upper <code class="reqn">100*\gamma</code>% confidence limits
LX(<code>p</code>,<code class="reqn">\gamma</code>) and UX(<code>p</code>,<code class="reqn">\gamma</code>).  The upper confidence limit is used to
test the null hypothesis that the exposure profile is &quot;unacceptable&quot;.
If UX(<code>p</code>,<code class="reqn">\gamma) &lt; L</code> the null hypothesis is rejected and workplace
is considered &quot;safe&quot; or the object/area is not contaminated.  The
Type I error is <code class="reqn">\le \alpha = 1 - \gamma</code>.  The resulting interval (LX,UX)
is an approximate <code class="reqn">100*(2\gamma - 1)</code> percent confidence interval for Xp.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>percentile.ml(dd, p = 0.95, gam = 0.95, dat = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="percentile.ml_+3A_dd">dd</code></td>
<td>
<p>An n by 2 matrix or data frame with <br />
x (exposure) variable in column 1, and <br />
det= 0 for non-detect or 1 for detect in column 2</p>
</td></tr>
<tr><td><code id="percentile.ml_+3A_p">p</code></td>
<td>
<p>is probability for Xp the 100p<em>th</em> percentile. Default is 0.95</p>
</td></tr>
<tr><td><code id="percentile.ml_+3A_gam">gam</code></td>
<td>
<p> one-sided confidence level <code class="reqn">\gamma</code>. Default is 0.95</p>
</td></tr>
<tr><td><code id="percentile.ml_+3A_dat">dat</code></td>
<td>
<p> if <code>dat</code> is <code>FALSE</code> then <code>dd</code> is a list from
<code><a href="#topic+lnorm.ml">lnorm.ml</a></code>. Default is <code>TRUE</code> </p>
</td></tr>  
</table>


<h3>Details</h3>

<p>The point estimate of <code class="reqn">Yp = log(Xp)</code> is <code class="reqn">\mu  + z \sigma</code> where <code class="reqn">\mu</code> and
<code class="reqn">\sigma</code> are ML estimates and <code class="reqn">z</code> is qnorm(p). The variance of the estimate is
</p>
<p style="text-align: center;"><code class="reqn">var(\mu  + z\sigma ) = var(\mu ) + Z^2p var (\sigma )+ 2z 
cov(\mu ,\sigma)</code>
</p>
 
<p>The <code class="reqn">100\gamma {\%}</code> LCL and UCL for Xp are
</p>
<p style="text-align: center;"><code class="reqn">LX(p,\gamma ) = exp[Yp- t(\gamma ,(m-1))var(Yp)^{1/2}],</code>
</p>

<p style="text-align: center;"><code class="reqn">UX(p,\gamma ) = exp[Yp + t(\gamma ,(m-1))var(Yp)^{1/2}].</code>
</p>

<p>The ML estimates of <code class="reqn">var(\mu)</code>, <code class="reqn">var(\sigma)</code>, and <code class="reqn">cov(\mu 
,\sigma)</code> are obtained from the ML variance-covariance matrix using 
<code><a href="#topic+lnorm.ml">lnorm.ml</a></code>. The null hypothesis <code class="reqn">Ho: Xp \ge Lp</code> is rejected at the <code class="reqn">\alpha = (1-
\gamma )</code> significance level if the <code class="reqn">100\gamma\%</code> UCL for Xp &lt; Lp (indicating the exposure profile is acceptable).
</p>


<h3>Value</h3>

<p>A LIST with components:
</p>
<table role = "presentation">
<tr><td><code>Xp</code></td>
<td>
<p>ML estimate of the p<em>th</em> percentile of lognormal distribution</p>
</td></tr>
<tr><td><code>Xp.LCL</code></td>
<td>
 <p><code class="reqn">100*\gamma</code>% lower confidence limit for Xp</p>
</td></tr>
<tr><td><code>Xp.UCL</code></td>
<td>
 <p><code class="reqn">100*\gamma</code>% upper confidence limit for Xp</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>probability for Xp the 100p<em>th</em> percentile. Default 0.95</p>
</td></tr>
<tr><td><code>gam</code></td>
<td>
<p>one-sided confidence level <code class="reqn">\gamma</code>. Default is 0.95</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The UCL is also referred to as an upper tolerance limit(UTL),
i.e., if p = 0.95 and gam = 0.99 then Xp.UCL is the UTL-95%-99%.
</p>


<h3>Author(s)</h3>

<p>E. L. Frome </p>


<h3>References</h3>

<p>Cohen, A. C. (1991), <em>Truncated and Censored Samples</em>, Marcel Decker, New York
</p>
<p>Cox, D. R. and D. V. Hinkley (1979), <em>Theoretical Statistics</em>, Chapman and Hall, New York.
</p>
<p>Frome, E. L. and Wambach, P. F. (2005), &quot;Statistical Methods and Software for 
the Analysis of Occupational Exposure Data with Non-Detectable Values,&quot; 
<em>ORNL/TM-2005/52,Oak Ridge National Laboratory</em>, Oak Ridge, TN 37830.
Available at: <a href="http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf">http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf</a>
</p>


<h3>See Also</h3>

<p> Help files for <code><a href="#topic+lnorm.ml">lnorm.ml</a></code>,<code><a href="#topic+efraction.ml">efraction.ml</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(beTWA)
# calculate ML estimate of 95th percentile and CLs for Example 2 in ORNLTM2005-52 
unlist(percentile.ml(beTWA,0.95,0.95))

</code></pre>

<hr>
<h2 id='percentile.ple'>Calculate Nonparametric Estimate of Xp and Confidence Limits </h2><span id='topic+percentile.ple'></span>

<h3>Description</h3>

<p>Find Xp, the 100p<em>th</em> percentile, and the  <code class="reqn">100\gamma</code>%
nonparametric confidence limits from PLE of F(x).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>percentile.ple(dd, p = 0.95, gam = 0.95, interp = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="percentile.ple_+3A_dd">dd</code></td>
<td>
<p> An n by 2 matrix or data frame with <br />
x (exposure) variable in column 1, and <br />
det= 0 for non-detect or 1 for detect in column 2 </p>
</td></tr>
<tr><td><code id="percentile.ple_+3A_p">p</code></td>
<td>
<p> Find x such that the PLE of F(x) = <code>p</code>. Default 0.95 </p>
</td></tr>
<tr><td><code id="percentile.ple_+3A_gam">gam</code></td>
<td>
<p> one-sided confidence level <code class="reqn">\gamma</code>. Default is 0.95 </p>
</td></tr>
<tr><td><code id="percentile.ple_+3A_interp">interp</code></td>
<td>
<p> if <code>interp</code> is <code>TRUE</code> use linear interpolation. Default <code>TRUE</code> </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Find Xp the 100p<em>th</em> percentile and confidence limits from the
PLE of F(x) &ndash; see <code><a href="#topic+plekm">plekm</a></code> for additional details.
If interp is <code>TRUE</code> use linear interpolation; otherwise, the upper confidence
limit (UCL) for Xp, UX(<code>p</code>,<code class="reqn">\gamma</code>), is the smallest value of <code class="reqn">x</code> such that
the LCL for F(x) is <code class="reqn">\ge</code> <code>p</code>, the lower confidence limit (LCL),
LX(<code>p</code>,<code class="reqn">\gamma</code>), is the largest value of <code class="reqn">x</code> such that the UCL for F(x)
is <code class="reqn">\le</code> <code>p</code>.
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table role = "presentation">
<tr><td><code>Xp</code></td>
<td>
<p> PLE of the 100p<em>th</em> percentile</p>
</td></tr>
<tr><td><code>LXp</code></td>
<td>
<p> PLE of LX(<code>p</code>,<code class="reqn">\gamma</code>) the <code class="reqn">100*\gamma</code>% LCL for Xp</p>
</td></tr>
<tr><td><code>UXp</code></td>
<td>
<p> PLE of UX(<code>p</code>,<code class="reqn">\gamma</code>) the <code class="reqn">100*\gamma</code>% UCL for Xp</p>
</td></tr> 
<tr><td><code>p</code></td>
<td>
<p>probability for Xp the 100p<em>th</em> percentile</p>
</td></tr>
<tr><td><code>gam</code></td>
<td>
<p>one-sided confidence level <code class="reqn">\gamma</code>. Default is 0.95</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> E. L. Frome </p>


<h3>References</h3>

<p>Frome, E. L. and Wambach, P. F. (2005), &quot;Statistical Methods and Software for 
the Analysis of Occupational Exposure Data with Non-Detectable Values,&quot; 
<em>ORNL/TM-2005/52,Oak Ridge National Laboratory</em>, Oak Ridge, TN 37830.
Available at: <a href="http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf">http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf</a>
</p>


<h3>See Also</h3>

<p> See Also as <code><a href="#topic+plekm">plekm</a></code> and <code><a href="#topic+SESdata">SESdata</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'># use data from example 2 in ORNL/TM-2005/52 to calculate
# 95 percent UCL for 95th percentile
data(beTWA) 
unlist(percentile.ple(beTWA))
unlist(percentile.ml(beTWA)) # compare ML estimates
</code></pre>

<hr>
<h2 id='ple.plot'> Plot PLE With Confidence Limits  </h2><span id='topic+ple.plot'></span>

<h3>Description</h3>

<p>Plot the product limit estimate (PLE) of F(x) and <code class="reqn">100(2\gamma -1)\%</code>
two-sided confidence limits (CLs) for left censored data. A horizontal line
corresponding to the Xp = 100p<em>th</em> percentile is added to the plot and
the nonparametric confidence limits for Xp are displayed in the title.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ple.plot(dd, gam = 0.95, p = 0.95, xlow = 0, xh = NA, ylow = 0, yh = 1,...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ple.plot_+3A_dd">dd</code></td>
<td>
<p>An n by 2 matrix or data frame with <br />
x (exposure) variable in column 1, and <br />
det = 0 for non-detect or 1 for detect in column 2  </p>
</td></tr>
<tr><td><code id="ple.plot_+3A_gam">gam</code></td>
<td>
<p> one-sided confidence level <code class="reqn">\gamma</code>. Default is 0.95 </p>
</td></tr>
<tr><td><code id="ple.plot_+3A_p">p</code></td>
<td>
<p>probability for Xp the 100p<em>th</em> percentile. Default is 0.95</p>
</td></tr>
<tr><td><code id="ple.plot_+3A_xlow">xlow</code></td>
<td>
<p> minimum value on x axis. Default = 0 </p>
</td></tr>
<tr><td><code id="ple.plot_+3A_xh">xh</code></td>
<td>
<p> maximum value on the x axis. Default = maximum value of x </p>
</td></tr>
<tr><td><code id="ple.plot_+3A_ylow">ylow</code></td>
<td>
<p> minimum value on y axis. Default = 0  </p>
</td></tr>
<tr><td><code id="ple.plot_+3A_yh">yh</code></td>
<td>
<p> maximum value on the y axis. Default = 1 </p>
</td></tr>
<tr><td><code id="ple.plot_+3A_...">...</code></td>
<td>
<p>Additional parameters to plot</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data frame with columns
</p>
<table role = "presentation">
<tr><td><code>a</code></td>
<td>
<p> value of j<em>th</em> detect (ordered) </p>
</td></tr>
<tr><td><code>ple</code></td>
<td>
<p> PLE of F(x) at a  </p>
</td></tr>
<tr><td><code>stder</code></td>
<td>
<p> standard error of F(x) at a </p>
</td></tr>
<tr><td><code>lower</code></td>
<td>
<p> lower CL for PLE at a </p>
</td></tr>
<tr><td><code>upper</code></td>
<td>
<p> upper CL for PLE at a </p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p> number of detects or non-detects <code class="reqn">\ge</code> a </p>
</td></tr>
<tr><td><code>r</code></td>
<td>
<p> number of detects equal to a </p>
</td></tr>
</table>


<h3>Note</h3>

<p> If the solid horizontal line does not intersect the lower
CL for the PLE, then the upper CL for Xp UX(<code>p</code>,<code class="reqn">\gamma</code>) is not defined.
</p>


<h3>Author(s)</h3>

<p> E. L. Frome </p>


<h3>See Also</h3>

<p> See Also <code><a href="#topic+plekm">plekm</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(beTWA)
par( mfrow=c(1,2) )
ple.plot(beTWA)  #  plot the PLE of F(x) for the beTWA data
ple.plot(beTWA,ylow=0.8) #  plot the upper right tail 
# Lognormal ML estimates of 95th percentile and CLs
unlist(percentile.ml(beTWA))
# PLE   estimates of 95th percentile and CLs
unlist(percentile.ple(beTWA))
#
</code></pre>

<hr>
<h2 id='pleicf'>Product Limit Estimate for Interval Censored Data </h2><span id='topic+pleicf'></span>

<h3>Description</h3>

<p>Compute Product Limit Estimate (PLE) of F(x) for interval censored
data (i.e., the nonparametric maximum likelihood estimate that generalizes 
the Kaplan-Meier estimate to interval censored data).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pleicf(dd,nondet=TRUE,mine=1e-06,maxc=10000,eps=1e-14)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pleicf_+3A_dd">dd</code></td>
<td>
<p> n by 2 matrix or data frame (see note below)</p>
</td></tr>
<tr><td><code id="pleicf_+3A_nondet">nondet</code></td>
<td>
<p> if <code>TRUE</code>, <code>dd</code> is left censored data </p>
</td></tr>
<tr><td><code id="pleicf_+3A_mine">mine</code></td>
<td>
<p> minimum error for convergence in icfit. Default = 1e-06.</p>
</td></tr>
<tr><td><code id="pleicf_+3A_maxc">maxc</code></td>
<td>
<p> maximum number of iterations. Default is 10000.</p>
</td></tr>
<tr><td><code id="pleicf_+3A_eps">eps</code></td>
<td>
<p> adjustment factor described by Ng. Default is 1e-14.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a driver function for <code><a href="#topic+icfit">icfit</a></code> 
that uses an EM-algorithm applied to interval censored data (see Turnbull, 1976).
</p>


<h3>Value</h3>

<p>Data frame with columns
</p>
<table role = "presentation">
<tr><td><code>a</code></td>
<td>
<p> value of j<em>th</em> uncensored value (ordered)</p>
</td></tr>
<tr><td><code>ple</code></td>
<td>
<p> PLE of F(x) at a</p>
</td></tr>
<tr><td><code>surv</code></td>
<td>
 <p><code class="reqn">1 - F()</code>, i.e the &quot;survival&quot; or &quot;exceedance&quot; function</p>
</td></tr>
<tr><td><code>prob</code></td>
<td>
<p> prob[X = x] from <code>icfit</code> </p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p> sample size</p>
</td></tr>
</table>


<h3>Note</h3>

<p>If <code>nondet</code> is <code>TRUE</code> column 1 of dd is the data value
and column 2 is 1 if a detect and 0 otherwise.  If <code>nondet</code> is
<code>FALSE</code> <code>dd</code> contains the left and right endpoints required
by <code><a href="#topic+icfit">icfit</a></code>.
</p>


<h3>Author(s)</h3>

<p>E. L. Frome </p>


<h3>References</h3>

<p>Fay, M. P. (1999), &quot;Comparing Several Score Tests for Interval Censored Data,&quot;
<em>Statistics in  Medicine</em>,18:273-85.
(Corr: 1999,  Vol 19, p.2681).  
</p>
<p>Ng, M. P. (2002), &quot;A Modification of Peto's Nonparametric Estimation of
Survival Curves for Interval-Censored Data,&quot; <em>Biometrics</em>, 58,
439-442. 
</p>
<p>Turnbull, B. W. (1976), &quot;The Empirical Distribution Function with
Arbitrarily Grouped, Censored and Truncated Data,&quot; <em>Journal of the
Royal Statistical Society</em>, Series B (Methodological), 38(3), 290-295.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+icfit">icfit</a></code>,  <code><a href="#topic+plend">plend</a></code>, <code><a href="#topic+plekm">plekm</a></code>  </p>


<h3>Examples</h3>

<pre><code class='language-R'># PLE for interval censored filmbadge data
data(filmbadge)
ple.fb&lt;-pleicf(filmbadge[,1:2],FALSE) # PLE for input to qq.lnorm
tmp &lt;- qq.lnorm(ple.fb) ; GM&lt;-round(exp(tmp$par[1]));GSD&lt;-round(exp(tmp$par[2]),2)
tp&lt;-paste("Lognormal Q-Q plot for Filmbadge Data GM= ",GM,"GSD= ",GSD)
title(tp) # title for q-q plot with graphical parameter estimates

</code></pre>

<hr>
<h2 id='plekm'>Product Limit Estimate for Non-detects Using Kaplan-Meier </h2><span id='topic+plekm'></span>

<h3>Description</h3>

<p>Compute Product Limit Estimate (PLE) of F(x) and Confidence Limits for data with
non-detects (left censored data).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plekm(dd,gam)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plekm_+3A_dd">dd</code></td>
<td>
<p>An n by 2 matrix or data frame with <br />
x (exposure) variable in column 1, and <br />
det= 0 for non-detect or 1 for detect in column 2 </p>
</td></tr>
<tr><td><code id="plekm_+3A_gam">gam</code></td>
<td>
<p> one-sided confidence level <code class="reqn">\gamma</code>. Default is 0.95 </p>
</td></tr>
</table>


<h3>Details</h3>

<p>R function <code>survreg</code> is used to calculate Kaplan-Meier estimate
of S(z), where z = k - x and k is greater than the largest x value.
This technique of &quot;reversing the data&quot; to convert left censored data
to right censored data was first suggested by Nelson (1972). conf.type
= &quot;plain&quot; is required in survreg for correct CLs.  The value of S(z)
is then used to calculate F(x).  Note that if <code class="reqn">\gamma</code> = 0.95 the 90%
two-sided CLs are calculated.
</p>


<h3>Value</h3>

<p>Data frame with columns
</p>
<table role = "presentation">
<tr><td><code>a</code></td>
<td>
<p> is the  value of j<em>th</em> detect (ordered) </p>
</td></tr>
<tr><td><code>ple</code></td>
<td>
<p> is PLE of F(x) at <code>a</code>  </p>
</td></tr>
<tr><td><code>stder</code></td>
<td>
<p> standard error of F(x) at <code>a</code> </p>
</td></tr>
<tr><td><code>lower</code></td>
<td>
<p> lower CL for PLE at <code>a</code> </p>
</td></tr>
<tr><td><code>upper</code></td>
<td>
<p> upper CL for PLE at <code>a</code> </p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p> number of detects or non-detects  <code class="reqn">\le</code><code>a</code> </p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p> number of detects equal to <code>a</code> </p>
</td></tr>
</table>


<h3>Note</h3>

<p>In survival analysis S(x) = 1 - F(x) is the survival function
i.e., S(x) = P [X &gt; x]. In environmental and occupational situations
S(x) is the &quot;exceedance&quot; function, i.e., S(x) = is the proportion of
X values that exceed x. The PLE is the sample estimate of F(x), i.e.,
the proportion of values in the sample that are less than x.
</p>


<h3>Author(s)</h3>

<p>E. L. Frome </p>


<h3>References</h3>

<p>Nelson, W.(1972), &quot;Theory and Application of Hazard Plotting for
Censored Failure  Data&quot;, <em>Technometrics</em>, 14, 945-66
</p>
<p>Frome, E. L. and Wambach, P.F. (2005) &quot;Statistical Methods and Software for
the Analysis of Occupational Exposure Data with Non-Detectable Values&quot;,
<em>ORNL/TM-2005/52,Oak Ridge National Laboratory</em>, Oak Ridge, TN 37830.
Available at: <a href="http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf">http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf</a>
</p>
<p>Schmoyer, R. L., J. J. Beauchamp, C. C. Brandt and F. O. Hoffman, Jr.
(1996), &quot;Difficulties with the Lognormal Model in Mean Estimation and
Testing,&quot; <em>Environmental and Ecological Statistics</em>, 3, 81-97.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plend">plend</a></code>, <code><a href="#topic+pleicf">pleicf</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(SESdata) ## use SESdata data set Example 1 from ORNLTM-2005/52
pkm&lt;- plekm(SESdata)
qq.lnorm(pkm) #  lognormal q-q plot based on PLE
round(pkm,3)

</code></pre>

<hr>
<h2 id='plend'> Compute Product Limit Estimate for Non-detects </h2><span id='topic+plend'></span>

<h3>Description</h3>

<p>Compute Product Limit Estimate(PLE) of F(x) for positive data with 
non-detects (left censored data) 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plend(dd)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plend_+3A_dd">dd</code></td>
<td>
<p> An n by 2 matrix or data frame with <br />
x (exposure) variable in column 1, and <br />
det = 0 for non-detect or 1 for detect in column 2 </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The product limit estimate (PLE) of the cumulative distribution function
was first proposed by Kaplan and Meier (1958) for right censored data.
Turnbull (1976) provides a more general treatment of nonparametric
estimation of the distribution function for arbitrary censoring. For
randomly left censored data, the PLE is defined as follows [Schmoyer et al.
(1996)]. Let <code class="reqn">a[1]&lt; \ldots &lt; a[m]</code> be the m distinct values at
which detects occur, r[j] is the number of detects at a[j], and n[j] is the
sum of non-detects and detects that are less than or equal to a[j]. Then the
PLE is defined to be 0 for <code class="reqn">0 \le  x \le  a0</code>, where a0 is a[1] or the
value of the detection limit for the smallest non-detect if it is less than
a[1]. For <code class="reqn">a0 \le  x &lt; a[m]</code> the PLE is <code class="reqn">F[j]= \prod (n[j] --
r[j])/n[j]</code>, where the product is over all <code class="reqn">a[j] &gt; x</code>, and the PLE is 1 for
<code class="reqn">x \ge a[m]</code>. When there are only detects this reduces to the usual
definition of the empirical cumulative distribution function.
</p>


<h3>Value</h3>

<p>Data frame with columns
</p>
<table role = "presentation">
<tr><td><code>a(j)</code></td>
<td>
<p> value of j<em>th</em> detect (ordered)</p>
</td></tr>
<tr><td><code>ple(j)</code></td>
<td>
<p> PLE of F(x) at a(j)</p>
</td></tr>
<tr><td><code>n(j)</code></td>
<td>
<p> number of detects or non-detects <code class="reqn">\le</code> a(j)</p>
</td></tr>
<tr><td><code>r(j)</code></td>
<td>
<p> number of detects equal to a(j)</p>
</td></tr>
<tr><td><code>surv(j)</code></td>
<td>
<p> 1 - ple(j) is PLE of S(x)</p>
</td></tr>
</table>


<h3>Note</h3>

  
<p>In survival analysis <code class="reqn">S(x) = 1 - F(x)</code> is the survival function
i.e., <code class="reqn">S(x) = P[X &gt; x]</code>. In environmental and occupational situations
<code class="reqn">1 - F(x)</code> is the &quot;exceedance&quot; function, i.e., <code class="reqn">C(x) = 1 - F(x) = P [X &gt; x]</code>.
</p>


<h3>Author(s)</h3>

<p>E. L. Frome </p>


<h3>References</h3>

<p>Frome, E. L. and Wambach, P. F. (2005), &quot;Statistical Methods and Software for 
the Analysis of Occupational Exposure Data with Non-Detectable Values,&quot; 
<em>ORNL/TM-2005/52,Oak Ridge National Laboratory</em>, Oak Ridge, TN 37830.
Available at: <a href="http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf">http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf</a>
</p>
<p>Kaplan, E. L. and Meier, P. (1958), &quot;Nonparametric Estimation from Incomplete Observations,&quot; <em>Journal of the American Statistical Association</em>, 457-481. 
</p>
<p>Schmoyer, R. L., J. J. Beauchamp, C. C. Brandt and F. O. Hoffman, Jr.
(1996), &quot;Difficulties with the Lognormal Model in Mean Estimation and
Testing,&quot; <em>Environmental and Ecological Statistics</em>, 3, 81-97.
</p>
<p>Turnbull, B. W. (1976), &quot;The Empirical Distribution Function with Arbitrarily Grouped, Censored and Truncated Data,&quot; <em>Journal of the Royal Statistical Society</em>, Series B (Methodological), 38(3), 290-295.
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+plekm">plekm</a></code>, <code><a href="#topic+pleicf">pleicf</a></code>, <code><a href="#topic+qq.lnorm">qq.lnorm</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(SESdata) #  use SESdata data set Example 1 from ORNLTM-2005/52
pnd&lt;- plend(SESdata)
Ia&lt;-"Q-Q plot For SESdata "
qq.lnorm(pnd,main=Ia) #  lognormal q-q plot based on PLE 
pnd
</code></pre>

<hr>
<h2 id='qq.lnorm'> Quantile-Quantile Plot for Censored Lognormal Data </h2><span id='topic+qq.lnorm'></span>

<h3>Description</h3>

<p>qq.lnorm produces a lognormal quantile-quantile (q-q) plot based on the product limit estimate (PLE) 
of the cumulative distribution function (CDF) F(x) for censored data.  A line is added to the plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qq.lnorm(pl, mu, sigma, aveple = TRUE,...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="qq.lnorm_+3A_pl">pl</code></td>
<td>
<p> A data frame with the data(x) in column 1 and PLE in column 2</p>
</td></tr>
<tr><td><code id="qq.lnorm_+3A_mu">mu</code></td>
<td>
<p> estimate of the log scale mean </p>
</td></tr>
<tr><td><code id="qq.lnorm_+3A_sigma">sigma</code></td>
<td>
<p> estimate of log scale standard deviation </p>
</td></tr>
<tr><td><code id="qq.lnorm_+3A_aveple">aveple</code></td>
<td>
<p> if <code>TRUE</code>, calculate plotting positions by averaging</p>
</td></tr>
<tr><td><code id="qq.lnorm_+3A_...">...</code></td>
<td>
<p>Additional parameters to plot</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The PLE is used to determine the plotting positions on the horizontal
axis for the censored data version of a theoretical q-q plot for the lognormal distribution.  Waller and Turnbull (1992)
provide a good overview of q-q plots and other graphical methods for
censored data.  The lognormal q-q plot is obtained by plotting
detected values <code class="reqn">a[j]</code>(on log scale) versus <code class="reqn">H[p(j)]</code> where <code class="reqn">H(p)</code> is the
inverse of the distribution function of the standard normal
distribution. If the largest data value is not censored then the PLE
is 1 and H(1) is off scale.  The &quot;plotting positions&quot; <code class="reqn">p[j]</code> are
determined from the PLE of F(x) by multiplying each estimate by
<code class="reqn">n /(n+1)</code>, or by averaging adjacent values&ndash;see Meeker and Escobar
(1998, Chap 6)]. In complete data case without ties the first approach
is equivalent to replacing the sample CDF <code class="reqn">j / n</code> with <code class="reqn">j / (n+1)</code>, and for
the second approach the plotting positions are equal to <code class="reqn">(j - .5) / n</code>. If
the lognormal distribution is a close approximation to the empirical
distribution, the points on the plot will fall near a straight line.
An objective evaluation of this is obtained by calculating <code>Rsq</code> the
square of the correlation coefficient associated with the plot.
</p>
<p>A line is added to the plot based on the values of <code>mu</code> and <code>sigma</code>.
If either of these is missing <code>mu</code> and <code>sigma</code> are estimated by
linear regression of <code class="reqn">log(y)</code> on <code class="reqn">H[p(j)]</code>.
</p>


<h3>Value</h3>

<p>A list with components
</p>
<table role = "presentation">
<tr><td><code>x</code></td>
<td>
<p> The x coordinates of the points that were plotted</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p> The y coordinates of the points that were plotted</p>
</td></tr>
<tr><td><code>pp</code></td>
<td>
<p> The adjusted probabilities use to determine <code>x</code> </p>
</td></tr>
<tr><td><code>par</code></td>
<td>
<p> The values of <code>mu</code>, <code>sigma</code>, and  <code>Rsq</code> </p>
</td></tr>
</table>


<h3>Note</h3>

<p> Helsel and Cohen (1988) consider alternative procedures that
can be used for calculating plotting positions for left censored
data. Waller and Turnbull (1992) describe a modification of the
Kaplan-Meier estimator that can be used for right censored data and
note that for the purpose of assessing goodness of fit the choice of
plotting positions makes little qualitative difference in the
appearance of any particular plot. The two options in this function
can be used for any type of censoring.
</p>


<h3>Author(s)</h3>

<p> E. L. Frome </p>


<h3>References</h3>

<p>Fay, M. P. (1999), &quot;Comparing Several Score Tests for Interval Censored Data,&quot; 
<em>Statistics in Medicine</em>, 1999; 18:273-85. (Corr: 1999, Vol 19, p.2681).
</p>
<p>Frome, E. L. and Wambach, P. F. (2005), &quot;Statistical Methods and Software for 
the Analysis of Occupational Exposure Data with Non-Detectable Values,&quot; 
<em>ORNL/TM-2005/52,Oak Ridge National Laboratory</em>, Oak Ridge, TN 37830.
Available at: <a href="http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf">http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf</a>
</p>
<p>Hesel, D. R. and T. A. Cohn (1988), &quot;Estimation of Descriptive
Statistics for Multiply Censored Water Quality Data,&quot; <em>Water
Resources Research</em>, 24, 1997-2004.  
</p>
<p>Meeker, W. Q. and L. A. Escobar (1998), <em>Statistical Methods for
Reliability Data</em>, John Wiley and Sons, New York.  
</p>
<p>Ny, M. P. (2002), &quot;A Modification of Peto's Nonparametric Estimation of
Survival Curves for Interval-Censored Data,&quot; <em>Biometrics</em>, 58,
439-442. 
</p>
<p>Waller, L. A. and B. W. Turnbull (1992), &quot;Probability Plotting with Censored Data,&quot; 
<em>The American Statistician</em>, 46(1), 5-12.
</p>


<h3>See Also</h3>

  <p><code><a href="#topic+plekm">plekm</a></code>, <code><a href="#topic+plend">plend</a></code>, <code><a href="#topic+pleicf">pleicf</a></code>  </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(SESdata) #  use SESdata data set Example 1 from ORNLTM-2005/52
pnd&lt;- plend(SESdata)
qq.lnorm(pnd) #  lognormal q-q plot based on PLE 
Ia &lt;- "Q-Q plot For SESdata "
qqout &lt;- qq.lnorm(pnd,main=Ia) #  lognormal q-q plot based on PLE 
qqout
</code></pre>

<hr>
<h2 id='readss'>Read Analyze Data From ASCII  File </h2><span id='topic+readss'></span>

<h3>Description</h3>

<p> Read data from fn.txt (space delimited text file) or fn.csv
(comma delimited text file) and calculate all summary statistics using
<code><a href="#topic+IH.summary">IH.summary</a></code> .  Output results to an ASCII text file fnout.csv in CSV format
</p>


<h3>Usage</h3>

<pre><code class='language-R'>readss(fn,L,bcol=NA,rto=5,pstat=NA,reverse=FALSE,p=0.95,gam=0.95,comma=FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="readss_+3A_fn">fn</code></td>
<td>
<p>name of input data file in double quotes without the .txt or .csv extension</p>
</td></tr>
<tr><td><code id="readss_+3A_l">L</code></td>
<td>
<p> L is specified limit for the exceedance fraction; e.g., the occupational exposure limit</p>
</td></tr>
<tr><td><code id="readss_+3A_bcol">bcol</code></td>
<td>
<p> Column that contains the BY variable&ndash;see details. Default NA</p>
</td></tr>
<tr><td><code id="readss_+3A_rto">rto</code></td>
<td>
<p>Round values to rto. Default = 5</p>
</td></tr>
<tr><td><code id="readss_+3A_pstat">pstat</code></td>
<td>
<p>Select a subset of statistics calculated by  <code><a href="#topic+IH.summary">IH.summary</a></code>.Dafault All</p>
</td></tr>
<tr><td><code id="readss_+3A_reverse">reverse</code></td>
<td>
<p>If <code>reverse</code> is TRUE reverse rows and columns in output file. Default=FALSE</p>
</td></tr>
<tr><td><code id="readss_+3A_p">p</code></td>
<td>
<p> probability for Xp the 100p<em>th</em> percentile. Default is 0.95</p>
</td></tr>
<tr><td><code id="readss_+3A_gam">gam</code></td>
<td>
<p> one-sided confidence level <code class="reqn">\gamma</code>. Default is 0.95</p>
</td></tr>
<tr><td><code id="readss_+3A_comma">comma</code></td>
<td>
<p> if <code>TRUE</code>,the input file is in csv format with column names. Default is <code>FALSE</code> </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Read data from a tab or comma delimited text file in the
current folder/directory.  The first column must contain measurements
(observed X values). The second column is an indicator variable with 1 for a
detected value and 0 for a non-detect.  Additional columns can contain
factors that can be used to define a BY variable.  The first record
in the file must contain valid R names. Valid names may contain
letters (case sensitive), numbers, period, and underscore and should
start with a letter ( no spaces).  This file would most likely be
obtained from an Excel spread sheet using the file &quot;Save As&quot; option,
with file Save as type:<br /> Text(Tab delimited)(*.txt) or CSV(Comma
delimited)(*.csv).</p>


<h3>Value</h3>

<p>Returns invisible data.frame from file fn.txt
</p>
<table role = "presentation">
<tr><td><code>Column 1</code></td>
<td>
<p>value of measurement </p>
</td></tr>
<tr><td><code>Column 2</code></td>
<td>
<p>indicator variable ; 1 for detect 0 for non-detect</p>
</td></tr>
<tr><td><code>Column 3</code></td>
<td>
<p><code class="reqn">\ldots</code> additional variables</p>
</td></tr>
</table>


<h3>Side effects</h3>

<p>Summary statistics calculated by
<code><a href="#topic+IH.summary">IH.summary</a></code> are computed for each subset of data as
defined by the levels of the BY variable.  A data frame with row names
from IH.summary (or subset based on value of <code>pstat</code>) and column
names defined by the values of the BY variable is output as an ASCII
text file in CSV format fnout.csv in the working folder.  If
<code>reverse</code> is TRUE the rows and columns are reversed.</p>


<h3>Note</h3>

<p>For information about <code>factor</code> see R help file <code><a href="base.html#topic+factor">factor</a></code>
Each level of the BY variable must have at least two non-detects for this function.
If this is not the case an error message is printed to the R console and the
levels of the BY variable with less than 3 non-detects are printed.
</p>


<h3>Author(s)</h3>

<p>E. L. Frome </p>


<h3>References</h3>

<p> see the help file for <code><a href="#topic+lnorm.ml">lnorm.ml</a></code>, <code><a href="#topic+efclnp">efclnp</a></code>
<code><a href="#topic+efraction.ml">efraction.ml</a></code>, <code><a href="#topic+percentile.ml">percentile.ml</a></code>, <code><a href="#topic+kmms">kmms</a></code>  </p>


<h3>See Also</h3>

  <p><code><a href="#topic+About-STAND">About-STAND</a></code> for more details and a complete reference list</p>


<h3>Examples</h3>

<pre><code class='language-R'># to demonstrate the use of readss add a new factor grp to the cansdata
# this factor with four levels (A_1 A_2 B_1 B_2) combines strata and sample
data(cansdata)
grp &lt;- paste(cansdata$strata,cansdata$sample,sep="_")
temp &lt;- data.frame(cansdata,grp) # add four level factor grp to cansdata

#    the next line is NOT executable  use CUT AND PASTE
#    sink("demoread.txt") ; print(temp) ; sink()

# The preceding line writes temp to a text file demoread.txt in the current folder
# This file would normally be created by another program, e.g. Excel
#   now use readss() to read this space delimited text file and calculate
#   all of the summary statistics for each level of grp and output
#   the results to a new text file demoreadout.csv in the current folder

#     rdemo &lt;- readss("demoread",L=0.2,bcol=5)

#  rdemo is the R data frame that was used to calculate results in demoreadout.csv
#  to see same results rounded to three places in R console use
#  round( IH.summary(rdemo,L=0.2,bcol=5), 3)

#  To select a subset of statistics from IH.summary first define the subset
#  psel&lt;-c("Xp.obs","Xp","Xp.UCL","f","f.UCL","Rsq","m","n")
#  entering the following command will overwrite demoreadout.csv
#  with rows and columns reversed and the subset of statistics as columns
#  and the results will be rounded to 4 places
#  rdemo &lt;- readss("demoread",L=0.2,bcol=5,rto=4,pstat=psel,rev=TRUE)
#
#  to see same results rounded to three places in R console use
#  t(round( IH.summary(rdemo,L=0.2,bcol=5)[psel,], 3))
</code></pre>

<hr>
<h2 id='SESdata'>Samples from Elevated Surfaces of a Smelter </h2><span id='topic+SESdata'></span>

<h3>Description</h3>

<p>The Department of Energy (DOE) Chronic Beryllium
Disease Prevention Program is concerned with monitoring
objects (e.g., equipment, buildings) for beryllium contamination and
workers for exposure to beryllium in the workplace.  
</p>
<p>The SESdata is the results of a survey to evaluate possible beryllium
contamination based on 31 surface wipe samples from elevated surfaces (SES) 
of a smelter at a DOE facility. For equipment that is being evaluated for release to
the public, or for non beryllium use, the DOE has established a release
limit for removable beryllium contamination of <code class="reqn"> 0.2 \mu g/100cm^2</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(SESdata)</code></pre>


<h3>Format</h3>

<p>A data frame with 31 observations on the following 2 variables
</p>
   
<dl>
<dt>x</dt><dd><p> beryllium <code class="reqn">\mu g/100cm^2</code> </p>
</dd>
<dt>detect</dt><dd><p> 0 if non-detect; 1 if detected </p>
</dd>
</dl>



<h3>Details</h3>

<p>Statistics of interest are the exceedance fraction and 
the 95th percentile. The exceedance fraction is an estimate of the
percentage of surface area that is expected to exceed the release
limit Lp = 0.2 <code class="reqn">\mu g/100cm^2</code> with p = 0.95.  Both the point estimate and
the UCL for F exceed Fo = 100 (1-p) = 5%, indicating that the equipment
is not acceptable.  In fact, at the 95
confidence level at least 19.5% of the surface area exceeds the
release limit.  
</p>
<p>A more detailed description and analysis of this data is given as Example 1
in Section 4 of Frome and Wambach (2005)</p>


<h3>References</h3>

<p>Frome, E. L. and Wambach, P. F. (2005), &quot;Statistical Methods and Software for 
the Analysis of Occupational Exposure Data with Non-Detectable Values,&quot; 
<em>ORNL/TM-2005/52,Oak Ridge National Laboratory</em>, Oak Ridge, TN 37830.
Available at: <a href="http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf">http://www.csm.ornl.gov/esh/aoed/ORNLTM2005-52.pdf</a> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(SESdata)
mle.ses &lt;- unlist(lnorm.ml(SESdata)) # ML for SESdata
print(mle.ses[1:4])  #  ML estimates of parameters
print(mle.ses[5:8])  #  Standard errors of ML estimates
#  Next line produces a lognormal q-q plot with ML line 
qq.lnorm(plend(SESdata),mle.ses[1],mle.ses[2])
title("Lognormal Q-Q plot For SESdata  Example 1 in ORNLTM2005-52")
unlist(efraction.ml(SESdata,gam=0.95,L=0.2))   #  MLE of exceedance fraction and CLs
unlist(percentile.ml(SESdata,p=0.95,gam=0.95)) #  MLE of 95 percentile and CLs

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
