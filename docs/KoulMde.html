<!DOCTYPE html><html><head><title>Help for package KoulMde</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {KoulMde}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#CheckNonNumeric'><p>Detecting Non-numeric Values.</p></a></li>
<li><a href='#GenImg'><p>Generate black-and-white images</p></a></li>
<li><a href='#GetSegImage'><p>Perform image segmentation</p></a></li>
<li><a href='#Koul2StageMde'><p>Two-stage minimum distance estimation in linear regression model with autoregressive error.</p></a></li>
<li><a href='#KoulArMde'><p>Minimum distance estimation in the autoregression model of the known order.</p></a></li>
<li><a href='#KoulLrMde'><p>Minimum distance estimation in linear regression model.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Koul's Minimum Distance Estimation in Regression and Image
Segmentation Problems</td>
</tr>
<tr>
<td>Version:</td>
<td>3.2.1</td>
</tr>
<tr>
<td>Author:</td>
<td>Jiwoong Kim &lt;jwboys26 at gmail.com&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jiwoong Kim &lt;jwboys26@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Many methods are developed to deal with two major statistical problems: image segmentation 
    and nonparametric estimation in various regression models. Image segmentation is nowadays 
    gaining a lot of attention from various scientific subfields. Especially, image segmentation 
    has been popular in medical research such as magnetic resonance imaging (MRI) analysis. When 
    a patient suffers from some brain diseases such as dementia and Parkinson's disease, 
    those diseases can be easily diagnosed in brain MRI: the area affected by those diseases is 
    brightly expressed in MRI, which is called a white lesion. For the purpose of medical research,
    locating and segment those white lesions in MRI is a critical issue; it can be done
    manually. However, manual segmentation is very expensive in that it is error-prone and demands a huge
    amount of time. Therefore, supervised machine learning has emerged as an alternative solution. 
    Despite its powerful performance in a classification problem such as hand-written digits, supervised 
    machine learning has not shown the same satisfactory result in MRI analysis. Setting aside all issues
    of the supervised machine learning, it exposed a critical problem when employed for MRI analysis: it 
    requires time-consuming data labeling. Thus, there is a strong demand for an unsupervised approach, 
    and this package - based on Hira L. Koul (1986) &lt;<a href="https://doi.org/10.1214%2Faos%2F1176350059">doi:10.1214/aos/1176350059</a>&gt; - proposes an efficient
    method for simple image segmentation - here, "simple" means that an image is black-and-white - which 
    can easily be applied to MRI analysis. This package includes a function GetSegImage(): when a black-and-white
    image is given as an input, GetSegImage() separates an area of white pixels - which corresponds to 
    a white lesion in MRI - from the given image. For the second problem, consider linear regression model and autoregressive model of
    order q where errors in the linear regression model and innovations in the
    autoregression model are independent and symmetrically distributed. Hira L. Koul
    (1986) &lt;<a href="https://doi.org/10.1214%2Faos%2F1176350059">doi:10.1214/aos/1176350059</a>&gt; proposed a nonparametric minimum distance
    estimation method by minimizing L2-type distance between certain weighted
    residual empirical processes. He also proposed a simpler version of the loss
    function by using symmetry of the integrating measure in the distance. Kim
    (2018) &lt;<a href="https://doi.org/10.1080%2F00949655.2017.1392527">doi:10.1080/00949655.2017.1392527</a>&gt; proposed a fast computational method
    which enables practitioners to compute the minimum distance estimator of the vector
    of general multiple regression parameters for several integrating measures. This
    package contains three functions: KoulLrMde(), KoulArMde(), and Koul2StageMde().
    The former two provide minimum distance estimators for linear regression model
    and autoregression model, respectively, where both are based on Koul's method.
    These two functions take much less time for the computation than those based
    on parametric minimum distance estimation methods. Koul2StageMde() provides
    estimators for regression and autoregressive coefficients of linear regression
    model with autoregressive errors through minimum distant method of two stages.
    The new version is written in Rcpp and dramatically reduces computational time. </td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2.2)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.7), expm</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-09-09 21:03:30 UTC; Jason</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-09-10 06:30:26 UTC</td>
</tr>
</table>
<hr>
<h2 id='CheckNonNumeric'>Detecting Non-numeric Values.</h2><span id='topic+CheckNonNumeric'></span>

<h3>Description</h3>

<p>Check whether or not an input matrix includes any non-numeric values (NA, NULL, &quot;&quot;, character, etc) before being used for training. If any non-numeric values exist, then TrainBuddle() or FetchBuddle() will return non-numeric results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CheckNonNumeric(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CheckNonNumeric_+3A_x">X</code></td>
<td>
<p>an n-by-p matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of (n+1) values where n is the number of non-numeric values. The first element of the list is n, and all other elements are entries of X where non-numeric values occur. For example, when the (1,1)th and the (2,3)th entries of a 5-by-5 matrix X are non-numeric, then the list returned by CheckNonNumeric() will contain 2, (1,1), and (2,3).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n = 5;
p = 5;
X = matrix(0, n, p)       #### Generate a 5-by-5 matrix which includes two NA's.
X[1,1] = NA
X[2,3] = NA

lst = CheckNonNumeric(X)

lst

</code></pre>

<hr>
<h2 id='GenImg'>Generate black-and-white images</h2><span id='topic+GenImg'></span>

<h3>Description</h3>

<p>Create various images such as circle, rectangle and random dots.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GenImg(nx, ny, Type = 1, bNoise = FALSE, sig_noise = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GenImg_+3A_nx">nx</code></td>
<td>
<p>- Width of an image.</p>
</td></tr>
<tr><td><code id="GenImg_+3A_ny">ny</code></td>
<td>
<p>- Length of an image.</p>
</td></tr>
<tr><td><code id="GenImg_+3A_type">Type</code></td>
<td>
<p>- Type of an image: 1, 2, and 3 for rectangle, circle, and random dots, respectively.</p>
</td></tr>
<tr><td><code id="GenImg_+3A_bnoise">bNoise</code></td>
<td>
<p>- Option for including noise: TRUE or FALSE.</p>
</td></tr>
<tr><td><code id="GenImg_+3A_sig_noise">sig_noise</code></td>
<td>
<p>- Strength of noise: numeric value between 0 and 0.5.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of information of a generated image.
</p>

<ul>
<li><p> ImgMat - a matrix whose entries are pixel values of a generated image.
</p>
</li>
<li><p> S1 - an n1x2 matrix whose entries denote coordinates of white pixels of the image. n1 denotes the number of the white pixels.
</p>
</li>
<li><p> S2 - an n2x2 matrix whose entries denote coordinates of black pixels of the image. n2 denotes the number of the black pixels.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>

######## Generate a 10x10 black-and-white rectangle image with some noise
nx=10
ny=10
Type=1
bNoise=TRUE
sig_noise=0.1
lst = GenImg(nx,ny,Type, bNoise, sig_noise)
ImgMat = lst$ImgMat
image(ImgMat, axes = FALSE, col = grey(seq(0, 1, length = 256)))



</code></pre>

<hr>
<h2 id='GetSegImage'>Perform image segmentation</h2><span id='topic+GetSegImage'></span>

<h3>Description</h3>

<p>Seperate an area of white pixels from a given image when there is some noise.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GetSegImage(ImgMat, p1, p2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GetSegImage_+3A_imgmat">ImgMat</code></td>
<td>
<p>- a matrix whose entries are pixel values of the image.</p>
</td></tr>
<tr><td><code id="GetSegImage_+3A_p1">p1</code></td>
<td>
<p>- a known value of white pixel (usually 1).</p>
</td></tr>
<tr><td><code id="GetSegImage_+3A_p2">p2</code></td>
<td>
<p>- a known value of black pixel (usually 0).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of information of a segmented image.
</p>

<ul>
<li><p> SegImgMat - a matrix as a result of the image segmentation.
</p>
</li>
<li><p> Estimated_S1 - an n1x2 matrix whose entries denote estimated coordinates of white pixels, corresponding to p1.
</p>
</li>
<li><p> Estimated_S2 - an n2x2 matrix whose entries denote estimated coordinates of black pixels, corresponding to p2.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>

######## Generate a 10x10 black-and-white rectangle image with some noise
nx=10
ny=10
Type=1
bNoise=TRUE
sig_noise=0.1
lst = GenImg(nx,ny,Type, bNoise, sig_noise)
ImgMat = lst$ImgMat
image(ImgMat, axes = FALSE, col = grey(seq(0, 1, length = 256)))

######## Perform image segmentation
p1=1     ### value of a white pixel
p2=0     ### value of a black pixel


lst = GetSegImage(ImgMat, p1, p2)
EstImgMat = lst$SegImgMat
image(EstImgMat, axes = FALSE, col = grey(seq(0, 1, length = 256)))


</code></pre>

<hr>
<h2 id='Koul2StageMde'>Two-stage minimum distance estimation in linear regression model with autoregressive error.</h2><span id='topic+Koul2StageMde'></span>

<h3>Description</h3>

<p>Estimates both regression and autoregressive coefficients in the model <code class="reqn">Y=X\beta + \epsilon</code> where <code class="reqn">\epsilon</code> is autoregressive process of known order <code>q</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Koul2StageMde(
  Y,
  X,
  D,
  b0,
  RegIntMeasure,
  AR_Order,
  ArIntMeasure,
  TuningConst = 1.345
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Koul2StageMde_+3A_y">Y</code></td>
<td>
<p>- Vector of response variables in linear regression model.</p>
</td></tr>
<tr><td><code id="Koul2StageMde_+3A_x">X</code></td>
<td>
<p>- Design matrix of explanatory variables in linear regression model.</p>
</td></tr>
<tr><td><code id="Koul2StageMde_+3A_d">D</code></td>
<td>
<p>- Weight Matrix. Dimension of D should match that of X. Default value is XA where A=(X'X)^(-1/2).</p>
</td></tr>
<tr><td><code id="Koul2StageMde_+3A_b0">b0</code></td>
<td>
<p>- Initial value for beta.</p>
</td></tr>
<tr><td><code id="Koul2StageMde_+3A_regintmeasure">RegIntMeasure</code></td>
<td>
<p>- Symmetric and <code class="reqn">\sigma</code>-finite measure used for estimating <code class="reqn">\beta</code>: Lebesgue, Degenerate or Robust.</p>
</td></tr>
<tr><td><code id="Koul2StageMde_+3A_ar_order">AR_Order</code></td>
<td>
<p>- Order of the autoregressive error.</p>
</td></tr>
<tr><td><code id="Koul2StageMde_+3A_arintmeasure">ArIntMeasure</code></td>
<td>
<p>- Symmetric and <code class="reqn">\sigma</code>-finite measure used for estimating autoregressive coefficients of the error: Lebesgue, Degenerate or Robust.</p>
</td></tr>
<tr><td><code id="Koul2StageMde_+3A_tuningconst">TuningConst</code></td>
<td>
<p>- Used only for Robust measure.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>MDE1stage - The list of the first stage minimum distance estimation result. It contains betahat1stage, residual1stage, and rho1stage.
</p>

<ul>
<li><p> betahat1stage - The first stage minimum distance estimators of regression coefficients.
</p>
</li>
<li><p> residual1stage - Residuals after the first stage minimum distance estimation.
</p>
</li>
<li><p> rho1stage - The first stage minimum distance estimators of autoregressive coefficients of the error.
</p>
</li></ul>

<p>MDE2stage - The list of the second stage minimum distance estimation result. It contains betahat2stage, residual2stage, and rho2stage.
</p>

<ul>
<li><p> betahat2stage - The second stage minimum distance estimators of regression coefficients.
</p>
</li>
<li><p> residual2stage - Residuals after the second stage minimum distance estimation.
</p>
</li>
<li><p> rho2stage - The second stage minimum distance estimators of autoregressive coefficients of the error.
</p>
</li></ul>



<h3>References</h3>

<p>[1] Kim, J. (2018). A fast algorithm for the coordinate-wise minimum distance estimation. J. Stat. Comput. Simul., 3: 482 - 497
</p>
<p>[2] Kim, J. (2020). Minimum distance estimation in linear regression model with strong mixing errors. Commun. Stat. - Theory Methods., 49(6): 1475 - 1494
</p>
<p>[3] Koul, H. L (1985). Minimum distance estimation in linear regression with unknown error distributions. Statist. Probab. Lett., 3: 1-8.
</p>
<p>[4] Koul, H. L (1986). Minimum distance estimation and goodness-of-fit tests in first-order autoregression. Ann. Statist., 14 1194-1213.
</p>
<p>[5] Koul, H. L (2002). Weighted empirical process in nonlinear dynamic models. Springer, Berlin, Vol. 166
</p>


<h3>See Also</h3>

<p>KoulArMde() and KoulLrMde()
</p>


<h3>Examples</h3>

<pre><code class='language-R'>####################
n &lt;- 10
p &lt;- 3
X &lt;- matrix(runif(n*p, 0,50), nrow=n, ncol=p)  #### Generate n-by-p design matrix X
beta &lt;- c(-2, 0.3, 1.5)                        #### Generate true beta = (-2, 0.3, 1.5)'
rho  &lt;- 0.4                                    #### True rho = 0.4
eps &lt;- vector(length=n)
xi &lt;- rnorm(n, 0,1)                            #### Generate innovation from N(0,1)
                                              #### Generate autoregressive process of order 1
for(i in 1:n){
 if(i==1){eps[i] &lt;- xi[i]}
 else{eps[i] &lt;- rho*eps[i-1] + xi[i]}
}
Y &lt;- X%*%beta + eps
#####################
D &lt;- "default"                                  #### Use the default weight matrix
b0 &lt;- solve(t(X)%*%X)%*%(t(X)%*%Y)              #### Set initial value for beta

IntMeasure &lt;- "Lebesgue"                                ##### Define Lebesgue measure
MDEResult &lt;- Koul2StageMde(Y,X, "default", b0, IntMeasure, 1, IntMeasure, TuningConst = 1.345)
MDE1stageResult &lt;- MDEResult[[1]]
MDE2stageResult &lt;- MDEResult[[2]]

beta1 &lt;- MDE1stageResult$betahat1stage
residual1 &lt;- MDE1stageResult$residual1stage
rho1 &lt;- MDE1stageResult$rhohat1stage

beta2 &lt;- MDE2stageResult$betahat2stage
residual2 &lt;- MDE1stageResult$residual2stage
rho2 &lt;- MDE2stageResult$rhohat2stage
</code></pre>

<hr>
<h2 id='KoulArMde'>Minimum distance estimation in the autoregression model of the known order.</h2><span id='topic+KoulArMde'></span>

<h3>Description</h3>

<p>Estimates the autoressive coefficients in the <code class="reqn">X_t = \rho' Z_t + \xi_t </code> where <code class="reqn">Z_t</code> is the vector of <code class="reqn">q</code> observations at times <code class="reqn">t-1,...,t-q</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KoulArMde(X, AR_Order, IntMeasure, TuningConst = 1.345)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KoulArMde_+3A_x">X</code></td>
<td>
<p>- Vector of <code>n</code> observed values.</p>
</td></tr>
<tr><td><code id="KoulArMde_+3A_ar_order">AR_Order</code></td>
<td>
<p>- Order of the autoregression model.</p>
</td></tr>
<tr><td><code id="KoulArMde_+3A_intmeasure">IntMeasure</code></td>
<td>
<p>- Symmetric and <code class="reqn">\sigma</code>-finite measure: Lebesgue, Degenerate, and Robust</p>
</td></tr>
<tr><td><code id="KoulArMde_+3A_tuningconst">TuningConst</code></td>
<td>
<p>- Used only for Robust measure.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>rhohat     - Minimum distance estimator of <code class="reqn">\rho</code>.
</p>
<p>residual   - Residuals after minimum distance estimation.
</p>
<p>ObjVal     - Value of the objective function at minimum distance estimator.
</p>


<h3>References</h3>

<p>[1] Kim, J. (2018). A fast algorithm for the coordinate-wise minimum distance estimation. J. Stat. Comput. Simul., 3: 482 - 497
</p>
<p>[2] Kim, J. (2020). Minimum distance estimation in linear regression model with strong mixing errors. Commun. Stat. - Theory Methods., 49(6): 1475 - 1494
</p>
<p>[3] Koul, H. L (1985). Minimum distance estimation in linear regression with unknown error distributions. Statist. Probab. Lett., 3: 1-8.
</p>
<p>[4] Koul, H. L (1986). Minimum distance estimation and goodness-of-fit tests in first-order autoregression. Ann. Statist., 14 1194-1213.
</p>
<p>[5] Koul, H. L (2002). Weighted empirical process in nonlinear dynamic models. Springer, Berlin, Vol. 166
</p>


<h3>See Also</h3>

<p>KoulLrMde() and Koul2StageMde()
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##### Generate stationary AR(2) process with 10 observations
n &lt;- 10
q &lt;- 2
rho &lt;- c(-0.2, 0.8)    ##### Generate true parameters rho = (-0.2, 0.8)'
eps &lt;- rnorm(n, 0,1)   ##### Generate innovations from N(0,1)
X &lt;- rep(0, times=n)
for (i in 1:n){
 tempCol &lt;- rep(0, times=q)
 for (j in 1:q){
   if(i-j&lt;=0){
     tempCol[j] &lt;- 0
   }else{
     tempCol[j] &lt;- X[i-j]
   }
 }
X[i] &lt;- t(tempCol)%*% rho + eps[i]
}

IntMeasure &lt;- "Lebesgue"                       ##### Define Lebesgue measure

MDEResult &lt;- KoulArMde(X, q, IntMeasure, TuningConst=1.345)
rhohat &lt;- MDEResult$rhohat                     ##### Obtain minimum distance estimator
resid  &lt;- MDEResult$residual                   ##### Obtain residual
objVal &lt;- MDEResult$ObjVal                     ##### Obtain the value of the objective function


IntMeasure &lt;- "Degenerate"                     ##### Define degenerate measure at 0
MDEResult &lt;- KoulArMde(X, q, IntMeasure, TuningConst=1.345)
rhohat &lt;- MDEResult$rhohat                     ##### Obtain minimum distance estimator
resid &lt;- MDEResult$residual                    ##### Obtain residual
objVal &lt;- MDEResult$ObjVal                     ##### Obtain the value of the objective function


IntMeasure &lt;- "Robust"                         ##### Define "Robust" measure at 0
TuningConst &lt;- 3                               ##### Define the tuning constant
MDEResult &lt;- KoulArMde(X, q, IntMeasure, TuningConst)

resid &lt;- MDEResult$residual                    ##### Obtain residual
objVal &lt;- MDEResult$ObjVal                     ##### Obtain the value of the objective function

</code></pre>

<hr>
<h2 id='KoulLrMde'>Minimum distance estimation in linear regression model.</h2><span id='topic+KoulLrMde'></span>

<h3>Description</h3>

<p>Estimates the regression coefficients in the model <code class="reqn">Y=X\beta + \epsilon</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KoulLrMde(Y, X, D, b0, IntMeasure, TuningConst = 1.345)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KoulLrMde_+3A_y">Y</code></td>
<td>
<p>- Vector of response variables in linear regression model.</p>
</td></tr>
<tr><td><code id="KoulLrMde_+3A_x">X</code></td>
<td>
<p>- Design matrix of explanatory variables in linear regression model.</p>
</td></tr>
<tr><td><code id="KoulLrMde_+3A_d">D</code></td>
<td>
<p>- Weight Matrix. Dimension of D should match that of X. Default value is XA where A=(X'X)^(-1/2).</p>
</td></tr>
<tr><td><code id="KoulLrMde_+3A_b0">b0</code></td>
<td>
<p>- Initial value for beta.</p>
</td></tr>
<tr><td><code id="KoulLrMde_+3A_intmeasure">IntMeasure</code></td>
<td>
<p>- Symmetric and <code class="reqn">\sigma</code>-finite measure: Lebesgue, Degenerate, and Robust</p>
</td></tr>
<tr><td><code id="KoulLrMde_+3A_tuningconst">TuningConst</code></td>
<td>
<p>- Used only for Robust measure.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>betahat    - Minimum distance estimator of <code class="reqn">\beta</code>.
</p>
<p>residual   - Residuals after minimum distance estimation.
</p>
<p>ObjVal     - Value of the objective function at minimum distance estimator.
</p>


<h3>References</h3>

<p>[1] Kim, J. (2018). A fast algorithm for the coordinate-wise minimum distance estimation. J. Stat. Comput. Simul., 3: 482 - 497
</p>
<p>[2] Kim, J. (2020). Minimum distance estimation in linear regression model with strong mixing errors. Commun. Stat. - Theory Methods., 49(6): 1475 - 1494
</p>
<p>[3] Koul, H. L (1985). Minimum distance estimation in linear regression with unknown error distributions. Statist. Probab. Lett., 3: 1-8.
</p>
<p>[4] Koul, H. L (1986). Minimum distance estimation and goodness-of-fit tests in first-order autoregression. Ann. Statist., 14 1194-1213.
</p>
<p>[5] Koul, H. L (2002). Weighted empirical process in nonlinear dynamic models. Springer, Berlin, Vol. 166
</p>


<h3>See Also</h3>

<p>KoulArMde() and Koul2StageMde()
</p>


<h3>Examples</h3>

<pre><code class='language-R'>####################
n &lt;- 10
p &lt;- 3
X &lt;- matrix(runif(n*p, 0,50), nrow=n, ncol=p)  #### Generate n-by-p design matrix X
beta &lt;- c(-2, 0.3, 1.5)                        #### Generate true beta = (-2, 0.3, 1.5)'
eps &lt;- rnorm(n, 0,1)                           #### Generate errors from N(0,1)
Y &lt;- X%*%beta + eps

D &lt;- "default"                                 #### Use the default weight matrix
b0 &lt;- solve(t(X)%*%X)%*%(t(X)%*%Y)             #### Set initial value for beta
IntMeasure &lt;- "Lebesgue"                       ##### Define Lebesgue measure


MDEResult &lt;- KoulLrMde(Y,X,D, b0, IntMeasure, TuningConst=1.345)

betahat &lt;- MDEResult$betahat                   ##### Obtain minimum distance estimator
resid &lt;- MDEResult$residual                    ##### Obtain residual
objVal &lt;- MDEResult$ObjVal                     ##### Obtain the value of the objective function


IntMeasure &lt;- "Degenerate"                     ##### Define degenerate measure at 0

MDEResult &lt;- KoulLrMde(Y,X,D, b0, IntMeasure, TuningConst=1.345)
betahat &lt;- MDEResult$betahat                   ##### Obtain minimum distance estimator
resid &lt;- MDEResult$residual                    ##### Obtain residual
objVal &lt;- MDEResult$ObjVal                     ##### Obtain the value of the objective function



IntMeasure &lt;- "Robust"                        ##### Define "Robust" measure
TuningConst &lt;- 3                              ##### Define the tuning constant
MDEResult &lt;- KoulLrMde(Y,X,D, b0, IntMeasure, TuningConst)


betahat &lt;- MDEResult$betahat                   ##### Obtain minimum distance estimator
resid &lt;- MDEResult$residual                    ##### Obtain residual
objVal &lt;- MDEResult$ObjVal                     ##### Obtain the value of the objective function
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
