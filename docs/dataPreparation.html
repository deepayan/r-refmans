<!DOCTYPE html><html><head><title>Help for package dataPreparation</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {dataPreparation}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#adult'><p>Adult for UCI repository</p></a></li>
<li><a href='#aggregate_by_key'><p>Automatic data_set aggregation by key</p></a></li>
<li><a href='#as.POSIXct_fast'><p>Faster date transformation</p></a></li>
<li><a href='#build_bins'><p>Compute bins</p></a></li>
<li><a href='#build_date_factor'><p>Date Factor</p></a></li>
<li><a href='#build_encoding'><p>Compute encoding</p></a></li>
<li><a href='#build_scales'><p>Compute scales</p></a></li>
<li><a href='#build_target_encoding'><p>Build target encoding</p></a></li>
<li><a href='#compute_probability_ratio'><p>Compute probability ratio</p></a></li>
<li><a href='#compute_weight_of_evidence'><p>Compute weight of evidence</p></a></li>
<li><a href='#data_preparation_news'><p>Show the NEWS file</p></a></li>
<li><a href='#date_format_unifier'><p>Unify dates format</p></a></li>
<li><a href='#description'><p>Describe data set</p></a></li>
<li><a href='#fast_discretization'><p>Discretization</p></a></li>
<li><a href='#fast_filter_variables'><p>Filtering useless variables</p></a></li>
<li><a href='#fast_handle_na'><p>Handle NA values</p></a></li>
<li><a href='#fast_is_equal'><p>Fast checks of equality</p></a></li>
<li><a href='#fast_round'><p>Fast round</p></a></li>
<li><a href='#fast_scale'><p>scale</p></a></li>
<li><a href='#find_and_transform_dates'><p>Identify date columns</p></a></li>
<li><a href='#find_and_transform_numerics'><p>Identify numeric columns in a data_set set</p></a></li>
<li><a href='#generate_date_diffs'><p>Date difference</p></a></li>
<li><a href='#generate_factor_from_date'><p>Generate factor from dates</p></a></li>
<li><a href='#generate_from_character'><p>Recode character</p></a></li>
<li><a href='#generate_from_factor'><p>Recode factor</p></a></li>
<li><a href='#get_most_frequent_element'><p>Get most frequent element</p></a></li>
<li><a href='#identify_dates'><p>Identify date columns</p></a></li>
<li><a href='#messy_adult'><p>Adult with some ugly columns added</p></a></li>
<li><a href='#one_hot_encoder'><p>One hot encoder</p></a></li>
<li><a href='#prepare_set'><p>Preparation pipeline</p></a></li>
<li><a href='#remove_percentile_outlier'><p>Percentile outlier filtering</p></a></li>
<li><a href='#remove_rare_categorical'><p>Filter rare categories</p></a></li>
<li><a href='#remove_sd_outlier'><p>Standard deviation outlier filtering</p></a></li>
<li><a href='#same_shape'><p>Give same shape</p></a></li>
<li><a href='#set_as_numeric_matrix'><p>Numeric matrix preparation for Machine Learning.</p></a></li>
<li><a href='#set_col_as_character'><p>Set columns as character</p></a></li>
<li><a href='#set_col_as_date'><p>Set columns as POSIXct</p></a></li>
<li><a href='#set_col_as_factor'><p>Set columns as factor</p></a></li>
<li><a href='#set_col_as_numeric'><p>Set columns as numeric</p></a></li>
<li><a href='#shape_set'><p>Final preparation before ML algorithm</p></a></li>
<li><a href='#target_encode'><p>Target encode</p></a></li>
<li><a href='#tiny_messy_adult'><p>First 500 rows of <code>messy_adult</code></p></a></li>
<li><a href='#un_factor'><p>Unfactor factor with too many values</p></a></li>
<li><a href='#which_are_bijection'><p>Identify bijections</p></a></li>
<li><a href='#which_are_constant'><p>Identify constant columns</p></a></li>
<li><a href='#which_are_in_double'><p>Identify double columns</p></a></li>
<li><a href='#which_are_included'><p>Identify columns that are included in others</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Automated Data Preparation</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Do most of the painful data preparation for a data science project with a minimum amount of code; Take advantages of 'data.table' efficiency and use some algorithmic trick in order to perform data preparation in a time and RAM efficient way.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6.0),</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> | file LICENSE</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 2.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>data.table, lubridate, stringr, Matrix, progress</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ELToulemonde/dataPreparation/issues">https://github.com/ELToulemonde/dataPreparation/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-07-04 12:53:17 UTC; eltoulemonde</td>
</tr>
<tr>
<td>Author:</td>
<td>Emmanuel-Lin Toulemonde [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Emmanuel-Lin Toulemonde &lt;el.toulemonde@protonmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-07-04 13:13:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='adult'>Adult for UCI repository</h2><span id='topic+adult'></span>

<h3>Description</h3>

<p>For examples and tutorials, and in order to build <code>messy_adult</code>, UCI adult data set is used. <br />
Data Set Information: <br />
<br />
Extraction was done by Barry Becker from the 1994 Census database. A set of reasonably clean records was
extracted using the following conditions: ((AAGE&gt;16) &amp;&amp; (AGI&gt;100) &amp;&amp; (AFNLWGT&gt;1)&amp;&amp; (HRSWK&gt;0))<br />
<br />
Prediction task is to determine whether a person makes over 50K a year.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("adult")
</code></pre>


<h3>Format</h3>

<p>A data.frame with 32561 rows and 15 variables.
</p>


<h3>References</h3>

<p><a href="https://archive.ics.uci.edu/ml/datasets/adult">https://archive.ics.uci.edu/ml/datasets/adult</a>
</p>

<hr>
<h2 id='aggregate_by_key'>Automatic data_set aggregation by key</h2><span id='topic+aggregate_by_key'></span>

<h3>Description</h3>

<p>Automatic aggregation of a data_set set according to a <code>key</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aggregate_by_key(data_set, key, verbose = TRUE, thresh = 53, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aggregate_by_key_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table (with only numeric, integer, factor, logical, character columns)</p>
</td></tr>
<tr><td><code id="aggregate_by_key_+3A_key">key</code></td>
<td>
<p>Name of a column of data_set according to which the set should be aggregated (character)</p>
</td></tr>
<tr><td><code id="aggregate_by_key_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk? (logical, default to TRUE)</p>
</td></tr>
<tr><td><code id="aggregate_by_key_+3A_thresh">thresh</code></td>
<td>
<p>Number of max values for frequencies count (numerical, default to 53)</p>
</td></tr>
<tr><td><code id="aggregate_by_key_+3A_...">...</code></td>
<td>
<p>Optional argument: <code>functions</code>:  aggregation functions for numeric columns
(vector of function names (character), optional, if not set we use: c(&quot;mean&quot;, &quot;min&quot;, &quot;max&quot;, &quot;sd&quot;))</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Perform aggregation depending on column type:
</p>

<ul>
<li><p> If column is numeric <code>functions</code> are performed on the column. So 1 numeric column
give length(functions) new columns,
</p>
</li>
<li><p> If column is character or factor and have less than <code>thresh</code> different values,
frequency count of values is performed,
</p>
</li>
<li><p> If column is character or factor with more than <code>thresh</code> different values, number
of different values for each <code>key</code> is performed,
</p>
</li>
<li><p> If column is logical, number of TRUE is computed.
</p>
</li></ul>

<p>In all cases, if the set as more rows than unique <code>key</code>, a number of lines will be computed.
</p>
<p>Be careful using functions argument, given functions should be an aggregation function,
meaning that for multiple values it should only return one value.
</p>


<h3>Value</h3>

<p>A <code><a href="data.table.html#topic+data.table">data.table</a></code> with one line per <code>key</code> elements and multiple  new columns.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Get generic dataset from R
data("adult")

# Aggregate it using aggregate_by_key, in order to extract characteristics for each country
adult_aggregated &lt;- aggregate_by_key(adult, key = 'country')

# Example with other functions
power &lt;- function(x) {sum(x^2)}
adult_aggregated &lt;- aggregate_by_key(adult, key = 'country', functions = c("power", "sqrt"))

# sqrt is not an aggregation function, so it wasn't used.

## End(Not run)
# "##NOT RUN:" mean that this example hasn't been run on CRAN since its long. But you can run it!
</code></pre>

<hr>
<h2 id='as.POSIXct_fast'>Faster date transformation</h2><span id='topic+as.POSIXct_fast'></span>

<h3>Description</h3>

<p>Based on the trick that often dates are repeated in a column, we make date transformation
faster by computing date transformation only on uniques.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.POSIXct_fast(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.POSIXct_fast_+3A_x">x</code></td>
<td>
<p>An object to be converted</p>
</td></tr>
<tr><td><code id="as.POSIXct_fast_+3A_...">...</code></td>
<td>
<p>other argument to pass to  <code><a href="base.html#topic+as.POSIXct">as.POSIXct</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The more
</p>


<h3>Value</h3>

<p>as.POSIXct and as.POSIXlt return an object of the appropriate class. If tz was specified,
as.POSIXlt will give an appropriate &quot;tzone&quot; attribute. Date-times known to be invalid will be returned as NA.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Work the same as as.POSIXct
as.POSIXct_fast("2018-01-01", format="%Y-%m-%d")
</code></pre>

<hr>
<h2 id='build_bins'>Compute bins</h2><span id='topic+build_bins'></span>

<h3>Description</h3>

<p>Compute bins for discretization of numeric variable (either equal_width or equal_fred).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>build_bins(
  data_set,
  cols = "auto",
  n_bins = 10,
  type = "equal_width",
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="build_bins_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="build_bins_+3A_cols">cols</code></td>
<td>
<p>List of numeric column(s) name(s) of data_set to transform. To transform all
characters, set it to &quot;auto&quot;.  (character, default to &quot;auto&quot;)</p>
</td></tr>
<tr><td><code id="build_bins_+3A_n_bins">n_bins</code></td>
<td>
<p>Number of group to compute (numeric, default to 10)</p>
</td></tr>
<tr><td><code id="build_bins_+3A_type">type</code></td>
<td>
<p>Type of discretization (&quot;equal_width&quot; or &quot;equal_freq&quot;)</p>
</td></tr>
<tr><td><code id="build_bins_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk? (Logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Using equal freq first bin will start at -Inf and last bin will end at +Inf.
</p>


<h3>Value</h3>

<p>A list where each element name is a column name of data set and each element contains
bins to discretize this column.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load data
data(tiny_messy_adult)
head(tiny_messy_adult)

# Compute bins
bins &lt;- build_bins(tiny_messy_adult, cols = "auto", n_bins = 5, type = "equal_freq")
print(bins)
</code></pre>

<hr>
<h2 id='build_date_factor'>Date Factor</h2><span id='topic+build_date_factor'></span>

<h3>Description</h3>

<p>Map a vector of dates to a factor at one of these levels &quot;yearmonth&quot;, &quot;yearquarter&quot;, &quot;quarter&quot;, &quot;month&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>build_date_factor(data_set, type = "yearmonth")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="build_date_factor_+3A_data_set">data_set</code></td>
<td>
<p>A vector of date values</p>
</td></tr>
<tr><td><code id="build_date_factor_+3A_type">type</code></td>
<td>
<p>One of &quot;year&quot;, &quot;yearquarter&quot;, &quot;yearmonth&quot;, &quot;quarter&quot;, &quot;month&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The resulting vector is an ordered factor of the specified <code>type</code> (e.g. yearmonth)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(data.table)
data_set &lt;- as.Date(c("2014-01-01", "2015-01-01", "2015-06-01"))
build_date_factor(data_set, type = "yearmonth")
build_date_factor(data_set, type = "yearquarter")
build_date_factor(data_set, type = "yearquarter")

</code></pre>

<hr>
<h2 id='build_encoding'>Compute encoding</h2><span id='topic+build_encoding'></span>

<h3>Description</h3>

<p>Build a list of one hot encoding for each <code>cols</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>build_encoding(data_set, cols = "auto", verbose = TRUE, min_frequency = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="build_encoding_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="build_encoding_+3A_cols">cols</code></td>
<td>
<p>List of numeric column(s) name(s) of data_set to transform. To transform all
characters, set it to &quot;auto&quot;. (character, default to &quot;auto&quot;)</p>
</td></tr>
<tr><td><code id="build_encoding_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk? (Logical, default to TRUE)</p>
</td></tr>
<tr><td><code id="build_encoding_+3A_min_frequency">min_frequency</code></td>
<td>
<p>The minimal share of lines that a category should represent (numeric,
between 0 and 1, default to 0)</p>
</td></tr>
<tr><td><code id="build_encoding_+3A_...">...</code></td>
<td>
<p>Other arguments such as <code>name_separator</code> to separate words in new columns names
(character, default to &quot;.&quot;)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To avoid creating really large sparce matrices, one can use  param <code>min_frequency</code> to be
sure that only most representative values will be used to create a new column (and not
out-layers or mistakes in data). <br />
Setting <code>min_frequency</code> to something greater than 0 may cause the function to be slower
(especially for large data_set).
</p>


<h3>Value</h3>

<p>A list where each element name is a column name of data set and each element new_cols
and values the new columns that will be built during encoding.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Get a data set
data(adult)
encoding &lt;- build_encoding(adult, cols = "auto", verbose = TRUE)

print(encoding)

# To limit the number of generated columns, one can use min_frequency parameter:
build_encoding(adult, cols = "auto", verbose = TRUE, min_frequency = 0.1)
# Set to 0.1, it will create columns only for values that are present 10% of the time.
</code></pre>

<hr>
<h2 id='build_scales'>Compute scales</h2><span id='topic+build_scales'></span>

<h3>Description</h3>

<p>Build a list of means and standard deviation for each <code>cols</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>build_scales(data_set, cols = "auto", verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="build_scales_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="build_scales_+3A_cols">cols</code></td>
<td>
<p>List of numeric column(s) name(s) of data_set to transform. To transform all
characters, set it to &quot;auto&quot;. (character, default to &quot;auto&quot;)</p>
</td></tr>
<tr><td><code id="build_scales_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk? (Logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list where each element name is a column name of data set and each element contains means and sd.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Get a data set
data(adult)
scales &lt;- build_scales(adult, cols = "auto", verbose = TRUE)

print(scales)
</code></pre>

<hr>
<h2 id='build_target_encoding'>Build target encoding</h2><span id='topic+build_target_encoding'></span>

<h3>Description</h3>

<p>Target encoding is the process of replacing a categorical value with the aggregation of the
target variable. <code>build_target_encoding</code> is used to compute aggregations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>build_target_encoding(
  data_set,
  cols_to_encode,
  target_col,
  functions = "mean",
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="build_target_encoding_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="build_target_encoding_+3A_cols_to_encode">cols_to_encode</code></td>
<td>
<p>columns to aggregate according to (list)</p>
</td></tr>
<tr><td><code id="build_target_encoding_+3A_target_col">target_col</code></td>
<td>
<p>column to aggregate (character)</p>
</td></tr>
<tr><td><code id="build_target_encoding_+3A_functions">functions</code></td>
<td>
<p>functions of aggregation (list or character, default to &quot;mean&quot;). Functions <code>compute_probability_ratio</code>
and <code>compute_weight_of_evidence</code> are classically used functions</p>
</td></tr>
<tr><td><code id="build_target_encoding_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk? (Logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>list</code> of <code><a href="data.table.html#topic+data.table">data.table</a></code> a data.table for each <code>cols_to_encode</code>
each data.table containing a line by unique value of column and <code>len(functions) + 1</code> columns.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Build a data set
require(data.table)
data_set &lt;- data.table(student = c("Marie", "Marie", "Pierre", "Louis", "Louis"),
                      grades = c(1, 1, 2, 3, 4))

# Perform target_encoding construction
build_target_encoding(data_set, cols_to_encode = "student", target_col = "grades",
                      functions = c("mean", "sum"))
</code></pre>

<hr>
<h2 id='compute_probability_ratio'>Compute probability ratio</h2><span id='topic+compute_probability_ratio'></span>

<h3>Description</h3>

<p>Probability ratio is an aggregation function that can be used for 
<code>build_target_encoding</code>. Probability ratio is the P(most freq element) / (1 - P(most frq element)).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_probability_ratio(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_probability_ratio_+3A_x">x</code></td>
<td>
<p>A <code>list</code> of categorical elements</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To be more generic, the library compute P(most freq element) inplace of traditional formula P(1)/P(0)
</p>


<h3>Value</h3>

<p>P(most freq element) / (1 - P(most frq element))
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Build example list
example_list &lt;- c(1, 1, 1, 2, 2, 3)

# Compute probability ratio
compute_probability_ratio(example_list)
</code></pre>

<hr>
<h2 id='compute_weight_of_evidence'>Compute weight of evidence</h2><span id='topic+compute_weight_of_evidence'></span>

<h3>Description</h3>

<p>Weight of evidence is an aggregation function that can be used for 
<code>build_target_encoding</code>. Weight of evidence is the ln(P(most freq element) / (1 - P(most frq element))).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_weight_of_evidence(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_weight_of_evidence_+3A_x">x</code></td>
<td>
<p>A <code>list</code> of categorical elements</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To be more generic, the library compute P(most freq element) inplace of traditional formula ln(P(1)/P(0))
</p>


<h3>Value</h3>

<p>Weight of evidence
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Build example list
example_list &lt;- c(1, 1, 1, 2, 2, 3)

# Compute weight of evidence
compute_weight_of_evidence(example_list)
</code></pre>

<hr>
<h2 id='data_preparation_news'>Show the NEWS file</h2><span id='topic+data_preparation_news'></span>

<h3>Description</h3>

<p>Show the NEWS file of the dataPreparation package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_preparation_news()
</code></pre>

<hr>
<h2 id='date_format_unifier'>Unify dates format</h2><span id='topic+date_format_unifier'></span>

<h3>Description</h3>

<p>Unify every column in a date format to the same date format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>date_format_unifier(data_set, format = "Date")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="date_format_unifier_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="date_format_unifier_+3A_format">format</code></td>
<td>
<p>Desired target format: Date, POSIXct or POSIXlt, (character, default to Date)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function only handle Date, POSIXct and POSIXlt dates.  <br />
POSIXct format is a bit slower than Date but can keep hours-min.
</p>


<h3>Value</h3>

<p>The same data_set set but with dates column with the desired format.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># build a data.table
require(data.table)
data_set &lt;- data.table( column1 = as.Date("2016-01-01"), column2 = as.POSIXct("2017-01-01") )

# Use the function
data_set = date_format_unifier(data_set, format = "Date")

# Control result
sapply(data_set, class)
# return Date for both columns
</code></pre>

<hr>
<h2 id='description'>Describe data set</h2><span id='topic+description'></span>

<h3>Description</h3>

<p>Generate extensive description of a data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>description(data_set, level = 1, path_to_write = NULL, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="description_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="description_+3A_level">level</code></td>
<td>
<p>Level of description (0: generic, 1: column by column)
(numeric, default to 1)</p>
</td></tr>
<tr><td><code id="description_+3A_path_to_write">path_to_write</code></td>
<td>
<p>Path where the report should be written (character, default to NULL)</p>
</td></tr>
<tr><td><code id="description_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk? (Logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># Load exemple set
data(tiny_messy_adult)

# Describe it
description(tiny_messy_adult)
</code></pre>

<hr>
<h2 id='fast_discretization'>Discretization</h2><span id='topic+fast_discretization'></span>

<h3>Description</h3>

<p>Discretization of numeric variable (either equal_width or equal_fred).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fast_discretization(data_set, bins = NULL, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fast_discretization_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="fast_discretization_+3A_bins">bins</code></td>
<td>
<p>Result of function <code><a href="#topic+build_bins">build_bins</a></code>, (list, default to NULL). <br />
To perform the same discretization on train and test, it is recommended to compute <code><a href="#topic+build_bins">build_bins</a></code>
before. If it is kept to NULL, build_bins will be called.<br />
<code>bins</code> could also be carefully hand written.</p>
</td></tr>
<tr><td><code id="fast_discretization_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk? (Logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>NAs will be putted in an NA category.
</p>


<h3>Value</h3>

<p>Same dataset discretized by <strong>reference</strong>. <br />
If you don't want to edit by reference please provide set <code>data_set = copy(data_set)</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load data
data(tiny_messy_adult)
head(tiny_messy_adult)

# Compute bins
bins &lt;- build_bins(tiny_messy_adult, cols = "auto", n_bins = 5, type = "equal_freq")

# Discretize
tiny_messy_adult &lt;- fast_discretization(tiny_messy_adult, bins = bins)

# Control
head(tiny_messy_adult)

# Example with hand written bins
data("adult")
adult &lt;-  fast_discretization(adult, bins = list(age = c(0, 40, +Inf)))
print(table(adult$age))
</code></pre>

<hr>
<h2 id='fast_filter_variables'>Filtering useless variables</h2><span id='topic+fast_filter_variables'></span>

<h3>Description</h3>

<p>Delete columns that are constant or in double in your data_set set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fast_filter_variables(
  data_set,
  level = 3,
  keep_cols = NULL,
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fast_filter_variables_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="fast_filter_variables_+3A_level">level</code></td>
<td>
<p>which columns do you want to filter (1 = constant, 2 = constant and doubles,
3 = constant doubles and bijections, 4 = constant doubles bijections and included)(numeric, default to 3)</p>
</td></tr>
<tr><td><code id="fast_filter_variables_+3A_keep_cols">keep_cols</code></td>
<td>
<p>List of columns not to drop (list of character, default to NULL)</p>
</td></tr>
<tr><td><code id="fast_filter_variables_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk (logical or 1 or 2, default to TRUE)</p>
</td></tr>
<tr><td><code id="fast_filter_variables_+3A_...">...</code></td>
<td>
<p>optional parameters to be passed to the function when called from another function</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>verbose</code> can be set to 2 have full details from which functions, otherwise they
don't log. (<code>verbose = 1</code> is equivalent to <code>verbose = TRUE</code>).
</p>


<h3>Value</h3>

<p>The same data_set but with fewer columns. Columns that are constant, in double,
or bijection of another have been deleted.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># First let's build a data.frame with 3 columns: a constant column, and a column in double
## Not run: 
df &lt;- data.frame(col1 = 1, col2 = rnorm(1e6), col3 = sample(c(1, 2), 1e6, replace = TRUE))
df$col4 &lt;- df$col2
df$col5[df$col3 == 1] = "a"
df$col5[df$col3 == 2] = "b" # Same info than in col1 but with a for 1 and b for 2
head(df)

# Let's filter columns:
df &lt;- fast_filter_variables(df)
head(df)

## End(Not run)
# Don't run for CRAN, you can run example
</code></pre>

<hr>
<h2 id='fast_handle_na'>Handle NA values</h2><span id='topic+fast_handle_na'></span>

<h3>Description</h3>

<p>Handle NAs values depending on the class of the column.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fast_handle_na(
  data_set,
  set_num = 0,
  set_logical = FALSE,
  set_char = "",
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fast_handle_na_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="fast_handle_na_+3A_set_num">set_num</code></td>
<td>
<p>NAs replacement for numeric column, (numeric or function, default to 0)</p>
</td></tr>
<tr><td><code id="fast_handle_na_+3A_set_logical">set_logical</code></td>
<td>
<p>NAs replacement for logical column, (logical or function, default to FALSE)</p>
</td></tr>
<tr><td><code id="fast_handle_na_+3A_set_char">set_char</code></td>
<td>
<p>NAs replacement for character column, (character or function, default to &quot;&quot;)</p>
</td></tr>
<tr><td><code id="fast_handle_na_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk (logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To preserve RAM this function edits data_set by <strong>reference</strong>. To keep object unchanged,
please use <code><a href="data.table.html#topic+copy">copy</a></code>. <br />
If you provide a function, it will be applied to the full column. So this function should handle NAs. <br />
For factor columns, it will add NA to list of values.
</p>


<h3>Value</h3>

<p>data_set as a <code><a href="data.table.html#topic+data.table">data.table</a></code> with NAs replaced.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Build a useful data_set set for example
require(data.table)
data_set &lt;- data.table(numCol = c(1, 2, 3, NA),
                   charCol = c("", "a", NA, "c"),
                   booleanCol = c(TRUE, NA, FALSE, NA))

# To set NAs to 0, FALSE and "" (respectively for numeric, logical, character)
fast_handle_na(copy(data_set))

# In a numeric column to set NAs as "missing"
fast_handle_na(copy(data_set), set_char = "missing")

# In a numeric column, to set NAs to the minimum value of the column#'
fast_handle_na(copy(data_set), set_num = min) # Won't work because min(c(1, NA)) = NA so put back NA
fast_handle_na(copy(data_set), set_num = function(x)min(x,na.rm = TRUE)) # Now we handle NAs

# In a numeric column, to set NAs to the share of NAs values
rateNA &lt;- function(x) {
  sum(is.na(x)) / length(x)
}
fast_handle_na(copy(data_set), set_num = rateNA)

</code></pre>

<hr>
<h2 id='fast_is_equal'>Fast checks of equality</h2><span id='topic+fast_is_equal'></span>

<h3>Description</h3>

<p>Performs quick check if two objects are equal.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fast_is_equal(object1, object2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fast_is_equal_+3A_object1">object1</code></td>
<td>
<p>An element, a vector, a data.frame, a data.table</p>
</td></tr>
<tr><td><code id="fast_is_equal_+3A_object2">object2</code></td>
<td>
<p>An element, a vector, a data.frame, a data.table</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses exponential search trick, so it is fast for very large vectors, data.frame and data.table.
This function is also very robust; you can compare a lot of stuff without failing.
</p>


<h3>Value</h3>

<p>Logical (TRUE or FALSE) if the two objects are equals.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Test on a character
fast_is_equal("a", "a")
fast_is_equal("a", "b")

# Test on a vector
myVector &lt;- rep(x = "a", 10000)
fast_is_equal(myVector, myVector)

# Test on a data.table
fast_is_equal(tiny_messy_adult, messy_adult)
</code></pre>

<hr>
<h2 id='fast_round'>Fast round</h2><span id='topic+fast_round'></span>

<h3>Description</h3>

<p>Fast round of numeric columns in a data.table. Will only round numeric, so don't worry about characters.
Also, it computes it column by column so your RAM is safe too.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fast_round(data_set, cols = "auto", digits = 2, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fast_round_+3A_data_set">data_set</code></td>
<td>
<p>matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="fast_round_+3A_cols">cols</code></td>
<td>
<p>List of numeric column(s) name(s) of data_set to transform. To transform all
numerics columns, set it to &quot;auto&quot; (characters, default to &quot;auto&quot;)</p>
</td></tr>
<tr><td><code id="fast_round_+3A_digits">digits</code></td>
<td>
<p>The number of digits after comma (numeric, default to 2)</p>
</td></tr>
<tr><td><code id="fast_round_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk? (logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is performing round by <strong>reference</strong> on data_set, column by column, only on numercial columns.
So that it avoid copying data_set in RAM.
</p>


<h3>Value</h3>

<p>The same datasets but as a data.table and with numeric rounded.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># First let's build a very large data.table with random numbers
require(data.table)
M &lt;- as.data.table(matrix(runif (3e4), ncol = 10))

M_rounded &lt;- fast_round(M, 2)
# Lets add some character
M[, stringColumn := "a string"]

# And use our function
M_rounded &lt;- fast_round(M, 2)
# It still work :) and you don't have to worry about the string.
</code></pre>

<hr>
<h2 id='fast_scale'>scale</h2><span id='topic+fast_scale'></span>

<h3>Description</h3>

<p>Perform efficient scaling on a data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fast_scale(data_set, scales = NULL, way = "scale", verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fast_scale_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="fast_scale_+3A_scales">scales</code></td>
<td>
<p>Result of function <code><a href="#topic+build_scales">build_scales</a></code>, (list, default to NULL). <br />
To perform the same scaling on train and test, it is recommended to compute <code><a href="#topic+build_scales">build_scales</a></code>
before. If it is kept to NULL, build_scales will be called.</p>
</td></tr>
<tr><td><code id="fast_scale_+3A_way">way</code></td>
<td>
<p>should scaling or unscaling be performed? (character either &quot;scale&quot; or &quot;unscale&quot;, default to &quot;scale&quot;)</p>
</td></tr>
<tr><td><code id="fast_scale_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk? (Logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Scaling numeric values is useful for some machine learning algorithm such as
logistic regression or neural networks. <br />
Unscaling numeric values can be very useful for most post-model analysis to do so set way to &quot;unscale&quot;. <br />
This implementation of scale will be faster that <code><a href="base.html#topic+scale">scale</a></code> for large data sets.
</p>


<h3>Value</h3>

<p><code>data_set</code> with columns scaled (or unscaled) by <strong>reference</strong>. Scaled means that each
column mean will be 0 and each column standard deviation will be 1.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load data
data(adult)

# compute scales
scales &lt;- build_scales(adult, cols = "auto", verbose = TRUE)

# Scale data set
adult &lt;- fast_scale(adult, scales = scales, verbose = TRUE)

# Control
print(mean(adult$age)) # Almost 0
print(sd(adult$age)) # 1

# To unscale it:
adult &lt;- fast_scale(adult, scales = scales, way = "unscale", verbose = TRUE)

# Control
print(mean(adult$age)) # About 38.6
print(sd(adult$age)) # About 13.6
</code></pre>

<hr>
<h2 id='find_and_transform_dates'>Identify date columns</h2><span id='topic+find_and_transform_dates'></span>

<h3>Description</h3>

<p>Find and transform dates that are hidden in a character column. <br />
It use a bunch of default formats, and you can also add your own formats.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_and_transform_dates(
  data_set,
  cols = "auto",
  formats = NULL,
  n_test = 30,
  ambiguities = "IGNORE",
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find_and_transform_dates_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="find_and_transform_dates_+3A_cols">cols</code></td>
<td>
<p>List of column(s) name(s) of data_set to look into. To check all all columns, set it
to &quot;auto&quot;. (characters, default to &quot;auto&quot;)</p>
</td></tr>
<tr><td><code id="find_and_transform_dates_+3A_formats">formats</code></td>
<td>
<p>List of additional Date formats to check (see <code><a href="base.html#topic+strptime">strptime</a></code>)</p>
</td></tr>
<tr><td><code id="find_and_transform_dates_+3A_n_test">n_test</code></td>
<td>
<p>Number of non-null rows on which to test (numeric, default to 30)</p>
</td></tr>
<tr><td><code id="find_and_transform_dates_+3A_ambiguities">ambiguities</code></td>
<td>
<p>How ambiguities should be treated (see details in ambiguities section)
(character, default to IGNORE)</p>
</td></tr>
<tr><td><code id="find_and_transform_dates_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk? (Logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is using <code><a href="#topic+identify_dates">identify_dates</a></code> to find formats. Please see it's documentation.
In case <code><a href="#topic+identify_dates">identify_dates</a></code> doesn't find wanted formats you can either provide format
in param <code>formats</code>
or use <code><a href="#topic+set_col_as_date">set_col_as_date</a></code> to force transformation.
</p>


<h3>Value</h3>

<p>data_set set (as a data.table) with identified dates transformed by <strong>reference</strong>.
</p>


<h3>Ambiguity</h3>

<p>Ambiguities are often present in dates. For example, in date: 2017/01/01, there is no way to know
if format is YYYY/MM/DD or YYYY/DD/MM. <br />
Some times ambiguity can be solved by a human. For example
17/12/31, a human might guess that it is YY/MM/DD, but there is no sure way to know.  <br />
To be safe, find_and_transform_dates doesn't try to guess ambiguities. <br />
To answer ambiguities problem, param <code>ambiguities</code> is now available. It can take one of the following values
</p>

<ul>
<li> <p><code>IGNORE</code> function will then take the first format which match (fast, but can make some mistakes)
</p>
</li>
<li> <p><code>WARN</code> function will try all format and tell you - via prints - that there are
multiple matches (and won't perform date transformation)
</p>
</li>
<li> <p><code>SOLVE</code> function will try to solve ambiguity by going through more lines, so will be slower.
If it is able to solve it, it will transform the column, if not it will print the various acceptable formats.
</p>
</li></ul>

<p>If there are some columns that have no chance to be a match think of removing them from <code>cols</code>
to save some computation time.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load exemple set
data(tiny_messy_adult)
head(tiny_messy_adult)
# using the find_and_transform_dates
find_and_transform_dates(tiny_messy_adult, n_test = 5)
head(tiny_messy_adult)

# Example with ambiguities
## Not run: 
require(data.table)
data(tiny_messy_adult) # reload data
# Add an ambiguity by sorting date1
tiny_messy_adult$date1 = sort(tiny_messy_adult$date1, na.last = TRUE)
# Try all three methods:
result_1 = find_and_transform_dates(copy(tiny_messy_adult))
result_2 = find_and_transform_dates(copy(tiny_messy_adult), ambiguities = "WARN")
result_3 = find_and_transform_dates(copy(tiny_messy_adult), ambiguities = "SOLVE")

## End(Not run)
# "##NOT RUN:" mean that this example hasn't been run on CRAN since its long. But you can run it!
</code></pre>

<hr>
<h2 id='find_and_transform_numerics'>Identify numeric columns in a data_set set</h2><span id='topic+find_and_transform_numerics'></span>

<h3>Description</h3>

<p>Function to find and transform characters that are in fact numeric.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_and_transform_numerics(
  data_set,
  cols = "auto",
  n_test = 30,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find_and_transform_numerics_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="find_and_transform_numerics_+3A_cols">cols</code></td>
<td>
<p>List of column(s) name(s) of data_set to look into. To check all all columns, set it
to &quot;auto&quot;. (characters, default to &quot;auto&quot;)</p>
</td></tr>
<tr><td><code id="find_and_transform_numerics_+3A_n_test">n_test</code></td>
<td>
<p>Number of non-null rows on which to test (numeric, default to 30)</p>
</td></tr>
<tr><td><code id="find_and_transform_numerics_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk? (logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is looking for perfect transformation.
If there are some mistakes in data_set, consider setting them to NA before. <br />
If there are some columns that have no chance to be a match think of removing them from <code>cols</code>
to save some computation time.
</p>


<h3>Value</h3>

<p>The data_set set (as a data.table) with identified numeric transformed.
</p>


<h3>Warning</h3>

<p>All these changes will happen <strong>by reference</strong>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Let's build a data_set set
data_set &lt;- data.frame(ID = seq_len(5),
                  col1 = c("1.2", "1.3", "1.2", "1", "6"),
                  col2 = c("1,2", "1,3", "1,2", "1", "6")
                  )

# using the find_and_transform_numerics
find_and_transform_numerics(data_set, n_test = 5)
</code></pre>

<hr>
<h2 id='generate_date_diffs'>Date difference</h2><span id='topic+generate_date_diffs'></span>

<h3>Description</h3>

<p>Perform the differences between all dates of the data_set set and optionally with a static date.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_date_diffs(
  data_set,
  cols = "auto",
  analysis_date = NULL,
  units = "years",
  drop = FALSE,
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generate_date_diffs_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="generate_date_diffs_+3A_cols">cols</code></td>
<td>
<p>List of date column(s) name(s) of data_set to commute difference on. To transform all
dates, set it to &quot;auto&quot;. (character, default to &quot;auto&quot;)</p>
</td></tr>
<tr><td><code id="generate_date_diffs_+3A_analysis_date">analysis_date</code></td>
<td>
<p>Static date (Date or POSIXct, optional)</p>
</td></tr>
<tr><td><code id="generate_date_diffs_+3A_units">units</code></td>
<td>
<p>Unit of difference between too dates (string, default to 'years')</p>
</td></tr>
<tr><td><code id="generate_date_diffs_+3A_drop">drop</code></td>
<td>
<p>Should <code>cols</code> be dropped after generation (logical, default to FALSE)</p>
</td></tr>
<tr><td><code id="generate_date_diffs_+3A_verbose">verbose</code></td>
<td>
<p>should the function log (logical, default to TRUE)</p>
</td></tr>
<tr><td><code id="generate_date_diffs_+3A_...">...</code></td>
<td>
<p>Other arguments such as <code>name_separator</code> to separate words in new columns names
(character, default to &quot;.&quot;)</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>units</code> is the same as <code><a href="base.html#topic+difftime">difftime</a></code> units, but with one more possibility: years.
</p>


<h3>Value</h3>

<p>data_set (as a <code><a href="data.table.html#topic+data.table">data.table</a></code>) with more columns.
A numeric column has been added for every couple of Dates. The result is in years.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># First build a useful data_set set
require(data.table)
data_set &lt;- data.table(ID = seq_len(100),
                  date1 = seq(from = as.Date("2010-01-01"),
                              to = as.Date("2015-01-01"),
                              length.out = 100),
                  date2 = seq(from = as.Date("1910-01-01"),
                              to = as.Date("2000-01-01"),
                              length.out = 100)
                  )

# Now let's compute
data_set &lt;- generate_date_diffs(data_set, cols = "auto", analysis_date = as.Date("2016-11-14"))
</code></pre>

<hr>
<h2 id='generate_factor_from_date'>Generate factor from dates</h2><span id='topic+generate_factor_from_date'></span>

<h3>Description</h3>

<p>Taking Date or POSIXct colums, and building factor columns from them.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_factor_from_date(
  data_set,
  cols = "auto",
  type = "yearmonth",
  drop = FALSE,
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generate_factor_from_date_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="generate_factor_from_date_+3A_cols">cols</code></td>
<td>
<p>List of date column(s) name(s) of data_set to transform into factor. To transform all
dates, set it to &quot;auto&quot;. (characters, default to &quot;auto&quot;)</p>
</td></tr>
<tr><td><code id="generate_factor_from_date_+3A_type">type</code></td>
<td>
<p>&quot;year&quot;, &quot;yearquarter&quot;, &quot;yearmonth&quot;, &quot;quarter&quot; or &quot;month&quot;, way to aggregate a date,
(character, default to &quot;yearmonth&quot;)</p>
</td></tr>
<tr><td><code id="generate_factor_from_date_+3A_drop">drop</code></td>
<td>
<p>Should <code>cols</code> be dropped after generation (logical, default to FALSE)</p>
</td></tr>
<tr><td><code id="generate_factor_from_date_+3A_verbose">verbose</code></td>
<td>
<p>Should the function log (logical, default to TRUE)</p>
</td></tr>
<tr><td><code id="generate_factor_from_date_+3A_...">...</code></td>
<td>
<p>Other arguments such as <code>name_separator</code> to separate words in new columns names
(character, default to &quot;.&quot;)</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>data_set</code> with new columns. <code>data_set</code> is edited by <strong>reference</strong>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load set, and find dates
data(tiny_messy_adult)
tiny_messy_adult &lt;- find_and_transform_dates(tiny_messy_adult, verbose = FALSE)

# Generate new columns
# Generate year month columns
tiny_messy_adult &lt;- generate_factor_from_date(tiny_messy_adult, cols = c("date1", "date2", "num1"))
head(tiny_messy_adult[, .(date1.yearmonth, date2.yearmonth)])


# Generate quarter columns
tiny_messy_adult &lt;- generate_factor_from_date(tiny_messy_adult, 
                                              cols = c("date1", "date2"), type = "quarter")
head(tiny_messy_adult[, .(date1.quarter, date2.quarter)])
</code></pre>

<hr>
<h2 id='generate_from_character'>Recode character</h2><span id='topic+generate_from_character'></span>

<h3>Description</h3>

<p>Recode character into 3 new columns: <br />
</p>

<ul>
<li><p> was the value not NA, &quot;NA&quot;, &quot;&quot;,
</p>
</li>
<li><p> how often this value occurs,
</p>
</li>
<li><p> the order of the value (ex: M/F =&gt; 2/1 because F comes before M in alphabet).
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>generate_from_character(
  data_set,
  cols = "auto",
  verbose = TRUE,
  drop = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generate_from_character_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="generate_from_character_+3A_cols">cols</code></td>
<td>
<p>List of character column(s) name(s) of data_set to transform. To transform all
characters, set it to &quot;auto&quot;. (character, default to &quot;auto&quot;)</p>
</td></tr>
<tr><td><code id="generate_from_character_+3A_verbose">verbose</code></td>
<td>
<p>Should the function log (logical, default to TRUE)</p>
</td></tr>
<tr><td><code id="generate_from_character_+3A_drop">drop</code></td>
<td>
<p>Should <code>cols</code> be dropped after generation (logical, default to FALSE)</p>
</td></tr>
<tr><td><code id="generate_from_character_+3A_...">...</code></td>
<td>
<p>Other arguments such as <code>name_separator</code> to separate words in new columns names
(character, default to &quot;.&quot;)</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>data_set</code> with new columns. <code>data_set</code> is edited by <strong>reference</strong>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load data set
data(tiny_messy_adult)
tiny_messy_adult &lt;- un_factor(tiny_messy_adult, verbose = FALSE) # un factor ugly factors

# transform column "mail"
tiny_messy_adult &lt;- generate_from_character(tiny_messy_adult, cols = "mail")
head(tiny_messy_adult)

# To transform all characters columns:
tiny_messy_adult &lt;- generate_from_character(tiny_messy_adult, cols = "auto")
</code></pre>

<hr>
<h2 id='generate_from_factor'>Recode factor</h2><span id='topic+generate_from_factor'></span>

<h3>Description</h3>

<p>Recode factors into 3 new columns:
</p>

<ul>
<li><p> was the value not NA, &quot;NA&quot;, &quot;&quot;,
</p>
</li>
<li><p> how often this value occurs,
</p>
</li>
<li><p> the order of the value (ex: M/F =&gt; 2/1 because F comes before M in alphabet).
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>generate_from_factor(
  data_set,
  cols = "auto",
  verbose = TRUE,
  drop = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generate_from_factor_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="generate_from_factor_+3A_cols">cols</code></td>
<td>
<p>list of character column(s) name(s) of data_set to transform. To transform all
factors, set it to &quot;auto&quot;. (character, default to &quot;auto&quot;)</p>
</td></tr>
<tr><td><code id="generate_from_factor_+3A_verbose">verbose</code></td>
<td>
<p>Should the function log (logical, default to TRUE)</p>
</td></tr>
<tr><td><code id="generate_from_factor_+3A_drop">drop</code></td>
<td>
<p>Should <code>cols</code> be dropped after generation (logical, default to FALSE)</p>
</td></tr>
<tr><td><code id="generate_from_factor_+3A_...">...</code></td>
<td>
<p>Other arguments such as <code>name_separator</code> to separate words in new columns names
(character, default to &quot;.&quot;)</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>data_set</code> with new columns. <code>data_set</code> is edited by <strong>reference</strong>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load data set
data(tiny_messy_adult)

# transform column "type_employer"
tiny_messy_adult &lt;- generate_from_factor(tiny_messy_adult, cols = "type_employer")
head(tiny_messy_adult)

# To transform all factor columns:
tiny_messy_adult &lt;- generate_from_factor(tiny_messy_adult, cols = "auto")
</code></pre>

<hr>
<h2 id='get_most_frequent_element'>Get most frequent element</h2><span id='topic+get_most_frequent_element'></span>

<h3>Description</h3>

<p>Provide most frequent element in a list, a <code>data.frame</code> or <code>data.table</code> 
column
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_most_frequent_element(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_most_frequent_element_+3A_x">x</code></td>
<td>
<p>A list, <code>data.frame</code> or <code>data.table</code> column</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The most frequent element
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Build example list
example_list &lt;- c(1, 1, 2, 3, 1, 4, 1)

# Compute most frequent element
get_most_frequent_element(example_list)
</code></pre>

<hr>
<h2 id='identify_dates'>Identify date columns</h2><span id='topic+identify_dates'></span>

<h3>Description</h3>

<p>Function to identify dates columns and give there format. It use a bunch of default formats.
But you can also add your own formats.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>identify_dates(
  data_set,
  cols = "auto",
  formats = NULL,
  n_test = 30,
  ambiguities = "IGNORE",
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="identify_dates_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="identify_dates_+3A_cols">cols</code></td>
<td>
<p>List of column(s) name(s) of data_set to look into. To check all all columns, set it
to &quot;auto&quot;. (characters, default to &quot;auto&quot;)</p>
</td></tr>
<tr><td><code id="identify_dates_+3A_formats">formats</code></td>
<td>
<p>List of additional Date formats to check (see <code><a href="base.html#topic+strptime">strptime</a></code>)</p>
</td></tr>
<tr><td><code id="identify_dates_+3A_n_test">n_test</code></td>
<td>
<p>Number of non-null rows on which to test (numeric, default to 30)</p>
</td></tr>
<tr><td><code id="identify_dates_+3A_ambiguities">ambiguities</code></td>
<td>
<p>How ambiguities should be treated (see details in ambiguities section)
(character, default to IGNORE)</p>
</td></tr>
<tr><td><code id="identify_dates_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk? (Logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is looking for perfect transformation.
If there are some mistakes in data_set, consider setting them to NA before. <br />
In the unlikely case where you have numeric higher than <code>as.numeric(as.POSIXct("1990-01-01"))</code>
they will be considered as timestamps and you might have some issues. On the other side,
if you have timestamps before 1990-01-01, they won't be found, but you can use
<code><a href="#topic+set_col_as_date">set_col_as_date</a></code> to force transformation.
</p>


<h3>Value</h3>

<p>A named list with names being col names of <code>data_set</code> and values being formats.
</p>


<h3>Ambiguity</h3>

<p>Ambiguities are often present in dates. For example, in date: 2017/01/01, there is no way to know
if format is YYYY/MM/DD or YYYY/DD/MM. <br />
Some times ambiguity can be solved by a human. For example
17/12/31, a human might guess that it is YY/MM/DD, but there is no sure way to know.  <br />
To be safe, find_and_transform_dates doesn't try to guess ambiguities. <br />
To answer ambiguities problem, param <code>ambiguities</code> is now available. It can take one of the following values
</p>

<ul>
<li> <p><code>IGNORE</code> function will then take the first format which match (fast, but can make some mistakes)
</p>
</li>
<li> <p><code>WARN</code> function will try all format and tell you - via prints - that there are multiple matches
(and won't perform date transformation)
</p>
</li>
<li> <p><code>SOLVE</code> function will try to solve ambiguity by going through more lines, so will be slower.
If it is able to solve it, it will transform the column, if not it will print the various acceptable formats.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Load exemple set
data(tiny_messy_adult)
head(tiny_messy_adult)
# using the find_and_transform_dates
identify_dates(tiny_messy_adult, n_test = 5)
</code></pre>

<hr>
<h2 id='messy_adult'>Adult with some ugly columns added</h2><span id='topic+messy_adult'></span>

<h3>Description</h3>

<p>For examples and tutorials, messy_adult has been built using UCI <code>adult</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(tiny_messy_adult)
</code></pre>


<h3>Format</h3>

<p>A data.table with 32561 rows and 24 variables.
</p>


<h3>Details</h3>

<p>We added 9 really ugly columns to the data set:
</p>

<ul>
<li><p> 4 dates with various formats and time stamp, containing NAs
</p>
</li>
<li><p> 1 constant column
</p>
</li>
<li><p> 3 numeric with different decimal separator
</p>
</li>
<li><p> 1 email address
</p>
</li></ul>


<hr>
<h2 id='one_hot_encoder'>One hot encoder</h2><span id='topic+one_hot_encoder'></span>

<h3>Description</h3>

<p>Transform factor column into 0/1 columns with one column per values of the column.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>one_hot_encoder(
  data_set,
  encoding = NULL,
  type = "integer",
  verbose = TRUE,
  drop = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="one_hot_encoder_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="one_hot_encoder_+3A_encoding">encoding</code></td>
<td>
<p>Result of function <code><a href="#topic+build_encoding">build_encoding</a></code>, (list, default to NULL). <br />
To perform the same encoding on train and test, it is recommended to compute <code><a href="#topic+build_encoding">build_encoding</a></code>
before. If it is kept to NULL, build_encoding will be called.</p>
</td></tr>
<tr><td><code id="one_hot_encoder_+3A_type">type</code></td>
<td>
<p>What class of columns is expected? &quot;integer&quot; (0L/1L), &quot;numeric&quot; (0/1), or &quot;logical&quot; (TRUE/FALSE),
(character, default to &quot;integer&quot;)</p>
</td></tr>
<tr><td><code id="one_hot_encoder_+3A_verbose">verbose</code></td>
<td>
<p>Should the function log (logical, default to TRUE)</p>
</td></tr>
<tr><td><code id="one_hot_encoder_+3A_drop">drop</code></td>
<td>
<p>Should <code>cols</code> be dropped after generation (logical, default to FALSE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If you don't want to edit your data set consider sending <code>copy(data_set)</code> as an input.<br />
Please <strong>be careful</strong> using this function, it will generate as many columns as there different values
in your column and might use a lot of RAM. To be safe, you can use parameter
<code>min_frequency</code> in <code><a href="#topic+build_encoding">build_encoding</a></code>.
</p>


<h3>Value</h3>

<p><code>data_set</code> edited by <strong>reference</strong> with new columns.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(tiny_messy_adult)

# Compute encoding
encoding &lt;- build_encoding(tiny_messy_adult, cols = c("marital", "occupation"), verbose = TRUE)

# Apply it
tiny_messy_adult &lt;- one_hot_encoder(tiny_messy_adult, encoding = encoding, drop = TRUE)

# Apply same encoding to adult
data(adult)
adult &lt;- one_hot_encoder(adult, encoding = encoding, drop = TRUE)

# To have encoding as logical (TRUE/FALSE), pass it in type argument
data(adult)
adult &lt;- one_hot_encoder(adult, encoding = encoding, type = "logical", drop = TRUE)
</code></pre>

<hr>
<h2 id='prepare_set'>Preparation pipeline</h2><span id='topic+prepare_set'></span>

<h3>Description</h3>

<p>Full pipeline for preparing your data_set set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_set(data_set, final_form = "data.table", verbose = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prepare_set_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="prepare_set_+3A_final_form">final_form</code></td>
<td>
<p>&quot;data.table&quot; or &quot;numerical_matrix&quot; (default to data.table)</p>
</td></tr>
<tr><td><code id="prepare_set_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk? (logical, default to TRUE)</p>
</td></tr>
<tr><td><code id="prepare_set_+3A_...">...</code></td>
<td>
<p>Additional parameters to tune pipeline (see details)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Additional arguments are available to tune pipeline:
</p>

<ul>
<li> <p><code>key</code> Name of a column of data_set according to which data_set should be aggregated
(character)
</p>
</li>
<li> <p><code>analysis_date</code> A date at which the data_set should be aggregated
(differences between every date and analysis_date will be computed) (Date)
</p>
</li>
<li> <p><code>n_unfactor</code> Number of max value in a factor, set it to -1 to disable
<code><a href="#topic+un_factor">un_factor</a></code> function.  (numeric, default to 53)
</p>
</li>
<li> <p><code>digits</code> The number of digits after comma (optional, numeric, if set will perform
<code><a href="#topic+fast_round">fast_round</a></code>)
</p>
</li>
<li> <p><code>dateFormats</code> List of format of Dates in data_set (list of characters)
</p>
</li>
<li> <p><code>name_separator</code> character to separate parts of new column names (character, default to &quot;.&quot;)
</p>
</li>
<li> <p><code>functions</code>  Aggregation functions for numeric columns, see <code><a href="#topic+aggregate_by_key">aggregate_by_key</a></code>
(list of functions names (character))
</p>
</li>
<li> <p><code>factor_date_type</code> Aggregation level to factorize date (see
<code><a href="#topic+generate_factor_from_date">generate_factor_from_date</a></code>) (character, default to &quot;yearmonth&quot;)
</p>
</li>
<li> <p><code>target_col</code> A target column to perform target encoding, see <code><a href="#topic+target_encode">target_encode</a></code>
(character)
</p>
</li>
<li> <p><code>target_encoding_functions</code> Functions to perform target encoding, see
<code><a href="#topic+build_target_encoding">build_target_encoding</a></code>,
if <code>target_col</code> is not given will not do anything, (list, default to <code>"mean"</code>)
</p>
</li></ul>



<h3>Value</h3>

<p>A data.table or a numerical matrix (according to <code>final_form</code>). <br />
It will perform the following steps:
</p>

<ul>
<li><p> Correct set: unfactor factor with many values, id dates and numeric that are hiden in character
</p>
</li>
<li><p> Transform set: compute differences between every date, transform dates into factors, generate
features from character..., if <code>key</code> is provided, will perform aggregate according to this <code>key</code>
</p>
</li>
<li><p> Filter set: filter constant, in double or bijection variables. If 'digits' is provided,
will round numeric
</p>
</li>
<li><p> Handle NA: will perform <code><a href="#topic+fast_handle_na">fast_handle_na</a></code>)
</p>
</li>
<li><p> Shape set: will put the result in asked shape (<code>final_form</code>) with acceptable columns format.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Load ugly set
## Not run: 
data(tiny_messy_adult)

# Have a look to set
head(tiny_messy_adult)

# Compute full pipeline
clean_adult &lt;- prepare_set(tiny_messy_adult)

# With a reference date
adult_agg &lt;- prepare_set(tiny_messy_adult, analysis_date = as.Date("2017-01-01"))

# Add aggregation by country
adult_agg &lt;- prepare_set(tiny_messy_adult, analysis_date = as.Date("2017-01-01"), key = "country")

# With some new aggregation functions
power &lt;- function(x) {sum(x^2)}
adult_agg &lt;- prepare_set(tiny_messy_adult, analysis_date = as.Date("2017-01-01"), key = "country",
                        functions = c("min", "max", "mean", "power"))

## End(Not run)
# "##NOT RUN:" mean that this example hasn't been run on CRAN since its long. But you can run it!
</code></pre>

<hr>
<h2 id='remove_percentile_outlier'>Percentile outlier filtering</h2><span id='topic+remove_percentile_outlier'></span>

<h3>Description</h3>

<p>Remove outliers based on percentiles. <br />
Only values within <code>n</code>th and <code>100 - n</code>th percentiles are kept.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>remove_percentile_outlier(
  data_set,
  cols = "auto",
  percentile = 1,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="remove_percentile_outlier_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="remove_percentile_outlier_+3A_cols">cols</code></td>
<td>
<p>List of numeric column(s) name(s) of data_set to transform. To transform all
numeric columns, set it to &quot;auto&quot;.  (character, default to &quot;auto&quot;)</p>
</td></tr>
<tr><td><code id="remove_percentile_outlier_+3A_percentile">percentile</code></td>
<td>
<p>percentiles to filter (numeric, default to 1)</p>
</td></tr>
<tr><td><code id="remove_percentile_outlier_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk? (logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Filtering is made column by column, meaning that extreme values from first element
of <code>cols</code> are removed, then extreme values from second element of <code>cols</code> are removed,
... <br />
So if filtering is performed on too many column, there ia high risk that a lot of rows will be dropped.
</p>


<h3>Value</h3>

<p>Same dataset with less rows, edited by <strong>reference</strong>. <br />
If you don't want to edit by reference please provide set <code>data_set = copy(data_set)</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Given
library(data.table)
data_set &lt;- data.table(num_col = seq_len(100))

# When
data_set &lt;- remove_percentile_outlier(data_set, cols = "auto", percentile = 1, verbose = TRUE)

# Then extreme value is no longer in set
1 %in% data_set[["num_col"]] # Is false
2 %in% data_set[["num_col"]] # Is true
</code></pre>

<hr>
<h2 id='remove_rare_categorical'>Filter rare categories</h2><span id='topic+remove_rare_categorical'></span>

<h3>Description</h3>

<p>Filter rows that have a rare occurrences
</p>


<h3>Usage</h3>

<pre><code class='language-R'>remove_rare_categorical(
  data_set,
  cols = "auto",
  threshold = 0.01,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="remove_rare_categorical_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="remove_rare_categorical_+3A_cols">cols</code></td>
<td>
<p>List of column(s) name(s) of data_set to transform. To transform all
columns, set it to &quot;auto&quot;.  (character, default to &quot;auto&quot;)</p>
</td></tr>
<tr><td><code id="remove_rare_categorical_+3A_threshold">threshold</code></td>
<td>
<p>share of occurrences under which row should be removed (numeric, default to 0.01)</p>
</td></tr>
<tr><td><code id="remove_rare_categorical_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk? (logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Filtering is made column by column, meaning that extreme values from first element
of <code>cols</code> are removed, then extreme values from second element of <code>cols</code> are removed,
... <br />
So if filtering is performed on too many column, there ia high risk that a lot of rows will be dropped.
</p>


<h3>Value</h3>

<p>Same dataset with less rows, edited by <strong>reference</strong>. <br />
If you don't want to edit by reference please provide set <code>data_set = copy(data_set)</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Given a set with rare "C"
library(data.table)
data_set &lt;- data.table(cat_col = c(sample(c("A", "B"), 1000, replace=TRUE), "C"))

# When calling function
data_set &lt;- remove_rare_categorical(data_set, cols = "cat_col",
                                   threshold = 0.01, verbose = TRUE)

# Then there are no "C"
unique(data_set[["cat_col"]])
</code></pre>

<hr>
<h2 id='remove_sd_outlier'>Standard deviation outlier filtering</h2><span id='topic+remove_sd_outlier'></span>

<h3>Description</h3>

<p>Remove outliers based on standard deviation thresholds. <br />
Only values within <code>mean - sd * n_sigmas</code> and <code>mean + sd * n_sigmas</code> are kept.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>remove_sd_outlier(data_set, cols = "auto", n_sigmas = 3, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="remove_sd_outlier_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="remove_sd_outlier_+3A_cols">cols</code></td>
<td>
<p>List of numeric column(s) name(s) of data_set to transform. To transform all
numeric columns, set it to &quot;auto&quot;.  (character, default to &quot;auto&quot;)</p>
</td></tr>
<tr><td><code id="remove_sd_outlier_+3A_n_sigmas">n_sigmas</code></td>
<td>
<p>number of times standard deviation is accepted (integer, default to 3)</p>
</td></tr>
<tr><td><code id="remove_sd_outlier_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk? (logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Filtering is made column by column, meaning that extreme values from first element
of <code>cols</code> are removed, then extreme values from second element of <code>cols</code> are removed,
... <br />
So if filtering is performed on too many column, there ia high risk that a lot of rows will be dropped.
</p>


<h3>Value</h3>

<p>Same dataset with less rows, edited by <strong>reference</strong>. <br />
If you don't want to edit by reference please provide set <code>data_set = copy(data_set)</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Given
library(data.table)
col_vals &lt;- runif(1000)
col_mean &lt;- mean(col_vals)
col_sd &lt;- sd(col_vals)
extreme_val &lt;- col_mean + 6 * col_sd
data_set &lt;- data.table(num_col = c(col_vals, extreme_val))

# When
data_set &lt;- remove_sd_outlier(data_set, cols = "auto", n_sigmas = 3, verbose = TRUE)

# Then extreme value is no longer in set
extreme_val %in% data_set[["num_col"]] # Is false
</code></pre>

<hr>
<h2 id='same_shape'>Give same shape</h2><span id='topic+same_shape'></span>

<h3>Description</h3>

<p>Transform <code>data_set</code> into the same shape as <code>reference_set</code>. Especially this
function will be useful to make your test set have the same shape as your train set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>same_shape(data_set, reference_set, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="same_shape_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table to transform</p>
</td></tr>
<tr><td><code id="same_shape_+3A_reference_set">reference_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="same_shape_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk? (logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function will make sure that <code>data_set</code> and <code>reference_set</code>
</p>

<ul>
<li><p> have the same class
</p>
</li>
<li><p> have exactly the same columns
</p>
</li>
<li><p> have columns with exactly the same class
</p>
</li>
<li><p> have factor factor with exactly the same levels
</p>
</li></ul>

<p>You should always use this function before applying your model on a new data set to make sure
that everything will go smoothly. But if this function change a lot of stuff you should have a
look to your preparation process, there might be something wrong.
</p>


<h3>Value</h3>

<p>Return <code>data_set</code> transformed in order to make it have the same shape as
<code>reference_set</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Build a train and a test
data(tiny_messy_adult)
data(adult)
train &lt;- messy_adult
test &lt;- adult # So test will have missing columns

# Prepare them
train &lt;- prepare_set(train, verbose = FALSE, key = "country")
test &lt;- prepare_set(test, verbose = FALSE, key = "country")

# Give them the same shape
test &lt;- same_shape(test, train)
# As one can see in log, a lot of small change had to be done.
# This is an extreme case but you get the idea.

## End(Not run)
# "##NOT RUN:" mean that this example hasn't been run on CRAN since its long. But you can run it!
</code></pre>

<hr>
<h2 id='set_as_numeric_matrix'>Numeric matrix preparation for Machine Learning.</h2><span id='topic+set_as_numeric_matrix'></span>

<h3>Description</h3>

<p>Prepare a numeric matrix from a data.table. This matrix is suitable for
machine learning purposes, since factors are binary. It may be sparse,
include an intercept, and drop a reference column for each factor if
required (when using <code>lm()</code>, for instance)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_as_numeric_matrix(
  data_set,
  intercept = FALSE,
  all_cols = FALSE,
  sparse = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_as_numeric_matrix_+3A_data_set">data_set</code></td>
<td>
<p>data.table</p>
</td></tr>
<tr><td><code id="set_as_numeric_matrix_+3A_intercept">intercept</code></td>
<td>
<p>Should a constant column be added? (logical, default to FALSE)</p>
</td></tr>
<tr><td><code id="set_as_numeric_matrix_+3A_all_cols">all_cols</code></td>
<td>
<p>For each factor, should we create all possible
dummies, or should we drop a reference dummy? (logical, default to FALSE)</p>
</td></tr>
<tr><td><code id="set_as_numeric_matrix_+3A_sparse">sparse</code></td>
<td>
<p>Should the resulting matrix be of a (sparse) Matrix
class? (logical, default to FALSE)</p>
</td></tr>
</table>

<hr>
<h2 id='set_col_as_character'>Set columns as character</h2><span id='topic+set_col_as_character'></span>

<h3>Description</h3>

<p>Set as character a column (or a list of columns) from a data.table.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_col_as_character(data_set, cols = "auto", verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_col_as_character_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="set_col_as_character_+3A_cols">cols</code></td>
<td>
<p>List of column(s) name(s) of data_set to transform into characters. To transform
all columns, set it to &quot;auto&quot;. (characters, default to &quot;auto&quot;)</p>
</td></tr>
<tr><td><code id="set_col_as_character_+3A_verbose">verbose</code></td>
<td>
<p>Should the function log (logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data_set (as a <code><a href="data.table.html#topic+data.table">data.table</a></code>), with specified columns set as character.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Build a fake data.frame
data_set &lt;- data.frame(numCol = c(1, 2, 3), factorCol = as.factor(c("a", "b", "c")))

# Set numCol and factorCol as character
data_set &lt;- set_col_as_character(data_set, cols = c("numCol", "factorCol"))
</code></pre>

<hr>
<h2 id='set_col_as_date'>Set columns as POSIXct</h2><span id='topic+set_col_as_date'></span>

<h3>Description</h3>

<p>Set as POSIXct a character column (or a list of columns) from a data.table.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_col_as_date(data_set, cols = NULL, format = NULL, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_col_as_date_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="set_col_as_date_+3A_cols">cols</code></td>
<td>
<p>List of column(s) name(s) of data_set to transform into dates</p>
</td></tr>
<tr><td><code id="set_col_as_date_+3A_format">format</code></td>
<td>
<p>Date's format (function will be faster if the format is provided)
(character or list of character, default to NULL).<br />
For timestamps, format need to be provided (&quot;s&quot; or &quot;ms&quot; or second or millisecond timestamps)</p>
</td></tr>
<tr><td><code id="set_col_as_date_+3A_verbose">verbose</code></td>
<td>
<p>Should the function log (logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>set_col_as_date is way faster when format is provided. If you want to identify dates and format
automatically, have a look to <code><a href="#topic+identify_dates">identify_dates</a></code>. <br />
If input column is a factor, it will be returned as a POSIXct column. <br />
If <code>cols</code> is kept to default (NULL) set_col_as_date won't do anything.
</p>


<h3>Value</h3>

<p><code>data_set</code> (as a <code><a href="data.table.html#topic+data.table">data.table</a></code>), with specified columns set as Date.
If the transformation generated only NA, the column is set back to its original value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Lets build a data_set set
data_set &lt;- data.frame(ID = seq_len(5),
                  date1 = c("2015-01-01", "2016-01-01", "2015-09-01", "2015-03-01", "2015-01-31"),
                  date2 = c("2015_01_01", "2016_01_01", "2015_09_01", "2015_03_01", "2015_01_31")
                  )

# Using set_col_as_date for date2
data_transformed &lt;- set_col_as_date(data_set, cols = "date2", format = "%Y_%m_%d")

# Control the results
lapply(data_transformed, class)

# With multiple formats:
data_transformed &lt;- set_col_as_date(data_set, format = list(date1 = "%Y-%m-%d", date2 = "%Y_%m_%d"))
lapply(data_transformed, class)

# It also works with timestamps
data_set &lt;- data.frame(time_stamp = c(1483225200, 1485990000, 1488495600))
set_col_as_date(data_set, cols = "time_stamp", format = "s")
</code></pre>

<hr>
<h2 id='set_col_as_factor'>Set columns as factor</h2><span id='topic+set_col_as_factor'></span>

<h3>Description</h3>

<p>Set columns as factor and control number of unique element, to avoid having too large factors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_col_as_factor(data_set, cols = "auto", n_levels = 53, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_col_as_factor_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="set_col_as_factor_+3A_cols">cols</code></td>
<td>
<p>List of column(s) name(s) of data_set to transform into factor. To transform all columns
set it to &quot;auto&quot;, (characters, default to auto).</p>
</td></tr>
<tr><td><code id="set_col_as_factor_+3A_n_levels">n_levels</code></td>
<td>
<p>Max number of levels for factor (integer, default to 53)
set it to -1 to disable control.</p>
</td></tr>
<tr><td><code id="set_col_as_factor_+3A_verbose">verbose</code></td>
<td>
<p>Should the function log (logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Control number of levels will help you to distinguish true categorical columns from just characters
that should be handled in another way.
</p>


<h3>Value</h3>

<p><code>data_set</code>(as a <code><a href="data.table.html#topic+data.table">data.table</a></code>), with specified columns set as factor or logical.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load messy_adult
data(tiny_messy_adult)

# we wil change education
tiny_messy_adult &lt;- set_col_as_factor(tiny_messy_adult, cols = "education")

sapply(tiny_messy_adult[, .(education)], class)
# education is now a factor
</code></pre>

<hr>
<h2 id='set_col_as_numeric'>Set columns as numeric</h2><span id='topic+set_col_as_numeric'></span>

<h3>Description</h3>

<p>Set as numeric a character column (or a list of columns) from a data.table.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_col_as_numeric(data_set, cols, strip_string = FALSE, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_col_as_numeric_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="set_col_as_numeric_+3A_cols">cols</code></td>
<td>
<p>List of column(s) name(s) of data_set to transform into numerics</p>
</td></tr>
<tr><td><code id="set_col_as_numeric_+3A_strip_string">strip_string</code></td>
<td>
<p>should I change &quot;,&quot; to &quot;.&quot; in the string? (logical, default to FALSE)
If set to TRUE, computation will be a bit longer</p>
</td></tr>
<tr><td><code id="set_col_as_numeric_+3A_verbose">verbose</code></td>
<td>
<p>Should the function log (logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data_set (as a <code><a href="data.table.html#topic+data.table">data.table</a></code>), with specified columns set as numeric.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Build a fake data.table
data_set &lt;- data.frame(charCol1 = c("1", "2", "3"),
						 charCol2 = c("4", "5", "6"))

# Set charCol1 and charCol2 as numeric
data_set &lt;- set_col_as_numeric(data_set, cols = c("charCol1", "charCol2"))

# Using strip string when spaces or wrong decimal separator is used
data_set &lt;- data.frame(charCol1 = c("1", "2", "3"),
                      charCol2 = c("4, 1", "5, 2", "6, 3"))

# Set charCol1 and charCol2 as numeric
set_col_as_numeric(data_set, cols = c("charCol1", "charCol2"))
# generate mistakes
set_col_as_numeric(data_set, cols = c("charCol1", "charCol2"), strip_string = TRUE)
# Doesn't generate any mistake (but is a bit slower)
</code></pre>

<hr>
<h2 id='shape_set'>Final preparation before ML algorithm</h2><span id='topic+shape_set'></span>

<h3>Description</h3>

<p>Prepare a data.table by:
</p>

<ul>
<li><p> transforming numeric variables into factors whenever they take less than <code>thresh</code> unique
variables
</p>
</li>
<li><p> transforming characters using <code><a href="#topic+generate_from_character">generate_from_character</a></code>
</p>
</li>
<li><p> transforming logical into binary integers
</p>
</li>
<li><p> dropping constant columns
</p>
</li>
<li><p> Sending the data.table to <code><a href="#topic+set_as_numeric_matrix">set_as_numeric_matrix</a></code> (when <code>final_form == "numerical_matrix"</code>)
will then allow you to get a numerical matrix usable by most Machine Learning Algorithms.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>shape_set(data_set, final_form = "data.table", thresh = 10, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="shape_set_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="shape_set_+3A_final_form">final_form</code></td>
<td>
<p>&quot;data.table&quot; or &quot;numerical_matrix&quot; (default to data.table)</p>
</td></tr>
<tr><td><code id="shape_set_+3A_thresh">thresh</code></td>
<td>
<p>Threshold such that  a numerical column is transformed into
a factor whenever its number of unique modalities is smaller or equal to
<code>thresh</code> (numeric, default to 10)</p>
</td></tr>
<tr><td><code id="shape_set_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk? (logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>All these changes will happen <strong>by reference</strong>.
</p>

<hr>
<h2 id='target_encode'>Target encode</h2><span id='topic+target_encode'></span>

<h3>Description</h3>

<p>Target encoding is the process of replacing a categorical value with the aggregation of the target variable.
the target variable. <code>target_encode</code> is used to apply this transformations on a data set.
Function <code><a href="#topic+build_target_encoding">build_target_encoding</a></code> must be used first to compute aggregations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>target_encode(data_set, target_encoding, drop = FALSE, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="target_encode_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="target_encode_+3A_target_encoding">target_encoding</code></td>
<td>
<p>result of function <code><a href="#topic+build_target_encoding">build_target_encoding</a></code> (list)</p>
</td></tr>
<tr><td><code id="target_encode_+3A_drop">drop</code></td>
<td>
<p>Should <code>col_to_encode</code> be dropped after generation (logical, default to FALSE)</p>
</td></tr>
<tr><td><code id="target_encode_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk? (Logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>data_set</code> with new cols of <code>target_encoding</code> merged to <code>data_set</code>
using <code>target_encoding</code> names as merging key. <code>data_set</code> is edited by <strong>reference</strong>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Build a data set
require(data.table)
data_set &lt;- data.table(student = c("Marie", "Marie", "Pierre", "Louis", "Louis"),
                      grades = c(1, 1, 2, 3, 4))

# Construct encoding
target_encoding &lt;- build_target_encoding(data_set, cols_to_encode = "student",
                                         target_col = "grades", functions = c("mean", "sum"))

# Apply them
target_encode(data_set, target_encoding = target_encoding)
</code></pre>

<hr>
<h2 id='tiny_messy_adult'>First 500 rows of <code><a href="#topic+messy_adult">messy_adult</a></code></h2><span id='topic+tiny_messy_adult'></span>

<h3>Description</h3>

<p>First 500 rows of <code><a href="#topic+messy_adult">messy_adult</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(tiny_messy_adult)
</code></pre>


<h3>Format</h3>

<p>A data.table with 500 rows and 24 variables.
</p>

<hr>
<h2 id='un_factor'>Unfactor factor with too many values</h2><span id='topic+un_factor'></span>

<h3>Description</h3>

<p>To un-factorize all columns that have more than a given amount of various values. This
function will be usefully after using some reading functions that put every string as factor.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>un_factor(data_set, cols = "auto", n_unfactor = 53, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="un_factor_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="un_factor_+3A_cols">cols</code></td>
<td>
<p>List of column(s) name(s) of data_set to look into. To check all all columns, set it
to &quot;auto&quot;. (characters, default to &quot;auto&quot;)</p>
</td></tr>
<tr><td><code id="un_factor_+3A_n_unfactor">n_unfactor</code></td>
<td>
<p>Number of max element in a factor (numeric, default to 53)</p>
</td></tr>
<tr><td><code id="un_factor_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk? (logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If a factor has (strictly) more than <code>n_unfactor</code> values it is un-factored. <br />
It is recommended to use <code><a href="#topic+find_and_transform_numerics">find_and_transform_numerics</a></code> and
<code><a href="#topic+find_and_transform_dates">find_and_transform_dates</a></code> after this function.<br />
If <code>n_unfactor</code> is set to -1, nothing will be performed. <br />
If there are a lot of column that have been transformed, you might want to look at the
documentation of your data reader in order to stop transforming everything into a factor.
</p>


<h3>Value</h3>

<p>Same data_set (as a data.table) with less factor columns.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Let's build a data_set
data_set &lt;- data.frame(true_factor = factor(rep(c(1,2), 13)),
                      false_factor = factor(LETTERS))

# Let's un factorize all factor that have more than 5 different values
data_set &lt;- un_factor(data_set, n_unfactor = 5)
sapply(data_set, class)
# Let's un factorize all factor that have more than 5 different values
data_set &lt;- un_factor(data_set, n_unfactor = 0)
sapply(data_set, class)

</code></pre>

<hr>
<h2 id='which_are_bijection'>Identify bijections</h2><span id='topic+which_are_bijection'></span>

<h3>Description</h3>

<p>Find all the columns that are bijections of another column.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>which_are_bijection(data_set, keep_cols = NULL, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="which_are_bijection_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="which_are_bijection_+3A_keep_cols">keep_cols</code></td>
<td>
<p>List of columns not to drop (list of character, default to NULL)</p>
</td></tr>
<tr><td><code id="which_are_bijection_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk (logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Bijection, meaning that there is another column containing the exact same information (but maybe
coded differently) for example col1: Men/Women, col2 M/W. <br />
This function is performing search by looking to every couple of columns.
It computes numbers of unique elements in each column, and number of unique tuples of values. <br />
Computation is made by exponential search, so that the function is faster. <br />
If <code>verbose</code> is TRUE, the column logged will be the one returned. <br />
Ex: if column i and column j (with j &gt; i) are bijections it will return j, expect if j is a
character then it return i.
</p>


<h3>Value</h3>

<p>A list of index of columns that have an exact bijection in the data_set set.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># First let's get a data set
data("adult")

# Now let's check which columns are equals
which_are_in_double(adult)
# It doesn't give any result.

# Let's look of bijections
which_are_bijection(adult)
# Return education_num index because education_num and education which
# contain the same info
</code></pre>

<hr>
<h2 id='which_are_constant'>Identify constant columns</h2><span id='topic+which_are_constant'></span>

<h3>Description</h3>

<p>Find all the columns that are constant.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>which_are_constant(data_set, keep_cols = NULL, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="which_are_constant_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="which_are_constant_+3A_keep_cols">keep_cols</code></td>
<td>
<p>List of columns not to drop (list of character, default to NULL)</p>
</td></tr>
<tr><td><code id="which_are_constant_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk (logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Algorithm is performing exponential search: it check constancy on row 1 to 10,
if it's not constant it stops, if it's constant then on 11 to 100 ... <br />
If you have a lot of columns than aren't constant, this function is way faster than a simple
<code>length(unique())</code>! The larger the data_set set is, the more interesting it is to use this function.
</p>


<h3>Value</h3>

<p>List of column's indexes that are constant in the data_set set.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Let's load our data_set
data(tiny_messy_adult)

# Let's try our function
which_are_constant(tiny_messy_adult)
# Indeed it return constant the name of the constant column.
</code></pre>

<hr>
<h2 id='which_are_in_double'>Identify double columns</h2><span id='topic+which_are_in_double'></span>

<h3>Description</h3>

<p>Find all the columns that are in double.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>which_are_in_double(data_set, keep_cols = NULL, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="which_are_in_double_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="which_are_in_double_+3A_keep_cols">keep_cols</code></td>
<td>
<p>List of columns not to drop (list of character, default to NULL)</p>
</td></tr>
<tr><td><code id="which_are_in_double_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk (logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is performing search by looking to every couple of columns. First it compares the
first 10 lines of both columns. If they are not equal then the columns aren't identical, else
it compares lines 11 to 100; then 101 to 1000... So this function is fast with data_set set
with a large number of lines and a lot of columns that aren't equals. <br />
If <code>verbose</code> is TRUE, the column logged will be the one returned.
</p>


<h3>Value</h3>

<p>A list of index of columns that have an exact duplicate in the data_set set.
Ex: if column i and column j (with j &gt; i) are equal it will return j.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># First let's build a matrix with 3 columns and a lot of lines, with 1's everywhere
M &lt;- matrix(1, nrow = 1e6, ncol = 3)

# Now let's check which columns are equals
which_are_in_double(M)
# It return 2 and 3: you should only keep column 1.

# Let's change the column 2, line 1 to 0. And check again
M[1, 2] &lt;- 0
which_are_in_double(M)
# It only returns 3

# What about NA? NA vs not NA =&gt; not equal
M[1, 2] &lt;- NA
which_are_in_double(M)
# It only returns 3

# What about NA?  Na vs NA =&gt; yep it's the same
M[1, 1] &lt;- NA
which_are_in_double(M)
# It only returns 2
</code></pre>

<hr>
<h2 id='which_are_included'>Identify columns that are included in others</h2><span id='topic+which_are_included'></span>

<h3>Description</h3>

<p>Find all the columns that don't contain more information than another column. For example if
you have a column with an amount and another with the same amount but rounded, the second
column is included in the first.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>which_are_included(data_set, keep_cols = NULL, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="which_are_included_+3A_data_set">data_set</code></td>
<td>
<p>Matrix, data.frame or data.table</p>
</td></tr>
<tr><td><code id="which_are_included_+3A_keep_cols">keep_cols</code></td>
<td>
<p>List of columns not to drop (list of character, default to NULL)</p>
</td></tr>
<tr><td><code id="which_are_included_+3A_verbose">verbose</code></td>
<td>
<p>Should the algorithm talk (logical, default to TRUE)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is performing exponential search and is looking to every couple of columns. <br />
Be very careful while using this function: <br />
- if there is an id column, it will say everything is included in the id column; <br />
- the order of columns will influence the result.<br />
<br />
For example if
you have a column with an amount and another with the same amount but rounded, the second
column is included in the first.<br />
<br />
And last but not least, with some machine learning algorithm it's not always smart to drop
columns even if they don't give more info: the extreme example is the id example.
</p>


<h3>Value</h3>

<p>A list of index of columns that have an exact duplicate in the <code>data_set</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load toy data set
require(data.table)
data(tiny_messy_adult)

# Check for included columns
which_are_included(tiny_messy_adult)

# Return columns that are also constant, double and bijection
# Let's add a truly just included column
tiny_messy_adult$are50OrMore &lt;- tiny_messy_adult$age &gt; 50
which_are_included(tiny_messy_adult[, .(age, are50OrMore)])

# As one can, see this column that doesn't have additional info than age is spotted.

# But you should be careful, if there is a column id, every column will be dropped:
tiny_messy_adult$id = seq_len(nrow(tiny_messy_adult)) # build id
which_are_included(tiny_messy_adult)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
