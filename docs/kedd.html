<!DOCTYPE html><html><head><title>Help for package kedd</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {kedd}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Claw, Bimodal, Kurtotic, Outlier, Trimodal'>
<p>Datasets</p></a></li>
<li><a href='#dkde'>
<p>Derivatives of Kernel Density Estimator</p></a></li>
<li><a href='#h.amise'>
<p>AMISE for Optimal Bandwidth Selectors</p></a></li>
<li><a href='#h.bcv'>
<p>Biased Cross-Validation for Bandwidth Selection</p></a></li>
<li><a href='#h.ccv'>
<p>Complete Cross-Validation for Bandwidth Selection</p></a></li>
<li><a href='#h.mcv'>
<p>Modified Cross-Validation for Bandwidth Selection</p></a></li>
<li><a href='#h.mlcv'>
<p>Maximum-Likelihood Cross-validation for Bandwidth Selection</p></a></li>
<li><a href='#h.tcv'>
<p>Trimmed Cross-Validation for Bandwidth Selection</p></a></li>
<li><a href='#h.ucv'>
<p>Unbiased (Least-Squares) Cross-Validation for Bandwidth Selection</p></a></li>
<li><a href='#kedd-package'>
<p>Kernel Estimator and Bandwidth Selection for Density and Its Derivatives</p></a></li>
<li><a href='#kernel.conv'>
<p>Convolutions of r'th Derivative for Kernel Function</p></a></li>
<li><a href='#kernel.fun'>
<p>Derivatives of Kernel Function</p></a></li>
<li><a href='#plot.dkde'>
<p>Plot for Kernel Density Derivative Estimate</p></a></li>
<li><a href='#plot.h.amise'>
<p>Plot for Asymptotic Mean Integrated Squared Error</p></a></li>
<li><a href='#plot.h.bcv'>
<p>Plot for Biased Cross-Validation</p></a></li>
<li><a href='#plot.h.ccv'>
<p>Plot for Complete Cross-Validation</p></a></li>
<li><a href='#plot.h.mcv'>
<p>Plot for Modified Cross-Validation</p></a></li>
<li><a href='#plot.h.mlcv'>
<p>Plot for Maximum-Likelihood Cross-validation</p></a></li>
<li><a href='#plot.h.tcv'>
<p>Plot for Trimmed Cross-Validation</p></a></li>
<li><a href='#plot.h.ucv'>
<p>Plot for Unbiased Cross-Validation</p></a></li>
<li><a href='#plot.kernel.conv'>
<p>Plot for Convolutions of r'th Derivative Kernel Function</p></a></li>
<li><a href='#plot.kernel.fun'>
<p>Plot of r'th Derivative Kernel Function</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.0.4</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Description:</td>
<td>Smoothing techniques and computing bandwidth selectors of the nth derivative of a probability density for one-dimensional data (described in Arsalane Chouaib Guidoum (2020) &lt;<a href="https://arxiv.org/abs/2012.06102">arXiv:2012.06102</a>&gt; [stat.CO]).</td>
</tr>
<tr>
<td>Title:</td>
<td>Kernel Estimator and Bandwidth Selection for Density and Its
Derivatives</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-01-27</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.15.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>nor1mix, ks, sm, locfit, orthopolynom</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://gitlab.com/iagogv/kedd">https://gitlab.com/iagogv/kedd</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://gitlab.com/iagogv/kedd/-/issues">https://gitlab.com/iagogv/kedd/-/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Classification/MSC:</td>
<td>62G05, 62G07, 65D10, 68N15</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-30 23:24:45 UTC; iago</td>
</tr>
<tr>
<td>Author:</td>
<td>Iago Giné-Vázquez <a href="https://orcid.org/0000-0002-6725-2638"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [cre],
  Arsalane Chouaib Guidoum [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Iago Giné-Vázquez &lt;iago.gin-vaz@protonmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-01 11:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='Claw+2C+20Bimodal+2C+20Kurtotic+2C+20Outlier+2C+20Trimodal'>
Datasets
</h2><span id='topic+claw'></span><span id='topic+bimodal'></span><span id='topic+kurtotic'></span><span id='topic+outlier'></span><span id='topic+trimodal'></span>

<h3>Description</h3>

<p>A random sample of size 200 from the claw, bimodal, kurtotic, outlier and trimodal Gaussian density.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(claw)
data(bimodal)
data(kurtotic)
data(outlier)
data(trimodal)
</code></pre>


<h3>Format</h3>

<p>Numeric vector with length 200.
</p>


<h3>Details</h3>

<p>Generate 200 random numbers, distributed according to a normal mixture, using 
<code><a href="nor1mix.html#topic+rnorMix">rnorMix</a></code> in package <a href="https://CRAN.R-project.org/package=nor1mix"><span class="pkg">nor1mix</span></a>.
</p>
<pre>
  ## Claw density
  claw &lt;- rnorMix(n=200, MW.nm10)
  plot(MW.nm10)
  
  ## Bimodal density
  bimodal &lt;- rnorMix(n=200, MW.nm7)
  plot( MW.nm7)
  
  ## Kurtotic density
  kurtotic &lt;- rnorMix(n=200, MW.nm4)
  plot(MW.nm4)
  
  ## Outlier density
  outlier &lt;- rnorMix(n=200, MW.nm5)
  plot( MW.nm5)
  
  ## Trimodal density
  trimodal &lt;- rnorMix(n=200, MW.nm9)
  plot(MW.nm9)
  </pre>   


<h3>Source</h3>

<p>Randomly generated a normal mixture with the function <code><a href="nor1mix.html#topic+rnorMix">rnorMix</a></code> in package <a href="https://CRAN.R-project.org/package=nor1mix"><span class="pkg">nor1mix</span></a>.
</p>


<h3>References</h3>

<p>Martin, M. (2013). 
<a href="https://CRAN.R-project.org/package=nor1mix"><span class="pkg">nor1mix</span></a>: Normal (1-d) mixture models (S3 classes and methods). 
<em><span class="rlang"><b>R</b></span> package version 1.1-4</em>.
</p>

<hr>
<h2 id='dkde'>
Derivatives of Kernel Density Estimator
</h2><span id='topic+dkde'></span><span id='topic+dkde.default'></span><span id='topic+print.dkde'></span>

<h3>Description</h3>

<p>The (S3) generic function <code>dkde</code> computes the r'th 
derivative of kernel density estimator for one-dimensional 
data. Its default method does so with the given kernel 
and bandwidth <code class="reqn">h</code> for one-dimensional observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dkde(x, ...)
## Default S3 method:
dkde(x, y = NULL, deriv.order = 0, h, kernel = c("gaussian", 
         "epanechnikov", "uniform", "triangular", "triweight", 
         "tricube", "biweight", "cosine"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dkde_+3A_x">x</code></td>
<td>
<p>the data from which the estimate is to be computed.</p>
</td></tr>
<tr><td><code id="dkde_+3A_y">y</code></td>
<td>
<p>the points of the grid at which the
density derivative is to be estimated; the defaults are <code class="reqn">\tau * h</code> outside
of range(<code class="reqn">x</code>), where <code class="reqn">\tau = 4</code>.</p>
</td></tr>
<tr><td><code id="dkde_+3A_deriv.order">deriv.order</code></td>
<td>
<p>derivative order (scalar).</p>
</td></tr>
<tr><td><code id="dkde_+3A_h">h</code></td>
<td>
<p>the smoothing bandwidth to be used, can also be a character 
string giving a rule to choose the bandwidth, see <code><a href="#topic+h.bcv">h.bcv</a></code>. The default <code><a href="#topic+h.ucv">h.ucv</a></code>.</p>
</td></tr>
<tr><td><code id="dkde_+3A_kernel">kernel</code></td>
<td>
<p>a character string giving the smoothing kernel to be used, with default
<code>"gaussian"</code>.</p>
</td></tr>
<tr><td><code id="dkde_+3A_...">...</code></td>
<td>
<p>further arguments for (non-default) methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A simple estimator for the density derivative can be obtained by taking the derivative
of the kernel density estimate. If the kernel <code class="reqn">K(x)</code> is differentiable <code class="reqn">r</code> times 
then the r'th density derivative estimate can be written as:
</p>
<p style="text-align: center;"><code class="reqn">\hat{f}^{(r)}_{h}(x)=\frac{1}{nh^{r+1}}\sum_{i=1}^{n} K^{(r)}\left(\frac{x-X_{i}}{h}\right)</code>
</p>

<p>where, </p>
<p style="text-align: center;"><code class="reqn">K^{(r)}(x) = \frac{d^{r}}{d x^{r}} K(x)</code>
</p>

<p>for <code class="reqn">r = 0, 1, 2, \dots</code>
</p>
<p>The following assumptions on the density <code class="reqn">f^{(r)}(x)</code>, the bandwidth <code class="reqn">h</code>, and the kernel <code class="reqn">K(x)</code>:
</p>

<ol>
<li><p> The <code class="reqn">(r+2)</code> derivative <code class="reqn">f^{(r+2)}(x)</code> is continuous, square integrable and ultimately monotone.
</p>
</li>
<li> <p><code class="reqn">\lim_{n \to \infty} h = 0</code> and <code class="reqn">\lim_{n \to \infty}n h^{2r+1} = \infty</code> i.e., as the number of samples <code class="reqn">n</code> is increased <code class="reqn">h</code> approaches zero at a rate slower than <code class="reqn">1/n^{2r+1}</code>.
</p>
</li>
<li> <p><code class="reqn">K(x) \geq 0</code> and <code class="reqn">\int_{R} K(x) dx = 1</code>. The kernel function is assumed to be symmetric about the origin i.e., <code class="reqn">\int_{R} xK^{(r)}(x) dx = 0</code> for even <code class="reqn">r</code> and has finite second moment i.e., <code class="reqn">\mu_{2}(K)=\int_{R}x^{2} K(x) dx &lt; \infty</code>.
</p>
</li></ol>

<p>Some theoretical properties of the estimator <code class="reqn">\hat{f}^{(r)}_{h}</code> have been investigated, among others, by Bhattacharya (1967), Schuster (1969). Let us now turn to the statistical properties of estimator. We are interested in the mean squared error since it combines squared bias and variance.  
</p>
<p>The <b>bias</b> can be written as:
</p>
<p style="text-align: center;"><code class="reqn">E\left[\hat{f}^{(r)}_{h}(x)\right]- f^{(r)}(x) = \frac{1}{2}h^{2}\mu_{2}(K) f^{(r+2)}(x)+o(h^{2})</code>
</p>

<p>The <b>variance</b> of the estimator can be written as:
</p>
<p style="text-align: center;"><code class="reqn">VAR\left[\hat{f}^{(r)}_{h}(x)\right]=\frac{f(x) R\left(K^{(r)}\right)}{nh^{2r+1}} + o(1/nh^{2r+1})</code>
</p>

<p>with, <code class="reqn">R\left(K^{(r)}\right) = \int_{R} \left(K^{(r)}(x)\right)^{2}dx.</code>
</p>
<p>The <b>MSE</b> (Mean Squared Error) for kernel density derivative estimators can be written as:
</p>
<p style="text-align: center;"><code class="reqn">MSE\left(\hat{f}^{(r)}_{h}(x),f^{(r)}(x)\right)=\frac{f(x)R\left(K^{(r)}\right)}{nh^{2r+1}}+\frac{1}{4}h^{4}\mu_{2}^{2}(K) f^{(r+1)}(x)^{2}+o(h^{4}+1/nh^{2r+1})</code>
</p>

<p>It follows that the MSE-optimal bandwidth for estimating <code class="reqn">\hat{f}^{(r)}_{h}S(x)</code>, is of order <code class="reqn">n^{-1/(2r+5)}</code>. Therefore, 
the estimation of <code class="reqn">\hat{f}^{(1)}_{h}(x)</code> requires a bandwidth of order <code class="reqn">n^{-1/7}</code> compared to the optimal <code class="reqn">n^{-1/5}</code> 
for estimating <code class="reqn">f(x)</code> itself. It reveals the increasing difficulty in problems of estimating higher derivatives.<br />
</p>
<p>The <b>MISE</b> (Mean Integrated Squared Error) can be written as:
</p>
<p style="text-align: center;"><code class="reqn">MISE\left(\hat{f}^{(r)}_{h}(x),f^{(r)}(x)\right)=AMISE\left(\hat{f}^{(r)}_{h}(x),f^{(r)}(x)\right)+o(h^{4}+1/nh^{2r+1})</code>
</p>

<p>where,
</p>
<p style="text-align: center;"><code class="reqn">AMISE\left(\hat{f}^{(r)}_{h}(x),f^{(r)}(x)\right)=\frac{1}{nh^{2r+1}}R\left(K^{(r)}\right)+\frac{1}{4}h^{4}\mu_{2}^{2}(K)R\left(f^{(r+2)}\right)</code>
</p>

<p>with: <code class="reqn">R\left(f^{(r)}(x)\right) = \int_{R} \left(f^{(r)}(x)\right)^{2}dx.</code><br />
The performance of kernel is measured by <b>MISE</b> or <b>AMISE</b> (Asymptotic MISE).<br />  
</p>
<p>If the bandwidth <code>h</code> is missing from <code>dkde</code>, then the default bandwidth is 
<code>h.ucv(x,deriv.order,kernel)</code> (Unbiased cross-validation, see <code><a href="#topic+h.ucv">h.ucv</a></code>).<br />
For more details see references.
</p>


<h3>Value</h3>

<table>
<tr><td><code>x</code></td>
<td>
<p>data points - same as input.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>the deparsed name of the <code>x</code> argument.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>the sample size after elimination of missing values.</p>
</td></tr>
<tr><td><code>kernel</code></td>
<td>
<p>name of kernel to use.</p>
</td></tr>
<tr><td><code>deriv.order</code></td>
<td>
<p>the derivative order to use.</p>
</td></tr>
<tr><td><code>h</code></td>
<td>
<p>the bandwidth value to use.</p>
</td></tr>
<tr><td><code>eval.points</code></td>
<td>
<p>the coordinates of the points where the 
density derivative is estimated.</p>
</td></tr>
<tr><td><code>est.fx</code></td>
<td>
<p>the estimated density derivative values.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function are available in other packages such as <a href="https://CRAN.R-project.org/package=KernSmooth"><span class="pkg">KernSmooth</span></a>, <a href="https://CRAN.R-project.org/package=sm"><span class="pkg">sm</span></a>, 
<a href="https://CRAN.R-project.org/package=np"><span class="pkg">np</span></a>, <a href="https://CRAN.R-project.org/package=GenKern"><span class="pkg">GenKern</span></a> and <a href="https://CRAN.R-project.org/package=locfit"><span class="pkg">locfit</span></a> if <code>deriv.order=0</code>, and in <a href="https://CRAN.R-project.org/package=ks"><span class="pkg">ks</span></a> package 
for Gaussian kernel only if <code>0 &lt;= deriv.order &lt;= 10</code>.
</p>


<h3>Author(s)</h3>

<p>Arsalane Chouaib Guidoum <a href="mailto:acguidoum@usthb.dz">acguidoum@usthb.dz</a>
</p>


<h3>References</h3>

<p>Alekseev, V. G. (1972).
Estimation of a probability density function and its derivatives.
<em>Mathematical notes of the Academy of Sciences of the USSR</em>. <b>12</b> (5), 808&ndash;811.
</p>
<p>Alexandre, B. T. (2009).
<em>Introduction to Nonparametric Estimation</em>.
Springer-Verlag, New York.
</p>
<p>Bowman, A. W. and Azzalini, A. (1997). 
<em>Applied Smoothing Techniques for
Data Analysis: the Kernel Approach with 
S-Plus Illustrations</em>.
Oxford University Press, Oxford.
</p>
<p>Bhattacharya, P. K. (1967).
Estimation of a probability density function and Its derivatives.
<em>Sankhya: The Indian Journal of Statistics, Series A</em>, <b>29</b>, 373&ndash;382.  
</p>
<p>Jeffrey, S. S. (1996).
<em>Smoothing Methods in Statistics</em>.
Springer-Verlag, New York.
</p>
<p>Radhey, S. S. (1987).
MISE of kernel estimates of a density and its derivatives.
<em>Statistics and Probability Letters</em>, <b>5</b>, 153&ndash;159.
</p>
<p>Scott, D. W. (1992).
<em>Multivariate Density Estimation. Theory, Practice and Visualization</em>.
New York: Wiley.
</p>
<p>Schuster, E. F. (1969) 
Estimation of a probability density function and its derivatives. 
<em>The Annals of Mathematical Statistics</em>, <b>40</b> (4), 1187&ndash;1195.
</p>
<p>Silverman, B. W. (1986).
<em>Density Estimation for Statistics and Data Analysis</em>.
Chapman &amp; Hall/CRC. London.
</p>
<p>Stoker, T. M. (1993).
Smoothing bias in density derivative estimation. 
<em>Journal of the American Statistical Association</em>, <b>88</b>, 855&ndash;863.
</p>
<p>Venables, W. N. and Ripley, B. D. (2002).
<em>Modern Applied Statistics with S</em>.
New York: Springer.
</p>
<p>Wand, M. P. and Jones, M. C. (1995).
<em>Kernel Smoothing</em>.
Chapman and Hall, London.
</p>
<p>Wolfgang, H. (1991).
<em>Smoothing Techniques</em>, 
<em>With Implementation in S</em>.
Springer-Verlag, New York.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.dkde">plot.dkde</a></code>, see <code><a href="stats.html#topic+density">density</a></code> in package &quot;stats&quot; if <code>deriv.order = 0</code>, and <code><a href="ks.html#topic+kdde">kdde</a></code> in package <a href="https://CRAN.R-project.org/package=ks"><span class="pkg">ks</span></a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## EXAMPLE 1:  Simple example of a Gaussian density derivative

x &lt;- rnorm(100)
dkde(x,deriv.order=0)  ## KDE of f
dkde(x,deriv.order=1)  ## KDDE of d/dx f
dkde(x,deriv.order=2)  ## KDDE of d^2/x^2 f
dkde(x,deriv.order=3)  ## KDDE of d^3/x^3 f
oldpar &lt;- par(no.readonly = TRUE)
dev.new()
par(mfrow=c(2,2))
plot(dkde(x,deriv.order=0))
plot(dkde(x,deriv.order=1))
plot(dkde(x,deriv.order=2))
plot(dkde(x,deriv.order=3))
par(oldpar)

## EXAMPLE 2: Bimodal Gaussian density derivative
## show the kernels in the dkde parametrization

fx  &lt;- function(x) 0.5 * dnorm(x,-1.5,0.5) + 0.5 * dnorm(x,1.5,0.5)
fx1 &lt;- function(x) 0.5 *(-4*x-6)* dnorm(x,-1.5,0.5) + 0.5 *(-4*x+6) * 
                   dnorm(x,1.5,0.5)
				   
## 'h = 0.3' ; 'Derivative order = 0'

kernels &lt;- eval(formals(dkde.default)$kernel)
dev.new()
plot(dkde(bimodal,h=0.3),sub=paste("Derivative order = 0",";",
     "Bandwidth =0.3 "),ylim=c(0,0.5), main = "Bimodal Gaussian Density")
for(i in 2:length(kernels))
   lines(dkde(bimodal, h = 0.3, kernel =  kernels[i]), col = i)
curve(fx,add=TRUE,lty=8)
legend("topright", legend = c(TRUE,kernels), col = c("black",seq(kernels)),
          lty = c(8,rep(1,length(kernels))),cex=0.7, inset = .015)
	   
## 'h = 0.6' ; 'Derivative order = 1'

kernels &lt;- eval(formals(dkde.default)$kernel)[-3]
dev.new()
plot(dkde(bimodal,deriv.order=1,h=0.6),main = "Bimodal Gaussian Density Derivative",sub=paste
         ("Derivative order = 1",";","Bandwidth =0.6"),ylim=c(-0.6,0.6))
for(i in 2:length(kernels))
   lines(dkde(bimodal,deriv.order=1, h = 0.6, kernel =  kernels[i]), col = i)
curve(fx1,add=TRUE,lty=8)
legend("topright", legend = c(TRUE,kernels), col = c("black",seq(kernels)),
          lty = c(8,rep(1,length(kernels))),cex=0.7, inset = .015)
</code></pre>

<hr>
<h2 id='h.amise'>
AMISE for Optimal Bandwidth Selectors
</h2><span id='topic+h.amise'></span><span id='topic+h.amise.default'></span><span id='topic+print.h.amise'></span>

<h3>Description</h3>

<p>The (S3) generic function <code>h.amise</code> evaluates the asymptotic 
mean integrated squared error <b>AMISE</b> for optimal smoothing
parameters <code class="reqn">h</code> of r'th derivative of kernel density 
estimator one-dimensional.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>h.amise(x, ...)
## Default S3 method:
h.amise(x, deriv.order = 0, lower = 0.1 * hos, upper = 2 * hos, 
         tol = 0.1 * lower, kernel = c("gaussian", "epanechnikov", "triweight", 
         "tricube", "biweight", "cosine"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="h.amise_+3A_x">x</code></td>
<td>
<p>vector of data values.</p>
</td></tr>
<tr><td><code id="h.amise_+3A_deriv.order">deriv.order</code></td>
<td>
<p>derivative order (scalar).</p>
</td></tr>
<tr><td><code id="h.amise_+3A_lower">lower</code>, <code id="h.amise_+3A_upper">upper</code></td>
<td>
<p>range over which to minimize. The default is
almost always satisfactory. <code>hos</code> (Over-smoothing) is calculated internally
from an <code>kernel</code>, see details.</p>
</td></tr>
<tr><td><code id="h.amise_+3A_tol">tol</code></td>
<td>
<p>the convergence tolerance for <code><a href="stats.html#topic+optimize">optimize</a></code>.</p>
</td></tr>
<tr><td><code id="h.amise_+3A_kernel">kernel</code></td>
<td>
<p>a character string giving the smoothing kernel to be used, with default
<code>"gaussian"</code>.</p>
</td></tr>
<tr><td><code id="h.amise_+3A_...">...</code></td>
<td>
<p>further arguments for (non-default) methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>h.amise</code> asymptotic mean integrated squared error implements for choosing 
the optimal bandwidth <code class="reqn">h</code> of a r'th derivative kernel density estimator.<br />
</p>
<p>We Consider the following AMISE version of the r'th derivative of <code class="reqn">f</code> the r'th 
derivative of the kernel estimate (see Scott 1992, pp 131):
</p>
<p style="text-align: center;"><code class="reqn">AMISE(h;r)= \frac{R\left(K^{(r)}\right)}{nh^{2r+1}} + \frac{1}{4} h^{4} \mu_{2}^{2}(K) R\left(f^{(r+2)}\right)</code>
</p>

<p>The optimal bandwidth minimizing this function is:
</p>
<p style="text-align: center;"><code class="reqn">h_{(r)}^{\ast} = \left[\frac{(2r+1)R\left(K^{(r)}\right)}{\mu_{2}^{2}(K) R\left(f^{(r+2)}\right)}\right]^{1/(2r+5)} n^{-1/(2r+5)}</code>
</p>

<p>whereof
</p>
<p style="text-align: center;"><code class="reqn">\inf_{h &gt; 0} AMISE(h;r) = \frac{2r+5}{4} R\left(K^{(r)}\right)^{\frac{4}{(2r+5)}} \left[ \frac{\mu_{2}^{2}(K)R\left(f^{(r+2)}\right)}{2r+1} \right]^{\frac{2r+1}{2r+5}} n^{-\frac{4}{2r+5}}</code>
</p>

<p>which is the smallest possible AMISE for estimation of <code class="reqn">f^{(r)}(x)</code> using the kernel <code class="reqn">K(x)</code>, 
where <code class="reqn">R\left(K^{(r)}\right) = \int_{R} K^{(r)}(x)^{2} dx</code> and <code class="reqn">\mu_{2}(K) = \int_{R}x^{2} K(x) dx</code>.<br />
</p>
<p>The range over which to minimize is <code>hos</code> Oversmoothing bandwidth, the default is almost always 
satisfactory. See George and Scott (1985), George (1990), Scott (1992, pp 165), Wand and Jones (1995, pp 61).
</p>


<h3>Value</h3>

<table>
<tr><td><code>x</code></td>
<td>
<p>data points - same as input.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>the deparsed name of the <code>x</code> argument.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>the sample size after elimination of missing values.</p>
</td></tr>
<tr><td><code>kernel</code></td>
<td>
<p>name of kernel to use</p>
</td></tr>
<tr><td><code>deriv.order</code></td>
<td>
<p>the derivative order to use.</p>
</td></tr>
<tr><td><code>h</code></td>
<td>
<p>value of bandwidth parameter.</p>
</td></tr>
<tr><td><code>amise</code></td>
<td>
<p>the AMISE value.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Arsalane Chouaib Guidoum <a href="mailto:acguidoum@usthb.dz">acguidoum@usthb.dz</a>
</p>


<h3>References</h3>

<p>Bowman, A. W. and Azzalini, A. (1997).
<em>Applied Smoothing Techniques for
Data Analysis: the Kernel Approach with 
S-Plus Illustrations</em>.
Oxford University Press, Oxford.
</p>
<p>Radhey, S. S. (1987).
MISE of kernel estimates of a density and its derivatives.
<em>Statistics and Probability Letters</em>, <b>5</b>, 153&ndash;159.
</p>
<p>Scott, D. W. (1992).
<em>Multivariate Density Estimation. Theory, Practice and Visualization</em>.
New York: Wiley.
</p>
<p>Sheather, S. J. (2004).
Density estimation.
<em>Statistical Science</em>, <b>19</b>, 588&ndash;597.
</p>
<p>Silverman, B. W. (1986).
<em>Density Estimation for Statistics and Data Analysis</em>.
Chapman &amp; Hall/CRC. London.
</p>
<p>Wand, M. P. and Jones, M. C. (1995).
<em>Kernel Smoothing</em>.
Chapman and Hall, London.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.h.amise">plot.h.amise</a></code>, see <code><a href="sm.html#topic+nmise">nmise</a></code> in package <a href="https://CRAN.R-project.org/package=sm"><span class="pkg">sm</span></a> this function 
evaluates the mean integrated squared error of a density estimate (<code>deriv.order = 0</code>) 
which is constructed from data which follow a normal distribution.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Derivative order = 0

h.amise(kurtotic,deriv.order = 0)

## Derivative order = 1

h.amise(kurtotic,deriv.order = 1)
</code></pre>

<hr>
<h2 id='h.bcv'>
Biased Cross-Validation for Bandwidth Selection
</h2><span id='topic+h.bcv'></span><span id='topic+h.bcv.default'></span><span id='topic+print.h.bcv'></span>

<h3>Description</h3>

<p>The (S3) generic function <code>h.bcv</code> computes the biased 
cross-validation bandwidth selector of r'th derivative of 
kernel density estimator one-dimensional.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>h.bcv(x, ...)
## Default S3 method:
h.bcv(x, whichbcv = 1, deriv.order = 0, lower = 0.1 * hos, upper = 2 * hos, 
         tol = 0.1 * lower, kernel = c("gaussian","epanechnikov",
         "triweight","tricube","biweight","cosine"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="h.bcv_+3A_x">x</code></td>
<td>
<p>vector of data values.</p>
</td></tr>
<tr><td><code id="h.bcv_+3A_whichbcv">whichbcv</code></td>
<td>
<p>method selected, <code>1 = BCV1</code> or <code>2 = BCV2</code>, see details.</p>
</td></tr>
<tr><td><code id="h.bcv_+3A_deriv.order">deriv.order</code></td>
<td>
<p>derivative order (scalar).</p>
</td></tr>
<tr><td><code id="h.bcv_+3A_lower">lower</code>, <code id="h.bcv_+3A_upper">upper</code></td>
<td>
<p>range over which to minimize. The default is
almost always satisfactory. <code>hos</code> (Over-smoothing) is calculated internally
from an <code>kernel</code>, see details.</p>
</td></tr>
<tr><td><code id="h.bcv_+3A_tol">tol</code></td>
<td>
<p>the convergence tolerance for <code><a href="stats.html#topic+optimize">optimize</a></code>.</p>
</td></tr>
<tr><td><code id="h.bcv_+3A_kernel">kernel</code></td>
<td>
<p>a character string giving the smoothing kernel to be used, with default
<code>"gaussian"</code>.</p>
</td></tr>
<tr><td><code id="h.bcv_+3A_...">...</code></td>
<td>
<p>further arguments for (non-default) methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>h.bcv</code> biased cross-validation implements for choosing the bandwidth <code class="reqn">h</code> of a 
r'th derivative kernel density estimator. if <code>whichbcv = 1</code> then <b>BCV1</b> is selected
(Scott and George 1987), and if <code>whichbcv = 2</code> used <b>BCV2</b> (Jones and Kappenman 1991).<br />
</p>
<p>Scott and George (1987) suggest a method which has as its immediate target the <b>AMISE</b>
(e.g. Silverman 1986, section 3.3). We denote <code class="reqn">\hat{\theta}_{r}(h)</code> and 
<code class="reqn">\bar{\theta}_{r}(h)</code> (Peter and Marron 1987, Jones and Kappenman 1991) by:

</p>
<p style="text-align: center;"><code class="reqn">\hat{\theta}_{r}(h)= \frac{(-1)^{r}}{n(n-1)h^{2r+1}} \sum_{i=1}^{n} \sum_{j=1;j \neq i}^{n} K^{(r)} \ast K^{(r)} \left(\frac{X_{j}-X_{i}}{h}\right)</code>
</p>

<p>and

</p>
<p style="text-align: center;"><code class="reqn">\bar{\theta}_{r}(h)= \frac{(-1)^r}{n(n-1) h^{2r+1}} \sum_{i=1}^{n} \sum_{j=1;j \neq i}^{n} K^{(2r)} \left(\frac{X_{j}-X_{i}}{h}\right)</code>
</p>

<p>Scott and George (1987) proposed using <code class="reqn">\hat{\theta}_{r}(h)</code> to estimate <code class="reqn">f^{(r)}(x)</code>. 
Thus, <code class="reqn">\hat{h}^{(r)}_{BCV1}</code>, say, is the <code class="reqn">h</code> that minimises:
</p>
<p style="text-align: center;"><code class="reqn">BCV1(h;r)= \frac{R\left(K^{(r)}\right)}{nh^{2r+1}} + \frac{1}{4} \mu_{2}^{2}(K) h^{4} \hat{\theta}_{r+2}(h)</code>
</p>

<p>and we define <code class="reqn">\hat{h}^{(r)}_{BCV2}</code> as the minimiser of (Jones and Kappenman 1991):
</p>
<p style="text-align: center;"><code class="reqn">BCV2(h;r)= \frac{R\left(K^{(r)}\right)}{nh^{2r+1}} + \frac{1}{4} \mu_{2}^{2}(K) h^{4} \bar{\theta}_{r+2}(h)</code>
</p>

<p>where <code class="reqn">K^{(r)} \ast K^{(r)} (x)</code> is the convolution of the r'th derivative kernel function <code class="reqn">K^{(r)}(x)</code>
(see <code><a href="#topic+kernel.conv">kernel.conv</a></code> and <code><a href="#topic+kernel.fun">kernel.fun</a></code>); <code class="reqn">R\left(K^{(r)}\right) = \int_{R} K^{(r)}(x)^{2} dx</code> and <code class="reqn">\mu_{2}(K) = \int_{R}x^{2} K(x) dx</code>.<br />
</p>
<p>The range over which to minimize is <code>hos</code> Oversmoothing bandwidth, the default is almost always 
satisfactory. See George and Scott (1985), George (1990), Scott (1992, pp 165), Wand and Jones (1995, pp 61).
</p>


<h3>Value</h3>

<table>
<tr><td><code>x</code></td>
<td>
<p>data points - same as input.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>the deparsed name of the <code>x</code> argument.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>the sample size after elimination of missing values.</p>
</td></tr>
<tr><td><code>kernel</code></td>
<td>
<p>name of kernel to use</p>
</td></tr>
<tr><td><code>deriv.order</code></td>
<td>
<p>the derivative order to use.</p>
</td></tr>
<tr><td><code>whichbcv</code></td>
<td>
<p>method selected.</p>
</td></tr>
<tr><td><code>h</code></td>
<td>
<p>value of bandwidth parameter.</p>
</td></tr>
<tr><td><code>min.bcv</code></td>
<td>
<p>the minimal BCV value.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Arsalane Chouaib Guidoum <a href="mailto:acguidoum@usthb.dz">acguidoum@usthb.dz</a>
</p>


<h3>References</h3>

<p>Jones, M. C. and Kappenman, R. F. (1991). 
On a class of kernel density estimate bandwidth selectors. 
<em>Scandinavian Journal of Statistics</em>, <b>19</b>, 337&ndash;349.
</p>
<p>Jones, M. C., Marron, J. S. and Sheather,S. J. (1996).
A brief survey of bandwidth selection for density estimation. 
<em>Journal of the American Statistical Association</em>, <b>91</b>, 401&ndash;407.
</p>
<p>Peter, H. and Marron, J.S. (1987).
Estimation of integrated squared density derivatives.
<em>Statistics and Probability Letters</em>, <b>6</b>, 109&ndash;115.
</p>
<p>Scott, D.W. and George, R. T. (1987).
Biased and unbiased cross-validation in density estimation. 
<em>Journal of the American Statistical Association</em>, <b>82</b>, 1131&ndash;1146.
</p>
<p>Sheather,S. J. (2004).
Density estimation.
<em>Statistical Science</em>, <b>19</b>, 588&ndash;597.
</p>
<p>Tarn, D. (2007).
<a href="https://CRAN.R-project.org/package=ks"><span class="pkg">ks</span></a>: Kernel density estimation and kernel discriminant
analysis for multivariate data in <span class="rlang"><b>R</b></span>.
<em>Journal of Statistical Software</em>, <b>21</b>(7), 1&ndash;16. 
</p>
<p>Wand, M. P. and Jones, M. C. (1995).
<em>Kernel Smoothing</em>.
Chapman and Hall, London.
</p>
<p>Wolfgang, H. (1991).
<em>Smoothing Techniques</em>, 
<em>With Implementation in S</em>.
Springer-Verlag, New York. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.h.bcv">plot.h.bcv</a></code>, see <code><a href="stats.html#topic+bw.bcv">bw.bcv</a></code> in package &quot;stats&quot; and 
<code><a href="MASS.html#topic+bcv">bcv</a></code> in package <a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a> for Gaussian kernel only if <code>deriv.order = 0</code>,
<code><a href="ks.html#topic+Hbcv">Hbcv</a></code> for bivariate data in package <a href="https://CRAN.R-project.org/package=ks"><span class="pkg">ks</span></a> for Gaussian kernel 
only if <code>deriv.order = 0</code>, <code><a href="locfit.html#topic+kdeb">kdeb</a></code> in package <a href="https://CRAN.R-project.org/package=locfit"><span class="pkg">locfit</span></a> 
if <code>deriv.order = 0</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## EXAMPLE 1:

x &lt;- rnorm(100)
h.bcv(x,whichbcv = 1, deriv.order = 0)
h.bcv(x,whichbcv = 2, deriv.order = 0)

## EXAMPLE 2:

## Derivative order = 0

h.bcv(kurtotic,deriv.order = 0)

## Derivative order = 1

h.bcv(kurtotic,deriv.order = 1)
</code></pre>

<hr>
<h2 id='h.ccv'>
Complete Cross-Validation for Bandwidth Selection
</h2><span id='topic+h.ccv'></span><span id='topic+h.ccv.default'></span><span id='topic+print.h.ccv'></span>

<h3>Description</h3>

<p>The (S3) generic function <code>h.ccv</code> computes the complete 
cross-validation bandwidth selector of r'th derivative of 
kernel density estimator one-dimensional.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>h.ccv(x, ...)
## Default S3 method:
h.ccv(x, deriv.order = 0, lower = 0.1 * hos, upper = hos, 
         tol = 0.1 * lower, kernel = c("gaussian", "triweight", 
         "tricube", "biweight", "cosine"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="h.ccv_+3A_x">x</code></td>
<td>
<p>vector of data values.</p>
</td></tr>
<tr><td><code id="h.ccv_+3A_deriv.order">deriv.order</code></td>
<td>
<p>derivative order (scalar).</p>
</td></tr>
<tr><td><code id="h.ccv_+3A_lower">lower</code>, <code id="h.ccv_+3A_upper">upper</code></td>
<td>
<p>range over which to minimize. The default is
almost always satisfactory. <code>hos</code> (Over-smoothing) is calculated internally
from an <code>kernel</code>, see details.</p>
</td></tr>
<tr><td><code id="h.ccv_+3A_tol">tol</code></td>
<td>
<p>the convergence tolerance for <code><a href="stats.html#topic+optimize">optimize</a></code>.</p>
</td></tr>
<tr><td><code id="h.ccv_+3A_kernel">kernel</code></td>
<td>
<p>a character string giving the smoothing kernel to be used, with default
<code>"gaussian"</code>.</p>
</td></tr>
<tr><td><code id="h.ccv_+3A_...">...</code></td>
<td>
<p>further arguments for (non-default) methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>h.ccv</code> complete cross-validation implements for choosing the bandwidth <code class="reqn">h</code> 
of a r'th derivative kernel density estimator.<br />
</p>
<p>Jones and Kappenman (1991) proposed a so-called complete cross-validation (CCV)
in kernel density estimator. This method can be extended to the estimation of 
derivative of the density, basing our estimate of integrated squared density 
derivative (Peter and Marron 1987) on the <code class="reqn">\bar{\theta}_{r}(h)</code>'s, 
we get the following, start from <code class="reqn">R\left(\hat{f}_{h}^{(r)}\right) - \bar{\theta}_{r}(h)</code> as an estimate 
of MISE. Thus, <code class="reqn">\hat{h}^{(r)}_{CCV}</code>, say, is the <code class="reqn">h</code> that minimises:
</p>
<p style="text-align: center;"><code class="reqn">CCV(h;r)=R\left(\hat{f}_{h}^{(r)}\right)-\bar{\theta}_{r}(h)+\frac{1}{2}\mu_{2}(K) h^{2} \bar{\theta}_{r+1}(h)+\frac{1}{24}\left(6\mu_{2}^{2}(K) -\delta(K)\right)h^{4}\bar{\theta}_{r+2}(h)</code>
</p>
 
<p>with 
</p>
<p style="text-align: center;"><code class="reqn">R\left(\hat{f}_{h}^{(r)}\right) = \int \left(\hat{f}_{h}^{(r)}(x)\right)^{2} dx = \frac{R\left(K^{(r)}\right)}{nh^{2r+1}} + \frac{(-1)^{r}}{n (n-1) h^{2r+1}} \sum_{i=1}^{n}\sum_{j=1;j \neq i}^{n} K^{(r)} \ast K^{(r)} \left(\frac{X_{j}-X_{i}}{h}\right)</code>
</p>

<p>and
</p>
<p style="text-align: center;"><code class="reqn">\bar{\theta}_{r}(h)= \frac{(-1)^r}{n(n-1) h^{2r+1}} \sum_{i=1}^{n} \sum_{j=1;j \neq i}^{n} K^{(2r)} \left(\frac{X_{j}-X_{i}}{h}\right)</code>
</p>

<p>and <code class="reqn">K^{(r)} \ast K^{(r)} (x)</code> is the convolution of the r'th derivative kernel function <code class="reqn">K^{(r)}(x)</code>
(see <code><a href="#topic+kernel.conv">kernel.conv</a></code> and <code><a href="#topic+kernel.fun">kernel.fun</a></code>); <code class="reqn">R\left(K^{(r)}\right) = \int_{R} K^{(r)}(x)^{2} dx</code> and 
<code class="reqn">\mu_{2}(K) = \int_{R}x^{2} K(x) dx</code>, <code class="reqn">\delta(K) = \int_{R}x^{4} K(x) dx</code>.<br />
</p>
<p>The range over which to minimize is <code>hos</code> Oversmoothing bandwidth, the default is almost always 
satisfactory. See George and Scott (1985), George (1990), Scott (1992, pp 165), Wand and Jones (1995, pp 61). 
</p>


<h3>Value</h3>

<table>
<tr><td><code>x</code></td>
<td>
<p>data points - same as input.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>the deparsed name of the <code>x</code> argument.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>the sample size after elimination of missing values.</p>
</td></tr>
<tr><td><code>kernel</code></td>
<td>
<p>name of kernel to use</p>
</td></tr>
<tr><td><code>deriv.order</code></td>
<td>
<p>the derivative order to use.</p>
</td></tr>
<tr><td><code>h</code></td>
<td>
<p>value of bandwidth parameter.</p>
</td></tr>
<tr><td><code>min.ccv</code></td>
<td>
<p>the minimal CCV value.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Arsalane Chouaib Guidoum <a href="mailto:acguidoum@usthb.dz">acguidoum@usthb.dz</a>
</p>


<h3>References</h3>

<p>Jones, M. C. and Kappenman, R. F. (1991). 
On a class of kernel density estimate bandwidth selectors. 
<em>Scandinavian Journal of Statistics</em>, <b>19</b>, 337&ndash;349.
</p>
<p>Peter, H. and Marron, J.S. (1987).
Estimation of integrated squared density derivatives.
<em>Statistics and Probability Letters</em>, <b>6</b>, 109&ndash;115.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.h.ccv">plot.h.ccv</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Derivative order = 0

h.ccv(kurtotic,deriv.order = 0)

## Derivative order = 1

h.ccv(kurtotic,deriv.order = 1)
</code></pre>

<hr>
<h2 id='h.mcv'>
Modified Cross-Validation for Bandwidth Selection
</h2><span id='topic+h.mcv'></span><span id='topic+h.mcv.default'></span><span id='topic+print.h.mcv'></span>

<h3>Description</h3>

<p>The (S3) generic function <code>h.mcv</code> computes the modified 
cross-validation bandwidth selector of r'th derivative of 
kernel density estimator one-dimensional.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>h.mcv(x, ...)
## Default S3 method:
h.mcv(x, deriv.order = 0, lower = 0.1 * hos, upper = 2 * hos, 
         tol = 0.1 * lower, kernel = c("gaussian", "epanechnikov", "triweight", 
         "tricube", "biweight", "cosine"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="h.mcv_+3A_x">x</code></td>
<td>
<p>vector of data values.</p>
</td></tr>
<tr><td><code id="h.mcv_+3A_deriv.order">deriv.order</code></td>
<td>
<p>derivative order (scalar).</p>
</td></tr>
<tr><td><code id="h.mcv_+3A_lower">lower</code>, <code id="h.mcv_+3A_upper">upper</code></td>
<td>
<p>range over which to minimize. The default is
almost always satisfactory. <code>hos</code> (Over-smoothing) is calculated internally
from an <code>kernel</code>, see details.</p>
</td></tr>
<tr><td><code id="h.mcv_+3A_tol">tol</code></td>
<td>
<p>the convergence tolerance for <code><a href="stats.html#topic+optimize">optimize</a></code>.</p>
</td></tr>
<tr><td><code id="h.mcv_+3A_kernel">kernel</code></td>
<td>
<p>a character string giving the smoothing kernel to be used, with default
<code>"gaussian"</code>.</p>
</td></tr>
<tr><td><code id="h.mcv_+3A_...">...</code></td>
<td>
<p>further arguments for (non-default) methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>h.mcv</code> modified cross-validation implements for choosing the bandwidth <code class="reqn">h</code> 
of a r'th derivative kernel density estimator.<br />
</p>
<p>Stute (1992) proposed a so-called modified cross-validation (MCV) in kernel 
density estimator. This method can be extended to the estimation of derivative 
of a density, the essential idea based on approximated the problematic term 
by the aid of the Hajek projection (see Stute 1992). The minimization criterion is defined by:
</p>
<p style="text-align: center;"><code class="reqn">MCV(h;r) = \frac{R\left(K^{(r)}\right)}{nh^{2r+1}} + \frac{(-1)^{r}}{n(n-1)h^{2r+1}}\sum_{i=1}^{n} \sum_{j=1;j \neq i}^{n} \varphi^{(r)} \left(\frac{X_{j}-X_{i}}{h}\right)</code>
</p>

<p>whit </p>
<p style="text-align: center;"><code class="reqn">\varphi^{(r)}(c) = \left(K^{(r)} \ast K^{(r)} - K^{(2r)} - \frac{\mu_{2}(K)}{2}K^{(2r+2)} \right)(c)</code>
</p>
 
<p>and <code class="reqn">K^{(r)} \ast K^{(r)} (x)</code> is the convolution of the r'th derivative kernel function <code class="reqn">K^{(r)}(x)</code>
(see <code><a href="#topic+kernel.conv">kernel.conv</a></code> and <code><a href="#topic+kernel.fun">kernel.fun</a></code>); <code class="reqn">R\left(K^{(r)}\right) = \int_{R} K^{(r)}(x)^{2} dx</code> and <code class="reqn">\mu_{2}(K) = \int_{R}x^{2} K(x) dx</code>.<br />
</p>
<p>The range over which to minimize is <code>hos</code> Oversmoothing bandwidth, the default is almost always 
satisfactory. See George and Scott (1985), George (1990), Scott (1992, pp 165), Wand and Jones (1995, pp 61). 
</p>


<h3>Value</h3>

<table>
<tr><td><code>x</code></td>
<td>
<p>data points - same as input.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>the deparsed name of the <code>x</code> argument.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>the sample size after elimination of missing values.</p>
</td></tr>
<tr><td><code>kernel</code></td>
<td>
<p>name of kernel to use</p>
</td></tr>
<tr><td><code>deriv.order</code></td>
<td>
<p>the derivative order to use.</p>
</td></tr>
<tr><td><code>h</code></td>
<td>
<p>value of bandwidth parameter.</p>
</td></tr>
<tr><td><code>min.mcv</code></td>
<td>
<p>the minimal MCV value.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Arsalane Chouaib Guidoum <a href="mailto:acguidoum@usthb.dz">acguidoum@usthb.dz</a>
</p>


<h3>References</h3>

<p>Heidenreich, N. B., Schindler, A. and Sperlich, S. (2013).
Bandwidth selection for kernel density estimation: 
a review of fully automatic selectors.
<em>Advances in Statistical Analysis</em>.
</p>
<p>Stute, W. (1992).
Modified cross validation in density estimation.
<em>Journal of Statistical Planning and Inference</em>, <b>30</b>, 293&ndash;305.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.h.mcv">plot.h.mcv</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Derivative order = 0

h.mcv(kurtotic,deriv.order = 0)

## Derivative order = 1

h.mcv(kurtotic,deriv.order = 1)

</code></pre>

<hr>
<h2 id='h.mlcv'>
Maximum-Likelihood Cross-validation for Bandwidth Selection
</h2><span id='topic+h.mlcv'></span><span id='topic+h.mlcv.default'></span><span id='topic+print.h.mlcv'></span>

<h3>Description</h3>

<p>The (S3) generic function <code>h.mlcv</code> computes the maximum 
likelihood cross-validation (Kullback-Leibler information) 
bandwidth selector of a one-dimensional kernel density estimate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>h.mlcv(x, ...)
## Default S3 method:
h.mlcv(x, lower = 0.1, upper = 5, tol = 0.1 * lower, 
         kernel = c("gaussian", "epanechnikov", "uniform", "triangular", 
         "triweight", "tricube", "biweight", "cosine"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="h.mlcv_+3A_x">x</code></td>
<td>
<p>vector of data values.</p>
</td></tr>
<tr><td><code id="h.mlcv_+3A_lower">lower</code>, <code id="h.mlcv_+3A_upper">upper</code></td>
<td>
<p>range over which to maximize. The default is
almost always satisfactory.</p>
</td></tr>
<tr><td><code id="h.mlcv_+3A_tol">tol</code></td>
<td>
<p>the convergence tolerance for <code><a href="stats.html#topic+optimize">optimize</a></code>.</p>
</td></tr>
<tr><td><code id="h.mlcv_+3A_kernel">kernel</code></td>
<td>
<p>a character string giving the smoothing kernel to be used, with default
<code>"gaussian"</code>.</p>
</td></tr>
<tr><td><code id="h.mlcv_+3A_...">...</code></td>
<td>
<p>further arguments for (non-default) methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>h.mlcv</code> maximum-likelihood cross-validation implements for choosing 
the optimal bandwidth <code class="reqn">h</code> of kernel density estimator.<br />
</p>
<p>This method was proposed by Habbema, Hermans, and Van den Broeck (1971) and by Duin (1976). The maximum-likelihood 
cross-validation (MLCV) function is defined by:
</p>
<p style="text-align: center;"><code class="reqn">MLCV(h) = n^{-1} \sum_{i=1}^{n} \log\left[\hat{f}_{h,i}(x)\right]</code>
</p>

<p>the estimate <code class="reqn">\hat{f}_{h,i}(x)</code> on the subset <code class="reqn">\{X_{j}\}_{j \neq i}</code>
denoting the leave-one-out estimator, can be written:
</p>
<p style="text-align: center;"><code class="reqn">\hat{f}_{h,i}(X_{i}) = \frac{1}{(n-1) h} \sum_{j \neq i} K \left(\frac{X_{j}-X_{i}}{h}\right)</code>
</p>

<p>Define that <code class="reqn">h_{mlcv}</code> as good which approaches the finite maximum of <code class="reqn">MLCV(h)</code>:
</p>
<p style="text-align: center;"><code class="reqn">h_{mlcv} = \arg \max_{h} MLCV(h) = \arg \max_{h} \left(n^{-1} \sum_{i=1}^{n} \log\left[\sum_{j \neq i} K \left(\frac{X_{j}-X_{i}}{h}\right)\right]-\log[(n-1)h]\right)</code>
</p>



<h3>Value</h3>

<table>
<tr><td><code>x</code></td>
<td>
<p>data points - same as input.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>the deparsed name of the <code>x</code> argument.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>the sample size after elimination of missing values.</p>
</td></tr>
<tr><td><code>kernel</code></td>
<td>
<p>name of kernel to use</p>
</td></tr>
<tr><td><code>h</code></td>
<td>
<p>value of bandwidth parameter.</p>
</td></tr>
<tr><td><code>mlcv</code></td>
<td>
<p>the maximal likelihood CV value.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Arsalane Chouaib Guidoum <a href="mailto:acguidoum@usthb.dz">acguidoum@usthb.dz</a>
</p>


<h3>References</h3>

<p>Habbema, J. D. F., Hermans, J., and Van den Broek, K. (1974)
A stepwise discrimination analysis program using density estimation. 
<em>Compstat 1974: Proceedings in Computational Statistics</em>. Physica Verlag, Vienna.
</p>
<p>Duin, R. P. W. (1976).
On the choice of smoothing parameters of Parzen estimators of probability density functions. 
<em>IEEE Transactions on Computers</em>, <b>C-25</b>, 1175&ndash;1179.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.h.mlcv">plot.h.mlcv</a></code>, see <code><a href="locfit.html#topic+lcv">lcv</a></code> in package <a href="https://CRAN.R-project.org/package=locfit"><span class="pkg">locfit</span></a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>h.mlcv(bimodal)
h.mlcv(bimodal, kernel ="epanechnikov")
</code></pre>

<hr>
<h2 id='h.tcv'>
Trimmed Cross-Validation for Bandwidth Selection
</h2><span id='topic+h.tcv'></span><span id='topic+h.tcv.default'></span><span id='topic+print.h.tcv'></span>

<h3>Description</h3>

<p>The (S3) generic function <code>h.tcv</code> computes the trimmed 
cross-validation bandwidth selector of r'th derivative of 
kernel density estimator one-dimensional.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>h.tcv(x, ...)
## Default S3 method:
h.tcv(x, deriv.order = 0, lower = 0.1 * hos, upper = 2 * hos, 
         tol = 0.1 * lower, kernel = c("gaussian", "epanechnikov", "uniform", 
         "triangular", "triweight", "tricube", "biweight", "cosine"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="h.tcv_+3A_x">x</code></td>
<td>
<p>vector of data values.</p>
</td></tr>
<tr><td><code id="h.tcv_+3A_deriv.order">deriv.order</code></td>
<td>
<p>derivative order (scalar).</p>
</td></tr>
<tr><td><code id="h.tcv_+3A_lower">lower</code>, <code id="h.tcv_+3A_upper">upper</code></td>
<td>
<p>range over which to minimize. The default is
almost always satisfactory. <code>hos</code> (Over-smoothing) is calculated internally
from an <code>kernel</code>, see details.</p>
</td></tr>
<tr><td><code id="h.tcv_+3A_tol">tol</code></td>
<td>
<p>the convergence tolerance for <code><a href="stats.html#topic+optimize">optimize</a></code>.</p>
</td></tr>
<tr><td><code id="h.tcv_+3A_kernel">kernel</code></td>
<td>
<p>a character string giving the smoothing kernel to be used, with default
<code>"gaussian"</code>.</p>
</td></tr>
<tr><td><code id="h.tcv_+3A_...">...</code></td>
<td>
<p>further arguments for (non-default) methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>h.tcv</code> trimmed cross-validation implements for choosing the bandwidth <code class="reqn">h</code> 
of a r'th derivative kernel density estimator.<br />
</p>
<p>Feluch and Koronacki (1992) proposed a so-called trimmed cross-validation (TCV) in kernel 
density estimator, a simple modification of the unbiased (least-squares)  cross-validation 
criterion. We consider the following &quot;trimmed&quot; version of &quot;unbiased&quot;, to be minimized with 
respect to <code class="reqn">h</code>:
</p>
<p style="text-align: center;"><code class="reqn">\int \left(\hat{f}_{h}^{(r)}(x)\right)^{2} - 2 \frac{(-1)^{r}}{n(n-1) h^{2r+1}} \sum_{i=1}^{n}\sum_{j=1; j \neq i} K^{(2r)} \left(\frac{X_{j}-X_{i}}{h}\right)\chi\left(|X_{i}-X_{j}| &gt; c_{n}\right)</code>
</p>

<p>where <code class="reqn">\chi(.)</code> denotes the indicator function and <code class="reqn">c_{n}</code> is a sequence of positive 
constants, <code class="reqn">c_{n}/ h^{2r+1} \rightarrow 0</code> as <code class="reqn">n \rightarrow \infty</code>, and 
</p>
<p style="text-align: center;"><code class="reqn">\int \left(\hat{f}_{h}^{(r)}(x)\right)^{2} = \frac{R\left(K^{(r)}\right)}{nh^{2r+1}} + \frac{(-1)^{r}}{n (n-1) h^{2r+1}} \sum_{i=1}^{n}\sum_{j=1;j \neq i}^{n} K^{(r)} \ast K^{(r)} \left(\frac{X_{j}-X_{i}}{h}\right)</code>
</p>

<p>the trimmed cross-validation function is defined by:
</p>
<p style="text-align: center;"><code class="reqn">TCV(h;r) = \frac{R\left(K^{(r)}\right)}{nh^{2r+1}} + \frac{(-1)^{r}}{n(n-1)h^{2r+1}}\sum_{i=1}^{n} \sum_{j=1;j \neq i}^{n} \varphi^{(r)} \left(\frac{X_{j}-X_{i}}{h}\right)</code>
</p>

<p>whit </p>
<p style="text-align: center;"><code class="reqn">\varphi^{(r)}(c) = \left(K^{(r)} \ast K^{(r)} - 2 K^{(2r)} \chi\left(|c| &gt; c_{n}/h^{2r+1}\right) \right)(c)</code>
</p>

<p>here we take <code class="reqn">c_{n} = 1/n</code>, for assure the convergence. Where <code class="reqn">K^{(r)} \ast K^{(r)} (x)</code> is the convolution of the r'th derivative kernel function <code class="reqn">K^{(r)}(x)</code>
(see <code><a href="#topic+kernel.conv">kernel.conv</a></code> and <code><a href="#topic+kernel.fun">kernel.fun</a></code>).<br />
</p>
<p>The range over which to minimize is <code>hos</code> Oversmoothing bandwidth, the default is almost always 
satisfactory. See George and Scott (1985), George (1990), Scott (1992, pp 165), Wand and Jones (1995, pp 61). 
</p>


<h3>Value</h3>

<table>
<tr><td><code>x</code></td>
<td>
<p>data points - same as input.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>the deparsed name of the <code>x</code> argument.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>the sample size after elimination of missing values.</p>
</td></tr>
<tr><td><code>kernel</code></td>
<td>
<p>name of kernel to use</p>
</td></tr>
<tr><td><code>deriv.order</code></td>
<td>
<p>the derivative order to use.</p>
</td></tr>
<tr><td><code>h</code></td>
<td>
<p>value of bandwidth parameter.</p>
</td></tr>
<tr><td><code>min.tcv</code></td>
<td>
<p>the minimal TCV value.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Arsalane Chouaib Guidoum <a href="mailto:acguidoum@usthb.dz">acguidoum@usthb.dz</a>
</p>


<h3>References</h3>

<p>Feluch, W. and Koronacki, J. (1992).
A note on modified cross-validation in density estimation.
<em>Computational Statistics and Data Analysis</em>, <b>13</b>, 143&ndash;151.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.h.tcv">plot.h.tcv</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Derivative order = 0

h.tcv(kurtotic,deriv.order = 0)

## Derivative order = 1

h.tcv(kurtotic,deriv.order = 1)
</code></pre>

<hr>
<h2 id='h.ucv'>
Unbiased (Least-Squares) Cross-Validation for Bandwidth Selection
</h2><span id='topic+h.ucv'></span><span id='topic+h.ucv.default'></span><span id='topic+print.h.ucv'></span>

<h3>Description</h3>

<p>The (S3) generic function <code>h.ucv</code> computes the unbiased 
(least-squares) cross-validation bandwidth selector
of r'th derivative of kernel density estimator one-dimensional.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>h.ucv(x, ...)
## Default S3 method:
h.ucv(x, deriv.order = 0, lower = 0.1 * hos, upper = 2 * hos, 
         tol = 0.1 * lower, kernel = c("gaussian", "epanechnikov", "uniform", 
         "triangular", "triweight", "tricube", "biweight", "cosine"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="h.ucv_+3A_x">x</code></td>
<td>
<p>vector of data values.</p>
</td></tr>
<tr><td><code id="h.ucv_+3A_deriv.order">deriv.order</code></td>
<td>
<p>derivative order (scalar).</p>
</td></tr>
<tr><td><code id="h.ucv_+3A_lower">lower</code>, <code id="h.ucv_+3A_upper">upper</code></td>
<td>
<p>range over which to minimize. The default is
almost always satisfactory. <code>hos</code> (Over-smoothing) is calculated internally
from an <code>kernel</code>, see details.</p>
</td></tr>
<tr><td><code id="h.ucv_+3A_tol">tol</code></td>
<td>
<p>the convergence tolerance for <code><a href="stats.html#topic+optimize">optimize</a></code>.</p>
</td></tr>
<tr><td><code id="h.ucv_+3A_kernel">kernel</code></td>
<td>
<p>a character string giving the smoothing kernel to be used, with default
<code>"gaussian"</code>.</p>
</td></tr>
<tr><td><code id="h.ucv_+3A_...">...</code></td>
<td>
<p>further arguments for (non-default) methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>h.ucv</code> unbiased (least-squares) cross-validation implements for choosing the bandwidth <code class="reqn">h</code> 
of a r'th derivative kernel density estimator.<br />
</p>
<p>Rudemo (1982) and Bowman (1984) proposed a so-called unbiased (least-squares)  cross-validation 
(UCV) in kernel density estimator. An adaptation of unbiased cross-validation is proposed by 
Wolfgang et al. (1990) for bandwidth choice in the r'th derivative of kernel density estimator. 
The essential idea of this methods, for the estimation of <code class="reqn">f^{(r)}(x)</code> (<code class="reqn">r</code> is derivative order), 
is to use the bandwidth <code class="reqn">h</code> which minimizes the function:
</p>
<p style="text-align: center;"><code class="reqn">UCV(h;r) = \int \left(\hat{f}_{h}^{(r)}(x)\right)^{2} - 2n^{-1}(-1)^{r}\sum_{i=1}^{n} \hat{f}_{h,i}^{(2r)}(X_{i})</code>
</p>

<p>The bandwidth minimizing this function is:
</p>
<p style="text-align: center;"><code class="reqn">\hat{h}^{(r)}_{ucv} = \arg \min_{h^{(r)}} UCV(h;r)</code>
</p>

<p>for <code class="reqn">r = 0, 1, 2, \dots</code><br />
where  
</p>
<p style="text-align: center;"><code class="reqn">\int \left(\hat{f}_{h}^{(r)}(x)\right)^{2} = \frac{R\left(K^{(r)}\right)}{nh^{2r+1}} + \frac{(-1)^{r}}{n (n-1) h^{2r+1}} \sum_{i=1}^{n}\sum_{j=1;j \neq i}^{n} K^{(r)} \ast K^{(r)} \left(\frac{X_{j}-X_{i}}{h}\right)</code>
</p>

<p>and <code class="reqn">K^{(r)} \ast K^{(r)} (x)</code> is the convolution of the r'th derivative kernel function <code class="reqn">K^{(r)}(x)</code>
(see <code><a href="#topic+kernel.conv">kernel.conv</a></code> and <code><a href="#topic+kernel.fun">kernel.fun</a></code>).<br />
The estimate <code class="reqn">\hat{f}_{h,i}^{(2r)}(x)</code> on the subset <code class="reqn">\{X_{j}\}_{j \neq i}</code>
denoting the leave-one-out estimator, can be written:
</p>
<p style="text-align: center;"><code class="reqn">\hat{f}_{h,i}^{(2r)}(X_{i}) = \frac{1}{(n-1) h^{2r+1}} \sum_{j \neq i} K^{(2r)} \left(\frac{X_{j}-X_{i}}{h}\right)</code>
</p>

<p>The function <code class="reqn">UCV(h;r)</code> is unbiased cross-validation in the sense that <code class="reqn">E[UCV]=MISE[\hat{f}_{h}^{(r)}(x)]-R(f^{(r)}(x))</code>
(see, Scott and George 1987). Can be simplified to give the computationally:
</p>
<p style="text-align: center;"><code class="reqn">UCV(h;r) = \frac{R\left(K^{(r)}\right)}{nh^{2r+1}} + \frac{(-1)^{r}}{n (n-1) h^{2r+1}} \sum_{i=1}^{n}\sum_{j=1 ;j \neq i}^{n} \left(K^{(r)} \ast K^{(r)} -2K^{(2r)}\right) \left(\frac{X_{j}-X_{i}}{h}\right)</code>
</p>

<p>where <code class="reqn">R\left(K^{(r)}\right) = \int_{R} K^{(r)}(x)^{2} dx</code>.<br /> 
</p>
<p>The range over which to minimize is <code>hos</code> Oversmoothing bandwidth, the default is almost always 
satisfactory. See George and Scott (1985), George (1990), Scott (1992, pp 165), Wand and Jones (1995, pp 61).    
</p>


<h3>Value</h3>

<table>
<tr><td><code>x</code></td>
<td>
<p>data points - same as input.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>the deparsed name of the <code>x</code> argument.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>the sample size after elimination of missing values.</p>
</td></tr>
<tr><td><code>kernel</code></td>
<td>
<p>name of kernel to use</p>
</td></tr>
<tr><td><code>deriv.order</code></td>
<td>
<p>the derivative order to use.</p>
</td></tr>
<tr><td><code>h</code></td>
<td>
<p>value of bandwidth parameter.</p>
</td></tr>
<tr><td><code>min.ucv</code></td>
<td>
<p>the minimal UCV value.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Arsalane Chouaib Guidoum <a href="mailto:acguidoum@usthb.dz">acguidoum@usthb.dz</a>
</p>


<h3>References</h3>

  
<p>Bowman, A. (1984). 
An alternative method of cross-validation for the smoothing 
of kernel density estimates. 
<em>Biometrika</em>, <b>71</b>, 353&ndash;360.
</p>
<p>Jones, M. C. and Kappenman, R. F. (1991). 
On a class of kernel density estimate bandwidth selectors. 
<em>Scandinavian Journal of Statistics</em>, <b>19</b>, 337&ndash;349.
</p>
<p>Jones, M. C., Marron, J. S. and Sheather,S. J. (1996).
A brief survey of bandwidth selection for density estimation. 
<em>Journal of the American Statistical Association</em>, <b>91</b>, 401&ndash;407.
</p>
<p>Peter, H. and Marron, J.S. (1987).
Estimation of integrated squared density derivatives.
<em>Statistics and Probability Letters</em>, <b>6</b>, 109&ndash;115.
</p>
<p>Rudemo, M. (1982). 
Empirical choice of histograms and kernel density estimators. 
<em>Scandinavian Journal of Statistics</em>, <b>9</b>, 65&ndash;78. 
</p>
<p>Scott, D.W. and George, R. T. (1987).
Biased and unbiased cross-validation in density estimation. 
<em>Journal of the American Statistical Association</em>, <b>82</b>, 1131&ndash;1146.  
</p>
<p>Sheather, S. J. (2004).
Density estimation.
<em>Statistical Science</em>, <b>19</b>, 588&ndash;597.
</p>
<p>Tarn, D. (2007).
<a href="https://CRAN.R-project.org/package=ks"><span class="pkg">ks</span></a>: Kernel density estimation and kernel discriminant
analysis for multivariate data in <span class="rlang"><b>R</b></span>.
<em>Journal of Statistical Software</em>, <b>21</b>(7), 1&ndash;16. 
</p>
<p>Wand, M. P. and Jones, M. C. (1995).
<em>Kernel Smoothing</em>.
Chapman and Hall, London.
</p>
<p>Wolfgang, H. (1991).
<em>Smoothing Techniques</em>, 
<em>With Implementation in S</em>.
Springer-Verlag, New York. 
</p>
<p>Wolfgang, H., Marron, J. S. and Wand, M. P. (1990).
Bandwidth choice for density derivatives.
<em>Journal of the Royal Statistical Society, Series B</em>, 223&ndash;232. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.h.ucv">plot.h.ucv</a></code>, see <code><a href="stats.html#topic+bw.ucv">bw.ucv</a></code> in package &quot;stats&quot; and 
<code><a href="MASS.html#topic+ucv">ucv</a></code> in package <a href="https://CRAN.R-project.org/package=MASS"><span class="pkg">MASS</span></a> for Gaussian kernel only if <code>deriv.order = 0</code>,
<code><a href="ks.html#topic+hlscv">hlscv</a></code> in package <a href="https://CRAN.R-project.org/package=ks"><span class="pkg">ks</span></a> for Gaussian kernel only if <code>0 &lt;= deriv.order &lt;= 5</code>, 
<code><a href="locfit.html#topic+kdeb">kdeb</a></code> in package <a href="https://CRAN.R-project.org/package=locfit"><span class="pkg">locfit</span></a> if <code>deriv.order = 0</code>. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Derivative order = 0

h.ucv(kurtotic,deriv.order = 0)

## Derivative order = 1

h.ucv(kurtotic,deriv.order = 1)
</code></pre>

<hr>
<h2 id='kedd-package'>
Kernel Estimator and Bandwidth Selection for Density and Its Derivatives
</h2><span id='topic+kedd-package'></span><span id='topic+kedd'></span>

<h3>Description</h3>

<p>Smoothing techniques and computing bandwidth selectors of the 
r'th derivative of a probability density for one-dimensional data.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> kedd</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.0.4</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2024-01-27</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;= 2) </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>There are four main types of functions in this package:
</p>

<ol>
<li><p> Compute the derivatives and convolutions of a kernel function (1-d). 
</p>
</li>
<li><p> Compute the kernel estimators for density and its derivatives (1-d). 
</p>
</li>
<li><p> Computing the bandwidth selectors (1-d).
</p>
</li>
<li><p> Displaying kernel estimators.
</p>
</li></ol>



<h3>Main Features</h3>

<p><b>Convolutions and derivatives in kernel function:</b><br /><br />
In non-parametric statistics, a kernel is a weighting function used in non-parametric estimation techniques. 
The kernels functions <code class="reqn">K(x)</code> are used in derivatives of kernel density estimator to estimate 
<code class="reqn">\hat{f}^{(r)}_{h}(x)</code>, satisfying the following three requirements:
</p>

<ol>
<li> <p><code class="reqn">\int_{R} K(x) dx = 1</code>
</p>
</li>
<li> <p><code class="reqn">\int_{R} xK(x) dx = 0</code>
</p>
</li>
<li> <p><code class="reqn">\mu_{2}(K) = \int_{R}x^{2} K(x) dx &lt; \infty</code>
</p>
</li></ol>

<p>Several types of kernel functions <code class="reqn">K(x)</code> are commonly used in this package: Gaussian, Epanechnikov, Uniform (rectangular), Triangular, 
Triweight, Tricube, Biweight (quartic), Cosine.<br />
</p>
<p>The function <code><a href="#topic+kernel.fun">kernel.fun</a></code> for kernel derivative <code class="reqn">K^{(r)}(x)</code> and <code><a href="#topic+kernel.conv">kernel.conv</a></code> for 
kernel convolution <code class="reqn">K^{(r)}\ast K^{(r)} (x)</code>, where the write formally:
</p>
<p style="text-align: center;"><code class="reqn">K^{(r)}(x) = \frac{d^{r}}{d x^{r}} K(x)</code>
</p>
  
<p style="text-align: center;"><code class="reqn">K^{(r)} \ast K^{(r)} (x) = \int_{-\infty}^{+\infty} K^{(r)}(y)K^{(r)}(x-y)dy</code>
</p>

<p>for <code class="reqn">r = 0, 1, 2, \dots</code><br />
</p>
<p><b>Estimators of r'th derivative of a density function:</b><br /><br />
A <dfn>natural estimator</dfn> of the r'th derivative of a density function <code class="reqn">f(x)</code> is:   
</p>
<p style="text-align: center;"><code class="reqn">\hat{f}^{(r)}_{h}(x)= \frac{d^{r}}{d x^{r}} \frac{1}{nh} \sum_{i=1}^{n} K\left(\frac{x-X_{i}}{h}\right) =
                          \frac{1}{nh^{r+1}}\sum_{i=1}^{n} K^{(r)}\left(\frac{x-X_{i}}{h}\right)</code>
</p>

<p>Here, <code class="reqn">X_{1}, X_{2}, \dots,X_{n}</code> is an i.i.d, sample of size <code class="reqn">n</code> from the distribution with density 
<code class="reqn">f(x)</code>, <code class="reqn">K(x)</code> is the kernel function which we take to be a symmetric probability density with 
at least <code class="reqn">r</code> non zero derivatives when estimating <code class="reqn">f^{(r)}(x)</code>, and <code class="reqn">h</code> is the bandwidth,
this parameter is very important that controls the degree of smoothing applied to the data.<br />
</p>
<p>The case <code class="reqn">(r=0)</code> is the standard kernel density estimator (e.g. Silverman 1986, Wolfgang 1991, Scott 1992, 
Wand and Jones 1995, Jeffrey 1996, Bowman and Azzalini 1997, Alexandre 2009), properties of such derivative 
estimators are well known e.g. Sheather and Jones (1991), Jones and Kappenman (1991), Wolfgang (1991). For 
the case <code class="reqn">(r &gt; 0)</code>, is derivative of kernel density estimator (e.g. Bhattacharya 1967, Schuster 1969, Alekseev 1972, 
Wolfgang et all 1990, Jones 1992, Stoker 1993) and for applications which require the estimation of density derivatives can 
be found in Singh (1977).<br />
</p>
<p>For r'th derivatives of kernel density estimator one-dimensional, the main function is <code><a href="#topic+dkde">dkde</a></code>. For display, 
its plot method calls <code><a href="#topic+plot.dkde">plot.dkde</a></code>, and if to add a plot using <code><a href="#topic+lines.dkde">lines.dkde</a></code>.
</p>
<pre>
  R&gt; data(trimodal)
  R&gt; dkde(x = trimodal, deriv.order = 0, kernel = "gaussian")
   
    Data: trimodal (200 obs.);      Kernel: gaussian
    Derivative order: 0;    Bandwidth 'h' = 0.1007
          eval.points           est.fx         
    Min.   :-2.91274   Min.   :0.0000066  
    1st Qu.:-1.46519   1st Qu.:0.0669750  
    Median :-0.01765   Median :0.1682045  
    Mean   :-0.01765   Mean   :0.1723692  
    3rd Qu.: 1.42989   3rd Qu.:0.2484626  
    Max.   : 2.87743   Max.   :0.4157340 
   
  R&gt; dkde(x = trimodal, deriv.order = 1, kernel = "gaussian")
  
    Data: trimodal (200 obs.);      Kernel: gaussian
    Derivative order: 1;    Bandwidth 'h' = 0.09094
          eval.points           est.fx         
    Min.   :-2.87358   Min.   :-1.740447  
    1st Qu.:-1.44562   1st Qu.:-0.343952  
    Median :-0.01765   Median : 0.009057  
    Mean   :-0.01765   Mean   : 0.000000  
    3rd Qu.: 1.41031   3rd Qu.: 0.415343  
    Max.   : 2.83828   Max.   : 1.256891  
  </pre>
<p><b>Bandwidth selectors:</b><br /><br />
The most important factor in the r'th derivative kernel density estimate is a choice of the bandwidth 
<code class="reqn">h</code> for one-dimensional observations. Because of its role in controlling both the amount and 
the direction of smoothing, this choice is particularly important. We present the popular bandwidth 
selection (for more details see references) methods in this package:
</p>

<ul>
<li><p> Optimal Bandwidth (AMISE); with <code>deriv.order &gt;= 0</code>, name of this function is <code><a href="#topic+h.amise">h.amise</a></code>.<br /> 
For display, its plot method calls <code><a href="#topic+plot.h.amise">plot.h.amise</a></code>, and to add a plot used <code><a href="#topic+lines.h.amise">lines.h.amise</a></code>.
</p>
</li>
<li><p> Maximum-likelihood cross-validation (MLCV); with <code>deriv.order = 0</code>, name of this function is <code><a href="#topic+h.mlcv">h.mlcv</a></code>.<br />
For display, its plot method calls <code><a href="#topic+plot.h.mlcv">plot.h.mlcv</a></code>, and to add a plot used <code><a href="#topic+lines.h.mlcv">lines.h.mlcv</a></code>. 
</p>
</li>
<li><p> Unbiased cross validation (UCV); with <code>deriv.order &gt;= 0</code>, name of this function is <code><a href="#topic+h.ucv">h.ucv</a></code>.<br />
For display, its plot method calls <code><a href="#topic+plot.h.ucv">plot.h.ucv</a></code>, and to add a plot used <code><a href="#topic+lines.h.ucv">lines.h.ucv</a></code>.
</p>
</li>
<li><p> Biased cross validation (BCV); with <code>deriv.order &gt;= 0</code>, name of this function is <code><a href="#topic+h.bcv">h.bcv</a></code>.<br />
For display, its plot method calls <code><a href="#topic+plot.h.bcv">plot.h.bcv</a></code>, and to add a plot used <code><a href="#topic+lines.h.bcv">lines.h.bcv</a></code>.
</p>
</li>
<li><p> Complete cross-validation (CCV); with <code>deriv.order &gt;= 0</code>, name of this function is <code><a href="#topic+h.ccv">h.ccv</a></code>.<br />
For display, its plot method calls <code><a href="#topic+plot.h.ccv">plot.h.ccv</a></code>, and to add a plot used <code><a href="#topic+lines.h.ccv">lines.h.ccv</a></code>.
</p>
</li>
<li><p> Modified cross-validation (MCV); with <code>deriv.order &gt;= 0</code>, name of this function is <code><a href="#topic+h.mcv">h.mcv</a></code>.<br />
For display, its plot method calls <code><a href="#topic+plot.h.mcv">plot.h.mcv</a></code>, and to add a plot used <code><a href="#topic+lines.h.mcv">lines.h.mcv</a></code>.
</p>
</li>
<li><p> Trimmed cross-validation (TCV); with <code>deriv.order &gt;= 0</code>, name of this function is <code><a href="#topic+h.tcv">h.tcv</a></code>.<br />
For display, its plot method calls <code><a href="#topic+plot.h.tcv">plot.h.tcv</a></code>, and to add a plot used <code><a href="#topic+lines.h.tcv">lines.h.tcv</a></code>.
</p>
</li></ul>

<pre>
  R&gt; data(trimodal)
  R&gt; h.bcv(x = trimodal, whichbcv = 1, deriv.order = 0, kernel = "gaussian")
  
    Call:           Biased Cross-Validation 1
    Derivative order = 0
    Data: trimodal (200 obs.);      Kernel: gaussian
    Min BCV = 0.004511636;  Bandwidth 'h' = 0.4357812 
	
  R&gt; h.ccv(x = trimodal, deriv.order = 1, kernel = "gaussian")	
  
    Call:           Complete Cross-Validation
    Derivative order = 1 
    Data: trimodal (200 obs.);      Kernel: gaussian
    Min CCV = 0.01985078;   Bandwidth 'h' = 0.5828336
	
  R&gt; h.tcv(x = trimodal, deriv.order = 2, kernel = "gaussian")
  
    Call:           Trimmed Cross-Validation
    Derivative order = 2
    Data: trimodal (200 obs.);      Kernel: gaussian
    Min TCV = -295.563;     Bandwidth 'h' = 0.08908582
	
  R&gt; h.ucv(x = trimodal, deriv.order = 3, kernel = "gaussian")

    Call:           Unbiased Cross-Validation
    Derivative order = 3
    Data: trimodal (200 obs.);      Kernel: gaussian
    Min UCV = -63165.18;    Bandwidth 'h' = 0.1067236  
  </pre>
<p>For an overview of this package, see <code>vignette("kedd")</code>.  
</p>


<h3>Requirements</h3>

<p><span class="rlang"><b>R</b></span> version &gt;= 2.15.0
</p>


<h3>Licence</h3>

<p>This package and its documentation are usable under the terms of the &quot;GNU
General Public License&quot;, a copy of which is distributed with the package.
</p>


<h3>References</h3>

<p>Alekseev, V. G. (1972).
Estimation of a probability density function and its derivatives.
<em>Mathematical notes of the Academy of Sciences of the USSR</em>. <b>12</b>(5), 808&ndash;811.
</p>
<p>Alexandre, B. T. (2009).
<em>Introduction to Nonparametric Estimation</em>.
Springer-Verlag, New York.
</p>
<p>Bowman, A. W. (1984). 
An alternative method of cross-validation for the smoothing 
of kernel density estimates. 
<em>Biometrika</em>, <b>71</b>, 353&ndash;360.
</p>
<p>Bowman, A. W. and Azzalini, A. (1997). 
<em>Applied Smoothing Techniques for
Data Analysis: the Kernel Approach with 
S-Plus Illustrations</em>.
Oxford University Press, Oxford.
</p>
<p>Bowman, A.W. and Azzalini, A. (2003).
Computational aspects of nonparametric smoothing
with illustrations from the <a href="https://CRAN.R-project.org/package=sm"><span class="pkg">sm</span></a> library.
<em>Computational Statistics and Data Analysis</em>, <b>42</b>, 545&ndash;560.
</p>
<p>Bowman, A.W. and Azzalini, A. (2013).
<a href="https://CRAN.R-project.org/package=sm"><span class="pkg">sm</span></a>: Smoothing methods for nonparametric 
regression and density estimation. 
<em><span class="rlang"><b>R</b></span> package version 2.2-5.3</em>. Ported to <span class="rlang"><b>R</b></span> by B. D. Ripley.
</p>
<p>Bhattacharya, P. K. (1967).
Estimation of a probability density function and Its derivatives.
<em>Sankhya: The Indian Journal of Statistics, Series A</em>, <b>29</b>, 373&ndash;382. 
</p>
<p>Duin, R. P. W. (1976).
On the choice of smoothing parameters of Parzen estimators of probability density functions. 
<em>IEEE Transactions on Computers</em>, <b>C-25</b>, 1175&ndash;1179.
</p>
<p>Feluch, W. and Koronacki, J. (1992).
A note on modified cross-validation in density estimation.
<em>Computational Statistics and Data Analysis</em>, <b>13</b>, 143&ndash;151.
</p>
<p>George, R. T. (1990).
The maximal smoothing principle in density estimation. 
<em>Journal of the American Statistical Association</em>, <b>85</b>, 470&ndash;477.
</p>
<p>George, R. T. and Scott, D. W. (1985).
Oversmoothed nonparametric density estimates. 
<em>Journal of the American Statistical Association</em>, <b>80</b>, 209&ndash;214.
</p>
<p>Habbema, J. D. F., Hermans, J., and Van den Broek, K. (1974)
A stepwise discrimination analysis program using density estimation. 
<em>Compstat 1974: Proceedings in Computational Statistics</em>. Physica Verlag, Vienna.
</p>
<p>Heidenreich, N. B., Schindler, A. and Sperlich, S. (2013).
Bandwidth selection for kernel density estimation: 
a review of fully automatic selectors.
<em>Advances in Statistical Analysis</em>.
</p>
<p>Jeffrey, S. S. (1996).
<em>Smoothing Methods in Statistics</em>.
Springer-Verlag, New York.
</p>
<p>Jones, M. C. (1992).
Differences and derivatives in kernel estimation. 
<em>Metrika</em>, <b>39</b>, 335&ndash;340.
</p>
<p>Jones, M. C., Marron, J. S. and Sheather,S. J. (1996).
A brief survey of bandwidth selection for density estimation. 
<em>Journal of the American Statistical Association</em>, <b>91</b>, 401&ndash;407.
</p>
<p>Jones, M. C. and Kappenman, R. F. (1991). 
On a class of kernel density estimate bandwidth selectors. 
<em>Scandinavian Journal of Statistics</em>, <b>19</b>, 337&ndash;349.
</p>
<p>Loader, C. (1999). 
<em>Local Regression and Likelihood</em>. 
Springer, New York.
</p>
<p>Olver, F. W., Lozier, D. W., Boisvert, R. F. and Clark, C. W. (2010).
<em>NIST Handbook of Mathematical Functions</em>.
Cambridge University Press, New York, USA.
</p>
<p>Peter, H. and Marron, J.S. (1987).
Estimation of integrated squared density derivatives.
<em>Statistics and Probability Letters</em>, <b>6</b>, 109&ndash;115.
</p>
<p>Peter, H. and Marron, J.S. (1991).
Local minima in cross-validation functions.
<em>Journal of the Royal Statistical Society, Series B</em>, <b>53</b>, 245&ndash;252.
</p>
<p>Radhey, S. S. (1987).
MISE of kernel estimates of a density and its derivatives.
<em>Statistics and Probability Letters</em>, <b>5</b>, 153&ndash;159.
</p>
<p>Rudemo, M. (1982). 
Empirical choice of histograms and kernel density estimators. 
<em>Scandinavian Journal of Statistics</em>, <b>9</b>, 65&ndash;78.
</p>
<p>Scott, D. W. (1992).
<em>Multivariate Density Estimation. Theory, Practice and Visualization</em>.
New York: Wiley.
</p>
<p>Scott, D.W. and George, R. T. (1987).
Biased and unbiased cross-validation in density estimation. 
<em>Journal of the American Statistical Association</em>, <b>82</b>, 1131&ndash;1146.
</p>
<p>Schuster, E. F. (1969) 
Estimation of a probability density function and its derivatives. 
<em>The Annals of Mathematical Statistics</em>, <b>40</b> (4), 1187&ndash;1195.
</p>
<p>Sheather, S. J. (2004).
Density estimation.
<em>Statistical Science</em>, <b>19</b>, 588&ndash;597.
</p>
<p>Sheather, S. J. and Jones, M. C. (1991).
A reliable data-based bandwidth selection method for 
kernel density estimation.
<em>Journal of the Royal Statistical Society, Series B</em>, <b>53</b>, 683&ndash;690.
</p>
<p>Silverman, B. W. (1986).
<em>Density Estimation for Statistics and Data Analysis</em>.
Chapman &amp; Hall/CRC. London.
</p>
<p>Singh, R. S. (1977).                            
Applications of estimators of a density and 
its derivatives to certain statistical problems.
<em>Journal of the Royal Statistical Society, Series B</em>, <b>39</b>(3), 357&ndash;363.
</p>
<p>Stoker, T. M. (1993).
Smoothing bias in density derivative estimation. 
<em>Journal of the American Statistical Association</em>, <b>88</b>, 855&ndash;863.
</p>
<p>Stute, W. (1992).
Modified cross validation in density estimation.
<em>Journal of Statistical Planning and Inference</em>, <b>30</b>, 293&ndash;305.
</p>
<p>Tarn, D. (2007).
<a href="https://CRAN.R-project.org/package=ks"><span class="pkg">ks</span></a>: Kernel density estimation and kernel discriminant
analysis for multivariate data in <span class="rlang"><b>R</b></span>.
<em>Journal of Statistical Software</em>, <b>21</b>(7), 1&ndash;16. 
</p>
<p>Tristen, H. and Jeffrey, S. R. (2008).
Nonparametric Econometrics: The <a href="https://CRAN.R-project.org/package=np"><span class="pkg">np</span></a> Package.
<em>Journal of Statistical Software</em>,<b>27</b>(5).
</p>
<p>Venables, W. N. and Ripley, B. D. (2002).
<em>Modern Applied Statistics with S</em>.
New York: Springer.
</p>
<p>Wand, M. P. and Jones, M. C. (1995).
<em>Kernel Smoothing</em>.
Chapman and Hall, London.
</p>
<p>Wand, M.P. and Ripley, B. D. (2013). 
<a href="https://CRAN.R-project.org/package=KernSmooth"><span class="pkg">KernSmooth</span></a>: Functions for Kernel Smoothing 
for Wand and Jones (1995). 
<em><span class="rlang"><b>R</b></span> package version 2.23-10</em>. 
</p>
<p>Wolfgang, H. (1991).
<em>Smoothing Techniques</em>, 
<em>With Implementation in S</em>.
Springer-Verlag, New York.
</p>
<p>Wolfgang, H., Marlene, M., Stefan, S. and Axel, W. (2004).
<em>Nonparametric and Semiparametric Models</em>.
Springer-Verlag, Berlin Heidelberg.
</p>
<p>Wolfgang, H., Marron, J. S. and Wand, M. P. (1990).
Bandwidth choice for density derivatives.
<em>Journal of the Royal Statistical Society, Series B</em>, 223&ndash;232.
</p>


<h3>See Also</h3>

<p><a href="https://CRAN.R-project.org/package=ks"><span class="pkg">ks</span></a>, <a href="https://CRAN.R-project.org/package=KernSmooth"><span class="pkg">KernSmooth</span></a>, <a href="https://CRAN.R-project.org/package=sm"><span class="pkg">sm</span></a>, <a href="https://CRAN.R-project.org/package=np"><span class="pkg">np</span></a>, <a href="https://CRAN.R-project.org/package=locfit"><span class="pkg">locfit</span></a>, <a href="https://CRAN.R-project.org/package=feature"><span class="pkg">feature</span></a>, <a href="https://CRAN.R-project.org/package=GenKern"><span class="pkg">GenKern</span></a>.
</p>

<hr>
<h2 id='kernel.conv'>
Convolutions of r'th Derivative for Kernel Function
</h2><span id='topic+kernel.conv'></span><span id='topic+kernel.conv.default'></span>

<h3>Description</h3>

<p>The (S3) generic function <code>kernel.conv</code> computes the convolution 
of r'th derivative for kernel function. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kernel.conv(x, ...)
## Default S3 method:
kernel.conv(x = NULL, deriv.order = 0,kernel = c("gaussian","epanechnikov", 
             "uniform", "triangular", "triweight", "tricube", 
             "biweight", "cosine", "silverman"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kernel.conv_+3A_x">x</code></td>
<td>
<p>points at which the convolution of kernel derivative 
is to be evaluated.</p>
</td></tr>
<tr><td><code id="kernel.conv_+3A_deriv.order">deriv.order</code></td>
<td>
<p>derivative order (scalar).</p>
</td></tr>
<tr><td><code id="kernel.conv_+3A_kernel">kernel</code></td>
<td>
<p>a character string giving the smoothing kernel to be used, 
with default <code>"gaussian"</code>.</p>
</td></tr>
<tr><td><code id="kernel.conv_+3A_...">...</code></td>
<td>
<p>further arguments for (non-default) methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The convolution of r'th derivative for kernel function is written <code class="reqn">K^{(r)}\ast K^{(r)}</code>. It is defined as 
the integral of the product of the derivative for kernel. As such, it is a particular kind of integral transform:
</p>
<p style="text-align: center;"><code class="reqn">K^{(r)} \ast K^{(r)}(x) = \int_{-\infty}^{+\infty} K^{(r)}(y)K^{(r)}(x-y)dy</code>
</p>

<p>where: </p>
<p style="text-align: center;"><code class="reqn">K^{(r)}(x) = \frac{d^{r}}{d x^{r}} K(x)</code>
</p>

<p>for <code class="reqn">r = 0, 1, 2, \dots</code>
</p>


<h3>Value</h3>

<table>
<tr><td><code>kernel</code></td>
<td>
<p>name of kernel to use.</p>
</td></tr>
<tr><td><code>deriv.order</code></td>
<td>
<p>the derivative order to use.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>the n coordinates of the points where the convolution of kernel 
derivative is evaluated.</p>
</td></tr>
<tr><td><code>kx</code></td>
<td>
<p>the convolution of kernel derivative values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Arsalane Chouaib Guidoum <a href="mailto:acguidoum@usthb.dz">acguidoum@usthb.dz</a>
</p>


<h3>References</h3>

<p>Olver, F. W., Lozier, D. W., Boisvert, R. F. and Clark, C. W. (2010).
<em>NIST Handbook of Mathematical Functions</em>.
Cambridge University Press, New York, USA.
</p>
<p>Scott, D. W. (1992).
<em>Multivariate Density Estimation. Theory, Practice and Visualization</em>.
New York: Wiley.
</p>
<p>Silverman, B. W. (1986).
<em>Density Estimation for Statistics and Data Analysis</em>.
Chapman &amp; Hall/CRC. London.
</p>
<p>Wand, M. P. and Jones, M. C. (1995).
<em>Kernel Smoothing.</em>
Chapman and Hall, London.
</p>
<p>Wolfgang, H. (1991).
<em>Smoothing Techniques, With Implementation in S.</em>
Springer-Verlag, New York.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.kernel.conv">plot.kernel.conv</a></code>, <code><a href="stats.html#topic+kernapply">kernapply</a></code> in package &quot;stats&quot;
for computes the convolution between an input sequence, and <code><a href="stats.html#topic+convolve">convolve</a></code>
use the Fast Fourier Transform (<code><a href="stats.html#topic+fft">fft</a></code>) to compute the several kinds of 
convolutions of two sequences.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>kernels &lt;- eval(formals(kernel.conv.default)$kernel)
kernels

## gaussian
kernel.conv(x = 0,kernel=kernels[1],deriv.order=0)
kernel.conv(x = 0,kernel=kernels[1],deriv.order=1)

## silverman
kernel.conv(x = 0,kernel=kernels[9],deriv.order=0)
kernel.conv(x = 0,kernel=kernels[9],deriv.order=1)
</code></pre>

<hr>
<h2 id='kernel.fun'>
Derivatives of Kernel Function
</h2><span id='topic+kernel.fun'></span><span id='topic+kernel.fun.default'></span>

<h3>Description</h3>

<p>The (S3) generic function <code>kernel.fun</code> computes the 
r'th derivative for kernel density.</p>


<h3>Usage</h3>

<pre><code class='language-R'>kernel.fun(x, ...)
## Default S3 method:
kernel.fun(x = NULL, deriv.order = 0, kernel = c("gaussian","epanechnikov", 
            "uniform", "triangular", "triweight", "tricube", 
            "biweight", "cosine", "silverman"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kernel.fun_+3A_x">x</code></td>
<td>
<p>points at which the derivative of kernel function 
is to be evaluated.</p>
</td></tr>
<tr><td><code id="kernel.fun_+3A_deriv.order">deriv.order</code></td>
<td>
<p>derivative order (scalar).</p>
</td></tr>
<tr><td><code id="kernel.fun_+3A_kernel">kernel</code></td>
<td>
<p>a character string giving the smoothing kernel to be used, 
with default <code>"gaussian"</code>.</p>
</td></tr>
<tr><td><code id="kernel.fun_+3A_...">...</code></td>
<td>
<p>further arguments for (non-default) methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>We give a short survey of some kernels functions <code class="reqn">K(x;r)</code>; where <code class="reqn">r</code> is derivative order,
</p>

<ul>
<li><p> Gaussian:  <code class="reqn">K(x;\infty) =\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{x^{2}}{2}\right)1_{]-\infty,+\infty[}</code>
</p>
</li>
<li><p> Epanechnikov: <code class="reqn">K(x;2)=\frac{3}{4}(1-x^{2})1_{(|x| \leq 1)}</code>
</p>
</li>
<li><p> uniform (rectangular): <code class="reqn">K(x;0)=\frac{1}{2}1_{(|x| \leq 1)}</code>
</p>
</li>
<li><p> triangular: <code class="reqn">K(x;1)=(1-|x|)1_{(|x| \leq 1)}</code>
</p>
</li>
<li><p> triweight: <code class="reqn">K(x;6)=\frac{35}{32}(1-x^{2})^{3} 1_{(|x| \leq 1)}</code>
</p>
</li>
<li><p> tricube: <code class="reqn">K(x;9)=\frac{70}{81}(1-|x|^{3})^{3} 1_{(|x| \leq 1)}</code>
</p>
</li>
<li><p> biweight: <code class="reqn">K(x;4)=\frac{15}{16}(1-x^{2})^{2} 1_{(|x| \leq 1)}</code>
</p>
</li>
<li><p> cosine: <code class="reqn">K(x;\infty)=\frac{\pi}{4}\cos\left(\frac{\pi}{2}x\right) 1_{(|x| \leq 1)}</code>
</p>
</li>
<li><p> Silverman: <code class="reqn">K(x;r \bmod 8)=\frac{1}{2}\exp\left(-\frac{|x|}{\sqrt{2}}\right)\sin\left(\frac{|x|}{\sqrt{2}}+\frac{\pi}{4}\right)1_{]-\infty,+\infty[}</code>	
</p>
</li></ul>

<p>The r'th derivative for kernel function <code class="reqn">K(x)</code> is written:
</p>
<p style="text-align: center;"><code class="reqn">K^{(r)}(x) = \frac{d^{r}}{d x^{r}} K(x)</code>
</p>

<p>for <code class="reqn">r = 0, 1, 2, \dots</code><br />
The r'th derivative of the <b>Gaussian kernel</b> <code class="reqn">K(x)</code> is given by:
</p>
<p style="text-align: center;"><code class="reqn">K^{(r)}(x) = (-1)^{r} H_{r}(x) K(x)</code>
</p>

<p>where <code class="reqn">H_{r}(x)</code> is the r'th <b>Hermite polynomial</b>. This polynomials 
are set of orthogonal polynomials, for more details see, <code><a href="orthopolynom.html#topic+hermite.h.polynomials">hermite.h.polynomials</a></code> 
in package <a href="https://CRAN.R-project.org/package=orthopolynom"><span class="pkg">orthopolynom</span></a>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>kernel</code></td>
<td>
<p>name of kernel to use.</p>
</td></tr>
<tr><td><code>deriv.order</code></td>
<td>
<p>the derivative order to use.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>the n coordinates of the points where the derivative of kernel 
function is evaluated.</p>
</td></tr>
<tr><td><code>kx</code></td>
<td>
<p>the kernel derivative values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Arsalane Chouaib Guidoum <a href="mailto:acguidoum@usthb.dz">acguidoum@usthb.dz</a>
</p>


<h3>References</h3>

<p>Jones, M. C. (1992).
Differences and derivatives in kernel estimation. 
<em>Metrika</em>, <b>39</b>, 335&ndash;340.
</p>
<p>Olver, F. W., Lozier, D. W., Boisvert, R. F. and Clark, C. W. (2010).
<em>NIST Handbook of Mathematical Functions</em>.
Cambridge University Press, New York, USA.
</p>
<p>Silverman, B. W. (1986).
<em>Density Estimation for Statistics and Data Analysis</em>.
Chapman &amp; Hall/CRC. London.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.kernel.fun">plot.kernel.fun</a></code>, <code><a href="stats.html#topic+deriv">deriv</a></code> and <code><a href="stats.html#topic+D">D</a></code> in 
package &quot;stats&quot; for symbolic and algorithmic derivatives of simple expressions. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>kernels &lt;- eval(formals(kernel.fun.default)$kernel)
kernels

## gaussian
kernel.fun(x = 0,kernel=kernels[1],deriv.order=0)
kernel.fun(x = 0,kernel=kernels[1],deriv.order=1)

## silverman
kernel.fun(x = 0,kernel=kernels[9],deriv.order=0)
kernel.fun(x = 0,kernel=kernels[9],deriv.order=1)
</code></pre>

<hr>
<h2 id='plot.dkde'>
Plot for Kernel Density Derivative Estimate
</h2><span id='topic+plot.dkde'></span><span id='topic+lines.dkde'></span>

<h3>Description</h3>

<p>The <code><a href="#topic+plot.dkde">plot.dkde</a></code> function loops through calls to 
the <code><a href="#topic+dkde">dkde</a></code> function. Plot for kernel density 
derivative estimate for 1-dimensional data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'dkde'
plot(x, fx = NULL, ...)
## S3 method for class 'dkde'
lines(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.dkde_+3A_x">x</code></td>
<td>
<p>object of class <code>dkde</code> (output from <code><a href="#topic+dkde">dkde</a></code>).</p>
</td></tr>
<tr><td><code id="plot.dkde_+3A_fx">fx</code></td>
<td>
<p>add to graphics the true density derivative (class :<code><a href="base.html#topic+function">function</a></code>), 
to compare it by the density derivative to estimate.</p>
</td></tr>
<tr><td><code id="plot.dkde_+3A_...">...</code></td>
<td>
<p>other graphics parameters, see <code><a href="graphics.html#topic+par">par</a></code> in package &quot;graphics&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The 1-d plot is a standard plot of a 1-d curve. If
<code>!is.null(fx)</code> then a true density derivative is added. 
</p>


<h3>Value</h3>

<p>Plot of 1-d kernel density derivative estimates are sent to graphics window.
</p>


<h3>Author(s)</h3>

<p>Arsalane Chouaib Guidoum <a href="mailto:acguidoum@usthb.dz">acguidoum@usthb.dz</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dkde">dkde</a></code>, <code><a href="stats.html#topic+plot.density">plot.density</a></code> in package &quot;stats&quot; if <code>deriv.order = 0</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>plot(dkde(kurtotic,deriv.order=0,kernel="gaussian"),sub="")
lines(dkde(kurtotic,deriv.order=0,kernel="biweight"),col="red")
</code></pre>

<hr>
<h2 id='plot.h.amise'>
Plot for Asymptotic Mean Integrated Squared Error
</h2><span id='topic+plot.h.amise'></span><span id='topic+lines.h.amise'></span>

<h3>Description</h3>

<p>The <code><a href="#topic+plot.h.amise">plot.h.amise</a></code> function loops through calls to 
the <code><a href="#topic+h.amise">h.amise</a></code> function. Plot for asymptotic mean integrated 
squared error function for 1-dimensional data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'h.amise'
plot(x, seq.bws=NULL, ...)
## S3 method for class 'h.amise'
lines(x,seq.bws=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.h.amise_+3A_x">x</code></td>
<td>
<p>object of class <code>h.amise</code> (output from <code><a href="#topic+h.amise">h.amise</a></code>).</p>
</td></tr>
<tr><td><code id="plot.h.amise_+3A_seq.bws">seq.bws</code></td>
<td>
<p>the sequence of bandwidths in which to compute the AMISE function. 
By default, the procedure defines a sequence of 50 points, from <code>0.15*hos</code> 
to <code>2*hos</code> (Over-smoothing).</p>
</td></tr>
<tr><td><code id="plot.h.amise_+3A_...">...</code></td>
<td>
<p>other graphics parameters, see <code><a href="graphics.html#topic+par">par</a></code> in package &quot;graphics&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Plot of 1-d AMISE function are sent to graphics window.<br />
</p>
<table>
<tr><td><code>kernel</code></td>
<td>
<p>name of kernel to use.</p>
</td></tr>
<tr><td><code>deriv.order</code></td>
<td>
<p>the derivative order to use.</p>
</td></tr>
<tr><td><code>seq.bws</code></td>
<td>
<p>the sequence of bandwidths.</p>
</td></tr>
<tr><td><code>amise</code></td>
<td>
<p>the values of the AMISE function in the bandwidths grid.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Arsalane Chouaib Guidoum <a href="mailto:acguidoum@usthb.dz">acguidoum@usthb.dz</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+h.amise">h.amise</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>plot(h.amise(bimodal,deriv.order=0))
</code></pre>

<hr>
<h2 id='plot.h.bcv'>
Plot for Biased Cross-Validation
</h2><span id='topic+plot.h.bcv'></span><span id='topic+lines.h.bcv'></span>

<h3>Description</h3>

<p>The <code><a href="#topic+plot.h.bcv">plot.h.bcv</a></code> function loops through calls to 
the <code><a href="#topic+h.bcv">h.bcv</a></code> function. Plot for biased cross-validation 
function for 1-dimensional data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'h.bcv'
plot(x, seq.bws=NULL, ...)
## S3 method for class 'h.bcv'
lines(x,seq.bws=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.h.bcv_+3A_x">x</code></td>
<td>
<p>object of class <code>h.bcv</code> (output from <code><a href="#topic+h.bcv">h.bcv</a></code>).</p>
</td></tr>
<tr><td><code id="plot.h.bcv_+3A_seq.bws">seq.bws</code></td>
<td>
<p>the sequence of bandwidths in which to compute the biased 
cross-validation function. By default, the procedure defines a sequence of 
50 points, from <code>0.15*hos</code> to <code>2*hos</code> (Over-smoothing).</p>
</td></tr>
<tr><td><code id="plot.h.bcv_+3A_...">...</code></td>
<td>
<p>other graphics parameters, see <code><a href="graphics.html#topic+par">par</a></code> in package &quot;graphics&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Plot of 1-d biased cross-validation function are sent to graphics window.<br />
</p>
<table>
<tr><td><code>kernel</code></td>
<td>
<p>name of kernel to use.</p>
</td></tr>
<tr><td><code>deriv.order</code></td>
<td>
<p>the derivative order to use.</p>
</td></tr>
<tr><td><code>seq.bws</code></td>
<td>
<p>the sequence of bandwidths.</p>
</td></tr>
<tr><td><code>bcv</code></td>
<td>
<p>the values of the biased cross-validation function in the bandwidths grid.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Arsalane Chouaib Guidoum <a href="mailto:acguidoum@usthb.dz">acguidoum@usthb.dz</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+h.bcv">h.bcv</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## EXAMPLE 1:

plot(h.bcv(trimodal, whichbcv = 1, deriv.order = 0),main="",sub="")
lines(h.bcv(trimodal, whichbcv = 2, deriv.order = 0),col="red")
legend("topright", c("BCV1","BCV2"),lty=1,col=c("black","red"),inset = .015)

## EXAMPLE 2:

plot(h.bcv(trimodal, whichbcv = 1, deriv.order = 1),main="",sub="")
lines(h.bcv(trimodal, whichbcv = 2, deriv.order = 1),col="red")
legend("topright", c("BCV1","BCV2"),lty=1,col=c("black","red"),inset = .015)
</code></pre>

<hr>
<h2 id='plot.h.ccv'>
Plot for Complete Cross-Validation
</h2><span id='topic+plot.h.ccv'></span><span id='topic+lines.h.ccv'></span>

<h3>Description</h3>

<p>The <code><a href="#topic+plot.h.ccv">plot.h.ccv</a></code> function loops through calls to 
the <code><a href="#topic+h.ccv">h.ccv</a></code> function. Plot for complete cross-validation 
function for 1-dimensional data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'h.ccv'
plot(x, seq.bws=NULL, ...)
## S3 method for class 'h.ccv'
lines(x,seq.bws=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.h.ccv_+3A_x">x</code></td>
<td>
<p>object of class <code>h.ccv</code> (output from <code><a href="#topic+h.ccv">h.ccv</a></code>).</p>
</td></tr>
<tr><td><code id="plot.h.ccv_+3A_seq.bws">seq.bws</code></td>
<td>
<p>the sequence of bandwidths in which to compute the complete 
cross-validation function. By default, the procedure defines a sequence of 
50 points, from <code>0.15*hos</code> to <code>2*hos</code> (Over-smoothing).</p>
</td></tr>
<tr><td><code id="plot.h.ccv_+3A_...">...</code></td>
<td>
<p>other graphics parameters, see <code><a href="graphics.html#topic+par">par</a></code> in package &quot;graphics&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Plot of 1-d complete cross-validation function are sent to graphics window.<br />
</p>
<table>
<tr><td><code>kernel</code></td>
<td>
<p>name of kernel to use.</p>
</td></tr>
<tr><td><code>deriv.order</code></td>
<td>
<p>the derivative order to use.</p>
</td></tr>
<tr><td><code>seq.bws</code></td>
<td>
<p>the sequence of bandwidths.</p>
</td></tr>
<tr><td><code>ccv</code></td>
<td>
<p>the values of the complete cross-validation function in the bandwidths grid.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Arsalane Chouaib Guidoum <a href="mailto:acguidoum@usthb.dz">acguidoum@usthb.dz</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+h.ccv">h.ccv</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>oldpar &lt;- par(no.readonly = TRUE)
par(mfrow=c(2,1))
plot(h.ccv(trimodal,deriv.order=0),main="")
plot(h.ccv(trimodal,deriv.order=1),main="")
par(oldpar)
</code></pre>

<hr>
<h2 id='plot.h.mcv'>
Plot for Modified Cross-Validation
</h2><span id='topic+plot.h.mcv'></span><span id='topic+lines.h.mcv'></span>

<h3>Description</h3>

<p>The <code><a href="#topic+plot.h.mcv">plot.h.mcv</a></code> function loops through calls to 
the <code><a href="#topic+h.mcv">h.mcv</a></code> function. Plot for modified cross-validation 
function for 1-dimensional data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'h.mcv'
plot(x, seq.bws=NULL, ...)
## S3 method for class 'h.mcv'
lines(x,seq.bws=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.h.mcv_+3A_x">x</code></td>
<td>
<p>object of class <code>h.mcv</code> (output from <code><a href="#topic+h.mcv">h.mcv</a></code>).</p>
</td></tr>
<tr><td><code id="plot.h.mcv_+3A_seq.bws">seq.bws</code></td>
<td>
<p>the sequence of bandwidths in which to compute the modified 
cross-validation function. By default, the procedure defines a sequence of 
50 points, from <code>0.15*hos</code> to <code>2*hos</code> (Over-smoothing).</p>
</td></tr>
<tr><td><code id="plot.h.mcv_+3A_...">...</code></td>
<td>
<p>other graphics parameters, see <code><a href="graphics.html#topic+par">par</a></code> in package &quot;graphics&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Plot of 1-d modified cross-validation function are sent to graphics window.<br />
</p>
<table>
<tr><td><code>kernel</code></td>
<td>
<p>name of kernel to use.</p>
</td></tr>
<tr><td><code>deriv.order</code></td>
<td>
<p>the derivative order to use.</p>
</td></tr>
<tr><td><code>seq.bws</code></td>
<td>
<p>the sequence of bandwidths.</p>
</td></tr>
<tr><td><code>mcv</code></td>
<td>
<p>the values of the modified cross-validation function in the bandwidths grid.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Arsalane Chouaib Guidoum <a href="mailto:acguidoum@usthb.dz">acguidoum@usthb.dz</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+h.mcv">h.mcv</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>oldpar &lt;- par(no.readonly = TRUE)
par(mfrow=c(2,1))
plot(h.mcv(trimodal,deriv.order=0),main="")
plot(h.mcv(trimodal,deriv.order=1),main="")
par(oldpar)
</code></pre>

<hr>
<h2 id='plot.h.mlcv'>
Plot for Maximum-Likelihood Cross-validation
</h2><span id='topic+plot.h.mlcv'></span><span id='topic+lines.h.mlcv'></span>

<h3>Description</h3>

<p>The <code><a href="#topic+plot.h.mlcv">plot.h.mlcv</a></code> function loops through calls to 
the <code><a href="#topic+h.mlcv">h.mlcv</a></code> function. Plot for maximum-likelihood 
cross-validation function for 1-dimensional data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'h.mlcv'
plot(x, seq.bws=NULL, ...)
## S3 method for class 'h.mlcv'
lines(x,seq.bws=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.h.mlcv_+3A_x">x</code></td>
<td>
<p>object of class <code>h.mlcv</code> (output from <code><a href="#topic+h.mlcv">h.mlcv</a></code>).</p>
</td></tr>
<tr><td><code id="plot.h.mlcv_+3A_seq.bws">seq.bws</code></td>
<td>
<p>the sequence of bandwidths in which to compute the maximum-
likelihood cross-validation function. By default, the procedure defines a 
sequence of 50 points, from <code>0.15*hos</code> to <code>2*hos</code> (Over-smoothing).</p>
</td></tr>
<tr><td><code id="plot.h.mlcv_+3A_...">...</code></td>
<td>
<p>other graphics parameters, see <code><a href="graphics.html#topic+par">par</a></code> in package &quot;graphics&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Plot of 1-d maximum-likelihood cross-validation function are sent to graphics window.<br />
</p>
<table>
<tr><td><code>kernel</code></td>
<td>
<p>name of kernel to use.</p>
</td></tr>
<tr><td><code>seq.bws</code></td>
<td>
<p>the sequence of bandwidths.</p>
</td></tr>
<tr><td><code>mlcv</code></td>
<td>
<p>the values of the maximum-likelihood cross-validation function in the bandwidths grid.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Arsalane Chouaib Guidoum <a href="mailto:acguidoum@usthb.dz">acguidoum@usthb.dz</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+h.mlcv">h.mlcv</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>plot(h.mlcv(bimodal))
</code></pre>

<hr>
<h2 id='plot.h.tcv'>
Plot for Trimmed Cross-Validation
</h2><span id='topic+plot.h.tcv'></span><span id='topic+lines.h.tcv'></span>

<h3>Description</h3>

<p>The <code><a href="#topic+plot.h.tcv">plot.h.tcv</a></code> function loops through calls to 
the <code><a href="#topic+h.tcv">h.tcv</a></code> function. Plot for trimmed cross-validation 
function for 1-dimensional data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'h.tcv'
plot(x, seq.bws=NULL, ...)
## S3 method for class 'h.tcv'
lines(x,seq.bws=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.h.tcv_+3A_x">x</code></td>
<td>
<p>object of class <code>h.tcv</code> (output from <code><a href="#topic+h.tcv">h.tcv</a></code>).</p>
</td></tr>
<tr><td><code id="plot.h.tcv_+3A_seq.bws">seq.bws</code></td>
<td>
<p>the sequence of bandwidths in which to compute the trimmed 
cross-validation function. By default, the procedure defines a sequence of 
50 points, from <code>0.15*hos</code> to <code>2*hos</code> (Over-smoothing).</p>
</td></tr>
<tr><td><code id="plot.h.tcv_+3A_...">...</code></td>
<td>
<p>other graphics parameters, see <code><a href="graphics.html#topic+par">par</a></code> in package &quot;graphics&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Plot of 1-d trimmed cross-validation function are sent to graphics window.<br />
</p>
<table>
<tr><td><code>kernel</code></td>
<td>
<p>name of kernel to use.</p>
</td></tr>
<tr><td><code>deriv.order</code></td>
<td>
<p>the derivative order to use.</p>
</td></tr>
<tr><td><code>seq.bws</code></td>
<td>
<p>the sequence of bandwidths.</p>
</td></tr>
<tr><td><code>tcv</code></td>
<td>
<p>the values of the trimmed cross-validation function in the bandwidths grid.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Arsalane Chouaib Guidoum <a href="mailto:acguidoum@usthb.dz">acguidoum@usthb.dz</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+h.tcv">h.tcv</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>oldpar &lt;- par(no.readonly = TRUE)
par(mfrow=c(2,1))
plot(h.tcv(trimodal,deriv.order=0),main="")
plot(h.tcv(trimodal,deriv.order=1),seq.bws=seq(0.1,0.5,length.out=50),main="")
par(oldpar)
</code></pre>

<hr>
<h2 id='plot.h.ucv'>
Plot for Unbiased Cross-Validation
</h2><span id='topic+plot.h.ucv'></span><span id='topic+lines.h.ucv'></span>

<h3>Description</h3>

<p>The <code><a href="#topic+plot.h.ucv">plot.h.ucv</a></code> function loops through calls to 
the <code><a href="#topic+h.ucv">h.ucv</a></code> function. Plot for unbiased cross-validation 
function for 1-dimensional data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'h.ucv'
plot(x, seq.bws=NULL, ...)
## S3 method for class 'h.ucv'
lines(x,seq.bws=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.h.ucv_+3A_x">x</code></td>
<td>
<p>object of class <code>h.ucv</code> (output from <code><a href="#topic+h.ucv">h.ucv</a></code>).</p>
</td></tr>
<tr><td><code id="plot.h.ucv_+3A_seq.bws">seq.bws</code></td>
<td>
<p>the sequence of bandwidths in which to compute the unbiased 
cross-validation function. By default, the procedure defines a sequence of 
50 points, from <code>0.15*hos</code> to <code>2*hos</code> (Over-smoothing).</p>
</td></tr>
<tr><td><code id="plot.h.ucv_+3A_...">...</code></td>
<td>
<p>other graphics parameters, see <code><a href="graphics.html#topic+par">par</a></code> in package &quot;graphics&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Plot of 1-d unbiased cross-validation function are sent to graphics window.<br />
</p>
<table>
<tr><td><code>kernel</code></td>
<td>
<p>name of kernel to use.</p>
</td></tr>
<tr><td><code>deriv.order</code></td>
<td>
<p>the derivative order to use.</p>
</td></tr>
<tr><td><code>seq.bws</code></td>
<td>
<p>the sequence of bandwidths.</p>
</td></tr>
<tr><td><code>ucv</code></td>
<td>
<p>the values of the unbiased cross-validation function in the bandwidths grid.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Arsalane Chouaib Guidoum <a href="mailto:acguidoum@usthb.dz">acguidoum@usthb.dz</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+h.ucv">h.ucv</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>oldpar &lt;- par(no.readonly = TRUE)
par(mfrow=c(2,1))
plot(h.ucv(trimodal,deriv.order=0),seq.bws=seq(0.06,0.2,length=50))
plot(h.ucv(trimodal,deriv.order=1),seq.bws=seq(0.06,0.2,length=50))
par(oldpar)
</code></pre>

<hr>
<h2 id='plot.kernel.conv'>
Plot for Convolutions of r'th Derivative Kernel Function
</h2><span id='topic+plot.kernel.conv'></span>

<h3>Description</h3>

<p>The <code><a href="#topic+plot.kernel.conv">plot.kernel.conv</a></code> function loops through calls to 
the <code><a href="#topic+kernel.conv">kernel.conv</a></code> function. Plot for convolutions of 
r'th derivative kernel function one-dimensional.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'kernel.conv'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.kernel.conv_+3A_x">x</code></td>
<td>
<p>object of class <code>kernel.conv</code> (output from <code><a href="#topic+kernel.conv">kernel.conv</a></code>).</p>
</td></tr>
<tr><td><code id="plot.kernel.conv_+3A_...">...</code></td>
<td>
<p>other graphics parameters, see <code><a href="graphics.html#topic+par">par</a></code> in package &quot;graphics&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Plot of 1-d for convolution of r'th derivative kernel function are sent to graphics window.
</p>


<h3>Author(s)</h3>

<p>Arsalane Chouaib Guidoum <a href="mailto:acguidoum@usthb.dz">acguidoum@usthb.dz</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kernel.conv">kernel.conv</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Gaussian kernel
oldpar &lt;- par(no.readonly = TRUE)
dev.new()
par(mfrow=c(2,2))
plot(kernel.conv(kernel="gaussian",deriv.order=0))
plot(kernel.conv(kernel="gaussian",deriv.order=1))
plot(kernel.conv(kernel="gaussian",deriv.order=2))
plot(kernel.conv(kernel="gaussian",deriv.order=3))

## Silverman kernel

dev.new()
par(mfrow=c(2,2))
plot(kernel.conv(kernel="silverman",deriv.order=0))
plot(kernel.conv(kernel="silverman",deriv.order=1))
plot(kernel.conv(kernel="silverman",deriv.order=2))
plot(kernel.conv(kernel="silverman",deriv.order=3))

par(oldpar)
</code></pre>

<hr>
<h2 id='plot.kernel.fun'>
Plot of r'th Derivative Kernel Function
</h2><span id='topic+plot.kernel.fun'></span>

<h3>Description</h3>

<p>The <code><a href="#topic+plot.kernel.fun">plot.kernel.fun</a></code> function loops through calls to 
the <code><a href="#topic+kernel.fun">kernel.fun</a></code> function. Plot for r'th derivative 
kernel function one-dimensional.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'kernel.fun'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.kernel.fun_+3A_x">x</code></td>
<td>
<p>object of class <code>kernel.fun</code> (output from <code><a href="#topic+kernel.fun">kernel.fun</a></code>).</p>
</td></tr>
<tr><td><code id="plot.kernel.fun_+3A_...">...</code></td>
<td>
<p>other graphics parameters, see <code><a href="graphics.html#topic+par">par</a></code> in package &quot;graphics&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Plot of 1-d for r'th derivative kernel function are sent to graphics window.
</p>


<h3>Author(s)</h3>

<p>Arsalane Chouaib Guidoum <a href="mailto:acguidoum@usthb.dz">acguidoum@usthb.dz</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kernel.fun">kernel.fun</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Gaussian kernel
oldpar &lt;- par(no.readonly = TRUE)
dev.new()
par(mfrow=c(2,2))
plot(kernel.fun(kernel="gaussian",deriv.order=0))
plot(kernel.fun(kernel="gaussian",deriv.order=1))
plot(kernel.fun(kernel="gaussian",deriv.order=2))
plot(kernel.fun(kernel="gaussian",deriv.order=3))

## Silverman kernel

dev.new()
par(mfrow=c(2,2))
plot(kernel.fun(kernel="silverman",deriv.order=0))
plot(kernel.fun(kernel="silverman",deriv.order=1))
plot(kernel.fun(kernel="silverman",deriv.order=2))
plot(kernel.fun(kernel="silverman",deriv.order=3))

par(oldpar)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
