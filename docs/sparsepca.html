<!DOCTYPE html><html><head><title>Help for package sparsepca</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sparsepca}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#robspca'><p>Robust Sparse Principal Component Analysis (robspca).</p></a></li>
<li><a href='#rspca'><p>Randomized Sparse Principal Component Analysis (rspca).</p></a></li>
<li><a href='#spca'><p>Sparse Principal Component Analysis (spca).</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Sparse Principal Component Analysis (SPCA)</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.2</td>
</tr>
<tr>
<td>Author:</td>
<td>N. Benjamin Erichson, Peng Zheng, and Sasha Aravkin</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>N. Benjamin Erichson &lt;erichson@uw.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Sparse principal component analysis (SPCA) attempts to find sparse weight vectors (loadings), i.e., a weight vector with only a few 'active' (nonzero) values. This approach provides better interpretability for the principal components in high-dimensional data settings. This is, because the principal components are formed as a linear combination of only a few of the original variables. This package provides efficient routines to compute SPCA. Specifically, a variable projection solver is used to compute the sparse solution. In addition, a fast randomized accelerated SPCA routine and a robust SPCA routine is provided. Robust SPCA allows to capture grossly corrupted entries in the data. The methods are discussed in detail by N. Benjamin Erichson et al. (2018) &lt;<a href="https://doi.org/10.48550/arXiv.1804.00341">doi:10.48550/arXiv.1804.00341</a>&gt;. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/erichson/spca">https://github.com/erichson/spca</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/erichson/spca/issues">https://github.com/erichson/spca/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>rsvd</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-04-09 22:02:31 UTC; benli</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-04-11 08:17:42 UTC</td>
</tr>
</table>
<hr>
<h2 id='robspca'>Robust Sparse Principal Component Analysis (robspca).</h2><span id='topic+robspca'></span>

<h3>Description</h3>

<p>Implementation of robust SPCA, using variable projection as an optimization strategy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>robspca(X, k = NULL, alpha = 1e-04, beta = 1e-04, gamma = 100,
  center = TRUE, scale = FALSE, max_iter = 1000, tol = 1e-05,
  verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="robspca_+3A_x">X</code></td>
<td>
<p>array_like; <br />
a real <code class="reqn">(n, p)</code> input matrix (or data frame) to be decomposed.</p>
</td></tr>
<tr><td><code id="robspca_+3A_k">k</code></td>
<td>
<p>integer; <br />
specifies the target rank, i.e., the number of components to be computed.</p>
</td></tr>
<tr><td><code id="robspca_+3A_alpha">alpha</code></td>
<td>
<p>float; <br />
Sparsity controlling parameter. Higher values lead to sparser components.</p>
</td></tr>
<tr><td><code id="robspca_+3A_beta">beta</code></td>
<td>
<p>float; <br />
Amount of ridge shrinkage to apply in order to improve conditioning.</p>
</td></tr>
<tr><td><code id="robspca_+3A_gamma">gamma</code></td>
<td>
<p>float; <br />
Sparsity controlling parameter for the error matrix S.
Smaller values lead to a larger amount of noise removeal.</p>
</td></tr>
<tr><td><code id="robspca_+3A_center">center</code></td>
<td>
<p>bool; <br />
logical value which indicates whether the variables should be
shifted to be zero centered (TRUE by default).</p>
</td></tr>
<tr><td><code id="robspca_+3A_scale">scale</code></td>
<td>
<p>bool; <br />
logical value which indicates whether the variables should
be scaled to have unit variance (FALSE by default).</p>
</td></tr>
<tr><td><code id="robspca_+3A_max_iter">max_iter</code></td>
<td>
<p>integer; <br />
maximum number of iterations to perform before exiting.</p>
</td></tr>
<tr><td><code id="robspca_+3A_tol">tol</code></td>
<td>
<p>float; <br />
stopping tolerance for the convergence criterion.</p>
</td></tr>
<tr><td><code id="robspca_+3A_verbose">verbose</code></td>
<td>
<p>bool; <br />
logical value which indicates whether progress is printed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Sparse principal component analysis is a modern variant of PCA. Specifically, SPCA attempts to find sparse
weight vectors (loadings), i.e., a weight vector with only a few 'active' (nonzero) values. This approach
leads to an improved interpretability of the model, because the principal components are formed as a
linear combination of only a few of the original variables. Further, SPCA avoids overfitting in a
high-dimensional data setting where the number of variables <code class="reqn">p</code> is greater than the number of
observations <code class="reqn">n</code>.
</p>
<p>Such a parsimonious model is obtained by introducing prior information like sparsity promoting regularizers.
More concreatly, given an <code class="reqn">(n,p)</code> data matrix <code class="reqn">X</code>, robust SPCA attemps to minimize the following
objective function:
</p>
<p style="text-align: center;"><code class="reqn"> f(A,B) = \frac{1}{2} \| X - X B A^\top - S \|^2_F + \psi(B) + \gamma \|S\|_1 </code>
</p>

<p>where <code class="reqn">B</code> is the sparse weight matrix (loadings) and <code class="reqn">A</code> is an orthonormal matrix.
<code class="reqn">\psi</code> denotes a sparsity inducing regularizer such as the LASSO (<code class="reqn">\ell_1</code> norm) or the elastic net
(a combination of the <code class="reqn">\ell_1</code> and <code class="reqn">\ell_2</code> norm). The matrix <code class="reqn">S</code> captures grossly corrupted outliers in the data.
</p>
<p>The principal components <code class="reqn">Z</code> are formed as
</p>
<p style="text-align: center;"><code class="reqn"> Z = X B </code>
</p>

<p>and the data can be approximately rotated back as
</p>
<p style="text-align: center;"><code class="reqn"> \tilde{X} = Z A^\top </code>
</p>

<p>The print and summary method can be used to present the results in a nice format.
</p>


<h3>Value</h3>

<p><code>spca</code> returns a list containing the following three components:
</p>
<table>
<tr><td><code>loadings</code></td>
<td>
<p>  array_like; <br />
sparse loadings (weight) vector;  <code class="reqn">(p, k)</code> dimensional array.
</p>
</td></tr>
<tr><td><code>transform</code></td>
<td>
<p>  array_like; <br />
the approximated inverse transform; <code class="reqn">(p, k)</code> dimensional array.
</p>
</td></tr>
<tr><td><code>scores</code></td>
<td>
<p>  array_like; <br />
the principal component scores; <code class="reqn">(n, k)</code> dimensional array.
</p>
</td></tr>
<tr><td><code>sparse</code></td>
<td>
<p>  array_like; <br />
sparse matrix capturing outliers in the data; <code class="reqn">(n, p)</code> dimensional array.
</p>
</td></tr>
<tr><td><code>eigenvalues</code></td>
<td>
<p>  array_like; <br />
the approximated eigenvalues; <code class="reqn">(k)</code> dimensional array.
</p>
</td></tr>
<tr><td><code>center</code>, <code>scale</code></td>
<td>
<p>  array_like; <br />
the centering and scaling used.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>N. Benjamin Erichson, Peng Zheng, and Sasha Aravkin
</p>


<h3>References</h3>


<ul>
<li><p> [1] N. B. Erichson, P. Zheng, K. Manohar, S. Brunton, J. N. Kutz, A. Y. Aravkin.
&quot;Sparse Principal Component Analysis via Variable Projection.&quot;
Submitted to IEEE Journal of Selected Topics on Signal Processing (2018).
(available at 'arXiv <a href="https://arxiv.org/abs/1804.00341">https://arxiv.org/abs/1804.00341</a>).
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+rspca">rspca</a></code>, <code><a href="#topic+spca">spca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Create artifical data
m &lt;- 10000
V1 &lt;- rnorm(m, 0, 290)
V2 &lt;- rnorm(m, 0, 300)
V3 &lt;- -0.1*V1 + 0.1*V2 + rnorm(m,0,100)

X &lt;- cbind(V1,V1,V1,V1, V2,V2,V2,V2, V3,V3)
X &lt;- X + matrix(rnorm(length(X),0,1), ncol = ncol(X), nrow = nrow(X))

# Compute SPCA
out &lt;- robspca(X, k=3, alpha=1e-3, beta=1e-5, gamma=5, center = TRUE, scale = FALSE, verbose=0)
print(out)
summary(out)

</code></pre>

<hr>
<h2 id='rspca'>Randomized Sparse Principal Component Analysis (rspca).</h2><span id='topic+rspca'></span>

<h3>Description</h3>

<p>Randomized accelerated implementation of SPCA, using variable projection as an optimization strategy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rspca(X, k = NULL, alpha = 1e-04, beta = 1e-04, center = TRUE,
  scale = FALSE, max_iter = 1000, tol = 1e-05, o = 20, q = 2,
  verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rspca_+3A_x">X</code></td>
<td>
<p>array_like; <br />
a real <code class="reqn">(n, p)</code> input matrix (or data frame) to be decomposed.</p>
</td></tr>
<tr><td><code id="rspca_+3A_k">k</code></td>
<td>
<p>integer; <br />
specifies the target rank, i.e., the number of components to be computed.</p>
</td></tr>
<tr><td><code id="rspca_+3A_alpha">alpha</code></td>
<td>
<p>float; <br />
Sparsity controlling parameter. Higher values lead to sparser components.</p>
</td></tr>
<tr><td><code id="rspca_+3A_beta">beta</code></td>
<td>
<p>float; <br />
Amount of ridge shrinkage to apply in order to improve conditioning.</p>
</td></tr>
<tr><td><code id="rspca_+3A_center">center</code></td>
<td>
<p>bool; <br />
logical value which indicates whether the variables should be
shifted to be zero centered (TRUE by default).</p>
</td></tr>
<tr><td><code id="rspca_+3A_scale">scale</code></td>
<td>
<p>bool; <br />
logical value which indicates whether the variables should
be scaled to have unit variance (FALSE by default).</p>
</td></tr>
<tr><td><code id="rspca_+3A_max_iter">max_iter</code></td>
<td>
<p>integer; <br />
maximum number of iterations to perform before exiting.</p>
</td></tr>
<tr><td><code id="rspca_+3A_tol">tol</code></td>
<td>
<p>float; <br />
stopping tolerance for the convergence criterion.</p>
</td></tr>
<tr><td><code id="rspca_+3A_o">o</code></td>
<td>
<p>integer; <br />
oversampling parameter (default <code class="reqn">o=20</code>).</p>
</td></tr>
<tr><td><code id="rspca_+3A_q">q</code></td>
<td>
<p>integer; <br />
number of additional power iterations (default <code class="reqn">q=2</code>).</p>
</td></tr>
<tr><td><code id="rspca_+3A_verbose">verbose</code></td>
<td>
<p>bool; <br />
logical value which indicates whether progress is printed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Sparse principal component analysis is a modern variant of PCA. Specifically, SPCA attempts to find sparse
weight vectors (loadings), i.e., a weight vector with only a few 'active' (nonzero) values. This approach
leads to an improved interpretability of the model, because the principal components are formed as a
linear combination of only a few of the original variables. Further, SPCA avoids overfitting in a
high-dimensional data setting where the number of variables <code class="reqn">p</code> is greater than the number of
observations <code class="reqn">n</code>.
</p>
<p>Such a parsimonious model is obtained by introducing prior information like sparsity promoting regularizers.
More concreatly, given an <code class="reqn">(n,p)</code> data matrix <code class="reqn">X</code>, SPCA attemps to minimize the following
objective function:
</p>
<p style="text-align: center;"><code class="reqn"> f(A,B) = \frac{1}{2} \| X - X B A^\top \|^2_F + \psi(B) </code>
</p>

<p>where <code class="reqn">B</code> is the sparse weight (loadings) matrix and <code class="reqn">A</code> is an orthonormal matrix.
<code class="reqn">\psi</code> denotes a sparsity inducing regularizer such as the LASSO (<code class="reqn">\ell_1</code> norm) or the elastic net
(a combination of the <code class="reqn">\ell_1</code> and <code class="reqn">\ell_2</code> norm). The principal components <code class="reqn">Z</code> are formed as
</p>
<p style="text-align: center;"><code class="reqn"> Z = X B </code>
</p>

<p>and the data can be approximately rotated back as
</p>
<p style="text-align: center;"><code class="reqn"> \tilde{X} = Z A^\top </code>
</p>

<p>The print and summary method can be used to present the results in a nice format.
</p>


<h3>Value</h3>

<p><code>spca</code> returns a list containing the following three components:
</p>
<table>
<tr><td><code>loadings</code></td>
<td>
<p>  array_like; <br />
sparse loadings (weight) vector;  <code class="reqn">(p, k)</code> dimensional array.
</p>
</td></tr>
<tr><td><code>transform</code></td>
<td>
<p>  array_like; <br />
the approximated inverse transform; <code class="reqn">(p, k)</code> dimensional array.
</p>
</td></tr>
<tr><td><code>scores</code></td>
<td>
<p>  array_like; <br />
the principal component scores; <code class="reqn">(n, k)</code> dimensional array.
</p>
</td></tr>
<tr><td><code>eigenvalues</code></td>
<td>
<p>  array_like; <br />
the approximated eigenvalues; <code class="reqn">(k)</code> dimensional array.
</p>
</td></tr>
<tr><td><code>center</code>, <code>scale</code></td>
<td>
<p>  array_like; <br />
the centering and scaling used.
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This implementation uses randomized methods for linear algebra to speedup the computations.
<code class="reqn">o</code> is an oversampling parameter to improve the approximation.
A value of at least 10 is recommended, and <code class="reqn">o=20</code> is set by default.
</p>
<p>The parameter <code class="reqn">q</code> specifies the number of power (subspace) iterations
to reduce the approximation error. The power scheme is recommended,
if the singular values decay slowly. In practice, 2 or 3 iterations
achieve good results, however, computing power iterations increases the
computational costs. The power scheme is set to <code class="reqn">q=2</code> by default.
</p>
<p>If <code class="reqn">k &gt; (min(n,p)/4)</code>, a the deterministic <code><a href="#topic+spca">spca</a></code>
algorithm might be faster.
</p>


<h3>Author(s)</h3>

<p>N. Benjamin Erichson, Peng Zheng, and Sasha Aravkin
</p>


<h3>References</h3>


<ul>
<li><p> [1] N. B. Erichson, P. Zheng, K. Manohar, S. Brunton, J. N. Kutz, A. Y. Aravkin.
&quot;Sparse Principal Component Analysis via Variable Projection.&quot;
Submitted to IEEE Journal of Selected Topics on Signal Processing (2018).
(available at 'arXiv <a href="https://arxiv.org/abs/1804.00341">https://arxiv.org/abs/1804.00341</a>).
</p>
</li>
<li><p>  [1] N. B. Erichson, S. Voronin, S. Brunton, J. N. Kutz.
&quot;Randomized matrix decompositions using R.&quot;
Submitted to Journal of Statistical Software (2016).
(available at 'arXiv <a href="http://arxiv.org/abs/1608.02148">http://arxiv.org/abs/1608.02148</a>).
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+spca">spca</a></code>, <code><a href="#topic+robspca">robspca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Create artifical data
m &lt;- 10000
V1 &lt;- rnorm(m, 0, 290)
V2 &lt;- rnorm(m, 0, 300)
V3 &lt;- -0.1*V1 + 0.1*V2 + rnorm(m,0,100)

X &lt;- cbind(V1,V1,V1,V1, V2,V2,V2,V2, V3,V3)
X &lt;- X + matrix(rnorm(length(X),0,1), ncol = ncol(X), nrow = nrow(X))

# Compute SPCA
out &lt;- rspca(X, k=3, alpha=1e-3, beta=1e-3, center = TRUE, scale = FALSE, verbose=0)
print(out)
summary(out)

</code></pre>

<hr>
<h2 id='spca'>Sparse Principal Component Analysis (spca).</h2><span id='topic+spca'></span>

<h3>Description</h3>

<p>Implementation of SPCA, using variable projection as an optimization strategy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>spca(X, k = NULL, alpha = 1e-04, beta = 1e-04, center = TRUE,
  scale = FALSE, max_iter = 1000, tol = 1e-05, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spca_+3A_x">X</code></td>
<td>
<p>array_like; <br />
a real <code class="reqn">(n, p)</code> input matrix (or data frame) to be decomposed.</p>
</td></tr>
<tr><td><code id="spca_+3A_k">k</code></td>
<td>
<p>integer; <br />
specifies the target rank, i.e., the number of components to be computed.</p>
</td></tr>
<tr><td><code id="spca_+3A_alpha">alpha</code></td>
<td>
<p>float; <br />
Sparsity controlling parameter. Higher values lead to sparser components.</p>
</td></tr>
<tr><td><code id="spca_+3A_beta">beta</code></td>
<td>
<p>float; <br />
Amount of ridge shrinkage to apply in order to improve conditioning.</p>
</td></tr>
<tr><td><code id="spca_+3A_center">center</code></td>
<td>
<p>bool; <br />
logical value which indicates whether the variables should be
shifted to be zero centered (TRUE by default).</p>
</td></tr>
<tr><td><code id="spca_+3A_scale">scale</code></td>
<td>
<p>bool; <br />
logical value which indicates whether the variables should
be scaled to have unit variance (FALSE by default).</p>
</td></tr>
<tr><td><code id="spca_+3A_max_iter">max_iter</code></td>
<td>
<p>integer; <br />
maximum number of iterations to perform before exiting.</p>
</td></tr>
<tr><td><code id="spca_+3A_tol">tol</code></td>
<td>
<p>float; <br />
stopping tolerance for the convergence criterion.</p>
</td></tr>
<tr><td><code id="spca_+3A_verbose">verbose</code></td>
<td>
<p>bool; <br />
logical value which indicates whether progress is printed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Sparse principal component analysis is a modern variant of PCA. Specifically, SPCA attempts to find sparse
weight vectors (loadings), i.e., a weight vector with only a few 'active' (nonzero) values. This approach
leads to an improved interpretability of the model, because the principal components are formed as a
linear combination of only a few of the original variables. Further, SPCA avoids overfitting in a
high-dimensional data setting where the number of variables <code class="reqn">p</code> is greater than the number of
observations <code class="reqn">n</code>.
</p>
<p>Such a parsimonious model is obtained by introducing prior information like sparsity promoting regularizers.
More concreatly, given an <code class="reqn">(n,p)</code> data matrix <code class="reqn">X</code>, SPCA attemps to minimize the following
objective function:
</p>
<p style="text-align: center;"><code class="reqn"> f(A,B) = \frac{1}{2} \| X - X B A^\top \|^2_F + \psi(B) </code>
</p>

<p>where <code class="reqn">B</code> is the sparse weight (loadings) matrix and <code class="reqn">A</code> is an orthonormal matrix.
<code class="reqn">\psi</code> denotes a sparsity inducing regularizer such as the LASSO (<code class="reqn">\ell_1</code> norm) or the elastic net
(a combination of the <code class="reqn">\ell_1</code> and <code class="reqn">\ell_2</code> norm). The principal components <code class="reqn">Z</code> are formed as
</p>
<p style="text-align: center;"><code class="reqn"> Z = X B </code>
</p>

<p>and the data can be approximately rotated back as
</p>
<p style="text-align: center;"><code class="reqn"> \tilde{X} = Z A^\top </code>
</p>

<p>The print and summary method can be used to present the results in a nice format.
</p>


<h3>Value</h3>

<p><code>spca</code> returns a list containing the following three components:
</p>
<table>
<tr><td><code>loadings</code></td>
<td>
<p>  array_like; <br />
sparse loadings (weight) vector;  <code class="reqn">(p, k)</code> dimensional array.
</p>
</td></tr>
<tr><td><code>transform</code></td>
<td>
<p>  array_like; <br />
the approximated inverse transform; <code class="reqn">(p, k)</code> dimensional array.
</p>
</td></tr>
<tr><td><code>scores</code></td>
<td>
<p>  array_like; <br />
the principal component scores; <code class="reqn">(n, k)</code> dimensional array.
</p>
</td></tr>
<tr><td><code>eigenvalues</code></td>
<td>
<p>  array_like; <br />
the approximated eigenvalues; <code class="reqn">(k)</code> dimensional array.
</p>
</td></tr>
<tr><td><code>center</code>, <code>scale</code></td>
<td>
<p>  array_like; <br />
the centering and scaling used.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>N. Benjamin Erichson, Peng Zheng, and Sasha Aravkin
</p>


<h3>References</h3>


<ul>
<li><p> [1] N. B. Erichson, P. Zheng, K. Manohar, S. Brunton, J. N. Kutz, A. Y. Aravkin.
&quot;Sparse Principal Component Analysis via Variable Projection.&quot;
Submitted to IEEE Journal of Selected Topics on Signal Processing (2018).
(available at 'arXiv <a href="https://arxiv.org/abs/1804.00341">https://arxiv.org/abs/1804.00341</a>).
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+rspca">rspca</a></code>, <code><a href="#topic+robspca">robspca</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Create artifical data
m &lt;- 10000
V1 &lt;- rnorm(m, 0, 290)
V2 &lt;- rnorm(m, 0, 300)
V3 &lt;- -0.1*V1 + 0.1*V2 + rnorm(m,0,100)

X &lt;- cbind(V1,V1,V1,V1, V2,V2,V2,V2, V3,V3)
X &lt;- X + matrix(rnorm(length(X),0,1), ncol = ncol(X), nrow = nrow(X))

# Compute SPCA
out &lt;- spca(X, k=3, alpha=1e-3, beta=1e-3, center = TRUE, scale = FALSE, verbose=0)
print(out)
summary(out)


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
