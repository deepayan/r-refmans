<!DOCTYPE html><html><head><title>Help for package multiROC</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {multiROC}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#cal_auc'><p>Area under ROC curve</p></a></li>
<li><a href='#cal_confus'><p>Calculate confusion matrices</p></a></li>
<li><a href='#multi_pr'><p>Multi-class classification PR</p></a></li>
<li><a href='#multi_roc'><p>Multi-class classification ROC</p></a></li>
<li><a href='#plot_pr_data'>
<p>Generate PR plotting data</p></a></li>
<li><a href='#plot_roc_data'>
<p>Generate ROC plotting data</p></a></li>
<li><a href='#pr_auc_with_ci'><p>Output of PR bootstrap confidence intervals</p></a></li>
<li><a href='#pr_ci'>
<p>PR bootstrap confidence intervals</p></a></li>
<li><a href='#roc_auc_with_ci'><p>Output of ROC bootstrap confidence intervals</p></a></li>
<li><a href='#roc_ci'><p>ROC bootstrap confidence intervals</p></a></li>
<li><a href='#test_data'>
<p>Example dataset</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Calculating and Visualizing ROC and PR Curves Across Multi-Class
Classifications</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Tools to solve real-world problems with multiple classes classifications by computing the areas under ROC and PR curve via micro-averaging and macro-averaging. The vignettes of this package can be found via <a href="https://github.com/WandeRum/multiROC">https://github.com/WandeRum/multiROC</a>. The methodology is described in V. Van Asch (2013) <a href="https://www.clips.uantwerpen.be/~vincent/pdf/microaverage.pdf">https://www.clips.uantwerpen.be/~vincent/pdf/microaverage.pdf</a> and Pedregosa et al. (2011) <a href="http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html">http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html</a>.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Imports:</td>
<td>zoo, magrittr, boot, stats</td>
</tr>
<tr>
<td>Suggests:</td>
<td>dplyr, ggplot2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-06-26 20:24:05 UTC</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1.9000</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-06-26 20:14:17 UTC; jingye</td>
</tr>
<tr>
<td>Author:</td>
<td>Runmin Wei [aut, cre],
  Jingye Wang [aut],
  Wei Jia [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Runmin Wei &lt;runmin@hawaii.edu&gt;</td>
</tr>
</table>
<hr>
<h2 id='cal_auc'>Area under ROC curve</h2><span id='topic+cal_auc'></span>

<h3>Description</h3>

<p>This function calculates the area under ROC curve</p>


<h3>Usage</h3>

<pre><code class='language-R'>cal_auc(X, Y)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cal_auc_+3A_x">X</code></td>
<td>
<p>A vector of true positive rate</p>
</td></tr>
<tr><td><code id="cal_auc_+3A_y">Y</code></td>
<td>
<p>A vector of false positive rate, same length with TPR</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the area under ROC curve.
</p>


<h3>Value</h3>

<p>A numeric value of AUC will be returned.
</p>


<h3>References</h3>

<p><a href="https://www.r-bloggers.com/calculating-auc-the-area-under-a-roc-curve/">https://www.r-bloggers.com/calculating-auc-the-area-under-a-roc-curve/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cal_confus">cal_confus()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(test_data)
true_vec &lt;- test_data[, 1]
pred_vec &lt;- test_data[, 5]
confus_res &lt;- cal_confus(true_vec, pred_vec)
AUC_res &lt;- cal_auc(confus_res$TPR, confus_res$FPR)
</code></pre>

<hr>
<h2 id='cal_confus'>Calculate confusion matrices</h2><span id='topic+cal_confus'></span>

<h3>Description</h3>

<p>This function calculates the confusion matrices across different cutoff points.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cal_confus(true_vec, pred_vec, force_diag=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cal_confus_+3A_true_vec">true_vec</code></td>
<td>
<p>A binary vector of real labels</p>
</td></tr>
<tr><td><code id="cal_confus_+3A_pred_vec">pred_vec</code></td>
<td>
<p>A continuous predicted score(probabilities) vector, must be the same length with <code>true_vec</code></p>
</td></tr>
<tr><td><code id="cal_confus_+3A_force_diag">force_diag</code></td>
<td>
<p>If TRUE, TPR and FPR will be forced to across (0, 0) and (1, 1)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the TP, FP, FN, TN, TPR, FPR and PPV across different cutoff points of <code>pred_vec</code>. TPR and FPR are forced to across (0, 0) and (1, 1) if  <code>force_diag=TRUE</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>TP</code></td>
<td>
<p>True positive</p>
</td></tr>
<tr><td><code>FP</code></td>
<td>
<p>False positive</p>
</td></tr>
<tr><td><code>FN</code></td>
<td>
<p>False negative</p>
</td></tr>
<tr><td><code>TN</code></td>
<td>
<p>True negative</p>
</td></tr>
<tr><td><code>TPR</code></td>
<td>
<p>True positive rate</p>
</td></tr>
<tr><td><code>FPR</code></td>
<td>
<p>False positive rate</p>
</td></tr>
<tr><td><code>PPV</code></td>
<td>
<p>Positive predictive value</p>
</td></tr>
</table>


<h3>References</h3>

<p><a href="https://en.wikipedia.org/wiki/Confusion_matrix">https://en.wikipedia.org/wiki/Confusion_matrix</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(test_data)
true_vec &lt;- test_data[, 1]
pred_vec &lt;- test_data[, 5]
confus_res &lt;- cal_confus(true_vec, pred_vec)
</code></pre>

<hr>
<h2 id='multi_pr'>Multi-class classification PR</h2><span id='topic+multi_pr'></span>

<h3>Description</h3>

<p>This function calculates the Precision, Recall and AUC of multi-class classifications.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multi_pr(data, force_diag=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multi_pr_+3A_data">data</code></td>
<td>
<p>A data frame contain true labels of multiple groups and corresponding predictive scores</p>
</td></tr>
<tr><td><code id="multi_pr_+3A_force_diag">force_diag</code></td>
<td>
<p>If TRUE, TPR and FPR will be forced to across (0, 0) and (1, 1)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A data frame is required for this function as input. This data frame should contains true label (0 - Negative, 1 - Positive) columns named as XX_true (e.g. S1_true, S2_true and S3_true) and predictive scores (continuous) columns named as XX_pred_YY (e.g. S1_pred_SVM, S2_pred_RF), thus this function allows calcluating ROC on mulitiple classifiers.
</p>
<p>Predictive scores could be probabilities among [0, 1] and other continuous values.
For each classifier, the number of columns should be equal to the number of groups of true labels. The order of columns won't affect results.
</p>
<p>Recall, Precision, AUC for each group and each method will be calculated. Macro/Micro-average AUC for all groups and each method will be calculated.
</p>
<p>Micro-average ROC/AUC was calculated by stacking all groups together, thus converting the multi-class classification into binary classification. Macro-average ROC/AUC was calculated by averaging all groups results (one vs rest) and linear interpolation was used between points of ROC.
</p>
<p>AUC will be calculated using function <code>cal_auc()</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>Recall</code></td>
<td>
<p>A list of recalls for each group, each method and micro-/macro- average</p>
</td></tr>
<tr><td><code>Precision</code></td>
<td>
<p>A list of precisions for each group, each method and micro-/macro- average</p>
</td></tr>
<tr><td><code>AUC</code></td>
<td>
<p>A list of AUCs for each group, each method and micro-/macro- average</p>
</td></tr>
<tr><td><code>Methods</code></td>
<td>
<p>A vector contains the name of different classifiers</p>
</td></tr>
<tr><td><code>Groups</code></td>
<td>
<p>A vector contains the name of different groups</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(test_data)
pr_test &lt;- multi_pr(test_data)
pr_test$AUC 
</code></pre>

<hr>
<h2 id='multi_roc'>Multi-class classification ROC</h2><span id='topic+multi_roc'></span>

<h3>Description</h3>

<p>This function calculates the Specificity, Sensitivity and AUC of multi-class classifications.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multi_roc(data, force_diag=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multi_roc_+3A_data">data</code></td>
<td>
<p>A data frame contain true labels of multiple groups and corresponding predictive scores</p>
</td></tr>
<tr><td><code id="multi_roc_+3A_force_diag">force_diag</code></td>
<td>
<p>If TRUE, TPR and FPR will be forced to across (0, 0) and (1, 1)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A data frame is required for this function as input. This data frame should contains true label (0 - Negative, 1 - Positive) columns named as XX_true (e.g. S1_true, S2_true and S3_true) and predictive scores (continuous) columns named as XX_pred_YY (e.g. S1_pred_SVM, S2_pred_RF), thus this function allows calcluating ROC on mulitiple classifiers.
</p>
<p>Predictive scores could be probabilities among [0, 1] and other continuous values.
For each classifier, the number of columns should be equal to the number of groups of true labels. The order of columns won't affect results.
</p>
<p>Specificity, Sensitivity, AUC for each group and each method will be calculated. Macro/Micro-average AUC for all groups and each method will be calculated.
</p>
<p>Micro-average ROC/AUC was calculated by stacking all groups together, thus converting the multi-class classification into binary classification. Macro-average ROC/AUC was calculated by averaging all groups results (one vs rest) and linear interpolation was used between points of ROC.
</p>
<p>AUC will be calculated using function <code>cal_auc()</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>Specificity</code></td>
<td>
<p>A list of specificities for each group, each method and micro-/macro- average</p>
</td></tr>
<tr><td><code>Sensitivity</code></td>
<td>
<p>A list of sensitivities for each group, each method and micro-/macro- average</p>
</td></tr>
<tr><td><code>AUC</code></td>
<td>
<p>A list of AUCs for each group, each method and micro-/macro- average</p>
</td></tr>
<tr><td><code>Methods</code></td>
<td>
<p>A vector contains the name of different classifiers</p>
</td></tr>
<tr><td><code>Groups</code></td>
<td>
<p>A vector contains the name of different groups</p>
</td></tr>
</table>


<h3>References</h3>

<p><a href="http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html">http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(test_data)
roc_test &lt;- multi_roc(test_data)
roc_test$AUC 
</code></pre>

<hr>
<h2 id='plot_pr_data'>
Generate PR plotting data
</h2><span id='topic+plot_pr_data'></span>

<h3>Description</h3>

<p>This function generates plotting PR data for following data visualization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_pr_data(pr_res)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_pr_data_+3A_pr_res">pr_res</code></td>
<td>
<p>A list of results from multi_pr function.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>pr_res_df</code></td>
<td>
<p>The dataframe of results from multi_pr function, which is easy be visualized by ggplot2.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(test_data)
pr_res &lt;- multi_pr(test_data)
pr_res_df &lt;- plot_pr_data(pr_res)
</code></pre>

<hr>
<h2 id='plot_roc_data'>
Generate ROC plotting data
</h2><span id='topic+plot_roc_data'></span>

<h3>Description</h3>

<p>This function generates plotting ROC data for following data visualization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_roc_data(roc_res)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_roc_data_+3A_roc_res">roc_res</code></td>
<td>
<p>A list of results from multi_roc function.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>roc_res_df</code></td>
<td>
<p>The dataframe of results from multi_roc function, which is easy be visualized by ggplot2.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data(test_data)
roc_res &lt;- multi_roc(test_data)
roc_res_df &lt;- plot_roc_data(roc_res)
</code></pre>

<hr>
<h2 id='pr_auc_with_ci'>Output of PR bootstrap confidence intervals</h2><span id='topic+pr_auc_with_ci'></span>

<h3>Description</h3>

<p>This function uses bootstrap to generate five types of equi-tailed two-sided confidence intervals of PR-AUC with different required percentages and output a dataframe with AUCs, lower CIs, and higher CIs of all methods and groups.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pr_auc_with_ci(data, conf= 0.95, type='bca', R = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pr_auc_with_ci_+3A_data">data</code></td>
<td>
<p>A data frame contains true labels of multiple groups and corresponding predictive scores.</p>
</td></tr>
<tr><td><code id="pr_auc_with_ci_+3A_conf">conf</code></td>
<td>
<p>A scalar contains the required level of confidence intervals, and the default number is 0.95.</p>
</td></tr>
<tr><td><code id="pr_auc_with_ci_+3A_type">type</code></td>
<td>
<p>A vector of character strings includes five different types of equi-tailed two-sided nonparametric confidence intervals (e.g., &quot;norm&quot;,&quot;basic&quot;, &quot;stud&quot;, &quot;perc&quot;, &quot;bca&quot;).</p>
</td></tr>
<tr><td><code id="pr_auc_with_ci_+3A_r">R</code></td>
<td>
<p>A scalar contains the number of bootstrap replicates, and the default number is 100.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A data frame is required for this function as input. This data frame should contains true label (0 - Negative, 1 - Positive) columns named as XX_true (e.g. S1_true, S2_true and S3_true) and predictive scores (continuous) columns named as XX_pred_YY (e.g. S1_pred_SVM, S2_pred_RF).
Predictive scores could be probabilities among [0, 1] and other continuous values.
For each classifier, the number of columns should be equal to the number of groups of true labels. The order of columns won't affect results.
</p>


<h3>Value</h3>

<table>
<tr><td><code>norm</code></td>
<td>
<p>Using the normal approximation to calculate the confidence intervals.</p>
</td></tr>
<tr><td><code>basic</code></td>
<td>
<p>Using the basic bootstrap method to calculate the confidence intervals.</p>
</td></tr>
<tr><td><code>stud</code></td>
<td>
<p>Using the studentized bootstrap method to calculate the confidence intervals.</p>
</td></tr>
<tr><td><code>perc</code></td>
<td>
<p>Using the bootstrap percentile method to calculate the confidence intervals.</p>
</td></tr>
<tr><td><code>bca</code></td>
<td>
<p>Using the adjusted bootstrap percentile method to calculate the confidence intervals.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: data(test_data)
pr_auc_with_ci_res &lt;- pr_auc_with_ci(test_data, conf= 0.95, type='bca', R = 100)
## End(Not run)
</code></pre>

<hr>
<h2 id='pr_ci'>
PR bootstrap confidence intervals
</h2><span id='topic+pr_ci'></span>

<h3>Description</h3>

<p>This function uses bootstrap to generate five types of equi-tailed two-sided confidence intervals of PR-AUC with different required percentages.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pr_ci(data, conf= 0.95, type='basic', R = 100, index = 4)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pr_ci_+3A_data">data</code></td>
<td>
<p>A data frame contains true labels of multiple groups and corresponding predictive scores.</p>
</td></tr>
<tr><td><code id="pr_ci_+3A_conf">conf</code></td>
<td>
<p>A scalar contains the required level of confidence intervals, and the default number is 0.95.</p>
</td></tr>
<tr><td><code id="pr_ci_+3A_type">type</code></td>
<td>
<p>A vector of character strings includes five different types of equi-tailed two-sided nonparametric confidence intervals (e.g., &quot;norm&quot;,&quot;basic&quot;, &quot;stud&quot;, &quot;perc&quot;, &quot;bca&quot;, &quot;all&quot;).</p>
</td></tr>
<tr><td><code id="pr_ci_+3A_r">R</code></td>
<td>
<p>A scalar contains the number of bootstrap replicates, and the default number is 100.</p>
</td></tr>
<tr><td><code id="pr_ci_+3A_index">index</code></td>
<td>
<p>A scalar contains the position of the variable of interest.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A data frame is required for this function as input. This data frame should contains true label (0 - Negative, 1 - Positive) columns named as XX_true (e.g. S1_true, S2_true and S3_true) and predictive scores (continuous) columns named as XX_pred_YY (e.g. S1_pred_SVM, S2_pred_RF).
Predictive scores could be probabilities among [0, 1] and other continuous values.
For each classifier, the number of columns should be equal to the number of groups of true labels. The order of columns won't affect results.
</p>


<h3>Value</h3>

<table>
<tr><td><code>norm</code></td>
<td>
<p>Using the normal approximation to calculate the confidence intervals.</p>
</td></tr>
<tr><td><code>basic</code></td>
<td>
<p>Using the basic bootstrap method to calculate the confidence intervals.</p>
</td></tr>
<tr><td><code>stud</code></td>
<td>
<p>Using the studentized bootstrap method to calculate the confidence intervals.</p>
</td></tr>
<tr><td><code>perc</code></td>
<td>
<p>Using the bootstrap percentile method to calculate the confidence intervals.</p>
</td></tr>
<tr><td><code>bca</code></td>
<td>
<p>Using the adjusted bootstrap percentile method to calculate the confidence intervals.</p>
</td></tr>
<tr><td><code>all</code></td>
<td>
<p>Using all previous bootstrap methods to calculate the confidence intervals.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: data(test_data)
pr_ci_res &lt;- pr_ci(test_data, conf= 0.95, type='basic', R = 1000, index = 4)
## End(Not run)
</code></pre>

<hr>
<h2 id='roc_auc_with_ci'>Output of ROC bootstrap confidence intervals</h2><span id='topic+roc_auc_with_ci'></span>

<h3>Description</h3>

<p>This function uses bootstrap to generate five types of equi-tailed two-sided confidence intervals of ROC-AUC with different required percentages and output a dataframe with AUCs, lower CIs, and higher CIs of all methods and groups.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>roc_auc_with_ci(data, conf= 0.95, type='bca', R = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="roc_auc_with_ci_+3A_data">data</code></td>
<td>
<p>A data frame contains true labels of multiple groups and corresponding predictive scores.</p>
</td></tr>
<tr><td><code id="roc_auc_with_ci_+3A_conf">conf</code></td>
<td>
<p>A scalar contains the required level of confidence intervals, and the default number is 0.95.</p>
</td></tr>
<tr><td><code id="roc_auc_with_ci_+3A_type">type</code></td>
<td>
<p>A vector of character strings includes five different types of equi-tailed two-sided nonparametric confidence intervals (e.g., &quot;norm&quot;,&quot;basic&quot;, &quot;stud&quot;, &quot;perc&quot;, &quot;bca&quot;).</p>
</td></tr>
<tr><td><code id="roc_auc_with_ci_+3A_r">R</code></td>
<td>
<p>A scalar contains the number of bootstrap replicates, and the default number is 100.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A data frame is required for this function as input. This data frame should contains true label (0 - Negative, 1 - Positive) columns named as XX_true (e.g. S1_true, S2_true and S3_true) and predictive scores (continuous) columns named as XX_pred_YY (e.g. S1_pred_SVM, S2_pred_RF).
Predictive scores could be probabilities among [0, 1] and other continuous values.
For each classifier, the number of columns should be equal to the number of groups of true labels. The order of columns won't affect results.
</p>


<h3>Value</h3>

<table>
<tr><td><code>norm</code></td>
<td>
<p>Using the normal approximation to calculate the confidence intervals.</p>
</td></tr>
<tr><td><code>basic</code></td>
<td>
<p>Using the basic bootstrap method to calculate the confidence intervals.</p>
</td></tr>
<tr><td><code>stud</code></td>
<td>
<p>Using the studentized bootstrap method to calculate the confidence intervals.</p>
</td></tr>
<tr><td><code>perc</code></td>
<td>
<p>Using the bootstrap percentile method to calculate the confidence intervals.</p>
</td></tr>
<tr><td><code>bca</code></td>
<td>
<p>Using the adjusted bootstrap percentile method to calculate the confidence intervals.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: data(test_data)
roc_auc_with_ci_res &lt;- roc_auc_with_ci(test_data, conf= 0.95, type='bca', R = 100)
## End(Not run)
</code></pre>

<hr>
<h2 id='roc_ci'>ROC bootstrap confidence intervals</h2><span id='topic+roc_ci'></span>

<h3>Description</h3>

<p>This function uses bootstrap to generate five types of equi-tailed two-sided confidence intervals of ROC-AUC with different required percentages.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>roc_ci(data, conf= 0.95, type='basic', R = 100, index = 4)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="roc_ci_+3A_data">data</code></td>
<td>
<p>A data frame contains true labels of multiple groups and corresponding predictive scores.</p>
</td></tr>
<tr><td><code id="roc_ci_+3A_conf">conf</code></td>
<td>
<p>A scalar contains the required level of confidence intervals, and the default number is 0.95.</p>
</td></tr>
<tr><td><code id="roc_ci_+3A_type">type</code></td>
<td>
<p>A vector of character strings includes five different types of equi-tailed two-sided nonparametric confidence intervals (e.g., &quot;norm&quot;,&quot;basic&quot;, &quot;stud&quot;, &quot;perc&quot;, &quot;bca&quot;, &quot;all&quot;).</p>
</td></tr>
<tr><td><code id="roc_ci_+3A_r">R</code></td>
<td>
<p>A scalar contains the number of bootstrap replicates, and the default number is 100.</p>
</td></tr>
<tr><td><code id="roc_ci_+3A_index">index</code></td>
<td>
<p>A scalar contains the position of the variable of interest.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A data frame is required for this function as input. This data frame should contains true label (0 - Negative, 1 - Positive) columns named as XX_true (e.g. S1_true, S2_true and S3_true) and predictive scores (continuous) columns named as XX_pred_YY (e.g. S1_pred_SVM, S2_pred_RF).
Predictive scores could be probabilities among [0, 1] and other continuous values.
For each classifier, the number of columns should be equal to the number of groups of true labels. The order of columns won't affect results.
</p>


<h3>Value</h3>

<table>
<tr><td><code>norm</code></td>
<td>
<p>Using the normal approximation to calculate the confidence intervals.</p>
</td></tr>
<tr><td><code>basic</code></td>
<td>
<p>Using the basic bootstrap method to calculate the confidence intervals.</p>
</td></tr>
<tr><td><code>stud</code></td>
<td>
<p>Using the studentized bootstrap method to calculate the confidence intervals.</p>
</td></tr>
<tr><td><code>perc</code></td>
<td>
<p>Using the bootstrap percentile method to calculate the confidence intervals.</p>
</td></tr>
<tr><td><code>bca</code></td>
<td>
<p>Using the adjusted bootstrap percentile method to calculate the confidence intervals.</p>
</td></tr>
<tr><td><code>all</code></td>
<td>
<p>Using all previous bootstrap methods to calculate the confidence intervals.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: data(test_data)
roc_ci_res &lt;- roc_ci(test_data, conf= 0.95, type='basic', R = 1000, index = 4)
## End(Not run)
</code></pre>

<hr>
<h2 id='test_data'>
Example dataset
</h2><span id='topic+test_data'></span>

<h3>Description</h3>

<p>This example dataset contains two classifiers (m1, m2), and three groups (G1, G2, G3).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("test_data")</code></pre>


<h3>Format</h3>

<p>A data frame with 85 observations on the following 9 variables.
</p>

<dl>
<dt><code>G1_true</code></dt><dd><p>true labels of G1 (0 - Negative, 1 - Positive)</p>
</dd>
<dt><code>G2_true</code></dt><dd><p>true labels of G2 (0 - Negative, 1 - Positive)</p>
</dd>
<dt><code>G3_true</code></dt><dd><p>true labels of G3 (0 - Negative, 1 - Positive)</p>
</dd>
<dt><code>G1_pred_m1</code></dt><dd><p>predictive scores of G1 in the classifier m1</p>
</dd>
<dt><code>G2_pred_m1</code></dt><dd><p>predictive scores of G2 in the classifier m1</p>
</dd>
<dt><code>G3_pred_m1</code></dt><dd><p>predictive scores of G3 in the classifier m1</p>
</dd>
<dt><code>G1_pred_m2</code></dt><dd><p>predictive scores of G1 in the classifier m2</p>
</dd>
<dt><code>G2_pred_m2</code></dt><dd><p>predictive scores of G2 in the classifier m2</p>
</dd>
<dt><code>G3_pred_m2</code></dt><dd><p>predictive scores of G3 in the classifier m2</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>data(test_data)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
