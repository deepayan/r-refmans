<!DOCTYPE html><html><head><title>Help for package rags2ridges</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {rags2ridges}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#.armaRidgeP'><p>Core ridge precision estimators</p></a></li>
<li><a href='#ADdata'><p>R-objects related to metabolomics data on patients with Alzheimer's Disease</p></a></li>
<li><a href='#adjacentMat'><p>Transform real matrix into an adjacency matrix</p></a></li>
<li><a href='#CNplot'><p>Visualize the spectral condition number against the regularization parameter</p></a></li>
<li><a href='#Communities'><p>Search and visualize community-structures</p></a></li>
<li><a href='#conditionNumberPlot'><p>Visualize the spectral condition number against the regularization parameter</p></a></li>
<li><a href='#covML'><p>Maximum likelihood estimation of the covariance matrix</p></a></li>
<li><a href='#covMLknown'><p>Maximum likelihood estimation of the covariance matrix with assumptions on</p>
its structure</a></li>
<li><a href='#createS'><p>Simulate sample covariances or datasets</p></a></li>
<li><a href='#default.penalty'><p>Construct commonly used penalty matrices</p></a></li>
<li><a href='#default.target'><p>Generate a (data-driven) default target for usage in ridge-type shrinkage</p>
estimation</a></li>
<li><a href='#default.target.fused'><p>Generate data-driven targets for fused ridge estimation</p></a></li>
<li><a href='#DiffGraph'><p>Visualize the differential graph</p></a></li>
<li><a href='#edgeHeat'><p>Visualize (precision) matrix as a heatmap</p></a></li>
<li><a href='#evaluateS'><p>Evaluate numerical properties square matrix</p></a></li>
<li><a href='#evaluateSfit'><p>Visual inspection of the fit of a regularized precision matrix</p></a></li>
<li><a href='#fullMontyS'><p>Wrapper function</p></a></li>
<li><a href='#fused.test'><p>Test the necessity of fusion</p></a></li>
<li><a href='#getKEGGPathway'><p>Download KEGG pathway</p></a></li>
<li><a href='#GGMblockNullPenalty'><p>Generate the distribution of the penalty parameter under the null hypothesis</p>
of block-independence</a></li>
<li><a href='#GGMblockTest'><p>Test for block-indepedence</p></a></li>
<li><a href='#GGMmutualInfo'><p>Mutual information between two sets of variates within a multivariate normal</p>
distribution</a></li>
<li><a href='#GGMnetworkStats'><p>Gaussian graphical model network statistics</p></a></li>
<li><a href='#GGMnetworkStats.fused'><p>Gaussian graphical model network statistics</p></a></li>
<li><a href='#GGMpathStats'><p>Gaussian graphical model node pair path statistics</p></a></li>
<li><a href='#GGMpathStats.fused'><p>Fused gaussian graphical model node pair path statistics</p></a></li>
<li><a href='#hist.ptest'><p>Plot the results of a fusion test</p></a></li>
<li><a href='#is.Xlist'><p>Test if fused list-formats are correctly used</p></a></li>
<li><a href='#isSymmetricPD'><p>Test for symmetric positive (semi-)definiteness</p></a></li>
<li><a href='#kegg.target'><p>Construct target matrix from KEGG</p></a></li>
<li><a href='#KLdiv'><p>Kullback-Leibler divergence between two multivariate normal distributions</p></a></li>
<li><a href='#KLdiv.fused'><p>Fused Kullback-Leibler divergence for sets of distributions</p></a></li>
<li><a href='#loss'><p>Evaluate regularized precision under various loss functions</p></a></li>
<li><a href='#momentS'><p>Moments of the sample covariance matrix.</p></a></li>
<li><a href='#NLL'><p>Evaluate the (penalized) (fused) likelihood</p></a></li>
<li><a href='#optPenalty.aLOOCV'><p>Select optimal penalty parameter by approximate leave-one-out</p>
cross-validation</a></li>
<li><a href='#optPenalty.fused.grid'><p>Identify optimal ridge and fused ridge penalties</p></a></li>
<li><a href='#optPenalty.kCV'><p>Select optimal penalty parameter by <code class="reqn">K</code>-fold cross-validation</p></a></li>
<li><a href='#optPenalty.kCVauto'><p>Automatic search for optimal penalty parameter</p></a></li>
<li><a href='#optPenalty.LOOCV'><p>Select optimal penalty parameter by leave-one-out cross-validation</p></a></li>
<li><a href='#optPenalty.LOOCVauto'><p>Automatic search for optimal penalty parameter</p></a></li>
<li><a href='#pcor'><p>Compute partial correlation matrix or standardized precision matrix</p></a></li>
<li><a href='#pooledS'><p>Compute the pooled covariance or precision matrix estimate</p></a></li>
<li><a href='#print.optPenaltyFusedGrid'><p>Print and plot functions for fused grid-based cross-validation</p></a></li>
<li><a href='#print.ptest'><p>Print and summarize fusion test</p></a></li>
<li><a href='#pruneMatrix'><p>Prune square matrix to those variables having nonzero entries</p></a></li>
<li><a href='#rags2ridges-package'><p>Ridge estimation for high-dimensional precision matrices</p></a></li>
<li><a href='#ridgeP'><p>Ridge estimation for high-dimensional precision matrices</p></a></li>
<li><a href='#ridgeP.fused'><p>Fused ridge estimation</p></a></li>
<li><a href='#ridgePathS'><p>Visualize the regularization path</p></a></li>
<li><a href='#ridgeS'><p>Ridge estimation for high-dimensional precision matrices</p></a></li>
<li><a href='#rmvnormal'><p>Multivariate Gaussian simulation</p></a></li>
<li><a href='#sparsify'><p>Determine the support of a partial correlation/precision matrix</p></a></li>
<li><a href='#sparsify.fused'><p>Determine support of multiple partial correlation/precision matrices</p></a></li>
<li><a href='#symm'><p>Symmetrize matrix</p></a></li>
<li><a href='#Ugraph'><p>Visualize undirected graph</p></a></li>
<li><a href='#Union'><p>Subset 2 square matrices to union of variables having nonzero entries</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Ridge Estimation of Precision Matrices from High-Dimensional
Data</td>
</tr>
<tr>
<td>Version:</td>
<td>2.2.7</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Proper L2-penalized maximum likelihood estimators for precision 
  matrices and supporting functions to employ these estimators in a graphical 
  modeling setting. For details, see Peeters, Bilgrau, &amp; van Wieringen (2022) 
  &lt;<a href="https://doi.org/10.18637%2Fjss.v102.i04">doi:10.18637/jss.v102.i04</a>&gt; and associated publications.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.15.1)</td>
</tr>
<tr>
<td>Imports:</td>
<td>igraph, stats, methods, expm, reshape, ggplot2, Hmisc,
fdrtool, snowfall, sfsmisc, utils, grDevices, graphics, gRbase,
RBGL, graph, Rcpp, RSpectra</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>KeepSource:</td>
<td>yes</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://cfwp.github.io/rags2ridges/">https://cfwp.github.io/rags2ridges/</a>,
<a href="https://github.com/CFWP/rags2ridges">https://github.com/CFWP/rags2ridges</a></td>
</tr>
<tr>
<td>Suggests:</td>
<td>KEGGgraph, testthat, knitr, rmarkdown</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-13 20:18:22 UTC; cfwpe</td>
</tr>
<tr>
<td>Author:</td>
<td>Carel F.W. Peeters
    <a href="https://orcid.org/0000-0001-5766-9969"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre,
    cph],
  Anders Ellern Bilgrau
    <a href="https://orcid.org/0000-0001-9875-2902"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cph],
  Wessel N. van Wieringen
    <a href="https://orcid.org/0000-0002-5100-9123"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-14 14:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='.armaRidgeP'>Core ridge precision estimators</h2><span id='topic+.armaRidgeP'></span><span id='topic+.armaRidgePAnyTarget'></span><span id='topic+.armaRidgePScalarTarget'></span><span id='topic+armaRidgeP'></span><span id='topic+armaRidgePAnyTarget'></span><span id='topic+armaRidgePScalarTarget'></span>

<h3>Description</h3>

<p>This is the interface to the <code>C++</code> implementations of the ridge
precision estimators. They perform core computations for many other
functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.armaRidgeP(S, target, lambda, invert = 2L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".armaRidgeP_+3A_s">S</code></td>
<td>
<p>A sample covariance <code>matrix</code>.</p>
</td></tr>
<tr><td><code id=".armaRidgeP_+3A_target">target</code></td>
<td>
<p>A <code>numeric</code> symmetric positive definite target
<code>matrix</code> of the same size as <code>S</code>.</p>
</td></tr>
<tr><td><code id=".armaRidgeP_+3A_lambda">lambda</code></td>
<td>
<p>The ridge penalty. A <code>double</code> of length 1.</p>
</td></tr>
<tr><td><code id=".armaRidgeP_+3A_invert">invert</code></td>
<td>
<p>An <code>integer</code>. Should the estimate be computed using
inversion?  Permitted values are <code>0L</code>, <code>1L</code>, and <code>2L</code> meaning
&quot;no&quot;, &quot;yes&quot;, and &quot;automatic&quot; (default).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functions are R-interfaces to low-level <code>C++</code> implementations
of the ridge estimators in the reference below
(cf. Lemma 1, Remark 6, Remark 7, and section 4.1 therein).
</p>
<p><code>.armaRidgeP</code> is simply a wrapper (on the C++ side) for
<code>.armaRidgePAnyTarget</code> and <code>.armaRidgePScalarTarget</code> which are
the estimators for arbitrary and scalar targets, respectively.
The <code>invert</code> argument of the functions indicates if the computation
uses matrix inversion or not.
</p>
<p>Essentially, the functions compute
</p>
<p style="text-align: center;"><code class="reqn">
      \hat{\mathbf{\Omega}}^{\mathrm{I}a}(\lambda_{a}) =
        \left\{\left[\lambda_{a}\mathbf{I}_{p} + \frac{1}{4}(\mathbf{S} -
        \lambda_{a}\mathbf{T})^{2}\right]^{1/2} + \frac{1}{2}(\mathbf{S} -
        \lambda_{a}\mathbf{T})\right\}^{-1},
    </code>
</p>

<p>if <code>invert == 1</code> or
</p>
<p style="text-align: center;"><code class="reqn">
      \hat{\mathbf{\Omega}}^{\mathrm{I}a}(\lambda_{a}) =
        \frac{1}{\lambda}\left\{\left[\lambda_{a}\mathbf{I}_{p} + \frac{1}{4}(\mathbf{S} -
                 \lambda_{a}\mathbf{T})^{2}\right]^{1/2} - \frac{1}{2}(\mathbf{S} -
        \lambda_{a}\mathbf{T})\right\}
    </code>
</p>

<p>if <code>invert == 0</code> using appropriate eigenvalue decompositions.
See the <span class="rlang"><b>R</b></span> implementations in the example section below.
</p>


<h3>Value</h3>

<p>Returns a symmetric positive definite <code>matrix</code> of the same size
as <code>S</code>.
</p>


<h3>Warning</h3>

<p>The functions themselves perform no checks on the input.
Correct input should be ensured by wrappers.
</p>


<h3>Author(s)</h3>

<p>Anders Ellern Bilgrau, Carel F.W. Peeters &lt;cf.peeters@vumc.nl&gt;
</p>


<h3>References</h3>

<p>van Wieringen, W.N. &amp; Peeters, C.F.W. (2016).  Ridge Estimation
of Inverse Covariance Matrices from High-Dimensional Data, Computational
Statistics &amp; Data Analysis, vol. 103: 284-303.  Also available as
arXiv:1403.0904v3 [stat.ME].
</p>


<h3>See Also</h3>

<p>Used as a backbone in <code><a href="#topic+ridgeP">ridgeP</a></code>,
<code><a href="#topic+ridgeP.fused">ridgeP.fused</a></code>, etc.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
S &lt;- createS(n = 3, p = 4)
tgt &lt;- diag(4)
rags2ridges:::.armaRidgeP(S, tgt, 1.2)

rags2ridges:::.armaRidgePAnyTarget(S, tgt, 1.2)
rags2ridges:::.armaRidgePScalarTarget(S, 1, 1.2)


################################################################################
# The C++ estimators essentially amount to the following functions.
################################################################################

rev_eig &lt;- function(evalues, evectors) { # "Reverse" eigen decomposition
  evectors %*% diag(evalues) %*% t(evectors)
}

# R implementations

# As armaRidgePScalarTarget Inv
rRidgePScalarTarget &lt;- function(S, a, l, invert = 2) {
  ED &lt;- eigen(S, symmetric = TRUE)
  eigvals &lt;- 0.5*(ED$values - l*a)
  sqroot &lt;- sqrt(l + eigvals^2)

  if (l &gt; 1e6 &amp;&amp; (any(!is.finite(eigvals)) || any(!is.finite(sqroot)))) {
    return(diag(a, nrow(S)))
  }

  D_inv &lt;- 1.0/(sqroot + eigvals)
  D_noinv &lt;- (sqroot - eigvals)/l

  if (invert == 2) {   # Determine to invert or not
    if (l &gt; 1) {  # Generally, use inversion for "small" lambda
      invert = 0;
    } else {
      invert &lt;- ifelse(any(!is.finite(D_inv)), 0, 1)
    }
  }

  if (invert) {
    eigvals &lt;- D_inv
  } else {
    eigvals &lt;- D_noinv
  }
  return(rev_eig(eigvals, ED$vectors))
}

# As armaRidgePAnyTarget
rRidgePAnyTarget &lt;- function(S, tgt, l, invert = 2) {
  ED &lt;- eigen(S - l*tgt, symmetric = TRUE)
  eigvals &lt;- 0.5*ED$values
  sqroot &lt;- sqrt(l + eigvals^2)

  if (l &gt; 1e6 &amp;&amp; (any(!is.finite(eigvals)) || any(!is.finite(sqroot)))) {
    return(tgt)
  }

  D_inv &lt;- 1.0/(sqroot + eigvals)
  D_noinv &lt;- (sqroot - eigvals)/l

  if (invert == 2) {   # Determine to invert or not
    if (l &gt; 1) {  # Generally, use inversion for "small" lambda
      invert = 0;
    } else {
      invert &lt;- ifelse(any(!is.finite(D_inv)), 0, 1)
    }
  }

  if (invert == 1) {
    eigvals &lt;- D_inv
  } else {
    eigvals &lt;- D_noinv
  }
  return(rev_eig(eigvals, ED$vectors))
}

rRidgeP &lt;- function(S, tgt, l, invert = 2) {
  if (l == Inf) {
    return(tgt)
  }
  a &lt;- tgt[1,1]
  if (tgt == diag(a, nrow(tgt))) {
    rRidgePScalarTarget(S, tgt, l, invert)
  } else {
    rRidgePAnyTarget(S, tgt, l, invert)
  }

}

# Contrasted to the straight forward implementations:
sqrtm &lt;- function(X) { # Matrix square root
  ed &lt;- eigen(X, symmetric = TRUE)
  rev_eig(sqrt(ed$values), ed$vectors)
}

# Straight forward (Lemma 1)
ridgeP1 &lt;- function(S, tgt, l) {
  solve(sqrtm( l*diag(nrow(S)) + 0.25*crossprod(S - l*tgt) ) + 0.5*(S - l*tgt))
}

# Straight forward  (Lemma 1 + remark 6 + 7)
ridgeP2 &lt;- function(S, tgt, l) {
  1.0/l*(sqrtm(l*diag(nrow(S)) + 0.25*crossprod(S - l*tgt)) - 0.5*(S - l*tgt))
}

set.seed(1)
n &lt;- 3
p &lt;- 6
S &lt;- covML(matrix(rnorm(p*n), n, p))
a &lt;- 2.2
tgt &lt;- diag(a, p)
l &lt;- 1.21

(A &lt;- ridgeP1(S, tgt, l))
(B &lt;- ridgeP2(S, tgt, l))

(C  &lt;- rags2ridges:::.armaRidgePAnyTarget(S, tgt, l))
(CR &lt;-                   rRidgePAnyTarget(S, tgt, l))
(D  &lt;- rags2ridges:::.armaRidgePScalarTarget(S, a, l))
(DR &lt;-                   rRidgePScalarTarget(S, a, l))
(E  &lt;- rags2ridges:::.armaRidgeP(S, tgt, l))

# Check
equal &lt;- function(x, y) {isTRUE(all.equal(x, y))}
stopifnot(equal(A, B) &amp; equal(A, C) &amp; equal(A, D) &amp; equal(A, E))
stopifnot(equal(C, CR) &amp; equal(D, DR))
</code></pre>

<hr>
<h2 id='ADdata'>R-objects related to metabolomics data on patients with Alzheimer's Disease</h2><span id='topic+ADdata'></span><span id='topic+ADmetabolites'></span><span id='topic+sampleInfo'></span><span id='topic+variableInfo'></span>

<h3>Description</h3>

<p><code>ADdata</code> contains 3 objects related to metabolomics data on patients
with Alzheimer's Disease (AD).
</p>


<h3>Details</h3>

<p><code>ADmetabolites</code> is a <code>matrix</code> containing metabolic expressions of
230 metabolites (rows) on 127 samples (columns).
</p>
<p><code>sampleInfo</code> is a <code>data.frame</code> containing information on the
samples. Information pertains to diagnosis: AD class 1 or AD class 2.
</p>
<p><code>variableInfo</code> is a <code>data.frame</code> containing information on the
metabolic features. Information pertains to compound families: Amines,
organic acids, lipids, and oxidative stress compounds.
</p>
<p>See description.
</p>


<h3>Author(s)</h3>

<p>Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;
</p>


<h3>Source</h3>

<p>de Leeuw, F., Peeters, C.F.W., Kester, M.I., Harms, A.C., Struys,
E., Hankemeijer, T., van Vlijmen, H.W.T., van Duijn, C.M., Scheltens, P.,
Demirkan, A., van de Wiel, M.A., van der Flier, W.M., and Teunissen, C.E.
(2017). Blood-based metabolic signatures in Alzheimer's Disease. Alzheimer's
&amp; Dementia: Diagnosis, Assessment &amp; Disease Monitoring, 8: 196-207.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(ADdata)

## Look at sample information
sampleInfo

## Look at feature information
variableInfo

</code></pre>

<hr>
<h2 id='adjacentMat'>Transform real matrix into an adjacency matrix</h2><span id='topic+adjacentMat'></span>

<h3>Description</h3>

<p>Function that transforms a real matrix into an adjacency matrix. Intended
use: Turn sparsified precision matrix into an adjacency matrix for
undirected graphical representation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adjacentMat(M, diag = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adjacentMat_+3A_m">M</code></td>
<td>
<p>(Possibly sparsified precision) <code>matrix</code>.</p>
</td></tr>
<tr><td><code id="adjacentMat_+3A_diag">diag</code></td>
<td>
<p>A <code>logical</code> indicating if the diagonal elements should be
retained.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Function returns an adjacency <code>matrix</code>.
</p>


<h3>Author(s)</h3>

<p>Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;, Wessel N. van Wieringen
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ridgeP">ridgeP</a></code>, <code><a href="#topic+covML">covML</a></code>, <code><a href="#topic+sparsify">sparsify</a></code>,
<code><a href="#topic+edgeHeat">edgeHeat</a></code>, <code><a href="#topic+Ugraph">Ugraph</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some (high-dimensional) data
p = 25
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]
Cx &lt;- covML(X)

## Obtain regularized precision matrix
P &lt;- ridgeP(Cx, lambda = 10, type = "Alt")

## Obtain sparsified partial correlation matrix
PC0 &lt;- sparsify(P, threshold = "localFDR", FDRcut = .8)

## Obtain adjacency matrix
adjacentMat(PC0$sparsePrecision)

</code></pre>

<hr>
<h2 id='CNplot'>Visualize the spectral condition number against the regularization parameter</h2><span id='topic+CNplot'></span>

<h3>Description</h3>

<p>Function that visualizes the spectral condition number of the regularized
precision matrix against the domain of the regularization parameter. The
function can be used to heuristically determine an acceptable (minimal)
value for the penalty parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CNplot(
  S,
  lambdaMin,
  lambdaMax,
  step,
  type = "Alt",
  target = default.target(S, type = "DUPV"),
  norm = "2",
  Iaids = FALSE,
  vertical = FALSE,
  value = 1e-100,
  main = "",
  nOutput = FALSE,
  verbose = TRUE,
  suppressChecks = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CNplot_+3A_s">S</code></td>
<td>
<p>Sample covariance <code>matrix</code>.</p>
</td></tr>
<tr><td><code id="CNplot_+3A_lambdamin">lambdaMin</code></td>
<td>
<p>A <code>numeric</code> giving the minimum value for the penalty
parameter.</p>
</td></tr>
<tr><td><code id="CNplot_+3A_lambdamax">lambdaMax</code></td>
<td>
<p>A <code>numeric</code> giving the maximum value for the penalty
parameter.</p>
</td></tr>
<tr><td><code id="CNplot_+3A_step">step</code></td>
<td>
<p>An <code>integer</code> determining the number of steps in moving
through the grid [<code>lambdaMin</code>, <code>lambdaMax</code>].</p>
</td></tr>
<tr><td><code id="CNplot_+3A_type">type</code></td>
<td>
<p>A <code>character</code> indicating the type of ridge estimator to be
used. Must be one of: &quot;Alt&quot;, &quot;ArchI&quot;, &quot;ArchII&quot;.</p>
</td></tr>
<tr><td><code id="CNplot_+3A_target">target</code></td>
<td>
<p>A target <code>matrix</code> (in precision terms) for Type I ridge
estimators.</p>
</td></tr>
<tr><td><code id="CNplot_+3A_norm">norm</code></td>
<td>
<p>A <code>character</code> indicating the norm under which the condition
number is to be calculated/estimated. Must be one of: &quot;1&quot;, &quot;2&quot;.</p>
</td></tr>
<tr><td><code id="CNplot_+3A_iaids">Iaids</code></td>
<td>
<p>A <code>logical</code> indicating if the basic condition number plot
should be amended with interpretational aids.</p>
</td></tr>
<tr><td><code id="CNplot_+3A_vertical">vertical</code></td>
<td>
<p>A <code>logical</code> indicating if output graph should come with
a vertical line at a pre-specified value for the penalty parameter.</p>
</td></tr>
<tr><td><code id="CNplot_+3A_value">value</code></td>
<td>
<p>A <code>numeric</code> indicating a pre-specified value for the
penalty parameter.</p>
</td></tr>
<tr><td><code id="CNplot_+3A_main">main</code></td>
<td>
<p>A <code>character</code> with which to specify the main title of the
output graph.</p>
</td></tr>
<tr><td><code id="CNplot_+3A_noutput">nOutput</code></td>
<td>
<p>A <code>logical</code> indicating if numeric output should be
returned.</p>
</td></tr>
<tr><td><code id="CNplot_+3A_verbose">verbose</code></td>
<td>
<p>A <code>logical</code> indicating if information on progress should
be printed on screen.</p>
</td></tr>
<tr><td><code id="CNplot_+3A_suppresschecks">suppressChecks</code></td>
<td>
<p>A <code>logical</code> indicating if the input checks should
be suppressed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Under certain target choices the proposed ridge estimators (see
<code><a href="#topic+ridgeP">ridgeP</a></code>) are rotation equivariant, i.e., the eigenvectors of
<code class="reqn">\mathbf{S}</code> are left intact. Such rotation equivariant situations help
to understand the effect of the ridge penalty on the precision estimate: The
effect can be understood in terms of shrinkage of the eigenvalues of the
unpenalized precision estimate <code class="reqn">\mathbf{S}^{-1}</code>. Maximum shrinkage
implies that all eigenvalues are forced to be equal (in the rotation
equivariant situation). The spectral condition number w.r.t. inversion
(ratio of maximum to minimum eigenvalue) of the regularized precision matrix
may function as a heuristic in determining the (minimal) value of the
penalty parameter. A matrix with a high condition number is near-singular
(the relative distance to the set of singular matrices equals the reciprocal
of the condition number; Demmel, 1987) and its inversion is numerically
unstable. Such a matrix is said to be ill-conditioned. Numerically,
ill-conditioning will mean that small changes in the penalty parameter lead
to dramatic changes in the condition number. From a numerical point of view
one can thus track the domain of the penalty parameter for which the
regularized precision matrix is ill-conditioned. When plotting the condition
number against the (domain of the) penalty parameter, there is a point of
relative stabilization (when working in the <code class="reqn">p &gt; n</code> situation) that can
be characterized by a leveling-off of the acceleration along the curve when
plotting the condition number against the (chosen) domain of the penalty
parameter. This suggest the following fast heuristic for determining the
(minimal) value of the penalty parameter: The value of the penalty parameter
for which the spectral condition number starts to stabilize may be termed an
acceptable (minimal) value.
</p>
<p>The function outputs a graph of the (spectral) matrix condition number over
the domain [<code>lambdaMin</code>, <code>lambdaMax</code>]. When <code>norm = "2"</code> the
spectral condition number is calculated. It is determined by exact
calculation using the spectral decomposition. For most purposes this exact
calculation is fast enough, especially when considering rotation equivariant
situations (see <code><a href="#topic+ridgeP">ridgeP</a></code>). For such situations the amenities for
fast eigenvalue calculation as provided by
<a href="https://CRAN.R-project.org/package=RSpectra">RSpectra</a> are used
internally. When exact computation of the spectral condition number is
deemed too costly one may approximate the computationally friendly
L1-condition number. This approximation is accessed through the
<a href="base.html#topic+kappa">rcond</a> function (Anderson et al. 1999).
</p>
<p>When <code>Iaids = TRUE</code> the basic condition number plot is amended/enhanced
with two additional plots (over the same domain of the penalty parameter as
the basic plot): The approximate loss in digits of accuracy (for the
operation of inversion) and an approximation to the second-order derivative
of the curvature in the basic plot. These interpretational aids can enhance
interpretation of the basic condition number plot and may support choosing a
value for the penalty parameter (see Peeters, van de Wiel, &amp; van Wieringen,
2016). When <code>vertical = TRUE</code> a vertical line is added at the constant
<code>value</code>. This option can be used to assess if the optimal penalty
obtained by, e.g., the routines <code><a href="#topic+optPenalty.LOOCV">optPenalty.LOOCV</a></code> or
<code><a href="#topic+optPenalty.aLOOCV">optPenalty.aLOOCV</a></code>, has led to a precision estimate that is
well-conditioned.
</p>


<h3>Value</h3>

<p>The function returns a graph. If <code>nOutput = TRUE</code> the function
also returns an object of class <code>list</code>: </p>
<table>
<tr><td><code>lambdas</code></td>
<td>
<p>A <code>numeric</code>
vector representing all values of the penalty parameter for which the
condition number was calculated. The values of the penalty parameter are
log-equidistant.</p>
</td></tr> <tr><td><code>conditionNumbers</code></td>
<td>
<p>A <code>numeric</code> vector containing
the condition number for each value of the penalty parameter given in
<code>lambdas</code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The condition number of a (regularized) covariance matrix is
equivalent to the condition number of its corresponding inverse, the
(regularized) precision matrix. Please note that the <code>target</code> argument
(for Type I ridge estimators) is assumed to be specified in precision terms.
In case one is interested in the condition number of a Type I estimator
under a covariance target, say <code class="reqn">\mathbf{\Gamma}</code>, then just specify
<code>target = solve</code>(<code class="reqn">\mathbf{\Gamma}</code>).
</p>


<h3>Author(s)</h3>

<p>Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;
</p>


<h3>References</h3>

<p>Anderson, E, Bai, Z., ..., Sorenson, D. (1999). LAPACK Users'
Guide (3rd ed.). Philadelphia, PA, USA: Society for Industrial and Applied
Mathematics.
</p>
<p>Demmel, J.W. (1987). On condition numbers and the distance to the nearest
ill-posed problem. Numerische Mathematik, 51: 251&ndash;289.
</p>
<p>Peeters, C.F.W., van de Wiel, M.A., &amp; van Wieringen, W.N. (2020). The
spectral condition number plot for regularization parameter evaluation.
Computational Statistics, 35: 629&ndash;646.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+covML">covML</a></code>, <code><a href="#topic+ridgeP">ridgeP</a></code>,
<code><a href="#topic+optPenalty.LOOCV">optPenalty.LOOCV</a></code>, <code><a href="#topic+optPenalty.aLOOCV">optPenalty.aLOOCV</a></code>,
<code><a href="#topic+default.target">default.target</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some (high-dimensional) data
p = 25
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]
Cx &lt;- covML(X)

## Assess spectral condition number across grid of penalty parameter
CNplot(Cx, lambdaMin = .0001, lambdaMax = 50, step = 1000)

## Include interpretational aids
CNplot(Cx, lambdaMin = .0001, lambdaMax = 50, step = 1000, Iaids = TRUE)

</code></pre>

<hr>
<h2 id='Communities'>Search and visualize community-structures</h2><span id='topic+Communities'></span>

<h3>Description</h3>

<p>Function that searches for and visualizes community-structures in graphs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Communities(
  P,
  graph = TRUE,
  lay = "layout_with_fr",
  coords = NULL,
  Vsize = 15,
  Vcex = 1,
  Vcolor = "orangered",
  VBcolor = "darkred",
  VLcolor = "black",
  main = ""
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Communities_+3A_p">P</code></td>
<td>
<p>Sparsified precision <code>matrix</code></p>
</td></tr>
<tr><td><code id="Communities_+3A_graph">graph</code></td>
<td>
<p>A <code>logical</code> indicating if the results should be
visualized.</p>
</td></tr>
<tr><td><code id="Communities_+3A_lay">lay</code></td>
<td>
<p>A <code>character</code> mimicking a call to <code><a href="igraph.html#topic+igraph">igraph</a></code>
layout functions. Determines the placement of vertices.</p>
</td></tr>
<tr><td><code id="Communities_+3A_coords">coords</code></td>
<td>
<p>A <code>matrix</code> containing coordinates. Alternative to the
lay-argument for determining the placement of vertices.</p>
</td></tr>
<tr><td><code id="Communities_+3A_vsize">Vsize</code></td>
<td>
<p>A <code>numeric</code> determining the vertex size.</p>
</td></tr>
<tr><td><code id="Communities_+3A_vcex">Vcex</code></td>
<td>
<p>A <code>numeric</code> determining the size of the vertex labels.</p>
</td></tr>
<tr><td><code id="Communities_+3A_vcolor">Vcolor</code></td>
<td>
<p>A <code>character</code> (scalar or vector) determining the vertex
color.</p>
</td></tr>
<tr><td><code id="Communities_+3A_vbcolor">VBcolor</code></td>
<td>
<p>A <code>character</code> determining the color of the vertex
border.</p>
</td></tr>
<tr><td><code id="Communities_+3A_vlcolor">VLcolor</code></td>
<td>
<p>A <code>character</code> determining the color of the vertex
labels.</p>
</td></tr>
<tr><td><code id="Communities_+3A_main">main</code></td>
<td>
<p>A <code>character</code> giving the main figure title.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Communities in a network are groups of vertices (modules) that are densely
connected within. Community search is performed by the Girvan-Newman
algorithm (Newman and Girvan, 2004).
</p>
<p>When <code>graph = TRUE</code> the community structure in the graph is visualized.
The default layout is according to the Fruchterman-Reingold algorithm
(1991). Most layout functions supported by <code><a href="igraph.html#topic+igraph">igraph</a></code> are
supported (the function is partly a wrapper around certain
<code><a href="igraph.html#topic+igraph">igraph</a></code> functions). The igraph layouts can be invoked by a
<code>character</code> that mimicks a call to a <code><a href="igraph.html#topic+igraph">igraph</a></code> layout
functions in the <code>lay</code> argument. When using <code>lay = NULL</code> one can
specify the placement of vertices with the <code>coords</code> argument. The row
dimension of this matrix should equal the number of vertices. The column
dimension then should equal 2 (for 2D layouts) or 3 (for 3D layouts). The
<code>coords</code> argument can also be viewed as a convenience argument as it
enables one, e.g., to layout a graph according to the coordinates of a
previous call to <code>Ugraph</code>. If both the the lay and the coords arguments
are not <code>NULL</code>, the lay argument takes precedence. Communities are
indicated by color markings.
</p>


<h3>Value</h3>

<p>An object of class list: </p>
<table>
<tr><td><code>membership</code></td>
<td>
<p><code>numeric</code> vector
indicating, for each vertex, community membership.</p>
</td></tr>
<tr><td><code>modularityscore</code></td>
<td>
<p><code>numeric</code> scalar indicating the modularity value
of the community structure.</p>
</td></tr>
</table>
<p>When <code>graph = TRUE</code> the function also returns a graph.
</p>


<h3>Author(s)</h3>

<p>Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;
</p>


<h3>References</h3>

<p>Csardi, G. and Nepusz, T. (2006). The igraph software package
for complex network research. InterJournal, Complex Systems 1695.
http://igraph.sf.net
</p>
<p>Fruchterman, T.M.J., and Reingold, E.M. (1991). Graph Drawing by
Force-Directed Placement. Software: Practice &amp; Experience, 21: 1129-1164.
</p>
<p>Newman, M. and Girvan, M. (2004). Finding and evaluating community structure
in networks. Physical Review E, 69: 026113.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Ugraph">Ugraph</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some (high-dimensional) data
p = 25
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]

## Obtain regularized precision under optimal penalty
OPT &lt;- optPenalty.LOOCV(X, lambdaMin = .5, lambdaMax = 30, step = 100)

## Determine support regularized standardized precision under optimal penalty
PC0 &lt;- sparsify(symm(OPT$optPrec), threshold = "localFDR")$sparseParCor

## Search and visualize communities
Commy &lt;- Communities(PC0)
</code></pre>

<hr>
<h2 id='conditionNumberPlot'>Visualize the spectral condition number against the regularization parameter</h2><span id='topic+conditionNumberPlot'></span>

<h3>Description</h3>

<p>This function is now deprecated. Please use <code>CNplot</code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>conditionNumberPlot(
  S,
  lambdaMin,
  lambdaMax,
  step,
  type = "Alt",
  target = default.target(S),
  norm = "2",
  digitLoss = FALSE,
  rlDist = FALSE,
  vertical = FALSE,
  value,
  main = TRUE,
  nOutput = FALSE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="conditionNumberPlot_+3A_s">S</code></td>
<td>
<p>Sample covariance <code>matrix</code>.</p>
</td></tr>
<tr><td><code id="conditionNumberPlot_+3A_lambdamin">lambdaMin</code></td>
<td>
<p>A <code>numeric</code> giving the minimum value for the penalty
parameter.</p>
</td></tr>
<tr><td><code id="conditionNumberPlot_+3A_lambdamax">lambdaMax</code></td>
<td>
<p>A <code>numeric</code> giving the maximum value for the penalty
parameter.</p>
</td></tr>
<tr><td><code id="conditionNumberPlot_+3A_step">step</code></td>
<td>
<p>An <code>integer</code> determining the number of steps in moving
through the grid [<code>lambdaMin</code>, <code>lambdaMax</code>].</p>
</td></tr>
<tr><td><code id="conditionNumberPlot_+3A_type">type</code></td>
<td>
<p>A <code>character</code> indicating the type of ridge estimator to be
used. Must be one of: &quot;Alt&quot;, &quot;ArchI&quot;, &quot;ArchII&quot;.</p>
</td></tr>
<tr><td><code id="conditionNumberPlot_+3A_target">target</code></td>
<td>
<p>A target <code>matrix</code> (in precision terms) for Type I ridge
estimators.</p>
</td></tr>
<tr><td><code id="conditionNumberPlot_+3A_norm">norm</code></td>
<td>
<p>A <code>character</code> indicating the norm under which the condition
number is to be calculated/estimated. Must be one of: &quot;1&quot;, &quot;2&quot;.</p>
</td></tr>
<tr><td><code id="conditionNumberPlot_+3A_digitloss">digitLoss</code></td>
<td>
<p>A <code>logical</code> indicating if the approximate loss in
digits of accuracy should also be visualized in the output graph.</p>
</td></tr>
<tr><td><code id="conditionNumberPlot_+3A_rldist">rlDist</code></td>
<td>
<p>A <code>logical</code> indicating if the relative distance to the
set of singular matrices should also be visualized in the output graph.</p>
</td></tr>
<tr><td><code id="conditionNumberPlot_+3A_vertical">vertical</code></td>
<td>
<p>A <code>logical</code> indicating if output graph should come with
a vertical line at a pre-specified value for the penalty parameter.</p>
</td></tr>
<tr><td><code id="conditionNumberPlot_+3A_value">value</code></td>
<td>
<p>A <code>numeric</code> indicating a pre-specified value for the
penalty parameter.</p>
</td></tr>
<tr><td><code id="conditionNumberPlot_+3A_main">main</code></td>
<td>
<p>A <code>logical</code> indicating if output graph should contain type
of estimator as main title.</p>
</td></tr>
<tr><td><code id="conditionNumberPlot_+3A_noutput">nOutput</code></td>
<td>
<p>A <code>logical</code> indicating if numeric output should be
returned.</p>
</td></tr>
<tr><td><code id="conditionNumberPlot_+3A_verbose">verbose</code></td>
<td>
<p>A <code>logical</code> indicating if information on progress should
be printed on screen.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code>CNplot</code>.
</p>


<h3>Value</h3>

<p>The function returns a graph. If <code>nOutput = TRUE</code> the function
also returns an object of class <code>list</code>: </p>
<table>
<tr><td><code>lambdas</code></td>
<td>
<p>A <code>numeric</code>
vector representing all values of the penalty parameter for which the
condition number was calculated.</p>
</td></tr> <tr><td><code>conditionNumbers</code></td>
<td>
<p>A <code>numeric</code>
vector containing the condition number for each value of the penalty
parameter given in <code>lambdas</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+CNplot">CNplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some (high-dimensional) data
p = 25
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]
Cx &lt;- covML(X)

## Assess spectral condition number across grid of penalty parameter
conditionNumberPlot(Cx, lambdaMin = .0001, lambdaMax = 50, step = 1000)

</code></pre>

<hr>
<h2 id='covML'>Maximum likelihood estimation of the covariance matrix</h2><span id='topic+covML'></span>

<h3>Description</h3>

<p>Function that gives the maximum likelihood estimate of the covariance
matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>covML(Y, cor = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="covML_+3A_y">Y</code></td>
<td>
<p>Data <code>matrix</code>. Variables assumed to be represented by columns.</p>
</td></tr>
<tr><td><code id="covML_+3A_cor">cor</code></td>
<td>
<p>A <code>logical</code> indicating if the correlation matrix should be
returned</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function gives the maximum likelihood (ML) estimate of the covariance
matrix. The input matrix <code>Y</code> assumes that the variables are represented
by the columns. Note that when the input data is standardized, the ML
covariance matrix of the scaled data is computed. If a correlation matrix is
desired, use <code>cor = TRUE</code>.
</p>


<h3>Value</h3>

<p>Function returns the maximum likelihood estimate of the covariance
<code>matrix</code>. In case <code>cor = TRUE</code>, the correlation matrix is
returned.
</p>


<h3>Author(s)</h3>

<p>Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;, Wessel N. van Wieringen
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ridgeP">ridgeP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some (high-dimensional) data
p = 25
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]

## Obtain ML estimate covariance matrix
Cx &lt;- covML(X)

## Obtain correlation matrix
Cx &lt;- covML(X, cor = TRUE)

</code></pre>

<hr>
<h2 id='covMLknown'>Maximum likelihood estimation of the covariance matrix with assumptions on
its structure</h2><span id='topic+covMLknown'></span>

<h3>Description</h3>

<p>Function that performs maximum likelihood estimation of the covariance
matrix, with various types of assumptions on its structure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>covMLknown(
  Y,
  covMat = NULL,
  corMat = NULL,
  corType = "none",
  varType = "none",
  nInit = 100
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="covMLknown_+3A_y">Y</code></td>
<td>
<p>Data <code>matrix</code>. Variables assumed to be represented by columns.</p>
</td></tr>
<tr><td><code id="covMLknown_+3A_covmat">covMat</code></td>
<td>
<p>A positive-definite covariance <code>matrix</code>. When specified,
the to-be-estimated covariance matrix is assumed to be proportional to the
specified covariance matrix. Hence, only a constant needs to estimated.</p>
</td></tr>
<tr><td><code id="covMLknown_+3A_cormat">corMat</code></td>
<td>
<p>A positive-definite correlation <code>matrix</code>. When specified,
the to-be-estimated covariance matrix is assumed to have this correlation
structure. Hence, only the marginal variances need to be estimated.</p>
</td></tr>
<tr><td><code id="covMLknown_+3A_cortype">corType</code></td>
<td>
<p>A <code>character</code>, either <code>"none"</code> (no structure on the
correlation among variate assumed) or <code>"equi"</code> (variates are
equi-correlated).</p>
</td></tr>
<tr><td><code id="covMLknown_+3A_vartype">varType</code></td>
<td>
<p>A <code>character</code>, either <code>"none"</code> (no structure on the
marginal variances of the variates assumed) or <code>"common"</code> (variates
have equal marginal variances).</p>
</td></tr>
<tr><td><code id="covMLknown_+3A_ninit">nInit</code></td>
<td>
<p>An <code>integer</code> specifying the maximum number of iterations
for likelihood maximization when <code>corType="equi"</code> .</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function gives the maximum likelihood estimate of the covariance matrix.
The input matrix <code>Y</code> assumes that the variables are represented by the
columns.
</p>
<p>When simultaneously <code>covMat=NULL</code>, <code>corMat=NULL</code>,
<code>corType="none"</code> and <code>varType="none"</code> the <code>covML</code>-function is
invoked and the regular maximum likelihood estimate of the covariance matrix
is returned.
</p>


<h3>Value</h3>

<p>The maximum likelihood estimate of the covariance <code>matrix</code>
under the specified assumptions on its structure.
</p>


<h3>Author(s)</h3>

<p>Wessel N. van Wieringen, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+covML">covML</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some data
p = 10
n = 100
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:10] = letters[1:10]

## Obtain maximum likelihood estimate covariance matrix
Cx &lt;- covMLknown(X, corType="equi", varType="common")

</code></pre>

<hr>
<h2 id='createS'>Simulate sample covariances or datasets</h2><span id='topic+createS'></span>

<h3>Description</h3>

<p>Simulate data from a p-dimensional (zero-mean) gaussian graphical model (GGM)
with a specified (or random) topology and return the sample covariance matrix
or matrices. Can also return the original simulated data or underlying
precision matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createS(
  n,
  p,
  topology = "identity",
  dataset = FALSE,
  precision = FALSE,
  nonzero = 0.25,
  m = 1L,
  banded.n = 2L,
  invwishart = FALSE,
  nu = p + 1,
  Plist
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="createS_+3A_n">n</code></td>
<td>
<p>A <code>numeric</code> vector giving number of samples. If the length is
larger than 1, the covariance matrices are returned as a list.</p>
</td></tr>
<tr><td><code id="createS_+3A_p">p</code></td>
<td>
<p>A <code>numeric</code> of length 1 giving the dimension of the
samples/covariance.</p>
</td></tr>
<tr><td><code id="createS_+3A_topology">topology</code></td>
<td>
<p>character. The topology to use for the simulations. See the
details.</p>
</td></tr>
<tr><td><code id="createS_+3A_dataset">dataset</code></td>
<td>
<p>A <code>logical</code> value specifying whether the sample
covariance or the simulated data itself should be returned.</p>
</td></tr>
<tr><td><code id="createS_+3A_precision">precision</code></td>
<td>
<p>A <code>logical</code> value. If <code>TRUE</code> the constructed
precision matrix is returned.</p>
</td></tr>
<tr><td><code id="createS_+3A_nonzero">nonzero</code></td>
<td>
<p>A <code>numeric</code> of length 1 giving the value of the nonzero
entries used in some topologies.</p>
</td></tr>
<tr><td><code id="createS_+3A_m">m</code></td>
<td>
<p>A <code>integer</code> giving the number of blocks (i.e. conditionally
independent components) to create. If <code>m</code> is greater than 1, then the
given <code>topology</code> is used on <code>m</code> blocks of approximately equal
size.</p>
</td></tr>
<tr><td><code id="createS_+3A_banded.n">banded.n</code></td>
<td>
<p>A <code>integer</code> of length one giving the number of bands.
Only used if <code>topology</code> is one of <code>"banded"</code>,
<code>"small-world"</code>, or <code>"Watts-Strogatz"</code>.</p>
</td></tr>
<tr><td><code id="createS_+3A_invwishart">invwishart</code></td>
<td>
<p><code>logical</code>. If <code>TRUE</code> the constructed precision
matrix is used as the scale matrix of an inverse Wishart distribution and
class covariance matrices are drawn from this distribution.</p>
</td></tr>
<tr><td><code id="createS_+3A_nu">nu</code></td>
<td>
<p><code>numeric</code> greater than <code>p + 1</code> giving the degrees of
freedom in the inverse Wishart distribution.  A large <code>nu</code> implies
high class homogeneity.  A small <code>nu</code> near <code>p + 1</code> implies high
class heterogeneity.</p>
</td></tr>
<tr><td><code id="createS_+3A_plist">Plist</code></td>
<td>
<p>An optional <code>list</code> of <code>numeric</code> matrices giving the
precision matrices to simulate from. Useful when random matrices have
already been generated by setting <code>precision = TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data is simulated from a zero-mean <code>p</code>-dimensional multivariate
gaussian distribution with some precision matrix determined by the argument
<code>topology</code> which defines the GGM. If <code>precision</code> is <code>TRUE</code> the
population precision matrix is returned. This is useful to see what the
actual would-be-used precision matrices are. The available values of
<code>topology</code> are described below. Unless otherwise stated the diagonal
entries are always one. If <code>m</code> is 2 or greater block diagonal precision
matrices are constructed and used. </p>
 <ul>
<li> <p><code>"identity"</code>: uses
the identity matrix (<code>diag(p)</code>) as precision matrix.  Corresponds to no
conditional dependencies.  </p>
</li>
<li> <p><code>"star"</code>: simulate from a star
topology. Within each block the first node is selected as the &quot;hub&quot;. The
off-diagonal entries <code class="reqn">(1,j)</code> and <code class="reqn">(j,1)</code> values taper off with the
value <code class="reqn">1/(j + 1)</code>.  </p>
</li>
<li> <p><code>"clique"</code>: simulate from clique topology
where each block is a complete graph with off-diagonal elements equal to
<code>nonzero</code>.  </p>
</li>
<li> <p><code>"complete"</code>: alias for (and identical to)
<code>"clique"</code>.  </p>
</li>
<li> <p><code>"chain"</code>: simulate from a chain topology where
the precision matrix is a tridiagonal matrix with off-diagonal elements (in
each block) given by argument <code>nonzero</code>.  </p>
</li>
<li> <p><code>"banded"</code>:
precision elements <code>(i,j)</code> are given by <code class="reqn">1/(|i-j|+1)</code> if <code class="reqn">|i-j|</code>
is less than or equal to <code>banded.n</code> and zero otherwise. </p>
</li>
<li>
<p><code>"scale-free"</code>: The non-zero pattern of each block is generated by a
Barabassi random graph. Non-zero off-diagonal values are given by
<code>nonzero</code>.  Gives are very &quot;hubby&quot; network.  </p>
</li>
<li> <p><code>"Barabassi"</code>:
alias for <code>"scale-free"</code>.  </p>
</li>
<li> <p><code>"small-world"</code>: The non-zero
pattern of each block is generated by a 1-dimensional Watts-Strogatz random
graph with <code>banded.n</code> starting neighbors and <code class="reqn">5\%</code> probability of
rewiring.  Non-zero off-diagonal values are given by <code>nonzero</code>. Gives
are very &quot;bandy&quot; network.  </p>
</li>
<li> <p><code>"Watts-Strogatz"</code>: alias for
<code>"small-world"</code> </p>
</li>
<li> <p><code>"random-graph"</code>: The non-zero pattern of
each block is generated by a Erdos-Renyi random graph where each edge is
present with probability <code class="reqn">1/p</code>.  Non-zero off-diagonal values are given
by <code>nonzero</code>.  </p>
</li>
<li> <p><code>"Erdos-Renyi"</code>: alias for
<code>"random-graph"</code> </p>
</li></ul>
<p> When <code>n</code> has length greater than 1, the datasets
are generated i.i.d. given the topology and number of blocks.
</p>
<p>Arguments <code>invwishart</code> and <code>nu</code> allows for introducing class
homogeneity. Large values of <code>nu</code> imply high class homogeneity.
<code>nu</code> must be greater than <code>p + 1</code>. More precisely, if
<code>invwishart == TRUE</code> then the constructed precision matrix is used as
the scale parameter in an inverse Wishart distribution with <code>nu</code> degrees
of freedom. Each class covariance is distributed according to this inverse
Wishart and independent.
</p>


<h3>Value</h3>

<p>The returned type is dependent on <code>n</code> and <code>covariance</code>. The
function generally returns a <code>list</code> of <code>numeric</code> matrices with
the same length as <code>n</code>. If <code>covariance</code> is <code>FALSE</code> the
simulated datasets with size <code>n[i]</code> by <code>p</code> are given in the
<code>i</code> entry of the output. If <code>covariance</code> is <code>TRUE</code> the
<code>p</code> by <code>p</code> sample covariances of the datasets are given. When
<code>n</code> has length 1 the <code>list</code> structure is dropped and the matrix
is returned.
</p>


<h3>Author(s)</h3>

<p>Anders E. Bilgrau, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;, Wessel
N. van Wieringen
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate some simple sample covariance matrices
createS(n = 10, p = 3)
createS(n = c(3, 4, 5), p = 3)
createS(n = c(32, 55), p = 7)

## Generate some datasets and not sample covariance matrices
createS(c(3, 4), p = 6, dataset = TRUE)

## Generate sample covariance matrices from other topologies:
A &lt;- createS(2000, p = 4, topology = "star")
round(solve(A), 3)
B &lt;- createS(2000, p = 4, topology = "banded", banded.n = 2)
round(solve(B), 3)
C &lt;- createS(2000, p = 4, topology = "clique")  # The complete graph (as m = 1)
round(solve(C), 3)
D &lt;- createS(2000, p = 4, topology = "chain")
round(solve(D), 3)

## Generate smaple covariance matrices from block topologies:
C3 &lt;- createS(2000, p = 10, topology = "clique", m = 3)
round(solve(C3), 1)
C5 &lt;- createS(2000, p = 10, topology = "clique", m = 5)
round(solve(C5), 1)

## Can also return the precision matrix to see what happens
## m = 2 blocks, each "banded" with 4 off-diagonal bands
round(createS(1, 12, "banded", m = 2, banded.n = 4, precision = TRUE), 2)

## Simulation using graph-games
round(createS(1, 10, "small-world", precision = TRUE), 2)
round(createS(1, 5, "scale-free", precision = TRUE), 2)
round(createS(1, 5, "random-graph", precision = TRUE), 2)

## Simulation using inverse Wishart distributed class covariance
## Low class homogeneity
createS(n = c(10,10), p = 5, "banded", invwishart = TRUE, nu = 10)
## Extremely high class homogeneity
createS(n = c(10,10), p = 5, "banded", invwishart = TRUE, nu = 1e10)

# The precision argument can again be used to see the actual realised class
# precision matrices used when invwishart = TRUE.

# The Plist argument is used to reuse old precision matrices or
# user-generated ones
P &lt;- createS(n = 1, p = 5, "banded", precision = TRUE)
lapply(createS(n = c(1e5, 1e5), p = 5, Plist = list(P, P+1)), solve)

</code></pre>

<hr>
<h2 id='default.penalty'>Construct commonly used penalty matrices</h2><span id='topic+default.penalty'></span>

<h3>Description</h3>

<p>Function that constructs default or commonly use penalty matrices according
to a (factorial) study design.  The constructed penalty matrix can be used
directly in <code><a href="#topic+optPenalty.fused.auto">optPenalty.fused.auto</a></code> or serve as basis for
modification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>default.penalty(
  G,
  df,
  type = c("Complete", "CartesianEqual", "CartesianUnequal", "TensorProd")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="default.penalty_+3A_g">G</code></td>
<td>
<p>A <code>numeric</code> giving the number of classes. Can also be a
<code>list</code> of length <code>G</code> such as the usual argument <code>Slist</code> from
other <span class="pkg">rags2ridges</span> functions.  Can be omitted if <code>df</code> is given.</p>
</td></tr>
<tr><td><code id="default.penalty_+3A_df">df</code></td>
<td>
<p>A <code>data.frame</code> with <code>G</code> rows and factors in the columns.
Note, the columns has to be of type <code>factor</code>.  Can be omitted when
<code>G</code> is given and <code>type == "Complete"</code>.  The factors can be
ordered.</p>
</td></tr>
<tr><td><code id="default.penalty_+3A_type">type</code></td>
<td>
<p>A character giving the type of fused penalty graph to construct.
Should be <code>'Complete'</code> (default), <code>'CartesianEqual'</code>, or
<code>'CartesianUnequal'</code> or <code>'TensorProd'</code> or an unique abbreviation
hereof. See details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>type</code> gives a number of common choices for the penalty matrix:
</p>
 <ul>
<li> <p><code>'Complete'</code> is the complete penalty graph with equal
penalties.  </p>
</li>
<li> <p><code>'CartesianEqual'</code> corresponds to a penalizing along
each &quot;direction&quot; of factors with a common penalty. The choice is named
Cartesian as it is the Cartesian graph product of the complete penalty
graphs for the individual factors.  </p>
</li>
<li> <p><code>'CartesianUnequal'</code>
corresponds to a penalizing each direction of factors with individual
penalties.  </p>
</li>
<li> <p><code>'TensorProd'</code> correspond to penalizing the
&quot;diagonals&quot; only.  It is equivalent to the graph tensor products of the
complete graphs for each individual factor.  </p>
</li></ul>



<h3>Value</h3>

<p>Returns a <code>G</code> by <code>G</code> character matrix which specify the
class of penalty graphs to be used.  The output is suitable as input for
the penalty matrix used in <code><a href="#topic+optPenalty.fused.auto">optPenalty.fused.auto</a></code>.
</p>


<h3>Author(s)</h3>

<p>Anders E. Bilgrau, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;, Wessel
N. van Wieringen
</p>


<h3>References</h3>

<p>Bilgrau, A.E., Peeters, C.F.W., Eriksen, P.S., Boegsted, M., and
van Wieringen, W.N. (2020).  Targeted Fused Ridge Estimation of Inverse
Covariance Matrices from Multiple High-Dimensional Data Classes.  Journal
of Machine Learning Research, 21(26): 1-52.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ridgeP.fused">ridgeP.fused</a></code>, <code><a href="#topic+optPenalty.fused">optPenalty.fused</a></code>,
<code><a href="#topic+default.target">default.target</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  # Handling one-way designs
  default.penalty(2)
  default.penalty(4)
  Slist &lt;- vector("list", 6)
  default.penalty(Slist)   # The function uses only the length of the list
  df0 &lt;- expand.grid(Factor = c("lvl1", "lvl2"))
  default.penalty(df0)

  # A more elaborate example
  df1 &lt;- expand.grid(DS = c("DS1", "DS2", "DS3"), ER = c("ER+", "ER-"))

  # Usage (various interface demonstrations)
  default.penalty(6, df1, type = "Complete")
  default.penalty(6, type = "CartesianEqual")  # GIVES WARNING
  default.penalty(6, df1, type = "CartesianEqual")
  default.penalty(Slist, df1, type = "CartesianEqual")
  default.penalty(6, df1, type = "CartesianUnequal")
  default.penalty(df1)

  # A 2 by 2 by 2 design
  df2 &lt;- expand.grid(A = c("A1", "A2"), B = c("B1", "B2"), C = c("C1", "C3"))
  default.penalty(df2)
  default.penalty(df2, type = "CartesianEqual")
  default.penalty(df2, type = "CartesianUnequal")

</code></pre>

<hr>
<h2 id='default.target'>Generate a (data-driven) default target for usage in ridge-type shrinkage
estimation</h2><span id='topic+default.target'></span>

<h3>Description</h3>

<p>Function that generates a (data-driven) default target for usage in (type I)
ridge shrinkage estimation of the precision matrix (see
<code><a href="#topic+ridgeP">ridgeP</a></code>). The target that is generated is to be understood in
precision terms. Most options for target generation result in a target that
implies a situation of rotation equivariant estimation (see
<code><a href="#topic+ridgeP">ridgeP</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>default.target(S, type = "DAIE", fraction = 1e-04, const)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="default.target_+3A_s">S</code></td>
<td>
<p>Sample covariance <code>matrix</code>.</p>
</td></tr>
<tr><td><code id="default.target_+3A_type">type</code></td>
<td>
<p>A <code>character</code> determining the type of default target. Must
be one of: &quot;DAIE&quot;, &quot;DIAES&quot;, &quot;DUPV&quot;, &quot;DAPV&quot;, &quot;DCPV&quot;, &quot;DEPV&quot;, &quot;Null&quot;.</p>
</td></tr>
<tr><td><code id="default.target_+3A_fraction">fraction</code></td>
<td>
<p>A <code>numeric</code> indicating the fraction of the largest
eigenvalue below which an eigenvalue is considered zero.</p>
</td></tr>
<tr><td><code id="default.target_+3A_const">const</code></td>
<td>
<p>A <code>numeric</code> constant representing the partial variance.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function can generate the following default target matrices:
</p>

<ul>
<li> <p><code>DAIE</code>: Diagonal matrix with average of inverse nonzero
eigenvalues of S as entries;
</p>
</li>
<li> <p><code>DIAES</code>: Diagonal matrix with
inverse of average of eigenvalues of S as entries;
</p>
</li>
<li> <p><code>DUPV</code>:
Diagonal matrix with unit partial variance as entries (identity matrix);
</p>
</li>
<li> <p><code>DAPV</code>: Diagonal matrix with average of inverse variances of
<code>S</code> as entries;
</p>
</li>
<li> <p><code>DCPV</code>: Diagonal matrix with constant
partial variance as entries. Allows one to use other constant than DAIE,
DIAES, DUPV, DAPV, and in a sense Null;
</p>
</li>
<li> <p><code>DEPV</code>: Diagonal matrix
with the inverse variances of <code>S</code> as entries;
</p>
</li>
<li> <p><code>Null</code>: Null matrix.
</p>
</li></ul>

<p>The targets <code>DUPV</code>, <code>DCPV</code>, and <code>Null</code> are not
data-driven in the sense that the input matrix <code>S</code> only provides
information on the size of the desired target. The targets <code>DAIE</code>,
<code>DIAES</code>, <code>DAPV</code>, and <code>DEPV</code> are data-driven in the sense that
the input matrix <code>S</code> provides the information for the diagonal entries.
The argument <code>fraction</code> is only used when <code>type = "DAIE"</code>. The
argument <code>const</code> is only used when <code>type = "DCPV"</code>. All types
except <code>DEPV</code> and <code>Null</code> lead to rotation equivariant alternative
and archetypal Type I ridge estimators. The target <code>Null</code> also leads to
a rotation equivariant alternative Type II ridge estimator (see
<code><a href="#topic+ridgeP">ridgeP</a></code>). Note that the <code>DIAES</code>, <code>DAPV</code>, and
<code>DEPV</code> targets amount to the identity matrix when the sample covariance
matrix <code>S</code> is standardized to be the correlation matrix. The same goes,
naturally, for the <code>DCPV</code> target when <code>const</code> is specified to be
1.
</p>


<h3>Value</h3>

<p>Function returns a target <code>matrix</code>.
</p>


<h3>Author(s)</h3>

<p>Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;, Wessel N. van Wieringen
</p>


<h3>References</h3>

<p>van Wieringen, W.N. &amp; Peeters, C.F.W. (2016).  Ridge Estimation
of Inverse Covariance Matrices from High-Dimensional Data, Computational
Statistics &amp; Data Analysis, vol. 103: 284-303.  Also available as
arXiv:1403.0904v3 [stat.ME].
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ridgeP">ridgeP</a></code>, <code><a href="#topic+covML">covML</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some (high-dimensional) data
p = 25
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]
Cx &lt;- covML(X)

## Obtain default diagonal target matrix
default.target(Cx)

</code></pre>

<hr>
<h2 id='default.target.fused'>Generate data-driven targets for fused ridge estimation</h2><span id='topic+default.target.fused'></span>

<h3>Description</h3>

<p>Generates a list of (data-driven) targets to use in fused ridge estimation.
Simply a wrapper for <code><a href="#topic+default.target">default.target</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>default.target.fused(Slist, ns, type = "DAIE", by, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="default.target.fused_+3A_slist">Slist</code></td>
<td>
<p>A <code>list</code> of length <code class="reqn">K</code> of <code>numeric</code> covariance
matrices of the same size for <code class="reqn">K</code> classes.</p>
</td></tr>
<tr><td><code id="default.target.fused_+3A_ns">ns</code></td>
<td>
<p>A <code>numeric</code> vector of sample sizes corresponding to the
entries of <code>Slist</code>.</p>
</td></tr>
<tr><td><code id="default.target.fused_+3A_type">type</code></td>
<td>
<p>A <code>character</code> giving the choice of target to construct. See
<code><a href="#topic+default.target">default.target</a></code> for the available options. Default is
<code>"DAIE"</code>.</p>
</td></tr>
<tr><td><code id="default.target.fused_+3A_by">by</code></td>
<td>
<p>A <code>character</code> vector with the same length as <code>Slist</code>
specifying which groups should share target.  For each unique entry of
<code>by</code> a target is constructed.  If omitted, the default is to assign a
unique target to each class.  If not given as a <code>character</code> coercion
into one is attempted.</p>
</td></tr>
<tr><td><code id="default.target.fused_+3A_...">...</code></td>
<td>
<p>Arguments passed to <code><a href="#topic+default.target">default.target</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>list</code> of <code class="reqn">K</code> covariance target matrices of the same size.
</p>


<h3>Author(s)</h3>

<p>Anders E. Bilgrau, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;, Wessel
N. van Wieringen
</p>


<h3>See Also</h3>

<p><code><a href="#topic+default.target">default.target</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Make some toy data
ns &lt;- c(3, 4)  # Two classes with sample size 3 and 4
Slist &lt;- createS(ns, p = 3)  # Generate two 3-dimensional covariance matrices
Slist

# Different choices:
default.target.fused(Slist, ns)
default.target.fused(Slist, ns, by = seq_along(Slist)) # The same as before
default.target.fused(Slist, ns, type = "Null")
default.target.fused(Slist, ns, type = "DAPV")
default.target.fused(Slist, ns, type = "DAPV", by = rep(1, length(Slist)))


# Make some (more) toy data
ns &lt;- c(3, 4, 6, 7)  # Two classes with sample size 3 and 4
Slist &lt;- createS(ns, p = 2)  # Generate four 2-dimensional covariance matrices

# Use the same target in class 1 and 2, but another in class 3 and 4:
default.target.fused(Slist, ns, by = c("A", "A", "B", "B"))

</code></pre>

<hr>
<h2 id='DiffGraph'>Visualize the differential graph</h2><span id='topic+DiffGraph'></span>

<h3>Description</h3>

<p>Function visualizing the differential graph, i.e., the network of edges that
are unique for 2 class-specific graphs over the same vertices
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DiffGraph(
  P1,
  P2,
  lay = "layout_with_fr",
  coords = NULL,
  Vsize = 15,
  Vcex = 1,
  Vcolor = "orangered",
  VBcolor = "darkred",
  VLcolor = "black",
  P1color = "red",
  P2color = "green",
  main = ""
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DiffGraph_+3A_p1">P1</code></td>
<td>
<p>Sparsified precision <code>matrix</code> for class 1.</p>
</td></tr>
<tr><td><code id="DiffGraph_+3A_p2">P2</code></td>
<td>
<p>Sparsified precision <code>matrix</code> for class 2.</p>
</td></tr>
<tr><td><code id="DiffGraph_+3A_lay">lay</code></td>
<td>
<p>A <code>character</code> mimicking a call to <code><a href="igraph.html#topic+igraph">igraph</a></code>
layout functions. Determines the placement of vertices.</p>
</td></tr>
<tr><td><code id="DiffGraph_+3A_coords">coords</code></td>
<td>
<p>A <code>matrix</code> containing coordinates. Alternative to the
lay-argument for determining the placement of vertices.</p>
</td></tr>
<tr><td><code id="DiffGraph_+3A_vsize">Vsize</code></td>
<td>
<p>A <code>numeric</code> determining the vertex size.</p>
</td></tr>
<tr><td><code id="DiffGraph_+3A_vcex">Vcex</code></td>
<td>
<p>A <code>numeric</code> determining the size of the vertex labels.</p>
</td></tr>
<tr><td><code id="DiffGraph_+3A_vcolor">Vcolor</code></td>
<td>
<p>A <code>character</code> (scalar or vector) determining the vertex
color.</p>
</td></tr>
<tr><td><code id="DiffGraph_+3A_vbcolor">VBcolor</code></td>
<td>
<p>A <code>character</code> determining the color of the vertex
border.</p>
</td></tr>
<tr><td><code id="DiffGraph_+3A_vlcolor">VLcolor</code></td>
<td>
<p>A <code>character</code> determining the color of the vertex
labels.</p>
</td></tr>
<tr><td><code id="DiffGraph_+3A_p1color">P1color</code></td>
<td>
<p>A <code>character</code> determining the color of edges unique to
P1.</p>
</td></tr>
<tr><td><code id="DiffGraph_+3A_p2color">P2color</code></td>
<td>
<p>A <code>character</code> determining the color of edges unique to
P2.</p>
</td></tr>
<tr><td><code id="DiffGraph_+3A_main">main</code></td>
<td>
<p>A <code>character</code> giving the main figure title.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Say you have 2 class-specific precision matrices that are estimated over the
same variables/features. This function visualizes in a single graph the
edges that are unique to the respective classes. Hence, it gives the
differential graph. Edges unique to <code>P1</code> are colored according to
<code>P1color</code>. Edges unique to <code>P2</code> are colored according to
<code>P2color</code>. Dashed lines indicate negative precision elements while
solid lines indicate positive precision elements.
</p>
<p>The default layout is according to the Fruchterman-Reingold algorithm
(1991). Most layout functions supported by <code><a href="igraph.html#topic+igraph">igraph</a></code> are
supported (the function is partly a wrapper around certain
<code><a href="igraph.html#topic+igraph">igraph</a></code> functions). The igraph layouts can be invoked by a
<code>character</code> that mimicks a call to a <code><a href="igraph.html#topic+igraph">igraph</a></code> layout
functions in the <code>lay</code> argument. When using <code>lay = NULL</code> one can
specify the placement of vertices with the <code>coords</code> argument. The row
dimension of this matrix should equal the number of vertices. The column
dimension then should equal 2 (for 2D layouts) or 3 (for 3D layouts). The
<code>coords</code> argument can also be viewed as a convenience argument as it
enables one, e.g., to layout a graph according to the coordinates of a
previous call to <code>Ugraph</code>. If both the the lay and the coords arguments
are not <code>NULL</code>, the lay argument takes precedence.
</p>


<h3>Value</h3>

<p>The function returns a graph.
</p>


<h3>Author(s)</h3>

<p>Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;
</p>


<h3>References</h3>

<p>Csardi, G. and Nepusz, T. (2006). The igraph software package
for complex network research. InterJournal, Complex Systems 1695.
http://igraph.sf.net
</p>
<p>Fruchterman, T.M.J., and Reingold, E.M. (1991). Graph Drawing by
Force-Directed Placement. Software: Practice &amp; Experience, 21: 1129-1164.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Ugraph">Ugraph</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some (high-dimensional) data, class 1
p = 25
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]

## Obtain some (high-dimensional) data, class 2
set.seed(123456)
X2 = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X2)[1:25] = letters[1:25]

## Obtain regularized precision under optimal penalty, classes 1 and 2
OPT  &lt;- optPenalty.LOOCV(X, lambdaMin = .5, lambdaMax = 30, step = 100)
OPT2 &lt;- optPenalty.LOOCV(X2, lambdaMin = .5, lambdaMax = 30, step = 100)

## Determine support regularized standardized precision under optimal penalty
PC0  &lt;- sparsify(symm(OPT$optPrec), threshold = "localFDR")$sparseParCor
PC02 &lt;- sparsify(symm(OPT2$optPrec), threshold = "localFDR")$sparseParCor

## Visualize differential graph
DiffGraph(PC0, PC02)

</code></pre>

<hr>
<h2 id='edgeHeat'>Visualize (precision) matrix as a heatmap</h2><span id='topic+edgeHeat'></span>

<h3>Description</h3>

<p>Function that visualizes a (precision) matrix as a heatmap. May be used to
assess visually the elements of a single (possibly sparsified precision)
matrix. May also be used in assessing the performance of edge selection
techniques.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>edgeHeat(
  M,
  lowColor = "blue",
  highColor = "red",
  textsize = 10,
  diag = TRUE,
  legend = TRUE,
  main = ""
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="edgeHeat_+3A_m">M</code></td>
<td>
<p>(Possibly sparsified precision) <code>matrix</code>.</p>
</td></tr>
<tr><td><code id="edgeHeat_+3A_lowcolor">lowColor</code></td>
<td>
<p>A <code>character</code> that determines the color scale in the
negative range.</p>
</td></tr>
<tr><td><code id="edgeHeat_+3A_highcolor">highColor</code></td>
<td>
<p>A <code>character</code> that determines the color scale in the
positive range.</p>
</td></tr>
<tr><td><code id="edgeHeat_+3A_textsize">textsize</code></td>
<td>
<p>A <code>numeric</code> scaling the text size of row and column
labels.</p>
</td></tr>
<tr><td><code id="edgeHeat_+3A_diag">diag</code></td>
<td>
<p>A <code>logical</code> determining if the diagonal elements of the
matrix should be included in the color scaling. This argument is only used
when <code>M</code> is a square <code>matrix</code>.</p>
</td></tr>
<tr><td><code id="edgeHeat_+3A_legend">legend</code></td>
<td>
<p>A <code>logical</code> indicating whether a color legend should be
included.</p>
</td></tr>
<tr><td><code id="edgeHeat_+3A_main">main</code></td>
<td>
<p>A <code>character</code> giving the main figure title.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function utilizes
<a href="https://cran.r-project.org/package=ggplot2">ggplot2</a> (Wickham, 2009) to
visualize a matrix as a heatmap: a false color plot in which the individual
matrix entries are represented by colors. <code>lowColor</code> determines the
color scale for matrix entries in the negative range. <code>highColor</code>
determines the color scale for matrix entries in the positive range. For the
colors supported by the arguments <code>lowColor</code> and <code>highColor</code>, see
<a href="https://www.nceas.ucsb.edu/sites/default/files/2020-04/colorPaletteCheatsheet.pdf">https://www.nceas.ucsb.edu/sites/default/files/2020-04/colorPaletteCheatsheet.pdf</a>. White entries in
the plot represent the midscale value of 0. One can opt to set the diagonal
entries to the midscale color of white when one is interested in
(heatmapping) the off-diagonal elements only. To achieve this, set
<code>diag = FALSE</code>. Naturally, the <code>diag</code> argument is only used when
the input matrix <code>M</code> is a square matrix.
</p>
<p>The intended use of the function is to visualize a, possibly sparsified,
precision matrix as a heatmap. The function may also be used, in a graphical
modeling setting, to assess the performance of edge selection techniques.
However, the function is quite general, in the sense that it can represent
any <code>matrix</code> as a heatmap.
</p>


<h3>Author(s)</h3>

<p>Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;, Wessel N. van Wieringen
</p>


<h3>References</h3>

<p>Wickham, H. (2009). ggplot2: elegant graphics for data analysis.
New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+covML">covML</a></code>, <code><a href="#topic+ridgeP">ridgeP</a></code>, <code><a href="#topic+sparsify">sparsify</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some (high-dimensional) data
p = 25
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]
Cx &lt;- covML(X)

## Obtain regularized precision matrix
P &lt;- ridgeP(Cx, lambda = 10, type = "Alt")

## Obtain sparsified partial correlation matrix
PC0 &lt;- sparsify(P, threshold = "localFDR", FDRcut = .8)$sparseParCor

## Visualize sparsified partial correlation matrix as heatmap
edgeHeat(PC0)

</code></pre>

<hr>
<h2 id='evaluateS'>Evaluate numerical properties square matrix</h2><span id='topic+evaluateS'></span>

<h3>Description</h3>

<p>Function that evaluates various numerical properties of a square input
matrix. The intended use is to evaluate the various numerical properties of
what is assumed to be a covariance matrix. Another use is to evaluate the
various numerical properties of a (regularized) precision matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evaluateS(S, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="evaluateS_+3A_s">S</code></td>
<td>
<p>Covariance or (regularized) precision <code>matrix</code>.</p>
</td></tr>
<tr><td><code id="evaluateS_+3A_verbose">verbose</code></td>
<td>
<p>A <code>logical</code> indicating if output should be printed on
screen.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function evaluates various numerical properties of a covariance or
precision input matrix. The function assesses if the input matrix is
symmetric, if all its eigenvalues are real, if all its eigenvalues are
strictly positive, and if it is a diagonally dominant matrix. In addition,
the function calculates the trace, the determinant, and the spectral
condition number of the input matrix. See, e.g., Harville (1997) for more
details on the mentioned (numerical) matrix properties.
</p>


<h3>Value</h3>

<table>
<tr><td><code>symm</code></td>
<td>
<p>A <code>logical</code> indicating if the matrix is symmetric.</p>
</td></tr>
<tr><td><code>realEigen</code></td>
<td>
<p>A <code>logical</code> indicating if the eigenvalues are real.</p>
</td></tr>
<tr><td><code>posEigen</code></td>
<td>
<p>A <code>logical</code> indicating if the eigenvalues are strictly
positive.</p>
</td></tr> <tr><td><code>dd</code></td>
<td>
<p>A <code>logical</code> indicating if the matrix is diagonally
dominant.</p>
</td></tr> <tr><td><code>trace</code></td>
<td>
<p>A <code>numerical</code> giving the value of the trace.</p>
</td></tr>
<tr><td><code>det</code></td>
<td>
<p>A <code>numerical</code> giving the value of the determinant.</p>
</td></tr>
<tr><td><code>condNumber</code></td>
<td>
<p>A <code>numerical</code> giving the value of the spectral
condition number.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wessel N. van Wieringen, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;
</p>


<h3>References</h3>

<p>Harville, D.A.(1997). Matrix algebra from a statistician's
perspective. New York: Springer-Verlag.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+covML">covML</a></code>, <code><a href="#topic+ridgeP">ridgeP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some (high-dimensional) data
p = 25
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]
Cx &lt;- covML(X)

## Evaluate numerical properties covariance matrix
## Obtain, e.g., value trace
Seval &lt;- evaluateS(Cx); Seval
Seval$trace

## Evaluate numerical properties precision matrix after regularization
P &lt;- ridgeP(Cx, lambda = 10, type = 'Alt')
Peval &lt;- evaluateS(P); Peval

</code></pre>

<hr>
<h2 id='evaluateSfit'>Visual inspection of the fit of a regularized precision matrix</h2><span id='topic+evaluateSfit'></span>

<h3>Description</h3>

<p>Function aiding the visual inspection of the fit of an estimated (possibly
regularized) precision matrix vis-a-vis the sample covariance matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evaluateSfit(
  Phat,
  S,
  diag = FALSE,
  fileType = "pdf",
  nameExt = "",
  dir = getwd()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="evaluateSfit_+3A_phat">Phat</code></td>
<td>
<p>(Regularized) estimate of the precision <code>matrix</code>.</p>
</td></tr>
<tr><td><code id="evaluateSfit_+3A_s">S</code></td>
<td>
<p>Sample covariance <code>matrix</code></p>
</td></tr>
<tr><td><code id="evaluateSfit_+3A_diag">diag</code></td>
<td>
<p>A <code>logical</code> determining if the diagonal elements should be
retained for plotting.</p>
</td></tr>
<tr><td><code id="evaluateSfit_+3A_filetype">fileType</code></td>
<td>
<p>A <code>character</code> determining the output file type. Must be
one of: &quot;pdf&quot;, &quot;eps&quot;.</p>
</td></tr>
<tr><td><code id="evaluateSfit_+3A_nameext">nameExt</code></td>
<td>
<p>A <code>character</code> determining the extension of default
output names generated by the function.</p>
</td></tr>
<tr><td><code id="evaluateSfit_+3A_dir">dir</code></td>
<td>
<p>A <code>character</code> specifying the directory in which the visual
output is stored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function outputs various visualizations to aid the visual inspection of
an estimated and possibly regularized precision matrix vis-a-vis the sample
covariance matrix. The inverse of the estimated precision matrix <code>P</code> is
taken to represent the estimated covariance matrix. The function then
outputs a QQ-plot and a heatmap of the observed covariances against the
estimated ones. The heatmap has the estimated covariances as
lower-triangular elements and the observed covariances as the
upper-triangular elements. The function outputs analogous plots for the
estimated and observed correlations. In case the observed covariance matrix
<code>S</code> is non-singular also a QQ-plot an a heatmap are generated for the
estimated and observed partial correlations.
</p>
<p>The function generates files with extension <code>fileType</code> under default
output names. These files are stored in the directory <code>dir</code> (default is
the working directory). To avoid overwriting of files when working in a
single directory one may employ the argument <code>nameExt</code>. By using
<code>nameExt</code> the default output names are extended with a character of
choice.
</p>


<h3>Author(s)</h3>

<p>Wessel N. van Wieringen, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ridgeP">ridgeP</a></code>, <code><a href="#topic+covML">covML</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
## Obtain some (high-dimensional) data
p = 25
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]
Cx &lt;- covML(X)

## Obtain regularized precision matrix
P &lt;- ridgeP(Cx, lambda = 10, type = 'Alt')

## Evaluate visually fit of regularized precision matrix vis-a-vis sample covariance
evaluateSfit(P, Cx, diag = FALSE, fileType = "pdf", nameExt = "test")
## End(Not run)

</code></pre>

<hr>
<h2 id='fullMontyS'>Wrapper function</h2><span id='topic+fullMontyS'></span>

<h3>Description</h3>

<p>Function that forms a wrapper around certain <code>rags2ridges</code>
functionalities. More specifically, it (automatically) invokes
functionalities to get from high-dimensional data to a penalized precision
estimate, to the corresponding conditional independence graph and topology
summaries.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fullMontyS(
  Y,
  lambdaMin,
  lambdaMax,
  target = default.target(covML(Y)),
  dir = getwd(),
  fileTypeFig = "pdf",
  FDRcut = 0.9,
  nOutput = TRUE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fullMontyS_+3A_y">Y</code></td>
<td>
<p>Data <code>matrix</code>. Variables assumed to be represented by columns.</p>
</td></tr>
<tr><td><code id="fullMontyS_+3A_lambdamin">lambdaMin</code></td>
<td>
<p>A <code>numeric</code> giving the minimum value for the penalty
parameter.</p>
</td></tr>
<tr><td><code id="fullMontyS_+3A_lambdamax">lambdaMax</code></td>
<td>
<p>A <code>numeric</code> giving the maximum value for the penalty
parameter.</p>
</td></tr>
<tr><td><code id="fullMontyS_+3A_target">target</code></td>
<td>
<p>A target <code>matrix</code> (in precision terms) for Type I ridge
estimators.</p>
</td></tr>
<tr><td><code id="fullMontyS_+3A_dir">dir</code></td>
<td>
<p>A <code>character</code> specifying the directory in which the (visual)
output is to be stored.</p>
</td></tr>
<tr><td><code id="fullMontyS_+3A_filetypefig">fileTypeFig</code></td>
<td>
<p>A <code>character</code> determining the file type of visual
output. Must be one of: &quot;pdf&quot;, &quot;eps&quot;.</p>
</td></tr>
<tr><td><code id="fullMontyS_+3A_fdrcut">FDRcut</code></td>
<td>
<p>A <code>numeric</code> indicating the cut-off for partial
correlation element selection based on local FDR thresholding.</p>
</td></tr>
<tr><td><code id="fullMontyS_+3A_noutput">nOutput</code></td>
<td>
<p>A <code>logical</code> indicating if numeric output should be
returned.</p>
</td></tr>
<tr><td><code id="fullMontyS_+3A_verbose">verbose</code></td>
<td>
<p>A <code>logical</code> indicating if progress updates should be
printed on screen.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The wrapper always uses the alternative ridge precision estimator (see
<code><a href="#topic+ridgeP">ridgeP</a></code>) with <code>target</code> as the target matrix. The optimal
value for the penalty parameter is determined by employing Brent's method to
the calculation of a cross-validated negative log-likelihood score (see
<code><a href="#topic+optPenalty.LOOCVauto">optPenalty.LOOCVauto</a></code>). The support of the regularized
precision matrix is determined by way of local FDR thresholding (see
<code><a href="#topic+sparsify">sparsify</a></code>). The corresponding conditional independence graph is
visualized using <code><a href="#topic+Ugraph">Ugraph</a></code> with <code>type = "fancy"</code>. This
visualization as well as the calculation of network statistics (see
<code><a href="#topic+GGMnetworkStats">GGMnetworkStats</a></code>) is based on the standardization of the
regularized and sparsified precision matrix to a partial correlation matrix.
</p>


<h3>Value</h3>

<p>The function stores in the specified directory <code>dir</code> a
condition number plot (either .pdf or .eps file), a visualization of the
network (either .pdf or .eps file), and a file containing network statistics
(.txt file). When <code>nOutput = TRUE</code> the function also returns an object
of class <code>list</code>: </p>
<table>
<tr><td><code>optLambda</code></td>
<td>
<p>A <code>numeric</code> giving the optimal
value of the penalty parameter.</p>
</td></tr> <tr><td><code>optPrec</code></td>
<td>
<p>A <code>matrix</code> representing
the regularized precision matrix under the optimal value of the penalty
parameter.</p>
</td></tr> <tr><td><code>sparseParCor</code></td>
<td>
<p>A <code>matrix</code> representing the sparsified
partial correlation matrix.</p>
</td></tr> <tr><td><code>networkStats</code></td>
<td>
<p>A <code>matrix</code> giving the
calculated network statistics.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>We consider this to be a preliminary version of an envisioned wrapper
than will take better form with subsequent versions of <code>rags2ridges</code>.
</p>


<h3>Author(s)</h3>

<p>Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;, Wessel N. van Wieringen
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ridgeP">ridgeP</a></code>, <code><a href="#topic+conditionNumberPlot">conditionNumberPlot</a></code>,
<code><a href="#topic+optPenalty.LOOCVauto">optPenalty.LOOCVauto</a></code>, <code><a href="#topic+sparsify">sparsify</a></code>,
<code><a href="#topic+Ugraph">Ugraph</a></code>, <code><a href="#topic+GGMnetworkStats">GGMnetworkStats</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
## Obtain some (high-dimensional) data
p = 25
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]

## Employ the wrapper function
theWorks &lt;- fullMontyS(X, lambdaMin = .5, lambdaMax = 30)
## End(Not run)
</code></pre>

<hr>
<h2 id='fused.test'>Test the necessity of fusion</h2><span id='topic+fused.test'></span>

<h3>Description</h3>

<p>Function for testing the null hypothesis that all population precision
matrices are equal and thus the necessity for the fusion penalty. Note, the
test performed is conditional on the supplied penalties and targets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fused.test(Ylist, Tlist, lambda, n.permutations = 100, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fused.test_+3A_ylist">Ylist</code></td>
<td>
<p>A <code>list</code> of length <code class="reqn">G</code> of observations matrices for
each class.  Variables are assumed to correspond to the columns.</p>
</td></tr>
<tr><td><code id="fused.test_+3A_tlist">Tlist</code></td>
<td>
<p>A <code>list</code> of target matrices for each class. Should be same
length as <code>Ylist</code>-</p>
</td></tr>
<tr><td><code id="fused.test_+3A_lambda">lambda</code></td>
<td>
<p>A non-negative, symmetric <code class="reqn">G</code> by <code class="reqn">G</code> <code>matrix</code>
giving the ridge and fusion penalties.</p>
</td></tr>
<tr><td><code id="fused.test_+3A_n.permutations">n.permutations</code></td>
<td>
<p>The number of permutations to approximate the null
distribution.  Default is 100. Should be increased if sufficient computing
power is available.</p>
</td></tr>
<tr><td><code id="fused.test_+3A_verbose">verbose</code></td>
<td>
<p>Print out extra progress information</p>
</td></tr>
<tr><td><code id="fused.test_+3A_...">...</code></td>
<td>
<p>Arguments passed to <code><a href="#topic+ridgeP.fused">ridgeP.fused</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function computes the observed score statistic <code class="reqn">U_obs</code> using the
fused ridge estimator on the given data. Next, the score statistic is
computed a number of times (given by <code>n.permutations</code>) under the
null-hypothesis by effectively permuting the class labels of the data.
</p>


<h3>Value</h3>

<p>Returns a <code>list</code> values containing the observed test statistic
and the test statistic under the null distribution.
</p>


<h3>Author(s)</h3>

<p>Anders Ellern Bilgrau, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;,
Wessel, N. van Wieringen
</p>


<h3>References</h3>

<p>Bilgrau, A.E., Peeters, C.F.W., Eriksen, P.S., Boegsted, M., and
van Wieringen, W.N. (2020).  Targeted Fused Ridge Estimation of Inverse
Covariance Matrices from Multiple High-Dimensional Data Classes.  Journal
of Machine Learning Research, 21(26): 1-52.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ridgeP.fused">ridgeP.fused</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ns &lt;- c(10, 5, 23)
Ylist &lt;- createS(ns, p = 15, topology = "banded", dataset = TRUE)

# Use the identity target matrix for each class
Tlist &lt;- replicate(length(ns), diag(15), simplify = FALSE)

# Do the test
lm &lt;- matrix(10, 3, 3)
diag(lm) &lt;- 1
ft &lt;- fused.test(Ylist, Tlist, lambda = lm,
                 n.permutations = 500)
print(ft)

# Summary spits out a bit more information
summary(ft)

# The returned object can alo be plotted via
hist(ft)
# or via the alias
plot(ft)

# Customization and parameters work a usual:
hist(ft, col = "steelblue", main = "Null distribution", add.extra = FALSE,
     xlab = "Score statistic", freq = FALSE)

</code></pre>

<hr>
<h2 id='getKEGGPathway'>Download KEGG pathway</h2><span id='topic+getKEGGPathway'></span>

<h3>Description</h3>

<p>Download information and graph object of a given pathway from the Kyoto
Encyclopedia of Genes and Genomes (KEGG) database.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getKEGGPathway(kegg.id)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getKEGGPathway_+3A_kegg.id">kegg.id</code></td>
<td>
<p>A <code>character</code> giving the KEGG ID, e.g. <code>"map04210"</code>,
<code>"map04064"</code>, <code>"map04115"</code>. Can be prefixed with <code>"path:"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Usage of this function requires an internet connection.  The igraph objects
can be obtained with <code>igraph::igraph.from.graphNEL</code>.  The moral graph
can be obtained with <code>gRbase::moralize</code>. To obtain the adjacency matrix,
use <code>gRbase::as.adjMAT</code> or <code>igraph::get.adjacency</code>
</p>


<h3>Value</h3>

<p>Returns a <code>list</code> with entries: </p>
<table>
<tr><td><code>df</code></td>
<td>
<p>A <code>data.frame</code>
description of the KEGG pathway.</p>
</td></tr> <tr><td><code>graph</code></td>
<td>
<p>The KEGG pathway represented
as a <code>graphNEL</code> object.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>It is currently necessary to <code>require("KEGGgraph")</code> (or
<code>require("KEGGgraph")</code>) due to a bug in <span class="pkg">KEGGgraph</span>.
</p>


<h3>Author(s)</h3>

<p>Anders Ellern Bilgrau, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;,
Wessel N. van Wieringen
</p>


<h3>References</h3>

<p><a href="https://www.genome.jp/kegg/">https://www.genome.jp/kegg/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kegg.target">kegg.target</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
if (require("KEGGgraph")) {
  getKEGGPathway("map04064")
}

## End(Not run)
</code></pre>

<hr>
<h2 id='GGMblockNullPenalty'>Generate the distribution of the penalty parameter under the null hypothesis
of block-independence</h2><span id='topic+GGMblockNullPenalty'></span>

<h3>Description</h3>

<p>Function that serves as a precursor function to the block-independence test
(see <code><a href="#topic+GGMblockTest">GGMblockTest</a></code>). It generates an empirical distribution of
the penalty parameter under the null hypothesis of block independence (in
the regularized precision matrix).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GGMblockNullPenalty(
  Y,
  id,
  nPerm = 25,
  lambdaMin,
  lambdaMax,
  lambdaInit = (lambdaMin + lambdaMax)/2,
  target = default.target(covML(Y)),
  type = "Alt",
  ncpus = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GGMblockNullPenalty_+3A_y">Y</code></td>
<td>
<p>Data <code>matrix</code>. Variables assumed to be represented by columns.</p>
</td></tr>
<tr><td><code id="GGMblockNullPenalty_+3A_id">id</code></td>
<td>
<p>A <code>numeric</code> vector acting as an indicator variable for two
blocks of the precision matrix. The blocks should be coded as <code>0</code> and
<code>1</code>.</p>
</td></tr>
<tr><td><code id="GGMblockNullPenalty_+3A_nperm">nPerm</code></td>
<td>
<p>A <code>numeric</code> or <code>integer</code> determining the number of
permutations.</p>
</td></tr>
<tr><td><code id="GGMblockNullPenalty_+3A_lambdamin">lambdaMin</code></td>
<td>
<p>A <code>numeric</code> giving the minimum value for the penalty
parameter.</p>
</td></tr>
<tr><td><code id="GGMblockNullPenalty_+3A_lambdamax">lambdaMax</code></td>
<td>
<p>A <code>numeric</code> giving the maximum value for the penalty
parameter.</p>
</td></tr>
<tr><td><code id="GGMblockNullPenalty_+3A_lambdainit">lambdaInit</code></td>
<td>
<p>A <code>numeric</code> giving the initial value for the penalty
parameter for starting optimization.</p>
</td></tr>
<tr><td><code id="GGMblockNullPenalty_+3A_target">target</code></td>
<td>
<p>A target <code>matrix</code> (in precision terms) for Type I ridge
estimators.</p>
</td></tr>
<tr><td><code id="GGMblockNullPenalty_+3A_type">type</code></td>
<td>
<p>A <code>character</code> indicating the type of ridge estimator to be
used. Must be one of: &quot;Alt&quot;, &quot;ArchI&quot;, &quot;ArchII&quot;.</p>
</td></tr>
<tr><td><code id="GGMblockNullPenalty_+3A_ncpus">ncpus</code></td>
<td>
<p>A <code>numeric</code> or <code>integer</code> indicating the desired
number of cpus to be used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can be viewed as a precursor to the function for the
block-independence test (see <code><a href="#topic+GGMblockTest">GGMblockTest</a></code>). The mentioned test
evaluates the null hypothesis of block-independence against the alternative
of block-dependence (presence of non-zero elements in the off-diagonal
block) in the precision matrix using high-dimensional data. To accommodate
the high-dimensionality the parameters of interest are estimated in a
penalized manner (ridge-type penalization, see <code><a href="#topic+ridgeP">ridgeP</a></code>).
Penalization involves a degree of freedom (the penalty parameter) which
needs to be fixed before testing. This function then generates an empirical
distribution of this penalty parameter. Hereto the samples are permutated
within block. The resulting permuted data sets represent the null
hypothesis. To avoid the dependence on a single permutation, many permuted
data sets are generated. For each permutation the optimal penalty parameter
is determined by means of cross-validation (see
<code><a href="#topic+optPenalty.LOOCVauto">optPenalty.LOOCVauto</a></code>). The resulting optimal penalty
parameters are returned. An estimate of the location (such as the median) is
recommended for use in the block-independence test.
</p>


<h3>Value</h3>

<p>A <code>numeric</code> vector, representing the distribution of the (LOOCV
optimal) penalty parameter under the null hypothesis of block-independence.
</p>


<h3>Author(s)</h3>

<p>Wessel N. van Wieringen, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ridgeP">ridgeP</a></code>, <code><a href="#topic+optPenalty.LOOCVauto">optPenalty.LOOCVauto</a></code>,
<code><a href="#topic+default.target">default.target</a></code>, <code><a href="#topic+GGMblockTest">GGMblockTest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some (high-dimensional) data
p = 15
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:15] = letters[1:15]
id &lt;- c(rep(0, 10), rep(1, 5))

## Generate null distribution of the penalty parameter
lambda0dist &lt;- GGMblockNullPenalty(X, id, 5, 0.001, 10)

## Location of null distribution
lambdaNull &lt;- median(lambda0dist)

</code></pre>

<hr>
<h2 id='GGMblockTest'>Test for block-indepedence</h2><span id='topic+GGMblockTest'></span>

<h3>Description</h3>

<p>Function performing a test that evaluates the null hypothesis of
block-independence against the alternative of block-dependence (presence of
non-zero elements in the off-diagonal block) in the precision matrix using
high-dimensional data. The mentioned test is a permutation-based test (see
details).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GGMblockTest(
  Y,
  id,
  nPerm = 1000,
  lambda,
  target = default.target(covML(Y)),
  type = "Alt",
  lowCiThres = 0.1,
  ncpus = 1,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GGMblockTest_+3A_y">Y</code></td>
<td>
<p>Data <code>matrix</code>. Variables assumed to be represented by columns.</p>
</td></tr>
<tr><td><code id="GGMblockTest_+3A_id">id</code></td>
<td>
<p>A <code>numeric</code> vector acting as an indicator variable for two
blocks of the precision matrix. The blocks should be coded as <code>0</code> and
<code>1</code>.</p>
</td></tr>
<tr><td><code id="GGMblockTest_+3A_nperm">nPerm</code></td>
<td>
<p>A <code>numeric</code> or <code>integer</code> determining the number of
permutations.</p>
</td></tr>
<tr><td><code id="GGMblockTest_+3A_lambda">lambda</code></td>
<td>
<p>A <code>numeric</code> representing the penalty parameter employed
in the permutation test.</p>
</td></tr>
<tr><td><code id="GGMblockTest_+3A_target">target</code></td>
<td>
<p>A target <code>matrix</code> (in precision terms) for Type I ridge
estimators.</p>
</td></tr>
<tr><td><code id="GGMblockTest_+3A_type">type</code></td>
<td>
<p>A <code>character</code> indicating the type of ridge estimator to be
used. Must be one of: &quot;Alt&quot;, &quot;ArchI&quot;, &quot;ArchII&quot;.</p>
</td></tr>
<tr><td><code id="GGMblockTest_+3A_lowcithres">lowCiThres</code></td>
<td>
<p>A <code>numeric</code> taking a value between 0 and 1.
Determines speed of efficient p-value calculation.</p>
</td></tr>
<tr><td><code id="GGMblockTest_+3A_ncpus">ncpus</code></td>
<td>
<p>A <code>numeric</code> or <code>integer</code> indicating the desired
number of cpus to be used.</p>
</td></tr>
<tr><td><code id="GGMblockTest_+3A_verbose">verbose</code></td>
<td>
<p>A <code>logical</code> indicating if information on progress and
output should be printed on screen.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function performs a permutation test for the null hypothesis of
block-independence against the alternative of block-dependence (presence of
non-zero elements in the off-diagonal block) in the precision matrix using
high-dimensional data. In the low-dimensional setting the common test
statistic under multivariate normality (cf. Anderson, 2003) is:
</p>
<p style="text-align: center;"><code class="reqn">
  \log( \| \hat{\mathbf{\Sigma}}_a \| ) +
  \log( \| \hat{\mathbf{\Sigma}}_b \| ) -
  \log( \| \hat{\mathbf{\Sigma}}   \| ),
</code>
</p>

<p>where the
<code class="reqn">\hat{\mathbf{\Sigma}}_a</code>,
<code class="reqn">\hat{\mathbf{\Sigma}}_b</code>,
<code class="reqn">\hat{\mathbf{\Sigma}}</code>
are the estimates of the covariance matrix in the sub- and whole group(s),
respectively.
</p>
<p>To accommodate the high-dimensionality the parameters of interest are
estimated in a penalized manner (ridge-type penalization, see
<code><a href="#topic+ridgeP">ridgeP</a></code>). Penalization involves a degree of freedom (the
penalty parameter: <code>lambda</code>) which needs to be fixed before testing. To
decide on the penalty parameter for testing we refer to the
<code><a href="#topic+GGMblockNullPenalty">GGMblockNullPenalty</a></code> function. With an informed choice on the
penalty parameter at hand, the null hypothesis is evaluated by permutation.
Hereto the samples are permutated within block. The resulting permuted data
set represents the null hypothesis. Many permuted data sets are generated.
For each permutation the test statistic is calculated. The observed test
statistic is compared to the null distribution from the permutations.
</p>
<p>The function implements an efficient permutation resampling algorithm (see
van Wieringen et al., 2008, for details.): If the probability of a p-value
being below <code>lowCiThres</code> is smaller than 0.001 (read: the test is
unlikely to become significant), the permutation analysis is terminated and
a p-value of unity (1) is reported.
</p>
<p>When <code>verbose = TRUE</code> also graphical output is generated: A histogram
of the null-distribution. Note that, when <code>ncpus</code> is larger than 1,
functionalities from
<a href="https://cran.r-project.org/package=snowfall">snowfall</a> are imported.
</p>


<h3>Value</h3>

<p>An object of class list: </p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>A <code>numeric</code>
representing the observed test statistic (i.e., likelihood ratio).</p>
</td></tr>
<tr><td><code>pvalue</code></td>
<td>
<p>A <code>numeric</code> giving the p-value for the block-independence
test.</p>
</td></tr> <tr><td><code>nulldist</code></td>
<td>
<p>A <code>numeric</code> vector representing the permutation
null distribution for the test statistic.</p>
</td></tr> <tr><td><code>nperm</code></td>
<td>
<p>A <code>numeric</code>
indicating the number of permutations used for p-value calculation.</p>
</td></tr>
<tr><td><code>remark</code></td>
<td>
<p>A <code>"character"</code> that states whether the permutation
algorithm was terminated prematurely or not.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Wessel N. van Wieringen, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;
</p>


<h3>References</h3>

<p>Anderson, T.W. (2003). An Introduction to Multivariate
Statistical Analysis, 3rd Edition. John Wiley.
</p>
<p>van Wieringen, W.N., van de Wiel, M.A., and van der Vaart, A.W. (2008). A
Test for Partial Differential Expression. Journal of the American
Statistical Association 103: 1039-1049.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ridgeP">ridgeP</a></code>, <code><a href="#topic+optPenalty.LOOCVauto">optPenalty.LOOCVauto</a></code>,
<code><a href="#topic+default.target">default.target</a></code>, <code><a href="#topic+GGMblockNullPenalty">GGMblockNullPenalty</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some (high-dimensional) data
p = 15
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:15] = letters[1:15]
id &lt;- c(rep(0, 10), rep(1, 5))

## Generate null distribution of the penalty parameter
lambda0dist &lt;- GGMblockNullPenalty(X, id, 5, 0.001, 10)

## Location of null distribution
lambdaNull &lt;- median(lambda0dist)

## Perform test
testRes &lt;- GGMblockTest(X, id, nPerm = 100, lambdaNull)

</code></pre>

<hr>
<h2 id='GGMmutualInfo'>Mutual information between two sets of variates within a multivariate normal
distribution</h2><span id='topic+GGMmutualInfo'></span>

<h3>Description</h3>

<p>Function computing the mutual information between two exhaustive and
mutually exclusive splits of a set of multivariate normal random variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GGMmutualInfo(S, split1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GGMmutualInfo_+3A_s">S</code></td>
<td>
<p>A positive-definite covariance <code>matrix</code>.</p>
</td></tr>
<tr><td><code id="GGMmutualInfo_+3A_split1">split1</code></td>
<td>
<p>A <code>numeric</code>, indicating the variates (by column number)
forming the first split. The second split is automatically formed from its
complement.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>numeric</code>, the mutual information between the variates
forming <code>split1</code> and those forming its complement.
</p>


<h3>Author(s)</h3>

<p>Wessel N. van Wieringen, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;
</p>


<h3>References</h3>

<p>Cover, T.M., Thomas, J.A. (2012), Elements of information
theory.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+covML">covML</a></code>, <code><a href="#topic+ridgeP">ridgeP</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# create a covariance matrix
Sigma &lt;- covML(matrix(rnorm(100), ncol=5))

# impulse response analysis
GGMmutualInfo(Sigma, c(1,2))

</code></pre>

<hr>
<h2 id='GGMnetworkStats'>Gaussian graphical model network statistics</h2><span id='topic+GGMnetworkStats'></span>

<h3>Description</h3>

<p>Function that calculates various network statistics from a sparse precision
matrix. The sparse precision matrix is taken to represent the conditional
indepence graph of a Gaussian graphical model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GGMnetworkStats(sparseP, as.table = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GGMnetworkStats_+3A_sparsep">sparseP</code></td>
<td>
<p>Sparse precision/partial correlation <code>matrix</code>.</p>
</td></tr>
<tr><td><code id="GGMnetworkStats_+3A_as.table">as.table</code></td>
<td>
<p>A <code>logical</code> indicating if the output should be in
tabular format.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function calculates various network statistics from a sparse matrix. The
input matrix <code>P</code> is assumed to be a sparse precision or partial
correlation matrix. The sparse matrix is taken to represent a conditional
independence graph. In the Gaussian setting, conditional independence
corresponds to zero entries in the (standardized) precision matrix. Each
node in the graph represents a Gaussian variable, and each undirected edge
represents conditional dependence in the sense of a nonzero corresponding
precision entry.
</p>
<p>The function calculates various measures of centrality: node degree,
betweenness centrality, closeness centrality, and eigenvalue centrality. It
also calculates the number of positive and the number of negative edges for
each node. In addition, for each variate the mutual information (with all
other variates), the variance, and the partial variance is represented. It
is also indicated if the graph is chordal (i.e., triangulated). For more
information on network measures, consult, e.g., Newman (2010).
</p>


<h3>Value</h3>

<p>An object of class <code>list</code> when <code>as.table = FALSE</code>:
</p>
<table>
<tr><td><code>degree</code></td>
<td>
<p>A <code>numeric</code> vector with the node degree for each node.</p>
</td></tr>
<tr><td><code>betweenness</code></td>
<td>
<p>A <code>numeric</code> vector representing the betweenness
centrality for each node.</p>
</td></tr> <tr><td><code>closeness</code></td>
<td>
<p>A <code>numeric</code> vector
representing the closeness centrality for each node.</p>
</td></tr>
<tr><td><code>eigenCentrality</code></td>
<td>
<p>A <code>numeric</code> vector representing the eigenvalue
centrality for each node.</p>
</td></tr> <tr><td><code>nNeg</code></td>
<td>
<p>An <code>integer</code> vector representing
the number of negative edges for each node.</p>
</td></tr> <tr><td><code>nPos</code></td>
<td>
<p>An <code>integer</code>
vector representing the number of positive edges for each node.</p>
</td></tr>
<tr><td><code>chordal</code></td>
<td>
<p>A <code>logical</code> indicating if the implied graph is chordal.</p>
</td></tr>
<tr><td><code>mutualInfo</code></td>
<td>
<p>A <code>numeric</code> vector with the mutual information (with
all other nodes) for each node.</p>
</td></tr> <tr><td><code>variance</code></td>
<td>
<p>A <code>numeric</code> vector
representing the variance of each node.</p>
</td></tr> <tr><td><code>partialVariance</code></td>
<td>
<p>A
<code>numeric</code> vector representing the partial variance of each node.</p>
</td></tr></table>
<p> When
<code>as.table = TRUE</code> the list items above (with the exception of
<code>chordal</code>) are represented in tabular form as an object of class
<code>matrix</code>.
</p>


<h3>Author(s)</h3>

<p>Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;, Wessel N. van Wieringen
</p>


<h3>References</h3>

<p>Newman, M.E.J. (2010). &quot;Networks: an introduction&quot;, Oxford
University Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ridgeP">ridgeP</a></code>, <code><a href="#topic+covML">covML</a></code>, <code><a href="#topic+sparsify">sparsify</a></code>,
<code><a href="#topic+Ugraph">Ugraph</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some (high-dimensional) data
p = 25
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]
Cx &lt;- covML(X)

## Obtain sparsified partial correlation matrix
Pridge   &lt;- ridgeP(Cx, 10, type = "Alt")
PCsparse &lt;- sparsify(Pridge , threshold = "top")$sparseParCor

## Represent the graph and calculate GGM network statistics
Ugraph(PCsparse, "fancy")
## Not run: GGMnetworkStats(PCsparse)

</code></pre>

<hr>
<h2 id='GGMnetworkStats.fused'>Gaussian graphical model network statistics</h2><span id='topic+GGMnetworkStats.fused'></span>

<h3>Description</h3>

<p>Compute various network statistics from a <code>list</code> sparse precision
matrices. The sparse precision matrix is taken to represent the conditional
independence graph of a Gaussian graphical model. This function is a simple
wrapper for <code><a href="#topic+GGMnetworkStats">GGMnetworkStats</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GGMnetworkStats.fused(Plist)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GGMnetworkStats.fused_+3A_plist">Plist</code></td>
<td>
<p>A <code>list</code> of sparse precision/partial correlation matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For details on the columns see <code><a href="#topic+GGMnetworkStats">GGMnetworkStats</a></code>.
</p>


<h3>Value</h3>

<p>A <code>data.frame</code> of the various network statistics for each
class. The names of <code>Plist</code> is prefixed to column-names.
</p>


<h3>Author(s)</h3>

<p>Anders E. Bilgrau, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;, Wessel
N. van Wieringen
</p>


<h3>See Also</h3>

<p><code><a href="#topic+GGMnetworkStats">GGMnetworkStats</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Create some "high-dimensional" data
set.seed(1)
p &lt;- 10
ns &lt;- c(5, 6)
Slist &lt;- createS(ns, p)

## Obtain sparsified partial correlation matrix
Plist    &lt;- ridgeP.fused(Slist, ns, lambda = c(5.2, 1.3), verbose = FALSE)
PCsparse &lt;- sparsify.fused(Plist , threshold = "absValue", absValueCut = 0.2)
SPlist &lt;- lapply(PCsparse, "[[", "sparsePrecision") # Get sparse precisions

## Calculate GGM network statistics in each class
## Not run: GGMnetworkStats.fused(SPlist)

</code></pre>

<hr>
<h2 id='GGMpathStats'>Gaussian graphical model node pair path statistics</h2><span id='topic+GGMpathStats'></span>

<h3>Description</h3>

<p>Function that calculates, for a specified node pair representing endpoints,
path statistics from a sparse precision matrix. The sparse precision matrix
is taken to represent the conditional independence graph of a Gaussian
graphical model. The contribution to the observed covariance between the
specified endpoints is calculated for each (heuristically) determined path
between the endpoints.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GGMpathStats(
  P0,
  node1,
  node2,
  neiExpansions = 2,
  verbose = TRUE,
  graph = TRUE,
  nrPaths = 2,
  lay = "layout_in_circle",
  coords = NULL,
  nodecol = "skyblue",
  Vsize = 15,
  Vcex = 0.6,
  VBcolor = "darkblue",
  VLcolor = "black",
  all.edges = TRUE,
  prune = TRUE,
  legend = TRUE,
  scale = 1,
  Lcex = 0.8,
  PTcex = 2,
  main = ""
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GGMpathStats_+3A_p0">P0</code></td>
<td>
<p>Sparse (possibly standardized) precision matrix.</p>
</td></tr>
<tr><td><code id="GGMpathStats_+3A_node1">node1</code></td>
<td>
<p>A <code>numeric</code> specifying an endpoint. The numeric should
correspond to a row/column of the precision matrix and as such represents
the corresponding variable.</p>
</td></tr>
<tr><td><code id="GGMpathStats_+3A_node2">node2</code></td>
<td>
<p>A <code>numeric</code> specifying a second endpoint. The numeric
should correspond to a row/column of the precision matrix and as such
represents the corresponding variable.</p>
</td></tr>
<tr><td><code id="GGMpathStats_+3A_neiexpansions">neiExpansions</code></td>
<td>
<p>A <code>numeric</code> determining how many times the
neighborhood around the respective endpoints should be expanded in the
search for shortest paths between the node pair.</p>
</td></tr>
<tr><td><code id="GGMpathStats_+3A_verbose">verbose</code></td>
<td>
<p>A <code>logical</code> indicating if a summary of the results
should be printed on screen.</p>
</td></tr>
<tr><td><code id="GGMpathStats_+3A_graph">graph</code></td>
<td>
<p>A <code>logical</code> indicating if the strongest paths should be
visualized with a graph.</p>
</td></tr>
<tr><td><code id="GGMpathStats_+3A_nrpaths">nrPaths</code></td>
<td>
<p>A <code>numeric</code> indicating the number of paths (with the
highest contribution to the marginal covariance between the indicated node
pair) to be visualized/highlighted.</p>
</td></tr>
<tr><td><code id="GGMpathStats_+3A_lay">lay</code></td>
<td>
<p>A <code>character</code> mimicking a call to <code><a href="igraph.html#topic+igraph">igraph</a></code>
layout functions. Determines the placement of vertices.</p>
</td></tr>
<tr><td><code id="GGMpathStats_+3A_coords">coords</code></td>
<td>
<p>A <code>matrix</code> containing coordinates. Alternative to the
lay-argument for determining the placement of vertices.</p>
</td></tr>
<tr><td><code id="GGMpathStats_+3A_nodecol">nodecol</code></td>
<td>
<p>A <code>character</code> determining the color of <code>node1</code> and
<code>node2</code>.</p>
</td></tr>
<tr><td><code id="GGMpathStats_+3A_vsize">Vsize</code></td>
<td>
<p>A <code>numeric</code> determining the vertex size.</p>
</td></tr>
<tr><td><code id="GGMpathStats_+3A_vcex">Vcex</code></td>
<td>
<p>A <code>numeric</code> determining the size of the vertex labels.</p>
</td></tr>
<tr><td><code id="GGMpathStats_+3A_vbcolor">VBcolor</code></td>
<td>
<p>A <code>character</code> determining the color of the vertex
borders.</p>
</td></tr>
<tr><td><code id="GGMpathStats_+3A_vlcolor">VLcolor</code></td>
<td>
<p>A <code>character</code> determining the color of the vertex
labels.</p>
</td></tr>
<tr><td><code id="GGMpathStats_+3A_all.edges">all.edges</code></td>
<td>
<p>A <code>logical</code> indicating if edges other than those
implied by the <code>nrPaths</code>-paths between <code>node1</code> and node2 should
also be visualized.</p>
</td></tr>
<tr><td><code id="GGMpathStats_+3A_prune">prune</code></td>
<td>
<p>A <code>logical</code> determining if vertices of degree 0 should be
removed.</p>
</td></tr>
<tr><td><code id="GGMpathStats_+3A_legend">legend</code></td>
<td>
<p>A <code>logical</code> indicating if the graph should come with a
legend.</p>
</td></tr>
<tr><td><code id="GGMpathStats_+3A_scale">scale</code></td>
<td>
<p>A <code>numeric</code> representing a scale factor for visualizing
strenght of edges. It is a relative scaling factor, in the sense that the
edges implied by the <code>nrPaths</code>-paths between <code>node1</code> and node2
have edge thickness that is twice this scaling factor (so it is a scaling
factor vis-a-vis the unimplied edges).</p>
</td></tr>
<tr><td><code id="GGMpathStats_+3A_lcex">Lcex</code></td>
<td>
<p>A <code>numeric</code> determining the size of the legend box.</p>
</td></tr>
<tr><td><code id="GGMpathStats_+3A_ptcex">PTcex</code></td>
<td>
<p>A <code>numeric</code> determining the size of the exemplary lines in
the legend box.</p>
</td></tr>
<tr><td><code id="GGMpathStats_+3A_main">main</code></td>
<td>
<p>A <code>character</code> giving the main figure title.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The conditional independence graph (as implied by the sparse precision
matrix) is undirected. In undirected graphs origin and destination are
interchangeable and are both referred to as 'endpoints' of a path. The
function searches for shortest paths between the specified endpoints
<code>node1</code> and <code>node2</code>. It searches for shortest paths that visit
nodes only once. The shortest paths between the provided endpoints are
determined heuristically by the following procedure. The search is initiated
by application of the <code>get.all.shortest.paths</code>-function from the
<code><a href="igraph.html#topic+igraph">igraph</a></code>-package, which yields all shortest paths between the
nodes. Next, the neighborhoods of the endpoints are defined (excluding the
endpoints themselves). Then, the shortest paths are found between: (a)
<code>node1</code> and node <em>Vs</em> in its neighborhood; (b) node <em>Vs</em> in
the <code>node1</code>-neighborhood and node <em>Ve</em> in the
<code>node2</code>-neighborhood; and (c) node <em>Ve</em> in the
<code>node2</code>-neighborhood and <code>node2</code>. These paths are glued and new
shortest path candidates are obtained (preserving only novel paths). In
additional iterations (specified by <code>neiExpansions</code>) the <code>node1</code>-
and <code>node2</code>-neighborhood are expanded by including their neighbors
(still excluding the endpoints) and shortest paths are again searched as
described above.
</p>
<p>The contribution of a particular path to the observed covariance between the
specified node pair is calculated in accordance with Theorem 1 of Jones and
West (2005). As in Jones and West (2005), paths whose weights have an
opposite sign to the marginal covariance (between endnodes of the path) are
referred to as 'moderating paths' while paths whose weights have the same
sign as the marginal covariance are referred to as 'mediating' paths. Such
paths are visualized when <code>graph = TRUE</code>.
</p>
<p>All arguments following the <code>graph</code> argument are only (potentially)
used when <code>graph = TRUE</code>. When <code>graph = TRUE</code> the conditional
independence graph is returned with the paths highlighted that have the
highest contribution to the marginal covariance between the specified
endpoints. The number of paths highlighted is indicated by <code>nrPaths</code>.
The edges of mediating paths are represented in green while the edges of
moderating paths are represented in red. When <code>all.edges = TRUE</code> the
edges other than those implied by the <code>nrPaths</code>-paths between
<code>node1</code> and node2 are also visualized (in lightgrey). When
<code>all.edges = FALSE</code> only the mediating and moderating paths implied by
<code>nrPaths</code> are visualized.
</p>
<p>The default layout gives a circular placement of the vertices. Most layout
functions supported by <code><a href="igraph.html#topic+igraph">igraph</a></code> are supported (the function is
partly a wrapper around certain <code><a href="igraph.html#topic+igraph">igraph</a></code> functions). The igraph
layouts can be invoked by a <code>character</code> that mimicks a call to a
<code><a href="igraph.html#topic+igraph">igraph</a></code> layout functions in the <code>lay</code> argument. When using
<code>lay = NULL</code> one can specify the placement of vertices with the
<code>coords</code> argument. The row dimension of this matrix should equal the
number of (pruned) vertices. The column dimension then should equal 2 (for
2D layouts) or 3 (for 3D layouts). The <code>coords</code> argument can also be
viewed as a convenience argument as it enables one, e.g., to layout a graph
according to the coordinates of a previous call to <code>Ugraph</code>. If both
the the lay and the coords arguments are not <code>NULL</code>, the lay argument
takes precedence
</p>
<p>The arguments <code>Lcex</code> and <code>PTcex</code> are only used when <code>legend =
TRUE</code>. If <code>prune = TRUE</code> the vertices of degree 0 (vertices not
implicated by any edge) are removed. For the colors supported by the
arguments <code>nodecol</code>, <code>Vcolor</code>, and <code>VBcolor</code>, see
<a href="https://www.nceas.ucsb.edu/sites/default/files/2020-04/colorPaletteCheatsheet.pdf">https://www.nceas.ucsb.edu/sites/default/files/2020-04/colorPaletteCheatsheet.pdf</a>.
</p>


<h3>Value</h3>

<p>An object of class list: </p>
<table>
<tr><td><code>pathStats</code></td>
<td>
<p>A <code>matrix</code> specifying
the paths, their respective lengths, and their respective contributions to
the marginal covariance between the endpoints.</p>
</td></tr> <tr><td><code>paths</code></td>
<td>
<p>A <code>list</code>
representing the respective paths as numeric vectors.</p>
</td></tr> <tr><td><code>Identifier</code></td>
<td>
<p>A
<code>data.frame</code> in which each numeric from <code>paths</code> is connected to an
identifier such as a variable name.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Eppstein (1998) describes a more sophisticated algorithm for finding
the top <em>k</em> shortest paths in a graph.
</p>


<h3>Author(s)</h3>

<p>Wessel N. van Wieringen, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;
</p>


<h3>References</h3>

<p>Eppstein, D. (1998). Finding the k Shortest Paths. SIAM Journal
on computing 28: 652-673.
</p>
<p>Jones, B., and West, M. (2005). Covariance Decomposition in Undirected
Gaussian Graphical Models. Biometrika 92: 779-786.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ridgeP">ridgeP</a></code>, <code><a href="#topic+optPenalty.LOOCVauto">optPenalty.LOOCVauto</a></code>,
<code><a href="#topic+sparsify">sparsify</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some (high-dimensional) data
p &lt;- 25
n &lt;- 10
set.seed(333)
X &lt;- matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X) &lt;- letters[1:p]

## Obtain regularized precision under optimal penalty
OPT &lt;- optPenalty.LOOCVauto(X, lambdaMin = .5, lambdaMax = 30)

## Determine support regularized standardized precision under optimal penalty
PC0 &lt;- sparsify(OPT$optPrec, threshold = "localFDR")$sparseParCor

## Obtain information on mediating and moderating paths between nodes 14 and 23
pathStats &lt;- GGMpathStats(PC0, 14, 23, verbose = TRUE, prune = FALSE)
pathStats

</code></pre>

<hr>
<h2 id='GGMpathStats.fused'>Fused gaussian graphical model node pair path statistics</h2><span id='topic+GGMpathStats.fused'></span>

<h3>Description</h3>

<p>A simple wrapper for <code><a href="#topic+GGMpathStats">GGMpathStats</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GGMpathStats.fused(sparsePlist, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GGMpathStats.fused_+3A_sparseplist">sparsePlist</code></td>
<td>
<p>A <code>list</code> of sparsified precision matrices.</p>
</td></tr>
<tr><td><code id="GGMpathStats.fused_+3A_...">...</code></td>
<td>
<p>Arguments passed to <code><a href="#topic+GGMpathStats">GGMpathStats</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>list</code> of path stats.
</p>


<h3>Note</h3>

<p>The function currently fails if no paths are present in one of the
groups.
</p>


<h3>Author(s)</h3>

<p>Anders E. Bilgrau, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;, Wessel
N. van Wieringen
</p>


<h3>See Also</h3>

<p><code><a href="#topic+GGMpathStats">GGMpathStats</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Obtain some (high-dimensional) data
set.seed(1)
ns &lt;- c(10, 11)
Slist &lt;- createS(ns, p = 7, topology = "banded")
Tlist &lt;- default.target.fused(Slist, ns)

## Obtain regularized precision and sparsify
Plist &lt;- ridgeP.fused(Slist, ns, Tlist, lambda = c(1, 1.6))
sparsePlist &lt;- sparsify.fused(Plist, threshold = "absValue", absValueCut = 0.20)
SPlist &lt;- lapply(sparsePlist, "[[", "sparsePrecision")

## Obtain information on mediating and moderating paths between nodes 14 and 23
res &lt;- GGMpathStats.fused(SPlist, node1 = 3, node2 = 4, graph = FALSE)

</code></pre>

<hr>
<h2 id='hist.ptest'>Plot the results of a fusion test</h2><span id='topic+hist.ptest'></span><span id='topic+plot.ptest'></span>

<h3>Description</h3>

<p>Plot a histogram of the null distribution and the observed test statistic in
a permutation type &quot;fusion test&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ptest'
hist(x, add.extra = TRUE, ...)

## S3 method for class 'ptest'
plot(x, add.extra = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hist.ptest_+3A_x">x</code></td>
<td>
<p>A <code>ptest</code> object (a list). Usually the output of
<code><a href="#topic+fused.test">fused.test</a></code>.</p>
</td></tr>
<tr><td><code id="hist.ptest_+3A_add.extra">add.extra</code></td>
<td>
<p>A logical. Add extra information to the plot.</p>
</td></tr>
<tr><td><code id="hist.ptest_+3A_...">...</code></td>
<td>
<p>Arguments passed to <code>plot</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>plot.ptest</code> is simply a wrapper for <code>hist.ptest</code>.
</p>


<h3>Value</h3>

<p>Invisibly returns <code>x</code> with extra additions.
</p>


<h3>Author(s)</h3>

<p>Anders Ellern Bilgrau, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;,
Wessel N. van Wieringen
</p>


<h3>References</h3>

<p>Bilgrau, A.E., Peeters, C.F.W., Eriksen, P.S., Boegsted, M., and
van Wieringen, W.N. (2020).  Targeted Fused Ridge Estimation of Inverse
Covariance Matrices from Multiple High-Dimensional Data Classes.  Journal
of Machine Learning Research, 21(26): 1-52.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fused.test">fused.test</a></code>, <code><a href="#topic+print.ptest">print.ptest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ns &lt;- c(10, 5, 23)
Ylist &lt;- createS(ns, p = 15, topology = "banded", dataset = TRUE)

# Use the identity target matrix for each class
Tlist &lt;- replicate(length(ns), diag(15), simplify = FALSE)

# Do the test
lam &lt;- matrix(10, 3, 3)
diag(lam) &lt;- 1
ft &lt;- fused.test(Ylist, Tlist, lambda = lam, n.permutations = 500)

# The returned object can alo be plotted via
hist(ft)
# or via the alias
plot(ft)
</code></pre>

<hr>
<h2 id='is.Xlist'>Test if fused list-formats are correctly used</h2><span id='topic+is.Xlist'></span>

<h3>Description</h3>

<p>Function to check if the argument submits to the various <code>list</code>-formats
used by the fused ridge estimator and related functions are correct. That
is, it tests if generic fused list arguments (such as <code>Slist</code>,
<code>Tlist</code>, <code>Plist</code>, <code>Ylist</code>) are properly formatted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.Xlist(Xlist, Ylist = FALSE, semi = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.Xlist_+3A_xlist">Xlist</code></td>
<td>
<p>A <code>list</code> of precision matrices of equal size
(<code>Plist</code>), sample covariance matrices (<code>Slist</code>), data matrices
(<code>Ylist</code>)</p>
</td></tr>
<tr><td><code id="is.Xlist_+3A_ylist">Ylist</code></td>
<td>
<p><code>logical</code>. Is <code>Xlist</code> a <code>list</code> of data matrices
with the same number of columns (<code>Ylist</code>).</p>
</td></tr>
<tr><td><code id="is.Xlist_+3A_semi">semi</code></td>
<td>
<p><code>logical</code>. Should the matrices in the list be tested to be
positive semi definite or positive definite?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns <code>TRUE</code> if all tests are passed, throws error if not.
</p>


<h3>Author(s)</h3>

<p>Anders Bilgrau, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;, Wessel N.
van Wieringen
</p>


<h3>References</h3>

<p>Bilgrau, A.E., Peeters, C.F.W., Eriksen, P.S., Boegsted, M., and
van Wieringen, W.N. (2020).  Targeted Fused Ridge Estimation of Inverse
Covariance Matrices from Multiple High-Dimensional Data Classes.  Journal
of Machine Learning Research, 21(26): 1-52.
</p>
<p>van Wieringen, W.N. &amp; Peeters, C.F.W. (2016).  Ridge Estimation of Inverse
Covariance Matrices from High-Dimensional Data, Computational Statistics &amp;
Data Analysis, vol. 103: 284-303.  Also available as arXiv:1403.0904v3
[stat.ME].
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ridgeP.fused">ridgeP.fused</a></code>, <code><a href="#topic+optPenalty.fused">optPenalty.fused</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Slist &lt;- createS(n = c(4, 6, 9), p = 10)
is.Xlist(Slist, semi = TRUE)
</code></pre>

<hr>
<h2 id='isSymmetricPD'>Test for symmetric positive (semi-)definiteness</h2><span id='topic+isSymmetricPD'></span><span id='topic+isSymmetricPSD'></span>

<h3>Description</h3>

<p>Function to test if a <code>matrix</code> is symmetric positive (semi)definite or
not.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>isSymmetricPD(M)

isSymmetricPSD(M, tol = 1e-04)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="isSymmetricPD_+3A_m">M</code></td>
<td>
<p>A square symmetric matrix.</p>
</td></tr>
<tr><td><code id="isSymmetricPD_+3A_tol">tol</code></td>
<td>
<p>A numeric giving the tolerance for determining positive
semi-definiteness.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Tests positive definiteness by Cholesky decomposition.  Tests positive
semi-definiteness by checking if all eigenvalues are larger than
<code class="reqn">-\epsilon|\lambda_1|</code> where <code class="reqn">\epsilon</code> is the tolerance and
<code class="reqn">\lambda_1</code> is the largest eigenvalue.
</p>
<p>While <code>isSymmetricPSD</code> returns <code>TRUE</code> if the matrix is
symmetric positive definite and <code>FASLE</code> if not. In practice, it tests
if all eigenvalues are larger than -tol*|l| where l is the largest
eigenvalue. More
<a href="https://scicomp.stackexchange.com/questions/12979/testing-if-a-matrix-is-positive-semi-definite">here.</a>
</p>


<h3>Value</h3>

<p>Returns a <code>logical</code> value. Returns <code>TRUE</code> if the <code>M</code>
is symmetric positive (semi)definite and <code>FALSE</code> if not.  If <code>M</code>
is not even symmetric, the function throws an error.
</p>


<h3>Author(s)</h3>

<p>Anders Ellern Bilgrau Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;,
Wessel N. van Wieringen
</p>


<h3>See Also</h3>

<p><code><a href="Matrix.html#topic+isSymmetric">isSymmetric</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>A &lt;- matrix(rnorm(25), 5, 5)
## Not run: 
isSymmetricPD(A)

## End(Not run)
B &lt;- symm(A)
isSymmetricPD(B)

C &lt;- crossprod(B)
isSymmetricPD(C)

isSymmetricPSD(C)

</code></pre>

<hr>
<h2 id='kegg.target'>Construct target matrix from KEGG</h2><span id='topic+kegg.target'></span>

<h3>Description</h3>

<p>Construct a target matrix by combining topology information from the Kyoto
Encyclopedia of Genes and Genomes (KEGG) database and pilot data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kegg.target(
  Y,
  kegg.id,
  method = "linreg",
  organism = "hsa",
  graph = getKEGGPathway(kegg.id)$graph
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kegg.target_+3A_y">Y</code></td>
<td>
<p>The complete observation matrix of observations with variables in
columns. The column names should be on the form e.g.  <code>"hsa:3988"</code>
(&quot;<code>&lt;organism&gt;:&lt;Entrez id&gt;</code>&quot;). It can however also be just the Entrez
id with or without the post-fixed <code>"_at"</code> and then the specified
<code>organism</code> will be assumed.</p>
</td></tr>
<tr><td><code id="kegg.target_+3A_kegg.id">kegg.id</code></td>
<td>
<p>A <code>character</code> giving the KEGG ID, e.g. <code>"map04210"</code>,
<code>"map04064"</code>, or <code>"map04115"</code>.</p>
</td></tr>
<tr><td><code id="kegg.target_+3A_method">method</code></td>
<td>
<p>The method for estimating the non-zero entries moralized graph
of the KEGG topology.  Currently, only <code>"linreg"</code> is implemented.</p>
</td></tr>
<tr><td><code id="kegg.target_+3A_organism">organism</code></td>
<td>
<p>A <code>character</code> giving the organism, the default is
<code>"hsa"</code> (homo-sapiens).</p>
</td></tr>
<tr><td><code id="kegg.target_+3A_graph">graph</code></td>
<td>
<p>A <code>graphNEL</code> object specifying the topology of the pathway.
Can be used to avoid repeatedly downloading the information.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function estimates the precision matrix based on the topology given by
the KEGG database.  Requires a connection to the internet.
</p>


<h3>Value</h3>

<p>Returns a target <code>matrix</code> with size depending on the
<code>kegg.id</code>.
</p>


<h3>Note</h3>

<p>It is currently nessesary to <code>require("KEGGgraph")</code> (or
<code>require("KEGGgraph")</code>) due to a bug in <span class="pkg">KEGGgraph</span>.
</p>


<h3>Author(s)</h3>

<p>Anders Ellern Bilgrau, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;,
Wessel N. van Wieringen
</p>


<h3>References</h3>

<p><a href="https://www.genome.jp/kegg/">https://www.genome.jp/kegg/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getKEGGPathway">getKEGGPathway</a></code>, <code><a href="#topic+default.target">default.target</a></code>, and
<code><a href="#topic+default.target.fused">default.target.fused</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
if (require("KEGGgraph")) {
kegg.g &lt;- getKEGGPathway("map04115")$graph

# Create some toy data with the correct names
Y &lt;- createS(n = 10, p = numNodes(kegg.g), dataset = TRUE)
colnames(Y) &lt;- nodes(kegg.g)

T &lt;- kegg.target(Y, "map04115")
print(T[1:10, 1:10])
}

## End(Not run)

</code></pre>

<hr>
<h2 id='KLdiv'>Kullback-Leibler divergence between two multivariate normal distributions</h2><span id='topic+KLdiv'></span>

<h3>Description</h3>

<p>Function calculating the Kullback-Leibler divergence between two
multivariate normal distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KLdiv(Mtest, Mref, Stest, Sref, symmetric = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KLdiv_+3A_mtest">Mtest</code></td>
<td>
<p>A <code>numeric</code> mean vector for the approximating multivariate
normal distribution.</p>
</td></tr>
<tr><td><code id="KLdiv_+3A_mref">Mref</code></td>
<td>
<p>A <code>numeric</code> mean vector for the true/reference multivariate
normal distribution.</p>
</td></tr>
<tr><td><code id="KLdiv_+3A_stest">Stest</code></td>
<td>
<p>A covariance <code>matrix</code> for the approximating multivariate
normal distribution.</p>
</td></tr>
<tr><td><code id="KLdiv_+3A_sref">Sref</code></td>
<td>
<p>A covariance <code>matrix</code> for the true/reference multivariate
normal distribution.</p>
</td></tr>
<tr><td><code id="KLdiv_+3A_symmetric">symmetric</code></td>
<td>
<p>A <code>logical</code> indicating if the symmetric version of
Kullback-Leibler divergence should be calculated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Kullback-Leibler (KL) information (Kullback and Leibler, 1951; also known
as relative entropy) is a measure of divergence between two probability
distributions. Typically, one distribution is taken to represent the &lsquo;true&rsquo;
distribution and functions as the reference distribution while the other is
taken to be an approximation of the true distribution. The criterion then
measures the loss of information in approximating the reference distribution.
The KL divergence between two <code class="reqn">p</code>-dimensional multivariate normal
distributions
<code class="reqn">\mathcal{N}^{0}_{p}(\boldsymbol{\mu}_{0}, \mathbf{\Sigma}_{0})</code> and <code class="reqn">\mathcal{N}^{1}_{p}(\boldsymbol{\mu}_{1}, \mathbf{\Sigma}_{1})</code>
is given as
</p>
<p style="text-align: center;"><code class="reqn">
    \mathrm{I}_{KL}(\mathcal{N}^{0}_{p} \| \mathcal{N}^{1}_{p}) =
     \frac{1}{2}\left\{\mathrm{tr}(\mathbf{\Omega}_{1}\mathbf{\Sigma}_{0})
     + (\boldsymbol{\mu}_{1} - \boldsymbol{\mu}_{0})^{\mathrm{T}}
     \mathbf{\Omega}_{1}(\boldsymbol{\mu}_{1} - \boldsymbol{\mu}_{0}) - p
     - \ln|\mathbf{\Sigma}_{0}| + \ln|\mathbf{\Sigma}_{1}| \right\},
  </code>
</p>

<p>where <code class="reqn">\mathbf{\Omega} = \mathbf{\Sigma}^{-1}</code>. The KL divergence is not
a proper metric as <code class="reqn">\mathrm{I}_{KL}(\mathcal{N}^{0}_{p} \|
\mathcal{N}^{1}_{p}) \neq \mathrm{I}_{KL}(\mathcal{N}^{1}_{p} \|
\mathcal{N}^{0}_{p})</code>. When <code>symmetric = TRUE</code> the function calculates
the symmetric KL divergence (also referred to as Jeffreys information), given
as
</p>
<p style="text-align: center;"><code class="reqn">
    \mathrm{I}_{KL}(\mathcal{N}^{0}_{p} \| \mathcal{N}^{1}_{p}) +
    \mathrm{I}_{KL}(\mathcal{N}^{1}_{p} \| \mathcal{N}^{0}_{p}).
  </code>
</p>



<h3>Value</h3>

<p>Function returns a <code>numeric</code> representing the (symmetric)
Kullback-Leibler divergence.
</p>


<h3>Author(s)</h3>

<p>Wessel N. van Wieringen, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;
</p>


<h3>References</h3>

<p>Kullback, S. and Leibler, R.A. (1951). On Information and
Sufficiency. Annals of Mathematical Statistics 22: 79-86.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+covML">covML</a></code>, <code><a href="#topic+ridgeP">ridgeP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Define population
set.seed(333)
p = 25
n = 1000
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]
Cov0  &lt;- covML(X)
mean0 &lt;- colMeans(X)

## Obtain sample from population
samples &lt;- X[sample(nrow(X), 10),]
Cov1  &lt;- covML(samples)
mean1 &lt;- colMeans(samples)

## Regularize singular Cov1
P &lt;- ridgeP(Cov1, 10)
CovR &lt;- solve(P)

## Obtain KL divergence
KLdiv(mean1, mean0, CovR, Cov0)

</code></pre>

<hr>
<h2 id='KLdiv.fused'>Fused Kullback-Leibler divergence for sets of distributions</h2><span id='topic+KLdiv.fused'></span>

<h3>Description</h3>

<p>Function calculating the Kullback-Leibler divergence between two sets of
multivariate normal distributions. In other words, it calculates a weigthed
mean of Kullback-Leibler divergences between multiple paired normal
distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KLdiv.fused(MtestList, MrefList, StestList, SrefList, ns, symmetric = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KLdiv.fused_+3A_mtestlist">MtestList</code></td>
<td>
<p>A <code>list</code> of mean vectors of the approximating
multivariate normal distribution for each class. Assumed to be zero vectors
if not supplied.</p>
</td></tr>
<tr><td><code id="KLdiv.fused_+3A_mreflist">MrefList</code></td>
<td>
<p>A <code>list</code> of mean vectors of the reference multivariate
normal distribution for each class. Assumed to be zero vectors if not
supplied.</p>
</td></tr>
<tr><td><code id="KLdiv.fused_+3A_stestlist">StestList</code></td>
<td>
<p>A <code>list</code> of covariance matrices of the approximating
multivariate normal distribtuion for each class. Usually a <code>list</code> of
sample covariance matrices.</p>
</td></tr>
<tr><td><code id="KLdiv.fused_+3A_sreflist">SrefList</code></td>
<td>
<p>A <code>list</code> of covariance matrices of the references
multivariate normal distribtuion for each class. Usually a <code>list</code> of
the population or reference covariance matrices.</p>
</td></tr>
<tr><td><code id="KLdiv.fused_+3A_ns">ns</code></td>
<td>
<p>a <code>numeric</code> of the same length as the previous arguments
giving the sample sizes. Used as weights in the weighted mean.</p>
</td></tr>
<tr><td><code id="KLdiv.fused_+3A_symmetric">symmetric</code></td>
<td>
<p>a <code>logical</code> indicating if original symmetric version of
KL divergence should be calculated.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Function returns a <code>numeric</code> representing the (optionally
symmetric) fused Kullback-Leibler divergence.
</p>


<h3>Author(s)</h3>

<p>Anders Ellern Bilgrau, Wessel N. van Wieringen, Carel F.W. Peeters
&lt;carel.peeters@wur.nl&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+KLdiv">KLdiv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create some toy data
n &lt;- c(40, 60, 80)
p &lt;- 10
Stest &lt;- replicate(length(n), diag(p), simplify = FALSE)
Sref &lt;- createS(n, p = p)

KLdiv.fused(StestList = Stest, SrefList = Sref, ns = n, symmetric = FALSE)
KLdiv.fused(StestList = Stest, SrefList = Sref, ns = n, symmetric = TRUE)

</code></pre>

<hr>
<h2 id='loss'>Evaluate regularized precision under various loss functions</h2><span id='topic+loss'></span>

<h3>Description</h3>

<p>Function that evaluates an estimated and possibly regularized precision
matrix under various loss functions. The loss functions are formulated in
precision terms. This function may be used to estimate the risk (vis-a-vis,
say, the true precision matrix) of the various ridge estimators employed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loss(E, T, precision = TRUE, type = c("frobenius", "quadratic"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loss_+3A_e">E</code></td>
<td>
<p>Estimated (possibly regularized) precision <code>matrix</code>.</p>
</td></tr>
<tr><td><code id="loss_+3A_t">T</code></td>
<td>
<p>True (population) covariance or precision <code>matrix</code>.</p>
</td></tr>
<tr><td><code id="loss_+3A_precision">precision</code></td>
<td>
<p>A <code>logical</code> indicating if T is a precision matrix.</p>
</td></tr>
<tr><td><code id="loss_+3A_type">type</code></td>
<td>
<p>A <code>character</code> indicating which loss function is to be used.
Must be one of: &quot;frobenius&quot;, &quot;quadratic&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Let <code class="reqn">\mathbf{\Omega}</code> denote a generic <code class="reqn">(p \times p)</code> population precision matrix and let
<code class="reqn">\hat{\mathbf{\Omega}}(\lambda)</code> denote a generic ridge estimator of the precision matrix under
generic regularization parameter <code class="reqn">\lambda</code> (see also <code><a href="#topic+ridgeP">ridgeP</a></code>). The function then
considers the following loss functions:
</p>

<ol>
<li><p> Squared Frobenius loss, given by:
</p>
<p style="text-align: center;"><code class="reqn">
        L_{F}[\hat{\mathbf{\Omega}}(\lambda), \mathbf{\Omega}] = \|\hat{\mathbf{\Omega}}(\lambda) -
          \mathbf{\Omega}\|_{F}^{2};
      </code>
</p>

</li>
<li><p> Quadratic loss, given by:
</p>
<p style="text-align: center;"><code class="reqn">
        L_{Q}[\hat{\mathbf{\Omega}}(\lambda), \mathbf{\Omega}] = \|\hat{\mathbf{\Omega}}(\lambda)
        \mathbf{\Omega}^{-1} - \mathbf{I}_{p}\|_{F}^{2}.
      </code>
</p>

</li></ol>

<p>The argument <code>T</code> is considered to be the true precision matrix when <code>precision = TRUE</code>.
If <code>precision</code> <code>= FALSE</code> the argument <code>T</code> is considered to represent the true covariance matrix.
This statement is needed so that the loss is properly evaluated over the precision, i.e., depending
on the value of the <code>logical</code> argument <code>precision</code> inversions are employed where needed.
</p>
<p>The function can be employed to assess the risk of a certain ridge precision estimator (see also <code><a href="#topic+ridgeP">ridgeP</a></code>).
The risk <code class="reqn">\mathcal{R}_{f}</code> of the estimator <code class="reqn">\hat{\mathbf{\Omega}}(\lambda)</code> given a loss function <code class="reqn">L_{f}</code>,
with <code class="reqn">f \in \{F, Q\}</code> can be defined as the expected loss:
</p>
<p style="text-align: center;"><code class="reqn">
    \mathcal{R}_{f}[\hat{\mathbf{\Omega}}(\lambda)] =
      \mathrm{E}\{L_{f}[\hat{\mathbf{\Omega}}(\lambda),
                         \mathbf{\Omega}]\},
  </code>
</p>

<p>which can be approximated by the mean or median of losses over repeated simulation runs.
</p>


<h3>Value</h3>

<p>Function returns a <code>numeric</code> representing the loss under the
chosen loss function.
</p>


<h3>Author(s)</h3>

<p>Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;, Wessel N. van Wieringen
</p>


<h3>References</h3>

<p>van Wieringen, W.N. &amp; Peeters, C.F.W. (2016).  Ridge Estimation
of Inverse Covariance Matrices from High-Dimensional Data, Computational
Statistics &amp; Data Analysis, vol. 103: 284-303.  Also available as
arXiv:1403.0904v3 [stat.ME].
</p>


<h3>See Also</h3>

<p><code><a href="#topic+covML">covML</a></code>, <code><a href="#topic+ridgeP">ridgeP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Define population covariance
set.seed(333)
p = 25
n = 1000
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]
Truecov &lt;- covML(X)

## Obtain sample
samples &lt;- X[sample(nrow(X), 10), ]
Cxx &lt;- covML(samples)

## Obtain regularized precision
P &lt;- ridgeP(Cxx, 10, type = "Alt")

## Evaluate estimated precision against population
## precision under Frobenius loss
loss(P, Truecov, precision = FALSE, type = "frobenius")

</code></pre>

<hr>
<h2 id='momentS'>Moments of the sample covariance matrix.</h2><span id='topic+momentS'></span>

<h3>Description</h3>

<p>Calculates the moments of the sample covariance matrix. It assumes that the
summands (the outer products of the samples' random data vector) that
constitute the sample covariance matrix follow a Wishart-distribution with
scale parameter <code class="reqn">\mathbf{\Sigma}</code> and shape parameter <code class="reqn">\nu</code>. The
latter is equal to the number of summands in the sample covariance estimate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>momentS(Sigma, shape, moment = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="momentS_+3A_sigma">Sigma</code></td>
<td>
<p>Positive-definite <code>matrix</code>, the scale parameter
<code class="reqn">\mathbf{\Sigma}</code> of the Wishart distribution.</p>
</td></tr>
<tr><td><code id="momentS_+3A_shape">shape</code></td>
<td>
<p>A <code>numeric</code>, the shape parameter <code class="reqn">\nu</code> of the Wishart
distribution. Should exceed the number of variates (number of rows or
columns of <code>Sigma</code>).</p>
</td></tr>
<tr><td><code id="momentS_+3A_moment">moment</code></td>
<td>
<p>An <code>integer</code>. Should be in the set <code class="reqn">\{-4, -3, -2, -1,
0, 1, 2, 3, 4\}</code> (only those are explicitly specified in Lesac, Massam,
2004).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code class="reqn">r</code>-th moment of a sample covariance matrix:
<code class="reqn">E(\mathbf{S}^r)</code>.
</p>


<h3>Author(s)</h3>

<p>Wessel N. van Wieringen.
</p>


<h3>References</h3>

<p>Lesac, G., Massam, H. (2004), &quot;All invariant moments of the
Wishart distribution&quot;, <em>Scandinavian Journal of Statistics</em>, 31(2),
295-318.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# create scale parameter
Sigma &lt;- matrix(c(1, 0.5, 0, 0.5, 1, 0, 0, 0, 1), byrow=TRUE, ncol=3)

# evaluate expectation of the square of a sample covariance matrix
# that is assumed to Wishart-distributed random variable with the
# above scale parameter Sigma and shape parameter equal to 40.
momentS(Sigma, 40, 2)
</code></pre>

<hr>
<h2 id='NLL'>Evaluate the (penalized) (fused) likelihood</h2><span id='topic+NLL'></span><span id='topic+PNLL'></span><span id='topic+NLL.fused'></span><span id='topic+PNLL.fused'></span>

<h3>Description</h3>

<p>Functions that evaluate the (penalized) (fused) likelihood.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NLL(S, P)

PNLL(S, P, T, lambda)

NLL.fused(Slist, Plist, ns)

PNLL.fused(Slist, Plist, ns, Tlist, lambda)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NLL_+3A_s">S</code>, <code id="NLL_+3A_slist">Slist</code></td>
<td>
<p>A (list of) positive semi definite sample covariance
matrices.</p>
</td></tr>
<tr><td><code id="NLL_+3A_p">P</code>, <code id="NLL_+3A_plist">Plist</code></td>
<td>
<p>A (list of) positive definite precision matrices.</p>
</td></tr>
<tr><td><code id="NLL_+3A_t">T</code>, <code id="NLL_+3A_tlist">Tlist</code></td>
<td>
<p>A (list of) positive definite target matrices.</p>
</td></tr>
<tr><td><code id="NLL_+3A_lambda">lambda</code></td>
<td>
<p>A <code>numeric</code> penalty parameter. For the <code>.fused</code>
functions, this is a penalty <code>matrix</code>.</p>
</td></tr>
<tr><td><code id="NLL_+3A_ns">ns</code></td>
<td>
<p>A <code>numeric</code> of sample sizes.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A single number.
</p>


<h3>Author(s)</h3>

<p>Anders Ellern Bilgrau, Carel F.W. Peeters &lt;cf.peeters@vumc.nl&gt;,
Wessel N. van Wieringen
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ridgeP">ridgeP</a></code>, <code><a href="#topic+ridgeP.fused">ridgeP.fused</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
ns &lt;- c(4,5)
Slist &lt;- createS(n = ns, p = 5)
Plist &lt;- list(diag(5), diag(2,5))
Tlist &lt;- list(diag(5), diag(5))

NLL(Slist[[1]], Plist[[1]])
PNLL(Slist[[1]], Plist[[1]], Tlist[[1]], lambda = 1)
NLL.fused(Slist, Plist, ns)
PNLL.fused(Slist, Plist, ns, Tlist, lambda = diag(2))

</code></pre>

<hr>
<h2 id='optPenalty.aLOOCV'>Select optimal penalty parameter by approximate leave-one-out
cross-validation</h2><span id='topic+optPenalty.aLOOCV'></span>

<h3>Description</h3>

<p>Function that selects the optimal penalty parameter for the
<code><a href="#topic+ridgeP">ridgeP</a></code> call by usage of approximate leave-one-out
cross-validation. Its output includes (a.o.) the precision matrix under the
optimal value of the penalty parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optPenalty.aLOOCV(
  Y,
  lambdaMin,
  lambdaMax,
  step,
  type = "Alt",
  cor = FALSE,
  target = default.target(covML(Y)),
  output = "light",
  graph = TRUE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optPenalty.aLOOCV_+3A_y">Y</code></td>
<td>
<p>Data <code>matrix</code>. Variables assumed to be represented by columns.</p>
</td></tr>
<tr><td><code id="optPenalty.aLOOCV_+3A_lambdamin">lambdaMin</code></td>
<td>
<p>A <code>numeric</code> giving the minimum value for the penalty
parameter.</p>
</td></tr>
<tr><td><code id="optPenalty.aLOOCV_+3A_lambdamax">lambdaMax</code></td>
<td>
<p>A <code>numeric</code> giving the maximum value for the penalty
parameter.</p>
</td></tr>
<tr><td><code id="optPenalty.aLOOCV_+3A_step">step</code></td>
<td>
<p>An <code>integer</code> determining the number of steps in moving
through the grid [<code>lambdaMin</code>, <code>lambdaMax</code>].</p>
</td></tr>
<tr><td><code id="optPenalty.aLOOCV_+3A_type">type</code></td>
<td>
<p>A <code>character</code> indicating the type of ridge estimator to be
used. Must be one of: &quot;Alt&quot;, &quot;ArchI&quot;, &quot;ArchII&quot;.</p>
</td></tr>
<tr><td><code id="optPenalty.aLOOCV_+3A_cor">cor</code></td>
<td>
<p>A <code>logical</code> indicating if the evaluation of the approximate
LOOCV score should be performed on the correlation scale.</p>
</td></tr>
<tr><td><code id="optPenalty.aLOOCV_+3A_target">target</code></td>
<td>
<p>A target <code>matrix</code> (in precision terms) for Type I ridge
estimators.</p>
</td></tr>
<tr><td><code id="optPenalty.aLOOCV_+3A_output">output</code></td>
<td>
<p>A <code>character</code> indicating if the output is either heavy or
light. Must be one of: &quot;all&quot;, &quot;light&quot;.</p>
</td></tr>
<tr><td><code id="optPenalty.aLOOCV_+3A_graph">graph</code></td>
<td>
<p>A <code>logical</code> indicating if the grid search for the optimal
penalty parameter should be visualized.</p>
</td></tr>
<tr><td><code id="optPenalty.aLOOCV_+3A_verbose">verbose</code></td>
<td>
<p>A <code>logical</code> indicating if information on progress should
be printed on screen.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function calculates an approximate leave-one-out cross-validated
(aLOOCV) negative log-likelihood score (using a regularized ridge estimator
for the precision matrix) for each value of the penalty parameter contained
in the search grid. The utilized aLOOCV score was proposed by Lian (2011)
and Vujacic et al. (2014). The aLOOCV negative log-likeliho od score is
computationally more efficient than its non-approximate counterpart (see
<code><a href="#topic+optPenalty.LOOCV">optPenalty.LOOCV</a></code>). For details on the aLOOCV negative
log-likelihood score see Lian (2011) and Vujacic et al (2014). For scalar
matrix targets (see <code><a href="#topic+default.target">default.target</a></code>) the complete solution path
of the alternative Type I and II ridge estimators (see <code><a href="#topic+ridgeP">ridgeP</a></code>)
depends on only 1 eigendecomposition and 1 matrix inversion, making the
determination of the optimal penalty value particularly efficient (see van
Wieringen and Peeters, 2015).
</p>
<p>The value of the penalty parameter that achieves the lowest aLOOCV negative
log-likelihood score is deemed optimal. The penalty parameter must be
positive such that <code>lambdaMin</code> must be a positive scalar. The maximum
allowable value of <code>lambdaMax</code> depends on the type of ridge estimator
employed. For details on the type of ridge estimator one may use (one of:
&quot;Alt&quot;, &quot;ArchI&quot;, &quot;ArchII&quot;) see <code><a href="#topic+ridgeP">ridgeP</a></code>. The ouput consists of an
object of class list (see below). When <code>output = "light"</code> (default)
only the <code>optLambda</code> and <code>optPrec</code> elements of the list are given.
</p>


<h3>Value</h3>

<p>An object of class list: </p>
<table>
<tr><td><code>optLambda</code></td>
<td>
<p>A <code>numeric</code> giving
the optimal value of the penalty parameter.</p>
</td></tr> <tr><td><code>optPrec</code></td>
<td>
<p>A <code>matrix</code>
representing the precision matrix of the chosen type (see
<code><a href="#topic+ridgeS">ridgeS</a></code>) under the optimal value of the penalty parameter.</p>
</td></tr>
<tr><td><code>lambdas</code></td>
<td>
<p>A <code>numeric</code> vector representing all values of the
penalty parameter for which approximate cross-validation was performed; Only
given when <code>output = "all"</code>.</p>
</td></tr> <tr><td><code>aLOOCVs</code></td>
<td>
<p>A <code>numeric</code> vector
representing the approximate cross-validated negative log-likelihoods for
each value of the penalty parameter given in <code>lambdas</code>; Only given when
<code>output = "all"</code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>When <code>cor = TRUE</code> correlation matrices are used in the
computation of the approximate (cross-validated) negative log-likelihood
score, i.e., the sample covariance matrix is a matrix on the correlation
scale. When performing evaluation on the correlation scale the data are
assumed to be standardized. If <code>cor = TRUE</code> and one wishes to used the
default target specification one may consider using <code>target =
default.target(covML(Y, cor = TRUE))</code>. This gives a default target under the
assumption of standardized data.
</p>


<h3>Author(s)</h3>

<p>Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;, Wessel N. van Wieringen
</p>


<h3>References</h3>

<p>Lian, H. (2011). Shrinkage tuning parameter selection in
precision matrices estimation. Journal of Statistical Planning and
Inference, 141: 2839-2848.
</p>
<p>van Wieringen, W.N. &amp; Peeters, C.F.W. (2016). Ridge Estimation of Inverse
Covariance Matrices from High-Dimensional Data, Computational Statistics &amp;
Data Analysis, vol. 103: 284-303. Also available as arXiv:1403.0904v3
[stat.ME].
</p>
<p>Vujacic, I., Abbruzzo, A., and Wit, E.C. (2014). A computationally fast
alternative to cross-validation in penalized Gaussian graphical models.
arXiv: 1309.6216v2 [stat.ME].
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ridgeP">ridgeP</a></code>, <code><a href="#topic+optPenalty.LOOCV">optPenalty.LOOCV</a></code>,
<code><a href="#topic+optPenalty.LOOCVauto">optPenalty.LOOCVauto</a></code>, <br /> <code><a href="#topic+default.target">default.target</a></code>,
<code><a href="#topic+covML">covML</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some (high-dimensional) data
p = 25
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]

## Obtain regularized precision under optimal penalty
OPT  &lt;- optPenalty.aLOOCV(X, lambdaMin = .001, lambdaMax = 30, step = 400); OPT
OPT$optLambda	# Optimal penalty
OPT$optPrec	  # Regularized precision under optimal penalty

## Another example with standardized data
X &lt;- scale(X, center = TRUE, scale = TRUE)
OPT  &lt;- optPenalty.aLOOCV(X, lambdaMin = .001, lambdaMax = 30,
                          step = 400, cor = TRUE,
                          target = default.target(covML(X, cor = TRUE))); OPT
OPT$optLambda	# Optimal penalty
OPT$optPrec	  # Regularized precision under optimal penalty

</code></pre>

<hr>
<h2 id='optPenalty.fused.grid'>Identify optimal ridge and fused ridge penalties</h2><span id='topic+optPenalty.fused.grid'></span><span id='topic+optPenalty.fused.auto'></span><span id='topic+optPenalty.fused'></span>

<h3>Description</h3>

<p>Functions to find the optimal ridge and fusion penalty parameters via
leave-one-out cross validation. The functions support leave-one-out
cross-validation (LOOCV), <code class="reqn">k</code>-fold CV, and two forms of approximate
LOOCV. Depending on the used function, general numerical optimization or a
grid-based search is used.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optPenalty.fused.grid(
  Ylist,
  Tlist,
  lambdas = 10^seq(-5, 5, length.out = 15),
  lambdaFs = lambdas,
  cv.method = c("LOOCV", "aLOOCV", "sLOOCV", "kCV"),
  k = 10,
  verbose = TRUE,
  ...
)

optPenalty.fused.auto(
  Ylist,
  Tlist,
  lambda,
  cv.method = c("LOOCV", "aLOOCV", "sLOOCV", "kCV"),
  k = 10,
  verbose = TRUE,
  lambda.init,
  maxit.ridgeP.fused = 1000,
  optimizer = "optim",
  maxit.optimizer = 1000,
  debug = FALSE,
  optim.control = list(trace = verbose, maxit = maxit.optimizer),
  ...
)

optPenalty.fused(
  Ylist,
  Tlist,
  lambda = default.penalty(Ylist),
  cv.method = c("LOOCV", "aLOOCV", "sLOOCV", "kCV"),
  k = 10,
  grid = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optPenalty.fused.grid_+3A_ylist">Ylist</code></td>
<td>
<p>A <code>list</code> of <code class="reqn">G</code> matrices of data with <code class="reqn">n_g</code> samples
in the rows and <code class="reqn">p</code> variables in the columns corresponding to <code class="reqn">G</code>
classes of data.</p>
</td></tr>
<tr><td><code id="optPenalty.fused.grid_+3A_tlist">Tlist</code></td>
<td>
<p>A <code>list</code> of <code class="reqn">G</code> of p.d. class target matrices of size
<code class="reqn">p</code> times <code class="reqn">p</code>.</p>
</td></tr>
<tr><td><code id="optPenalty.fused.grid_+3A_lambdas">lambdas</code></td>
<td>
<p>A <code>numeric</code> vector of positive ridge penalties.</p>
</td></tr>
<tr><td><code id="optPenalty.fused.grid_+3A_lambdafs">lambdaFs</code></td>
<td>
<p>A <code>numeric</code> vector of non-negative fusion penalties.</p>
</td></tr>
<tr><td><code id="optPenalty.fused.grid_+3A_cv.method">cv.method</code></td>
<td>
<p><code>character</code> giving the cross-validation (CV) to use.
The allowed values are <code>"LOOCV"</code>, <code>"aLOOCV"</code>, <code>"sLOOCV"</code>,
<code>"kCV"</code> for leave-one-out cross validation (LOOCV), appproximate
LOOCV, special LOOCV, and k-fold CV, respectively.</p>
</td></tr>
<tr><td><code id="optPenalty.fused.grid_+3A_k">k</code></td>
<td>
<p><code>integer</code> giving the number of approximately equally sized
parts each class is partioned into for <code class="reqn">k</code>-fold CV.  Only use if
<code>cv.method</code> is <code>"kCV"</code>.</p>
</td></tr>
<tr><td><code id="optPenalty.fused.grid_+3A_verbose">verbose</code></td>
<td>
<p><code>logical</code>. If <code>TRUE</code>, progress information is
printed to the console.</p>
</td></tr>
<tr><td><code id="optPenalty.fused.grid_+3A_...">...</code></td>
<td>
<p>For <code>optPenalty.fused</code>, arguments are passed to
<code>optPenalty.fused.grid</code> or <code>optPenalty.fused.auto</code> depending on
the value of <code>grid</code>.  In <code>optPenalty.fused.grid</code>, arguments are
passed to <code>ridgeP.fused</code>.  In <code>optPenalty.fused.auto</code>, arguments
are passed to the optimizer.</p>
</td></tr>
<tr><td><code id="optPenalty.fused.grid_+3A_lambda">lambda</code></td>
<td>
<p>A symmetric <code>character</code> <code>matrix</code> encoding the class
of penalty matrices to cross-validate over.  The diagonal elements
correspond to the class-specific ridge penalties whereas the off-diagonal
elements correspond to the fusion penalties.  The unique elements of lambda
specify the penalties to determine by the method specified by
<code>cv.method</code>.  The penalties can be fixed if they are coercible to
numeric values, such as e.g. <code>"0"</code>, <code>"2.71"</code> or <code>"3.14"</code>.
Fusion between pairs can be &quot;left out&quot;&quot; using either of <code>""</code>,
<code>NA</code>, <code>"NA"</code>, or <code>"0"</code>.  See <code><a href="#topic+default.penalty">default.penalty</a></code>
for help on the construction hereof and more details.  Unused and can be
omitted if <code>grid == TRUE</code>.</p>
</td></tr>
<tr><td><code id="optPenalty.fused.grid_+3A_lambda.init">lambda.init</code></td>
<td>
<p>A <code>numeric</code> penalty <code>matrix</code> of initial values
passed to the optimizer. If omitted, the function selects a starting values
using a common ridge penaltiy (determined by 1D optimization) and all
fusion penalties to zero.</p>
</td></tr>
<tr><td><code id="optPenalty.fused.grid_+3A_maxit.ridgep.fused">maxit.ridgeP.fused</code></td>
<td>
<p>A <code>integer</code> giving the maximum number of
iterations allowed for each fused ridge fit.</p>
</td></tr>
<tr><td><code id="optPenalty.fused.grid_+3A_optimizer">optimizer</code></td>
<td>
<p><code>character</code>. Either <code>"optim"</code> or <code>"nlm"</code>
determining which optimizer to use.</p>
</td></tr>
<tr><td><code id="optPenalty.fused.grid_+3A_maxit.optimizer">maxit.optimizer</code></td>
<td>
<p>A <code>integer</code> giving the maximum number of
iterations allowed in the optimization procedure.</p>
</td></tr>
<tr><td><code id="optPenalty.fused.grid_+3A_debug">debug</code></td>
<td>
<p><code>logical</code>. If <code>TRUE</code> additional output from the
optimizer is appended to the output as an attribute.</p>
</td></tr>
<tr><td><code id="optPenalty.fused.grid_+3A_optim.control">optim.control</code></td>
<td>
<p>A <code>list</code> of control arguments for
<code><a href="stats.html#topic+optim">optim</a></code>.</p>
</td></tr>
<tr><td><code id="optPenalty.fused.grid_+3A_grid">grid</code></td>
<td>
<p><code>logical.</code> Should a grid based search be used? Default is
<code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>optPenalty.fused.auto</code> serves a utilizes <code><a href="stats.html#topic+optim">optim</a></code> for
identifying the optimal fused parameters and works for general classes of
penalty graphs.
</p>
<p><code>optPenalty.fused.grid</code> gives a grid-based evaluation of the
(approximate) LOOCV loss.
</p>


<h3>Value</h3>

<p><code>optPenalty.fused.auto</code> returns a <code>list</code>:<br /> </p>
<table>
<tr><td><code>Plist</code></td>
<td>
<p>A
<code>list</code> of the precision estimates for the optimal parameters.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>The estimated optimal fused penalty matrix.</p>
</td></tr>
<tr><td><code>lambda.unique</code></td>
<td>
<p>The unique entries of the <code>lambda</code>.  A more
concise overview of <code>lambda</code></p>
</td></tr> <tr><td><code>value</code></td>
<td>
<p>The value of the loss
function in the estimated optimum.</p>
</td></tr>
</table>
<p><code>optPenalty.fused.LOOCV</code> returns a <code>list</code>:<br /> </p>
<table>
<tr><td><code>ridge</code></td>
<td>
<p>A
<code>numeric</code> vector of grid values for the ridge penalty</p>
</td></tr>
<tr><td><code>fusion</code></td>
<td>
<p>The <code>numeric</code> vector of grid values for the fusion
penalty</p>
</td></tr> <tr><td><code>fcvl</code></td>
<td>
<p>The <code>numeric</code> <code>matrix</code> of evaluations of the
loss function</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Anders Ellern Bilgrau, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;,
Wessel N. van Wieringen
</p>


<h3>References</h3>

<p>Bilgrau, A.E., Peeters, C.F.W., Eriksen, P.S., Boegsted, M., and
van Wieringen, W.N. (2020).  Targeted Fused Ridge Estimation of Inverse
Covariance Matrices from Multiple High-Dimensional Data Classes.  Journal
of Machine Learning Research, 21(26): 1-52.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+default.penalty">default.penalty</a></code>, <code>optPenalty.LOOCV</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Generate some (not so) high-dimensional data witn (not so) many samples
ns &lt;- c(4, 5, 6)
Ylist &lt;- createS(n = ns, p = 6, dataset = TRUE)
Slist &lt;- lapply(Ylist, covML)
Tlist &lt;- default.target.fused(Slist, ns, type = "DIAES")


# Grid-based
lambdas &lt;- 10^seq(-5, 3, length.out = 7)
a &lt;- optPenalty.fused.grid(Ylist, Tlist,
                           lambdas = lambdas,
                           cv.method = "LOOCV", maxit = 1000)
b &lt;- optPenalty.fused.grid(Ylist, Tlist,
                           lambdas = lambdas,
                           cv.method = "aLOOCV", maxit = 1000)
c &lt;- optPenalty.fused.grid(Ylist, Tlist,
                           lambdas = lambdas,
                           cv.method = "sLOOCV", maxit = 1000)
d &lt;- optPenalty.fused.grid(Ylist, Tlist,
                           lambdas = lambdas,
                           cv.method = "kCV", k = 2, maxit = 1000)

# Numerical optimization (uses the default "optim" optimizer with method "BFGS")
aa &lt;- optPenalty.fused.auto(Ylist, Tlist, cv.method = "LOOCV", method = "BFGS")
print(aa)
bb &lt;- optPenalty.fused.auto(Ylist, Tlist, cv.method = "aLOOCV", method = "BFGS")
print(bb)
cc &lt;- optPenalty.fused.auto(Ylist, Tlist, cv.method = "sLOOCV", method = "BFGS")
print(cc)
dd &lt;- optPenalty.fused.auto(Ylist, Tlist, cv.method = "kCV", k=3, method="BFGS")
print(dd)


#
# Plot the results
#

# LOOCV
# Get minimums and plot
amin  &lt;- log(expand.grid(a$lambda, a$lambdaF))[which.min(a$fcvl), ]
aamin &lt;- c(log(aa$lambda[1,1]), log(aa$lambda[1,2]))

# Plot
filled.contour(log(a$lambda), log(a$lambdaF), log(a$fcvl), color = heat.colors,
               plot.axes = {points(amin[1], amin[2], pch = 16);
                            points(aamin[1], aamin[2], pch = 16, col = "purple");
                            axis(1); axis(2)},
               xlab = "lambda", ylab = "lambdaF", main = "LOOCV")

# Approximate LOOCV
# Get minimums and plot
bmin &lt;- log(expand.grid(b$lambda, b$lambdaF))[which.min(b$fcvl), ]
bbmin &lt;- c(log(bb$lambda[1,1]), log(unique(bb$lambda[1,2])))

filled.contour(log(b$lambda), log(b$lambdaF), log(b$fcvl), color = heat.colors,
               plot.axes = {points(bmin[1], bmin[2], pch = 16);
                            points(bbmin[1], bbmin[2], pch = 16, col ="purple");
                            axis(1); axis(2)},
               xlab = "lambda", ylab = "lambdaF", main = "Approximate LOOCV")


#
# Arbitrary penalty graphs
#

# Generate some new high-dimensional data and a 2 by 2 factorial design
ns &lt;- c(6, 5, 3, 2)
df &lt;- expand.grid(Factor1 = LETTERS[1:2], Factor2 = letters[3:4])
Ylist &lt;- createS(n = ns, p = 4, dataset = TRUE)
Tlist &lt;- lapply(lapply(Ylist, covML), default.target, type = "Null")

# Construct penalty matrix
lambda &lt;- default.penalty(df, type = "CartesianUnequal")

# Find optimal parameters,
# Using optim with method "Nelder-Mead" with "special" LOOCV
ans1 &lt;- optPenalty.fused(Ylist, Tlist, lambda = lambda,
                         cv.method = "sLOOCV", verbose = FALSE)
print(ans1$lambda.unique)

# By approximate LOOCV using optim with method "BFGS"
ans2 &lt;- optPenalty.fused(Ylist, Tlist, lambda = lambda,
                         cv.method = "aLOOCV", verbose = FALSE,
                         method = "BFGS")
print(ans2$lambda.unique)

# By LOOCV using nlm
lambda.init &lt;- matrix(1, 4, 4)
lambda.init[cbind(1:4,4:1)] &lt;- 0
ans3 &lt;- optPenalty.fused(Ylist, Tlist, lambda = lambda,
                         lambda.init = lambda.init,
                         cv.method = "LOOCV", verbose = FALSE,
                         optimizer = "nlm")
print(ans3$lambda.unique)

# Quite different results!


#
# Arbitrary penalty graphs with fixed penalties!
#

# Generate some new high-dimensional data and a 2 by 2 factorial design
ns &lt;- c(6, 5, 5, 5)
df &lt;- expand.grid(DS = LETTERS[1:2], ER = letters[3:4])
Ylist &lt;- createS(n = ns, p = 4, dataset = TRUE)
Tlist &lt;- lapply(lapply(Ylist, covML), default.target, type = "Null")

lambda &lt;- default.penalty(df, type = "Tensor")
print(lambda)  # Say we want to penalize the pair (1,2) with strength 2.1;
lambda[2,1] &lt;- lambda[1,2] &lt;- 2.1
print(lambda)

# Specifiying starting values is also possible:
init &lt;- diag(length(ns))
init[2,1] &lt;- init[1,2] &lt;- 2.1

res &lt;- optPenalty.fused(Ylist, Tlist, lambda = lambda, lambda.init = init,
                        cv.method = "aLOOCV", optimizer = "nlm")
print(res)

## End(Not run)

</code></pre>

<hr>
<h2 id='optPenalty.kCV'>Select optimal penalty parameter by <code class="reqn">K</code>-fold cross-validation</h2><span id='topic+optPenalty.kCV'></span>

<h3>Description</h3>

<p>Function that selects the optimal penalty parameter for the
<code><a href="#topic+ridgeP">ridgeP</a></code> call by usage of <code class="reqn">K</code>-fold cross-validation. Its
output includes (a.o.) the precision matrix under the optimal value of the
penalty parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optPenalty.kCV(
  Y,
  lambdaMin,
  lambdaMax,
  step,
  fold = nrow(Y),
  cor = FALSE,
  target = default.target(covML(Y)),
  type = "Alt",
  output = "light",
  graph = TRUE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optPenalty.kCV_+3A_y">Y</code></td>
<td>
<p>Data <code>matrix</code>. Variables assumed to be represented by columns.</p>
</td></tr>
<tr><td><code id="optPenalty.kCV_+3A_lambdamin">lambdaMin</code></td>
<td>
<p>A <code>numeric</code> giving the minimum value for the penalty
parameter.</p>
</td></tr>
<tr><td><code id="optPenalty.kCV_+3A_lambdamax">lambdaMax</code></td>
<td>
<p>A <code>numeric</code> giving the maximum value for the penalty
parameter.</p>
</td></tr>
<tr><td><code id="optPenalty.kCV_+3A_step">step</code></td>
<td>
<p>An <code>integer</code> determining the number of steps in moving
through the grid [<code>lambdaMin</code>, <code>lambdaMax</code>].</p>
</td></tr>
<tr><td><code id="optPenalty.kCV_+3A_fold">fold</code></td>
<td>
<p>A <code>numeric</code> or <code>integer</code> specifying the number of
folds to apply in the cross-validation.</p>
</td></tr>
<tr><td><code id="optPenalty.kCV_+3A_cor">cor</code></td>
<td>
<p>A <code>logical</code> indicating if the evaluation of the LOOCV score
should be performed on the correlation scale.</p>
</td></tr>
<tr><td><code id="optPenalty.kCV_+3A_target">target</code></td>
<td>
<p>A target <code>matrix</code> (in precision terms) for Type I ridge
estimators.</p>
</td></tr>
<tr><td><code id="optPenalty.kCV_+3A_type">type</code></td>
<td>
<p>A <code>character</code> indicating the type of ridge estimator to be
used. Must be one of: &quot;Alt&quot;, &quot;ArchI&quot;, &quot;ArchII&quot;.</p>
</td></tr>
<tr><td><code id="optPenalty.kCV_+3A_output">output</code></td>
<td>
<p>A <code>character</code> indicating if the output is either heavy or
light. Must be one of: &quot;all&quot;, &quot;light&quot;.</p>
</td></tr>
<tr><td><code id="optPenalty.kCV_+3A_graph">graph</code></td>
<td>
<p>A <code>logical</code> indicating if the grid search for the optimal
penalty parameter should be visualized.</p>
</td></tr>
<tr><td><code id="optPenalty.kCV_+3A_verbose">verbose</code></td>
<td>
<p>A <code>logical</code> indicating if information on progress should
be printed on screen.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function calculates a cross-validated negative log-likelihood score
(using a regularized ridge estimator for the precision matrix) for each
value of the penalty parameter contained in the search grid by way of
<code class="reqn">K</code>-fold cross-validation. The value of the penalty parameter that
achieves the lowest cross-validated negative log-likelihood score is deemed
optimal. The penalty parameter must be positive such that <code>lambdaMin</code>
must be a positive scalar. The maximum allowable value of <code>lambdaMax</code>
depends on the type of ridge estimator employed. For details on the type of
ridge estimator one may use (one of: &quot;Alt&quot;, &quot;ArchI&quot;, &quot;ArchII&quot;) see
<code><a href="#topic+ridgeP">ridgeP</a></code>. The ouput consists of an object of class list (see
below). When <code>output = "light"</code> (default) only the <code>optLambda</code> and
<code>optPrec</code> elements of the list are given.
</p>


<h3>Value</h3>

<p>An object of class list: </p>
<table>
<tr><td><code>optLambda</code></td>
<td>
<p>A <code>numeric</code> giving
the optimal value of the penalty parameter.</p>
</td></tr> <tr><td><code>optPrec</code></td>
<td>
<p>A <code>matrix</code>
representing the precision matrix of the chosen type (see
<code><a href="#topic+ridgeP">ridgeP</a></code>) under the optimal value of the penalty parameter.</p>
</td></tr>
<tr><td><code>lambdas</code></td>
<td>
<p>A <code>numeric</code> vector representing all values of the
penalty parameter for which cross-validation was performed; Only given when
<code>output = "all"</code>.</p>
</td></tr> <tr><td><code>LLs</code></td>
<td>
<p>A <code>numeric</code> vector representing the
mean of cross-validated negative log-likelihoods for each value of the
penalty parameter given in <code>lambdas</code>; Only given when <code>output =
"all"</code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>When <code>cor = TRUE</code> correlation matrices are used in the
computation of the (cross-validated) negative log-likelihood score, i.e.,
the <code class="reqn">K</code>-fold sample covariance matrix is a matrix on the correlation
scale. When performing evaluation on the correlation scale the data are
assumed to be standardized. If <code>cor = TRUE</code> and one wishes to used the
default target specification one may consider using <code>target =
default.target(covML(Y, cor = TRUE))</code>. This gives a default target under the
assumption of standardized data.
</p>
<p>Under the default setting of the fold-argument, <code>fold = nrow(Y)</code>, one
performes leave-one-out cross-validation.
</p>


<h3>Author(s)</h3>

<p>Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;, Wessel N. van Wieringen
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ridgeP">ridgeP</a></code>, <code><a href="#topic+optPenalty.kCVauto">optPenalty.kCVauto</a></code>,
<code><a href="#topic+optPenalty.aLOOCV">optPenalty.aLOOCV</a></code>, <br /> <code><a href="#topic+default.target">default.target</a></code>,
<code><a href="#topic+covML">covML</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some (high-dimensional) data
p = 25
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]

## Obtain regularized precision under optimal penalty using K = n
OPT  &lt;- optPenalty.kCV(X, lambdaMin = .5, lambdaMax = 30, step = 100); OPT
OPT$optLambda	# Optimal penalty
OPT$optPrec	  # Regularized precision under optimal penalty

## Another example with standardized data
X &lt;- scale(X, center = TRUE, scale = TRUE)
OPT  &lt;- optPenalty.kCV(X, lambdaMin = .5, lambdaMax = 30, step = 100, cor = TRUE,
                       target = default.target(covML(X, cor = TRUE))); OPT
OPT$optLambda	# Optimal penalty
OPT$optPrec	  # Regularized precision under optimal penalty

## Another example using K = 5
OPT  &lt;- optPenalty.kCV(X, lambdaMin = .5, lambdaMax = 30, step = 100, fold = 5); OPT
OPT$optLambda	# Optimal penalty
OPT$optPrec	  # Regularized precision under optimal penalty

</code></pre>

<hr>
<h2 id='optPenalty.kCVauto'>Automatic search for optimal penalty parameter</h2><span id='topic+optPenalty.kCVauto'></span>

<h3>Description</h3>

<p>Function that performs an 'automatic' search for the optimal penalty
parameter for the <code><a href="#topic+ridgeP">ridgeP</a></code> call by employing Brent's method to
the calculation of a cross-validated negative log-likelihood score.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optPenalty.kCVauto(
  Y,
  lambdaMin,
  lambdaMax,
  lambdaInit = (lambdaMin + lambdaMax)/2,
  fold = nrow(Y),
  cor = FALSE,
  target = default.target(covML(Y)),
  type = "Alt"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optPenalty.kCVauto_+3A_y">Y</code></td>
<td>
<p>Data <code>matrix</code>. Variables assumed to be represented by columns.</p>
</td></tr>
<tr><td><code id="optPenalty.kCVauto_+3A_lambdamin">lambdaMin</code></td>
<td>
<p>A <code>numeric</code> giving the minimum value for the penalty
parameter.</p>
</td></tr>
<tr><td><code id="optPenalty.kCVauto_+3A_lambdamax">lambdaMax</code></td>
<td>
<p>A <code>numeric</code> giving the maximum value for the penalty
parameter.</p>
</td></tr>
<tr><td><code id="optPenalty.kCVauto_+3A_lambdainit">lambdaInit</code></td>
<td>
<p>A <code>numeric</code> giving the initial (starting) value for
the penalty parameter.</p>
</td></tr>
<tr><td><code id="optPenalty.kCVauto_+3A_fold">fold</code></td>
<td>
<p>A <code>numeric</code> or <code>integer</code> specifying the number of
folds to apply in the cross-validation.</p>
</td></tr>
<tr><td><code id="optPenalty.kCVauto_+3A_cor">cor</code></td>
<td>
<p>A <code>logical</code> indicating if the evaluation of the LOOCV score
should be performed on the correlation scale.</p>
</td></tr>
<tr><td><code id="optPenalty.kCVauto_+3A_target">target</code></td>
<td>
<p>A target <code>matrix</code> (in precision terms) for Type I ridge
estimators.</p>
</td></tr>
<tr><td><code id="optPenalty.kCVauto_+3A_type">type</code></td>
<td>
<p>A <code>character</code> indicating the type of ridge estimator to be
used. Must be one of: &quot;Alt&quot;, &quot;ArchI&quot;, &quot;ArchII&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function determines the optimal value of the penalty parameter by
application of the Brent algorithm (1971) to the <code class="reqn">K</code>-fold
cross-validated negative log-likelihood score (using a regularized ridge
estimator for the precision matrix). The search for the optimal value is
automatic in the sense that in order to invoke the root-finding abilities of
the Brent method, only a minimum value and a maximum value for the penalty
parameter need to be specified as well as a starting penalty value. The
value at which the <code class="reqn">K</code>-fold cross-validated negative log-likelihood
score is minimized is deemed optimal. The function employs the Brent
algorithm as implemented in the
<a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/optim.html">optim</a>
function.
</p>


<h3>Value</h3>

<p>An object of class <code>list</code>: </p>
<table>
<tr><td><code>optLambda</code></td>
<td>
<p>A <code>numeric</code>
giving the optimal value for the penalty parameter.</p>
</td></tr> <tr><td><code>optPrec</code></td>
<td>
<p>A
<code>matrix</code> representing the precision matrix of the chosen type (see
<code><a href="#topic+ridgeP">ridgeP</a></code>) under the optimal value of the penalty parameter.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>When <code>cor = TRUE</code> correlation matrices are used in the
computation of the (cross-validated) negative log-likelihood score, i.e.,
the <code class="reqn">K</code>-fold sample covariance matrix is a matrix on the correlation
scale. When performing evaluation on the correlation scale the data are
assumed to be standardized. If <code>cor = TRUE</code> and one wishes to used the
default target specification one may consider using <code>target =
default.target(covML(Y, cor = TRUE))</code>. This gives a default target under the
assumption of standardized data.
</p>
<p>Under the default setting of the fold-argument, <code>fold = nrow(Y)</code>, one
performes leave-one-out cross-validation.
</p>


<h3>Author(s)</h3>

<p>Wessel N. van Wieringen, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;
</p>


<h3>References</h3>

<p>Brent, R.P. (1971). An Algorithm with Guaranteed Convergence for
Finding a Zero of a Function. Computer Journal 14: 422-425.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+GGMblockNullPenalty">GGMblockNullPenalty</a></code>, <code><a href="#topic+GGMblockTest">GGMblockTest</a></code>,
<code><a href="#topic+ridgeP">ridgeP</a></code>, <code><a href="#topic+optPenalty.aLOOCV">optPenalty.aLOOCV</a></code>,
<code><a href="#topic+optPenalty.kCV">optPenalty.kCV</a></code>, <br /> <code><a href="#topic+default.target">default.target</a></code>,
<code><a href="#topic+covML">covML</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some (high-dimensional) data
p = 25
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]

## Obtain regularized precision under optimal penalty using K = n
OPT &lt;- optPenalty.kCVauto(X, lambdaMin = .001, lambdaMax = 30); OPT
OPT$optLambda # Optimal penalty
OPT$optPrec   # Regularized precision under optimal penalty

## Another example with standardized data
X &lt;- scale(X, center = TRUE, scale = TRUE)
OPT &lt;- optPenalty.kCVauto(X, lambdaMin = .001, lambdaMax = 30, cor = TRUE,
                          target = default.target(covML(X, cor = TRUE))); OPT
OPT$optLambda # Optimal penalty
OPT$optPrec   # Regularized precision under optimal penalty

## Another example using K = 5
OPT &lt;- optPenalty.kCVauto(X, lambdaMin = .001, lambdaMax = 30, fold = 5); OPT
OPT$optLambda # Optimal penalty
OPT$optPrec   # Regularized precision under optimal penalty

</code></pre>

<hr>
<h2 id='optPenalty.LOOCV'>Select optimal penalty parameter by leave-one-out cross-validation</h2><span id='topic+optPenalty.LOOCV'></span>

<h3>Description</h3>

<p>This function is now deprecated. Please use <code>optPenalty.kCV</code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optPenalty.LOOCV(
  Y,
  lambdaMin,
  lambdaMax,
  step,
  type = "Alt",
  cor = FALSE,
  target = default.target(covML(Y)),
  output = "light",
  graph = TRUE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optPenalty.LOOCV_+3A_y">Y</code></td>
<td>
<p>Data <code>matrix</code>. Variables assumed to be represented by columns.</p>
</td></tr>
<tr><td><code id="optPenalty.LOOCV_+3A_lambdamin">lambdaMin</code></td>
<td>
<p>A <code>numeric</code> giving the minimum value for the penalty
parameter.</p>
</td></tr>
<tr><td><code id="optPenalty.LOOCV_+3A_lambdamax">lambdaMax</code></td>
<td>
<p>A <code>numeric</code> giving the maximum value for the penalty
parameter.</p>
</td></tr>
<tr><td><code id="optPenalty.LOOCV_+3A_step">step</code></td>
<td>
<p>An <code>integer</code> determining the number of steps in moving
through the grid [<code>lambdaMin</code>, <code>lambdaMax</code>].</p>
</td></tr>
<tr><td><code id="optPenalty.LOOCV_+3A_type">type</code></td>
<td>
<p>A <code>character</code> indicating the type of ridge estimator to be
used. Must be one of: &quot;Alt&quot;, &quot;ArchI&quot;, &quot;ArchII&quot;.</p>
</td></tr>
<tr><td><code id="optPenalty.LOOCV_+3A_cor">cor</code></td>
<td>
<p>A <code>logical</code> indicating if the evaluation of the LOOCV score
should be performed on the correlation scale.</p>
</td></tr>
<tr><td><code id="optPenalty.LOOCV_+3A_target">target</code></td>
<td>
<p>A target <code>matrix</code> (in precision terms) for Type I ridge
estimators.</p>
</td></tr>
<tr><td><code id="optPenalty.LOOCV_+3A_output">output</code></td>
<td>
<p>A <code>character</code> indicating if the output is either heavy or
light. Must be one of: &quot;all&quot;, &quot;light&quot;.</p>
</td></tr>
<tr><td><code id="optPenalty.LOOCV_+3A_graph">graph</code></td>
<td>
<p>A <code>logical</code> indicating if the grid search for the optimal
penalty parameter should be visualized.</p>
</td></tr>
<tr><td><code id="optPenalty.LOOCV_+3A_verbose">verbose</code></td>
<td>
<p>A <code>logical</code> indicating if information on progress should
be printed on screen.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function that selects the optimal penalty parameter for the
<code><a href="#topic+ridgeP">ridgeP</a></code> call by usage of leave-one-out cross-validation. Its
output includes (a.o.) the precision matrix under the optimal value of the
penalty parameter.
</p>
<p>The function calculates a cross-validated negative log-likelihood score
(using a regularized ridge estimator for the precision matrix) for each
value of the penalty parameter contained in the search grid by way of
leave-one-out cross-validation. The value of the penalty parameter that
achieves the lowest cross-validated negative log-likelihood score is deemed
optimal. The penalty parameter must be positive such that <code>lambdaMin</code>
must be a positive scalar. The maximum allowable value of <code>lambdaMax</code>
depends on the type of ridge estimator employed. For details on the type of
ridge estimator one may use (one of: &quot;Alt&quot;, &quot;ArchI&quot;, &quot;ArchII&quot;) see
<code><a href="#topic+ridgeP">ridgeP</a></code>. The ouput consists of an object of class list (see
below). When <code>output = "light"</code> (default) only the <code>optLambda</code> and
<code>optPrec</code> elements of the list are given.
</p>


<h3>Value</h3>

<p>An object of class list: </p>
<table>
<tr><td><code>optLambda</code></td>
<td>
<p>A <code>numeric</code> giving
the optimal value of the penalty parameter.</p>
</td></tr> <tr><td><code>optPrec</code></td>
<td>
<p>A <code>matrix</code>
representing the precision matrix of the chosen type (see
<code><a href="#topic+ridgeP">ridgeP</a></code>) under the optimal value of the penalty parameter.</p>
</td></tr>
<tr><td><code>lambdas</code></td>
<td>
<p>A <code>numeric</code> vector representing all values of the
penalty parameter for which cross-validation was performed; Only given when
<code>output = "all"</code>.</p>
</td></tr> <tr><td><code>LLs</code></td>
<td>
<p>A <code>numeric</code> vector representing the
mean of cross-validated negative log-likelihoods for each value of the
penalty parameter given in <code>lambdas</code>; Only given when <code>output =
"all"</code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>When <code>cor = TRUE</code> correlation matrices are used in the
computation of the (cross-validated) negative log-likelihood score, i.e.,
the leave-one-out sample covariance matrix is a matrix on the correlation
scale. When performing evaluation on the correlation scale the data are
assumed to be standardized. If <code>cor = TRUE</code> and one wishes to used the
default target specification one may consider using <code>target =
default.target(covML(Y, cor = TRUE))</code>. This gives a default target under the
assumption of standardized data.
</p>


<h3>Author(s)</h3>

<p>Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;, Wessel N. van Wieringen
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ridgeP">ridgeP</a></code>, <code><a href="#topic+optPenalty.LOOCVauto">optPenalty.LOOCVauto</a></code>,
<code><a href="#topic+optPenalty.aLOOCV">optPenalty.aLOOCV</a></code>, <br /> <code><a href="#topic+default.target">default.target</a></code>,
<code><a href="#topic+covML">covML</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some (high-dimensional) data
p = 25
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]

## Obtain regularized precision under optimal penalty
OPT  &lt;- optPenalty.LOOCV(X, lambdaMin = .5, lambdaMax = 30, step = 100); OPT
OPT$optLambda	# Optimal penalty
OPT$optPrec	  # Regularized precision under optimal penalty

## Another example with standardized data
X &lt;- scale(X, center = TRUE, scale = TRUE)
OPT  &lt;- optPenalty.LOOCV(X, lambdaMin = .5, lambdaMax = 30, step = 100, cor = TRUE,
                         target = default.target(covML(X, cor = TRUE))); OPT
OPT$optLambda	# Optimal penalty
OPT$optPrec	  # Regularized precision under optimal penalty

</code></pre>

<hr>
<h2 id='optPenalty.LOOCVauto'>Automatic search for optimal penalty parameter</h2><span id='topic+optPenalty.LOOCVauto'></span>

<h3>Description</h3>

<p>This function is now deprecated. Please use <code>optPenalty.kCVauto</code>
instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optPenalty.LOOCVauto(
  Y,
  lambdaMin,
  lambdaMax,
  lambdaInit = (lambdaMin + lambdaMax)/2,
  cor = FALSE,
  target = default.target(covML(Y)),
  type = "Alt"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optPenalty.LOOCVauto_+3A_y">Y</code></td>
<td>
<p>Data <code>matrix</code>. Variables assumed to be represented by columns.</p>
</td></tr>
<tr><td><code id="optPenalty.LOOCVauto_+3A_lambdamin">lambdaMin</code></td>
<td>
<p>A <code>numeric</code> giving the minimum value for the penalty
parameter.</p>
</td></tr>
<tr><td><code id="optPenalty.LOOCVauto_+3A_lambdamax">lambdaMax</code></td>
<td>
<p>A <code>numeric</code> giving the maximum value for the penalty
parameter.</p>
</td></tr>
<tr><td><code id="optPenalty.LOOCVauto_+3A_lambdainit">lambdaInit</code></td>
<td>
<p>A <code>numeric</code> giving the initial (starting) value for
the penalty parameter.</p>
</td></tr>
<tr><td><code id="optPenalty.LOOCVauto_+3A_cor">cor</code></td>
<td>
<p>A <code>logical</code> indicating if the evaluation of the LOOCV score
should be performed on the correlation scale.</p>
</td></tr>
<tr><td><code id="optPenalty.LOOCVauto_+3A_target">target</code></td>
<td>
<p>A target <code>matrix</code> (in precision terms) for Type I ridge
estimators.</p>
</td></tr>
<tr><td><code id="optPenalty.LOOCVauto_+3A_type">type</code></td>
<td>
<p>A <code>character</code> indicating the type of ridge estimator to be
used. Must be one of: &quot;Alt&quot;, &quot;ArchI&quot;, &quot;ArchII&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function that performs an 'automatic' search for the optimal penalty
parameter for the <code><a href="#topic+ridgeP">ridgeP</a></code> call by employing Brent's method to
the calculation of a cross-validated negative log-likelihood score.
</p>
<p>The function determines the optimal value of the penalty parameter by
application of the Brent algorithm (1971) to the (leave-one-out)
cross-validated negative log-likelihood score (using a regularized ridge
estimator for the precision matrix). The search for the optimal value is
automatic in the sense that in order to invoke the root-finding abilities of
the Brent method, only a minimum value and a maximum value for the penalty
parameter need to be specified as well as a starting penalty value. The
value at which the (leave-one-out) cross-validated negative log-likelihood
score is minimized is deemed optimal. The function employs the Brent
algorithm as implemented in the
<a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/optim.html">optim</a>
function.
</p>


<h3>Value</h3>

<p>An object of class <code>list</code>: </p>
<table>
<tr><td><code>optLambda</code></td>
<td>
<p>A <code>numeric</code>
giving the optimal value for the penalty parameter.</p>
</td></tr> <tr><td><code>optPrec</code></td>
<td>
<p>A
<code>matrix</code> representing the precision matrix of the chosen type (see
<code><a href="#topic+ridgeP">ridgeP</a></code>) under the optimal value of the penalty parameter.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>When <code>cor = TRUE</code> correlation matrices are used in the
computation of the (cross-validated) negative log-likelihood score, i.e.,
the leave-one-out sample covariance matrix is a matrix on the correlation
scale. When performing evaluation on the correlation scale the data are
assumed to be standardized. If <code>cor = TRUE</code> and one wishes to used the
default target specification one may consider using <code>target =
default.target(covML(Y, cor = TRUE))</code>. This gives a default target under the
assumption of standardized data.
</p>


<h3>Author(s)</h3>

<p>Wessel N. van Wieringen, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;
</p>


<h3>References</h3>

<p>Brent, R.P. (1971). An Algorithm with Guaranteed Convergence for
Finding a Zero of a Function. Computer Journal 14: 422-425.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+GGMblockNullPenalty">GGMblockNullPenalty</a></code>, <code><a href="#topic+GGMblockTest">GGMblockTest</a></code>,
<code><a href="#topic+ridgeP">ridgeP</a></code>, <code><a href="#topic+optPenalty.aLOOCV">optPenalty.aLOOCV</a></code>,
<code><a href="#topic+optPenalty.LOOCV">optPenalty.LOOCV</a></code>, <br /> <code><a href="#topic+default.target">default.target</a></code>,
<code><a href="#topic+covML">covML</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some (high-dimensional) data
p = 25
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]

## Obtain regularized precision under optimal penalty
OPT &lt;- optPenalty.LOOCVauto(X, lambdaMin = .001, lambdaMax = 30); OPT
OPT$optLambda # Optimal penalty
OPT$optPrec   # Regularized precision under optimal penalty

## Another example with standardized data
X &lt;- scale(X, center = TRUE, scale = TRUE)
OPT &lt;- optPenalty.LOOCVauto(X, lambdaMin = .001, lambdaMax = 30, cor = TRUE,
                            target = default.target(covML(X, cor = TRUE))); OPT
OPT$optLambda # Optimal penalty
OPT$optPrec   # Regularized precision under optimal penalty

</code></pre>

<hr>
<h2 id='pcor'>Compute partial correlation matrix or standardized precision matrix</h2><span id='topic+pcor'></span>

<h3>Description</h3>

<p>Function computing the partial correlation matrix or standardized precision
matrix from an input precision matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pcor(P, pc = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pcor_+3A_p">P</code></td>
<td>
<p>(Possibly regularized) precision <code>matrix</code>.</p>
</td></tr>
<tr><td><code id="pcor_+3A_pc">pc</code></td>
<td>
<p>A <code>logical</code> indicating if the partial correlation matrix
should be computed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function assumes that the input <code>matrix</code> is a precision matrix. If
<code>pc = FALSE</code> the standardized precision matrix, rather than the partial
correlation matrix, is given as the output value. The standardized precision
matrix is equal to the partial correlation matrix up to the sign of
off-diagonal entries.
</p>


<h3>Value</h3>

<p>A partial correlation <code>matrix</code> or a standardized precision
<code>matrix</code>.
</p>


<h3>Author(s)</h3>

<p>Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;, Wessel N. van Wieringen
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ridgeP">ridgeP</a></code>, <code><a href="#topic+covML">covML</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some (high-dimensional) data
p = 25
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]
Cx &lt;- covML(X)

## Obtain regularized precision matrix
P &lt;- ridgeP(Cx, lambda = 10, type = "Alt")

## Obtain partial correlation matrix
pcor(P)

</code></pre>

<hr>
<h2 id='pooledS'>Compute the pooled covariance or precision matrix estimate</h2><span id='topic+pooledS'></span><span id='topic+pooledP'></span>

<h3>Description</h3>

<p>Compute the pooled covariance or precision matrix estimate from a <code>list</code>
of covariance matrices or precision matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pooledS(Slist, ns, subset = rep(TRUE, length(ns)), mle = TRUE)

pooledP(Plist, ns, subset = rep(TRUE, length(ns)), mle = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pooledS_+3A_slist">Slist</code></td>
<td>
<p>A <code>list</code> of length <code class="reqn">G</code> of <code>numeric</code> covariance
matrices of the same size.</p>
</td></tr>
<tr><td><code id="pooledS_+3A_ns">ns</code></td>
<td>
<p>A <code>numeric</code> vector for length <code class="reqn">G</code> giving the sample sizes
in the corresponding entries of <code>Slist</code></p>
</td></tr>
<tr><td><code id="pooledS_+3A_subset">subset</code></td>
<td>
<p><code>logical</code> vector of the same length as <code>Slist</code> giving
the classes to pool. Default is all classes.</p>
</td></tr>
<tr><td><code id="pooledS_+3A_mle">mle</code></td>
<td>
<p><code>logical</code>. If <code>TRUE</code>, the (biased) MLE is given. If
<code>FALSE</code>, the biased corrected estimate is given. Default is
<code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="pooledS_+3A_plist">Plist</code></td>
<td>
<p>A <code>list</code> of length <code class="reqn">G</code> of invertible <code>numeric</code>
precision matrices of the same size.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>mle</code> is <code>FALSE</code> the given covariance/precision matrices is
assumed to have been computed using the denominator <code>ns[i] - 1</code>. Hence,
the sum of all <code>ns</code> minus <code class="reqn">G</code> is used a the denominator of the
pooled estimate. Conversely, when <code>mle</code> is <code>TRUE</code> the total sum of
the sample sizes <code>ns</code> is used as the denominator in the pooled estimate.
</p>
<p>The function <code>pooledP</code> is equivalent to a wrapper for <code>pooledS</code>.
That is, it inverts all the precision matrices in <code>Plist</code>, applies
<code>pooledS</code>, and inverts the resulting matrix.
</p>


<h3>Value</h3>

<p><code>pooledS</code> returns the pooled covariance matrix, that is a
<code>numeric</code> matrix with the same size as the elements of <code>Slist</code>.
Similarly, <code>pooledP</code> returns the pooled precision matrix, i.e. a
<code>numeric</code> matrix with the same size as the elements of <code>Plist</code>.
</p>


<h3>Author(s)</h3>

<p>Anders Ellern Bilgrau, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;,
Wessel N. van Wieringen
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ns &lt;- c(4, 6, 8)
Slist &lt;- createS(ns, p = 6)

pooledS(Slist, ns)
pooledS(Slist, ns, mle = FALSE)

# Pool the first two classes only, leave out the remaning
pooledS(Slist, ns, subset = c(TRUE, TRUE, FALSE))
pooledS(Slist, ns, subset = ns &gt; 5) # Pool studies with sample size &gt; 5

# Pooled precision matrices
ns &lt;- c(7, 8, 9)
Plist &lt;- lapply(createS(ns, p = 6), solve)
pooledS(Plist, ns)

</code></pre>

<hr>
<h2 id='print.optPenaltyFusedGrid'>Print and plot functions for fused grid-based cross-validation</h2><span id='topic+print.optPenaltyFusedGrid'></span><span id='topic+plot.optPenaltyFusedGrid'></span>

<h3>Description</h3>

<p>Print and plot functions for the output from
<code><a href="#topic+optPenalty.fused.grid">optPenalty.fused.grid</a></code> which performs a grid based
cross-validation (CV) search to find optimal penalty parameters. Currently,
only the complete penalty graph is supported.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'optPenaltyFusedGrid'
print(x, ...)

## S3 method for class 'optPenaltyFusedGrid'
plot(
  x,
  add.text = TRUE,
  add.contour = TRUE,
  col = rainbow(100, end = 0.8),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.optPenaltyFusedGrid_+3A_x">x</code></td>
<td>
<p>A <code>optPenaltyFusedGrid</code>-object print or plot.  Usually the
output of <br /> <code><a href="#topic+optPenalty.fused.grid">optPenalty.fused.grid</a></code>.</p>
</td></tr>
<tr><td><code id="print.optPenaltyFusedGrid_+3A_...">...</code></td>
<td>
<p>Arguments passed on.  In <code>print.optPenaltyFusedGrid</code> the
arguments are passed to <code>print.matrix</code>.  In
<code>plot.optPenaltyFusedGrid</code> are passed to the standard <code>plot</code>
function.</p>
</td></tr>
<tr><td><code id="print.optPenaltyFusedGrid_+3A_add.text">add.text</code></td>
<td>
<p>A <code>logical</code> value controlling if the text should be
added to the plot.</p>
</td></tr>
<tr><td><code id="print.optPenaltyFusedGrid_+3A_add.contour">add.contour</code></td>
<td>
<p>A <code>logical</code> value controlling if the contour lines
should be added to the plot.</p>
</td></tr>
<tr><td><code id="print.optPenaltyFusedGrid_+3A_col">col</code></td>
<td>
<p>A <code>character</code> vector of colours used in the image plot.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly returns the object (<code>x</code>).
</p>


<h3>Author(s)</h3>

<p>Anders Ellern Bilgrau, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;,
Wessel N. van Wieringen
</p>


<h3>References</h3>

<p>Bilgrau, A.E., Peeters, C.F.W., Eriksen, P.S., Boegsted, M., and
van Wieringen, W.N. (2020).  Targeted Fused Ridge Estimation of Inverse
Covariance Matrices from Multiple High-Dimensional Data Classes.  Journal
of Machine Learning Research, 21(26): 1-52.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+optPenalty.fused.grid">optPenalty.fused.grid</a></code>
</p>

<hr>
<h2 id='print.ptest'>Print and summarize fusion test</h2><span id='topic+print.ptest'></span><span id='topic+summary.ptest'></span>

<h3>Description</h3>

<p>Print and summary functions for the fusion test performed by
<code><a href="#topic+fused.test">fused.test</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ptest'
print(x, digits = 4L, ...)

## S3 method for class 'ptest'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.ptest_+3A_x">x</code>, <code id="print.ptest_+3A_object">object</code></td>
<td>
<p>The object to print or summarize. Usually the output of
<code><a href="#topic+fused.test">fused.test</a></code>.</p>
</td></tr>
<tr><td><code id="print.ptest_+3A_digits">digits</code></td>
<td>
<p>An <code>integer</code> controlling the number of printed digits.</p>
</td></tr>
<tr><td><code id="print.ptest_+3A_...">...</code></td>
<td>
<p>Arguments passed on.  In <code>summary.ptest</code> the arguments are
passed to <code>print.ptest</code>.  In <code>print.ptest</code> are passed to the
standard <code>summary</code> function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly returns the object.
</p>


<h3>Author(s)</h3>

<p>Anders Ellern Bilgrau, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;,
Wessel N. van Wieringen
</p>


<h3>References</h3>

<p>Bilgrau, A.E., Peeters, C.F.W., Eriksen, P.S., Boegsted, M., and
van Wieringen, W.N. (2020).  Targeted Fused Ridge Estimation of Inverse
Covariance Matrices from Multiple High-Dimensional Data Classes.  Journal
of Machine Learning Research, 21(26): 1-52.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fused.test">fused.test</a></code>, <code><a href="#topic+hist.ptest">hist.ptest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ns &lt;- c(10, 5, 23)
Ylist &lt;- createS(ns, p = 15, topology = "banded", dataset = TRUE)

# Use the identity target matrix for each class
Tlist &lt;- replicate(length(ns), diag(15), simplify = FALSE)

# Do the test
lam &lt;- matrix(10, 3, 3)
diag(lam) &lt;- 1
ft &lt;- fused.test(Ylist, Tlist, lambda = lam, n.permutations = 500)

</code></pre>

<hr>
<h2 id='pruneMatrix'>Prune square matrix to those variables having nonzero entries</h2><span id='topic+pruneMatrix'></span>

<h3>Description</h3>

<p>Convenience function that prunes a square matrix to those variables
(features) having nonzero row (column) entries (i.e., to features implied in
graphical connections).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pruneMatrix(M)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pruneMatrix_+3A_m">M</code></td>
<td>
<p>(Possibly sparsified) square <code>matrix</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A pruned <code>matrix</code>.
</p>


<h3>Author(s)</h3>

<p>Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some (high-dimensional) data
p = 25
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]

## Obtain regularized precision under optimal penalty
OPT &lt;- optPenalty.LOOCV(X, lambdaMin = .5, lambdaMax = 30, step = 100)

## Determine support regularized standardized precision under optimal penalty
PC0 &lt;- sparsify(symm(OPT$optPrec), threshold = "localFDR")$sparseParCor

## Prune sparsified partial correlation matrix
PC0P &lt;- pruneMatrix(PC0)

</code></pre>

<hr>
<h2 id='rags2ridges-package'>Ridge estimation for high-dimensional precision matrices</h2><span id='topic+rags2ridges-package'></span><span id='topic+rags2ridges'></span>

<h3>Description</h3>

<p>Package contains proper L2-penalized ML estimators for the precision matrix
as well as supporting functions to employ these estimators in a (integrative
or meta-analytic) graphical modeling setting.
</p>


<h3>Details</h3>

<p>The main function of the package is <code><a href="#topic+ridgeP">ridgeP</a></code> which enables
archetypal and proper alternative ML ridge estimation of the precision
matrix. The alternative ridge estimators can be found in van Wieringen and
Peeters (2015) and encapsulate both target and non-target shrinkage for the
multivariate normal precision matrix. The estimators are analytic and enable
estimation in large <code class="reqn">p</code> small <code class="reqn">n</code> settings. Supporting functions to
employ these estimators in a graphical modeling setting are also given.
These supporting functions enable, a.o., the determination of the optimal
value of the penalty parameter, the determination of the support of a
shrunken precision estimate, as well as various visualization options.
</p>
<p>The package has a modular setup. The <em>core module</em> (rags2ridges.R)
contains the functionality stated above. The <em>fused module</em>
(rags2ridgesFused.R) extends the functionality of the core module to the
joint estimation of multiple precision matrices from (aggregated)
high-dimensional data consisting of distinct classes. The result is a
targeted fused ridge estimator that is of use when the precision matrices of
the constituent classes are believed to chiefly share the same structure
while potentially differing in a number of locations of interest. The fused
module also contains supporting functions for integrative or meta-analytic
Gaussian graphical modeling. The third module is the <em>miscellaneous
module</em> (rags2RidgesMisc.R) which contains assorted hidden functions.
</p>
<p>Function overview <em>core module</em>: </p>
 <ul>
<li><p> Function for (proper)
ridge estimation of the precision matrix </p>
 <ul>
<li>
<p><code><a href="#topic+ridgeP">ridgeP</a></code> </p>
</li></ul>
 </li>
<li><p> Functions for penalty parameter selection
</p>
 <ul>
<li> <p><code><a href="#topic+CNplot">CNplot</a></code> </p>
</li>
<li> <p><code><a href="#topic+optPenalty.aLOOCV">optPenalty.aLOOCV</a></code>
</p>
</li>
<li> <p><code><a href="#topic+optPenalty.kCV">optPenalty.kCV</a></code> </p>
</li>
<li> <p><code><a href="#topic+optPenalty.kCVauto">optPenalty.kCVauto</a></code> </p>
</li></ul>

</li>
<li><p> Functions for loss/entropy/fit evaluation </p>
 <ul>
<li>
<p><code><a href="#topic+evaluateSfit">evaluateSfit</a></code> </p>
</li>
<li> <p><code><a href="#topic+KLdiv">KLdiv</a></code> </p>
</li>
<li>
<p><code><a href="#topic+loss">loss</a></code> </p>
</li></ul>
 </li>
<li><p> Functions for block-independence testing
</p>
 <ul>
<li> <p><code><a href="#topic+GGMblockNullPenalty">GGMblockNullPenalty</a></code> </p>
</li>
<li>
<p><code><a href="#topic+GGMblockTest">GGMblockTest</a></code> </p>
</li></ul>
 </li>
<li><p> Function for support determination
</p>
 <ul>
<li> <p><code><a href="#topic+sparsify">sparsify</a></code> </p>
</li></ul>
 </li>
<li><p> Functions for (network)
visualization </p>
 <ul>
<li> <p><code><a href="#topic+edgeHeat">edgeHeat</a></code> </p>
</li>
<li>
<p><code><a href="#topic+ridgePathS">ridgePathS</a></code> </p>
</li>
<li> <p><code><a href="#topic+Ugraph">Ugraph</a></code> </p>
</li></ul>
 </li>
<li><p> Functions for
topology statistics </p>
 <ul>
<li> <p><code><a href="#topic+GGMmutualInfo">GGMmutualInfo</a></code> </p>
</li>
<li>
<p><code><a href="#topic+GGMnetworkStats">GGMnetworkStats</a></code> </p>
</li>
<li> <p><code><a href="#topic+GGMpathStats">GGMpathStats</a></code> </p>
</li></ul>
 </li>
<li>
<p>Wrapper function </p>
 <ul>
<li> <p><code><a href="#topic+fullMontyS">fullMontyS</a></code> </p>
</li></ul>
 </li>
<li><p> Support
functions </p>
 <ul>
<li> <p><code><a href="#topic+adjacentMat">adjacentMat</a></code> </p>
</li>
<li>
<p><code><a href="#topic+covML">covML</a></code> </p>
</li>
<li> <p><code><a href="#topic+covMLknown">covMLknown</a></code> </p>
</li>
<li>
<p><code><a href="#topic+default.target">default.target</a></code> </p>
</li>
<li> <p><code><a href="#topic+evaluateS">evaluateS</a></code> </p>
</li>
<li>
<p><code><a href="#topic+pcor">pcor</a></code> </p>
</li>
<li> <p><code><a href="#topic+symm">symm</a></code> </p>
</li></ul>
 </li></ul>

<p>Function overview <em>fused module</em>: </p>
 <ul>
<li><p> Function for targeted
fused ridge estimation of multiple precision matrices </p>
 <ul>
<li>
<p><code><a href="#topic+ridgeP.fused">ridgeP.fused</a></code> </p>
</li></ul>
 </li>
<li><p> Function for fused penalty parameter
selection </p>
 <ul>
<li> <p><code><a href="#topic+optPenalty.fused">optPenalty.fused</a></code> </p>
</li></ul>
 </li>
<li><p> Functions
for loss/entropy/fit evaluation </p>
 <ul>
<li> <p><code><a href="#topic+KLdiv.fused">KLdiv.fused</a></code>
</p>
</li>
<li> <p><code><a href="#topic+NLL">NLL</a></code> </p>
</li></ul>
 </li>
<li><p> Function for testing the necessity of fusion
</p>
 <ul>
<li> <p><code><a href="#topic+fused.test">fused.test</a></code> </p>
</li></ul>
 </li>
<li><p> Function for support
determination </p>
 <ul>
<li> <p><code><a href="#topic+sparsify.fused">sparsify.fused</a></code> </p>
</li></ul>
 </li>
<li><p> Functions
for topology statistics </p>
 <ul>
<li> <p><code><a href="#topic+GGMnetworkStats.fused">GGMnetworkStats.fused</a></code>
</p>
</li>
<li> <p><code><a href="#topic+GGMpathStats.fused">GGMpathStats.fused</a></code> </p>
</li></ul>
 </li>
<li><p> Support functions </p>

<ul>
<li> <p><code><a href="#topic+createS">createS</a></code> </p>
</li>
<li> <p><code><a href="#topic+default.penalty">default.penalty</a></code> </p>
</li>
<li>
<p><code><a href="#topic+default.target.fused">default.target.fused</a></code> </p>
</li>
<li> <p><code><a href="#topic+getKEGGPathway">getKEGGPathway</a></code> </p>
</li>
<li>
<p><code><a href="#topic+isSymmetricPD">isSymmetricPD</a></code> </p>
</li>
<li> <p><code><a href="#topic+is.Xlist">is.Xlist</a></code> </p>
</li>
<li>
<p><code><a href="#topic+kegg.target">kegg.target</a></code> </p>
</li>
<li> <p><code><a href="#topic+plot.ptest">plot.ptest</a></code> </p>
</li>
<li>
<p><code><a href="#topic+pooledS">pooledS</a></code> </p>
</li>
<li> <p><code><a href="#topic+print.optPenaltyFusedGrid">print.optPenaltyFusedGrid</a></code> </p>
</li>
<li>
<p><code><a href="#topic+print.ptest">print.ptest</a></code> </p>
</li>
<li> <p><code><a href="#topic+rmvnormal">rmvnormal</a></code> </p>
</li></ul>
 </li></ul>

<p>Calls of interest to <em>miscellaneous module</em>: </p>
 <ul>
<li>
<p><code>rags2ridges:::.TwoCents()</code> ~~(Unsolicited advice) </p>
</li>
<li>
<p><code>rags2ridges:::.Brooke()</code> ~~(Endorsement) </p>
</li>
<li>
<p><code>rags2ridges:::.JayZScore()</code> ~~(The truth) </p>
</li>
<li>
<p><code>rags2ridges:::.theHoff()</code> ~~(Wish) </p>
</li>
<li>
<p><code>rags2ridges:::.rags2logo()</code> ~~(Warm welcome) </p>
</li></ul>



<h3>Author(s)</h3>

<p>Carel F.W. Peeters, Anders Ellern Bilgrau, Wessel, N. van Wieringen
<br /> Maintainer: Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;
</p>


<h3>References</h3>

<p>Peeters, C.F.W., Bilgrau, A.E., and van Wieringen, W.N. (2022).
rags2ridges: A One-Stop-l2-Shop for Graphical Modeling of High-Dimensional
Precision Matrices. Journal of Statistical Software, vol. 102(4): 1-32.
</p>
<p>Bilgrau, A.E., Peeters, C.F.W., Eriksen, P.S., Boegsted, M., and
van Wieringen, W.N. (2020).  Targeted Fused Ridge Estimation of Inverse
Covariance Matrices from Multiple High-Dimensional Data Classes.  Journal of
Machine Learning Research, 21(26): 1-52.  Also available as
arXiv:1509.07982v2 [stat.ME].
</p>
<p>Peeters, C.F.W., van de Wiel, M.A., &amp; van Wieringen, W.N. (2020).  The
Spectral Condition Number Plot for Regularization Parameter Evaluation.
Computational Statistics, 35: 629-646.  Also available as arXiv:1608.04123
[stat.CO].
</p>
<p>van Wieringen, W.N. &amp; Peeters, C.F.W. (2016).  Ridge Estimation of Inverse
Covariance Matrices from High-Dimensional Data.  Computational Statistics &amp;
Data Analysis, vol. 103: 284-303.  Also available as arXiv:1403.0904v3
[stat.ME].
</p>
<p>van Wieringen, W.N. &amp; Peeters, C.F.W. (2015).  Application of a New Ridge
Estimator of the Inverse Covariance Matrix to the Reconstruction of
Gene-Gene Interaction Networks.  In: di Serio, C., Lio, P., Nonis, A., and
Tagliaferri, R. (Eds.)  'Computational Intelligence Methods for
Bioinformatics and Biostatistics'.  Lecture Notes in Computer Science, vol.
8623. Springer, pp. 170-179.
</p>

<hr>
<h2 id='ridgeP'>Ridge estimation for high-dimensional precision matrices</h2><span id='topic+ridgeP'></span>

<h3>Description</h3>

<p>Function that calculates various Ridge estimators for high-dimensional
precision matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ridgeP(S, lambda, type = "Alt", target = default.target(S))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ridgeP_+3A_s">S</code></td>
<td>
<p>Sample covariance <code>matrix</code>.</p>
</td></tr>
<tr><td><code id="ridgeP_+3A_lambda">lambda</code></td>
<td>
<p>A <code>numeric</code> representing the value of the penalty
parameter.</p>
</td></tr>
<tr><td><code id="ridgeP_+3A_type">type</code></td>
<td>
<p>A <code>character</code> indicating the type of ridge estimator to be
used. Must be one of: &quot;Alt&quot;, &quot;ArchI&quot;, &quot;ArchII&quot;.</p>
</td></tr>
<tr><td><code id="ridgeP_+3A_target">target</code></td>
<td>
<p>A target <code>matrix</code> (in precision terms) for Type I ridge
estimators.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function can calculate various ridge estimators for high-dimensional
precision matrices. Current (well-known) ridge estimators can be roughly
divided in two archetypes. The first archetypal form employs a convex
combination of <code class="reqn">\mathbf{S}</code> and a positive definite (p.d.) target matrix
<code class="reqn">\mathbf{T}</code>:
<code class="reqn">\hat{\mathbf{\Omega}}^{\mathrm{I}}(\lambda_{\mathrm{I}})
  = [(1-\lambda_{\mathrm{I}}) \mathbf{S} + \lambda_{\mathrm{I}}\mathbf{T}]^{-1}</code>,
with <code class="reqn">\lambda_{\mathrm{I}} \in (0,1]</code>.
A common target choice is for <code class="reqn">\mathbf{T}</code> to be diagonal with
<code class="reqn">(\mathbf{T})_{jj} = (\mathbf{S})_{jj}</code> for <code class="reqn">j=1, \ldots, p</code>.
The second archetypal form can be given as
<code class="reqn">\hat{\mathbf{\Omega}}^{\mathrm{II}}(\lambda_{\mathrm{II}})
   = (\mathbf{S} + \lambda_{\mathrm{II}} \mathbf{I}_{p})^{-1}</code>
with <code class="reqn">\lambda_{\mathrm{II}} \in (0, \infty)</code>. Viewed from a penalized
estimation perspective, the two archetypes utilize penalties that do not
coincide with the matrix-analogue of the common ridge penalty. van Wieringen
and Peeters (2015) derive analytic expressions for alternative Type I and
Type II ridge precision estimators based on a proper L2-penalty. Their
alternative Type I estimator (target shrinkage) takes the form
</p>
<p style="text-align: center;"><code class="reqn">
    \hat{\mathbf{\Omega}}^{\mathrm{I}a}(\lambda_{a})
    = \left\{
        \left[\lambda_{a}\mathbf{I}_{p} + \frac{1}{4}(\mathbf{S} -
              \lambda_{a}\mathbf{T})^{2}\right]^{1/2} +
        \frac{1}{2}(\mathbf{S} - \lambda_{a}\mathbf{T})
      \right\}^{-1},
  </code>
</p>

<p>while their alternative Type II estimator can be given as a special case of
the former:
</p>
<p style="text-align: center;"><code class="reqn">
    \hat{\mathbf{\Omega}}^{\mathrm{II}a}(\lambda_{a}) =
    \left\{
      \left[\lambda_{a}\mathbf{I}_{p} +
              \frac{1}{4}\mathbf{S}^{2}\right]^{1/2} +
      \frac{1}{2}\mathbf{S}
    \right\}^{-1}.
  </code>
</p>

<p>These alternative estimators were shown to be superior to the archetypes in
terms of risk under various loss functions (van Wieringen and Peeters, 2015).
</p>
<p>The <code>lambda</code> parameter in <code>ridgeP</code> generically indicates the
penalty parameter. It must be chosen in accordance with the type of ridge
estimator employed. The domains for the penalty parameter in the archetypal
estimators are given above. The domain for <code>lambda</code> in the alternative
estimators is <code class="reqn">(0, \infty)</code>. The <code>type</code> parameter specifies the type
of ridge estimator. Specifying <code>type = "ArchI"</code> leads to usage of the
archetypal I estimator while specifying <code>type = "ArchII"</code> leads to usage
of the archetypal II estimator. In the latter situation the argument
<code>target</code> remains unused. Specifying <code>type = "Alt"</code> enables usage of
the alternative ridge estimators: when <code>type = "Alt"</code> and the
<code>target</code> matrix is p.d. one obtains the alternative Type I estimator;
when <code>type = "Alt"</code> and the <code>target</code> matrix is specified to be the
null-matrix one obtains the alternative Type II estimator.
</p>
<p>The Type I estimators thus employ target shrinkage. The default target for
both the archetype and alternative is <code>default.target(S)</code>. When
<code>target</code> is not the null-matrix it is expected to be p.d. for the
alternative type I estimator. The target is always expected to be p.d. in
case of the archetypal I estimator. The archetypal Type I ridge estimator is
rotation equivariant when the target is of the form <code class="reqn">\mu\mathbf{I}_{p}</code>
with <code class="reqn">\mu \in (0,\infty)</code>. The archetypal Type II estimator is rotation
equivariant by definition. When the target is of the form
<code class="reqn">\varphi\mathbf{I}_{p}</code> with <code class="reqn">\varphi \in [0,\infty)</code>, then the
alternative ridge estimator is rotation equivariant. Its analytic computation
is then particularly speedy as the (relatively) expensive matrix square root
can then be circumvented.
</p>


<h3>Value</h3>

<p>Function returns a regularized precision <code>matrix</code>.
</p>


<h3>Author(s)</h3>

<p>Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;, Anders E. Bilgrau
</p>


<h3>References</h3>

<p>van Wieringen, W.N. &amp; Peeters, C.F.W. (2016). Ridge Estimation
of Inverse Covariance Matrices from High-Dimensional Data, Computational
Statistics &amp; Data Analysis, vol. 103: 284-303. Also available as
arXiv:1403.0904v3 [stat.ME].
</p>
<p>van Wieringen, W.N. &amp; Peeters, C.F.W. (2015). Application of a New Ridge
Estimator of the Inverse Covariance Matrix to the Reconstruction of
Gene-Gene Interaction Networks. In: di Serio, C., Lio, P., Nonis, A., and
Tagliaferri, R. (Eds.) 'Computational Intelligence Methods for
Bioinformatics and Biostatistics'. Lecture Notes in Computer Science, vol.
8623. Springer, pp. 170-179.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+default.target">default.target</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(333)

## Obtain some (high-dimensional) data
p &lt;- 25
n &lt;- 10
X &lt;- matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:p] = letters[1:p]
Cx &lt;- covML(X)

## Obtain regularized precision matrix
P &lt;- ridgeP(Cx, lambda = 10, type = "Alt")
summary(P)
print(P)
plot(P)
</code></pre>

<hr>
<h2 id='ridgeP.fused'>Fused ridge estimation</h2><span id='topic+ridgeP.fused'></span>

<h3>Description</h3>

<p>Performs fused ridge estimation of multiple precision matrices in cases
where multiple classes of data is present for given a penalty matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ridgeP.fused(
  Slist,
  ns,
  Tlist = default.target.fused(Slist, ns),
  lambda,
  Plist,
  maxit = 100L,
  verbose = TRUE,
  relative = TRUE,
  eps = sqrt(.Machine$double.eps)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ridgeP.fused_+3A_slist">Slist</code></td>
<td>
<p>A <code>list</code> of length <code class="reqn">G</code> of covariance matrices, i.e.
square, symmetric <code>numeric</code> matrices of the same size.  The <code class="reqn">g</code>th
matrix should correspond to the <code class="reqn">g</code>th class.</p>
</td></tr>
<tr><td><code id="ridgeP.fused_+3A_ns">ns</code></td>
<td>
<p>A <code>numeric</code> vector of sample sizes on which the matrices in
<code>Slist</code> are based.  I.e. <code>ns[g]</code> correspond to <code>Slist[[g]]</code>.</p>
</td></tr>
<tr><td><code id="ridgeP.fused_+3A_tlist">Tlist</code></td>
<td>
<p>A <code>list</code> of length <code class="reqn">G</code> of <code>numeric</code> p.d. target
matrices corresponding to the matrices in <code>Slist</code>.  If not supplied,
the default is given by <code><a href="#topic+default.target">default.target</a></code>.</p>
</td></tr>
<tr><td><code id="ridgeP.fused_+3A_lambda">lambda</code></td>
<td>
<p>The <code class="reqn">G</code> by <code class="reqn">G</code> penalty matrix.  That is, a symmetric,
non-negative <code>numeric</code> <code>matrix</code> of size <code class="reqn">G</code> times <code class="reqn">G</code>
giving the class- and pair-specific penalties.  The diagonal entries are the
class specific ridge penalties.  I.e. <code>lambda[i, i]</code> is the ridge
penalty for class <code class="reqn">i</code>.  The off-diagonal entries are the pair-specific
fusion penalties.  I.e. <code>lambda[i, j]</code> is the fusion penalty applied on
the pair of classes <code class="reqn">i</code> and <code class="reqn">j</code>.
</p>
<p>Alternatively, can be supplied as a <code>numeric</code> of length 1 or 2.  If a
single number, a diagonal penalty with lambda in the diagonal is used If
supplied as a <code>numeric</code> vector of two numbers, the first is used as a
common ridge penalty and the second as a common fusion penalty.</p>
</td></tr>
<tr><td><code id="ridgeP.fused_+3A_plist">Plist</code></td>
<td>
<p>An optional <code>list</code> of initial precision matrices for the
fused ridge algorithm the same size as <code>Slist</code>.  Can be omitted.
Default is the nonfused ridge precision estimate using the pooled
covariance matrix corresponding to setting all fusion penalties to zero.</p>
</td></tr>
<tr><td><code id="ridgeP.fused_+3A_maxit">maxit</code></td>
<td>
<p>A single <code>integer</code> giving the maximum number of allowed
iterations.  Can be set to <code>Inf</code>.  If <code>maxit</code> is hit, a warning
is given.</p>
</td></tr>
<tr><td><code id="ridgeP.fused_+3A_verbose">verbose</code></td>
<td>
<p><code>logical</code>. Set to <code>TRUE</code> for extra output.</p>
</td></tr>
<tr><td><code id="ridgeP.fused_+3A_relative">relative</code></td>
<td>
<p><code>logical</code> indicating if the convergence criterion should
be on a relative scale.</p>
</td></tr>
<tr><td><code id="ridgeP.fused_+3A_eps">eps</code></td>
<td>
<p>A single positive <code>numeric</code> giving the convergence threshold.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Performs a coordinate ascent to find the maximum likelihood of the fused
likelihood problem for a given ridge penalty <code class="reqn">lambda</code> and fused penalty
matrix <code class="reqn">Lambda_f</code>.
</p>


<h3>Value</h3>

<p>Returns a <code>list</code> as <code>Slist</code> with precision estimates of the
corresponding classes.
</p>


<h3>Note</h3>

<p>For extreme fusion penalties in <code>lambda</code> the algorithm is quite
sensitive to the initial values given in <code>Plist</code>.
</p>


<h3>Author(s)</h3>

<p>Anders Ellern Bilgrau, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;,
Wessel N. van Wieringen
</p>


<h3>References</h3>

<p>Bilgrau, A.E., Peeters, C.F.W., Eriksen, P.S., Boegsted, M., and
van Wieringen, W.N. (2020).  Targeted Fused Ridge Estimation of Inverse
Covariance Matrices from Multiple High-Dimensional Data Classes.  Journal
of Machine Learning Research, 21(26): 1-52.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+default.penalty">default.penalty</a></code> <br /> <code><a href="#topic+ridgeP">ridgeP</a></code> for the
regular ridge estimate
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create some (not at all high-dimensional) data on three classes
p &lt;- 5  # Dimension
ns &lt;- c(4, 6, 8)  # Sample sizes (K = 3 classes)
Slist &lt;- createS(ns, p = p)
str(Slist, max.level = 2)  # The structure of Slist

#
# Estimate the precisions (using the complete penalty graph)
#

res1 &lt;- ridgeP.fused(Slist, ns, lambda = c(1.3, 2.1))
print(res1)

# The same using the penalty matrix (the diagnal is ignored)
mylambda  &lt;- matrix(c(1.3, 2.1, 2.1,
                      2.1, 1.3, 2.1,
                      2.1, 2.1, 1.3), 3, 3, byrow = TRUE)
res2 &lt;- ridgeP.fused(Slist, ns, lambda = mylambda)
stopifnot(all.equal(res1, res2))


#
# Estimate the precisions (using a non-complete penalty graph)
#

# Say we only want to shrink pairs (1,2) and (2,3) and not (1,3)
mylambda[1,3] &lt;- mylambda[3,1] &lt;- 0
print(mylambda)
res3 &lt;- ridgeP.fused(Slist, ns, lambda = mylambda)
# which similar to, but not the same as res1 and res2.


#
# Using other custom target matrices
#

# Construct a custom target list
myTlist &lt;- list(diag(p), matrix(1, p, p), matrix(0, p, p))
res4 &lt;- ridgeP.fused(Slist, ns, Tlist = myTlist, lambda = c(1.3, 2.1))
print(res4)

# Alternative, see ?default.target.fused
myTlist2 &lt;- default.target.fused(Slist, ns, type = "Null")  # For the null target
res5 &lt;- ridgeP.fused(Slist, ns, Tlist = myTlist2, lambda = c(1.3, 2.1))
print(res5)

</code></pre>

<hr>
<h2 id='ridgePathS'>Visualize the regularization path</h2><span id='topic+ridgePathS'></span>

<h3>Description</h3>

<p>Function that visualizes the regularization paths of the nonredundant
elements of a regularized precision matrix against the (range of the)
penalty parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ridgePathS(
  S,
  lambdaMin,
  lambdaMax,
  step,
  type = "Alt",
  target = default.target(S),
  plotType = "pcor",
  diag = FALSE,
  vertical = FALSE,
  value,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ridgePathS_+3A_s">S</code></td>
<td>
<p>Sample covariance <code>matrix</code>.</p>
</td></tr>
<tr><td><code id="ridgePathS_+3A_lambdamin">lambdaMin</code></td>
<td>
<p>A <code>numeric</code> giving the minimum value for the penalty
parameter.</p>
</td></tr>
<tr><td><code id="ridgePathS_+3A_lambdamax">lambdaMax</code></td>
<td>
<p>A <code>numeric</code> giving the maximum value for the penalty
parameter.</p>
</td></tr>
<tr><td><code id="ridgePathS_+3A_step">step</code></td>
<td>
<p>An <code>integer</code> determining the number of steps in moving
through the grid [<code>lambdaMin</code>, <code>lambdaMax</code>].</p>
</td></tr>
<tr><td><code id="ridgePathS_+3A_type">type</code></td>
<td>
<p>A <code>character</code> indicating the type of ridge estimator to be
used. Must be one of: &quot;Alt&quot;, &quot;ArchI&quot;, &quot;ArchII&quot;.</p>
</td></tr>
<tr><td><code id="ridgePathS_+3A_target">target</code></td>
<td>
<p>A target <code>matrix</code> (in precision terms) for Type I ridge
estimators.</p>
</td></tr>
<tr><td><code id="ridgePathS_+3A_plottype">plotType</code></td>
<td>
<p>A <code>character</code> indicating the type of element for which
a visualization of the regularization paths is desired. Must be one of:
&quot;pcor&quot;, &quot;cor&quot;, &quot;cov&quot;, &quot;prec&quot;.</p>
</td></tr>
<tr><td><code id="ridgePathS_+3A_diag">diag</code></td>
<td>
<p>A <code>logical</code> indicating if the diagonal elements should be
retained for visualization.</p>
</td></tr>
<tr><td><code id="ridgePathS_+3A_vertical">vertical</code></td>
<td>
<p>A <code>logical</code> indicating if output graph should come with
a vertical line at a pre-specified value for the penalty parameter.</p>
</td></tr>
<tr><td><code id="ridgePathS_+3A_value">value</code></td>
<td>
<p>A <code>numeric</code> indicating a pre-specified value for the
penalty parameter.</p>
</td></tr>
<tr><td><code id="ridgePathS_+3A_verbose">verbose</code></td>
<td>
<p>A <code>logical</code> indicating if information on progress should
be printed on screen.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function visualizes the regularization path of the individual elements
of a regularized precision matrix against the penalty parameter. The range
of the penalty parameter is given by [<code>lambdaMin</code>,<code>lambdaMax</code>].
The penalty parameter must be positive such that <code>lambdaMin</code> must be a
positive scalar. The maximum allowable value of <code>lambdaMax</code> depends on
the type of ridge estimator employed. For details on the type of ridge
estimator one may use (one of: &quot;Alt&quot;, &quot;ArchI&quot;, &quot;ArchII&quot;) see
<code><a href="#topic+ridgeP">ridgeP</a></code>.
</p>
<p>Regularization paths may be visualized for (partial) correlations,
covariances and precision elements. The type of element for which a
visualization of the regularization paths is desired can be indicated by the
argument <code>plotType</code>. When <code>vertical = TRUE</code> a vertical line is
added at the constant <code>value</code>. This option can be used to assess
whereabouts the optimal penalty obtained by, e.g., the routines
<code><a href="#topic+optPenalty.LOOCV">optPenalty.LOOCV</a></code> or <code><a href="#topic+optPenalty.aLOOCV">optPenalty.aLOOCV</a></code>, finds
itself along the regularization path.
</p>


<h3>Author(s)</h3>

<p>Wessel N. van Wieringen, Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ridgeP">ridgeP</a></code>, <code><a href="#topic+covML">covML</a></code>,
<code><a href="#topic+optPenalty.LOOCV">optPenalty.LOOCV</a></code>, <code><a href="#topic+optPenalty.aLOOCV">optPenalty.aLOOCV</a></code>,
<code><a href="#topic+default.target">default.target</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some (high-dimensional) data
p = 25
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]
Cx &lt;- covML(X)

## Visualize regularization paths
ridgePathS(Cx, .001, 50, 200, plotType = "pcor")

</code></pre>

<hr>
<h2 id='ridgeS'>Ridge estimation for high-dimensional precision matrices</h2><span id='topic+ridgeS'></span>

<h3>Description</h3>

<p>This function is now deprecated. Please use <code>ridgeP</code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ridgeS(S, lambda, type = "Alt", target = default.target(S))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ridgeS_+3A_s">S</code></td>
<td>
<p>Sample covariance <code>matrix</code>.</p>
</td></tr>
<tr><td><code id="ridgeS_+3A_lambda">lambda</code></td>
<td>
<p>A <code>numeric</code> representing the value of the penalty
parameter.</p>
</td></tr>
<tr><td><code id="ridgeS_+3A_type">type</code></td>
<td>
<p>A <code>character</code> indicating the type of ridge estimator to be
used. Must be one of: &quot;Alt&quot;, &quot;ArchI&quot;, &quot;ArchII&quot;.</p>
</td></tr>
<tr><td><code id="ridgeS_+3A_target">target</code></td>
<td>
<p>A target <code>matrix</code> (in precision terms) for Type I ridge
estimators.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code>ridgeP</code>.
</p>


<h3>Value</h3>

<p>Function returns a regularized precision <code>matrix</code>.
</p>


<h3>Author(s)</h3>

<p>Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;, Wessel N. van Wieringen
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ridgeP">ridgeP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some (high-dimensional) data
p = 25
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]
Cx &lt;- covML(X)

## Obtain regularized precision matrix
ridgeS(Cx, lambda = 10, type = "Alt")

</code></pre>

<hr>
<h2 id='rmvnormal'>Multivariate Gaussian simulation</h2><span id='topic+rmvnormal'></span>

<h3>Description</h3>

<p>Fast simulation from multivariate Gaussian probability distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmvnormal(n, mu, sigma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmvnormal_+3A_n">n</code></td>
<td>
<p>An <code>integer</code> giving the number of observations to be
simulated.</p>
</td></tr>
<tr><td><code id="rmvnormal_+3A_mu">mu</code></td>
<td>
<p>A <code>numeric</code> vector of dimension <code class="reqn">p</code> giving the means of
normal distribution.</p>
</td></tr>
<tr><td><code id="rmvnormal_+3A_sigma">sigma</code></td>
<td>
<p>A variance-covariance <code>matrix</code> of dimension <code class="reqn">p</code> times
<code class="reqn">p</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>rmvnormal</code> function is copied from the <code>GMCM</code>-package. It is
similar to <code>rmvnorm</code> from the <code>mvtnorm</code>-package.
</p>


<h3>Value</h3>

<p>Returns a <code class="reqn">n</code> by <code class="reqn">p</code> matrix of observations from a
multivariate normal distribution with the given mean <code>mu</code> and
covariance
</p>


<h3>Author(s)</h3>

<p>Anders Ellern Bilgrau
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
rmvnormal(n = 10, mu = 1:4, sigma = diag(4))

</code></pre>

<hr>
<h2 id='sparsify'>Determine the support of a partial correlation/precision matrix</h2><span id='topic+sparsify'></span>

<h3>Description</h3>

<p>Function that determines the support of a partial correlation/precision
matrix by thresholding and sparsifies it accordingly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sparsify(
  P,
  threshold = c("absValue", "connected", "localFDR", "top"),
  absValueCut = 0.25,
  FDRcut = 0.9,
  top = 10,
  output = "heavy",
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sparsify_+3A_p">P</code></td>
<td>
<p>(Possibly regularized) precision <code>matrix</code>.</p>
</td></tr>
<tr><td><code id="sparsify_+3A_threshold">threshold</code></td>
<td>
<p>A <code>character</code> signifying type of sparsification by
thresholding. Must be one of: &quot;absValue&quot;, &quot;connected&quot;, &quot;localFDR&quot;, &quot;top&quot;.</p>
</td></tr>
<tr><td><code id="sparsify_+3A_absvaluecut">absValueCut</code></td>
<td>
<p>A <code>numeric</code> giving the cut-off for partial
correlation element selection based on absolute value thresholding.</p>
</td></tr>
<tr><td><code id="sparsify_+3A_fdrcut">FDRcut</code></td>
<td>
<p>A <code>numeric</code> giving the cut-off for partial correlation
element selection based on local false discovery rate (FDR) thresholding.</p>
</td></tr>
<tr><td><code id="sparsify_+3A_top">top</code></td>
<td>
<p>A <code>numeric</code> specifying the exact number of partial
correlation elements to retain based on absolute value.</p>
</td></tr>
<tr><td><code id="sparsify_+3A_output">output</code></td>
<td>
<p>A <code>character</code> specifying the type of output required.
Must be one of: &quot;heavy&quot;, &quot;light&quot;.</p>
</td></tr>
<tr><td><code id="sparsify_+3A_verbose">verbose</code></td>
<td>
<p>A <code>logical</code> indicating if intermediate output should be
printed on screen.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function transforms the possibly regularized input precision matrix to a
partial correlation matrix. Subsequently, the support of this partial
correlation matrix is determined. Support determination is performed either
by simple thresholding on the absolute values of matrix entries
(<code>threshold = "absValue"</code>) or by usage of local FDR (<code>threshold =
"localFDR"</code>). A third option is to retain a prespecified number of matrix
entries based on absolute values. For example, one could wish to retain
those entries representing the ten strongest absolute partial correlations
(<code>threshold = "top"</code>). As a variation on this theme, a fourth option
(<code>threshold = "connected"</code>) retains the top edges such that the
resulting graph is connected (this may result in dense graphs in practice).
The argument <code>absValueCut</code> is only used when <code>threshold =
"absValue"</code>. The argument <code>top</code> is only used when <code>threshold =
"top"</code>. The argument <code>FDRcut</code> is only used when <code>threshold =
"localFDR"</code>.
</p>
<p>The function is to some extent a wrapper around certain
<a href="https://cran.r-project.org/package=fdrtool">fdrtool</a> functions when
<code>threshold = "localFDR"</code>. In that case a mixture model is fitted to the
nonredundant partial correlations by
<a href="https://cran.r-project.org/package=fdrtool">fdrtool</a>. The decision to
retain elements is then based on the argument <code>FDRcut</code>. Elements with a
posterior probability <code class="reqn">\geq</code> FDRcut (equalling 1 - local FDR) are
retained. See Schaefer and Strimmer (2005) for further details on usage of
local FDR in graphical modeling.
</p>


<h3>Value</h3>

<p>If the input <code>P</code> is a standardized precision (or partial
correlation) matrix the function returns a sparsified precision (or partial
correlation) <code>matrix</code> whenever <code>output = "heavy"</code>. If the input
<code>P</code> is an unstandardized precision matrix the function returns an
object of class <code>list</code> whenever <code>output = "heavy"</code>:
</p>
<table>
<tr><td><code>sparseParCor</code></td>
<td>
<p>A <code>matrix</code> representing the sparsified partial
correlation matrix.</p>
</td></tr> <tr><td><code>sparsePrecision</code></td>
<td>
<p>A <code>matrix</code> representing the
sparsified precision matrix.</p>
</td></tr>
</table>
<p>When <code>output = "light"</code>, only the (matrix) positions of the zero and
non-zero elements are returned in an object of class <code>list</code>:
</p>
<table>
<tr><td><code>zeros</code></td>
<td>
<p>A <code>matrix</code> representing the row and column positions of
zero entries.</p>
</td></tr> <tr><td><code>nonzeros</code></td>
<td>
<p>A <code>matrix</code> representing the row and
column positions of non-zero entries.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;, Wessel N. van Wieringen
</p>


<h3>References</h3>

<p>Schaefer, J., and Strimmer, K. (2005). A shrinkage approach to
large-scale covariance estimation and implications for functional genomics.
Statistical Applications in Genetics and Molecular Biology, 4:32.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ridgeP">ridgeP</a></code>, <code><a href="#topic+optPenalty.aLOOCV">optPenalty.aLOOCV</a></code>,
<code><a href="#topic+optPenalty.LOOCV">optPenalty.LOOCV</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some (high-dimensional) data
p = 25
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]

## Obtain regularized precision under optimal penalty
OPT &lt;- optPenalty.LOOCV(X, lambdaMin = .5, lambdaMax = 30, step = 100)

## Determine support regularized (standardized) precision under optimal penalty
sparsify(OPT$optPrec, threshold = "localFDR")

</code></pre>

<hr>
<h2 id='sparsify.fused'>Determine support of multiple partial correlation/precision matrices</h2><span id='topic+sparsify.fused'></span>

<h3>Description</h3>

<p>A simple wrapper for <code><a href="#topic+sparsify">sparsify</a></code> which determines the support of
a <code>list</code> of partial correlation/precision matrix by various methods and
returns the sparsified matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sparsify.fused(Plist, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sparsify.fused_+3A_plist">Plist</code></td>
<td>
<p>A <code>list</code> of <code>numeric</code> precision matrices.</p>
</td></tr>
<tr><td><code id="sparsify.fused_+3A_...">...</code></td>
<td>
<p>Arguments passed to <code><a href="#topic+sparsify">sparsify</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>list</code> of the same length as <code>Plist</code> with the output from
<code><a href="#topic+sparsify">sparsify</a></code>.
</p>


<h3>Author(s)</h3>

<p>Anders Ellern Bilgrau, Wessel N. van Wierigen, Carel F.W. Peeters
&lt;carel.peeters@wur.nl&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sparsify">sparsify</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ns &lt;- c(10, 11)
Ylist &lt;- createS(ns, p = 16, dataset = TRUE)
Slist &lt;- lapply(Ylist, covML)
Tlist &lt;- default.target.fused(Slist, ns)

# Obtain regularized precision under optimal penalty
opt &lt;- optPenalty.fused.auto(Ylist, Tlist, cv.method = "aLOOCV",
                            maxit.ridgeP.fused = 1500)
# Use the optimal penalties
Plist &lt;- ridgeP.fused(Slist, ns, lambda = opt$lambda, maxit = 1000)

# Determine support regularized (standardized) precision under optimal penalty
res &lt;- sparsify.fused(Plist, threshold = "top", verbose = FALSE)
round(res[[1]]$sparsePrecision, 1)
round(res[[2]]$sparsePrecision, 1)

</code></pre>

<hr>
<h2 id='symm'>Symmetrize matrix</h2><span id='topic+symm'></span>

<h3>Description</h3>

<p>Function that symmetrizes matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>symm(M)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="symm_+3A_m">M</code></td>
<td>
<p>(In numeric ideality symmetric) square <code>matrix</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Large objects that are symmetric sometimes fail to be recognized as such by
R due to rounding under machine precision. This function symmetrizes for
computational purposes matrices that are symmetric in numeric ideality.
</p>


<h3>Value</h3>

<p>A symmetric <code>matrix</code>.
</p>


<h3>Author(s)</h3>

<p>Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;, Wessel N. van Wieringen
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some (high-dimensional) data
p = 25
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]

## Obtain regularized precision under optimal penalty
OPT &lt;- optPenalty.LOOCV(X, 10, 30, 10, target = diag(diag(1/covML(X))))

## Check symmetry
## OPT$optPrec is symmetric by definition
## But is not recognized as such due to rounding peculiarities
isSymmetric(OPT$optPrec)

## Symmetrize
symm(OPT$optPrec)

</code></pre>

<hr>
<h2 id='Ugraph'>Visualize undirected graph</h2><span id='topic+Ugraph'></span>

<h3>Description</h3>

<p>Function that visualizes the sparsified precision matrix as an undirected
graph.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Ugraph(
  M,
  type = c("plain", "fancy", "weighted"),
  lay = "layout_in_circle",
  coords = NULL,
  Vsize = 15,
  Vcex = 1,
  Vcolor = "orangered",
  VBcolor = "darkred",
  VLcolor = "black",
  prune = FALSE,
  legend = FALSE,
  label = "",
  Lcex = 1.3,
  PTcex = 4,
  cut = 0.5,
  scale = 10,
  pEcolor = "black",
  nEcolor = "grey",
  main = ""
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Ugraph_+3A_m">M</code></td>
<td>
<p>(Possibly sparsified) precision <code>matrix</code></p>
</td></tr>
<tr><td><code id="Ugraph_+3A_type">type</code></td>
<td>
<p>A <code>character</code> indicating the type of graph to be produced.
Must be one of: &quot;plain&quot;, &quot;fancy&quot;, &quot;weighted&quot;.</p>
</td></tr>
<tr><td><code id="Ugraph_+3A_lay">lay</code></td>
<td>
<p>A <code>character</code> mimicking a call to <code><a href="igraph.html#topic+igraph">igraph</a></code>
layout functions. Determines the placement of vertices.</p>
</td></tr>
<tr><td><code id="Ugraph_+3A_coords">coords</code></td>
<td>
<p>A <code>matrix</code> containing coordinates. Alternative to the
lay-argument for determining the placement of vertices.</p>
</td></tr>
<tr><td><code id="Ugraph_+3A_vsize">Vsize</code></td>
<td>
<p>A <code>numeric</code> determining the vertex size.</p>
</td></tr>
<tr><td><code id="Ugraph_+3A_vcex">Vcex</code></td>
<td>
<p>A <code>numeric</code> determining the size of the vertex labels.</p>
</td></tr>
<tr><td><code id="Ugraph_+3A_vcolor">Vcolor</code></td>
<td>
<p>A <code>character</code> (scalar or vector) determining the vertex
color.</p>
</td></tr>
<tr><td><code id="Ugraph_+3A_vbcolor">VBcolor</code></td>
<td>
<p>A <code>character</code> determining the color of the vertex
border.</p>
</td></tr>
<tr><td><code id="Ugraph_+3A_vlcolor">VLcolor</code></td>
<td>
<p>A <code>character</code> determining the color of the vertex
labels.</p>
</td></tr>
<tr><td><code id="Ugraph_+3A_prune">prune</code></td>
<td>
<p>A <code>logical</code> determining if vertices of degree 0 should be
removed.</p>
</td></tr>
<tr><td><code id="Ugraph_+3A_legend">legend</code></td>
<td>
<p>A <code>logical</code> indicating if the graph should come with a
legend.</p>
</td></tr>
<tr><td><code id="Ugraph_+3A_label">label</code></td>
<td>
<p>A <code>character</code> giving a name to the legend label.</p>
</td></tr>
<tr><td><code id="Ugraph_+3A_lcex">Lcex</code></td>
<td>
<p>A <code>numeric</code> determining the size of the legend box.</p>
</td></tr>
<tr><td><code id="Ugraph_+3A_ptcex">PTcex</code></td>
<td>
<p>A <code>numeric</code> determining the size of the exemplary vertex
in the legend box.</p>
</td></tr>
<tr><td><code id="Ugraph_+3A_cut">cut</code></td>
<td>
<p>A <code>numeric</code> indicating the cut-off for indicating strong
edges when <code>type = "fancy"</code>.</p>
</td></tr>
<tr><td><code id="Ugraph_+3A_scale">scale</code></td>
<td>
<p>A <code>numeric</code> representing a scale factor for visualizing
strength of edges when <code>type = "weighted"</code>.</p>
</td></tr>
<tr><td><code id="Ugraph_+3A_pecolor">pEcolor</code></td>
<td>
<p>A <code>character</code> determining the color of the edges tied to
positive precision elements. Only when <code>type = "weighted"</code>.</p>
</td></tr>
<tr><td><code id="Ugraph_+3A_necolor">nEcolor</code></td>
<td>
<p>A <code>character</code> determining the color of the edges tied to
negative precision elements. Only when <code>type = "weighted"</code>.</p>
</td></tr>
<tr><td><code id="Ugraph_+3A_main">main</code></td>
<td>
<p>A <code>character</code> giving the main figure title.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The intended use of this function is to visualize a sparsified
precision/partial correlation matrix as an undirected graph. When <code>type
= "plain"</code> a plain undirected graph is given representing the conditional
(in)dependencies exemplified by the sparsified precision.
</p>
<p>When <code>type = "fancy"</code> a more elaborate graph is given in which dashed
lines indicate negative partial correlations while solid lines indicate
positive partial correlations, and in which grey lines indicate strong
edges. Strong edges are deemed such by setting <code>cut</code>. If a the absolute
value of a precision element <code class="reqn">\geq</code> <code>cut</code> the corresponding edge is
deemed strong and colored grey in the graph. The argument <code>cut</code> is thus
only used when <code>type = "fancy"</code>.
</p>
<p>When <code>type = "weighted"</code> an undirected graph is given in which edge
thickness represents the strength of the partial correlations. The
<code>nEcolor</code> colored edges then represent negative partial correlations
while <code>pEcolor</code> colored edges represent positive partial correlations.
(Relative) edge thickness in this type of graph can be set by the argument
<code>scale</code>. The arguments <code>scale</code>, <code>nEcolor</code>, and <code>pEcolor</code>
are thus only used when <code>type = "weighted"</code>.
</p>
<p>The default layout gives a circular placement of the vertices. Most layout
functions supported by <code><a href="igraph.html#topic+igraph">igraph</a></code> are supported (the function is
partly a wrapper around certain <code><a href="igraph.html#topic+igraph">igraph</a></code> functions). The igraph
layouts can be invoked by a <code>character</code> that mimicks a call to a
<code><a href="igraph.html#topic+igraph">igraph</a></code> layout functions in the <code>lay</code> argument. When using
<code>lay = NULL</code> one can specify the placement of vertices with the
<code>coords</code> argument. The row dimension of this matrix should equal the
number of (pruned) vertices. The column dimension then should equal 2 (for
2D layouts) or 3 (for 3D layouts). The <code>coords</code> argument can also be
viewed as a convenience argument as it enables one, e.g., to layout a graph
according to the coordinates of a previous call to <code>Ugraph</code>. If both
the the lay and the coords arguments are not <code>NULL</code>, the lay argument
takes precedence
</p>
<p>The legend allows one to specify the kind of variable the vertices
represent, such as, e.g., mRNA transcripts. The arguments <code>label</code>,
<code>Lcex</code>, and <code>PTcex</code> are only used when <code>legend = TRUE</code>.
</p>
<p>If <code>prune = TRUE</code> the vertices of degree 0 (vertices not implicated by
any edge) are removed. For the colors supported by the arguments
<code>Vcolor</code>, <code>VBcolor</code>, <code>VLcolor</code>, <code>pEcolor</code>, and
<code>nEcolor</code> see <a href="https://www.nceas.ucsb.edu/sites/default/files/2020-04/colorPaletteCheatsheet.pdf">https://www.nceas.ucsb.edu/sites/default/files/2020-04/colorPaletteCheatsheet.pdf</a>.
</p>


<h3>Value</h3>

<p>The function returns a graph. The function also returns a
<code>matrix</code> object containing the coordinates of the vertices in the given
graph.
</p>


<h3>Author(s)</h3>

<p>Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;
</p>


<h3>References</h3>

<p>Csardi, G. and Nepusz, T. (2006). The igraph software package
for complex network research. InterJournal, Complex Systems 1695.
http://igraph.sf.net
</p>
<p>van Wieringen, W.N. &amp; Peeters, C.F.W. (2016). Ridge Estimation of Inverse
Covariance Matrices from High-Dimensional Data, Computational Statistics &amp;
Data Analysis, vol. 103: 284-303. Also available as arXiv:1403.0904v3
[stat.ME].
</p>
<p>van Wieringen, W.N. &amp; Peeters, C.F.W. (2015). Application of a New Ridge
Estimator of the Inverse Covariance Matrix to the Reconstruction of
Gene-Gene Interaction Networks. In: di Serio, C., Lio, P., Nonis, A., and
Tagliaferri, R. (Eds.) 'Computational Intelligence Methods for
Bioinformatics and Biostatistics'. Lecture Notes in Computer Science, vol.
8623. Springer, pp. 170-179.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ridgeP">ridgeP</a></code>, <code><a href="#topic+optPenalty.LOOCV">optPenalty.LOOCV</a></code>,
<code><a href="#topic+optPenalty.aLOOCV">optPenalty.aLOOCV</a></code>, <code><a href="#topic+sparsify">sparsify</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Obtain some (high-dimensional) data
p = 25
n = 10
set.seed(333)
X = matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X)[1:25] = letters[1:25]

## Obtain regularized precision under optimal penalty
OPT &lt;- optPenalty.LOOCV(X, lambdaMin = .5, lambdaMax = 30, step = 100)

## Determine support regularized standardized precision under optimal penalty
PC0 &lt;- sparsify(symm(OPT$optPrec), threshold = "localFDR")$sparseParCor

## Obtain graphical representation
Ugraph(PC0, type = "fancy", cut = 0.07)

## Obtain graphical representation with Fruchterman-Reingold layout
Ugraph(PC0, type = "fancy", lay = "layout_with_fr", cut = 0.07)

## Add pruning
Ugraph(PC0, type = "fancy", lay = "layout_with_fr",
       cut = 0.07, prune = TRUE)

## Obtain graph and its coordinates
Coordinates &lt;- Ugraph(PC0, type = "fancy", lay = "layout_with_fr",
                      cut = 0.07, prune = TRUE)
Coordinates

</code></pre>

<hr>
<h2 id='Union'>Subset 2 square matrices to union of variables having nonzero entries</h2><span id='topic+Union'></span>

<h3>Description</h3>

<p>Convenience function that subsets 2 square matrices (over the same features)
to the union of features that have nonzero row (column) entries (i.e.,
features implied in graphical connections).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Union(M1, M2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Union_+3A_m1">M1</code></td>
<td>
<p>(Possibly sparsified) square <code>matrix</code>.</p>
</td></tr>
<tr><td><code id="Union_+3A_m2">M2</code></td>
<td>
<p>(Possibly sparsified) square <code>matrix</code> over the same features
as <code>M1</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Say you have 2 class-specific precision matrices that are estimated over the
same variables/features. For various reasons (such as, e.g., the desire to
visualize pruned class-specific networks in the same coordinates) one may
want to prune these matrices to those features that are implied in graphical
connections in at least 1 class.
</p>


<h3>Value</h3>

<p>An object of class list: </p>
<table>
<tr><td><code>M1subset</code></td>
<td>
<p>A pruned <code>matrix</code> for
class 1.</p>
</td></tr> <tr><td><code>M2subset</code></td>
<td>
<p>A pruned <code>matrix</code> for class 2.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Carel F.W. Peeters &lt;carel.peeters@wur.nl&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Ugraph">Ugraph</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Invoke data
data(ADdata)

## Subset
ADclass1 &lt;- ADmetabolites[, sampleInfo$ApoEClass == "Class 1"]
ADclass2 &lt;- ADmetabolites[, sampleInfo$ApoEClass == "Class 2"]

## Transpose data
ADclass1 &lt;- t(ADclass1)
ADclass2 &lt;- t(ADclass2)

## Correlations for subsets
rAD1 &lt;- cor(ADclass1)
rAD2 &lt;- cor(ADclass2)

## Simple precision estimates
P1 &lt;- ridgeP(rAD1, 2)
P2 &lt;- ridgeP(rAD2, 2)
Plist = list(P1 = P1, P2 = P2)

## Threshold matrices
Mats &lt;- sparsify.fused(Plist, threshold = "top", top = 20)

## Prune sparsified partial correlation matrices
## To union of features implied by edge
MatsPrune &lt;- Union(Mats$P1$sparseParCor, Mats$P2$sparseParCor)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
