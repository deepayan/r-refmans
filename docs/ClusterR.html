<!DOCTYPE html><html><head><title>Help for package ClusterR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ClusterR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#AP_affinity_propagation'><p>Affinity propagation clustering</p></a></li>
<li><a href='#AP_preferenceRange'><p>Affinity propagation preference range</p></a></li>
<li><a href='#center_scale'><p>Function to scale and/or center the data</p></a></li>
<li><a href='#Clara_Medoids'><p>Clustering large applications</p></a></li>
<li><a href='#Cluster_Medoids'><p>Partitioning around medoids</p></a></li>
<li><a href='#cost_clusters_from_dissim_medoids'><p>Compute the cost and clusters based on an input dissimilarity matrix and medoids</p></a></li>
<li><a href='#dietary_survey_IBS'>
<p>Synthetic data using a dietary survey of patients	with irritable	bowel syndrome   (IBS)</p></a></li>
<li><a href='#distance_matrix'><p>Distance matrix calculation</p></a></li>
<li><a href='#entropy_formula'><p>entropy formula (used in external_validation function)</p></a></li>
<li><a href='#external_validation'><p>external clustering validation</p></a></li>
<li><a href='#function_interactive'><p>Interactive function for consecutive plots ( using dissimilarities or the silhouette widths of the observations )</p></a></li>
<li><a href='#GMM'><p>Gaussian Mixture Model clustering</p></a></li>
<li><a href='#KMeans_arma'><p>k-means using the Armadillo library</p></a></li>
<li><a href='#KMeans_rcpp'><p>k-means using RcppArmadillo</p></a></li>
<li><a href='#MiniBatchKmeans'><p>Mini-batch-k-means using RcppArmadillo</p></a></li>
<li><a href='#mushroom'>
<p>The mushroom data</p></a></li>
<li><a href='#Optimal_Clusters_GMM'><p>Optimal number of Clusters for the gaussian mixture models</p></a></li>
<li><a href='#Optimal_Clusters_KMeans'><p>Optimal number of Clusters for Kmeans or Mini-Batch-Kmeans</p></a></li>
<li><a href='#Optimal_Clusters_Medoids'><p>Optimal number of Clusters for the partitioning around Medoids functions</p></a></li>
<li><a href='#plot_2d'><p>2-dimensional plots</p></a></li>
<li><a href='#predict_GMM'><p>Prediction function for a Gaussian Mixture Model object</p></a></li>
<li><a href='#predict_KMeans'><p>Prediction function for the k-means</p></a></li>
<li><a href='#predict_MBatchKMeans'><p>Prediction function for Mini-Batch-k-means</p></a></li>
<li><a href='#predict_Medoids'><p>Predictions for the Medoid functions</p></a></li>
<li><a href='#Silhouette_Dissimilarity_Plot'><p>Plot of silhouette widths or dissimilarities</p></a></li>
<li><a href='#silhouette_of_clusters'><p>Silhouette width based on pre-computed clusters</p></a></li>
<li><a href='#soybean'>
<p>The soybean (large) data set from the UCI repository</p></a></li>
<li><a href='#tryCatch_GMM'><p>tryCatch function to prevent armadillo errors</p></a></li>
<li><a href='#tryCatch_KMEANS_arma'><p>tryCatch function to prevent armadillo errors in KMEANS_arma</p></a></li>
<li><a href='#tryCatch_optimal_clust_GMM'><p>tryCatch function to prevent armadillo errors in GMM_arma_AIC_BIC</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Gaussian Mixture Models, K-Means, Mini-Batch-Kmeans, K-Medoids
and Affinity Propagation Clustering</td>
</tr>
<tr>
<td>Version:</td>
<td>1.3.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-12-04</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Lampros Mouselimis &lt;mouselimislampros@gmail.com&gt;</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/mlampros/ClusterR/issues">https://github.com/mlampros/ClusterR/issues</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/mlampros/ClusterR">https://github.com/mlampros/ClusterR</a></td>
</tr>
<tr>
<td>Description:</td>
<td>Gaussian mixture models, k-means, mini-batch-kmeans, k-medoids and affinity propagation clustering with the option to plot, validate, predict (new data) and estimate the optimal number of clusters. The package takes advantage of 'RcppArmadillo' to speed up the computationally intensive parts of the functions. For more information, see (i) "Clustering in an Object-Oriented Environment" by Anja Struyf, Mia Hubert, Peter Rousseeuw (1997), Journal of Statistical Software, &lt;<a href="https://doi.org/10.18637%2Fjss.v001.i04">doi:10.18637/jss.v001.i04</a>&gt;; (ii) "Web-scale k-means clustering" by D. Sculley (2010), ACM Digital Library, &lt;<a href="https://doi.org/10.1145%2F1772690.1772862">doi:10.1145/1772690.1772862</a>&gt;; (iii) "Armadillo: a template-based C++ library for linear algebra" by Sanderson et al (2016), The Journal of Open Source Software, &lt;<a href="https://doi.org/10.21105%2Fjoss.00026">doi:10.21105/joss.00026</a>&gt;; (iv) "Clustering by Passing Messages Between Data Points" by Brendan J. Frey and Delbert Dueck, Science 16 Feb 2007: Vol. 315, Issue 5814, pp. 972-976, &lt;<a href="https://doi.org/10.1126%2Fscience.1136800">doi:10.1126/science.1136800</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>libarmadillo: apt-get install -y libarmadillo-dev
(deb), libblas: apt-get install -y libblas-dev (deb),
liblapack: apt-get install -y liblapack-dev (deb),
libarpack++2: apt-get install -y libarpack++2-dev (deb),
gfortran: apt-get install -y gfortran (deb), libgmp3: apt-get
install -y libgmp3-dev (deb), libfftw3: apt-get install -y
libfftw3-dev (deb), libtiff5: apt-get install -y libtiff5-dev
(deb)</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>Depends:</td>
<td>R(&ge; 3.2)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.5), graphics, grDevices, utils, stats, gmp,
ggplot2, lifecycle</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo (&ge; 0.9.1)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>OpenImageR, FD, testthat, covr, knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-12-04 16:44:28 UTC; lampros</td>
</tr>
<tr>
<td>Author:</td>
<td>Lampros Mouselimis
    <a href="https://orcid.org/0000-0002-8024-1546"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, cre],
  Conrad Sanderson [cph] (Author of the C++ Armadillo library),
  Ryan Curtin [cph] (Author of the C++ Armadillo library),
  Siddharth Agrawal [cph] (Author of the C code of the Mini-Batch-Kmeans
    algorithm
    (https://github.com/siddharth-agrawal/Mini-Batch-K-Means)),
  Brendan Frey [cph] (Author of the matlab code of the Affinity
    propagation algorithm (for commercial use please contact the author
    of the matlab code)),
  Delbert Dueck [cph] (Author of the matlab code of the Affinity
    propagation algorithm),
  Vitalie Spinu [ctb] (Github Contributor)</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-12-04 18:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='AP_affinity_propagation'>Affinity propagation clustering</h2><span id='topic+AP_affinity_propagation'></span>

<h3>Description</h3>

<p>Affinity propagation clustering
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AP_affinity_propagation(
  data,
  p,
  maxits = 1000,
  convits = 100,
  dampfact = 0.9,
  details = FALSE,
  nonoise = 0,
  time = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AP_affinity_propagation_+3A_data">data</code></td>
<td>
<p>a matrix. Either a similarity matrix (where number of rows equal to number of columns) or a 3-dimensional matrix where the 1st, 2nd and 3rd column correspond to (i-index, j-index, value) triplet of a similarity matrix.</p>
</td></tr>
<tr><td><code id="AP_affinity_propagation_+3A_p">p</code></td>
<td>
<p>a numeric vector of size 1 or size equal to the number of rows of the input matrix. See the details section for more information.</p>
</td></tr>
<tr><td><code id="AP_affinity_propagation_+3A_maxits">maxits</code></td>
<td>
<p>a numeric value specifying the maximum number of iterations (defaults to 1000)</p>
</td></tr>
<tr><td><code id="AP_affinity_propagation_+3A_convits">convits</code></td>
<td>
<p>a numeric value. If the estimated exemplars stay fixed for convits iterations, the affinity propagation algorithm terminates early (defaults to 100)</p>
</td></tr>
<tr><td><code id="AP_affinity_propagation_+3A_dampfact">dampfact</code></td>
<td>
<p>a float number specifying the update equation damping level in [0.5, 1). Higher values correspond to heavy damping, which may be needed if oscillations occur (defaults to 0.9)</p>
</td></tr>
<tr><td><code id="AP_affinity_propagation_+3A_details">details</code></td>
<td>
<p>a boolean specifying if details should be printed in the console</p>
</td></tr>
<tr><td><code id="AP_affinity_propagation_+3A_nonoise">nonoise</code></td>
<td>
<p>a float number. The affinity propagation algorithm adds a small amount of noise to <em>data</em> to prevent degenerate cases; this disables that.</p>
</td></tr>
<tr><td><code id="AP_affinity_propagation_+3A_time">time</code></td>
<td>
<p>a boolean. If TRUE then the elapsed time will be printed in the console.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <em>affinity propagation</em> algorithm automatically determines the number of clusters based on the input preference <em>p</em>, a real-valued N-vector. p(i) indicates the preference that data point i be
chosen as an exemplar. Often a good choice is to set all preferences to median(data). The number of clusters identified can be adjusted by changing this value accordingly. If <em>p</em> is a scalar, assumes all
preferences are that shared value.
</p>
<p>The number of clusters eventually emerges by iteratively passing messages between data points to update two matrices, A and R (Frey and Dueck 2007). The &quot;responsibility&quot; matrix R has values r(i, k)
that quantify how well suited point k is to serve as the exemplar for point i relative to other candidate exemplars for point i. The &quot;availability&quot; matrix A contains values a(i, k) representing how
&quot;appropriate&quot; point k would be as an exemplar for point i, taking into account other points' preferences for point k as an exemplar. Both matrices R and A are initialized with all zeros. The AP
algorithm then performs updates iteratively over the two matrices. First, &quot;Responsibilities&quot; r(i, k) are sent from data points to candidate exemplars to indicate how strongly each data point favors
the candidate exemplar over other candidate exemplars. &quot;Availabilities&quot; a(i, k) then are sent from candidate exemplars to data points to indicate the degree to which each candidate exemplar is
available to be a cluster center for the data point. In this case, the responsibilities and availabilities are messages that provide evidence about whether each data point should be an exemplar and,
if not, to what exemplar that data point should be assigned. For each iteration in the message-passing procedure, the sum of r(k; k) + a(k; k) can be used to identify exemplars. After the messages
have converged, two ways exist to identify exemplars. In the first approach, for data point i, if r(i, i) + a(i, i) &gt; 0, then data point i is an exemplar. In the second approach, for data point i,
if r(i, i) + a(i, i) &gt; r(i, j) + a(i, j) for all i not equal to j, then data point i is an exemplar. The entire procedure terminates after it reaches a predefined number of iterations or if the
determined clusters have remained constant for a certain number of iterations... ( https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5650075/  &ndash; See chapter 2 )
</p>
<p>Excluding the main diagonal of the similarity matrix when calculating the median as preference ('p') value can be considered as another option too.
</p>


<h3>References</h3>

<p>https://www.psi.toronto.edu/index.php?q=affinity
</p>
<p>https://www.psi.toronto.edu/affinitypropagation/faq.html
</p>
<p>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5650075/    ( SEE chapter 2 )
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1)
dat = matrix(sample(1:255, 2500, replace = TRUE), 100, 25)

smt = 1.0 - distance_matrix(dat, method = 'euclidean', upper = TRUE, diagonal = TRUE)
diag(smt) = 0.0

ap = AP_affinity_propagation(smt, p = median(as.vector(smt)))

str(ap)

</code></pre>

<hr>
<h2 id='AP_preferenceRange'>Affinity propagation preference range</h2><span id='topic+AP_preferenceRange'></span>

<h3>Description</h3>

<p>Affinity propagation preference range
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AP_preferenceRange(data, method = "bound", threads = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AP_preferenceRange_+3A_data">data</code></td>
<td>
<p>a matrix. Either a similarity matrix (where number of rows equal to number of columns) or a 3-dimensional matrix where the 1st, 2nd and 3rd column correspond to (i-index, j-index, value) triplet of a similarity matrix.</p>
</td></tr>
<tr><td><code id="AP_preferenceRange_+3A_method">method</code></td>
<td>
<p>a character string specifying the preference range method to use. One of 'exact', 'bound'. See the details section for more information.</p>
</td></tr>
<tr><td><code id="AP_preferenceRange_+3A_threads">threads</code></td>
<td>
<p>an integer specifying the number of cores to run in parallel ( applies only if <em>method</em> is set to 'exact' which is more computationally intensive )</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a set of similarities, <em>data</em>, this function computes a lower bound, pmin, on the value for the preference where the optimal number of clusters (exemplars) changes from 1 to 2,
and the exact value of the preference, pmax, where the optimal number of clusters changes from n-1 to n. For N data points, there may be as many as N^2-N pair-wise similarities (note that
the similarity of data point i to k need not be equal to the similarity of data point k to i). These may be passed in an NxN matrix of similarities, <em>data</em>, where data(i,k) is the similarity of
point i to point k. In fact, only a smaller number of relevant similarities need to be provided, in which case the others are assumed to be -Inf. M similarity values are known, can be passed
in an Mx3 matrix <em>data</em>, where each row of <em>data</em> contains a pair of data point indices and a corresponding similarity value: data(j,3) is the similarity of data point data(j,1) to
data point data(j,2).
</p>
<p>A single-cluster solution may not exist, in which case pmin is set to NaN. The <em>AP_preferenceRange</em> uses one of the methods below to compute pmin and pmax:
</p>
<p><em>exact</em> : Computes the exact values for pmin and pmax (Warning: This can be quite slow)
<em>bound</em> : Computes the exact value for pmax, but estimates pmin using a bound (default)
</p>


<h3>References</h3>

<p>https://www.psi.toronto.edu/affinitypropagation/preferenceRange.m
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1)
dat = matrix(sample(1:255, 2500, replace = TRUE), 100, 25)

smt = 1.0 - distance_matrix(dat, method = 'euclidean', upper = TRUE, diagonal = TRUE)
diag(smt) = 0.0

ap_range = AP_preferenceRange(smt, method = "bound")

</code></pre>

<hr>
<h2 id='center_scale'>Function to scale and/or center the data</h2><span id='topic+center_scale'></span>

<h3>Description</h3>

<p>Function to scale and/or center the data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>center_scale(data, mean_center = TRUE, sd_scale = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="center_scale_+3A_data">data</code></td>
<td>
<p>matrix or data frame</p>
</td></tr>
<tr><td><code id="center_scale_+3A_mean_center">mean_center</code></td>
<td>
<p>either TRUE or FALSE. If mean_center is TRUE then the mean of each column will be subtracted</p>
</td></tr>
<tr><td><code id="center_scale_+3A_sd_scale">sd_scale</code></td>
<td>
<p>either TRUE or FALSE. See the details section for more information</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If sd_scale is TRUE and mean_center is TRUE then each column will be divided by the standard deviation. If sd_scale is TRUE and mean_center is FALSE then each
column will be divided by sqrt( sum(x^2) / (n-1) ).
In case of missing values the function raises an error.
In case that the standard deviation equals zero then the standard deviation will be replaced with 1.0, so that NaN's can be avoided by division
</p>


<h3>Value</h3>

<p>a matrix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(dietary_survey_IBS)

dat = dietary_survey_IBS[, -ncol(dietary_survey_IBS)]

dat = center_scale(dat, mean_center = TRUE, sd_scale = TRUE)

</code></pre>

<hr>
<h2 id='Clara_Medoids'>Clustering large applications</h2><span id='topic+Clara_Medoids'></span>

<h3>Description</h3>

<p>Clustering large applications
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Clara_Medoids(
  data,
  clusters,
  samples,
  sample_size,
  distance_metric = "euclidean",
  minkowski_p = 1,
  threads = 1,
  swap_phase = TRUE,
  fuzzy = FALSE,
  verbose = FALSE,
  seed = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Clara_Medoids_+3A_data">data</code></td>
<td>
<p>matrix or data frame</p>
</td></tr>
<tr><td><code id="Clara_Medoids_+3A_clusters">clusters</code></td>
<td>
<p>the number of clusters</p>
</td></tr>
<tr><td><code id="Clara_Medoids_+3A_samples">samples</code></td>
<td>
<p>number of samples to draw from the data set</p>
</td></tr>
<tr><td><code id="Clara_Medoids_+3A_sample_size">sample_size</code></td>
<td>
<p>fraction of data to draw in each sample iteration. It should be a float number greater than 0.0 and less or equal to 1.0</p>
</td></tr>
<tr><td><code id="Clara_Medoids_+3A_distance_metric">distance_metric</code></td>
<td>
<p>a string specifying the distance method. One of,  <em>euclidean</em>,  <em>manhattan</em>,  <em>chebyshev</em>,  <em>canberra</em>,  <em>braycurtis</em>,  <em>pearson_correlation</em>,  <em>simple_matching_coefficient</em>,  <em>minkowski</em>,  <em>hamming</em>,  <em>jaccard_coefficient</em>,  <em>Rao_coefficient</em>,  <em>mahalanobis</em>, <em>cosine</em></p>
</td></tr>
<tr><td><code id="Clara_Medoids_+3A_minkowski_p">minkowski_p</code></td>
<td>
<p>a numeric value specifying the minkowski parameter in case that distance_metric = &quot;minkowski&quot;</p>
</td></tr>
<tr><td><code id="Clara_Medoids_+3A_threads">threads</code></td>
<td>
<p>an integer specifying the number of cores to run in parallel. Openmp will be utilized to parallelize the number of the different sample draws</p>
</td></tr>
<tr><td><code id="Clara_Medoids_+3A_swap_phase">swap_phase</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then both phases ('build' and 'swap') will take place. The 'swap_phase' is considered more computationally intensive.</p>
</td></tr>
<tr><td><code id="Clara_Medoids_+3A_fuzzy">fuzzy</code></td>
<td>
<p>either TRUE or FALSE. If TRUE, then probabilities for each cluster will be returned based on the distance between observations and medoids</p>
</td></tr>
<tr><td><code id="Clara_Medoids_+3A_verbose">verbose</code></td>
<td>
<p>either TRUE or FALSE, indicating whether progress is printed during clustering</p>
</td></tr>
<tr><td><code id="Clara_Medoids_+3A_seed">seed</code></td>
<td>
<p>integer value for random number generator (RNG)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Clara_Medoids function is implemented in the same way as the 'clara' (clustering large applications) algorithm (Kaufman and Rousseeuw(1990)). In the 'Clara_Medoids'
the 'Cluster_Medoids' function will be applied to each sample draw.
</p>


<h3>Value</h3>

<p>a list with the following attributes : medoids, medoid_indices, sample_indices, best_dissimilarity, clusters, fuzzy_probs (if fuzzy = TRUE), clustering_stats, dissimilarity_matrix, silhouette_matrix
</p>


<h3>Author(s)</h3>

<p>Lampros Mouselimis
</p>


<h3>References</h3>

<p>Anja Struyf, Mia Hubert, Peter J. Rousseeuw, (Feb. 1997), Clustering in an Object-Oriented Environment, Journal of Statistical Software, Vol 1, Issue 4
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(dietary_survey_IBS)

dat = dietary_survey_IBS[, -ncol(dietary_survey_IBS)]

dat = center_scale(dat)

clm = Clara_Medoids(dat, clusters = 3, samples = 5, sample_size = 0.2, swap_phase = TRUE)

</code></pre>

<hr>
<h2 id='Cluster_Medoids'>Partitioning around medoids</h2><span id='topic+Cluster_Medoids'></span>

<h3>Description</h3>

<p>Partitioning around medoids
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Cluster_Medoids(
  data,
  clusters,
  distance_metric = "euclidean",
  minkowski_p = 1,
  threads = 1,
  swap_phase = TRUE,
  fuzzy = FALSE,
  verbose = FALSE,
  seed = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Cluster_Medoids_+3A_data">data</code></td>
<td>
<p>matrix or data frame. The data parameter can be also a dissimilarity matrix, where the main diagonal equals 0.0 and the number of rows equals the number of columns</p>
</td></tr>
<tr><td><code id="Cluster_Medoids_+3A_clusters">clusters</code></td>
<td>
<p>the number of clusters</p>
</td></tr>
<tr><td><code id="Cluster_Medoids_+3A_distance_metric">distance_metric</code></td>
<td>
<p>a string specifying the distance method. One of,  <em>euclidean</em>,  <em>manhattan</em>,  <em>chebyshev</em>,  <em>canberra</em>,  <em>braycurtis</em>,  <em>pearson_correlation</em>,  <em>simple_matching_coefficient</em>,  <em>minkowski</em>,  <em>hamming</em>,  <em>jaccard_coefficient</em>,  <em>Rao_coefficient</em>,  <em>mahalanobis</em>, <em>cosine</em></p>
</td></tr>
<tr><td><code id="Cluster_Medoids_+3A_minkowski_p">minkowski_p</code></td>
<td>
<p>a numeric value specifying the minkowski parameter in case that distance_metric = &quot;minkowski&quot;</p>
</td></tr>
<tr><td><code id="Cluster_Medoids_+3A_threads">threads</code></td>
<td>
<p>an integer specifying the number of cores to run in parallel</p>
</td></tr>
<tr><td><code id="Cluster_Medoids_+3A_swap_phase">swap_phase</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then both phases ('build' and 'swap') will take place. The 'swap_phase' is considered more computationally intensive.</p>
</td></tr>
<tr><td><code id="Cluster_Medoids_+3A_fuzzy">fuzzy</code></td>
<td>
<p>either TRUE or FALSE. If TRUE, then probabilities for each cluster will be returned based on the distance between observations and medoids</p>
</td></tr>
<tr><td><code id="Cluster_Medoids_+3A_verbose">verbose</code></td>
<td>
<p>either TRUE or FALSE, indicating whether progress is printed during clustering</p>
</td></tr>
<tr><td><code id="Cluster_Medoids_+3A_seed">seed</code></td>
<td>
<p>'r lifecycle::badge(&quot;deprecated&quot;)' 'seed' (integer value for random number generator (RNG)) is no longer supported and will be removed in version 1.4.0</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Due to the fact that I didn't have access to the book 'Finding Groups in Data, Kaufman and Rousseeuw, 1990' (which includes the exact algorithm) I implemented the 'Cluster_Medoids' function based on the paper 'Clustering in an Object-Oriented Environment' (see 'References').
Therefore, the 'Cluster_Medoids' function is an approximate implementation and not an exact one. Furthermore, in comparison to k-means clustering, the function 'Cluster_Medoids' is more robust, because it minimizes the sum of unsquared dissimilarities. Moreover, it doesn't need initial guesses for the cluster centers.
</p>


<h3>Value</h3>

<p>a list with the following attributes: medoids, medoid_indices, best_dissimilarity, dissimilarity_matrix, clusters, fuzzy_probs (if fuzzy = TRUE), silhouette_matrix, clustering_stats
</p>


<h3>Author(s)</h3>

<p>Lampros Mouselimis
</p>


<h3>References</h3>

<p>Anja Struyf, Mia Hubert, Peter J. Rousseeuw, (Feb. 1997), Clustering in an Object-Oriented Environment, Journal of Statistical Software, Vol 1, Issue 4
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(dietary_survey_IBS)

dat = dietary_survey_IBS[, -ncol(dietary_survey_IBS)]

dat = center_scale(dat)

cm = Cluster_Medoids(dat, clusters = 3, distance_metric = 'euclidean', swap_phase = TRUE)

</code></pre>

<hr>
<h2 id='cost_clusters_from_dissim_medoids'>Compute the cost and clusters based on an input dissimilarity matrix and medoids</h2><span id='topic+cost_clusters_from_dissim_medoids'></span>

<h3>Description</h3>

<p>Compute the cost and clusters based on an input dissimilarity matrix and medoids
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cost_clusters_from_dissim_medoids(data, medoids)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cost_clusters_from_dissim_medoids_+3A_data">data</code></td>
<td>
<p>a dissimilarity matrix, where the main diagonal equals 0.0 and the number of rows equals the number of columns</p>
</td></tr>
<tr><td><code id="cost_clusters_from_dissim_medoids_+3A_medoids">medoids</code></td>
<td>
<p>a vector of output medoids of the 'Cluster_Medoids', 'Clara_Medoids' or any other 'partition around medoids' function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list object that includes the cost and the clusters
</p>


<h3>Author(s)</h3>

<p>Lampros Mouselimis
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(dietary_survey_IBS)
dat = dietary_survey_IBS[, -ncol(dietary_survey_IBS)]
dat = center_scale(dat)

cm = Cluster_Medoids(dat, clusters = 3, distance_metric = 'euclidean', swap_phase = TRUE)
res = cost_clusters_from_dissim_medoids(data = cm$dissimilarity_matrix, medoids = cm$medoid_indices)

# cm$best_dissimilarity == res$cost
# table(cm$clusters, res$clusters)
</code></pre>

<hr>
<h2 id='dietary_survey_IBS'>
Synthetic data using a dietary survey of patients	with irritable	bowel syndrome   (IBS)
</h2><span id='topic+dietary_survey_IBS'></span>

<h3>Description</h3>

<p>The data are based on the article &quot;A dietary survey	of patients	with irritable bowel syndrome&quot;. The mean and standard deviation of the table 1 (Foods perceived as causing or worsening irritable bowel syndrome symptoms in the IBS group and digestive symptoms in the healthy comparative group) were used to generate the synthetic data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(dietary_survey_IBS)</code></pre>


<h3>Format</h3>

<p>A data frame with 400 Instances and 43 attributes (including the class attribute, &quot;class&quot;)
</p>


<h3>Details</h3>

<p>The predictors are: bread, wheat, pasta, breakfast_cereal, yeast, spicy_food, curry, chinese_takeaway, chilli, cabbage, onion, garlic, 
potatoes, pepper, vegetables_unspecified, tomato, beans_and_pulses, mushroom, fatty_foods_unspecified, sauces, chocolate,
fries, crisps, desserts, eggs, red_meat, processed_meat, pork, chicken, fish_shellfish, dairy_products_unspecified, cheese, 
cream, milk, fruit_unspecified, nuts_and_seeds, orange, apple, banana, grapes, alcohol, caffeine
</p>
<p>The response variable (&quot;class&quot;) consists of two groups: healthy-group (class == 0) vs. the IBS-patients (class == 1)
</p>


<h3>References</h3>

<p>P. Hayes, C. Corish, E. O'Mahony, E. M. M. Quigley (May 2013). A dietary survey of patients with irritable bowel syndrome. 
Journal of Human Nutrition and Dietetics.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(dietary_survey_IBS)

X = dietary_survey_IBS[, -ncol(dietary_survey_IBS)]

y = dietary_survey_IBS[, ncol(dietary_survey_IBS)]
</code></pre>

<hr>
<h2 id='distance_matrix'>Distance matrix calculation</h2><span id='topic+distance_matrix'></span>

<h3>Description</h3>

<p>Distance matrix calculation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>distance_matrix(
  data,
  method = "euclidean",
  upper = FALSE,
  diagonal = FALSE,
  minkowski_p = 1,
  threads = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="distance_matrix_+3A_data">data</code></td>
<td>
<p>matrix or data frame</p>
</td></tr>
<tr><td><code id="distance_matrix_+3A_method">method</code></td>
<td>
<p>a string specifying the distance method. One of,  <em>euclidean</em>,  <em>manhattan</em>,  <em>chebyshev</em>,  <em>canberra</em>,  <em>braycurtis</em>,  <em>pearson_correlation</em>,  <em>simple_matching_coefficient</em>,  <em>minkowski</em>,  <em>hamming</em>,  <em>jaccard_coefficient</em>,  <em>Rao_coefficient</em>,  <em>mahalanobis</em>, <em>cosine</em></p>
</td></tr>
<tr><td><code id="distance_matrix_+3A_upper">upper</code></td>
<td>
<p>either TRUE or FALSE specifying if the upper triangle of the distance matrix should be returned. If FALSE then the upper triangle will be filled with NA's</p>
</td></tr>
<tr><td><code id="distance_matrix_+3A_diagonal">diagonal</code></td>
<td>
<p>either TRUE or FALSE specifying if the diagonal of the distance matrix should be returned. If FALSE then the diagonal will be filled with NA's</p>
</td></tr>
<tr><td><code id="distance_matrix_+3A_minkowski_p">minkowski_p</code></td>
<td>
<p>a numeric value specifying the minkowski parameter in case that method = &quot;minkowski&quot;</p>
</td></tr>
<tr><td><code id="distance_matrix_+3A_threads">threads</code></td>
<td>
<p>the number of cores to run in parallel (if OpenMP is available)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(dietary_survey_IBS)

dat = dietary_survey_IBS[, -ncol(dietary_survey_IBS)]

dat = distance_matrix(dat, method = 'euclidean', upper = TRUE, diagonal = TRUE)

</code></pre>

<hr>
<h2 id='entropy_formula'>entropy formula (used in external_validation function)</h2><span id='topic+entropy_formula'></span>

<h3>Description</h3>

<p>entropy formula (used in external_validation function)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>entropy_formula(x_vec)
</code></pre>

<hr>
<h2 id='external_validation'>external clustering validation</h2><span id='topic+external_validation'></span>

<h3>Description</h3>

<p>external clustering validation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>external_validation(
  true_labels,
  clusters,
  method = "adjusted_rand_index",
  summary_stats = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="external_validation_+3A_true_labels">true_labels</code></td>
<td>
<p>a numeric vector of length equal to the length of the clusters vector</p>
</td></tr>
<tr><td><code id="external_validation_+3A_clusters">clusters</code></td>
<td>
<p>a numeric vector ( the result of a clustering method ) of length equal to the length of the true_labels</p>
</td></tr>
<tr><td><code id="external_validation_+3A_method">method</code></td>
<td>
<p>one of <em>rand_index</em>,  <em>adjusted_rand_index</em>,  <em>jaccard_index</em>,  <em>fowlkes_Mallows_index</em>,  <em>mirkin_metric</em>,  <em>purity</em>,  <em>entropy</em>,  <em>nmi</em> (normalized mutual information), <em>var_info</em> (variation of information), and <em>nvi</em> (normalized variation of information)</p>
</td></tr>
<tr><td><code id="external_validation_+3A_summary_stats">summary_stats</code></td>
<td>
<p>besides the available methods the summary_stats parameter prints also the specificity, sensitivity, precision, recall and F-measure of the clusters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses external validation methods to evaluate the clustering results
</p>


<h3>Value</h3>

<p>if summary_stats is FALSE the function returns a float number, otherwise it returns also a summary statistics table
</p>


<h3>Author(s)</h3>

<p>Lampros Mouselimis
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(dietary_survey_IBS)

dat = dietary_survey_IBS[, -ncol(dietary_survey_IBS)]

X = center_scale(dat)

km = KMeans_rcpp(X, clusters = 2, num_init = 5, max_iters = 100, initializer = 'kmeans++')

res = external_validation(dietary_survey_IBS$class, km$clusters, method = "adjusted_rand_index")

</code></pre>

<hr>
<h2 id='function_interactive'>Interactive function for consecutive plots ( using dissimilarities or the silhouette widths of the observations )</h2><span id='topic+function_interactive'></span>

<h3>Description</h3>

<p>Interactive function for consecutive plots ( using dissimilarities or the silhouette widths of the observations )
</p>


<h3>Usage</h3>

<pre><code class='language-R'>function_interactive(evaluation_objects, max_clusters, silhouette = FALSE)
</code></pre>

<hr>
<h2 id='GMM'>Gaussian Mixture Model clustering</h2><span id='topic+GMM'></span>

<h3>Description</h3>

<p>Gaussian Mixture Model clustering
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GMM(
  data,
  gaussian_comps = 1,
  dist_mode = "eucl_dist",
  seed_mode = "random_subset",
  km_iter = 10,
  em_iter = 5,
  verbose = FALSE,
  var_floor = 1e-10,
  seed = 1,
  full_covariance_matrices = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GMM_+3A_data">data</code></td>
<td>
<p>matrix or data frame</p>
</td></tr>
<tr><td><code id="GMM_+3A_gaussian_comps">gaussian_comps</code></td>
<td>
<p>the number of gaussian mixture components</p>
</td></tr>
<tr><td><code id="GMM_+3A_dist_mode">dist_mode</code></td>
<td>
<p>the distance used during the seeding of initial means and k-means clustering. One of, <em>eucl_dist</em>, <em>maha_dist</em>.</p>
</td></tr>
<tr><td><code id="GMM_+3A_seed_mode">seed_mode</code></td>
<td>
<p>how the initial means are seeded prior to running k-means and/or EM algorithms. One of, <em>static_subset</em>, <em>random_subset</em>, <em>static_spread</em>, <em>random_spread</em>.</p>
</td></tr>
<tr><td><code id="GMM_+3A_km_iter">km_iter</code></td>
<td>
<p>the number of iterations of the k-means algorithm</p>
</td></tr>
<tr><td><code id="GMM_+3A_em_iter">em_iter</code></td>
<td>
<p>the number of iterations of the EM algorithm</p>
</td></tr>
<tr><td><code id="GMM_+3A_verbose">verbose</code></td>
<td>
<p>either TRUE or FALSE; enable or disable printing of progress during the k-means and EM algorithms</p>
</td></tr>
<tr><td><code id="GMM_+3A_var_floor">var_floor</code></td>
<td>
<p>the variance floor (smallest allowed value) for the diagonal covariances</p>
</td></tr>
<tr><td><code id="GMM_+3A_seed">seed</code></td>
<td>
<p>integer value for random number generator (RNG)</p>
</td></tr>
<tr><td><code id="GMM_+3A_full_covariance_matrices">full_covariance_matrices</code></td>
<td>
<p>a boolean. If FALSE &quot;diagonal&quot; covariance matrices (i.e. in each covariance matrix, all entries outside the main diagonal are assumed to be zero) otherwise &quot;full&quot; covariance matrices will be returned. Be aware in case of &quot;full&quot; covariance matrices a cube (3-dimensional) rather than a matrix for the output &quot;covariance_matrices&quot; value will be returned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is an R implementation of the 'gmm_diag' class of the Armadillo library. The only exception is that user defined parameter settings are not supported, such as seed_mode = 'keep_existing'.
For probabilistic applications, better model parameters are typically learned with dist_mode set to maha_dist.
For vector quantisation applications, model parameters should be learned with dist_mode set to eucl_dist, and the number of EM iterations set to zero.
In general, a sufficient number of k-means and EM iterations is typically about 10.
The number of training samples should be much larger than the number of Gaussians.
Seeding the initial means with static_spread and random_spread can be much more time consuming than with static_subset and random_subset.
The k-means and EM algorithms will run faster on multi-core machines when OpenMP is enabled in your compiler (eg. -fopenmp in GCC)
</p>


<h3>Value</h3>

<p>a list consisting of the centroids, covariance matrix ( where each row of the matrix represents a diagonal covariance matrix), weights and the log-likelihoods for each gaussian component. In case of Error it returns the error message and the possible causes.
</p>


<h3>References</h3>

<p>http://arma.sourceforge.net/docs.html
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(dietary_survey_IBS)

dat = as.matrix(dietary_survey_IBS[, -ncol(dietary_survey_IBS)])

dat = center_scale(dat)

gmm = GMM(dat, 2, "maha_dist", "random_subset", 10, 10)
</code></pre>

<hr>
<h2 id='KMeans_arma'>k-means using the Armadillo library</h2><span id='topic+KMeans_arma'></span>

<h3>Description</h3>

<p>k-means using the Armadillo library
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KMeans_arma(
  data,
  clusters,
  n_iter = 10,
  seed_mode = "random_subset",
  verbose = FALSE,
  CENTROIDS = NULL,
  seed = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KMeans_arma_+3A_data">data</code></td>
<td>
<p>matrix or data frame</p>
</td></tr>
<tr><td><code id="KMeans_arma_+3A_clusters">clusters</code></td>
<td>
<p>the number of clusters</p>
</td></tr>
<tr><td><code id="KMeans_arma_+3A_n_iter">n_iter</code></td>
<td>
<p>the number of clustering iterations (about 10 is typically sufficient)</p>
</td></tr>
<tr><td><code id="KMeans_arma_+3A_seed_mode">seed_mode</code></td>
<td>
<p>how the initial centroids are seeded. One of, <em>keep_existing</em>, <em>static_subset</em>, <em>random_subset</em>, <em>static_spread</em>, <em>random_spread</em>.</p>
</td></tr>
<tr><td><code id="KMeans_arma_+3A_verbose">verbose</code></td>
<td>
<p>either TRUE or FALSE, indicating whether progress is printed during clustering</p>
</td></tr>
<tr><td><code id="KMeans_arma_+3A_centroids">CENTROIDS</code></td>
<td>
<p>a matrix of initial cluster centroids. The rows of the CENTROIDS matrix should be equal to the number of clusters and the columns should be equal to the columns of the data. CENTROIDS should be used in combination with seed_mode 'keep_existing'.</p>
</td></tr>
<tr><td><code id="KMeans_arma_+3A_seed">seed</code></td>
<td>
<p>integer value for random number generator (RNG)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is an R implementation of the 'kmeans' class of the Armadillo library.
It is faster than the KMeans_rcpp function but it lacks some features. For more info see the details section of the KMeans_rcpp function.
The number of columns should be larger than the number of clusters or CENTROIDS.
If the clustering fails, the means matrix is reset and a bool set to false is returned.
The clustering will run faster on multi-core machines when OpenMP is enabled in your compiler (eg. -fopenmp in GCC)
</p>


<h3>Value</h3>

<p>the centroids as a matrix. In case of Error it returns the error message, whereas in case of an empty centroids-matrix it returns a warning-message.
</p>


<h3>References</h3>

<p>http://arma.sourceforge.net/docs.html
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(dietary_survey_IBS)

dat = dietary_survey_IBS[, -ncol(dietary_survey_IBS)]

dat = center_scale(dat)

km = KMeans_arma(dat, clusters = 2, n_iter = 10, "random_subset")

</code></pre>

<hr>
<h2 id='KMeans_rcpp'>k-means using RcppArmadillo</h2><span id='topic+KMeans_rcpp'></span>

<h3>Description</h3>

<p>k-means using RcppArmadillo
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KMeans_rcpp(
  data,
  clusters,
  num_init = 1,
  max_iters = 100,
  initializer = "kmeans++",
  fuzzy = FALSE,
  verbose = FALSE,
  CENTROIDS = NULL,
  tol = 1e-04,
  tol_optimal_init = 0.3,
  seed = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KMeans_rcpp_+3A_data">data</code></td>
<td>
<p>matrix or data frame</p>
</td></tr>
<tr><td><code id="KMeans_rcpp_+3A_clusters">clusters</code></td>
<td>
<p>the number of clusters</p>
</td></tr>
<tr><td><code id="KMeans_rcpp_+3A_num_init">num_init</code></td>
<td>
<p>number of times the algorithm will be run with different centroid seeds</p>
</td></tr>
<tr><td><code id="KMeans_rcpp_+3A_max_iters">max_iters</code></td>
<td>
<p>the maximum number of clustering iterations</p>
</td></tr>
<tr><td><code id="KMeans_rcpp_+3A_initializer">initializer</code></td>
<td>
<p>the method of initialization. One of, <em>optimal_init</em>, <em>quantile_init</em>, <em>kmeans++</em> and <em>random</em>. See details for more information</p>
</td></tr>
<tr><td><code id="KMeans_rcpp_+3A_fuzzy">fuzzy</code></td>
<td>
<p>either TRUE or FALSE. If TRUE, then prediction probabilities will be calculated using the distance between observations and centroids</p>
</td></tr>
<tr><td><code id="KMeans_rcpp_+3A_verbose">verbose</code></td>
<td>
<p>either TRUE or FALSE, indicating whether progress is printed during clustering.</p>
</td></tr>
<tr><td><code id="KMeans_rcpp_+3A_centroids">CENTROIDS</code></td>
<td>
<p>a matrix of initial cluster centroids. The rows of the CENTROIDS matrix should be equal to the number of clusters and the columns should be equal to the columns of the data.</p>
</td></tr>
<tr><td><code id="KMeans_rcpp_+3A_tol">tol</code></td>
<td>
<p>a float number. If, in case of an iteration (iteration &gt; 1 and iteration &lt; max_iters) 'tol' is greater than the squared norm of the centroids, then kmeans has converged</p>
</td></tr>
<tr><td><code id="KMeans_rcpp_+3A_tol_optimal_init">tol_optimal_init</code></td>
<td>
<p>tolerance value for the 'optimal_init' initializer. The higher this value is, the far appart from each other the centroids are.</p>
</td></tr>
<tr><td><code id="KMeans_rcpp_+3A_seed">seed</code></td>
<td>
<p>integer value for random number generator (RNG)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function has the following features in comparison to the KMeans_arma function:
</p>
<p>Besides optimal_init, quantile_init, random and kmeans++ initilizations one can specify the centroids using the CENTROIDS parameter.
</p>
<p>The running time and convergence of the algorithm can be adjusted using the num_init, max_iters and tol parameters.
</p>
<p>If num_init &gt; 1 then KMeans_rcpp returns the attributes of the best initialization using as criterion the within-cluster-sum-of-squared-error.
</p>
<p>&mdash;&mdash;&mdash;&mdash;&mdash;initializers&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>
<p><strong>optimal_init</strong>   : this initializer adds rows of the data incrementally, while checking that they do not already exist in the centroid-matrix [ experimental ]
</p>
<p><strong>quantile_init</strong>  : initialization of centroids by using the cummulative distance between observations and by removing potential duplicates [ experimental ]
</p>
<p><strong>kmeans++</strong>       : kmeans++ initialization. Reference : http://theory.stanford.edu/~sergei/papers/kMeansPP-soda.pdf AND http://stackoverflow.com/questions/5466323/how-exactly-does-k-means-work
</p>
<p><strong>random</strong>         : random selection of data rows as initial centroids
</p>


<h3>Value</h3>

<p>a list with the following attributes: clusters, fuzzy_clusters (if fuzzy = TRUE), centroids, total_SSE, best_initialization, WCSS_per_cluster, obs_per_cluster, between.SS_DIV_total.SS
</p>


<h3>Author(s)</h3>

<p>Lampros Mouselimis
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(dietary_survey_IBS)

dat = dietary_survey_IBS[, -ncol(dietary_survey_IBS)]

dat = center_scale(dat)

km = KMeans_rcpp(dat, clusters = 2, num_init = 5, max_iters = 100, initializer = 'kmeans++')

</code></pre>

<hr>
<h2 id='MiniBatchKmeans'>Mini-batch-k-means using RcppArmadillo</h2><span id='topic+MiniBatchKmeans'></span>

<h3>Description</h3>

<p>Mini-batch-k-means using RcppArmadillo
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MiniBatchKmeans(
  data,
  clusters,
  batch_size = 10,
  num_init = 1,
  max_iters = 100,
  init_fraction = 1,
  initializer = "kmeans++",
  early_stop_iter = 10,
  verbose = FALSE,
  CENTROIDS = NULL,
  tol = 1e-04,
  tol_optimal_init = 0.3,
  seed = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MiniBatchKmeans_+3A_data">data</code></td>
<td>
<p>matrix or data frame</p>
</td></tr>
<tr><td><code id="MiniBatchKmeans_+3A_clusters">clusters</code></td>
<td>
<p>the number of clusters</p>
</td></tr>
<tr><td><code id="MiniBatchKmeans_+3A_batch_size">batch_size</code></td>
<td>
<p>the size of the mini batches</p>
</td></tr>
<tr><td><code id="MiniBatchKmeans_+3A_num_init">num_init</code></td>
<td>
<p>number of times the algorithm will be run with different centroid seeds</p>
</td></tr>
<tr><td><code id="MiniBatchKmeans_+3A_max_iters">max_iters</code></td>
<td>
<p>the maximum number of clustering iterations</p>
</td></tr>
<tr><td><code id="MiniBatchKmeans_+3A_init_fraction">init_fraction</code></td>
<td>
<p>percentage of data to use for the initialization centroids (applies if initializer is <em>kmeans++</em> or <em>optimal_init</em>). Should be a float number between 0.0 and 1.0.</p>
</td></tr>
<tr><td><code id="MiniBatchKmeans_+3A_initializer">initializer</code></td>
<td>
<p>the method of initialization. One of, <em>optimal_init</em>, <em>quantile_init</em>, <em>kmeans++</em> and <em>random</em>. See details for more information</p>
</td></tr>
<tr><td><code id="MiniBatchKmeans_+3A_early_stop_iter">early_stop_iter</code></td>
<td>
<p>continue that many iterations after calculation of the best within-cluster-sum-of-squared-error</p>
</td></tr>
<tr><td><code id="MiniBatchKmeans_+3A_verbose">verbose</code></td>
<td>
<p>either TRUE or FALSE, indicating whether progress is printed during clustering</p>
</td></tr>
<tr><td><code id="MiniBatchKmeans_+3A_centroids">CENTROIDS</code></td>
<td>
<p>a matrix of initial cluster centroids. The rows of the CENTROIDS matrix should be equal to the number of clusters and the columns should be equal to the columns of the data</p>
</td></tr>
<tr><td><code id="MiniBatchKmeans_+3A_tol">tol</code></td>
<td>
<p>a float number. If, in case of an iteration (iteration &gt; 1 and iteration &lt; max_iters) 'tol' is greater than the squared norm of the centroids, then kmeans has converged</p>
</td></tr>
<tr><td><code id="MiniBatchKmeans_+3A_tol_optimal_init">tol_optimal_init</code></td>
<td>
<p>tolerance value for the 'optimal_init' initializer. The higher this value is, the far appart from each other the centroids are.</p>
</td></tr>
<tr><td><code id="MiniBatchKmeans_+3A_seed">seed</code></td>
<td>
<p>integer value for random number generator (RNG)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function performs k-means clustering using mini batches.
</p>
<p>&mdash;&mdash;&mdash;&mdash;&mdash;initializers&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>
<p><strong>optimal_init</strong>   : this initializer adds rows of the data incrementally, while checking that they do not already exist in the centroid-matrix   [ experimental ]
</p>
<p><strong>quantile_init</strong>  : initialization of centroids by using the cummulative distance between observations and by removing potential duplicates  [ experimental ]
</p>
<p><strong>kmeans++</strong>       : kmeans++ initialization. Reference : http://theory.stanford.edu/~sergei/papers/kMeansPP-soda.pdf AND http://stackoverflow.com/questions/5466323/how-exactly-does-k-means-work
</p>
<p><strong>random</strong>         : random selection of data rows as initial centroids
</p>


<h3>Value</h3>

<p>a list with the following attributes: centroids, WCSS_per_cluster, best_initialization, iters_per_initialization
</p>


<h3>Author(s)</h3>

<p>Lampros Mouselimis
</p>


<h3>References</h3>

<p>http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf, https://github.com/siddharth-agrawal/Mini-Batch-K-Means
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(dietary_survey_IBS)

dat = dietary_survey_IBS[, -ncol(dietary_survey_IBS)]

dat = center_scale(dat)

MbatchKm = MiniBatchKmeans(dat, clusters = 2, batch_size = 20, num_init = 5, early_stop_iter = 10)

</code></pre>

<hr>
<h2 id='mushroom'>
The mushroom data
</h2><span id='topic+mushroom'></span>

<h3>Description</h3>

<p>This data set includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family (pp. 500-525). Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like 'leaflets three, let it be' for Poisonous Oak and Ivy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(mushroom)</code></pre>


<h3>Format</h3>

<p>A data frame with 8124 Instances and 23 attributes (including the class attribute, &quot;class&quot;)
</p>


<h3>Details</h3>

<p>The column names of the data (including the class) appear in the following order: 
</p>
<p>1. class: edible=e, poisonous=p
</p>
<p>2. cap-shape: bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s
</p>
<p>3. cap-surface: fibrous=f, grooves=g, scaly=y, smooth=s
</p>
<p>4. cap-color: brown=n, buff=b, cinnamon=c, gray=g, green=r, pink=p, purple=u, red=e, white=w, yellow=y
</p>
<p>5. bruises: bruises=t, no=f
</p>
<p>6. odor: almond=a, anise=l, creosote=c, fishy=y, foul=f, musty=m, none=n, pungent=p, spicy=s
</p>
<p>7. gill-attachment: attached=a, descending=d, free=f, notched=n
</p>
<p>8. gill-spacing: close=c, crowded=w, distant=d
</p>
<p>9. gill-size: broad=b, narrow=n
</p>
<p>10. gill-color: black=k, brown=n, buff=b, chocolate=h, gray=g, green=r, orange=o, pink=p, purple=u, red=e, white=w, yellow=y
</p>
<p>11. stalk-shape: enlarging=e, tapering=t
</p>
<p>12. stalk-root: bulbous=b, club=c, cup=u, equal=e, rhizomorphs=z, rooted=r, missing=?
</p>
<p>13. stalk-surface-above-ring: fibrous=f, scaly=y, silky=k, smooth=s
</p>
<p>14. stalk-surface-below-ring: fibrous=f, scaly=y, silky=k, smooth=s
</p>
<p>15. stalk-color-above-ring: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y
</p>
<p>16. stalk-color-below-ring: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y
</p>
<p>17. veil-type: partial=p, universal=u
</p>
<p>18. veil-color: brown=n, orange=o, white=w, yellow=y
</p>
<p>19. ring-number: none=n, one=o, two=t
</p>
<p>20. ring-type: cobwebby=c, evanescent=e, flaring=f, large=l, none=n, pendant=p, sheathing=s, zone=z
</p>
<p>21. spore-print-color: black=k, brown=n, buff=b, chocolate=h, green=r, orange=o, purple=u, white=w, yellow=y
</p>
<p>22. population: abundant=a, clustered=c, numerous=n, scattered=s, several=v, solitary=y
</p>
<p>23. habitat: grasses=g, leaves=l, meadows=m, paths=p, urban=u, waste=w, woods=d
</p>


<h3>References</h3>

<p>Mushroom records drawn from The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf
</p>
<p>Donor:  Jeff Schlimmer (Jeffrey.Schlimmer@a.gp.cs.cmu.edu)
</p>
<p>download source: https://archive.ics.uci.edu/ml/datasets/Mushroom
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(mushroom)

X = mushroom[, -1]

y = mushroom[, 1]
</code></pre>

<hr>
<h2 id='Optimal_Clusters_GMM'>Optimal number of Clusters for the gaussian mixture models</h2><span id='topic+Optimal_Clusters_GMM'></span>

<h3>Description</h3>

<p>Optimal number of Clusters for the gaussian mixture models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Optimal_Clusters_GMM(
  data,
  max_clusters,
  criterion = "AIC",
  dist_mode = "eucl_dist",
  seed_mode = "random_subset",
  km_iter = 10,
  em_iter = 5,
  verbose = FALSE,
  var_floor = 1e-10,
  plot_data = TRUE,
  seed = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Optimal_Clusters_GMM_+3A_data">data</code></td>
<td>
<p>matrix or data frame</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_GMM_+3A_max_clusters">max_clusters</code></td>
<td>
<p>either a numeric value, a contiguous or non-continguous numeric vector specifying the cluster search space</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_GMM_+3A_criterion">criterion</code></td>
<td>
<p>one of 'AIC' or 'BIC'</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_GMM_+3A_dist_mode">dist_mode</code></td>
<td>
<p>the distance used during the seeding of initial means and k-means clustering. One of, <em>eucl_dist</em>, <em>maha_dist</em>.</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_GMM_+3A_seed_mode">seed_mode</code></td>
<td>
<p>how the initial means are seeded prior to running k-means and/or EM algorithms. One of, <em>static_subset</em>, <em>random_subset</em>, <em>static_spread</em>, <em>random_spread</em>.</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_GMM_+3A_km_iter">km_iter</code></td>
<td>
<p>the number of iterations of the k-means algorithm</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_GMM_+3A_em_iter">em_iter</code></td>
<td>
<p>the number of iterations of the EM algorithm</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_GMM_+3A_verbose">verbose</code></td>
<td>
<p>either TRUE or FALSE; enable or disable printing of progress during the k-means and EM algorithms</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_GMM_+3A_var_floor">var_floor</code></td>
<td>
<p>the variance floor (smallest allowed value) for the diagonal covariances</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_GMM_+3A_plot_data">plot_data</code></td>
<td>
<p>either TRUE or FALSE indicating whether the results of the function should be plotted</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_GMM_+3A_seed">seed</code></td>
<td>
<p>integer value for random number generator (RNG)</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong>AIC</strong>  : the Akaike information criterion
</p>
<p><strong>BIC</strong>  : the Bayesian information criterion
</p>
<p>In case that the <em>max_clusters</em> parameter is a contiguous or non-contiguous vector then plotting is disabled. Therefore, plotting is enabled only if the <em>max_clusters</em> parameter is of length 1.
</p>


<h3>Value</h3>

<p>a vector with either the AIC or BIC for each iteration. In case of Error it returns the error message and the possible causes.
</p>


<h3>Author(s)</h3>

<p>Lampros Mouselimis
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(dietary_survey_IBS)

dat = dietary_survey_IBS[, -ncol(dietary_survey_IBS)]

dat = center_scale(dat)

opt_gmm = Optimal_Clusters_GMM(dat, 10, criterion = "AIC", plot_data = FALSE)


#----------------------------
# non-contiguous search space
#----------------------------

search_space = c(2,5)

opt_gmm = Optimal_Clusters_GMM(dat, search_space, criterion = "AIC", plot_data = FALSE)

</code></pre>

<hr>
<h2 id='Optimal_Clusters_KMeans'>Optimal number of Clusters for Kmeans or Mini-Batch-Kmeans</h2><span id='topic+Optimal_Clusters_KMeans'></span>

<h3>Description</h3>

<p>Optimal number of Clusters for Kmeans or Mini-Batch-Kmeans
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Optimal_Clusters_KMeans(
  data,
  max_clusters,
  criterion = "variance_explained",
  fK_threshold = 0.85,
  num_init = 1,
  max_iters = 200,
  initializer = "kmeans++",
  tol = 1e-04,
  plot_clusters = TRUE,
  verbose = FALSE,
  tol_optimal_init = 0.3,
  seed = 1,
  mini_batch_params = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Optimal_Clusters_KMeans_+3A_data">data</code></td>
<td>
<p>matrix or data frame</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_KMeans_+3A_max_clusters">max_clusters</code></td>
<td>
<p>either a numeric value, a contiguous or non-continguous numeric vector specifying the cluster search space</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_KMeans_+3A_criterion">criterion</code></td>
<td>
<p>one of <em>variance_explained</em>, <em>WCSSE</em>, <em>dissimilarity</em>, <em>silhouette</em>, <em>distortion_fK</em>, <em>AIC</em>, <em>BIC</em> and <em>Adjusted_Rsquared</em>. See details for more information.</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_KMeans_+3A_fk_threshold">fK_threshold</code></td>
<td>
<p>a float number used in the 'distortion_fK' criterion</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_KMeans_+3A_num_init">num_init</code></td>
<td>
<p>number of times the algorithm will be run with different centroid seeds</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_KMeans_+3A_max_iters">max_iters</code></td>
<td>
<p>the maximum number of clustering iterations</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_KMeans_+3A_initializer">initializer</code></td>
<td>
<p>the method of initialization. One of, <em>optimal_init</em>, <em>quantile_init</em>, <em>kmeans++</em> and <em>random</em>. See details for more information</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_KMeans_+3A_tol">tol</code></td>
<td>
<p>a float number. If, in case of an iteration (iteration &gt; 1 and iteration &lt; max_iters) 'tol' is greater than the squared norm of the centroids, then kmeans has converged</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_KMeans_+3A_plot_clusters">plot_clusters</code></td>
<td>
<p>either TRUE or FALSE, indicating whether the results of the <em>Optimal_Clusters_KMeans</em> function should be plotted</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_KMeans_+3A_verbose">verbose</code></td>
<td>
<p>either TRUE or FALSE, indicating whether progress is printed during clustering</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_KMeans_+3A_tol_optimal_init">tol_optimal_init</code></td>
<td>
<p>tolerance value for the 'optimal_init' initializer. The higher this value is, the far appart from each other the centroids are.</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_KMeans_+3A_seed">seed</code></td>
<td>
<p>integer value for random number generator (RNG)</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_KMeans_+3A_mini_batch_params">mini_batch_params</code></td>
<td>
<p>either NULL or a list of the following parameters : <em>batch_size</em>, <em>init_fraction</em>, <em>early_stop_iter</em>. If not NULL then the optimal number of clusters will be found based on the Mini-Batch-Kmeans. See the details and examples sections for more information.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>&mdash;&mdash;&mdash;&mdash;&mdash;criteria&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;
</p>
<p><strong>variance_explained</strong> : the sum of the within-cluster-sum-of-squares-of-all-clusters divided by the total sum of squares
</p>
<p><strong>WCSSE</strong>              : the sum of the within-cluster-sum-of-squares-of-all-clusters
</p>
<p><strong>dissimilarity</strong>      : the average intra-cluster-dissimilarity of all clusters (the distance metric defaults to euclidean)
</p>
<p><strong>silhouette</strong>         : the average silhouette width where first the average per cluster silhouette is computed and then the global average (the distance metric defaults to euclidean). To compute the silhouette width for each cluster separately see the 'silhouette_of_clusters()' function
</p>
<p><strong>distortion_fK</strong>      : this criterion is based on the following paper, 'Selection of K in K-means clustering' (https://www.ee.columbia.edu/~dpwe/papers/PhamDN05-kmeans.pdf)
</p>
<p><strong>AIC</strong>                : the Akaike information criterion
</p>
<p><strong>BIC</strong>                : the Bayesian information criterion
</p>
<p><strong>Adjusted_Rsquared</strong>  : the adjusted R^2 statistic
</p>
<p>&mdash;&mdash;&mdash;&mdash;&mdash;initializers&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-
</p>
<p><strong>optimal_init</strong>   : this initializer adds rows of the data incrementally, while checking that they do not already exist in the centroid-matrix  [ experimental ]
</p>
<p><strong>quantile_init</strong>  : initialization of centroids by using the cummulative distance between observations and by removing potential duplicates   [ experimental ]
</p>
<p><strong>kmeans++</strong>       : kmeans++ initialization. Reference : http://theory.stanford.edu/~sergei/papers/kMeansPP-soda.pdf AND http://stackoverflow.com/questions/5466323/how-exactly-does-k-means-work
</p>
<p><strong>random</strong>         : random selection of data rows as initial centroids
</p>
<p>If the <em>mini_batch_params</em> parameter is not NULL then the optimal number of clusters will be found based on the Mini-batch-Kmeans algorithm, otherwise based on the Kmeans. The higher the <em>init_fraction</em>
parameter is the more close the results between Mini-Batch-Kmeans and Kmeans will be.
</p>
<p>In case that the <em>max_clusters</em> parameter is a contiguous or non-contiguous vector then plotting is disabled. Therefore, plotting is enabled only if the <em>max_clusters</em> parameter is of length 1.
Moreover, the <em>distortion_fK</em> criterion can't be computed if the <em>max_clusters</em> parameter is a contiguous or non-continguous vector ( the <em>distortion_fK</em> criterion requires consecutive clusters ).
The same applies also to the <em>Adjusted_Rsquared</em> criterion which returns incorrect output.
</p>


<h3>Value</h3>

<p>a vector with the results for the specified criterion. If plot_clusters is TRUE then it plots also the results.
</p>


<h3>Author(s)</h3>

<p>Lampros Mouselimis
</p>


<h3>References</h3>

<p>https://www.ee.columbia.edu/~dpwe/papers/PhamDN05-kmeans.pdf
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(dietary_survey_IBS)

dat = dietary_survey_IBS[, -ncol(dietary_survey_IBS)]

dat = center_scale(dat)


#-------
# kmeans
#-------

opt_km = Optimal_Clusters_KMeans(dat, max_clusters = 10, criterion = "distortion_fK",

                                 plot_clusters = FALSE)

#------------------
# mini-batch-kmeans
#------------------


params_mbkm = list(batch_size = 10, init_fraction = 0.3, early_stop_iter = 10)

opt_mbkm = Optimal_Clusters_KMeans(dat, max_clusters = 10, criterion = "distortion_fK",

                                   plot_clusters = FALSE, mini_batch_params = params_mbkm)


#----------------------------
# non-contiguous search space
#----------------------------

search_space = c(2,5)

opt_km = Optimal_Clusters_KMeans(dat, max_clusters = search_space,

                                 criterion = "variance_explained",

                                 plot_clusters = FALSE)

</code></pre>

<hr>
<h2 id='Optimal_Clusters_Medoids'>Optimal number of Clusters for the partitioning around Medoids functions</h2><span id='topic+Optimal_Clusters_Medoids'></span>

<h3>Description</h3>

<p>Optimal number of Clusters for the partitioning around Medoids functions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Optimal_Clusters_Medoids(
  data,
  max_clusters,
  distance_metric,
  criterion = "dissimilarity",
  clara_samples = 0,
  clara_sample_size = 0,
  minkowski_p = 1,
  swap_phase = TRUE,
  threads = 1,
  verbose = FALSE,
  plot_clusters = TRUE,
  seed = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Optimal_Clusters_Medoids_+3A_data">data</code></td>
<td>
<p>matrix or data.frame. If both clara_samples and clara_sample_size equal 0, then the data parameter can be also a dissimilarity matrix, where the main diagonal equals 0.0 and the number of rows equals the number of columns</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_Medoids_+3A_max_clusters">max_clusters</code></td>
<td>
<p>either a numeric value, a contiguous or non-continguous numeric vector specifying the cluster search space</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_Medoids_+3A_distance_metric">distance_metric</code></td>
<td>
<p>a string specifying the distance method. One of,  <em>euclidean</em>,  <em>manhattan</em>,  <em>chebyshev</em>,  <em>canberra</em>,  <em>braycurtis</em>,  <em>pearson_correlation</em>,  <em>simple_matching_coefficient</em>,  <em>minkowski</em>,  <em>hamming</em>,  <em>jaccard_coefficient</em>,  <em>Rao_coefficient</em>,  <em>mahalanobis</em>, <em>cosine</em></p>
</td></tr>
<tr><td><code id="Optimal_Clusters_Medoids_+3A_criterion">criterion</code></td>
<td>
<p>one of 'dissimilarity' or 'silhouette'</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_Medoids_+3A_clara_samples">clara_samples</code></td>
<td>
<p>number of samples to draw from the data set in case of clustering large applications (clara)</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_Medoids_+3A_clara_sample_size">clara_sample_size</code></td>
<td>
<p>fraction of data to draw in each sample iteration in case of clustering large applications (clara). It should be a float number greater than 0.0 and less or equal to 1.0</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_Medoids_+3A_minkowski_p">minkowski_p</code></td>
<td>
<p>a numeric value specifying the minkowski parameter in case that distance_metric = &quot;minkowski&quot;</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_Medoids_+3A_swap_phase">swap_phase</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then both phases ('build' and 'swap') will take place. The 'swap_phase' is considered more computationally intensive.</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_Medoids_+3A_threads">threads</code></td>
<td>
<p>an integer specifying the number of cores to run in parallel. Openmp will be utilized to parallelize the number of sample draws</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_Medoids_+3A_verbose">verbose</code></td>
<td>
<p>either TRUE or FALSE, indicating whether progress is printed during clustering</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_Medoids_+3A_plot_clusters">plot_clusters</code></td>
<td>
<p>TRUE or FALSE, indicating whether the iterative results should be plotted. See the details section for more information</p>
</td></tr>
<tr><td><code id="Optimal_Clusters_Medoids_+3A_seed">seed</code></td>
<td>
<p>integer value for random number generator (RNG)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In case of plot_clusters = TRUE, the first plot will be either a plot of dissimilarities or both dissimilarities and silhouette widths giving an indication of the optimal number
of the clusters. Then, the user will be asked to give an optimal value for the number of the clusters and after that the second plot will appear with either the dissimilarities or the
silhouette widths belonging to each cluster.
</p>
<p>In case that the <em>max_clusters</em> parameter is a contiguous or non-contiguous vector then plotting is disabled. Therefore, plotting is enabled only if the <em>max_clusters</em> parameter is of length 1.
</p>


<h3>Value</h3>

<p>a list of length equal to the max_clusters parameter (the first sublist equals NULL, as dissimilarities and silhouette widths can be calculated if the number of clusters &gt; 1). If plot_clusters is TRUE then the function plots also the results.
</p>


<h3>Author(s)</h3>

<p>Lampros Mouselimis
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
data(soybean)

dat = soybean[, -ncol(soybean)]

opt_md = Optimal_Clusters_Medoids(dat, 10, 'jaccard_coefficient', plot_clusters = FALSE)


#----------------------------
# non-contiguous search space
#----------------------------

search_space = c(2,5)

opt_md = Optimal_Clusters_Medoids(dat, search_space, 'jaccard_coefficient', plot_clusters = FALSE)


## End(Not run)
</code></pre>

<hr>
<h2 id='plot_2d'>2-dimensional plots</h2><span id='topic+plot_2d'></span>

<h3>Description</h3>

<p>2-dimensional plots
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_2d(data, clusters, centroids_medoids)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_2d_+3A_data">data</code></td>
<td>
<p>a 2-dimensional matrix or data frame</p>
</td></tr>
<tr><td><code id="plot_2d_+3A_clusters">clusters</code></td>
<td>
<p>numeric vector of length equal to the number of rows of the data, which is the result of a clustering method</p>
</td></tr>
<tr><td><code id="plot_2d_+3A_centroids_medoids">centroids_medoids</code></td>
<td>
<p>a matrix of centroids or medoids. The rows of the centroids_medoids should be equal to the length of the unique values of the clusters and
the columns should be equal to the columns of the data.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function plots the clusters using 2-dimensional data and medoids or centroids.
</p>


<h3>Value</h3>

<p>a plot
</p>


<h3>Author(s)</h3>

<p>Lampros Mouselimis
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# data(dietary_survey_IBS)

# dat = dietary_survey_IBS[, -ncol(dietary_survey_IBS)]

# dat = center_scale(dat)

# pca_dat = stats::princomp(dat)$scores[, 1:2]

# km = KMeans_rcpp(pca_dat, clusters = 2, num_init = 5, max_iters = 100)

# plot_2d(pca_dat, km$clusters, km$centroids)

</code></pre>

<hr>
<h2 id='predict_GMM'>Prediction function for a Gaussian Mixture Model object</h2><span id='topic+predict_GMM'></span><span id='topic+predict.GMMCluster'></span>

<h3>Description</h3>

<p>Prediction function for a Gaussian Mixture Model object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_GMM(data, CENTROIDS, COVARIANCE, WEIGHTS)

## S3 method for class 'GMMCluster'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_GMM_+3A_data">data</code></td>
<td>
<p>matrix or data frame</p>
</td></tr>
<tr><td><code id="predict_GMM_+3A_centroids">CENTROIDS</code></td>
<td>
<p>matrix or data frame containing the centroids (means), stored as row vectors</p>
</td></tr>
<tr><td><code id="predict_GMM_+3A_covariance">COVARIANCE</code></td>
<td>
<p>matrix or data frame containing the diagonal covariance matrices, stored as row vectors</p>
</td></tr>
<tr><td><code id="predict_GMM_+3A_weights">WEIGHTS</code></td>
<td>
<p>vector containing the weights</p>
</td></tr>
<tr><td><code id="predict_GMM_+3A_object">object</code>, <code id="predict_GMM_+3A_newdata">newdata</code>, <code id="predict_GMM_+3A_...">...</code></td>
<td>
<p>arguments for the 'predict' generic</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function takes the centroids, covariance matrix and weights from a trained model and returns the log-likelihoods, cluster probabilities and cluster labels for new data.
</p>


<h3>Value</h3>

<p>a list consisting of the log-likelihoods, cluster probabilities and cluster labels.
</p>


<h3>Author(s)</h3>

<p>Lampros Mouselimis
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(dietary_survey_IBS)

dat = as.matrix(dietary_survey_IBS[, -ncol(dietary_survey_IBS)])

dat = center_scale(dat)

gmm = GMM(dat, 2, "maha_dist", "random_subset", 10, 10)

# pr = predict_GMM(dat, gmm$centroids, gmm$covariance_matrices, gmm$weights)

</code></pre>

<hr>
<h2 id='predict_KMeans'>Prediction function for the k-means</h2><span id='topic+predict_KMeans'></span><span id='topic+predict.KMeansCluster'></span>

<h3>Description</h3>

<p>Prediction function for the k-means
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_KMeans(data, CENTROIDS, threads = 1, fuzzy = FALSE)

## S3 method for class 'KMeansCluster'
predict(object, newdata, fuzzy = FALSE, threads = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_KMeans_+3A_data">data</code></td>
<td>
<p>matrix or data frame</p>
</td></tr>
<tr><td><code id="predict_KMeans_+3A_centroids">CENTROIDS</code></td>
<td>
<p>a matrix of initial cluster centroids. The rows of the CENTROIDS matrix should be equal to the number of clusters and the columns should be equal to the columns of the data.</p>
</td></tr>
<tr><td><code id="predict_KMeans_+3A_threads">threads</code></td>
<td>
<p>an integer specifying the number of cores to run in parallel</p>
</td></tr>
<tr><td><code id="predict_KMeans_+3A_fuzzy">fuzzy</code></td>
<td>
<p>either TRUE or FALSE. If TRUE, then probabilities for each cluster will be returned based on the distance between observations and centroids.</p>
</td></tr>
<tr><td><code id="predict_KMeans_+3A_object">object</code>, <code id="predict_KMeans_+3A_newdata">newdata</code>, <code id="predict_KMeans_+3A_...">...</code></td>
<td>
<p>arguments for the 'predict' generic</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function takes the data and the output centroids and returns the clusters.
</p>


<h3>Value</h3>

<p>a vector (clusters)
</p>


<h3>Author(s)</h3>

<p>Lampros Mouselimis
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(dietary_survey_IBS)

dat = dietary_survey_IBS[, -ncol(dietary_survey_IBS)]

dat = center_scale(dat)

km = KMeans_rcpp(dat, clusters = 2, num_init = 5, max_iters = 100, initializer = 'kmeans++')

pr = predict_KMeans(dat, km$centroids, threads = 1)
</code></pre>

<hr>
<h2 id='predict_MBatchKMeans'>Prediction function for Mini-Batch-k-means</h2><span id='topic+predict_MBatchKMeans'></span><span id='topic+predict.MBatchKMeans'></span>

<h3>Description</h3>

<p>Prediction function for Mini-Batch-k-means
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_MBatchKMeans(data, CENTROIDS, fuzzy = FALSE, updated_output = FALSE)

## S3 method for class 'MBatchKMeans'
predict(object, newdata, fuzzy = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_MBatchKMeans_+3A_data">data</code></td>
<td>
<p>matrix or data frame</p>
</td></tr>
<tr><td><code id="predict_MBatchKMeans_+3A_centroids">CENTROIDS</code></td>
<td>
<p>a matrix of initial cluster centroids. The rows of the CENTROIDS matrix should be equal to the number of clusters and the columns should equal the columns of the data.</p>
</td></tr>
<tr><td><code id="predict_MBatchKMeans_+3A_fuzzy">fuzzy</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then prediction probabilities will be calculated using the distance between observations and centroids.</p>
</td></tr>
<tr><td><code id="predict_MBatchKMeans_+3A_updated_output">updated_output</code></td>
<td>
<p>either TRUE or FALSE. If TRUE then the 'predict_MBatchKMeans' function will follow the same output object behaviour as the 'predict_KMeans' function (if fuzzy is TRUE it will return probabilities otherwise it will return the hard clusters). This parameter will be removed in version 1.4.0 because this will become the default output format.</p>
</td></tr>
<tr><td><code id="predict_MBatchKMeans_+3A_object">object</code>, <code id="predict_MBatchKMeans_+3A_newdata">newdata</code>, <code id="predict_MBatchKMeans_+3A_...">...</code></td>
<td>
<p>arguments for the 'predict' generic</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function takes the data and the output centroids and returns the clusters.
</p>


<h3>Value</h3>

<p>if fuzzy = TRUE the function returns a list with two attributes: a vector with the clusters and a matrix with cluster probabilities. Otherwise, it returns a vector with the clusters.
</p>


<h3>Author(s)</h3>

<p>Lampros Mouselimis
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(dietary_survey_IBS)

dat = dietary_survey_IBS[, -ncol(dietary_survey_IBS)]

dat = center_scale(dat)

MbatchKm = MiniBatchKmeans(dat, clusters = 2, batch_size = 20, num_init = 5, early_stop_iter = 10)

pr = predict_MBatchKMeans(dat, MbatchKm$centroids, fuzzy = FALSE)

</code></pre>

<hr>
<h2 id='predict_Medoids'>Predictions for the Medoid functions</h2><span id='topic+predict_Medoids'></span><span id='topic+predict.MedoidsCluster'></span>

<h3>Description</h3>

<p>Predictions for the Medoid functions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_Medoids(
  data,
  MEDOIDS = NULL,
  distance_metric = "euclidean",
  fuzzy = FALSE,
  minkowski_p = 1,
  threads = 1
)

## S3 method for class 'MedoidsCluster'
predict(object, newdata, fuzzy = FALSE, threads = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_Medoids_+3A_data">data</code></td>
<td>
<p>matrix or data frame</p>
</td></tr>
<tr><td><code id="predict_Medoids_+3A_medoids">MEDOIDS</code></td>
<td>
<p>a matrix of initial cluster medoids (data observations). The rows of the MEDOIDS matrix should be equal to the number of clusters and the columns of the MEDOIDS matrix should be equal to the columns of the data.</p>
</td></tr>
<tr><td><code id="predict_Medoids_+3A_distance_metric">distance_metric</code></td>
<td>
<p>a string specifying the distance method. One of,  <em>euclidean</em>,  <em>manhattan</em>,  <em>chebyshev</em>,  <em>canberra</em>,  <em>braycurtis</em>,  <em>pearson_correlation</em>,  <em>simple_matching_coefficient</em>,  <em>minkowski</em>,  <em>hamming</em>,  <em>jaccard_coefficient</em>,  <em>Rao_coefficient</em>,  <em>mahalanobis</em>, <em>cosine</em></p>
</td></tr>
<tr><td><code id="predict_Medoids_+3A_fuzzy">fuzzy</code></td>
<td>
<p>either TRUE or FALSE. If TRUE, then probabilities for each cluster will be returned based on the distance between observations and medoids.</p>
</td></tr>
<tr><td><code id="predict_Medoids_+3A_minkowski_p">minkowski_p</code></td>
<td>
<p>a numeric value specifying the minkowski parameter in case that distance_metric = &quot;minkowski&quot;</p>
</td></tr>
<tr><td><code id="predict_Medoids_+3A_threads">threads</code></td>
<td>
<p>an integer specifying the number of cores to run in parallel. Openmp will be utilized to parallelize the number of initializations (num_init)</p>
</td></tr>
<tr><td><code id="predict_Medoids_+3A_object">object</code>, <code id="predict_Medoids_+3A_newdata">newdata</code>, <code id="predict_Medoids_+3A_...">...</code></td>
<td>
<p>arguments for the 'predict' generic</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with the following attributes will be returned : clusters, fuzzy_clusters (if fuzzy = TRUE), dissimilarity.
</p>


<h3>Author(s)</h3>

<p>Lampros Mouselimis
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(dietary_survey_IBS)

dat = dietary_survey_IBS[, -ncol(dietary_survey_IBS)]

dat = center_scale(dat)

cm = Cluster_Medoids(dat, clusters = 3, distance_metric = 'euclidean', swap_phase = TRUE)

pm = predict_Medoids(dat, MEDOIDS = cm$medoids, 'euclidean', fuzzy = TRUE)
</code></pre>

<hr>
<h2 id='Silhouette_Dissimilarity_Plot'>Plot of silhouette widths or dissimilarities</h2><span id='topic+Silhouette_Dissimilarity_Plot'></span>

<h3>Description</h3>

<p>Plot of silhouette widths or dissimilarities
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Silhouette_Dissimilarity_Plot(evaluation_object, silhouette = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Silhouette_Dissimilarity_Plot_+3A_evaluation_object">evaluation_object</code></td>
<td>
<p>the output of either a <em>Cluster_Medoids</em> or <em>Clara_Medoids</em> function</p>
</td></tr>
<tr><td><code id="Silhouette_Dissimilarity_Plot_+3A_silhouette">silhouette</code></td>
<td>
<p>either TRUE or FALSE, indicating whether the silhouette widths or the dissimilarities should be plotted</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function takes the result-object of the <em>Cluster_Medoids</em> or <em>Clara_Medoids</em> function and depending on the argument <em>silhouette</em> it plots either the dissimilarities or
the silhouette widths of the observations belonging to each cluster.
</p>


<h3>Value</h3>

<p>TRUE if either the silhouette widths or the dissimilarities are plotted successfully, otherwise FALSE
</p>


<h3>Author(s)</h3>

<p>Lampros Mouselimis
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# data(soybean)

# dat = soybean[, -ncol(soybean)]

# cm = Cluster_Medoids(dat, clusters = 5, distance_metric = 'jaccard_coefficient')

# plt_sd = Silhouette_Dissimilarity_Plot(cm, silhouette = TRUE)

</code></pre>

<hr>
<h2 id='silhouette_of_clusters'>Silhouette width based on pre-computed clusters</h2><span id='topic+silhouette_of_clusters'></span>

<h3>Description</h3>

<p>Silhouette width based on pre-computed clusters
</p>


<h3>Usage</h3>

<pre><code class='language-R'>silhouette_of_clusters(data, clusters)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="silhouette_of_clusters_+3A_data">data</code></td>
<td>
<p>a matrix or a data frame</p>
</td></tr>
<tr><td><code id="silhouette_of_clusters_+3A_clusters">clusters</code></td>
<td>
<p>a numeric vector which corresponds to the pre-computed clusters (see the example section for more details). The size of the 'clusters' vector must be equal to the number of rows of the input data</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list object where the first sublist is the 'silhouette summary', the second sublist is the 'silhouette matrix' and the third sublist is the 'global average silhouette' (based on the silhouette values of all observations)
</p>


<h3>Author(s)</h3>

<p>Lampros Mouselimis
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(dietary_survey_IBS)
dat = dietary_survey_IBS[, -ncol(dietary_survey_IBS)]
dat = center_scale(dat)

clusters = 2

# compute k-means
km = KMeans_rcpp(dat, clusters = clusters, num_init = 5, max_iters = 100, initializer = 'kmeans++')

# compute the silhouette width
silh_km = silhouette_of_clusters(data = dat, clusters = km$clusters)

# silhouette summary
silh_summary = silh_km$silhouette_summary

# silhouette matrix (including cluster &amp; dissimilarity)
silh_mtrx = silh_km$silhouette_matrix

# global average silhouette
glob_avg = silh_km$silhouette_global_average

</code></pre>

<hr>
<h2 id='soybean'>
The soybean (large) data set from the UCI repository
</h2><span id='topic+soybean'></span>

<h3>Description</h3>

<p>There are 19 classes, only the first 15 of which have been used in prior work. The folklore seems to be that the last four classes are unjustified by the data since they have so few examples. There are 35 categorical attributes, some nominal and some ordered. The value 'dna' means does not apply. The values for attributes are encoded numerically, with the first value encoded as '0', the second as '1', and so forth. Unknown values were imputated using the mice package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(soybean)</code></pre>


<h3>Format</h3>

<p>A data frame with 307 Instances and 36 attributes (including the class attribute, &quot;class&quot;)
</p>


<h3>Details</h3>

<p>The column names of the data (including the class) appear in the following order: 
</p>
<p>date, plant-stand, precip, temp, hail, crop-hist, area-damaged, severity, seed-tmt, germination, plant-growth, 
leaves, leafspots-halo, leafspots-marg, leafspot-size, leaf-shread, leaf-malf, leaf-mild, stem, lodging, 
stem-cankers, canker-lesion, fruiting-bodies, external decay, mycelium, int-discolor, sclerotia, fruit-pods, 
fruit spots, seed, mold-growth, seed-discolor, seed-size, shriveling, roots, class
</p>


<h3>References</h3>

<p>R.S. Michalski and R.L. Chilausky, Learning by Being Told and Learning from Examples: An Experimental Comparison of the Two Methods of Knowledge Acquisition in the Context of Developing an Expert System for Soybean Disease Diagnosis, International Journal of Policy Analysis and Information Systems, Vol. 4, No. 2, 1980. 
</p>
<p>Donor: Ming Tan &amp; Jeff Schlimmer (Jeff.Schlimmer cs.cmu.edu)
</p>
<p>download source: https://archive.ics.uci.edu/ml/datasets/Soybean+(Large)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(soybean)

X = soybean[, -ncol(soybean)]

y = soybean[, ncol(soybean)]
</code></pre>

<hr>
<h2 id='tryCatch_GMM'>tryCatch function to prevent armadillo errors</h2><span id='topic+tryCatch_GMM'></span>

<h3>Description</h3>

<p>tryCatch function to prevent armadillo errors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tryCatch_GMM(
  data,
  gaussian_comps,
  dist_mode,
  seed_mode,
  km_iter,
  em_iter,
  verbose,
  var_floor,
  seed,
  full_covariance_matrices
)
</code></pre>

<hr>
<h2 id='tryCatch_KMEANS_arma'>tryCatch function to prevent armadillo errors in KMEANS_arma</h2><span id='topic+tryCatch_KMEANS_arma'></span>

<h3>Description</h3>

<p>tryCatch function to prevent armadillo errors in KMEANS_arma
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tryCatch_KMEANS_arma(
  data,
  clusters,
  n_iter,
  verbose,
  seed_mode,
  CENTROIDS,
  seed
)
</code></pre>

<hr>
<h2 id='tryCatch_optimal_clust_GMM'>tryCatch function to prevent armadillo errors in GMM_arma_AIC_BIC</h2><span id='topic+tryCatch_optimal_clust_GMM'></span>

<h3>Description</h3>

<p>tryCatch function to prevent armadillo errors in GMM_arma_AIC_BIC
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tryCatch_optimal_clust_GMM(
  data,
  max_clusters,
  dist_mode,
  seed_mode,
  km_iter,
  em_iter,
  verbose,
  var_floor,
  criterion,
  seed
)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
