<!DOCTYPE html><html><head><title>Help for package lessSEM</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {lessSEM}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#lessSEM'><p>lessSEM</p></a></li>
<li><a href='#.adaptBreakingForWls'><p>.adaptBreakingForWls</p></a></li>
<li><a href='#.addMeanStructure'><p>.addMeanStructure</p></a></li>
<li><a href='#.checkLavaanModel'><p>.checkLavaanModel</p></a></li>
<li><a href='#.checkPenalties'><p>.checkPenalties</p></a></li>
<li><a href='#.compileTransformations'><p>.compileTransformations</p></a></li>
<li><a href='#.computeInitialHessian'><p>.computeInitialHessian</p></a></li>
<li><a href='#.createMultiGroupTransformations'><p>.createMultiGroupTransformations</p></a></li>
<li><a href='#.createParameterTable'><p>.createParameterTable</p></a></li>
<li><a href='#.createRcppTransformationFunction'><p>.createRcppTransformationFunction</p></a></li>
<li><a href='#.createTransformations'><p>.createTransformations</p></a></li>
<li><a href='#.cvregsem2LavaanParameters'><p>.cvregsem2LavaanParameters</p></a></li>
<li><a href='#.cvRegularizeSEMInternal'><p>.cvRegularizeSEMInternal</p></a></li>
<li><a href='#.cvRegularizeSmoothSEMInternal'><p>.cvRegularizeSmoothSEMInternal</p></a></li>
<li><a href='#.defineDerivatives'><p>.defineDerivatives</p></a></li>
<li><a href='#.extractParametersFromSyntax'><p>.extractParametersFromSyntax</p></a></li>
<li><a href='#.extractSEMFromLavaan'><p>.extractSEMFromLavaan</p></a></li>
<li><a href='#.fit'><p>.fit</p></a></li>
<li><a href='#.fitElasticNetMix'><p>.fitElasticNetMix</p></a></li>
<li><a href='#.fitFunction'><p>.fitFunction</p></a></li>
<li><a href='#.fitMix'><p>.fitMix</p></a></li>
<li><a href='#.getGradients'><p>.getGradients</p></a></li>
<li><a href='#.getHessian'><p>.getHessian</p></a></li>
<li><a href='#.getMaxLambda_C'><p>.getMaxLambda_C</p></a></li>
<li><a href='#.getParameters'><p>.getParameters</p></a></li>
<li><a href='#.getRawData'><p>.getRawData</p></a></li>
<li><a href='#.getScores'><p>.getScores</p></a></li>
<li><a href='#.gpGetMaxLambda'><p>.gpGetMaxLambda</p></a></li>
<li><a href='#.gpOptimizationInternal'><p>.gpOptimizationInternal</p></a></li>
<li><a href='#.gradientFunction'><p>.gradientFunction</p></a></li>
<li><a href='#.initializeMultiGroupSEMForRegularization'><p>.initializeMultiGroupSEMForRegularization</p></a></li>
<li><a href='#.initializeSEMForRegularization'><p>.initializeSEMForRegularization</p></a></li>
<li><a href='#.initializeWeights'><p>.initializeWeights</p></a></li>
<li><a href='#.labelLavaanParameters'><p>.labelLavaanParameters</p></a></li>
<li><a href='#.lavaan2regsemLabels'><p>.lavaan2regsemLabels</p></a></li>
<li><a href='#.likelihoodRatioFit'><p>.likelihoodRatioFit</p></a></li>
<li><a href='#.makeSingleLine'><p>.makeSingleLine</p></a></li>
<li><a href='#.multiGroupSEMFromLavaan'><p>.multiGroupSEMFromLavaan</p></a></li>
<li><a href='#.noDotDotDot'><p>.noDotDotDot</p></a></li>
<li><a href='#.penaltyTypes'><p>.penaltyTypes</p></a></li>
<li><a href='#.reduceSyntax'><p>.reduceSyntax</p></a></li>
<li><a href='#.regularizeSEMInternal'><p>.regularizeSEMInternal</p></a></li>
<li><a href='#.regularizeSEMWithCustomPenaltyRsolnp'><p>.regularizeSEMWithCustomPenaltyRsolnp</p></a></li>
<li><a href='#.regularizeSmoothSEMInternal'><p>.regularizeSmoothSEMInternal</p></a></li>
<li><a href='#.ridgeGradient'><p>.ridgeGradient</p></a></li>
<li><a href='#.ridgeHessian'><p>.ridgeHessian</p></a></li>
<li><a href='#.ridgeValue'><p>.ridgeValue</p></a></li>
<li><a href='#.SEMdata'><p>.SEMdata</p></a></li>
<li><a href='#.SEMdataWLS'><p>.SEMdataWLS</p></a></li>
<li><a href='#.SEMFromLavaan'><p>.SEMFromLavaan</p></a></li>
<li><a href='#.setAMatrix'><p>.setAMatrix</p></a></li>
<li><a href='#.setFmatrix'><p>.setFmatrix</p></a></li>
<li><a href='#.setMVector'><p>.setMVector</p></a></li>
<li><a href='#.setParameters'><p>.setParameters</p></a></li>
<li><a href='#.setSMatrix'><p>.setSMatrix</p></a></li>
<li><a href='#.setupMulticore'><p>.setupMulticore</p></a></li>
<li><a href='#.smoothAdaptiveLASSOGradient'><p>.smoothAdaptiveLASSOGradient</p></a></li>
<li><a href='#.smoothAdaptiveLASSOHessian'><p>.smoothAdaptiveLASSOHessian</p></a></li>
<li><a href='#.smoothAdaptiveLASSOValue'><p>.smoothAdaptiveLASSOValue</p></a></li>
<li><a href='#.smoothCappedL1Value'><p>.smoothCappedL1Value</p></a></li>
<li><a href='#.smoothElasticNetGradient'><p>.smoothElasticNetGradient</p></a></li>
<li><a href='#.smoothElasticNetHessian'><p>.smoothElasticNetHessian</p></a></li>
<li><a href='#.smoothElasticNetValue'><p>.smoothElasticNetValue</p></a></li>
<li><a href='#.smoothLASSOGradient'><p>.smoothLASSOGradient</p></a></li>
<li><a href='#.smoothLASSOHessian'><p>.smoothLASSOHessian</p></a></li>
<li><a href='#.smoothLASSOValue'><p>.smoothLASSOValue</p></a></li>
<li><a href='#.smoothLspValue'><p>.smoothLspValue</p></a></li>
<li><a href='#.smoothMcpValue'><p>.smoothMcpValue</p></a></li>
<li><a href='#.smoothScadValue'><p>.smoothScadValue</p></a></li>
<li><a href='#.standardErrors'><p>.standardErrors</p></a></li>
<li><a href='#.updateLavaan'><p>.updateLavaan</p></a></li>
<li><a href='#.useElasticNet'><p>.useElasticNet</p></a></li>
<li><a href='#adaptiveLasso'><p>adaptiveLasso</p></a></li>
<li><a href='#addCappedL1'><p>addCappedL1</p></a></li>
<li><a href='#addElasticNet'><p>addElasticNet</p></a></li>
<li><a href='#addLasso'><p>addLasso</p></a></li>
<li><a href='#addLsp'><p>addLsp</p></a></li>
<li><a href='#addMcp'><p>addMcp</p></a></li>
<li><a href='#addScad'><p>addScad</p></a></li>
<li><a href='#AIC,gpRegularized-method'><p>AIC</p></a></li>
<li><a href='#AIC,Rcpp_mgSEM-method'><p>AIC</p></a></li>
<li><a href='#AIC,Rcpp_SEMCpp-method'><p>AIC</p></a></li>
<li><a href='#AIC,regularizedSEM-method'><p>AIC</p></a></li>
<li><a href='#AIC,regularizedSEMMixedPenalty-method'><p>AIC</p></a></li>
<li><a href='#AIC,regularizedSEMWithCustomPenalty-method'><p>AIC</p></a></li>
<li><a href='#bfgs'><p>bfgs</p></a></li>
<li><a href='#bfgsEnet'><p>smoothly approximated elastic net</p></a></li>
<li><a href='#bfgsEnetMgSEM'><p>smoothly approximated elastic net</p></a></li>
<li><a href='#bfgsEnetSEM'><p>smoothly approximated elastic net</p></a></li>
<li><a href='#BIC,gpRegularized-method'><p>BIC</p></a></li>
<li><a href='#BIC,Rcpp_mgSEM-method'><p>BIC</p></a></li>
<li><a href='#BIC,Rcpp_SEMCpp-method'><p>BIC</p></a></li>
<li><a href='#BIC,regularizedSEM-method'><p>BIC</p></a></li>
<li><a href='#BIC,regularizedSEMMixedPenalty-method'><p>BIC</p></a></li>
<li><a href='#BIC,regularizedSEMWithCustomPenalty-method'><p>BIC</p></a></li>
<li><a href='#callFitFunction'><p>callFitFunction</p></a></li>
<li><a href='#cappedL1'><p>cappedL1</p></a></li>
<li><a href='#coef,cvRegularizedSEM-method'><p>coef</p></a></li>
<li><a href='#coef,gpRegularized-method'><p>coef</p></a></li>
<li><a href='#coef,Rcpp_mgSEM-method'><p>coef</p></a></li>
<li><a href='#coef,Rcpp_SEMCpp-method'><p>coef</p></a></li>
<li><a href='#coef,regularizedSEM-method'><p>coef</p></a></li>
<li><a href='#coef,regularizedSEMMixedPenalty-method'><p>coef</p></a></li>
<li><a href='#coef,regularizedSEMWithCustomPenalty-method'><p>coef</p></a></li>
<li><a href='#controlBFGS'><p>controlBFGS</p></a></li>
<li><a href='#controlGlmnet'><p>controlGlmnet</p></a></li>
<li><a href='#controlIsta'><p>controlIsta</p></a></li>
<li><a href='#covariances'><p>covariances</p></a></li>
<li><a href='#createSubsets'><p>createSubsets</p></a></li>
<li><a href='#curveLambda'><p>curveLambda</p></a></li>
<li><a href='#cvAdaptiveLasso'><p>cvAdaptiveLasso</p></a></li>
<li><a href='#cvCappedL1'><p>cvCappedL1</p></a></li>
<li><a href='#cvElasticNet'><p>cvElasticNet</p></a></li>
<li><a href='#cvLasso'><p>cvLasso</p></a></li>
<li><a href='#cvLsp'><p>cvLsp</p></a></li>
<li><a href='#cvMcp'><p>cvMcp</p></a></li>
<li><a href='#cvRegularizedSEM-class'><p>Class for cross-validated regularized SEM</p></a></li>
<li><a href='#cvRidge'><p>cvRidge</p></a></li>
<li><a href='#cvRidgeBfgs'><p>cvRidgeBfgs</p></a></li>
<li><a href='#cvScad'><p>cvScad</p></a></li>
<li><a href='#cvScaler'><p>cvScaler</p></a></li>
<li><a href='#cvSmoothAdaptiveLasso'><p>cvSmoothAdaptiveLasso</p></a></li>
<li><a href='#cvSmoothElasticNet'><p>cvSmoothElasticNet</p></a></li>
<li><a href='#cvSmoothLasso'><p>cvSmoothLasso</p></a></li>
<li><a href='#elasticNet'><p>elasticNet</p></a></li>
<li><a href='#estimates'><p>S4 method to exract the estimates of an object</p></a></li>
<li><a href='#estimates,cvRegularizedSEM-method'><p>estimates</p></a></li>
<li><a href='#estimates,regularizedSEM-method'><p>estimates</p></a></li>
<li><a href='#estimates,regularizedSEMMixedPenalty-method'><p>estimates</p></a></li>
<li><a href='#fit'><p>fit</p></a></li>
<li><a href='#fitIndices'><p>S4 method to compute fit indices (e.g., AIC, BIC, ...)</p></a></li>
<li><a href='#fitIndices,cvRegularizedSEM-method'><p>fitIndices</p></a></li>
<li><a href='#fitIndices,regularizedSEM-method'><p>fitIndices</p></a></li>
<li><a href='#fitIndices,regularizedSEMMixedPenalty-method'><p>fitIndices</p></a></li>
<li><a href='#getLavaanParameters'><p>getLavaanParameters</p></a></li>
<li><a href='#getTuningParameterConfiguration'><p>getTuningParameterConfiguration</p></a></li>
<li><a href='#glmnetCappedL1MgSEM'><p>CappedL1 optimization with glmnet optimizer</p></a></li>
<li><a href='#glmnetCappedL1SEM'><p>CappedL1 optimization with glmnet optimizer</p></a></li>
<li><a href='#glmnetEnetGeneralPurpose'><p>elastic net optimization with glmnet optimizer</p></a></li>
<li><a href='#glmnetEnetGeneralPurposeCpp'><p>elastic net optimization with glmnet optimizer</p></a></li>
<li><a href='#glmnetEnetMgSEM'><p>elastic net optimization with glmnet optimizer</p></a></li>
<li><a href='#glmnetEnetSEM'><p>elastic net optimization with glmnet optimizer</p></a></li>
<li><a href='#glmnetLspMgSEM'><p>lsp optimization with glmnet optimizer</p></a></li>
<li><a href='#glmnetLspSEM'><p>lsp optimization with glmnet optimizer</p></a></li>
<li><a href='#glmnetMcpMgSEM'><p>mcp optimization with glmnet optimizer</p></a></li>
<li><a href='#glmnetMcpSEM'><p>mcp optimization with glmnet optimizer</p></a></li>
<li><a href='#glmnetMixedMgSEM'><p>mixed optimization with glmnet optimizer</p></a></li>
<li><a href='#glmnetMixedPenaltyGeneralPurpose'><p>mixed optimization with glmnet optimizer</p></a></li>
<li><a href='#glmnetMixedPenaltyGeneralPurposeCpp'><p>mixed optimization with glmnet optimizer</p></a></li>
<li><a href='#glmnetMixedSEM'><p>mixed optimization with glmnet optimizer</p></a></li>
<li><a href='#glmnetScadMgSEM'><p>scad optimization with glmnet optimizer</p></a></li>
<li><a href='#glmnetScadSEM'><p>scad optimization with glmnet optimizer</p></a></li>
<li><a href='#gpAdaptiveLasso'><p>gpAdaptiveLasso</p></a></li>
<li><a href='#gpAdaptiveLassoCpp'><p>gpAdaptiveLassoCpp</p></a></li>
<li><a href='#gpCappedL1'><p>gpCappedL1</p></a></li>
<li><a href='#gpCappedL1Cpp'><p>gpCappedL1Cpp</p></a></li>
<li><a href='#gpElasticNet'><p>gpElasticNet</p></a></li>
<li><a href='#gpElasticNetCpp'><p>gpElasticNetCpp</p></a></li>
<li><a href='#gpLasso'><p>gpLasso</p></a></li>
<li><a href='#gpLassoCpp'><p>gpLassoCpp</p></a></li>
<li><a href='#gpLsp'><p>gpLsp</p></a></li>
<li><a href='#gpLspCpp'><p>gpLspCpp</p></a></li>
<li><a href='#gpMcp'><p>gpMcp</p></a></li>
<li><a href='#gpMcpCpp'><p>gpMcpCpp</p></a></li>
<li><a href='#gpRegularized-class'><p>Class for regularized model using general purpose optimization interface</p></a></li>
<li><a href='#gpRidge'><p>gpRidge</p></a></li>
<li><a href='#gpRidgeCpp'><p>gpRidgeCpp</p></a></li>
<li><a href='#gpScad'><p>gpScad</p></a></li>
<li><a href='#gpScadCpp'><p>gpScadCpp</p></a></li>
<li><a href='#istaCappedL1mgSEM'><p>cappedL1 optimization with ista</p></a></li>
<li><a href='#istaCappedL1SEM'><p>cappedL1 optimization with ista</p></a></li>
<li><a href='#istaEnetGeneralPurpose'><p>elastic net optimization with ista</p></a></li>
<li><a href='#istaEnetGeneralPurposeCpp'><p>elastic net optimization with ista</p></a></li>
<li><a href='#istaEnetMgSEM'><p>elastic net optimization with ista optimizer</p></a></li>
<li><a href='#istaEnetSEM'><p>elastic net optimization with ista optimizer</p></a></li>
<li><a href='#istaLSPMgSEM'><p>lsp optimization with ista</p></a></li>
<li><a href='#istaLSPSEM'><p>lsp optimization with ista</p></a></li>
<li><a href='#istaMcpMgSEM'><p>mcp optimization with ista</p></a></li>
<li><a href='#istaMcpSEM'><p>mcp optimization with ista</p></a></li>
<li><a href='#istaMixedPenaltyGeneralPurpose'><p>mixed penalty optimization with ista</p></a></li>
<li><a href='#istaMixedPenaltyGeneralPurposeCpp'><p>mixed penalty optimization with ista</p></a></li>
<li><a href='#istaMixedPenaltymgSEM'><p>mixed penalty optimization with ista</p></a></li>
<li><a href='#istaMixedPenaltySEM'><p>mixed penalty optimization with ista</p></a></li>
<li><a href='#istaScadMgSEM'><p>scad optimization with ista</p></a></li>
<li><a href='#istaScadSEM'><p>scad optimization with ista</p></a></li>
<li><a href='#lasso'><p>lasso</p></a></li>
<li><a href='#lavaan2lslxLabels'><p>lavaan2lslxLabels</p></a></li>
<li><a href='#lessSEM2Lavaan'><p>lessSEM2Lavaan</p></a></li>
<li><a href='#lessSEMCoef-class'><p>Class for the coefficients estimated by lessSEM.</p></a></li>
<li><a href='#loadings'><p>loadings</p></a></li>
<li><a href='#logicalMatch'><p>logicalMatch</p></a></li>
<li><a href='#logLik,Rcpp_mgSEM-method'><p>logLik</p></a></li>
<li><a href='#logLik,Rcpp_SEMCpp-method'><p>logLik</p></a></li>
<li><a href='#logLikelihood-class'><p>Class for log-likelihood of regularized SEM. Note: we define a custom logLik -</p>
Function because the generic one is using df = number of parameters which might be confusing.</a></li>
<li><a href='#lsp'><p>lsp</p></a></li>
<li><a href='#makePtrs'><p>makePtrs</p></a></li>
<li><a href='#mcp'><p>mcp</p></a></li>
<li><a href='#mcpPenalty_C'><p>mcpPenalty_C</p></a></li>
<li><a href='#mgSEM'><p>mgSEM class</p></a></li>
<li><a href='#mixedPenalty'><p>mixedPenalty</p></a></li>
<li><a href='#modifyModel'><p>modifyModel</p></a></li>
<li><a href='#newTau'><p>newTau</p></a></li>
<li><a href='#plot,cvRegularizedSEM,missing-method'><p>plots the cross-validation fits</p></a></li>
<li><a href='#plot,gpRegularized,missing-method'><p>plots the regularized and unregularized parameters for all levels of lambda</p></a></li>
<li><a href='#plot,regularizedSEM,missing-method'><p>plots the regularized and unregularized parameters for all levels of lambda</p></a></li>
<li><a href='#plot,stabSel,missing-method'><p>plots the regularized and unregularized parameters for all levels of the tuning parameters</p></a></li>
<li><a href='#Rcpp_bfgsEnetMgSEM-class'><p>Wrapper for C++ module. See ?lessSEM::bfgsEnetMgSEM</p></a></li>
<li><a href='#Rcpp_bfgsEnetSEM-class'><p>Wrapper for C++ module. See ?lessSEM::bfgsEnetSEM</p></a></li>
<li><a href='#Rcpp_glmnetCappedL1MgSEM-class'><p>Wrapper for C++ module. See ?lessSEM::glmnetCappedL1MgSEM</p></a></li>
<li><a href='#Rcpp_glmnetCappedL1SEM-class'><p>Wrapper for C++ module. See ?lessSEM:::glmnetCappedL1SEM</p></a></li>
<li><a href='#Rcpp_glmnetEnetGeneralPurpose-class'><p>Wrapper for C++ module. See ?lessSEM::glmnetEnetGeneralPurpose</p></a></li>
<li><a href='#Rcpp_glmnetEnetGeneralPurposeCpp-class'><p>Wrapper for C++ module. See ?lessSEM::glmnetEnetGeneralPurposeCpp</p></a></li>
<li><a href='#Rcpp_glmnetEnetMgSEM-class'><p>Wrapper for C++ module. See ?lessSEM::glmnetEnetMgSEM</p></a></li>
<li><a href='#Rcpp_glmnetEnetSEM-class'><p>Wrapper for C++ module. See ?lessSEM::glmnetEnetSEM</p></a></li>
<li><a href='#Rcpp_glmnetLspMgSEM-class'><p>Wrapper for C++ module. See ?lessSEM::glmnetLspMgSEM</p></a></li>
<li><a href='#Rcpp_glmnetLspSEM-class'><p>Wrapper for C++ module. See ?lessSEM:::glmnetLspSEM</p></a></li>
<li><a href='#Rcpp_glmnetMcpMgSEM-class'><p>Wrapper for C++ module. See ?lessSEM::glmnetMcpMgSEM</p></a></li>
<li><a href='#Rcpp_glmnetMcpSEM-class'><p>Wrapper for C++ module. See ?lessSEM:::glmnetMcpSEM</p></a></li>
<li><a href='#Rcpp_glmnetScadMgSEM-class'><p>Wrapper for C++ module. See ?lessSEM::glmnetScadMgSEM</p></a></li>
<li><a href='#Rcpp_glmnetScadSEM-class'><p>Wrapper for C++ module. See ?lessSEM:::glmnetScadSEM</p></a></li>
<li><a href='#Rcpp_istaCappedL1GeneralPurpose-class'><p>Wrapper for C++ module. See ?lessSEM::istaCappedL1GeneralPurpose</p></a></li>
<li><a href='#Rcpp_istaCappedL1GeneralPurposeCpp-class'><p>Wrapper for C++ module. See ?lessSEM::istaCappedL1GeneralPurposeCpp</p></a></li>
<li><a href='#Rcpp_istaCappedL1mgSEM-class'><p>Wrapper for C++ module. See ?lessSEM::istaCappedL1MgSEM</p></a></li>
<li><a href='#Rcpp_istaCappedL1SEM-class'><p>Wrapper for C++ module. See ?lessSEM::istaCappedL1SEM</p></a></li>
<li><a href='#Rcpp_istaEnetGeneralPurpose-class'><p>Wrapper for C++ module. See ?lessSEM::istaEnetGeneralPurpose</p></a></li>
<li><a href='#Rcpp_istaEnetGeneralPurposeCpp-class'><p>Wrapper for C++ module. See ?lessSEM::istaEnetGeneralPurposeCpp</p></a></li>
<li><a href='#Rcpp_istaEnetMgSEM-class'><p>Wrapper for C++ module. See ?lessSEM::istaEnetMgSEM</p></a></li>
<li><a href='#Rcpp_istaEnetSEM-class'><p>Wrapper for C++ module. See ?lessSEM::istaEnetSEM</p></a></li>
<li><a href='#Rcpp_istaLspGeneralPurpose-class'><p>Wrapper for C++ module. See ?lessSEM::istaLspGeneralPurpose</p></a></li>
<li><a href='#Rcpp_istaLspGeneralPurposeCpp-class'><p>Wrapper for C++ module. See ?lessSEM::istaLspGeneralPurposeCpp</p></a></li>
<li><a href='#Rcpp_istaLSPMgSEM-class'><p>Wrapper for C++ module. See ?lessSEM::istaLSPMgSEM</p></a></li>
<li><a href='#Rcpp_istaLSPSEM-class'><p>Wrapper for C++ module. See ?lessSEM::istaLSPSEM</p></a></li>
<li><a href='#Rcpp_istaMcpGeneralPurpose-class'><p>Wrapper for C++ module. See ?lessSEM::istaMcpGeneralPurpose</p></a></li>
<li><a href='#Rcpp_istaMcpGeneralPurposeCpp-class'><p>Wrapper for C++ module. See ?lessSEM::istaMcpGeneralPurposeCpp</p></a></li>
<li><a href='#Rcpp_istaMcpMgSEM-class'><p>Wrapper for C++ module. See ?lessSEM::istaMcpMgSEM</p></a></li>
<li><a href='#Rcpp_istaMcpSEM-class'><p>Wrapper for C++ module. See ?lessSEM::istaMcpSEM</p></a></li>
<li><a href='#Rcpp_istaMixedPenaltymgSEM-class'><p>Wrapper for C++ module. See ?lessSEM::istaMixedPenaltymgSEM</p></a></li>
<li><a href='#Rcpp_istaMixedPenaltySEM-class'><p>Wrapper for C++ module. See ?lessSEM::istaMixedPenaltySEM</p></a></li>
<li><a href='#Rcpp_istaScadGeneralPurpose-class'><p>Wrapper for C++ module. See ?lessSEM::istaScadGeneralPurpose</p></a></li>
<li><a href='#Rcpp_istaScadGeneralPurposeCpp-class'><p>Wrapper for C++ module. See ?lessSEM::istaScadGeneralPurposeCpp</p></a></li>
<li><a href='#Rcpp_istaScadMgSEM-class'><p>Wrapper for C++ module. See ?lessSEM::istaScadMgSEM</p></a></li>
<li><a href='#Rcpp_istaScadSEM-class'><p>Wrapper for C++ module. See ?lessSEM::istaScadSEM</p></a></li>
<li><a href='#Rcpp_mgSEM-class'><p>internal representation of SEM in C++</p></a></li>
<li><a href='#Rcpp_SEMCpp-class'><p>internal representation of SEM in C++</p></a></li>
<li><a href='#regressions'><p>regressions</p></a></li>
<li><a href='#regsem2LavaanParameters'><p>regsem2LavaanParameters</p></a></li>
<li><a href='#regularizedSEM-class'><p>Class for regularized SEM</p></a></li>
<li><a href='#regularizedSEMMixedPenalty-class'><p>Class for regularized SEM</p></a></li>
<li><a href='#regularizedSEMWithCustomPenalty-class'><p>Class for regularized SEM using Rsolnp</p></a></li>
<li><a href='#ridge'><p>ridge</p></a></li>
<li><a href='#ridgeBfgs'><p>ridgeBfgs</p></a></li>
<li><a href='#scad'><p>scad</p></a></li>
<li><a href='#scadPenalty_C'><p>scadPenalty_C</p></a></li>
<li><a href='#SEMCpp'><p>SEMCpp class</p></a></li>
<li><a href='#show,cvRegularizedSEM-method'><p>Show method for objects of class <code>cvRegularizedSEM</code>.</p></a></li>
<li><a href='#show,gpRegularized-method'><p>show</p></a></li>
<li><a href='#show,lessSEMCoef-method'><p>show</p></a></li>
<li><a href='#show,logLikelihood-method'><p>show</p></a></li>
<li><a href='#show,Rcpp_mgSEM-method'><p>show</p></a></li>
<li><a href='#show,Rcpp_SEMCpp-method'><p>show</p></a></li>
<li><a href='#show,regularizedSEM-method'><p>show</p></a></li>
<li><a href='#show,regularizedSEMMixedPenalty-method'><p>show</p></a></li>
<li><a href='#show,stabSel-method'><p>show</p></a></li>
<li><a href='#simulateExampleData'><p>simulateExampleData</p></a></li>
<li><a href='#smoothAdaptiveLasso'><p>smoothAdaptiveLasso</p></a></li>
<li><a href='#smoothElasticNet'><p>smoothElasticNet</p></a></li>
<li><a href='#smoothLasso'><p>smoothLasso</p></a></li>
<li><a href='#stabilitySelection'><p>stabilitySelection</p></a></li>
<li><a href='#stabSel-class'><p>Class for stability selection</p></a></li>
<li><a href='#summary,cvRegularizedSEM-method'><p>summary method for objects of class <code>cvRegularizedSEM</code>.</p></a></li>
<li><a href='#summary,gpRegularized-method'><p>summary</p></a></li>
<li><a href='#summary,regularizedSEM-method'><p>summary</p></a></li>
<li><a href='#summary,regularizedSEMMixedPenalty-method'><p>summary</p></a></li>
<li><a href='#summary,regularizedSEMWithCustomPenalty-method'><p>summary</p></a></li>
<li><a href='#variances'><p>variances</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Non-Smooth Regularization for Structural Equation Models</td>
</tr>
<tr>
<td>Version:</td>
<td>1.5.5</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jannik H. Orzek &lt;jannik.orzek@mailbox.org&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides regularized structural equation modeling 
  (regularized SEM) with non-smooth penalty functions (e.g., lasso) building 
  on 'lavaan'. The package is heavily inspired by the 
  ['regsem'](<a href="https://github.com/Rjacobucci/regsem">https://github.com/Rjacobucci/regsem</a>) and 
  ['lslx'](<a href="https://github.com/psyphh/lslx">https://github.com/psyphh/lslx</a>) packages.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Depends:</td>
<td>lavaan, methods</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 1.0.8), RcppArmadillo, RcppParallel, ggplot2, tidyr,
stringr, numDeriv, utils, stats, graphics, rlang, mvtnorm</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, plotly, rmarkdown, Rsolnp</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo, RcppParallel</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>GNU make, C++17</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/jhorzek/lessSEM">https://github.com/jhorzek/lessSEM</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/jhorzek/lessSEM/issues">https://github.com/jhorzek/lessSEM/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-21 10:06:17 UTC; jannik</td>
</tr>
<tr>
<td>Author:</td>
<td>Jannik H. Orzek <a href="https://orcid.org/0000-0002-3123-2248"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre, cph]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-22 13:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='lessSEM'>lessSEM</h2><span id='topic+lessSEM'></span><span id='topic+lessSEM-package'></span>

<h3>Description</h3>

<p><strong>Please see the vignettes and the readme on <a href="https://jhorzek.github.io/lessSEM/">GitHub</a>
for the most up to date description of the package</strong>
</p>


<h3>Details</h3>

<p><strong>lessSEM</strong> (<strong>l</strong>essSEM <strong>es</strong>timates <strong>s</strong>parse <strong>SEM</strong>) is an R package for
regularized structural equation modeling (regularized SEM) with non-smooth
penalty functions (e.g., lasso) building on <a href="https://github.com/yrosseel/lavaan"><strong>lavaan</strong></a>.
<strong>lessSEM</strong> is heavily inspired by the <a href="https://github.com/Rjacobucci/regsem"><strong>regsem</strong></a> package
and the <a href="https://github.com/psyphh/lslx"><strong>lslx</strong></a> packages that have similar functionality.
<strong>If you use lessSEM, please also cite <a href="https://github.com/Rjacobucci/regsem">regsem</a> and
and <a href="https://github.com/psyphh/lslx">lslx</a>!</strong>
</p>
<p>The objectives of <strong>lessSEM</strong> are to provide ...
</p>

<ol>
<li><p> a flexible framework for regularizing SEM and
</p>
</li>
<li><p> optimizers for other SEM packages which can be used with an interface
similar to <code>optim</code>.
</p>
</li></ol>

<p><strong>Important</strong>: Please also check out the implementations of regularized SEM in the more
mature R packages <a href="https://github.com/Rjacobucci/regsem"><strong>regsem</strong></a> and <a href="https://github.com/psyphh/lslx"><strong>lslx</strong></a>.
Finally, you may want to check out the julia package
<a href="https://github.com/StructuralEquationModels/StructuralEquationModels.jl"><strong>StructuralEquationModels.jl</strong></a>.
</p>


<h4><a href="https://github.com/Rjacobucci/regsem"><strong>regsem</strong></a>, <a href="https://github.com/psyphh/lslx"><strong>lslx</strong></a>, and <strong>lessSEM</strong></h4>

<p>The packages <a href="https://github.com/Rjacobucci/regsem"><strong>regsem</strong></a>,
<a href="https://github.com/psyphh/lslx"><strong>lslx</strong></a>, and <strong>lessSEM</strong> can all be used to
regularize basic SEM. In fact, as outlined above, <strong>lessSEM</strong> is heavily inspired
by <a href="https://github.com/Rjacobucci/regsem"><strong>regsem</strong></a>
and <a href="https://github.com/psyphh/lslx"><strong>lslx</strong></a>. However, the packages differ in their targets: The objective
of <strong>lessSEM</strong> is not to replace the more major packages <a href="https://github.com/Rjacobucci/regsem"><strong>regsem</strong></a>
and <a href="https://github.com/psyphh/lslx"><strong>lslx</strong></a>. Instead, our objective is to
provide method developers with a flexible framework for regularized SEM.
The following shows an incomplete comparison of some features implemented in
the three packages:</p>

<table>
<tr>
 <td style="text-align: left;">
    </td><td style="text-align: left;"> <strong>regsem</strong> </td><td style="text-align: left;"> <strong>lslx</strong> </td><td style="text-align: left;"> <strong>lessSEM</strong> </td>
</tr>
<tr>
 <td style="text-align: left;">
   Model specification </td><td style="text-align: left;"> based on lavaan </td><td style="text-align: left;"> similar to lavaan </td><td style="text-align: left;"> based on lavaan </td>
</tr>
<tr>
 <td style="text-align: left;">
   Maximum likelihood estimation </td><td style="text-align: left;"> Yes </td><td style="text-align: left;"> Yes </td><td style="text-align: left;"> Yes </td>
</tr>
<tr>
 <td style="text-align: left;">
   Least squares estimation </td><td style="text-align: left;"> No </td><td style="text-align: left;"> Yes </td><td style="text-align: left;"> No </td>
</tr>
<tr>
 <td style="text-align: left;">
   Confidence Intervals </td><td style="text-align: left;"> No </td><td style="text-align: left;"> Yes </td><td style="text-align: left;"> No </td>
</tr>
<tr>
 <td style="text-align: left;">
   Missing Data </td><td style="text-align: left;"> FIML </td><td style="text-align: left;"> Auxiliary Variables </td><td style="text-align: left;"> FIML </td>
</tr>
<tr>
 <td style="text-align: left;">
   Multi-group models </td><td style="text-align: left;"> No </td><td style="text-align: left;"> Yes </td><td style="text-align: left;"> Yes </td>
</tr>
<tr>
 <td style="text-align: left;">
   Stability selection </td><td style="text-align: left;"> Yes </td><td style="text-align: left;"> No </td><td style="text-align: left;"> No </td>
</tr>
<tr>
 <td style="text-align: left;">
   Mixed penalties </td><td style="text-align: left;"> No </td><td style="text-align: left;"> No </td><td style="text-align: left;"> Yes </td>
</tr>
<tr>
 <td style="text-align: left;">
   Equality constraints </td><td style="text-align: left;"> Yes </td><td style="text-align: left;"> No </td><td style="text-align: left;"> Yes </td>
</tr>
<tr>
 <td style="text-align: left;">
   Parameter transformations </td><td style="text-align: left;"> diff_lasso </td><td style="text-align: left;"> No </td><td style="text-align: left;"> Yes </td>
</tr>
<tr>
 <td style="text-align: left;">
   Definition variables </td><td style="text-align: left;"> No </td><td style="text-align: left;"> No </td><td style="text-align: left;"> Yes </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Because <strong>lessSEM</strong> is fairly new, we currently recommend using <strong>lslx</strong> for cases
that are covered by both, <strong>lessSEM</strong> and <strong>lslx</strong>.
</p>



<h3>Introduction</h3>

<p>You will find a short introduction to regularized SEM with the <strong>lessSEM</strong>
package in <code>vignette('lessSEM', package = 'lessSEM')</code>. More information is also
provided in the documentation of the individual functions (e.g., see <code>?lessSEM::scad</code>).
Finally, you will find templates for a selection of models which can be used with <strong>lessSEM</strong>
(e.g., the cross-lagged panel model) in the package <a href="https://github.com/jhorzek/lessTemplates"><strong>lessTemplates</strong></a>.
</p>


<h3>Example</h3>

<div class="sourceCode"><pre>library(lessSEM)
library(lavaan)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
      f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 + 
           l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 + 
           l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
      f ~~ 1*f
      "

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

lsem &lt;- lasso(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = c("l6", "l7", "l8", "l9", "l10",
                  "l11", "l12", "l13", "l14", "l15"),
  # in case of lasso and adaptive lasso, we can specify the number of lambda
  # values to use. lessSEM will automatically find lambda_max and fit
  # models for nLambda values between 0 and lambda_max. For the other
  # penalty functions, lambdas must be specified explicitly
  nLambdas = 50)

# use the plot-function to plot the regularized parameters:
plot(lsem)

# use the coef-function to show the estimates
coef(lsem)

# The best parameters can be extracted with:
coef(lsem, criterion = "AIC")
coef(lsem, criterion = "BIC")

# elements of lsem can be accessed with the @ operator:
lsem@parameters[1,]

# AIC and BIC for all tuning parameter configurations:
AIC(lsem)
BIC(lsem)

# cross-validation
cv &lt;- cvLasso(lavaanModel = lavaanModel,
              regularized = c("l6", "l7", "l8", "l9", "l10",
                              "l11", "l12", "l13", "l14", "l15"),
              lambdas = seq(0,1,.1),
              standardize = TRUE)

# get best model according to cross-validation:
coef(cv)

#### Advanced ###
# Switching the optimizer # 
# Use the "method" argument to switch the optimizer. The control argument
# must also be changed to the corresponding function:
lsemIsta &lt;- lasso(
  lavaanModel = lavaanModel,
  regularized = paste0("l", 6:15),
  nLambdas = 50,
  method = "ista",
  control = controlIsta(
    # Here, we can also specify that we want to use multiple cores:
    nCores = 2))

# Note: The results are basically identical:
lsemIsta@parameters - lsem@parameters
</pre></div>


<h3>Transformations</h3>

<p><strong>lessSEM</strong> allows for parameter transformations which could, for instance, be used to test
measurement invariance in longitudinal models (e.g., Liang, 2018; Bauer et al., 2020).
A thorough introduction is provided in <code>vignette('Parameter-transformations', package = 'lessSEM')</code>.
As an example, we will test measurement invariance in the <code>PoliticalDemocracy</code>
data set.
</p>
<div class="sourceCode"><pre>library(lessSEM)
library(lavaan)
# we will use the PoliticalDemocracy from lavaan (see ?lavaan::sem)
model &lt;- ' 
  # latent variable definitions
     ind60 =~ x1 + x2 + x3
     # assuming different loadings for different time points:
     dem60 =~ y1 + a1*y2 + b1*y3 + c1*y4
     dem65 =~ y5 + a2*y6 + b2*y7 + c2*y8

  # regressions
    dem60 ~ ind60
    dem65 ~ ind60 + dem60

  # residual correlations
    y1 ~~ y5
    y2 ~~ y4 + y6
    y3 ~~ y7
    y4 ~~ y8
    y6 ~~ y8
'

fit &lt;- sem(model, data = PoliticalDemocracy)

# We will define a transformation which regularizes differences
# between loadings over time:

transformations &lt;- "
// which parameters do we want to use?
parameters: a1, a2, b1, b2, c1, c2, delta_a2, delta_b2, delta_c2

// transformations:
a2 = a1 + delta_a2;
b2 = b1 + delta_b2;
c2 = c1 + delta_c2;
"

# setting delta_a2, delta_b2, or delta_c2 to zero implies measurement invariance
# for the respective parameters (a1, b1, c1)
lassoFit &lt;- lasso(lavaanModel = fit, 
                  # we want to regularize the differences between the parameters
                  regularized = c("delta_a2", "delta_b2", "delta_c2"),
                  nLambdas = 100,
                  # Our model modification must make use of the modifyModel - function:
                  modifyModel = modifyModel(transformations = transformations)
)
</pre></div>
<p>Finally, we can extract the best parameters:
</p>
<div class="sourceCode"><pre>coef(lassoFit, criterion = "BIC")
</pre></div>
<p>As all differences (<code>delta_a2</code>, <code>delta_b2</code>, and <code>delta_c2</code>) have been zeroed, we can
assume measurement invariance.
</p>


<h3>Experimental Features</h3>

<p>The following features are relatively new and you may still experience some bugs.
Please be aware of that when using these features.
</p>


<h4>From <strong>lessSEM</strong> to <strong>lavaan</strong></h4>

<p><strong>lessSEM</strong> supports exporting specific models to <strong>lavaan</strong>. This can be very useful when plotting the
final model. In our case, the best model is given by:
</p>
<div class="sourceCode"><pre>lambdaBest &lt;- coef(lsem, criterion = "BIC")@tuningParameters$lambda 
</pre></div>
<p>We can get the <strong>lavaan</strong> model with the parameters corresponding to those of the
regularized model with <code>lambda = lambdaBest</code> as follows:
</p>
<div class="sourceCode"><pre>lavaanModel &lt;- lessSEM2Lavaan(regularizedSEM = lsem, 
                              lambda = lambdaBest)
</pre></div>
<p>The result can be plotted with, for instance, <a href="https://github.com/SachaEpskamp/semPlot"><strong>semPlot</strong></a>:
</p>
<div class="sourceCode"><pre>library(semPlot)
semPaths(lavaanModel,
         what = "est",
         fade = FALSE)
</pre></div>



<h4>Multi-Group Models and Definition Variables</h4>

<p><strong>lessSEM</strong> supports multi-group SEM and, to some degree, definition variables.
Regularized multi-group SEM have been proposed by Huang (2018) and are
implemented in <strong>lslx</strong> (Huang, 2020). Here, differences between groups are regularized.
A detailed introduction can be found in
<code>vignette(topic = "Definition-Variables-and-Multi-Group-SEM", package = "lessSEM")</code>.
Therein it is also explained how the multi-group SEM can be used to implement
definition variables (e.g., for latent growth curve models).
</p>



<h4>Mixed Penalties</h4>

<p><strong>lessSEM</strong> allows for defining different penalties for different parts
of the model. This feature is new and very experimental. Please keep that
in mind when using the procedure. A detailed introduction
can be found in <code>vignette(topic = "Mixed-Penalties", package = "lessSEM")</code>.
</p>
<p>To provide a short example, we will regularize the loadings and the regression
parameters of the Political Democracy data set with different penalties. The
following script is adapted from <code>?lavaan::sem</code>.
</p>
<div class="sourceCode"><pre>model &lt;- ' 
  # latent variable definitions
     ind60 =~ x1 + x2 + x3 + c2*y2 + c3*y3 + c4*y4
     dem60 =~ y1 + y2 + y3 + y4
     dem65 =~ y5 + y6 + y7 + c*y8

  # regressions
    dem60 ~ r1*ind60
    dem65 ~ r2*ind60 + r3*dem60
'

lavaanModel &lt;- sem(model,
                   data = PoliticalDemocracy)

# Let's add a lasso penalty on the cross-loadings c2 - c4 and 
# scad penalty on the regressions r1-r3
fitMp &lt;- lavaanModel |&gt;
  mixedPenalty() |&gt;
  addLasso(regularized = c("c2", "c3", "c4"), 
           lambdas = seq(0,1,.1)) |&gt;
  addScad(regularized = c("r1", "r2", "r3"), 
          lambdas = seq(0,1,.2),
          thetas = 3.7) |&gt;
  fit()
</pre></div>
<p>The best model according to the BIC can be extracted with:
</p>
<div class="sourceCode"><pre>coef(fitMp, criterion = "BIC")
</pre></div>



<h3>Optimizers</h3>

<p>Currently, <strong>lessSEM</strong> has the following optimizers:
</p>

<ul>
<li><p> (variants of) iterative shrinkage and thresholding (e.g., Beck &amp; Teboulle, 2009;
Gong et al., 2013; Parikh &amp; Boyd, 2013); optimization of cappedL1, lsp, scad, and mcp is based on Gong et al. (2013)
</p>
</li>
<li><p> glmnet (Friedman et al., 2010; Yuan et al., 2012; Huang, 2020)
</p>
</li></ul>

<p>These optimizers are implemented based on the
<a href="https://github.com/jhorzek/regCtsem"><strong>regCtsem</strong></a> package. Most importantly,
<strong>all optimizers in lessSEM are available for other packages.</strong>
There are three ways to implement them which are documented in
<code>vignette("General-Purpose-Optimization", package = "lessSEM")</code>.
In short, these are:
</p>

<ol>
<li><p> using the R interface: All general purpose implementations of the functions
are called with prefix &quot;gp&quot; (<code>gpLasso</code>, <code>gpScad</code>, ...). More information and
examples can be found in the documentation of these functions (e.g., <code>?lessSEM::gpLasso</code>,
<code>?lessSEM::gpAdaptiveLasso</code>, <code>?lessSEM::gpElasticNet</code>). The interface is similar to
the optim optimizers in R.
</p>
</li>
<li><p> using Rcpp, we can pass C++ function pointers to the general purpose optimizers
<code>gpLassoCpp</code>, <code>gpScadCpp</code>, ... (e.g., <code>?lessSEM::gpLassoCpp</code>)
</p>
</li>
<li><p> All optimizers are implemented as C++ header-only files in <strong>lessSEM</strong>. Thus,
they can be accessed from other packages using C++. The interface is similar
to that of the <a href="https://ensmallen.org/"><strong>ensmallen</strong></a> library. We have implemented
a simple example for elastic net regularization of linear regressions in the
<a href="https://github.com/jhorzek/lessLM"><strong>lessLM</strong></a> package. You can also find more
details on the general design of the optimizer interface in <code>vignette("The-optimizer-interface", package = "lessSEM")</code>.
</p>
</li></ol>



<h3>References</h3>



<h4>R - Packages / Software</h4>


<ul>
<li> <p><a href="https://github.com/yrosseel/lavaan">lavaan</a> Rosseel, Y. (2012). lavaan: An R Package for Structural Equation Modeling. Journal of Statistical Software, 48(2), 1-36. https://doi.org/10.18637/jss.v048.i02
</p>
</li>
<li> <p><a href="https://github.com/Rjacobucci/regsem">regsem</a>: Jacobucci, R. (2017). regsem:
Regularized Structural Equation Modeling. ArXiv:1703.08489 <a href="ggplot2.html#topic+Stat">Stat</a>. https://arxiv.org/abs/1703.08489
</p>
</li>
<li> <p><a href="https://github.com/psyphh/lslx">lslx</a>: Huang, P.-H. (2020). lslx:
Semi-confirmatory structural equation modeling via penalized likelihood. Journal
of Statistical Software, 93(7). https://doi.org/10.18637/jss.v093.i07
</p>
</li>
<li> <p><a href="https://CRAN.R-project.org/package=fasta">fasta</a>:
Another implementation of the fista algorithm (Beck &amp; Teboulle, 2009).
</p>
</li>
<li> <p><a href="https://ensmallen.org/">ensmallen</a>: Curtin, R. R., Edel, M., Prabhu, R. G.,
Basak, S., Lou, Z., &amp; Sanderson, C. (2021). The ensmallen library for ﬂexible
numerical optimization. Journal of Machine Learning Research, 22, 1–6.
</p>
</li>
<li> <p><a href="https://github.com/jhorzek/regCtsem">regCtsem</a>: Orzek, J. H., &amp; Voelkle, M. C. (in press).
Regularized continuous time structural equation models: A network perspective. Psychological Methods.
</p>
</li></ul>




<h4>Regularized Structural Equation Modeling</h4>


<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method
for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Huang, P.-H. (2018). A penalized likelihood method for multi-group structural equation modelling. British Journal of Mathematical and Statistical Psychology, 71(3), 499–522. https://doi.org/10.1111/bmsp.12130
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural
Equation Modeling. Structural Equation Modeling: A Multidisciplinary Journal, 23(4),
555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>




<h4>Penalty Functions</h4>


<ul>
<li><p> Candès, E. J., Wakin, M. B., &amp; Boyd, S. P. (2008). Enhancing Sparsity by
Reweighted l1 Minimization. Journal of Fourier Analysis and Applications, 14(5–6),
877–905. https://doi.org/10.1007/s00041-008-9045-x
</p>
</li>
<li><p> Fan, J., &amp; Li, R. (2001). Variable selection via nonconcave penalized
likelihood and its oracle properties. Journal of the American Statistical
Association, 96(456), 1348–1360. https://doi.org/10.1198/016214501753382273
</p>
</li>
<li><p> Hoerl, A. E., &amp; Kennard, R. W. (1970). Ridge Regression: Biased Estimation
for Nonorthogonal Problems. Technometrics, 12(1), 55–67. https://doi.org/10.1080/00401706.1970.10488634
</p>
</li>
<li><p> Tibshirani, R. (1996). Regression shrinkage and selection via the lasso.
Journal of the Royal Statistical Society. Series B (Methodological), 58(1), 267–288.
</p>
</li>
<li><p> Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty.
The Annals of Statistics, 38(2), 894–942. https://doi.org/10.1214/09-AOS729
</p>
</li>
<li><p> Zhang, T. (2010). Analysis of Multi-stage Convex Relaxation for Sparse Regularization.
Journal of Machine Learning Research, 11, 1081–1107.
</p>
</li>
<li><p> Zou, H. (2006). The adaptive lasso and its oracle properties. Journal of the
American Statistical Association, 101(476), 1418–1429. https://doi.org/10.1198/016214506000000735
</p>
</li>
<li><p> Zou, H., &amp; Hastie, T. (2005). Regularization and variable selection via the
elastic net. Journal of the Royal Statistical Society: Series B, 67(2), 301–320.
https://doi.org/10.1111/j.1467-9868.2005.00503.x
</p>
</li></ul>




<h4>Optimizer</h4>



<h5>GLMNET</h5>


<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010). Regularization paths for
generalized linear models via coordinate descent. Journal of Statistical
Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012). An improved GLMNET for
l1-regularized logistic regression. The Journal of Machine Learning Research,
13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>




<h5>Variants of ISTA</h5>


<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013). A general iterative
shrinkage and thresholding algorithm for non-convex regularized optimization problems.
Proceedings of the 30th International Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>





<h4>Miscellaneous</h4>


<ul>
<li><p> Liang, X., Yang, Y., &amp; Huang, J. (2018). Evaluation of structural relationships in autoregressive cross-lagged models under longitudinal approximate invariance: A Bayesian analysis. Structural Equation Modeling: A Multidisciplinary Journal, 25(4), 558–572. https://doi.org/10.1080/10705511.2017.1410706
</p>
</li>
<li><p> Bauer, D. J., Belzak, W. C. M., &amp; Cole, V. T. (2020). Simplifying the Assessment of Measurement Invariance over Multiple Background Variables: Using Regularized Moderated Nonlinear Factor Analysis to Detect Differential Item Functioning. Structural Equation Modeling: A Multidisciplinary Journal, 27(1), 43–55. https://doi.org/10.1080/10705511.2019.1642754
</p>
</li></ul>




<h3>Important Notes</h3>

<p>THE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,
INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
</p>


<h3>Author(s)</h3>

<p>Jannik Orzek <a href="mailto:orzek@mpib-berlin.mpg.de">orzek@mpib-berlin.mpg.de</a>
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/jhorzek/lessSEM">https://github.com/jhorzek/lessSEM</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/jhorzek/lessSEM/issues">https://github.com/jhorzek/lessSEM/issues</a>
</p>
</li></ul>


<hr>
<h2 id='.adaptBreakingForWls'>.adaptBreakingForWls</h2><span id='topic+.adaptBreakingForWls'></span>

<h3>Description</h3>

<p>wls needs smaller breaking points than ml
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.adaptBreakingForWls(lavaanModel, currentBreaking, selectedDefault)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".adaptBreakingForWls_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>single model or vector of models</p>
</td></tr>
<tr><td><code id=".adaptBreakingForWls_+3A_currentbreaking">currentBreaking</code></td>
<td>
<p>current breaking condition value</p>
</td></tr>
<tr><td><code id=".adaptBreakingForWls_+3A_selecteddefault">selectedDefault</code></td>
<td>
<p>was default breaking condition selected?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>updated breaking
</p>

<hr>
<h2 id='.addMeanStructure'>.addMeanStructure</h2><span id='topic+.addMeanStructure'></span>

<h3>Description</h3>

<p>adds a mean strucuture to the parameter table
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.addMeanStructure(parameterTable, manifestNames, MvectorElements)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".addMeanStructure_+3A_parametertable">parameterTable</code></td>
<td>
<p>table with parameters</p>
</td></tr>
<tr><td><code id=".addMeanStructure_+3A_manifestnames">manifestNames</code></td>
<td>
<p>names of manifest variables</p>
</td></tr>
<tr><td><code id=".addMeanStructure_+3A_mvectorelements">MvectorElements</code></td>
<td>
<p>elements of the means vector</p>
</td></tr>
</table>


<h3>Value</h3>

<p>parameterTable
</p>

<hr>
<h2 id='.checkLavaanModel'>.checkLavaanModel</h2><span id='topic+.checkLavaanModel'></span>

<h3>Description</h3>

<p>checks model of type lavaan
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.checkLavaanModel(lavaanModel)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".checkLavaanModel_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>m0del of type lavaan</p>
</td></tr>
</table>


<h3>Value</h3>

<p>nothing
</p>

<hr>
<h2 id='.checkPenalties'>.checkPenalties</h2><span id='topic+.checkPenalties'></span>

<h3>Description</h3>

<p>Internal function to check a mixedPenalty object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.checkPenalties(mixedPenalty)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".checkPenalties_+3A_mixedpenalty">mixedPenalty</code></td>
<td>
<p>object of class mixedPenalty. This object can be created
with the mixedPenalty function. Penalties can be added with the addCappedL1,
addLasso, addLsp, addMcp, and addScad functions.</p>
</td></tr>
</table>

<hr>
<h2 id='.compileTransformations'>.compileTransformations</h2><span id='topic+.compileTransformations'></span>

<h3>Description</h3>

<p>compile user defined parameter transformations to a
pass to a SEM
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.compileTransformations(syntax, parameterLabels, compile = TRUE, notes = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".compileTransformations_+3A_syntax">syntax</code></td>
<td>
<p>string with user defined transformations</p>
</td></tr>
<tr><td><code id=".compileTransformations_+3A_parameterlabels">parameterLabels</code></td>
<td>
<p>names of parameters in the model</p>
</td></tr>
<tr><td><code id=".compileTransformations_+3A_compile">compile</code></td>
<td>
<p>if set to FALSE, the function will not be compiled -&gt; for visual inspection</p>
</td></tr>
<tr><td><code id=".compileTransformations_+3A_notes">notes</code></td>
<td>
<p>option to pass a notes to function. All notes of the current
function will be added</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with parameter names and two Rcpp functions: (1) the transformation function and
(2) a function to create a pointer to the transformation function.
If starting values were defined, these are returned as well.
</p>

<hr>
<h2 id='.computeInitialHessian'>.computeInitialHessian</h2><span id='topic+.computeInitialHessian'></span>

<h3>Description</h3>

<p>computes the initial Hessian used in the optimization. Because we use the parameter
estimates from lavaan as starting values, it typcially makes sense to just use the
Hessian of the lavaan model as initial Hessian
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.computeInitialHessian(
  initialHessian,
  rawParameters,
  lavaanModel,
  SEM,
  addMeans,
  stepSize,
  notes = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".computeInitialHessian_+3A_initialhessian">initialHessian</code></td>
<td>
<p>option to provide an initial Hessian to the optimizer.
Must have row and column names corresponding to the parameter labels. use
getLavaanParameters(lavaanModel) to
see those labels. If set to &quot;scoreBased&quot;, the outer product of the scores
will be used as an approximation
(see https://en.wikipedia.org/wiki/Berndt%E2%80%93Hall%E2%80%93Hall%E2%80%93Hausman_algorithm).
If set to &quot;compute&quot;, the initial hessian will be computed. If set to a single
value, a diagonal matrix with the single value along the diagonal will be used.
The default is &quot;lavaan&quot; which extracts the Hessian from the lavaanModel. This Hessian
will typically deviate from that of the internal SEM represenation of lessSEM (due to
the transformation of the variances), but works quite well in practice.</p>
</td></tr>
<tr><td><code id=".computeInitialHessian_+3A_rawparameters">rawParameters</code></td>
<td>
<p>vector with raw parameters</p>
</td></tr>
<tr><td><code id=".computeInitialHessian_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>lavaan model object</p>
</td></tr>
<tr><td><code id=".computeInitialHessian_+3A_sem">SEM</code></td>
<td>
<p>internal SEM representation</p>
</td></tr>
<tr><td><code id=".computeInitialHessian_+3A_addmeans">addMeans</code></td>
<td>
<p>should a mean structure be added to the model?</p>
</td></tr>
<tr><td><code id=".computeInitialHessian_+3A_stepsize">stepSize</code></td>
<td>
<p>initial step size</p>
</td></tr>
<tr><td><code id=".computeInitialHessian_+3A_notes">notes</code></td>
<td>
<p>option to pass a notes to function. All notes of the current
function will be added</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Hessian matrix and notes
</p>

<hr>
<h2 id='.createMultiGroupTransformations'>.createMultiGroupTransformations</h2><span id='topic+.createMultiGroupTransformations'></span>

<h3>Description</h3>

<p>compiles the transformation function and adapts the parameter vector
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.createMultiGroupTransformations(transformations, parameterValues)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".createMultiGroupTransformations_+3A_transformations">transformations</code></td>
<td>
<p>string with transformations</p>
</td></tr>
<tr><td><code id=".createMultiGroupTransformations_+3A_parametervalues">parameterValues</code></td>
<td>
<p>values of parameters already in the model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with extended parameter vector and transformation function pointer
</p>

<hr>
<h2 id='.createParameterTable'>.createParameterTable</h2><span id='topic+.createParameterTable'></span>

<h3>Description</h3>

<p>create a parameter table using the elements extracted from lavaan
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.createParameterTable(
  parameterValues,
  parameterLabels,
  modelParameters,
  parameterIDs
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".createParameterTable_+3A_parametervalues">parameterValues</code></td>
<td>
<p>values of parameters</p>
</td></tr>
<tr><td><code id=".createParameterTable_+3A_parameterlabels">parameterLabels</code></td>
<td>
<p>names of the parameters</p>
</td></tr>
<tr><td><code id=".createParameterTable_+3A_modelparameters">modelParameters</code></td>
<td>
<p>model parameters from lavaan</p>
</td></tr>
<tr><td><code id=".createParameterTable_+3A_parameterids">parameterIDs</code></td>
<td>
<p>unique parameter ids from lavaan -&gt; identify each parameter
with a unique number</p>
</td></tr>
</table>


<h3>Value</h3>

<p>parameter table for lessSEM
</p>

<hr>
<h2 id='.createRcppTransformationFunction'>.createRcppTransformationFunction</h2><span id='topic+.createRcppTransformationFunction'></span>

<h3>Description</h3>

<p>create an Rcpp function which uses the user-defined parameter transformation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.createRcppTransformationFunction(syntax, parameters)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".createRcppTransformationFunction_+3A_syntax">syntax</code></td>
<td>
<p>syntax with user defined transformations</p>
</td></tr>
<tr><td><code id=".createRcppTransformationFunction_+3A_parameters">parameters</code></td>
<td>
<p>labels of parameters used in these transformations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>string with functions for compilations with Rcpp
</p>

<hr>
<h2 id='.createTransformations'>.createTransformations</h2><span id='topic+.createTransformations'></span>

<h3>Description</h3>

<p>compiles the transformation function and adapts the parameterTable
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.createTransformations(transformations, parameterLabels, parameterTable)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".createTransformations_+3A_transformations">transformations</code></td>
<td>
<p>string with transformations</p>
</td></tr>
<tr><td><code id=".createTransformations_+3A_parameterlabels">parameterLabels</code></td>
<td>
<p>labels of parameteres already in the model</p>
</td></tr>
<tr><td><code id=".createTransformations_+3A_parametertable">parameterTable</code></td>
<td>
<p>existing parameter table</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with parameterTable and transformation function pointer
</p>

<hr>
<h2 id='.cvregsem2LavaanParameters'>.cvregsem2LavaanParameters</h2><span id='topic+.cvregsem2LavaanParameters'></span>

<h3>Description</h3>

<p>helper function: regsem and lavaan use slightly different parameter labels. This function
can be used to translate the parameter labels of a cv_regsem object to lavaan labels
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.cvregsem2LavaanParameters(cvregsemModel, lavaanModel)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".cvregsem2LavaanParameters_+3A_cvregsemmodel">cvregsemModel</code></td>
<td>
<p>model of class cvregsem</p>
</td></tr>
<tr><td><code id=".cvregsem2LavaanParameters_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
</table>


<h3>Value</h3>

<p>regsem parameters with lavaan labels
</p>

<hr>
<h2 id='.cvRegularizeSEMInternal'>.cvRegularizeSEMInternal</h2><span id='topic+.cvRegularizeSEMInternal'></span>

<h3>Description</h3>

<p>Combination of regularized structural equation model and cross-validation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.cvRegularizeSEMInternal(
  lavaanModel,
  k,
  standardize,
  penalty,
  weights,
  returnSubsetParameters,
  tuningParameters,
  method,
  modifyModel,
  control
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".cvRegularizeSEMInternal_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id=".cvRegularizeSEMInternal_+3A_k">k</code></td>
<td>
<p>the number of cross-validation folds. Alternatively, a matrix with pre-defined subsets can be passed to the function.
See ?lessSEM::cvLasso for an example</p>
</td></tr>
<tr><td><code id=".cvRegularizeSEMInternal_+3A_standardize">standardize</code></td>
<td>
<p>should training and test sets be standardized?</p>
</td></tr>
<tr><td><code id=".cvRegularizeSEMInternal_+3A_penalty">penalty</code></td>
<td>
<p>string: name of the penalty used in the model</p>
</td></tr>
<tr><td><code id=".cvRegularizeSEMInternal_+3A_weights">weights</code></td>
<td>
<p>labeled vector with weights for each of the parameters in the
model.</p>
</td></tr>
<tr><td><code id=".cvRegularizeSEMInternal_+3A_returnsubsetparameters">returnSubsetParameters</code></td>
<td>
<p>if set to TRUE, the parameter estimates of the individual cross-validation training sets will be returned</p>
</td></tr>
<tr><td><code id=".cvRegularizeSEMInternal_+3A_tuningparameters">tuningParameters</code></td>
<td>
<p>data.frame with tuning parameter values</p>
</td></tr>
<tr><td><code id=".cvRegularizeSEMInternal_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet. With ista, the control argument can be used to switch to related procedures
(currently gist).</p>
</td></tr>
<tr><td><code id=".cvRegularizeSEMInternal_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id=".cvRegularizeSEMInternal_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta() and controlGlmnet() functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Internal function: This function computes the regularized models
for all penalty functions which are implemented for glmnet and gist.
Use the dedicated penalty functions (e.g., lessSEM::cvLasso) to penalize
the model.
</p>


<h3>Value</h3>

<p>model of class cvRegularizedSEM
</p>

<hr>
<h2 id='.cvRegularizeSmoothSEMInternal'>.cvRegularizeSmoothSEMInternal</h2><span id='topic+.cvRegularizeSmoothSEMInternal'></span>

<h3>Description</h3>

<p>Combination of smoothly regularized structural equation model and cross-validation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.cvRegularizeSmoothSEMInternal(
  lavaanModel,
  k,
  standardize,
  penalty,
  weights,
  returnSubsetParameters,
  tuningParameters,
  epsilon,
  modifyModel,
  method = "bfgs",
  control
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".cvRegularizeSmoothSEMInternal_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id=".cvRegularizeSmoothSEMInternal_+3A_k">k</code></td>
<td>
<p>the number of cross-validation folds. Alternatively, a matrix with pre-defined subsets can be passed to the function.
See ?lessSEM::cvSmoothLasso for an example</p>
</td></tr>
<tr><td><code id=".cvRegularizeSmoothSEMInternal_+3A_standardize">standardize</code></td>
<td>
<p>should training and test sets be standardized?</p>
</td></tr>
<tr><td><code id=".cvRegularizeSmoothSEMInternal_+3A_penalty">penalty</code></td>
<td>
<p>string: name of the penalty used in the model</p>
</td></tr>
<tr><td><code id=".cvRegularizeSmoothSEMInternal_+3A_weights">weights</code></td>
<td>
<p>labeled vector with weights for each of the parameters in the
model.</p>
</td></tr>
<tr><td><code id=".cvRegularizeSmoothSEMInternal_+3A_returnsubsetparameters">returnSubsetParameters</code></td>
<td>
<p>if set to TRUE, the parameter estimates of the individual cross-validation training sets will be returned</p>
</td></tr>
<tr><td><code id=".cvRegularizeSmoothSEMInternal_+3A_tuningparameters">tuningParameters</code></td>
<td>
<p>data.frame with tuning parameter values</p>
</td></tr>
<tr><td><code id=".cvRegularizeSmoothSEMInternal_+3A_epsilon">epsilon</code></td>
<td>
<p>epsilon &gt; 0; controls the smoothness of the approximation. Larger values = smoother</p>
</td></tr>
<tr><td><code id=".cvRegularizeSmoothSEMInternal_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id=".cvRegularizeSmoothSEMInternal_+3A_method">method</code></td>
<td>
<p>optimizer used. Currently only &quot;bfgs&quot; is supported.</p>
</td></tr>
<tr><td><code id=".cvRegularizeSmoothSEMInternal_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlBFGS function. See ?controlBFGS for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Internal function: This function computes the regularized models
for all penalty functions which are implemented for bfgs.
Use the dedicated penalty functions (e.g., lessSEM::cvSmoothLasso) to penalize
the model.
</p>


<h3>Value</h3>

<p>model of class cvRegularizedSEM
</p>

<hr>
<h2 id='.defineDerivatives'>.defineDerivatives</h2><span id='topic+.defineDerivatives'></span>

<h3>Description</h3>

<p>adds all elements required to compute the derivatives of the fitting function
with respect to the parameters to the SEMList
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.defineDerivatives(SEMList, parameterTable, modelMatrices)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".defineDerivatives_+3A_semlist">SEMList</code></td>
<td>
<p>list representing SEM</p>
</td></tr>
<tr><td><code id=".defineDerivatives_+3A_parametertable">parameterTable</code></td>
<td>
<p>table with parameters</p>
</td></tr>
<tr><td><code id=".defineDerivatives_+3A_modelmatrices">modelMatrices</code></td>
<td>
<p>matrices of the RAM model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>SEMList
</p>

<hr>
<h2 id='.extractParametersFromSyntax'>.extractParametersFromSyntax</h2><span id='topic+.extractParametersFromSyntax'></span>

<h3>Description</h3>

<p>extract the names of the parameters in a syntax
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.extractParametersFromSyntax(syntax, parameterLabels)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".extractParametersFromSyntax_+3A_syntax">syntax</code></td>
<td>
<p>syntax for parameter transformations</p>
</td></tr>
<tr><td><code id=".extractParametersFromSyntax_+3A_parameterlabels">parameterLabels</code></td>
<td>
<p>names of parameters in the model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector with names of parameters used in the syntax and vector with
boolean indicating if parameter is transformation result
</p>

<hr>
<h2 id='.extractSEMFromLavaan'>.extractSEMFromLavaan</h2><span id='topic+.extractSEMFromLavaan'></span>

<h3>Description</h3>

<p>internal function. Translates an object of class lavaan to the internal model representation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.extractSEMFromLavaan(
  lavaanModel,
  whichPars = "est",
  fit = TRUE,
  addMeans = TRUE,
  activeSet = NULL,
  dataSet = NULL,
  transformations = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".extractSEMFromLavaan_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id=".extractSEMFromLavaan_+3A_whichpars">whichPars</code></td>
<td>
<p>which parameters should be used to initialize the model. If set to &quot;est&quot;, the parameters will be set to the
estimated parameters of the lavaan model. If set to &quot;start&quot;, the starting values of lavaan will be used. The latter can be useful if parameters are to
be optimized afterwards as setting the parameters to &quot;est&quot; may result in the model getting stuck in a local minimum.</p>
</td></tr>
<tr><td><code id=".extractSEMFromLavaan_+3A_fit">fit</code></td>
<td>
<p>should the model be fitted and compared to the lavaanModel?</p>
</td></tr>
<tr><td><code id=".extractSEMFromLavaan_+3A_addmeans">addMeans</code></td>
<td>
<p>If lavaanModel has meanstructure = FALSE, addMeans = TRUE will add a mean structure. FALSE will set the means of the observed variables to the average</p>
</td></tr>
<tr><td><code id=".extractSEMFromLavaan_+3A_activeset">activeSet</code></td>
<td>
<p>Option to only use a subset of the individuals in the data set. Logical vector of length N indicating which subjects should remain in the sample.</p>
</td></tr>
<tr><td><code id=".extractSEMFromLavaan_+3A_dataset">dataSet</code></td>
<td>
<p>optional: Pass an alternative data set to lessSEM:::.SEMFromLavaan which will replace the original data set in lavaanModel.</p>
</td></tr>
<tr><td><code id=".extractSEMFromLavaan_+3A_transformations">transformations</code></td>
<td>
<p>optional: transform parameter values.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with SEMList (model in RAM representation) and fit (boolean indicating if the model should
be fit and compared to lavaan)
</p>

<hr>
<h2 id='.fit'>.fit</h2><span id='topic+.fit'></span>

<h3>Description</h3>

<p>fits an object of class Rcpp_SEMCpp.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.fit(SEM)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".fit_+3A_sem">SEM</code></td>
<td>
<p>model of class Rcpp_SEMCpp.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>fitted SEM
</p>

<hr>
<h2 id='.fitElasticNetMix'>.fitElasticNetMix</h2><span id='topic+.fitElasticNetMix'></span>

<h3>Description</h3>

<p>Optimizes an object with mixed penalty. See ?mixedPenalty for more details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.fitElasticNetMix(mixedPenalty)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".fitElasticNetMix_+3A_mixedpenalty">mixedPenalty</code></td>
<td>
<p>object of class mixedPenalty. This object can be created
with the mixedPenalty function. Penalties can be added with the addCappedL1, addElastiNet,
addLasso, addLsp, addMcp, and addScad functions.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>object of class regularizedSEMMixedPenalty
</p>

<hr>
<h2 id='.fitFunction'>.fitFunction</h2><span id='topic+.fitFunction'></span>

<h3>Description</h3>

<p>internal function which returns the objective value of the fitting function of an object of class Rcpp_SEMCpp. This function can be used in optimizers
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.fitFunction(par, SEM, raw)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".fitFunction_+3A_par">par</code></td>
<td>
<p>labeled vector with parameter values</p>
</td></tr>
<tr><td><code id=".fitFunction_+3A_sem">SEM</code></td>
<td>
<p>model of class Rcpp_SEMCpp.</p>
</td></tr>
<tr><td><code id=".fitFunction_+3A_raw">raw</code></td>
<td>
<p>controls if the internal transformations of lessSEM is used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>objective value of the fitting function
</p>

<hr>
<h2 id='.fitMix'>.fitMix</h2><span id='topic+.fitMix'></span>

<h3>Description</h3>

<p>Optimizes an object with mixed penalty. See ?mixedPenalty for more details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.fitMix(mixedPenalty)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".fitMix_+3A_mixedpenalty">mixedPenalty</code></td>
<td>
<p>object of class mixedPenalty. This object can be created
with the mixedPenalty function. Penalties can be added with the addCappedL1, addElastiNet,
addLasso, addLsp, addMcp, and addScad functions.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>object of class regularizedSEMMixedPenalty
</p>

<hr>
<h2 id='.getGradients'>.getGradients</h2><span id='topic+.getGradients'></span>

<h3>Description</h3>

<p>returns the gradients of a model of class Rcpp_SEMCpp. This is the internal model
representation. Models of this class
can be generated with the lessSEM:::.SEMFromLavaan-function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.getGradients(SEM, raw)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".getGradients_+3A_sem">SEM</code></td>
<td>
<p>model of class Rcpp_SEMCpp</p>
</td></tr>
<tr><td><code id=".getGradients_+3A_raw">raw</code></td>
<td>
<p>controls if the internal transformations of lessSEM should be used.
lessSEM will use an exponential function for all variances to
avoid negative variances. When set to TRUE, the gradients will be given for the
internal parameter representation. Set to FALSE to get the usual
gradients</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector with derivatives of the -2log-Likelihood with respect to each parameter
</p>

<hr>
<h2 id='.getHessian'>.getHessian</h2><span id='topic+.getHessian'></span>

<h3>Description</h3>

<p>returns the Hessian of a model of class Rcpp_SEMCpp. This is the internal
model representation. Models of this class
can be generated with the lessSEM:::.SEMFromLavaan-function. The function is adapted
from lavaan::lav_model_hessian.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.getHessian(SEM, raw, eps = 1e-07)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".getHessian_+3A_sem">SEM</code></td>
<td>
<p>model of class Rcpp_SEMCpp</p>
</td></tr>
<tr><td><code id=".getHessian_+3A_raw">raw</code></td>
<td>
<p>controls if the internal transformations of lessSEM should be used.
lessSEM will use an exponential function for all variances to
avoid negative variances. When set to TRUE, the gradients will be given for
the internal parameter representation. Set to FALSE to get the usual
gradients</p>
</td></tr>
<tr><td><code id=".getHessian_+3A_eps">eps</code></td>
<td>
<p>eps controls the step size of the numerical approximation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>matrix with second derivatives of the -2log-Likelihood with respect
to each parameter
</p>

<hr>
<h2 id='.getMaxLambda_C'>.getMaxLambda_C</h2><span id='topic+.getMaxLambda_C'></span>

<h3>Description</h3>

<p>generates a the first lambda which sets all regularized parameters to zero
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.getMaxLambda_C(
  regularizedModel,
  SEM,
  rawParameters,
  weights,
  N,
  approx = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".getMaxLambda_C_+3A_regularizedmodel">regularizedModel</code></td>
<td>
<p>Model combining likelihood and lasso type penalty</p>
</td></tr>
<tr><td><code id=".getMaxLambda_C_+3A_sem">SEM</code></td>
<td>
<p>model of class Rcpp_SEMCpp</p>
</td></tr>
<tr><td><code id=".getMaxLambda_C_+3A_rawparameters">rawParameters</code></td>
<td>
<p>labeled vector with starting values</p>
</td></tr>
<tr><td><code id=".getMaxLambda_C_+3A_weights">weights</code></td>
<td>
<p>weights given to each parameter in the penalty function</p>
</td></tr>
<tr><td><code id=".getMaxLambda_C_+3A_n">N</code></td>
<td>
<p>sample size</p>
</td></tr>
<tr><td><code id=".getMaxLambda_C_+3A_approx">approx</code></td>
<td>
<p>When set to TRUE, .Machine$double.xmax^(.01) is used instead of .Machine$double.xmax^(.05)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>first lambda value which sets all regularized parameters to zero (plus some tolerance)
</p>

<hr>
<h2 id='.getParameters'>.getParameters</h2><span id='topic+.getParameters'></span>

<h3>Description</h3>

<p>returns the parameters of the internal model representation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.getParameters(SEM, raw = FALSE, transformations = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".getParameters_+3A_sem">SEM</code></td>
<td>
<p>model of class Rcpp_SEMCpp. Models of this class</p>
</td></tr>
<tr><td><code id=".getParameters_+3A_raw">raw</code></td>
<td>
<p>controls if the parameter are returned in raw format or transformed</p>
</td></tr>
<tr><td><code id=".getParameters_+3A_transformations">transformations</code></td>
<td>
<p>should transformed parameters be included?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>labeled vector with parameter values
</p>

<hr>
<h2 id='.getRawData'>.getRawData</h2><span id='topic+.getRawData'></span>

<h3>Description</h3>

<p>Extracts the raw data from lavaan or adapts a user supplied data set to
the structure of the lavaan data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.getRawData(lavaanModel, dataSet, estimator)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".getRawData_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model fitted with lavaan</p>
</td></tr>
<tr><td><code id=".getRawData_+3A_dataset">dataSet</code></td>
<td>
<p>user supplied data set</p>
</td></tr>
<tr><td><code id=".getRawData_+3A_estimator">estimator</code></td>
<td>
<p>which estimator is used?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>raw data
</p>

<hr>
<h2 id='.getScores'>.getScores</h2><span id='topic+.getScores'></span>

<h3>Description</h3>

<p>returns the scores of a model of class Rcpp_SEMCpp. This is the internal model
representation. Models of this class
can be generated with the lessSEM:::.SEMFromLavaan-function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.getScores(SEM, raw)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".getScores_+3A_sem">SEM</code></td>
<td>
<p>model of class Rcpp_SEMCpp</p>
</td></tr>
<tr><td><code id=".getScores_+3A_raw">raw</code></td>
<td>
<p>controls if the internal transformations of lessSEM should be used.
lessSEM will use an exponential function for all variances to
avoid negative variances. When set to TRUE, the scores will be given for the
internal parameter representation. Set to FALSE to get the usual
scores</p>
</td></tr>
</table>


<h3>Value</h3>

<p>matrix with derivatives of the -2log-Likelihood for each person and parameter (rows are persons, columns are parameters)
</p>

<hr>
<h2 id='.gpGetMaxLambda'>.gpGetMaxLambda</h2><span id='topic+.gpGetMaxLambda'></span>

<h3>Description</h3>

<p>generates a the first lambda which sets all regularized parameters to zero
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.gpGetMaxLambda(
  regularizedModel,
  par,
  fitFunction,
  gradientFunction,
  userSuppliedArguments,
  weights
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".gpGetMaxLambda_+3A_regularizedmodel">regularizedModel</code></td>
<td>
<p>Model combining likelihood and lasso type penalty</p>
</td></tr>
<tr><td><code id=".gpGetMaxLambda_+3A_par">par</code></td>
<td>
<p>labeled vector with starting values</p>
</td></tr>
<tr><td><code id=".gpGetMaxLambda_+3A_fitfunction">fitFunction</code></td>
<td>
<p>R fit function</p>
</td></tr>
<tr><td><code id=".gpGetMaxLambda_+3A_gradientfunction">gradientFunction</code></td>
<td>
<p>R gradient functions</p>
</td></tr>
<tr><td><code id=".gpGetMaxLambda_+3A_usersuppliedarguments">userSuppliedArguments</code></td>
<td>
<p>list with arguments for fitFunction and gradientFunction</p>
</td></tr>
<tr><td><code id=".gpGetMaxLambda_+3A_weights">weights</code></td>
<td>
<p>weights given to each parameter in the penalty function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>first lambda value which sets all regularized parameters to zero (plus some tolerance)
</p>

<hr>
<h2 id='.gpOptimizationInternal'>.gpOptimizationInternal</h2><span id='topic+.gpOptimizationInternal'></span>

<h3>Description</h3>

<p>Internal function: This function computes the regularized models
for all penaltiy functions which are implemented for glmnet and gist.
Use the dedicated penalty functions (e.g., lessSEM::gpLasso) to penalize
the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.gpOptimizationInternal(
  par,
  weights,
  fn,
  gr = NULL,
  additionalArguments,
  isCpp = FALSE,
  penalty,
  tuningParameters,
  method,
  control
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".gpOptimizationInternal_+3A_par">par</code></td>
<td>
<p>labeled vector with starting values</p>
</td></tr>
<tr><td><code id=".gpOptimizationInternal_+3A_weights">weights</code></td>
<td>
<p>labeled vector with weights for each of the parameters in the
model.</p>
</td></tr>
<tr><td><code id=".gpOptimizationInternal_+3A_fn">fn</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the fit value (a single value)</p>
</td></tr>
<tr><td><code id=".gpOptimizationInternal_+3A_gr">gr</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the gradients of the objective function.
If set to NULL, numDeriv will be used to approximate the gradients</p>
</td></tr>
<tr><td><code id=".gpOptimizationInternal_+3A_additionalarguments">additionalArguments</code></td>
<td>
<p>additional argument passed to fn and gr</p>
</td></tr>
<tr><td><code id=".gpOptimizationInternal_+3A_iscpp">isCpp</code></td>
<td>
<p>boolean: are fn and gr C++ function pointers?</p>
</td></tr>
<tr><td><code id=".gpOptimizationInternal_+3A_penalty">penalty</code></td>
<td>
<p>string: name of the penalty used in the model</p>
</td></tr>
<tr><td><code id=".gpOptimizationInternal_+3A_tuningparameters">tuningParameters</code></td>
<td>
<p>data.frame with tuning parameter values</p>
</td></tr>
<tr><td><code id=".gpOptimizationInternal_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet. With ista, the control argument can be used to switch to related procedures
(currently gist).</p>
</td></tr>
<tr><td><code id=".gpOptimizationInternal_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta() and controlGlmnet() functions.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class gpRegularized
</p>

<hr>
<h2 id='.gradientFunction'>.gradientFunction</h2><span id='topic+.gradientFunction'></span>

<h3>Description</h3>

<p>internal function which returns the gradients of an object of class Rcpp_SEMCpp. This function can be used in optimizers
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.gradientFunction(par, SEM, raw)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".gradientFunction_+3A_par">par</code></td>
<td>
<p>labeled vector with parameter values</p>
</td></tr>
<tr><td><code id=".gradientFunction_+3A_sem">SEM</code></td>
<td>
<p>model of class Rcpp_SEMCpp.</p>
</td></tr>
<tr><td><code id=".gradientFunction_+3A_raw">raw</code></td>
<td>
<p>controls if the internal transformations of lessSEM is used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>gradients of the model
</p>

<hr>
<h2 id='.initializeMultiGroupSEMForRegularization'>.initializeMultiGroupSEMForRegularization</h2><span id='topic+.initializeMultiGroupSEMForRegularization'></span>

<h3>Description</h3>

<p>initializes the internal C++ SEM for regularization functions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.initializeMultiGroupSEMForRegularization(
  lavaanModels,
  startingValues,
  modifyModel
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".initializeMultiGroupSEMForRegularization_+3A_lavaanmodels">lavaanModels</code></td>
<td>
<p>vector with models of class lavaan</p>
</td></tr>
<tr><td><code id=".initializeMultiGroupSEMForRegularization_+3A_startingvalues">startingValues</code></td>
<td>
<p>either set to est, start, or labeled vector with starting values</p>
</td></tr>
<tr><td><code id=".initializeMultiGroupSEMForRegularization_+3A_modifymodel">modifyModel</code></td>
<td>
<p>user supplied model modifications</p>
</td></tr>
</table>


<h3>Value</h3>

<p>model to be used by the regularization procedure
</p>

<hr>
<h2 id='.initializeSEMForRegularization'>.initializeSEMForRegularization</h2><span id='topic+.initializeSEMForRegularization'></span>

<h3>Description</h3>

<p>initializes the internal C++ SEM for regularization functions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.initializeSEMForRegularization(lavaanModel, startingValues, modifyModel)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".initializeSEMForRegularization_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id=".initializeSEMForRegularization_+3A_startingvalues">startingValues</code></td>
<td>
<p>either set to est, start, or labeled vector with starting values</p>
</td></tr>
<tr><td><code id=".initializeSEMForRegularization_+3A_modifymodel">modifyModel</code></td>
<td>
<p>user supplied model modifications</p>
</td></tr>
</table>


<h3>Value</h3>

<p>model to be used by the regularization procedure
</p>

<hr>
<h2 id='.initializeWeights'>.initializeWeights</h2><span id='topic+.initializeWeights'></span>

<h3>Description</h3>

<p>initialize the adaptive lasso weights
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.initializeWeights(
  weights,
  penalty,
  method,
  createAdaptiveLassoWeights,
  control,
  lavaanModel,
  modifyModel,
  startingValues,
  rawParameters
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".initializeWeights_+3A_weights">weights</code></td>
<td>
<p>weight argument passed to function</p>
</td></tr>
<tr><td><code id=".initializeWeights_+3A_penalty">penalty</code></td>
<td>
<p>penalty used</p>
</td></tr>
<tr><td><code id=".initializeWeights_+3A_createadaptivelassoweights">createAdaptiveLassoWeights</code></td>
<td>
<p>should adaptive lasso weights be created?</p>
</td></tr>
<tr><td><code id=".initializeWeights_+3A_control">control</code></td>
<td>
<p>list with control elements for optimizer</p>
</td></tr>
<tr><td><code id=".initializeWeights_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of type lavaan</p>
</td></tr>
<tr><td><code id=".initializeWeights_+3A_modifymodel">modifyModel</code></td>
<td>
<p>list with model modifications</p>
</td></tr>
<tr><td><code id=".initializeWeights_+3A_startingvalues">startingValues</code></td>
<td>
<p>either set to est, start, or labeled vector with starting values</p>
</td></tr>
<tr><td><code id=".initializeWeights_+3A_rawparameters">rawParameters</code></td>
<td>
<p>raw parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector with weights
</p>

<hr>
<h2 id='.labelLavaanParameters'>.labelLavaanParameters</h2><span id='topic+.labelLavaanParameters'></span>

<h3>Description</h3>

<p>Adds labels to unlabeled parameters in the lavaan parameter table. Also
removes fixed parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.labelLavaanParameters(lavaanModel)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".labelLavaanParameters_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>fitted lavaan model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>parameterTable with labeled parameters
</p>

<hr>
<h2 id='.lavaan2regsemLabels'>.lavaan2regsemLabels</h2><span id='topic+.lavaan2regsemLabels'></span>

<h3>Description</h3>

<p>helper function: regsem and lavaan use slightly different parameter labels. This function
can be used to get both sets of labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.lavaan2regsemLabels(lavaanModel)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".lavaan2regsemLabels_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with lavaan and regsem labels
</p>

<hr>
<h2 id='.likelihoodRatioFit'>.likelihoodRatioFit</h2><span id='topic+.likelihoodRatioFit'></span>

<h3>Description</h3>

<p>internal function which returns the likelihood ratio fit statistic
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.likelihoodRatioFit(par, SEM, raw)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".likelihoodRatioFit_+3A_par">par</code></td>
<td>
<p>labeled vector with parameter values</p>
</td></tr>
<tr><td><code id=".likelihoodRatioFit_+3A_sem">SEM</code></td>
<td>
<p>model of class Rcpp_SEMCpp.</p>
</td></tr>
<tr><td><code id=".likelihoodRatioFit_+3A_raw">raw</code></td>
<td>
<p>controls if the internal transformations of lessSEM is used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>likelihood ratio fit statistic
</p>

<hr>
<h2 id='.makeSingleLine'>.makeSingleLine</h2><span id='topic+.makeSingleLine'></span>

<h3>Description</h3>

<p>checks if a parameter: or a start: statement spans multiple lines
and reduces it to one line.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.makeSingleLine(syntax, what)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".makeSingleLine_+3A_syntax">syntax</code></td>
<td>
<p>reduced syntax</p>
</td></tr>
<tr><td><code id=".makeSingleLine_+3A_what">what</code></td>
<td>
<p>which statement to look for (parameters or start)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a syntax where multi-line statements are condensed to one line
</p>

<hr>
<h2 id='.multiGroupSEMFromLavaan'>.multiGroupSEMFromLavaan</h2><span id='topic+.multiGroupSEMFromLavaan'></span>

<h3>Description</h3>

<p>internal function. Translates a vector of objects of class lavaan to the
internal model representation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.multiGroupSEMFromLavaan(
  lavaanModels,
  whichPars = "est",
  fit = TRUE,
  addMeans = TRUE,
  transformations = NULL,
  transformationList = list(),
  transformationGradientStepSize = 1e-06
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".multiGroupSEMFromLavaan_+3A_lavaanmodels">lavaanModels</code></td>
<td>
<p>vector with lavaan models</p>
</td></tr>
<tr><td><code id=".multiGroupSEMFromLavaan_+3A_whichpars">whichPars</code></td>
<td>
<p>which parameters should be used to initialize the model. If set to &quot;est&quot;, the parameters will be set to the
estimated parameters of the lavaan model. If set to &quot;start&quot;, the starting values of lavaan will be used. The latter can be useful if parameters are to
be optimized afterwards as setting the parameters to &quot;est&quot; may result in the model getting stuck in a local minimum.</p>
</td></tr>
<tr><td><code id=".multiGroupSEMFromLavaan_+3A_fit">fit</code></td>
<td>
<p>should the model be fitted</p>
</td></tr>
<tr><td><code id=".multiGroupSEMFromLavaan_+3A_addmeans">addMeans</code></td>
<td>
<p>If lavaanModel has meanstructure = FALSE, addMeans = TRUE will add a mean structure. FALSE will set the means of the observed variables to the average</p>
</td></tr>
<tr><td><code id=".multiGroupSEMFromLavaan_+3A_transformations">transformations</code></td>
<td>
<p>string with transformations</p>
</td></tr>
<tr><td><code id=".multiGroupSEMFromLavaan_+3A_transformationlist">transformationList</code></td>
<td>
<p>list for transformations</p>
</td></tr>
<tr><td><code id=".multiGroupSEMFromLavaan_+3A_transformationgradientstepsize">transformationGradientStepSize</code></td>
<td>
<p>step size used to compute the gradients of the
transformations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class Rcpp_mgSEMCpp
</p>

<hr>
<h2 id='.noDotDotDot'>.noDotDotDot</h2><span id='topic+.noDotDotDot'></span>

<h3>Description</h3>

<p>remplaces the dot dot dot part of the fitting and gradient fuction
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.noDotDotDot(fn, fnName, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".noDotDotDot_+3A_fn">fn</code></td>
<td>
<p>fit or gradient function. IMPORTANT: THE FIRST ARGUMENT TO
THE FUNCTION MUST BE THE PARAMETER VECTOR</p>
</td></tr>
<tr><td><code id=".noDotDotDot_+3A_fnname">fnName</code></td>
<td>
<p>name of the function fn</p>
</td></tr>
<tr><td><code id=".noDotDotDot_+3A_...">...</code></td>
<td>
<p>additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with (1) new function which wraps fn and (2) list with arguments passed to fn
</p>

<hr>
<h2 id='.penaltyTypes'>.penaltyTypes</h2><span id='topic+.penaltyTypes'></span>

<h3>Description</h3>

<p>translates the penalty from a numeric value to the character or from
the character to the numeric value. The numeric value is used by the C++ backend.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.penaltyTypes(penalty)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".penaltyTypes_+3A_penalty">penalty</code></td>
<td>
<p>either a number or the name of the penalty</p>
</td></tr>
</table>


<h3>Value</h3>

<p>number corresponding to one of the penalties
</p>

<hr>
<h2 id='.reduceSyntax'>.reduceSyntax</h2><span id='topic+.reduceSyntax'></span>

<h3>Description</h3>

<p>reduce user defined parameter transformation syntax to basic elements
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.reduceSyntax(syntax)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".reduceSyntax_+3A_syntax">syntax</code></td>
<td>
<p>string with user defined transformations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a cut and simplified version of the syntax
</p>

<hr>
<h2 id='.regularizeSEMInternal'>.regularizeSEMInternal</h2><span id='topic+.regularizeSEMInternal'></span>

<h3>Description</h3>

<p>Internal function: This function computes the regularized models
for all penaltiy functions which are implemented for glmnet and gist.
Use the dedicated penalty functions (e.g., lessSEM::lasso) to penalize
the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.regularizeSEMInternal(
  lavaanModel,
  penalty,
  weights,
  tuningParameters,
  method,
  modifyModel,
  control,
  notes = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".regularizeSEMInternal_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id=".regularizeSEMInternal_+3A_penalty">penalty</code></td>
<td>
<p>string: name of the penalty used in the model</p>
</td></tr>
<tr><td><code id=".regularizeSEMInternal_+3A_weights">weights</code></td>
<td>
<p>labeled vector with weights for each of the parameters in the
model.</p>
</td></tr>
<tr><td><code id=".regularizeSEMInternal_+3A_tuningparameters">tuningParameters</code></td>
<td>
<p>data.frame with tuning parameter values</p>
</td></tr>
<tr><td><code id=".regularizeSEMInternal_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet. With ista, the control argument can be used to switch to related procedures
(currently gist).</p>
</td></tr>
<tr><td><code id=".regularizeSEMInternal_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id=".regularizeSEMInternal_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta() and controlGlmnet() functions.</p>
</td></tr>
<tr><td><code id=".regularizeSEMInternal_+3A_notes">notes</code></td>
<td>
<p>option to pass a notes to function. All notes of the current
function will be added</p>
</td></tr>
</table>


<h3>Value</h3>

<p>regularized SEM
</p>

<hr>
<h2 id='.regularizeSEMWithCustomPenaltyRsolnp'>.regularizeSEMWithCustomPenaltyRsolnp</h2><span id='topic+.regularizeSEMWithCustomPenaltyRsolnp'></span>

<h3>Description</h3>

<p>Optimize a SEM with custom penalty function using the Rsolnp optimizer (see ?Rsolnp::solnp). This optimizer is the default in regsem (see ?regsem::cv_regsem).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.regularizeSEMWithCustomPenaltyRsolnp(
  lavaanModel,
  individualPenaltyFunction,
  tuningParameters,
  penaltyFunctionArguments,
  startingValues = "est",
  carryOverParameters = TRUE,
  control = list(trace = 0)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".regularizeSEMWithCustomPenaltyRsolnp_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id=".regularizeSEMWithCustomPenaltyRsolnp_+3A_individualpenaltyfunction">individualPenaltyFunction</code></td>
<td>
<p>penalty function which takes the current parameter values as first argument, the tuning parameters as second, and the penaltyFunctionArguments as third argument and
returns a single value - the value of the penalty function for a single person. If the true penalty function is non-differentiable (e.g., lasso) a smooth
approximation of this function should be provided.</p>
</td></tr>
<tr><td><code id=".regularizeSEMWithCustomPenaltyRsolnp_+3A_tuningparameters">tuningParameters</code></td>
<td>
<p>data.frame with tuning parameter values. Important: The function will iterate over the rows of these tuning parameters and pass them to your penalty function</p>
</td></tr>
<tr><td><code id=".regularizeSEMWithCustomPenaltyRsolnp_+3A_penaltyfunctionarguments">penaltyFunctionArguments</code></td>
<td>
<p>arguments passed to individualPenaltyFunction, individualPenaltyFunctionGradient, and individualPenaltyFunctionHessian</p>
</td></tr>
<tr><td><code id=".regularizeSEMWithCustomPenaltyRsolnp_+3A_startingvalues">startingValues</code></td>
<td>
<p>option to provide initial starting values. Only used for the first lambda. Three options are supported. Setting to &quot;est&quot; will use the estimates
from the lavaan model object. Setting to &quot;start&quot; will use the starting values of the lavaan model. Finally, a labeled vector with parameter
values can be passed to the function which will then be used as starting values.</p>
</td></tr>
<tr><td><code id=".regularizeSEMWithCustomPenaltyRsolnp_+3A_carryoverparameters">carryOverParameters</code></td>
<td>
<p>should parameters from the previous iteration be used as starting values of
the next iteration?</p>
</td></tr>
<tr><td><code id=".regularizeSEMWithCustomPenaltyRsolnp_+3A_control">control</code></td>
<td>
<p>option to set parameters of the optimizer; see ?Rsolnp::solnp</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Model of class regularizedSEMWithCustomPenalty
</p>

<hr>
<h2 id='.regularizeSmoothSEMInternal'>.regularizeSmoothSEMInternal</h2><span id='topic+.regularizeSmoothSEMInternal'></span>

<h3>Description</h3>

<p>Internal function: This function computes the regularized models
for all smooth penalty functions which are implemented for bfgs.
Use the dedicated penalty functions (e.g., lessSEM::smoothLasso) to penalize
the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.regularizeSmoothSEMInternal(
  lavaanModel,
  penalty,
  weights,
  tuningParameters,
  epsilon,
  tau,
  method = "bfgs",
  modifyModel,
  control,
  notes = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".regularizeSmoothSEMInternal_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id=".regularizeSmoothSEMInternal_+3A_penalty">penalty</code></td>
<td>
<p>string: name of the penalty used in the model</p>
</td></tr>
<tr><td><code id=".regularizeSmoothSEMInternal_+3A_weights">weights</code></td>
<td>
<p>labeled vector with weights for each of the parameters in the
model.</p>
</td></tr>
<tr><td><code id=".regularizeSmoothSEMInternal_+3A_tuningparameters">tuningParameters</code></td>
<td>
<p>data.frame with tuning parameter values</p>
</td></tr>
<tr><td><code id=".regularizeSmoothSEMInternal_+3A_epsilon">epsilon</code></td>
<td>
<p>epsilon &gt; 0; controls the smoothness of the approximation. Larger values = smoother</p>
</td></tr>
<tr><td><code id=".regularizeSmoothSEMInternal_+3A_tau">tau</code></td>
<td>
<p>parameters below threshold tau will be seen as zeroed</p>
</td></tr>
<tr><td><code id=".regularizeSmoothSEMInternal_+3A_method">method</code></td>
<td>
<p>optimizer used. Currently only &quot;bfgs&quot; is supported.</p>
</td></tr>
<tr><td><code id=".regularizeSmoothSEMInternal_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id=".regularizeSmoothSEMInternal_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlBFGS function. See ?controlBFGS for more details.</p>
</td></tr>
<tr><td><code id=".regularizeSmoothSEMInternal_+3A_notes">notes</code></td>
<td>
<p>option to pass a notes to function. All notes of the current
function will be added</p>
</td></tr>
</table>


<h3>Value</h3>

<p>regularizedSEM
</p>

<hr>
<h2 id='.ridgeGradient'>.ridgeGradient</h2><span id='topic+.ridgeGradient'></span>

<h3>Description</h3>

<p>ridge gradient function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.ridgeGradient(parameters, tuningParameters, penaltyFunctionArguments)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".ridgeGradient_+3A_parameters">parameters</code></td>
<td>
<p>vector with labeled parameter values</p>
</td></tr>
<tr><td><code id=".ridgeGradient_+3A_tuningparameters">tuningParameters</code></td>
<td>
<p>list with field lambda (tuning parameter value)</p>
</td></tr>
<tr><td><code id=".ridgeGradient_+3A_penaltyfunctionarguments">penaltyFunctionArguments</code></td>
<td>
<p>list with field regularizedParameterLabels (labels of regularized parameters)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>gradient values
</p>

<hr>
<h2 id='.ridgeHessian'>.ridgeHessian</h2><span id='topic+.ridgeHessian'></span>

<h3>Description</h3>

<p>ridge Hessian function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.ridgeHessian(parameters, tuningParameters, penaltyFunctionArguments)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".ridgeHessian_+3A_parameters">parameters</code></td>
<td>
<p>vector with labeled parameter values</p>
</td></tr>
<tr><td><code id=".ridgeHessian_+3A_tuningparameters">tuningParameters</code></td>
<td>
<p>list with field lambda (tuning parameter value)</p>
</td></tr>
<tr><td><code id=".ridgeHessian_+3A_penaltyfunctionarguments">penaltyFunctionArguments</code></td>
<td>
<p>list with field regularizedParameterLabels (labels of regularized parameters)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Hessian matrix
</p>

<hr>
<h2 id='.ridgeValue'>.ridgeValue</h2><span id='topic+.ridgeValue'></span>

<h3>Description</h3>

<p>ridge penalty function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.ridgeValue(parameters, tuningParameters, penaltyFunctionArguments)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".ridgeValue_+3A_parameters">parameters</code></td>
<td>
<p>vector with labeled parameter values</p>
</td></tr>
<tr><td><code id=".ridgeValue_+3A_tuningparameters">tuningParameters</code></td>
<td>
<p>list with field lambda (tuning parameter value)</p>
</td></tr>
<tr><td><code id=".ridgeValue_+3A_penaltyfunctionarguments">penaltyFunctionArguments</code></td>
<td>
<p>list with field regularizedParameterLabels (labels of regularized parameters)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>penalty function value
</p>

<hr>
<h2 id='.SEMdata'>.SEMdata</h2><span id='topic+.SEMdata'></span>

<h3>Description</h3>

<p>internal function. Creates internal data representation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.SEMdata(rawData)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".SEMdata_+3A_rawdata">rawData</code></td>
<td>
<p>matrix with raw data set</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with internal representation of data
</p>

<hr>
<h2 id='.SEMdataWLS'>.SEMdataWLS</h2><span id='topic+.SEMdataWLS'></span>

<h3>Description</h3>

<p>internal function. Creates internal data representation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.SEMdataWLS(rawData, lavaanModel)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".SEMdataWLS_+3A_rawdata">rawData</code></td>
<td>
<p>matrix with raw data set</p>
</td></tr>
<tr><td><code id=".SEMdataWLS_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>lavaan model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with internal representation of data
</p>

<hr>
<h2 id='.SEMFromLavaan'>.SEMFromLavaan</h2><span id='topic+.SEMFromLavaan'></span>

<h3>Description</h3>

<p>internal function. Translates an object of class lavaan to the internal model representation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.SEMFromLavaan(
  lavaanModel,
  whichPars = "est",
  fit = TRUE,
  addMeans = TRUE,
  activeSet = NULL,
  dataSet = NULL,
  transformations = NULL,
  transformationList = list(),
  transformationGradientStepSize = 1e-06
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".SEMFromLavaan_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id=".SEMFromLavaan_+3A_whichpars">whichPars</code></td>
<td>
<p>which parameters should be used to initialize the model. If set to &quot;est&quot;, the parameters will be set to the
estimated parameters of the lavaan model. If set to &quot;start&quot;, the starting values of lavaan will be used. The latter can be useful if parameters are to
be optimized afterwards as setting the parameters to &quot;est&quot; may result in the model getting stuck in a local minimum.</p>
</td></tr>
<tr><td><code id=".SEMFromLavaan_+3A_fit">fit</code></td>
<td>
<p>should the model be fitted and compared to the lavaanModel?</p>
</td></tr>
<tr><td><code id=".SEMFromLavaan_+3A_addmeans">addMeans</code></td>
<td>
<p>If lavaanModel has meanstructure = FALSE, addMeans = TRUE will add a mean structure. FALSE will set the means of the observed variables to the average</p>
</td></tr>
<tr><td><code id=".SEMFromLavaan_+3A_activeset">activeSet</code></td>
<td>
<p>Option to only use a subset of the individuals in the data set. Logical vector of length N indicating which subjects should remain in the sample.</p>
</td></tr>
<tr><td><code id=".SEMFromLavaan_+3A_dataset">dataSet</code></td>
<td>
<p>optional: Pass an alternative data set to lessSEM:::.SEMFromLavaan which will replace the original data set in lavaanModel.</p>
</td></tr>
<tr><td><code id=".SEMFromLavaan_+3A_transformationgradientstepsize">transformationGradientStepSize</code></td>
<td>
<p>step size used to compute the gradients of the
transformations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class Rcpp_SEMCpp
</p>

<hr>
<h2 id='.setAMatrix'>.setAMatrix</h2><span id='topic+.setAMatrix'></span>

<h3>Description</h3>

<p>internal function. Populates the matrix with directed effects in RAM notation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.setAMatrix(
  model,
  lavaanParameterTable,
  nLatent,
  nManifest,
  latentNames,
  manifestNames
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".setAMatrix_+3A_model">model</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id=".setAMatrix_+3A_lavaanparametertable">lavaanParameterTable</code></td>
<td>
<p>parameter table from lavaan</p>
</td></tr>
<tr><td><code id=".setAMatrix_+3A_nlatent">nLatent</code></td>
<td>
<p>number of latent variables</p>
</td></tr>
<tr><td><code id=".setAMatrix_+3A_nmanifest">nManifest</code></td>
<td>
<p>number of manifest variables</p>
</td></tr>
<tr><td><code id=".setAMatrix_+3A_latentnames">latentNames</code></td>
<td>
<p>names of latent variables</p>
</td></tr>
<tr><td><code id=".setAMatrix_+3A_manifestnames">manifestNames</code></td>
<td>
<p>names of manifest variables</p>
</td></tr>
</table>

<hr>
<h2 id='.setFmatrix'>.setFmatrix</h2><span id='topic+.setFmatrix'></span>

<h3>Description</h3>

<p>returns the filter matrix of a RAM
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.setFmatrix(nManifest, manifestNames, nLatent, latentNames)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".setFmatrix_+3A_nmanifest">nManifest</code></td>
<td>
<p>number of manifest variables</p>
</td></tr>
<tr><td><code id=".setFmatrix_+3A_manifestnames">manifestNames</code></td>
<td>
<p>names of manifest variables</p>
</td></tr>
<tr><td><code id=".setFmatrix_+3A_nlatent">nLatent</code></td>
<td>
<p>number of latent variables</p>
</td></tr>
<tr><td><code id=".setFmatrix_+3A_latentnames">latentNames</code></td>
<td>
<p>names of latent variables</p>
</td></tr>
</table>


<h3>Value</h3>

<p>matrix
</p>

<hr>
<h2 id='.setMVector'>.setMVector</h2><span id='topic+.setMVector'></span>

<h3>Description</h3>

<p>internal function. Populates the vector with means in RAM notation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.setMVector(
  model,
  lavaanParameterTable,
  nLatent,
  nManifest,
  latentNames,
  manifestNames,
  rawData
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".setMVector_+3A_model">model</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id=".setMVector_+3A_lavaanparametertable">lavaanParameterTable</code></td>
<td>
<p>parameter table from lavaan</p>
</td></tr>
<tr><td><code id=".setMVector_+3A_nlatent">nLatent</code></td>
<td>
<p>number of latent variables</p>
</td></tr>
<tr><td><code id=".setMVector_+3A_nmanifest">nManifest</code></td>
<td>
<p>number of manifest variables</p>
</td></tr>
<tr><td><code id=".setMVector_+3A_latentnames">latentNames</code></td>
<td>
<p>names of latent variables</p>
</td></tr>
<tr><td><code id=".setMVector_+3A_manifestnames">manifestNames</code></td>
<td>
<p>names of manifest variables</p>
</td></tr>
<tr><td><code id=".setMVector_+3A_rawdata">rawData</code></td>
<td>
<p>matrix with raw data</p>
</td></tr>
</table>

<hr>
<h2 id='.setParameters'>.setParameters</h2><span id='topic+.setParameters'></span>

<h3>Description</h3>

<p>change the parameters of the internal model representation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.setParameters(SEM, labels, values, raw)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".setParameters_+3A_sem">SEM</code></td>
<td>
<p>model of class Rcpp_SEMCpp. Models of this class</p>
</td></tr>
<tr><td><code id=".setParameters_+3A_labels">labels</code></td>
<td>
<p>vector with parameter labels</p>
</td></tr>
<tr><td><code id=".setParameters_+3A_values">values</code></td>
<td>
<p>vector with parameter values</p>
</td></tr>
<tr><td><code id=".setParameters_+3A_raw">raw</code></td>
<td>
<p>are the parameters given in raw format or transformed?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>SEM with changed parameter values
</p>

<hr>
<h2 id='.setSMatrix'>.setSMatrix</h2><span id='topic+.setSMatrix'></span>

<h3>Description</h3>

<p>internal function. Populates the matrix with undirected paths in RAM notation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.setSMatrix(
  model,
  lavaanParameterTable,
  nLatent,
  nManifest,
  latentNames,
  manifestNames
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".setSMatrix_+3A_model">model</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id=".setSMatrix_+3A_lavaanparametertable">lavaanParameterTable</code></td>
<td>
<p>parameter table from lavaan</p>
</td></tr>
<tr><td><code id=".setSMatrix_+3A_nlatent">nLatent</code></td>
<td>
<p>number of latent variables</p>
</td></tr>
<tr><td><code id=".setSMatrix_+3A_nmanifest">nManifest</code></td>
<td>
<p>number of manifest variables</p>
</td></tr>
<tr><td><code id=".setSMatrix_+3A_latentnames">latentNames</code></td>
<td>
<p>names of latent variables</p>
</td></tr>
<tr><td><code id=".setSMatrix_+3A_manifestnames">manifestNames</code></td>
<td>
<p>names of manifest variables</p>
</td></tr>
</table>

<hr>
<h2 id='.setupMulticore'>.setupMulticore</h2><span id='topic+.setupMulticore'></span>

<h3>Description</h3>

<p>setup for multi-core support
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.setupMulticore(control)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".setupMulticore_+3A_control">control</code></td>
<td>
<p>object created with controlBFGS, controlIsta or controlGlmnet function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>nothing
</p>

<hr>
<h2 id='.smoothAdaptiveLASSOGradient'>.smoothAdaptiveLASSOGradient</h2><span id='topic+.smoothAdaptiveLASSOGradient'></span>

<h3>Description</h3>

<p>smoothed version of non-differentiable adaptive LASSO gradient
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.smoothAdaptiveLASSOGradient(
  parameters,
  tuningParameters,
  penaltyFunctionArguments
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".smoothAdaptiveLASSOGradient_+3A_parameters">parameters</code></td>
<td>
<p>vector with labeled parameter values</p>
</td></tr>
<tr><td><code id=".smoothAdaptiveLASSOGradient_+3A_tuningparameters">tuningParameters</code></td>
<td>
<p>list with fields lambdas (vector with one tuning parameter value for each parameter)</p>
</td></tr>
<tr><td><code id=".smoothAdaptiveLASSOGradient_+3A_penaltyfunctionarguments">penaltyFunctionArguments</code></td>
<td>
<p>list with fields regularizedParameterLabels (labels of regularized parameters), and eps (controls the smooth approximation of non-differential penalty functions (e.g., lasso, adaptive lasso, or elastic net). Smaller values result in closer approximation, but may also cause larger issues in optimization.)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>gradient values
</p>

<hr>
<h2 id='.smoothAdaptiveLASSOHessian'>.smoothAdaptiveLASSOHessian</h2><span id='topic+.smoothAdaptiveLASSOHessian'></span>

<h3>Description</h3>

<p>smoothed version of non-differentiable adaptive LASSO Hessian
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.smoothAdaptiveLASSOHessian(
  parameters,
  tuningParameters,
  penaltyFunctionArguments
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".smoothAdaptiveLASSOHessian_+3A_parameters">parameters</code></td>
<td>
<p>vector with labeled parameter values</p>
</td></tr>
<tr><td><code id=".smoothAdaptiveLASSOHessian_+3A_tuningparameters">tuningParameters</code></td>
<td>
<p>list with fields lambdas (vector with one tuning parameter value for each parameter)</p>
</td></tr>
<tr><td><code id=".smoothAdaptiveLASSOHessian_+3A_penaltyfunctionarguments">penaltyFunctionArguments</code></td>
<td>
<p>list with fields regularizedParameterLabels (labels of regularized parameters), and eps (controls the smooth approximation of non-differential penalty functions (e.g., lasso, adaptive lasso, or elastic net). Smaller values result in closer approximation, but may also cause larger issues in optimization.)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Hessian matrix
</p>

<hr>
<h2 id='.smoothAdaptiveLASSOValue'>.smoothAdaptiveLASSOValue</h2><span id='topic+.smoothAdaptiveLASSOValue'></span>

<h3>Description</h3>

<p>smoothed version of non-differentiable adaptive LASSO penalty
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.smoothAdaptiveLASSOValue(
  parameters,
  tuningParameters,
  penaltyFunctionArguments
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".smoothAdaptiveLASSOValue_+3A_parameters">parameters</code></td>
<td>
<p>vector with labeled parameter values</p>
</td></tr>
<tr><td><code id=".smoothAdaptiveLASSOValue_+3A_tuningparameters">tuningParameters</code></td>
<td>
<p>list with fields lambdas (vector with one tuning parameter value for each parameter)</p>
</td></tr>
<tr><td><code id=".smoothAdaptiveLASSOValue_+3A_penaltyfunctionarguments">penaltyFunctionArguments</code></td>
<td>
<p>list with fields regularizedParameterLabels (labels of regularized parameters), and eps (controls the smooth approximation of non-differential penalty functions (e.g., lasso, adaptive lasso, or elastic net). Smaller values result in closer approximation, but may also cause larger issues in optimization.)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>penalty function value
</p>

<hr>
<h2 id='.smoothCappedL1Value'>.smoothCappedL1Value</h2><span id='topic+.smoothCappedL1Value'></span>

<h3>Description</h3>

<p>smoothed version of capped L1 penalty
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.smoothCappedL1Value(parameters, tuningParameters, penaltyFunctionArguments)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".smoothCappedL1Value_+3A_parameters">parameters</code></td>
<td>
<p>vector with labeled parameter values</p>
</td></tr>
<tr><td><code id=".smoothCappedL1Value_+3A_tuningparameters">tuningParameters</code></td>
<td>
<p>list with field lambda (tuning parameter value)</p>
</td></tr>
<tr><td><code id=".smoothCappedL1Value_+3A_penaltyfunctionarguments">penaltyFunctionArguments</code></td>
<td>
<p>list with field regularizedParameterLabels (labels of regularized parameters), and eps (controls the smooth approximation of non-differential penalty functions (e.g., lasso, adaptive lasso, or elastic net). Smaller values result in closer approximation, but may also cause larger issues in optimization.)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>penalty function value
</p>

<hr>
<h2 id='.smoothElasticNetGradient'>.smoothElasticNetGradient</h2><span id='topic+.smoothElasticNetGradient'></span>

<h3>Description</h3>

<p>smoothed version of non-differentiable elastic LASSO gradient
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.smoothElasticNetGradient(
  parameters,
  tuningParameters,
  penaltyFunctionArguments
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".smoothElasticNetGradient_+3A_parameters">parameters</code></td>
<td>
<p>vector with labeled parameter values</p>
</td></tr>
<tr><td><code id=".smoothElasticNetGradient_+3A_tuningparameters">tuningParameters</code></td>
<td>
<p>list with fields lambda (tuning parameter value), alpha (0&lt;alpha&lt;1. Controls the weight of ridge and lasso terms. alpha = 1 is lasso, alpha = 0 ridge)</p>
</td></tr>
<tr><td><code id=".smoothElasticNetGradient_+3A_penaltyfunctionarguments">penaltyFunctionArguments</code></td>
<td>
<p>list with fields regularizedParameterLabels (labels of regularized parameters), and eps (controls the smooth approximation of non-differential penalty functions (e.g., lasso, adaptive lasso, or elastic net). Smaller values result in closer approximation, but may also cause larger issues in optimization.)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>gradient values
</p>

<hr>
<h2 id='.smoothElasticNetHessian'>.smoothElasticNetHessian</h2><span id='topic+.smoothElasticNetHessian'></span>

<h3>Description</h3>

<p>smoothed version of non-differentiable elastic LASSO Hessian
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.smoothElasticNetHessian(
  parameters,
  tuningParameters,
  penaltyFunctionArguments
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".smoothElasticNetHessian_+3A_parameters">parameters</code></td>
<td>
<p>vector with labeled parameter values</p>
</td></tr>
<tr><td><code id=".smoothElasticNetHessian_+3A_tuningparameters">tuningParameters</code></td>
<td>
<p>list with fields lambda (tuning parameter value), alpha (0&lt;alpha&lt;1. Controls the weight of ridge and lasso terms. alpha = 1 is lasso, alpha = 0 ridge)</p>
</td></tr>
<tr><td><code id=".smoothElasticNetHessian_+3A_penaltyfunctionarguments">penaltyFunctionArguments</code></td>
<td>
<p>list with fields regularizedParameterLabels (labels of regularized parameters), and eps (controls the smooth approximation of non-differential penalty functions (e.g., lasso, adaptive lasso, or elastic net). Smaller values result in closer approximation, but may also cause larger issues in optimization.)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Hessian matrix
</p>

<hr>
<h2 id='.smoothElasticNetValue'>.smoothElasticNetValue</h2><span id='topic+.smoothElasticNetValue'></span>

<h3>Description</h3>

<p>smoothed version of non-differentiable elastic LASSO penalty
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.smoothElasticNetValue(parameters, tuningParameters, penaltyFunctionArguments)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".smoothElasticNetValue_+3A_parameters">parameters</code></td>
<td>
<p>vector with labeled parameter values</p>
</td></tr>
<tr><td><code id=".smoothElasticNetValue_+3A_tuningparameters">tuningParameters</code></td>
<td>
<p>list with fields lambda (tuning parameter value), alpha (0&lt;alpha&lt;1. Controls the weight of ridge and lasso terms. alpha = 1 is lasso, alpha = 0 ridge)</p>
</td></tr>
<tr><td><code id=".smoothElasticNetValue_+3A_penaltyfunctionarguments">penaltyFunctionArguments</code></td>
<td>
<p>list with fields regularizedParameterLabels (labels of regularized parameters), and eps (controls the smooth approximation of non-differential penalty functions (e.g., lasso, adaptive lasso, or elastic net). Smaller values result in closer approximation, but may also cause larger issues in optimization.)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>penalty function value
</p>

<hr>
<h2 id='.smoothLASSOGradient'>.smoothLASSOGradient</h2><span id='topic+.smoothLASSOGradient'></span>

<h3>Description</h3>

<p>smoothed version of non-differentiable LASSO gradient
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.smoothLASSOGradient(parameters, tuningParameters, penaltyFunctionArguments)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".smoothLASSOGradient_+3A_parameters">parameters</code></td>
<td>
<p>vector with labeled parameter values</p>
</td></tr>
<tr><td><code id=".smoothLASSOGradient_+3A_tuningparameters">tuningParameters</code></td>
<td>
<p>list with field lambda (tuning parameter value)</p>
</td></tr>
<tr><td><code id=".smoothLASSOGradient_+3A_penaltyfunctionarguments">penaltyFunctionArguments</code></td>
<td>
<p>list with field regularizedParameterLabels (labels of regularized parameters), and eps (controls the smooth approximation of non-differential penalty functions (e.g., lasso, adaptive lasso, or elastic net). Smaller values result in closer approximation, but may also cause larger issues in optimization.)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>gradient values
</p>

<hr>
<h2 id='.smoothLASSOHessian'>.smoothLASSOHessian</h2><span id='topic+.smoothLASSOHessian'></span>

<h3>Description</h3>

<p>smoothed version of non-differentiable LASSO Hessian
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.smoothLASSOHessian(parameters, tuningParameters, penaltyFunctionArguments)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".smoothLASSOHessian_+3A_parameters">parameters</code></td>
<td>
<p>vector with labeled parameter values</p>
</td></tr>
<tr><td><code id=".smoothLASSOHessian_+3A_tuningparameters">tuningParameters</code></td>
<td>
<p>list with field lambda (tuning parameter value)</p>
</td></tr>
<tr><td><code id=".smoothLASSOHessian_+3A_penaltyfunctionarguments">penaltyFunctionArguments</code></td>
<td>
<p>list with field regularizedParameterLabels (labels of regularized parameters), and eps (controls the smooth approximation of non-differential penalty functions (e.g., lasso, adaptive lasso, or elastic net). Smaller values result in closer approximation, but may also cause larger issues in optimization.)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Hessian matrix
</p>

<hr>
<h2 id='.smoothLASSOValue'>.smoothLASSOValue</h2><span id='topic+.smoothLASSOValue'></span>

<h3>Description</h3>

<p>smoothed version of non-differentiable LASSO penalty
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.smoothLASSOValue(parameters, tuningParameters, penaltyFunctionArguments)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".smoothLASSOValue_+3A_parameters">parameters</code></td>
<td>
<p>vector with labeled parameter values</p>
</td></tr>
<tr><td><code id=".smoothLASSOValue_+3A_tuningparameters">tuningParameters</code></td>
<td>
<p>list with field lambda (tuning parameter value)</p>
</td></tr>
<tr><td><code id=".smoothLASSOValue_+3A_penaltyfunctionarguments">penaltyFunctionArguments</code></td>
<td>
<p>list with field regularizedParameterLabels (labels of regularized parameters), and eps (controls the smooth approximation of non-differential penalty functions (e.g., lasso, adaptive lasso, or elastic net). Smaller values result in closer approximation, but may also cause larger issues in optimization.)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>penalty function value
</p>

<hr>
<h2 id='.smoothLspValue'>.smoothLspValue</h2><span id='topic+.smoothLspValue'></span>

<h3>Description</h3>

<p>smoothed version of lsp penalty
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.smoothLspValue(parameters, tuningParameters, penaltyFunctionArguments)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".smoothLspValue_+3A_parameters">parameters</code></td>
<td>
<p>vector with labeled parameter values</p>
</td></tr>
<tr><td><code id=".smoothLspValue_+3A_tuningparameters">tuningParameters</code></td>
<td>
<p>list with field lambda (tuning parameter value)</p>
</td></tr>
<tr><td><code id=".smoothLspValue_+3A_penaltyfunctionarguments">penaltyFunctionArguments</code></td>
<td>
<p>list with field regularizedParameterLabels (labels of regularized parameters), and eps (controls the smooth approximation of non-differential penalty functions (e.g., lasso, adaptive lasso, or elastic net). Smaller values result in closer approximation, but may also cause larger issues in optimization.)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>penalty function value
</p>

<hr>
<h2 id='.smoothMcpValue'>.smoothMcpValue</h2><span id='topic+.smoothMcpValue'></span>

<h3>Description</h3>

<p>smoothed version of mcp penalty
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.smoothMcpValue(parameters, tuningParameters, penaltyFunctionArguments)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".smoothMcpValue_+3A_parameters">parameters</code></td>
<td>
<p>vector with labeled parameter values</p>
</td></tr>
<tr><td><code id=".smoothMcpValue_+3A_tuningparameters">tuningParameters</code></td>
<td>
<p>list with field lambda (tuning parameter value)</p>
</td></tr>
<tr><td><code id=".smoothMcpValue_+3A_penaltyfunctionarguments">penaltyFunctionArguments</code></td>
<td>
<p>list with field regularizedParameterLabels (labels of regularized parameters), and eps (controls the smooth approximation of non-differential penalty functions (e.g., lasso, adaptive lasso, or elastic net). Smaller values result in closer approximation, but may also cause larger issues in optimization.)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>penalty function value
</p>

<hr>
<h2 id='.smoothScadValue'>.smoothScadValue</h2><span id='topic+.smoothScadValue'></span>

<h3>Description</h3>

<p>smoothed version of scad penalty
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.smoothScadValue(parameters, tuningParameters, penaltyFunctionArguments)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".smoothScadValue_+3A_parameters">parameters</code></td>
<td>
<p>vector with labeled parameter values</p>
</td></tr>
<tr><td><code id=".smoothScadValue_+3A_tuningparameters">tuningParameters</code></td>
<td>
<p>list with field lambda (tuning parameter value)</p>
</td></tr>
<tr><td><code id=".smoothScadValue_+3A_penaltyfunctionarguments">penaltyFunctionArguments</code></td>
<td>
<p>list with field regularizedParameterLabels (labels of regularized parameters), and eps (controls the smooth approximation of non-differential penalty functions (e.g., lasso, adaptive lasso, or elastic net). Smaller values result in closer approximation, but may also cause larger issues in optimization.)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>penalty function value
</p>

<hr>
<h2 id='.standardErrors'>.standardErrors</h2><span id='topic+.standardErrors'></span>

<h3>Description</h3>

<p>compute the standard errors of a fitted SEM. IMPORTANT: Assumes that the
SEM has been fitted and the parameter estimates are at the ordinary maximum
likelihood estimates
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.standardErrors(SEM, raw)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".standardErrors_+3A_sem">SEM</code></td>
<td>
<p>model of class Rcpp_SEMCpp.</p>
</td></tr>
<tr><td><code id=".standardErrors_+3A_raw">raw</code></td>
<td>
<p>controls if the internal transformations of lessSEM is used. If
set to TRUE, the standard errors will be returned for the internally used
parameter specification</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector with standard errors
</p>

<hr>
<h2 id='.updateLavaan'>.updateLavaan</h2><span id='topic+.updateLavaan'></span>

<h3>Description</h3>

<p>updates a lavaan model. lavaan has an update function that does exactly that,
but it seems to not work with testthat. This is an attempt to hack around the
issue...
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.updateLavaan(lavaanModel, key, value)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".updateLavaan_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>fitted lavaan model</p>
</td></tr>
<tr><td><code id=".updateLavaan_+3A_key">key</code></td>
<td>
<p>label of the element that should be updated</p>
</td></tr>
<tr><td><code id=".updateLavaan_+3A_value">value</code></td>
<td>
<p>new value for the updated element</p>
</td></tr>
</table>


<h3>Value</h3>

<p>lavaan model
</p>

<hr>
<h2 id='.useElasticNet'>.useElasticNet</h2><span id='topic+.useElasticNet'></span>

<h3>Description</h3>

<p>Internal function checking if elastic net is used
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.useElasticNet(mixedPenalty)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".useElasticNet_+3A_mixedpenalty">mixedPenalty</code></td>
<td>
<p>object of class mixedPenalty. This object can be created
with the mixedPenalty function. Penalties can be added with the addCappedL1,
addLasso, addLsp, addMcp, and addScad functions.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>TRUE if elastic net, FALSE otherwise
</p>

<hr>
<h2 id='adaptiveLasso'>adaptiveLasso</h2><span id='topic+adaptiveLasso'></span>

<h3>Description</h3>

<p>Implements adaptive lasso regularization for structural equation models.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = p( x_j) = \frac{1}{w_j}\lambda| x_j|</code>
</p>

<p>Adaptive lasso regularization will set parameters to zero if <code class="reqn">\lambda</code>
is large enough.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adaptiveLasso(
  lavaanModel,
  regularized,
  weights = NULL,
  lambdas = NULL,
  nLambdas = NULL,
  reverse = TRUE,
  curve = 1,
  method = "glmnet",
  modifyModel = lessSEM::modifyModel(),
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adaptiveLasso_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="adaptiveLasso_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="adaptiveLasso_+3A_weights">weights</code></td>
<td>
<p>labeled vector with weights for each of the parameters in the
model. If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object. If set to NULL,
the default weights will be used: the inverse of the absolute values of
the unregularized parameter estimates</p>
</td></tr>
<tr><td><code id="adaptiveLasso_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="adaptiveLasso_+3A_nlambdas">nLambdas</code></td>
<td>
<p>alternative to lambda: If alpha = 1, lessSEM can automatically
compute the first lambda value which sets all regularized parameters to zero.
It will then generate nLambda values between 0 and the computed lambda.</p>
</td></tr>
<tr><td><code id="adaptiveLasso_+3A_reverse">reverse</code></td>
<td>
<p>if set to TRUE and nLambdas is used, lessSEM will start with the
largest lambda and gradually decrease lambda. Otherwise, lessSEM will start with
the smallest lambda and gradually increase it.</p>
</td></tr>
<tr><td><code id="adaptiveLasso_+3A_curve">curve</code></td>
<td>
<p>Allows for unequally spaced lambda steps (e.g., .01,.02,.05,1,5,20).
If curve is close to 1 all lambda values will be equally spaced, if curve is large
lambda values will be more concentrated close to 0. See ?lessSEM::curveLambda for more information.</p>
</td></tr>
<tr><td><code id="adaptiveLasso_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet. With ista, the control argument can be used to switch to related procedures
(currently gist).</p>
</td></tr>
<tr><td><code id="adaptiveLasso_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="adaptiveLasso_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currently,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>Adaptive lasso regularization:
</p>

<ul>
<li><p> Zou, H. (2006). The adaptive lasso and its oracle properties. Journal of the American Statistical Association,
101(476), 1418–1429. https://doi.org/10.1198/016214506000000735
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Model of class regularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

lsem &lt;- adaptiveLasso(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = paste0("l", 6:15),
  # in case of lasso and adaptive lasso, we can specify the number of lambda
  # values to use. lessSEM will automatically find lambda_max and fit
  # models for nLambda values between 0 and lambda_max. For the other
  # penalty functions, lambdas must be specified explicitly
  nLambdas = 50)

# use the plot-function to plot the regularized parameters:
plot(lsem)

# the coefficients can be accessed with:
coef(lsem)
# if you are only interested in the estimates and not the tuning parameters, use
coef(lsem)@estimates
# or
estimates(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters[1,]

# fit Measures:
fitIndices(lsem)

# The best parameters can also be extracted with:
coef(lsem, criterion = "AIC")
# or
estimates(lsem, criterion = "AIC")

#### Advanced ###
# Switching the optimizer #
# Use the "method" argument to switch the optimizer. The control argument
# must also be changed to the corresponding function:
lsemIsta &lt;- adaptiveLasso(
  lavaanModel = lavaanModel,
  regularized = paste0("l", 6:15),
  nLambdas = 50,
  method = "ista",
  control = controlIsta())

# Note: The results are basically identical:
lsemIsta@parameters - lsem@parameters
</code></pre>

<hr>
<h2 id='addCappedL1'>addCappedL1</h2><span id='topic+addCappedL1'></span>

<h3>Description</h3>

<p>Implements cappedL1 regularization for structural equation models.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \lambda \min(| x_j|, \theta)</code>
</p>

<p>where <code class="reqn">\theta &gt; 0</code>. The cappedL1 penalty is identical to the lasso for
parameters which are below <code class="reqn">\theta</code> and identical to a constant for parameters
above <code class="reqn">\theta</code>. As adding a constant to the fitting function will not change its
minimum, larger parameters can stay unregularized while smaller ones are set to zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addCappedL1(mixedPenalty, regularized, lambdas, thetas)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="addCappedL1_+3A_mixedpenalty">mixedPenalty</code></td>
<td>
<p>model of class mixedPenalty created with the mixedPenalty function (see ?mixedPenalty)</p>
</td></tr>
<tr><td><code id="addCappedL1_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="addCappedL1_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="addCappedL1_+3A_thetas">thetas</code></td>
<td>
<p>parameters whose absolute value is above this threshold will be penalized with
a constant (theta)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currently,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>CappedL1 regularization:
</p>

<ul>
<li><p> Zhang, T. (2010). Analysis of Multi-stage Convex Relaxation for Sparse Regularization.
Journal of Machine Learning Research, 11, 1081–1107.
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Model of class mixedPenalty. Use the fit() - function to fit the model
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 + 
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 + 
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# We can add mixed penalties as follows:

regularized &lt;- lavaanModel |&gt;
  # create template for regularized model with mixed penalty:
  mixedPenalty() |&gt;
  # add penalty on loadings l6 - l10:
  addCappedL1(regularized = paste0("l", 11:15), 
          lambdas = seq(0,1,.1),
          thetas = 2.3) |&gt;
  # fit the model:
  fit()
</code></pre>

<hr>
<h2 id='addElasticNet'>addElasticNet</h2><span id='topic+addElasticNet'></span>

<h3>Description</h3>

<p>Adds an elastic net penalty to specified parameters.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \alpha\lambda|x_j| + (1-\alpha)\lambda x_j^2</code>
</p>

<p>Note that the elastic net combines ridge and lasso regularization. If <code class="reqn">\alpha = 0</code>,
the elastic net reduces to ridge regularization. If <code class="reqn">\alpha = 1</code> it reduces
to lasso regularization. In between, elastic net is a compromise between the shrinkage of
the lasso and the ridge penalty.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addElasticNet(mixedPenalty, regularized, alphas, lambdas, weights = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="addElasticNet_+3A_mixedpenalty">mixedPenalty</code></td>
<td>
<p>model of class mixedPenalty created with the mixedPenalty function (see ?mixedPenalty)</p>
</td></tr>
<tr><td><code id="addElasticNet_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="addElasticNet_+3A_alphas">alphas</code></td>
<td>
<p>numeric vector: values for the tuning parameter alpha. Set to 1 for lasso and
to zero for ridge. Anything in between is an elastic net penalty.</p>
</td></tr>
<tr><td><code id="addElasticNet_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="addElasticNet_+3A_weights">weights</code></td>
<td>
<p>can be used to give different weights to the different parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currently,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>Elastic net regularization:
</p>

<ul>
<li><p> Zou, H., &amp; Hastie, T. (2005). Regularization and variable selection via the elastic net.
Journal of the Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Model of class mixedPenalty. Use the fit() - function to fit the model
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 + 
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 + 
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# We can add mixed penalties as follows:

regularized &lt;- lavaanModel |&gt;
  # create template for regularized model with mixed penalty:
  mixedPenalty() |&gt;
  # add penalty on loadings l6 - l10:
  addElasticNet(regularized = paste0("l", 11:15), 
          lambdas = seq(0,1,.1),
          alphas = .4) |&gt;
  # fit the model:
  fit()
</code></pre>

<hr>
<h2 id='addLasso'>addLasso</h2><span id='topic+addLasso'></span>

<h3>Description</h3>

<p>Implements lasso regularization for structural equation models.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \lambda |x_j|</code>
</p>

<p>Lasso regularization will set parameters to zero if <code class="reqn">\lambda</code> is large enough
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addLasso(mixedPenalty, regularized, weights = 1, lambdas)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="addLasso_+3A_mixedpenalty">mixedPenalty</code></td>
<td>
<p>model of class mixedPenalty created with the mixedPenalty function (see ?mixedPenalty)</p>
</td></tr>
<tr><td><code id="addLasso_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="addLasso_+3A_weights">weights</code></td>
<td>
<p>can be used to give different weights to the different parameters</p>
</td></tr>
<tr><td><code id="addLasso_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currently,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>Lasso regularization:
</p>

<ul>
<li><p> Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), 58(1), 267–288.
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Model of class mixedPenalty. Use the fit() - function to fit the model
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 + 
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 + 
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# We can add mixed penalties as follows:

regularized &lt;- lavaanModel |&gt;
  # create template for regularized model with mixed penalty:
  mixedPenalty() |&gt;
  # add penalty on loadings l6 - l10:
  addLasso(regularized = paste0("l", 11:15), 
          lambdas = seq(0,1,.1)) |&gt;
  # fit the model:
  fit()
</code></pre>

<hr>
<h2 id='addLsp'>addLsp</h2><span id='topic+addLsp'></span>

<h3>Description</h3>

<p>Implements lsp regularization for structural equation models.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \lambda \log(1 + |x_j|/\theta)</code>
</p>

<p>where <code class="reqn">\theta &gt; 0</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addLsp(mixedPenalty, regularized, lambdas, thetas)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="addLsp_+3A_mixedpenalty">mixedPenalty</code></td>
<td>
<p>model of class mixedPenalty created with the mixedPenalty function (see ?mixedPenalty)</p>
</td></tr>
<tr><td><code id="addLsp_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="addLsp_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="addLsp_+3A_thetas">thetas</code></td>
<td>
<p>parameters whose absolute value is above this threshold will be penalized with
a constant (theta)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currently,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>lsp regularization:
</p>

<ul>
<li><p> Candès, E. J., Wakin, M. B., &amp; Boyd, S. P. (2008). Enhancing Sparsity by
Reweighted l1 Minimization. Journal of Fourier Analysis and Applications, 14(5–6),
877–905. https://doi.org/10.1007/s00041-008-9045-x
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Model of class mixedPenalty. Use the fit() - function to fit the model
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 + 
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 + 
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# We can add mixed penalties as follows:

regularized &lt;- lavaanModel |&gt;
  # create template for regularized model with mixed penalty:
  mixedPenalty() |&gt;
  # add penalty on loadings l6 - l10:
  addLsp(regularized = paste0("l", 11:15), 
          lambdas = seq(0,1,.1),
          thetas = 2.3) |&gt;
  # fit the model:
  fit()
</code></pre>

<hr>
<h2 id='addMcp'>addMcp</h2><span id='topic+addMcp'></span>

<h3>Description</h3>

<p>Implements mcp regularization for structural equation models.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \begin{cases}
\lambda |x_j| - x_j^2/(2\theta) &amp; \text{if } |x_j| \leq \theta\lambda\\
\theta\lambda^2/2 &amp; \text{if } |x_j| &gt; \lambda\theta
\end{cases}</code>
</p>
<p> where <code class="reqn">\theta &gt; 0</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addMcp(mixedPenalty, regularized, lambdas, thetas)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="addMcp_+3A_mixedpenalty">mixedPenalty</code></td>
<td>
<p>model of class mixedPenalty created with the mixedPenalty function (see ?mixedPenalty)</p>
</td></tr>
<tr><td><code id="addMcp_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="addMcp_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="addMcp_+3A_thetas">thetas</code></td>
<td>
<p>parameters whose absolute value is above this threshold will be penalized with
a constant (theta)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currently,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>mcp regularization:
</p>

<ul>
<li><p> Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty.
The Annals of Statistics, 38(2), 894–942. https://doi.org/10.1214/09-AOS729
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Model of class mixedPenalty. Use the fit() - function to fit the model
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 + 
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 + 
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# We can add mixed penalties as follows:

regularized &lt;- lavaanModel |&gt;
  # create template for regularized model with mixed penalty:
  mixedPenalty() |&gt;
  # add penalty on loadings l6 - l10:
  addMcp(regularized = paste0("l", 11:15), 
          lambdas = seq(0,1,.1),
          thetas = 2.3) |&gt;
  # fit the model:
  fit()
</code></pre>

<hr>
<h2 id='addScad'>addScad</h2><span id='topic+addScad'></span>

<h3>Description</h3>

<p>Implements scad regularization for structural equation models.
The penalty function is given by:

</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \begin{cases}
\lambda |x_j| &amp; \text{if } |x_j| \leq \theta\\
\frac{-x_j^2 + 2\theta\lambda |x_j| - \lambda^2}{2(\theta -1)} &amp; 
\text{if } \lambda &lt; |x_j| \leq \lambda\theta \\
(\theta + 1) \lambda^2/2 &amp; \text{if } |x_j| \geq \theta\lambda\\
\end{cases}</code>
</p>

<p>where <code class="reqn">\theta &gt; 2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addScad(mixedPenalty, regularized, lambdas, thetas)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="addScad_+3A_mixedpenalty">mixedPenalty</code></td>
<td>
<p>model of class mixedPenalty created with the mixedPenalty function (see ?mixedPenalty)</p>
</td></tr>
<tr><td><code id="addScad_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="addScad_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="addScad_+3A_thetas">thetas</code></td>
<td>
<p>parameters whose absolute value is above this threshold will be penalized with
a constant (theta)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currently,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>scad regularization:
</p>

<ul>
<li><p> Fan, J., &amp; Li, R. (2001). Variable selection via nonconcave penalized
likelihood and its oracle properties. Journal of the American Statistical Association,
96(456), 1348–1360. https://doi.org/10.1198/016214501753382273
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Model of class mixedPenalty. Use the fit() - function to fit the model
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 + 
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 + 
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# We can add mixed penalties as follows:

regularized &lt;- lavaanModel |&gt;
  # create template for regularized model with mixed penalty:
  mixedPenalty() |&gt;
  # add penalty on loadings l6 - l10:
  addScad(regularized = paste0("l", 11:15), 
          lambdas = seq(0,1,.1),
          thetas = 3.1) |&gt;
  # fit the model:
  fit()
</code></pre>

<hr>
<h2 id='AIC+2CgpRegularized-method'>AIC</h2><span id='topic+AIC+2CgpRegularized-method'></span>

<h3>Description</h3>

<p>returns the AIC
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'gpRegularized'
AIC(object, ..., k = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AIC+2B2CgpRegularized-method_+3A_object">object</code></td>
<td>
<p>object of class gpRegularized</p>
</td></tr>
<tr><td><code id="AIC+2B2CgpRegularized-method_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
<tr><td><code id="AIC+2B2CgpRegularized-method_+3A_k">k</code></td>
<td>
<p>multiplier for number of parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data frame with fit values, appended with AIC
</p>

<hr>
<h2 id='AIC+2CRcpp_mgSEM-method'>AIC</h2><span id='topic+AIC+2CRcpp_mgSEM-method'></span>

<h3>Description</h3>

<p>AIC
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'Rcpp_mgSEM'
AIC(object, ..., k = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AIC+2B2CRcpp_mgSEM-method_+3A_object">object</code></td>
<td>
<p>object of class Rcpp_mgSEM</p>
</td></tr>
<tr><td><code id="AIC+2B2CRcpp_mgSEM-method_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
<tr><td><code id="AIC+2B2CRcpp_mgSEM-method_+3A_k">k</code></td>
<td>
<p>multiplier for number of parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>AIC values
</p>

<hr>
<h2 id='AIC+2CRcpp_SEMCpp-method'>AIC</h2><span id='topic+AIC+2CRcpp_SEMCpp-method'></span>

<h3>Description</h3>

<p>AIC
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'Rcpp_SEMCpp'
AIC(object, ..., k = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AIC+2B2CRcpp_SEMCpp-method_+3A_object">object</code></td>
<td>
<p>object of class Rcpp_SEMCpp</p>
</td></tr>
<tr><td><code id="AIC+2B2CRcpp_SEMCpp-method_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
<tr><td><code id="AIC+2B2CRcpp_SEMCpp-method_+3A_k">k</code></td>
<td>
<p>multiplier for number of parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>AIC values
</p>

<hr>
<h2 id='AIC+2CregularizedSEM-method'>AIC</h2><span id='topic+AIC+2CregularizedSEM-method'></span>

<h3>Description</h3>

<p>returns the AIC
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'regularizedSEM'
AIC(object, ..., k = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AIC+2B2CregularizedSEM-method_+3A_object">object</code></td>
<td>
<p>object of class regularizedSEM</p>
</td></tr>
<tr><td><code id="AIC+2B2CregularizedSEM-method_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
<tr><td><code id="AIC+2B2CregularizedSEM-method_+3A_k">k</code></td>
<td>
<p>multiplier for number of parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>AIC values
</p>

<hr>
<h2 id='AIC+2CregularizedSEMMixedPenalty-method'>AIC</h2><span id='topic+AIC+2CregularizedSEMMixedPenalty-method'></span>

<h3>Description</h3>

<p>returns the AIC
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'regularizedSEMMixedPenalty'
AIC(object, ..., k = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AIC+2B2CregularizedSEMMixedPenalty-method_+3A_object">object</code></td>
<td>
<p>object of class regularizedSEMMixedPenalty</p>
</td></tr>
<tr><td><code id="AIC+2B2CregularizedSEMMixedPenalty-method_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
<tr><td><code id="AIC+2B2CregularizedSEMMixedPenalty-method_+3A_k">k</code></td>
<td>
<p>multiplier for number of parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>AIC values
</p>

<hr>
<h2 id='AIC+2CregularizedSEMWithCustomPenalty-method'>AIC</h2><span id='topic+AIC+2CregularizedSEMWithCustomPenalty-method'></span>

<h3>Description</h3>

<p>returns the AIC. Expects penalizedParameterLabels and zeroThreshold
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'regularizedSEMWithCustomPenalty'
AIC(object, ..., k = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AIC+2B2CregularizedSEMWithCustomPenalty-method_+3A_object">object</code></td>
<td>
<p>object of class regularizedSEMWithCustomPenalty</p>
</td></tr>
<tr><td><code id="AIC+2B2CregularizedSEMWithCustomPenalty-method_+3A_...">...</code></td>
<td>
<p>Expects penalizedParameterLabels and zeroThreshold.
penalizedParameterLabels: vector with labels of penalized parameters.
zeroThreshold: penalized parameters below this threshold will be counted as zeroed.</p>
</td></tr>
<tr><td><code id="AIC+2B2CregularizedSEMWithCustomPenalty-method_+3A_k">k</code></td>
<td>
<p>multiplier for number of parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>AIC values
</p>

<hr>
<h2 id='bfgs'>bfgs</h2><span id='topic+bfgs'></span>

<h3>Description</h3>

<p>This function allows for optimizing models built in lavaan using the BFGS optimizer
implemented in lessSEM. Its elements can be accessed
with the &quot;@&quot; operator (see examples). The main purpose is to make transformations
of lavaan models more accessible.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bfgs(
  lavaanModel,
  modifyModel = lessSEM::modifyModel(),
  control = lessSEM::controlBFGS()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bfgs_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="bfgs_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="bfgs_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. See ?controlBFGS for more details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Model of class regularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)


lsem &lt;- bfgs(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel)

# the coefficients can be accessed with:
coef(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters
</code></pre>

<hr>
<h2 id='bfgsEnet'>smoothly approximated elastic net</h2><span id='topic+bfgsEnet'></span>

<h3>Description</h3>

<p>Object for smoothly approximated elastic net optimization with
bfgs optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (1) a vector with weights for each
parameter and (2) a list with control elements</p>
</dd>
<dt><code>setHessian</code></dt><dd><p>changes the Hessian of the model. Expects a matrix</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a lambda and an alpha value.</p>
</dd>
</dl>

<hr>
<h2 id='bfgsEnetMgSEM'>smoothly approximated elastic net</h2><span id='topic+bfgsEnetMgSEM'></span>

<h3>Description</h3>

<p>Object for smoothly approximated elastic net optimization with
bfgs optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (1) a vector with weights for each
parameter and (2) a list with control elements</p>
</dd>
<dt><code>setHessian</code></dt><dd><p>changes the Hessian of the model. Expects a matrix</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a lambda and an alpha value.</p>
</dd>
</dl>

<hr>
<h2 id='bfgsEnetSEM'>smoothly approximated elastic net</h2><span id='topic+bfgsEnetSEM'></span>

<h3>Description</h3>

<p>Object for smoothly approximated elastic net optimization with
bfgs optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (1) a vector with weights for each
parameter and (2) a list with control elements</p>
</dd>
<dt><code>setHessian</code></dt><dd><p>changes the Hessian of the model. Expects a matrix</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a lambda and an alpha value.</p>
</dd>
</dl>

<hr>
<h2 id='BIC+2CgpRegularized-method'>BIC</h2><span id='topic+BIC+2CgpRegularized-method'></span>

<h3>Description</h3>

<p>returns the BIC
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'gpRegularized'
BIC(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BIC+2B2CgpRegularized-method_+3A_object">object</code></td>
<td>
<p>object of class gpRegularized</p>
</td></tr>
<tr><td><code id="BIC+2B2CgpRegularized-method_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data frame with fit values, appended with BIC
</p>

<hr>
<h2 id='BIC+2CRcpp_mgSEM-method'>BIC</h2><span id='topic+BIC+2CRcpp_mgSEM-method'></span>

<h3>Description</h3>

<p>BIC
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'Rcpp_mgSEM'
BIC(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BIC+2B2CRcpp_mgSEM-method_+3A_object">object</code></td>
<td>
<p>object of class Rcpp_mgSEM</p>
</td></tr>
<tr><td><code id="BIC+2B2CRcpp_mgSEM-method_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>BIC values
</p>

<hr>
<h2 id='BIC+2CRcpp_SEMCpp-method'>BIC</h2><span id='topic+BIC+2CRcpp_SEMCpp-method'></span>

<h3>Description</h3>

<p>BIC
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'Rcpp_SEMCpp'
BIC(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BIC+2B2CRcpp_SEMCpp-method_+3A_object">object</code></td>
<td>
<p>object of class Rcpp_SEMCpp</p>
</td></tr>
<tr><td><code id="BIC+2B2CRcpp_SEMCpp-method_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>BIC values
</p>

<hr>
<h2 id='BIC+2CregularizedSEM-method'>BIC</h2><span id='topic+BIC+2CregularizedSEM-method'></span>

<h3>Description</h3>

<p>returns the BIC
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'regularizedSEM'
BIC(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BIC+2B2CregularizedSEM-method_+3A_object">object</code></td>
<td>
<p>object of class regularizedSEM</p>
</td></tr>
<tr><td><code id="BIC+2B2CregularizedSEM-method_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>BIC values
</p>

<hr>
<h2 id='BIC+2CregularizedSEMMixedPenalty-method'>BIC</h2><span id='topic+BIC+2CregularizedSEMMixedPenalty-method'></span>

<h3>Description</h3>

<p>returns the BIC
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'regularizedSEMMixedPenalty'
BIC(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BIC+2B2CregularizedSEMMixedPenalty-method_+3A_object">object</code></td>
<td>
<p>object of class regularizedSEMMixedPenalty</p>
</td></tr>
<tr><td><code id="BIC+2B2CregularizedSEMMixedPenalty-method_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>BIC values
</p>

<hr>
<h2 id='BIC+2CregularizedSEMWithCustomPenalty-method'>BIC</h2><span id='topic+BIC+2CregularizedSEMWithCustomPenalty-method'></span>

<h3>Description</h3>

<p>returns the BIC
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'regularizedSEMWithCustomPenalty'
BIC(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BIC+2B2CregularizedSEMWithCustomPenalty-method_+3A_object">object</code></td>
<td>
<p>object of class regularizedSEMWithCustomPenalty</p>
</td></tr>
<tr><td><code id="BIC+2B2CregularizedSEMWithCustomPenalty-method_+3A_...">...</code></td>
<td>
<p>Expects penalizedParameterLabels and zeroThreshold.
penalizedParameterLabels: vector with labels of penalized parameters.
zeroThreshold: penalized parameters below this threshold will be counted as zeroed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>BIC values
</p>

<hr>
<h2 id='callFitFunction'>callFitFunction</h2><span id='topic+callFitFunction'></span>

<h3>Description</h3>

<p>wrapper to call user defined fit function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>callFitFunction(fitFunctionSEXP, parameters, userSuppliedElements)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="callFitFunction_+3A_fitfunctionsexp">fitFunctionSEXP</code></td>
<td>
<p>pointer to fit function</p>
</td></tr>
<tr><td><code id="callFitFunction_+3A_parameters">parameters</code></td>
<td>
<p>vector with parameter values</p>
</td></tr>
<tr><td><code id="callFitFunction_+3A_usersuppliedelements">userSuppliedElements</code></td>
<td>
<p>list with additional elements</p>
</td></tr>
</table>


<h3>Value</h3>

<p>fit value (double)
</p>

<hr>
<h2 id='cappedL1'>cappedL1</h2><span id='topic+cappedL1'></span>

<h3>Description</h3>

<p>Implements cappedL1 regularization for structural equation models.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \lambda \min(| x_j|, \theta)</code>
</p>

<p>where <code class="reqn">\theta &gt; 0</code>. The cappedL1 penalty is identical to the lasso for
parameters which are below <code class="reqn">\theta</code> and identical to a constant for parameters
above <code class="reqn">\theta</code>. As adding a constant to the fitting function will not change its
minimum, larger parameters can stay unregularized while smaller ones are set to zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cappedL1(
  lavaanModel,
  regularized,
  lambdas,
  thetas,
  modifyModel = lessSEM::modifyModel(),
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cappedL1_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="cappedL1_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="cappedL1_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="cappedL1_+3A_thetas">thetas</code></td>
<td>
<p>parameters whose absolute value is above this threshold will be penalized with
a constant (theta)</p>
</td></tr>
<tr><td><code id="cappedL1_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="cappedL1_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet. With ista, the control argument can be used to switch to related procedures</p>
</td></tr>
<tr><td><code id="cappedL1_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta (see ?controlIsta)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currently,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>CappedL1 regularization:
</p>

<ul>
<li><p> Zhang, T. (2010). Analysis of Multi-stage Convex Relaxation for Sparse Regularization.
Journal of Machine Learning Research, 11, 1081–1107.
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Model of class regularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

lsem &lt;- cappedL1(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = paste0("l", 6:15),
  lambdas = seq(0,1,length.out = 20),
  thetas = seq(0.01,2,length.out = 5))

# the coefficients can be accessed with:
coef(lsem)
# if you are only interested in the estimates and not the tuning parameters, use
coef(lsem)@estimates
# or
estimates(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters[1,]

# fit Measures:
fitIndices(lsem)

# The best parameters can also be extracted with:
coef(lsem, criterion = "AIC")
# or
estimates(lsem, criterion = "AIC")

# optional: plotting the paths requires installation of plotly
# plot(lsem)
</code></pre>

<hr>
<h2 id='coef+2CcvRegularizedSEM-method'>coef</h2><span id='topic+coef+2CcvRegularizedSEM-method'></span>

<h3>Description</h3>

<p>Returns the parameter estimates of an cvRegularizedSEM
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'cvRegularizedSEM'
coef(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef+2B2CcvRegularizedSEM-method_+3A_object">object</code></td>
<td>
<p>object of class cvRegularizedSEM</p>
</td></tr>
<tr><td><code id="coef+2B2CcvRegularizedSEM-method_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the parameter estimates of an cvRegularizedSEM
</p>

<hr>
<h2 id='coef+2CgpRegularized-method'>coef</h2><span id='topic+coef+2CgpRegularized-method'></span>

<h3>Description</h3>

<p>Returns the parameter estimates of a gpRegularized
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'gpRegularized'
coef(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef+2B2CgpRegularized-method_+3A_object">object</code></td>
<td>
<p>object of class gpRegularized</p>
</td></tr>
<tr><td><code id="coef+2B2CgpRegularized-method_+3A_...">...</code></td>
<td>
<p>criterion can be one of: &quot;AIC&quot;, &quot;BIC&quot;. If set to NULL, all parameters will be returned</p>
</td></tr>
</table>


<h3>Value</h3>

<p>parameter estimates
</p>

<hr>
<h2 id='coef+2CRcpp_mgSEM-method'>coef</h2><span id='topic+coef+2CRcpp_mgSEM-method'></span>

<h3>Description</h3>

<p>coef
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'Rcpp_mgSEM'
coef(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef+2B2CRcpp_mgSEM-method_+3A_object">object</code></td>
<td>
<p>object of class Rcpp_mgSEM</p>
</td></tr>
<tr><td><code id="coef+2B2CRcpp_mgSEM-method_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>all coefficients of the model in transformed form
</p>

<hr>
<h2 id='coef+2CRcpp_SEMCpp-method'>coef</h2><span id='topic+coef+2CRcpp_SEMCpp-method'></span>

<h3>Description</h3>

<p>coef
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'Rcpp_SEMCpp'
coef(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef+2B2CRcpp_SEMCpp-method_+3A_object">object</code></td>
<td>
<p>object of class Rcpp_SEMCpp</p>
</td></tr>
<tr><td><code id="coef+2B2CRcpp_SEMCpp-method_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>all coefficients of the model in transformed form
</p>

<hr>
<h2 id='coef+2CregularizedSEM-method'>coef</h2><span id='topic+coef+2CregularizedSEM-method'></span>

<h3>Description</h3>

<p>Returns the parameter estimates of a regularizedSEM
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'regularizedSEM'
coef(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef+2B2CregularizedSEM-method_+3A_object">object</code></td>
<td>
<p>object of class regularizedSEM</p>
</td></tr>
<tr><td><code id="coef+2B2CregularizedSEM-method_+3A_...">...</code></td>
<td>
<p>criterion can be one of the ones returned by fitIndices. If set to NULL, all parameters will be returned</p>
</td></tr>
</table>


<h3>Value</h3>

<p>parameters of the model as data.frame
</p>

<hr>
<h2 id='coef+2CregularizedSEMMixedPenalty-method'>coef</h2><span id='topic+coef+2CregularizedSEMMixedPenalty-method'></span>

<h3>Description</h3>

<p>Returns the parameter estimates of a regularizedSEMMixedPenalty
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'regularizedSEMMixedPenalty'
coef(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef+2B2CregularizedSEMMixedPenalty-method_+3A_object">object</code></td>
<td>
<p>object of class regularizedSEMMixedPenalty</p>
</td></tr>
<tr><td><code id="coef+2B2CregularizedSEMMixedPenalty-method_+3A_...">...</code></td>
<td>
<p>criterion can be one of: &quot;AIC&quot;, &quot;BIC&quot;. If set to NULL, all parameters will be returned</p>
</td></tr>
</table>


<h3>Value</h3>

<p>parameters of the model as data.frame
</p>

<hr>
<h2 id='coef+2CregularizedSEMWithCustomPenalty-method'>coef</h2><span id='topic+coef+2CregularizedSEMWithCustomPenalty-method'></span>

<h3>Description</h3>

<p>Returns the parameter estimates of a regularizedSEMWithCustomPenalty
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'regularizedSEMWithCustomPenalty'
coef(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef+2B2CregularizedSEMWithCustomPenalty-method_+3A_object">object</code></td>
<td>
<p>object of class regularizedSEMWithCustomPenalty</p>
</td></tr>
<tr><td><code id="coef+2B2CregularizedSEMWithCustomPenalty-method_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame with all parameter estimates
</p>

<hr>
<h2 id='controlBFGS'>controlBFGS</h2><span id='topic+controlBFGS'></span>

<h3>Description</h3>

<p>Control the BFGS optimizer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>controlBFGS(
  startingValues = "est",
  initialHessian = ifelse(all(startingValues == "est"), "lavaan", "compute"),
  saveDetails = FALSE,
  stepSize = 0.9,
  sigma = 1e-05,
  gamma = 0,
  maxIterOut = 1000,
  maxIterIn = 1000,
  maxIterLine = 500,
  breakOuter = 1e-08,
  breakInner = 1e-10,
  convergenceCriterion = 0,
  verbose = 0,
  nCores = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="controlBFGS_+3A_startingvalues">startingValues</code></td>
<td>
<p>option to provide initial starting values. Only used for the first lambda. Three options are supported. Setting to &quot;est&quot; will use the estimates
from the lavaan model object. Setting to &quot;start&quot; will use the starting values of the lavaan model. Finally, a labeled vector with parameter
values can be passed to the function which will then be used as starting values.</p>
</td></tr>
<tr><td><code id="controlBFGS_+3A_initialhessian">initialHessian</code></td>
<td>
<p>option to provide an initial Hessian to the optimizer. Must have row and column names corresponding to the parameter labels. use getLavaanParameters(lavaanModel) to
see those labels. If set to &quot;gradNorm&quot;, the maximum of the gradients at the starting
values times the stepSize will be used. This is adapted from Optim.jl
https://github.com/JuliaNLSolvers/Optim.jl/blob/f43e6084aacf2dabb2b142952acd3fbb0e268439/src/multivariate/solvers/first_order/bfgs.jl#L104
If set to a single value, a diagonal matrix with the single value along the diagonal will be used.
The default is &quot;lavaan&quot; which extracts the Hessian from the lavaanModel. This Hessian
will typically deviate from that of the internal SEM represenation of lessSEM (due to
the transformation of the variances), but works quite well in practice.</p>
</td></tr>
<tr><td><code id="controlBFGS_+3A_savedetails">saveDetails</code></td>
<td>
<p>when set to TRUE, additional details about the individual
models are save. Currently, this are the Hessian and the implied means and covariances.
Note: This may take a lot of memory!</p>
</td></tr>
<tr><td><code id="controlBFGS_+3A_stepsize">stepSize</code></td>
<td>
<p>Initial stepSize of the outer iteration (theta_next = theta_previous + stepSize * Stepdirection)</p>
</td></tr>
<tr><td><code id="controlBFGS_+3A_sigma">sigma</code></td>
<td>
<p>only relevant when lineSearch = 'GLMNET'. Controls the sigma parameter in Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012). An improved GLMNET for l1-regularized logistic regression. The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421.</p>
</td></tr>
<tr><td><code id="controlBFGS_+3A_gamma">gamma</code></td>
<td>
<p>Controls the gamma parameter in Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012). An improved GLMNET for l1-regularized logistic regression. The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421. Defaults to 0.</p>
</td></tr>
<tr><td><code id="controlBFGS_+3A_maxiterout">maxIterOut</code></td>
<td>
<p>Maximal number of outer iterations</p>
</td></tr>
<tr><td><code id="controlBFGS_+3A_maxiterin">maxIterIn</code></td>
<td>
<p>Maximal number of inner iterations</p>
</td></tr>
<tr><td><code id="controlBFGS_+3A_maxiterline">maxIterLine</code></td>
<td>
<p>Maximal number of iterations for the line search procedure</p>
</td></tr>
<tr><td><code id="controlBFGS_+3A_breakouter">breakOuter</code></td>
<td>
<p>Stopping criterion for outer iterations</p>
</td></tr>
<tr><td><code id="controlBFGS_+3A_breakinner">breakInner</code></td>
<td>
<p>Stopping criterion for inner iterations</p>
</td></tr>
<tr><td><code id="controlBFGS_+3A_convergencecriterion">convergenceCriterion</code></td>
<td>
<p>which convergence criterion should be used for the outer iterations? possible are 0 = GLMNET, 1 = fitChange, 2 = gradients.
Note that in case of gradients and GLMNET, we divide the gradients (and the Hessian) of the log-Likelihood by N as it would otherwise be
considerably more difficult for larger sample sizes to reach the convergence criteria.</p>
</td></tr>
<tr><td><code id="controlBFGS_+3A_verbose">verbose</code></td>
<td>
<p>0 prints no additional information, &gt; 0 prints GLMNET iterations</p>
</td></tr>
<tr><td><code id="controlBFGS_+3A_ncores">nCores</code></td>
<td>
<p>number of core to use. Multi-core support is provided by RcppParallel and only supported for SEM, not for general purpose optimization.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>object of class controlBFGS
</p>


<h3>Examples</h3>

<pre><code class='language-R'>control &lt;- controlBFGS()
</code></pre>

<hr>
<h2 id='controlGlmnet'>controlGlmnet</h2><span id='topic+controlGlmnet'></span>

<h3>Description</h3>

<p>Control the GLMNET optimizer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>controlGlmnet(
  startingValues = "est",
  initialHessian = ifelse(all(startingValues == "est"), "lavaan", "compute"),
  saveDetails = FALSE,
  stepSize = 0.9,
  sigma = 1e-05,
  gamma = 0,
  maxIterOut = 1000,
  maxIterIn = 1000,
  maxIterLine = 500,
  breakOuter = 1e-08,
  breakInner = 1e-10,
  convergenceCriterion = 0,
  verbose = 0,
  nCores = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="controlGlmnet_+3A_startingvalues">startingValues</code></td>
<td>
<p>option to provide initial starting values. Only used
for the first lambda. Three options are supported. Setting to &quot;est&quot; will use the estimates
from the lavaan model object. Setting to &quot;start&quot; will use the starting values
of the lavaan model. Finally, a labeled vector with parameter
values can be passed to the function which will then be used as starting values.</p>
</td></tr>
<tr><td><code id="controlGlmnet_+3A_initialhessian">initialHessian</code></td>
<td>
<p>option to provide an initial Hessian to the optimizer.
Must have row and column names corresponding to the parameter labels. use
getLavaanParameters(lavaanModel) to
see those labels. If set to &quot;gradNorm&quot;, the maximum of the gradients at the starting
values times the stepSize will be used. This is adapted from Optim.jl
https://github.com/JuliaNLSolvers/Optim.jl/blob/f43e6084aacf2dabb2b142952acd3fbb0e268439/src/multivariate/solvers/first_order/bfgs.jl#L104
If set to &quot;compute&quot;, the initial hessian will be computed. If set to a single
value, a diagonal matrix with the single value along the diagonal will be used.
The default is &quot;lavaan&quot; which extracts the Hessian from the lavaanModel. This Hessian
will typically deviate from that of the internal SEM represenation of lessSEM (due to
the transformation of the variances), but works quite well in practice.</p>
</td></tr>
<tr><td><code id="controlGlmnet_+3A_savedetails">saveDetails</code></td>
<td>
<p>when set to TRUE, additional details about the individual
models are save. Currently, this are the Hessian and the implied means and covariances.
Note: This may take a lot of memory!</p>
</td></tr>
<tr><td><code id="controlGlmnet_+3A_stepsize">stepSize</code></td>
<td>
<p>Initial stepSize of the outer iteration
(theta_next = theta_previous + stepSize * Stepdirection)</p>
</td></tr>
<tr><td><code id="controlGlmnet_+3A_sigma">sigma</code></td>
<td>
<p>only relevant when lineSearch = 'GLMNET'. Controls the sigma
parameter in Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012). An improved GLMNET
for l1-regularized logistic regression. The Journal of Machine Learning Research,
13, 1999–2030. https://doi.org/10.1145/2020408.2020421.</p>
</td></tr>
<tr><td><code id="controlGlmnet_+3A_gamma">gamma</code></td>
<td>
<p>Controls the gamma parameter in Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression. The Journal of Machine Learning
Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421. Defaults to 0.</p>
</td></tr>
<tr><td><code id="controlGlmnet_+3A_maxiterout">maxIterOut</code></td>
<td>
<p>Maximal number of outer iterations</p>
</td></tr>
<tr><td><code id="controlGlmnet_+3A_maxiterin">maxIterIn</code></td>
<td>
<p>Maximal number of inner iterations</p>
</td></tr>
<tr><td><code id="controlGlmnet_+3A_maxiterline">maxIterLine</code></td>
<td>
<p>Maximal number of iterations for the line search procedure</p>
</td></tr>
<tr><td><code id="controlGlmnet_+3A_breakouter">breakOuter</code></td>
<td>
<p>Stopping criterion for outer iterations</p>
</td></tr>
<tr><td><code id="controlGlmnet_+3A_breakinner">breakInner</code></td>
<td>
<p>Stopping criterion for inner iterations</p>
</td></tr>
<tr><td><code id="controlGlmnet_+3A_convergencecriterion">convergenceCriterion</code></td>
<td>
<p>which convergence criterion should be used for the outer iterations? possible are 0 = GLMNET, 1 = fitChange, 2 = gradients.
Note that in case of gradients and GLMNET, we divide the gradients (and the Hessian) of the log-Likelihood by N as it would otherwise be
considerably more difficult for larger sample sizes to reach the convergence criteria.</p>
</td></tr>
<tr><td><code id="controlGlmnet_+3A_verbose">verbose</code></td>
<td>
<p>0 prints no additional information, &gt; 0 prints GLMNET iterations</p>
</td></tr>
<tr><td><code id="controlGlmnet_+3A_ncores">nCores</code></td>
<td>
<p>number of core to use. Multi-core support is provided by RcppParallel and only supported for SEM, not for general purpose optimization.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>object of class controlGlmnet
</p>


<h3>Examples</h3>

<pre><code class='language-R'>control &lt;- controlGlmnet()
</code></pre>

<hr>
<h2 id='controlIsta'>controlIsta</h2><span id='topic+controlIsta'></span>

<h3>Description</h3>

<p>controlIsta
</p>


<h3>Usage</h3>

<pre><code class='language-R'>controlIsta(
  startingValues = "est",
  saveDetails = FALSE,
  L0 = 0.1,
  eta = 2,
  accelerate = TRUE,
  maxIterOut = 10000,
  maxIterIn = 1000,
  breakOuter = 1e-08,
  convCritInner = 1,
  sigma = 0.1,
  stepSizeInheritance = ifelse(accelerate, 1, 3),
  verbose = 0,
  nCores = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="controlIsta_+3A_startingvalues">startingValues</code></td>
<td>
<p>option to provide initial starting values. Only used for the first lambda. Three options are supported. Setting to &quot;est&quot; will use the estimates
from the lavaan model object. Setting to &quot;start&quot; will use the starting values of the lavaan model. Finally, a labeled vector with parameter
values can be passed to the function which will then be used as starting values.</p>
</td></tr>
<tr><td><code id="controlIsta_+3A_savedetails">saveDetails</code></td>
<td>
<p>when set to TRUE, additional details about the individual
models are save. Currently, this are the implied means and covariances.
Note: This may take a lot of memory!</p>
</td></tr>
<tr><td><code id="controlIsta_+3A_l0">L0</code></td>
<td>
<p>L0 controls the step size used in the first iteration</p>
</td></tr>
<tr><td><code id="controlIsta_+3A_eta">eta</code></td>
<td>
<p>eta controls by how much the step size changes in the
inner iterations with (eta^i)*L, where i is the inner iteration</p>
</td></tr>
<tr><td><code id="controlIsta_+3A_accelerate">accelerate</code></td>
<td>
<p>boolean: Should the acceleration outlined in
Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and Trends
in Optimization, 1(3), 123–231., p. 152 be used?</p>
</td></tr>
<tr><td><code id="controlIsta_+3A_maxiterout">maxIterOut</code></td>
<td>
<p>maximal number of outer iterations</p>
</td></tr>
<tr><td><code id="controlIsta_+3A_maxiterin">maxIterIn</code></td>
<td>
<p>maximal number of inner iterations</p>
</td></tr>
<tr><td><code id="controlIsta_+3A_breakouter">breakOuter</code></td>
<td>
<p>change in fit required to break the outer iteration. Note: The
value will be multiplied internally with sample size N as the -2log-Likelihood
depends directly on the sample size</p>
</td></tr>
<tr><td><code id="controlIsta_+3A_convcritinner">convCritInner</code></td>
<td>
<p>this is related to the inner breaking condition.
0 = ista, as presented by Beck &amp; Teboulle (2009); see Remark 3.1 on p. 191 (ISTA with backtracking)
1 = gist, as presented by Gong et al. (2013) (Equation 3)</p>
</td></tr>
<tr><td><code id="controlIsta_+3A_sigma">sigma</code></td>
<td>
<p>sigma in (0,1) is used by the gist convergence criterion. larger
sigma enforce larger improvement in fit</p>
</td></tr>
<tr><td><code id="controlIsta_+3A_stepsizeinheritance">stepSizeInheritance</code></td>
<td>
<p>how should step sizes be carried forward from iteration to iteration?
0 = resets the step size to L0 in each iteration
1 = takes the previous step size as initial value for the next iteration
3 = Barzilai-Borwein procedure
4 = Barzilai-Borwein procedure, but sometimes resets the step size; this can help when the optimizer is caught in a bad spot.</p>
</td></tr>
<tr><td><code id="controlIsta_+3A_verbose">verbose</code></td>
<td>
<p>if set to a value &gt; 0, the fit every &quot;verbose&quot; iterations is printed.</p>
</td></tr>
<tr><td><code id="controlIsta_+3A_ncores">nCores</code></td>
<td>
<p>number of core to use. Multi-core support is provided by RcppParallel and only supported for SEM, not for general purpose optimization.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>object of class controlIsta
</p>


<h3>Examples</h3>

<pre><code class='language-R'>control &lt;- controlIsta()
</code></pre>

<hr>
<h2 id='covariances'>covariances</h2><span id='topic+covariances'></span>

<h3>Description</h3>

<p>Extract the labels of all covariances found in a lavaan model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>covariances(lavaanModel)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="covariances_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>fitted lavaan model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector with parameter labels
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The following is adapted from ?lavaan::sem
library(lessSEM)
model &lt;- ' 
  # latent variable definitions
  ind60 =~ x1 + x2 + x3
  dem60 =~ y1 + a*y2 + b*y3 + c*y4
  dem65 =~ y5 + a*y6 + b*y7 + c*y8

  # regressions
  dem60 ~ ind60
  dem65 ~ ind60 + dem60

  # residual correlations
  y1 ~~ y5
  y2 ~~ y4 + y6
  y3 ~~ y7
  y4 ~~ y8
  y6 ~~ y8
'

fit &lt;- sem(model, data = PoliticalDemocracy)

covariances(fit)
</code></pre>

<hr>
<h2 id='createSubsets'>createSubsets</h2><span id='topic+createSubsets'></span>

<h3>Description</h3>

<p>create subsets for cross-validation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createSubsets(N, k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="createSubsets_+3A_n">N</code></td>
<td>
<p>number of samples in the data set</p>
</td></tr>
<tr><td><code id="createSubsets_+3A_k">k</code></td>
<td>
<p>number of subsets to create</p>
</td></tr>
</table>


<h3>Value</h3>

<p>matrix with subsets
</p>


<h3>Examples</h3>

<pre><code class='language-R'>createSubsets(N=100, k = 5)
</code></pre>

<hr>
<h2 id='curveLambda'>curveLambda</h2><span id='topic+curveLambda'></span>

<h3>Description</h3>

<p>generates lambda values between 0 and lambdaMax using the function described here:
https://math.stackexchange.com/questions/384613/exponential-function-with-values-between-0-and-1-for-x-values-between-0-and-1.
The function is identical to the one implemented in the regCtsem package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>curveLambda(maxLambda, lambdasAutoCurve, lambdasAutoLength)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="curveLambda_+3A_maxlambda">maxLambda</code></td>
<td>
<p>maximal lambda value</p>
</td></tr>
<tr><td><code id="curveLambda_+3A_lambdasautocurve">lambdasAutoCurve</code></td>
<td>
<p>controls the curve. A value close to 1 will result in a linear increase, larger values in lambdas more concentrated around 0</p>
</td></tr>
<tr><td><code id="curveLambda_+3A_lambdasautolength">lambdasAutoLength</code></td>
<td>
<p>number of lambda values to generate</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)
plot(curveLambda(maxLambda = 10, lambdasAutoCurve = 1, lambdasAutoLength = 100))
plot(curveLambda(maxLambda = 10, lambdasAutoCurve = 5, lambdasAutoLength = 100))
plot(curveLambda(maxLambda = 10, lambdasAutoCurve = 100, lambdasAutoLength = 100))
</code></pre>

<hr>
<h2 id='cvAdaptiveLasso'>cvAdaptiveLasso</h2><span id='topic+cvAdaptiveLasso'></span>

<h3>Description</h3>

<p>Implements cross-validated adaptive lasso regularization for structural equation models.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = p( x_j) = \frac{1}{w_j}\lambda| x_j|</code>
</p>

<p>Adaptive lasso regularization will set parameters to zero if <code class="reqn">\lambda</code>
is large enough.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvAdaptiveLasso(
  lavaanModel,
  regularized,
  weights = NULL,
  lambdas,
  k = 5,
  standardize = FALSE,
  returnSubsetParameters = FALSE,
  method = "glmnet",
  modifyModel = lessSEM::modifyModel(),
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvAdaptiveLasso_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="cvAdaptiveLasso_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="cvAdaptiveLasso_+3A_weights">weights</code></td>
<td>
<p>labeled vector with weights for each of the parameters in the
model. If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object. If set to NULL,
the default weights will be used: the inverse of the absolute values of
the unregularized parameter estimates</p>
</td></tr>
<tr><td><code id="cvAdaptiveLasso_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="cvAdaptiveLasso_+3A_k">k</code></td>
<td>
<p>the number of cross-validation folds. Alternatively, you can pass
a matrix with booleans (TRUE, FALSE) which indicates for each person which subset
it belongs to. See ?lessSEM::createSubsets for an example of how this matrix should look like.</p>
</td></tr>
<tr><td><code id="cvAdaptiveLasso_+3A_standardize">standardize</code></td>
<td>
<p>Standardizing your data prior to the analysis can undermine the cross-
validation. Set standardize=TRUE to automatically standardize the data.</p>
</td></tr>
<tr><td><code id="cvAdaptiveLasso_+3A_returnsubsetparameters">returnSubsetParameters</code></td>
<td>
<p>set to TRUE to return the parameters for each training set</p>
</td></tr>
<tr><td><code id="cvAdaptiveLasso_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista and glmnet.
With ista, the control argument can be used to switch to related procedures (currently gist).</p>
</td></tr>
<tr><td><code id="cvAdaptiveLasso_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="cvAdaptiveLasso_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currenlty,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>Adaptive lasso regularization:
</p>

<ul>
<li><p> Zou, H. (2006). The adaptive lasso and its oracle properties. Journal of the American Statistical Association,
101(476), 1418–1429. https://doi.org/10.1198/016214506000000735
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>model of class cvRegularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

lsem &lt;- cvAdaptiveLasso(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = paste0("l", 6:15),
  lambdas = seq(0,1,.1))

# use the plot-function to plot the cross-validation fit
plot(lsem)

# the coefficients can be accessed with:
coef(lsem)
# if you are only interested in the estimates and not the tuning parameters, use
coef(lsem)@estimates
# or
estimates(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters

# The best parameters can also be extracted with:
estimates(lsem)
</code></pre>

<hr>
<h2 id='cvCappedL1'>cvCappedL1</h2><span id='topic+cvCappedL1'></span>

<h3>Description</h3>

<p>Implements cappedL1 regularization for structural equation models.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \lambda \min(| x_j|, \theta)</code>
</p>

<p>where <code class="reqn">\theta &gt; 0</code>. The cappedL1 penalty is identical to the lasso for
parameters which are below <code class="reqn">\theta</code> and identical to a constant for parameters
above <code class="reqn">\theta</code>. As adding a constant to the fitting function will not change its
minimum, larger parameters can stay unregularized while smaller ones are set to zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvCappedL1(
  lavaanModel,
  regularized,
  lambdas,
  thetas,
  k = 5,
  standardize = FALSE,
  returnSubsetParameters = FALSE,
  modifyModel = lessSEM::modifyModel(),
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvCappedL1_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="cvCappedL1_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="cvCappedL1_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="cvCappedL1_+3A_thetas">thetas</code></td>
<td>
<p>parameters whose absolute value is above this threshold will be penalized with
a constant (theta)</p>
</td></tr>
<tr><td><code id="cvCappedL1_+3A_k">k</code></td>
<td>
<p>the number of cross-validation folds. Alternatively, you can pass
a matrix with booleans (TRUE, FALSE) which indicates for each person which subset
it belongs to. See ?lessSEM::createSubsets for an example of how this matrix should look like.</p>
</td></tr>
<tr><td><code id="cvCappedL1_+3A_standardize">standardize</code></td>
<td>
<p>Standardizing your data prior to the analysis can undermine the cross-
validation. Set standardize=TRUE to automatically standardize the data.</p>
</td></tr>
<tr><td><code id="cvCappedL1_+3A_returnsubsetparameters">returnSubsetParameters</code></td>
<td>
<p>set to TRUE to return the parameters for each training set</p>
</td></tr>
<tr><td><code id="cvCappedL1_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="cvCappedL1_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista and glmnet.
With ista, the control argument can be used to switch to related procedures.</p>
</td></tr>
<tr><td><code id="cvCappedL1_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta function. See ?controlIsta
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currenlty,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>CappedL1 regularization:
</p>

<ul>
<li><p> Zhang, T. (2010). Analysis of Multi-stage Convex Relaxation for Sparse Regularization.
Journal of Machine Learning Research, 11, 1081–1107.
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>model of class cvRegularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

lsem &lt;- cvCappedL1(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = paste0("l", 6:15),
  lambdas = seq(0,1,length.out = 5),
  thetas = seq(0.01,2,length.out = 3))

# the coefficients can be accessed with:
coef(lsem)
# if you are only interested in the estimates and not the tuning parameters, use
coef(lsem)@estimates
# or
estimates(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters

# optional: plotting the cross-validation fit requires installation of plotly
# plot(lsem)
</code></pre>

<hr>
<h2 id='cvElasticNet'>cvElasticNet</h2><span id='topic+cvElasticNet'></span>

<h3>Description</h3>

<p>Implements elastic net regularization for structural equation models.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \alpha\lambda| x_j| + (1-\alpha)\lambda x_j^2</code>
</p>

<p>Note that the elastic net combines ridge and lasso regularization. If <code class="reqn">\alpha = 0</code>,
the elastic net reduces to ridge regularization. If <code class="reqn">\alpha = 1</code> it reduces
to lasso regularization. In between, elastic net is a compromise between the shrinkage of
the lasso and the ridge penalty.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvElasticNet(
  lavaanModel,
  regularized,
  lambdas,
  alphas,
  k = 5,
  standardize = FALSE,
  returnSubsetParameters = FALSE,
  method = "glmnet",
  modifyModel = lessSEM::modifyModel(),
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvElasticNet_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="cvElasticNet_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="cvElasticNet_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="cvElasticNet_+3A_alphas">alphas</code></td>
<td>
<p>numeric vector with values of the tuning parameter alpha. Must be
between 0 and 1. 0 = ridge, 1 = lasso.</p>
</td></tr>
<tr><td><code id="cvElasticNet_+3A_k">k</code></td>
<td>
<p>the number of cross-validation folds. Alternatively, you can pass
a matrix with booleans (TRUE, FALSE) which indicates for each person which subset
it belongs to. See ?lessSEM::createSubsets for an example of how this matrix should look like.</p>
</td></tr>
<tr><td><code id="cvElasticNet_+3A_standardize">standardize</code></td>
<td>
<p>Standardizing your data prior to the analysis can undermine the cross-
validation. Set standardize=TRUE to automatically standardize the data.</p>
</td></tr>
<tr><td><code id="cvElasticNet_+3A_returnsubsetparameters">returnSubsetParameters</code></td>
<td>
<p>set to TRUE to return the parameters for each training set</p>
</td></tr>
<tr><td><code id="cvElasticNet_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista and glmnet.
With ista, the control argument can be used to switch to related procedures.</p>
</td></tr>
<tr><td><code id="cvElasticNet_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="cvElasticNet_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currenlty,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>Elastic net regularization:
</p>

<ul>
<li><p> Zou, H., &amp; Hastie, T. (2005). Regularization and variable selection via the elastic net.
Journal of the Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>model of class cvRegularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

lsem &lt;- cvElasticNet(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = paste0("l", 6:15),
  lambdas = seq(0,1,length.out = 5),
  alphas = seq(0,1,length.out = 3))

# the coefficients can be accessed with:
coef(lsem)
# if you are only interested in the estimates and not the tuning parameters, use
coef(lsem)@estimates
# or
estimates(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters

# optional: plotting the cross-validation fit requires installation of plotly
# plot(lsem)
</code></pre>

<hr>
<h2 id='cvLasso'>cvLasso</h2><span id='topic+cvLasso'></span>

<h3>Description</h3>

<p>Implements cross-validated lasso regularization for structural equation models.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \lambda |x_j|</code>
</p>

<p>Lasso regularization will set parameters to zero if <code class="reqn">\lambda</code> is large enough
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvLasso(
  lavaanModel,
  regularized,
  lambdas,
  k = 5,
  standardize = FALSE,
  returnSubsetParameters = FALSE,
  method = "glmnet",
  modifyModel = lessSEM::modifyModel(),
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvLasso_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="cvLasso_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="cvLasso_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="cvLasso_+3A_k">k</code></td>
<td>
<p>the number of cross-validation folds. Alternatively, you can pass
a matrix with booleans (TRUE, FALSE) which indicates for each person which subset
it belongs to. See ?lessSEM::createSubsets for an example of how this matrix should look like.</p>
</td></tr>
<tr><td><code id="cvLasso_+3A_standardize">standardize</code></td>
<td>
<p>Standardizing your data prior to the analysis can undermine the cross-
validation. Set standardize=TRUE to automatically standardize the data.</p>
</td></tr>
<tr><td><code id="cvLasso_+3A_returnsubsetparameters">returnSubsetParameters</code></td>
<td>
<p>set to TRUE to return the parameters for each training set</p>
</td></tr>
<tr><td><code id="cvLasso_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista and glmnet.
With ista, the control argument can be used to switch to related procedures.</p>
</td></tr>
<tr><td><code id="cvLasso_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="cvLasso_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currently,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>Lasso regularization:
</p>

<ul>
<li><p> Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), 58(1), 267–288.
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>model of class cvRegularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 + 
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 + 
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

lsem &lt;- cvLasso(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = paste0("l", 6:15),
  lambdas = seq(0,1,.1),
  k = 5, # number of cross-validation folds
  standardize = TRUE) # automatic standardization

# use the plot-function to plot the cross-validation fit:
plot(lsem)

# the coefficients can be accessed with:
coef(lsem)
# if you are only interested in the estimates and not the tuning parameters, use
coef(lsem)@estimates
# or
estimates(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters

# The best parameters can also be extracted with:
estimates(lsem)
</code></pre>

<hr>
<h2 id='cvLsp'>cvLsp</h2><span id='topic+cvLsp'></span>

<h3>Description</h3>

<p>Implements lsp regularization for structural equation models.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \lambda \log(1 + |x_j|/\theta)</code>
</p>

<p>where <code class="reqn">\theta &gt; 0</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvLsp(
  lavaanModel,
  regularized,
  lambdas,
  thetas,
  k = 5,
  standardize = FALSE,
  returnSubsetParameters = FALSE,
  modifyModel = lessSEM::modifyModel(),
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvLsp_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="cvLsp_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="cvLsp_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="cvLsp_+3A_thetas">thetas</code></td>
<td>
<p>parameters whose absolute value is above this threshold will be penalized with
a constant (theta)</p>
</td></tr>
<tr><td><code id="cvLsp_+3A_k">k</code></td>
<td>
<p>the number of cross-validation folds. Alternatively, you can pass
a matrix with booleans (TRUE, FALSE) which indicates for each person which subset
it belongs to. See ?lessSEM::createSubsets for an example of how this matrix should look like.</p>
</td></tr>
<tr><td><code id="cvLsp_+3A_standardize">standardize</code></td>
<td>
<p>Standardizing your data prior to the analysis can undermine the cross-
validation. Set standardize=TRUE to automatically standardize the data.</p>
</td></tr>
<tr><td><code id="cvLsp_+3A_returnsubsetparameters">returnSubsetParameters</code></td>
<td>
<p>set to TRUE to return the parameters for each training set</p>
</td></tr>
<tr><td><code id="cvLsp_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="cvLsp_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista and glmnet.
With ista, the control argument can be used to switch to related procedures.</p>
</td></tr>
<tr><td><code id="cvLsp_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta function. See ?controlIsta</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currenlty,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>lsp regularization:
</p>

<ul>
<li><p> Candès, E. J., Wakin, M. B., &amp; Boyd, S. P. (2008). Enhancing Sparsity by
Reweighted l1 Minimization. Journal of Fourier Analysis and Applications, 14(5–6),
877–905. https://doi.org/10.1007/s00041-008-9045-x
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>model of class cvRegularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

lsem &lt;- cvLsp(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = paste0("l", 6:15),
  lambdas = seq(0,1,length.out = 5),
  thetas = seq(0.01,2,length.out = 3))

# the coefficients can be accessed with:
coef(lsem)
# if you are only interested in the estimates and not the tuning parameters, use
coef(lsem)@estimates
# or
estimates(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters

# optional: plotting the cross-validation fit requires installation of plotly
# plot(lsem)
</code></pre>

<hr>
<h2 id='cvMcp'>cvMcp</h2><span id='topic+cvMcp'></span>

<h3>Description</h3>

<p>Implements mcp regularization for structural equation models.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \begin{cases}
\lambda |x_j| - x_j^2/(2\theta) &amp; \text{if } |x_j| \leq \theta\lambda\\
\theta\lambda^2/2 &amp; \text{if } |x_j| &gt; \lambda\theta
\end{cases}</code>
</p>
<p> where <code class="reqn">\theta &gt; 0</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvMcp(
  lavaanModel,
  regularized,
  lambdas,
  thetas,
  k = 5,
  standardize = FALSE,
  returnSubsetParameters = FALSE,
  modifyModel = lessSEM::modifyModel(),
  method = "ista",
  control = lessSEM::controlIsta()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvMcp_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="cvMcp_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="cvMcp_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="cvMcp_+3A_thetas">thetas</code></td>
<td>
<p>parameters whose absolute value is above this threshold will be penalized with
a constant (theta)</p>
</td></tr>
<tr><td><code id="cvMcp_+3A_k">k</code></td>
<td>
<p>the number of cross-validation folds. Alternatively, you can pass
a matrix with booleans (TRUE, FALSE) which indicates for each person which subset
it belongs to. See ?lessSEM::createSubsets for an example of how this matrix should look like.</p>
</td></tr>
<tr><td><code id="cvMcp_+3A_standardize">standardize</code></td>
<td>
<p>Standardizing your data prior to the analysis can undermine the cross-
validation. Set standardize=TRUE to automatically standardize the data.</p>
</td></tr>
<tr><td><code id="cvMcp_+3A_returnsubsetparameters">returnSubsetParameters</code></td>
<td>
<p>set to TRUE to return the parameters for each training set</p>
</td></tr>
<tr><td><code id="cvMcp_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="cvMcp_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista and glmnet.
With ista, the control argument can be used to switch to related procedures.</p>
</td></tr>
<tr><td><code id="cvMcp_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta function. See ?controlIsta</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currenlty,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>mcp regularization:
</p>

<ul>
<li><p> Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty.
The Annals of Statistics, 38(2), 894–942. https://doi.org/10.1214/09-AOS729
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>model of class cvRegularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

lsem &lt;- cvMcp(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = paste0("l", 6:15),
  lambdas = seq(0,1,length.out = 5),
  thetas = seq(0.01,2,length.out = 3))

# the coefficients can be accessed with:
coef(lsem)
# if you are only interested in the estimates and not the tuning parameters, use
coef(lsem)@estimates
# or
estimates(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters

# optional: plotting the cross-validation fit requires installation of plotly
# plot(lsem)
</code></pre>

<hr>
<h2 id='cvRegularizedSEM-class'>Class for cross-validated regularized SEM</h2><span id='topic+cvRegularizedSEM-class'></span>

<h3>Description</h3>

<p>Class for cross-validated regularized SEM
</p>


<h3>Slots</h3>


<dl>
<dt><code>parameters</code></dt><dd><p>data.frame with parameter estimates for the best combination of the
tuning parameters</p>
</dd>
<dt><code>transformations</code></dt><dd><p>transformed parameters</p>
</dd>
<dt><code>cvfits</code></dt><dd><p>data.frame with all combinations of the
tuning parameters and the sum of the cross-validation fits</p>
</dd>
<dt><code>parameterLabels</code></dt><dd><p>character vector with names of all parameters</p>
</dd>
<dt><code>regularized</code></dt><dd><p>character vector with names of regularized parameters</p>
</dd>
<dt><code>cvfitsDetails</code></dt><dd><p>data.frame with cross-validation fits for each subset</p>
</dd>
<dt><code>subsets</code></dt><dd><p>matrix indicating which person is in which subset</p>
</dd>
<dt><code>subsetParameters</code></dt><dd><p>optional: data.frame with parameter estimates for all
combinations of the tuning parameters in all subsets</p>
</dd>
<dt><code>misc</code></dt><dd><p>list with additional return elements</p>
</dd>
<dt><code>notes</code></dt><dd><p>internal notes that have come up when fitting the model</p>
</dd>
</dl>

<hr>
<h2 id='cvRidge'>cvRidge</h2><span id='topic+cvRidge'></span>

<h3>Description</h3>

<p>Implements ridge regularization for structural equation models.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \lambda x_j^2</code>
</p>

<p>Note that ridge regularization will not set any of the parameters to zero
but result in a shrinkage towards zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvRidge(
  lavaanModel,
  regularized,
  lambdas,
  k = 5,
  standardize = FALSE,
  returnSubsetParameters = FALSE,
  method = "glmnet",
  modifyModel = lessSEM::modifyModel(),
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvRidge_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="cvRidge_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="cvRidge_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="cvRidge_+3A_k">k</code></td>
<td>
<p>the number of cross-validation folds. Alternatively, you can pass
a matrix with booleans (TRUE, FALSE) which indicates for each person which subset
it belongs to. See ?lessSEM::createSubsets for an example of how this matrix should look like.</p>
</td></tr>
<tr><td><code id="cvRidge_+3A_standardize">standardize</code></td>
<td>
<p>Standardizing your data prior to the analysis can undermine the cross-
validation. Set standardize=TRUE to automatically standardize the data.</p>
</td></tr>
<tr><td><code id="cvRidge_+3A_returnsubsetparameters">returnSubsetParameters</code></td>
<td>
<p>set to TRUE to return the parameters for each training set</p>
</td></tr>
<tr><td><code id="cvRidge_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista and glmnet.
With ista, the control argument can be used to switch to related procedures (currently gist).</p>
</td></tr>
<tr><td><code id="cvRidge_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="cvRidge_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currenlty,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>Ridge regularization:
</p>

<ul>
<li><p> Hoerl, A. E., &amp; Kennard, R. W. (1970). Ridge Regression: Biased Estimation
for Nonorthogonal Problems. Technometrics, 12(1), 55–67.
https://doi.org/10.1080/00401706.1970.10488634
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>model of class cvRegularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

lsem &lt;- cvRidge(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = paste0("l", 6:15),
  lambdas = seq(0,1,length.out = 20))

# use the plot-function to plot the cross-validation fit:
plot(lsem)

# the coefficients can be accessed with:
coef(lsem)
# if you are only interested in the estimates and not the tuning parameters, use
coef(lsem)@estimates
# or
estimates(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters

</code></pre>

<hr>
<h2 id='cvRidgeBfgs'>cvRidgeBfgs</h2><span id='topic+cvRidgeBfgs'></span>

<h3>Description</h3>

<p>Implements cross-validated ridge regularization for structural equation models.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \lambda x_j^2</code>
</p>

<p>Note that ridge regularization will not set any of the parameters to zero
but result in a shrinkage towards zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvRidgeBfgs(
  lavaanModel,
  regularized,
  lambdas,
  k = 5,
  standardize = FALSE,
  returnSubsetParameters = FALSE,
  modifyModel = lessSEM::modifyModel(),
  control = lessSEM::controlBFGS()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvRidgeBfgs_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="cvRidgeBfgs_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="cvRidgeBfgs_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="cvRidgeBfgs_+3A_k">k</code></td>
<td>
<p>the number of cross-validation folds. Alternatively, you can pass
a matrix with booleans (TRUE, FALSE) which indicates for each person which subset
it belongs to. See ?lessSEM::createSubsets for an example of how this matrix should look like.</p>
</td></tr>
<tr><td><code id="cvRidgeBfgs_+3A_standardize">standardize</code></td>
<td>
<p>Standardizing your data prior to the analysis can undermine the cross-
validation. Set standardize=TRUE to automatically standardize the data.</p>
</td></tr>
<tr><td><code id="cvRidgeBfgs_+3A_returnsubsetparameters">returnSubsetParameters</code></td>
<td>
<p>set to TRUE to return the parameters for each training set</p>
</td></tr>
<tr><td><code id="cvRidgeBfgs_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="cvRidgeBfgs_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlBFGS function. See ?controlBFGS
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currently,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>Ridge regularization:
</p>

<ul>
<li><p> Hoerl, A. E., &amp; Kennard, R. W. (1970). Ridge Regression: Biased Estimation
for Nonorthogonal Problems. Technometrics, 12(1), 55–67.
https://doi.org/10.1080/00401706.1970.10488634
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>



<h3>Value</h3>

<p>model of class cvRegularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

lsem &lt;- cvRidgeBfgs(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = paste0("l", 6:15),
  lambdas = seq(0,1,length.out = 20))

# use the plot-function to plot the cross-validation fit:
plot(lsem)

# the coefficients can be accessed with:
coef(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters

</code></pre>

<hr>
<h2 id='cvScad'>cvScad</h2><span id='topic+cvScad'></span>

<h3>Description</h3>

<p>Implements scad regularization for structural equation models.
The penalty function is given by:

</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \begin{cases}
\lambda |x_j| &amp; \text{if } |x_j| \leq \theta\\
\frac{-x_j^2 + 2\theta\lambda |x_j| - \lambda^2}{2(\theta -1)} &amp; 
\text{if } \lambda &lt; |x_j| \leq \lambda\theta \\
(\theta + 1) \lambda^2/2 &amp; \text{if } |x_j| \geq \theta\lambda\\
\end{cases}</code>
</p>

<p>where <code class="reqn">\theta &gt; 2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvScad(
  lavaanModel,
  regularized,
  lambdas,
  thetas,
  k = 5,
  standardize = FALSE,
  returnSubsetParameters = FALSE,
  modifyModel = lessSEM::modifyModel(),
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvScad_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="cvScad_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="cvScad_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="cvScad_+3A_thetas">thetas</code></td>
<td>
<p>parameters whose absolute value is above this threshold will be penalized with
a constant (theta)</p>
</td></tr>
<tr><td><code id="cvScad_+3A_k">k</code></td>
<td>
<p>the number of cross-validation folds. Alternatively, you can pass
a matrix with booleans (TRUE, FALSE) which indicates for each person which subset
it belongs to. See ?lessSEM::createSubsets for an example of how this matrix should look like.</p>
</td></tr>
<tr><td><code id="cvScad_+3A_standardize">standardize</code></td>
<td>
<p>Standardizing your data prior to the analysis can undermine the cross-
validation. Set standardize=TRUE to automatically standardize the data.</p>
</td></tr>
<tr><td><code id="cvScad_+3A_returnsubsetparameters">returnSubsetParameters</code></td>
<td>
<p>set to TRUE to return the parameters for each training set</p>
</td></tr>
<tr><td><code id="cvScad_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="cvScad_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista and glmnet.
With ista, the control argument can be used to switch to related procedures.</p>
</td></tr>
<tr><td><code id="cvScad_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta function. See ?controlIsta</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currenlty,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>scad regularization:
</p>

<ul>
<li><p> Fan, J., &amp; Li, R. (2001). Variable selection via nonconcave penalized
likelihood and its oracle properties. Journal of the American Statistical Association,
96(456), 1348–1360. https://doi.org/10.1198/016214501753382273
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>model of class cvRegularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

lsem &lt;- cvScad(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = paste0("l", 6:15),
  lambdas = seq(0,1,length.out = 3),
  thetas = seq(2.01,5,length.out = 3))

# the coefficients can be accessed with:
coef(lsem)
# if you are only interested in the estimates and not the tuning parameters, use
coef(lsem)@estimates
# or
estimates(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters

# optional: plotting the cross-validation fit requires installation of plotly
# plot(lsem)
</code></pre>

<hr>
<h2 id='cvScaler'>cvScaler</h2><span id='topic+cvScaler'></span>

<h3>Description</h3>

<p>uses the means and standard deviations of the training set to standardize
the test set. See, e.g., https://scikit-learn.org/stable/modules/cross_validation.html .
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvScaler(testSet, means, standardDeviations)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvScaler_+3A_testset">testSet</code></td>
<td>
<p>test data set</p>
</td></tr>
<tr><td><code id="cvScaler_+3A_means">means</code></td>
<td>
<p>means of the training set</p>
</td></tr>
<tr><td><code id="cvScaler_+3A_standarddeviations">standardDeviations</code></td>
<td>
<p>standard deviations of the training set</p>
</td></tr>
</table>


<h3>Value</h3>

<p>scaled test set
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)
data &lt;- matrix(rnorm(50),10,5)

cvScaler(testSet = data, 
         means = 1:5, 
         standardDeviations = 1:5)
</code></pre>

<hr>
<h2 id='cvSmoothAdaptiveLasso'>cvSmoothAdaptiveLasso</h2><span id='topic+cvSmoothAdaptiveLasso'></span>

<h3>Description</h3>

<p>Implements cross-validated smooth adaptive lasso regularization for structural equation models.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = p( x_j) = \frac{1}{w_j}\lambda\sqrt{(x_j + \epsilon)^2}</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>cvSmoothAdaptiveLasso(
  lavaanModel,
  regularized,
  weights = NULL,
  lambdas,
  epsilon,
  k = 5,
  standardize = FALSE,
  returnSubsetParameters = FALSE,
  modifyModel = lessSEM::modifyModel(),
  control = lessSEM::controlBFGS()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvSmoothAdaptiveLasso_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="cvSmoothAdaptiveLasso_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="cvSmoothAdaptiveLasso_+3A_weights">weights</code></td>
<td>
<p>labeled vector with weights for each of the parameters in the
model. If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object. If set to NULL,
the default weights will be used: the inverse of the absolute values of
the unregularized parameter estimates</p>
</td></tr>
<tr><td><code id="cvSmoothAdaptiveLasso_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="cvSmoothAdaptiveLasso_+3A_epsilon">epsilon</code></td>
<td>
<p>epsilon &gt; 0; controls the smoothness of the approximation. Larger values = smoother</p>
</td></tr>
<tr><td><code id="cvSmoothAdaptiveLasso_+3A_k">k</code></td>
<td>
<p>the number of cross-validation folds. Alternatively, you can pass
a matrix with booleans (TRUE, FALSE) which indicates for each person which subset
it belongs to. See ?lessSEM::createSubsets for an example of how this matrix should look like.</p>
</td></tr>
<tr><td><code id="cvSmoothAdaptiveLasso_+3A_standardize">standardize</code></td>
<td>
<p>Standardizing your data prior to the analysis can undermine the cross-
validation. Set standardize=TRUE to automatically standardize the data.</p>
</td></tr>
<tr><td><code id="cvSmoothAdaptiveLasso_+3A_returnsubsetparameters">returnSubsetParameters</code></td>
<td>
<p>set to TRUE to return the parameters for each training set</p>
</td></tr>
<tr><td><code id="cvSmoothAdaptiveLasso_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="cvSmoothAdaptiveLasso_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlBFGS function. See ?controlBFGS for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currently,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>Adaptive lasso regularization:
</p>

<ul>
<li><p> Zou, H. (2006). The adaptive lasso and its oracle properties. Journal of the American Statistical Association,
101(476), 1418–1429. https://doi.org/10.1198/016214506000000735
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>



<h3>Value</h3>

<p>model of class cvRegularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

lsem &lt;- cvSmoothAdaptiveLasso(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = paste0("l", 6:15),
  lambdas = seq(0,1,.1),
  epsilon = 1e-8)

# use the plot-function to plot the cross-validation fit
plot(lsem)

# the coefficients can be accessed with:
coef(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters

# The best parameters can also be extracted with:
coef(lsem)
</code></pre>

<hr>
<h2 id='cvSmoothElasticNet'>cvSmoothElasticNet</h2><span id='topic+cvSmoothElasticNet'></span>

<h3>Description</h3>

<p>Implements cross-validated  smooth elastic net regularization for structural equation models.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \alpha\lambda\sqrt{(x_j + \epsilon)^2} + (1-\alpha)\lambda x_j^2</code>
</p>

<p>Note that the smooth elastic net combines ridge and smooth lasso regularization. If <code class="reqn">\alpha = 0</code>,
the elastic net reduces to ridge regularization. If <code class="reqn">\alpha = 1</code> it reduces
to smooth lasso regularization. In between, elastic net is a compromise between the shrinkage of
the lasso and the ridge penalty.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cvSmoothElasticNet(
  lavaanModel,
  regularized,
  lambdas,
  alphas,
  epsilon,
  k = 5,
  standardize = FALSE,
  returnSubsetParameters = FALSE,
  modifyModel = lessSEM::modifyModel(),
  control = lessSEM::controlBFGS()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvSmoothElasticNet_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="cvSmoothElasticNet_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="cvSmoothElasticNet_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="cvSmoothElasticNet_+3A_alphas">alphas</code></td>
<td>
<p>numeric vector with values of the tuning parameter alpha. Must be
between 0 and 1. 0 = ridge, 1 = lasso.</p>
</td></tr>
<tr><td><code id="cvSmoothElasticNet_+3A_epsilon">epsilon</code></td>
<td>
<p>epsilon &gt; 0; controls the smoothness of the approximation. Larger values = smoother</p>
</td></tr>
<tr><td><code id="cvSmoothElasticNet_+3A_k">k</code></td>
<td>
<p>the number of cross-validation folds. Alternatively, you can pass
a matrix with booleans (TRUE, FALSE) which indicates for each person which subset
it belongs to. See ?lessSEM::createSubsets for an example of how this matrix should look like.</p>
</td></tr>
<tr><td><code id="cvSmoothElasticNet_+3A_standardize">standardize</code></td>
<td>
<p>Standardizing your data prior to the analysis can undermine the cross-
validation. Set standardize=TRUE to automatically standardize the data.</p>
</td></tr>
<tr><td><code id="cvSmoothElasticNet_+3A_returnsubsetparameters">returnSubsetParameters</code></td>
<td>
<p>set to TRUE to return the parameters for each training set</p>
</td></tr>
<tr><td><code id="cvSmoothElasticNet_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="cvSmoothElasticNet_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlBFGS function. See ?controlBFGS
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currently,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>Elastic net regularization:
</p>

<ul>
<li><p> Zou, H., &amp; Hastie, T. (2005). Regularization and variable selection via the elastic net.
Journal of the Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>



<h3>Value</h3>

<p>model of class cvRegularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

lsem &lt;- cvSmoothElasticNet(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = paste0("l", 6:15),
  epsilon = 1e-8,
  lambdas = seq(0,1,length.out = 5),
  alphas = .3)

# the coefficients can be accessed with:
coef(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters

# optional: plotting the cross-validation fit requires installation of plotly
# plot(lsem)
</code></pre>

<hr>
<h2 id='cvSmoothLasso'>cvSmoothLasso</h2><span id='topic+cvSmoothLasso'></span>

<h3>Description</h3>

<p>Implements cross-validated smooth lasso regularization for structural equation models.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \lambda \sqrt{(x_j + \epsilon)^2}</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>cvSmoothLasso(
  lavaanModel,
  regularized,
  lambdas,
  epsilon,
  k = 5,
  standardize = FALSE,
  returnSubsetParameters = FALSE,
  modifyModel = lessSEM::modifyModel(),
  control = lessSEM::controlBFGS()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cvSmoothLasso_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="cvSmoothLasso_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="cvSmoothLasso_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="cvSmoothLasso_+3A_epsilon">epsilon</code></td>
<td>
<p>epsilon &gt; 0; controls the smoothness of the approximation. Larger values = smoother</p>
</td></tr>
<tr><td><code id="cvSmoothLasso_+3A_k">k</code></td>
<td>
<p>the number of cross-validation folds. Alternatively, you can pass
a matrix with booleans (TRUE, FALSE) which indicates for each person which subset
it belongs to. See ?lessSEM::createSubsets for an example of how this matrix should look like.</p>
</td></tr>
<tr><td><code id="cvSmoothLasso_+3A_standardize">standardize</code></td>
<td>
<p>Standardizing your data prior to the analysis can undermine the cross-
validation. Set standardize=TRUE to automatically standardize the data.</p>
</td></tr>
<tr><td><code id="cvSmoothLasso_+3A_returnsubsetparameters">returnSubsetParameters</code></td>
<td>
<p>set to TRUE to return the parameters for each training set</p>
</td></tr>
<tr><td><code id="cvSmoothLasso_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="cvSmoothLasso_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlBFGS function. See ?controlBFGS for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currently,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>Lasso regularization:
</p>

<ul>
<li><p> Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), 58(1), 267–288.
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>



<h3>Value</h3>

<p>model of class cvRegularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 + 
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 + 
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

lsem &lt;- cvSmoothLasso(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = paste0("l", 6:15),
  lambdas = seq(0,1,.1),
  k = 5, # number of cross-validation folds
  epsilon = 1e-8,
  standardize = TRUE) # automatic standardization

# use the plot-function to plot the cross-validation fit:
plot(lsem)

# the coefficients can be accessed with:
coef(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters

# The best parameters can also be extracted with:
coef(lsem)
</code></pre>

<hr>
<h2 id='elasticNet'>elasticNet</h2><span id='topic+elasticNet'></span>

<h3>Description</h3>

<p>Implements elastic net regularization for structural equation models.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \alpha\lambda| x_j| + (1-\alpha)\lambda x_j^2</code>
</p>

<p>Note that the elastic net combines ridge and lasso regularization. If <code class="reqn">\alpha = 0</code>,
the elastic net reduces to ridge regularization. If <code class="reqn">\alpha = 1</code> it reduces
to lasso regularization. In between, elastic net is a compromise between the shrinkage of
the lasso and the ridge penalty.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>elasticNet(
  lavaanModel,
  regularized,
  lambdas,
  alphas,
  method = "glmnet",
  modifyModel = lessSEM::modifyModel(),
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="elasticNet_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="elasticNet_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="elasticNet_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="elasticNet_+3A_alphas">alphas</code></td>
<td>
<p>numeric vector with values of the tuning parameter alpha. Must be
between 0 and 1. 0 = ridge, 1 = lasso.</p>
</td></tr>
<tr><td><code id="elasticNet_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet. With ista, the control argument can be used to switch to related procedures
(currently gist).</p>
</td></tr>
<tr><td><code id="elasticNet_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="elasticNet_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the lessSEM::controlIsta() and controlGlmnet() functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currently,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>Elastic net regularization:
</p>

<ul>
<li><p> Zou, H., &amp; Hastie, T. (2005). Regularization and variable selection via the elastic net.
Journal of the Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Model of class regularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

lsem &lt;- elasticNet(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = paste0("l", 6:15),
  lambdas = seq(0,1,length.out = 5),
  alphas = seq(0,1,length.out = 3))

# the coefficients can be accessed with:
coef(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters[1,]

# optional: plotting the paths requires installation of plotly
# plot(lsem)

#### Advanced ###
# Switching the optimizer #
# Use the "method" argument to switch the optimizer. The control argument
# must also be changed to the corresponding function:
lsemIsta &lt;- elasticNet(
  lavaanModel = lavaanModel,
  regularized = paste0("l", 6:15),
  lambdas = seq(0,1,length.out = 5),
  alphas = seq(0,1,length.out = 3),
  method = "ista",
  control = controlIsta())

# Note: The results are basically identical:
lsemIsta@parameters - lsem@parameters
</code></pre>

<hr>
<h2 id='estimates'>S4 method to exract the estimates of an object</h2><span id='topic+estimates'></span>

<h3>Description</h3>

<p>S4 method to exract the estimates of an object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimates(object, criterion = NULL, transformations = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimates_+3A_object">object</code></td>
<td>
<p>a model fitted with lessSEM</p>
</td></tr>
<tr><td><code id="estimates_+3A_criterion">criterion</code></td>
<td>
<p>fitIndice used to select the parameters</p>
</td></tr>
<tr><td><code id="estimates_+3A_transformations">transformations</code></td>
<td>
<p>boolean: Should transformations be returned?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a matrix with estimates
</p>

<hr>
<h2 id='estimates+2CcvRegularizedSEM-method'>estimates</h2><span id='topic+estimates+2CcvRegularizedSEM-method'></span>

<h3>Description</h3>

<p>estimates
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'cvRegularizedSEM'
estimates(object, criterion = NULL, transformations = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimates+2B2CcvRegularizedSEM-method_+3A_object">object</code></td>
<td>
<p>object of class cvRegularizedSEM</p>
</td></tr>
<tr><td><code id="estimates+2B2CcvRegularizedSEM-method_+3A_criterion">criterion</code></td>
<td>
<p>not used</p>
</td></tr>
<tr><td><code id="estimates+2B2CcvRegularizedSEM-method_+3A_transformations">transformations</code></td>
<td>
<p>boolean: Should transformations be returned?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a matrix with estimates
</p>

<hr>
<h2 id='estimates+2CregularizedSEM-method'>estimates</h2><span id='topic+estimates+2CregularizedSEM-method'></span>

<h3>Description</h3>

<p>estimates
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'regularizedSEM'
estimates(object, criterion = NULL, transformations = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimates+2B2CregularizedSEM-method_+3A_object">object</code></td>
<td>
<p>object of class regularizedSEM</p>
</td></tr>
<tr><td><code id="estimates+2B2CregularizedSEM-method_+3A_criterion">criterion</code></td>
<td>
<p>fit index (e.g., AIC) used to select the parameters</p>
</td></tr>
<tr><td><code id="estimates+2B2CregularizedSEM-method_+3A_transformations">transformations</code></td>
<td>
<p>boolean: Should transformations be returned?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a matrix with estimates
</p>

<hr>
<h2 id='estimates+2CregularizedSEMMixedPenalty-method'>estimates</h2><span id='topic+estimates+2CregularizedSEMMixedPenalty-method'></span>

<h3>Description</h3>

<p>estimates
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'regularizedSEMMixedPenalty'
estimates(object, criterion = NULL, transformations = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimates+2B2CregularizedSEMMixedPenalty-method_+3A_object">object</code></td>
<td>
<p>object of class regularizedSEMMixedPenalty</p>
</td></tr>
<tr><td><code id="estimates+2B2CregularizedSEMMixedPenalty-method_+3A_criterion">criterion</code></td>
<td>
<p>fit index (e.g., AIC) used to select the parameters</p>
</td></tr>
<tr><td><code id="estimates+2B2CregularizedSEMMixedPenalty-method_+3A_transformations">transformations</code></td>
<td>
<p>boolean: Should transformations be returned?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a matrix with estimates
</p>

<hr>
<h2 id='fit'>fit</h2><span id='topic+fit'></span>

<h3>Description</h3>

<p>Optimizes an object with mixed penalty. See ?mixedPenalty for more details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit(mixedPenalty)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit_+3A_mixedpenalty">mixedPenalty</code></td>
<td>
<p>object of class mixedPenalty. This object can be created
with the mixedPenalty function. Penalties can be added with the addCappedL1, addElastiNet,
addLasso, addLsp, addMcp, and addScad functions.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>throws error in case of undefined penalty combinations.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 + 
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 + 
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# We can add mixed penalties as follows:

regularized &lt;- lavaanModel |&gt;
  # create template for regularized model with mixed penalty:
  mixedPenalty() |&gt;
  # add penalty on loadings l6 - l10:
  addElasticNet(regularized = paste0("l", 11:15), 
          lambdas = seq(0,1,.1),
          alphas = .4) |&gt;
  # fit the model:
  fit()
</code></pre>

<hr>
<h2 id='fitIndices'>S4 method to compute fit indices (e.g., AIC, BIC, ...)</h2><span id='topic+fitIndices'></span>

<h3>Description</h3>

<p>S4 method to compute fit indices (e.g., AIC, BIC, ...)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fitIndices(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fitIndices_+3A_object">object</code></td>
<td>
<p>a model fitted with lessSEM</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a data.frame with fit indices
</p>

<hr>
<h2 id='fitIndices+2CcvRegularizedSEM-method'>fitIndices</h2><span id='topic+fitIndices+2CcvRegularizedSEM-method'></span>

<h3>Description</h3>

<p>fitIndices
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'cvRegularizedSEM'
fitIndices(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fitIndices+2B2CcvRegularizedSEM-method_+3A_object">object</code></td>
<td>
<p>object of class cvRegularizedSEM</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a data.frame with fit indices
</p>

<hr>
<h2 id='fitIndices+2CregularizedSEM-method'>fitIndices</h2><span id='topic+fitIndices+2CregularizedSEM-method'></span>

<h3>Description</h3>

<p>fitIndices
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'regularizedSEM'
fitIndices(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fitIndices+2B2CregularizedSEM-method_+3A_object">object</code></td>
<td>
<p>object of class regularizedSEM</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a data.frame with fit indices
</p>

<hr>
<h2 id='fitIndices+2CregularizedSEMMixedPenalty-method'>fitIndices</h2><span id='topic+fitIndices+2CregularizedSEMMixedPenalty-method'></span>

<h3>Description</h3>

<p>fitIndices
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'regularizedSEMMixedPenalty'
fitIndices(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fitIndices+2B2CregularizedSEMMixedPenalty-method_+3A_object">object</code></td>
<td>
<p>object of class regularizedSEMMixedPenalty</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a data.frame with fit indices
</p>

<hr>
<h2 id='getLavaanParameters'>getLavaanParameters</h2><span id='topic+getLavaanParameters'></span>

<h3>Description</h3>

<p>helper function: returns a labeled vector with parameters from lavaan
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getLavaanParameters(lavaanModel, removeDuplicates = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getLavaanParameters_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="getLavaanParameters_+3A_removeduplicates">removeDuplicates</code></td>
<td>
<p>should duplicated parameters be removed?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns a labeled vector with parameters from lavaan
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)
getLavaanParameters(lavaanModel)                       
</code></pre>

<hr>
<h2 id='getTuningParameterConfiguration'>getTuningParameterConfiguration</h2><span id='topic+getTuningParameterConfiguration'></span>

<h3>Description</h3>

<p>Returns the lambda, theta, and alpha values for the tuning parameters
of a regularized SEM with mixed penalty.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getTuningParameterConfiguration(
  regularizedSEMMixedPenalty,
  tuningParameterConfiguration
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getTuningParameterConfiguration_+3A_regularizedsemmixedpenalty">regularizedSEMMixedPenalty</code></td>
<td>
<p>object of type regularizedSEMMixedPenalty (see ?mixedPenalty)</p>
</td></tr>
<tr><td><code id="getTuningParameterConfiguration_+3A_tuningparameterconfiguration">tuningParameterConfiguration</code></td>
<td>
<p>integer indicating which tuningParameterConfiguration
should be extracted (e.g., 1). See the entry in the row tuningParameterConfiguration of
regularizedSEMMixedPenalty@fits and regularizedSEMMixedPenalty@parameters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data frame with penalty and tuning parameter settings
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 + 
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 + 
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# We can add mixed penalties as follows:

regularized &lt;- lavaanModel |&gt;
  # create template for regularized model with mixed penalty:
  mixedPenalty() |&gt;
  # add penalty on loadings l6 - l10:
  addLsp(regularized = paste0("l", 11:15), 
         lambdas = seq(0,1,.1),
         thetas = 2.3) |&gt;
  # fit the model:
  fit()

getTuningParameterConfiguration(regularizedSEMMixedPenalty = regularized, 
                                tuningParameterConfiguration = 2)
</code></pre>

<hr>
<h2 id='glmnetCappedL1MgSEM'>CappedL1 optimization with glmnet optimizer</h2><span id='topic+glmnetCappedL1MgSEM'></span>

<h3>Description</h3>

<p>Object for cappedL1 optimization with
glmnet optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (2) a list with control elements</p>
</dd>
<dt><code>setHessian</code></dt><dd><p>changes the Hessian of the model. Expects a matrix</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a theta and a lambda value.</p>
</dd>
</dl>

<hr>
<h2 id='glmnetCappedL1SEM'>CappedL1 optimization with glmnet optimizer</h2><span id='topic+glmnetCappedL1SEM'></span>

<h3>Description</h3>

<p>Object for cappedL1 optimization with
glmnet optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires a list with control elements</p>
</dd>
<dt><code>setHessian</code></dt><dd><p>changes the Hessian of the model. Expects a matrix</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a theta and a lambda value.</p>
</dd>
</dl>

<hr>
<h2 id='glmnetEnetGeneralPurpose'>elastic net optimization with glmnet optimizer</h2><span id='topic+glmnetEnetGeneralPurpose'></span>

<h3>Description</h3>

<p>Object for elastic net optimization with
glmnet optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (1) a vector with weights for each
parameter and (2) a list with control elements</p>
</dd>
<dt><code>setHessian</code></dt><dd><p>changes the Hessian of the model. Expects a matrix</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
an R function to compute the fit, an R function to compute the gradients, a
list with elements the fit and gradient function require, a lambda and an alpha value.</p>
</dd>
</dl>

<hr>
<h2 id='glmnetEnetGeneralPurposeCpp'>elastic net optimization with glmnet optimizer</h2><span id='topic+glmnetEnetGeneralPurposeCpp'></span>

<h3>Description</h3>

<p>Object for elastic net optimization with
glmnet optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (1) a vector with weights for each
parameter and (2) a list with control elements</p>
</dd>
<dt><code>setHessian</code></dt><dd><p>changes the Hessian of the model. Expects a matrix</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEXP function pointer to compute the fit, a SEXP function pointer to compute the gradients, a
list with elements the fit and gradient function require, a lambda and an alpha value.</p>
</dd>
</dl>

<hr>
<h2 id='glmnetEnetMgSEM'>elastic net optimization with glmnet optimizer</h2><span id='topic+glmnetEnetMgSEM'></span>

<h3>Description</h3>

<p>Object for elastic net optimization with
glmnet optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (1) a vector with weights for each
parameter and (2) a list with control elements</p>
</dd>
<dt><code>setHessian</code></dt><dd><p>changes the Hessian of the model. Expects a matrix</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a lambda and an alpha value.</p>
</dd>
</dl>

<hr>
<h2 id='glmnetEnetSEM'>elastic net optimization with glmnet optimizer</h2><span id='topic+glmnetEnetSEM'></span>

<h3>Description</h3>

<p>Object for elastic net optimization with
glmnet optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (1) a vector with weights for each
parameter and (2) a list with control elements</p>
</dd>
<dt><code>setHessian</code></dt><dd><p>changes the Hessian of the model. Expects a matrix</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a lambda and an alpha value.</p>
</dd>
</dl>

<hr>
<h2 id='glmnetLspMgSEM'>lsp optimization with glmnet optimizer</h2><span id='topic+glmnetLspMgSEM'></span>

<h3>Description</h3>

<p>Object for lsp optimization with
glmnet optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (2) a list with control elements</p>
</dd>
<dt><code>setHessian</code></dt><dd><p>changes the Hessian of the model. Expects a matrix</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a theta and a lambda value.</p>
</dd>
</dl>

<hr>
<h2 id='glmnetLspSEM'>lsp optimization with glmnet optimizer</h2><span id='topic+glmnetLspSEM'></span>

<h3>Description</h3>

<p>Object for lsp optimization with
glmnet optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires a list with control elements</p>
</dd>
<dt><code>setHessian</code></dt><dd><p>changes the Hessian of the model. Expects a matrix</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a theta and a lambda value.</p>
</dd>
</dl>

<hr>
<h2 id='glmnetMcpMgSEM'>mcp optimization with glmnet optimizer</h2><span id='topic+glmnetMcpMgSEM'></span>

<h3>Description</h3>

<p>Object for mcp optimization with
glmnet optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (2) a list with control elements</p>
</dd>
<dt><code>setHessian</code></dt><dd><p>changes the Hessian of the model. Expects a matrix</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a theta and a lambda value.</p>
</dd>
</dl>

<hr>
<h2 id='glmnetMcpSEM'>mcp optimization with glmnet optimizer</h2><span id='topic+glmnetMcpSEM'></span>

<h3>Description</h3>

<p>Object for mcp optimization with
glmnet optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires a list with control elements</p>
</dd>
<dt><code>setHessian</code></dt><dd><p>changes the Hessian of the model. Expects a matrix</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a theta and a lambda value.</p>
</dd>
</dl>

<hr>
<h2 id='glmnetMixedMgSEM'>mixed optimization with glmnet optimizer</h2><span id='topic+glmnetMixedMgSEM'></span>

<h3>Description</h3>

<p>Object for mixed optimization with
glmnet optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (2) a list with control elements</p>
</dd>
<dt><code>setHessian</code></dt><dd><p>changes the Hessian of the model. Expects a matrix</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a theta and a lambda value.</p>
</dd>
</dl>

<hr>
<h2 id='glmnetMixedPenaltyGeneralPurpose'>mixed optimization with glmnet optimizer</h2><span id='topic+glmnetMixedPenaltyGeneralPurpose'></span>

<h3>Description</h3>

<p>Object for mixed optimization with
glmnet optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires a list with control elements</p>
</dd>
<dt><code>setHessian</code></dt><dd><p>changes the Hessian of the model. Expects a matrix</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a theta and a lambda value.</p>
</dd>
</dl>

<hr>
<h2 id='glmnetMixedPenaltyGeneralPurposeCpp'>mixed optimization with glmnet optimizer</h2><span id='topic+glmnetMixedPenaltyGeneralPurposeCpp'></span>

<h3>Description</h3>

<p>Object for mixed optimization with
glmnet optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires a list with control elements</p>
</dd>
<dt><code>setHessian</code></dt><dd><p>changes the Hessian of the model. Expects a matrix</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a theta and a lambda value.</p>
</dd>
</dl>

<hr>
<h2 id='glmnetMixedSEM'>mixed optimization with glmnet optimizer</h2><span id='topic+glmnetMixedSEM'></span>

<h3>Description</h3>

<p>Object for mixed optimization with
glmnet optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires a list with control elements</p>
</dd>
<dt><code>setHessian</code></dt><dd><p>changes the Hessian of the model. Expects a matrix</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a theta and a lambda value.</p>
</dd>
</dl>

<hr>
<h2 id='glmnetScadMgSEM'>scad optimization with glmnet optimizer</h2><span id='topic+glmnetScadMgSEM'></span>

<h3>Description</h3>

<p>Object for scad optimization with
glmnet optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (2) a list with control elements</p>
</dd>
<dt><code>setHessian</code></dt><dd><p>changes the Hessian of the model. Expects a matrix</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a theta and a lambda value.</p>
</dd>
</dl>

<hr>
<h2 id='glmnetScadSEM'>scad optimization with glmnet optimizer</h2><span id='topic+glmnetScadSEM'></span>

<h3>Description</h3>

<p>Object for scad optimization with
glmnet optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires a list with control elements</p>
</dd>
<dt><code>setHessian</code></dt><dd><p>changes the Hessian of the model. Expects a matrix</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a theta and a lambda value.</p>
</dd>
</dl>

<hr>
<h2 id='gpAdaptiveLasso'>gpAdaptiveLasso</h2><span id='topic+gpAdaptiveLasso'></span>

<h3>Description</h3>

<p>Implements adaptive lasso regularization for general purpose optimization problems.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = p( x_j) = \frac{1}{w_j}\lambda| x_j|</code>
</p>

<p>Adaptive lasso regularization will set parameters to zero if <code class="reqn">\lambda</code> is large enough.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpAdaptiveLasso(
  par,
  regularized,
  weights = NULL,
  fn,
  gr = NULL,
  lambdas = NULL,
  nLambdas = NULL,
  reverse = TRUE,
  curve = 1,
  ...,
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpAdaptiveLasso_+3A_par">par</code></td>
<td>
<p>labeled vector with starting values</p>
</td></tr>
<tr><td><code id="gpAdaptiveLasso_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.</p>
</td></tr>
<tr><td><code id="gpAdaptiveLasso_+3A_weights">weights</code></td>
<td>
<p>labeled vector with adaptive lasso weights. NULL will use 1/abs(par)</p>
</td></tr>
<tr><td><code id="gpAdaptiveLasso_+3A_fn">fn</code></td>
<td>
<p>R function which takes the parameters as input and returns the
fit value (a single value)</p>
</td></tr>
<tr><td><code id="gpAdaptiveLasso_+3A_gr">gr</code></td>
<td>
<p>R function which takes the parameters as input and returns the
gradients of the objective function. If set to NULL, numDeriv will be used
to approximate the gradients</p>
</td></tr>
<tr><td><code id="gpAdaptiveLasso_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="gpAdaptiveLasso_+3A_nlambdas">nLambdas</code></td>
<td>
<p>alternative to lambda: If alpha = 1, lessSEM can automatically
compute the first lambda value which sets all regularized parameters to zero.
It will then generate nLambda values between 0 and the computed lambda.</p>
</td></tr>
<tr><td><code id="gpAdaptiveLasso_+3A_reverse">reverse</code></td>
<td>
<p>if set to TRUE and nLambdas is used, lessSEM will start with the
largest lambda and gradually decrease lambda. Otherwise, lessSEM will start with
the smallest lambda and gradually increase it.</p>
</td></tr>
<tr><td><code id="gpAdaptiveLasso_+3A_curve">curve</code></td>
<td>
<p>Allows for unequally spaced lambda steps (e.g., .01,.02,.05,1,5,20).
If curve is close to 1 all lambda values will be equally spaced, if curve is large
lambda values will be more concentrated close to 0. See ?lessSEM::curveLambda for more information.</p>
</td></tr>
<tr><td><code id="gpAdaptiveLasso_+3A_...">...</code></td>
<td>
<p>additional arguments passed to fn and gr</p>
</td></tr>
<tr><td><code id="gpAdaptiveLasso_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet.</p>
</td></tr>
<tr><td><code id="gpAdaptiveLasso_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The interface is similar to that of optim. Users have to supply a vector
with starting values (important: This vector <em>must</em> have labels) and a fitting
function. This fitting functions <em>must</em> take a labeled vector with parameter
values as first argument. The remaining arguments are passed with the ... argument.
This is similar to optim.
</p>
<p>The gradient function gr is optional. If set to NULL, the <span class="pkg">numDeriv</span> package
will be used to approximate the gradients. Supplying a gradient function
can result in considerable speed improvements.
</p>
<p>Adaptive lasso regularization:
</p>

<ul>
<li><p> Zou, H. (2006). The adaptive lasso and its oracle properties. Journal of the American Statistical Association,
101(476), 1418–1429. https://doi.org/10.1198/016214506000000735
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Object of class gpRegularized
</p>


<h3>Examples</h3>

<pre><code class='language-R'># This example shows how to use the optimizers
# for other objective functions. We will use
# a linear regression as an example. Note that
# this is not a useful application of the optimizers
# as there are specialized packages for linear regression
# (e.g., glmnet)

library(lessSEM)
set.seed(123)

# first, we simulate data for our
# linear regression.
N &lt;- 100 # number of persons
p &lt;- 10 # number of predictors
X &lt;- matrix(rnorm(N*p),	nrow = N, ncol = p) # design matrix
b &lt;- c(rep(1,4), 
       rep(0,6)) # true regression weights
y &lt;- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)

# First, we must construct a fiting function
# which returns a single value. We will use
# the residual sum squared as fitting function.

# Let's start setting up the fitting function:
fittingFunction &lt;- function(par, y, X, N){
  # par is the parameter vector
  # y is the observed dependent variable
  # X is the design matrix
  # N is the sample size
  pred &lt;- X %*% matrix(par, ncol = 1) #be explicit here: 
  # we need par to be a column vector
  sse &lt;- sum((y - pred)^2)
  # we scale with .5/N to get the same results as glmnet
  return((.5/N)*sse)
}

# let's define the starting values:
b &lt;- c(solve(t(X)%*%X)%*%t(X)%*%y) # we will use the lm estimates
names(b) &lt;- paste0("b", 1:length(b))
# names of regularized parameters
regularized &lt;- paste0("b",1:p)

# define the weight for each of the parameters
weights &lt;- 1/abs(b)
# we will re-scale the weights for equivalence to glmnet.
# see ?glmnet for more details
weights &lt;- length(b)*weights/sum(weights)

# optimize
adaptiveLassoPen &lt;- gpAdaptiveLasso(
  par = b, 
  regularized = regularized, 
  weights = weights,
  fn = fittingFunction, 
  lambdas = seq(0,1,.01), 
  X = X,
  y = y,
  N = N
)
plot(adaptiveLassoPen)
# You can access the fit results as follows:
adaptiveLassoPen@fits
# Note that we won't compute any fit measures automatically, as
# we cannot be sure how the AIC, BIC, etc are defined for your objective function 

# for comparison:
# library(glmnet)
# coef(glmnet(x = X,
#            y = y,
#            penalty.factor = weights,
#            lambda = adaptiveLassoPen@fits$lambda[20],
#            intercept = FALSE,
#            standardize = FALSE))[,1]
# adaptiveLassoPen@parameters[20,]
</code></pre>

<hr>
<h2 id='gpAdaptiveLassoCpp'>gpAdaptiveLassoCpp</h2><span id='topic+gpAdaptiveLassoCpp'></span>

<h3>Description</h3>

<p>Implements adaptive lasso regularization for general purpose optimization problems with C++ functions.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = p( x_j) = \frac{1}{w_j}\lambda| x_j|</code>
</p>

<p>Adaptive lasso regularization will set parameters to zero if <code class="reqn">\lambda</code> is large enough.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpAdaptiveLassoCpp(
  par,
  regularized,
  weights = NULL,
  fn,
  gr,
  lambdas = NULL,
  nLambdas = NULL,
  curve = 1,
  additionalArguments,
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpAdaptiveLassoCpp_+3A_par">par</code></td>
<td>
<p>labeled vector with starting values</p>
</td></tr>
<tr><td><code id="gpAdaptiveLassoCpp_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.</p>
</td></tr>
<tr><td><code id="gpAdaptiveLassoCpp_+3A_weights">weights</code></td>
<td>
<p>labeled vector with adaptive lasso weights. NULL will use 1/abs(par)</p>
</td></tr>
<tr><td><code id="gpAdaptiveLassoCpp_+3A_fn">fn</code></td>
<td>
<p>R function which takes the parameters as input and returns the
fit value (a single value)</p>
</td></tr>
<tr><td><code id="gpAdaptiveLassoCpp_+3A_gr">gr</code></td>
<td>
<p>R function which takes the parameters as input and returns the
gradients of the objective function. If set to NULL, numDeriv will be used
to approximate the gradients</p>
</td></tr>
<tr><td><code id="gpAdaptiveLassoCpp_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="gpAdaptiveLassoCpp_+3A_nlambdas">nLambdas</code></td>
<td>
<p>alternative to lambda: If alpha = 1, lessSEM can automatically
compute the first lambda value which sets all regularized parameters to zero.
It will then generate nLambda values between 0 and the computed lambda.</p>
</td></tr>
<tr><td><code id="gpAdaptiveLassoCpp_+3A_curve">curve</code></td>
<td>
<p>Allows for unequally spaced lambda steps (e.g., .01,.02,.05,1,5,20).
If curve is close to 1 all lambda values will be equally spaced, if curve is large
lambda values will be more concentrated close to 0. See ?lessSEM::curveLambda for more information.</p>
</td></tr>
<tr><td><code id="gpAdaptiveLassoCpp_+3A_additionalarguments">additionalArguments</code></td>
<td>
<p>list with additional arguments passed to fn and gr</p>
</td></tr>
<tr><td><code id="gpAdaptiveLassoCpp_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet.</p>
</td></tr>
<tr><td><code id="gpAdaptiveLassoCpp_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The interface is inspired by optim, but a bit more restrictive. Users have to supply a vector
with starting values (important: This vector <em>must</em> have labels), a fitting
function, and a gradient function. These fitting functions <em>must</em> take an const Rcpp::NumericVector&amp; with parameter
values as first argument and an Rcpp::List&amp; as second argument
</p>
<p>Adaptive lasso regularization:
</p>

<ul>
<li><p> Zou, H. (2006). The adaptive lasso and its oracle properties. Journal of the American Statistical Association,
101(476), 1418–1429. https://doi.org/10.1198/016214506000000735
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Object of class gpRegularized
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# This example shows how to use the optimizers
# for C++ objective functions. We will use
# a linear regression as an example. Note that
# this is not a useful application of the optimizers
# as there are specialized packages for linear regression
# (e.g., glmnet)

library(Rcpp)
library(lessSEM)

linreg &lt;- '
// [[Rcpp::depends(RcppArmadillo)]]
#include &lt;RcppArmadillo.h&gt;

// [[Rcpp::export]]
double fitfunction(const Rcpp::NumericVector&amp; parameters, Rcpp::List&amp; data){
  // extract all required elements:
  arma::colvec b = Rcpp::as&lt;arma::colvec&gt;(parameters);
  arma::colvec y = Rcpp::as&lt;arma::colvec&gt;(data["y"]); // the dependent variable
  arma::mat X = Rcpp::as&lt;arma::mat&gt;(data["X"]); // the design matrix
  
  // compute the sum of squared errors:
    arma::mat sse = arma::trans(y-X*b)*(y-X*b);
    
    // other packages, such as glmnet, scale the sse with 
    // 1/(2*N), where N is the sample size. We will do that here as well
    
    sse *= 1.0/(2.0 * y.n_elem);
    
    // note: We must return a double, but the sse is a matrix
    // To get a double, just return the single value that is in 
    // this matrix:
      return(sse(0,0));
}

// [[Rcpp::export]]
arma::rowvec gradientfunction(const Rcpp::NumericVector&amp; parameters, Rcpp::List&amp; data){
  // extract all required elements:
  arma::colvec b = Rcpp::as&lt;arma::colvec&gt;(parameters);
  arma::colvec y = Rcpp::as&lt;arma::colvec&gt;(data["y"]); // the dependent variable
  arma::mat X = Rcpp::as&lt;arma::mat&gt;(data["X"]); // the design matrix
  
  // note: we want to return our gradients as row-vector; therefore,
  // we have to transpose the resulting column-vector:
    arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);
    
    // other packages, such as glmnet, scale the sse with 
    // 1/(2*N), where N is the sample size. We will do that here as well
    
    gradients *= (.5/y.n_rows);
    
    return(gradients);
}

// Dirk Eddelbuettel at
// https://gallery.rcpp.org/articles/passing-cpp-function-pointers/
typedef double (*fitFunPtr)(const Rcpp::NumericVector&amp;, //parameters
                Rcpp::List&amp; //additional elements
);
typedef Rcpp::XPtr&lt;fitFunPtr&gt; fitFunPtr_t;

typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&amp;, //parameters
                      Rcpp::List&amp; //additional elements
);
typedef Rcpp::XPtr&lt;gradientFunPtr&gt; gradientFunPtr_t;

// [[Rcpp::export]]
fitFunPtr_t fitfunPtr() {
        return(fitFunPtr_t(new fitFunPtr(&amp;fitfunction)));
}

// [[Rcpp::export]]
gradientFunPtr_t gradfunPtr() {
        return(gradientFunPtr_t(new gradientFunPtr(&amp;gradientfunction)));
}
'

Rcpp::sourceCpp(code = linreg)

ffp &lt;- fitfunPtr()
gfp &lt;- gradfunPtr()

N &lt;- 100 # number of persons
p &lt;- 10 # number of predictors
X &lt;- matrix(rnorm(N*p),	nrow = N, ncol = p) # design matrix
b &lt;- c(rep(1,4), 
       rep(0,6)) # true regression weights
y &lt;- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)

data &lt;- list("y" = y,
             "X" = cbind(1,X))
parameters &lt;- rep(0, ncol(data$X))
names(parameters) &lt;- paste0("b", 0:(length(parameters)-1))

al1 &lt;- gpAdaptiveLassoCpp(par = parameters, 
                 regularized = paste0("b", 1:(length(b)-1)),
                 fn = ffp, 
                 gr = gfp, 
                 lambdas = seq(0,1,.1), 
                 additionalArguments = data)

al1@parameters

</code></pre>

<hr>
<h2 id='gpCappedL1'>gpCappedL1</h2><span id='topic+gpCappedL1'></span>

<h3>Description</h3>

<p>Implements cappedL1 regularization for general purpose optimization problems.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \lambda \min(| x_j|, \theta)</code>
</p>

<p>where <code class="reqn">\theta &gt; 0</code>. The cappedL1 penalty is identical to the lasso for
parameters which are below <code class="reqn">\theta</code> and identical to a constant for parameters
above <code class="reqn">\theta</code>. As adding a constant to the fitting function will not change its
minimum, larger parameters can stay unregularized while smaller ones are set to zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpCappedL1(
  par,
  fn,
  gr = NULL,
  ...,
  regularized,
  lambdas,
  thetas,
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpCappedL1_+3A_par">par</code></td>
<td>
<p>labeled vector with starting values</p>
</td></tr>
<tr><td><code id="gpCappedL1_+3A_fn">fn</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the fit value (a single value)</p>
</td></tr>
<tr><td><code id="gpCappedL1_+3A_gr">gr</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the gradients of the objective function.
If set to NULL, numDeriv will be used to approximate the gradients</p>
</td></tr>
<tr><td><code id="gpCappedL1_+3A_...">...</code></td>
<td>
<p>additional arguments passed to fn and gr</p>
</td></tr>
<tr><td><code id="gpCappedL1_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.</p>
</td></tr>
<tr><td><code id="gpCappedL1_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="gpCappedL1_+3A_thetas">thetas</code></td>
<td>
<p>parameters whose absolute value is above this threshold will be penalized with
a constant (theta)</p>
</td></tr>
<tr><td><code id="gpCappedL1_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet.</p>
</td></tr>
<tr><td><code id="gpCappedL1_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The interface is similar to that of optim. Users have to supply a vector
with starting values (important: This vector <em>must</em> have labels) and a fitting
function. This fitting functions <em>must</em> take a labeled vector with parameter
values as first argument. The remaining arguments are passed with the ... argument.
This is similar to optim.
</p>
<p>The gradient function gr is optional. If set to NULL, the <span class="pkg">numDeriv</span> package
will be used to approximate the gradients. Supplying a gradient function
can result in considerable speed improvements.
</p>
<p>CappedL1 regularization:
</p>

<ul>
<li><p> Zhang, T. (2010). Analysis of Multi-stage Convex Relaxation for Sparse Regularization.
Journal of Machine Learning Research, 11, 1081–1107.
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Object of class gpRegularized
</p>


<h3>Examples</h3>

<pre><code class='language-R'># This example shows how to use the optimizers
# for other objective functions. We will use
# a linear regression as an example. Note that
# this is not a useful application of the optimizers
# as there are specialized packages for linear regression
# (e.g., glmnet)

# This example shows how to use the optimizers
# for other objective functions. We will use
# a linear regression as an example. Note that
# this is not a useful application of the optimizers
# as there are specialized packages for linear regression
# (e.g., glmnet)

library(lessSEM)
set.seed(123)

# first, we simulate data for our
# linear regression.
N &lt;- 100 # number of persons
p &lt;- 10 # number of predictors
X &lt;- matrix(rnorm(N*p),	nrow = N, ncol = p) # design matrix
b &lt;- c(rep(1,4),
       rep(0,6)) # true regression weights
y &lt;- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)

# First, we must construct a fiting function
# which returns a single value. We will use
# the residual sum squared as fitting function.

# Let's start setting up the fitting function:
fittingFunction &lt;- function(par, y, X, N){
  # par is the parameter vector
  # y is the observed dependent variable
  # X is the design matrix
  # N is the sample size
  pred &lt;- X %*% matrix(par, ncol = 1) #be explicit here:
  # we need par to be a column vector
  sse &lt;- sum((y - pred)^2)
  # we scale with .5/N to get the same results as glmnet
  return((.5/N)*sse)
}

# let's define the starting values:
b &lt;- c(solve(t(X)%*%X)%*%t(X)%*%y) # we will use the lm estimates
names(b) &lt;- paste0("b", 1:length(b))
# names of regularized parameters
regularized &lt;- paste0("b",1:p)

# optimize
cL1 &lt;- gpCappedL1(
  par = b,
  regularized = regularized,
  fn = fittingFunction,
  lambdas = seq(0,1,.1),
  thetas = c(0.001, .5, 1),
  X = X,
  y = y,
  N = N
)

# optional: plot requires plotly package
# plot(cL1)

# for comparison

fittingFunction &lt;- function(par, y, X, N, lambda, theta){
  pred &lt;- X %*% matrix(par, ncol = 1)
  sse &lt;- sum((y - pred)^2)
  smoothAbs &lt;- sqrt(par^2 + 1e-8)
  pen &lt;- lambda * ifelse(smoothAbs &lt; theta, smoothAbs, theta)
  return((.5/N)*sse + sum(pen))
}

round(
  optim(par = b,
      fn = fittingFunction,
      y = y,
      X = X,
      N = N,
      lambda =  cL1@fits$lambda[15],
      theta =  cL1@fits$theta[15],
      method = "BFGS")$par,
  4)
cL1@parameters[15,]
</code></pre>

<hr>
<h2 id='gpCappedL1Cpp'>gpCappedL1Cpp</h2><span id='topic+gpCappedL1Cpp'></span>

<h3>Description</h3>

<p>Implements cappedL1 regularization for general purpose optimization problems with C++ functions.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \lambda \min(| x_j|, \theta)</code>
</p>

<p>where <code class="reqn">\theta &gt; 0</code>. The cappedL1 penalty is identical to the lasso for
parameters which are below <code class="reqn">\theta</code> and identical to a constant for parameters
above <code class="reqn">\theta</code>. As adding a constant to the fitting function will not change its
minimum, larger parameters can stay unregularized while smaller ones are set to zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpCappedL1Cpp(
  par,
  fn,
  gr,
  additionalArguments,
  regularized,
  lambdas,
  thetas,
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpCappedL1Cpp_+3A_par">par</code></td>
<td>
<p>labeled vector with starting values</p>
</td></tr>
<tr><td><code id="gpCappedL1Cpp_+3A_fn">fn</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the fit value (a single value)</p>
</td></tr>
<tr><td><code id="gpCappedL1Cpp_+3A_gr">gr</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the gradients of the objective function.
If set to NULL, numDeriv will be used to approximate the gradients</p>
</td></tr>
<tr><td><code id="gpCappedL1Cpp_+3A_additionalarguments">additionalArguments</code></td>
<td>
<p>list with additional arguments passed to fn and gr</p>
</td></tr>
<tr><td><code id="gpCappedL1Cpp_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="gpCappedL1Cpp_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="gpCappedL1Cpp_+3A_thetas">thetas</code></td>
<td>
<p>parameters whose absolute value is above this threshold will be penalized with
a constant (theta)</p>
</td></tr>
<tr><td><code id="gpCappedL1Cpp_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet.</p>
</td></tr>
<tr><td><code id="gpCappedL1Cpp_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The interface is inspired by optim, but a bit more restrictive. Users have to supply a vector
with starting values (important: This vector <em>must</em> have labels), a fitting
function, and a gradient function. These fitting functions <em>must</em> take an const Rcpp::NumericVector&amp; with parameter
values as first argument and an Rcpp::List&amp; as second argument
</p>
<p>CappedL1 regularization:
</p>

<ul>
<li><p> Zhang, T. (2010). Analysis of Multi-stage Convex Relaxation for Sparse Regularization.
Journal of Machine Learning Research, 11, 1081–1107.
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Object of class gpRegularized
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# This example shows how to use the optimizers
# for C++ objective functions. We will use
# a linear regression as an example. Note that
# this is not a useful application of the optimizers
# as there are specialized packages for linear regression
# (e.g., glmnet)

library(Rcpp)
library(lessSEM)

linreg &lt;- '
// [[Rcpp::depends(RcppArmadillo)]]
#include &lt;RcppArmadillo.h&gt;

// [[Rcpp::export]]
double fitfunction(const Rcpp::NumericVector&amp; parameters, Rcpp::List&amp; data){
  // extract all required elements:
  arma::colvec b = Rcpp::as&lt;arma::colvec&gt;(parameters);
  arma::colvec y = Rcpp::as&lt;arma::colvec&gt;(data["y"]); // the dependent variable
  arma::mat X = Rcpp::as&lt;arma::mat&gt;(data["X"]); // the design matrix
  
  // compute the sum of squared errors:
    arma::mat sse = arma::trans(y-X*b)*(y-X*b);
    
    // other packages, such as glmnet, scale the sse with 
    // 1/(2*N), where N is the sample size. We will do that here as well
    
    sse *= 1.0/(2.0 * y.n_elem);
    
    // note: We must return a double, but the sse is a matrix
    // To get a double, just return the single value that is in 
    // this matrix:
      return(sse(0,0));
}

// [[Rcpp::export]]
arma::rowvec gradientfunction(const Rcpp::NumericVector&amp; parameters, Rcpp::List&amp; data){
  // extract all required elements:
  arma::colvec b = Rcpp::as&lt;arma::colvec&gt;(parameters);
  arma::colvec y = Rcpp::as&lt;arma::colvec&gt;(data["y"]); // the dependent variable
  arma::mat X = Rcpp::as&lt;arma::mat&gt;(data["X"]); // the design matrix
  
  // note: we want to return our gradients as row-vector; therefore,
  // we have to transpose the resulting column-vector:
    arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);
    
    // other packages, such as glmnet, scale the sse with 
    // 1/(2*N), where N is the sample size. We will do that here as well
    
    gradients *= (.5/y.n_rows);
    
    return(gradients);
}

// https://gallery.rcpp.org/articles/passing-cpp-function-pointers/
typedef double (*fitFunPtr)(const Rcpp::NumericVector&amp;, //parameters
                Rcpp::List&amp; //additional elements
);
typedef Rcpp::XPtr&lt;fitFunPtr&gt; fitFunPtr_t;

typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&amp;, //parameters
                      Rcpp::List&amp; //additional elements
);
typedef Rcpp::XPtr&lt;gradientFunPtr&gt; gradientFunPtr_t;

// [[Rcpp::export]]
fitFunPtr_t fitfunPtr() {
        return(fitFunPtr_t(new fitFunPtr(&amp;fitfunction)));
}

// [[Rcpp::export]]
gradientFunPtr_t gradfunPtr() {
        return(gradientFunPtr_t(new gradientFunPtr(&amp;gradientfunction)));
}
'

Rcpp::sourceCpp(code = linreg)

ffp &lt;- fitfunPtr()
gfp &lt;- gradfunPtr()

N &lt;- 100 # number of persons
p &lt;- 10 # number of predictors
X &lt;- matrix(rnorm(N*p),	nrow = N, ncol = p) # design matrix
b &lt;- c(rep(1,4), 
       rep(0,6)) # true regression weights
y &lt;- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)

data &lt;- list("y" = y,
             "X" = cbind(1,X))
parameters &lt;- rep(0, ncol(data$X))
names(parameters) &lt;- paste0("b", 0:(length(parameters)-1))

cL1 &lt;- gpCappedL1Cpp(par = parameters, 
                 regularized = paste0("b", 1:(length(b)-1)),
                 fn = ffp, 
                 gr = gfp, 
                 lambdas = seq(0,1,.1), 
                 thetas = seq(0.1,1,.1),
                 additionalArguments = data)

cL1@parameters

</code></pre>

<hr>
<h2 id='gpElasticNet'>gpElasticNet</h2><span id='topic+gpElasticNet'></span>

<h3>Description</h3>

<p>Implements elastic net regularization for general purpose optimization problems.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = p( x_j) = \frac{1}{w_j}\lambda| x_j|</code>
</p>

<p>Note that the elastic net combines ridge and lasso regularization. If <code class="reqn">\alpha = 0</code>,
the elastic net reduces to ridge regularization. If <code class="reqn">\alpha = 1</code> it reduces
to lasso regularization. In between, elastic net is a compromise between the shrinkage of
the lasso and the ridge penalty.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpElasticNet(
  par,
  regularized,
  fn,
  gr = NULL,
  lambdas,
  alphas,
  ...,
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpElasticNet_+3A_par">par</code></td>
<td>
<p>labeled vector with starting values</p>
</td></tr>
<tr><td><code id="gpElasticNet_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.</p>
</td></tr>
<tr><td><code id="gpElasticNet_+3A_fn">fn</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the fit value (a single value)</p>
</td></tr>
<tr><td><code id="gpElasticNet_+3A_gr">gr</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the gradients of the objective function.
If set to NULL, numDeriv will be used to approximate the gradients</p>
</td></tr>
<tr><td><code id="gpElasticNet_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="gpElasticNet_+3A_alphas">alphas</code></td>
<td>
<p>numeric vector with values of the tuning parameter alpha. Must be
between 0 and 1. 0 = ridge, 1 = lasso.</p>
</td></tr>
<tr><td><code id="gpElasticNet_+3A_...">...</code></td>
<td>
<p>additional arguments passed to fn and gr</p>
</td></tr>
<tr><td><code id="gpElasticNet_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet.</p>
</td></tr>
<tr><td><code id="gpElasticNet_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The interface is similar to that of optim. Users have to supply a vector
with starting values (important: This vector <em>must</em> have labels) and a fitting
function. This fitting functions <em>must</em> take a labeled vector with parameter
values as first argument. The remaining arguments are passed with the ... argument.
This is similar to optim.
</p>
<p>The gradient function gr is optional. If set to NULL, the <span class="pkg">numDeriv</span> package
will be used to approximate the gradients. Supplying a gradient function
can result in considerable speed improvements.
</p>
<p>Elastic net regularization:
</p>

<ul>
<li><p> Zou, H., &amp; Hastie, T. (2005). Regularization and variable selection via the elastic net.
Journal of the Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Object of class gpRegularized
</p>


<h3>Examples</h3>

<pre><code class='language-R'># This example shows how to use the optimizers
# for other objective functions. We will use
# a linear regression as an example. Note that
# this is not a useful application of the optimizers
# as there are specialized packages for linear regression
# (e.g., glmnet)

library(lessSEM)
set.seed(123)

# first, we simulate data for our
# linear regression.
N &lt;- 100 # number of persons
p &lt;- 10 # number of predictors
X &lt;- matrix(rnorm(N*p),	nrow = N, ncol = p) # design matrix
b &lt;- c(rep(1,4),
       rep(0,6)) # true regression weights
y &lt;- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)

# First, we must construct a fiting function
# which returns a single value. We will use
# the residual sum squared as fitting function.

# Let's start setting up the fitting function:
fittingFunction &lt;- function(par, y, X, N){
  # par is the parameter vector
  # y is the observed dependent variable
  # X is the design matrix
  # N is the sample size
  pred &lt;- X %*% matrix(par, ncol = 1) #be explicit here:
  # we need par to be a column vector
  sse &lt;- sum((y - pred)^2)
  # we scale with .5/N to get the same results as glmnet
  return((.5/N)*sse)
}

# let's define the starting values:
b &lt;- c(solve(t(X)%*%X)%*%t(X)%*%y) # we will use the lm estimates
names(b) &lt;- paste0("b", 1:length(b))
# names of regularized parameters
regularized &lt;- paste0("b",1:p)

# optimize
elasticNetPen &lt;- gpElasticNet(
  par = b,
  regularized = regularized,
  fn = fittingFunction,
  lambdas = seq(0,1,.1),
  alphas = c(0, .5, 1),
  X = X,
  y = y,
  N = N
)

# optional: plot requires plotly package
# plot(elasticNetPen)

# for comparison:
fittingFunction &lt;- function(par, y, X, N, lambda, alpha){
  pred &lt;- X %*% matrix(par, ncol = 1)
  sse &lt;- sum((y - pred)^2)
  return((.5/N)*sse + (1-alpha)*lambda * sum(par^2) + alpha*lambda *sum(sqrt(par^2 + 1e-8)))
}

round(
  optim(par = b,
      fn = fittingFunction,
      y = y,
      X = X,
      N = N,
      lambda =  elasticNetPen@fits$lambda[15],
      alpha =  elasticNetPen@fits$alpha[15],
      method = "BFGS")$par,
  4)
elasticNetPen@parameters[15,]
</code></pre>

<hr>
<h2 id='gpElasticNetCpp'>gpElasticNetCpp</h2><span id='topic+gpElasticNetCpp'></span>

<h3>Description</h3>

<p>Implements elastic net regularization for general purpose optimization problems with C++ functions.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = p( x_j) = \frac{1}{w_j}\lambda| x_j|</code>
</p>

<p>Note that the elastic net combines ridge and lasso regularization. If <code class="reqn">\alpha = 0</code>,
the elastic net reduces to ridge regularization. If <code class="reqn">\alpha = 1</code> it reduces
to lasso regularization. In between, elastic net is a compromise between the shrinkage of
the lasso and the ridge penalty.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpElasticNetCpp(
  par,
  regularized,
  fn,
  gr,
  lambdas,
  alphas,
  additionalArguments,
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpElasticNetCpp_+3A_par">par</code></td>
<td>
<p>labeled vector with starting values</p>
</td></tr>
<tr><td><code id="gpElasticNetCpp_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.</p>
</td></tr>
<tr><td><code id="gpElasticNetCpp_+3A_fn">fn</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the fit value (a single value)</p>
</td></tr>
<tr><td><code id="gpElasticNetCpp_+3A_gr">gr</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the gradients of the objective function.
If set to NULL, numDeriv will be used to approximate the gradients</p>
</td></tr>
<tr><td><code id="gpElasticNetCpp_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="gpElasticNetCpp_+3A_alphas">alphas</code></td>
<td>
<p>numeric vector with values of the tuning parameter alpha. Must be
between 0 and 1. 0 = ridge, 1 = lasso.</p>
</td></tr>
<tr><td><code id="gpElasticNetCpp_+3A_additionalarguments">additionalArguments</code></td>
<td>
<p>list with additional arguments passed to fn and gr</p>
</td></tr>
<tr><td><code id="gpElasticNetCpp_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet.</p>
</td></tr>
<tr><td><code id="gpElasticNetCpp_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The interface is inspired by optim, but a bit more restrictive. Users have to supply a vector
with starting values (important: This vector <em>must</em> have labels), a fitting
function, and a gradient function. These fitting functions <em>must</em> take an const Rcpp::NumericVector&amp; with parameter
values as first argument and an Rcpp::List&amp; as second argument
</p>
<p>Elastic net regularization:
</p>

<ul>
<li><p> Zou, H., &amp; Hastie, T. (2005). Regularization and variable selection via the elastic net.
Journal of the Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Object of class gpRegularized
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# This example shows how to use the optimizers
# for C++ objective functions. We will use
# a linear regression as an example. Note that
# this is not a useful application of the optimizers
# as there are specialized packages for linear regression
# (e.g., glmnet)

library(Rcpp)
library(lessSEM)

linreg &lt;- '
// [[Rcpp::depends(RcppArmadillo)]]
#include &lt;RcppArmadillo.h&gt;

// [[Rcpp::export]]
double fitfunction(const Rcpp::NumericVector&amp; parameters, Rcpp::List&amp; data){
  // extract all required elements:
  arma::colvec b = Rcpp::as&lt;arma::colvec&gt;(parameters);
  arma::colvec y = Rcpp::as&lt;arma::colvec&gt;(data["y"]); // the dependent variable
  arma::mat X = Rcpp::as&lt;arma::mat&gt;(data["X"]); // the design matrix
  
  // compute the sum of squared errors:
    arma::mat sse = arma::trans(y-X*b)*(y-X*b);
    
    // other packages, such as glmnet, scale the sse with 
    // 1/(2*N), where N is the sample size. We will do that here as well
    
    sse *= 1.0/(2.0 * y.n_elem);
    
    // note: We must return a double, but the sse is a matrix
    // To get a double, just return the single value that is in 
    // this matrix:
      return(sse(0,0));
}

// [[Rcpp::export]]
arma::rowvec gradientfunction(const Rcpp::NumericVector&amp; parameters, Rcpp::List&amp; data){
  // extract all required elements:
  arma::colvec b = Rcpp::as&lt;arma::colvec&gt;(parameters);
  arma::colvec y = Rcpp::as&lt;arma::colvec&gt;(data["y"]); // the dependent variable
  arma::mat X = Rcpp::as&lt;arma::mat&gt;(data["X"]); // the design matrix
  
  // note: we want to return our gradients as row-vector; therefore,
  // we have to transpose the resulting column-vector:
    arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);
    
    // other packages, such as glmnet, scale the sse with 
    // 1/(2*N), where N is the sample size. We will do that here as well
    
    gradients *= (.5/y.n_rows);
    
    return(gradients);
}

// Dirk Eddelbuettel at
// https://gallery.rcpp.org/articles/passing-cpp-function-pointers/
typedef double (*fitFunPtr)(const Rcpp::NumericVector&amp;, //parameters
                Rcpp::List&amp; //additional elements
);
typedef Rcpp::XPtr&lt;fitFunPtr&gt; fitFunPtr_t;

typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&amp;, //parameters
                      Rcpp::List&amp; //additional elements
);
typedef Rcpp::XPtr&lt;gradientFunPtr&gt; gradientFunPtr_t;

// [[Rcpp::export]]
fitFunPtr_t fitfunPtr() {
        return(fitFunPtr_t(new fitFunPtr(&amp;fitfunction)));
}

// [[Rcpp::export]]
gradientFunPtr_t gradfunPtr() {
        return(gradientFunPtr_t(new gradientFunPtr(&amp;gradientfunction)));
}
'

Rcpp::sourceCpp(code = linreg)

ffp &lt;- fitfunPtr()
gfp &lt;- gradfunPtr()

N &lt;- 100 # number of persons
p &lt;- 10 # number of predictors
X &lt;- matrix(rnorm(N*p),	nrow = N, ncol = p) # design matrix
b &lt;- c(rep(1,4), 
       rep(0,6)) # true regression weights
y &lt;- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)

data &lt;- list("y" = y,
             "X" = cbind(1,X))
parameters &lt;- rep(0, ncol(data$X))
names(parameters) &lt;- paste0("b", 0:(length(parameters)-1))

en &lt;- gpElasticNetCpp(par = parameters, 
                 regularized = paste0("b", 1:(length(b)-1)),
                 fn = ffp, 
                 gr = gfp, 
                 lambdas = seq(0,1,.1), 
                 alphas = c(0,.5,1),
                 additionalArguments = data)

en@parameters

</code></pre>

<hr>
<h2 id='gpLasso'>gpLasso</h2><span id='topic+gpLasso'></span>

<h3>Description</h3>

<p>Implements lasso regularization for general purpose optimization problems.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \lambda |x_j|</code>
</p>

<p>Lasso regularization will set parameters to zero if <code class="reqn">\lambda</code> is large enough
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpLasso(
  par,
  regularized,
  fn,
  gr = NULL,
  lambdas = NULL,
  nLambdas = NULL,
  reverse = TRUE,
  curve = 1,
  ...,
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpLasso_+3A_par">par</code></td>
<td>
<p>labeled vector with starting values</p>
</td></tr>
<tr><td><code id="gpLasso_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.</p>
</td></tr>
<tr><td><code id="gpLasso_+3A_fn">fn</code></td>
<td>
<p>R function which takes the parameters as input and returns the
fit value (a single value)</p>
</td></tr>
<tr><td><code id="gpLasso_+3A_gr">gr</code></td>
<td>
<p>R function which takes the parameters as input and returns the
gradients of the objective function. If set to NULL, numDeriv will be used
to approximate the gradients</p>
</td></tr>
<tr><td><code id="gpLasso_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="gpLasso_+3A_nlambdas">nLambdas</code></td>
<td>
<p>alternative to lambda: If alpha = 1, lessSEM can automatically
compute the first lambda value which sets all regularized parameters to zero.
It will then generate nLambda values between 0 and the computed lambda.</p>
</td></tr>
<tr><td><code id="gpLasso_+3A_reverse">reverse</code></td>
<td>
<p>if set to TRUE and nLambdas is used, lessSEM will start with the
largest lambda and gradually decrease lambda. Otherwise, lessSEM will start with
the smallest lambda and gradually increase it.</p>
</td></tr>
<tr><td><code id="gpLasso_+3A_curve">curve</code></td>
<td>
<p>Allows for unequally spaced lambda steps (e.g., .01,.02,.05,1,5,20).
If curve is close to 1 all lambda values will be equally spaced, if curve is large
lambda values will be more concentrated close to 0. See ?lessSEM::curveLambda for more information.</p>
</td></tr>
<tr><td><code id="gpLasso_+3A_...">...</code></td>
<td>
<p>additional arguments passed to fn and gr</p>
</td></tr>
<tr><td><code id="gpLasso_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet.</p>
</td></tr>
<tr><td><code id="gpLasso_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The interface is similar to that of optim. Users have to supply a vector
with starting values (important: This vector <em>must</em> have labels) and a fitting
function. This fitting functions <em>must</em> take a labeled vector with parameter
values as first argument. The remaining arguments are passed with the ... argument.
This is similar to optim.
</p>
<p>The gradient function gr is optional. If set to NULL, the <span class="pkg">numDeriv</span> package
will be used to approximate the gradients. Supplying a gradient function
can result in considerable speed improvements.
</p>
<p>Lasso regularization:
</p>

<ul>
<li><p> Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), 58(1), 267–288.
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Object of class gpRegularized
</p>


<h3>Examples</h3>

<pre><code class='language-R'># This example shows how to use the optimizers
# for other objective functions. We will use
# a linear regression as an example. Note that
# this is not a useful application of the optimizers
# as there are specialized packages for linear regression
# (e.g., glmnet)

library(lessSEM)
set.seed(123)

# first, we simulate data for our
# linear regression.
N &lt;- 100 # number of persons
p &lt;- 10 # number of predictors
X &lt;- matrix(rnorm(N*p),	nrow = N, ncol = p) # design matrix
b &lt;- c(rep(1,4), 
       rep(0,6)) # true regression weights
y &lt;- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)

# First, we must construct a fiting function
# which returns a single value. We will use
# the residual sum squared as fitting function.

# Let's start setting up the fitting function:
fittingFunction &lt;- function(par, y, X, N){
  # par is the parameter vector
  # y is the observed dependent variable
  # X is the design matrix
  # N is the sample size
  pred &lt;- X %*% matrix(par, ncol = 1) #be explicit here: 
  # we need par to be a column vector
  sse &lt;- sum((y - pred)^2)
  # we scale with .5/N to get the same results as glmnet
  return((.5/N)*sse)
}

# let's define the starting values:
b &lt;- rep(0,p)
names(b) &lt;- paste0("b", 1:length(b))
# names of regularized parameters
regularized &lt;- paste0("b",1:p)

# optimize
lassoPen &lt;- gpLasso(
  par = b, 
  regularized = regularized, 
  fn = fittingFunction, 
  nLambdas = 100, 
  X = X,
  y = y,
  N = N
)
plot(lassoPen)

# You can access the fit results as follows:
lassoPen@fits
# Note that we won't compute any fit measures automatically, as
# we cannot be sure how the AIC, BIC, etc are defined for your objective function 

</code></pre>

<hr>
<h2 id='gpLassoCpp'>gpLassoCpp</h2><span id='topic+gpLassoCpp'></span>

<h3>Description</h3>

<p>Implements lasso regularization for general purpose optimization problems with C++ functions.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \lambda |x_j|</code>
</p>

<p>Lasso regularization will set parameters to zero if <code class="reqn">\lambda</code> is large enough
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpLassoCpp(
  par,
  regularized,
  fn,
  gr,
  lambdas = NULL,
  nLambdas = NULL,
  curve = 1,
  additionalArguments,
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpLassoCpp_+3A_par">par</code></td>
<td>
<p>labeled vector with starting values</p>
</td></tr>
<tr><td><code id="gpLassoCpp_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.</p>
</td></tr>
<tr><td><code id="gpLassoCpp_+3A_fn">fn</code></td>
<td>
<p>pointer to Rcpp function which takes the parameters as input and returns the
fit value (a single value)</p>
</td></tr>
<tr><td><code id="gpLassoCpp_+3A_gr">gr</code></td>
<td>
<p>pointer to Rcpp function which takes the parameters as input and returns the
gradients of the objective function.</p>
</td></tr>
<tr><td><code id="gpLassoCpp_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="gpLassoCpp_+3A_nlambdas">nLambdas</code></td>
<td>
<p>alternative to lambda: If alpha = 1, lessSEM can automatically
compute the first lambda value which sets all regularized parameters to zero.
It will then generate nLambda values between 0 and the computed lambda.</p>
</td></tr>
<tr><td><code id="gpLassoCpp_+3A_curve">curve</code></td>
<td>
<p>Allows for unequally spaced lambda steps (e.g., .01,.02,.05,1,5,20).
If curve is close to 1 all lambda values will be equally spaced, if curve is large
lambda values will be more concentrated close to 0. See ?lessSEM::curveLambda for more information.</p>
</td></tr>
<tr><td><code id="gpLassoCpp_+3A_additionalarguments">additionalArguments</code></td>
<td>
<p>list with additional arguments passed to fn and gr</p>
</td></tr>
<tr><td><code id="gpLassoCpp_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet.</p>
</td></tr>
<tr><td><code id="gpLassoCpp_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The interface is inspired by optim, but a bit more restrictive. Users have to supply a vector
with starting values (important: This vector <em>must</em> have labels), a fitting
function, and a gradient function. These fitting functions <em>must</em> take an const Rcpp::NumericVector&amp; with parameter
values as first argument and an Rcpp::List&amp; as second argument
</p>
<p>Lasso regularization:
</p>

<ul>
<li><p> Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), 58(1), 267–288.
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Object of class gpRegularized
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# This example shows how to use the optimizers
# for C++ objective functions. We will use
# a linear regression as an example. Note that
# this is not a useful application of the optimizers
# as there are specialized packages for linear regression
# (e.g., glmnet)

library(Rcpp)
library(lessSEM)

linreg &lt;- '
// [[Rcpp::depends(RcppArmadillo)]]
#include &lt;RcppArmadillo.h&gt;

// [[Rcpp::export]]
double fitfunction(const Rcpp::NumericVector&amp; parameters, Rcpp::List&amp; data){
  // extract all required elements:
  arma::colvec b = Rcpp::as&lt;arma::colvec&gt;(parameters);
  arma::colvec y = Rcpp::as&lt;arma::colvec&gt;(data["y"]); // the dependent variable
  arma::mat X = Rcpp::as&lt;arma::mat&gt;(data["X"]); // the design matrix
  
  // compute the sum of squared errors:
    arma::mat sse = arma::trans(y-X*b)*(y-X*b);
    
    // other packages, such as glmnet, scale the sse with 
    // 1/(2*N), where N is the sample size. We will do that here as well
    
    sse *= 1.0/(2.0 * y.n_elem);
    
    // note: We must return a double, but the sse is a matrix
    // To get a double, just return the single value that is in 
    // this matrix:
      return(sse(0,0));
}

// [[Rcpp::export]]
arma::rowvec gradientfunction(const Rcpp::NumericVector&amp; parameters, Rcpp::List&amp; data){
  // extract all required elements:
  arma::colvec b = Rcpp::as&lt;arma::colvec&gt;(parameters);
  arma::colvec y = Rcpp::as&lt;arma::colvec&gt;(data["y"]); // the dependent variable
  arma::mat X = Rcpp::as&lt;arma::mat&gt;(data["X"]); // the design matrix
  
  // note: we want to return our gradients as row-vector; therefore,
  // we have to transpose the resulting column-vector:
    arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);
    
    // other packages, such as glmnet, scale the sse with 
    // 1/(2*N), where N is the sample size. We will do that here as well
    
    gradients *= (.5/y.n_rows);
    
    return(gradients);
}

// Dirk Eddelbuettel at
// https://gallery.rcpp.org/articles/passing-cpp-function-pointers/
typedef double (*fitFunPtr)(const Rcpp::NumericVector&amp;, //parameters
                Rcpp::List&amp; //additional elements
);
typedef Rcpp::XPtr&lt;fitFunPtr&gt; fitFunPtr_t;

typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&amp;, //parameters
                      Rcpp::List&amp; //additional elements
);
typedef Rcpp::XPtr&lt;gradientFunPtr&gt; gradientFunPtr_t;

// [[Rcpp::export]]
fitFunPtr_t fitfunPtr() {
        return(fitFunPtr_t(new fitFunPtr(&amp;fitfunction)));
}

// [[Rcpp::export]]
gradientFunPtr_t gradfunPtr() {
        return(gradientFunPtr_t(new gradientFunPtr(&amp;gradientfunction)));
}
'

Rcpp::sourceCpp(code = linreg)

ffp &lt;- fitfunPtr()
gfp &lt;- gradfunPtr()

N &lt;- 100 # number of persons
p &lt;- 10 # number of predictors
X &lt;- matrix(rnorm(N*p),	nrow = N, ncol = p) # design matrix
b &lt;- c(rep(1,4), 
       rep(0,6)) # true regression weights
y &lt;- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)

data &lt;- list("y" = y,
             "X" = cbind(1,X))
parameters &lt;- rep(0, ncol(data$X))
names(parameters) &lt;- paste0("b", 0:(length(parameters)-1))

l1 &lt;- gpLassoCpp(par = parameters, 
                 regularized = paste0("b", 1:(length(b)-1)),
                 fn = ffp, 
                 gr = gfp, 
                 lambdas = seq(0,1,.1), 
                 additionalArguments = data)

l1@parameters


</code></pre>

<hr>
<h2 id='gpLsp'>gpLsp</h2><span id='topic+gpLsp'></span>

<h3>Description</h3>

<p>Implements lsp regularization for general purpose optimization problems.
The penalty function is given by:
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpLsp(
  par,
  fn,
  gr = NULL,
  ...,
  regularized,
  lambdas,
  thetas,
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpLsp_+3A_par">par</code></td>
<td>
<p>labeled vector with starting values</p>
</td></tr>
<tr><td><code id="gpLsp_+3A_fn">fn</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the fit value (a single value)</p>
</td></tr>
<tr><td><code id="gpLsp_+3A_gr">gr</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the gradients of the objective function.
If set to NULL, numDeriv will be used to approximate the gradients</p>
</td></tr>
<tr><td><code id="gpLsp_+3A_...">...</code></td>
<td>
<p>additional arguments passed to fn and gr</p>
</td></tr>
<tr><td><code id="gpLsp_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.</p>
</td></tr>
<tr><td><code id="gpLsp_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="gpLsp_+3A_thetas">thetas</code></td>
<td>
<p>numeric vector: values for the tuning parameter theta</p>
</td></tr>
<tr><td><code id="gpLsp_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet.</p>
</td></tr>
<tr><td><code id="gpLsp_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The interface is similar to that of optim. Users have to supply a vector
with starting values (important: This vector must have labels) and a fitting
function. This fitting functions must take a labeled vector with parameter
values as first argument. The remaining arguments are passed with the ... argument.
This is similar to optim.
</p>
<p>The gradient function gr is optional. If set to NULL, the <span class="pkg">numDeriv</span> package
will be used to approximate the gradients. Supplying a gradient function
can result in considerable speed improvements.
</p>
<p>lsp regularization:
</p>

<ul>
<li><p> Candès, E. J., Wakin, M. B., &amp; Boyd, S. P. (2008). Enhancing Sparsity by
Reweighted l1 Minimization. Journal of Fourier Analysis and Applications, 14(5–6),
877–905. https://doi.org/10.1007/s00041-008-9045-x
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Object of class gpRegularized
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)
set.seed(123)

# first, we simulate data for our
# linear regression.
N &lt;- 100 # number of persons
p &lt;- 10 # number of predictors
X &lt;- matrix(rnorm(N*p),	nrow = N, ncol = p) # design matrix
b &lt;- c(rep(1,4),
       rep(0,6)) # true regression weights
y &lt;- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)

# First, we must construct a fiting function
# which returns a single value. We will use
# the residual sum squared as fitting function.

# Let's start setting up the fitting function:
fittingFunction &lt;- function(par, y, X, N){
  # par is the parameter vector
  # y is the observed dependent variable
  # X is the design matrix
  # N is the sample size
  pred &lt;- X %*% matrix(par, ncol = 1) #be explicit here:
  # we need par to be a column vector
  sse &lt;- sum((y - pred)^2)
  # we scale with .5/N to get the same results as glmnet
  return((.5/N)*sse)
}

# let's define the starting values:
b &lt;- c(solve(t(X)%*%X)%*%t(X)%*%y) # we will use the lm estimates
names(b) &lt;- paste0("b", 1:length(b))
# names of regularized parameters
regularized &lt;- paste0("b",1:p)

# optimize
lspPen &lt;- gpLsp(
  par = b,
  regularized = regularized,
  fn = fittingFunction,
  lambdas = seq(0,1,.1),
  thetas = c(0.001, .5, 1),
  X = X,
  y = y,
  N = N
)

# optional: plot requires plotly package
# plot(lspPen)

# for comparison

fittingFunction &lt;- function(par, y, X, N, lambda, theta){
  pred &lt;- X %*% matrix(par, ncol = 1)
  sse &lt;- sum((y - pred)^2)
  smoothAbs &lt;- sqrt(par^2 + 1e-8)
  pen &lt;- lambda * log(1.0 + smoothAbs / theta)
  return((.5/N)*sse + sum(pen))
}

round(
  optim(par = b,
      fn = fittingFunction,
      y = y,
      X = X,
      N = N,
      lambda =  lspPen@fits$lambda[15],
      theta =  lspPen@fits$theta[15],
      method = "BFGS")$par,
  4)
lspPen@parameters[15,]
</code></pre>

<hr>
<h2 id='gpLspCpp'>gpLspCpp</h2><span id='topic+gpLspCpp'></span>

<h3>Description</h3>

<p>Implements lsp regularization for general purpose optimization problems with C++ functions.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \lambda \log(1 + |x_j|/\theta)</code>
</p>

<p>where <code class="reqn">\theta &gt; 0</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpLspCpp(
  par,
  fn,
  gr,
  additionalArguments,
  regularized,
  lambdas,
  thetas,
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpLspCpp_+3A_par">par</code></td>
<td>
<p>labeled vector with starting values</p>
</td></tr>
<tr><td><code id="gpLspCpp_+3A_fn">fn</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the fit value (a single value)</p>
</td></tr>
<tr><td><code id="gpLspCpp_+3A_gr">gr</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the gradients of the objective function.
If set to NULL, numDeriv will be used to approximate the gradients</p>
</td></tr>
<tr><td><code id="gpLspCpp_+3A_additionalarguments">additionalArguments</code></td>
<td>
<p>list with additional arguments passed to fn and gr</p>
</td></tr>
<tr><td><code id="gpLspCpp_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="gpLspCpp_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="gpLspCpp_+3A_thetas">thetas</code></td>
<td>
<p>numeric vector: values for the tuning parameter theta</p>
</td></tr>
<tr><td><code id="gpLspCpp_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet.</p>
</td></tr>
<tr><td><code id="gpLspCpp_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The interface is inspired by optim, but a bit more restrictive. Users have to supply a vector
with starting values (important: This vector must have labels), a fitting
function, and a gradient function. These fitting functions must take an const Rcpp::NumericVector&amp; with parameter
values as first argument and an Rcpp::List&amp; as second argument
</p>
<p>lsp regularization:
</p>

<ul>
<li><p> Candès, E. J., Wakin, M. B., &amp; Boyd, S. P. (2008). Enhancing Sparsity by
Reweighted l1 Minimization. Journal of Fourier Analysis and Applications, 14(5–6),
877–905. https://doi.org/10.1007/s00041-008-9045-x
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Object of class gpRegularized
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# This example shows how to use the optimizers
# for C++ objective functions. We will use
# a linear regression as an example. Note that
# this is not a useful application of the optimizers
# as there are specialized packages for linear regression
# (e.g., glmnet)

library(Rcpp)
library(lessSEM)

linreg &lt;- '
// [[Rcpp::depends(RcppArmadillo)]]
#include &lt;RcppArmadillo.h&gt;

// [[Rcpp::export]]
double fitfunction(const Rcpp::NumericVector&amp; parameters, Rcpp::List&amp; data){
  // extract all required elements:
  arma::colvec b = Rcpp::as&lt;arma::colvec&gt;(parameters);
  arma::colvec y = Rcpp::as&lt;arma::colvec&gt;(data["y"]); // the dependent variable
  arma::mat X = Rcpp::as&lt;arma::mat&gt;(data["X"]); // the design matrix
  
  // compute the sum of squared errors:
    arma::mat sse = arma::trans(y-X*b)*(y-X*b);
    
    // other packages, such as glmnet, scale the sse with 
    // 1/(2*N), where N is the sample size. We will do that here as well
    
    sse *= 1.0/(2.0 * y.n_elem);
    
    // note: We must return a double, but the sse is a matrix
    // To get a double, just return the single value that is in 
    // this matrix:
      return(sse(0,0));
}

// [[Rcpp::export]]
arma::rowvec gradientfunction(const Rcpp::NumericVector&amp; parameters, Rcpp::List&amp; data){
  // extract all required elements:
  arma::colvec b = Rcpp::as&lt;arma::colvec&gt;(parameters);
  arma::colvec y = Rcpp::as&lt;arma::colvec&gt;(data["y"]); // the dependent variable
  arma::mat X = Rcpp::as&lt;arma::mat&gt;(data["X"]); // the design matrix
  
  // note: we want to return our gradients as row-vector; therefore,
  // we have to transpose the resulting column-vector:
    arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);
    
    // other packages, such as glmnet, scale the sse with 
    // 1/(2*N), where N is the sample size. We will do that here as well
    
    gradients *= (.5/y.n_rows);
    
    return(gradients);
}

// Dirk Eddelbuettel at
// https://gallery.rcpp.org/articles/passing-cpp-function-pointers/
typedef double (*fitFunPtr)(const Rcpp::NumericVector&amp;, //parameters
                Rcpp::List&amp; //additional elements
);
typedef Rcpp::XPtr&lt;fitFunPtr&gt; fitFunPtr_t;

typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&amp;, //parameters
                      Rcpp::List&amp; //additional elements
);
typedef Rcpp::XPtr&lt;gradientFunPtr&gt; gradientFunPtr_t;

// [[Rcpp::export]]
fitFunPtr_t fitfunPtr() {
        return(fitFunPtr_t(new fitFunPtr(&amp;fitfunction)));
}

// [[Rcpp::export]]
gradientFunPtr_t gradfunPtr() {
        return(gradientFunPtr_t(new gradientFunPtr(&amp;gradientfunction)));
}
'

Rcpp::sourceCpp(code = linreg)

ffp &lt;- fitfunPtr()
gfp &lt;- gradfunPtr()

N &lt;- 100 # number of persons
p &lt;- 10 # number of predictors
X &lt;- matrix(rnorm(N*p),	nrow = N, ncol = p) # design matrix
b &lt;- c(rep(1,4), 
       rep(0,6)) # true regression weights
y &lt;- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)

data &lt;- list("y" = y,
             "X" = cbind(1,X))
parameters &lt;- rep(0, ncol(data$X))
names(parameters) &lt;- paste0("b", 0:(length(parameters)-1))

l &lt;- gpLspCpp(par = parameters, 
                 regularized = paste0("b", 1:(length(b)-1)),
                 fn = ffp, 
                 gr = gfp, 
                 lambdas = seq(0,1,.1), 
                 thetas = seq(0.1,1,.1),
                 additionalArguments = data)

l@parameters

</code></pre>

<hr>
<h2 id='gpMcp'>gpMcp</h2><span id='topic+gpMcp'></span>

<h3>Description</h3>

<p>Implements mcp regularization for general purpose optimization problems.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \begin{cases}
\lambda |x_j| - x_j^2/(2\theta) &amp; \text{if } |x_j| \leq \theta\lambda\\
\theta\lambda^2/2 &amp; \text{if } |x_j| &gt; \lambda\theta
\end{cases}</code>
</p>
<p> where <code class="reqn">\theta &gt; 0</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpMcp(
  par,
  fn,
  gr = NULL,
  ...,
  regularized,
  lambdas,
  thetas,
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpMcp_+3A_par">par</code></td>
<td>
<p>labeled vector with starting values</p>
</td></tr>
<tr><td><code id="gpMcp_+3A_fn">fn</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the fit value (a single value)</p>
</td></tr>
<tr><td><code id="gpMcp_+3A_gr">gr</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the gradients of the objective function.
If set to NULL, numDeriv will be used to approximate the gradients</p>
</td></tr>
<tr><td><code id="gpMcp_+3A_...">...</code></td>
<td>
<p>additional arguments passed to fn and gr</p>
</td></tr>
<tr><td><code id="gpMcp_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.</p>
</td></tr>
<tr><td><code id="gpMcp_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="gpMcp_+3A_thetas">thetas</code></td>
<td>
<p>numeric vector: values for the tuning parameter theta</p>
</td></tr>
<tr><td><code id="gpMcp_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet.</p>
</td></tr>
<tr><td><code id="gpMcp_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The interface is similar to that of optim. Users have to supply a vector
with starting values (important: This vector <em>must</em> have labels) and a fitting
function. This fitting functions <em>must</em> take a labeled vector with parameter
values as first argument. The remaining arguments are passed with the ... argument.
This is similar to optim.
</p>
<p>The gradient function gr is optional. If set to NULL, the <span class="pkg">numDeriv</span> package
will be used to approximate the gradients. Supplying a gradient function
can result in considerable speed improvements.
</p>
<p>mcp regularization:
</p>

<ul>
<li><p> Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty.
The Annals of Statistics, 38(2), 894–942. https://doi.org/10.1214/09-AOS729
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Object of class gpRegularized
</p>


<h3>Examples</h3>

<pre><code class='language-R'># This example shows how to use the optimizers
# for other objective functions. We will use
# a linear regression as an example. Note that
# this is not a useful application of the optimizers
# as there are specialized packages for linear regression
# (e.g., glmnet)

library(lessSEM)
set.seed(123)

# first, we simulate data for our
# linear regression.
N &lt;- 100 # number of persons
p &lt;- 10 # number of predictors
X &lt;- matrix(rnorm(N*p),	nrow = N, ncol = p) # design matrix
b &lt;- c(rep(1,4),
       rep(0,6)) # true regression weights
y &lt;- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)

# First, we must construct a fiting function
# which returns a single value. We will use
# the residual sum squared as fitting function.

# Let's start setting up the fitting function:
fittingFunction &lt;- function(par, y, X, N){
  # par is the parameter vector
  # y is the observed dependent variable
  # X is the design matrix
  # N is the sample size
  pred &lt;- X %*% matrix(par, ncol = 1) #be explicit here:
  # we need par to be a column vector
  sse &lt;- sum((y - pred)^2)
  # we scale with .5/N to get the same results as glmnet
  return((.5/N)*sse)
}

# let's define the starting values:
# first, let's add an intercept
X &lt;- cbind(1, X)

b &lt;- c(solve(t(X)%*%X)%*%t(X)%*%y) # we will use the lm estimates
names(b) &lt;- paste0("b", 0:(length(b)-1))
# names of regularized parameters
regularized &lt;- paste0("b",1:p)

# optimize
mcpPen &lt;- gpMcp(
  par = b,
  regularized = regularized,
  fn = fittingFunction,
  lambdas = seq(0,1,.1),
  thetas = c(1.001, 1.5, 2),
  X = X,
  y = y,
  N = N
)

# optional: plot requires plotly package
# plot(mcpPen)

</code></pre>

<hr>
<h2 id='gpMcpCpp'>gpMcpCpp</h2><span id='topic+gpMcpCpp'></span>

<h3>Description</h3>

<p>Implements mcp regularization for general purpose optimization problems with C++ functions.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \begin{cases}
\lambda |x_j| - x_j^2/(2\theta) &amp; \text{if } |x_j| \leq \theta\lambda\\
\theta\lambda^2/2 &amp; \text{if } |x_j| &gt; \lambda\theta
\end{cases}</code>
</p>
<p> where <code class="reqn">\theta &gt; 0</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpMcpCpp(
  par,
  fn,
  gr,
  additionalArguments,
  regularized,
  lambdas,
  thetas,
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpMcpCpp_+3A_par">par</code></td>
<td>
<p>labeled vector with starting values</p>
</td></tr>
<tr><td><code id="gpMcpCpp_+3A_fn">fn</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the fit value (a single value)</p>
</td></tr>
<tr><td><code id="gpMcpCpp_+3A_gr">gr</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the gradients of the objective function.
If set to NULL, numDeriv will be used to approximate the gradients</p>
</td></tr>
<tr><td><code id="gpMcpCpp_+3A_additionalarguments">additionalArguments</code></td>
<td>
<p>list with additional arguments passed to fn and gr</p>
</td></tr>
<tr><td><code id="gpMcpCpp_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="gpMcpCpp_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="gpMcpCpp_+3A_thetas">thetas</code></td>
<td>
<p>numeric vector: values for the tuning parameter theta</p>
</td></tr>
<tr><td><code id="gpMcpCpp_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet.</p>
</td></tr>
<tr><td><code id="gpMcpCpp_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The interface is inspired by optim, but a bit more restrictive. Users have to supply a vector
with starting values (important: This vector <em>must</em> have labels), a fitting
function, and a gradient function. These fitting functions <em>must</em> take an const Rcpp::NumericVector&amp; with parameter
values as first argument and an Rcpp::List&amp; as second argument
</p>
<p>mcp regularization:
</p>

<ul>
<li><p> Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty.
The Annals of Statistics, 38(2), 894–942. https://doi.org/10.1214/09-AOS729
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Object of class gpRegularized
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# This example shows how to use the optimizers
# for C++ objective functions. We will use
# a linear regression as an example. Note that
# this is not a useful application of the optimizers
# as there are specialized packages for linear regression
# (e.g., glmnet)

library(Rcpp)
library(lessSEM)

linreg &lt;- '
// [[Rcpp::depends(RcppArmadillo)]]
#include &lt;RcppArmadillo.h&gt;

// [[Rcpp::export]]
double fitfunction(const Rcpp::NumericVector&amp; parameters, Rcpp::List&amp; data){
  // extract all required elements:
  arma::colvec b = Rcpp::as&lt;arma::colvec&gt;(parameters);
  arma::colvec y = Rcpp::as&lt;arma::colvec&gt;(data["y"]); // the dependent variable
  arma::mat X = Rcpp::as&lt;arma::mat&gt;(data["X"]); // the design matrix
  
  // compute the sum of squared errors:
    arma::mat sse = arma::trans(y-X*b)*(y-X*b);
    
    // other packages, such as glmnet, scale the sse with 
    // 1/(2*N), where N is the sample size. We will do that here as well
    
    sse *= 1.0/(2.0 * y.n_elem);
    
    // note: We must return a double, but the sse is a matrix
    // To get a double, just return the single value that is in 
    // this matrix:
      return(sse(0,0));
}

// [[Rcpp::export]]
arma::rowvec gradientfunction(const Rcpp::NumericVector&amp; parameters, Rcpp::List&amp; data){
  // extract all required elements:
  arma::colvec b = Rcpp::as&lt;arma::colvec&gt;(parameters);
  arma::colvec y = Rcpp::as&lt;arma::colvec&gt;(data["y"]); // the dependent variable
  arma::mat X = Rcpp::as&lt;arma::mat&gt;(data["X"]); // the design matrix
  
  // note: we want to return our gradients as row-vector; therefore,
  // we have to transpose the resulting column-vector:
    arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);
    
    // other packages, such as glmnet, scale the sse with 
    // 1/(2*N), where N is the sample size. We will do that here as well
    
    gradients *= (.5/y.n_rows);
    
    return(gradients);
}

// Dirk Eddelbuettel at
// https://gallery.rcpp.org/articles/passing-cpp-function-pointers/
typedef double (*fitFunPtr)(const Rcpp::NumericVector&amp;, //parameters
                Rcpp::List&amp; //additional elements
);
typedef Rcpp::XPtr&lt;fitFunPtr&gt; fitFunPtr_t;

typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&amp;, //parameters
                      Rcpp::List&amp; //additional elements
);
typedef Rcpp::XPtr&lt;gradientFunPtr&gt; gradientFunPtr_t;

// [[Rcpp::export]]
fitFunPtr_t fitfunPtr() {
        return(fitFunPtr_t(new fitFunPtr(&amp;fitfunction)));
}

// [[Rcpp::export]]
gradientFunPtr_t gradfunPtr() {
        return(gradientFunPtr_t(new gradientFunPtr(&amp;gradientfunction)));
}
'

Rcpp::sourceCpp(code = linreg)

ffp &lt;- fitfunPtr()
gfp &lt;- gradfunPtr()

N &lt;- 100 # number of persons
p &lt;- 10 # number of predictors
X &lt;- matrix(rnorm(N*p),	nrow = N, ncol = p) # design matrix
b &lt;- c(rep(1,4), 
       rep(0,6)) # true regression weights
y &lt;- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)

data &lt;- list("y" = y,
             "X" = cbind(1,X))
parameters &lt;- rep(0, ncol(data$X))
names(parameters) &lt;- paste0("b", 0:(length(parameters)-1))

m &lt;- gpMcpCpp(par = parameters, 
                 regularized = paste0("b", 1:(length(b)-1)),
                 fn = ffp, 
                 gr = gfp, 
                 lambdas = seq(0,1,.1), 
                 thetas = seq(.1,1,.1),
                 additionalArguments = data)

m@parameters

</code></pre>

<hr>
<h2 id='gpRegularized-class'>Class for regularized model using general purpose optimization interface</h2><span id='topic+gpRegularized-class'></span>

<h3>Description</h3>

<p>Class for regularized model using general purpose optimization interface
</p>


<h3>Slots</h3>


<dl>
<dt><code>penalty</code></dt><dd><p>penalty used (e.g., &quot;lasso&quot;)</p>
</dd>
<dt><code>parameters</code></dt><dd><p>data.frame with all parameter estimates</p>
</dd>
<dt><code>fits</code></dt><dd><p>data.frame with all fit results</p>
</dd>
<dt><code>parameterLabels</code></dt><dd><p>character vector with names of all parameters</p>
</dd>
<dt><code>weights</code></dt><dd><p>vector with weights given to each of the parameters in the penalty</p>
</dd>
<dt><code>regularized</code></dt><dd><p>character vector with names of regularized parameters</p>
</dd>
<dt><code>internalOptimization</code></dt><dd><p>list of elements used internally</p>
</dd>
<dt><code>inputArguments</code></dt><dd><p>list with elements passed by the user to the general
purpose optimizer</p>
</dd>
</dl>

<hr>
<h2 id='gpRidge'>gpRidge</h2><span id='topic+gpRidge'></span>

<h3>Description</h3>

<p>Implements ridge regularization for general purpose optimization problems.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \lambda x_j^2</code>
</p>

<p>Note that ridge regularization will not set any of the parameters to zero
but result in a shrinkage towards zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpRidge(
  par,
  regularized,
  fn,
  gr = NULL,
  lambdas,
  ...,
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpRidge_+3A_par">par</code></td>
<td>
<p>labeled vector with starting values</p>
</td></tr>
<tr><td><code id="gpRidge_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.</p>
</td></tr>
<tr><td><code id="gpRidge_+3A_fn">fn</code></td>
<td>
<p>R function which takes the parameters as input and returns the
fit value (a single value)</p>
</td></tr>
<tr><td><code id="gpRidge_+3A_gr">gr</code></td>
<td>
<p>R function which takes the parameters as input and returns the
gradients of the objective function. If set to NULL, numDeriv will be used
to approximate the gradients</p>
</td></tr>
<tr><td><code id="gpRidge_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="gpRidge_+3A_...">...</code></td>
<td>
<p>additional arguments passed to fn and gr</p>
</td></tr>
<tr><td><code id="gpRidge_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet.</p>
</td></tr>
<tr><td><code id="gpRidge_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The interface is similar to that of optim. Users have to supply a vector
with starting values (important: This vector <em>must</em> have labels) and a fitting
function. This fitting functions <em>must</em> take a labeled vector with parameter
values as first argument. The remaining arguments are passed with the ... argument.
This is similar to optim.
</p>
<p>The gradient function gr is optional. If set to NULL, the <span class="pkg">numDeriv</span> package
will be used to approximate the gradients. Supplying a gradient function
can result in considerable speed improvements.
</p>
<p>Ridge regularization:
</p>

<ul>
<li><p> Hoerl, A. E., &amp; Kennard, R. W. (1970). Ridge Regression: Biased Estimation
for Nonorthogonal Problems. Technometrics, 12(1), 55–67.
https://doi.org/10.1080/00401706.1970.10488634
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Object of class gpRegularized
</p>


<h3>Examples</h3>

<pre><code class='language-R'># This example shows how to use the optimizers
# for other objective functions. We will use
# a linear regression as an example. Note that
# this is not a useful application of the optimizers
# as there are specialized packages for linear regression
# (e.g., glmnet)

library(lessSEM)
set.seed(123)

# first, we simulate data for our
# linear regression.
N &lt;- 100 # number of persons
p &lt;- 10 # number of predictors
X &lt;- matrix(rnorm(N*p),	nrow = N, ncol = p) # design matrix
b &lt;- c(rep(1,4),
       rep(0,6)) # true regression weights
y &lt;- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)

# First, we must construct a fiting function
# which returns a single value. We will use
# the residual sum squared as fitting function.

# Let's start setting up the fitting function:
fittingFunction &lt;- function(par, y, X, N){
  # par is the parameter vector
  # y is the observed dependent variable
  # X is the design matrix
  # N is the sample size
  pred &lt;- X %*% matrix(par, ncol = 1) #be explicit here:
  # we need par to be a column vector
  sse &lt;- sum((y - pred)^2)
  # we scale with .5/N to get the same results as glmnet
  return((.5/N)*sse)
}

# let's define the starting values:
b &lt;- c(solve(t(X)%*%X)%*%t(X)%*%y) # we will use the lm estimates
names(b) &lt;- paste0("b", 1:length(b))
# names of regularized parameters
regularized &lt;- paste0("b",1:p)

# optimize
ridgePen &lt;- gpRidge(
  par = b,
  regularized = regularized,
  fn = fittingFunction,
  lambdas = seq(0,1,.01),
  X = X,
  y = y,
  N = N
)
plot(ridgePen)

# for comparison:
# fittingFunction &lt;- function(par, y, X, N, lambda){
#   pred &lt;- X %*% matrix(par, ncol = 1) 
#   sse &lt;- sum((y - pred)^2)
#   return((.5/N)*sse + lambda * sum(par^2))
# }
# 
# optim(par = b, 
#       fn = fittingFunction, 
#       y = y,
#       X = X,
#       N = N,
#       lambda =  ridgePen@fits$lambda[20], 
#       method = "BFGS")$par
# ridgePen@parameters[20,]
</code></pre>

<hr>
<h2 id='gpRidgeCpp'>gpRidgeCpp</h2><span id='topic+gpRidgeCpp'></span>

<h3>Description</h3>

<p>Implements ridge regularization for general purpose optimization problems with C++ functions.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \lambda x_j^2</code>
</p>

<p>Note that ridge regularization will not set any of the parameters to zero
but result in a shrinkage towards zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpRidgeCpp(
  par,
  regularized,
  fn,
  gr,
  lambdas,
  additionalArguments,
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpRidgeCpp_+3A_par">par</code></td>
<td>
<p>labeled vector with starting values</p>
</td></tr>
<tr><td><code id="gpRidgeCpp_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.</p>
</td></tr>
<tr><td><code id="gpRidgeCpp_+3A_fn">fn</code></td>
<td>
<p>R function which takes the parameters as input and returns the
fit value (a single value)</p>
</td></tr>
<tr><td><code id="gpRidgeCpp_+3A_gr">gr</code></td>
<td>
<p>R function which takes the parameters as input and returns the
gradients of the objective function. If set to NULL, numDeriv will be used
to approximate the gradients</p>
</td></tr>
<tr><td><code id="gpRidgeCpp_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="gpRidgeCpp_+3A_additionalarguments">additionalArguments</code></td>
<td>
<p>list with additional arguments passed to fn and gr</p>
</td></tr>
<tr><td><code id="gpRidgeCpp_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet.</p>
</td></tr>
<tr><td><code id="gpRidgeCpp_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The interface is inspired by optim, but a bit more restrictive. Users have to supply a vector
with starting values (important: This vector <em>must</em> have labels), a fitting
function, and a gradient function. These fitting functions <em>must</em> take an const Rcpp::NumericVector&amp; with parameter
values as first argument and an Rcpp::List&amp; as second argument
</p>
<p>Ridge regularization:
</p>

<ul>
<li><p> Hoerl, A. E., &amp; Kennard, R. W. (1970). Ridge Regression: Biased Estimation
for Nonorthogonal Problems. Technometrics, 12(1), 55–67.
https://doi.org/10.1080/00401706.1970.10488634
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Object of class gpRegularized
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# This example shows how to use the optimizers
# for C++ objective functions. We will use
# a linear regression as an example. Note that
# this is not a useful application of the optimizers
# as there are specialized packages for linear regression
# (e.g., glmnet)

library(Rcpp)
library(lessSEM)

linreg &lt;- '
// [[Rcpp::depends(RcppArmadillo)]]
#include &lt;RcppArmadillo.h&gt;

// [[Rcpp::export]]
double fitfunction(const Rcpp::NumericVector&amp; parameters, Rcpp::List&amp; data){
  // extract all required elements:
  arma::colvec b = Rcpp::as&lt;arma::colvec&gt;(parameters);
  arma::colvec y = Rcpp::as&lt;arma::colvec&gt;(data["y"]); // the dependent variable
  arma::mat X = Rcpp::as&lt;arma::mat&gt;(data["X"]); // the design matrix
  
  // compute the sum of squared errors:
    arma::mat sse = arma::trans(y-X*b)*(y-X*b);
    
    // other packages, such as glmnet, scale the sse with 
    // 1/(2*N), where N is the sample size. We will do that here as well
    
    sse *= 1.0/(2.0 * y.n_elem);
    
    // note: We must return a double, but the sse is a matrix
    // To get a double, just return the single value that is in 
    // this matrix:
      return(sse(0,0));
}

// [[Rcpp::export]]
arma::rowvec gradientfunction(const Rcpp::NumericVector&amp; parameters, Rcpp::List&amp; data){
  // extract all required elements:
  arma::colvec b = Rcpp::as&lt;arma::colvec&gt;(parameters);
  arma::colvec y = Rcpp::as&lt;arma::colvec&gt;(data["y"]); // the dependent variable
  arma::mat X = Rcpp::as&lt;arma::mat&gt;(data["X"]); // the design matrix
  
  // note: we want to return our gradients as row-vector; therefore,
  // we have to transpose the resulting column-vector:
    arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);
    
    // other packages, such as glmnet, scale the sse with 
    // 1/(2*N), where N is the sample size. We will do that here as well
    
    gradients *= (.5/y.n_rows);
    
    return(gradients);
}

// https://gallery.rcpp.org/articles/passing-cpp-function-pointers/
typedef double (*fitFunPtr)(const Rcpp::NumericVector&amp;, //parameters
                Rcpp::List&amp; //additional elements
);
typedef Rcpp::XPtr&lt;fitFunPtr&gt; fitFunPtr_t;

typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&amp;, //parameters
                      Rcpp::List&amp; //additional elements
);
typedef Rcpp::XPtr&lt;gradientFunPtr&gt; gradientFunPtr_t;

// [[Rcpp::export]]
fitFunPtr_t fitfunPtr() {
        return(fitFunPtr_t(new fitFunPtr(&amp;fitfunction)));
}

// [[Rcpp::export]]
gradientFunPtr_t gradfunPtr() {
        return(gradientFunPtr_t(new gradientFunPtr(&amp;gradientfunction)));
}
'

Rcpp::sourceCpp(code = linreg)

ffp &lt;- fitfunPtr()
gfp &lt;- gradfunPtr()

N &lt;- 100 # number of persons
p &lt;- 10 # number of predictors
X &lt;- matrix(rnorm(N*p),	nrow = N, ncol = p) # design matrix
b &lt;- c(rep(1,4), 
       rep(0,6)) # true regression weights
y &lt;- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)

data &lt;- list("y" = y,
             "X" = cbind(1,X))
parameters &lt;- rep(0, ncol(data$X))
names(parameters) &lt;- paste0("b", 0:(length(parameters)-1))

r &lt;- gpRidgeCpp(par = parameters, 
                 regularized = paste0("b", 1:(length(b)-1)),
                 fn = ffp, 
                 gr = gfp, 
                 lambdas = seq(0,1,.1), 
                 additionalArguments = data)

r@parameters

</code></pre>

<hr>
<h2 id='gpScad'>gpScad</h2><span id='topic+gpScad'></span>

<h3>Description</h3>

<p>Implements scad regularization for general purpose optimization problems.
The penalty function is given by:

</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \begin{cases}
\lambda |x_j| &amp; \text{if } |x_j| \leq \theta\\
\frac{-x_j^2 + 2\theta\lambda |x_j| - \lambda^2}{2(\theta -1)} &amp; 
\text{if } \lambda &lt; |x_j| \leq \lambda\theta \\
(\theta + 1) \lambda^2/2 &amp; \text{if } |x_j| \geq \theta\lambda\\
\end{cases}</code>
</p>

<p>where <code class="reqn">\theta &gt; 2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpScad(
  par,
  fn,
  gr = NULL,
  ...,
  regularized,
  lambdas,
  thetas,
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpScad_+3A_par">par</code></td>
<td>
<p>labeled vector with starting values</p>
</td></tr>
<tr><td><code id="gpScad_+3A_fn">fn</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the fit value (a single value)</p>
</td></tr>
<tr><td><code id="gpScad_+3A_gr">gr</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the gradients of the objective function.
If set to NULL, numDeriv will be used to approximate the gradients</p>
</td></tr>
<tr><td><code id="gpScad_+3A_...">...</code></td>
<td>
<p>additional arguments passed to fn and gr</p>
</td></tr>
<tr><td><code id="gpScad_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.</p>
</td></tr>
<tr><td><code id="gpScad_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="gpScad_+3A_thetas">thetas</code></td>
<td>
<p>numeric vector: values for the tuning parameter theta</p>
</td></tr>
<tr><td><code id="gpScad_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet.</p>
</td></tr>
<tr><td><code id="gpScad_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The interface is similar to that of optim. Users have to supply a vector
with starting values (important: This vector <em>must</em> have labels) and a fitting
function. This fitting functions <em>must</em> take a labeled vector with parameter
values as first argument. The remaining arguments are passed with the ... argument.
This is similar to optim.
</p>
<p>The gradient function gr is optional. If set to NULL, the <span class="pkg">numDeriv</span> package
will be used to approximate the gradients. Supplying a gradient function
can result in considerable speed improvements.
</p>
<p>scad regularization:
</p>

<ul>
<li><p> Fan, J., &amp; Li, R. (2001). Variable selection via nonconcave penalized
likelihood and its oracle properties. Journal of the American Statistical Association,
96(456), 1348–1360. https://doi.org/10.1198/016214501753382273
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Object of class gpRegularized
</p>


<h3>Examples</h3>

<pre><code class='language-R'># This example shows how to use the optimizers
# for other objective functions. We will use
# a linear regression as an example. Note that
# this is not a useful application of the optimizers
# as there are specialized packages for linear regression
# (e.g., glmnet)

library(lessSEM)
set.seed(123)

# first, we simulate data for our
# linear regression.
N &lt;- 100 # number of persons
p &lt;- 10 # number of predictors
X &lt;- matrix(rnorm(N*p),	nrow = N, ncol = p) # design matrix
b &lt;- c(rep(1,4),
       rep(0,6)) # true regression weights
y &lt;- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)

# First, we must construct a fiting function
# which returns a single value. We will use
# the residual sum squared as fitting function.

# Let's start setting up the fitting function:
fittingFunction &lt;- function(par, y, X, N){
  # par is the parameter vector
  # y is the observed dependent variable
  # X is the design matrix
  # N is the sample size
  pred &lt;- X %*% matrix(par, ncol = 1) #be explicit here:
  # we need par to be a column vector
  sse &lt;- sum((y - pred)^2)
  # we scale with .5/N to get the same results as glmnet
  return((.5/N)*sse)
}

# let's define the starting values:
# first, let's add an intercept
X &lt;- cbind(1, X)

b &lt;- c(solve(t(X)%*%X)%*%t(X)%*%y) # we will use the lm estimates
names(b) &lt;- paste0("b", 0:(length(b)-1))
# names of regularized parameters
regularized &lt;- paste0("b",1:p)

# optimize
scadPen &lt;- gpScad(
  par = b,
  regularized = regularized,
  fn = fittingFunction,
  lambdas = seq(0,1,.1),
  thetas = c(2.001, 2.5, 5),
  X = X,
  y = y,
  N = N
)

# optional: plot requires plotly package
# plot(scadPen)

# for comparison
#library(ncvreg)
#scadFit &lt;- ncvreg(X = X[,-1], 
#                  y = y, 
#                  penalty = "SCAD",
#                  lambda =  scadPen@fits$lambda[15],
#                  gamma =  scadPen@fits$theta[15])
#coef(scadFit)
#scadPen@parameters[15,]
</code></pre>

<hr>
<h2 id='gpScadCpp'>gpScadCpp</h2><span id='topic+gpScadCpp'></span>

<h3>Description</h3>

<p>Implements scad regularization for general purpose optimization problems with C++ functions.
The penalty function is given by:

</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \begin{cases}
\lambda |x_j| &amp; \text{if } |x_j| \leq \theta\\
\frac{-x_j^2 + 2\theta\lambda |x_j| - \lambda^2}{2(\theta -1)} &amp; 
\text{if } \lambda &lt; |x_j| \leq \lambda\theta \\
(\theta + 1) \lambda^2/2 &amp; \text{if } |x_j| \geq \theta\lambda\\
\end{cases}</code>
</p>

<p>where <code class="reqn">\theta &gt; 2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpScadCpp(
  par,
  fn,
  gr,
  additionalArguments,
  regularized,
  lambdas,
  thetas,
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpScadCpp_+3A_par">par</code></td>
<td>
<p>labeled vector with starting values</p>
</td></tr>
<tr><td><code id="gpScadCpp_+3A_fn">fn</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the fit value (a single value)</p>
</td></tr>
<tr><td><code id="gpScadCpp_+3A_gr">gr</code></td>
<td>
<p>R function which takes the parameters AND their labels
as input and returns the gradients of the objective function.
If set to NULL, numDeriv will be used to approximate the gradients</p>
</td></tr>
<tr><td><code id="gpScadCpp_+3A_additionalarguments">additionalArguments</code></td>
<td>
<p>list with additional arguments passed to fn and gr</p>
</td></tr>
<tr><td><code id="gpScadCpp_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="gpScadCpp_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="gpScadCpp_+3A_thetas">thetas</code></td>
<td>
<p>numeric vector: values for the tuning parameter theta</p>
</td></tr>
<tr><td><code id="gpScadCpp_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet.</p>
</td></tr>
<tr><td><code id="gpScadCpp_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The interface is inspired by optim, but a bit more restrictive. Users have to supply a vector
with starting values (important: This vector <em>must</em> have labels), a fitting
function, and a gradient function. These fitting functions <em>must</em> take an const Rcpp::NumericVector&amp; with parameter
values as first argument and an Rcpp::List&amp; as second argument
</p>
<p>scad regularization:
</p>

<ul>
<li><p> Fan, J., &amp; Li, R. (2001). Variable selection via nonconcave penalized
likelihood and its oracle properties. Journal of the American Statistical Association,
96(456), 1348–1360. https://doi.org/10.1198/016214501753382273
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Object of class gpRegularized
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# This example shows how to use the optimizers
# for C++ objective functions. We will use
# a linear regression as an example. Note that
# this is not a useful application of the optimizers
# as there are specialized packages for linear regression
# (e.g., glmnet)

library(Rcpp)
library(lessSEM)

linreg &lt;- '
// [[Rcpp::depends(RcppArmadillo)]]
#include &lt;RcppArmadillo.h&gt;

// [[Rcpp::export]]
double fitfunction(const Rcpp::NumericVector&amp; parameters, Rcpp::List&amp; data){
  // extract all required elements:
  arma::colvec b = Rcpp::as&lt;arma::colvec&gt;(parameters);
  arma::colvec y = Rcpp::as&lt;arma::colvec&gt;(data["y"]); // the dependent variable
  arma::mat X = Rcpp::as&lt;arma::mat&gt;(data["X"]); // the design matrix
  
  // compute the sum of squared errors:
    arma::mat sse = arma::trans(y-X*b)*(y-X*b);
    
    // other packages, such as glmnet, scale the sse with 
    // 1/(2*N), where N is the sample size. We will do that here as well
    
    sse *= 1.0/(2.0 * y.n_elem);
    
    // note: We must return a double, but the sse is a matrix
    // To get a double, just return the single value that is in 
    // this matrix:
      return(sse(0,0));
}

// [[Rcpp::export]]
arma::rowvec gradientfunction(const Rcpp::NumericVector&amp; parameters, Rcpp::List&amp; data){
  // extract all required elements:
  arma::colvec b = Rcpp::as&lt;arma::colvec&gt;(parameters);
  arma::colvec y = Rcpp::as&lt;arma::colvec&gt;(data["y"]); // the dependent variable
  arma::mat X = Rcpp::as&lt;arma::mat&gt;(data["X"]); // the design matrix
  
  // note: we want to return our gradients as row-vector; therefore,
  // we have to transpose the resulting column-vector:
    arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);
    
    // other packages, such as glmnet, scale the sse with 
    // 1/(2*N), where N is the sample size. We will do that here as well
    
    gradients *= (.5/y.n_rows);
    
    return(gradients);
}

// Dirk Eddelbuettel at
// https://gallery.rcpp.org/articles/passing-cpp-function-pointers/
typedef double (*fitFunPtr)(const Rcpp::NumericVector&amp;, //parameters
                Rcpp::List&amp; //additional elements
);
typedef Rcpp::XPtr&lt;fitFunPtr&gt; fitFunPtr_t;

typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&amp;, //parameters
                      Rcpp::List&amp; //additional elements
);
typedef Rcpp::XPtr&lt;gradientFunPtr&gt; gradientFunPtr_t;

// [[Rcpp::export]]
fitFunPtr_t fitfunPtr() {
        return(fitFunPtr_t(new fitFunPtr(&amp;fitfunction)));
}

// [[Rcpp::export]]
gradientFunPtr_t gradfunPtr() {
        return(gradientFunPtr_t(new gradientFunPtr(&amp;gradientfunction)));
}
'

Rcpp::sourceCpp(code = linreg)

ffp &lt;- fitfunPtr()
gfp &lt;- gradfunPtr()

N &lt;- 100 # number of persons
p &lt;- 10 # number of predictors
X &lt;- matrix(rnorm(N*p),	nrow = N, ncol = p) # design matrix
b &lt;- c(rep(1,4), 
       rep(0,6)) # true regression weights
y &lt;- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)

data &lt;- list("y" = y,
             "X" = cbind(1,X))
parameters &lt;- rep(0, ncol(data$X))
names(parameters) &lt;- paste0("b", 0:(length(parameters)-1))

s &lt;- gpScadCpp(par = parameters, 
                 regularized = paste0("b", 1:(length(b)-1)),
                 fn = ffp, 
                 gr = gfp, 
                 lambdas = seq(0,1,.1), 
                 thetas = seq(2.1,3,.1),
                 additionalArguments = data)

s@parameters

</code></pre>

<hr>
<h2 id='istaCappedL1mgSEM'>cappedL1 optimization with ista</h2><span id='topic+istaCappedL1mgSEM'></span>

<h3>Description</h3>

<p>Object for elastic net optimization with
ista optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (1) a vector with weights for each
parameter and (2) a list with control elements</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a theta value, a lambda and an alpha value (alpha must be 1).</p>
</dd>
</dl>

<hr>
<h2 id='istaCappedL1SEM'>cappedL1 optimization with ista</h2><span id='topic+istaCappedL1SEM'></span>

<h3>Description</h3>

<p>Object for elastic net optimization with
ista optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (1) a vector with weights for each
parameter and (2) a list with control elements</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a theta value, a lambda and an alpha value (alpha must be 1).</p>
</dd>
</dl>

<hr>
<h2 id='istaEnetGeneralPurpose'>elastic net optimization with ista</h2><span id='topic+istaEnetGeneralPurpose'></span>

<h3>Description</h3>

<p>Object for elastic net optimization with
ista optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (1) a vector with weights for each
parameter and (2) a list with control elements</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
an R function to compute the fit, an R function to compute the gradients, a
list with elements the fit and gradient function require, a lambda and an alpha value.</p>
</dd>
</dl>

<hr>
<h2 id='istaEnetGeneralPurposeCpp'>elastic net optimization with ista</h2><span id='topic+istaEnetGeneralPurposeCpp'></span>

<h3>Description</h3>

<p>Object for elastic net optimization with
ista optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (1) a vector with weights for each
parameter and (2) a list with control elements</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEXP function pointer to compute the fit, a SEXP function pointer to compute the gradients, a
list with elements the fit and gradient function require, a lambda and an alpha value.</p>
</dd>
</dl>

<hr>
<h2 id='istaEnetMgSEM'>elastic net optimization with ista optimizer</h2><span id='topic+istaEnetMgSEM'></span>

<h3>Description</h3>

<p>Object for elastic net optimization with
glmnet optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (1) a vector with weights for each
parameter and (2) a list with control elements</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a lambda and an alpha value.</p>
</dd>
</dl>

<hr>
<h2 id='istaEnetSEM'>elastic net optimization with ista optimizer</h2><span id='topic+istaEnetSEM'></span>

<h3>Description</h3>

<p>Object for elastic net optimization with
glmnet optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (1) a vector with weights for each
parameter and (2) a list with control elements</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a lambda and an alpha value.</p>
</dd>
</dl>

<hr>
<h2 id='istaLSPMgSEM'>lsp optimization with ista</h2><span id='topic+istaLSPMgSEM'></span>

<h3>Description</h3>

<p>Object for lsp optimization with
ista optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (1) a vector with weights for each
parameter and (2) a list with control elements</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a theta and a lambda value.</p>
</dd>
</dl>

<hr>
<h2 id='istaLSPSEM'>lsp optimization with ista</h2><span id='topic+istaLSPSEM'></span>

<h3>Description</h3>

<p>Object for lsp optimization with
ista optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (1) a vector with weights for each
parameter and (2) a list with control elements</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a theta and a lambda value.</p>
</dd>
</dl>

<hr>
<h2 id='istaMcpMgSEM'>mcp optimization with ista</h2><span id='topic+istaMcpMgSEM'></span>

<h3>Description</h3>

<p>Object for mcp optimization with
ista optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (1) a vector with weights for each
parameter and (2) a list with control elements</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a theta and a lambda value.</p>
</dd>
</dl>

<hr>
<h2 id='istaMcpSEM'>mcp optimization with ista</h2><span id='topic+istaMcpSEM'></span>

<h3>Description</h3>

<p>Object for mcp optimization with
ista optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (1) a vector with weights for each
parameter and (2) a list with control elements</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a theta and a lambda value.</p>
</dd>
</dl>

<hr>
<h2 id='istaMixedPenaltyGeneralPurpose'>mixed penalty optimization with ista</h2><span id='topic+istaMixedPenaltyGeneralPurpose'></span>

<h3>Description</h3>

<p>Object for elastic net optimization with
ista optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object.</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model.</p>
</dd>
</dl>

<hr>
<h2 id='istaMixedPenaltyGeneralPurposeCpp'>mixed penalty optimization with ista</h2><span id='topic+istaMixedPenaltyGeneralPurposeCpp'></span>

<h3>Description</h3>

<p>Object for elastic net optimization with
ista optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (1) a vector with weights for each
parameter, (2) a vector indicating which penalty is used, and (3) a list with control elements</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model.</p>
</dd>
</dl>

<hr>
<h2 id='istaMixedPenaltymgSEM'>mixed penalty optimization with ista</h2><span id='topic+istaMixedPenaltymgSEM'></span>

<h3>Description</h3>

<p>Object for elastic net optimization with
ista optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (1) a vector with weights for each
parameter, (2) a vector indicating which penalty is used, and (3) a list with control elements</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a theta value, a lambda and an alpha value (alpha must be 1).</p>
</dd>
</dl>

<hr>
<h2 id='istaMixedPenaltySEM'>mixed penalty optimization with ista</h2><span id='topic+istaMixedPenaltySEM'></span>

<h3>Description</h3>

<p>Object for elastic net optimization with
ista optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (1) a vector with weights for each
parameter, (2) a vector indicating which penalty is used, and (3) a list with control elements</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a theta value, a lambda and an alpha value (alpha must be 1).</p>
</dd>
</dl>

<hr>
<h2 id='istaScadMgSEM'>scad optimization with ista</h2><span id='topic+istaScadMgSEM'></span>

<h3>Description</h3>

<p>Object for scad optimization with
ista optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (1) a vector with weights for each
parameter and (2) a list with control elements</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a theta and a lambda value.</p>
</dd>
</dl>

<hr>
<h2 id='istaScadSEM'>scad optimization with ista</h2><span id='topic+istaScadSEM'></span>

<h3>Description</h3>

<p>Object for scad optimization with
ista optimizer
</p>


<h3>Value</h3>

<p>a list with fit results
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>creates a new object. Requires (1) a vector with weights for each
parameter and (2) a list with control elements</p>
</dd>
<dt><code>optimize</code></dt><dd><p>optimize the model. Expects a vector with starting values,
a SEM of type SEM_Cpp, a theta and a lambda value.</p>
</dd>
</dl>

<hr>
<h2 id='lasso'>lasso</h2><span id='topic+lasso'></span>

<h3>Description</h3>

<p>Implements lasso regularization for structural equation models.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \lambda |x_j|</code>
</p>

<p>Lasso regularization will set parameters to zero if <code class="reqn">\lambda</code> is large enough
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lasso(
  lavaanModel,
  regularized,
  lambdas = NULL,
  nLambdas = NULL,
  reverse = TRUE,
  curve = 1,
  method = "glmnet",
  modifyModel = lessSEM::modifyModel(),
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lasso_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="lasso_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="lasso_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="lasso_+3A_nlambdas">nLambdas</code></td>
<td>
<p>alternative to lambda: If alpha = 1, lessSEM can automatically
compute the first lambda value which sets all regularized parameters to zero.
It will then generate nLambda values between 0 and the computed lambda.</p>
</td></tr>
<tr><td><code id="lasso_+3A_reverse">reverse</code></td>
<td>
<p>if set to TRUE and nLambdas is used, lessSEM will start with the
largest lambda and gradually decrease lambda. Otherwise, lessSEM will start with
the smallest lambda and gradually increase it.</p>
</td></tr>
<tr><td><code id="lasso_+3A_curve">curve</code></td>
<td>
<p>Allows for unequally spaced lambda steps (e.g., .01,.02,.05,1,5,20).
If curve is close to 1 all lambda values will be equally spaced, if curve is large
lambda values will be more concentrated close to 0. See ?lessSEM::curveLambda for more information.</p>
</td></tr>
<tr><td><code id="lasso_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet. With ista, the control argument can be used to switch to related procedures
(currently gist).</p>
</td></tr>
<tr><td><code id="lasso_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="lasso_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currently,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>Lasso regularization:
</p>

<ul>
<li><p> Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), 58(1), 267–288.
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Model of class regularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 + 
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 + 
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

lsem &lt;- lasso(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = paste0("l", 6:15),
  # in case of lasso and adaptive lasso, we can specify the number of lambda
  # values to use. lessSEM will automatically find lambda_max and fit
  # models for nLambda values between 0 and lambda_max. For the other
  # penalty functions, lambdas must be specified explicitly
  nLambdas = 50)

# use the plot-function to plot the regularized parameters:
plot(lsem)

# the coefficients can be accessed with:
coef(lsem)
# if you are only interested in the estimates and not the tuning parameters, use
coef(lsem)@estimates
# or
estimates(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters[1,]

# fit Measures:
fitIndices(lsem)

# The best parameters can also be extracted with:
coef(lsem, criterion = "AIC")
# or
estimates(lsem, criterion = "AIC") 

#### Advanced ###
# Switching the optimizer # 
# Use the "method" argument to switch the optimizer. The control argument
# must also be changed to the corresponding function:
lsemIsta &lt;- lasso(
  lavaanModel = lavaanModel,
  regularized = paste0("l", 6:15),
  nLambdas = 50,
  method = "ista",
  control = controlIsta())

# Note: The results are basically identical:
lsemIsta@parameters - lsem@parameters
</code></pre>

<hr>
<h2 id='lavaan2lslxLabels'>lavaan2lslxLabels</h2><span id='topic+lavaan2lslxLabels'></span>

<h3>Description</h3>

<p>helper function: lslx and lavaan use slightly different parameter labels. This function
can be used to get both sets of labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lavaan2lslxLabels(lavaanModel)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lavaan2lslxLabels_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with lavaan labels and lslx labels
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 + 
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 + 
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

lavaan2lslxLabels(lavaanModel)
</code></pre>

<hr>
<h2 id='lessSEM2Lavaan'>lessSEM2Lavaan</h2><span id='topic+lessSEM2Lavaan'></span>

<h3>Description</h3>

<p>Creates a lavaan model object from lessSEM (only if possible). Pass either
a criterion or a combination of lambda, alpha, and theta.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lessSEM2Lavaan(
  regularizedSEM,
  criterion = NULL,
  lambda = NULL,
  alpha = NULL,
  theta = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lessSEM2Lavaan_+3A_regularizedsem">regularizedSEM</code></td>
<td>
<p>object created with lessSEM</p>
</td></tr>
<tr><td><code id="lessSEM2Lavaan_+3A_criterion">criterion</code></td>
<td>
<p>criterion used for model selection. Currently supported are
&quot;AIC&quot; or &quot;BIC&quot;</p>
</td></tr>
<tr><td><code id="lessSEM2Lavaan_+3A_lambda">lambda</code></td>
<td>
<p>value for tuning parameter lambda</p>
</td></tr>
<tr><td><code id="lessSEM2Lavaan_+3A_alpha">alpha</code></td>
<td>
<p>value for tuning parameter alpha</p>
</td></tr>
<tr><td><code id="lessSEM2Lavaan_+3A_theta">theta</code></td>
<td>
<p>value for tuning parameter theta</p>
</td></tr>
</table>


<h3>Value</h3>

<p>lavaan model
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 + 
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 + 
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:
regularized &lt;- lasso(lavaanModel,
                     regularized = paste0("l", 11:15), 
                     lambdas = seq(0,1,.1))

# using criterion
lessSEM2Lavaan(regularizedSEM = regularized, 
               criterion = "AIC")
               
# using tuning parameters (note: we only have to specify the tuning
# parameters that are actually used by the penalty function. In case
# of lasso, this is lambda):
lessSEM2Lavaan(regularizedSEM = regularized, 
               lambda = 1)
</code></pre>

<hr>
<h2 id='lessSEMCoef-class'>Class for the coefficients estimated by lessSEM.</h2><span id='topic+lessSEMCoef-class'></span>

<h3>Description</h3>

<p>Class for the coefficients estimated by lessSEM.
</p>


<h3>Slots</h3>


<dl>
<dt><code>tuningParameters</code></dt><dd><p>tuning parameters</p>
</dd>
<dt><code>estimates</code></dt><dd><p>parameter estimates</p>
</dd>
<dt><code>transformations</code></dt><dd><p>transformations of parameters</p>
</dd>
</dl>

<hr>
<h2 id='loadings'>loadings</h2><span id='topic+loadings'></span>

<h3>Description</h3>

<p>Extract the labels of all loadings found in a lavaan model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loadings(lavaanModel)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loadings_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>fitted lavaan model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector with parameter labels
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The following is adapted from ?lavaan::sem
library(lessSEM)
model &lt;- ' 
  # latent variable definitions
  ind60 =~ x1 + x2 + x3
  dem60 =~ y1 + a*y2 + b*y3 + c*y4
  dem65 =~ y5 + a*y6 + b*y7 + c*y8

  # regressions
  dem60 ~ ind60
  dem65 ~ ind60 + dem60

  # residual correlations
  y1 ~~ y5
  y2 ~~ y4 + y6
  y3 ~~ y7
  y4 ~~ y8
  y6 ~~ y8
'

fit &lt;- sem(model, data = PoliticalDemocracy)

loadings(fit)
</code></pre>

<hr>
<h2 id='logicalMatch'>logicalMatch</h2><span id='topic+logicalMatch'></span>

<h3>Description</h3>

<p>Returns the rows for which all elements of a boolean matrix X are equal
to the elements in boolean vector x
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logicalMatch(X, x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logicalMatch_+3A_x">X</code></td>
<td>
<p>matrix with booleans</p>
</td></tr>
<tr><td><code id="logicalMatch_+3A_x">x</code></td>
<td>
<p>vector of booleans</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numerical vector with indices of matching rows
</p>

<hr>
<h2 id='logLik+2CRcpp_mgSEM-method'>logLik</h2><span id='topic+logLik+2CRcpp_mgSEM-method'></span>

<h3>Description</h3>

<p>logLik
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'Rcpp_mgSEM'
logLik(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logLik+2B2CRcpp_mgSEM-method_+3A_object">object</code></td>
<td>
<p>object of class Rcpp_mgSEM</p>
</td></tr>
<tr><td><code id="logLik+2B2CRcpp_mgSEM-method_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>log-likelihood of the model
</p>

<hr>
<h2 id='logLik+2CRcpp_SEMCpp-method'>logLik</h2><span id='topic+logLik+2CRcpp_SEMCpp-method'></span>

<h3>Description</h3>

<p>logLik
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'Rcpp_SEMCpp'
logLik(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logLik+2B2CRcpp_SEMCpp-method_+3A_object">object</code></td>
<td>
<p>object of class Rcpp_SEMCpp</p>
</td></tr>
<tr><td><code id="logLik+2B2CRcpp_SEMCpp-method_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>log-likelihood of the model
</p>

<hr>
<h2 id='logLikelihood-class'>Class for log-likelihood of regularized SEM. Note: we define a custom logLik -
Function because the generic one is using df = number of parameters which might be confusing.</h2><span id='topic+logLikelihood-class'></span>

<h3>Description</h3>

<p>Class for log-likelihood of regularized SEM. Note: we define a custom logLik -
Function because the generic one is using df = number of parameters which might be confusing.
</p>


<h3>Slots</h3>


<dl>
<dt><code>logLik</code></dt><dd><p>log-Likelihood</p>
</dd>
<dt><code>nParameters</code></dt><dd><p>number of parameters in the model</p>
</dd>
<dt><code>N</code></dt><dd><p>number of persons in the data set</p>
</dd>
</dl>

<hr>
<h2 id='lsp'>lsp</h2><span id='topic+lsp'></span>

<h3>Description</h3>

<p>Implements lsp regularization for structural equation models.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \lambda \log(1 + |x_j|/\theta)</code>
</p>

<p>where <code class="reqn">\theta &gt; 0</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lsp(
  lavaanModel,
  regularized,
  lambdas,
  thetas,
  modifyModel = lessSEM::modifyModel(),
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lsp_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="lsp_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="lsp_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="lsp_+3A_thetas">thetas</code></td>
<td>
<p>parameters whose absolute value is above this threshold will be penalized with
a constant (theta)</p>
</td></tr>
<tr><td><code id="lsp_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="lsp_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet. With ista, the control argument can be used to switch to related procedures</p>
</td></tr>
<tr><td><code id="lsp_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta (see ?controlIsta)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currently,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>lsp regularization:
</p>

<ul>
<li><p> Candès, E. J., Wakin, M. B., &amp; Boyd, S. P. (2008). Enhancing Sparsity by
Reweighted l1 Minimization. Journal of Fourier Analysis and Applications, 14(5–6),
877–905. https://doi.org/10.1007/s00041-008-9045-x
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Model of class regularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

lsem &lt;- lsp(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = paste0("l", 6:15),
  lambdas = seq(0,1,length.out = 20),
  thetas = seq(0.01,2,length.out = 5))

# the coefficients can be accessed with:
coef(lsem)
# if you are only interested in the estimates and not the tuning parameters, use
coef(lsem)@estimates
# or
estimates(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters[1,]

# fit Measures:
fitIndices(lsem)

# The best parameters can also be extracted with:
coef(lsem, criterion = "AIC")
# or
estimates(lsem, criterion = "AIC")

# optional: plotting the paths requires installation of plotly
# plot(lsem)
</code></pre>

<hr>
<h2 id='makePtrs'>makePtrs</h2><span id='topic+makePtrs'></span>

<h3>Description</h3>

<p>This function helps you create the pointers necessary to use the Cpp interface
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makePtrs(fitFunName, gradFunName)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="makePtrs_+3A_fitfunname">fitFunName</code></td>
<td>
<p>name of your C++ fit function (IMPORTANT: This must be the name
used in C++)</p>
</td></tr>
<tr><td><code id="makePtrs_+3A_gradfunname">gradFunName</code></td>
<td>
<p>name of your C++ gradient function (IMPORTANT: This must be the name
used in C++)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a string which can be copied in the C++ function to create the pointers.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># see vignette("General-Purpose-Optimization", package = "lessSEM") for an example
</code></pre>

<hr>
<h2 id='mcp'>mcp</h2><span id='topic+mcp'></span>

<h3>Description</h3>

<p>Implements mcp regularization for structural equation models.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \begin{cases}
\lambda |x_j| - x_j^2/(2\theta) &amp; \text{if } |x_j| \leq \theta\lambda\\
\theta\lambda^2/2 &amp; \text{if } |x_j| &gt; \lambda\theta
\end{cases}</code>
</p>
<p> where <code class="reqn">\theta &gt; 1</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcp(
  lavaanModel,
  regularized,
  lambdas,
  thetas,
  modifyModel = lessSEM::modifyModel(),
  method = "ista",
  control = lessSEM::controlIsta()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcp_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="mcp_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="mcp_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="mcp_+3A_thetas">thetas</code></td>
<td>
<p>parameters whose absolute value is above this threshold will be penalized with
a constant (theta)</p>
</td></tr>
<tr><td><code id="mcp_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="mcp_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet. With ista, the control argument can be used to switch to related procedures
(currently gist).</p>
</td></tr>
<tr><td><code id="mcp_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta (see ?controlIsta)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currently,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>In our experience, the glmnet optimizer can run in issues with the mcp penalty.
Therefor, we default to using ista.
</p>
<p>mcp regularization:
</p>

<ul>
<li><p> Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty.
The Annals of Statistics, 38(2), 894–942. https://doi.org/10.1214/09-AOS729
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Model of class regularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

lsem &lt;- mcp(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = paste0("l", 6:15),
  lambdas = seq(0,1,length.out = 20),
  thetas = seq(0.01,2,length.out = 5))

# the coefficients can be accessed with:
coef(lsem)

# if you are only interested in the estimates and not the tuning parameters, use
coef(lsem)@estimates
# or
estimates(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters[1,]

# fit Measures:
fitIndices(lsem)

# The best parameters can also be extracted with:
coef(lsem, criterion = "AIC")
# or
estimates(lsem, criterion = "AIC")

# optional: plotting the paths requires installation of plotly
# plot(lsem)
</code></pre>

<hr>
<h2 id='mcpPenalty_C'>mcpPenalty_C</h2><span id='topic+mcpPenalty_C'></span>

<h3>Description</h3>

<p>mcpPenalty_C
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mcpPenalty_C(par, lambda_p, theta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mcpPenalty_C_+3A_par">par</code></td>
<td>
<p>single parameter value</p>
</td></tr>
<tr><td><code id="mcpPenalty_C_+3A_lambda_p">lambda_p</code></td>
<td>
<p>lambda value for this parameter</p>
</td></tr>
<tr><td><code id="mcpPenalty_C_+3A_theta">theta</code></td>
<td>
<p>theta value for this parameter</p>
</td></tr>
</table>


<h3>Value</h3>

<p>penalty value
</p>

<hr>
<h2 id='mgSEM'>mgSEM class</h2><span id='topic+mgSEM'></span>

<h3>Description</h3>

<p>internal mgSEM representation
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>Creates a new mgSEM.</p>
</dd>
<dt><code>addModel</code></dt><dd><p>add a model. Expects Rcpp::List</p>
</dd>
<dt><code>addTransformation</code></dt><dd><p>adds transforamtions to a model</p>
</dd>
<dt><code>implied</code></dt><dd><p>Computes implied means and covariance matrix</p>
</dd>
<dt><code>fit</code></dt><dd><p>Fits the model. Returns objective value of the fitting function</p>
</dd>
<dt><code>getParameters</code></dt><dd><p>Returns a data frame with model parameters.</p>
</dd>
<dt><code>getParameterLabels</code></dt><dd><p>Returns a vector with unique parameter labels as used internally.</p>
</dd>
<dt><code>getEstimator</code></dt><dd><p>Returns a vector with names of the estimators used in the submodels.</p>
</dd>
<dt><code>getGradients</code></dt><dd><p>Returns a matrix with scores.</p>
</dd>
<dt><code>getScores</code></dt><dd><p>Returns a matrix with scores. Not yet implemented</p>
</dd>
<dt><code>getHessian</code></dt><dd><p>Returns the hessian of the model. Expects the labels of the
parameters and the values of the parameters as well as a boolean indicating if
these are raw. Finally, a double (eps) controls the precision of the approximation.</p>
</dd>
<dt><code>computeTransformations</code></dt><dd><p>compute the transformations.</p>
</dd>
<dt><code>setTransformationGradientStepSize</code></dt><dd><p>change the step size of the gradient computation for the transformations</p>
</dd>
</dl>

<hr>
<h2 id='mixedPenalty'>mixedPenalty</h2><span id='topic+mixedPenalty'></span>

<h3>Description</h3>

<p>Provides possibility to impose different penalties on different parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mixedPenalty(
  lavaanModel,
  modifyModel = lessSEM::modifyModel(),
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mixedPenalty_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="mixedPenalty_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="mixedPenalty_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently supported are &quot;glmnet&quot; and &quot;ista&quot;.</p>
</td></tr>
<tr><td><code id="mixedPenalty_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>mixedPenalty</code> function allows you to add multiple penalties to a single model.
For instance, you may want to regularize both loadings and regressions in a SEM.
In this case, using the same penalty (e.g., lasso) for both types of penalties may
actually not be what you want to use because the penalty function is sensitive to
the scales of the parameters. Instead, you may want to use two separate lasso
penalties for loadings and regressions. Similarly, separate penalties for
different parameters have, for instance, been proposed in multi-group models
(Geminiani et al., 2021).
</p>
<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currently,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well. Models are fitted with the glmnet or ista optimizer. Note that the
optimizers differ in which penalties they support. The following table provides
an overview:</p>

<table>
<tr>
 <td style="text-align: left;">
   Penalty </td><td style="text-align: left;"> Function </td><td style="text-align: left;"> glmnet </td><td style="text-align: left;"> ista </td>
</tr>
<tr>
 <td style="text-align: left;">
   lasso </td><td style="text-align: left;"> addLasso </td><td style="text-align: left;"> x </td><td style="text-align: left;"> x </td>
</tr>
<tr>
 <td style="text-align: left;">
   elastic net </td><td style="text-align: left;"> addElasticNet </td><td style="text-align: left;"> x* </td><td style="text-align: left;"> - </td>
</tr>
<tr>
 <td style="text-align: left;">
   cappedL1 </td><td style="text-align: left;"> addCappedL1 </td><td style="text-align: left;"> x </td><td style="text-align: left;"> x </td>
</tr>
<tr>
 <td style="text-align: left;">
   lsp </td><td style="text-align: left;"> addLsp </td><td style="text-align: left;"> x </td><td style="text-align: left;"> x </td>
</tr>
<tr>
 <td style="text-align: left;">
   scad </td><td style="text-align: left;"> addScad </td><td style="text-align: left;"> x </td><td style="text-align: left;"> x </td>
</tr>
<tr>
 <td style="text-align: left;">
   mcp </td><td style="text-align: left;"> addMcp </td><td style="text-align: left;"> x </td><td style="text-align: left;"> x </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>By default, glmnet will be used. Note that the elastic net penalty can
only be combined with other elastic net penalties.
</p>
<p>Check vignette(topic = &quot;Mixed-Penalties&quot;, package = &quot;lessSEM&quot;) for more details.
</p>
<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Geminiani, E., Marra, G., &amp; Moustaki, I. (2021). Single- and multiple-group penalized factor analysis:
A trust-region algorithm approach with integrated automatic multiple tuning parameter selection.
Psychometrika, 86(1), 65–95. https://doi.org/10.1007/s11336-021-09751-8
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Model of class regularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 + 
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 + 
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

# In this example, we want to regularize the loadings l6-l10 
# independently of the loadings l11-15. This could, for instance,
# reflect that the items y6-y10 and y11-y15 may belong to different
# subscales. 

regularized &lt;- lavaanModel |&gt;
  # create template for regularized model with mixed penalty:
  mixedPenalty() |&gt;
  # add lasso penalty on loadings l6 - l10:
  addLasso(regularized = paste0("l", 6:10), 
           lambdas = seq(0,1,length.out = 4)) |&gt;
  # add scad penalty on loadings l11 - l15:
  addScad(regularized = paste0("l", 11:15), 
          lambdas = seq(0,1,length.out = 3),
          thetas = 3.1) |&gt;
  # fit the model:
  fit()

# elements of regularized can be accessed with the @ operator:
regularized@parameters[1,]

# AIC and BIC:
AIC(regularized)
BIC(regularized)

# The best parameters can also be extracted with:
coef(regularized, criterion = "AIC")
coef(regularized, criterion = "BIC")

# The tuningParameterConfiguration corresponds to the rows
# in the lambda, theta, and alpha matrices in regularized@tuningParamterConfigurations.
# Configuration 3, for example, is given by
regularized@tuningParameterConfigurations$lambda[3,]
regularized@tuningParameterConfigurations$theta[3,]
regularized@tuningParameterConfigurations$alpha[3,] 
# Note that lambda, theta, and alpha may correspond to tuning parameters
# of different penalties for different parameters (e.g., lambda for l6 is the lambda
# of the lasso penalty, while lambda for l12 is the lambda of the scad penalty).

</code></pre>

<hr>
<h2 id='modifyModel'>modifyModel</h2><span id='topic+modifyModel'></span>

<h3>Description</h3>

<p>Modify the model from lavaan to fit your needs
</p>


<h3>Usage</h3>

<pre><code class='language-R'>modifyModel(
  addMeans = FALSE,
  activeSet = NULL,
  dataSet = NULL,
  transformations = NULL,
  transformationList = list(),
  transformationGradientStepSize = 1e-06
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="modifyModel_+3A_addmeans">addMeans</code></td>
<td>
<p>If lavaanModel has meanstructure = FALSE, addMeans = TRUE will add a mean structure. FALSE will set the means of the observed variables to their observed means.</p>
</td></tr>
<tr><td><code id="modifyModel_+3A_activeset">activeSet</code></td>
<td>
<p>Option to only use a subset of the individuals in the data set. Logical vector of length N indicating which subjects should remain in the sample.</p>
</td></tr>
<tr><td><code id="modifyModel_+3A_dataset">dataSet</code></td>
<td>
<p>option to replace the data set in the lavaan model with a different data set. Can be useful for cross-validation</p>
</td></tr>
<tr><td><code id="modifyModel_+3A_transformations">transformations</code></td>
<td>
<p>allows for transformations of parameters - useful for measurement invariance tests etc.</p>
</td></tr>
<tr><td><code id="modifyModel_+3A_transformationlist">transformationList</code></td>
<td>
<p>optional list used within the transformations. NOTE: This must be used as an Rcpp::List.</p>
</td></tr>
<tr><td><code id="modifyModel_+3A_transformationgradientstepsize">transformationGradientStepSize</code></td>
<td>
<p>step size used to compute the gradients of the
transformations</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class modifyModel
</p>


<h3>Examples</h3>

<pre><code class='language-R'>modification &lt;- modifyModel(addMeans = TRUE) # adds intercepts to a lavaan object
# that was fitted without explicit intercepts
</code></pre>

<hr>
<h2 id='newTau'>newTau</h2><span id='topic+newTau'></span>

<h3>Description</h3>

<p>assign new value to parameter tau used by approximate optimization. Any regularized
value below tau will be evaluated as zeroed which directly impacts the AIC, BIC, etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>newTau(regularizedSEM, tau)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="newTau_+3A_regularizedsem">regularizedSEM</code></td>
<td>
<p>object fitted with approximate optimization</p>
</td></tr>
<tr><td><code id="newTau_+3A_tau">tau</code></td>
<td>
<p>new tau value</p>
</td></tr>
</table>


<h3>Value</h3>

<p>regularizedSEM, but with new regularizedSEM@fits$nonZeroParameters
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

lsem &lt;- smoothLasso(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = paste0("l", 6:15),
  epsilon = 1e-10,
  tau = 1e-4,
  lambdas = seq(0,1,length.out = 50))
newTau(regularizedSEM = lsem, tau = .1)
</code></pre>

<hr>
<h2 id='plot+2CcvRegularizedSEM+2Cmissing-method'>plots the cross-validation fits</h2><span id='topic+plot+2CcvRegularizedSEM+2Cmissing-method'></span>

<h3>Description</h3>

<p>plots the cross-validation fits
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'cvRegularizedSEM,missing'
plot(x, y, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot+2B2CcvRegularizedSEM+2B2Cmissing-method_+3A_x">x</code></td>
<td>
<p>object of class cvRegularizedSEM</p>
</td></tr>
<tr><td><code id="plot+2B2CcvRegularizedSEM+2B2Cmissing-method_+3A_y">y</code></td>
<td>
<p>not used</p>
</td></tr>
<tr><td><code id="plot+2B2CcvRegularizedSEM+2B2Cmissing-method_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>either an object of ggplot2 or of plotly
</p>

<hr>
<h2 id='plot+2CgpRegularized+2Cmissing-method'>plots the regularized and unregularized parameters for all levels of lambda</h2><span id='topic+plot+2CgpRegularized+2Cmissing-method'></span>

<h3>Description</h3>

<p>plots the regularized and unregularized parameters for all levels of lambda
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'gpRegularized,missing'
plot(x, y, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot+2B2CgpRegularized+2B2Cmissing-method_+3A_x">x</code></td>
<td>
<p>object of class gpRegularized</p>
</td></tr>
<tr><td><code id="plot+2B2CgpRegularized+2B2Cmissing-method_+3A_y">y</code></td>
<td>
<p>not used</p>
</td></tr>
<tr><td><code id="plot+2B2CgpRegularized+2B2Cmissing-method_+3A_...">...</code></td>
<td>
<p>use regularizedOnly=FALSE to plot all parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>either an object of ggplot2 or of plotly
</p>

<hr>
<h2 id='plot+2CregularizedSEM+2Cmissing-method'>plots the regularized and unregularized parameters for all levels of lambda</h2><span id='topic+plot+2CregularizedSEM+2Cmissing-method'></span>

<h3>Description</h3>

<p>plots the regularized and unregularized parameters for all levels of lambda
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'regularizedSEM,missing'
plot(x, y, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot+2B2CregularizedSEM+2B2Cmissing-method_+3A_x">x</code></td>
<td>
<p>object of class gpRegularized</p>
</td></tr>
<tr><td><code id="plot+2B2CregularizedSEM+2B2Cmissing-method_+3A_y">y</code></td>
<td>
<p>not used</p>
</td></tr>
<tr><td><code id="plot+2B2CregularizedSEM+2B2Cmissing-method_+3A_...">...</code></td>
<td>
<p>use regularizedOnly=FALSE to plot all parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>either an object of ggplot2 or of plotly
</p>

<hr>
<h2 id='plot+2CstabSel+2Cmissing-method'>plots the regularized and unregularized parameters for all levels of the tuning parameters</h2><span id='topic+plot+2CstabSel+2Cmissing-method'></span>

<h3>Description</h3>

<p>plots the regularized and unregularized parameters for all levels of the tuning parameters
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'stabSel,missing'
plot(x, y, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot+2B2CstabSel+2B2Cmissing-method_+3A_x">x</code></td>
<td>
<p>object of class stabSel</p>
</td></tr>
<tr><td><code id="plot+2B2CstabSel+2B2Cmissing-method_+3A_y">y</code></td>
<td>
<p>not used</p>
</td></tr>
<tr><td><code id="plot+2B2CstabSel+2B2Cmissing-method_+3A_...">...</code></td>
<td>
<p>use regularizedOnly=FALSE to plot all parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>either an object of ggplot2 or of plotly
</p>

<hr>
<h2 id='Rcpp_bfgsEnetMgSEM-class'>Wrapper for C++ module. See ?lessSEM::bfgsEnetMgSEM</h2><span id='topic+Rcpp_bfgsEnetMgSEM-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::bfgsEnetMgSEM
</p>

<hr>
<h2 id='Rcpp_bfgsEnetSEM-class'>Wrapper for C++ module. See ?lessSEM::bfgsEnetSEM</h2><span id='topic+Rcpp_bfgsEnetSEM-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::bfgsEnetSEM
</p>

<hr>
<h2 id='Rcpp_glmnetCappedL1MgSEM-class'>Wrapper for C++ module. See ?lessSEM::glmnetCappedL1MgSEM</h2><span id='topic+Rcpp_glmnetCappedL1MgSEM-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::glmnetCappedL1MgSEM
</p>

<hr>
<h2 id='Rcpp_glmnetCappedL1SEM-class'>Wrapper for C++ module. See ?lessSEM:::glmnetCappedL1SEM</h2><span id='topic+Rcpp_glmnetCappedL1SEM-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM:::glmnetCappedL1SEM
</p>

<hr>
<h2 id='Rcpp_glmnetEnetGeneralPurpose-class'>Wrapper for C++ module. See ?lessSEM::glmnetEnetGeneralPurpose</h2><span id='topic+Rcpp_glmnetEnetGeneralPurpose-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::glmnetEnetGeneralPurpose
</p>

<hr>
<h2 id='Rcpp_glmnetEnetGeneralPurposeCpp-class'>Wrapper for C++ module. See ?lessSEM::glmnetEnetGeneralPurposeCpp</h2><span id='topic+Rcpp_glmnetEnetGeneralPurposeCpp-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::glmnetEnetGeneralPurposeCpp
</p>

<hr>
<h2 id='Rcpp_glmnetEnetMgSEM-class'>Wrapper for C++ module. See ?lessSEM::glmnetEnetMgSEM</h2><span id='topic+Rcpp_glmnetEnetMgSEM-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::glmnetEnetMgSEM
</p>

<hr>
<h2 id='Rcpp_glmnetEnetSEM-class'>Wrapper for C++ module. See ?lessSEM::glmnetEnetSEM</h2><span id='topic+Rcpp_glmnetEnetSEM-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::glmnetEnetSEM
</p>

<hr>
<h2 id='Rcpp_glmnetLspMgSEM-class'>Wrapper for C++ module. See ?lessSEM::glmnetLspMgSEM</h2><span id='topic+Rcpp_glmnetLspMgSEM-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::glmnetLspMgSEM
</p>

<hr>
<h2 id='Rcpp_glmnetLspSEM-class'>Wrapper for C++ module. See ?lessSEM:::glmnetLspSEM</h2><span id='topic+Rcpp_glmnetLspSEM-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM:::glmnetLspSEM
</p>

<hr>
<h2 id='Rcpp_glmnetMcpMgSEM-class'>Wrapper for C++ module. See ?lessSEM::glmnetMcpMgSEM</h2><span id='topic+Rcpp_glmnetMcpMgSEM-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::glmnetMcpMgSEM
</p>

<hr>
<h2 id='Rcpp_glmnetMcpSEM-class'>Wrapper for C++ module. See ?lessSEM:::glmnetMcpSEM</h2><span id='topic+Rcpp_glmnetMcpSEM-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM:::glmnetMcpSEM
</p>

<hr>
<h2 id='Rcpp_glmnetScadMgSEM-class'>Wrapper for C++ module. See ?lessSEM::glmnetScadMgSEM</h2><span id='topic+Rcpp_glmnetScadMgSEM-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::glmnetScadMgSEM
</p>

<hr>
<h2 id='Rcpp_glmnetScadSEM-class'>Wrapper for C++ module. See ?lessSEM:::glmnetScadSEM</h2><span id='topic+Rcpp_glmnetScadSEM-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM:::glmnetScadSEM
</p>

<hr>
<h2 id='Rcpp_istaCappedL1GeneralPurpose-class'>Wrapper for C++ module. See ?lessSEM::istaCappedL1GeneralPurpose</h2><span id='topic+Rcpp_istaCappedL1GeneralPurpose-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::istaCappedL1GeneralPurpose
</p>

<hr>
<h2 id='Rcpp_istaCappedL1GeneralPurposeCpp-class'>Wrapper for C++ module. See ?lessSEM::istaCappedL1GeneralPurposeCpp</h2><span id='topic+Rcpp_istaCappedL1GeneralPurposeCpp-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::istaCappedL1GeneralPurposeCpp
</p>

<hr>
<h2 id='Rcpp_istaCappedL1mgSEM-class'>Wrapper for C++ module. See ?lessSEM::istaCappedL1MgSEM</h2><span id='topic+Rcpp_istaCappedL1mgSEM-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::istaCappedL1MgSEM
</p>

<hr>
<h2 id='Rcpp_istaCappedL1SEM-class'>Wrapper for C++ module. See ?lessSEM::istaCappedL1SEM</h2><span id='topic+Rcpp_istaCappedL1SEM-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::istaCappedL1SEM
</p>

<hr>
<h2 id='Rcpp_istaEnetGeneralPurpose-class'>Wrapper for C++ module. See ?lessSEM::istaEnetGeneralPurpose</h2><span id='topic+Rcpp_istaEnetGeneralPurpose-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::istaEnetGeneralPurpose
</p>

<hr>
<h2 id='Rcpp_istaEnetGeneralPurposeCpp-class'>Wrapper for C++ module. See ?lessSEM::istaEnetGeneralPurposeCpp</h2><span id='topic+Rcpp_istaEnetGeneralPurposeCpp-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::istaEnetGeneralPurposeCpp
</p>

<hr>
<h2 id='Rcpp_istaEnetMgSEM-class'>Wrapper for C++ module. See ?lessSEM::istaEnetMgSEM</h2><span id='topic+Rcpp_istaEnetMgSEM-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::istaEnetMgSEM
</p>
<p>Wrapper for C++ module. See ?lessSEM::istaEnetMgSEM
</p>

<hr>
<h2 id='Rcpp_istaEnetSEM-class'>Wrapper for C++ module. See ?lessSEM::istaEnetSEM</h2><span id='topic+Rcpp_istaEnetSEM-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::istaEnetSEM
</p>
<p>Wrapper for C++ module. See ?lessSEM::istaEnetSEM
</p>

<hr>
<h2 id='Rcpp_istaLspGeneralPurpose-class'>Wrapper for C++ module. See ?lessSEM::istaLspGeneralPurpose</h2><span id='topic+Rcpp_istaLspGeneralPurpose-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::istaLspGeneralPurpose
</p>

<hr>
<h2 id='Rcpp_istaLspGeneralPurposeCpp-class'>Wrapper for C++ module. See ?lessSEM::istaLspGeneralPurposeCpp</h2><span id='topic+Rcpp_istaLspGeneralPurposeCpp-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::istaLspGeneralPurposeCpp
</p>

<hr>
<h2 id='Rcpp_istaLSPMgSEM-class'>Wrapper for C++ module. See ?lessSEM::istaLSPMgSEM</h2><span id='topic+Rcpp_istaLSPMgSEM-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::istaLSPMgSEM
</p>

<hr>
<h2 id='Rcpp_istaLSPSEM-class'>Wrapper for C++ module. See ?lessSEM::istaLSPSEM</h2><span id='topic+Rcpp_istaLSPSEM-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::istaLSPSEM
</p>

<hr>
<h2 id='Rcpp_istaMcpGeneralPurpose-class'>Wrapper for C++ module. See ?lessSEM::istaMcpGeneralPurpose</h2><span id='topic+Rcpp_istaMcpGeneralPurpose-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::istaMcpGeneralPurpose
</p>

<hr>
<h2 id='Rcpp_istaMcpGeneralPurposeCpp-class'>Wrapper for C++ module. See ?lessSEM::istaMcpGeneralPurposeCpp</h2><span id='topic+Rcpp_istaMcpGeneralPurposeCpp-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::istaMcpGeneralPurposeCpp
</p>

<hr>
<h2 id='Rcpp_istaMcpMgSEM-class'>Wrapper for C++ module. See ?lessSEM::istaMcpMgSEM</h2><span id='topic+Rcpp_istaMcpMgSEM-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::istaMcpMgSEM
</p>

<hr>
<h2 id='Rcpp_istaMcpSEM-class'>Wrapper for C++ module. See ?lessSEM::istaMcpSEM</h2><span id='topic+Rcpp_istaMcpSEM-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::istaMcpSEM
</p>

<hr>
<h2 id='Rcpp_istaMixedPenaltymgSEM-class'>Wrapper for C++ module. See ?lessSEM::istaMixedPenaltymgSEM</h2><span id='topic+Rcpp_istaMixedPenaltymgSEM-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::istaMixedPenaltymgSEM
</p>

<hr>
<h2 id='Rcpp_istaMixedPenaltySEM-class'>Wrapper for C++ module. See ?lessSEM::istaMixedPenaltySEM</h2><span id='topic+Rcpp_istaMixedPenaltySEM-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::istaMixedPenaltySEM
</p>

<hr>
<h2 id='Rcpp_istaScadGeneralPurpose-class'>Wrapper for C++ module. See ?lessSEM::istaScadGeneralPurpose</h2><span id='topic+Rcpp_istaScadGeneralPurpose-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::istaScadGeneralPurpose
</p>

<hr>
<h2 id='Rcpp_istaScadGeneralPurposeCpp-class'>Wrapper for C++ module. See ?lessSEM::istaScadGeneralPurposeCpp</h2><span id='topic+Rcpp_istaScadGeneralPurposeCpp-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::istaScadGeneralPurposeCpp
</p>

<hr>
<h2 id='Rcpp_istaScadMgSEM-class'>Wrapper for C++ module. See ?lessSEM::istaScadMgSEM</h2><span id='topic+Rcpp_istaScadMgSEM-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::istaScadMgSEM
</p>

<hr>
<h2 id='Rcpp_istaScadSEM-class'>Wrapper for C++ module. See ?lessSEM::istaScadSEM</h2><span id='topic+Rcpp_istaScadSEM-class'></span>

<h3>Description</h3>

<p>Wrapper for C++ module. See ?lessSEM::istaScadSEM
</p>

<hr>
<h2 id='Rcpp_mgSEM-class'>internal representation of SEM in C++</h2><span id='topic+Rcpp_mgSEM-class'></span>

<h3>Description</h3>

<p>internal representation of SEM in C++
</p>

<hr>
<h2 id='Rcpp_SEMCpp-class'>internal representation of SEM in C++</h2><span id='topic+Rcpp_SEMCpp-class'></span>

<h3>Description</h3>

<p>internal representation of SEM in C++
</p>

<hr>
<h2 id='regressions'>regressions</h2><span id='topic+regressions'></span>

<h3>Description</h3>

<p>Extract the labels of all regressions found in a lavaan model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regressions(lavaanModel)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regressions_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>fitted lavaan model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector with parameter labels
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The following is adapted from ?lavaan::sem
library(lessSEM)
model &lt;- ' 
  # latent variable definitions
  ind60 =~ x1 + x2 + x3
  dem60 =~ y1 + a*y2 + b*y3 + c*y4
  dem65 =~ y5 + a*y6 + b*y7 + c*y8

  # regressions
  dem60 ~ ind60
  dem65 ~ ind60 + dem60

  # residual correlations
  y1 ~~ y5
  y2 ~~ y4 + y6
  y3 ~~ y7
  y4 ~~ y8
  y6 ~~ y8
'

fit &lt;- sem(model, data = PoliticalDemocracy)

regressions(fit)
</code></pre>

<hr>
<h2 id='regsem2LavaanParameters'>regsem2LavaanParameters</h2><span id='topic+regsem2LavaanParameters'></span>

<h3>Description</h3>

<p>helper function: regsem and lavaan use slightly different parameter labels. This function
can be used to translate the parameter labels of a cv_regsem object to lavaan labels
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regsem2LavaanParameters(regsemModel, lavaanModel)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="regsem2LavaanParameters_+3A_regsemmodel">regsemModel</code></td>
<td>
<p>model of class regsem</p>
</td></tr>
<tr><td><code id="regsem2LavaanParameters_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
</table>


<h3>Value</h3>

<p>regsem parameters with lavaan labels
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## The following is adapted from ?regsem::regsem.
#library(lessSEM)
#library(regsem)
## put variables on same scale for regsem
#HS &lt;- data.frame(scale(HolzingerSwineford1939[,7:15]))
#
#mod &lt;- '
#f =~ 1*x1 + l1*x2 + l2*x3 + l3*x4 + l4*x5 + l5*x6 + l6*x7 + l7*x8 + l8*x9
#'
## Recommended to specify meanstructure in lavaan
#lavaanModel &lt;- cfa(mod, HS, meanstructure=TRUE)
#
#regsemModel &lt;- regsem(lavaanModel, 
#                lambda = 0.3, 
#                gradFun = "ram",
#                type="lasso",
#                pars_pen=c("l1", "l2", "l6", "l7", "l8"))
# regsem2LavaanParameters(regsemModel = regsemModel,
#                         lavaanModel = lavaanModel)
</code></pre>

<hr>
<h2 id='regularizedSEM-class'>Class for regularized SEM</h2><span id='topic+regularizedSEM-class'></span>

<h3>Description</h3>

<p>Class for regularized SEM
</p>


<h3>Slots</h3>


<dl>
<dt><code>penalty</code></dt><dd><p>penalty used (e.g., &quot;lasso&quot;)</p>
</dd>
<dt><code>parameters</code></dt><dd><p>data.frame with parameter estimates</p>
</dd>
<dt><code>fits</code></dt><dd><p>data.frame with all fit results</p>
</dd>
<dt><code>parameterLabels</code></dt><dd><p>character vector with names of all parameters</p>
</dd>
<dt><code>weights</code></dt><dd><p>vector with weights given to each of the parameters in the penalty</p>
</dd>
<dt><code>regularized</code></dt><dd><p>character vector with names of regularized parameters</p>
</dd>
<dt><code>transformations</code></dt><dd><p>if the model has transformations, the transformed parameters
are returned</p>
</dd>
<dt><code>internalOptimization</code></dt><dd><p>list of elements used internally</p>
</dd>
<dt><code>inputArguments</code></dt><dd><p>list with elements passed by the user to the general</p>
</dd>
<dt><code>notes</code></dt><dd><p>internal notes that have come up when fitting the model</p>
</dd>
</dl>

<hr>
<h2 id='regularizedSEMMixedPenalty-class'>Class for regularized SEM</h2><span id='topic+regularizedSEMMixedPenalty-class'></span>

<h3>Description</h3>

<p>Class for regularized SEM
</p>


<h3>Slots</h3>


<dl>
<dt><code>penalty</code></dt><dd><p>penalty used (e.g., &quot;lasso&quot;)</p>
</dd>
<dt><code>tuningParameterConfigurations</code></dt><dd><p>list with settings for the lambda, theta,
and alpha tuning parameters.</p>
</dd>
<dt><code>parameters</code></dt><dd><p>data.frame with parameter estimates</p>
</dd>
<dt><code>fits</code></dt><dd><p>data.frame with all fit results</p>
</dd>
<dt><code>parameterLabels</code></dt><dd><p>character vector with names of all parameters</p>
</dd>
<dt><code>weights</code></dt><dd><p>vector with weights given to each of the parameters in the penalty</p>
</dd>
<dt><code>regularized</code></dt><dd><p>character vector with names of regularized parameters</p>
</dd>
<dt><code>transformations</code></dt><dd><p>if the model has transformations, the transformed parameters
are returned</p>
</dd>
<dt><code>internalOptimization</code></dt><dd><p>list of elements used internally</p>
</dd>
<dt><code>inputArguments</code></dt><dd><p>list with elements passed by the user to the general</p>
</dd>
<dt><code>notes</code></dt><dd><p>internal notes that have come up when fitting the model</p>
</dd>
</dl>

<hr>
<h2 id='regularizedSEMWithCustomPenalty-class'>Class for regularized SEM using Rsolnp</h2><span id='topic+regularizedSEMWithCustomPenalty-class'></span>

<h3>Description</h3>

<p>Class for regularized SEM using Rsolnp
</p>


<h3>Slots</h3>


<dl>
<dt><code>parameters</code></dt><dd><p>data.frame with parameter estimates</p>
</dd>
<dt><code>fits</code></dt><dd><p>data.frame with all fit results</p>
</dd>
<dt><code>parameterLabels</code></dt><dd><p>character vector with names of all parameters</p>
</dd>
<dt><code>internalOptimization</code></dt><dd><p>list of elements used internally</p>
</dd>
<dt><code>inputArguments</code></dt><dd><p>list with elements passed by the user to the general</p>
</dd>
<dt><code>notes</code></dt><dd><p>internal notes that have come up when fitting the model</p>
</dd>
</dl>

<hr>
<h2 id='ridge'>ridge</h2><span id='topic+ridge'></span>

<h3>Description</h3>

<p>Implements ridge regularization for structural equation models.
The penalty function is given by:
</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \lambda x_j^2</code>
</p>

<p>Note that ridge regularization will not set any of the parameters to zero
but result in a shrinkage towards zero.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ridge(
  lavaanModel,
  regularized,
  lambdas,
  method = "glmnet",
  modifyModel = lessSEM::modifyModel(),
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ridge_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="ridge_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="ridge_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="ridge_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet. With ista, the control argument can be used to switch to related procedures
(currently gist).</p>
</td></tr>
<tr><td><code id="ridge_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="ridge_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta and controlGlmnet functions. See ?controlIsta and ?controlGlmnet
for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currently,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>Ridge regularization:
</p>

<ul>
<li><p> Hoerl, A. E., &amp; Kennard, R. W. (1970). Ridge Regression: Biased Estimation
for Nonorthogonal Problems. Technometrics, 12(1), 55–67.
https://doi.org/10.1080/00401706.1970.10488634
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Model of class regularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

lsem &lt;- ridge(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = paste0("l", 6:15),
  lambdas = seq(0,1,length.out = 20))

# use the plot-function to plot the regularized parameters:
plot(lsem)

# the coefficients can be accessed with:
coef(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters[1,]


#### Advanced ###
# Switching the optimizer #
# Use the "method" argument to switch the optimizer. The control argument
# must also be changed to the corresponding function:
lsemIsta &lt;- ridge(
  lavaanModel = lavaanModel,
  regularized = paste0("l", 6:15),
  lambdas = seq(0,1,length.out = 20),
  method = "ista",
  control = controlIsta())

# Note: The results are basically identical:
lsemIsta@parameters - lsem@parameters
</code></pre>

<hr>
<h2 id='ridgeBfgs'>ridgeBfgs</h2><span id='topic+ridgeBfgs'></span>

<h3>Description</h3>

<p>This function allows for regularization of models built in lavaan with the
ridge penalty. Its elements can be accessed
with the &quot;@&quot; operator (see examples).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ridgeBfgs(
  lavaanModel,
  regularized,
  lambdas = NULL,
  modifyModel = lessSEM::modifyModel(),
  control = lessSEM::controlBFGS()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ridgeBfgs_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="ridgeBfgs_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="ridgeBfgs_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="ridgeBfgs_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="ridgeBfgs_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlBFGS function. See ?controlBFGS for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For more details, see:
</p>

<ol>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016).
Regularized Structural Equation Modeling. Structural Equation Modeling:
A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood
Method for Structural Equation Modeling. Psychometrika, 82(2),
329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li></ol>



<h3>Value</h3>

<p>Model of class regularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

# names of the regularized parameters:
regularized = paste0("l", 6:15)

lsem &lt;- ridgeBfgs(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  regularized = regularized,
  lambdas = seq(0,1,length.out = 50))

plot(lsem)

# the coefficients can be accessed with:
coef(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters[1,]
</code></pre>

<hr>
<h2 id='scad'>scad</h2><span id='topic+scad'></span>

<h3>Description</h3>

<p>Implements scad regularization for structural equation models.
The penalty function is given by:

</p>
<p style="text-align: center;"><code class="reqn">p( x_j) = \begin{cases}
\lambda |x_j| &amp; \text{if } |x_j| \leq \theta\\
\frac{-x_j^2 + 2\theta\lambda |x_j| - \lambda^2}{2(\theta -1)} &amp; 
\text{if } \lambda &lt; |x_j| \leq \lambda\theta \\
(\theta + 1) \lambda^2/2 &amp; \text{if } |x_j| \geq \theta\lambda\\
\end{cases}</code>
</p>

<p>where <code class="reqn">\theta &gt; 2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scad(
  lavaanModel,
  regularized,
  lambdas,
  thetas,
  modifyModel = lessSEM::modifyModel(),
  method = "glmnet",
  control = lessSEM::controlGlmnet()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scad_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="scad_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="scad_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="scad_+3A_thetas">thetas</code></td>
<td>
<p>parameters whose absolute value is above this threshold will be penalized with
a constant (theta)</p>
</td></tr>
<tr><td><code id="scad_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="scad_+3A_method">method</code></td>
<td>
<p>which optimizer should be used? Currently implemented are ista
and glmnet. With ista, the control argument can be used to switch to related procedures
(currently gist).</p>
</td></tr>
<tr><td><code id="scad_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlIsta (see ?controlIsta)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Identical to <span class="pkg">regsem</span>, models are specified using <span class="pkg">lavaan</span>. Currently,
most standard SEM are supported. <span class="pkg">lessSEM</span> also provides full information
maximum likelihood for missing data. To use this functionality,
fit your <span class="pkg">lavaan</span> model with the argument <code>sem(..., missing = 'ml')</code>.
<span class="pkg">lessSEM</span> will then automatically switch to full information maximum likelihood
as well.
</p>
<p>scad regularization:
</p>

<ul>
<li><p> Fan, J., &amp; Li, R. (2001). Variable selection via nonconcave penalized
likelihood and its oracle properties. Journal of the American Statistical Association,
96(456), 1348–1360. https://doi.org/10.1198/016214501753382273
</p>
</li></ul>

<p>Regularized SEM
</p>

<ul>
<li><p> Huang, P.-H., Chen, H., &amp; Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ul>

<p>For more details on GLMNET, see:
</p>

<ul>
<li><p> Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
</p>
</li>
<li><p> Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183–3234.
</p>
</li>
<li><p> Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
</p>
</li></ul>

<p>For more details on ISTA, see:
</p>

<ul>
<li><p> Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
</p>
</li>
<li><p> Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
</p>
</li>
<li><p> Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
</p>
</li></ul>



<h3>Value</h3>

<p>Model of class regularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

lsem &lt;- scad(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = paste0("l", 6:15),
  lambdas = seq(0,1,length.out = 20),
  thetas = seq(2.01,5,length.out = 5))

# the coefficients can be accessed with:
coef(lsem)

# if you are only interested in the estimates and not the tuning parameters, use
coef(lsem)@estimates
# or
estimates(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters[1,]

# fit Measures:
fitIndices(lsem)

# The best parameters can also be extracted with:
coef(lsem, criterion = "AIC")
# or
estimates(lsem, criterion = "AIC")

# optional: plotting the paths requires installation of plotly
# plot(lsem)
</code></pre>

<hr>
<h2 id='scadPenalty_C'>scadPenalty_C</h2><span id='topic+scadPenalty_C'></span>

<h3>Description</h3>

<p>scadPenalty_C
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scadPenalty_C(par, lambda_p, theta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scadPenalty_C_+3A_par">par</code></td>
<td>
<p>single parameter value</p>
</td></tr>
<tr><td><code id="scadPenalty_C_+3A_lambda_p">lambda_p</code></td>
<td>
<p>lambda value for this parameter</p>
</td></tr>
<tr><td><code id="scadPenalty_C_+3A_theta">theta</code></td>
<td>
<p>theta value for this parameter</p>
</td></tr>
</table>


<h3>Value</h3>

<p>penalty value
</p>

<hr>
<h2 id='SEMCpp'>SEMCpp class</h2><span id='topic+SEMCpp'></span>

<h3>Description</h3>

<p>internal SEM representation
</p>


<h3>Fields</h3>


<dl>
<dt><code>new</code></dt><dd><p>Creates a new SEMCpp.</p>
</dd>
<dt><code>fill</code></dt><dd><p>fills the SEM with the elements from an Rcpp::List</p>
</dd>
<dt><code>addTransformation</code></dt><dd><p>adds transforamtions to a model</p>
</dd>
<dt><code>implied</code></dt><dd><p>Computes implied means and covariance matrix</p>
</dd>
<dt><code>fit</code></dt><dd><p>Fits the model. Returns objective value of the fitting function</p>
</dd>
<dt><code>getParameters</code></dt><dd><p>Returns a data frame with model parameters.</p>
</dd>
<dt><code>getEstimator</code></dt><dd><p>returns the estimator used in the model (e.g., fiml)</p>
</dd>
<dt><code>getParameterLabels</code></dt><dd><p>Returns a vector with unique parameter labels as used internally.</p>
</dd>
<dt><code>getGradients</code></dt><dd><p>Returns a matrix with scores.</p>
</dd>
<dt><code>getScores</code></dt><dd><p>Returns a matrix with scores.</p>
</dd>
<dt><code>getHessian</code></dt><dd><p>Returns the hessian of the model. Expects the labels of the
parameters and the values of the parameters as well as a boolean indicating if
these are raw. Finally, a double (eps) controls the precision of the approximation.</p>
</dd>
<dt><code>computeTransformations</code></dt><dd><p>compute the transformations.</p>
</dd>
<dt><code>setTransformationGradientStepSize</code></dt><dd><p>change the step size of the gradient computation for the transformations</p>
</dd>
</dl>

<hr>
<h2 id='show+2CcvRegularizedSEM-method'>Show method for objects of class <code>cvRegularizedSEM</code>.</h2><span id='topic+show+2CcvRegularizedSEM-method'></span>

<h3>Description</h3>

<p>Show method for objects of class <code>cvRegularizedSEM</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'cvRegularizedSEM'
show(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="show+2B2CcvRegularizedSEM-method_+3A_object">object</code></td>
<td>
<p>object of class cvRegularizedSEM</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, just prints estimates
</p>

<hr>
<h2 id='show+2CgpRegularized-method'>show</h2><span id='topic+show+2CgpRegularized-method'></span>

<h3>Description</h3>

<p>show
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'gpRegularized'
show(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="show+2B2CgpRegularized-method_+3A_object">object</code></td>
<td>
<p>object of class gpRegularized</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, just prints estimates
</p>

<hr>
<h2 id='show+2ClessSEMCoef-method'>show</h2><span id='topic+show+2ClessSEMCoef-method'></span>

<h3>Description</h3>

<p>show
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'lessSEMCoef'
show(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="show+2B2ClessSEMCoef-method_+3A_object">object</code></td>
<td>
<p>object of class lessSEMCoef</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, just prints estimates
</p>

<hr>
<h2 id='show+2ClogLikelihood-method'>show</h2><span id='topic+show+2ClogLikelihood-method'></span>

<h3>Description</h3>

<p>show
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'logLikelihood'
show(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="show+2B2ClogLikelihood-method_+3A_object">object</code></td>
<td>
<p>object of class logLikelihood</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, just prints estimates
</p>

<hr>
<h2 id='show+2CRcpp_mgSEM-method'>show</h2><span id='topic+show+2CRcpp_mgSEM-method'></span>

<h3>Description</h3>

<p>show
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'Rcpp_mgSEM'
show(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="show+2B2CRcpp_mgSEM-method_+3A_object">object</code></td>
<td>
<p>object of class Rcpp_mgSEM</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, just prints estimates
</p>

<hr>
<h2 id='show+2CRcpp_SEMCpp-method'>show</h2><span id='topic+show+2CRcpp_SEMCpp-method'></span>

<h3>Description</h3>

<p>show
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'Rcpp_SEMCpp'
show(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="show+2B2CRcpp_SEMCpp-method_+3A_object">object</code></td>
<td>
<p>object of class Rcpp_SEMCpp</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, just prints estimates
</p>

<hr>
<h2 id='show+2CregularizedSEM-method'>show</h2><span id='topic+show+2CregularizedSEM-method'></span>

<h3>Description</h3>

<p>show
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'regularizedSEM'
show(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="show+2B2CregularizedSEM-method_+3A_object">object</code></td>
<td>
<p>object of class regularizedSEM</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, just prints estimates
</p>

<hr>
<h2 id='show+2CregularizedSEMMixedPenalty-method'>show</h2><span id='topic+show+2CregularizedSEMMixedPenalty-method'></span>

<h3>Description</h3>

<p>show
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'regularizedSEMMixedPenalty'
show(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="show+2B2CregularizedSEMMixedPenalty-method_+3A_object">object</code></td>
<td>
<p>object of class regularizedSEM</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, just prints estimates
</p>

<hr>
<h2 id='show+2CstabSel-method'>show</h2><span id='topic+show+2CstabSel-method'></span>

<h3>Description</h3>

<p>show
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'stabSel'
show(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="show+2B2CstabSel-method_+3A_object">object</code></td>
<td>
<p>object of class stabSel</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, just prints estimates
</p>

<hr>
<h2 id='simulateExampleData'>simulateExampleData</h2><span id='topic+simulateExampleData'></span>

<h3>Description</h3>

<p>simulate data for a simple CFA model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulateExampleData(
  N = 100,
  loadings = c(rep(1, 5), rep(0.4, 5), rep(0, 5)),
  percentMissing = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulateExampleData_+3A_n">N</code></td>
<td>
<p>number of persons in the data set</p>
</td></tr>
<tr><td><code id="simulateExampleData_+3A_loadings">loadings</code></td>
<td>
<p>loadings of the latent variable on the manifest observations</p>
</td></tr>
<tr><td><code id="simulateExampleData_+3A_percentmissing">percentMissing</code></td>
<td>
<p>percentage of missing data</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data set for a single-factor CFA.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- lessSEM::simulateExampleData()
</code></pre>

<hr>
<h2 id='smoothAdaptiveLasso'>smoothAdaptiveLasso</h2><span id='topic+smoothAdaptiveLasso'></span>

<h3>Description</h3>

<p>This function allows for regularization of models built in lavaan with the
smooth adaptive lasso penalty. The returned object is an S4 class; its elements can be accessed
with the &quot;@&quot; operator (see examples).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smoothAdaptiveLasso(
  lavaanModel,
  regularized,
  weights = NULL,
  lambdas,
  epsilon,
  tau,
  modifyModel = lessSEM::modifyModel(),
  control = lessSEM::controlBFGS()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smoothAdaptiveLasso_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="smoothAdaptiveLasso_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="smoothAdaptiveLasso_+3A_weights">weights</code></td>
<td>
<p>labeled vector with weights for each of the parameters in the
model. If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object. If set to NULL,
the default weights will be used: the inverse of the absolute values of
the unregularized parameter estimates</p>
</td></tr>
<tr><td><code id="smoothAdaptiveLasso_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="smoothAdaptiveLasso_+3A_epsilon">epsilon</code></td>
<td>
<p>epsilon &gt; 0; controls the smoothness of the approximation. Larger values = smoother</p>
</td></tr>
<tr><td><code id="smoothAdaptiveLasso_+3A_tau">tau</code></td>
<td>
<p>parameters below threshold tau will be seen as zeroed</p>
</td></tr>
<tr><td><code id="smoothAdaptiveLasso_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="smoothAdaptiveLasso_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlBFGS function. See ?controlBFGS for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For more details, see:
</p>

<ol>
<li><p> Zou, H. (2006). The Adaptive Lasso and Its Oracle Properties.
Journal of the American Statistical Association, 101(476), 1418–1429.
https://doi.org/10.1198/016214506000000735
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016).
Regularized Structural Equation Modeling. Structural Equation Modeling:
A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li>
<li><p> Lee, S.-I., Lee, H., Abbeel, P., &amp; Ng, A. Y. (2006).
Efficient L1 Regularized Logistic Regression. Proceedings of the
Twenty-First National Conference on Artificial Intelligence (AAAI-06), 401–408.
</p>
</li></ol>



<h3>Value</h3>

<p>Model of class regularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

# names of the regularized parameters:
regularized = paste0("l", 6:15)

# define adaptive lasso weights:
# We use the inverse of the absolute unregularized parameters
# (this is the default in adaptiveLasso and can also specified
# by setting weights = NULL)
weights &lt;- 1/abs(getLavaanParameters(lavaanModel))
weights[!names(weights) %in% regularized] &lt;- 0

lsem &lt;- smoothAdaptiveLasso(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  regularized = regularized,
  weights = weights,
  epsilon = 1e-10,
  tau = 1e-4,
  lambdas = seq(0,1,length.out = 50))

# use the plot-function to plot the regularized parameters:
plot(lsem)

# the coefficients can be accessed with:
coef(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters[1,]

# AIC and BIC:
AIC(lsem)
BIC(lsem)

# The best parameters can also be extracted with:
coef(lsem, criterion = "AIC")
coef(lsem, criterion = "BIC")
</code></pre>

<hr>
<h2 id='smoothElasticNet'>smoothElasticNet</h2><span id='topic+smoothElasticNet'></span>

<h3>Description</h3>

<p>This function allows for regularization of models built in lavaan with the
smooth elastic net penalty. Its elements can be accessed
with the &quot;@&quot; operator (see examples).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smoothElasticNet(
  lavaanModel,
  regularized,
  lambdas = NULL,
  nLambdas = NULL,
  alphas,
  epsilon,
  tau,
  modifyModel = lessSEM::modifyModel(),
  control = lessSEM::controlBFGS()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smoothElasticNet_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="smoothElasticNet_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="smoothElasticNet_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="smoothElasticNet_+3A_nlambdas">nLambdas</code></td>
<td>
<p>alternative to lambda: If alpha = 1, lessSEM can automatically
compute the first lambda value which sets all regularized parameters to zero.
It will then generate nLambda values between 0 and the computed lambda.</p>
</td></tr>
<tr><td><code id="smoothElasticNet_+3A_alphas">alphas</code></td>
<td>
<p>numeric vector with values of the tuning parameter alpha. Must be
between 0 and 1. 0 = ridge, 1 = lasso.</p>
</td></tr>
<tr><td><code id="smoothElasticNet_+3A_epsilon">epsilon</code></td>
<td>
<p>epsilon &gt; 0; controls the smoothness of the approximation. Larger values = smoother</p>
</td></tr>
<tr><td><code id="smoothElasticNet_+3A_tau">tau</code></td>
<td>
<p>parameters below threshold tau will be seen as zeroed</p>
</td></tr>
<tr><td><code id="smoothElasticNet_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="smoothElasticNet_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlBFGS function. See ?controlBFGS for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For more details, see:
</p>

<ol>
<li><p> Zou, H., &amp; Hastie, T. (2005). Regularization and variable selection via
the elastic net. Journal of the Royal Statistical Society:
Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x
for the details of this regularization technique.
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016).
Regularized Structural Equation Modeling. Structural Equation Modeling:
A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li>
<li><p> Lee, S.-I., Lee, H., Abbeel, P., &amp; Ng, A. Y. (2006).
Efficient L1 Regularized Logistic Regression. Proceedings of the
Twenty-First National Conference on Artificial Intelligence (AAAI-06), 401–408.
</p>
</li></ol>



<h3>Value</h3>

<p>Model of class regularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

# names of the regularized parameters:
regularized = paste0("l", 6:15)

lsem &lt;- smoothElasticNet(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  regularized = regularized,
  epsilon = 1e-10,
  tau = 1e-4,
  lambdas = seq(0,1,length.out = 5),
  alphas = seq(0,1,length.out = 3))

# the coefficients can be accessed with:
coef(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters[1,]
</code></pre>

<hr>
<h2 id='smoothLasso'>smoothLasso</h2><span id='topic+smoothLasso'></span>

<h3>Description</h3>

<p>This function allows for regularization of models built in lavaan with the
smoothed lasso penalty. The returned object is an S4 class; its elements can be accessed
with the &quot;@&quot; operator (see examples). We don't recommend using this function.
Use lasso() instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smoothLasso(
  lavaanModel,
  regularized,
  lambdas,
  epsilon,
  tau,
  modifyModel = lessSEM::modifyModel(),
  control = lessSEM::controlBFGS()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smoothLasso_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>model of class lavaan</p>
</td></tr>
<tr><td><code id="smoothLasso_+3A_regularized">regularized</code></td>
<td>
<p>vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object</p>
</td></tr>
<tr><td><code id="smoothLasso_+3A_lambdas">lambdas</code></td>
<td>
<p>numeric vector: values for the tuning parameter lambda</p>
</td></tr>
<tr><td><code id="smoothLasso_+3A_epsilon">epsilon</code></td>
<td>
<p>epsilon &gt; 0; controls the smoothness of the approximation. Larger values = smoother</p>
</td></tr>
<tr><td><code id="smoothLasso_+3A_tau">tau</code></td>
<td>
<p>parameters below threshold tau will be seen as zeroed</p>
</td></tr>
<tr><td><code id="smoothLasso_+3A_modifymodel">modifyModel</code></td>
<td>
<p>used to modify the lavaanModel. See ?modifyModel.</p>
</td></tr>
<tr><td><code id="smoothLasso_+3A_control">control</code></td>
<td>
<p>used to control the optimizer. This element is generated with
the controlBFGS function. See ?controlBFGS for more details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For more details, see:
</p>

<ol>
<li><p> Lee, S.-I., Lee, H., Abbeel, P., &amp; Ng, A. Y. (2006).
Efficient L1 Regularized Logistic Regression. Proceedings of the
Twenty-First National Conference on Artificial Intelligence (AAAI-06), 401–408.
</p>
</li>
<li><p> Jacobucci, R., Grimm, K. J., &amp; McArdle, J. J. (2016).
Regularized Structural Equation Modeling. Structural Equation Modeling:
A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
</p>
</li></ol>



<h3>Value</h3>

<p>Model of class regularizedSEM
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Regularization:

lsem &lt;- smoothLasso(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = paste0("l", 6:15),
  epsilon = 1e-10,
  tau = 1e-4,
  lambdas = seq(0,1,length.out = 50))

# use the plot-function to plot the regularized parameters:
plot(lsem)

# the coefficients can be accessed with:
coef(lsem)

# elements of lsem can be accessed with the @ operator:
lsem@parameters[1,]

# AIC and BIC:
AIC(lsem)
BIC(lsem)

# The best parameters can also be extracted with:
coef(lsem, criterion = "AIC")
coef(lsem, criterion = "BIC")
</code></pre>

<hr>
<h2 id='stabilitySelection'>stabilitySelection</h2><span id='topic+stabilitySelection'></span>

<h3>Description</h3>

<p>Provides rudimentary stability selection for regularized SEM. Stability
selection has been proposed by Meinshausen &amp; Bühlmann (2010) and was
extended to SEM by Li &amp; Jacobucci (2021). The problem that stabiltiy selection
tries to solve is the instability of regularization procedures: Small changes in
the data set may result in different parameters being selected. To address
this issue, stability selection uses random subsamples from the initial data
set and fits models in these subsamples. For each parameter, we can now check
how often it is included in the model for a given set of tuning parameters.
Plotting these probabilities can provide an overview over which of the parameters
are often removed and which remain in the model most of the time. To get
a final selection, a threshold t can be defined: If a parameter is in the model
t% of the time, it is retained.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stabilitySelection(
  modelSpecification,
  subsampleSize,
  numberOfSubsamples = 100,
  threshold = 70,
  maxTries = 10 * numberOfSubsamples
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stabilitySelection_+3A_modelspecification">modelSpecification</code></td>
<td>
<p>a call to one of the penalty functions in lessSEM. See
examples for details</p>
</td></tr>
<tr><td><code id="stabilitySelection_+3A_subsamplesize">subsampleSize</code></td>
<td>
<p>number of subjects in each subsample. Must be smaller than
the number of subjects in the original data set</p>
</td></tr>
<tr><td><code id="stabilitySelection_+3A_numberofsubsamples">numberOfSubsamples</code></td>
<td>
<p>number of times the procedure should subsample and
recompute the model. According to Meinshausen &amp; Bühlmann (2010), 100 seems to
work quite well and is also the default in regsem</p>
</td></tr>
<tr><td><code id="stabilitySelection_+3A_threshold">threshold</code></td>
<td>
<p>percentage of models, where the parameter should be contained in order
to be in the final model</p>
</td></tr>
<tr><td><code id="stabilitySelection_+3A_maxtries">maxTries</code></td>
<td>
<p>fitting models in a subset may fail. maxTries sets the maximal
number of subsets to try.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>estimates for each subsample and aggregated percentages for each parameter
</p>


<h3>References</h3>


<ul>
<li><p> Li, X., &amp; Jacobucci, R. (2021). Regularized structural equation modeling with
stability selection. Psychological Methods, 27(4), 497–518. https://doi.org/10.1037/met0000389
</p>
</li>
<li><p> Meinshausen, N., &amp; Bühlmann, P. (2010). Stability selection. Journal of the
Royal Statistical Society: Series B (Statistical Methodology), 72(4), 417–473.
https://doi.org/10.1111/j.1467-9868.2010.00740.x
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>library(lessSEM)

# Identical to regsem, lessSEM builds on the lavaan
# package for model specification. The first step
# therefore is to implement the model in lavaan.

dataset &lt;- simulateExampleData()

lavaanSyntax &lt;- "
f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 + 
     l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 + 
     l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15
f ~~ 1*f
"

lavaanModel &lt;- lavaan::sem(lavaanSyntax,
                           data = dataset,
                           meanstructure = TRUE,
                           std.lv = TRUE)

# Stability selection
stabSel &lt;- stabilitySelection(
  # IMPORTANT: Wrap your call to the penalty function in an rlang::expr-Block:
  modelSpecification = 
    rlang::expr(
      lasso(
        # pass the fitted lavaan model
        lavaanModel = lavaanModel,
        # names of the regularized parameters:
        regularized = paste0("l", 6:15),
        # in case of lasso and adaptive lasso, we can specify the number of lambda
        # values to use. lessSEM will automatically find lambda_max and fit
        # models for nLambda values between 0 and lambda_max. For the other
        # penalty functions, lambdas must be specified explicitly
        nLambdas = 50)
    ),
  subsampleSize = 80,
  numberOfSubsamples = 5, # should be set to a much higher number (e.g., 100)
  threshold = 70
)
stabSel
plot(stabSel)
</code></pre>

<hr>
<h2 id='stabSel-class'>Class for stability selection</h2><span id='topic+stabSel-class'></span>

<h3>Description</h3>

<p>Class for stability selection
</p>


<h3>Slots</h3>


<dl>
<dt><code>regularized</code></dt><dd><p>names of regularized parameters</p>
</dd>
<dt><code>tuningParameters</code></dt><dd><p>data.frame with tuning parameter values</p>
</dd>
<dt><code>stabilityPaths</code></dt><dd><p>matrix with percentage of parameters being non-zero
averaged over all subsets for each setting of the tuning parameters</p>
</dd>
<dt><code>percentSelected</code></dt><dd><p>percentage with which a parameter was selected over all
tuning parameter settings</p>
</dd>
<dt><code>selectedParameters</code></dt><dd><p>final selected parameters</p>
</dd>
<dt><code>settings</code></dt><dd><p>internal</p>
</dd>
</dl>

<hr>
<h2 id='summary+2CcvRegularizedSEM-method'>summary method for objects of class <code>cvRegularizedSEM</code>.</h2><span id='topic+summary+2CcvRegularizedSEM-method'></span>

<h3>Description</h3>

<p>summary method for objects of class <code>cvRegularizedSEM</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'cvRegularizedSEM'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary+2B2CcvRegularizedSEM-method_+3A_object">object</code></td>
<td>
<p>object of class cvRegularizedSEM</p>
</td></tr>
<tr><td><code id="summary+2B2CcvRegularizedSEM-method_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, just prints estimates
</p>

<hr>
<h2 id='summary+2CgpRegularized-method'>summary</h2><span id='topic+summary+2CgpRegularized-method'></span>

<h3>Description</h3>

<p>summary
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'gpRegularized'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary+2B2CgpRegularized-method_+3A_object">object</code></td>
<td>
<p>object of class gpRegularized</p>
</td></tr>
<tr><td><code id="summary+2B2CgpRegularized-method_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, just prints estimates
</p>

<hr>
<h2 id='summary+2CregularizedSEM-method'>summary</h2><span id='topic+summary+2CregularizedSEM-method'></span>

<h3>Description</h3>

<p>summary
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'regularizedSEM'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary+2B2CregularizedSEM-method_+3A_object">object</code></td>
<td>
<p>object of class regularizedSEM</p>
</td></tr>
<tr><td><code id="summary+2B2CregularizedSEM-method_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, just prints estimates
</p>

<hr>
<h2 id='summary+2CregularizedSEMMixedPenalty-method'>summary</h2><span id='topic+summary+2CregularizedSEMMixedPenalty-method'></span>

<h3>Description</h3>

<p>summary
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'regularizedSEMMixedPenalty'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary+2B2CregularizedSEMMixedPenalty-method_+3A_object">object</code></td>
<td>
<p>object of class regularizedSEMMixedPenalty</p>
</td></tr>
<tr><td><code id="summary+2B2CregularizedSEMMixedPenalty-method_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, just prints estimates
</p>

<hr>
<h2 id='summary+2CregularizedSEMWithCustomPenalty-method'>summary</h2><span id='topic+summary+2CregularizedSEMWithCustomPenalty-method'></span>

<h3>Description</h3>

<p>summary
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'regularizedSEMWithCustomPenalty'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary+2B2CregularizedSEMWithCustomPenalty-method_+3A_object">object</code></td>
<td>
<p>object of class regularizedSEMWithCustomPenalty</p>
</td></tr>
<tr><td><code id="summary+2B2CregularizedSEMWithCustomPenalty-method_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, just prints estimates
</p>

<hr>
<h2 id='variances'>variances</h2><span id='topic+variances'></span>

<h3>Description</h3>

<p>Extract the labels of all variances found in a lavaan model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>variances(lavaanModel)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="variances_+3A_lavaanmodel">lavaanModel</code></td>
<td>
<p>fitted lavaan model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector with parameter labels
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The following is adapted from ?lavaan::sem
library(lessSEM)
model &lt;- ' 
  # latent variable definitions
  ind60 =~ x1 + x2 + x3
  dem60 =~ y1 + a*y2 + b*y3 + c*y4
  dem65 =~ y5 + a*y6 + b*y7 + c*y8

  # regressions
  dem60 ~ ind60
  dem65 ~ ind60 + dem60

  # residual correlations
  y1 ~~ y5
  y2 ~~ y4 + y6
  y3 ~~ y7
  y4 ~~ y8
  y6 ~~ y8
'

fit &lt;- sem(model, data = PoliticalDemocracy)

variances(fit)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
