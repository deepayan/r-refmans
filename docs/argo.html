<!DOCTYPE html><html lang="en"><head><title>Help for package argo</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {argo}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#argo'><p>Construct ARGO object</p></a></li>
<li><a href='#argo_main'><p>main function for argo</p></a></li>
<li><a href='#argo2'><p>ARGO second step</p></a></li>
<li><a href='#argo2_main'><p>main function for argo2</p></a></li>
<li><a href='#argox_main'><p>main function for argox</p></a></li>
<li><a href='#boot_re'><p>wrapper for bootstrap relative efficiency confidence interval</p></a></li>
<li><a href='#bootstrap_relative_efficiency'><p>bootstrap relative efficiency confidence interval</p></a></li>
<li><a href='#gt.parser.pub.api'><p>Parsing each Google Trends file downloaded from Google Trends API</p></a></li>
<li><a href='#gt.parser.pub.web'><p>Parsing each Google Trends file downloaded from website</p></a></li>
<li><a href='#heatmap_argo'><p>Heatmap plot of ARGO coefficients applied on CDC's ILI data</p></a></li>
<li><a href='#heatmap_cor'><p>Heatmap plot of correlation matrix</p></a></li>
<li><a href='#load_data'><p>Parsing of raw data</p></a></li>
<li><a href='#load_reg_data'><p>Parsing of raw data for regional ILI estimation</p></a></li>
<li><a href='#logit'><p>logit function</p></a></li>
<li><a href='#logit_inv'><p>inverse logit function</p></a></li>
<li><a href='#parse_gt_weekly'><p>Parsing of Google Trends data</p></a></li>
<li><a href='#parse_unrevised_ili'><p>Parsing of unrevised ili from online source</p></a></li>
<li><a href='#plot_argo'><p>Time series plot of ARGO applied on CDC's ILI data</p></a></li>
<li><a href='#summary_argo'><p>performance summary of ARGO applied on CDC's ILI data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Accurate Estimation of Influenza Epidemics using Google Search
Data</td>
</tr>
<tr>
<td>Version:</td>
<td>3.0.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-05-23</td>
</tr>
<tr>
<td>Author:</td>
<td>Shaoyang Ning, Shihao Yang, S. C. Kou</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Shihao Yang &lt;shihao.yang@isye.gatech.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Augmented Regression with General Online data (ARGO) for accurate estimation of influenza epidemics in United States on national level, regional level and state level. It replicates the method introduced in paper Yang, S., Santillana, M. and Kou, S.C. (2015) &lt;<a href="https://doi.org/10.1073%2Fpnas.1515373112">doi:10.1073/pnas.1515373112</a>&gt;; Ning, S., Yang, S. and Kou, S.C. (2019) &lt;<a href="https://doi.org/10.1038%2Fs41598-019-41559-6">doi:10.1038/s41598-019-41559-6</a>&gt;; Yang, S., Ning, S. and Kou, S.C. (2021) &lt;<a href="https://doi.org/10.1038%2Fs41598-021-83084-5">doi:10.1038/s41598-021-83084-5</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>xts, glmnet, zoo, XML, xtable, Matrix, boot</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-05-23 15:11:19 UTC; shihaoyang</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-05-24 11:20:14 UTC</td>
</tr>
</table>
<hr>
<h2 id='argo'>Construct ARGO object</h2><span id='topic+argo'></span>

<h3>Description</h3>

<p>Wrapper for ARGO. The real work horse is glmnet package and/or linear model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>argo(
  data,
  exogen = xts::xts(NULL),
  N_lag = 1:52,
  N_training = 104,
  alpha = 1,
  use_all_previous = FALSE,
  mc.cores = 1,
  schedule = list()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="argo_+3A_data">data</code></td>
<td>
<p>response variable as xts, last element can be NA. If the response
is later revised, it should be an xts that resembles upper triangular square
matrix, with each column being the data available as of date of column name</p>
</td></tr>
<tr><td><code id="argo_+3A_exogen">exogen</code></td>
<td>
<p>exogenous predictors, default is NULL</p>
</td></tr>
<tr><td><code id="argo_+3A_n_lag">N_lag</code></td>
<td>
<p>vector of the AR model lags used,
if NULL then no AR lags will be used</p>
</td></tr>
<tr><td><code id="argo_+3A_n_training">N_training</code></td>
<td>
<p>number of training points, if <code>use_all_previous</code> is true,
this is the least number of training points required</p>
</td></tr>
<tr><td><code id="argo_+3A_alpha">alpha</code></td>
<td>
<p>penalty between lasso and ridge, alpha=1 represents lasso,
alpha=0 represents ridge, alpha=NA represents no penalty</p>
</td></tr>
<tr><td><code id="argo_+3A_use_all_previous">use_all_previous</code></td>
<td>
<p>boolean variable indicating whether to use &quot;all available data&quot;
(when <code>TRUE</code>) or &quot;a sliding window&quot; (when <code>FALSE</code>) for training</p>
</td></tr>
<tr><td><code id="argo_+3A_mc.cores">mc.cores</code></td>
<td>
<p>number of cores to compute argo in parallel</p>
</td></tr>
<tr><td><code id="argo_+3A_schedule">schedule</code></td>
<td>
<p>list to specify prediction schedule. Default to have <code>y_gap</code> as 1, and <code>forecast</code> as 0, 
i.e., nowcasting with past week ILI available from CDC.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function takes the time series and exogenous variables (optional) as
input, and produces out-of-sample prediction for each time point.
</p>


<h3>Value</h3>

<p>A list of following named objects
</p>

<ul>
<li> <p><code>pred</code> An xts object with the same index as input,
which contains historical nowcast estimation
</p>
</li>
<li> <p><code>coef</code> A matrix contains historical coefficient values of the predictors.
</p>
</li>
<li> <p><code>parm</code> Parameter values passed to argo function.
</p>
</li>
<li> <p><code>penalfac</code> the value of lambda ratio selected by cross-validation,
NULL if <code>lamid</code> is NULL or has only one level.
</p>
</li>
<li> <p><code>penalregion</code> the lambda ratios that has a cross validation error
within one standard error of minimum cross validation error
</p>
</li></ul>



<h3>References</h3>

<p>Yang, S., Santillana, M., &amp; Kou, S. C. (2015). Accurate estimation of influenza epidemics using Google search data via ARGO. Proceedings of the National Academy of Sciences. &lt;doi:10.1073/pnas.1515373112&gt;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>GFT_xts &lt;- xts::xts(exp(matrix(rnorm(180), ncol=1)), order.by = Sys.Date() - (180:1))
randomx &lt;- xts::xts(exp(matrix(rnorm(180*100), ncol=100)), order.by = Sys.Date() - (180:1))

argo_result1 &lt;- argo(GFT_xts)
argo_result2 &lt;- argo(GFT_xts, exogen = randomx)

</code></pre>

<hr>
<h2 id='argo_main'>main function for argo</h2><span id='topic+argo_main'></span>

<h3>Description</h3>

<p>main function that reproduce the results in ARGO paper
</p>


<h3>Usage</h3>

<pre><code class='language-R'>argo_main(save.folder = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="argo_main_+3A_save.folder">save.folder</code></td>
<td>
<p>output folder to save graphics. If NULL then do not output graphics.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
argo_main()


</code></pre>

<hr>
<h2 id='argo2'>ARGO second step</h2><span id='topic+argo2'></span>

<h3>Description</h3>

<p>Wrapper for ARGO second step. Best linear predictor / Bayesian posterior
</p>


<h3>Usage</h3>

<pre><code class='language-R'>argo2(truth, argo1.p, argo.nat.p)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="argo2_+3A_truth">truth</code></td>
<td>
<p>prediction target</p>
</td></tr>
<tr><td><code id="argo2_+3A_argo1.p">argo1.p</code></td>
<td>
<p>argo first step prediction</p>
</td></tr>
<tr><td><code id="argo2_+3A_argo.nat.p">argo.nat.p</code></td>
<td>
<p>argo national level prediction</p>
</td></tr>
</table>


<h3>References</h3>

<p>Shaoyang Ning, Shihao Yang, S. C. Kou. Accurate Regional Influenza Epidemics Tracking Using Internet Search Data. Scientific Reports
</p>


<h3>Examples</h3>

<pre><code class='language-R'>truth &lt;- xts::xts(exp(matrix(rnorm(180*10), ncol=10)), order.by = Sys.Date() - (180:1))
argo1.p &lt;- xts::xts(exp(matrix(rnorm(180*10), ncol=10)), order.by = Sys.Date() - (180:1))
argo.nat.p &lt;- xts::xts(exp(matrix(rnorm(180*10), ncol=10)), order.by = Sys.Date() - (180:1))
argo2result &lt;- argo2(truth, argo1.p, argo.nat.p)
</code></pre>

<hr>
<h2 id='argo2_main'>main function for argo2</h2><span id='topic+argo2_main'></span>

<h3>Description</h3>

<p>main function that reproduce the results in ARGO2 paper
</p>


<h3>Usage</h3>

<pre><code class='language-R'>argo2_main(
  gt.folder,
  ili.folder,
  population.file,
  gft.file,
  save.folder = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="argo2_main_+3A_gt.folder">gt.folder</code></td>
<td>
<p>folder with Google Trends files, which should be thousands of csv file such as &quot;US-MA_fever cough.csv&quot; or &quot;US-NY_cold or flu.csv&quot;</p>
</td></tr>
<tr><td><code id="argo2_main_+3A_ili.folder">ili.folder</code></td>
<td>
<p>folder with ILINet data files: &quot;ILINet_nat.csv&quot; and &quot;ILINet_regional.csv&quot;</p>
</td></tr>
<tr><td><code id="argo2_main_+3A_population.file">population.file</code></td>
<td>
<p>file path to population csv file</p>
</td></tr>
<tr><td><code id="argo2_main_+3A_gft.file">gft.file</code></td>
<td>
<p>file path to Google Flu Trends csv file</p>
</td></tr>
<tr><td><code id="argo2_main_+3A_save.folder">save.folder</code></td>
<td>
<p>output folder to save graphics. If NULL then do not output graphics.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Shaoyang Ning, Shihao Yang, S. C. Kou. Accurate Regional Influenza Epidemics Tracking Using Internet Search Data. Scientific Reports
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
download.file("https://scholar.harvard.edu/files/syang/files/gt2016-10-24.zip",
file.path(tempdir(), "gt2016-10-24.zip"))
unzip(file.path(tempdir(), "gt2016-10-24.zip"), exdir = tempdir())
gt.folder &lt;- file.path(tempdir(), "2016-10-19")
argo2_main(
  gt.folder=gt.folder,
  ili.folder=system.file("regiondata", "ili20161121", package = "argo"),
  population.file=system.file("regiondata", "Population.csv", package = "argo"),
  gft.file=system.file("regiondata", "GFT.txt", package = "argo")
)

## End(Not run)

</code></pre>

<hr>
<h2 id='argox_main'>main function for argox</h2><span id='topic+argox_main'></span>

<h3>Description</h3>

<p>Main function that reproduce the results in ARGOX paper. The datasets are available at Harvard Dataverse &lt;doi:10.7910/DVN/2IVDGK&gt;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>argox_main(
  gt.folder,
  ili.folder,
  population.file,
  gft.file,
  mix,
  save.folder = NULL,
  NCORES = 8
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="argox_main_+3A_gt.folder">gt.folder</code></td>
<td>
<p>folder with Google Trends files, which should be thousands of csv file such as &quot;US-MA_fever cough.csv&quot; or &quot;US-NY_cold or flu.csv&quot;</p>
</td></tr>
<tr><td><code id="argox_main_+3A_ili.folder">ili.folder</code></td>
<td>
<p>folder with ILINet data files: &quot;ILINet_nat.csv&quot; and &quot;ILINet_regional.csv&quot;</p>
</td></tr>
<tr><td><code id="argox_main_+3A_population.file">population.file</code></td>
<td>
<p>file path to population csv file</p>
</td></tr>
<tr><td><code id="argox_main_+3A_gft.file">gft.file</code></td>
<td>
<p>file path to Google Flu Trends csv file</p>
</td></tr>
<tr><td><code id="argox_main_+3A_mix">mix</code></td>
<td>
<p>the weighted avarage mixing of raw state-level Google Trends data. Set to be 0 for stand-alone model. Set to be 1/3 for spatial-pooling model.</p>
</td></tr>
<tr><td><code id="argox_main_+3A_save.folder">save.folder</code></td>
<td>
<p>output folder to save graphics. If NULL then do not output graphics.</p>
</td></tr>
<tr><td><code id="argox_main_+3A_ncores">NCORES</code></td>
<td>
<p>number of parallel cpu cores to be used.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Yang, S., Ning, S. &amp; Kou, S.C. Use Internet search data to accurately track state level influenza epidemics. Sci Rep 11, 4023 (2021)
</p>

<hr>
<h2 id='boot_re'>wrapper for bootstrap relative efficiency confidence interval</h2><span id='topic+boot_re'></span>

<h3>Description</h3>

<p>This function is used to wrap the <code>bootstrap_relative_efficiency</code>,
taking vectorized arguments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>boot_re(
  pred_data,
  period.all,
  model_good,
  bench.all,
  type,
  truth = "CDC.data",
  l = 50,
  N = 10000,
  sim = "geom",
  conf = 0.95
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="boot_re_+3A_pred_data">pred_data</code></td>
<td>
<p>A matrix that contains the truth vector and the predictions.
It can be data.frame or xts object</p>
</td></tr>
<tr><td><code id="boot_re_+3A_period.all">period.all</code></td>
<td>
<p>vector of the periods to evaluate relative efficiency</p>
</td></tr>
<tr><td><code id="boot_re_+3A_model_good">model_good</code></td>
<td>
<p>The model to evaluate, must be in the column names of pred_data</p>
</td></tr>
<tr><td><code id="boot_re_+3A_bench.all">bench.all</code></td>
<td>
<p>vector of the models to compare to, must be in the column names of pred_data</p>
</td></tr>
<tr><td><code id="boot_re_+3A_type">type</code></td>
<td>
<p>Must be one of &quot;mse&quot; (mean square error),
&quot;mape&quot; (mean absolute percentage error), or
&quot;mae&quot; (mean absolute error)</p>
</td></tr>
<tr><td><code id="boot_re_+3A_truth">truth</code></td>
<td>
<p>the column name of the truth</p>
</td></tr>
<tr><td><code id="boot_re_+3A_l">l</code></td>
<td>
<p>stationary bootstrap mean block length</p>
</td></tr>
<tr><td><code id="boot_re_+3A_n">N</code></td>
<td>
<p>number of bootstrap samples</p>
</td></tr>
<tr><td><code id="boot_re_+3A_sim">sim</code></td>
<td>
<p>simulation method, pass to boot::tsboot</p>
</td></tr>
<tr><td><code id="boot_re_+3A_conf">conf</code></td>
<td>
<p>confidence level</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of point estimate and corresponding bootstrap confidence interval
</p>


<h3>Examples</h3>

<pre><code class='language-R'>GFT_xts = xts::xts(exp(matrix(rnorm(500), ncol=5)), order.by = Sys.Date() - (100:1))
names(GFT_xts) &lt;- paste0("col", 1:ncol(GFT_xts))
names(GFT_xts)[1] &lt;- "CDC.data"

boot_re(
  pred_data = GFT_xts,
  period.all = c(paste0(zoo::index(GFT_xts)[1], "/", zoo::index(GFT_xts)[50]),
                 paste0(zoo::index(GFT_xts)[51], "/", zoo::index(GFT_xts)[100])),
  model_good = "col2",
  bench.all = c("col3", "col4"),
  type = "mse",
  truth="CDC.data",
  l = 5,
  N = 20
)

</code></pre>

<hr>
<h2 id='bootstrap_relative_efficiency'>bootstrap relative efficiency confidence interval</h2><span id='topic+bootstrap_relative_efficiency'></span>

<h3>Description</h3>

<p>This function is used to reproduce the ARGO bootstrap confidence interval
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootstrap_relative_efficiency(
  pred_data,
  model_good,
  model_bench,
  l = 50,
  N = 10000,
  truth = "CDC.data",
  sim = "geom",
  conf = 0.95,
  type = c("mse", "mape", "mae", "mspe", "rmse", "rmspe")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bootstrap_relative_efficiency_+3A_pred_data">pred_data</code></td>
<td>
<p>A matrix that contains the truth vector and the predictions.
It can be data.frame or xts object</p>
</td></tr>
<tr><td><code id="bootstrap_relative_efficiency_+3A_model_good">model_good</code></td>
<td>
<p>The model to evaluate, must be in the column names of pred_data</p>
</td></tr>
<tr><td><code id="bootstrap_relative_efficiency_+3A_model_bench">model_bench</code></td>
<td>
<p>The model to compare to, must be in the column names of pred_data</p>
</td></tr>
<tr><td><code id="bootstrap_relative_efficiency_+3A_l">l</code></td>
<td>
<p>stationary bootstrap mean block length</p>
</td></tr>
<tr><td><code id="bootstrap_relative_efficiency_+3A_n">N</code></td>
<td>
<p>number of bootstrap samples</p>
</td></tr>
<tr><td><code id="bootstrap_relative_efficiency_+3A_truth">truth</code></td>
<td>
<p>the column name of the truth</p>
</td></tr>
<tr><td><code id="bootstrap_relative_efficiency_+3A_sim">sim</code></td>
<td>
<p>simulation method, pass to boot::tsboot</p>
</td></tr>
<tr><td><code id="bootstrap_relative_efficiency_+3A_conf">conf</code></td>
<td>
<p>confidence level</p>
</td></tr>
<tr><td><code id="bootstrap_relative_efficiency_+3A_type">type</code></td>
<td>
<p>Must be one of &quot;mse&quot; (mean square error),
&quot;mape&quot; (mean absolute percentage error), or
&quot;mae&quot; (mean absolute error)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of point estimate and corresponding bootstrap confidence interval
</p>


<h3>Examples</h3>

<pre><code class='language-R'>GFT_xts = xts::xts(exp(matrix(rnorm(1000), ncol=5)), order.by = Sys.Date() - (200:1))
names(GFT_xts) &lt;- paste0("col", 1:ncol(GFT_xts))
names(GFT_xts)[1] &lt;- "CDC.data"
bootstrap_relative_efficiency(
  pred_data = GFT_xts,
  model_good = "col2",
  model_bench = "col3",
  truth="CDC.data",
  N = 100
)
</code></pre>

<hr>
<h2 id='gt.parser.pub.api'>Parsing each Google Trends file downloaded from Google Trends API</h2><span id='topic+gt.parser.pub.api'></span>

<h3>Description</h3>

<p>Parsing each Google Trends file downloaded from Google Trends API
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gt.parser.pub.api(gt.folder, f)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gt.parser.pub.api_+3A_gt.folder">gt.folder</code></td>
<td>
<p>folder that contains Google Trends file</p>
</td></tr>
<tr><td><code id="gt.parser.pub.api_+3A_f">f</code></td>
<td>
<p>filename for Google Trends file</p>
</td></tr>
</table>

<hr>
<h2 id='gt.parser.pub.web'>Parsing each Google Trends file downloaded from website</h2><span id='topic+gt.parser.pub.web'></span>

<h3>Description</h3>

<p>Parsing each Google Trends file downloaded from website
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gt.parser.pub.web(gt.folder, f)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gt.parser.pub.web_+3A_gt.folder">gt.folder</code></td>
<td>
<p>folder that contains Google Trends file</p>
</td></tr>
<tr><td><code id="gt.parser.pub.web_+3A_f">f</code></td>
<td>
<p>filename for Google Trends file</p>
</td></tr>
</table>

<hr>
<h2 id='heatmap_argo'>Heatmap plot of ARGO coefficients applied on CDC's ILI data</h2><span id='topic+heatmap_argo'></span>

<h3>Description</h3>

<p>Heatmap plot of ARGO coefficients applied on CDC's ILI data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>heatmap_argo(argo_coef, lim = 0.1, na.grey = TRUE, scale = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="heatmap_argo_+3A_argo_coef">argo_coef</code></td>
<td>
<p>The coefficient matrix</p>
</td></tr>
<tr><td><code id="heatmap_argo_+3A_lim">lim</code></td>
<td>
<p>the limit to truncate for large coefficients for better presentation</p>
</td></tr>
<tr><td><code id="heatmap_argo_+3A_na.grey">na.grey</code></td>
<td>
<p>whether to plot grey for NA values</p>
</td></tr>
<tr><td><code id="heatmap_argo_+3A_scale">scale</code></td>
<td>
<p>margin scale</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a graph on the default plot window
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cor_coef &lt;- matrix(runif(100, -1, 1), ncol=10)
colnames(cor_coef) &lt;- as.character(Sys.Date() - 10:1)
rownames(cor_coef) &lt;- paste0("row", 1:10)
pdf(file.path(tempdir(), "heatmap_argo.pdf"), height=11,width=12)
heatmap_argo(cor_coef)
dev.off()

</code></pre>

<hr>
<h2 id='heatmap_cor'>Heatmap plot of correlation matrix</h2><span id='topic+heatmap_cor'></span>

<h3>Description</h3>

<p>Heatmap plot of correlation matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>heatmap_cor(cor_heat, lim = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="heatmap_cor_+3A_cor_heat">cor_heat</code></td>
<td>
<p>The coefficient matrix to draw heatmap</p>
</td></tr>
<tr><td><code id="heatmap_cor_+3A_lim">lim</code></td>
<td>
<p>the limit to truncate for large coefficients for better presentation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a graph on the default plot window
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cor_coef &lt;- matrix(runif(100, -1, 1), ncol=10)
colnames(cor_coef) &lt;- paste0("col", 1:10)
rownames(cor_coef) &lt;- paste0("row", 1:10)
heatmap_cor(cor_coef)

</code></pre>

<hr>
<h2 id='load_data'>Parsing of raw data</h2><span id='topic+load_data'></span>

<h3>Description</h3>

<p>Data related to the PNAS paper. Accessed on Nov 14, 2015.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_data(type = "extdata", ili.weighted = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="load_data_+3A_type">type</code></td>
<td>
<p>the type of the data to be loaded. If <code>type=="extdata"</code> it loads the data
to reproduce the PNAS paper, and if <code>type=="athdata"</code> it loads the data to reproduce the
CID(?) paper.</p>
</td></tr>
<tr><td><code id="load_data_+3A_ili.weighted">ili.weighted</code></td>
<td>
<p>logical indicator to specify whether to load weighted ILI or not,
if <code>FALSE</code> unweighted ILI is loaded.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Parse and load CDC's ILI data, Google Flu Trend data,
Google Correlate data trained with ILI as of 2010, Google Correlate data trained with ILI as of 2009,
Google Trend data with search terms identified from Google Correlate (2010 version).
</p>
<p>Each week ends on the Saturday indicated in the xts object
</p>
<p>Google Correlate data is standardized by Google, and we rescale it to 0 &ndash; 100 during parsing.
Google Trends data is in the scale of 0 &ndash; 100.
</p>


<h3>Value</h3>

<p>A list of following named xts objects if <code>type=="extdata"</code>
</p>

<ul>
<li> <p><code>GC10</code> Google Correlate trained with ILI available as of 2010.
Google Correlate has been deprecated by Google as of Dec 2019 and is no longer publicly available.
</p>
</li>
<li> <p><code>GC09</code> Google Correlate trained with ILI available as of 2009.
</p>
</li>
<li> <p><code>GT</code> Google Trends data for search queries identified using Google Correlate.
Not directly available online, you have to manually input query terms
at <a href="https://trends.google.com/trends/">https://trends.google.com/trends/</a>
</p>
</li>
<li> <p><code>CDC</code> CDC's ILI dataset.
Available online at <a href="https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html">https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html</a>
</p>
</li>
<li> <p><code>GFT</code> Google Flu Trend (historical predictions).
</p>
</li></ul>

<p>A list of following named xts objects if <code>type=="athdata"</code>
</p>

<ul>
<li> <p><code>GT</code> Google Trends data for search queries identified.
Not directly available online, you have to manually input query terms
at <a href="https://trends.google.com/trends/">https://trends.google.com/trends/</a>
</p>
</li>
<li> <p><code>CDC</code> CDC's ILI dataset.
Available online at <a href="https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html">https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html</a>
</p>
</li>
<li> <p><code>ili_idx</code> the indexing information that includes the week number and year
number, the date of ending Saturday, and the season number
Available online at <a href="https://www.cdc.gov/flu/weekly/">https://www.cdc.gov/flu/weekly/</a>
</p>
</li>
<li> <p><code>ATH</code> Athenahealth data that includes the proportion of &quot;Flu Visit&quot;,
&quot;ILI Visit&quot;, and &quot;Unspecified Viral or ILI Visit&quot; compared to total number of visit to the
Athenahealth partner healthcare providers.
</p>
</li>
<li> <p><code>ili_unrevised</code> Historical unrevised ILI activity level.
The unrevised ILI published on week ZZ of season XXXX-YYYY is available at
<code>www.cdc.gov/flu/weekly/weeklyarchivesXXXX-YYYY/data/senAllregtZZ.html</code>
or <code>.htm</code>. For example, original ILI report for week 7 of season 2015-2016 is available at
<a href="https://www.cdc.gov/flu/weekly/weeklyarchives2015-2016/data/senAllregt07.html">https://www.cdc.gov/flu/weekly/weeklyarchives2015-2016/data/senAllregt07.html</a>,
and original ILI report for week 50 of season 2012-2013 is available at
<a href="https://www.cdc.gov/flu/weekly/weeklyarchives2012-2013/data/senAllregt50.htm">https://www.cdc.gov/flu/weekly/weeklyarchives2012-2013/data/senAllregt50.htm</a>
</p>
</li></ul>



<h3>References</h3>

<p>Yang, S., Santillana, M., &amp; Kou, S. C. (2015). Accurate estimation of influenza epidemics using Google search data via ARGO. Proceedings of the National Academy of Sciences. &lt;doi:10.1073/pnas.1515373112&gt;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>system.file("extdata", "correlate-Influenza_like_Illness_h1n1_CDC_.csv", package = "argo")
system.file("extdata", "correlate-Influenza_like_Illness_CDC_.csv", package = "argo")
system.file("extdata", "GFT.csv", package = "argo")
system.file("extdata", "ILINet.csv", package = "argo")
load_data()

</code></pre>

<hr>
<h2 id='load_reg_data'>Parsing of raw data for regional ILI estimation</h2><span id='topic+load_reg_data'></span>

<h3>Description</h3>

<p>Parsing of raw data for regional ILI estimation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_reg_data(
  gt.folder,
  ili.folder,
  population.file,
  gft.file,
  gt.parser = gt.parser.pub.web
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="load_reg_data_+3A_gt.folder">gt.folder</code></td>
<td>
<p>folder with all Google Trends data</p>
</td></tr>
<tr><td><code id="load_reg_data_+3A_ili.folder">ili.folder</code></td>
<td>
<p>folder with all ILI data</p>
</td></tr>
<tr><td><code id="load_reg_data_+3A_population.file">population.file</code></td>
<td>
<p>csv file path with state population data</p>
</td></tr>
<tr><td><code id="load_reg_data_+3A_gft.file">gft.file</code></td>
<td>
<p>csv file path for Google Flu Trends</p>
</td></tr>
<tr><td><code id="load_reg_data_+3A_gt.parser">gt.parser</code></td>
<td>
<p>Google Trends data parser function, could be 'gt.parser.pub.web' or 'gt.parser.pub.api'</p>
</td></tr>
</table>


<h3>References</h3>

<p>Shaoyang Ning, Shihao Yang, S. C. Kou. Accurate Regional Influenza Epidemics Tracking Using Internet Search Data. Scientific Reports
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
download.file("https://scholar.harvard.edu/files/syang/files/gt2016-10-24.zip",
file.path(tempdir(), "gt2016-10-24.zip"))
unzip(file.path(tempdir(), "gt2016-10-24.zip"), exdir = tempdir())
gt.folder &lt;- file.path(tempdir(), "2016-10-19")

data_parsed &lt;- load_reg_data(
  gt.folder=gt.folder,
  ili.folder=system.file("regiondata", "ili20161121", package = "argo"),
  population.file=system.file("regiondata", "Population.csv", package = "argo"),
  gft.file=system.file("regiondata", "GFT.txt", package = "argo")
)



</code></pre>

<hr>
<h2 id='logit'>logit function</h2><span id='topic+logit'></span>

<h3>Description</h3>

<p>logit function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logit(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="logit_+3A_x">x</code></td>
<td>
<p>numeric value for logit transformation</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>logit(0.5)
</code></pre>

<hr>
<h2 id='logit_inv'>inverse logit function</h2><span id='topic+logit_inv'></span>

<h3>Description</h3>

<p>inverse logit function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logit_inv(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="logit_inv_+3A_x">x</code></td>
<td>
<p>numeric value for inverse logit transformation</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>logit_inv(0)
</code></pre>

<hr>
<h2 id='parse_gt_weekly'>Parsing of Google Trends data</h2><span id='topic+parse_gt_weekly'></span>

<h3>Description</h3>

<p>Parsing of Google Trends data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parse_gt_weekly(folder)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="parse_gt_weekly_+3A_folder">folder</code></td>
<td>
<p>folder with weekly Google Trends file</p>
</td></tr>
</table>


<h3>References</h3>

<p>Yang, S., Santillana, M., &amp; Kou, S. C. (2015). Accurate estimation of influenza epidemics using Google search data via ARGO. Proceedings of the National Academy of Sciences. &lt;doi:10.1073/pnas.1515373112&gt;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

download.file("https://scholar.harvard.edu/files/syang/files/gt2016-10-24.zip",
file.path(tempdir(), "gt2016-10-24.zip"))
unzip(file.path(tempdir(), "gt2016-10-24.zip"), exdir = tempdir())
gt.folder &lt;- file.path(tempdir(), "2016-10-19")
parsed_data &lt;- parse_gt_weekly(gt.folder)


</code></pre>

<hr>
<h2 id='parse_unrevised_ili'>Parsing of unrevised ili from online source</h2><span id='topic+parse_unrevised_ili'></span>

<h3>Description</h3>

<p>Parsing of unrevised ili from online source
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parse_unrevised_ili(type = "extdata", ili.weighted = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="parse_unrevised_ili_+3A_type">type</code></td>
<td>
<p>the type of data folder to parse</p>
</td></tr>
<tr><td><code id="parse_unrevised_ili_+3A_ili.weighted">ili.weighted</code></td>
<td>
<p>indicator to use weighted ILI or not</p>
</td></tr>
</table>


<h3>References</h3>

<p>Yang, S., Santillana, M., &amp; Kou, S. C. (2015). Accurate estimation of influenza epidemics using Google search data via ARGO. Proceedings of the National Academy of Sciences. &lt;doi:10.1073/pnas.1515373112&gt;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

parse_unrevised_ili()


</code></pre>

<hr>
<h2 id='plot_argo'>Time series plot of ARGO applied on CDC's ILI data</h2><span id='topic+plot_argo'></span>

<h3>Description</h3>

<p>This function is used to reproduce the ARGO plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_argo(GFT_xts, GC_GT_cut_date, model_names, legend_names, zoom_periods)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_argo_+3A_gft_xts">GFT_xts</code></td>
<td>
<p>dataframe with all predicted values</p>
</td></tr>
<tr><td><code id="plot_argo_+3A_gc_gt_cut_date">GC_GT_cut_date</code></td>
<td>
<p>cutting date for switching datasets</p>
</td></tr>
<tr><td><code id="plot_argo_+3A_model_names">model_names</code></td>
<td>
<p>name of predicting models</p>
</td></tr>
<tr><td><code id="plot_argo_+3A_legend_names">legend_names</code></td>
<td>
<p>legend for predicting models</p>
</td></tr>
<tr><td><code id="plot_argo_+3A_zoom_periods">zoom_periods</code></td>
<td>
<p>vector of periods to zoom into</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a graph on the default plot window
</p>


<h3>Examples</h3>

<pre><code class='language-R'>GFT_xts = xts::xts(exp(matrix(rnorm(1000), ncol=5)), order.by = Sys.Date() - (200:1))
names(GFT_xts) &lt;- paste0("col", 1:ncol(GFT_xts))
names(GFT_xts)[1] &lt;- "CDC.data"
zoom_periods = c()
for (i in 0:5){
  zoom_periods = c(
    zoom_periods,
    paste0(zoo::index(GFT_xts)[i*30+1], "/", zoo::index(GFT_xts)[i*30+30])
  )
}
plot_argo(
  GFT_xts = GFT_xts,
  GC_GT_cut_date = zoo::index(GFT_xts)[50],
  model_names = colnames(GFT_xts)[-1],
  legend_names = paste0(colnames(GFT_xts)[-1], "legend"),
  zoom_periods = zoom_periods
)

</code></pre>

<hr>
<h2 id='summary_argo'>performance summary of ARGO applied on CDC's ILI data</h2><span id='topic+summary_argo'></span>

<h3>Description</h3>

<p>performance summary of ARGO applied on CDC's ILI data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summary_argo(
  GFT_xts,
  model_names,
  legend_names,
  periods,
  whole_period = "2009-03/2015-10"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary_argo_+3A_gft_xts">GFT_xts</code></td>
<td>
<p>dataframe with all predicted values</p>
</td></tr>
<tr><td><code id="summary_argo_+3A_model_names">model_names</code></td>
<td>
<p>name of predicting models</p>
</td></tr>
<tr><td><code id="summary_argo_+3A_legend_names">legend_names</code></td>
<td>
<p>legend for predicting models</p>
</td></tr>
<tr><td><code id="summary_argo_+3A_periods">periods</code></td>
<td>
<p>vector of periods to zoom into</p>
</td></tr>
<tr><td><code id="summary_argo_+3A_whole_period">whole_period</code></td>
<td>
<p>the whole period duration</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of summary tables for the input periods, including RMSE, MAE, MAPE, corr
</p>


<h3>References</h3>

<p>Yang, S., Santillana, M., &amp; Kou, S. C. (2015). Accurate estimation of influenza epidemics using Google search data via ARGO. Proceedings of the National Academy of Sciences. &lt;doi:10.1073/pnas.1515373112&gt;.
Shaoyang Ning, Shihao Yang, S. C. Kou. Accurate Regional Influenza Epidemics Tracking Using Internet Search Data. Scientific Reports
</p>


<h3>Examples</h3>

<pre><code class='language-R'>GFT_xts = xts::xts(exp(matrix(rnorm(1000), ncol=10)), order.by = Sys.Date() - (100:1))
names(GFT_xts) &lt;- paste0("col", 1:10)
names(GFT_xts)[1] &lt;- "CDC.data"
summary_argo(
  GFT_xts = GFT_xts,
  model_names = colnames(GFT_xts)[-1],
  legend_names = paste0(colnames(GFT_xts)[-1], "legend"),
  periods = c(paste0(zoo::index(GFT_xts)[1], "/", zoo::index(GFT_xts)[49]),
              paste0(zoo::index(GFT_xts)[50], "/", zoo::index(GFT_xts)[100])),
  whole_period="2009-03/"
)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
