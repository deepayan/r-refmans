<!DOCTYPE html><html><head><title>Help for package brnn</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {brnn}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#brnn'><p>brnn</p></a></li>
<li><a href='#brnn_extended'><p>brnn_extended</p></a></li>
<li><a href='#brnn_ordinal'><p>brnn_ordinal</p></a></li>
<li><a href='#D'><p>Genomic dominant relationship matrix for the Jersey dataset.</p></a></li>
<li><a href='#estimate.trace'><p>estimate.trace</p></a></li>
<li><a href='#G'><p>Genomic additive relationship matrix for the Jersey dataset.</p></a></li>
<li><a href='#GOrd'><p>Genomic additive relationship matrix for the GLS dataset.</p></a></li>
<li><a href='#initnw'><p>Initialize networks weights and biases</p></a></li>
<li><a href='#jacobian'><p>Jacobian</p></a></li>
<li><a href='#normalize'><p>normalize</p></a></li>
<li><a href='#partitions'><p>Partitions for cross validation (CV)</p></a></li>
<li><a href='#pheno'><p>Phenotypic information for Jersey</p></a></li>
<li><a href='#phenoOrd'><p>Phenotypic information for GLS (ordinal trait)</p></a></li>
<li><a href='#predict.brnn'><p>predict.brnn</p></a></li>
<li><a href='#predict.brnn_extended'><p>predict.brnn_extended</p></a></li>
<li><a href='#predict.brnn_ordinal'><p>predict.brnn_ordinal</p></a></li>
<li><a href='#twoinput'><p>2 Inputs and 1 output.</p></a></li>
<li><a href='#un_normalize'><p>un_normalize</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>0.9.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-11-05</td>
</tr>
<tr>
<td>Title:</td>
<td>Bayesian Regularization for Feed-Forward Neural Networks</td>
</tr>
<tr>
<td>Author:</td>
<td>Paulino Perez Rodriguez, Daniel Gianola</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Paulino Perez Rodriguez &lt;perpdgo@colpos.mx&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0), Formula, truncnorm</td>
</tr>
<tr>
<td>Description:</td>
<td>Bayesian regularization for feed-forward neural networks.</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>true</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-11-10 03:26:57 UTC; paulino</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-11-10 05:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='brnn'>brnn</h2><span id='topic+brnn'></span><span id='topic+brnn.formula'></span><span id='topic+brnn.default'></span><span id='topic+coef.brnn'></span><span id='topic+print.brnn'></span><span id='topic+summary.brnn'></span>

<h3>Description</h3>

<p>The brnn function
fits a two layer neural network as described in MacKay (1992) and Foresee and Hagan (1997). It uses the 
Nguyen and Widrow algorithm (1990) to assign initial weights and the Gauss-Newton algorithm to 
perform the optimization. This function implements the functionality of the function trainbr in Matlab 2010b.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  brnn(x, ...)
  
  ## S3 method for class 'formula'
brnn(formula, data, contrasts=NULL,...)

  ## Default S3 method:
brnn(x,y,neurons=2,normalize=TRUE,epochs=1000,mu=0.005,mu_dec=0.1, 
       mu_inc=10,mu_max=1e10,min_grad=1e-10,change = 0.001,cores=1,
       verbose=FALSE,Monte_Carlo = FALSE,tol = 1e-06, samples = 40,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="brnn_+3A_formula">formula</code></td>
<td>
<p>A formula of the form <code>y ~ x1 + x2 + ...</code></p>
</td></tr>
<tr><td><code id="brnn_+3A_data">data</code></td>
<td>
<p>Data frame from which variables specified in  <code>formula</code> are preferentially to be taken.</p>
</td></tr>
<tr><td><code id="brnn_+3A_x">x</code></td>
<td>
<p>(numeric, <code class="reqn">n \times p</code>) incidence matrix.</p>
</td></tr>
<tr><td><code id="brnn_+3A_y">y</code></td>
<td>
<p>(numeric, <code class="reqn">n</code>) the response data-vector (NAs not  allowed).</p>
</td></tr>
<tr><td><code id="brnn_+3A_neurons">neurons</code></td>
<td>
<p>positive integer that indicates the number of neurons.</p>
</td></tr>
<tr><td><code id="brnn_+3A_normalize">normalize</code></td>
<td>
<p>logical, if TRUE will normalize inputs and output, the default value is TRUE.</p>
</td></tr>
<tr><td><code id="brnn_+3A_epochs">epochs</code></td>
<td>
<p>positive integer, maximum number of epochs(iterations) to train, default 1000.</p>
</td></tr>
<tr><td><code id="brnn_+3A_mu">mu</code></td>
<td>
<p>positive number that controls the behaviour of the Gauss-Newton optimization algorithm, default value 0.005.</p>
</td></tr>
<tr><td><code id="brnn_+3A_mu_dec">mu_dec</code></td>
<td>
<p>positive number, is the mu decrease ratio, default value 0.1.</p>
</td></tr>
<tr><td><code id="brnn_+3A_mu_inc">mu_inc</code></td>
<td>
<p>positive number, is the mu increase ratio, default value 10.</p>
</td></tr>
<tr><td><code id="brnn_+3A_mu_max">mu_max</code></td>
<td>
<p>maximum mu before training is stopped, strict positive number, default value <code class="reqn">1\times 10^{10}</code>.</p>
</td></tr>
<tr><td><code id="brnn_+3A_min_grad">min_grad</code></td>
<td>
<p>minimum gradient.</p>
</td></tr>
<tr><td><code id="brnn_+3A_change">change</code></td>
<td>
<p>The program  will stop if the maximum (in absolute value) of the differences of the F function in 3 consecutive iterations is less than this quantity.</p>
</td></tr>
<tr><td><code id="brnn_+3A_cores">cores</code></td>
<td>
<p>Number of cpu cores to use for calculations (only available in UNIX-like operating systems). The function detectCores in the R package 
parallel can be used to attempt to detect the number of CPUs in the machine that R is running, but not necessarily 
all the cores are available for the current user, because for example in multi-user 
systems it will depend on system policies. Further details can be found in the documentation for the parallel package.</p>
</td></tr>
<tr><td><code id="brnn_+3A_verbose">verbose</code></td>
<td>
<p>logical, if TRUE will print iteration history.</p>
</td></tr>
<tr><td><code id="brnn_+3A_monte_carlo">Monte_Carlo</code></td>
<td>
<p>If TRUE it will estimate the trace of the inverse of the hessian using Monte Carlo procedures, see Bai et al. (1996) for 
more details. This routine calls the function estimate.trace() to perform the computations.</p>
</td></tr>
<tr><td><code id="brnn_+3A_tol">tol</code></td>
<td>
<p>numeric tolerance, a tiny number useful for checking convergenge in the Bai's algorithm. </p>
</td></tr>
<tr><td><code id="brnn_+3A_samples">samples</code></td>
<td>
<p>positive integer, number of Monte Carlo replicates to estimate the trace of the inverse, see Bai et al. (1996) for more details.</p>
</td></tr>
<tr><td><code id="brnn_+3A_contrasts">contrasts</code></td>
<td>
<p>an optional list of contrasts to be used for some or all of the factors appearing as variables in the model formula.</p>
</td></tr>
<tr><td><code id="brnn_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The software fits a two layer network as described in MacKay (1992) and Foresee and Hagan (1997). 
The model is given by:
</p>
<p><code class="reqn">y_i=g(\boldsymbol{x}_i)+e_i = \sum_{k=1}^s w_k g_k (b_k + \sum_{j=1}^p x_{ij} \beta_j^{[k]}) + e_i, i=1,...,n</code>
</p>
<p>where:
</p>

<ul>
<li><p><code class="reqn">e_i \sim N(0,\sigma_e^2)</code>.
</p>
</li>
<li><p><code class="reqn">s</code> is the number of neurons.
</p>
</li>
<li><p><code class="reqn">w_k</code> is the weight of the <code class="reqn">k</code>-th neuron, <code class="reqn">k=1,...,s</code>.
</p>
</li>
<li><p><code class="reqn">b_k</code> is a bias for the <code class="reqn">k</code>-th neuron, <code class="reqn">k=1,...,s</code>.
</p>
</li>
<li><p><code class="reqn">\beta_j^{[k]}</code> is the weight of the <code class="reqn">j</code>-th input to the net, <code class="reqn">j=1,...,p</code>.
</p>
</li>
<li><p><code class="reqn">g_k(\cdot)</code> is the activation function, in this implementation <code class="reqn">g_k(x)=\frac{\exp(2x)-1}{\exp(2x)+1}</code>.
</p>
</li></ul>
  
<p>The software will minimize 
</p>
<p style="text-align: center;"><code class="reqn">F=\beta E_D + \alpha E_W</code>
</p>

<p>where 
</p>

<ul>
<li><p><code class="reqn">E_D=\sum_{i=1}^n (y_i-\hat y_i)^2</code>, i.e. the error sum of squares.
</p>
</li>
<li><p><code class="reqn">E_W</code> is the sum of squares of network parameters (weights and biases).
</p>
</li>
<li><p><code class="reqn">\beta=\frac{1}{2\sigma^2_e}</code>.
</p>
</li>
<li><p><code class="reqn">\alpha=\frac{1}{2\sigma_\theta^2}</code>, <code class="reqn">\sigma_\theta^2</code> is a dispersion parameter for weights and biases.
</p>
</li></ul>



<h3>Value</h3>

<p>object of class <code>"brnn"</code> or <code>"brnn.formula"</code>. Mostly internal structure, but it is a list containing:
</p>
<table>
<tr><td><code>$theta</code></td>
<td>
<p>A list containing weights and biases. The first <code class="reqn">s</code> components of the list contains vectors with the estimated parameters for
the <code class="reqn">k</code>-th neuron, i.e. <code class="reqn">(w_k, b_k, \beta_1^{[k]},...,\beta_p^{[k]})'</code>.</p>
</td></tr>
<tr><td><code>$message</code></td>
<td>
<p>String that indicates the stopping criteria for the training process.</p>
</td></tr>
<tr><td><code>$alpha</code></td>
<td>
<p><code class="reqn">\alpha</code> parameter.</p>
</td></tr>
<tr><td><code>$beta</code></td>
<td>
<p><code class="reqn">\beta</code> parameter.</p>
</td></tr>
<tr><td><code>$gamma</code></td>
<td>
<p>effective number of parameters.</p>
</td></tr>
<tr><td><code>$Ew</code></td>
<td>
<p>The sum of the squares of the bias and weights.</p>
</td></tr>
<tr><td><code>$Ed</code></td>
<td>
<p>The sum of the squares between observed and predicted values.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Bai, Z. J., M. Fahey and G. Golub. 1996. &quot;Some large-scale matrix computation problems.&quot; 
<em>Journal of Computational and Applied Mathematics</em>  <b>74(1-2)</b>, 71-89.
</p>
<p>Foresee, F. D., and M. T. Hagan. 1997. &quot;Gauss-Newton approximation to Bayesian regularization&quot;, 
<em>Proceedings of the 1997 International Joint Conference on Neural Networks</em>.
</p>
<p>Gianola, D. Okut, H., Weigel, K. and Rosa, G. 2011. &quot;Predicting complex quantitative traits with Bayesian neural networks: a case study with Jersey cows and wheat&quot;. <em>BMC Genetics</em>, 
<b>12</b>,87.
</p>
<p>MacKay, D. J. C. 1992. &quot;Bayesian interpolation&quot;, <em>Neural Computation</em>, 
<b>4(3)</b>, 415-447.
</p>
<p>Nguyen, D. and Widrow, B. 1990. &quot;Improving the learning speed of 2-layer neural networks by choosing initial values of the adaptive weights&quot;, <em>Proceedings of the IJCNN</em>, <b>3</b>, 21-26.
</p>
<p>Paciorek, C. J. and Schervish, M. J. 2004. &quot;Nonstationary Covariance Functions for
Gaussian Process Regression&quot;. In Thrun, S., Saul, L., and Scholkopf, B., editors, <em>Advances
in Neural Information Processing Systems 16</em>. MIT Press, Cambridge, MA.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.brnn">predict.brnn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

#Load the library
library(brnn)

###############################################################
#Example 1 
#Noise triangle wave function, similar to example 1 in Foresee and Hagan (1997)

#Generating the data
x1=seq(0,0.23,length.out=25)
y1=4*x1+rnorm(25,sd=0.1)
x2=seq(0.25,0.75,length.out=50)
y2=2-4*x2+rnorm(50,sd=0.1)
x3=seq(0.77,1,length.out=25)
y3=4*x3-4+rnorm(25,sd=0.1)
x=c(x1,x2,x3)
y=c(y1,y2,y3)


#With the formula interface
out=brnn(y~x,neurons=2)

#With the default S3 method the call is
#out=brnn(y=y,x=as.matrix(x),neurons=2)

plot(x,y,xlim=c(0,1),ylim=c(-1.5,1.5),
     main="Bayesian Regularization for ANN 1-2-1")
lines(x,predict(out),col="blue",lty=2)
legend("topright",legend="Fitted model",col="blue",lty=2,bty="n")

###############################################################
#Example 2
#sin wave function, example in the Matlab 2010b demo.

x = seq(-1,0.5,length.out=100)
y = sin(2*pi*x)+rnorm(length(x),sd=0.1)

#With the formula interface
out=brnn(y~x,neurons=3)

#With the default method the call is
#out=brnn(y=y,x=as.matrix(x),neurons=3)

plot(x,y)
lines(x,predict(out),col="blue",lty=2)

legend("bottomright",legend="Fitted model",col="blue",lty=2,bty="n")

###############################################################
#Example 3
#2 Inputs and 1 output
#the data used in Paciorek and
#Schervish (2004). The data is from a two input one output function with Gaussian noise
#with mean zero and standard deviation 0.25

data(twoinput)

#Formula interface
out=brnn(y~x1+x2,data=twoinput,neurons=10)

#With the default S3 method
#out=brnn(y=as.vector(twoinput$y),x=as.matrix(cbind(twoinput$x1,twoinput$x2)),neurons=10)

f=function(x1,x2) predict(out,cbind(x1,x2))
x1=seq(min(twoinput$x1),max(twoinput$x1),length.out=50)
x2=seq(min(twoinput$x2),max(twoinput$x2),length.out=50)
z=outer(x1,x2,f) # calculating the density values

transformation_matrix=persp(x1, x2, z,
                            main="Fitted model",
                            sub=expression(y==italic(g)~(bold(x))+e),
                            col="lightgreen",theta=30, phi=20,r=50, 
                            d=0.1,expand=0.5,ltheta=90, lphi=180,
                            shade=0.75, ticktype="detailed",nticks=5)
points(trans3d(twoinput$x1,twoinput$x2, f(twoinput$x1,twoinput$x2), 
               transformation_matrix), col = "red")

###############################################################
#Example 4
#Gianola et al. (2011).
#Warning, it will take a while

#Load the Jersey dataset
data(Jersey)

#Fit the model with the FULL DATA
#Formula interface
out=brnn(pheno$yield_devMilk~G,neurons=2,verbose=TRUE)

#Obtain predictions and plot them against fitted values
plot(pheno$yield_devMilk,predict(out))

#Predictive power of the model using the SECOND set for 10 fold CROSS-VALIDATION
data=pheno
data$X=G
data$partitions=partitions

#Fit the model for the TESTING DATA
out=brnn(yield_devMilk~X,
         data=subset(data,partitions!=2),neurons=2,verbose=TRUE)

#Plot the results
#Predicted vs observed values for the training set
par(mfrow=c(2,1))
plot(out$y,predict(out),xlab=expression(hat(y)),ylab="y")
cor(out$y,predict(out))

#Predicted vs observed values for the testing set
yhat_R_testing=predict(out,newdata=subset(data,partitions==2))
ytesting=pheno$yield_devMilk[partitions==2]
plot(ytesting,yhat_R_testing,xlab=expression(hat(y)),ylab="y")
cor(ytesting,yhat_R_testing)


## End(Not run)

</code></pre>

<hr>
<h2 id='brnn_extended'>brnn_extended</h2><span id='topic+brnn_extended'></span><span id='topic+brnn_extended.formula'></span><span id='topic+brnn_extended.default'></span><span id='topic+coef.brnn_extended'></span><span id='topic+print.brnn_extended'></span><span id='topic+summary.brnn_extended'></span>

<h3>Description</h3>

<p>The brnn_extended function
fits a two layer neural network as described in MacKay (1992) and Foresee and Hagan (1997). It uses the 
Nguyen and Widrow algorithm (1990) to assign initial weights and the Gauss-Newton algorithm to 
perform the optimization. The hidden layer contains two groups of neurons 
that allow us to assign different prior distributions for two groups of input variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  brnn_extended(x, ...)

  ## S3 method for class 'formula'
brnn_extended(formula, data, contrastsx=NULL,contrastsz=NULL,...)

  ## Default S3 method:
brnn_extended(x,y,z,neurons1,neurons2,normalize=TRUE,epochs=1000,
              mu=0.005,mu_dec=0.1, mu_inc=10,mu_max=1e10,min_grad=1e-10,
              change = 0.001, cores=1,verbose =FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="brnn_extended_+3A_formula">formula</code></td>
<td>
<p>A formula of the form <code>y ~ x1 + x2 ... | z1 + z2 ...</code>, the | is used to separate the two groups of input variables.</p>
</td></tr>
<tr><td><code id="brnn_extended_+3A_data">data</code></td>
<td>
<p>Data frame from which variables specified in  <code>formula</code> are preferentially to be taken.</p>
</td></tr>
<tr><td><code id="brnn_extended_+3A_y">y</code></td>
<td>
<p>(numeric, <code class="reqn">n</code>) the response data-vector (NAs not  allowed).</p>
</td></tr>
<tr><td><code id="brnn_extended_+3A_x">x</code></td>
<td>
<p>(numeric, <code class="reqn">n \times p</code>) incidence matrix for variables in group 1.</p>
</td></tr>
<tr><td><code id="brnn_extended_+3A_z">z</code></td>
<td>
<p>(numeric, <code class="reqn">n \times q</code>) incidence matrix for variables in group 2.</p>
</td></tr>
<tr><td><code id="brnn_extended_+3A_neurons1">neurons1</code></td>
<td>
<p>positive integer that indicates the number of neurons for variables in group 1.</p>
</td></tr>
<tr><td><code id="brnn_extended_+3A_neurons2">neurons2</code></td>
<td>
<p>positive integer that indicates the number of neurons for variables in group 2.</p>
</td></tr>
<tr><td><code id="brnn_extended_+3A_normalize">normalize</code></td>
<td>
<p>logical, if TRUE will normalize inputs and output, the default value is TRUE.</p>
</td></tr>
<tr><td><code id="brnn_extended_+3A_epochs">epochs</code></td>
<td>
<p>positive integer, maximum number of epochs to train, default 1000.</p>
</td></tr>
<tr><td><code id="brnn_extended_+3A_mu">mu</code></td>
<td>
<p>positive number that controls the behaviour of the Gauss-Newton optimization algorithm, default value 0.005.</p>
</td></tr>
<tr><td><code id="brnn_extended_+3A_mu_dec">mu_dec</code></td>
<td>
<p>positive number, is the mu decrease ratio, default value 0.1.</p>
</td></tr>
<tr><td><code id="brnn_extended_+3A_mu_inc">mu_inc</code></td>
<td>
<p>positive number, is the mu increase ratio, default value 10.</p>
</td></tr>
<tr><td><code id="brnn_extended_+3A_mu_max">mu_max</code></td>
<td>
<p>maximum mu before training is stopped, strict positive number, default value <code class="reqn">1\times 10^{10}</code>.</p>
</td></tr>
<tr><td><code id="brnn_extended_+3A_min_grad">min_grad</code></td>
<td>
<p>minimum gradient.</p>
</td></tr>
<tr><td><code id="brnn_extended_+3A_change">change</code></td>
<td>
<p>The program  will stop if the maximum (in absolute value) of the differences of the F 
function in 3 consecutive iterations is less than this quantity.</p>
</td></tr>
<tr><td><code id="brnn_extended_+3A_cores">cores</code></td>
<td>
<p>Number of cpu cores to use for calculations (only available in UNIX-like operating systems). The function detectCores in the R package 
parallel can be used to attempt to detect the number of CPUs in the machine that R is running, but not necessarily 
all the cores are available for the current user, because for example in multi-user 
systems it will depend on system policies. Further details can be found in the documentation for the parallel package</p>
</td></tr>
<tr><td><code id="brnn_extended_+3A_verbose">verbose</code></td>
<td>
<p>logical, if TRUE will print iteration history.</p>
</td></tr>
<tr><td><code id="brnn_extended_+3A_contrastsx">contrastsx</code></td>
<td>
<p>an optional list of contrasts to be used for some or 
all of the factors appearing as variables in the first group of input variables in the model formula.</p>
</td></tr>
<tr><td><code id="brnn_extended_+3A_contrastsz">contrastsz</code></td>
<td>
<p>an optional list of contrasts to be used for some or 
all of the factors appearing as variables in the second group of input variables in the model formula.</p>
</td></tr>
<tr><td><code id="brnn_extended_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The software fits a two layer network as described in MacKay (1992) and Foresee and Hagan (1997). 
The model is given by:
</p>
<p><code class="reqn">y_i= \sum_{k=1}^{s_1} w_k^{1} g_k (b_k^{1} + \sum_{j=1}^p x_{ij} \beta_j^{1[k]}) +
            \sum_{k=1}^{s_2} w_k^{2} g_k (b_k^{2} + \sum_{j=1}^q z_{ij} \beta_j^{2[k]})\,\,e_i, i=1,...,n</code>
</p>

<ul>
<li><p><code class="reqn">e_i \sim N(0,\sigma_e^2)</code>.
</p>
</li>
<li><p><code class="reqn">g_k(\cdot)</code> is the activation function, in this implementation <code class="reqn">g_k(x)=\frac{\exp(2x)-1}{\exp(2x)+1}</code>.
</p>
</li></ul>

<p>The software will minimize 
</p>
<p style="text-align: center;"><code class="reqn">F=\beta E_D + \alpha \theta_1' \theta_1 +\delta \theta_2' \theta_2 </code>
</p>

<p>where 
</p>

<ul>
<li><p><code class="reqn">E_D=\sum_{i=1}^n (y_i-\hat y_i)^2</code>, i.e. the sum of squared errors.
</p>
</li>
<li><p><code class="reqn">\beta=\frac{1}{2\sigma^2_e}</code>.
</p>
</li>
<li><p><code class="reqn">\alpha=\frac{1}{2\sigma_{\theta_1}^2}</code>, <code class="reqn">\sigma_{\theta_1}^2</code> is a dispersion parameter for weights and biases for the associated to 
the first group of neurons.
</p>
</li>
<li><p><code class="reqn">\delta=\frac{1}{2\sigma_{\theta_2}^2}</code>, <code class="reqn">\sigma_{\theta_2}^2</code> is a dispersion parameter for weights and biases for the associated to
the second group of neurons.
</p>
</li></ul>



<h3>Value</h3>

<p>object of class <code>"brnn_extended"</code> or <code>"brnn_extended.formula"</code>. Mostly internal structure, but it is a list containing:
</p>
<table>
<tr><td><code>$theta1</code></td>
<td>
<p>A list containing weights and biases. The first <code class="reqn">s_1</code> components of the list contain vectors with 
the estimated parameters for the <code class="reqn">k</code>-th neuron, i.e. <code class="reqn">(w_k^1, b_k^1, \beta_1^{1[k]},...,\beta_p^{1[k]})'</code>. 
<code class="reqn">s_1</code> corresponds to neurons1 in the argument list.</p>
</td></tr>
<tr><td><code>$theta2</code></td>
<td>
<p>A list containing weights and biases. The first <code class="reqn">s_2</code> components of the list contains vectors with 
the estimated parameters for the <code class="reqn">k</code>-th neuron, i.e. <code class="reqn">(w_k^2, b_k^2, \beta_1^{2[k]},...,\beta_q^{2[k]})'</code>.
<code class="reqn">s_2</code> corresponds to neurons2 in the argument list.</p>
</td></tr>
<tr><td><code>$message</code></td>
<td>
<p>String that indicates the stopping criteria for the training process.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Foresee, F. D., and M. T. Hagan. 1997. &quot;Gauss-Newton approximation to Bayesian regularization&quot;, 
<em>Proceedings of the 1997 International Joint Conference on Neural Networks</em>.
</p>
<p>MacKay, D. J. C. 1992. &quot;Bayesian interpolation&quot;, <em>Neural Computation</em>, <b>4(3)</b>, 415-447.
</p>
<p>Nguyen, D. and Widrow, B. 1990. &quot;Improving the learning speed of 2-layer neural networks by choosing initial values of the adaptive weights&quot;,
<em>Proceedings of the IJCNN</em>, <b>3</b>, 21-26.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.brnn_extended">predict.brnn_extended</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

#Example 5
#Warning, it will take a while

#Load the Jersey dataset
data(Jersey)

#Predictive power of the model using the SECOND set for 10 fold CROSS-VALIDATION
data=pheno
data$G=G
data$D=D
data$partitions=partitions

#Fit the model for the TESTING DATA for Additive + Dominant
out=brnn_extended(yield_devMilk ~ G | D,
                                  data=subset(data,partitions!=2),
                                  neurons1=2,neurons2=2,epochs=100,verbose=TRUE)

#Plot the results
#Predicted vs observed values for the training set
par(mfrow=c(2,1))
yhat_R_training=predict(out)
plot(out$y,yhat_R_training,xlab=expression(hat(y)),ylab="y")
cor(out$y,yhat_R_training)

#Predicted vs observed values for the testing set
newdata=subset(data,partitions==2,select=c(D,G))
ytesting=pheno$yield_devMilk[partitions==2]
yhat_R_testing=predict(out,newdata=newdata)
plot(ytesting,yhat_R_testing,xlab=expression(hat(y)),ylab="y")
cor(ytesting,yhat_R_testing)
  

## End(Not run)
 
</code></pre>

<hr>
<h2 id='brnn_ordinal'>brnn_ordinal</h2><span id='topic+brnn_ordinal'></span><span id='topic+brnn_ordinal.formula'></span><span id='topic+brnn_ordinal.default'></span><span id='topic+print.brnn_ordinal'></span><span id='topic+summary.brnn_ordinal'></span>

<h3>Description</h3>

<p>The brnn_ordinal function fits a Bayesian Regularized Neural Network for Ordinal data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
  brnn_ordinal(x, ...)
  
  ## S3 method for class 'formula'
brnn_ordinal(formula, data, contrasts=NULL,...)
  
  ## Default S3 method:
brnn_ordinal(x,
               y,
               neurons=2,
               normalize=TRUE,
               epochs=1000,
               mu=0.005,
               mu_dec=0.1,
               mu_inc=10,
               mu_max=1e10,
               min_grad=1e-10,
               change_F=0.01,
               change_par=0.01,
               iter_EM=1000,
               verbose=FALSE,
               ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="brnn_ordinal_+3A_formula">formula</code></td>
<td>
<p>A formula of the form <code>y ~ x1 + x2 + ...</code></p>
</td></tr>
<tr><td><code id="brnn_ordinal_+3A_data">data</code></td>
<td>
<p>Data frame from which variables specified in  <code>formula</code> are preferentially to be taken.</p>
</td></tr>
<tr><td><code id="brnn_ordinal_+3A_x">x</code></td>
<td>
<p>(numeric, <code class="reqn">n \times p</code>) incidence matrix.</p>
</td></tr>
<tr><td><code id="brnn_ordinal_+3A_y">y</code></td>
<td>
<p>(numeric, <code class="reqn">n</code>) the response data-vector (NAs not  allowed).</p>
</td></tr>
<tr><td><code id="brnn_ordinal_+3A_neurons">neurons</code></td>
<td>
<p>positive integer that indicates the number of neurons.</p>
</td></tr>
<tr><td><code id="brnn_ordinal_+3A_normalize">normalize</code></td>
<td>
<p>logical, if TRUE will normalize inputs and output, the default value is TRUE.</p>
</td></tr>
<tr><td><code id="brnn_ordinal_+3A_epochs">epochs</code></td>
<td>
<p>positive integer, maximum number of epochs(iterations) to train, default 1000.</p>
</td></tr>
<tr><td><code id="brnn_ordinal_+3A_mu">mu</code></td>
<td>
<p>positive number that controls the behaviour of the Gauss-Newton optimization algorithm, default value 0.005.</p>
</td></tr>
<tr><td><code id="brnn_ordinal_+3A_mu_dec">mu_dec</code></td>
<td>
<p>positive number, is the mu decrease ratio, default value 0.1.</p>
</td></tr>
<tr><td><code id="brnn_ordinal_+3A_mu_inc">mu_inc</code></td>
<td>
<p>positive number, is the mu increase ratio, default value 10.</p>
</td></tr>
<tr><td><code id="brnn_ordinal_+3A_mu_max">mu_max</code></td>
<td>
<p>maximum mu before training is stopped, strict positive number, default value <code class="reqn">1\times 10^{10}</code>.</p>
</td></tr>
<tr><td><code id="brnn_ordinal_+3A_min_grad">min_grad</code></td>
<td>
<p>minimum gradient.</p>
</td></tr>
<tr><td><code id="brnn_ordinal_+3A_change_f">change_F</code></td>
<td>
<p>the program  will stop if the maximum (in absolute value) of the differences of the F function in 3 consecutive iterations is less than this quantity.</p>
</td></tr>
<tr><td><code id="brnn_ordinal_+3A_change_par">change_par</code></td>
<td>
<p>the program will stop iterations of the EM algorithm when the maximum of absolute values of differences between parameters in two consecutive iterations ins less than this quantity.</p>
</td></tr>
<tr><td><code id="brnn_ordinal_+3A_iter_em">iter_EM</code></td>
<td>
<p>positive integer, maximum number of iteration for the EM algorithm.</p>
</td></tr>
<tr><td><code id="brnn_ordinal_+3A_verbose">verbose</code></td>
<td>
<p>logical, if TRUE will print iteration history.</p>
</td></tr>
<tr><td><code id="brnn_ordinal_+3A_contrasts">contrasts</code></td>
<td>
<p>an optional list of contrasts to be used for some or all of the factors appearing as variables in the model formula.</p>
</td></tr>
<tr><td><code id="brnn_ordinal_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The software fits a Bayesian Regularized Neural Network for Ordinal data. The model is an 
extension of the two layer network as described in MacKay (1992); Foresee and Hagan (1997), and
Gianola et al. (2011). We use the latent variable approach described in Albert and Chib (1993)
to model ordinal data, the Expectation maximization (EM) and Levenberg-Marquardt 
algorithm (Levenberg, 1944; Marquardt, 1963) to fit the model. 
</p>
<p>Following Albert and Chib (1993), suppose that <code class="reqn">Y_1,...,Y_n</code> are
observed and <code class="reqn">Y_i</code> can take values on <code class="reqn">L</code> ordered values. We are interested
in modelling the probability <code class="reqn">p_{ij}=P(Y_i=j)</code> using the covariates 
<code class="reqn">x_{i1},...,x_{ip}</code>. Let 
</p>
<p><code class="reqn">g(\boldsymbol{x}_i)= \sum_{k=1}^s w_k g_k (b_k + \sum_{j=1}^p x_{ij} \beta_j^{[k]})</code>, 
</p>
<p>where: 
</p>

<ul>
<li><p><code class="reqn">s</code> is the number of neurons.
</p>
</li>
<li><p><code class="reqn">w_k</code> is the weight of the <code class="reqn">k</code>-th neuron, <code class="reqn">k=1,...,s</code>.
</p>
</li>
<li><p><code class="reqn">b_k</code> is a bias for the <code class="reqn">k</code>-th neuron, <code class="reqn">k=1,...,s</code>.
</p>
</li>
<li><p><code class="reqn">\beta_j^{[k]}</code> is the weight of the <code class="reqn">j</code>-th input to the net, <code class="reqn">j=1,...,p</code>.
</p>
</li>
<li><p><code class="reqn">g_k(\cdot)</code> is the activation function, in this implementation <code class="reqn">g_k(x)=\frac{\exp(2x)-1}{\exp(2x)+1}</code>.
</p>
</li></ul>

<p>Let 
</p>
<p><code class="reqn">Z_i=g(\boldsymbol{x}_i)+e_i</code>,
</p>
<p>where:  
</p>

<ul>
<li> <p><code class="reqn">e_i \sim N(0,1)</code>.
</p>
</li>
<li> <p><code class="reqn">Z_i</code> is an unobserved (latent variable).
</p>
</li></ul>

<p>The output from the model for latent variable is related to 
observed data using the approach employed in the probit 
and logit ordered models, that is <code class="reqn">Y_i=j</code> if 
<code class="reqn">\lambda_{j-1}&lt;Z_i&lt;\lambda_{j}</code>, where <code class="reqn">\lambda_j</code>
are a set of unknown thresholds. We assign prior distributions 
to all unknown quantities (see Albert and Chib, 1993; Gianola et al., 2011) 
for further details. The Expectation maximization (EM) and Levenberg-Marquardt 
algorithm (Levenberg, 1944; Marquardt, 1963) to fit the model.
</p>


<h3>Value</h3>

<p>object of class <code>"brnn_ordinal"</code>. Mostly internal structure, but it is a list containing:
</p>
<table>
<tr><td><code>$theta</code></td>
<td>
<p>A list containing weights and biases. The first <code class="reqn">s</code> components of the list contains vectors with the estimated parameters for
the <code class="reqn">k</code>-th neuron, i.e. <code class="reqn">(w_k, b_k, \beta_1^{[k]},...,\beta_p^{[k]})'</code>.</p>
</td></tr>
<tr><td><code>$threshold</code></td>
<td>
<p>A vector with estimates of thresholds.</p>
</td></tr>
<tr><td><code>$alpha</code></td>
<td>
<p><code class="reqn">\alpha</code> parameter.</p>
</td></tr>
<tr><td><code>$gamma</code></td>
<td>
<p>effective number of parameters.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Albert J, and S. Chib. 1993. Bayesian Analysis of Binary and Polychotomus Response Data. <em>JASA</em>, <b>88</b>, 669-679.
</p>
<p>Foresee, F. D., and M. T. Hagan. 1997. &quot;Gauss-Newton approximation to Bayesian regularization&quot;, 
<em>Proceedings of the 1997 International Joint Conference on Neural Networks</em>.
</p>
<p>Gianola, D. Okut, H., Weigel, K. and Rosa, G. 2011. &quot;Predicting complex quantitative traits with Bayesian neural networks: a case study with Jersey cows and wheat&quot;. <em>BMC Genetics</em>, 
<b>12</b>,87.
</p>
<p>Levenberg, K. 1944. &quot;A method for the solution of certain problems in least squares&quot;, <em>Quart. Applied Math.</em>, <b>2</b>, 164-168. 
</p>
<p>MacKay, D. J. C. 1992. &quot;Bayesian interpolation&quot;, <em>Neural Computation</em>, <b>4(3)</b>, 415-447.
</p>
<p>Marquardt, D. W. 1963. &quot;An algorithm for least-squares estimation of non-linear parameters&quot;. <em>SIAM Journal on Applied Mathematics</em>, <b>11(2)</b>, 431-441. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.brnn_ordinal">predict.brnn_ordinal</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
#Load the library
library(brnn)

#Load the dataset
data(GLS)

#Subset of data for location Harare
HarareOrd=subset(phenoOrd,Loc=="Harare")

#Eigen value decomposition for GOrdm keep those 
#eigen vectors whose corresponding eigen-vectors are bigger than 1e-10
#and then compute principal components

evd=eigen(GOrd)
evd$vectors=evd$vectors[,evd$value&gt;1e-10]
evd$values=evd$values[evd$values&gt;1e-10]
PC=evd$vectors
rownames(PC)=rownames(GOrd)

#Response variable
y=phenoOrd$rating
gid=as.character(phenoOrd$Stock)

Z=model.matrix(~gid-1)
colnames(Z)=gsub("gid","",colnames(Z))

if(any(colnames(Z)!=rownames(PC))) stop("Ordering problem\n")

#Matrix of predictors for Neural net
X=Z%*%PC

#Cross-validation
set.seed(1)
testing=sample(1:length(y),size=as.integer(0.10*length(y)),replace=FALSE)
isNa=(1:length(y)%in%testing)
yTrain=y[!isNa]
XTrain=X[!isNa,]
nTest=sum(isNa)

neurons=2
	
fmOrd=brnn_ordinal(XTrain,yTrain,neurons=neurons,verbose=FALSE)

#Predictions for testing set
XTest=X[isNa,]
predictions=predict(fmOrd,XTest)
predictions



## End(Not run)
</code></pre>

<hr>
<h2 id='D'>Genomic dominant relationship matrix for the Jersey dataset.</h2><span id='topic+D'></span>

<h3>Description</h3>

<p>This matrix was calculated by using the dominance incidence matrix derived from 33,267 Single Nucleotide
Polymorphisms (SNPs) information on 297 individually cows, 
</p>
<p style="text-align: center;"><code class="reqn">D=\frac{X_d X_d'}{2 \sum_{j=1}^p (p_j^2+q_j^2) p_j q_j},</code>
</p>

<p>where
</p>

<ul>
<li><p><code class="reqn">X_d</code> is the design matrix for allele substitution effects for dominance.
</p>
</li>
<li><p><code class="reqn">p_j</code> is the frecuency of the second allele at locus <code class="reqn">j</code> and <code class="reqn">q_j=1-p_j</code>.
</p>
</li></ul>



<h3>Source</h3>

<p>University of Wisconsin at Madison, USA.
</p>

<hr>
<h2 id='estimate.trace'>estimate.trace</h2><span id='topic+estimate.trace'></span>

<h3>Description</h3>

<p>The estimate.trace function estimates the trace of the inverse of a possitive definite and symmetric 
matrix using the algorithm developed by Bai et al. (1996). It is specially useful 
when the matrix is huge.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  estimate.trace(A,tol=1E-6,samples=40,cores=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate.trace_+3A_a">A</code></td>
<td>
<p>(numeric), positive definite and symmetric matrix.</p>
</td></tr>
<tr><td><code id="estimate.trace_+3A_tol">tol</code></td>
<td>
<p>numeric tolerance, a very small number useful for checking convergenge in the Bai's algorithm.</p>
</td></tr>
<tr><td><code id="estimate.trace_+3A_samples">samples</code></td>
<td>
<p>integer, number of Monte Carlo replicates to estimate the trace of the inverse.</p>
</td></tr>
<tr><td><code id="estimate.trace_+3A_cores">cores</code></td>
<td>
<p>Number of cpu cores to use for calculations (only availible in UNIX-like operating systems).</p>
</td></tr>
</table>


<h3>References</h3>

<p>Bai, Z. J., M. Fahey and G. Golub. 1996. &quot;Some large-scale matrix computation problems.&quot; 
<em>Journal of Computational and Applied Mathematics</em>, <b>74(1-2)</b>, 71-89.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(brnn)
data(Jersey)

#Estimate the trace of the iverse of G matrix
estimate.trace(G)

#The TRUE value
sum(diag(solve(G)))


## End(Not run)
</code></pre>

<hr>
<h2 id='G'>Genomic additive relationship matrix for the Jersey dataset.</h2><span id='topic+G'></span>

<h3>Description</h3>

<p>A matrix, similar to this was used in Gianola et al. (2011) for predicting 
milk, fat and protein production in Jersey cows. In this software version we do not center the incidence 
matrix for the additive effects.
</p>
<p><code class="reqn">G=\frac{X_a X_a'}{2\sum_{j=1}^p p_j (1-p_j)},</code>
</p>
<p>where
</p>

<ul>
<li><p><code class="reqn">X_a</code> is the design matrix for allele substitution effects for additivity.
</p>
</li>
<li><p><code class="reqn">p_j</code> is the frecuency of the second allele at locus <code class="reqn">j</code> and <code class="reqn">q_j=1-p_j</code>.
</p>
</li></ul>



<h3>Source</h3>

<p>University of Wisconsin at Madison, USA.
</p>


<h3>References</h3>

<p>Gianola, D. Okut, H., Weigel, K. and Rosa, G. 2011. &quot;Predicting complex quantitative traits with Bayesian neural networks: a case study with Jersey cows and wheat&quot;. <em>BMC Genetics</em>, 
<b>12</b>,87.
</p>

<hr>
<h2 id='GOrd'>Genomic additive relationship matrix for the GLS dataset.</h2><span id='topic+GOrd'></span>

<h3>Description</h3>

<p>Genomic relationship matrix for the GLS dataset. 
The matrix was derived from 46347 markers for the 278 individuals. 
The matrix was calculated as follows G=MM'/p, where
M is the matrix of markers centered and standarized and p is the 
number of markers. 
</p>


<h3>Source</h3>

<p>International Maize and Wheat Improvement Center (CIMMYT), Mexico.
</p>


<h3>References</h3>

<p>Montesinos-Lopez, O., A. Montesinos-Lopez, J. Crossa, J. Burgueno and K. Eskridge. 2015. &quot;Genomic-Enabled Prediction of Ordinal Data with Bayesian Logistic Ordinal Regression&quot;. <em>G3: Genes | Genomes | Genetics</em>, <b>5</b>, 2113-2126.
</p>

<hr>
<h2 id='initnw'>Initialize networks weights and biases</h2><span id='topic+initnw'></span>

<h3>Description</h3>

<p>Function to initialize the weights and biases in a neural network. It uses the Nguyen-Widrow (1990) algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>     initnw(neurons,p,n,npar)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="initnw_+3A_neurons">neurons</code></td>
<td>
<p>Number of neurons.</p>
</td></tr>
<tr><td><code id="initnw_+3A_p">p</code></td>
<td>
<p>Number of predictors.</p>
</td></tr>
<tr><td><code id="initnw_+3A_n">n</code></td>
<td>
<p>Number of cases.</p>
</td></tr>
<tr><td><code id="initnw_+3A_npar">npar</code></td>
<td>
<p>Number of parameters to be estimate including only weights and biases, and should be equal to <code class="reqn">neurons \times (1+1+p)+1</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm is described in Nguyen-Widrow (1990) and in other books, see for example Sivanandam and Sumathi (2005). The algorithm is briefly described below.
</p>

<ul>
<li><p>1.-Compute the scaling factor <code class="reqn">\theta=0.7 p^{1/n}</code>.
</p>
</li>
<li><p>2.- Initialize the weight and biases for each neuron at random, for example generating random numbers from <code class="reqn">U(-0.5,0.5)</code>.
</p>
</li>
<li><p>3.- For each neuron:
</p>

<ul>
<li><p>compute <code class="reqn">\eta_k=\sqrt{\sum_{j=1}^p (\beta_j^{(k)})^2}</code>,
</p>
</li>
<li><p>update <code class="reqn">(\beta_1^{(k)},...,\beta_p^{(k)})'</code>,
</p>
<p style="text-align: center;"><code class="reqn">\beta_j^{(k)}=\frac{\theta \beta_j^{(k)}}{\eta_k}, j=1,...,p,</code>
</p>

</li>
<li><p>Update the bias <code class="reqn">(b_k)</code> generating a random number from <code class="reqn">U(-\theta,\theta)</code>.
</p>
</li></ul>

</li></ul>



<h3>Value</h3>

<p>A list containing initial values for weights and biases. The first <code class="reqn">s</code> components of the list contains vectors with the initial values for 
the weights and biases of the <code class="reqn">k</code>-th neuron, i.e. <code class="reqn">(\omega_k, b_k, \beta_1^{(k)},...,\beta_p^{(k)})'</code>.
</p>


<h3>References</h3>

<p>Nguyen, D. and Widrow, B. 1990. &quot;Improving the learning speed of 2-layer neural networks by choosing initial values of the adaptive weights&quot;,
<em>Proceedings of the IJCNN</em>, <b>3</b>, 21-26.
</p>
<p>Sivanandam, S.N. and Sumathi, S. 2005. Introduction to Neural Networks Using MATLAB 6.0. Ed. McGraw Hill, First edition. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#Load the library
library(brnn)

#Set parameters
neurons=3
p=4
n=10
npar=neurons*(1+1+p)+1
initnw(neurons=neurons,p=p,n=n,npar=npar)


## End(Not run)
</code></pre>

<hr>
<h2 id='jacobian'>Jacobian</h2><span id='topic+jacobian'></span>

<h3>Description</h3>

<p>Internal function for the calculation of the Jacobian.  
</p>

<hr>
<h2 id='normalize'>normalize</h2><span id='topic+normalize'></span>

<h3>Description</h3>

<p>Internal function for normalizing the data.
This function makes a linear transformation of the inputs such that the  values lie between -1 and 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   normalize(x,base,spread)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="normalize_+3A_x">x</code></td>
<td>
<p>a vector or matrix that needs to be normalized.</p>
</td></tr>
<tr><td><code id="normalize_+3A_base">base</code></td>
<td>
<p>If x is a vector, base is the minimum of x. If x is a matrix, base is a vector with the minimum for each of the columns of the matrix x.</p>
</td></tr>
<tr><td><code id="normalize_+3A_spread">spread</code></td>
<td>
<p>if x is a vector, spread=max(x)-base. If x is a matrix, spread is a vector calculated for each of the columns of x.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>z=2*(x-base)/spread - 1 
</p>


<h3>Value</h3>

<p>A vector or matrix with the resulting normalized values.
</p>

<hr>
<h2 id='partitions'>Partitions for cross validation (CV)</h2><span id='topic+partitions'></span>

<h3>Description</h3>

<p>Is a vector <code class="reqn">(297 \times 1)</code> that assigns observations to 10
disjoint sets; the assignment was generated at random.
This is used later to conduct a 10-fold CV.
</p>


<h3>Source</h3>

<p>University of Wisconsin at Madison, USA.
</p>

<hr>
<h2 id='pheno'>Phenotypic information for Jersey</h2><span id='topic+pheno'></span>

<h3>Description</h3>

 
<p>The format of the phenotype dataframe is animal ID, herd, 
number of lactations, average milk yield, average fat yield, 
average protein yield, yield deviation for milk, 
yield deviation for fat, and yield deviation for protein.  
Averages are adjusted for age, days in milk, and milking frequency.  
Yield deviations are adjusted further, for herd-year-season effect.  
You may wish to use yield deviations, because there are relatively 
few cows per herd (farmers don't pay to genotype all of their cows, just the good ones).
</p>


<h3>Source</h3>

<p>University of Wisconsin at Madison, USA.
</p>


<h3>References</h3>

<p>Gianola, D. Okut, H., Weigel, K. and Rosa, G. 2011. &quot;Predicting complex quantitative traits with Bayesian neural networks: a case study with Jersey cows and wheat&quot;. <em>BMC Genetics</em>, 
<b>12</b>,87.
</p>

<hr>
<h2 id='phenoOrd'>Phenotypic information for GLS (ordinal trait)</h2><span id='topic+phenoOrd'></span>

<h3>Description</h3>

 
<p>The format of the phenotype dataframe is Location (Loc), 
Replicate (Rep), Genotype ID (Stock) and rating (ordinal score).
Gray Leaf Spot (GLS) is a disease caused by the fungus 
<em>Cercospora zeae-maydis</em>. This dataset consists of 
genotypic and phenotypic information for 278 maize lines from 
the Drought Tolerance Maize (DTMA) project of CIMMYT's Global 
Maize Program. The dataset includes information on disease 
severity measured on an ordinal scale with 5 points: 
1= no disease, 2= low infection, 3=moderate infection, 
4=high infection and 5=totally infected.
</p>


<h3>Source</h3>

<p>International Maize and Wheat Improvement Center (CIMMYT), Mexico.
</p>


<h3>References</h3>

<p>Montesinos-Lopez, O., A. Montesinos-Lopez, J. Crossa, J. Burgueno and K. Eskridge. 2015. &quot;Genomic-Enabled Prediction of Ordinal Data with Bayesian Logistic Ordinal Regression&quot;. <em>G3: Genes | Genomes | Genetics</em>, <b>5</b>, 2113-2126.
</p>

<hr>
<h2 id='predict.brnn'>predict.brnn</h2><span id='topic+predict.brnn'></span>

<h3>Description</h3>

<p>The function produces the predictions for a two-layer feed-forward neural network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   ## S3 method for class 'brnn'
predict(object,newdata,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.brnn_+3A_object">object</code></td>
<td>
<p>an object of the class <code>brnn</code> as returned by <code>brnn</code></p>
</td></tr>
<tr><td><code id="predict.brnn_+3A_newdata">newdata</code></td>
<td>
<p>matrix or data frame of test examples. A vector is considered to be
a row vector comprising a single case.</p>
</td></tr>
<tr><td><code id="predict.brnn_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a method for the generic function
<code>predict()</code> for class <code>"brnn"</code>.
It can be invoked by calling <code>predict(x)</code> for an
object <code>x</code> of the appropriate class, or directly by
calling <code>predict.brnn(x)</code> regardless of the class of the object.
</p>


<h3>Value</h3>

<p>A vector containing the predictions
</p>

<hr>
<h2 id='predict.brnn_extended'>predict.brnn_extended</h2><span id='topic+predict.brnn_extended'></span>

<h3>Description</h3>

<p>The function produces the predictions for a two-layer feed-forward neural network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   ## S3 method for class 'brnn_extended'
predict(object,newdata,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.brnn_extended_+3A_object">object</code></td>
<td>
<p>an object of the class <code>brnn_extended</code> as returned by <code>brnn_extended</code></p>
</td></tr>
<tr><td><code id="predict.brnn_extended_+3A_newdata">newdata</code></td>
<td>
<p>matrix or data frame of test examples. A vector is considered to be
a row vector comprising a single case.</p>
</td></tr>
<tr><td><code id="predict.brnn_extended_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a method for the generic function
<code>predict()</code> for class <code>"brnn_extended"</code>.
It can be invoked by calling <code>predict(x)</code> for an
object <code>x</code> of the appropriate class, or directly by
calling <code>predict.brnn(x)</code> regardless of the class of the object.
</p>


<h3>Value</h3>

<p>A vector containig the predictions
</p>

<hr>
<h2 id='predict.brnn_ordinal'>predict.brnn_ordinal</h2><span id='topic+predict.brnn_ordinal'></span>

<h3>Description</h3>

<p>The function produces the predictions for a two-layer feed-forward neural network for ordinal data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   ## S3 method for class 'brnn_ordinal'
predict(object,newdata,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.brnn_ordinal_+3A_object">object</code></td>
<td>
<p>an object of the class <code>brnn_ordinal</code> as returned by <code>brnn_ordinal</code></p>
</td></tr>
<tr><td><code id="predict.brnn_ordinal_+3A_newdata">newdata</code></td>
<td>
<p>matrix or data frame of test examples. A vector is considered to be
a row vector comprising a single case.</p>
</td></tr>
<tr><td><code id="predict.brnn_ordinal_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a method for the generic function
<code>predict()</code> for class <code>"brnn_ordinal"</code>.
It can be invoked by calling <code>predict(x)</code> for an
object <code>x</code> of the appropriate class, or directly by
calling <code>predict.brnn_ordinal(x)</code> regardless of the class of the object.
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr><td><code>class</code></td>
<td>
<p>Predicted class (an integer).</p>
</td></tr>
<tr><td><code>probability</code></td>
<td>
<p>Posterior probability of belonging to a class given the covariates.</p>
</td></tr>
</table>

<hr>
<h2 id='twoinput'>2 Inputs and 1 output.</h2><span id='topic+twoinput'></span>

<h3>Description</h3>

<p>The data used in Paciorek and
Schervish (2004). This is a data.frame with 3 columns, columns 1 and 2 corresponds to the 
predictors and column 3 corresponds to the target.
</p>


<h3>References</h3>

<p>Paciorek, C. J. and Schervish, M. J. 2004. &quot;Nonstationary Covariance Functions for
Gaussian Process Regression&quot;. In Thrun, S., Saul, L., and Scholkopf, B., editors, <em>Advances
in Neural Information Processing Systems 16</em>. MIT Press, Cambridge, MA.
</p>

<hr>
<h2 id='un_normalize'>un_normalize</h2><span id='topic+un_normalize'></span>

<h3>Description</h3>

<p>Internal function for going back to the original scale.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   un_normalize(z,base,spread)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="un_normalize_+3A_z">z</code></td>
<td>
<p>a vector or matrix with values normalized between -1 and 1, this vector was obtained when normalizing a vector or matrix x.</p>
</td></tr>
<tr><td><code id="un_normalize_+3A_base">base</code></td>
<td>
<p>If z is a vector, base is the minimum of x. If x is a matrix, base is a vector with the minimum for each of the columns of the matrix x.</p>
</td></tr>
<tr><td><code id="un_normalize_+3A_spread">spread</code></td>
<td>
<p>if z is a vector, spread=base-max(x). If x is a matrix, spread is a vector calculated for each of the columns of x.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>x=base+0.5*spread*(z+1) 
</p>


<h3>Value</h3>

<p>A vector or matrix with the resulting un normalized values.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
