<!DOCTYPE html><html lang="en"><head><title>Help for package entropy</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {entropy}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#entropy-package'><p>The entropy Package</p></a></li>
<li><a href='#discretize'><p>Discretize Continuous Random Variables</p></a></li>
<li><a href='#entropy'><p>Estimating Entropy From Observed Counts</p></a></li>
<li><a href='#entropy-internal'><p>Internal entropy functions</p></a></li>
<li><a href='#entropy.ChaoShen'><p>Chao-Shen Entropy Estimator</p></a></li>
<li><a href='#entropy.Dirichlet'><p>Dirichlet Prior Bayesian Estimators of Entropy, Mutual Information</p>
and Other Related Quantities</a></li>
<li><a href='#entropy.empirical'><p>Empirical Estimators of Entropy and Mutual Information and Related Quantities</p></a></li>
<li><a href='#entropy.MillerMadow'><p>Miller-Madow Entropy Estimator</p></a></li>
<li><a href='#entropy.NSB'><p>R Interface to NSB Entropy Estimator</p></a></li>
<li><a href='#entropy.plugin'><p>Plug-In Entropy Estimator</p></a></li>
<li><a href='#entropy.shrink'><p>Shrinkage Estimators of Entropy, Mutual Information and Related Quantities</p></a></li>
<li><a href='#Gstat'><p>G Statistic and Chi-Squared Statistic</p></a></li>
<li><a href='#KL.plugin'><p>Plug-In Estimator of the Kullback-Leibler divergence and of the Chi-Squared Divergence</p></a></li>
<li><a href='#mi.plugin'><p>Plug-In Estimator of Mutual Information and of the Chi-Squared Statistic of Independence</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Version:</td>
<td>1.3.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-10-02</td>
</tr>
<tr>
<td>Title:</td>
<td>Estimation of Entropy, Mutual Information and Related Quantities</td>
</tr>
<tr>
<td>Author:</td>
<td>Jean Hausser and Korbinian Strimmer</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Korbinian Strimmer &lt;strimmerlab@gmail.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.2)</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements various estimators of entropy for discrete random 
  variables, including the shrinkage estimator by Hausser and Strimmer (2009),
  the maximum likelihood and the Millow-Madow estimator, various Bayesian
  estimators, and the Chao-Shen estimator.  It also offers an R interface to the
  NSB estimator.  Furthermore, the package provides functions for estimating the
  Kullback-Leibler divergence, the chi-squared divergence, mutual information,
  and the chi-squared divergence of independence.  It also computes the
  G statistic and the chi-squared statistic and corresponding p-values.
  Furthermore, there are functions for discretizing continuous random variables.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://strimmerlab.github.io/software/entropy/">https://strimmerlab.github.io/software/entropy/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-10-02 17:13:51 UTC; strimmer</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-10-02 18:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='entropy-package'>The entropy Package</h2><span id='topic+entropy-package'></span>

<h3>Description</h3>

<p>This package implements various estimators of the Shannon entropy.
Most estimators in this package can be applied in &ldquo;small n, large p&rdquo; situations, 
i.e. when there are many more bins than counts.
</p>
<p>The main function of this package is <code><a href="#topic+entropy">entropy</a></code>, which provides
a unified interface to various entropy estimators.  Other functions included in this package are
estimators of Kullback-Leibler divergence (<code><a href="#topic+KL.plugin">KL.plugin</a></code>), mutual information (<code><a href="#topic+mi.plugin">mi.plugin</a></code>) and of the chi-squared divergence (<code><a href="#topic+chi2.plugin">chi2.plugin</a></code>).
Furthermore, there are functions to compute the G statistic (<code><a href="#topic+Gstat">Gstat</a></code>)
and the chi-squared statistic (<code><a href="#topic+chi2stat">chi2stat</a></code>).
</p>
<p>If you use this package please cite:
Jean Hausser and Korbinian Strimmer. 2009.  Entropy inference and the James-Stein estimator, with application to nonlinear gene association networks.  J. Mach. Learn. Res. <b>10</b>: 1469-1484.  Available online from
<a href="https://jmlr.csail.mit.edu/papers/v10/hausser09a.html">https://jmlr.csail.mit.edu/papers/v10/hausser09a.html</a>.
</p>
<p>This paper contains a detailed statistical comparison of the estimators available 
in this package. It also describes the shrinkage entropy estimator <code><a href="#topic+entropy.shrink">entropy.shrink</a></code>.
</p>


<h3>Author(s)</h3>

<p>Jean Hausser and Korbinian Strimmer (<a href="https://strimmerlab.github.io/">https://strimmerlab.github.io/</a>)</p>


<h3>References</h3>

<p>See website: <a href="https://strimmerlab.github.io/software/entropy/">https://strimmerlab.github.io/software/entropy/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+entropy">entropy</a></code>
</p>

<hr>
<h2 id='discretize'>Discretize Continuous Random Variables</h2><span id='topic+discretize'></span><span id='topic+discretize2d'></span>

<h3>Description</h3>

<p><code>discretize</code> puts observations from a continuous random variable 
into bins and returns the corresponding vector of counts.
</p>
<p><code>discretize2d</code> puts observations from a pair of continuous random variables 
into bins and returns the corresponding table of counts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>discretize( x, numBins, r=range(x) )
discretize2d( x1, x2, numBins1, numBins2, r1=range(x1), r2=range(x2) )
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="discretize_+3A_x">x</code></td>
<td>
<p>vector of observations.</p>
</td></tr>
<tr><td><code id="discretize_+3A_x1">x1</code></td>
<td>
<p>vector of observations for the first random variable.</p>
</td></tr>
<tr><td><code id="discretize_+3A_x2">x2</code></td>
<td>
<p>vector of observations for the second random variable.</p>
</td></tr>
<tr><td><code id="discretize_+3A_numbins">numBins</code></td>
<td>
<p>number of bins.</p>
</td></tr>
<tr><td><code id="discretize_+3A_numbins1">numBins1</code></td>
<td>
<p>number of bins for the first random variable.</p>
</td></tr>
<tr><td><code id="discretize_+3A_numbins2">numBins2</code></td>
<td>
<p>number of bins for the second random variable.</p>
</td></tr>
<tr><td><code id="discretize_+3A_r">r</code></td>
<td>
<p>range of the random variable (default: observed range).</p>
</td></tr>
<tr><td><code id="discretize_+3A_r1">r1</code></td>
<td>
<p>range of the first random variable (default: observed range).</p>
</td></tr>
<tr><td><code id="discretize_+3A_r2">r2</code></td>
<td>
<p>range of the second random variable (default: observed range).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The bins for a random variable all have the same width. It is determined by the length of the range divided by the number of bins.
</p>


<h3>Value</h3>

<p><code>discretize</code> returns a vector containing the counts for each bin.
</p>
<p><code>discretize2d</code> returns a matrix containing the counts for each bin. 
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+entropy">entropy</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load entropy library 
library("entropy")

### 1D example ####

# sample from continuous uniform distribution
x1 = runif(10000)
hist(x1, xlim=c(0,1), freq=FALSE)

# discretize into 10 categories
y1 = discretize(x1, numBins=10, r=c(0,1))
y1

# compute entropy from counts
entropy(y1) # empirical estimate near theoretical maximum
log(10) # theoretical value for discrete uniform distribution with 10 bins 

# sample from a non-uniform distribution 
x2 = rbeta(10000, 750, 250)
hist(x2, xlim=c(0,1), freq=FALSE)

# discretize into 10 categories and estimate entropy
y2 = discretize(x2, numBins=10, r=c(0,1))
y2
entropy(y2) # almost zero

### 2D example ####

# two independent random variables
x1 = runif(10000)
x2 = runif(10000)

y2d = discretize2d(x1, x2, numBins1=10, numBins2=10)
sum(y2d)

# joint entropy
H12 = entropy(y2d )
H12
log(100) # theoretical maximum for 10x10 table

# mutual information
mi.empirical(y2d) # approximately zero


# another way to compute mutual information

# compute marginal entropies
H1 = entropy(rowSums(y2d))
H2 = entropy(colSums(y2d))

H1+H2-H12 # mutual entropy

</code></pre>

<hr>
<h2 id='entropy'>Estimating Entropy From Observed Counts</h2><span id='topic+entropy'></span><span id='topic+freqs'></span>

<h3>Description</h3>

<p><code>entropy</code> estimates the Shannon entropy H of the random variable Y
from the corresponding observed counts <code>y</code>.
</p>
<p><code>freqs</code> estimates bin frequencies from the counts <code>y</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>entropy(y, lambda.freqs, method=c("ML", "MM", "Jeffreys", "Laplace", "SG",
    "minimax", "CS", "NSB", "shrink"), unit=c("log", "log2", "log10"), verbose=TRUE, ...)
freqs(y, lambda.freqs, method=c("ML", "MM", "Jeffreys", "Laplace", "SG",
    "minimax", "CS", "NSB", "shrink"), verbose=TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="entropy_+3A_y">y</code></td>
<td>
<p>vector of counts.</p>
</td></tr>
<tr><td><code id="entropy_+3A_method">method</code></td>
<td>
<p>the method employed to estimate entropy (see Details).</p>
</td></tr>
<tr><td><code id="entropy_+3A_unit">unit</code></td>
<td>
<p>the unit in which entropy is measured. 
The default is &quot;nats&quot; (natural units). For 
computing entropy in &quot;bits&quot; set <code>unit="log2"</code>.</p>
</td></tr>
<tr><td><code id="entropy_+3A_lambda.freqs">lambda.freqs</code></td>
<td>
<p>shrinkage intensity (for &quot;shrink&quot; option). </p>
</td></tr>
<tr><td><code id="entropy_+3A_verbose">verbose</code></td>
<td>
<p>verbose option (for &quot;shrink&quot; option).  </p>
</td></tr>
<tr><td><code id="entropy_+3A_...">...</code></td>
<td>
<p>option passed on to <code><a href="#topic+entropy.NSB">entropy.NSB</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>entropy</code> function allows to estimate entropy from observed counts by a variety
of methods:
</p>

<ul>
<li><p><code>method="ML"</code>:maximum likelihood, see <code><a href="#topic+entropy.empirical">entropy.empirical</a></code> 
</p>
</li>
<li><p><code>method="MM"</code>:bias-corrected maximum likelihood, see <code><a href="#topic+entropy.MillerMadow">entropy.MillerMadow</a></code> 
</p>
</li>
<li><p><code>method="Jeffreys"</code>:<code><a href="#topic+entropy.Dirichlet">entropy.Dirichlet</a></code> with <code>a=1/2</code> 
</p>
</li>
<li><p><code>method="Laplace"</code>:<code><a href="#topic+entropy.Dirichlet">entropy.Dirichlet</a></code> with <code>a=1</code> 
</p>
</li>
<li><p><code>method="SG"</code>:<code><a href="#topic+entropy.Dirichlet">entropy.Dirichlet</a></code> with <code>a=a=1/length(y)</code> 
</p>
</li>
<li><p><code>method="minimax"</code>:<code><a href="#topic+entropy.Dirichlet">entropy.Dirichlet</a></code> with <code>a=sqrt(sum(y))/length(y</code> 
</p>
</li>
<li><p><code>method="CS"</code>:see <code><a href="#topic+entropy.ChaoShen">entropy.ChaoShen</a></code> 
</p>
</li>
<li><p><code>method="NSB"</code>:see <code><a href="#topic+entropy.NSB">entropy.NSB</a></code> 
</p>
</li>
<li><p><code>method="shrink"</code>:see <code><a href="#topic+entropy.shrink">entropy.shrink</a></code> 
</p>
</li></ul>

<p>The <code>freqs</code> function estimates the underlying bin frequencies.  Note that
estimated frequencies are not
available for <code>method="MM"</code>, <code>method="CS"</code> and <code>method="NSB"</code>. In these
instances a vector containing NAs is returned.
</p>


<h3>Value</h3>

<p><code>entropy</code> returns an estimate of the Shannon entropy. 
</p>
<p><code>freqs</code> returns a vector with estimated bin frequencies (if available).
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+entropy-package">entropy-package</a></code>, <code><a href="#topic+discretize">discretize</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load entropy library 
library("entropy")

# observed counts for each bin
y = c(4, 2, 3, 0, 2, 4, 0, 0, 2, 1, 1)  

entropy(y, method="ML")
entropy(y, method="MM")
entropy(y, method="Jeffreys")
entropy(y, method="Laplace")
entropy(y, method="SG")
entropy(y, method="minimax")
entropy(y, method="CS")
#entropy(y, method="NSB")
entropy(y, method="shrink")
</code></pre>

<hr>
<h2 id='entropy-internal'>Internal entropy functions</h2><span id='topic+nsbsave'></span><span id='topic+nsbload'></span><span id='topic+get.lambda.shrink'></span><span id='topic+pvt.chisq.pval'></span>

<h3>Description</h3>

<p>Internal entropy functions.
</p>


<h3>Note</h3>

<p>These are not to be called by the user (or in some cases are just
waiting for proper documentation to be written).
</p>

<hr>
<h2 id='entropy.ChaoShen'>Chao-Shen Entropy Estimator</h2><span id='topic+entropy.ChaoShen'></span>

<h3>Description</h3>

<p><code>entropy.ChaoShen</code> estimates the Shannon entropy H of the random variable Y
from the corresponding observed counts <code>y</code> using the method of Chao and Shen (2003).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>entropy.ChaoShen(y, unit=c("log", "log2", "log10"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="entropy.ChaoShen_+3A_y">y</code></td>
<td>
<p>vector of counts.</p>
</td></tr>
<tr><td><code id="entropy.ChaoShen_+3A_unit">unit</code></td>
<td>
<p>the unit in which entropy is measured. 
The default is &quot;nats&quot; (natural units). For 
computing entropy in &quot;bits&quot; set <code>unit="log2"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Chao-Shen entropy estimator (2003) is a  Horvitz-Thompson (1952) 
estimator applied to the problem of entropy estimation, 
with additional coverage correction as proposed by Good (1953). 
</p>
<p>Note that the Chao-Shen estimator is not a plug-in estimator, hence there
are no explicit underlying bin frequencies.
</p>


<h3>Value</h3>

<p><code>entropy.ChaoShen</code> returns an estimate of the Shannon entropy. 
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>References</h3>

<p>Chao, A., and T.-J. Shen. 2003.  Nonparametric estimation of Shannon's
index of diversity when there are unseen species in sample.
Environ. Ecol. Stat. <b>10</b>:429-443.
</p>
<p>Good, I. J. 1953. The population frequencies of species and the estimation of
population parameters. Biometrika <b>40</b>:237-264.
</p>
<p>Horvitz, D.G., and D. J. Thompson. 1952. A generalization of sampling without
replacement from a finite universe. J. Am. Stat. Assoc. <b>47</b>:663-685.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+entropy">entropy</a></code>, 
<code><a href="#topic+entropy.shrink">entropy.shrink</a></code>, 
<code><a href="#topic+entropy.Dirichlet">entropy.Dirichlet</a></code>, 
<code><a href="#topic+entropy.NSB">entropy.NSB</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load entropy library 
library("entropy")

# observed counts for each bin
y = c(4, 2, 3, 0, 2, 4, 0, 0, 2, 1, 1)  

# estimate entropy using Chao-Shen method
entropy.ChaoShen(y)

# compare to empirical estimate
entropy.empirical(y)
</code></pre>

<hr>
<h2 id='entropy.Dirichlet'>Dirichlet Prior Bayesian Estimators of Entropy, Mutual Information 
and Other Related Quantities</h2><span id='topic+freqs.Dirichlet'></span><span id='topic+entropy.Dirichlet'></span><span id='topic+KL.Dirichlet'></span><span id='topic+chi2.Dirichlet'></span><span id='topic+mi.Dirichlet'></span><span id='topic+chi2indep.Dirichlet'></span>

<h3>Description</h3>

<p><code>freqs.Dirichlet</code> computes the Bayesian estimates
of the bin frequencies using the  Dirichlet-multinomial       
pseudocount model.
</p>
<p><code>entropy.Dirichlet</code> estimates the Shannon entropy H of the random variable Y
from the corresponding observed counts <code>y</code> by plug-in of Bayesian estimates
of the bin frequencies using the  Dirichlet-multinomial       
pseudocount model.
</p>
<p><code>KL.Dirichlet</code> computes a Bayesian estimate of the Kullback-Leibler (KL) divergence 
from counts <code>y1</code> and <code>y2</code>.
</p>
<p><code>chi2.Dirichlet</code> computes a Bayesian version of the chi-squared divergence
from counts <code>y1</code> and <code>y2</code>.
</p>
<p><code>mi.Dirichlet</code> computes a Bayesian estimate of mutual information of two random variables.
</p>
<p><code>chi2indep.Dirichlet</code> computes a Bayesian version of the chi-squared divergence of 
independence from a table of counts <code>y2d</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>freqs.Dirichlet(y, a)
entropy.Dirichlet(y, a, unit=c("log", "log2", "log10"))
KL.Dirichlet(y1, y2, a1, a2, unit=c("log", "log2", "log10"))
chi2.Dirichlet(y1, y2, a1, a2, unit=c("log", "log2", "log10"))
mi.Dirichlet(y2d, a, unit=c("log", "log2", "log10"))
chi2indep.Dirichlet(y2d, a, unit=c("log", "log2", "log10"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="entropy.Dirichlet_+3A_y">y</code></td>
<td>
<p>vector of counts.</p>
</td></tr>
<tr><td><code id="entropy.Dirichlet_+3A_y1">y1</code></td>
<td>
<p>vector of counts.</p>
</td></tr>
<tr><td><code id="entropy.Dirichlet_+3A_y2">y2</code></td>
<td>
<p>vector of counts.</p>
</td></tr>
<tr><td><code id="entropy.Dirichlet_+3A_y2d">y2d</code></td>
<td>
<p>matrix of counts.</p>
</td></tr>
<tr><td><code id="entropy.Dirichlet_+3A_a">a</code></td>
<td>
<p>pseudocount per bin.</p>
</td></tr>
<tr><td><code id="entropy.Dirichlet_+3A_a1">a1</code></td>
<td>
<p>pseudocount per bin for first random variable.</p>
</td></tr>
<tr><td><code id="entropy.Dirichlet_+3A_a2">a2</code></td>
<td>
<p>pseudocount per bin for second random variable.</p>
</td></tr>
<tr><td><code id="entropy.Dirichlet_+3A_unit">unit</code></td>
<td>
<p>the unit in which entropy is measured. 
The default is &quot;nats&quot; (natural units). For 
computing entropy in &quot;bits&quot; set <code>unit="log2"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Dirichlet-multinomial pseudocount entropy estimator
is a Bayesian plug-in estimator: 
in the definition of the Shannon entropy the
bin probabilities are replaced by the respective Bayesian estimates
of the frequencies, using a model with a Dirichlet prior and a multinomial likelihood.
</p>
<p>The parameter <code>a</code> is a parameter of the Dirichlet prior, and in effect
specifies the pseudocount per bin.  Popular choices of <code>a</code> are:
</p>

<ul>
<li><p>a=0:maximum likelihood estimator (see <code><a href="#topic+entropy.empirical">entropy.empirical</a></code>)  
</p>
</li>
<li><p>a=1/2:Jeffreys' prior; Krichevsky-Trovimov (1991) entropy estimator
</p>
</li>
<li><p>a=1:Laplace's prior
</p>
</li>
<li><p>a=1/length(y):Schurmann-Grassberger (1996) entropy estimator
</p>
</li>
<li><p>a=sqrt(sum(y))/length(y):minimax prior
</p>
</li></ul>

<p>The pseudocount <code>a</code> can also be a vector so that for each bin an 
individual pseudocount is added.
</p>


<h3>Value</h3>

<p><code>freqs.Dirichlet</code> returns the Bayesian estimates of the frequencies . 
</p>
<p><code>entropy.Dirichlet</code> returns the Bayesian estimate of the Shannon entropy. 
</p>
<p><code>KL.Dirichlet</code> returns the Bayesian estimate of the KL divergence. 
</p>
<p><code>chi2.Dirichlet</code> returns the Bayesian version of the chi-squared divergence. 
</p>
<p><code>mi.Dirichlet</code> returns the Bayesian estimate of the mutual information. 
</p>
<p><code>chi2indep.Dirichlet</code> returns the Bayesian version of the chi-squared divergence of independence. 
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>References</h3>

<p>Agresti, A., and D. B. Hitchcock. 2005. Bayesian inference for categorical
data analysis. Stat. Methods. Appl. <b>14</b>:297&ndash;330.
</p>
<p>Krichevsky, R. E., and V. K. Trofimov. 1981. The performance of universal encoding.
IEEE Trans. Inf. Theory <b>27</b>: 199-207.  
</p>
<p>Schurmann, T., and P. Grassberger. 1996. Entropy estimation of symbol sequences.
Chaos <b>6</b>:41-427.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+entropy">entropy</a></code>, 
<code><a href="#topic+entropy.shrink">entropy.shrink</a></code>,
<code><a href="#topic+entropy.empirical">entropy.empirical</a></code>, 
<code><a href="#topic+entropy.plugin">entropy.plugin</a></code>,
<code><a href="#topic+mi.plugin">mi.plugin</a></code>, <code><a href="#topic+KL.plugin">KL.plugin</a></code>, <code><a href="#topic+discretize">discretize</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load entropy library 
library("entropy")


# a single variable

# observed counts for each bin
y = c(4, 2, 3, 0, 2, 4, 0, 0, 2, 1, 1)  

# Dirichlet estimate of frequencies with a=1/2
freqs.Dirichlet(y, a=1/2)

# Dirichlet estimate of entropy with a=0
entropy.Dirichlet(y, a=0)

# identical to empirical estimate
entropy.empirical(y)

# Dirichlet estimate with a=1/2 (Jeffreys' prior)
entropy.Dirichlet(y, a=1/2)

# Dirichlet estimate with a=1 (Laplace prior)
entropy.Dirichlet(y, a=1)

# Dirichlet estimate with a=1/length(y)
entropy.Dirichlet(y, a=1/length(y))

# Dirichlet estimate with a=sqrt(sum(y))/length(y)
entropy.Dirichlet(y, a=sqrt(sum(y))/length(y))


# example with two variables

# observed counts for two random variables
y1 = c(4, 2, 3, 1, 10, 4)
y2 = c(2, 3, 7, 1, 4, 3)

# Bayesian estimate of Kullback-Leibler divergence (a=1/6)
KL.Dirichlet(y1, y2, a1=1/6, a2=1/6)

# half of the corresponding chi-squared divergence
0.5*chi2.Dirichlet(y1, y2, a1=1/6, a2=1/6)


## joint distribution example

# contingency table with counts for two discrete variables
y2d = rbind( c(1,2,3), c(6,5,4) )

# Bayesian estimate of mutual information (a=1/6)
mi.Dirichlet(y2d, a=1/6)

# half of the Bayesian chi-squared divergence of independence
0.5*chi2indep.Dirichlet(y2d, a=1/6)


</code></pre>

<hr>
<h2 id='entropy.empirical'>Empirical Estimators of Entropy and Mutual Information and Related Quantities</h2><span id='topic+freqs.empirical'></span><span id='topic+entropy.empirical'></span><span id='topic+KL.empirical'></span><span id='topic+chi2.empirical'></span><span id='topic+mi.empirical'></span><span id='topic+chi2indep.empirical'></span>

<h3>Description</h3>

<p><code>freqs.empirical</code> computes the empirical frequencies from counts <code>y</code>.
</p>
<p><code>entropy.empirical</code> estimates the Shannon entropy H 
of the random variable Y from the corresponding observed counts <code>y</code>
by plug-in of the empirical frequencies.
</p>
<p><code>KL.empirical</code> computes the empirical Kullback-Leibler (KL) divergence 
from counts <code>y1</code> and <code>y2</code>.
</p>
<p><code>chi2.empirical</code> computes the empirical chi-squared divergence
from counts <code>y1</code> and <code>y2</code>.
</p>
<p><code>mi.empirical</code> computes the empirical mutual information from a table of counts <code>y2d</code>.
</p>
<p><code>chi2indep.empirical</code> computes the empirical chi-squared divergence of independence
from a table of counts <code>y2d</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>freqs.empirical(y)
entropy.empirical(y, unit=c("log", "log2", "log10"))
KL.empirical(y1, y2, unit=c("log", "log2", "log10"))
chi2.empirical(y1, y2, unit=c("log", "log2", "log10"))
mi.empirical(y2d, unit=c("log", "log2", "log10"))
chi2indep.empirical(y2d, unit=c("log", "log2", "log10"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="entropy.empirical_+3A_y">y</code></td>
<td>
<p>vector of counts.</p>
</td></tr>
<tr><td><code id="entropy.empirical_+3A_y1">y1</code></td>
<td>
<p>vector of counts.</p>
</td></tr>
<tr><td><code id="entropy.empirical_+3A_y2">y2</code></td>
<td>
<p>vector of counts.</p>
</td></tr>
<tr><td><code id="entropy.empirical_+3A_y2d">y2d</code></td>
<td>
<p>matrix of counts.</p>
</td></tr>
<tr><td><code id="entropy.empirical_+3A_unit">unit</code></td>
<td>
<p>the unit in which entropy is measured. 
The default is &quot;nats&quot; (natural units). For 
computing entropy in &quot;bits&quot; set <code>unit="log2"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The empirical entropy estimator is a plug-in estimator: 
in the definition of the Shannon entropy the
bin probabilities are replaced by the respective empirical frequencies.
</p>
<p>The empirical entropy estimator is the maximum likelihood estimator.
If there are many zero counts and the sample size is small
it is very inefficient and also strongly biased.
</p>


<h3>Value</h3>

<p><code>freqs.empirical</code> returns the empirical frequencies.
</p>
<p><code>entropy.empirical</code> returns an estimate of the Shannon entropy. 
</p>
<p><code>KL.empirical</code> returns an estimate of the KL divergence. 
</p>
<p><code>chi2.empirical</code> returns the empirical chi-squared divergence. 
</p>
<p><code>mi.empirical</code> returns an estimate of the mutual information. 
</p>
<p><code>chi2indep.empirical</code> returns the empirical chi-squared divergence of independence. 
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+entropy">entropy</a></code>, <code><a href="#topic+entropy.plugin">entropy.plugin</a></code>, <code><a href="#topic+KL.plugin">KL.plugin</a></code>,
<code><a href="#topic+chi2.plugin">chi2.plugin</a></code>, <code><a href="#topic+mi.plugin">mi.plugin</a></code>, <code><a href="#topic+chi2indep.plugin">chi2indep.plugin</a></code>,
<code><a href="#topic+Gstat">Gstat</a></code>, <code><a href="#topic+Gstatindep">Gstatindep</a></code>, <code><a href="#topic+chi2stat">chi2stat</a></code>, 
<code><a href="#topic+chi2statindep">chi2statindep</a></code>, <code><a href="#topic+discretize">discretize</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load entropy library 
library("entropy")


## a single variable: entropy

# observed counts for each bin
y = c(4, 2, 3, 0, 2, 4, 0, 0, 2, 1, 1)  

# empirical frequencies
freqs.empirical(y)

# empirical estimate of entropy
entropy.empirical(y)


## examples with two variables: KL and chi-squared divergence

# observed counts for first random variables (observed)
y1 = c(4, 2, 3, 1, 6, 4)
n = sum(y1) # 20

# counts for the second random variable (expected)
freqs.expected = c(0.10, 0.15, 0.35, 0.05, 0.20, 0.15)
y2 = n*freqs.expected

# empirical Kullback-Leibler divergence
KL.div = KL.empirical(y1, y2)
KL.div

# empirical chi-squared divergence
cs.div = chi2.empirical(y1, y2)
cs.div 
0.5*cs.div  # approximates KL.div

## note: see also Gstat and chi2stat


## joint distribution of two discrete random variables

# contingency table with counts for two discrete variables
y.mat = matrix(c(4, 5, 1, 2, 4, 4), ncol = 2)  # 3x2 example matrix of counts
n.mat = sum(y.mat) # 20

# empirical estimate of mutual information
mi = mi.empirical(y.mat)
mi

# empirical chi-squared divergence of independence
cs.indep = chi2indep.empirical(y.mat)
cs.indep
0.5*cs.indep # approximates mi

## note: see also Gstatindep and chi2statindep

</code></pre>

<hr>
<h2 id='entropy.MillerMadow'>Miller-Madow Entropy Estimator</h2><span id='topic+entropy.MillerMadow'></span>

<h3>Description</h3>

<p><code>entropy.MillerMadow</code> estimates the Shannon entropy H of the random variable Y
from the corresponding observed counts <code>y</code> using the Miller-Madow correction
to the empirical entropy).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>entropy.MillerMadow(y, unit=c("log", "log2", "log10"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="entropy.MillerMadow_+3A_y">y</code></td>
<td>
<p>vector of counts.</p>
</td></tr>
<tr><td><code id="entropy.MillerMadow_+3A_unit">unit</code></td>
<td>
<p>the unit in which entropy is measured. 
The default is &quot;nats&quot; (natural units). For 
computing entropy in &quot;bits&quot; set <code>unit="log2"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Miller-Madow entropy estimator (1955) is the bias-corrected empirical
entropy estimate. 
</p>
<p>Note that the Miller-Madow estimator is not a plug-in estimator, hence there
are no explicit underlying bin frequencies.
</p>


<h3>Value</h3>

<p><code>entropy.MillerMadow</code> returns an estimate of the Shannon entropy. 
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>References</h3>

<p>Miller, G.  1955. Note on the bias of information estimates. 
Info. Theory Psychol. Prob. Methods  <b>II-B</b>:95-100.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+entropy.empirical">entropy.empirical</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'># load entropy library 
library("entropy")

# observed counts for each bin
y = c(4, 2, 3, 0, 2, 4, 0, 0, 2, 1, 1)  

# estimate entropy using Miller-Madow method
entropy.MillerMadow(y)

# compare to empirical estimate
entropy.empirical(y)
</code></pre>

<hr>
<h2 id='entropy.NSB'>R Interface to NSB Entropy Estimator</h2><span id='topic+entropy.NSB'></span>

<h3>Description</h3>

<p><code>entropy.NSB</code> estimates the Shannon entropy H of the random variable Y
from the corresponding observed counts <code>y</code> using the method 
of Nemenman, Shafee and Bialek (2002).
</p>
<p>Note that this function is an R interface to the &quot;nsb-entropy&quot; program. 
Hence, this needs to be installed separately from <a href="http://nsb-entropy.sourceforge.net/">http://nsb-entropy.sourceforge.net/</a>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>entropy.NSB(y, unit=c("log", "log2", "log10"), CMD="nsb-entropy")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="entropy.NSB_+3A_y">y</code></td>
<td>
<p>vector of counts.</p>
</td></tr>
<tr><td><code id="entropy.NSB_+3A_unit">unit</code></td>
<td>
<p>the unit in which entropy is measured. 
The default is &quot;nats&quot; (natural units). For 
computing entropy in &quot;bits&quot; set <code>unit="log2"</code>.</p>
</td></tr>
<tr><td><code id="entropy.NSB_+3A_cmd">CMD</code></td>
<td>
<p>path to the &quot;nsb-entropy&quot; executable.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The NSB estimator is due to Nemenman, Shafee and Bialek (2002).
It is a Dirichlet-multinomial entropy estimator, with a hierarchical prior
over the Dirichlet pseudocount parameters.
</p>
<p>Note that the NSB estimator is not a plug-in estimator, hence there
are no explicit underlying bin frequencies.
</p>


<h3>Value</h3>

<p><code>entropy.NSB</code> returns an estimate of the Shannon entropy. 
</p>


<h3>Author(s)</h3>

<p>Jean Hausser.
</p>


<h3>References</h3>

<p>Nemenman, I., F. Shafee, and W. Bialek. 2002. Entropy and inference, revisited.
In: Dietterich, T., S. Becker, Z. Gharamani, eds. Advances in Neural
Information Processing Systems 14: 471-478. Cambridge (Massachusetts):
MIT Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+entropy">entropy</a></code>, <code><a href="#topic+entropy.shrink">entropy.shrink</a></code>,
<code><a href="#topic+entropy.Dirichlet">entropy.Dirichlet</a></code>, 
<code><a href="#topic+entropy.ChaoShen">entropy.ChaoShen</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load entropy library 
library("entropy")

# observed counts for each bin
y = c(4, 2, 3, 0, 2, 4, 0, 0, 2, 1, 1)  

## Not run: 
# estimate entropy using the NSB method
entropy.NSB(y) # 2.187774

## End(Not run)

# compare to empirical estimate
entropy.empirical(y)
</code></pre>

<hr>
<h2 id='entropy.plugin'>Plug-In Entropy Estimator</h2><span id='topic+entropy.plugin'></span>

<h3>Description</h3>

<p><code>entropy.plugin</code> computes the Shannon entropy H 
of a discrete random variable with the specified frequencies (probability mass function).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>entropy.plugin(freqs, unit=c("log", "log2", "log10"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="entropy.plugin_+3A_freqs">freqs</code></td>
<td>
<p>frequencies (probability mass function).</p>
</td></tr>
<tr><td><code id="entropy.plugin_+3A_unit">unit</code></td>
<td>
<p>the unit in which entropy is measured. 
The default is &quot;nats&quot; (natural units). For 
computing entropy in &quot;bits&quot; set <code>unit="log2"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Shannon entropy of a discrete random variable is 
defined as <code class="reqn">H = -\sum_k p(k) \log( p(k) )</code>, where <code class="reqn">p</code> is its probability mass function.
</p>


<h3>Value</h3>

<p><code>entropy.plugin</code> returns the Shannon entropy. 
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+entropy">entropy</a></code>, <code><a href="#topic+entropy.empirical">entropy.empirical</a></code>, <code><a href="#topic+entropy.shrink">entropy.shrink</a></code>, 
<code><a href="#topic+mi.plugin">mi.plugin</a></code>, <code><a href="#topic+KL.plugin">KL.plugin</a></code>, <code><a href="#topic+discretize">discretize</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load entropy library 
library("entropy")

# some frequencies
freqs = c(0.2, 0.1, 0.15, 0.05, 0, 0.3, 0.2)  

# and corresponding entropy
entropy.plugin(freqs)
</code></pre>

<hr>
<h2 id='entropy.shrink'>Shrinkage Estimators of Entropy, Mutual Information and Related Quantities</h2><span id='topic+freqs.shrink'></span><span id='topic+entropy.shrink'></span><span id='topic+KL.shrink'></span><span id='topic+chi2.shrink'></span><span id='topic+mi.shrink'></span><span id='topic+chi2indep.shrink'></span>

<h3>Description</h3>

<p><code>freq.shrink</code> estimates the bin frequencies from the counts <code>y</code>
using a James-Stein-type shrinkage estimator, where the shrinkage target is the uniform distribution.
</p>
<p><code>entropy.shrink</code> estimates the Shannon entropy H of the random variable Y
from the corresponding observed counts <code>y</code> by plug-in of shrinkage estimate
of the bin frequencies.
</p>
<p><code>KL.shrink</code> computes a shrinkage estimate of the Kullback-Leibler (KL) divergence 
from counts <code>y1</code> and <code>y2</code>.
</p>
<p><code>chi2.shrink</code> computes a shrinkage version of the chi-squared divergence
from counts <code>y1</code> and <code>y2</code>.
</p>
<p><code>mi.shrink</code> estimates a shrinkage estimate of mutual information of two random variables.
</p>
<p><code>chi2indep.shrink</code> computes a shrinkage version of the chi-squared divergence of independence
from a table of counts <code>y2d</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>freqs.shrink(y, lambda.freqs, verbose=TRUE)
entropy.shrink(y, lambda.freqs, unit=c("log", "log2", "log10"), verbose=TRUE)
KL.shrink(y1, y2, lambda.freqs1, lambda.freqs2, unit=c("log", "log2", "log10"),
            verbose=TRUE)
chi2.shrink(y1, y2, lambda.freqs1, lambda.freqs2, unit=c("log", "log2", "log10"),
            verbose=TRUE)
mi.shrink(y2d, lambda.freqs, unit=c("log", "log2", "log10"), verbose=TRUE)
chi2indep.shrink(y2d, lambda.freqs, unit=c("log", "log2", "log10"), verbose=TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="entropy.shrink_+3A_y">y</code></td>
<td>
<p>vector of counts.</p>
</td></tr>
<tr><td><code id="entropy.shrink_+3A_y1">y1</code></td>
<td>
<p>vector of counts.</p>
</td></tr>
<tr><td><code id="entropy.shrink_+3A_y2">y2</code></td>
<td>
<p>vector of counts.</p>
</td></tr>
<tr><td><code id="entropy.shrink_+3A_y2d">y2d</code></td>
<td>
<p>matrix of counts.</p>
</td></tr>
<tr><td><code id="entropy.shrink_+3A_unit">unit</code></td>
<td>
<p>the unit in which entropy is measured. 
The default is &quot;nats&quot; (natural units). For 
computing entropy in &quot;bits&quot; set <code>unit="log2"</code>.</p>
</td></tr>
<tr><td><code id="entropy.shrink_+3A_lambda.freqs">lambda.freqs</code></td>
<td>
<p>shrinkage intensity.  If not specified (default) it is estimated in a James-Stein-type fashion.</p>
</td></tr>
<tr><td><code id="entropy.shrink_+3A_lambda.freqs1">lambda.freqs1</code></td>
<td>
<p>shrinkage intensity for first random variable.  If not specified (default) it is estimated in a James-Stein-type fashion.</p>
</td></tr>
<tr><td><code id="entropy.shrink_+3A_lambda.freqs2">lambda.freqs2</code></td>
<td>
<p>shrinkage intensity for second random variable.  If not specified (default) it is estimated in a James-Stein-type fashion.</p>
</td></tr>
<tr><td><code id="entropy.shrink_+3A_verbose">verbose</code></td>
<td>
<p>report shrinkage intensity.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The shrinkage estimator is a James-Stein-type estimator.  It is essentially
a  <code><a href="#topic+entropy.Dirichlet">entropy.Dirichlet</a></code> estimator, where the pseudocount is
estimated from the data.
</p>
<p>For details see Hausser and Strimmer (2009).
</p>


<h3>Value</h3>

<p><code>freqs.shrink</code> returns a shrinkage estimate of the frequencies.
</p>
<p><code>entropy.shrink</code> returns a shrinkage estimate of the Shannon entropy. 
</p>
<p><code>KL.shrink</code> returns a shrinkage estimate of the KL divergence. 
</p>
<p><code>chi2.shrink</code> returns a shrinkage version of the chi-squared divergence. 
</p>
<p><code>mi.shrink</code> returns a shrinkage estimate of the mutual information. 
</p>
<p><code>chi2indep.shrink</code> returns a shrinkage version of the chi-squared divergence of independence. 
</p>
<p>In all instances the estimated shrinkage intensity is attached to the returned
value as attribute <code>lambda.freqs</code>.
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>References</h3>

<p>Hausser, J., and K. Strimmer. 2009.  Entropy inference and the James-Stein
estimator, with application to nonlinear gene association networks. 
J. Mach. Learn. Res. <b>10</b>: 1469-1484.  Available online from
<a href="https://jmlr.csail.mit.edu/papers/v10/hausser09a.html">https://jmlr.csail.mit.edu/papers/v10/hausser09a.html</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+entropy">entropy</a></code>, <code><a href="#topic+entropy.Dirichlet">entropy.Dirichlet</a></code>, 
<code><a href="#topic+entropy.plugin">entropy.plugin</a></code>,  
<code><a href="#topic+KL.plugin">KL.plugin</a></code>, <code><a href="#topic+mi.plugin">mi.plugin</a></code>, <code><a href="#topic+discretize">discretize</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load entropy library 
library("entropy")

# a single variable

# observed counts for each bin
y = c(4, 2, 3, 0, 2, 4, 0, 0, 2, 1, 1)  

# shrinkage estimate of frequencies
freqs.shrink(y)

# shrinkage estimate of entropy
entropy.shrink(y)


# example with two variables

# observed counts for two random variables
y1 = c(4, 2, 3, 1, 10, 4)
y2 = c(2, 3, 7, 1, 4, 3)

# shrinkage estimate of Kullback-Leibler divergence
KL.shrink(y1, y2)

# half of the shrinkage chi-squared divergence
0.5*chi2.shrink(y1, y2)


## joint distribution example

# contingency table with counts for two discrete variables
y2d = rbind( c(1,2,3), c(6,5,4) )

# shrinkage estimate of mutual information
mi.shrink(y2d)

# half of the shrinkage chi-squared divergence of independence
0.5*chi2indep.shrink(y2d)


</code></pre>

<hr>
<h2 id='Gstat'>G Statistic and Chi-Squared Statistic</h2><span id='topic+Gstat'></span><span id='topic+Gstatindep'></span><span id='topic+chi2stat'></span><span id='topic+chi2statindep'></span>

<h3>Description</h3>

<p><code>Gstat</code> computes the G statistic. 
</p>
<p><code>chi2stat</code> computes the Pearson chi-squared statistic. 
</p>
<p><code>Gstatindep</code> computes the G statistic between the empirical observed joint distribution and the product distribution obtained from its marginals.
</p>
<p><code>chi2statindep</code> computes the Pearson chi-squared statistic of independence.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Gstat(y, freqs, unit=c("log", "log2", "log10"))
chi2stat(y, freqs, unit=c("log", "log2", "log10"))
Gstatindep(y2d, unit=c("log", "log2", "log10"))
chi2statindep(y2d, unit=c("log", "log2", "log10"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Gstat_+3A_y">y</code></td>
<td>
<p>observed vector of counts.</p>
</td></tr>
<tr><td><code id="Gstat_+3A_freqs">freqs</code></td>
<td>
<p>vector of expected frequencies (probability mass function). Alternatively, counts may be provided.</p>
</td></tr>  
<tr><td><code id="Gstat_+3A_y2d">y2d</code></td>
<td>
<p>matrix of counts.</p>
</td></tr>
<tr><td><code id="Gstat_+3A_unit">unit</code></td>
<td>
<p>the unit in which entropy is measured. 
The default is &quot;nats&quot; (natural units). For 
computing entropy in &quot;bits&quot; set <code>unit="log2"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The observed counts in <code>y</code> and <code>y2d</code> are used to determine the total sample size.
</p>
<p>The G statistic equals two times the sample size times the KL divergence between
empirical observed frequencies and expected frequencies. 
</p>
<p>The Pearson chi-squared statistic equals sample size times chi-squared divergence
between empirical observed frequencies and expected frequencies.   It is a quadratic
approximation of the G statistic.
</p>
<p>The G statistic between the empirical observed joint distribution and the product
distribution obtained from its marginals is equal to  two times the sample size times mutual
information.  
</p>
<p>The Pearson chi-squared statistic of independence equals the Pearson chi-squared statistic 
between the empirical observed joint distribution and the product distribution obtained from 
its marginals.  It is a quadratic approximation of the corresponding G statistic.
</p>
<p>The G statistic and the Pearson chi-squared statistic are asymptotically chi-squared distributed 
which allows to compute corresponding p-values.
</p>


<h3>Value</h3>

<p>A list containing the test statistic <code>stat</code>, the degree of freedom <code>df</code> used to calculate the
p-value <code>pval</code>.
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+KL.plugin">KL.plugin</a></code>,
<code><a href="#topic+chi2.plugin">chi2.plugin</a></code>, <code><a href="#topic+mi.plugin">mi.plugin</a></code>, <code><a href="#topic+chi2indep.plugin">chi2indep.plugin</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load entropy library 
library("entropy")

## one discrete random variable

# observed counts in each class
y = c(4, 2, 3, 1, 6, 4)
n = sum(y) # 20

# expected frequencies and counts
freqs.expected = c(0.10, 0.15, 0.35, 0.05, 0.20, 0.15)
y.expected = n*freqs.expected


# G statistic (with p-value) 
Gstat(y, freqs.expected) # from expected frequencies
Gstat(y, y.expected) # alternatively from expected counts

# G statistic computed from empirical KL divergence
2*n*KL.empirical(y, y.expected)


## Pearson chi-squared statistic (with p-value) 
# this can be viewed an approximation of the G statistic
chi2stat(y, freqs.expected) # from expected frequencies
chi2stat(y, y.expected) # alternatively from expected counts

# computed from empirical chi-squared divergence
n*chi2.empirical(y, y.expected)

# compare with built-in function
chisq.test(y, p = freqs.expected) 


## joint distribution of two discrete random variables

# contingency table with counts
y.mat = matrix(c(4, 5, 1, 2, 4, 4), ncol = 2)  # 3x2 example matrix of counts
n.mat = sum(y.mat) # 20


# G statistic between empirical observed joint distribution and product distribution
Gstatindep( y.mat )

# computed from empirical mutual information
2*n.mat*mi.empirical(y.mat)


# Pearson chi-squared statistic of independence
chi2statindep( y.mat )

# computed from empirical chi-square divergence
n.mat*chi2indep.empirical(y.mat)

# compare with built-in function
chisq.test(y.mat) 

</code></pre>

<hr>
<h2 id='KL.plugin'>Plug-In Estimator of the Kullback-Leibler divergence and of the Chi-Squared Divergence</h2><span id='topic+KL.plugin'></span><span id='topic+chi2.plugin'></span>

<h3>Description</h3>

<p><code>KL.plugin</code> computes the Kullback-Leiber (KL) divergence between two discrete random variables <code class="reqn">x_1</code> and <code class="reqn">x_2</code>.  The corresponding probability mass functions are given by <code>freqs1</code> and <code>freqs2</code>. Note that the expectation is taken with regard to <code class="reqn">x_1</code> using <code>freqs1</code>.
</p>
<p><code>chi2.plugin</code> computes the chi-squared divergence between two discrete random variables <code class="reqn">x_1</code> and <code class="reqn">x_2</code> with <code>freqs1</code> and <code>freqs2</code> as corresponding probability mass functions.  Note that the denominator contains <code>freqs2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KL.plugin(freqs1, freqs2, unit=c("log", "log2", "log10"))
chi2.plugin(freqs1, freqs2, unit=c("log", "log2", "log10"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="KL.plugin_+3A_freqs1">freqs1</code></td>
<td>
<p>frequencies (probability mass function) for variable <code class="reqn">x_1</code>.</p>
</td></tr>
<tr><td><code id="KL.plugin_+3A_freqs2">freqs2</code></td>
<td>
<p>frequencies (probability mass function) for variable <code class="reqn">x_2</code>.</p>
</td></tr>
<tr><td><code id="KL.plugin_+3A_unit">unit</code></td>
<td>
<p>the unit in which entropy is measured. 
The default is &quot;nats&quot; (natural units). For 
computing entropy in &quot;bits&quot; set <code>unit="log2"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Kullback-Leibler divergence between the two discrete variables <code class="reqn">x_1</code>
to  <code class="reqn">x_2</code> is <code class="reqn"> \sum_k p_1(k) \log (p_1(k)/p_2(k)) </code>  where <code class="reqn">p_1</code> and <code class="reqn">p_2</code> are the probability mass functions of <code class="reqn">x_1</code> and <code class="reqn">x_2</code>, respectively, and <code class="reqn">k</code> is 
the index for the classes.
</p>
<p>The chi-squared divergence is given by <code class="reqn"> \sum_k (p_1(k)-p_2(k))^2/p_2(k) </code>.
</p>
<p>Note that both the KL divergence and the chi-squared divergence are not symmetric
in  <code class="reqn">x_1</code> and <code class="reqn">x_2</code>.    The chi-squared divergence can be derived as a 
quadratic approximation of twice the KL divergence.
</p>


<h3>Value</h3>

<p><code>KL.plugin</code> returns the KL divergence.
</p>
<p><code>chi2.plugin</code> returns the chi-squared divergence. 
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+KL.Dirichlet">KL.Dirichlet</a></code>, <code><a href="#topic+KL.shrink">KL.shrink</a></code>, <code><a href="#topic+KL.empirical">KL.empirical</a></code>, <code><a href="#topic+mi.plugin">mi.plugin</a></code>, <code><a href="#topic+discretize2d">discretize2d</a></code>. </p>


<h3>Examples</h3>

<pre><code class='language-R'># load entropy library 
library("entropy")

# probabilities for two random variables
freqs1 = c(1/5, 1/5, 3/5)
freqs2 = c(1/10, 4/10, 1/2) 

# KL divergence between x1 to x2
KL.plugin(freqs1, freqs2)

# and corresponding (half) chi-squared divergence
0.5*chi2.plugin(freqs1, freqs2)

## relationship to Pearson chi-squared statistic

# Pearson chi-squared statistic and p-value
n = 30 # sample size (observed counts)
chisq.test(n*freqs1, p = freqs2) # built-in function

# Pearson chi-squared statistic from Pearson divergence
pcs.stat = n*chi2.plugin(freqs1, freqs2) # note factor n
pcs.stat

# and p-value
df = length(freqs1)-1 # degrees of freedom
pcs.pval = 1-pchisq(pcs.stat, df)
pcs.pval
</code></pre>

<hr>
<h2 id='mi.plugin'>Plug-In Estimator of Mutual Information and of the Chi-Squared Statistic of Independence</h2><span id='topic+mi.plugin'></span><span id='topic+chi2indep.plugin'></span>

<h3>Description</h3>

<p><code>mi.plugin</code> computes the mutual information 
of two discrete random variables from the specified joint probability mass function.
</p>
<p><code>chi2indep.plugin</code> computes the chi-squared divergence of independence.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mi.plugin(freqs2d, unit=c("log", "log2", "log10"))
chi2indep.plugin(freqs2d, unit=c("log", "log2", "log10"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mi.plugin_+3A_freqs2d">freqs2d</code></td>
<td>
<p>matrix of joint bin frequencies (joint probability mass function).</p>
</td></tr>
<tr><td><code id="mi.plugin_+3A_unit">unit</code></td>
<td>
<p>the unit in which entropy is measured. 
The default is &quot;nats&quot; (natural units). For 
computing entropy in &quot;bits&quot; set <code>unit="log2"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The mutual information of two random variables <code class="reqn">X</code> and <code class="reqn">Y</code>
is the Kullback-Leibler divergence between the joint density/probability
mass function and the product independence density of the marginals.
</p>
<p>It can also defined using entropy as <code class="reqn">MI = H(X) + H(Y) - H(X, Y)</code>. 
</p>
<p>Similarly, the chi-squared divergence of independence is the chi-squared divergence
between the joint density and the product density. It is a second-order 
approximation of twice the mutual information.
</p>


<h3>Value</h3>

<p><code>mi.plugin</code> returns the mutual information.
</p>
<p><code>chi2indep.plugin</code> returns the chi-squared divergence of independence. 
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mi.Dirichlet">mi.Dirichlet</a></code>, <code><a href="#topic+mi.shrink">mi.shrink</a></code>, <code><a href="#topic+mi.empirical">mi.empirical</a></code>, <code><a href="#topic+KL.plugin">KL.plugin</a></code>, <code><a href="#topic+discretize2d">discretize2d</a></code>. </p>


<h3>Examples</h3>

<pre><code class='language-R'># load entropy library 
library("entropy")

# joint distribution of two discrete variables
freqs2d = rbind( c(0.2, 0.1, 0.15), c(0.1, 0.2, 0.25) )  

# corresponding mutual information
mi.plugin(freqs2d)

# MI computed via entropy
H1 = entropy.plugin(rowSums(freqs2d))
H2 = entropy.plugin(colSums(freqs2d))
H12 = entropy.plugin(freqs2d)
H1+H2-H12

# and corresponding (half) chi-squared divergence of independence
0.5*chi2indep.plugin(freqs2d)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
