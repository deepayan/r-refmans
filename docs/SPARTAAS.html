<!DOCTYPE html><html><head><title>Help for package SPARTAAS</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {SPARTAAS}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#SPARTAAS-package'>
<p>SPARTAAS</p></a></li>
<li><a href='#adjacency'>
<p>Dissimilarity matrix based on connectivity information.</p></a></li>
<li><a href='#arrondi'>
<p>Returns the rounded value.</p></a></li>
<li><a href='#CAdist'>
<p>Distance matrix based on correspondence analysis results</p></a></li>
<li><a href='#cerardat'>
<p>cerardat</p></a></li>
<li><a href='#cerardat_app'>
<p>Launch the shiny application.</p></a></li>
<li><a href='#cerardat_estim_nf'>
<p>Number of axes to keep</p></a></li>
<li><a href='#datacancer'>
<p>Data set of cancerology.</p></a></li>
<li><a href='#dataceram'>
<p>Data set of archeology</p></a></li>
<li><a href='#datacerardat'>
<p>Data set of archaeology.</p></a></li>
<li><a href='#datangkor'>
<p>Data set of archeology</p></a></li>
<li><a href='#datarcheo'>
<p>Data set of archeology</p></a></li>
<li><a href='#extract_results'>
<p>Generates all plots (in jpeg format) in a subfolder</p></a></li>
<li><a href='#hclust'>
<p>Hierarchical clustering of up to two datasets (Compromised clustering).</p></a></li>
<li><a href='#hclustcompro'><p>hclustcompro</p></a></li>
<li><a href='#hclustcompro_app'>
<p>Launch the shiny application.</p></a></li>
<li><a href='#hclustcompro_select_alpha'>
<p>Estimate the optimal value(s) of the <code class="reqn">\alpha</code> parameter.</p></a></li>
<li><a href='#hclustcompro_subdivide'>
<p>Subdivide a cluster after running hclustcompro.</p></a></li>
<li><a href='#mapclust'>
<p>Divise hierarchical Clustering using Spatialpatches algorithm.</p></a></li>
<li><a href='#mapclust_app'>
<p>Shiny application for Mapclust method</p></a></li>
<li><a href='#mapclust_cut_tree'>
<p>Function to cut the dendrogram for a new height (distance limit) or a new number of clusters and map the new partition</p></a></li>
<li><a href='#overlap'>
<p>Temporal overlap index</p></a></li>
<li><a href='#plot.cerardat_obj'>
<p>plot cerardat model</p></a></li>
<li><a href='#serio_app'>
<p>Launch the shiny application.</p></a></li>
<li><a href='#seriograph'>
<p>Plot seriograph (B. DESACHY).</p></a></li>
<li><a href='#timerange'>
<p>Plot the time range of observations sorted by cluster.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Statistical Pattern Recognition and daTing using Archaeological
Artefacts assemblageS</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.1</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Arthur Coulon &lt;arthur-coulon@outlook.fr&gt;</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://spartaas.gitpages.huma-num.fr/r-package/">https://spartaas.gitpages.huma-num.fr/r-package/</a></td>
</tr>
<tr>
<td>Description:</td>
<td>Statistical pattern recognition and dating using archaeological artefacts assemblages.
    Package of statistical tools for archaeology.
    hclustcompro(perioclust): Bellanger Lise, Coulon Arthur, Husi Philippe (2021, ISBN:978-3-030-60103-4).
    mapclust: Bellanger Lise, Coulon Arthur, Husi Philippe (2021) &lt;<a href="https://doi.org/10.1016%2Fj.jas.2021.105431">doi:10.1016/j.jas.2021.105431</a>&gt;.
    seriograph: Desachy Bruno (2004) &lt;<a href="https://doi.org/10.3406%2Fpica.2004.2396">doi:10.3406/pica.2004.2396</a>&gt;.
    cerardat: Bellanger Lise, Husi Philippe (2012) &lt;<a href="https://doi.org/10.1016%2Fj.jas.2011.06.031">doi:10.1016/j.jas.2011.06.031</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>FactoMineR,
grDevices,dplyr,tidyr,ggplot2,plotly,stringr,colorspace,crayon,
shiny,shinydashboard,shinyjs,shinyjqui,fpc,ggdendro,htmltools,
rstudioapi,htmlwidgets,shinythemes,explor,shinyWidgets,scatterD3,
sp,ks,foreign,grid,cluster,leaflet,ape,mapview,
MASS,ade4,lmtest,nor1mix,shinycssloaders,scales,fastcluster</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-06-06 12:01:55 UTC; arcoulon</td>
</tr>
<tr>
<td>Author:</td>
<td>Arthur Coulon [aut, cre],
  Lise Bellanger [aut],
  Philippe Husi [aut],
  Bruno Desachy [ctb],
  Benjamin Martineau [ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-06-06 13:10:09 UTC</td>
</tr>
</table>
<hr>
<h2 id='SPARTAAS-package'>
SPARTAAS
</h2><span id='topic+SPARTAAS-package'></span><span id='topic+SPARTAAS'></span>

<h3>Description</h3>

<p><strong>S</strong>tatistical <strong>Pa</strong>ttern <strong>R</strong>ecognition and da<strong>T</strong>ing using <strong>A</strong>rcheological <strong>A</strong>rtefacts assemblage<strong>S</strong>.
</p>


<h3>Details</h3>

<p>Statistical pattern recognition and dating using archaeological artefacts assemblages.
    Package of statistical tools for archaeology.
    hclustcompro(perioclust): Bellanger Lise, Coulon Arthur, Husi Philippe (2021, ISBN:978-3-030-60103-4).
    mapclust: Bellanger Lise, Coulon Arthur, Husi Philippe (2021) &lt;doi:10.1016/j.jas.2021.105431&gt;.
    seriograph: Desachy Bruno (2004) &lt;doi:10.3406/pica.2004.2396&gt;.
    cerardat: Bellanger Lise, Husi Philippe (2012) &lt;doi:10.1016/j.jas.2011.06.031&gt;.
</p>


<h3>Author(s)</h3>

<p>NA
</p>
<p>Maintainer: Arthur Coulon &lt;arthur-coulon@outlook.fr&gt;
</p>


<h3>References</h3>

<p>Bellanger L., Coulon A., Husi P., 2020 – Perioclust: a new Hierarchical agglomerative clustering method including temporal or spatial ordering constraints. Springer Series, Studies in Classification, Data Analysis, and Knowledge Organization. &lt;doi: 10.1007/978-3-030-60104-1&gt;
</p>
<p>Bellanger L., Husi P., Laghzali Y. (2015). Spatial statistic analysis of dating using pottery: an aid to the characterization of cultural areas in West Central France. In : Traviglia A. ed., Across Space and Time, Proceedings of the 41th International Conference on Computer Applications and Quantitative Methods in Archaeology (CAA-2013), Perth (Australie), Amsterdam University Press : 276-282.
</p>

<hr>
<h2 id='adjacency'>
Dissimilarity matrix based on connectivity information.
</h2><span id='topic+adjacency'></span>

<h3>Description</h3>

<p>From the data of a network, we build a contiguity matrix. Based on this matrix, we generate a dissimilarity matrix.
The matrix contains only 0 or 1, 1 if there is no relationship and 0 if there is a relationship.
The network object is a data frame with two columns. The first column contains the elements of the network and the second column contains a list of all other elements related to it. The list is a string consisting of the names of the elements separated by commas (see example).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adjacency(network)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adjacency_+3A_network">network</code></td>
<td>

<p>Data frame with 2 columns. The first contains all elements (nodes) and the second a string with all related elements (links).
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>D</code></td>
<td>
<p>Dissimilarity matrix based on adjacency.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>A. COULON
</p>
<p>L. BELLANGER
</p>
<p>P. HUSI
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##---- Should be DIRECTLY executable !! ----
##-- ==&gt;  Define data, use random,
##--  or do  help(data=index)  for the standard data sets.
library(SPARTAAS)
data(datangkor)

## network stratigraphic data (Network)
network &lt;- datangkor$stratigraphy

dissimilarity &lt;- adjacency(network)
dissimilarity
</code></pre>

<hr>
<h2 id='arrondi'>
Returns the rounded value.
</h2><span id='topic+arrondi'></span>

<h3>Description</h3>

<p>Always returns the upper value if the next digit is 5.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>arrondi(x, acc)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="arrondi_+3A_x">x</code></td>
<td>

<p>The number to round.
</p>
</td></tr>
<tr><td><code id="arrondi_+3A_acc">acc</code></td>
<td>

<p>Accuracy (number of digits). A negative number of digits means rounding to the power of ten, e.g. arrondi(x, digits = -2) will round to the nearest hundred.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>res</code></td>
<td>
<p>Value or vector of values rounded.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>A. COULON
</p>
<p>L. BELLANGER
</p>
<p>P. HUSI
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  library(SPARTAAS)

  x1 &lt;- c(15,25,35,45,55)
  round(x1,-1)
  arrondi(x1,-1)

  x2 &lt;- c(-15,-25,-35,-45,-55)
  round(x2,-1)
  arrondi(x2, -1)

  x3 &lt;- 1.125
  round(x3,2)
  arrondi(x3, 2)

  x4 &lt;- seq(-0.55,0.55,0.1)
  data.frame(
    val = x4,
    round = round(x4,1),
    arrondi = arrondi(x4, 1),
    equal = (arrondi(x4, 1) == round(x4,1))
  )

</code></pre>

<hr>
<h2 id='CAdist'>
Distance matrix based on correspondence analysis results</h2><span id='topic+CAdist'></span>

<h3>Description</h3>

<p>Perform a correspondence analysis on a contingency table and then return the distance matrix of the coordinates (you can choose the number of axes to use to build the distance matrix with the nCP parameter).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CAdist(df, nPC = NULL, graph = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CAdist_+3A_df">df</code></td>
<td>

<p>Data.frame, matrix or table with the data for the correspondence analysis.</p>
</td></tr>
<tr><td><code id="CAdist_+3A_npc">nPC</code></td>
<td>

<p>Number of principal components to be retained for the construction of the distance matrix. Must be between 1 and the minimum of ncol - 1 and nrow - 1. Could also be &quot;max&quot;.</p>
</td></tr>
<tr><td><code id="CAdist_+3A_graph">graph</code></td>
<td>

<p>Logical parameter for plotting the Correspondence Analysis (Axis1, Axis2).</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>D</code></td>
<td>
<p>The distance matrix</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>A. COULON
</p>
<p>L. BELLANGER
</p>
<p>P. HUSI
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##---- Should be DIRECTLY executable !! ----
##-- ==&gt;  Define data, use random,
##--	or do  help(data=index)  for the standard data sets.
library(SPARTAAS)
data(datangkor)

## contingency table
cont &lt;- datangkor$contingency

distance &lt;- CAdist(cont, nPC = "max")
distance

## run without printing the plot
distance &lt;- CAdist(cont, nPC = "max", graph=FALSE)
</code></pre>

<hr>
<h2 id='cerardat'>
cerardat
</h2><span id='topic+cerardat'></span>

<h3>Description</h3>

<p>The methodology is based on a statistical and visual approach using two estimated density curves to date each archaeological context. The statistical procedure required two steps, each leading to the construction of a density curve. The first allowed us to estimate a date corresponding to the terminus post quem of the context, a cursor reflecting an event dated in calendar time. This statistical tool allows the archaeologist to easily visualise and analyse chronological patterns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cerardat(df, row.sup, date, nf = NULL, confidence = 0.95, graph = T)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cerardat_+3A_df">df</code></td>
<td>

<p>The data (data.frame) is a contingency table with the observations in the rows and the technical groups in the columns.
</p>
</td></tr>
<tr><td><code id="cerardat_+3A_row.sup">row.sup</code></td>
<td>

<p>Index of supplementary rows in df (vector).
</p>
</td></tr>
<tr><td><code id="cerardat_+3A_date">date</code></td>
<td>

<p>The dates of each observation or NA (vector).
</p>
</td></tr>
<tr><td><code id="cerardat_+3A_nf">nf</code></td>
<td>

<p>an integer representing the number of axes retained in the correspondence analysis. If NULL, it is automatically chosen to keep a number corresponding to at least 60% of the inertia.
</p>
</td></tr>
<tr><td><code id="cerardat_+3A_confidence">confidence</code></td>
<td>

<p>The desired confidence interval (0.95 for 95%).
</p>
</td></tr>
<tr><td><code id="cerardat_+3A_graph">graph</code></td>
<td>

<p>logical to display the plots or not.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The corpus data is a contingency table with the observations in the rows and the technical groups in the columns. There are two types of observations: the reference corpus observations and the supplementary observations. The supplementary rows (observations) are identified by the argument 'row.sup'.
</p>
<p><strong>step 1: modelling events dated in calendar time (dateEv)</strong><br />
This step involves estimating the date of an event recorded in the ground (an archaeological context for the archaeologist) from the pottery assemblage of which it is composed, by fitting a regression model that relates a known date in calendar time, such as the date of issue of a coin, to its pottery profile. The reference corpus used to fit the regression model. We then used the previously fitted model to calculate a predicted value for contexts not included in the reference corpus, sometimes stratigraphically separated or poorly documented, with a 95% confidence interval for the predicted date.
</p>
<p>A correspondence analysis (<a href="FactoMineR.html#topic+CA">CA</a>) was carried out to summarize the information in the reference corpus data. We then kept only the first factorial axes. In this way, our contingency table becomes a reduced size table, an incomplete reconstruction of the data. This principle is used in many factor analysis techniques to reduce the number of explanatory variables in the linear regression model.
</p>
<p>After estimating the beta parameters of the model using the classical results of multiple regression analysis and checking that the model fits the data correctly, we can deduce the estimated date of an observation and also predict the date of another observation that has no coins and is therefore not dated.
</p>
<p><strong>step 2: from event time (dateEv) to accumulation time (dateAc)</strong><br />
We used the results of the first step and the properties of the CA to obtain an estimate of the date of each fabric. We could then define the archaeological time represented as dateAc, in other words the accumulation time of a context, as the weighted sum of the fabric dates; the weights being the proportions of MINVC of each fabric in the context. Assuming that the random variables dateEvj are independent, the distribution of the accumulation time of each context can be approximated by the Gaussian mixture. In this way, for each context, we obtained a plurimodal density curve representing the estimated law of accumulation time based on the mixture of normal densities (dates of each tissue). By definition, the area under the density curve has a value of 1 (i.e. 100%).
</p>
<p><strong>date</strong><br />
In order to estimate a date for the context, it is essential to refer to objects that have been dated by another source, such as coins. These contexts were selected on a very strict basis for their chronostratigraphic reliability, level of domestic occupation or enclosures with long urban stratigraphic sequences, thereby minimising any bias associated with the disparity between the date of the coin and that of the context.
</p>


<h3>Value</h3>

<table>
<tr><td><code>prediction</code></td>
<td>
<p>Estimated date for archaeological context (event: dateEV and accumulation: dateAC) with confidence interval.</p>
</td></tr>
<tr><td><code>date_gt</code></td>
<td>
<p>Estimated date for technical groups with confidence interval. (use for dateAC)</p>
</td></tr>
<tr><td><code>lm</code></td>
<td>
<p>Linear model on the components of the correspondance analysis.</p>
</td></tr>
<tr><td><code>predict_obj_row</code></td>
<td>
<p>date prediction of archaeological contexts (rows) using <a href="stats.html#topic+predict.lm">predict.lm</a>.</p>
</td></tr>
<tr><td><code>predict_obj_col</code></td>
<td>
<p>date prediction of technical groups (columns) using <a href="stats.html#topic+predict.lm">predict.lm</a>.</p>
</td></tr>
<tr><td><code>cont_gt</code></td>
<td>
<p>Contingency table of the reference corpus.</p>
</td></tr>
<tr><td><code>statistical.summary</code></td>
<td>
<p>Statistical summary of the model:<br />
Adjusted R-squared<br />
R-squared<br />
sigma (Residual standard error)<br />
The Shapiro-Wilks test is used to verify the normality of the residuals.<br />
The Durbin-Watson test checks for first order autocorrelation.<br />
The Breusch-Pagan test checks for heteroscedasticity.
</p>
</td></tr>
<tr><td><code>obs_ca_eval</code></td>
<td>
<p>Quality of row representation in the correspondence analysis.</p>
</td></tr>
<tr><td><code>check_ref</code></td>
<td>
<p>Plot of estimated dates (with confidence interval) and real dates of reference data. Only when the real date is known.</p>
</td></tr>
<tr><td><code>check_sup</code></td>
<td>
<p>Plot of estimated dates (with confidence interval) and real dates of supplementary data. Only when the real date is known.</p>
</td></tr>
<tr><td><code>Shapiro_Wilks</code></td>
<td>
<p>Summary of the Shapiro-Wilks test. see <a href="stats.html#topic+shapiro.test">shapiro.test</a>.</p>
</td></tr>
<tr><td><code>Durbin_Watson</code></td>
<td>
<p>Summary of the Durbin-Watson test. see <a href="lmtest.html#topic+dwtest">dwtest</a>.</p>
</td></tr>
<tr><td><code>Breusch_Pagan</code></td>
<td>
<p>Summary of the Breusch-Pagan test. see <a href="lmtest.html#topic+bptest">bptest</a>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>A. COULON
</p>
<p>L. BELLANGER
</p>
<p>P. HUSI
</p>


<h3>References</h3>

<p>Bellanger L. and Husi P. (2012) Statistical tool for dating and interpreting archaeological contexts using pottery. Journal of Archaeological Science, Elsevier, 39 (4), pp.777-790. doi:10.1016/j.jas.2011.06.031.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("datacerardat")

resultat = cerardat(df = datacerardat$df,
           row.sup = datacerardat$row.sup,
           date = datacerardat$date,
           nf = NULL,
           confidence = 0.95,
           graph = TRUE
        )

resultat
#The Shapiro-Wilks test is used to verify the normality of the residuals.
#The Durbin-Watson test checks for first order autocorrelation.
#The Breusch-Pagan test checks for heteroscedasticity.



#See the first plot
plot(resultat,
     which = 1,
     col1=rgb(0.93,0.23,0.23,0.5),
     col2="black",
     xlim=NULL,
     ylim=c(0,0.03)
    )

#See the first ten plots
#plot(resultat,
#     which = 1:10,
#     col1=rgb(0.93,0.23,0.23,0.5),
#     col2="black",
#     xlim=NULL,
#     ylim=c(0,0.03)
#    )

#See all plots
#plot(resultat,
#     which = NULL,
#     col1=rgb(0.93,0.23,0.23,0.5),
#     col2="black",
#     xlim=NULL,
#     ylim=c(0,0.03)
#    )

#You can extract the plots and find them in the directory :
paste0(getwd(),"/figures")
#With the 'extract_results' function
#extract_results(resultat,width=480, height=480, path="figures")

</code></pre>

<hr>
<h2 id='cerardat_app'>
Launch the shiny application.
</h2><span id='topic+cerardat_app'></span>

<h3>Description</h3>

<p>see <a href="#topic+cerardat">cerardat</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  cerardat_app()
</code></pre>


<h3>Value</h3>

<p>No return value</p>


<h3>Author(s)</h3>

<p>A. COULON
</p>
<p>L. BELLANGER
</p>
<p>P. HUSI
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  ##---- Should be DIRECTLY executable !! ----
  ##-- ==&gt;  Define data, use random,
  ##--	or do  help(data=index)  for the standard data sets.
  
    library(SPARTAAS)
    if(interactive()){
      cerardat_app()
    }
  

</code></pre>

<hr>
<h2 id='cerardat_estim_nf'>
Number of axes to keep
</h2><span id='topic+cerardat_estim_nf'></span>

<h3>Description</h3>

<p>Estimate the correct number of axes to keep in the regression model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cerardat_estim_nf(df, row.sup, date)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cerardat_estim_nf_+3A_df">df</code></td>
<td>

<p>The data (data.frame) is a contingency table with the observations in rows and the technical groups in columns.
</p>
</td></tr>
<tr><td><code id="cerardat_estim_nf_+3A_row.sup">row.sup</code></td>
<td>

<p>Index of supplementary rows in df (vector).
</p>
</td></tr>
<tr><td><code id="cerardat_estim_nf_+3A_date">date</code></td>
<td>

<p>The dates of each observation or NA (vector).
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>nf</code></td>
<td>
<p>Number of axes to keep (minimal PRESS value)</p>
</td></tr>
<tr><td><code>MSE</code></td>
<td>
<p>plot of the Mean Squared Error.</p>
</td></tr>
<tr><td><code>PRESS</code></td>
<td>
<p>plot of the PRediction Error Sum Of Squares.</p>
</td></tr>
<tr><td><code>adj.R_sq</code></td>
<td>
<p>plot of the Coefficient of determination R².</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>data frame of MSE, PRESS and R_sq values.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>A. COULON
</p>
<p>L. BELLANGER
</p>
<p>P. HUSI
</p>


<h3>References</h3>

<p>Bellanger L. and Husi P. (2012) Statistical tool for dating and interpreting archaeological contexts using pottery. Journal of Archaeological Science, Elsevier, 39 (4), pp.777-790. doi:10.1016/j.jas.2011.06.031.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(SPARTAAS)
data(datacerardat)
res = cerardat_estim_nf(datacerardat$df, datacerardat$row.sup, datacerardat$date)

#Number of axes to keep (minimal PRESS value)
res$nf

#the plots
res$MSE
res$PRESS
res$adj.R_sq
</code></pre>

<hr>
<h2 id='datacancer'>
Data set of cancerology.
</h2><span id='topic+datacancer'></span>

<h3>Description</h3>

<p>Longitude, latitude, number of thyroid cancers. The data concern two French departments (Loire-Atlantique and Vendee) between 1998 and 2012. For confidentiality reasons, the data are simulated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("datacancer")</code></pre>


<h3>Format</h3>

<p>List of two objects with 359 observations on the following 3 variables.
</p>
<p><strong>$coord (data.frame):</strong>
</p>

<dl>
<dt><code>longitude</code></dt><dd><p>a numeric vector: geographical coordinate</p>
</dd>
<dt><code>latitude</code></dt><dd><p>a numeric vector: geographical coordinate</p>
</dd>
</dl>

<p><strong>$var (vector):</strong>
</p>

<dl>
<dt><code>var</code></dt><dd><p>a numeric vector: number of thyroid cancers (simulated)</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>M. Karakachoff (IR CHU - l'institut du Thorax INSERM UMR 1087 - CNRS UMR 6291) Nantes, France
</p>
<p>F. Molinie (resp. Loire-Atlantique-Vendee cancer registry -
<a href="https://www.registre-cancers-44-85.fr/"><code>registre-des-cancers</code></a>) France
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  library(SPARTAAS)
  data(datacancer)
  str(datacancer)
  head(datacancer$coord)
  str(datacancer$var)
</code></pre>

<hr>
<h2 id='dataceram'>
Data set of archeology
</h2><span id='topic+dataceram'></span>

<h3>Description</h3>

<p>This important dataset comes from the Collective Research Project (CRP) on Medieval and Modern pottery in the Middle Loire Basin. This is a long-term project, since it began in 1996 and has already been the subject of two books on the subject (Husi dir. 2003 and 2013), as well as an online logical publication (Husi dir. 2022).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("dataceram")</code></pre>


<h3>Format</h3>

<p>List of three objects with 226 observations.
</p>

<dl>
<dt>$contingency</dt><dd><p>(data.frame) Contingency table of the quantities of 183 types of pottery sherds in the 226 sets.</p>
</dd>
<dt>$timerange</dt><dd><p>(data.frame) The first column corresponds to the identifier (sets), the second and the third to the lower and upper limits of the temporal range of the observations.</p>
</dd>
<dt>$geographic_area</dt><dd><p>Vector of the geographical area of each observation.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Husi P. (dir.) – La céramique médiévale et moderne du bassin de la Loire moyenne, chrono-typologie et transformation des aires culturelles dans la longue durée (6e —19e s.), XXe Supplément à la Revue Archéologique du Centre de la France, FERACF, Tours, 2022.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  library(SPARTAAS)
  data(dataceram)
  str(dataceram)
  str(dataceram$contingency)
  head(dataceram$timerange)
  head(dataceram$geographic_area)
</code></pre>

<hr>
<h2 id='datacerardat'>
Data set of archaeology.
</h2><span id='topic+datacerardat'></span>

<h3>Description</h3>

<p>Data sets on Tours pottery.
Contains three objects:
- Pottery sherd contingency table (data.frame 280 sets and 391 technical groups)
- The date (if known) of the sets (vector)
- Column index for supplementary  sets (vector)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("datacerardat")</code></pre>


<h3>Format</h3>

<p>A list with 3 objects.
</p>

<dl>
<dt><code>df</code></dt><dd><p>Ceramic contingency table. a integer data.frame</p>
</dd>
<dt><code>date</code></dt><dd><p>The date (if known) of the sets. a numeric or NA vector</p>
</dd>
<dt><code>col.sup</code></dt><dd><p>Column index for supplementary  sets. a numeric vector</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>data(datacerardat)
datacerardat
## maybe str(datacerardat);
</code></pre>

<hr>
<h2 id='datangkor'>
Data set of archeology
</h2><span id='topic+datangkor'></span>

<h3>Description</h3>

<p>The archaeological data come from excavations carried out at Angkor Thom (Cambodia), the capital of the Khmer Empire between the 9th and 15th centuries (Gaucher, 2004). The dataset consists of the pottery assemblages (quantities of different types of pottery sherds contained in the sets - ..$contingency) and the stratigrpahy of the sets from 3 disconnected archaeological sites (..$stratigraphy).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("datangkor")</code></pre>


<h3>Format</h3>

<p>List of two objects with 17 observations.
</p>

<dl>
<dt>$contingency</dt><dd><p>(data.frame) Contingency table of the quantities of 12 types of pottery sherds in the 17 sets.</p>
</dd>
<dt>$stratigraphy</dt><dd><p>(data.frame) Saves the stratigraphic network. The first column corresponds to the nodes (sets) and the second to the edges by listing the nodes connected to it.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Gaucher, J. (2004). Angkor Thom, une utopie réalisée ? Structuration de l’espace et modèle indien d’urbanisme dans le Cambodge ancien. Arts Asiatiques, Volume 59, pp. 58-86.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  library(SPARTAAS)
  data(datangkor)
  str(datangkor)
  str(datangkor$contingency)
  str(datangkor$stratigraphy)
</code></pre>

<hr>
<h2 id='datarcheo'>
Data set of archeology
</h2><span id='topic+datarcheo'></span>

<h3>Description</h3>

<p>Latitude, longitude, absolute difference between two dates and the name of the archaeological site. The data concern the dating of archaeological contexts in West-central France based on a large collection of medieval pottery finds. Two original statistical models are developed to estimate context dates using pottery. The  absolute difference is calculated for each context. The data are based on a collective research on medieval pottery directed by P. Husi (<a href="https://citeres.univ-tours.fr/spip45d1.html?article493"><code>"La céramique médiévale dans la vallée de la Loire moyenne"</code></a>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("datarcheo")</code></pre>


<h3>Format</h3>

<p>List of three objects with 240 observations on the following 4 variables.
</p>

<p><strong>$coord (data.frame):</strong>
</p>
<dl>
<dt><code>longitude</code></dt><dd><p>a numeric vector: geographical coordinate</p>
</dd>
<dt><code>latitude</code></dt><dd><p>a numeric vector: geographical coordinate</p>
</dd>
</dl>
<p><strong>$var (vector):</strong>
</p>
<dl>
<dt><code>regionalized_var</code></dt><dd><p>a numeric vector: difference between two dating model</p>
</dd>
</dl>
<p><strong>$label (vector):</strong>
</p>
<dl>
<dt><code>noms</code></dt><dd><p>a character vector(factor): name of archeological site</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>P. Husi IR CNRS, UMR CITERES-LAT, CNRS/Tours University, France :
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  library(SPARTAAS)
  data(datarcheo)
  str(datarcheo)
  head(datarcheo$coord)
  str(datarcheo$var)
  levels(datarcheo$label)
</code></pre>

<hr>
<h2 id='extract_results'>
Generates all plots (in jpeg format) in a subfolder
</h2><span id='topic+extract_results'></span>

<h3>Description</h3>

<p>Generates all plots (in jpeg format) in a subfolder (relative path from the working directory).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_results(cerardat, width=480, height=480, path="figures",
                  col1 = rgb(0.93,0.23,0.23,0.5), col2 = "black",
                  xlim=NULL, ylim=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_results_+3A_cerardat">cerardat</code></td>
<td>

<p>a <a href="#topic+cerardat">cerardat</a> output.
</p>
</td></tr>
<tr><td><code id="extract_results_+3A_width">width</code></td>
<td>

<p>the width of the graphics region in pixels.
</p>
</td></tr>
<tr><td><code id="extract_results_+3A_height">height</code></td>
<td>

<p>the width of the graphics region in pixels.
</p>
</td></tr>
<tr><td><code id="extract_results_+3A_path">path</code></td>
<td>

<p>directory where plots are exported. (relative path from the working directory)
</p>
</td></tr>
<tr><td><code id="extract_results_+3A_col1">col1</code></td>
<td>

<p>Color of the the Event curve.
</p>
</td></tr>
<tr><td><code id="extract_results_+3A_col2">col2</code></td>
<td>

<p>Color of the the Accumulation curve.
</p>
</td></tr>
<tr><td><code id="extract_results_+3A_xlim">xlim</code></td>
<td>

<p>Two numeric values, specifying the left limit and the right limit of the scale.
</p>
</td></tr>
<tr><td><code id="extract_results_+3A_ylim">ylim</code></td>
<td>

<p>Two numeric values, specifying the lower limit and the upper limit of the scale.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>A. COULON
</p>
<p>L. BELLANGER
</p>
<p>P. HUSI
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("datacerardat")

resultat = cerardat(df = datacerardat$df,
           row.sup = datacerardat$row.sup,
           date = datacerardat$date,
           nf = NULL,
           confidence = 0.95,
           graph = TRUE
        )
#Generates all plots (in jpeg format) in a subfolder 'figures'
#extract_results(resultat,width=480, height=480, path="figures",
#     col1=rgb(0.93,0.23,0.23,0.5),
#     col2="black",
#     xlim=NULL,
#     ylim=c(0,0.03)
#    )

#You can extract the plots and find them in the directory :
paste0(getwd(),"/figures")

</code></pre>

<hr>
<h2 id='hclust'>
Hierarchical clustering of up to two datasets (Compromised clustering).
</h2><span id='topic+hclust'></span>

<h3>Description</h3>

<p>Overload of <a href="stats.html#topic+hclust">hclust</a> for dealing with two dissimilarities matrices.
Hierarchical cluster analysis on a set of dissimilarities and methods for analyzing it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hclust(d, method = "complete", members = NULL, d2 = NULL, alpha = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hclust_+3A_d">d</code></td>
<td>

<p>a dissimilarity structure as produced by dist.
</p>
</td></tr>
<tr><td><code id="hclust_+3A_method">method</code></td>
<td>

<p>the agglomeration method to be used. This should be (an unambiguous abbreviation of) one of &quot;ward.D&quot;, &quot;ward.D2&quot;, &quot;single&quot;, &quot;complete&quot;, &quot;average&quot; (= UPGMA), &quot;mcquitty&quot; (= WPGMA), &quot;median&quot; (= WPGMC) or &quot;centroid&quot; (= UPGMC).
</p>
</td></tr>
<tr><td><code id="hclust_+3A_members">members</code></td>
<td>

<p>NULL or a vector with length size of d. See the ‘Details’ section.
</p>
</td></tr>
<tr><td><code id="hclust_+3A_d2">d2</code></td>
<td>

<p>a second dissimilarity structure as produced by dist.
</p>
</td></tr>
<tr><td><code id="hclust_+3A_alpha">alpha</code></td>
<td>

<p>The mixing parameter in order to generate the D_alpha matrix on which the classical hclust method is applied. Formula: D_alpha = alpha * d + (1-alpha) * d2.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Data fusion (parameter alpha: optimal value see <a href="#topic+hclustcompro_select_alpha">hclustcompro_select_alpha</a>). It is necessary to define the appropriate proportion for each data source. This is the first sensitive point of the method that the user has to consider. A tool is provided to help him in his decision.
</p>


<h3>Value</h3>

<p><a href="stats.html#topic+hclust">hclust</a>
</p>


<h3>Author(s)</h3>

<p>The hclust function is based on Fortran code contributed to STATLIB by F. Murtagh.
</p>
<p>A. COULON
</p>
<p>L. BELLANGER
</p>
<p>P. HUSI
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##---- Should be DIRECTLY executable !! ----
##-- ==&gt;  Define data, use random,
##--	or do  help(data=index)  for the standard data sets.

## The function is currently defined as
function (d, method = "complete", members = NULL, d2 = NULL,
    alpha = NULL)
{
    if (!is.null(d2)) {
        if (!length(d) == length(d2)) {
            stop("d and d2 have not the same size.")
        }
        if (is.null(alpha)) {
            sa &lt;- hclustcompro_select_alpha(d, d2, method = method,
                resampling = FALSE)
            alpha &lt;- sa$alpha[1]
        }
        alpha &lt;- as.numeric(alpha)
        if (!(alpha &gt; 0 &amp; alpha &lt; 1)) {
            warning("Alpha must be between 0 and 1.")
            sa &lt;- hclustcompro_select_alpha(d, d2, method = method,
                resampling = FALSE)
            alpha &lt;- sa$alpha[1]
        }
        d &lt;- dist(alpha * d + (1 - alpha) * d2)
    }
    stats::hclust(d, method, members)
  }
</code></pre>

<hr>
<h2 id='hclustcompro'>hclustcompro</h2><span id='topic+hclustcompro'></span><span id='topic+perioclust'></span>

<h3>Description</h3>

<p>Compromised Hierarchical bottom-up clustering method.
The method uses two sources of information. The merging of the two data sources is done by a parameter (<code class="reqn">\alpha</code>) that allows to weight each source.
</p>
<p style="text-align: center;"><code class="reqn">D_\alpha = \alpha D_1 + (1-\alpha) D_2</code>
</p>
<p><br />
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hclustcompro(
  D1,
  D2,
  alpha="EstimateAlphaForMe",
  k=NULL,
  title="notitle",
  method="ward.D2",
  suppl_plot=TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hclustcompro_+3A_d1">D1</code></td>
<td>

<p>First dissimilarity matrix (square matrix) or distance matrix. Could be a contingency table (see <a href="#topic+CAdist">CAdist</a>). A factorial correspondence analysis is performed using the distances (chi-square metric).
</p>
</td></tr>
<tr><td><code id="hclustcompro_+3A_d2">D2</code></td>
<td>

<p>Second dissimilarity matrix (square matrix), same size as D1, or distance matrix.
</p>
</td></tr>
<tr><td><code id="hclustcompro_+3A_alpha">alpha</code></td>
<td>

<p>The mixing parameter in order to generate the D_alpha matrix. Formula: D_alpha = alpha * D1 + (1-alpha) * D2
</p>
</td></tr>
<tr><td><code id="hclustcompro_+3A_k">k</code></td>
<td>

<p>The number of clusters you want.
</p>
</td></tr>
<tr><td><code id="hclustcompro_+3A_title">title</code></td>
<td>

<p>The title to be displayed on the dendogram plot.
</p>
</td></tr>
<tr><td><code id="hclustcompro_+3A_method">method</code></td>
<td>

<p>The agglomeration method to be used. This should be (an unambiguous abbreviation of) one of &quot;ward.D&quot;, &quot;ward.D2&quot;, &quot;single&quot;, &quot;complete&quot;, &quot;average&quot; (= UPGMA), &quot;mcquitty&quot; (= WPGMA), &quot;median&quot; (= WPGMC) or &quot;centroid&quot; (= UPGMC).
</p>
</td></tr>
<tr><td><code id="hclustcompro_+3A_suppl_plot">suppl_plot</code></td>
<td>

<p>Logical defines whether additional plots are to be displayed (WSS and average sil plot).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong>CAH</strong><br />
Data fusion (parameter <code class="reqn">\alpha</code> optimal value see <a href="#topic+hclustcompro_select_alpha">hclustcompro_select_alpha</a>). It is necessary to define the appropriate proportion for each data source. This is the first sensitive point of the method that the user has to consider. A tool is provided to help him in his decision.
</p>
<p><strong>Cut dendrogram</strong><br />
The division into classes and subclasses is the second crucial point. It has to be done based on the knowledge of the study area and some decision support tools such as the cluster silhouette or the calculation of the intra-cluster variability (WSS: Within Sum of Square). You can use <a href="#topic+hclustcompro_subdivide">hclustcompro_subdivide</a> to subdivide a cluster into sub-clusters.
</p>


<h3>Value</h3>

<p>The function returns a list (class: hclustcompro_cl).
</p>
<table>
<tr><td><code>D1</code></td>
<td>
<p>First dissimilarity matrix (square matrix)</p>
</td></tr>
<tr><td><code>D2</code></td>
<td>
<p>Second dissimilarity matrix (square matrix)</p>
</td></tr>
<tr><td><code>D_alpha</code></td>
<td>
<p>The matrix use in the CAH resulting from the mixing of the two matrices (D1 and D2)</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>Alpha</p>
</td></tr>
<tr><td><code>tree</code></td>
<td>
<p>An object of class hclust, describing the tree generated by the clustering process (see <a href="stats.html#topic+hclust">hclust</a>)</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>The cluster number vector of the selected partition</p>
</td></tr>
<tr><td><code>cutree</code></td>
<td>
<p>Plot of the cut dendrogram</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>How you call the function</p>
</td></tr>
<tr><td><code>cont</code></td>
<td>
<p>Original contingency data (if D1 is a contingency table)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>The hclust function is based on Fortran code contributed to STATLIB by F. Murtagh.
</p>
<p>A. COULON
</p>
<p>L. BELLANGER
</p>
<p>P. HUSI
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(SPARTAAS)
data(datangkor)

#network stratigraphic data (Network)
network &lt;- datangkor$stratigraphy

#contingency table
cont &lt;- datangkor$contingency

#obtain the dissimilarities matrices
distance &lt;- CAdist(cont, nPC = 11)
constraint &lt;- adjacency(network)

#You can also run hclustcompro with the dist matrix directly
hclustcompro(D1 = distance, D2 = constraint, alpha = 0.7, k = 4)
</code></pre>

<hr>
<h2 id='hclustcompro_app'>
Launch the shiny application.
</h2><span id='topic+hclustcompro_app'></span>

<h3>Description</h3>

<p>see <a href="#topic+hclustcompro">hclustcompro</a>, <a href="#topic+hclustcompro_select_alpha">hclustcompro_select_alpha</a>, <a href="#topic+seriograph">seriograph</a>.
You can also check the wiki on the application.</p>


<h3>Usage</h3>

<pre><code class='language-R'>hclustcompro_app()
</code></pre>


<h3>Value</h3>

<p>No return value</p>


<h3>Author(s)</h3>

<p>A. COULON
</p>
<p>L. BELLANGER
</p>
<p>P. HUSI
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##---- Should be DIRECTLY executable !! ----
##-- ==&gt;  Define data, use random,
##--	or do  help(data=index)  for the standard data sets.

library(SPARTAAS)
if(interactive()){
  hclustcompro_app()
}


</code></pre>

<hr>
<h2 id='hclustcompro_select_alpha'>
Estimate the optimal value(s) of the <code class="reqn">\alpha</code> parameter.</h2><span id='topic+hclustcompro_select_alpha'></span>

<h3>Description</h3>

<p>The following criterion &quot;balances&quot; the weight of <code class="reqn">D_1</code> and <code class="reqn">D_2</code> in the final clustering. The <code class="reqn">\alpha</code> value is only a point estimate but the confidence interval gives a range of possible values.
<br /><br />
Based on a resampling process, we generate clones and recalculate the criteria according to <code class="reqn">\alpha</code> (see below).</p>


<h3>Usage</h3>

<pre><code class='language-R'>hclustcompro_select_alpha(
    D1,
    D2,
    acc=2,
    resampling=TRUE,
    method="ward.D2",
    iter=5,
    suppl_plot=TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hclustcompro_select_alpha_+3A_d1">D1</code></td>
<td>

<p>First dissimilarity matrix or contingency table (square matrix).
</p>
</td></tr>
<tr><td><code id="hclustcompro_select_alpha_+3A_d2">D2</code></td>
<td>

<p>Second dissimilarity matrix or network data (square matrix) of the same size as D1.
</p>
</td></tr>
<tr><td><code id="hclustcompro_select_alpha_+3A_acc">acc</code></td>
<td>

<p>Number of digits after the decimal point for the alpha value.
</p>
</td></tr>
<tr><td><code id="hclustcompro_select_alpha_+3A_resampling">resampling</code></td>
<td>

<p>Logical for estimating the confidence interval with a resampling strategy. If you have a lot of data, you can save computation time by setting this option to FALSE.
</p>
</td></tr>
<tr><td><code id="hclustcompro_select_alpha_+3A_method">method</code></td>
<td>

<p>The agglomeration method to be used. This should be (an unambiguous abbreviation of) one of &quot;ward.D&quot;, &quot;ward.D2&quot;, &quot;single&quot;, &quot;complete&quot;, &quot;average&quot; (= UPGMA), &quot;mcquitty&quot; (= WPGMA), &quot;median&quot; (= WPGMC) or &quot;centroid&quot; (= UPGMC).
</p>
</td></tr>
<tr><td><code id="hclustcompro_select_alpha_+3A_iter">iter</code></td>
<td>

<p>The number of clones checked per observation. (200 observations iter=1: ~30 sec, 1000 observations iter=1: ~40 min).
</p>
</td></tr>
<tr><td><code id="hclustcompro_select_alpha_+3A_suppl_plot">suppl_plot</code></td>
<td>

<p>Logical defines whether additional plots should be displayed.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong>Definition of the criterion:</strong><br /><br />
</p>
<p>A criterion for choosing <code class="reqn">\alpha \in [0;1]</code> must be determined by balancing the weights between the two sources of information in the final classification. To obtain <code class="reqn">\alpha</code>, we define the following criterion
</p>
<p style="text-align: center;"><code class="reqn">CorCrit_\alpha = |Cor(dist_{cophenetic},D_1) - Cor(dist_{cophenetic},D_2)|</code>
</p>
  <p style="text-align: center;"><code class="reqn">Equation (1)</code>
</p>

<p>The <code class="reqn">CorCrit_\alpha</code> criterion in (1) represents the difference in absolute value between two cophenetic correlations (cophenetic correlation is defined as the correlation between two distance matrices. It is calculated by considering the half distance matrices as vectors. It measures how faithfully a dendrogram preserves the pairwise distances between the original unmodeled data points). The first correlation is related to the comparison between <code class="reqn">D_1</code> and the ultrametric distances of the clustering with <code class="reqn">\alpha</code> fixed, while the second compares <code class="reqn">D_2</code> and the ultrametric distances of the clustering with <code class="reqn">\alpha</code> fixed. Then, in order to compromise between the information provided by <code class="reqn">D_1</code> and <code class="reqn">D_2</code>, we decided to estimate <code class="reqn">\alpha</code> with <code class="reqn">\hat{\alpha}</code> such that:
</p>
<p style="text-align: center;"><code class="reqn">\hat{\alpha} = min CorCrit_\alpha</code>
</p>

<p style="text-align: center;"><code class="reqn">Equation (2)</code>
</p>

<p><strong>Resampling strategy:</strong><br /><br />
</p>
<p>This is done by creating a set of &quot;clones&quot; for each observation <code class="reqn">i</code>. A clone <code class="reqn">c</code> of observation <code class="reqn">i</code> is a copy of observation <code class="reqn">i</code> for which the distances from the second source have been modified. The modification is made by copying the distances for the second source from another observation <code class="reqn">j</code>. A clustering is then performed using the combination defined in (1) with <code class="reqn">D_1^{(c)}</code> an <code class="reqn">(n+1)\times(n+1)</code> matrix where observations <code class="reqn">i</code> and <code class="reqn">c</code> are identical and <code class="reqn">D_2^{(c)}</code> an <code class="reqn">(n+1)\times(n+1)</code> matrix where the clone <code class="reqn">c</code> of <code class="reqn">i</code> has different distances from those of <code class="reqn">i</code>. A set of clones is generated by varying <code class="reqn">j</code> for all observations except <code class="reqn">i</code>. We can generate a set of <code class="reqn">n-1</code> clones for each element <code class="reqn">i</code> in <code class="reqn">n</code>, so <code class="reqn">n(n-1)</code> clones in total.
</p>
<p>Intuitively, by varying <code class="reqn">\alpha</code> between 0 and 1, we will be able to identify when the clone and the original observation are separated on the dendrogram. This moment will correspond to the value of alpha above which the weight given to the information about the connection between observations contained in <code class="reqn">D_2</code> has too much influence on the results compared to that of <code class="reqn">D_1</code>.
</p>
<p>Let <code class="reqn">CorCrit_\alpha^{(c)}</code> define the same criterion as in (1), where <code class="reqn">D_1</code> and <code class="reqn">D_2</code> are replaced by <code class="reqn">D_1^{(c)}</code> and <code class="reqn">D_2^{(c)}</code> respectively.
The estimated <code class="reqn">\alpha</code> is the mean of the estimated values for each clone.<br />
For each clone <code class="reqn">c</code>:
</p>
<p style="text-align: center;"><code class="reqn">\hat{\alpha}^{(c)} = min   CorCrit_\alpha^{(c)}</code>
</p>

<p style="text-align: center;"><code class="reqn">Equation (3)</code>
</p>

<p><code class="reqn">\hat{\alpha}^*</code> is the mean of <code class="reqn">\hat{\alpha}^{(c)}</code>. In the same spirit as confidence intervals based on bootstrap percentiles (Efron &amp; Tibshirani, 1993), a percentile confidence interval based on replication is also be obtained using the empirical percentiles of the distribution of <code class="reqn">\hat{\alpha}^{(c)}</code>.
</p>
<p style="text-align: center;"><code class="reqn">\hat{\alpha}^* = \frac{1}{n(n-1)} \sum{ \hat\alpha^{(c)} }</code>
</p>

<p style="text-align: center;"><code class="reqn">Equation (4)</code>
</p>

<p style="text-align: center;"><code class="reqn">c  \in [1; n(n-1)]</code>
</p>

<p><strong>Warnings:</strong><br /><br />
It is possible to observe an <code class="reqn">\alpha</code> value outside the confidence interval. In some cases, this problem can be solved by increasing the number of iterations or by changing the number of axes used to construct the matrix D1 after the correspondence analysis. If the <code class="reqn">\alpha</code> value remains outside the interval, it means that the data are noisy and the resampling procedure is affected.
</p>


<h3>Value</h3>

<p>The function returns a list (class: selectAlpha_obj).
</p>
<table>
<tr><td><code>alpha</code></td>
<td>
<p>The estimated value of the alpha parameter (min CorCrit_alpha)</p>
</td></tr>
<tr><td><code>alpha.plot</code></td>
<td>
<p>The CorCrit curve for alpha between 0 and 1</p>
</td></tr>
</table>
<p>If resampling = TRUE
</p>
<table>
<tr><td><code>sd</code></td>
<td>
<p>The standard deviation</p>
</td></tr>
<tr><td><code>conf</code></td>
<td>
<p>The confidence interval of alpha</p>
</td></tr>
<tr><td><code>boxplot</code></td>
<td>
<p>The boxplot of alpha estimation with resampling</p>
</td></tr>
<tr><td><code>values</code></td>
<td>
<p>All potential alpha values obtained from each clone</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>A. COULON
</p>
<p>L. BELLANGER
</p>
<p>P. HUSI
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#################################

##---- Should be DIRECTLY executable !! ----
##-- ==&gt;  Define data, use random,
##--	or do  help(data=index)  for the standard data sets.
library(SPARTAAS)
data(datangkor)

#network stratigraphic data (Network)
network &lt;- datangkor$stratigraphy

#contingency table
cont &lt;- datangkor$contingency

dissimilarity &lt;- CAdist(cont,nPC="max",graph=FALSE)
constraint &lt;- adjacency(network)

hclustcompro_select_alpha(D1 = dissimilarity, D2 = constraint)
hclustcompro_select_alpha(D1 = dissimilarity, D2 = constraint, acc = 3, resampling = TRUE)

</code></pre>

<hr>
<h2 id='hclustcompro_subdivide'>
Subdivide a cluster after running hclustcompro.
</h2><span id='topic+hclustcompro_subdivide'></span>

<h3>Description</h3>

<p>Allow the user to split one cluster into sub-clusters. This function only works with 'hclustcompro_cl' object returned by the hclustcompro function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hclustcompro_subdivide(hclustcompro_cl,cluster,nb_class)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hclustcompro_subdivide_+3A_hclustcompro_cl">hclustcompro_cl</code></td>
<td>
<p>A hclustcompro_cl object</p>
</td></tr>
<tr><td><code id="hclustcompro_subdivide_+3A_cluster">cluster</code></td>
<td>
<p>The number of the cluster. Numbered from left to right on the dendrogram (1, 2, ...)</p>
</td></tr>
<tr><td><code id="hclustcompro_subdivide_+3A_nb_class">nb_class</code></td>
<td>
<p>The number of sub-clusters you want</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>hclustcompro_cl</code></td>
<td>
<p>A new hclustcompro_cl object updated see <a href="#topic+hclustcompro">hclustcompro</a></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>A. COULON
</p>
<p>L. BELLANGER
</p>
<p>P. HUSI
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##---- Should be DIRECTLY executable !! ----
##-- ==&gt;  Define data, use random,
##--	or do  help(data=index)  for the standard data sets.
library(SPARTAAS)
data(datangkor)

#network stratigraphic data (Network)
network &lt;- datangkor$stratigraphy

#contingency table
cont &lt;- datangkor$contingency

#obtain the dissimilarities matrices
distance &lt;- CAdist(cont, nPC = 11)
constraint &lt;- adjacency(network)

#You can also run hclustcompro with the dist matrix directly
clustering &lt;- hclustcompro(D1 = distance, D2 = constraint, alpha = 0.7, k = 7) #number of cluster 7
clustering &lt;- hclustcompro_subdivide(clustering,cluster = 5, nb_class = 2)

#subdivide more than one cluster
clustering2 &lt;- hclustcompro(D1 = distance, D2 = constraint,0.7,k=7) #number of cluster 7
clustering2 &lt;- hclustcompro_subdivide(clustering2,cluster = c(5,7), nb_class = c(2,2))
</code></pre>

<hr>
<h2 id='mapclust'>
Divise hierarchical Clustering using Spatialpatches algorithm.
</h2><span id='topic+mapclust'></span>

<h3>Description</h3>

<p>This function performs a divisive hierarchical clustering on a regionalised variable using the spatial patches algorithm (Woillez et al. 2007; Woillez, Rivoirard and Petitgas 2009). It is a top-down hierarchical clustering with a geographical constraint. It is possible to cut the tree by clicking on the dendrogram at the desired level. The results include a description of the clusters and graphics. When slicing the dendrogram, you can look at the two plots (WSSPlot and AveSilPlot) that show the relatively good quality of the partitions. The first shows the Within Sum of Square (WSS) for each partition and you can use the elbow approach to select a partition. The second graph shows the average silhouette width. This index is between -1 and 1. The closer it is to 1, the better the partition. See <a href="cluster.html#topic+silhouette">silhouette</a>.
</p>
<p>If you want to cut the dendogram to a different dlim or number of clusters, you can do so without re-running  <code>mapclust() with <a href="#topic+mapclust_cut_tree">mapclust_cut_tree</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mapclust(
    coord,
    var,
    label = "Nolabel",
    iter = 20,
    Plabel = TRUE,
    lonlat = TRUE,
    positive_var = FALSE,
    n = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mapclust_+3A_coord">coord</code></td>
<td>

<p>The x (longitude) and y (latitude) coordinates of the data.frame or matrix dimension 2.
</p>
</td></tr>
<tr><td><code id="mapclust_+3A_var">var</code></td>
<td>

<p>The regionalised variable(s) of interest
</p>
</td></tr>
<tr><td><code id="mapclust_+3A_label">label</code></td>
<td>

<p>(optional) The names of the samples or an identifier. Must be a factor.
</p>
</td></tr>
<tr><td><code id="mapclust_+3A_iter">iter</code></td>
<td>

<p>The number of iterations. The number of different dlim you want to test (must be greater than 10).
</p>
</td></tr>
<tr><td><code id="mapclust_+3A_plabel">Plabel</code></td>
<td>

<p>Logical parameter to activate or not the printing of labels on the dendrogram.
</p>
</td></tr>
<tr><td><code id="mapclust_+3A_lonlat">lonlat</code></td>
<td>

<p>Logical parameter to activate or not the cartography in lonlat system with leaflet (base map).
</p>
</td></tr>
<tr><td><code id="mapclust_+3A_positive_var">positive_var</code></td>
<td>

<p>logical parameter that defines whether your variable of interest is positive or not.
</p>
</td></tr>
<tr><td><code id="mapclust_+3A_n">n</code></td>
<td>

<p>Number of groups. If NULL, you can select the number of groups by clicking on the dendrogram.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Dlim is the selected minimum distance from the sample to the patch centre: to identify patches (units are those of coordinates). The dlim is automatically initialised and does not need to be set by the user. The minimum data is a data frame or matrix with at least 3 columns.
</p>


<h3>Value</h3>

<p>the function returns a list.
</p>
<p><strong>Plot:</strong>
</p>
<table>
<tr><td><code>dendrogram</code></td>
<td>
<p>The global dendrogram (hclust object)</p>
</td></tr>
<tr><td><code>dendrogram_ggplot</code></td>
<td>
<p>The global dendrogram (ggplot2 object)</p>
</td></tr>
<tr><td><code>cuttree</code></td>
<td>
<p>The cut dendrogram</p>
</td></tr>
<tr><td><code>map</code></td>
<td>
<p>The map of the selected partition</p>
</td></tr>
<tr><td><code>AveSilPlot</code></td>
<td>
<p>The average silhouette width plot (for each partition)</p>
</td></tr>
<tr><td><code>WSSPlot</code></td>
<td>
<p>The Within Sum of Square plot (for each partition)</p>
</td></tr>
<tr><td><code>silhouette</code></td>
<td>
<p>The silhouette plot of the selected partition</p>
</td></tr>
</table>
<p><strong>Value:</strong>
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p>The x-coordinate data you have used</p>
</td></tr>
<tr><td><code>Y</code></td>
<td>
<p>The y-coordinate data you have used</p>
</td></tr>
<tr><td><code>var</code></td>
<td>
<p>The regionalized variable(s) data you have used</p>
</td></tr>
<tr><td><code>label</code></td>
<td>
<p>The label vector you have used</p>
</td></tr>
<tr><td><code>density</code></td>
<td>
<p>The estimated density based on var. Equal to var if you used a unidimensional density variable.</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>The cluster vector of the selected partition</p>
</td></tr>
<tr><td><code>Plabel</code></td>
<td>
<p>Logical parameter to activate or not the printing of labels on the dendrogram</p>
</td></tr>
<tr><td><code>fullhist</code></td>
<td>
<p>The compositional cluster for each observation</p>
</td></tr>
<tr><td><code>hist</code></td>
<td>
<p>The compositional cluster without duplicates (corresponds to the split on the dendogram)</p>
</td></tr>
<tr><td><code>dlim</code></td>
<td>
<p>The vector of the different limit distances</p>
</td></tr>
<tr><td><code>cutdlim</code></td>
<td>
<p>The select dlim for the cut of the current partition</p>
</td></tr>
<tr><td><code>DiMatrix</code></td>
<td>
<p>The weighted euclidean distance matrix</p>
</td></tr>
<tr><td><code>silhouetteData</code></td>
<td>
<p>The silhouette data of the selected partition</p>
</td></tr>
<tr><td><code>AveSilData</code></td>
<td>
<p>The average silhouette value for each partition</p>
</td></tr>
<tr><td><code>Moran</code></td>
<td>
<p>The Moran index for each group for each partition</p>
</td></tr>
<tr><td><code>lonlat</code></td>
<td>
<p>Logical parameter, whether your coordinates are in latitude-longitude format or not</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>A. COULON
L. BELLANGER
P. HUSI
</p>


<h3>References</h3>

<p>Bellanger L., Coulon A. and Husi P. (2021) Determination of cultural areas based on medieval pottery using an original divisive hierarchical clustering method with geographical constraint (MapClust), Journal of Archaeological Science, Volume 132 <a href="https://doi.org/10.1016/j.jas.2021.105431">doi:10.1016/j.jas.2021.105431</a>.
</p>
<p>Bellanger L., Husi P., Laghzali Y. (2015). Spatial statistic analysis of dating using pottery: an aid to the characterization of cultural areas in West Central France. In : Traviglia A. ed., Across Space and Time, Proceedings of the 41th International Conference on Computer Applications and Quantitative Methods in Archaeology (CAA-2013), Perth (Australie), Amsterdam University Press : 276-282.
</p>
<p>Woillez M., Poulard J.C., Rivoirard J., Petitgas P., Bez N. (2007). Indices for capturing spatial patterns and their evolution in time,
with application to European hake (Merluccius merluccius) in the
Bay of Biscay. ICES J. Mar. Sci. 64, 537-550.
</p>
<p>Woillez M., Rivoirard J. and Petitgas P. (2009) Notes on survey-based spatial indicators for monitoring fish populations, Aquatic Living Resources, 22 :155-164.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
###################################
## loading data
 library(SPARTAAS)
 data(datarcheo)
 data(datacancer)

###################################
### Example: 1
## Function "mapclust"
# object &lt;- mapclust( coord = ..., var = ..., label = ...)

classification &lt;- mapclust(datarcheo$coord, datarcheo$var, datarcheo$label, n=4)

#Global dendrogram
 classification$dendrogram
#Cut dendrogram
 classification$cuttree
#silhouette of selected partition
 classification$silhouette


#You can cut the dendrogram for another dlim
 NewCut &lt;- mapclust_cut_tree(classification, dlim=0.30)

#See evaluation using Silhouette width by running:
 NewCut$silhouette
 #If the plot is empty try to increase the height of the window (full screen)

#See summary of the data by running:
 summary(NewCut$silhouetteData)


###################################
## kmeans comparison
# pepare data (only geographical data)
 datakmeans &lt;- datarcheo$coord

#kmeans
 number_cluster &lt;- 4
 cl &lt;- kmeans(datakmeans, number_cluster)
 plot(datakmeans, col = cl$cluster)



</code></pre>

<hr>
<h2 id='mapclust_app'>
Shiny application for Mapclust method
</h2><span id='topic+mapclust_app'></span>

<h3>Description</h3>

<p>see <a href="#topic+mapclust">mapclust</a>
You can also check the wiki on the application.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mapclust_app()
</code></pre>


<h3>Value</h3>

<p>No return value</p>


<h3>Author(s)</h3>

<p>A. COULON
</p>
<p>L. BELLANGER
</p>
<p>P. HUSI
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#open application
#library(SPARTAAS)
#if(interactive()){
  #mapclust_app()
#}
</code></pre>

<hr>
<h2 id='mapclust_cut_tree'>
Function to cut the dendrogram for a new height (distance limit) or a new number of clusters and map the new partition
</h2><span id='topic+mapclust_cut_tree'></span>

<h3>Description</h3>

<p>The function returns the new map, one dendrogram with the cut line, the silhouette width and the new vector cluster. You must call <a href="#topic+mapclust">mapclust</a> beforehand to get a <code>mapclust_cl</code> object that can be used by mapclust_cut_tree.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mapclust_cut_tree(classification, nb_grp = NA, dlim = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mapclust_cut_tree_+3A_classification">classification</code></td>
<td>

<p>The object return by mapclust  <code>Class: mapclust_cl</code>
</p>
</td></tr>
<tr><td><code id="mapclust_cut_tree_+3A_nb_grp">nb_grp</code></td>
<td>

<p>The number of groups you want to have on the partition. Must be an integer. (don't use dlim in this case)
</p>
</td></tr>
<tr><td><code id="mapclust_cut_tree_+3A_dlim">dlim</code></td>
<td>

<p>The value of dlim at which you wish to cut the dendrogram. You can enter any value (numerical) and the function will select the nearest lower dlim with the same partition. (do not use nb_grp in this case).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the function returns a list.
</p>
<p><strong>Plot:</strong>
</p>
<table>
<tr><td><code>dendrogram</code></td>
<td>
<p>The global dendrogram (hclust object)</p>
</td></tr>
<tr><td><code>dendrogram_ggplot</code></td>
<td>
<p>The global dendrogram (ggplot2 object)</p>
</td></tr>
<tr><td><code>cuttree</code></td>
<td>
<p>The cut dendrogram</p>
</td></tr>
<tr><td><code>map</code></td>
<td>
<p>The map of the selected partition</p>
</td></tr>
<tr><td><code>AveSilPlot</code></td>
<td>
<p>The average silhouette width plot (for each partition)</p>
</td></tr>
<tr><td><code>WSSPlot</code></td>
<td>
<p>The Within Sum of Square plot (for each partition)</p>
</td></tr>
<tr><td><code>silhouette</code></td>
<td>
<p>The silhouette plot of the selected partition</p>
</td></tr>
</table>
<p><strong>Value:</strong>
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p>The x-coordinate data you have used</p>
</td></tr>
<tr><td><code>Y</code></td>
<td>
<p>The y-coordinate data you have used</p>
</td></tr>
<tr><td><code>var</code></td>
<td>
<p>The regionalised variable data you have used</p>
</td></tr>
<tr><td><code>label</code></td>
<td>
<p>The label vector you have used</p>
</td></tr>
<tr><td><code>density</code></td>
<td>
<p>The estimated density based on var. Equal to var if you used a unidimensional density variable</p>
</td></tr>
<tr><td><code>cluster</code></td>
<td>
<p>The cluster vector of the selected partition</p>
</td></tr>
<tr><td><code>Plabel</code></td>
<td>
<p>Logical parameter to activate or not the printing of labels on the dendrogram</p>
</td></tr>
<tr><td><code>fullhist</code></td>
<td>
<p>The compositional cluster for each observation</p>
</td></tr>
<tr><td><code>hist</code></td>
<td>
<p>The compositional cluster without duplicates (corresponds to the split on the dendogram)</p>
</td></tr>
<tr><td><code>dlim</code></td>
<td>
<p>The vector of the different limit distances</p>
</td></tr>
<tr><td><code>cutdlim</code></td>
<td>
<p>The select dlim for the cut of the current partition</p>
</td></tr>
<tr><td><code>DiMatrix</code></td>
<td>
<p>The weighted euclidean distance matrix</p>
</td></tr>
<tr><td><code>silhouetteData</code></td>
<td>
<p>The silhouette data of the selected partition</p>
</td></tr>
<tr><td><code>AveSilData</code></td>
<td>
<p>The average silhouette value for each partition</p>
</td></tr>
<tr><td><code>Moran</code></td>
<td>
<p>The Moran index for each group for each partition</p>
</td></tr>
<tr><td><code>lonlat</code></td>
<td>
<p>Logical parameter, whether your coordinates are in latitude-longitude format or not</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>A. COULON
L. BELLANGER
P. HUSI
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## loading data
library(SPARTAAS)
data(datarcheo)

##First you need to run the mapclust function.
#This function allow you to obtain one partition
# object &lt;- mapclust( coord = ..., var = ..., label = ...)
OldClassif &lt;- mapclust(datarcheo$coord, datarcheo$var, datarcheo$label, n = 4)

##In order to cut the dendrogram for another dlim
NewCut &lt;- mapclust_cut_tree(classification = OldClassif, dlim = 0.37)
##In order to cut the dendrogram for another number of cluster
NewCut2 &lt;- mapclust_cut_tree(classification = OldClassif, nb_grp = 4)

#See evaluation using Silhouette width by running:
NewCut$silhouette
#If the plot is empty try to increase the height of the window (full screen).

#See summary of the data by running:
summary(NewCut$silhouetteData)

</code></pre>

<hr>
<h2 id='overlap'>
Temporal overlap index
</h2><span id='topic+overlap'></span>

<h3>Description</h3>

<p>The overlap index is the ratio between internal overlap and total overlap over time. We define the limit of total overlap as: the minimum of the lower limits of the pair of individuals and the maximum of the upper limits. We define the internal overlap limit as the maximum of the lower limits and the minimum of the upper limits.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>overlap(temporal)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="overlap_+3A_temporal">temporal</code></td>
<td>

<p>A data frame with tree columns: the name of the element, the lower limit and the upper limit.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The lower and upper limits must be numeric.
</p>
<p>The dissimilarity between time periods is calculated as the ratio of the overlap of the time periods (distance in the case of disjoint time periods) to the cumulative extent of the two time periods.
</p>
<p>As the ratio is bounded between -1 and 1, we add 1 to make it positive and normalise it so that it is between 0 and 1.
</p>
<p>This overlap index then needs to be transformed into a dissimilarity index between sets. To do this we use the 1 - ratio. It is equal to 0 if the two time periods are identical and 1 if they are infinitely different.
</p>


<h3>Value</h3>

<table>
<tr><td><code>D</code></td>
<td>
<p>The dissimilarity matrix base on the overlap index.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>A. COULON
</p>
<p>L. BELLANGER
</p>
<p>P. HUSI
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##---- Should be DIRECTLY executable !! ----
##-- ==&gt;  Define data, use random,
##--	or do  help(data=index)  for the standard data sets.
library(SPARTAAS)
data(dataceram)
dist &lt;- overlap(dataceram$timerange)

</code></pre>

<hr>
<h2 id='plot.cerardat_obj'>
plot cerardat model
</h2><span id='topic+plot.cerardat_obj'></span>

<h3>Description</h3>

<p>plot cerardat model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cerardat_obj'
plot(x, which = NULL,
                  col1 = rgb(0.93,0.23,0.23,0.5), col2 = "black",
                  xlim=NULL, ylim=NULL,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.cerardat_obj_+3A_x">x</code></td>
<td>

<p>a <a href="#topic+cerardat">cerardat</a> output.
</p>
</td></tr>
<tr><td><code id="plot.cerardat_obj_+3A_which">which</code></td>
<td>

<p>Vector containing the plots you want (row number on the contingence table: numeric).
</p>
</td></tr>
<tr><td><code id="plot.cerardat_obj_+3A_col1">col1</code></td>
<td>

<p>Color of the Event curve.
</p>
</td></tr>
<tr><td><code id="plot.cerardat_obj_+3A_col2">col2</code></td>
<td>

<p>Color of the Accumulation curve.
</p>
</td></tr>
<tr><td><code id="plot.cerardat_obj_+3A_xlim">xlim</code></td>
<td>

<p>Two numeric values, specifying the left limit and the right limit of the scale.
</p>
</td></tr>
<tr><td><code id="plot.cerardat_obj_+3A_ylim">ylim</code></td>
<td>

<p>Two numeric values, specifying the lower limit and the upper limit of the scale.
</p>
</td></tr>
<tr><td><code id="plot.cerardat_obj_+3A_...">...</code></td>
<td>

<p>other parameters to be passed through to plotting functions.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>A. COULON
</p>
<p>L. BELLANGER
</p>
<p>P. HUSI
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##---- Should be DIRECTLY executable !! ----
##-- ==&gt;  Define data, use random,
##--	or do  help(data=index)  for the standard data sets.

## The function is currently defined as

</code></pre>

<hr>
<h2 id='serio_app'>
Launch the shiny application.
</h2><span id='topic+serio_app'></span>

<h3>Description</h3>

<p>see <a href="#topic+seriograph">seriograph</a>.
You can also check the wiki on the application.</p>


<h3>Usage</h3>

<pre><code class='language-R'>serio_app()
</code></pre>


<h3>Value</h3>

<p>No return value</p>


<h3>Author(s)</h3>

<p>A. COULON
</p>
<p>L. BELLANGER
</p>
<p>P. HUSI
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##---- Should be DIRECTLY executable !! ----
##-- ==&gt;  Define data, use random,
##--	or do  help(data=index)  for the standard data sets.

library(SPARTAAS)
if(interactive()){
  serio_app()
}


</code></pre>

<hr>
<h2 id='seriograph'>
Plot seriograph (B. DESACHY).
</h2><span id='topic+seriograph'></span>

<h3>Description</h3>

<p>Visualization of contingency data over time. <strong>Rows</strong> must be individuals (archaeological site,...) and <strong>columns</strong> must be categories (type,...).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>seriograph(cont, order, insert, show, permute)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="seriograph_+3A_cont">cont</code></td>
<td>

<p>Contingency table or hclustcompro object. Note: Your contingency table must have the rows sorted in chronological order (the order parameter allows you to change the order of the rows if necessary).
</p>
</td></tr>
<tr><td><code id="seriograph_+3A_order">order</code></td>
<td>

<p>Vector to change the order of the rows (use row names or cluster names if cont is a hclustcompro object, as a character vector). The oldest (at the bottom) must be at the end of the vector. Missing names are not plotted. You can remove a row by simply removing the name in the vector.
</p>
</td></tr>
<tr><td><code id="seriograph_+3A_show">show</code></td>
<td>

<p>The element to be plotted. This should be (a unique abbreviation of) one of 'both', 'EPPM' or 'frequency'.
</p>
</td></tr>
<tr><td><code id="seriograph_+3A_permute">permute</code></td>
<td>

<p>Logical for permute columns in order to show seriation.
</p>
</td></tr>
<tr><td><code id="seriograph_+3A_insert">insert</code></td>
<td>

<p>Vector with the position after which you want to insert one or more hiatuses. Could be a list with two vectors: position and label to be printed instead of hiatus (see last examples).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong>Seriograph</strong><br /><br />
We have chosen the serigraph (DESACHY 2004). This tool makes it possible to highlight the evolution of ceramics over time as well as to understand the commercial relations thanks to the imported ceramics. The percentages of each category of ceramics per set are displayed. The percentages are calculated independently for each set (row). The display of the percentages allows comparison of the different sets but does not provide information on the differences in numbers. To fill this gap, the proportion of the numbers in each class is displayed on the seriograph (weight column).
</p>
<p>We can generalized this representation for other contingency data or with <a href="#topic+hclustcompro">hclustcompro</a> object.
</p>
<p>The visualization of positive deviations from the average percentage allows us to observe a series that results from changes in techniques and materials dedicated to ceramic manufacturing over time.
</p>
<p>In order to facilitate the exploitation of the data tables, we propose here a computerised graphic processing tool (EPPM serigraph - for Ecart Positif aux Pourcentages Moyens - positive deviation from the average percentage), which does not require specialised statistical skills and is adapted to the case of stratified sites, where the study of the evolution of artefacts can be based on the relative chronology provided by the excavation.
</p>
<p>The treatment consists firstly of transforming this table of counts into a table of percentages, the total number in each set (each row) being reduced to 100; these are the proportions, or frequencies, of the types in the sets are thus compared.
</p>
<p>The display of positive deviations from the mean percentages (EPPM) shows in black on a lighter background the percentage shares that are higher than the mean percentage of the variable, so as to highlight the most significant part of the values in the table.This display is simply adapted to the seriograph: when a percentage is greater than the average percentage of the type, the excess share (called here EPPM: positive deviation from the average percentage) is shown in black, centred around the axis of the type, on the grey background of the percentage bar.
</p>
<p>The table is then transformed into a graphic matrix where these percentages are expressed, for each type, by horizontal bars centred on the column axis. When the rows are ordered chronologically, the silhouette formed by the superposition of these frequency bars bars makes it possible to visualise the evolution over time of the type concerned.
</p>
<p>The display of the percentages allows comparison of the different sets but does not provide information on the differences in numbers. To fill this gap, the proportion of the numbers in each class is displayed on the seriograph (weight column).
</p>
<p>The processing technique applies to sets whose chronological order is not known; the lines of the graph are to be reorganised so as to obtain boat-shaped silhouettes following the hypothesis of a chronological evolution corresponding to the seriation model.
</p>
<p><strong>Positive deviation from the average percentage (EPPM in French)</strong><br /><br />
The average percentage is calculated for each ceramic category (columns) on the total number of accounts (all classes combined). From the average percentage we recover for each category and for each rows the difference between the percentage of the category in the class with the average percentage. The EPPM corresponds to the notion of independence deviation (between rows and columns, between categories and time classes) in a chi-square test approach. Although this approach is fundamental in statistical analysis, independence deviations are here purely indicative and are not associated with a p_value that could determine the significance of deviations.
</p>
<p><strong>Weight</strong><br /><br />
Weight is the number of observations divided by the total number of observations. It indicates for each row the percentage of the data set used to calculate the frequencies of the elements (row).
</p>
<p><strong>Permutation</strong><br /><br />
order argument:<br />
The rows of the contingency table are initially in the order of appearance (from top to bottom). It must be possible to re-order the classes in a temporal way (You can also order as you want your contingency table).
</p>
<p>permute argument:<br />
In addition, it is possible to swap ceramic categories (contingency table columns) in order to highlight a serialization phenomenon. Matrix permutation uses an algorithm called &quot;reciprocal averages&quot;. Each line is assigned a rank ranging from 1 to n the number of lines. A barycentre is calculated for each column by weighting according to the row rank. Finally, the columns are reorganized by sorting them by their barycentre.
</p>
<p><strong>Insert</strong><br /><br />
It's possible to insert a row in the seriograph in order to represent a archeological hiatus or other temporal discontinuities.
</p>


<h3>Value</h3>

<p>The function returns a list (class: seriograph).
</p>
<table>
<tr><td><code>seriograph</code></td>
<td>
<p>The seriograph plot</p>
</td></tr>
<tr><td><code>dendrogram</code></td>
<td>
<p>If cont is a hclustcompro object return the dendrogram with the period order as label</p>
</td></tr>
<tr><td><code>contingency</code></td>
<td>
<p>Data frame of the contingencies data group by cluster</p>
</td></tr>
<tr><td><code>frequency</code></td>
<td>
<p>Data frame of the frequencies data group by cluster</p>
</td></tr>
<tr><td><code>ecart</code></td>
<td>
<p>Data frame of the gap data group by cluster</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>B. DESACHY
</p>
<p>A. COULON
</p>
<p>L. BELLANGER
</p>
<p>P. HUSI
</p>


<h3>References</h3>

<p>Desachy B. (2004). Le sériographe EPPM : un outil informatisé de sériation graphique pour tableaux de comptages. In: Revue archéologique de Picardie, n°3-4, 2004. Céramiques domestiques et terres cuites architecturales. Actes des journées d'étude d'Amiens (2001-2002-2003) pp. 39-56 <a href="https://doi.org/10.3406/pica.2004.2396">doi:10.3406/pica.2004.2396</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##---- Should be DIRECTLY executable !! ----
##-- ==&gt;  Define data, use random,
##--	or do  help(data=index)  for the standard data sets.
library(SPARTAAS)
data(datangkor)

## network stratigraphic data (Network)
network &lt;- datangkor$stratigraphy

## contingency table
cont &lt;- datangkor$contingency

## default
seriograph(cont)

seriograph(cont,show = "EPPM")
seriograph(cont,show = "frequency")

## Don't allow permutation of columns
seriograph(cont, permute = FALSE)

## insert Hiatus (position, 1 -&gt; after first row from bottom: oldest)
seriograph(cont,insert = 2)
seriograph(cont,insert = c(2,3))

## insert costum label element
insert &lt;- list(
  position = c(2,3),
  label = c("Hiatus.100years","Missing data")
)
seriograph(cont,insert = insert)

## change order with cluster name (letters on dendrogram) to sort them in a chronological order
seriograph(cont,order=c("AI03","AI09","AI01","AI02","AI04","APQR01","AO05",
"AO03","AI05","AO01","APQR02","AI07","AI08","AO02","AI06","AO04","APQR03"))
## by omitting the row names, you delete the corresponding rows
seriograph(cont,order=c("AI02","AI08","APQR03","AI09"))

</code></pre>

<hr>
<h2 id='timerange'>
Plot the time range of observations sorted by cluster.
</h2><span id='topic+timerange'></span>

<h3>Description</h3>

<p>Cluster time range visualisation.</p>


<h3>Usage</h3>

<pre><code class='language-R'>timerange(
  data,
  cluster = NULL,
  add = NULL,
  density = NULL,
  color = NULL,
  reorder = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="timerange_+3A_data">data</code></td>
<td>

<p>data.frame containing the identifier, lower and upper bound (id, inf, sup) for each observation</p>
</td></tr>
<tr><td><code id="timerange_+3A_cluster">cluster</code></td>
<td>

<p>vector containing the cluster number of each observation.</p>
</td></tr>
<tr><td><code id="timerange_+3A_add">add</code></td>
<td>

<p>data.frame containing the information to be displayed on hover.</p>
</td></tr>
<tr><td><code id="timerange_+3A_density">density</code></td>
<td>

<p>vector of the density for each observation.</p>
</td></tr>
<tr><td><code id="timerange_+3A_color">color</code></td>
<td>

<p>vector of the colors for each observation (if you want the colors to correspond to something else than clusters). Character vector of the same length as the number of observations.</p>
</td></tr>
<tr><td><code id="timerange_+3A_reorder">reorder</code></td>
<td>

<p>Logical to rearrange the colors. If TRUE, the first color corresponds to the leftmost cluster on the plot. If FALSE, the first color is that of cluster number 1, wherever it is.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns a list.
</p>
<table>
<tr><td><code>plot</code></td>
<td>
<p>The timerange plot.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>A. COULON
</p>
<p>B. MARTINEAU
</p>
<p>L. BELLANGER
</p>
<p>P. HUSI
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##---- Should be DIRECTLY executable !! ----
##-- ==&gt;  Define data, use random,
##--	or do  help(data=index)  for the standard data sets.
library(SPARTAAS)
data &lt;- data.frame(
  id = c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20),
  Lower_bound = c(400,401,401,350,500,460,482,432,399,
    489,750,740,704,700,758,789,802,755,750,820),
  Upper_bound = c(550,689,755,700,780,700,700,699,650,
    850,1100,1100,1010,889,999,999,1050,1002,1000,1100)
)

cluster = c(1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2)

add &lt;- data.frame(
  Site = c("Angers","Angers","Angers","Angers","Angers",
    "Angers","Angers","Angers","Angers","Angers",
    "Blois","Blois","Blois","Blois","Blois",
    "Blois","Blois","Blois","Blois","Blois")
)

timerange(data, cluster, add)

## with sub group (cluster 1 is sub divided in 2: 1.1 and 1.2)
cluster_with_sub = c(1.1,1.1,1.1,1.1,1.1,1.2,1.2,1.2,1.2,1.2,2,2,2,2,2,2,2,2,2,2)

timerange(data, cluster_with_sub, add)

## with density
density &lt;- c(32,34,35,19,9,25,19,29,28,18,10,13,9,10,9,6,3,7,7,1)
timerange(data=data, cluster=cluster, density=density)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
