<!DOCTYPE html><html><head><title>Help for package DDoutlier</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {DDoutlier}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#COF'><p>Connectivity-based Outlier Factor (COF) algorithm</p></a></li>
<li><a href='#DB'><p>Distance-based outlier detection based on user-given neighborhood size</p></a></li>
<li><a href='#INFLO'><p>Influenced Outlierness (INFLO) algorithm</p></a></li>
<li><a href='#KDEOS'><p>Kernel Density Estimation Outlier Score (KDEOS) algorithm with gaussian kernel</p></a></li>
<li><a href='#KNN_AGG'><p>Aggregated k-nearest neighbors distance over different k's</p></a></li>
<li><a href='#KNN_IN'><p>In-degree for observations in a k-nearest neighbors graph</p></a></li>
<li><a href='#KNN_SUM'><p>Sum of distance to k-nearest neighbors</p></a></li>
<li><a href='#LDF'><p>Local Density Factor (LDF) algorithm with gaussian kernel</p></a></li>
<li><a href='#LDOF'><p>Local Distance-based Outlier Factor (LDOF) algorithm</p></a></li>
<li><a href='#LOCI'><p>Local Correlation Integral (LOCI) algorithm with constant nearest neighbor parameter</p></a></li>
<li><a href='#LOF'><p>Local Outlier Factor (LOF) algorithm</p></a></li>
<li><a href='#LOOP'><p>Local Outlier Probability (LOOP) algorithm</p></a></li>
<li><a href='#NAN'><p>Natural Neighbor (NAN) algorithm to return the self-adaptive neighborhood</p></a></li>
<li><a href='#NOF'><p>Natural Outlier Factor (NOF) algorithm</p></a></li>
<li><a href='#RDOS'><p>Relative Density-based Outlier Factor (RDOS) algorithm with gaussian kernel</p></a></li>
<li><a href='#RKOF'><p>Robust Kernel-based Outlier Factor (RKOF) algorithm with gaussian kernel</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Distance &amp; Density-Based Outlier Detection</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Author:</td>
<td>Jacob H. Madsen &lt;jacob.madsen1@mail.com&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jacob H. Madsen &lt;jacob.madsen1@mail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Outlier detection in multidimensional domains. Implementation of notable distance and density-based outlier algorithms. Allows users to identify local outliers by comparing observations to their nearest neighbors, reverse nearest neighbors, shared neighbors or natural neighbors. For distance-based approaches, see Knorr, M., &amp; Ng, R. T. (1997) &lt;<a href="https://doi.org/10.1145%2F782010.782021">doi:10.1145/782010.782021</a>&gt;, Angiulli, F., &amp; Pizzuti, C. (2002) &lt;<a href="https://doi.org/10.1007%2F3-540-45681-3_2">doi:10.1007/3-540-45681-3_2</a>&gt;, Hautamaki, V., &amp; Ismo, K. (2004) &lt;<a href="https://doi.org/10.1109%2FICPR.2004.1334558">doi:10.1109/ICPR.2004.1334558</a>&gt; and Zhang, K., Hutter, M. &amp; Jin, H. (2009) &lt;<a href="https://doi.org/10.1007%2F978-3-642-01307-2_84">doi:10.1007/978-3-642-01307-2_84</a>&gt;. For density-based approaches, see Tang, J., Chen, Z., Fu, A. W. C., &amp; Cheung, D. W. (2002) &lt;<a href="https://doi.org/10.1007%2F3-540-47887-6_53">doi:10.1007/3-540-47887-6_53</a>&gt;, Jin, W., Tung, A. K. H., Han, J., &amp; Wang, W. (2006) &lt;<a href="https://doi.org/10.1007%2F11731139_68">doi:10.1007/11731139_68</a>&gt;, Schubert, E., Zimek, A. &amp; Kriegel, H-P. (2014) &lt;<a href="https://doi.org/10.1137%2F1.9781611973440.63">doi:10.1137/1.9781611973440.63</a>&gt;, Latecki, L., Lazarevic, A. &amp; Prokrajac, D. (2007) &lt;<a href="https://doi.org/10.1007%2F978-3-540-73499-4_6">doi:10.1007/978-3-540-73499-4_6</a>&gt;, Papadimitriou, S., Gibbons, P. B., &amp; Faloutsos, C. (2003) &lt;<a href="https://doi.org/10.1109%2FICDE.2003.1260802">doi:10.1109/ICDE.2003.1260802</a>&gt;, Breunig, M. M., Kriegel, H.-P., Ng, R. T., &amp; Sander, J. (2000) &lt;<a href="https://doi.org/10.1145%2F342009.335388">doi:10.1145/342009.335388</a>&gt;, Kriegel, H.-P., Kröger, P., Schubert, E., &amp; Zimek, A. (2009) &lt;<a href="https://doi.org/10.1145%2F1645953.1646195">doi:10.1145/1645953.1646195</a>&gt;, Zhu, Q., Feng, Ji. &amp; Huang, J. (2016) &lt;<a href="https://doi.org/10.1016%2Fj.patrec.2016.05.007">doi:10.1016/j.patrec.2016.05.007</a>&gt;, Huang, J., Zhu, Q., Yang, L. &amp; Feng, J. (2015) &lt;<a href="https://doi.org/10.1016%2Fj.knosys.2015.10.014">doi:10.1016/j.knosys.2015.10.014</a>&gt;, Tang, B. &amp; Haibo, He. (2017) &lt;<a href="https://doi.org/10.1016%2Fj.neucom.2017.02.039">doi:10.1016/j.neucom.2017.02.039</a>&gt; and Gao, J., Hu, W., Zhang, X. &amp; Wu, Ou. (2011) &lt;<a href="https://doi.org/10.1007%2F978-3-642-20847-8_23">doi:10.1007/978-3-642-20847-8_23</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/jhmadsen/DDoutlier">https://github.com/jhmadsen/DDoutlier</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Imports:</td>
<td>dbscan, proxy, pracma</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-05-29 21:18:22 UTC; Jacob</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-05-30 13:24:41 UTC</td>
</tr>
</table>
<hr>
<h2 id='COF'>Connectivity-based Outlier Factor (COF) algorithm</h2><span id='topic+COF'></span>

<h3>Description</h3>

<p>Function to calculate the connectivity-based outlier factor as an outlier score for observations. Suggested by Tang, J., Chen, Z., Fu, A. W. C., &amp; Cheung, D. W. (2002)</p>


<h3>Usage</h3>

<pre><code class='language-R'>COF(dataset, k = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="COF_+3A_dataset">dataset</code></td>
<td>
<p>The dataset for which observations have a COF score returned</p>
</td></tr>
<tr><td><code id="COF_+3A_k">k</code></td>
<td>
<p>The number of k-nearest neighbors to construct a SBN-path with, being the number of neighbors for each observation to compare chaining-distance with. k has to be smaller than the number of observations in dataset</p>
</td></tr>
</table>


<h3>Details</h3>

<p>COF computes the connectivity-based outlier factor for observations, being the comparison of chaining-distances between observation subject to outlier scoring and neighboring observations.
The COF function is useful for outlier detection in clustering and other multidimensional domains.</p>


<h3>Value</h3>

<p>A vector of COF scores for observations. The greater the COF, the greater outlierness</p>


<h3>Author(s)</h3>

<p>Jacob H. Madsen</p>


<h3>References</h3>

<p>Tang, J., Chen, Z., Fu, A. W. C., &amp; Cheung, D. W. (2002). Enhancing Effectiveness of Outlier Detections for Low Density Patterns. In Pacific-Asia Conf. on Knowledge Discovery and Data Mining (PAKDD). Taipei. pp. 535-548. DOI: 10.1007/3-540-47887-6_53</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create dataset
X &lt;- iris[,1:4]

# Find outliers by setting an optional k
outlier_score &lt;- COF(dataset=X, k=10)

# Sort and find index for most outlying observations
names(outlier_score) &lt;- 1:nrow(X)
sort(outlier_score, decreasing = TRUE)

# Inspect the distribution of outlier scores
hist(outlier_score)
</code></pre>

<hr>
<h2 id='DB'>Distance-based outlier detection based on user-given neighborhood size</h2><span id='topic+DB'></span>

<h3>Description</h3>

<p>Function to calculate how many observations are within a certain sized neighborhood as an outlier score. Outliers are classified according to a user-given threshold of observations to be within the neighborhood. Suggested by Knorr, M., &amp; Ng, R. T. (1997)</p>


<h3>Usage</h3>

<pre><code class='language-R'>DB(dataset, d = 1, fraction = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DB_+3A_dataset">dataset</code></td>
<td>
<p>The dataset for which observations are classified as outliers/inliers</p>
</td></tr>
<tr><td><code id="DB_+3A_d">d</code></td>
<td>
<p>The radius of the neighborhood</p>
</td></tr>
<tr><td><code id="DB_+3A_fraction">fraction</code></td>
<td>
<p>The proportion of the number of observations to be within the neighborhood for observations to be classified as inliers. If the proportion of observations within the neighborhood is less than the given fraction, observations are classified as outliers</p>
</td></tr>
</table>


<h3>Details</h3>

<p>DB computes a neighborhood for each observation given a radius (argument 'd') and returns the number of neighbors within the neighborhood. Observations are classified as inliers or outliers, based on a proportion (argument 'fraction') of observations to be within the neighborhood</p>


<h3>Value</h3>

<table>
<tr><td><code>neighbors</code></td>
<td>
<p>The number of neighbors within the neighborhood</p>
</td></tr>
<tr><td><code>classification</code></td>
<td>
<p>Binary classification of observations as inlier or outlier</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jacob H. Madsen</p>


<h3>References</h3>

<p>Knorr, M., &amp; Ng, R. T. (1997). A Unified Approach for Mining Outliers. In Conf. of the Centre for Advanced Studies on Collaborative Research (CASCON). Toronto, Canada. pp. 236-248. DOI: 10.1145/782010.782021</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create dataset
X &lt;- iris[,1:4]

# Classify observations
cls_observations &lt;- DB(dataset=X, d=1, fraction=0.05)$classification

# Remove outliers from dataset
X &lt;- X[cls_observations=='Inlier',]
</code></pre>

<hr>
<h2 id='INFLO'>Influenced Outlierness (INFLO) algorithm</h2><span id='topic+INFLO'></span>

<h3>Description</h3>

<p>Function to calculate the influenced outlierness as an outlier score for observations. Suggested by Jin, W., Tung, A. K. H., Han, J., &amp; Wang, W. (2006)</p>


<h3>Usage</h3>

<pre><code class='language-R'>INFLO(dataset, k = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="INFLO_+3A_dataset">dataset</code></td>
<td>
<p>The dataset for which observations have an INFLO score returned</p>
</td></tr>
<tr><td><code id="INFLO_+3A_k">k</code></td>
<td>
<p>The number of reverse k-nearest neighbors to compare density with. k has to be smaller than the number of observations in dataset</p>
</td></tr>
</table>


<h3>Details</h3>

<p>INFLO computes the influenced outlierness score for observations, being the comparison of density in neighborhood of observation subject to outlier scoring and density in the reverse neighborhood. A kd-tree is used for kNN computation, using the kNN() function from the 'dbscan' package. The INFLO function is useful for outlier detection in clustering and other multidimensional domains</p>


<h3>Value</h3>

<p>A vector of INFLO scores for observations. The greater the INFLO, the greater outlierness</p>


<h3>Author(s)</h3>

<p>Jacob H. Madsen</p>


<h3>References</h3>

<p>Jin, W., Tung, A. K. H., Han, J., &amp; Wang, W. (2006). Ranking Outliers Using Symmetric Neighborhood Relationship. In Pacific-Asia Conf. on Knowledge Discovery and Data Mining (PAKDD). Singapore. pp 577-593. DOI: 10.1007/11731139_68</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create dataset
X &lt;- iris[,1:4]

# Find outliers by setting an optional k
outlier_score &lt;- INFLO(dataset=X, k=10)

# Sort and find index for most outlying observations
names(outlier_score) &lt;- 1:nrow(X)
sort(outlier_score, decreasing = TRUE)

# Inspect the distribution of outlier scores
hist(outlier_score)
</code></pre>

<hr>
<h2 id='KDEOS'>Kernel Density Estimation Outlier Score (KDEOS) algorithm with gaussian kernel</h2><span id='topic+KDEOS'></span>

<h3>Description</h3>

<p>Function to calculate a density estimation as an outlier score for observations, over a range of k-nearest neighbors. Suggested by Schubert, E., Zimek, A. &amp; Kriegel, H-P. (2014)</p>


<h3>Usage</h3>

<pre><code class='language-R'>KDEOS(dataset, k_min = 5, k_max = 10, eps = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KDEOS_+3A_dataset">dataset</code></td>
<td>
<p>The dataset for which observations have an KDEOS score returned</p>
</td></tr>
<tr><td><code id="KDEOS_+3A_k_min">k_min</code></td>
<td>
<p>The k parameter starting the k-range</p>
</td></tr>
<tr><td><code id="KDEOS_+3A_k_max">k_max</code></td>
<td>
<p>The k parameter ending the k-range. Has to be smaller than the number of observations in dataset and greater than or equal to k_min</p>
</td></tr>
<tr><td><code id="KDEOS_+3A_eps">eps</code></td>
<td>
<p>An optional minimum bandwidth. If eps is smaller than the mean reachability distance for observations, eps is used. Otherwise mean reachability distance is used as bandwidth</p>
</td></tr>
</table>


<h3>Details</h3>

<p>KDEOS computes a kernel density estimation over a user-given range of k-nearest neighbors. The score is normalized between 0 and 1, such that observation with 1 has the lowest density estimation and greatest outlierness. A gaussian kernel is used for estimation with a bandwidth being the reachability distance for neighboring observations. If a lower user-given bandwidth is desired, putting more weight on outlying observations, eps has to be lower than the mean reachability distance for observations.
A kd-tree is used for kNN computation, using the kNN() function from the 'dbscan' package. The KDEOS function is useful for outlier detection in clustering and other multidimensional domains</p>


<h3>Value</h3>

<p>A vector of KDEOS scores normalized between 1 and 0, with 1 being the greatest outlierness</p>


<h3>Author(s)</h3>

<p>Jacob H. Madsen</p>


<h3>References</h3>

<p>Schubert, E., Zimek, A. &amp; Kriegel, H-P. (2014). Generalized Outlier Detection with Flexible Kernel Density Estimates. Proceedings of the 2014 SIAM International Conference on Data Mining. Philadelphia, USA. pp. 542-550. DOI: 10.1137/1.9781611973440.63</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create dataset
X &lt;- iris[,1:4]

# Find outliers by setting an optional range of k's
outlier_score &lt;- KDEOS(dataset=X, k_min=10, k_max=15)

# Sort and find index for most outlying observations
names(outlier_score) &lt;- 1:nrow(X)
sort(outlier_score, decreasing = TRUE)

# Inspect the distribution of outlier scores
hist(outlier_score)
</code></pre>

<hr>
<h2 id='KNN_AGG'>Aggregated k-nearest neighbors distance over different k's</h2><span id='topic+KNN_AGG'></span>

<h3>Description</h3>

<p>Function to calculate aggregated distance to k-nearest neighbors over a range of k's, as an outlier score. Suggested by Angiulli, F., &amp; Pizzuti, C. (2002)</p>


<h3>Usage</h3>

<pre><code class='language-R'>KNN_AGG(dataset, k_min = 5, k_max = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KNN_AGG_+3A_dataset">dataset</code></td>
<td>
<p>The dataset for which observations have an aggregated k-nearest neighbors distance returned</p>
</td></tr>
<tr><td><code id="KNN_AGG_+3A_k_min">k_min</code></td>
<td>
<p>The k parameter starting the k-range</p>
</td></tr>
<tr><td><code id="KNN_AGG_+3A_k_max">k_max</code></td>
<td>
<p>The k parameter ending the k-range. Has to be smaller than the number of observations in dataset and greater than or equal to k_min</p>
</td></tr>
</table>


<h3>Details</h3>

<p>KNN_AGG computes the aggregated distance to neighboring observations by aggregating the results from k_min-NN to k_max-NN, such that if k_min=1 and k_max=3, results from 1NN, 2NN and 3NN are aggregated. A kd-tree is used for kNN computation, using the kNN function() from the 'dbscan' package.
The KNN_AGG function is useful for outlier detection in clustering and other multidimensional domains.
</p>


<h3>Value</h3>

<p>A vector of aggregated distance for observations. The greater the distance, the greater outlierness</p>


<h3>Author(s)</h3>

<p>Jacob H. Madsen</p>


<h3>References</h3>

<p>Angiulli, F., &amp; Pizzuti, C. (2002). Fast Outlier Detection in High Dimensional Spaces. In Int. Conf. on Knowledge Discovery and Data Mining (SIGKDD). Helsinki, Finland. pp. 15-26. DOI: 10.1007/3-540-45681-3_2</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create dataset
X &lt;- iris[,1:4]

# Find outliers by setting a range of k's
outlier_score &lt;- KNN_AGG(dataset=X, k_min=10, k_max=15)

# Sort and find index for most outlying observations
names(outlier_score) &lt;- 1:nrow(X)
sort(outlier_score, decreasing = TRUE)

# Inspect the distribution of outlier scores
hist(outlier_score)
</code></pre>

<hr>
<h2 id='KNN_IN'>In-degree for observations in a k-nearest neighbors graph</h2><span id='topic+KNN_IN'></span>

<h3>Description</h3>

<p>Function to calculate in-degree as an outlier score for observations, given a k-nearest neighbors graph. Suggested by Hautamaki, V., &amp; Ismo, K. (2004)</p>


<h3>Usage</h3>

<pre><code class='language-R'>KNN_IN(dataset, k = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KNN_IN_+3A_dataset">dataset</code></td>
<td>
<p>The dataset for which observations have an in-degree returned</p>
</td></tr>
<tr><td><code id="KNN_IN_+3A_k">k</code></td>
<td>
<p>The number of k-nearest neighbors to construct a graph with. Has to be smaller than the number of observations in dataset</p>
</td></tr>
</table>


<h3>Details</h3>

<p>KNN_IN computes the in-degree, being the number of reverse neighbors. For computing the in-degree, a k-nearest neighbors graph is computed. A kd-tree is used for kNN computation, using the kNN() function from the 'dbscan' package.
The KNN_IN function is useful for outlier detection in clustering and other multidimensional domains.</p>


<h3>Value</h3>

<p>A vector of in-degree for observations. The smaller the in-degree, the greater outlierness</p>


<h3>Author(s)</h3>

<p>Jacob H. Madsen</p>


<h3>References</h3>

<p>Hautamaki, V., &amp; Ismo, K. (2004). Outlier Detection Using k-Nearest Neighbour Graph. In International Conference on Pattern Recognition. Cambridge, UK. pp. 430-433. DOI: 10.1109/ICPR.2004.1334558</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create dataset
X &lt;- iris[,1:4]

# Find outliers by setting an optional k
outlier_score &lt;- KNN_IN(dataset=X, k=10)

# Sort and find index for most outlying observations
names(outlier_score) &lt;- 1:nrow(X)
sort(outlier_score, decreasing = FALSE)

# Inspect the distribution of outlier scores
hist(outlier_score)
</code></pre>

<hr>
<h2 id='KNN_SUM'>Sum of distance to k-nearest neighbors</h2><span id='topic+KNN_SUM'></span>

<h3>Description</h3>

<p>Function to calculate sum of distance to k-nearest neighbors as an outlier score, based on a kd-tree</p>


<h3>Usage</h3>

<pre><code class='language-R'>KNN_SUM(dataset, k=5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KNN_SUM_+3A_dataset">dataset</code></td>
<td>
<p>The dataset for which observations have a summed k-nearest neighbors distance returned</p>
</td></tr>
<tr><td><code id="KNN_SUM_+3A_k">k</code></td>
<td>
<p>The number of k-nearest neighbors. k has to be smaller than the number of observations in dataset</p>
</td></tr>
</table>


<h3>Details</h3>

<p>KNN_SUM computes the sum of distance to neighboring observations. A kd-tree is used for kNN computation, using the kNN() function from the 'dbscan' package.
The KNN_SUM function is useful for outlier detection in clustering and other multidimensional domains.</p>


<h3>Value</h3>

<p>A vector of summed distance for observations. The greater distance, the greater outlierness</p>


<h3>Author(s)</h3>

<p>Jacob H. Madsen</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create dataset and set an optional k
X &lt;- iris[,1:4]
K &lt;- 5

# Find outliers
outlier_score &lt;- KNN_SUM(dataset=X, k=K)

# Sort and find index for most outlying observations
names(outlier_score) &lt;- 1:nrow(X)
sort(outlier_score, decreasing = TRUE)

# Inspect the distribution of outlier scores
hist(outlier_score)
</code></pre>

<hr>
<h2 id='LDF'>Local Density Factor (LDF) algorithm with gaussian kernel</h2><span id='topic+LDF'></span>

<h3>Description</h3>

<p>Function to calculate a Local Density Estimate (LDE) and Local Density Factor (LDF), as an outlier score, with a gaussian kernel. Suggested by Latecki, L., Lazarevic, A. &amp; Prokrajac, D. (2007)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LDF(dataset, k = 5, h = 1, c = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LDF_+3A_dataset">dataset</code></td>
<td>
<p>The dataset for which observations have an LDE and LDF score returned</p>
</td></tr>
<tr><td><code id="LDF_+3A_k">k</code></td>
<td>
<p>The number of k-nearest neighbors to compare density estimation with. k has to be smaller than number of observations in dataset</p>
</td></tr>
<tr><td><code id="LDF_+3A_h">h</code></td>
<td>
<p>User-given bandwidth for kernel functions. The greater the bandwidth, the smoother kernels and lesser weight are put on outliers. Default is 1</p>
</td></tr>
<tr><td><code id="LDF_+3A_c">c</code></td>
<td>
<p>Scaling constant for comparison of LDE to neighboring observations. LDF is the comparison of average LDE for an observation and its neighboring observations. Thus, c=1 gives results in an LDF between 0 and 1, while c=0 can result in very large or infinite values of LDF. Default is 1</p>
</td></tr>
</table>


<h3>Details</h3>

<p>LDF computes a kernel density estimation, called LDE, over a user-given number of k-nearest neighbors. The LDF score is the comparison of Local Density Estimate (LDE) for an observation to its neighboring observations. Naturally, if an observation has a greater LDE than its neighboring observations, it has no outlierness whereas an observation with smaller LDE than its neighboring observations has great outlierness. A kd-tree is used for kNN computation, using the kNN() function from the 'dbscan' package. The LDF function is useful for outlier detection in clustering and other multidimensional domains
</p>


<h3>Value</h3>

<table>
<tr><td><code>LDE</code></td>
<td>
<p>A vector of Local Density Estimate for observations. The greater the LDE, the greater centrality</p>
</td></tr>
<tr><td><code>LDF</code></td>
<td>
<p>A vector of Local Density Factor for observations. The greater the LDF, the greater the outlierness</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jacob H. Madsen</p>


<h3>References</h3>

<p>Latecki, L., Lazarevic, A. &amp; Prokrajac, D. (2007). Outlier Detection with Kernel Density Functions. International Workshop on Machine Learning and Data Mining in Pattern Recognition: Machine Learning and Data Mining in Pattern Recognition. pp. 61-75. DOI: 10.1007/978-3-540-73499-4_6</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create dataset
X &lt;- iris[,1:4]

# Find outliers by setting an optional range of k's
outlier_score &lt;- LDF(dataset=X, k=10, h=2, c=1)$LDF

# Sort and find index for most outlying observations
names(outlier_score) &lt;- 1:nrow(X)
sort(outlier_score, decreasing = TRUE)

# Inspect the distribution of outlier scores
hist(outlier_score)
</code></pre>

<hr>
<h2 id='LDOF'>Local Distance-based Outlier Factor (LDOF) algorithm</h2><span id='topic+LDOF'></span>

<h3>Description</h3>

<p>Function to calculate Local Distance-based Outlier Factor (LDOF) as an outlier score for observations. Suggested by Zhang, K., Hutter, M. &amp; Jin, H. (2009)</p>


<h3>Usage</h3>

<pre><code class='language-R'>LDOF(dataset, k = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LDOF_+3A_dataset">dataset</code></td>
<td>
<p>The dataset for which observations have an LDOF score returned</p>
</td></tr>
<tr><td><code id="LDOF_+3A_k">k</code></td>
<td>
<p>The number of nearest neighbors to compare distances with</p>
</td></tr>
</table>


<h3>Details</h3>

<p>LDOF computes distance for an observations to its to k-nearest neighbors and compare the distance with the average distances between the nearest neighbors. The LDOF function is useful for outlier detection in clustering and other multidimensional domains
</p>


<h3>Value</h3>

<p>A vector of LDOF scores for observations. The greater the LDOF score, the greater outlierness
</p>


<h3>Author(s)</h3>

<p>Jacob H. Madsen</p>


<h3>References</h3>

<p>Zhang, K., Hutter, M. &amp; Jin, H. (2009). A New Local Distance-based Outlier Detection Approach for Scattered Real-World Data. Pacific-Asia Conference on Knowledge Discovery and Data Mining: Advances in Knowledge Discovery and Data Mining. pp. 813-822. DOI: 10.1007/978-3-642-01307-2_84</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create dataset
X &lt;- iris[,1:4]

# Find outliers by setting an optional range of k's
outlier_score &lt;- LDOF(dataset=X, k=10)

# Sort and find index for most outlying observations
names(outlier_score) &lt;- 1:nrow(X)
sort(outlier_score, decreasing = TRUE)

# Inspect the distribution of outlier scores
hist(outlier_score)
</code></pre>

<hr>
<h2 id='LOCI'>Local Correlation Integral (LOCI) algorithm with constant nearest neighbor parameter</h2><span id='topic+LOCI'></span>

<h3>Description</h3>

<p>Function to calculate Local Correlation Integral (LOCI) as an outlier score for observations. Suggested by Papadimitriou, S., Gibbons, P. B., &amp; Faloutsos, C. (2003). Uses a k number of nearest neighbors instead of a constant radius</p>


<h3>Usage</h3>

<pre><code class='language-R'>LOCI(dataset, alpha = 0.5, nn = 20, k = 3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LOCI_+3A_dataset">dataset</code></td>
<td>
<p>The dataset for which observations have a LOCI returned</p>
</td></tr>
<tr><td><code id="LOCI_+3A_alpha">alpha</code></td>
<td>
<p>The parameter setting the size of the sampling neighborhood, as a proportion of the counting neighborhood, for observations to identify other observations in their respective neighborhood. An alpha of 1 equals a sampling neighborhood the size of the counting neighborhood (the size of distance to nn). An alpha of 0.5 equals a sampling neighborhood half the size of the counting neighborhood</p>
</td></tr>
<tr><td><code id="LOCI_+3A_nn">nn</code></td>
<td>
<p>The number of nearest neighbors to compare sampling neighborhood with. Original paper suggest a constant user-given radius that includes at least 20 neighbors in order to introduce statistical errors in MDEF. Default is 20</p>
</td></tr>
<tr><td><code id="LOCI_+3A_k">k</code></td>
<td>
<p>The number of standard deviations the sampling neighborhood of an observation should differ from the sampling neighborhood of neighboring observations, to be an outlier. Default is set to 3 as used in original papers experiments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>LOCI computes a counting neighborhood to the nn nearest observations, where the radius is equal to the outermost observation. Within the counting neighborhood each observation has a sampling neighborhood of which the size is determined by the alpha input parameter. LOCI returns an outlier score based on the standard deviation of the sampling neighborhood, called the multi-granularity deviation factor (MDEF). The LOCI function is useful for outlier detection in clustering and other multidimensional domains</p>


<h3>Value</h3>

<table>
<tr><td><code>npar_pi</code></td>
<td>
<p>A vector of the number of observations within the sample neighborhood for observations</p>
</td></tr>
<tr><td><code>avg_npar</code></td>
<td>
<p>A vector of average number of observations within the sample neighborhood for neighboring observations</p>
</td></tr>
<tr><td><code>sd_npar</code></td>
<td>
<p>A vector of standard deviations for observations sample neighborhood</p>
</td></tr>
<tr><td><code>MDEF</code></td>
<td>
<p>A vector of the multi-granularity deviation factor (MDEF) for observations. The greater the MDEF, the greater the outlierness</p>
</td></tr>
<tr><td><code>norm_MDEF</code></td>
<td>
<p>A vector of normalized MDEF-values, being sd_npar/avg_npar</p>
</td></tr>
<tr><td><code>class</code></td>
<td>
<p>Classification of observations as inliers/outliers following the rule of k</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jacob H. Madsen</p>


<h3>References</h3>

<p>Papadimitriou, S., Gibbons, P. B., &amp; Faloutsos, C. (2003). LOCI: Fast Outlier Detection Using the Local Correlation Integral. In International Conference on Data Engineering. pp. 315-326. DOI: 10.1109/ICDE.2003.1260802</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create dataset
X &lt;- iris[,1:4]

# Classify observations
cls_observations &lt;- LOCI(dataset=X, alpha=0.5, nn=20, k=1)$class

# Remove outliers from dataset
X &lt;- X[cls_observations=='Inlier',]
</code></pre>

<hr>
<h2 id='LOF'>Local Outlier Factor (LOF) algorithm</h2><span id='topic+LOF'></span>

<h3>Description</h3>

<p>Function to calculate the Local Outlier Factor (LOF) as an outlier score for observations. Suggested by Breunig, M. M., Kriegel, H.-P., Ng, R. T., &amp; Sander, J. (2000)</p>


<h3>Usage</h3>

<pre><code class='language-R'>LOF(dataset, k = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LOF_+3A_dataset">dataset</code></td>
<td>
<p>The dataset for which observations have an LOF score returned</p>
</td></tr>
<tr><td><code id="LOF_+3A_k">k</code></td>
<td>
<p>The number of k-nearest neighbors to compare density with. k has to be smaller than number of observations in dataset</p>
</td></tr>
</table>


<h3>Details</h3>

<p>LOF computes a local density for observations with a user-given k-nearest neighbors. The density is compared to the density of the respective nearest neighbors, resulting in the local outlier factor.
A kd-tree is used for kNN computation, using the kNN() function from the 'dbscan' package. The LOF function is useful for outlier detection in clustering and other multidimensional domains</p>


<h3>Value</h3>

<p>A vector of LOF scores for observations. The greater the LOF, the greater outlierness</p>


<h3>Author(s)</h3>

<p>Jacob H. Madsen</p>


<h3>References</h3>

<p>Breunig, M. M., Kriegel, H.-P., Ng, R. T., &amp; Sander, J. (2000). LOF: Identifying Density-Based Local Outliers. In Int. Conf. On Management of Data. Dallas, TX. pp. 93-104. DOI: 10.1145/342009.335388</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create dataset
X &lt;- iris[,1:4]

# Find outliers by setting an optional k
outlier_score &lt;- LOF(dataset=X, k=10)

# Sort and find index for most outlying observations
names(outlier_score) &lt;- 1:nrow(X)
sort(outlier_score, decreasing = TRUE)

# Inspect the distribution of outlier scores
hist(outlier_score)
</code></pre>

<hr>
<h2 id='LOOP'>Local Outlier Probability (LOOP) algorithm</h2><span id='topic+LOOP'></span>

<h3>Description</h3>

<p>Function to calculate the Local Outlier Probability (LOOP) as an outlier score for observations. Suggested by Kriegel, H.-P., Kröger, P., Schubert, E., &amp; Zimek, A. (2009)</p>


<h3>Usage</h3>

<pre><code class='language-R'>LOOP(dataset, k = 5, lambda = 3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LOOP_+3A_dataset">dataset</code></td>
<td>
<p>The dataset for which observations have a LOOP score returned</p>
</td></tr>
<tr><td><code id="LOOP_+3A_k">k</code></td>
<td>
<p>The number of k-nearest neighbors to compare density with</p>
</td></tr>
<tr><td><code id="LOOP_+3A_lambda">lambda</code></td>
<td>
<p>Multiplication factor for standard deviation. The greater lambda, the smoother results. Default is 3 as used in original papers experiments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>LOOP computes a local density based on probabilistic set distance for observations, with a user-given k-nearest neighbors. The density is compared to the density of the respective nearest neighbors, resulting in the local outlier probability. The values ranges from 0 to 1, with 1 being the greatest outlierness.
A kd-tree is used for kNN computation, using the kNN() function from the 'dbscan' package. The LOOP function is useful for outlier detection in clustering and other multidimensional domains</p>


<h3>Value</h3>

<p>A vector of LOOP scores for observations. 1 indicates outlierness and 0 indicate inlierness
</p>


<h3>Author(s)</h3>

<p>Jacob H. Madsen</p>


<h3>References</h3>

<p>Kriegel, H.-P., Kröger, P., Schubert, E., &amp; Zimek, A. (2009). LoOP: Local Outlier Probabilities. In ACM Conference on Information and Knowledge Management, CIKM 2009, Hong Kong, China. pp. 1649-1652. DOI: 10.1145/1645953.1646195</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create dataset
X &lt;- iris[,1:4]

# Find outliers by setting an optional k
outlier_score &lt;- LOOP(dataset=X, k=10, lambda=3)

# Sort and find index for most outlying observations
names(outlier_score) &lt;- 1:nrow(X)
sort(outlier_score, decreasing = TRUE)

# Inspect the distribution of outlier scores
hist(outlier_score)
</code></pre>

<hr>
<h2 id='NAN'>Natural Neighbor (NAN) algorithm to return the self-adaptive neighborhood</h2><span id='topic+NAN'></span>

<h3>Description</h3>

<p>Function to identify natural neighbors and the right k-parameter for kNN graphs as suggested by Zhu, Q., Feng, Ji. &amp; Huang, J. (2016)</p>


<h3>Usage</h3>

<pre><code class='language-R'>NAN(dataset, NaN_Edges = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NAN_+3A_dataset">dataset</code></td>
<td>
<p>The dataset for which natural neighbors are identified along with a k-parameter</p>
</td></tr>
<tr><td><code id="NAN_+3A_nan_edges">NaN_Edges</code></td>
<td>
<p>Choice for computing natural neighbors. Computational heavy to compute</p>
</td></tr>
</table>


<h3>Details</h3>

<p>NAN computes the natural neighbor eigenvalue and identifies natural neighbors in a dataset. The natural neighbor eigenvalue is powerful as k-parameter for computing a k-nearest neighborhood, being suitable for outlier detection, clustering or predictive modelling. Natural neighbors are defined as two observations being mutual k-nearest neighbors.
A kd-tree is used for kNN computation, using the kNN() function from the 'dbscan' package</p>


<h3>Value</h3>

<table>
<tr><td><code>NaN_Num</code></td>
<td>
<p>The number of in-degrees for observations given r</p>
</td></tr>
<tr><td><code>r</code></td>
<td>
<p>Natural neighbor eigenvalue. Useful as k-parameter</p>
</td></tr>
<tr><td><code>NaN_Edges</code></td>
<td>
<p>Matrix of edges for natural neighbors</p>
</td></tr>
<tr><td><code>n_NaN</code></td>
<td>
<p>The number of natural neighbors</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jacob H. Madsen</p>


<h3>References</h3>

<p>Zhu, Q., Feng, Ji. &amp; Huang, J. (2016). Natural neighbor: A self-adaptive neighborhood method without parameter K. Pattern Recognition Letters. pp. 30-36. DOI: 10.1016/j.patrec.2016.05.007</p>


<h3>Examples</h3>

<pre><code class='language-R'># Select dataset
X &lt;- iris[,1:4]

# Identify the right k-parameter
K &lt;- NAN(X, NaN_Edges=FALSE)$r

# Use the k-setting in an abitrary outlier detection algorithm
outlier_score &lt;- LOF(dataset=X, k=K)

# Sort and find index for most outlying observations
names(outlier_score) &lt;- 1:nrow(X)
sort(outlier_score, decreasing = TRUE)

# Inspect the distribution of outlier scores
hist(outlier_score)
</code></pre>

<hr>
<h2 id='NOF'>Natural Outlier Factor (NOF) algorithm</h2><span id='topic+NOF'></span>

<h3>Description</h3>

<p>Function to calculate the Natural Outlier Factor (NOF) as an outlier score for observations. Suggested by Huang, J., Zhu, Q., Yang, L. &amp; Feng, J. (2015)</p>


<h3>Usage</h3>

<pre><code class='language-R'>NOF(dataset)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NOF_+3A_dataset">dataset</code></td>
<td>
<p>The dataset for which observations have a NOF score returned</p>
</td></tr>
</table>


<h3>Details</h3>

<p>NOF computes the nearest and reverse nearest neighborhood for observations, based on the natural neighborhood algorithm. Density is compared between observations and their neighbors. A kd-tree is used for kNN computation, using the kNN() function from the 'dbscan' package</p>


<h3>Value</h3>

<table>
<tr><td><code>nb</code></td>
<td>
<p>A vector of in-degrees for observations</p>
</td></tr>
<tr><td><code>max_nb</code></td>
<td>
<p>Maximum in-degree observations in nb vector. Used as k-parameter in outlier detection of NOF</p>
</td></tr>
<tr><td><code>r</code></td>
<td>
<p>The natural neighbor eigenvalue</p>
</td></tr>
<tr><td><code>NOF</code></td>
<td>
<p>A vector of Natural Outlier Factor scores. The greater the NOF, the greater the outlierness</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Jacob H. Madsen</p>


<h3>References</h3>

<p>Huang, J., Zhu, Q., Yang, L. &amp; Feng, J. (2015). A non-parameter outlier detection algorithm based on Natural Neighbor. Knowledge-Based Systems. pp. 71-77. DOI: 10.1016/j.knosys.2015.10.014</p>


<h3>Examples</h3>

<pre><code class='language-R'># Select dataset
X &lt;- iris[,1:4]

# Run NOF algorithm
outlier_score &lt;- NOF(dataset=X)$NOF

# Sort and find index for most outlying observations
names(outlier_score) &lt;- 1:nrow(X)
sort(outlier_score, decreasing = TRUE)

# Inspect the distribution of outlier scores
hist(outlier_score)
</code></pre>

<hr>
<h2 id='RDOS'>Relative Density-based Outlier Factor (RDOS) algorithm with gaussian kernel</h2><span id='topic+RDOS'></span>

<h3>Description</h3>

<p>Function to calculate the Relative Density-based Outlier Factor (RDOS) as an outlier score for observations. Suggested by Tang, B. &amp; Haibo, He. (2017)</p>


<h3>Usage</h3>

<pre><code class='language-R'>RDOS(dataset, k = 5, h = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RDOS_+3A_dataset">dataset</code></td>
<td>
<p>The dataset for which observations have an RDOS score returned</p>
</td></tr>
<tr><td><code id="RDOS_+3A_k">k</code></td>
<td>
<p>The number of k-nearest neighbors used to identify reverse- and shared nearest neighbors</p>
</td></tr>
<tr><td><code id="RDOS_+3A_h">h</code></td>
<td>
<p>Bandwidth parameter for gaussian kernel. A small h put more weight on outlying observations</p>
</td></tr>
</table>


<h3>Details</h3>

<p>RDOS computes a kernel density estimation by combining the nearest, reverse nearest and shared neighbors into one neighborhood. The density estimation is compared to the density estimation of the neighborhoods observations. A gaussian kernel is used for density estimation, given a bandwidth chosen by user. A kd-tree is used for kNN computation, using the kNN() function from the 'dbscan' package.
</p>
<p>It is a computational heavy task to identify reverse and shared neighbors from the kd-tree. Thus, the RDOS has high complexity and is not recommended to apply to datasets with n&gt;5000. The RDOS function is useful for outlier detection in clustering and other multidimensional domains</p>


<h3>Value</h3>

<p>A vector of RDOS scores for observations. The greater the RDOS score, the greater outlierness
</p>


<h3>Author(s)</h3>

<p>Jacob H. Madsen</p>


<h3>References</h3>

<p>Tang, B. &amp; Haibo, He. (2017). A local density-based approach for outlier detection. Neurocomputing. pp. 171-180. DOI: 10.1016/j.neucom.2017.02.039</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create dataset
X &lt;- iris[,1:4]

# Find outliers by setting an optional k
outlier_score &lt;- RDOS(dataset=X, k=10, h=2)

# Sort and find index for most outlying observations
names(outlier_score) &lt;- 1:nrow(X)
sort(outlier_score, decreasing = TRUE)

# Inspect the distribution of outlier scores
hist(outlier_score)
</code></pre>

<hr>
<h2 id='RKOF'>Robust Kernel-based Outlier Factor (RKOF) algorithm with gaussian kernel</h2><span id='topic+RKOF'></span>

<h3>Description</h3>

<p>Function to to calculate the RKOF score for observations as suggested by Gao, J., Hu, W., Zhang, X. &amp; Wu, Ou. (2011)</p>


<h3>Usage</h3>

<pre><code class='language-R'>RKOF(dataset, k = 5, C = 1, alpha = 1, sigma2 = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RKOF_+3A_dataset">dataset</code></td>
<td>
<p>The dataset for which observations have an RKOF score returned</p>
</td></tr>
<tr><td><code id="RKOF_+3A_k">k</code></td>
<td>
<p>The number of nearest neighbors to compare density estimation with</p>
</td></tr>
<tr><td><code id="RKOF_+3A_c">C</code></td>
<td>
<p>Multiplication parameter for k-distance of neighboring observations. Act as bandwidth increaser. Default is 1 such that k-distance is used for the gaussian kernel</p>
</td></tr>
<tr><td><code id="RKOF_+3A_alpha">alpha</code></td>
<td>
<p>Sensivity parameter for k-distance/bandwidth. Small alpha creates small variance in RKOF and vice versa. Default is 1</p>
</td></tr>
<tr><td><code id="RKOF_+3A_sigma2">sigma2</code></td>
<td>
<p>Variance parameter for weighting of neighboring observations</p>
</td></tr>
</table>


<h3>Details</h3>

<p>RKOF computes a kernel density estimation by comparing density estimation to the density of neighboring observations. A gaussian kernel is used for density estimation, given a bandwidth with k-distance. K-distance can be influenced with the parameters C and alpha. A kd-tree is used for kNN computation, using the kNN() function from the 'dbscan' package.
The RKOF function is useful for outlier detection in clustering and other multidimensional domains
</p>


<h3>Value</h3>

<p>A vector of RKOF scores for observations. The greater the RKOF score, the greater outlierness
</p>


<h3>Author(s)</h3>

<p>Jacob H. Madsen</p>


<h3>References</h3>

<p>Gao, J., Hu, W., Zhang, X. &amp; Wu, Ou. (2011). RKOF: Robust Kernel-Based Local Outlier Detection. Pacific-Asia Conference on Knowledge Discovery and Data Mining: Advances in Knowledge Discovery and Data Mining. pp. 270-283. DOI: 10.1007/978-3-642-20847-8_23</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create dataset
X &lt;- iris[,1:4]

# Find outliers by setting an optional k
outlier_score &lt;- RKOF(dataset=X, k = 10, C = 1, alpha = 1, sigma2 = 1)

# Sort and find index for most outlying observations
names(outlier_score) &lt;- 1:nrow(X)
sort(outlier_score, decreasing = TRUE)

# Inspect the distribution of outlier scores
hist(outlier_score)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
