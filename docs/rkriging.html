<!DOCTYPE html><html><head><title>Help for package rkriging</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {rkriging}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Evaluate.Kernel'><p>Evaluate Kernel</p></a></li>
<li><a href='#Fit.Kriging'><p>Fit or Create Kriging Models</p></a></li>
<li><a href='#Gaussian.Kernel'><p>Gaussian Kernel</p></a></li>
<li><a href='#Generalized.Rational.Kriging'><p>Generalized Rational Kriging</p></a></li>
<li><a href='#Get.Kernel'><p>Kernel</p></a></li>
<li><a href='#Get.Kriging.Parameters'><p>Get Kriging Parameters</p></a></li>
<li><a href='#Limit.Kriging'><p>Limit Kriging</p></a></li>
<li><a href='#Matern.Kernel'><p>Generalized Matern Kernel</p></a></li>
<li><a href='#Matern12.Kernel'><p>Matern(1/2) Kernel</p></a></li>
<li><a href='#Matern32.Kernel'><p>Matern(3/2) Kernel</p></a></li>
<li><a href='#Matern52.Kernel'><p>Matern(5/2) Kernel</p></a></li>
<li><a href='#MultiplicativeMatern.Kernel'><p>Multiplicative Generalized Matern Kernel</p></a></li>
<li><a href='#MultiplicativeRQ.Kernel'><p>Multiplicative Rational Quadratic (RQ) Kernel</p></a></li>
<li><a href='#MultiplicativeUDF.Kernel'><p>Multiplicative User Defined Function (UDF) Kernel</p></a></li>
<li><a href='#Ordinary.Kriging'><p>Ordinary Kriging</p></a></li>
<li><a href='#Predict.Kriging'><p>Kriging Prediction</p></a></li>
<li><a href='#Rational.Kriging'><p>Rational Kriging</p></a></li>
<li><a href='#RQ.Kernel'><p>Rational Quadratic (RQ) Kernel</p></a></li>
<li><a href='#UDF.Kernel'><p>User Defined Function (UDF) Kernel</p></a></li>
<li><a href='#Universal.Kriging'><p>Universal Kriging</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Kriging Modeling</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0</td>
</tr>
<tr>
<td>Description:</td>
<td>An 'Eigen'-based computationally efficient 'C++' implementation for fitting various kriging models to data. This research is supported by U.S. National Science Foundation grant DMS-2310637.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.2)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, nloptr (&ge; 1.2.0), methods, stats</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppEigen (&ge; 0.3.3.5.0), BH (&ge; 1.75.0.0), nloptr (&ge;
1.2.0)</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-07-04 19:08:48 UTC; billhuang</td>
</tr>
<tr>
<td>Author:</td>
<td>Chaofan Huang [aut, cre],
  V. Roshan Joseph [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Chaofan Huang &lt;10billhuang01@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-07-08 05:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='Evaluate.Kernel'>Evaluate Kernel</h2><span id='topic+Evaluate.Kernel'></span>

<h3>Description</h3>

<p>This function computes the kernel (correlation) matrix.
Given the kernel class object and the input data <code class="reqn">X</code> of size n, 
this function computes the corresponding <code class="reqn">n\times n</code> kernel (correlation) matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Evaluate.Kernel(kernel, X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Evaluate.Kernel_+3A_kernel">kernel</code></td>
<td>
<p>a kernel class object</p>
</td></tr>
<tr><td><code id="Evaluate.Kernel_+3A_x">X</code></td>
<td>
<p>input data</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The kernel (correlation) matrix of X evaluated by the kernel.
</p>


<h3>Author(s)</h3>

<p>Chaofan Huang and V. Roshan Joseph
</p>


<h3>See Also</h3>

<p><a href="#topic+Get.Kernel">Get.Kernel</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 5
p &lt;- 3
X &lt;- matrix(rnorm(n*p), ncol=p)
lengthscale &lt;- c(1:p)

kernel &lt;- Gaussian.Kernel(lengthscale)
Evaluate.Kernel(kernel, X)

</code></pre>

<hr>
<h2 id='Fit.Kriging'>Fit or Create Kriging Models</h2><span id='topic+Fit.Kriging'></span>

<h3>Description</h3>

<p>This function provides a common interface to fit various kriging models from the data.
OK (<a href="#topic+Ordinary.Kriging">Ordinary.Kriging</a>), UK (<a href="#topic+Universal.Kriging">Universal.Kriging</a>), LK (<a href="#topic+Limit.Kriging">Limit.Kriging</a>), 
RK (<a href="#topic+Rational.Kriging">Rational.Kriging</a>), and GRK (<a href="#topic+Generalized.Rational.Kriging">Generalized.Rational.Kriging</a>) are supported in this package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Fit.Kriging(
  X,
  y,
  interpolation = TRUE,
  fit = TRUE,
  model = "OK",
  model.parameters = list(),
  kernel = NULL,
  kernel.parameters = list(),
  nlopt.parameters = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Fit.Kriging_+3A_x">X</code></td>
<td>
<p>a matrix for input (feature)</p>
</td></tr>
<tr><td><code id="Fit.Kriging_+3A_y">y</code></td>
<td>
<p>a vector for output (target), only one-dimensional output is supported</p>
</td></tr>
<tr><td><code id="Fit.Kriging_+3A_interpolation">interpolation</code></td>
<td>
<p>whether to interpolate, for noisy data please set <code>interpolate=FALSE</code></p>
</td></tr>
<tr><td><code id="Fit.Kriging_+3A_fit">fit</code></td>
<td>
<p>whether to fit the length scale parameters from data</p>
</td></tr>
<tr><td><code id="Fit.Kriging_+3A_model">model</code></td>
<td>
<p>choice of kriging model: OK, UK, LK, RK, GRK</p>
</td></tr>
<tr><td><code id="Fit.Kriging_+3A_model.parameters">model.parameters</code></td>
<td>
<p>a list of parameters required for the specific kriging model, e.g. basis functions for universal kriging</p>
</td></tr>
<tr><td><code id="Fit.Kriging_+3A_kernel">kernel</code></td>
<td>
<p>a kernel class object</p>
</td></tr>
<tr><td><code id="Fit.Kriging_+3A_kernel.parameters">kernel.parameters</code></td>
<td>
<p>a list of parameters required for the kernel, if no kernel class object is provided</p>
</td></tr>
<tr><td><code id="Fit.Kriging_+3A_nlopt.parameters">nlopt.parameters</code></td>
<td>
<p>a list of parameters required for NLopt, including choice of optimization algorithm and maximum number of evaluation</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Kriging gives the best linear unbiased predictor given the data. 
It can also be given a Bayesian interpretation by assuming a Gaussian process (GP) prior for the underlying function.
Please see Santner et al. (2003), Rasmussen and Williams (2006), and Joseph (2024) for details.
</p>
<p>For data from deterministic computer experiments, use <code>interpolation=TRUE</code> and will give an interpolator. 
For noisy data, use <code>interpolation=FALSE</code>, which will give an approximator of the underlying function.
Currently, approximation is supported for OK (<a href="#topic+Ordinary.Kriging">Ordinary.Kriging</a>) and UK (<a href="#topic+Universal.Kriging">Universal.Kriging</a>). 
</p>
<p>The kernel choices are required and can be specified by 
(i) providing the kernel class object to <code>kernel</code>
or (ii) specifying the kernel type and other parameters in <code>kernel.parameters</code>. 
Please see examples section for detail usages. 
</p>
<p>When the lengthscale / correlation parameters are known, simply provide them in 
(i) the kernel class object to <code>kernel</code> or (ii) in <code>kernel.parameters</code>,
and set <code>fit=FALSE</code>. The kriging model will be fitted with the user provided parameters.
</p>
<p>When the lengthscale / correlation parameters are unknown, 
they can be estimated via Maximum Likelihood method by setting <code>fit=TRUE</code>. 
The initial / lower bound / upper bound of the lengthscale parameters can be provided in <code>kernel.parameters</code>, 
otherwise a good initial and range would be estimated from the data. 
The optimization is performed via <a href="https://nlopt.readthedocs.io/en/latest/">NLopt</a>, 
a open-source library for nonlinear optimization. 
All gradient-free optimization methods in <a href="https://nlopt.readthedocs.io/en/latest/">NLopt</a> 
are supported and can be specified in <code>nlopt.parameters</code>.
See <code>nloptr::nloptr.print.options()</code> for the list of available derivative-free algorithms (prefix with NLOPT_GN or NLOPT_LN).  
The maximum number of optimization steps can also be defined in <code>nlopt.parameters</code>.
Please see examples section for detail usages.
</p>


<h3>Value</h3>

<p>A Kriging Class Object.
</p>


<h3>Author(s)</h3>

<p>Chaofan Huang and V. Roshan Joseph
</p>


<h3>References</h3>

<p>Joseph, V. R. (2006). <em>Limit kriging</em>. Technometrics, 48(4), 458-466.
</p>
<p>Joseph, V. R. (2024). Rational Kriging. <em>Journal of the American Statistical Association</em>.
</p>
<p>Rasmussen, C. E. &amp; Williams, C. K. (2006). <em>Gaussian Processes for Machine Learning</em>. The MIT Press.
</p>
<p>Santner, T. J., Williams, B. J., Notz, W. I., &amp; Williams, B. J. (2003). <em>The design and analysis of computer experiments (Vol. 1)</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><a href="#topic+Predict.Kriging">Predict.Kriging</a>, <a href="#topic+Get.Kriging.Parameters">Get.Kriging.Parameters</a>, <a href="#topic+Get.Kernel">Get.Kernel</a>, 
<a href="#topic+Ordinary.Kriging">Ordinary.Kriging</a>, <a href="#topic+Universal.Kriging">Universal.Kriging</a>, <a href="#topic+Limit.Kriging">Limit.Kriging</a>, 
<a href="#topic+Rational.Kriging">Rational.Kriging</a>, <a href="#topic+Generalized.Rational.Kriging">Generalized.Rational.Kriging</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># one dimensional example 
f &lt;- function(x) {
  x &lt;- 0.5 + 2*x
  y &lt;- sin(10*pi*x)/(2*x) + (x-1)^4
  return (y)
}

set.seed(1234)
# train set
n &lt;- 30
p &lt;- 1
X &lt;- matrix(runif(n),ncol=p)
y &lt;- apply(X, 1, f)
newX &lt;- matrix(seq(0,1,length=1001), ncol=p)

#############################################################################
################ Minimal Example for Fitting a Kriging Model ################
#############################################################################
# Ordinary Kriging
kriging &lt;- Fit.Kriging(X, y, interpolation=TRUE, fit=TRUE, model="OK",
                       kernel.parameters=list(type="Gaussian"))
pred &lt;- Predict.Kriging(kriging, newX)
plot(newX, f(newX), "l")
points(X, y, pch=16, col="blue")
lines(newX, pred$mean, col="red", lty=2)
lines(newX, pred$mean-2*pred$sd, col="red", lty=3)
lines(newX, pred$mean+2*pred$sd, col="red", lty=3)
Get.Kriging.Parameters(kriging)

# Universal Kriging
basis.function &lt;- function(x) {c(1,x[1],x[1]^2)}
kriging &lt;- Fit.Kriging(X, y, interpolation=TRUE, fit=TRUE, model="UK",
                       model.parameters=list(basis.function=basis.function),
                       kernel.parameters=list(type="Gaussian"))
pred &lt;- Predict.Kriging(kriging, newX)
plot(newX, f(newX), "l")
points(X, y, pch=16, col="blue")
lines(newX, pred$mean, col="red", lty=2)
lines(newX, pred$mean-2*pred$sd, col="red", lty=3)
lines(newX, pred$mean+2*pred$sd, col="red", lty=3)
Get.Kriging.Parameters(kriging)

# Limit Kriging
kriging &lt;- Fit.Kriging(X, y, interpolation=TRUE, fit=TRUE, model="LK",
                       kernel.parameters=list(type="Gaussian"))
pred &lt;- Predict.Kriging(kriging, newX)
plot(newX, f(newX), "l")
points(X, y, pch=16, col="blue")
lines(newX, pred$mean, col="red", lty=2)
lines(newX, pred$mean-2*pred$sd, col="red", lty=3)
lines(newX, pred$mean+2*pred$sd, col="red", lty=3)
Get.Kriging.Parameters(kriging)

# Rational Kriging
kriging &lt;- Fit.Kriging(X, y, interpolation=TRUE, fit=TRUE, model="RK",
                       kernel.parameters=list(type="RQ",alpha=1))
pred &lt;- Predict.Kriging(kriging, newX)
plot(newX, f(newX), "l")
points(X, y, pch=16, col="blue")
lines(newX, pred$mean, col="red", lty=2)
lines(newX, pred$mean-2*pred$sd, col="red", lty=3)
lines(newX, pred$mean+2*pred$sd, col="red", lty=3)
Get.Kriging.Parameters(kriging)

# Generalized Rational Kriging
kriging &lt;- Fit.Kriging(X, y, interpolation=TRUE, fit=TRUE, model="GRK",
                       kernel.parameters=list(type="RQ",alpha=1))
pred &lt;- Predict.Kriging(kriging, newX)
plot(newX, f(newX), "l")
points(X, y, pch=16, col="blue")
lines(newX, pred$mean, col="red", lty=2)
lines(newX, pred$mean-2*pred$sd, col="red", lty=3)
lines(newX, pred$mean+2*pred$sd, col="red", lty=3)
Get.Kriging.Parameters(kriging)

#############################################################################
################ Fitting a Kriging Model with Kernel Object #################
#############################################################################
kernel &lt;- Gaussian.Kernel(rep(1,p))
kriging &lt;- Fit.Kriging(X, y, interpolation=TRUE, fit=TRUE, model="OK", kernel=kernel)
pred &lt;- Predict.Kriging(kriging, newX)
plot(newX, f(newX), "l")
points(X, y, pch=16, col="blue")
lines(newX, pred$mean, col="red", lty=2)
lines(newX, pred$mean-2*pred$sd, col="red", lty=3)
lines(newX, pred$mean+2*pred$sd, col="red", lty=3)
Get.Kriging.Parameters(kriging)

#############################################################################
############### Creating a Kriging Model with Kernel Object #################
#############################################################################
# set fit = FALSE to create Kriging model with user provided kernel
# no optimization for the length scale parameters 
kernel &lt;- Gaussian.Kernel(rep(1e-1,p))
kriging &lt;- Fit.Kriging(X, y, interpolation=TRUE, fit=FALSE, model="OK", kernel=kernel)
pred &lt;- Predict.Kriging(kriging, newX)
plot(newX, f(newX), "l")
points(X, y, pch=16, col="blue")
lines(newX, pred$mean, col="red", lty=2)
lines(newX, pred$mean-2*pred$sd, col="red", lty=3)
lines(newX, pred$mean+2*pred$sd, col="red", lty=3) 
Get.Kriging.Parameters(kriging)

#############################################################################
############ Fitting a Kriging Model with Range for Lengthscale #############
#############################################################################
kernel.parameters &lt;- list(
    type = 'Gaussian',
    lengthscale = rep(1,p), # initial value
    lengthscale.lower.bound = rep(1e-3,p), # lower bound
    lengthscale.upper.bound = rep(1e3,p) # upper bound
) # if not provided, a good estimate would be computed from data
kriging &lt;- Fit.Kriging(X, y, interpolation=TRUE, fit=TRUE, model="OK",
                       kernel.parameters=kernel.parameters)
pred &lt;- Predict.Kriging(kriging, newX)
plot(newX, f(newX), "l")
points(X, y, pch=16, col="blue")
lines(newX, pred$mean, col="red", lty=2)
lines(newX, pred$mean-2*pred$sd, col="red", lty=3)
lines(newX, pred$mean+2*pred$sd, col="red", lty=3)
Get.Kriging.Parameters(kriging)

#############################################################################
########### Fitting a Kriging Model with Different Optimization #############
#############################################################################
nlopt.parameters &lt;- list(
    algorithm = 'NLOPT_GN_DIRECT', # optimization method
    maxeval = 250 # maximum number of evaluation
)
kriging &lt;- Fit.Kriging(X, y, interpolation=TRUE, fit=TRUE, model="OK",
                       kernel.parameters=list(type="Gaussian"),
                       nlopt.parameters=nlopt.parameters)
pred &lt;- Predict.Kriging(kriging, newX)
plot(newX, f(newX), "l")
points(X, y, pch=16, col="blue")
lines(newX, pred$mean, col="red", lty=2)
lines(newX, pred$mean-2*pred$sd, col="red", lty=3)
lines(newX, pred$mean+2*pred$sd, col="red", lty=3)
Get.Kriging.Parameters(kriging)

# Global-Local Optimization
nlopt.parameters &lt;- list(
    algorithm = 'NLOPT_GN_MLSL_LDS', # optimization method
    local.algorithm = 'NLOPT_LN_SBPLX', # local algorithm
    maxeval = 250 # maximum number of evaluation
)
kriging &lt;- Fit.Kriging(X, y, interpolation=TRUE, fit=TRUE, model="OK",
                       kernel.parameters=list(type="Gaussian"),
                       nlopt.parameters=nlopt.parameters)
pred &lt;- Predict.Kriging(kriging, newX)
plot(newX, f(newX), "l")
points(X, y, pch=16, col="blue")
lines(newX, pred$mean, col="red", lty=2)
lines(newX, pred$mean-2*pred$sd, col="red", lty=3)
lines(newX, pred$mean+2*pred$sd, col="red", lty=3)
Get.Kriging.Parameters(kriging)

#############################################################################
################# Fitting a Kriging Model from Noisy Data ###################
#############################################################################
y &lt;- y + 0.1 * rnorm(length(y))
kriging &lt;- Fit.Kriging(X, y, interpolation=FALSE, fit=TRUE, model="OK",
                       kernel.parameters=list(type="Gaussian"))
pred &lt;- Predict.Kriging(kriging, newX)
plot(newX, f(newX), "l")
points(X, y, pch=16, col="blue")
lines(newX, pred$mean, col="red", lty=2)
lines(newX, pred$mean-2*pred$sd, col="red", lty=3)
lines(newX, pred$mean+2*pred$sd, col="red", lty=3)
Get.Kriging.Parameters(kriging)

</code></pre>

<hr>
<h2 id='Gaussian.Kernel'>Gaussian Kernel</h2><span id='topic+Gaussian.Kernel'></span>

<h3>Description</h3>

<p>This function specifies the Gaussian / Squared Exponential (SE) / Radial Basis Function (RBF) kernel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Gaussian.Kernel(lengthscale)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Gaussian.Kernel_+3A_lengthscale">lengthscale</code></td>
<td>
<p>a vector for the positive length scale parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Gaussian kernel is given by 
</p>
<p style="text-align: center;"><code class="reqn">k(r)=\exp(-r^2/2),</code>
</p>

<p>where 
</p>
<p style="text-align: center;"><code class="reqn">r(x,x^{\prime})=\sqrt{\sum_{i=1}^{p}\left(\frac{x_{i}-x_{i}^{\prime}}{l_{i}}\right)^2}</code>
</p>
 
<p>is the euclidean distance between <code class="reqn">x</code> and <code class="reqn">x^{\prime}</code> weighted by
the length scale parameters <code class="reqn">l_{i}</code>'s.
</p>


<h3>Value</h3>

<p>A Gaussian Kernel Class Object.
</p>


<h3>Author(s)</h3>

<p>Chaofan Huang and V. Roshan Joseph
</p>


<h3>References</h3>

<p>Duvenaud, D. (2014). <em>The kernel cookbook: Advice on covariance functions</em>.
</p>
<p>Rasmussen, C. E. &amp; Williams, C. K. (2006). <em>Gaussian Processes for Machine Learning</em>. The MIT Press.
</p>


<h3>See Also</h3>

<p><a href="#topic+Get.Kernel">Get.Kernel</a>, <a href="#topic+Evaluate.Kernel">Evaluate.Kernel</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 5
p &lt;- 3
X &lt;- matrix(rnorm(n*p), ncol=p)
lengthscale &lt;- c(1:p)

# approach 1
kernel &lt;- Gaussian.Kernel(lengthscale)
Evaluate.Kernel(kernel, X)

# approach 2
kernel &lt;- Get.Kernel(lengthscale, type="Gaussian")
Evaluate.Kernel(kernel, X) 

</code></pre>

<hr>
<h2 id='Generalized.Rational.Kriging'>Generalized Rational Kriging</h2><span id='topic+Generalized.Rational.Kriging'></span>

<h3>Description</h3>

<p>This functions fits the generalized rational kriging model to the data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Generalized.Rational.Kriging(
  X,
  y,
  fit = TRUE,
  kernel = NULL,
  kernel.parameters = list(),
  nlopt.parameters = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Generalized.Rational.Kriging_+3A_x">X</code></td>
<td>
<p>a matrix for input (feature)</p>
</td></tr>
<tr><td><code id="Generalized.Rational.Kriging_+3A_y">y</code></td>
<td>
<p>a vector for output (target), only one-dimensional output is supported</p>
</td></tr>
<tr><td><code id="Generalized.Rational.Kriging_+3A_fit">fit</code></td>
<td>
<p>whether to fit the length scale parameters from data</p>
</td></tr>
<tr><td><code id="Generalized.Rational.Kriging_+3A_kernel">kernel</code></td>
<td>
<p>a kernel class object</p>
</td></tr>
<tr><td><code id="Generalized.Rational.Kriging_+3A_kernel.parameters">kernel.parameters</code></td>
<td>
<p>a list of parameters required for the kernel, if no kernel class object is provided</p>
</td></tr>
<tr><td><code id="Generalized.Rational.Kriging_+3A_nlopt.parameters">nlopt.parameters</code></td>
<td>
<p>a list of parameters required for NLopt, including choice of optimization algorithm and maximum number of evaluation</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Ordinary kriging and rational kriging can be obtained as special cases of generalized rational kriging. 
Please see Joseph (2024) for details. 
The <a href="https://spectralib.org/">Spectra</a> library is used for fast computation of the first eigenvalues/vectors. 
Only interpolation is available. Noisy output is not supported for generalized rational kriging.  
</p>
<p>The kernel choices are required and can be specified by 
(i) providing the kernel class object to <code>kernel</code>
or (ii) specifying the kernel type and other parameters in <code>kernel.parameters</code>. 
Please see examples section of <a href="#topic+Fit.Kriging">Fit.Kriging</a> for detail usages. 
</p>
<p>When the lengthscale / correlation parameters are unknown, 
all parameters including the constant mean can be estimated via Maximum Likelihood method by setting <code>fit=TRUE</code>. 
The initial / lower bound / upper bound of the lengthscale parameters can be provided in <code>kernel.parameters</code>, 
otherwise a good initial and range would be estimated from the data. 
The optimization is performed via <a href="https://nlopt.readthedocs.io/en/latest/">NLopt</a> library, 
a open-source library for nonlinear optimization. 
All gradient-free optimization methods in <a href="https://nlopt.readthedocs.io/en/latest/">NLopt</a> 
are supported and can be specified in <code>nlopt.parameters</code>.
See <code>nloptr::nloptr.print.options()</code> for the list of available derivative-free algorithms (prefix with NLOPT_GN or NLOPT_LN).  
The maximum number of optimization steps can also be defined in <code>nlopt.parameters</code>.
Please see examples section of <a href="#topic+Fit.Kriging">Fit.Kriging</a> for detail usages.
</p>


<h3>Value</h3>

<p>A Generalized Rational Kriging Class Object.
</p>


<h3>Author(s)</h3>

<p>Chaofan Huang and V. Roshan Joseph
</p>


<h3>References</h3>

<p>Joseph, V. R. (2024). Rational Kriging. <em>Journal of the American Statistical Association</em>.
</p>
<p>Qiu, Y., Guennebaud, G., &amp; Niesen, J. (2015). Spectra: C++ library for large scale eigenvalue problems.
</p>


<h3>See Also</h3>

<p><a href="#topic+Fit.Kriging">Fit.Kriging</a>, <a href="#topic+Predict.Kriging">Predict.Kriging</a>, <a href="#topic+Get.Kriging.Parameters">Get.Kriging.Parameters</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># one dimensional example 
f &lt;- function(x) {
  x &lt;- 0.5 + 2*x
  y &lt;- sin(10*pi*x)/(2*x) + (x-1)^4
  return (y)
}

set.seed(1234)
# train set
n &lt;- 30
p &lt;- 1
X &lt;- matrix(runif(n),ncol=p)
y &lt;- apply(X, 1, f)
newX &lt;- matrix(seq(0,1,length=1001), ncol=p)

# approach 1
kriging &lt;- Generalized.Rational.Kriging(X, y, fit=TRUE, 
                                        kernel.parameters=list(type="RQ",alpha=1))
pred &lt;- Predict.Kriging(kriging, newX)
plot(newX, f(newX), "l")
points(X, y, pch=16, col="blue")
lines(newX, pred$mean, col="red", lty=2)
lines(newX, pred$mean-2*pred$sd, col="red", lty=3)
lines(newX, pred$mean+2*pred$sd, col="red", lty=3)
Get.Kriging.Parameters(kriging)

# approach 2
kriging &lt;- Fit.Kriging(X, y, interpolation=TRUE, fit=TRUE, model="GRK",
                       kernel.parameters=list(type="RQ",alpha=1))
pred &lt;- Predict.Kriging(kriging, newX)
plot(newX, f(newX), "l")
points(X, y, pch=16, col="blue")
lines(newX, pred$mean, col="red", lty=2)
lines(newX, pred$mean-2*pred$sd, col="red", lty=3)
lines(newX, pred$mean+2*pred$sd, col="red", lty=3)
Get.Kriging.Parameters(kriging)

</code></pre>

<hr>
<h2 id='Get.Kernel'>Kernel</h2><span id='topic+Get.Kernel'></span>

<h3>Description</h3>

<p>This function provides a common interface to specify various kernels.
See arguments section for the available kernels in this package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Get.Kernel(lengthscale, type, parameters = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Get.Kernel_+3A_lengthscale">lengthscale</code></td>
<td>
<p>a vector for the positive length scale parameters</p>
</td></tr>
<tr><td><code id="Get.Kernel_+3A_type">type</code></td>
<td>
<p>kernel type: Gaussian, RQ, Matern12, Matern32, Matern52, Matern, UDF, MultiplicativeRQ, MultiplicativeMatern, MultiplicativeUDF</p>
</td></tr>
<tr><td><code id="Get.Kernel_+3A_parameters">parameters</code></td>
<td>
<p>a list of parameters required for the specific kernel</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A Kernel Class Object.
</p>


<h3>Author(s)</h3>

<p>Chaofan Huang and V. Roshan Joseph
</p>


<h3>References</h3>

<p>Duvenaud, D. (2014). <em>The kernel cookbook: Advice on covariance functions</em>.
</p>
<p>Rasmussen, C. E. &amp; Williams, C. K. (2006). <em>Gaussian Processes for Machine Learning</em>. The MIT Press.
</p>


<h3>See Also</h3>

<p><a href="#topic+Evaluate.Kernel">Evaluate.Kernel</a>, <a href="#topic+Gaussian.Kernel">Gaussian.Kernel</a>, <a href="#topic+RQ.Kernel">RQ.Kernel</a>, 
<a href="#topic+Matern12.Kernel">Matern12.Kernel</a>, <a href="#topic+Matern32.Kernel">Matern32.Kernel</a>, <a href="#topic+Matern52.Kernel">Matern52.Kernel</a>
<a href="#topic+Matern.Kernel">Matern.Kernel</a>, <a href="#topic+UDF.Kernel">UDF.Kernel</a>,
<a href="#topic+MultiplicativeRQ.Kernel">MultiplicativeRQ.Kernel</a>, <a href="#topic+MultiplicativeMatern.Kernel">MultiplicativeMatern.Kernel</a>, <a href="#topic+MultiplicativeUDF.Kernel">MultiplicativeUDF.Kernel</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 5
p &lt;- 3
X &lt;- matrix(rnorm(n*p), ncol=p)
lengthscale &lt;- c(1:p)

# Gaussian 
kernel &lt;- Get.Kernel(lengthscale, type="Gaussian")
Evaluate.Kernel(kernel, X)

# Rational Quadratic (RQ)
kernel &lt;- Get.Kernel(lengthscale, type="RQ", parameters=list(alpha=1))
Evaluate.Kernel(kernel, X) 

# Matern(1/2)
kernel &lt;- Get.Kernel(lengthscale, type="Matern12")
Evaluate.Kernel(kernel, X) 

# Matern(3/2)
kernel &lt;- Get.Kernel(lengthscale, type="Matern32")
Evaluate.Kernel(kernel, X) 

# Matern(5/2)
kernel &lt;- Get.Kernel(lengthscale, type="Matern52")
Evaluate.Kernel(kernel, X) 

# Generalized Matern
kernel &lt;- Get.Kernel(lengthscale, type="Matern", parameters=list(nu=2.01))
Evaluate.Kernel(kernel, X) 

# User Defined Function (UDF) Kernel
kernel.function &lt;- function(sqdist) {return (exp(-sqrt(sqdist)))} 
kernel &lt;- Get.Kernel(lengthscale, type="UDF", 
                     parameters=list(kernel.function=kernel.function))
Evaluate.Kernel(kernel, X) 

# Multiplicative Rational Quadratic (RQ)
kernel &lt;- Get.Kernel(lengthscale, type="MultiplicativeRQ", parameters=list(alpha=1))
Evaluate.Kernel(kernel, X) 

# Multiplicative Generalized Matern
kernel &lt;- Get.Kernel(lengthscale, type="MultiplicativeMatern", parameters=list(nu=2.01))
Evaluate.Kernel(kernel, X)

# Multiplicative User Defined Function (UDF)
kernel.function &lt;- function(sqdist) {return (exp(-sqrt(sqdist)))} 
kernel &lt;- Get.Kernel(lengthscale, type="MultiplicativeUDF", 
                     parameters=list(kernel.function=kernel.function))
Evaluate.Kernel(kernel, X) 

</code></pre>

<hr>
<h2 id='Get.Kriging.Parameters'>Get Kriging Parameters</h2><span id='topic+Get.Kriging.Parameters'></span>

<h3>Description</h3>

<p>This function can be used for extracting the estimates of the kriging parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Get.Kriging.Parameters(kriging)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Get.Kriging.Parameters_+3A_kriging">kriging</code></td>
<td>
<p>a kriging class object</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>nllh</code></td>
<td>
<p>negative log-likelihood of the kriging model</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>mean of the kriging model</p>
</td></tr>
<tr><td><code>nu2</code></td>
<td>
<p>variance of the kriging model</p>
</td></tr>
<tr><td><code>sigma2</code></td>
<td>
<p>variance of the random noise when <code>interpolation=FALSE</code></p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>coefficients of the basis functions for universal kriging</p>
</td></tr>
<tr><td><code>c</code></td>
<td>
<p>c for the rational / generalized rational kriging, see Joseph (2024)</p>
</td></tr>
<tr><td><code>c0</code></td>
<td>
<p>c0 for the generalized rational kriging, see Joseph (2024)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Chaofan Huang and V. Roshan Joseph
</p>


<h3>References</h3>

<p>Joseph, V. R. (2006). <em>Limit kriging</em>. Technometrics, 48(4), 458-466.
</p>
<p>Joseph, V. R. (2024). Rational Kriging. <em>Journal of the American Statistical Association</em>.
</p>
<p>Rasmussen, C. E. &amp; Williams, C. K. (2006). <em>Gaussian Processes for Machine Learning</em>. The MIT Press.
</p>
<p>Santner, T. J., Williams, B. J., Notz, W. I., &amp; Williams, B. J. (2003). <em>The design and analysis of computer experiments (Vol. 1)</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><a href="#topic+Fit.Kriging">Fit.Kriging</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># one dimensional example 
f &lt;- function(x) {
  x &lt;- 0.5 + 2*x
  y &lt;- sin(10*pi*x)/(2*x) + (x-1)^4
  return (y)
}

set.seed(1234)
# train set
n &lt;- 30
p &lt;- 1
X &lt;- matrix(runif(n),ncol=p)
y &lt;- apply(X, 1, f)
newX &lt;- matrix(seq(0,1,length=1001), ncol=p)

kriging &lt;- Fit.Kriging(X, y, interpolation=TRUE, fit=TRUE, model="OK",
                       kernel.parameters=list(type="Gaussian"))
Get.Kriging.Parameters(kriging)

</code></pre>

<hr>
<h2 id='Limit.Kriging'>Limit Kriging</h2><span id='topic+Limit.Kriging'></span>

<h3>Description</h3>

<p>This functions fits the limit kriging model to the data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Limit.Kriging(
  X,
  y,
  fit = TRUE,
  kernel = NULL,
  kernel.parameters = list(),
  nlopt.parameters = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Limit.Kriging_+3A_x">X</code></td>
<td>
<p>a matrix for input (feature)</p>
</td></tr>
<tr><td><code id="Limit.Kriging_+3A_y">y</code></td>
<td>
<p>a vector for output (target), only one-dimensional output is supported</p>
</td></tr>
<tr><td><code id="Limit.Kriging_+3A_fit">fit</code></td>
<td>
<p>whether to fit the length scale parameters from data</p>
</td></tr>
<tr><td><code id="Limit.Kriging_+3A_kernel">kernel</code></td>
<td>
<p>a kernel class object</p>
</td></tr>
<tr><td><code id="Limit.Kriging_+3A_kernel.parameters">kernel.parameters</code></td>
<td>
<p>a list of parameters required for the kernel, if no kernel class object is provided</p>
</td></tr>
<tr><td><code id="Limit.Kriging_+3A_nlopt.parameters">nlopt.parameters</code></td>
<td>
<p>a list of parameters required for NLopt, including choice of optimization algorithm and maximum number of evaluation</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Limit kriging avoids the mean reversion issue of ordinary kriging. Please see Joseph (2006) for details.
Only interpolation is available. Noisy output is not supported for limit kriging.  
</p>
<p>The kernel choices are required and can be specified by 
(i) providing the kernel class object to <code>kernel</code>
or (ii) specifying the kernel type and other parameters in <code>kernel.parameters</code>. 
Please see examples section of <a href="#topic+Fit.Kriging">Fit.Kriging</a> for detail usages. 
</p>
<p>When the lengthscale / correlation parameters are unknown, 
all parameters including the constant mean can be estimated via Maximum Likelihood method by setting <code>fit=TRUE</code>. 
The initial / lower bound / upper bound of the lengthscale parameters can be provided in <code>kernel.parameters</code>, 
otherwise a good initial and range would be estimated from the data. 
The optimization is performed via <a href="https://nlopt.readthedocs.io/en/latest/">NLopt</a>, 
a open-source library for nonlinear optimization. 
All gradient-free optimization methods in <a href="https://nlopt.readthedocs.io/en/latest/">NLopt</a> 
are supported and can be specified in <code>nlopt.parameters</code>.
See <code>nloptr::nloptr.print.options()</code> for the list of available derivative-free algorithms (prefix with NLOPT_GN or NLOPT_LN).  
The maximum number of optimization steps can also be defined in <code>nlopt.parameters</code>.
Please see examples section of <a href="#topic+Fit.Kriging">Fit.Kriging</a> for detail usages.
</p>


<h3>Value</h3>

<p>A Limit Kriging Class Object.
</p>


<h3>Author(s)</h3>

<p>Chaofan Huang and V. Roshan Joseph
</p>


<h3>References</h3>

<p>Joseph, V. R. (2006). <em>Limit kriging</em>. Technometrics, 48(4), 458-466.
</p>


<h3>See Also</h3>

<p><a href="#topic+Fit.Kriging">Fit.Kriging</a>, <a href="#topic+Predict.Kriging">Predict.Kriging</a>, <a href="#topic+Get.Kriging.Parameters">Get.Kriging.Parameters</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># one dimensional example 
f &lt;- function(x) {
  x &lt;- 0.5 + 2*x
  y &lt;- sin(10*pi*x)/(2*x) + (x-1)^4
  return (y)
}

set.seed(1234)
# train set
n &lt;- 30
p &lt;- 1
X &lt;- matrix(runif(n),ncol=p)
y &lt;- apply(X, 1, f)
newX &lt;- matrix(seq(0,1,length=1001), ncol=p)

# approach 1
kriging &lt;- Limit.Kriging(X, y, fit=TRUE, kernel.parameters=list(type="Gaussian"))
pred &lt;- Predict.Kriging(kriging, newX)
plot(newX, f(newX), "l")
points(X, y, pch=16, col="blue")
lines(newX, pred$mean, col="red", lty=2)
lines(newX, pred$mean-2*pred$sd, col="red", lty=3)
lines(newX, pred$mean+2*pred$sd, col="red", lty=3)
Get.Kriging.Parameters(kriging)

# approach 2
kriging &lt;- Fit.Kriging(X, y, interpolation=TRUE, fit=TRUE, model="LK",
                       kernel.parameters=list(type="Gaussian"))
pred &lt;- Predict.Kriging(kriging, newX)
plot(newX, f(newX), "l")
points(X, y, pch=16, col="blue")
lines(newX, pred$mean, col="red", lty=2)
lines(newX, pred$mean-2*pred$sd, col="red", lty=3)
lines(newX, pred$mean+2*pred$sd, col="red", lty=3)
Get.Kriging.Parameters(kriging)

</code></pre>

<hr>
<h2 id='Matern.Kernel'>Generalized Matern Kernel</h2><span id='topic+Matern.Kernel'></span>

<h3>Description</h3>

<p>This function specifies the (Generalized) Matern kernel with any smoothness parameter <code class="reqn">\nu</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Matern.Kernel(lengthscale, nu = 2.01)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Matern.Kernel_+3A_lengthscale">lengthscale</code></td>
<td>
<p>a vector for the positive length scale parameters</p>
</td></tr>
<tr><td><code id="Matern.Kernel_+3A_nu">nu</code></td>
<td>
<p>a positive scalar parameter that controls the smoothness</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Generalized Matern kernel is given by 
</p>
<p style="text-align: center;"><code class="reqn">k(r;\nu)=\frac{2^{1-\nu}}{\Gamma(\nu)}(\sqrt{2\nu}r)^{\nu}K_{\nu}(\sqrt{2\nu}r),</code>
</p>

<p>where <code class="reqn">\nu</code> is the smoothness parameter, 
<code class="reqn">K_{\nu}</code> is the modified Bessel function, 
<code class="reqn">\Gamma</code> is the gamma function, 
and </p>
<p style="text-align: center;"><code class="reqn">r(x,x^{\prime})=\sqrt{\sum_{i=1}^{p}\left(\frac{x_{i}-x_{i}^{\prime}}{l_{i}}\right)^2}</code>
</p>
 
<p>is the euclidean distance between <code class="reqn">x</code> and <code class="reqn">x^{\prime}</code> weighted by
the length scale parameters <code class="reqn">l_{i}</code>'s. 
As <code class="reqn">\nu\to\infty</code>, it converges to the <a href="#topic+Gaussian.Kernel">Gaussian.Kernel</a>.
</p>


<h3>Value</h3>

<p>A Generalized Matern Kernel Class Object.
</p>


<h3>Author(s)</h3>

<p>Chaofan Huang and V. Roshan Joseph
</p>


<h3>References</h3>

<p>Duvenaud, D. (2014). <em>The kernel cookbook: Advice on covariance functions</em>.
</p>
<p>Rasmussen, C. E. &amp; Williams, C. K. (2006). <em>Gaussian Processes for Machine Learning</em>. The MIT Press.
</p>


<h3>See Also</h3>

<p><a href="#topic+Matern12.Kernel">Matern12.Kernel</a>, <a href="#topic+Matern32.Kernel">Matern32.Kernel</a>, <a href="#topic+Matern52.Kernel">Matern52.Kernel</a>, 
<a href="#topic+MultiplicativeMatern.Kernel">MultiplicativeMatern.Kernel</a>, <a href="#topic+Get.Kernel">Get.Kernel</a>, <a href="#topic+Evaluate.Kernel">Evaluate.Kernel</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 5
p &lt;- 3
X &lt;- matrix(rnorm(n*p), ncol=p)
lengthscale &lt;- c(1:p)

# approach 1
kernel &lt;- Matern.Kernel(lengthscale, nu=2.01)
Evaluate.Kernel(kernel, X)

# approach 2
kernel &lt;- Get.Kernel(lengthscale, type="Matern", parameters=list(nu=2.01))
Evaluate.Kernel(kernel, X) 

</code></pre>

<hr>
<h2 id='Matern12.Kernel'>Matern(1/2) Kernel</h2><span id='topic+Matern12.Kernel'></span>

<h3>Description</h3>

<p>This function specifies the Matern kernel with smoothness parameter <code class="reqn">\nu</code>=1/2. 
It is also known as the Exponential kernel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Matern12.Kernel(lengthscale)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Matern12.Kernel_+3A_lengthscale">lengthscale</code></td>
<td>
<p>a vector for the positive length scale parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Matern(1/2) kernel is given by 
</p>
<p style="text-align: center;"><code class="reqn">k(r)=\exp(-r),</code>
</p>

<p>where 
</p>
<p style="text-align: center;"><code class="reqn">r(x,x^{\prime})=\sqrt{\sum_{i=1}^{p}\left(\frac{x_{i}-x_{i}^{\prime}}{l_{i}}\right)^2}</code>
</p>
 
<p>is the euclidean distance between <code class="reqn">x</code> and <code class="reqn">x^{\prime}</code> weighted by
the length scale parameters <code class="reqn">l_{i}</code>'s.
</p>


<h3>Value</h3>

<p>A Matern(1/2) Kernel Class Object.
</p>


<h3>Author(s)</h3>

<p>Chaofan Huang and V. Roshan Joseph
</p>


<h3>References</h3>

<p>Duvenaud, D. (2014). <em>The kernel cookbook: Advice on covariance functions</em>.
</p>
<p>Rasmussen, C. E. &amp; Williams, C. K. (2006). <em>Gaussian Processes for Machine Learning</em>. The MIT Press.
</p>


<h3>See Also</h3>

<p><a href="#topic+Matern.Kernel">Matern.Kernel</a>, <a href="#topic+Get.Kernel">Get.Kernel</a>, <a href="#topic+Evaluate.Kernel">Evaluate.Kernel</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 5
p &lt;- 3
X &lt;- matrix(rnorm(n*p), ncol=p)
lengthscale &lt;- c(1:p)

# approach 1
kernel &lt;- Matern12.Kernel(lengthscale)
Evaluate.Kernel(kernel, X)

# approach 2
kernel &lt;- Get.Kernel(lengthscale, type="Matern12")
Evaluate.Kernel(kernel, X) 

</code></pre>

<hr>
<h2 id='Matern32.Kernel'>Matern(3/2) Kernel</h2><span id='topic+Matern32.Kernel'></span>

<h3>Description</h3>

<p>This function specifies the Matern kernel with smoothness parameter <code class="reqn">\nu</code>=3/2.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Matern32.Kernel(lengthscale)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Matern32.Kernel_+3A_lengthscale">lengthscale</code></td>
<td>
<p>a vector for the positive length scale parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Matern(3/2) kernel is given by 
</p>
<p style="text-align: center;"><code class="reqn">k(r)=(1+\sqrt{3}r)\exp(-\sqrt{3}r),</code>
</p>

<p>where 
</p>
<p style="text-align: center;"><code class="reqn">r(x,x^{\prime})=\sqrt{\sum_{i=1}^{p}\left(\frac{x_{i}-x_{i}^{\prime}}{l_{i}}\right)^2}</code>
</p>
 
<p>is the euclidean distance between <code class="reqn">x</code> and <code class="reqn">x^{\prime}</code> weighted by
the length scale parameters <code class="reqn">l_{i}</code>'s.
</p>


<h3>Value</h3>

<p>A Matern(3/2) Kernel Class Object.
</p>


<h3>Author(s)</h3>

<p>Chaofan Huang and V. Roshan Joseph
</p>


<h3>References</h3>

<p>Duvenaud, D. (2014). <em>The kernel cookbook: Advice on covariance functions</em>.
</p>
<p>Rasmussen, C. E. &amp; Williams, C. K. (2006). <em>Gaussian Processes for Machine Learning</em>. The MIT Press.
</p>


<h3>See Also</h3>

<p><a href="#topic+Matern.Kernel">Matern.Kernel</a>, <a href="#topic+Get.Kernel">Get.Kernel</a>, <a href="#topic+Evaluate.Kernel">Evaluate.Kernel</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 5
p &lt;- 3
X &lt;- matrix(rnorm(n*p), ncol=p)
lengthscale &lt;- c(1:p)

# approach 1
kernel &lt;- Matern32.Kernel(lengthscale)
Evaluate.Kernel(kernel, X)

# approach 2
kernel &lt;- Get.Kernel(lengthscale, type="Matern32")
Evaluate.Kernel(kernel, X) 

</code></pre>

<hr>
<h2 id='Matern52.Kernel'>Matern(5/2) Kernel</h2><span id='topic+Matern52.Kernel'></span>

<h3>Description</h3>

<p>This function specifies the Matern kernel with smoothness parameter <code class="reqn">\nu</code>=5/2.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Matern52.Kernel(lengthscale)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Matern52.Kernel_+3A_lengthscale">lengthscale</code></td>
<td>
<p>a vector for the positive length scale parameters</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Matern(5/2) kernel is given by 
</p>
<p style="text-align: center;"><code class="reqn">k(r)=(1+\sqrt{5}r+5r^2/3)\exp(-\sqrt{5}r),</code>
</p>

<p>where 
</p>
<p style="text-align: center;"><code class="reqn">r(x,x^{\prime})=\sqrt{\sum_{i=1}^{p}\left(\frac{x_{i}-x_{i}^{\prime}}{l_{i}}\right)^2}</code>
</p>
 
<p>is the euclidean distance between <code class="reqn">x</code> and <code class="reqn">x^{\prime}</code> weighted by
the length scale parameters <code class="reqn">l_{i}</code>'s.
</p>


<h3>Value</h3>

<p>A Matern(5/2) Kernel Class Object.
</p>


<h3>Author(s)</h3>

<p>Chaofan Huang and V. Roshan Joseph
</p>


<h3>References</h3>

<p>Duvenaud, D. (2014). <em>The kernel cookbook: Advice on covariance functions</em>.
</p>
<p>Rasmussen, C. E. &amp; Williams, C. K. (2006). <em>Gaussian Processes for Machine Learning</em>. The MIT Press.
</p>


<h3>See Also</h3>

<p><a href="#topic+Matern.Kernel">Matern.Kernel</a>, <a href="#topic+Get.Kernel">Get.Kernel</a>, <a href="#topic+Evaluate.Kernel">Evaluate.Kernel</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 5
p &lt;- 3
X &lt;- matrix(rnorm(n*p), ncol=p)
lengthscale &lt;- c(1:p)

# approach 1
kernel &lt;- Matern52.Kernel(lengthscale)
Evaluate.Kernel(kernel, X)

# approach 2
kernel &lt;- Get.Kernel(lengthscale, type="Matern52")
Evaluate.Kernel(kernel, X) 

</code></pre>

<hr>
<h2 id='MultiplicativeMatern.Kernel'>Multiplicative Generalized Matern Kernel</h2><span id='topic+MultiplicativeMatern.Kernel'></span>

<h3>Description</h3>

<p>This function specifies the Multiplicative Generalized Matern kernel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MultiplicativeMatern.Kernel(lengthscale, nu = 2.01)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MultiplicativeMatern.Kernel_+3A_lengthscale">lengthscale</code></td>
<td>
<p>a vector for the positive length scale parameters</p>
</td></tr>
<tr><td><code id="MultiplicativeMatern.Kernel_+3A_nu">nu</code></td>
<td>
<p>a positive scalar parameter that controls the smoothness</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Multiplicative Generalized Matern kernel is given by 
</p>
<p style="text-align: center;"><code class="reqn">k(r;\nu)=\prod_{i=1}^{p}\frac{2^{1-\nu}}{\Gamma(\nu)}(\sqrt{2\nu}r_{i})^{\nu}K_{\nu}(\sqrt{2\nu}r_{i}),</code>
</p>

<p>where <code class="reqn">\nu</code> is the smoothness parameter, 
<code class="reqn">K_{\nu}</code> is the modified Bessel function, 
<code class="reqn">\Gamma</code> is the gamma function, 
and </p>
<p style="text-align: center;"><code class="reqn">r_{i}(x,x^{\prime})=\sqrt{\left(\frac{x_{i}-x_{i}^{\prime}}{l_{i}}\right)^2}</code>
</p>
 
<p>is the dimension-wise euclidean distances between <code class="reqn">x</code> and <code class="reqn">x^{\prime}</code> weighted by
the length scale parameters <code class="reqn">l_{i}</code>'s.
</p>


<h3>Value</h3>

<p>A Multiplicative Generalized Matern Kernel Class Object.
</p>


<h3>Author(s)</h3>

<p>Chaofan Huang and V. Roshan Joseph
</p>


<h3>References</h3>

<p>Duvenaud, D. (2014). <em>The kernel cookbook: Advice on covariance functions</em>.
</p>
<p>Rasmussen, C. E. &amp; Williams, C. K. (2006). <em>Gaussian Processes for Machine Learning</em>. The MIT Press.
</p>


<h3>See Also</h3>

<p><a href="#topic+Matern.Kernel">Matern.Kernel</a>, <a href="#topic+Get.Kernel">Get.Kernel</a>, <a href="#topic+Evaluate.Kernel">Evaluate.Kernel</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 5
p &lt;- 3
X &lt;- matrix(rnorm(n*p), ncol=p)
lengthscale &lt;- c(1:p)

# approach 1
kernel &lt;- MultiplicativeMatern.Kernel(lengthscale, nu=2.01)
Evaluate.Kernel(kernel, X)

# approach 2
kernel &lt;- Get.Kernel(lengthscale, type="MultiplicativeMatern", parameters=list(nu=2.01))
Evaluate.Kernel(kernel, X) 

</code></pre>

<hr>
<h2 id='MultiplicativeRQ.Kernel'>Multiplicative Rational Quadratic (RQ) Kernel</h2><span id='topic+MultiplicativeRQ.Kernel'></span>

<h3>Description</h3>

<p>This function specifies the Multiplicative Rational Quadratic (RQ) kernel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MultiplicativeRQ.Kernel(lengthscale, alpha = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MultiplicativeRQ.Kernel_+3A_lengthscale">lengthscale</code></td>
<td>
<p>a vector for the positive length scale parameters</p>
</td></tr>
<tr><td><code id="MultiplicativeRQ.Kernel_+3A_alpha">alpha</code></td>
<td>
<p>a positive scalar for the scale mixture parameter that controls the relative weighting of large-scale and small-scale variations</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Multiplicative Rational Quadratic (RQ) kernel is given by 
</p>
<p style="text-align: center;"><code class="reqn">k(r;\alpha)=\prod_{i=1}^{p}\left(1+\frac{r_{i}^2}{2\alpha}\right)^{-\alpha},</code>
</p>

<p>where <code class="reqn">\alpha</code> is the scale mixture parameter and 
</p>
<p style="text-align: center;"><code class="reqn">r_{i}(x,x^{\prime})=\sqrt{\left(\frac{x_{i}-x_{i}^{\prime}}{l_{i}}\right)^2}</code>
</p>
 
<p>is the dimension-wise euclidean distances between <code class="reqn">x</code> and <code class="reqn">x^{\prime}</code> weighted by
the length scale parameters <code class="reqn">l_{i}</code>'s.
</p>


<h3>Value</h3>

<p>A Multiplicative Rational Quadratic (RQ) Kernel Class Object.
</p>


<h3>Author(s)</h3>

<p>Chaofan Huang and V. Roshan Joseph
</p>


<h3>References</h3>

<p>Duvenaud, D. (2014). <em>The kernel cookbook: Advice on covariance functions</em>.
</p>
<p>Rasmussen, C. E. &amp; Williams, C. K. (2006). <em>Gaussian Processes for Machine Learning</em>. The MIT Press.
</p>


<h3>See Also</h3>

<p><a href="#topic+RQ.Kernel">RQ.Kernel</a>, <a href="#topic+Get.Kernel">Get.Kernel</a>, <a href="#topic+Evaluate.Kernel">Evaluate.Kernel</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 5
p &lt;- 3
X &lt;- matrix(rnorm(n*p), ncol=p)
lengthscale &lt;- c(1:p)

# approach 1
kernel &lt;- MultiplicativeRQ.Kernel(lengthscale, alpha=1)
Evaluate.Kernel(kernel, X)

# approach 2
kernel &lt;- Get.Kernel(lengthscale, type="MultiplicativeRQ", parameters=list(alpha=1))
Evaluate.Kernel(kernel, X) 

</code></pre>

<hr>
<h2 id='MultiplicativeUDF.Kernel'>Multiplicative User Defined Function (UDF) Kernel</h2><span id='topic+MultiplicativeUDF.Kernel'></span>

<h3>Description</h3>

<p>This function specifies the Multiplicative kernel with the user defined R function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MultiplicativeUDF.Kernel(lengthscale, kernel.function)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MultiplicativeUDF.Kernel_+3A_lengthscale">lengthscale</code></td>
<td>
<p>a vector for the positive length scale parameters</p>
</td></tr>
<tr><td><code id="MultiplicativeUDF.Kernel_+3A_kernel.function">kernel.function</code></td>
<td>
<p>user defined kernel function</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Multiplicative User Defined Function (UDF) kernel is given by 
</p>
<p style="text-align: center;"><code class="reqn">k(r)=\prod_{i=1}^{p}f(r_{i}),</code>
</p>

<p>where <code class="reqn">f</code> is the user defined kernel function that takes <code class="reqn">r_{i}^2</code> as input, 
where </p>
<p style="text-align: center;"><code class="reqn">r_{i}(x,x^{\prime})=\sqrt{\left(\frac{x_{i}-x_{i}^{\prime}}{l_{i}}\right)^2}</code>
</p>
 
<p>is the dimension-wise euclidean distances between <code class="reqn">x</code> and <code class="reqn">x^{\prime}</code> weighted by
the length scale parameters <code class="reqn">l_{i}</code>'s.
</p>


<h3>Value</h3>

<p>A Multiplicative User Defined Function (UDF) Kernel Class Object.
</p>


<h3>Author(s)</h3>

<p>Chaofan Huang and V. Roshan Joseph
</p>


<h3>References</h3>

<p>Duvenaud, D. (2014). <em>The kernel cookbook: Advice on covariance functions</em>.
</p>
<p>Rasmussen, C. E. &amp; Williams, C. K. (2006). <em>Gaussian Processes for Machine Learning</em>. The MIT Press.
</p>


<h3>See Also</h3>

<p><a href="#topic+UDF.Kernel">UDF.Kernel</a>, <a href="#topic+Get.Kernel">Get.Kernel</a>, <a href="#topic+Evaluate.Kernel">Evaluate.Kernel</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 5
p &lt;- 3
X &lt;- matrix(rnorm(n*p), ncol=p)
lengthscale &lt;- c(1:p)

kernel.function &lt;- function(sqdist) {return (exp(-sqrt(sqdist)))} 

# approach 1
kernel &lt;- MultiplicativeUDF.Kernel(lengthscale, kernel.function=kernel.function)
Evaluate.Kernel(kernel, X)

# approach 2
kernel &lt;- Get.Kernel(lengthscale, type="MultiplicativeUDF", 
                     parameters=list(kernel.function=kernel.function))
Evaluate.Kernel(kernel, X) 

</code></pre>

<hr>
<h2 id='Ordinary.Kriging'>Ordinary Kriging</h2><span id='topic+Ordinary.Kriging'></span>

<h3>Description</h3>

<p>This functions fits the ordinary kriging model to the data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Ordinary.Kriging(
  X,
  y,
  interpolation = TRUE,
  fit = TRUE,
  kernel = NULL,
  kernel.parameters = list(),
  nlopt.parameters = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Ordinary.Kriging_+3A_x">X</code></td>
<td>
<p>a matrix for input (feature)</p>
</td></tr>
<tr><td><code id="Ordinary.Kriging_+3A_y">y</code></td>
<td>
<p>a vector for output (target), only one-dimensional output is supported</p>
</td></tr>
<tr><td><code id="Ordinary.Kriging_+3A_interpolation">interpolation</code></td>
<td>
<p>interpolation whether to interpolate, for noisy data please set <code>interpolate=FALSE</code></p>
</td></tr>
<tr><td><code id="Ordinary.Kriging_+3A_fit">fit</code></td>
<td>
<p>whether to fit the length scale parameters from data</p>
</td></tr>
<tr><td><code id="Ordinary.Kriging_+3A_kernel">kernel</code></td>
<td>
<p>a kernel class object</p>
</td></tr>
<tr><td><code id="Ordinary.Kriging_+3A_kernel.parameters">kernel.parameters</code></td>
<td>
<p>a list of parameters required for the kernel, if no kernel class object is provided</p>
</td></tr>
<tr><td><code id="Ordinary.Kriging_+3A_nlopt.parameters">nlopt.parameters</code></td>
<td>
<p>a list of parameters required for NLopt, including choice of optimization algorithm and maximum number of evaluation</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Ordinary kriging assumes a constant mean. Please see Santner et al. (2003) for details.
</p>
<p>For data from deterministic computer experiments, use <code>interpolation=TRUE</code> and will give an interpolator. 
For noisy data, use <code>interpolation=FALSE</code>, which will give an approximator of the underlying function.
</p>
<p>The kernel choices are required and can be specified by 
(i) providing the kernel class object to <code>kernel</code>
or (ii) specifying the kernel type and other parameters in <code>kernel.parameters</code>. 
Please see examples section of <a href="#topic+Fit.Kriging">Fit.Kriging</a> for detail usages. 
</p>
<p>When the lengthscale / correlation parameters are unknown, 
all parameters including the constant mean can be estimated via Maximum Likelihood method by setting <code>fit=TRUE</code>. 
The initial / lower bound / upper bound of the lengthscale parameters can be provided in <code>kernel.parameters</code>, 
otherwise a good initial and range would be estimated from the data. 
The optimization is performed via <a href="https://nlopt.readthedocs.io/en/latest/">NLopt</a>, 
a open-source library for nonlinear optimization. 
All gradient-free optimization methods in <a href="https://nlopt.readthedocs.io/en/latest/">NLopt</a> 
are supported and can be specified in <code>nlopt.parameters</code>.
See <code>nloptr::nloptr.print.options()</code> for the list of available derivative-free algorithms (prefix with NLOPT_GN or NLOPT_LN).  
The maximum number of optimization steps can also be defined in <code>nlopt.parameters</code>.
Please see examples section of <a href="#topic+Fit.Kriging">Fit.Kriging</a> for detail usages.
</p>


<h3>Value</h3>

<p>A Ordinary Kriging Class Object.
</p>


<h3>Author(s)</h3>

<p>Chaofan Huang and V. Roshan Joseph
</p>


<h3>References</h3>

<p>Santner, T. J., Williams, B. J., Notz, W. I., &amp; Williams, B. J. (2003). <em>The design and analysis of computer experiments (Vol. 1)</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><a href="#topic+Fit.Kriging">Fit.Kriging</a>, <a href="#topic+Predict.Kriging">Predict.Kriging</a>, <a href="#topic+Get.Kriging.Parameters">Get.Kriging.Parameters</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># one dimensional example 
f &lt;- function(x) {
  x &lt;- 0.5 + 2*x
  y &lt;- sin(10*pi*x)/(2*x) + (x-1)^4
  return (y)
}

set.seed(1234)
# train set
n &lt;- 30
p &lt;- 1
X &lt;- matrix(runif(n),ncol=p)
y &lt;- apply(X, 1, f)
newX &lt;- matrix(seq(0,1,length=1001), ncol=p)

# approach 1
kriging &lt;- Ordinary.Kriging(X, y, interpolation=TRUE, fit=TRUE, 
                            kernel.parameters=list(type="Gaussian"))
pred &lt;- Predict.Kriging(kriging, newX)
plot(newX, f(newX), "l")
points(X, y, pch=16, col="blue")
lines(newX, pred$mean, col="red", lty=2)
lines(newX, pred$mean-2*pred$sd, col="red", lty=3)
lines(newX, pred$mean+2*pred$sd, col="red", lty=3)
Get.Kriging.Parameters(kriging)

# approach 2
kriging &lt;- Fit.Kriging(X, y, interpolation=TRUE, fit=TRUE, model="OK",
                       kernel.parameters=list(type="Gaussian"))
pred &lt;- Predict.Kriging(kriging, newX)
plot(newX, f(newX), "l")
points(X, y, pch=16, col="blue")
lines(newX, pred$mean, col="red", lty=2)
lines(newX, pred$mean-2*pred$sd, col="red", lty=3)
lines(newX, pred$mean+2*pred$sd, col="red", lty=3)
Get.Kriging.Parameters(kriging)

</code></pre>

<hr>
<h2 id='Predict.Kriging'>Kriging Prediction</h2><span id='topic+Predict.Kriging'></span>

<h3>Description</h3>

<p>This function gives prediction and uncertainty quantification of the kriging model on a new input.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Predict.Kriging(kriging, X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Predict.Kriging_+3A_kriging">kriging</code></td>
<td>
<p>a kriging class object</p>
</td></tr>
<tr><td><code id="Predict.Kriging_+3A_x">X</code></td>
<td>
<p>a matrix for the new input (features) to perform predictions</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>mean</code></td>
<td>
<p>kriging mean computed at the new input</p>
</td></tr>
<tr><td><code>sd</code></td>
<td>
<p>kriging standard computed at the new input</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Chaofan Huang and V. Roshan Joseph
</p>


<h3>References</h3>

<p>Joseph, V. R. (2006). <em>Limit kriging</em>. Technometrics, 48(4), 458-466.
</p>
<p>Joseph, V. R. (2024). Rational Kriging. <em>Journal of the American Statistical Association</em>.
</p>
<p>Rasmussen, C. E. &amp; Williams, C. K. (2006). <em>Gaussian Processes for Machine Learning</em>. The MIT Press.
</p>
<p>Santner, T. J., Williams, B. J., Notz, W. I., &amp; Williams, B. J. (2003). <em>The design and analysis of computer experiments (Vol. 1)</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><a href="#topic+Fit.Kriging">Fit.Kriging</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># one dimensional example 
f &lt;- function(x) {
  x &lt;- 0.5 + 2*x
  y &lt;- sin(10*pi*x)/(2*x) + (x-1)^4
  return (y)
}

set.seed(1234)
# train set
n &lt;- 30
p &lt;- 1
X &lt;- matrix(runif(n),ncol=p)
y &lt;- apply(X, 1, f)
newX &lt;- matrix(seq(0,1,length=1001), ncol=p)

kriging &lt;- Fit.Kriging(X, y, interpolation=TRUE, fit=TRUE, model="OK",
                       kernel.parameters=list(type="Gaussian"))
pred &lt;- Predict.Kriging(kriging, newX)
plot(newX, f(newX), "l")
points(X, y, pch=16, col="blue")
lines(newX, pred$mean, col="red", lty=2)
lines(newX, pred$mean-2*pred$sd, col="red", lty=3)
lines(newX, pred$mean+2*pred$sd, col="red", lty=3)

</code></pre>

<hr>
<h2 id='Rational.Kriging'>Rational Kriging</h2><span id='topic+Rational.Kriging'></span>

<h3>Description</h3>

<p>This functions fits the rational kriging model to the data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Rational.Kriging(
  X,
  y,
  fit = TRUE,
  kernel = NULL,
  kernel.parameters = list(),
  nlopt.parameters = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Rational.Kriging_+3A_x">X</code></td>
<td>
<p>a matrix for input (feature)</p>
</td></tr>
<tr><td><code id="Rational.Kriging_+3A_y">y</code></td>
<td>
<p>a vector for output (target), only one-dimensional output is supported</p>
</td></tr>
<tr><td><code id="Rational.Kriging_+3A_fit">fit</code></td>
<td>
<p>whether to fit the length scale parameters from data</p>
</td></tr>
<tr><td><code id="Rational.Kriging_+3A_kernel">kernel</code></td>
<td>
<p>a kernel class object</p>
</td></tr>
<tr><td><code id="Rational.Kriging_+3A_kernel.parameters">kernel.parameters</code></td>
<td>
<p>a list of parameters required for the kernel, if no kernel class object is provided</p>
</td></tr>
<tr><td><code id="Rational.Kriging_+3A_nlopt.parameters">nlopt.parameters</code></td>
<td>
<p>a list of parameters required for NLopt, including choice of optimization algorithm and maximum number of evaluation</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Rational kriging gives a rational predictor in terms of the inputs, which gives a more stable estimate of the mean. 
Please see Joseph (2024) for details.
Only interpolation is available. Noisy output is not supported for rational kriging.  
</p>
<p>The kernel choices are required and can be specified by 
(i) providing the kernel class object to <code>kernel</code>
or (ii) specifying the kernel type and other parameters in <code>kernel.parameters</code>. 
Please see examples section of <a href="#topic+Fit.Kriging">Fit.Kriging</a> for detail usages. 
</p>
<p>When the lengthscale / correlation parameters are unknown, 
all parameters including the constant mean can be estimated via Maximum Likelihood method by setting <code>fit=TRUE</code>. 
The initial / lower bound / upper bound of the lengthscale parameters can be provided in <code>kernel.parameters</code>, 
otherwise a good initial and range would be estimated from the data. 
The optimization is performed via <a href="https://nlopt.readthedocs.io/en/latest/">NLopt</a>, 
a open-source library for nonlinear optimization. 
All gradient-free optimization methods in <a href="https://nlopt.readthedocs.io/en/latest/">NLopt</a> 
are supported and can be specified in <code>nlopt.parameters</code>.
See <code>nloptr::nloptr.print.options()</code> for the list of available derivative-free algorithms (prefix with NLOPT_GN or NLOPT_LN).  
The maximum number of optimization steps can also be defined in <code>nlopt.parameters</code>.
Please see examples section of <a href="#topic+Fit.Kriging">Fit.Kriging</a> for detail usages.
</p>


<h3>Value</h3>

<p>A Rational Kriging Class Object.
</p>


<h3>Author(s)</h3>

<p>Chaofan Huang and V. Roshan Joseph
</p>


<h3>References</h3>

<p>Joseph, V. R. (2024). Rational Kriging. <em>Journal of the American Statistical Association</em>.
</p>


<h3>See Also</h3>

<p><a href="#topic+Fit.Kriging">Fit.Kriging</a>, <a href="#topic+Predict.Kriging">Predict.Kriging</a>, <a href="#topic+Get.Kriging.Parameters">Get.Kriging.Parameters</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># one dimensional example 
f &lt;- function(x) {
  x &lt;- 0.5 + 2*x
  y &lt;- sin(10*pi*x)/(2*x) + (x-1)^4
  return (y)
}

set.seed(1234)
# train set
n &lt;- 30
p &lt;- 1
X &lt;- matrix(runif(n),ncol=p)
y &lt;- apply(X, 1, f)
newX &lt;- matrix(seq(0,1,length=1001), ncol=p)

# approach 1
kriging &lt;- Rational.Kriging(X, y, fit=TRUE, kernel.parameters=list(type="RQ",alpha=1))
pred &lt;- Predict.Kriging(kriging, newX)
plot(newX, f(newX), "l")
points(X, y, pch=16, col="blue")
lines(newX, pred$mean, col="red", lty=2)
lines(newX, pred$mean-2*pred$sd, col="red", lty=3)
lines(newX, pred$mean+2*pred$sd, col="red", lty=3)
Get.Kriging.Parameters(kriging)

# approach 2
kriging &lt;- Fit.Kriging(X, y, interpolation=TRUE, fit=TRUE, model="RK",
                       kernel.parameters=list(type="RQ",alpha=1))
pred &lt;- Predict.Kriging(kriging, newX)
plot(newX, f(newX), "l")
points(X, y, pch=16, col="blue")
lines(newX, pred$mean, col="red", lty=2)
lines(newX, pred$mean-2*pred$sd, col="red", lty=3)
lines(newX, pred$mean+2*pred$sd, col="red", lty=3)
Get.Kriging.Parameters(kriging)

</code></pre>

<hr>
<h2 id='RQ.Kernel'>Rational Quadratic (RQ) Kernel</h2><span id='topic+RQ.Kernel'></span>

<h3>Description</h3>

<p>This function specifies the Rational Quadratic (RQ) kernel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RQ.Kernel(lengthscale, alpha = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RQ.Kernel_+3A_lengthscale">lengthscale</code></td>
<td>
<p>a vector for the positive length scale parameters</p>
</td></tr>
<tr><td><code id="RQ.Kernel_+3A_alpha">alpha</code></td>
<td>
<p>a positive scalar for the scale mixture parameter that controls the relative weighting of large-scale and small-scale variations</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Rational Quadratic (RQ) kernel is given by 
</p>
<p style="text-align: center;"><code class="reqn">k(r;\alpha)=\left(1+\frac{r^2}{2\alpha}\right)^{-\alpha},</code>
</p>

<p>where <code class="reqn">\alpha</code> is the scale mixture parameter and 
</p>
<p style="text-align: center;"><code class="reqn">r(x,x^{\prime})=\sqrt{\sum_{i=1}^{p}\left(\frac{x_{i}-x_{i}^{\prime}}{l_{i}}\right)^2}</code>
</p>
 
<p>is the euclidean distance between <code class="reqn">x</code> and <code class="reqn">x^{\prime}</code> weighted by
the length scale parameters <code class="reqn">l_{i}</code>'s.
As <code class="reqn">\alpha\to\infty</code>, it converges to the <a href="#topic+Gaussian.Kernel">Gaussian.Kernel</a>.
</p>


<h3>Value</h3>

<p>A Rational Quadratic (RQ) Kernel Class Object.
</p>


<h3>Author(s)</h3>

<p>Chaofan Huang and V. Roshan Joseph
</p>


<h3>References</h3>

<p>Duvenaud, D. (2014). <em>The kernel cookbook: Advice on covariance functions</em>.
</p>
<p>Rasmussen, C. E. &amp; Williams, C. K. (2006). <em>Gaussian Processes for Machine Learning</em>. The MIT Press.
</p>


<h3>See Also</h3>

<p><a href="#topic+MultiplicativeRQ.Kernel">MultiplicativeRQ.Kernel</a>, <a href="#topic+Get.Kernel">Get.Kernel</a>, <a href="#topic+Evaluate.Kernel">Evaluate.Kernel</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 5
p &lt;- 3
X &lt;- matrix(rnorm(n*p), ncol=p)
lengthscale &lt;- c(1:p)

# approach 1
kernel &lt;- RQ.Kernel(lengthscale, alpha=1)
Evaluate.Kernel(kernel, X)

# approach 2
kernel &lt;- Get.Kernel(lengthscale, type="RQ", parameters=list(alpha=1))
Evaluate.Kernel(kernel, X) 

</code></pre>

<hr>
<h2 id='UDF.Kernel'>User Defined Function (UDF) Kernel</h2><span id='topic+UDF.Kernel'></span>

<h3>Description</h3>

<p>This function specifies a kernel with the user defined R function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>UDF.Kernel(lengthscale, kernel.function)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="UDF.Kernel_+3A_lengthscale">lengthscale</code></td>
<td>
<p>a vector for the positive length scale parameters</p>
</td></tr>
<tr><td><code id="UDF.Kernel_+3A_kernel.function">kernel.function</code></td>
<td>
<p>user defined kernel function</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The User Defined Function (UDF) kernel is given by
</p>
<p style="text-align: center;"><code class="reqn">k(r) = f(r)</code>
</p>

<p>where <code class="reqn">f</code> is the user defined kernel function that takes <code class="reqn">r^2</code> as input, 
where </p>
<p style="text-align: center;"><code class="reqn">r(x,x^{\prime})=\sqrt{\sum_{i=1}^{p}\left(\frac{x_{i}-x_{i}^{\prime}}{l_{i}}\right)^2},</code>
</p>
 
<p>is the euclidean distance between <code class="reqn">x</code> and <code class="reqn">x^{\prime}</code> weighted by
the length scale parameters <code class="reqn">l_{i}</code>'s.
</p>


<h3>Value</h3>

<p>A User Defined Function (UDF) Kernel Class Object.
</p>


<h3>Author(s)</h3>

<p>Chaofan Huang and V. Roshan Joseph
</p>


<h3>References</h3>

<p>Duvenaud, D. (2014). <em>The kernel cookbook: Advice on covariance functions</em>.
</p>
<p>Rasmussen, C. E. &amp; Williams, C. K. (2006). <em>Gaussian Processes for Machine Learning</em>. The MIT Press.
</p>


<h3>See Also</h3>

<p><a href="#topic+MultiplicativeUDF.Kernel">MultiplicativeUDF.Kernel</a>, <a href="#topic+Get.Kernel">Get.Kernel</a>, <a href="#topic+Evaluate.Kernel">Evaluate.Kernel</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 5
p &lt;- 3
X &lt;- matrix(rnorm(n*p), ncol=p)
lengthscale &lt;- c(1:p)

kernel.function &lt;- function(sqdist) {return (exp(-sqrt(sqdist)))} 

# approach 1
kernel &lt;- UDF.Kernel(lengthscale, kernel.function=kernel.function)
Evaluate.Kernel(kernel, X)

# approach 2
kernel &lt;- Get.Kernel(lengthscale, type="UDF", 
                     parameters=list(kernel.function=kernel.function))
Evaluate.Kernel(kernel, X) 

</code></pre>

<hr>
<h2 id='Universal.Kriging'>Universal Kriging</h2><span id='topic+Universal.Kriging'></span>

<h3>Description</h3>

<p>This functions fits the universal kriging model to the data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Universal.Kriging(
  X,
  y,
  basis.function,
  interpolation = TRUE,
  fit = TRUE,
  kernel = NULL,
  kernel.parameters = list(),
  nlopt.parameters = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Universal.Kriging_+3A_x">X</code></td>
<td>
<p>a matrix for input (feature)</p>
</td></tr>
<tr><td><code id="Universal.Kriging_+3A_y">y</code></td>
<td>
<p>a vector for output (target), only one-dimensional output is supported</p>
</td></tr>
<tr><td><code id="Universal.Kriging_+3A_basis.function">basis.function</code></td>
<td>
<p>the basis functions for specifying the prior mean</p>
</td></tr>
<tr><td><code id="Universal.Kriging_+3A_interpolation">interpolation</code></td>
<td>
<p>interpolation whether to interpolate, for noisy data please set <code>interpolate=FALSE</code></p>
</td></tr>
<tr><td><code id="Universal.Kriging_+3A_fit">fit</code></td>
<td>
<p>whether to fit the length scale parameters from data</p>
</td></tr>
<tr><td><code id="Universal.Kriging_+3A_kernel">kernel</code></td>
<td>
<p>a kernel class object</p>
</td></tr>
<tr><td><code id="Universal.Kriging_+3A_kernel.parameters">kernel.parameters</code></td>
<td>
<p>a list of parameters required for the kernel, if no kernel class object is provided</p>
</td></tr>
<tr><td><code id="Universal.Kriging_+3A_nlopt.parameters">nlopt.parameters</code></td>
<td>
<p>a list of parameters required for NLopt, including choice of optimization algorithm and maximum number of evaluation</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Universal kriging permits a more general function of mean, which can be specified using <code>basis.function</code>. 
Please see Santner et al. (2003) for details.
</p>
<p>For data from deterministic computer experiments, use <code>interpolation=TRUE</code> and will give an interpolator. 
For noisy data, use <code>interpolation=FALSE</code>, which will give an approximator of the underlying function.
</p>
<p>The kernel choices are required and can be specified by 
(i) providing the kernel class object to <code>kernel</code>
or (ii) specifying the kernel type and other parameters in <code>kernel.parameters</code>. 
Please see examples section of <a href="#topic+Fit.Kriging">Fit.Kriging</a> for detail usages. 
</p>
<p>When the lengthscale / correlation parameters are unknown, 
all parameters including the constant mean can be estimated via Maximum Likelihood method by setting <code>fit=TRUE</code>. 
The initial / lower bound / upper bound of the lengthscale parameters can be provided in <code>kernel.parameters</code>, 
otherwise a good initial and range would be estimated from the data. 
The optimization is performed via <a href="https://nlopt.readthedocs.io/en/latest/">NLopt</a>, 
a open-source library for nonlinear optimization. 
All gradient-free optimization methods in <a href="https://nlopt.readthedocs.io/en/latest/">NLopt</a> 
are supported and can be specified in <code>nlopt.parameters</code>.
See <code>nloptr::nloptr.print.options()</code> for the list of available derivative-free algorithms (prefix with NLOPT_GN or NLOPT_LN).  
The maximum number of optimization steps can also be defined in <code>nlopt.parameters</code>.
Please see examples section of <a href="#topic+Fit.Kriging">Fit.Kriging</a> for detail usages.
</p>


<h3>Value</h3>

<p>A Universal Kriging Class Object.
</p>


<h3>Author(s)</h3>

<p>Chaofan Huang and V. Roshan Joseph
</p>


<h3>References</h3>

<p>Santner, T. J., Williams, B. J., Notz, W. I., &amp; Williams, B. J. (2003). <em>The design and analysis of computer experiments (Vol. 1)</em>. New York: Springer.
</p>


<h3>See Also</h3>

<p><a href="#topic+Fit.Kriging">Fit.Kriging</a>, <a href="#topic+Predict.Kriging">Predict.Kriging</a>, <a href="#topic+Get.Kriging.Parameters">Get.Kriging.Parameters</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># one dimensional example 
f &lt;- function(x) {
  x &lt;- 0.5 + 2*x
  y &lt;- sin(10*pi*x)/(2*x) + (x-1)^4
  return (y)
}

set.seed(1234)
# train set
n &lt;- 30
p &lt;- 1
X &lt;- matrix(runif(n),ncol=p)
y &lt;- apply(X, 1, f)
newX &lt;- matrix(seq(0,1,length=1001), ncol=p)

basis.function &lt;- function(x) {c(1,x[1],x[1]^2)}

# approach 1
kriging &lt;- Universal.Kriging(X, y, basis.function=basis.function,
                             interpolation=TRUE, fit=TRUE, 
                             kernel.parameters=list(type="Gaussian"))
pred &lt;- Predict.Kriging(kriging, newX)
plot(newX, f(newX), "l")
points(X, y, pch=16, col="blue")
lines(newX, pred$mean, col="red", lty=2)
lines(newX, pred$mean-2*pred$sd, col="red", lty=3)
lines(newX, pred$mean+2*pred$sd, col="red", lty=3)
Get.Kriging.Parameters(kriging)

# approach 2
kriging &lt;- Fit.Kriging(X, y, interpolation=TRUE, fit=TRUE, model="UK",
                       model.parameters=list(basis.function=basis.function),
                       kernel.parameters=list(type="Gaussian"))
pred &lt;- Predict.Kriging(kriging, newX)
plot(newX, f(newX), "l")
points(X, y, pch=16, col="blue")
lines(newX, pred$mean, col="red", lty=2)
lines(newX, pred$mean-2*pred$sd, col="red", lty=3)
lines(newX, pred$mean+2*pred$sd, col="red", lty=3)
Get.Kriging.Parameters(kriging)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
