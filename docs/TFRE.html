<!DOCTYPE html><html><head><title>Help for package TFRE</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {TFRE}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#coef.TFRE'><p>Extract coefficients from a 'TFRE' object</p></a></li>
<li><a href='#est_lambda'><p>Estimate the tuning parameter for a TFRE Lasso regression</p></a></li>
<li><a href='#plot.TFRE'><p>Plot the second stage model curve for a 'TFRE' object</p></a></li>
<li><a href='#predict.TFRE'><p>Make predictions from a 'TFRE' object</p></a></li>
<li><a href='#TFRE'><p>Fit a TFRE regression model with Lasso, SCAD or MCP regularization</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>A Tuning-Free Robust and Efficient Approach to High-Dimensional
Regression</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Provide functions to estimate the coefficients in high-dimensional linear regressions via a tuning-free and robust approach. The method was published in Wang, L., Peng, B., Bradic, J., Li, R. and Wu, Y. (2020), "A Tuning-free Robust and Efficient Approach to High-dimensional Regression", Journal of the American Statistical Association, 115:532, 1700-1714(JASA’s discussion paper), &lt;<a href="https://doi.org/10.1080%2F01621459.2020.1840989">doi:10.1080/01621459.2020.1840989</a>&gt;. See also Wang, L., Peng, B., Bradic, J., Li, R. and Wu, Y. (2020), "Rejoinder to “A tuning-free robust and efficient approach to high-dimensional regression". Journal of the American Statistical Association, 115, 1726-1729, &lt;<a href="https://doi.org/10.1080%2F01621459.2020.1843865">doi:10.1080/01621459.2020.1843865</a>&gt;; Peng, B. and Wang, L. (2015), "An Iterative Coordinate Descent Algorithm for High-Dimensional Nonconvex Penalized Quantile Regression", Journal of Computational and Graphical Statistics, 24:3, 676-694, &lt;<a href="https://doi.org/10.1080%2F10618600.2014.913516">doi:10.1080/10618600.2014.913516</a>&gt;; Clémençon, S., Colin, I., and Bellet, A. (2016), "Scaling-up empirical risk minimization: optimization of incomplete u-statistics", The Journal of Machine Learning Research, 17(1):2682–2717; Fan, J. and Li, R. (2001), "Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties", Journal of the American Statistical Association, 96:456, 1348-1360, &lt;<a href="https://doi.org/10.1198%2F016214501753382273">doi:10.1198/016214501753382273</a>&gt;. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 1.0.9),RcppParallel</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp,RcppEigen,RcppParallel</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-29 18:32:42 UTC; WindyWu</td>
</tr>
<tr>
<td>Author:</td>
<td>Yunan Wu [aut, cre, cph],
  Lan Wang [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Yunan Wu &lt;yunan.wu@utdallas.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-31 19:50:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='coef.TFRE'>Extract coefficients from a 'TFRE' object</h2><span id='topic+coef.TFRE'></span>

<h3>Description</h3>

<p>Extract the coefficient vector from a fitted TFRE Lasso, SCAD or
MCP model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'TFRE'
coef(object, s, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.TFRE_+3A_object">object</code></td>
<td>
<p>Fitted &quot;TFRE&quot; model object.</p>
</td></tr>
<tr><td><code id="coef.TFRE_+3A_s">s</code></td>
<td>
<p>Regression model to use for coefficient extraction. Should be one of
&quot;1st&quot; and &quot;2nd&quot;. See more details in &quot;Details&quot;.</p>
</td></tr>
<tr><td><code id="coef.TFRE_+3A_...">...</code></td>
<td>
<p>Not used. Other arguments to extract coefficients.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>object$second_stage = "none"</code>, <code>s</code> cannot be &quot;2nd&quot;. If
<code>object$second_stage = "none"</code> and <code>s = "2nd"</code>, the function will
return the coefficient vector from the TFRE Lasso regression. If <code>object$second_stage = "scad"</code>
or <code>"mcp"</code>, and <code>s = "2nd"</code>, the function will return the coefficient
vector from the TFRE SCAD or MCP regression with the smallest HBIC.
</p>


<h3>Value</h3>

<p>The coefficient vector from the fitted TFRE model, with the first
element as the intercept.
</p>


<h3>Author(s)</h3>

<p>Yunan Wu and Lan Wang<br /> Maintainer:
Yunan Wu &lt;yunan.wu@utdallas.edu&gt;
</p>


<h3>References</h3>

<p>Wang, L., Peng, B., Bradic, J., Li, R. and Wu, Y. (2020),
<em>A Tuning-free Robust and Efficient Approach to High-dimensional Regression,
Journal of the American Statistical Association, 115:532, 1700-1714</em>,
<a href="https://doi.org/10.1080/01621459.2020.1840989">doi:10.1080/01621459.2020.1840989</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+TFRE">TFRE</a></code>, <code><a href="#topic+predict.TFRE">predict.TFRE</a></code>, <code><a href="#topic+plot.TFRE">plot.TFRE</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 20; p &lt;- 50
beta0 &lt;- c(1.5,-1.25,1,-0.75,0.5,rep(0,p-5))
eta_list &lt;- 0.1*6:15*sqrt(log(p)/n)
X &lt;- matrix(rnorm(n*p),n)
y &lt;- X %*% beta0 + rt(n,4)


Obj_TFRE_Lasso &lt;- TFRE(X, y, second_stage = "none", const_incomplete = 5)
coef(Obj_TFRE_Lasso, "1st")[1:10]
coef(Obj_TFRE_Lasso, "2nd")[1:10]

Obj_TFRE_SCAD &lt;- TFRE(X, y, eta_list = eta_list, const_incomplete = 5)
coef(Obj_TFRE_SCAD, "1st")[1:10]
coef(Obj_TFRE_SCAD, "2nd")[1:10]


Obj_TFRE_MCP &lt;- TFRE(X, y, second_stage = "mcp", eta_list = eta_list, const_incomplete = 5)
coef(Obj_TFRE_MCP, "1st")[1:10]
coef(Obj_TFRE_MCP, "2nd")[1:10]

</code></pre>

<hr>
<h2 id='est_lambda'>Estimate the tuning parameter for a TFRE Lasso regression</h2><span id='topic+est_lambda'></span>

<h3>Description</h3>

<p>Estimate the tuning parameter of the TFRE Lasso regression given 
the covariate matrix X.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>est_lambda(X, alpha0 = 0.1, const_lambda = 1.01, times = 500)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="est_lambda_+3A_x">X</code></td>
<td>
<p>Input matrix, of dimension n_obs x n_vars; each row is an observation vector.</p>
</td></tr>
<tr><td><code id="est_lambda_+3A_alpha0">alpha0</code></td>
<td>
<p>The level to estimate the tuning parameter. Default value is 0.1.
See more details in &quot;Details&quot;.</p>
</td></tr>
<tr><td><code id="est_lambda_+3A_const_lambda">const_lambda</code></td>
<td>
<p>The constant to estimate the tuning parameter, should be
greater than 1. Default value is 1.01. See more details in &quot;Details&quot;.</p>
</td></tr>
<tr><td><code id="est_lambda_+3A_times">times</code></td>
<td>
<p>The size of simulated samples to estimate the tuning parameter.
Default value is 500.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In TFRE Lasso regressions, the tuning parameter can be estimated independent
of errors. In Wang <em>et al.</em> (2020), the following tuning parameter is suggested:
</p>
<p style="text-align: center;"><code class="reqn">\lambda^* = const\_lambda * G^{-1}_{||\bm{S}_n||_\infty}(1-alpha0)</code>
</p>
<p>,
where <code class="reqn">\bm{S}_n = -2[n(n-1)]^{-1}\sum_{j=1}^n\bm{x}_j[2r_j-(n+1)]</code>, <code class="reqn">r_1,\ldots,r_n</code>
follows the uniform distribution on the per-mutations of the integers <code class="reqn">\{1,\ldots,n\}</code>,
and <code class="reqn">G^{-1}_{||\bm{S}_n||_\infty}(1-alpha0)</code> denotes the <code class="reqn">(1-alpha0)</code>-quantile
of the distribution of <code class="reqn">||\bm{S}_n||_\infty</code>.
</p>


<h3>Value</h3>

<p>The estimated tuning parameter of the TFRE Lasso regression given X.
</p>


<h3>Author(s)</h3>

<p>Yunan Wu and Lan Wang<br /> Maintainer:
Yunan Wu &lt;yunan.wu@utdallas.edu&gt;
</p>


<h3>References</h3>

<p>Wang, L., Peng, B., Bradic, J., Li, R. and Wu, Y. (2020),
<em>A Tuning-free Robust and Efficient Approach to High-dimensional Regression,
Journal of the American Statistical Association, 115:532, 1700-1714</em>,
<a href="https://doi.org/10.1080/01621459.2020.1840989">doi:10.1080/01621459.2020.1840989</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+TFRE">TFRE</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 20; p &lt;- 50
X &lt;- matrix(rnorm(n*p),n)
est_lambda(X)

</code></pre>

<hr>
<h2 id='plot.TFRE'>Plot the second stage model curve for a 'TFRE' object</h2><span id='topic+plot.TFRE'></span>

<h3>Description</h3>

<p>Plot the HBIC curve and the model size curve as a function of the 
<code>eta</code> values used, from a fitted TFRE SCAD or MCP model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'TFRE'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.TFRE_+3A_x">x</code></td>
<td>
<p>A fitted &quot;TFRE&quot; model object. It should contain a second stage model.</p>
</td></tr>
<tr><td><code id="plot.TFRE_+3A_...">...</code></td>
<td>
<p>Not used. Other arguments to be passed through plotting functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the output plot, the red line represents the HBIC curve as a function
of <code>eta</code> values, the blue line represents the number of nonzero coefficients
as a function of  <code>eta</code> values, and the purple vertical dashed line denotes
the model selected with the smallest HBIC.<br />
This function cannot plot the object if <code>object$second_stage = "none"</code>.
</p>


<h3>Value</h3>

<p>No return value, called for side effects.
</p>


<h3>Author(s)</h3>

<p>Yunan Wu and Lan Wang<br /> Maintainer:
Yunan Wu &lt;yunan.wu@utdallas.edu&gt;
</p>


<h3>References</h3>

<p>Wang, L., Peng, B., Bradic, J., Li, R. and Wu, Y. (2020),
<em>A Tuning-free Robust and Efficient Approach to High-dimensional Regression,
Journal of the American Statistical Association, 115:532, 1700-1714</em>,
<a href="https://doi.org/10.1080/01621459.2020.1840989">doi:10.1080/01621459.2020.1840989</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+TFRE">TFRE</a></code>, <code><a href="#topic+predict.TFRE">predict.TFRE</a></code>, <code><a href="#topic+coef.TFRE">coef.TFRE</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 20; p &lt;- 50
beta0 &lt;- c(1.5,-1.25,1,-0.75,0.5,rep(0,p-5))
eta_list &lt;- 0.1*6:15*sqrt(log(p)/n)
X &lt;- matrix(rnorm(n*p),n)
y &lt;- X %*% beta0 + rt(n,4)
 
Obj_TFRE_SCAD &lt;- TFRE(X, y, eta_list = eta_list, const_incomplete = 5)
plot(Obj_TFRE_SCAD)


Obj_TFRE_MCP &lt;- TFRE(X, y, second_stage = "mcp", eta_list = eta_list, const_incomplete = 5)
plot(Obj_TFRE_MCP)

</code></pre>

<hr>
<h2 id='predict.TFRE'>Make predictions from a 'TFRE' object</h2><span id='topic+predict.TFRE'></span>

<h3>Description</h3>

<p>Make predictions for new X values from a fitted TFRE Lasso, SCAD
or MCP model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'TFRE'
predict(object, newX, s, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.TFRE_+3A_object">object</code></td>
<td>
<p>Fitted &quot;TFRE&quot; model object.</p>
</td></tr>
<tr><td><code id="predict.TFRE_+3A_newx">newX</code></td>
<td>
<p>Matrix of new values for X at which predictions are to be made.</p>
</td></tr>
<tr><td><code id="predict.TFRE_+3A_s">s</code></td>
<td>
<p>Regression model to use for prediction. Should be one of &quot;1st&quot; and
&quot;2nd&quot;. See more details in &quot;Details&quot;.</p>
</td></tr>
<tr><td><code id="predict.TFRE_+3A_...">...</code></td>
<td>
<p>Not used. Other arguments to predict.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>object$second_stage = "none"</code>, <code>s</code> cannot be &quot;2nd&quot;. If
<code>object$second_stage = "none"</code> and <code>s = "2nd"</code>, the function will
return the predictions based on the TFRE Lasso regression. If <code>object$second_stage = "scad"</code>
or <code>"mcp"</code>, and <code>s = "2nd"</code>, the function will return the predictions
based on the TFRE SCAD or MCP regression with the smallest HBIC.
</p>


<h3>Value</h3>

<p>A vector of predictions for the new X values given the fitted TFRE model.
</p>


<h3>Author(s)</h3>

<p>Yunan Wu and Lan Wang<br /> Maintainer:
Yunan Wu &lt;yunan.wu@utdallas.edu&gt;
</p>


<h3>References</h3>

<p>Wang, L., Peng, B., Bradic, J., Li, R. and Wu, Y. (2020),
<em>A Tuning-free Robust and Efficient Approach to High-dimensional Regression,
Journal of the American Statistical Association, 115:532, 1700-1714</em>,
<a href="https://doi.org/10.1080/01621459.2020.1840989">doi:10.1080/01621459.2020.1840989</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+TFRE">TFRE</a></code>, <code><a href="#topic+coef.TFRE">coef.TFRE</a></code>, <code><a href="#topic+plot.TFRE">plot.TFRE</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 20; p &lt;- 50
beta0 &lt;- c(1.5,-1.25,1,-0.75,0.5,rep(0,p-5))
eta_list &lt;- 0.1*6:15*sqrt(log(p)/n)
X &lt;- matrix(rnorm(n*p),n)
y &lt;- X %*% beta0 + rt(n,4)
newX &lt;- matrix(rnorm(10*p),10)


Obj_TFRE_Lasso &lt;- TFRE(X, y, second_stage = "none", const_incomplete = 5)
predict(Obj_TFRE_Lasso, newX, "1st")
predict(Obj_TFRE_Lasso, newX, "2nd")

Obj_TFRE_SCAD &lt;- TFRE(X, y, eta_list = eta_list, const_incomplete = 5)
predict(Obj_TFRE_SCAD, newX, "1st")
predict(Obj_TFRE_SCAD, newX, "2nd")


Obj_TFRE_MCP &lt;- TFRE(X, y, second_stage = "mcp", eta_list = eta_list, const_incomplete = 5)
predict(Obj_TFRE_MCP, newX, "1st")
predict(Obj_TFRE_MCP, newX, "2nd")

</code></pre>

<hr>
<h2 id='TFRE'>Fit a TFRE regression model with Lasso, SCAD or MCP regularization</h2><span id='topic+TFRE'></span>

<h3>Description</h3>

<p>Fit a TFRE Lasso model and/or a TFRE SCAD or MCP model. The TFRE 
regression models are fitted via QICD algorithm and <em>Incomplete U-statistics</em> 
resampling technique (optional). The tuning parameter of TFRE Lasso regression 
is estimated by the covariate matrix X. The TFRE SCAD / MCP regressions are 
computed at a grid of values for the tuning parameter eta. High dimensional 
BIC (HBIC) will be used as the criterion on the TFRE SCAD / MCP tuning parameter 
searching.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TFRE(
  X,
  y,
  alpha0 = 0.1,
  const_lambda = 1.01,
  times = 500,
  incomplete = TRUE,
  const_incomplete = 10,
  thresh = 1e-06,
  maxin = 100,
  maxout = 20,
  second_stage = "scad",
  a = 3.7,
  eta_list = NULL,
  const_hbic = 6
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TFRE_+3A_x">X</code></td>
<td>
<p>Input matrix, of dimension n_obs x n_vars; each row is an observation vector.</p>
</td></tr>
<tr><td><code id="TFRE_+3A_y">y</code></td>
<td>
<p>Response variable.</p>
</td></tr>
<tr><td><code id="TFRE_+3A_alpha0">alpha0</code></td>
<td>
<p>The level to estimate the tuning parameter. Default value is 0.1.
See more details in the &quot;Details&quot; section of <code><a href="#topic+est_lambda">est_lambda</a></code>.</p>
</td></tr>
<tr><td><code id="TFRE_+3A_const_lambda">const_lambda</code></td>
<td>
<p>The constant to estimate the tuning parameter, should be
greater than 1. Default value is 1.01. See more details in the &quot;Details&quot; section
of <code><a href="#topic+est_lambda">est_lambda</a></code>.</p>
</td></tr>
<tr><td><code id="TFRE_+3A_times">times</code></td>
<td>
<p>The size of simulated samples to estimate the tuning parameter.
Default value is 500.</p>
</td></tr>
<tr><td><code id="TFRE_+3A_incomplete">incomplete</code></td>
<td>
<p>Logical. If <code>incomplete = TRUE</code>, the <em>Incomplete
U-statistics</em> resampling technique would be applied; if <code>incomplete = FALSE</code>,
the complete U-statistics would be used in computation. See more details in
Clémençon, Colin and Bellet (2016).</p>
</td></tr>
<tr><td><code id="TFRE_+3A_const_incomplete">const_incomplete</code></td>
<td>
<p>The constant for the <em>Incomplete U-statistics</em>
resampling technique. If <code>incomplete = TRUE</code>, <code>const_incomplete</code> x n_obs
samples will be randomly selected in the coefficient estimation. Default value is 10.
See more details in Clémençon, Colin and Bellet (2016).</p>
</td></tr>
<tr><td><code id="TFRE_+3A_thresh">thresh</code></td>
<td>
<p>Convergence threshold for QICD algorithm. Default value is 1e-6.
See more details in Peng and Wang (2015).</p>
</td></tr>
<tr><td><code id="TFRE_+3A_maxin">maxin</code></td>
<td>
<p>Maximum number of inner coordiante descent iterations in QICD
algorithm; default is 100. See more details in Peng and Wang (2015).</p>
</td></tr>
<tr><td><code id="TFRE_+3A_maxout">maxout</code></td>
<td>
<p>Maximum number of outter Majoriaztion Minimization step (MM)
iterations in QICD algorithm; default is 20. See more details in Peng and Wang (2015).</p>
</td></tr>
<tr><td><code id="TFRE_+3A_second_stage">second_stage</code></td>
<td>
<p>Penalty function for the second stage model. Character vector,
which can be &quot;scad&quot;, &quot;mcp&quot; and &quot;none&quot;. If <code>second_stage = "scad"</code>, the TFRE
SCAD regression would be fitted; if <code>second_stage = "mcp"</code>, the TFRE MCP
regression would be fitted; if <code>scad = "none"</code>, only the TFRE Lasso regression
outputs would be returned.</p>
</td></tr>
<tr><td><code id="TFRE_+3A_a">a</code></td>
<td>
<p>an unknown parameter in SCAD and MCP penalty functions. The default
value is 3.7, suggested by Fan and Li (2001).</p>
</td></tr>
<tr><td><code id="TFRE_+3A_eta_list">eta_list</code></td>
<td>
<p>A numerical vector for the tuning parameters to be used in the
TFRE SCAD or MCP regression. Cannot be <code>NULL</code> if <code>second_stage = "scad"</code>
or <code>"mcp"</code>.</p>
</td></tr>
<tr><td><code id="TFRE_+3A_const_hbic">const_hbic</code></td>
<td>
<p>The constant to be used in calculating HBIC in the TFRE SCAD
regression. Default value is 6. See more details in &quot;Details&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Wang <em>et al.</em> (2020) proposed the TFRE Lasso estimator for high-dimensional
linear regressions with heavy-tailed errors as below:
</p>
<p style="text-align: center;"><code class="reqn">\widehat{\bm{\beta}}(\lambda^*) = \arg\min_{\bm{\beta}}\frac{1}{n(n-1)}{\sum\sum}_{i\neq j}\left|(Y_i-\bm{x}_i^T\bm{\beta})-(Y_j-\bm{x}_j^T\bm{\beta})\right| + \lambda^*\sum_{k=1}^p|\beta_k|,</code>
</p>

<p>where <code class="reqn">\lambda^*</code> is the tuning parameter estimated by <code><a href="#topic+est_lambda">est_lambda</a></code>.
The TFRE Lasso model is fitted by QICD algorithm proposed in Peng and Wang (2015).
To overcome the computational barrier arising from the U-statistics structure of
the aforementioned loss function, we apply the <em>Incomplete U-statistics</em>
resampling technique which was first proposed in Clémençon, Colin and Bellet (2016).<br />
Wang <em>et al.</em> (2020) also proposed a second-stage enhancement by using the
TFRE Lasso estimator <code class="reqn">\widehat{\bm{\beta}}(\lambda^*)</code> as an initial estimator.
It is defined as:
</p>
<p style="text-align: center;"><code class="reqn">\widetilde{\bm{\beta}}^{(1)} = \arg\min_{\bm{\beta}}\frac{1}{n(n-1)}{\sum\sum}_{i\neq j}\left|(Y_i-\bm{x}_i^T\bm{\beta})-(Y_j-\bm{x}_j^T\bm{\beta})\right| + \sum_{k=1}^pp_{\eta}'(|\widehat{\beta}_k(\lambda^*)|)|\beta_k|,</code>
</p>

<p>where <code class="reqn">p'_{\eta}(\cdot)</code> denotes the derivative of some nonconvex penalty
function <code class="reqn">p_{\eta}(\cdot)</code>, <code class="reqn">\eta &gt; 0</code> is a tuning parameter. This
function implements the second-stage enhancement with two popular nonconvex
penalty functions: SCAD and MCP. The modified high-dimensional BIC criterion
in Wang <em>et al.</em> (2020) is employed for selecting <code class="reqn">\eta</code>. Define:
</p>
<p style="text-align: center;"><code class="reqn">HBIC(\eta) = \log\left\{{\sum\sum}_{i\neq j}\left|(Y_i-\bm{x}_i^T\widetilde{\bm{\beta}}_{\eta})-(Y_j-\bm{x}_j^T\widetilde{\bm{\beta}}_{\eta})\right|\right\} + |A_\eta|\frac{\log\log n}{n* const\_hbic}\log p,</code>
</p>

<p>where <code class="reqn">\widetilde{\bm{\beta}}_{\eta}</code> denotes the second-stage estimator with
the tuning parameter value <code class="reqn">\eta</code>, and <code class="reqn">|A_\eta|</code> denotes the cardinality
of the index set of the selected model. This function selects the value of <code class="reqn">\eta</code>
that minimizes HBIC(<code class="reqn">\eta</code>).
</p>


<h3>Value</h3>

<p>An object of class &quot;TFRE&quot;, which is a list containing at least the
following components:
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p>The input matrix used.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>The response variable used.</p>
</td></tr>
<tr><td><code>incomplete</code></td>
<td>
<p>Logical. <code>TRUE</code> if the <em>Incomplete U-statistics</em>
resampling technique is applied, and <code>FALSE</code> if not.</p>
</td></tr>
<tr><td><code>beta_TFRE_Lasso</code></td>
<td>
<p>The estimated coefficient vector of the TFRE Lasso regression.
The first element is the estimated intercept.</p>
</td></tr>
<tr><td><code>tfre_lambda</code></td>
<td>
<p>The estimated tuning parameter of the TFRE Lasso regression.</p>
</td></tr>
<tr><td><code>second_stage</code></td>
<td>
<p>Character vector, <code>"scad"</code> if the TFRE SCAD regression
is fitted,  <code>"mcp"</code> if the TFRE MCP regression is fitted, <code>"none"</code> if
only the TFRE Lasso regression is fitted.</p>
</td></tr>
</table>
<p>If <code>second_stage = "scad"</code>, then the fitted TFRE object will also contain
an object named as &quot;TFRE_scad&quot;, which is a list containing the following components:
</p>
<table>
<tr><td><code>Beta_TFRE_scad</code></td>
<td>
<p>The estimated coefficient matrix of the TFRE SCAD regression.
The diminsion is n_eta x (p+1) with the first column to be the intercepts,
where n_eta is the length of <code>eta_list</code> vector.</p>
</td></tr>
<tr><td><code>df_TFRE_scad</code></td>
<td>
<p>The number of nonzero coefficients (intercept excluded) for
each value in <code>eta_list</code>.</p>
</td></tr>
<tr><td><code>eta_list</code></td>
<td>
<p>The tuning parameter vector used in the TFRE SCAD regressions</p>
</td></tr>
<tr><td><code>hbic</code></td>
<td>
<p>A numerical vector of HBIC values for the TFRE SCAD model corresponding
to each value in <code>eta_list</code>.</p>
</td></tr>
<tr><td><code>eta_min</code></td>
<td>
<p>The eta value which yields the smallest HBIC value in the TFRE
SCAD regression.</p>
</td></tr>
<tr><td><code>Beta_TFRE_scad_min</code></td>
<td>
<p>The estimated coefficient vector which employs <code>eta_min</code>
as the eta value in the TFRE SCAD regression.</p>
</td></tr>
</table>
<p>If <code>second_stage = "mcp"</code>, then the fitted TFRE object will also contain
an object named as &quot;TFRE_mcp&quot;, which is a list containing the following components:
</p>
<table>
<tr><td><code>Beta_TFRE_mcp</code></td>
<td>
<p>The estimated coefficient matrix of the TFRE MCP regression.
The diminsion is n_eta x (p+1) with the first column to be the intercepts,
where n_eta is the length of <code>eta_list</code> vector.</p>
</td></tr>
<tr><td><code>df_TFRE_mcp</code></td>
<td>
<p>The number of nonzero coefficients (intercept excluded) for
each value in <code>eta_list</code>.</p>
</td></tr>
<tr><td><code>eta_list</code></td>
<td>
<p>The tuning parameter vector used in the TFRE MCP regressions</p>
</td></tr>
<tr><td><code>hbic</code></td>
<td>
<p>A numerical vector of HBIC values for the TFRE MCP model corresponding
to each value in <code>eta_list</code>.</p>
</td></tr>
<tr><td><code>eta_min</code></td>
<td>
<p>The eta value which yields the smallest HBIC value in the TFRE
MCP regression.</p>
</td></tr>
<tr><td><code>Beta_TFRE_mcp_min</code></td>
<td>
<p>The estimated coefficient vector which employs <code>eta_min</code>
as the eta value in the TFRE MCP regression.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yunan Wu and Lan Wang<br /> Maintainer:
Yunan Wu &lt;yunan.wu@utdallas.edu&gt;
</p>


<h3>References</h3>

<p>Wang, L., Peng, B., Bradic, J., Li, R. and Wu, Y. (2020),
<em>A Tuning-free Robust and Efficient Approach to High-dimensional Regression,
Journal of the American Statistical Association, 115:532, 1700-1714</em>,
<a href="https://doi.org/10.1080/01621459.2020.1840989">doi:10.1080/01621459.2020.1840989</a>.<br />
Peng, B. and Wang, L. (2015),
<em>An Iterative Coordinate Descent Algorithm for High-Dimensional Nonconvex
Penalized Quantile Regression, Journal of Computational and Graphical Statistics,
24:3, 676-694</em>, <a href="https://doi.org/10.1080/10618600.2014.913516">doi:10.1080/10618600.2014.913516</a>. <br />
Clémençon, S., Colin, I., and Bellet, A. (2016),
<em>Scaling-up empirical risk minimization: optimization of incomplete u-statistics.
The Journal of Machine Learning Research, 17(1):2682–2717</em>. <br />
Fan, J. and Li, R. (2001),
<em>Variable Selection via Nonconcave Penalized Likelihood and its Oracle
Properties, Journal of the American Statistical Association, 96:456, 1348-1360</em>,
<a href="https://doi.org/10.1198/016214501753382273">doi:10.1198/016214501753382273</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.TFRE">predict.TFRE</a></code>, <code><a href="#topic+coef.TFRE">coef.TFRE</a></code>, <code><a href="#topic+plot.TFRE">plot.TFRE</a></code>, <code><a href="#topic+est_lambda">est_lambda</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 20; p &lt;- 50
beta0 &lt;- c(1.5,-1.25,1,-0.75,0.5,rep(0,p-5))
eta_list &lt;- 0.1*6:15*sqrt(log(p)/n)
X &lt;- matrix(rnorm(n*p),n)
y &lt;- X %*% beta0 + rt(n,4)


Obj_TFRE_Lasso &lt;- TFRE(X, y, second_stage = "none", const_incomplete = 5)
Obj_TFRE_Lasso$beta_TFRE_Lasso[1:10]

Obj_TFRE_SCAD &lt;- TFRE(X, y, eta_list = eta_list, const_incomplete = 5)
Obj_TFRE_SCAD$TFRE_scad$hbic
Obj_TFRE_SCAD$TFRE_scad$df_TFRE_scad
Obj_TFRE_SCAD$TFRE_scad$Beta_TFRE_scad_min[1:10]


Obj_TFRE_MCP &lt;- TFRE(X, y, second_stage = "mcp", eta_list = eta_list, const_incomplete = 5)
Obj_TFRE_MCP$TFRE_mcp$hbic
Obj_TFRE_MCP$TFRE_mcp$df_TFRE_mcp
Obj_TFRE_MCP$TFRE_mcp$Beta_TFRE_mcp_min[1:10]

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
