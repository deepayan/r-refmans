<!DOCTYPE html><html><head><title>Help for package RSKC</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {RSKC}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#CER'>
<p>Classification Error Rate (CER)</p></a></li>
<li><a href='#Clest'>
<p>An implementation of Clest with robust sparse K-means.</p>
CER is used as a similarity measure.</a></li>
<li><a href='#DBWorld'>
<p>E-mails from DBWorld mailing list</p></a></li>
<li><a href='#DutchUtility'>
<p>Multiple Features Data Set of Robert P.W. Duin.</p></a></li>
<li><a href='#optd'>
<p>Optical Recognition of Handwritten Digits of Frank A, Asuncion A (2010).</p></a></li>
<li><a href='#revisedsil'>
<p>The revised silhouette</p></a></li>
<li><a href='#RSKC'><p>Robust Sparse K-means</p></a></li>
<li><a href='#RSKC-internal'><p>Internal RSKC functions</p></a></li>
<li><a href='#Sensitivity'>
<p>Compute the sensitivities (probability of true positive) of each cluster</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Robust Sparse K-Means</td>
</tr>
<tr>
<td>Version:</td>
<td>2.4.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2016-08-11</td>
</tr>
<tr>
<td>Author:</td>
<td>Yumi Kondo</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Yumi Kondo &lt;y.kondo@stat.ubc.ca&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>This RSKC package contains a function RSKC which runs the robust sparse K-means clustering algorithm.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Depends:</td>
<td>flexclust, stats, R (&ge; 2.14.0)</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2016-08-27 21:58:32 UTC; yumikondo</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2016-08-28 07:35:26</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
</table>
<hr>
<h2 id='CER'>
Classification Error Rate (CER)
</h2><span id='topic+CER'></span>

<h3>Description</h3>

<p>Compute the classification error rate of two partitions. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CER(ind, true.ind,nob=length(ind))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CER_+3A_ind">ind</code></td>
<td>

<p>Vector, containing the cluster labels of each case of a partition 1. 
</p>
</td></tr>
<tr><td><code id="CER_+3A_true.ind">true.ind</code></td>
<td>

<p>Vector, containing the cluster labels of each case of a partition 2. 
</p>
</td></tr>
<tr><td><code id="CER_+3A_nob">nob</code></td>
<td>

<p>The number of cases (the length of the vector ind and true ind)
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Return a CER value.
CER = 0 means perfect agreement between two partitions and CER = 1 means complete disagreement of two partitions.
Note: 0 &lt;= <code>CER</code> &lt;= 1
</p>


<h3>Note</h3>

<p>This function uses <code>comb</code>, which generates all combinations of the elements in the vector <code>ind</code>.
For this reason, the function <code>CER</code> is not suitable for vector in a large dimension.  
</p>


<h3>Author(s)</h3>

<p>Yumi Kondo &lt;y.kondo@stat.ubc.ca&gt;
</p>


<h3>References</h3>

<p>H. Chipman and R. Tibshirani. Hybrid hierarchical clustering with 
applications to microarray data. Biostatistics, 7(2):286-301, 2005.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>vec1&lt;-c(1,1,1,2,3,3,3,2,2)
vec2&lt;-c(3,3,3,1,1,2,2,1,1)
CER(vec1,vec2)

</code></pre>

<hr>
<h2 id='Clest'>
An implementation of Clest with robust sparse K-means. 
CER is used as a similarity measure.
</h2><span id='topic+Clest'></span>

<h3>Description</h3>

<p>The function <code>Clest</code> performs Clest ( Dudoit and Fridlyand (2002)) with CER as the measure of the agreement between two partitions (in each training set).
The following clustering algorithm can be used: <em>K</em>-means, trimmed <em>K</em>-means, sparse <em>K</em>-means and robust sparse <em>K</em>-means. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Clest(d, maxK, alpha, B = 15, B0 = 5, nstart = 1000, 

      L1 = 6, beta = 0.1, pca = TRUE, silent=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Clest_+3A_d">d</code></td>
<td>

<p>A numerical data matrix (<code>N</code> by <code>p</code>) where <code>N</code> is the number of cases and <code>p</code> is the number of features. The cases are clustered.
</p>
</td></tr>
<tr><td><code id="Clest_+3A_maxk">maxK</code></td>
<td>

<p>The maximum number of clusters that you suspect.
</p>
</td></tr>
<tr><td><code id="Clest_+3A_alpha">alpha</code></td>
<td>

<p>See <code><a href="#topic+RSKC">RSKC</a></code>.
</p>
</td></tr>
<tr><td><code id="Clest_+3A_b">B</code></td>
<td>

<p>The number of times that an observed dataset <code>d</code> is randomly partitioned into a learning set and a training set.
Note that each generated reference dataset is partitioned into a learning and a testing set only once to ease the computational cost.
</p>
</td></tr>
<tr><td><code id="Clest_+3A_b0">B0</code></td>
<td>

<p>The number of times that the reference dataset is generated.
</p>
</td></tr>
<tr><td><code id="Clest_+3A_nstart">nstart</code></td>
<td>

<p>The number of random initial sets of cluster centers at Step(a) of robust sparse <em>K</em>-means clustering.
</p>
</td></tr>
<tr><td><code id="Clest_+3A_l1">L1</code></td>
<td>

<p>See <code><a href="#topic+RSKC">RSKC</a></code>.
</p>
</td></tr>
<tr><td><code id="Clest_+3A_beta">beta</code></td>
<td>

<p>0 &lt;= <code>beta</code> &lt;= 1: significance level. 
Clest chooses the number of clusters that returns the strongest significant evidence against the hypothesis H0 : K = 1. 
</p>
</td></tr>
<tr><td><code id="Clest_+3A_pca">pca</code></td>
<td>

<p>Logical, if <code>TRUE</code>, then reference datasets are generated from a PCA reference distribution. 
If <code>FALSE</code>, then the reference data  set is generated from a simple reference distribution.
</p>
</td></tr>
<tr><td><code id="Clest_+3A_silent">silent</code></td>
<td>

<p>Logical, if <code>TRUE</code>, then the number of iteration on progress is not printed.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>K</code></td>
<td>

<p>The solution of Clest; the estimated number of clusters.
</p>
</td></tr>
<tr><td><code>result.table</code></td>
<td>

<p>A real matrix (<code>maxK-1</code> by 4). 
Each row represents <code>K=2</code>,...,<code>maxK</code> and columns represent the test statistics (=observed CER-reference CER), observed CER, reference CER and <em>P</em>-value. 
</p>
</td></tr>
<tr><td><code>referenceCERs</code></td>
<td>

<p>A matrix (<code>B0</code> by <code>maxK-1</code>), containing CERs of testing datasets from generated datasets for each <code>K=2,...,maxK</code>.
</p>
</td></tr>
<tr><td><code>observedCERs</code></td>
<td>

<p>A matrix (<code>B</code> by <code>maxK-1</code>), containing CERs of <code>B</code> testing sets for each <code>K=2,...,maxK</code>.
</p>
</td></tr> 
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yumi Kondo  &lt;y.kondo@stat.ubc.ca&gt;
</p>


<h3>References</h3>

<p>Yumi Kondo (2011), Robustificaiton of the sparse K-means clustering algorithm, MSc. Thesis, University of British Columbia
<a href="http://hdl.handle.net/2429/37093">http://hdl.handle.net/2429/37093</a>
</p>
<p>S. Dudoit and J. Fridlyand. A prediction-based resampling method for estimating the number of clusters in a dataset. Genome Biology, 3(7), 2002. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# little simulation function 
sim &lt;-
function(mu,f){
   D&lt;-matrix(rnorm(60*f),60,f)
   D[1:20,1:50]&lt;-D[1:20,1:50]+mu
   D[21:40,1:50]&lt;-D[21:40,1:50]-mu  
   return(D)
   }
 
 set.seed(1)
 d&lt;-sim(1.5,100); # non contaminated dataset with noise variables
 
# Clest with robust sparse K-means
rsk&lt;-Clest(d,5,alpha=1/20,B=3,B0=10, beta = 0.05, nstart=100,pca=TRUE,L1=3,silent=TRUE);
# Clest with K-means
k&lt;-Clest(d,5,alpha=0,B=3,B0=10, beta = 0.05, nstart=100,pca=TRUE,L1=NULL,silent=TRUE);

## End(Not run)
</code></pre>

<hr>
<h2 id='DBWorld'>
E-mails from DBWorld mailing list
</h2><span id='topic+DBWorld'></span><span id='topic+rawDBWorld'></span>

<h3>Description</h3>

<p>The dataset contains n= 64 bodies of e-mails in binary bag-of-words
representation which Filannino manually collected
from DBWorld mailing list.

DBWorld mailing list announces conferences, jobs, books, software and
grants.

Filannino applied supervised learning algorithm to classify e-mails
between &ldquo;announces of conferences&rdquo; and &ldquo;everything else&rdquo;.

Out of 64 e-mails, 29 are about conference announcements and 35 are not.
</p>
<p>Every e-mail is represented as a vector containing p binary values,
where p is the size of the vocabulary extracted from the entire
corpus with some constraints:
the common words such as &ldquo;the&rdquo;, &ldquo;is&rdquo; or &ldquo;which&rdquo;, so-called stop words, 
and words that have less than 3 characters or more than 30 chracters
are removed from the dataset. 

The entry of the vector is 1 if the corresponding word belongs to the
e-mail and 0 otherwise.


The number of unique words in the dataset is p=4702.

The dataset is originally from the UCI Machine Learning
Repository DBWorldData.
</p>
<p><code>rawDBWorld</code> is a list of 64 objects containing the original E-mails.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(DBWorld)
data(rawDBWorld)
</code></pre>


<h3>Details</h3>

<p>See Bache K, Lichman M (2013). for details of the data descriptions. 
The original dataset is freely available from USIMachine Learning Repository website 
<a href="http://archive.ics.uci.edu/ml/datasets/DBWorld+e-mails">http://archive.ics.uci.edu/ml/datasets/DBWorld+e-mails</a>
</p>


<h3>Author(s)</h3>

<p>Yumi Kondo &lt;y.kondo@stat.ubc.ca&gt;
</p>


<h3>References</h3>

<p>Bache K, Lichman M (2013). UCI Machine Learning Repository.&quot; <a href="http://archive.ics.uci.edu/ml/datasets">http://archive.ics.uci.edu/ml/datasets</a>
</p>
<p>Filannino, M., (2011). 'DBWorld e-mail classification using a very small corpus', Project of Machine Learning course, University of Manchester. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data(DBWorld)
data(rawDBWorld)

## End(Not run)
</code></pre>

<hr>
<h2 id='DutchUtility'>
Multiple Features Data Set of Robert P.W. Duin.
</h2><span id='topic+DutchUtility'></span><span id='topic+showDigit'></span>

<h3>Description</h3>

<p>This dataset consists of features of handwritten numerals (&lsquo;0&rsquo;&ndash;&lsquo;9&rsquo;) (<em>K</em>=10) extracted from a collection of Dutch utility maps.

Two hundred patterns per class (for a total of 2,000 (=<em>N</em>) patterns)
have been digitized in binary images.

Raw observations are 32x45 bitmmaps, which are divided into
nooverlapping blocks of 2x3 and the number of pixels are counted in
each block. 

This generate <em>p</em>=240 (16x15) variable, recodring the
normalized counts of pixels in each block and each element is an
integer in the range 0 to 6.

<code>rownames</code> of <code>DutchUtility</code> contains the true digits and <code>colnames</code> of it contains the position of the block matrix, from which the normalized counts of pixels are taken. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(DutchUtility)
showDigit(index,cex.main=1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DutchUtility_+3A_index">index</code></td>
<td>

<p>A scalar containing integers between 1 and 2000.
The function <code>ShowDigit</code> regenerates the sampled versions of the original images may be obtained (15x16 pixels).
(the source image (32x45) dataset is lost)
</p>
</td></tr>
<tr><td><code id="DutchUtility_+3A_cex.main">cex.main</code></td>
<td>

<p>Specify the size of the title text with a numeric value of length 1.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The original dataset is freely available from USIMachine Learning Repository (Frank and Asuncion (2010)) website http://archive
.ics.uci.edu/ml/datasets.html.
</p>


<h3>Author(s)</h3>

<p>Yumi Kondo &lt;y.kondo@stat.ubc.ca&gt;
</p>


<h3>References</h3>

<p>Frank A, Asuncion A (2010). UCI Machine Learning Repository.&quot; <a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

data(DutchUtility)

truedigit &lt;- rownames(DutchUtility)
(re &lt;- RSKC(DutchUtility,ncl=10,alpha=0.1,L1=5.7,nstart=1000))
Sensitivity(re$labels,truedigit)
table(re$labels,truedigit)

## Check the bitmap of the trimmed observations 
showDigit(re$oW[1])
## Check the features which receive zero weights
names(which(re$weights==0))

## End(Not run)
</code></pre>

<hr>
<h2 id='optd'>
Optical Recognition of Handwritten Digits of Frank A, Asuncion A (2010).
</h2><span id='topic+optd'></span><span id='topic+bitmapLab'></span><span id='topic+bitmapMat'></span><span id='topic+showbitmap'></span>

<h3>Description</h3>

<p>The dataset describes n = 1797 digits from 0 to 9 (K = 10), handwritten by 13 subjects. 
Raw observations are 32x32 bitmaps, which are divided into nonoverlapping
blocks of 4x4 and the number of on pixels are counted in each block. 
This generates p = 64 (= 8x8) variable, recording the normalized counts of pixels in each block and each element
is an integer in the range 0 to 16.
The row names of the matrix optd contains the true labels (between 0 and 9), and the column names of it contains the position of the block in original bitmap.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(optd)
showbitmap(index)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optd_+3A_index">index</code></td>
<td>

<p>A vector containing integers between 1 and 1797.
Given the observation indices, the <code>showbitmap</code> returns their original 32 by 32 bitmaps on R console. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The original dataset is freely available from USIMachine Learning Repository (Frank and Asuncion (2010)) website http://archive
.ics.uci.edu/ml/datasets.html.
</p>


<h3>Author(s)</h3>

<p>Yumi Kondo &lt;y.kondo@stat.ubc.ca&gt;
</p>


<h3>References</h3>

<p>Frank A, Asuncion A (2010). UCI Machine Learning Repository.&quot; <a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

data(optd)

truedigit &lt;- rownames(optd)
(re &lt;- RSKC(optd,ncl=10,alpha=0.1,L1=5.7,nstart=1000))
Sensitivity(re$labels,truedigit)
table(re$labels,truedigit)

## Check the bitmap of the trimmed observations 
showbitmap(re$oW)
## Check the features which receive zero weights
names(which(re$weights==0))

## End(Not run)
</code></pre>

<hr>
<h2 id='revisedsil'>
The revised silhouette
</h2><span id='topic+revisedsil'></span>

<h3>Description</h3>

<p>This function returns a revised silhouette plot, 
cluster centers in weighted squared Euclidean distances and  
a matrix containing the weighted squared Euclidean distances between cases and each cluster center.
Missing values are adjusted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>revisedsil(d,reRSKC=NULL,CASEofINT=NULL,col1="black",
	CASEofINT2 = NULL, col2="red", print.plot=TRUE, 
	W=NULL,C=NULL,out=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="revisedsil_+3A_d">d</code></td>
<td>

<p>A numerical data matrix, <code>N</code> by <code>p</code>, where <code>N</code> is the number of cases and <code>p</code> is the number of features.
</p>
</td></tr>
<tr><td><code id="revisedsil_+3A_rerskc">reRSKC</code></td>
<td>

<p>A list output from RSKC function.
</p>
</td></tr>
<tr><td><code id="revisedsil_+3A_caseofint">CASEofINT</code></td>
<td>

<p>Necessary if print.plot=TRUE.
A vector of the case indices that appear in the revised silhouette plot. 
The revised silhouette widths of these indices are colored in <code>col1</code> if <code>CASEofINT != NULL</code>.
The average silhouette of each cluster printed in the plot is computed EXCLUDING these cases.
</p>
</td></tr>
<tr><td><code id="revisedsil_+3A_col1">col1</code></td>
<td>

<p>See <code>CASEofINT</code>.
</p>
</td></tr>
<tr><td><code id="revisedsil_+3A_caseofint2">CASEofINT2</code></td>
<td>

<p>A vector of the case indices that appear in the revised silhouette plot.
The indices are colored in <code>col2</code>.
</p>
</td></tr>
<tr><td><code id="revisedsil_+3A_col2">col2</code></td>
<td>

<p>See <code>CASEofINT2</code>
</p>
</td></tr>
<tr><td><code id="revisedsil_+3A_print.plot">print.plot</code></td>
<td>

<p>If <code>TRUE</code>, the revised silhouette is plotted.
</p>
</td></tr>
<tr><td><code id="revisedsil_+3A_w">W</code></td>
<td>

<p>Necessary if <code>reRSKC = NULL</code>. A positive real vector of weights of length <code>p</code>.
</p>
</td></tr>
<tr><td><code id="revisedsil_+3A_c">C</code></td>
<td>

<p>Necessary if <code>reRSKC = NULL</code>.  An integer vector of class labels of length <code>N</code>.
</p>
</td></tr>
<tr><td><code id="revisedsil_+3A_out">out</code></td>
<td>

<p>Necessary if <code>reRSKC = NULL</code>. 
Vector of the case indices that should be excluded in the calculation of cluster centers.
In <code>RSKC</code>, cluster centers are calculated without the cases that have the furthest 100*<code>alpha</code> % Weighted squared Euclidean distances to their closest cluster centers.
If one wants to obtain the cluster centers from <code>RSKC</code> output, set <code>out</code> = <code>&lt;RSKCoutput&gt;$oW</code>. 
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>trans.mu</code></td>
<td>

<p>Cluster centers in reduced weighted dimension.
See example for more detail.
</p>
</td></tr>
<tr><td><code>WdisC</code></td>
<td>

<p><code>N</code> by <code>ncl</code> matrix, where <code>ncl</code> is the prespecified number of clusters. It contains the weighted distance between each case and all cluster centers.
See example for more detail.
</p>
</td></tr>
<tr><td><code>sil.order</code></td>
<td>

<p>Silhouette values of each case in the order of the case index.
</p>
</td></tr>
<tr><td><code>sil.i</code></td>
<td>

<p>Silhouette values of cases ranked by decreasing order within clusters.
The corresponding case index are in <code>obs.i</code>
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yumi Kondo &lt;y.kondo@stat.ubc.ca&gt;
</p>


<h3>References</h3>

<p>Yumi Kondo (2011), Robustificaiton of the sparse K-means clustering algorithm, MSc. Thesis, University of British Columbia
<a href="http://hdl.handle.net/2429/37093">http://hdl.handle.net/2429/37093</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# little simulation function 
sim &lt;-
function(mu,f){
   D&lt;-matrix(rnorm(60*f),60,f)
   D[1:20,1:50]&lt;-D[1:20,1:50]+mu
   D[21:40,1:50]&lt;-D[21:40,1:50]-mu  
   return(D)
   }


### output trans.mu ###

p&lt;-200;ncl&lt;-3
# simulate a 60 by p data matrix with 3 classes 
d&lt;-sim(2,p)
# run RSKC
re&lt;-RSKC(d,ncl,L1=2,alpha=0.05)
# cluster centers in weighted squared Euclidean distances by function sil
sil.mu&lt;-revisedsil(d,W=re$weights,C=re$labels,out=re$oW,print.plot=FALSE)$trans.mu
# calculation 
trans.d&lt;-sweep(d[,re$weights!=0],2,sqrt(re$weights[re$weights!=0]),FUN="*") 
class&lt;-re$labels;class[re$oW]&lt;-ncl+1
MEANs&lt;-matrix(NA,ncl,ncol(trans.d))
for ( i in 1 : 3) MEANs[i,]&lt;-colMeans(trans.d[class==i,,drop=FALSE])
sil.mu==MEANs
# coincides 

### output WdisC ###

p&lt;-200;ncl&lt;-3;N&lt;-60
# generate 60 by p data matrix with 3 classes 
d&lt;-sim(2,p)
# run RSKC
re&lt;-RSKC(d,ncl,L1=2,alpha=0.05)
si&lt;-revisedsil(d,W=re$weights,C=re$labels,out=re$oW,print.plot=FALSE)
si.mu&lt;-si$trans.mu
si.wdisc&lt;-si$WdisC
trans.d&lt;-sweep(d[,re$weights!=0],2,sqrt(re$weights[re$weights!=0]),FUN="*") 
WdisC&lt;-matrix(NA,N,ncl)
for ( i in 1 : ncl) WdisC[,i]&lt;-rowSums(scale(trans.d,center=si.mu[i,],scale=FALSE)^2)
# WdisC and si.wdisc coincides

</code></pre>

<hr>
<h2 id='RSKC'>Robust Sparse K-means</h2><span id='topic+RSKC'></span><span id='topic+RSKC.trimkmeans'></span><span id='topic+RSKC.trimkmeans.missing'></span>

<h3>Description</h3>

<p>The robust sparse <em>K</em>-means clustering method by Kondo (2011). In this algorithm, sparse <em>K</em>-means (Witten and Tibshirani (2010)) is robustified by iteratively trimming the prespecified proportion of cases in the weighted squared Euclidean distances and the squared Euclidean distances.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RSKC(d, ncl, alpha, L1 = 12, nstart = 200, 
silent=TRUE, scaling = FALSE, correlation = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RSKC_+3A_d">d</code></td>
<td>

<p>A numeric matrix of data, <code>N</code> by <code>p</code>, where <code>N</code> is the number of cases and <code>p</code> is the number of features. 
Cases are partitioned into <code>ncl</code> clusters. Missing values are accepted. 
</p>
</td></tr>
<tr><td><code id="RSKC_+3A_ncl">ncl</code></td>
<td>

<p>The prespecified number of clusters.
</p>
</td></tr>
<tr><td><code id="RSKC_+3A_alpha">alpha</code></td>
<td>

<p>0 &lt;= <code>alpha</code> &lt;= 1, the proportion of the cases to be trimmed in robust sparse <em>K</em>-means. 
</p>
<p>If <code>alpha</code> &gt; 0 and <code>L1</code> &gt;= 1 then <code>RSKC</code> performs robust sparse <em>K</em>-means. 
</p>
<p>If <code>alpha</code> &gt; 0 and <code>L1</code> = <code>NULL</code> then <code>RSKC</code> performs trimmed <em>K</em>-means. 
</p>
<p>If <code>alpha</code> = 0 and <code>L1</code> &gt;=1 then <code>RSKC</code> performs sparse <em>K</em>-means (with the algorithm of Lloyd (1982)).
</p>
<p>If <code>alpha</code> = 0 and <code>L1</code> = <code>NULL</code> then <code>RSKC</code> performs <em>K</em>-means (with the algorithm of Lloyd). 
</p>
<p>For more details on trimmed <em>K</em>-means, see Gordaliza (1991a), Gordaliza (1991b).
</p>
</td></tr>
<tr><td><code id="RSKC_+3A_l1">L1</code></td>
<td>

<p>A single L1 bound on weights (the feature weights). If <code>L1</code> is small, then few features will
have non-zero weights. If <code>L1</code> is large then all features
will have non-zero weights.
If <code>L1</code> = <code>NULL</code> then <code>RSKC</code> performs nonsparse clustering (see <code>alpha</code>).
</p>
</td></tr>
<tr><td><code id="RSKC_+3A_nstart">nstart</code></td>
<td>

<p>The number of random initial sets of cluster centers in every step (a) which performs <em>K</em>-means or trimmed <em>K</em>-means.
</p>
</td></tr>
<tr><td><code id="RSKC_+3A_silent">silent</code></td>
<td>

<p>If <code>TRUE</code>, then the processing step is not printed.
</p>
</td></tr>
<tr><td><code id="RSKC_+3A_scaling">scaling</code></td>
<td>

<p>If <code>TRUE</code>, <code>RSKC</code> subtracts the each entry of data matrix by the corresponding column mean and divide it by the corresponding column SD: see <code><a href="base.html#topic+scale">scale</a></code>
</p>
</td></tr>
<tr><td><code id="RSKC_+3A_correlation">correlation</code></td>
<td>

<p>If <code>TRUE</code>, <code>RSKC</code> centers and
scales the rows of <code>data</code> before the clustering is performed. i.e., <code>trans.d = t(scale(t(d)))</code>
The squared Euclidean distance between cases in the transformed dataset <code>trans.d</code> is proportional to the dissimilality measure based on the correlation between the cases in the dataset <code>d</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Robust sparse <em>K</em>-means is a clustering method that extends the sparse <em>K</em>-means clustering of Witten and Tibshirani to make it resistant to oultiers by trimming a fixed proportion of observations in each iteration. 

These outliers are flagged both in terms of their weighted and unweighted distances to eliminate the effects of outliers in the selection of feature weights and the selection of a partition.

In Step (a) of sparse <em>K</em>-means, given fixed weights, the algorithm aims to maximize the objective function over a partition i.e. it performs <em>K</em>-means on a weighted dataset.
Robust sparse <em>K</em>-means robustifies Step (a) of sparse <em>K</em>-means by performing trimmed <em>K</em>-means on a weighted dataset: it trims cases in weighted squared Euclidean distances.  

Before Step (b), where, given a partition, the algorithm aims to maximize objective function over weights, the robust sparse 
<em>K</em>-means has an intermediate robustifying step, Step (a-2).
At this step, it trims cases in squared Euclidean distances.

Given a partition and trimmed cases from Step (a) and Step (a-2), the objective function is maximized over weights at Step(b).
The objective function is calculated without the trimmed cases in Step (a) and Step(a-2).

The robust sparse <em>K</em>-means algorithm repeat Step (a), Step (a-2) and Step (b) until a stopping criterion is satisfied.

For the calculation of cluster centers in the weighted distances, see <code>revisedsil</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>N</code></td>
<td>
<p>The number of cases.</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>The number of features.</p>
</td></tr>
<tr><td><code>ncl</code></td>
<td>
<p>See <code>ncl</code> above.</p>
</td></tr>
<tr><td><code>L1</code></td>
<td>
<p>See <code>L1</code> above.</p>
</td></tr>
<tr><td><code>nstart</code></td>
<td>
<p>See <code>nstart</code> above.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>See <code>alpha</code> above.</p>
</td></tr>
<tr><td><code>scaling</code></td>
<td>
<p>See <code>scaling</code> above.</p>
</td></tr>
<tr><td><code>correlation</code></td>
<td>
<p>See <code>correlation</code> above.</p>
</td></tr>
<tr><td><code>missing</code></td>
<td>
<p>It is <code>TRUE</code> if at least one point is missing in the data matrix, <code>d</code>.</p>
</td></tr>
<tr><td><code>labels</code></td>
<td>

<p>An integer vector of length <code>N</code>, set of cluster labels for each case.
Note that trimmed cases also receive the cluster labels.
</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>

<p>A positive real vector of length <code>p</code>, containing weights on each feature.</p>
</td></tr>
<tr><td><code>WBSS</code></td>
<td>

<p>A real vector containing the weighted between sum of squares at each Step (b). The weighted between sum of squares is the objective function to maximize, excluding the prespecified proportions of cases.
The length of this vector is the number of times that the algorithm iterates the process steps (a),(a-2) and (b) before the stopping criterion is satisfied.
This is returned only if <code>L1</code> is numeric and &gt; 1.
</p>
</td></tr>
<tr><td><code>WWSS</code></td>
<td>

<p>A real number, the within cluster sum of squares at a local minimum.
This is the objective function to minimize in nonsparse methods.
For robust clustering methods, this quantity is calculated without the prespecified proportions of cases.
This is returned only if <code>L1</code>=<code>NULL</code>, 
</p>
</td></tr>
<tr><td><code>oE</code></td>
<td>

<p>Indices of the cases trimmed in squared Euclidean distances.
</p>
</td></tr>
<tr><td><code>oW</code></td>
<td>

<p>Indices of the cases trimmed in weighted squared Euclidean distances. 
If <code>L1</code> =<code>NULL</code>, then <code>oW</code> are the cases trimmed in the Euclidean distance, because all the features have the same weights, i.e., <code>1</code>'s.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yumi Kondo &lt;y.kondo@stat.ubc.ca&gt;
</p>


<h3>References</h3>

<p>Y. Kondo, M. Salibian-Barrera, R.H. Zamar. RSKC: An R Package for a Robust and Sparse K-Means Clustering Algorithm.,Journal of Statistical Software, 72(5), 1-26, 2016.
</p>
<p>A. Gordaliza. Best approximations to random variables based on trimming procedures. Journal of Approximation Theory, 64, 1991a.
</p>
<p>A. Gordaliza. On the breakdown point of multivariate location estimators based on trimming procedures. Statistics &amp; Probability Letters, 11, 1991b. 
</p>
<p>Y. Kondo (2011), Robustificaiton of the sparse K-means clustering algorithm, MSc. Thesis, University of British Columbia
<a href="http://hdl.handle.net/2429/37093">http://hdl.handle.net/2429/37093</a>
</p>
<p>D. M. Witten and R. Tibshirani. A framework for feature selection in 
clustering. Journal of the American Statistical Association, 105(490)
713-726, 2010. 
</p>
<p>S.P. Least Squares quantization in PCM. IEEE Transactions on information theory, 28(2): 129-136, 1982.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># little simulation function 
sim &lt;-
function(mu,f){
   D&lt;-matrix(rnorm(60*f),60,f)
   D[1:20,1:50]&lt;-D[1:20,1:50]+mu
   D[21:40,1:50]&lt;-D[21:40,1:50]-mu  
   return(D)
   }

set.seed(1);d0&lt;-sim(1,500)# generate a dataset
true&lt;-rep(1:3,each=20) # vector of true cluster labels
d&lt;-d0
ncl&lt;-3
for ( i in 1 : 10){
   d[sample(1:60,1),sample(1:500,1)]&lt;-rnorm(1,mean=0,sd=15)
}

# The generated dataset looks like this...
pairs(
      d[,c(1,2,3,200)],col=true, 
      labels=c("clustering feature 1",
      "clustering feature 2","clustering feature 3",
      "noise feature1"),
      main="The sampling distribution of 60 cases colored by true cluster labels", 
      lower.panel=NULL) 


# Compare the performance of four algorithms
###3-means
r0&lt;-kmeans(d,ncl,nstart=100)
CER(r0$cluster,true)

###Sparse 3-means
#This example requires sparcl package
#library(sparcl)
#r1&lt;-KMeansSparseCluster(d,ncl,wbounds=6)
# Partition result
#CER(r1$Cs,true)
# The number of nonzero weights
#sum(!r1$ws&lt;1e-3)

###Trimmed 3-means

r2&lt;-RSKC(d,ncl,alpha=10/60,L1=NULL,nstart=200)
CER(r2$labels,true)

###Robust Sparse 3-means
r3&lt;-RSKC(d,ncl,alpha=10/60,L1=6,nstart=200)
# Partition result
CER(r3$labels,true)
r3

### RSKC works with datasets containing missing values...
# add missing values to the dataset
set.seed(1)
for ( i in 1 : 100)
{   
d[sample(1:60,1),sample(1,500,1)]&lt;-NA
}
r4 &lt;- RSKC(d,ncl,alpha=10/60,L1=6,nstart=200)

</code></pre>

<hr>
<h2 id='RSKC-internal'>Internal RSKC functions</h2><span id='topic+print.rskc'></span><span id='topic+print.summary.rskc'></span><span id='topic+summary.rskc'></span>

<h3>Description</h3>

<p>Internal RSKC functions</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rskc'
print(x,...)
## S3 method for class 'rskc'
summary(object,...)
## S3 method for class 'rskc'
print.summary(x,...)
</code></pre>


<h3>Author(s)</h3>

<p>Yumi Kondo &lt;y.kondo@stat.ubc.ca&gt;</p>

<hr>
<h2 id='Sensitivity'>
Compute the sensitivities (probability of true positive) of each cluster
</h2><span id='topic+Sensitivity'></span>

<h3>Description</h3>

<p>The sensitivity or conditional probability of the correct classification of cluster <em>k</em> is calculated as follows:

First, the proportions of observations whose true cluster label is <em>k</em>
are computed for each classified clusters.

Then the largest proportion is selected as the conditional probability of the correct classification.

Since this calculation can return 1 for sensitivities of all clusters if all observations
belong to one cluster, we also report the observed cluster labels
returned by the algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Sensitivity(label1, label2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Sensitivity_+3A_label1">label1</code></td>
<td>

<p>A vector of length N, containing the cluster labels from any clustering algorithms.
</p>
</td></tr>
<tr><td><code id="Sensitivity_+3A_label2">label2</code></td>
<td>

<p>A vector of length N, containing the true cluster labels.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yumi Kondo &lt;y.kondo@stat.ubc.ca&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>vec1&lt;-c(1,1,1,2,3,3,3,2,2)
vec2&lt;-c(3,3,3,1,1,2,2,1,1)
Sensitivity(vec1,vec2)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
