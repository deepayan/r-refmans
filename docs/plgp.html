<!DOCTYPE html><html lang="en"><head><title>Help for package plgp</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {plgp}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#plgp-package'><p>Particle Learning of Gaussian Processes</p></a></li>
<li><a href='#addpall.GP'>
<p>Add data to pall</p></a></li>
<li><a href='#data.GP'>
<p>Supply GP data to PL</p></a></li>
<li><a href='#draw.GP'>
<p>Metropolis-Hastings draw for GP parameters</p></a></li>
<li><a href='#exp2d.C'>
<p>2-d Exponential Hessian Data</p></a></li>
<li><a href='#init.GP'>
<p>Initialize particles for GPs</p></a></li>
<li><a href='#lpredprob.GP'>
<p>Log-Predictive Probability Calculation for GPs</p></a></li>
<li><a href='#papply'>
<p>Extending apply to particles</p></a></li>
<li><a href='#params.GP'>
<p>Extract parameters from GP particles</p></a></li>
<li><a href='#PL'>
<p>Particle Learning Skeleton Method</p></a></li>
<li><a href='#plgp-internal'><p>Internal plgp Functions</p></a></li>
<li><a href='#pred.GP'>
<p>Prediction for GPs</p></a></li>
<li><a href='#prior.GP'>
<p>Generate priors for GP models</p></a></li>
<li><a href='#propagate.GP'>
<p>PL propagate rule for GPs</p></a></li>
<li><a href='#rectscale'>
<p>Un/Scale data in a bounding rectangle</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Particle Learning of Gaussian Processes</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1-12</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-10-19</td>
</tr>
<tr>
<td>Author:</td>
<td>Robert B. Gramacy &lt;rbg@vt.edu&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Robert B. Gramacy &lt;rbg@vt.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Sequential Monte Carlo (SMC) inference for fully Bayesian
  Gaussian process (GP) regression and classification models by
  particle learning (PL) following Gramacy &amp; Polson (2011) &lt;<a href="https://doi.org/10.48550/arXiv.0909.5262">doi:10.48550/arXiv.0909.5262</a>&gt;.
  The sequential nature of inference
  and the active learning (AL) hooks provided facilitate thrifty 
  sequential design (by entropy) and optimization
  (by improvement) for classification and
  regression models, respectively.
  This package essentially provides a generic
  PL interface, and functions (arguments to the interface) which
  implement the GP models and AL heuristics.  Functions for 
  a special, linked, regression/classification GP model and 
  an integrated expected conditional improvement (IECI) statistic 
  provide for optimization in the presence of unknown constraints.
  Separable and isotropic Gaussian, and single-index correlation
  functions are supported.
  See the examples section of ?plgp and demo(package="plgp") 
  for an index of demos.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.4), mvtnorm, tgp</td>
</tr>
<tr>
<td>Suggests:</td>
<td>ellipse, splancs, interp</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-2">LGPL-2</a> | <a href="https://www.r-project.org/Licenses/LGPL-2.1">LGPL-2.1</a> | <a href="https://www.r-project.org/Licenses/LGPL-3">LGPL-3</a> [expanded from: LGPL]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://bobby.gramacy.com/r_packages/plgp/">https://bobby.gramacy.com/r_packages/plgp/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-10-19 13:23:12 UTC; bobby</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-10-19 14:32:37 UTC</td>
</tr>
</table>
<hr>
<h2 id='plgp-package'>Particle Learning of Gaussian Processes</h2><span id='topic+plgp-package'></span>

<h3>Description</h3>

<p>Sequential Monte Carlo inference for fully Bayesian
Gaussian process (GP) regression and classification models by
particle learning (PL).  The sequential nature of inference
and the active learning (AL) hooks provided facilitate thrifty 
sequential design (by entropy) and optimization
(by improvement) for classification and
regression models, respectively.
This package essentially provides a generic
PL interface, and functions (arguments to the interface) which
implement the GP models and AL heuristics.  Functions for 
a special, linked, regression/classification GP model and 
an integrated expected conditional improvement (IECI) statistic 
is provides for optimization in the presence of unknown constraints.
Separable and isotropic Gaussian, and single-index correlation
functions are supported.
See the examples section of ?plgp and demo(package=&quot;plgp&quot;) 
for an index of demos</p>


<h3>Details</h3>

<p>For a fuller overview including a complete list of functions, and
demos, please use <code>help(package="plgp")</code>.
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>Gramacy, R. and Polson, N. (2011).
&ldquo;Particle learning of Gaussian process models for
sequential design and optimization.&rdquo;
Journal of Computational and Graphical Statistics, 20(1), 
pp. 102-118; arXiv:0909.5262
</p>
<p>Gramacy, R. and Lee, H. (2010).
&ldquo;Optimization under unknown constraints&rdquo;.
<em>Bayesian Statistics 9</em>, J. M. Bernardo, M. J. Bayarri,
J. O. Berger, A. P. Dawid, D. Heckerman, A. F. M. Smith and M. West
(Eds.); Oxford University Press
</p>
<p>Carvalho, C., Johannes, M., Lopes, H., and Polson, N. (2008).
&ldquo;Particle Learning and Smoothing&rdquo;.
Discussion Paper 2008-32, Duke University Dept. of Statistical
Science.
</p>
<p>Gramacy, R. (2020).
&ldquo;Surrogates: Gaussian Process Modeling, Design and Optimization for the Applied Sciences&rdquo;.
Chapman Hall/CRC; <a href="https://bobby.gramacy.com/surrogates/">https://bobby.gramacy.com/surrogates/</a>
</p>
<p><a href="https://bobby.gramacy.com/r_packages/plgp/">https://bobby.gramacy.com/r_packages/plgp/</a>
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+PL">PL</a></code>, <span class="pkg">tgp</span> </p>

<hr>
<h2 id='addpall.GP'>
Add data to pall
</h2><span id='topic+addpall.GP'></span><span id='topic+addpall.CGP'></span><span id='topic+addpall.ConstGP'></span>

<h3>Description</h3>

<p>Add sufficient 
data common to all particles to the global <code>pall</code>
variable, a mnemonic for &ldquo;particles-all&rdquo;, for
Gaussian process (GP)
regression, classification, or combined unknown constraint
models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addpall.GP(Z)
addpall.CGP(Z)
addpall.ConstGP(Z)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="addpall.GP_+3A_z">Z</code></td>
<td>

<p>new observation(s) (usually the next one in &ldquo;time&rdquo;) to add to
the <code>pall</code> global variable 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>All three functions add new <code>Z$x</code> to <code>pall$X</code>;
<code>addpall.GP</code> also adds <code>Z$y</code> to <code>pall$Y</code>,
<code>addpall.CGP</code> also adds <code>Z$c</code> to <code>pall$Y</code>,
and <code>addpall.ConstGP</code> does both
</p>


<h3>Value</h3>

<p>nothing is returned, but global variables are modified
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy, <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>Gramacy, R. and Polson, N. (2011).
&ldquo;Particle learning of Gaussian process models for
sequential design and optimization.&rdquo;
Journal of Computational and Graphical Statistics, 20(1), 
pp. 102-118; arXiv:0909.5262
</p>
<p>Gramacy, R. and Lee, H. (2010).
&ldquo;Optimization under unknown constraints&rdquo;.
<em>Bayesian Statistics 9</em>, J. M. Bernardo, M. J. Bayarri,
J. O. Berger, A. P. Dawid, D. Heckerman, A. F. M. Smith and M. West
(Eds.); Oxford University Press
</p>
<p>Gramacy, R. (2020).
&ldquo;Surrogates: Gaussian Process Modeling, Design and Optimization for the Applied Sciences&rdquo;.
Chapman Hall/CRC; <a href="https://bobby.gramacy.com/surrogates/">https://bobby.gramacy.com/surrogates/</a>
</p>
<p><a href="https://bobby.gramacy.com/r_packages/plgp/">https://bobby.gramacy.com/r_packages/plgp/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PL">PL</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## See the demos via demo(package="plgp") and the examples
## section of ?plgp
</code></pre>

<hr>
<h2 id='data.GP'>
Supply GP data to PL
</h2><span id='topic+data.GP'></span><span id='topic+data.GP.improv'></span><span id='topic+data.CGP'></span><span id='topic+data.CGP.adapt'></span><span id='topic+data.ConstGP'></span><span id='topic+data.ConstGP.improv'></span>

<h3>Description</h3>

<p>Functions to supply data to PL for Gaussian process (GP)
regression, classification, or combined unknown constraint
models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data.GP(begin, end = NULL, X, Y)
data.GP.improv(begin, end = NULL, f, rect, prior,
               adapt = ei.adapt, cands = 40,
               save = TRUE, oracle = TRUE, verb = 2,
               interp = interp.loess)
data.CGP(begin, end = NULL, X, C)
data.CGP.adapt(begin, end = NULL, f, rect, prior,
               cands = 40, verb = 2, interp=interp.loess)
data.ConstGP(begin, end = NULL, X, Y, C)
data.ConstGP.improv(begin, end = NULL, f, rect, prior,
                    adapt = ieci.const.adapt , cands = 40, 
                    save = TRUE, oracle = TRUE, verb = 2,
                    interp = interp.loess)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="data.GP_+3A_begin">begin</code></td>
<td>

<p>positive <code>integer</code> starting time for data to be returned
</p>
</td></tr>
<tr><td><code id="data.GP_+3A_end">end</code></td>
<td>

<p>positive <code>integer</code> (<code>end &gt;= begin</code>) ending time
for data being returned; may be <code>NULL</code> if only data
at time <code>begin</code> is needed
</p>
</td></tr>
<tr><td><code id="data.GP_+3A_x">X</code></td>
<td>

<p><code>data.frame</code> with at least <code>end</code> rows containing
covariates
</p>
</td></tr>
<tr><td><code id="data.GP_+3A_y">Y</code></td>
<td>

<p>vector of length at least <code>end</code> containing real-valued
responses
</p>
</td></tr>
<tr><td><code id="data.GP_+3A_c">C</code></td>
<td>

<p>vector of length at least <code>end</code> containing class labels
</p>
</td></tr>
<tr><td><code id="data.GP_+3A_f">f</code></td>
<td>

<p>function returning a responses when called as <code>f(X)</code>
for <code>matrix</code> <code>X</code>; for <code>data.GP.improv</code> the responses
must be real-valued returned as a vector;
for <code>data.CGP.adapt</code> they must be class
labels returned as a vector;
for <code>data.ConstGP.improv</code> they must be pairs of real-valued
and in {0,1} (1 indicates constraint violation), returned as
a 2-column <code>data.frame</code>
</p>
</td></tr>
<tr><td><code id="data.GP_+3A_rect">rect</code></td>
<td>

<p>bounding rectangle for the inputs <code>X</code> to <code>f(X)</code> with
two columns and rows equalling <code>nrow(X)</code>
</p>
</td></tr>
<tr><td><code id="data.GP_+3A_prior">prior</code></td>
<td>

<p>prior parameters passed from <code><a href="#topic+PL">PL</a></code> generated by one of
the prior functions, e.g., <code><a href="#topic+prior.GP">prior.GP</a></code>
</p>
</td></tr>
<tr><td><code id="data.GP_+3A_adapt">adapt</code></td>
<td>

<p>function that evaluates a sequential design criterion on 
some candidate locations; the default <code>ei.adapt</code> EI
about the minimum; <code>ieci.adapt</code> providing IECI is another
possibility , which is hard coded into <code>data.ConstGP.adapt</code>
</p>
</td></tr>
<tr><td><code id="data.GP_+3A_cands">cands</code></td>
<td>

<p>number of Latin Hypercube candidate locations used to choose the
next adaptively sampled input design point
</p>
</td></tr>
<tr><td><code id="data.GP_+3A_save">save</code></td>
<td>

<p>scalar <code>logical</code> indicating if the improvment information for
chosen candidate should be saved in the <code>psave</code> global variable
</p>
</td></tr>
<tr><td><code id="data.GP_+3A_oracle">oracle</code></td>
<td>

<p>scalar <code>logical</code> indicating if the candidates should be
augmented with the point found to maximize the predictive surface
(with a search starting at the most recently chosen input)
</p>
</td></tr>
<tr><td><code id="data.GP_+3A_verb">verb</code></td>
<td>

<p>verbosity level for printing the progress of improv and other
adaptive sampling calculations
</p>
</td></tr>
<tr><td><code id="data.GP_+3A_interp">interp</code></td>
<td>

<p>function for smoothing of 2-d image plots.  The default comes
from <code><a href="tgp.html#topic+interp.loess">interp.loess</a></code>, but what works best is
<code><a href="interp.html#topic+interp">interp</a></code> which requires the <span class="pkg">interp</span> or
<span class="pkg">akima</span> package
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions provide data to PL for Gaussian progress regression
and classification methods in a variety of ways.  The simplest,
<code>data.GP</code> and <code>data.CGP</code> supply pre-recorded regression and
classification data stored in data frames and vectors;
<code>data.ConstGP</code> is a hybrid that does joint regression and
classification. The other
functions provide data by active learning/sequential design:
</p>
<p>The <code>data.GP.improv</code> function uses expected improvement (EI);
<code>data.CGP.improv</code> uses predictive entropy;
<code>data.ConstGP.improv</code> 
uses integrated expected conditional improvement (IECI).  In these
cases, once the <code>x</code>-location(s) is/are chosen,
the function <code>f</code> is used to provide the response(s)
</p>


<h3>Value</h3>

<p>The output are vectors or <code>data.frame</code>s.
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy, <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>Gramacy, R. and Polson, N. (2011).
&ldquo;Particle learning of Gaussian process models for
sequential design and optimization.&rdquo;
Journal of Computational and Graphical Statistics, 20(1), 
pp. 102-118; arXiv:0909.5262
</p>
<p>Gramacy, R. and Lee, H. (2010).
&ldquo;Optimization under unknown constraints&rdquo;.
<em>Bayesian Statistics 9</em>, J. M. Bernardo, M. J. Bayarri,
J. O. Berger, A. P. Dawid, D. Heckerman, A. F. M. Smith and M. West
(Eds.); Oxford University Press
</p>
<p>Gramacy, R. (2020).
&ldquo;Surrogates: Gaussian Process Modeling, Design and Optimization for the Applied Sciences&rdquo;.
Chapman Hall/CRC; <a href="https://bobby.gramacy.com/surrogates/">https://bobby.gramacy.com/surrogates/</a>
</p>
<p><a href="https://bobby.gramacy.com/r_packages/plgp/">https://bobby.gramacy.com/r_packages/plgp/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PL">PL</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## See the demos via demo(package="plgp") and the examples
## section of ?plgp
</code></pre>

<hr>
<h2 id='draw.GP'>
Metropolis-Hastings draw for GP parameters
</h2><span id='topic+draw.GP'></span><span id='topic+draw.CGP'></span><span id='topic+draw.ConstGP'></span>

<h3>Description</h3>

<p>Functions for using Metropolis-Hastings (MH) to evolve a particle
according to the posterior distribution given by a
Gaussian process (GP) for regression, classification,
or combined unknown constraint model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>draw.GP(Zt, prior, l = 3, h = 4, thin = 10, Y = NULL)
draw.CGP(Zt, prior, l = 3, h = 4, thin = 10)
draw.ConstGP(Zt, prior, l = 3, h = 4, thin = 10)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="draw.GP_+3A_zt">Zt</code></td>
<td>

<p>the particle describing model parameters and sufficient statistics
that determines the predictive distribution
</p>
</td></tr>
<tr><td><code id="draw.GP_+3A_prior">prior</code></td>
<td>

<p>prior parameters passed from <code><a href="#topic+PL">PL</a></code> generated by one of
the prior functions, e.g., <code><a href="#topic+prior.GP">prior.GP</a></code>
</p>
</td></tr>
<tr><td><code id="draw.GP_+3A_l">l</code></td>
<td>

<p>positive uniform random walk parameter; for old parameter
<code>pold</code>, a new parameter is proposed as
<code>p = runif(1, p*l/h, p*h/l)</code>.  Such proposals are then
accepted (or rejected) via the MH acceptance ratio
</p>
</td></tr>
<tr><td><code id="draw.GP_+3A_h">h</code></td>
<td>

<p>positive uniform random walk parameter; see above
</p>
</td></tr>
<tr><td><code id="draw.GP_+3A_thin">thin</code></td>
<td>

<p>thinning level in the MCMC; describes the number of MH rounds
executed before the value is saved as a sample from the
(marginal) posterior distribution
</p>
</td></tr>
<tr><td><code id="draw.GP_+3A_y">Y</code></td>
<td>

<p>not for external use; used internally by CGP and ConstGP internal
routines
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions are used in two important places in <span class="pkg">plgp</span>.
At the user level, they can be used to initialize the particles
at time <code>start</code>; see <code><a href="#topic+PL">PL</a></code> and the demos.
Internally, they are used in the <code><a href="#topic+PL">PL</a></code> propagate
step, e.g.,  <code><a href="#topic+propagate.GP">propagate.GP</a></code>
</p>
<p><code>draw.ConstGP</code> is a combination
of the <code>draw.GP</code> and <code>draw.CGP</code> methods, which are
for regression and classification GPs, respectively
</p>


<h3>Value</h3>

<p>These functions return an updated particle <code>Zt</code>
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy, <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>Gramacy, R. and Polson, N. (2011).
&ldquo;Particle learning of Gaussian process models for
sequential design and optimization.&rdquo;
Journal of Computational and Graphical Statistics, 20(1), 
pp. 102-118; arXiv:0909.5262
</p>
<p>Gramacy, R. and Lee, H. (2010).
&ldquo;Optimization under unknown constraints&rdquo;.
<em>Bayesian Statistics 9</em>, J. M. Bernardo, M. J. Bayarri,
J. O. Berger, A. P. Dawid, D. Heckerman, A. F. M. Smith and M. West
(Eds.); Oxford University Press
</p>
<p>Gramacy, R. (2020).
&ldquo;Surrogates: Gaussian Process Modeling, Design and Optimization for the Applied Sciences&rdquo;.
Chapman Hall/CRC; <a href="https://bobby.gramacy.com/surrogates/">https://bobby.gramacy.com/surrogates/</a>
</p>
<p><a href="https://bobby.gramacy.com/r_packages/plgp/">https://bobby.gramacy.com/r_packages/plgp/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+init.GP">init.GP</a></code>, <code><a href="#topic+propagate.GP">propagate.GP</a></code>,
<code><a href="#topic+PL">PL</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## See the demos via demo(package="plgp") and the examples
## section of ?plgp
</code></pre>

<hr>
<h2 id='exp2d.C'>
2-d Exponential Hessian Data
</h2><span id='topic+exp2d.C'></span>

<h3>Description</h3>

<p>Generates 2-d classification data with two or three class labels,
based on the Hessian data from a 2-d real-valued response
</p>


<h3>Usage</h3>

<pre><code class='language-R'>exp2d.C(X, threed = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="exp2d.C_+3A_x">X</code></td>
<td>

<p>a <code>matrix</code> or <code>data.frame</code> describing the design at which
the response categories are desired
</p>
</td></tr>
<tr><td><code id="exp2d.C_+3A_threed">threed</code></td>
<td>

<p>a scalar <code>logical</code> indicating if the two or three-class version
of the class labels should be returned.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The underlying real-valued response is governed by
</p>
<p style="text-align: center;"><code class="reqn">Z(X)=x_1 * \exp(x_1^2-x_2^2).</code>
</p>

<p>Two class labels are generated by inspecting the sign of the sum of
the eigenvalues of the Hessian (Broderick &amp; Gramacy, 2010).  This
generates the first (-) and second (+) classes in a three-class
function. A third class label (the default) may
created from the first one where <code>X[,1] &gt; 0</code> (Gramacy &amp; Polson, 2011)
</p>


<h3>Value</h3>

<p>A vector of class labels of length <code>nrow(X)</code> is returned
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy, <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>Broderick, T. and Gramacy, R. (2010). &ldquo;Classification and
categorical inputs with treed Gaussian process models.&rdquo; Tech.
rep., University of Cambridge. ArXiv:0904.4891.
</p>
<p>Gramacy, R. and Polson, N. (2011).
&ldquo;Particle learning of Gaussian process models for
sequential design and optimization.&rdquo;
Journal of Computational and Graphical Statistics, 20(1), 
pp. 102-118; arXiv:0909.5262
</p>
<p>Gramacy, R. (2020).
&ldquo;Surrogates: Gaussian Process Modeling, Design and Optimization for the Applied Sciences&rdquo;.
Chapman Hall/CRC; <a href="https://bobby.gramacy.com/surrogates/">https://bobby.gramacy.com/surrogates/</a>
</p>
<p><a href="https://bobby.gramacy.com/r_packages/plgp/">https://bobby.gramacy.com/r_packages/plgp/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## The following demos use this data
## Not run: 
## Illustrates classification GPs on a simple 2-d exponential
## data generating mechanism
demo("plcgp_exp", ask=FALSE)

## Illustrates active learning via entropy with classification
## GPs on a simple 2-d exponential data generating mechanism
demo("plcgp_exp_entropy", ask=FALSE)

## End(Not run)
</code></pre>

<hr>
<h2 id='init.GP'>
Initialize particles for GPs
</h2><span id='topic+init.GP'></span><span id='topic+init.CGP'></span><span id='topic+init.ConstGP'></span>

<h3>Description</h3>

<p>Functions for initializing particles for Gaussian process (GP)
regression, classification, or combined unknown constraint
models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>init.GP(prior, d = NULL, g = NULL, Y = NULL)
init.CGP(prior, d = NULL, g = NULL)
init.ConstGP(prior)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="init.GP_+3A_prior">prior</code></td>
<td>

<p>prior parameters passed from <code><a href="#topic+PL">PL</a></code> generated by one of
the prior functions, e.g., <code><a href="#topic+prior.GP">prior.GP</a></code>
</p>
</td></tr>
<tr><td><code id="init.GP_+3A_d">d</code></td>
<td>

<p>initial range (or length-scale) parameter(s) for the GP correlation
function(s)
</p>
</td></tr>
<tr><td><code id="init.GP_+3A_g">g</code></td>
<td>

<p>initial nugget parameter for the GP correlation
</p>
</td></tr>
<tr><td><code id="init.GP_+3A_y">Y</code></td>
<td>

<p>data used to update GP sufficient information in the case of
<code>init.GP</code>; if <code>NULL</code> then <code>pall$Y</code> is used
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a particle for internal use in the <code><a href="#topic+PL">PL</a></code> method
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy, <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>Gramacy, R. and Polson, N. (2011).
&ldquo;Particle learning of Gaussian process models for
sequential design and optimization.&rdquo;
Journal of Computational and Graphical Statistics, 20(1), 
pp. 102-118; arXiv:0909.5262
</p>
<p>Gramacy, R. and Lee, H. (2010).
&ldquo;Optimization under unknown constraints&rdquo;.
<em>Bayesian Statistics 9</em>, J. M. Bernardo, M. J. Bayarri,
J. O. Berger, A. P. Dawid, D. Heckerman, A. F. M. Smith and M. West
(Eds.); Oxford University Press
</p>
<p>Gramacy, R. (2020).
&ldquo;Surrogates: Gaussian Process Modeling, Design and Optimization for the Applied Sciences&rdquo;.
Chapman Hall/CRC; <a href="https://bobby.gramacy.com/surrogates/">https://bobby.gramacy.com/surrogates/</a>
</p>
<p><a href="https://bobby.gramacy.com/r_packages/plgp/">https://bobby.gramacy.com/r_packages/plgp/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PL">PL</a></code>, <code><a href="#topic+draw.GP">draw.GP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## See the demos via demo(package="plgp") and the examples
## section of ?plgp
</code></pre>

<hr>
<h2 id='lpredprob.GP'>
Log-Predictive Probability Calculation for GPs
</h2><span id='topic+lpredprob.GP'></span><span id='topic+lpredprob.CGP'></span><span id='topic+lpredprob.ConstGP'></span>

<h3>Description</h3>

<p>Log-predictive probability calculation for Gaussian process (GP)
regression, classification, or combined unknown constraint
models; primarily to be used particle learning (PL) re-sample step
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lpredprob.GP(z, Zt, prior)
lpredprob.CGP(z, Zt, prior)
lpredprob.ConstGP(z, Zt, prior)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lpredprob.GP_+3A_z">z</code></td>
<td>

<p>new observation whose (log) predictive probability is to be
calculated given the particle <code>Zt</code>
</p>
</td></tr>
<tr><td><code id="lpredprob.GP_+3A_zt">Zt</code></td>
<td>

<p>the particle describing model parameters and sufficient statistics
that determines the predictive distribution
</p>
</td></tr>
<tr><td><code id="lpredprob.GP_+3A_prior">prior</code></td>
<td>

<p>prior parameters passed from <code><a href="#topic+PL">PL</a></code> generated by one of
the prior functions, e.g., <code><a href="#topic+prior.GP">prior.GP</a></code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is the workhorse of the <code><a href="#topic+PL">PL</a></code> re-sample step.  For
each new observation (in sequence), the
<code><a href="#topic+PL">PL</a></code> function calls <code>lpredprob</code> and these values
determine the weights used in the <code><a href="base.html#topic+sample">sample</a></code> function to
obtain the new particle set, which is then propagated, e.g., using
<code><a href="#topic+propagate.GP">propagate.GP</a></code>
</p>
<p>The <code><a href="#topic+lpredprob.ConstGP">lpredprob.ConstGP</a></code> is essentially the combination
(product) of <code><a href="#topic+lpredprob.GP">lpredprob.GP</a></code> and
<code><a href="#topic+lpredprob.CGP">lpredprob.CGP</a></code> for regression and classification GP
models, respectively
</p>


<h3>Value</h3>

<p>Returns a real-valued scalar - the log predictive probability
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy, <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>Gramacy, R. and Polson, N. (2011).
&ldquo;Particle learning of Gaussian process models for
sequential design and optimization.&rdquo;
Journal of Computational and Graphical Statistics, 20(1), 
pp. 102-118; arXiv:0909.5262
</p>
<p>Gramacy, R. and Lee, H. (2010).
&ldquo;Optimization under unknown constraints&rdquo;.
<em>Bayesian Statistics 9</em>, J. M. Bernardo, M. J. Bayarri,
J. O. Berger, A. P. Dawid, D. Heckerman, A. F. M. Smith and M. West
(Eds.); Oxford University Press
</p>
<p>Gramacy, R. (2020).
&ldquo;Surrogates: Gaussian Process Modeling, Design and Optimization for the Applied Sciences&rdquo;.
Chapman Hall/CRC; <a href="https://bobby.gramacy.com/surrogates/">https://bobby.gramacy.com/surrogates/</a>
</p>
<p><a href="https://bobby.gramacy.com/r_packages/plgp/">https://bobby.gramacy.com/r_packages/plgp/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PL">PL</a></code>, <code><a href="#topic+propagate.GP">propagate.GP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## See the demos via demo(package="plgp") and the examples
## section of ?plgp
</code></pre>

<hr>
<h2 id='papply'>
Extending apply to particles
</h2><span id='topic+papply'></span>

<h3>Description</h3>

<p>Applies a user-specified function to each particle contained in the
global variables <code>peach</code> and <code>pall</code>, collecting the
output in a <code><a href="base.html#topic+data.frame">data.frame</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>papply(fun, verb = 1, pre = "", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="papply_+3A_fun">fun</code></td>
<td>

<p>a user-defined function which which takes a particle as its first
input; the output of <code>fun</code> should be a vector, <code>matrix</code>
or <code>data.frame</code>
</p>
</td></tr>
<tr><td><code id="papply_+3A_verb">verb</code></td>
<td>

<p>a scalar <code>logical</code> indicating whether progress statements
should be printed to the screen
</p>
</td></tr>
<tr><td><code id="papply_+3A_pre">pre</code></td>
<td>

<p>an optional <code>character</code> prefix used in the progress print
statements; ignored if <code>verb = 0</code>
</p>
</td></tr>
<tr><td><code id="papply_+3A_...">...</code></td>
<td>

<p>these ellipses arguments are used to pass extra optional
arguments to the user-supplied function <code>fun</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a extension to the built-in <code><a href="base.html#topic+apply">apply</a></code> family of
function to particles, intended to be used with the particles created
by <code><a href="#topic+PL">PL</a></code>.  Perhaps the most common use of this function is
in obtaining samples form the posterior predictive distribution, i.e.,
with the user supplied <code>fun = <a href="#topic+pred.GP">pred.GP</a></code>
</p>
<p>The particles applied over must be present in the global variables
<code>pall</code>, containing sufficient information common to all
particles, <code>peach</code>, containing sufficient information
particular to each particle, as constructed by <code><a href="#topic+PL">PL</a></code>
</p>


<h3>Value</h3>

<p>Returns a data frame with the collected output of the user-specified
function <code>fun</code>
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy, <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>Carvalho, C., Johannes, M., Lopes, H., and Polson, N. (2008).
&ldquo;Particle Learning and Smoothing.&rdquo;
Discussion Paper 2008-32, Duke University Dept. of Statistical
Science.
</p>
<p><a href="https://bobby.gramacy.com/r_packages/plgp/">https://bobby.gramacy.com/r_packages/plgp/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PL">PL</a></code>, <code><a href="#topic+pred.GP">pred.GP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## See the demos via demo(package="plgp") and the examples
## section of ?plgp
</code></pre>

<hr>
<h2 id='params.GP'>
Extract parameters from GP particles
</h2><span id='topic+params.GP'></span><span id='topic+params.CGP'></span><span id='topic+params.ConstGP'></span>

<h3>Description</h3>

<p>Extract parameters from particles for Gaussian process (GP)
regression, classification, or combined unknown constraint
models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>params.GP()
params.CGP()
params.ConstGP()
</code></pre>


<h3>Details</h3>

<p>Collects the parameters from each of the particles (contained in
the global variable <code>peach</code>) into a
<code><a href="base.html#topic+data.frame">data.frame</a></code> that can be used for quick
<code><a href="base.html#topic+summary">summary</a></code> and visualization, e.g., via
<code><a href="graphics.html#topic+hist">hist</a></code>.  These functions are also called to make
<code>progress</code> visualizations in <code><a href="#topic+PL">PL</a></code>
</p>


<h3>Value</h3>

<p>returns a <code>data.frame</code> containing summaries for each
parameter in its columns
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy, <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>Gramacy, R. and Polson, N. (2011).
&ldquo;Particle learning of Gaussian process models for
sequential design and optimization.&rdquo;
Journal of Computational and Graphical Statistics, 20(1), 
pp. 102-118; arXiv:0909.5262
</p>
<p>Gramacy, R. and Lee, H. (2010).
&ldquo;Optimization under unknown constraints&rdquo;.
<em>Bayesian Statistics 9</em>, J. M. Bernardo, M. J. Bayarri,
J. O. Berger, A. P. Dawid, D. Heckerman, A. F. M. Smith and M. West
(Eds.); Oxford University Press
</p>
<p>Gramacy, R. (2020).
&ldquo;Surrogates: Gaussian Process Modeling, Design and Optimization for the Applied Sciences&rdquo;.
Chapman Hall/CRC; <a href="https://bobby.gramacy.com/surrogates/">https://bobby.gramacy.com/surrogates/</a>
</p>
<p><a href="https://bobby.gramacy.com/r_packages/plgp/">https://bobby.gramacy.com/r_packages/plgp/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PL">PL</a></code>, <code><a href="#topic+lpredprob.GP">lpredprob.GP</a></code>,
<code><a href="#topic+propagate.GP">propagate.GP</a></code>, <code><a href="#topic+init.GP">init.GP</a></code>,
<code><a href="#topic+pred.GP">pred.GP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## See the demos via demo(package="plgp") and the examples
## section of ?plgp
</code></pre>

<hr>
<h2 id='PL'>
Particle Learning Skeleton Method
</h2><span id='topic+PL'></span><span id='topic+plgp'></span><span id='topic+PL.env'></span>

<h3>Description</h3>

<p>Implements the Particle Learning sequential Monte Carlo
algorithm on the data sequence provided, using re-sample and
propagate steps
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PL(dstream, start, end, init, lpredprob, propagate, prior = NULL,
   addpall = NULL, params = NULL, save = NULL, P = 100,
   progress = 10, cont = FALSE, verb = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="PL_+3A_dstream">dstream</code></td>
<td>

<p>function generating the data stream; for examples see <code><a href="#topic+data.GP">data.GP</a></code>
</p>
</td></tr>
<tr><td><code id="PL_+3A_start">start</code></td>
<td>

<p>a scalar <code>integer</code> specifying the starting &ldquo;time&rdquo;;
the data entry/sample where PL will start
</p>
</td></tr>
<tr><td><code id="PL_+3A_end">end</code></td>
<td>

<p>a scalar <code>integer</code> specifying the ending &ldquo;time&rdquo;;
the data entry/sample where PL will stop
</p>
</td></tr>
<tr><td><code id="PL_+3A_init">init</code></td>
<td>

<p>function used to initialize the particles at the start of PL;
for examples see <code><a href="#topic+draw.GP">draw.GP</a></code>
</p>
</td></tr>
<tr><td><code id="PL_+3A_lpredprob">lpredprob</code></td>
<td>

<p>function used to calculate the predictive probability of an
observation (usually the next one in &ldquo;time&rdquo;) given a
particle.  This is the primary function used in the PL re-sample
step; for examples see <code><a href="#topic+lpredprob.GP">lpredprob.GP</a></code>
</p>
</td></tr>
<tr><td><code id="PL_+3A_propagate">propagate</code></td>
<td>

<p>function used to propagate particles given an observation (usually
the next one in &ldquo;time&rdquo;); for examples see
<code><a href="#topic+propagate.GP">propagate.GP</a></code>
</p>
</td></tr>
<tr><td><code id="PL_+3A_prior">prior</code></td>
<td>

<p>function used to generate prior parameters that may be
passed into the <code>dstream</code>, <code>init</code>,
<code>lpredprob</code> and <code>propagate</code>
functions as needed; for examples see <code><a href="#topic+prior.GP">prior.GP</a></code>
</p>
</td></tr>
<tr><td><code id="PL_+3A_addpall">addpall</code></td>
<td>

<p>an optional function that adds the new observation (usually
the next one in &ldquo;time&rdquo;) to the  <code>pall</code> variable
in the <code>PL.env</code> environment (i.e., <code>PL.env$pall</code>),
which stores the sufficient information shared by all particles;
for examples see <code><a href="#topic+addpall.GP">addpall.GP</a></code>
</p>
</td></tr>
<tr><td><code id="PL_+3A_params">params</code></td>
<td>

<p>an optional function called each <code>progress</code> rounds
that collects parameters from the particles for
summary and visualization; for examples see <code>params.GP</code>
</p>
</td></tr>
<tr><td><code id="PL_+3A_save">save</code></td>
<td>

<p>an option function that is called every round to save some
information about the particles
</p>
</td></tr>
<tr><td><code id="PL_+3A_p">P</code></td>
<td>

<p>number of particles to use
</p>
</td></tr>
<tr><td><code id="PL_+3A_progress">progress</code></td>
<td>

<p>number of PL rounds after which to collect <code>params</code> and
draws histograms; a non-positive value or <code>params = NULL</code>
skips the progress meter
</p>
</td></tr>
<tr><td><code id="PL_+3A_cont">cont</code></td>
<td>

<p>if <code>TRUE</code> then PL will try to use the existing set of particles
to &ldquo;continue&rdquo; where it left off; <code>start</code> and <code>end</code>
should be specified appropriately when continuing
</p>
</td></tr>
<tr><td><code id="PL_+3A_verb">verb</code></td>
<td>

<p>if nonzero, then screen prints will indicate the proportion of PL
updates finished so far; <code>verb = 1</code> will cause PL to pause on
<code>progress</code> drawings for inspection
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses the PL SMC algorithm via the functions provided.  This function
is just a skeleton framework.  The hard work is in specifying the
arguments/functions which execute the calculations needed in the
re-sample and propagate steps.
</p>
<p>PL and uses the variables stored in the <code>PL.env</code> environment:
<code>pall</code>, containing
sufficient information common to all particles, <code>peach</code>,
containing sufficient information particular to each of the <code>P</code>
particles, and <code>psave</code> containing any saved information.  
These variables may be accessed as <code>PL.env$psave</code>, for example.
</p>
<p>Note that PL is designed to be fast for sequential updating
(of GPs) when new data arrive.  This facilitates efficient sequential
design of experiments by active learning techniques, e.g.,
optimization by expected improvement and sequential exploration of
classification label boundaries by the predictive entropy.  PL is not
optimized for static inference when all of the data arrive at once,
in batch
</p>


<h3>Value</h3>

<p>PL modifies the <code>PL.env$peach</code> variable, containing sufficient
information particular to each (of the <code>P</code>) particles
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy, <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>Carvalho, C., Johannes, M., Lopes, H., and Polson, N. (2008).
&ldquo;Particle Learning and Smoothing.&rdquo;
Discussion Paper 2008-32, Duke University Dept. of Statistical
Science.
</p>
<p>Gramacy, R. and Polson, N. (2011).
&ldquo;Particle learning of Gaussian process models for
sequential design and optimization.&rdquo;
Journal of Computational and Graphical Statistics, 20(1), 
pp. 102-118; arXiv:0909.5262
</p>
<p>Gramacy, R. and Lee, H. (2010).
&ldquo;Optimization under unknown constraints&rdquo;.
<em>Bayesian Statistics 9</em>, J. M. Bernardo, M. J. Bayarri,
J. O. Berger, A. P. Dawid, D. Heckerman, A. F. M. Smith and M. West
(Eds.); Oxford University Press
</p>
<p>Gramacy, R. (2020).
&ldquo;Surrogates: Gaussian Process Modeling, Design and Optimization for the Applied Sciences&rdquo;.
Chapman Hall/CRC; <a href="https://bobby.gramacy.com/surrogates/">https://bobby.gramacy.com/surrogates/</a>
</p>
<p><a href="https://bobby.gramacy.com/r_packages/plgp/">https://bobby.gramacy.com/r_packages/plgp/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+papply">papply</a></code>, <code><a href="#topic+draw.GP">draw.GP</a></code>,
<code><a href="#topic+data.GP">data.GP</a></code>, <code><a href="#topic+lpredprob.GP">lpredprob.GP</a></code>,
<code><a href="#topic+propagate.GP">propagate.GP</a></code>, <code><a href="#topic+params.GP">params.GP</a></code>,
<code><a href="#topic+pred.GP">pred.GP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## See the demos via demo(package="plgp"); it is important to
## run them with the ask=FALSE argument so that the
## automatically generated plots may refresh automatically
## (without requiring the user to press RETURN)
## Not run: 
## Illustrates regression GPs on a simple 1-d sinusoidal
## data generating mechanism
demo("plgp_sin1d", ask=FALSE)

## Illustrates classification GPs on a simple 2-d exponential
## data generating mechanism
demo("plcgp_exp", ask=FALSE)

## Illustrates classification GPs on Ripley's Cushings data
demo("plcgp_cush", ask=FALSE)

## Illustrates active learning via the expected improvement
## statistic on a simple 1-d data generating mechanism
demo("plgp_exp_ei", ask=FALSE)

## Illustrates active learning via entropy with classification
## GPs on a simple 2-d exponential data generating mechanism
demo("plcgp_exp_entropy", ask=FALSE)

## Illustrates active learning via the integrated expected
## conditional improvement statistic for optimization
## under known constraints on a simple 1-d data generating
## mechanism
demo("plgp_1d_ieci", ask=FALSE)

## Illustrates active learning via the integrated expected
## conditional improvement statistic for optimization under
## unknown constraints on a simple 1-d data generating
## mechanism
demo("plconstgp_1d_ieci", ask=FALSE)

## Illustrates active learning via the integrated expected
## conditional improvement statistic for optimization under
## unknokn constraints on a simple 2-d data generating
## mechanism
demo("plconstgp_2d_ieci", ask=FALSE)

## End(Not run)
</code></pre>

<hr>
<h2 id='plgp-internal'>Internal plgp Functions</h2><span id='topic+calc.ecis'></span><span id='topic+calc.eis'></span><span id='topic+calc.ents'></span><span id='topic+calc.ieci'></span><span id='topic+calc.iecis'></span><span id='topic+calc.alcs'></span><span id='topic+calc.vars'></span><span id='topic+calc.ktKik.x'></span><span id='topic+calc2.ktKik.x'></span><span id='topic+covar'></span><span id='topic+covar.sep'></span><span id='topic+covar.sim'></span><span id='topic+cv.folds'></span><span id='topic+dist2covar.symm'></span><span id='topic+distance'></span><span id='topic+EI'></span><span id='topic+ei.adapt'></span><span id='topic+entropy'></span><span id='topic+entropy.adapt'></span><span id='topic+entropy.bvsb'></span><span id='topic+findmin.ConstGP'></span><span id='topic+findmin.GP'></span><span id='topic+hist.particle.params'></span><span id='topic+ieci.adapt'></span><span id='topic+alc.adapt'></span><span id='topic+mindist.adapt'></span><span id='topic+var.adapt'></span><span id='topic+ieci.const.adapt'></span><span id='topic+alc.const.adapt'></span><span id='topic+ieci.ConstGP'></span><span id='topic+alc.ConstGP'></span><span id='topic+ieci.GP'></span><span id='topic+alc.GP'></span><span id='topic+lpost.GP'></span><span id='topic+PL.clear'></span><span id='topic+pred.mean.GP'></span><span id='topic+resample'></span><span id='topic+tquants'></span><span id='topic+unif.propose.pos'></span><span id='topic+mvnorm.propose.rw'></span><span id='topic+updat.GP'></span><span id='topic+util.GP'></span><span id='topic+renorm.weights'></span><span id='topic+renorm.lweights'></span><span id='topic+latents.CGP'></span><span id='topic+phist'></span><span id='topic+getmap.CGP'></span><span id='topic+getmap.GP'></span>

<h3>Description</h3>

<p>Internal <span class="pkg">plgp</span> functions
</p>


<h3>Details</h3>

<p>These are not to be called by the user (or in some cases are just
waiting for proper documentation to be written :))
</p>

<hr>
<h2 id='pred.GP'>
Prediction for GPs
</h2><span id='topic+pred.GP'></span><span id='topic+pred.CGP'></span><span id='topic+pred.ConstGP'></span>

<h3>Description</h3>

<p>Prediction on a per-particle basis for Gaussian process (GP)
regression, classification, or combined unknown constraint
models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pred.GP(XX, Zt, prior, Y = NULL, quants = FALSE, Sigma = FALSE,
        sub = 1:Zt$t)
pred.CGP(XX, Zt, prior, mcreps = 100, cs = NULL)
pred.ConstGP(XX, Zt, prior, quants = TRUE) 
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pred.GP_+3A_xx">XX</code></td>
<td>

<p><code>matrix</code> or <code>data.frame</code> containing (a design of)
predictive locations where <code>ncol(XX) = ncol(X)</code>, on which the
data were trained and particle <code>Zt</code> thus obtained
</p>
</td></tr>
<tr><td><code id="pred.GP_+3A_zt">Zt</code></td>
<td>

<p>the particle describing model parameters and sufficient statistics
that determines the predictive distribution
</p>
</td></tr>
<tr><td><code id="pred.GP_+3A_prior">prior</code></td>
<td>

<p>prior parameters passed from <code><a href="#topic+PL">PL</a></code> generated by one of
the prior functions, e.g., <code><a href="#topic+prior.GP">prior.GP</a></code>
</p>
</td></tr>
<tr><td><code id="pred.GP_+3A_y">Y</code></td>
<td>

<p>not for external use; used internally by CGP and ConstGP internal
routines
</p>
</td></tr>
<tr><td><code id="pred.GP_+3A_quants">quants</code></td>
<td>

<p>a scalar <code>logical</code> indicating
if predictive quantiles should be
are desired
</p>
</td></tr>
<tr><td><code id="pred.GP_+3A_sigma">Sigma</code></td>
<td>

<p>a scalar <code>logical</code> indicating if the full predictive
variance-covariance matrix is desired; typically only used internally
by CGP and ConstGP
</p>
</td></tr>
<tr><td><code id="pred.GP_+3A_sub">sub</code></td>
<td>

<p>not for external used; used internally by CGP and ConstGP internal
routines
</p>
</td></tr>
<tr><td><code id="pred.GP_+3A_mcreps">mcreps</code></td>
<td>

<p>number of Monte Carlo iterations used in CGP prediction, integrating
over the latent real-valued <code>Y</code> variables at the <code>XX</code>
locations
</p>
</td></tr>
<tr><td><code id="pred.GP_+3A_cs">cs</code></td>
<td>

<p>indicates a class label at which the predictive probability is
desired; the entire probability distribution over all class labels
will be provided if not specified
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>pred.GP</code> the predictive mean (and quantiles if <code>quants
    = TRUE</code> is provided.  For <code>pred.CGP</code> the predictive
distribution over the class labels is provided, unless only one
class (<code>cs</code>) is desired.  <code>pred.ConstGP</code> is a combination
of the <code>pred.GP</code> and <code>pred.CGP</code> methods
</p>
<p>It is suggested that this function is used in as an argument to
<code><a href="#topic+papply">papply</a></code> to obtain many predictions - one for each
particle in a cloud - which are combined into a
<code><a href="base.html#topic+data.frame">data.frame</a></code>
</p>
<p>Some of the function arguments aren't meant to
be specified by the user, but are rather there to facilitate usage as a
subroutine inside other <code><a href="#topic+PL">PL</a></code> functions, such as
<code><a href="#topic+lpredprob.GP">lpredprob.GP</a></code> and others
</p>


<h3>Value</h3>

<p>A single-row <code><a href="base.html#topic+data.frame">data.frame</a></code> is returned with the desired
predictive; these rows are automatically combined when used with
<code><a href="#topic+papply">papply</a></code>
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy, <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>Gramacy, R. and Polson, N. (2011).
&ldquo;Particle learning of Gaussian process models for
sequential design and optimization.&rdquo;
Journal of Computational and Graphical Statistics, 20(1), 
pp. 102-118; arXiv:0909.5262
</p>
<p>Gramacy, R. and Lee, H. (2010).
&ldquo;Optimization under unknown constraints&rdquo;.
<em>Bayesian Statistics 9</em>, J. M. Bernardo, M. J. Bayarri,
J. O. Berger, A. P. Dawid, D. Heckerman, A. F. M. Smith and M. West
(Eds.); Oxford University Press
</p>
<p>Gramacy, R. (2020).
&ldquo;Surrogates: Gaussian Process Modeling, Design and Optimization for the Applied Sciences&rdquo;.
Chapman Hall/CRC; <a href="https://bobby.gramacy.com/surrogates/">https://bobby.gramacy.com/surrogates/</a>
</p>
<p><a href="https://bobby.gramacy.com/r_packages/plgp/">https://bobby.gramacy.com/r_packages/plgp/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+papply">papply</a></code>, <code><a href="#topic+PL">PL</a></code>, <code><a href="#topic+lpredprob.GP">lpredprob.GP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## See the demos via demo(package="plgp") and the examples
## section of ?plgp
</code></pre>

<hr>
<h2 id='prior.GP'>
Generate priors for GP models
</h2><span id='topic+prior.GP'></span><span id='topic+prior.CGP'></span><span id='topic+prior.ConstGP'></span>

<h3>Description</h3>

<p>Generate priors for Gaussian process (GP)
regression, classification, or combined unknown constraint
models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prior.GP(m, cov = c("isotropic", "separable", "sim"))
prior.CGP(m, cov = c("isotropic", "separable", "sim"))
prior.ConstGP(m, cov.GP = c("isotropic", "separable", "sim"),
              cov.CGP = cov.GP)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prior.GP_+3A_m">m</code></td>
<td>

<p>positive scalar integer specifying the dimensionality of the input
space
</p>
</td></tr>
<tr><td><code id="prior.GP_+3A_cov">cov</code></td>
<td>

<p>whether to use an <code>"isotropic"</code> or <code>"separable"</code> power
exponential correlation function with power 2 &ndash; nugget included;
a single index model (<code>"sim"</code>) capability is provided as &ldquo;beta&rdquo;
functionality; applies to both regression and classification GPs
</p>
</td></tr>
<tr><td><code id="prior.GP_+3A_cov.gp">cov.GP</code></td>
<td>

<p>specifies the covariance for the real-valued response in the
combined unknown constraint GP model
</p>
</td></tr>
<tr><td><code id="prior.GP_+3A_cov.cgp">cov.CGP</code></td>
<td>

<p>specifies the covariance for the categorical response in the
combined unknown constraint GP model
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These function generate a default prior object in the correct format
for use with the other PL routines, e.g.,
<code><a href="#topic+init.GP">init.GP</a></code> and <code><a href="#topic+pred.GP">pred.GP</a></code>.  The object returned
may be modified as necessary.
</p>
<p>The <code><a href="#topic+prior.ConstGP">prior.ConstGP</a></code> is essentially the combination
of <code><a href="#topic+prior.GP">prior.GP</a></code> and <code><a href="#topic+prior.CGP">prior.CGP</a></code>
for regression and classification GP models, respectively
</p>


<h3>Value</h3>

<p>a valid prior object for the appropriate GP model;
</p>
<p>By making the output <code>$drate</code> and/or <code>$grate</code>
values negative causes the corresponding lengthscale <code>d</code>
parameter(s) and nugget <code>d</code> parameter to be fixed at the
reciprocal of their absolute values, respectively.  This effectively
turns off inference for these values, and allows one to study the GP
predictive distribution as a function of fixed values.  When both
are fixed it is sensible to use only one particle (<code>P=1</code>, as an
argument to <code><a href="#topic+PL">PL</a></code>) 
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy, <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>Gramacy, R. and Polson, N. (2011).
&ldquo;Particle learning of Gaussian process models for
sequential design and optimization.&rdquo;
Journal of Computational and Graphical Statistics, 20(1), 
pp. 102-118; arXiv:0909.5262
</p>
<p>Gramacy, R. and Lee, H. (2010).
&ldquo;Optimization under unknown constraints&rdquo;.
<em>Bayesian Statistics 9</em>, J. M. Bernardo, M. J. Bayarri,
J. O. Berger, A. P. Dawid, D. Heckerman, A. F. M. Smith and M. West
(Eds.); Oxford University Press
</p>
<p>Gramacy, R. (2020).
&ldquo;Surrogates: Gaussian Process Modeling, Design and Optimization for the Applied Sciences&rdquo;.
Chapman Hall/CRC; <a href="https://bobby.gramacy.com/surrogates/">https://bobby.gramacy.com/surrogates/</a>
</p>
<p><a href="https://bobby.gramacy.com/r_packages/plgp/">https://bobby.gramacy.com/r_packages/plgp/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PL">PL</a></code>, <code><a href="#topic+lpredprob.GP">lpredprob.GP</a></code>,
<code><a href="#topic+propagate.GP">propagate.GP</a></code>, <code><a href="#topic+init.GP">init.GP</a></code>,
<code><a href="#topic+pred.GP">pred.GP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## See the demos via demo(package="plgp") and the examples
## section of ?plgp
</code></pre>

<hr>
<h2 id='propagate.GP'>
PL propagate rule for GPs
</h2><span id='topic+propagate.GP'></span><span id='topic+propagate.CGP'></span><span id='topic+propagate.ConstGP'></span>

<h3>Description</h3>

<p>Incorporation of a new data point for Gaussian process (GP)
regression, classification, or combined unknown constraint
models; primarily to be used particle learning (PL)
propagate step
</p>


<h3>Usage</h3>

<pre><code class='language-R'>propagate.GP(z, Zt, prior)
propagate.CGP(z, Zt, prior)
propagate.ConstGP(z, Zt, prior)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="propagate.GP_+3A_z">z</code></td>
<td>

<p>new observation whose to be incorporate into the
particle <code>Zt</code>
</p>
</td></tr>
<tr><td><code id="propagate.GP_+3A_zt">Zt</code></td>
<td>

<p>the particle describing model parameters and sufficient statistics
that the new data is being incorporated into
</p>
</td></tr>
<tr><td><code id="propagate.GP_+3A_prior">prior</code></td>
<td>

<p>prior parameters passed from <code><a href="#topic+PL">PL</a></code> generated by one of
the prior functions, e.g., <code><a href="#topic+prior.GP">prior.GP</a></code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is the workhorse of the <code><a href="#topic+PL">PL</a></code> propagate step.
After re-sampling the particles, <code><a href="#topic+PL">PL</a></code> calls
<code>propagate</code> on each of the particles to obtain the set used in
the next round/time-step
</p>
<p>The <code><a href="#topic+propagate.ConstGP">propagate.ConstGP</a></code> is essentially the combination
of <code><a href="#topic+propagate.GP">propagate.GP</a></code> and <code><a href="#topic+propagate.CGP">propagate.CGP</a></code>
for regression and classification GP models, respectively
</p>


<h3>Value</h3>

<p>These functions return a new particle with the new observation
incorporated
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy, <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p>Gramacy, R. and Polson, N. (2011).
&ldquo;Particle learning of Gaussian process models for
sequential design and optimization.&rdquo;
Journal of Computational and Graphical Statistics, 20(1), 
pp. 102-118; arXiv:0909.5262
</p>
<p>Gramacy, R. and Lee, H. (2010).
&ldquo;Optimization under unknown constraints&rdquo;.
<em>Bayesian Statistics 9</em>, J. M. Bernardo, M. J. Bayarri,
J. O. Berger, A. P. Dawid, D. Heckerman, A. F. M. Smith and M. West
(Eds.); Oxford University Press
</p>
<p>Gramacy, R. (2020).
&ldquo;Surrogates: Gaussian Process Modeling, Design and Optimization for the Applied Sciences&rdquo;.
Chapman Hall/CRC; <a href="https://bobby.gramacy.com/surrogates/">https://bobby.gramacy.com/surrogates/</a>
</p>
<p><a href="https://bobby.gramacy.com/r_packages/plgp/">https://bobby.gramacy.com/r_packages/plgp/</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PL">PL</a></code>, <code><a href="#topic+lpredprob.GP">lpredprob.GP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## See the demos via demo(package="plgp") and the examples
## section of ?plgp
</code></pre>

<hr>
<h2 id='rectscale'>
Un/Scale data in a bounding rectangle
</h2><span id='topic+rectscale'></span><span id='topic+rectunscale'></span>

<h3>Description</h3>

<p>Scale data lying in an arbitrary rectangle to lie in
the unit rectangle, and back again
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rectscale(X, rect)
rectunscale(X, rect)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rectscale_+3A_x">X</code></td>
<td>

<p>a <code>matrix</code> or <code>data.frame</code> of real-valued covariates
</p>
</td></tr>
<tr><td><code id="rectscale_+3A_rect">rect</code></td>
<td>

<p>a <code>matrix</code> describing a bounding rectangle for <code>X</code>
with 2 columns and <code>ncol(X)</code> rows
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>matrix</code> or <code>data.frame</code> with the same dimensions as
<code>X</code> scaled or un-scaled as appropriate
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy, <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>
</p>


<h3>References</h3>

<p><a href="https://bobby.gramacy.com/r_packages/plgp/">https://bobby.gramacy.com/r_packages/plgp/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X &lt;- matrix(runif(10, 1, 3), ncol=2)
rect &lt;- rbind(c(1,3), c(1,3))
Xs &lt;- rectscale(X, rect)
rectunscale(Xs, rect)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
