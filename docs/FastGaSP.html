<!DOCTYPE html><html lang="en"><head><title>Help for package FastGaSP</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {FastGaSP}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#FastGaSP-package'>
<p>Fast and Exact Computation of Gaussian Stochastic Process</p></a></li>
<li><a href='#A_t_times_x_particle'><p>Transpose matrix-vector multiplication for particle systems</p></a></li>
<li><a href='#A_times_x_particle'><p>Matrix-vector multiplication for particle systems</p></a></li>
<li><a href='#Construct_G_exp'>
<p>The coefficient matrix in the dynamic linear model when kernel is the exponential covariance</p></a></li>
<li><a href='#Construct_G_matern_5_2'>
<p>The coefficient matrix in the dynamic linear model when kernel is the Matern covariance with roughness parameter 2.5.</p></a></li>
<li><a href='#Construct_W_exp'>
<p>The conditional covariance matrix of the state in the dynamic linear model when kernel is the exponential covariance</p></a></li>
<li><a href='#Construct_W_matern_5_2'>
<p>The conditional covariance matrix for  matern covariance with roughness parameter 2.5</p></a></li>
<li><a href='#Construct_W0_exp'>
<p>covariance of the stationary distribution of the state when kernel is the exponential covariance.</p></a></li>
<li><a href='#Construct_W0_matern_5_2'>
<p>covariance of the stationary distribution of the state when kernel is the Matern covariance with roughness parameter 2.5.</p></a></li>
<li><a href='#extract_time_window'><p>Extract time window from particle data</p></a></li>
<li><a href='#f_Vicsek_variation'><p>Modified Vicsek Interaction Function</p></a></li>
<li><a href='#fgasp'><p> Setting up the Fast GaSP model</p></a></li>
<li><a href='#fgasp-class'><p>Fast GaSP class</p></a></li>
<li><a href='#fit'><p>Fit Particle Interaction Models</p></a></li>
<li><a href='#fit.fmou'>
<p>The fast EM algorithm of multivariate Ornstein-Uhlenbeck processes</p></a></li>
<li><a href='#fit.gppca'>
<p>Parameter estimation for generalized probabilistic principal component analysis of correlated data.</p></a></li>
<li><a href='#fit.particle.data'><p>Fit method for particle data</p></a></li>
<li><a href='#fmou'>
<p>Setting up the FMOU model</p></a></li>
<li><a href='#fmou-class'><p> FMOU class</p></a></li>
<li><a href='#Get_C_R_K_Q'>
<p>matrices and vectors for the inverse covariance in the predictive distribution</p></a></li>
<li><a href='#get_consecutive_data'><p>Extract consecutive time steps data from particle trajectories</p></a></li>
<li><a href='#Get_L_inv_y'>
<p>The multiplication of the inverse of L with y</p></a></li>
<li><a href='#Get_L_t_inv_y'>
<p>The multiplication of the inverse of the transpose of L with y</p></a></li>
<li><a href='#Get_L_t_y'>
<p>The multiplication of the transpose of L with y</p></a></li>
<li><a href='#Get_L_y'>
<p>The multiplication of L with y</p></a></li>
<li><a href='#Get_log_det_S2'>
<p>the natural logarithm of the determinant of the correlation matrix and the estimated sum of squares in the exponent of the profile likelihood</p></a></li>
<li><a href='#Get_Q_K'>
<p>one-step-ahead predictive variance and Kalman gain</p></a></li>
<li><a href='#Get_R_y'>
<p>The multiplication of R with y</p></a></li>
<li><a href='#gppca'><p>Setting up the GPPCA model</p></a></li>
<li><a href='#gppca-class'><p>GPPCA class</p></a></li>
<li><a href='#IKF_CG_particle'><p>IKF-CG algorithm for one-interaction physical model with 1D output</p></a></li>
<li><a href='#IKF_CG_particle_cell'><p>Inverse Kalman Filter with Conjugate Gradient for Particle Systems</p></a></li>
<li><a href='#Kalman_smoother'>
<p>the predictive mean and predictive variance by Kalman Smoother</p></a></li>
<li><a href='#log_lik'>
<p>Natural logarithm of profile likelihood by the fast computing algorithm</p></a></li>
<li><a href='#particle.data-class'><p>Particle trajectory data class</p></a></li>
<li><a href='#particle.est-class'><p>Particle interaction estimation class</p></a></li>
<li><a href='#predict'>
<p>Prediction and uncertainty quantification on the testing input using a GaSP model.</p></a></li>
<li><a href='#predict.fmou'>
<p>Prediction and uncertainty quantification on the future observations using a FMOU model.</p></a></li>
<li><a href='#predict.gppca'>
<p>Prediction and uncertainty quantification on the future observations using GPPCA.</p></a></li>
<li><a href='#predictobj.fgasp-class'><p>Predictive results for the Fast GaSP class</p></a></li>
<li><a href='#Sample_KF'>
<p>Sample the prior process using a dynamic linear model</p></a></li>
<li><a href='#Sample_KF_post'>
<p>Sample the posterior distribution of the process using the backward smoothing algorithm</p></a></li>
<li><a href='#show.fgasp'>
<p>Show an <code>fgasp</code> object.</p></a></li>
<li><a href='#show.particle.data'><p>Show method for particle data class</p></a></li>
<li><a href='#show.particle.est'><p>Show method for particle estimation class</p></a></li>
<li><a href='#simulate_particle'><p>Simulate particle trajectories</p></a></li>
<li><a href='#trajectory_data'><p>Convert experimental particle tracking data to particle.data object</p></a></li>
<li><a href='#Vicsek'><p>Vicsek Model Simulation</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Fast and Exact Computation of Gaussian Stochastic Process</td>
</tr>
<tr>
<td>Version:</td>
<td>0.6.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2025-01-22</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;</td>
</tr>
<tr>
<td>Author:</td>
<td>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements fast and exact computation of Gaussian stochastic process with the Matern kernel using forward filtering and backward smoothing algorithm. It includes efficient implementations of the inverse Kalman filter, with applications such as estimating particle interaction functions. These tools support models with or without noise. Additionally, the package offers algorithms for fast parameter estimation in latent factor models, where the factor loading matrix is orthogonal, and latent processes are modeled by Gaussian processes.  See the references: 1) Mengyang Gu and Yanxun Xu (2020), Journal of Computational and Graphical Statistics; 2) Xinyi Fang and Mengyang Gu (2024), &lt;<a href="https://doi.org/10.48550%2FarXiv.2407.10089">doi:10.48550/arXiv.2407.10089</a>&gt;; 3) Mengyang Gu and Weining Shen (2020), Journal of Machine Learning Research; 4) Yizi Lin, Xubo Liu, Paul Segall and Mengyang Gu (2025), &lt;<a href="https://doi.org/10.48550%2FarXiv.2501.01324">doi:10.48550/arXiv.2501.01324</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Depends:</td>
<td>methods</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp,rstiefel</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppEigen</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-02-12 05:12:30 UTC; mengyanggu</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-02-12 12:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='FastGaSP-package'>
Fast and Exact Computation of Gaussian Stochastic Process
</h2><span id='topic+FastGaSP-package'></span><span id='topic+FastGaSP'></span>

<h3>Description</h3>

<p>Implements fast and exact computation of Gaussian stochastic process with the Matern kernel using forward filtering and backward smoothing algorithm. It includes efficient implementations of the inverse Kalman filter, with applications such as estimating particle interaction functions. These tools support models with or without noise. Additionally, the package offers algorithms for fast parameter estimation in latent factor models, where the factor loading matrix is orthogonal, and latent processes are modeled by Gaussian processes.  See the references: 1) Mengyang Gu and Yanxun Xu (2020), Journal of Computational and Graphical Statistics; 2) Xinyi Fang and Mengyang Gu (2024), &lt;doi:10.48550/arXiv.2407.10089&gt;; 3) Mengyang Gu and Weining Shen (2020), Journal of Machine Learning Research; 4) Yizi Lin, Xubo Liu, Paul Segall and Mengyang Gu (2025), &lt;doi:10.48550/arXiv.2501.01324&gt;.
</p>


<h3>Details</h3>

<p>The DESCRIPTION file:
</p>

<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> FastGaSP</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Title: </td><td style="text-align: left;"> Fast and Exact Computation of Gaussian Stochastic Process</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 0.6.0</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2025-01-22</td>
</tr>
<tr>
 <td style="text-align: left;">
Authors@R: </td><td style="text-align: left;"> c(person(given="Mengyang", family="Gu", role=c("aut", "cre"),   email="mengyang@pstat.ucsb.edu"),
  person(given="Xinyi", family="Fang", role=c("aut"), 
  email="xinyifang@ucsb.edu"),
  person(given="Yizi", family="Lin", role=c("aut"), 
  email="lin768@ucsb.edu"))</td>
</tr>
<tr>
 <td style="text-align: left;">
Maintainer: </td><td style="text-align: left;"> Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;</td>
</tr>
<tr>
 <td style="text-align: left;">
Author: </td><td style="text-align: left;"> Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]</td>
</tr>
<tr>
 <td style="text-align: left;">
Description: </td><td style="text-align: left;"> Implements fast and exact computation of Gaussian stochastic process with the Matern kernel using forward filtering and backward smoothing algorithm. It includes efficient implementations of the inverse Kalman filter, with applications such as estimating particle interaction functions. These tools support models with or without noise. Additionally, the package offers algorithms for fast parameter estimation in latent factor models, where the factor loading matrix is orthogonal, and latent processes are modeled by Gaussian processes.  See the references: 1) Mengyang Gu and Yanxun Xu (2020), Journal of Computational and Graphical Statistics; 2) Xinyi Fang and Mengyang Gu (2024), &lt;doi:10.48550/arXiv.2407.10089&gt;; 3) Mengyang Gu and Weining Shen (2020), Journal of Machine Learning Research; 4) Yizi Lin, Xubo Liu, Paul Segall and Mengyang Gu (2025), &lt;doi:10.48550/arXiv.2501.01324&gt;.</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;= 2)</td>
</tr>
<tr>
 <td style="text-align: left;">
Depends: </td><td style="text-align: left;"> methods</td>
</tr>
<tr>
 <td style="text-align: left;">
Imports: </td><td style="text-align: left;"> Rcpp,rstiefel</td>
</tr>
<tr>
 <td style="text-align: left;">
LinkingTo: </td><td style="text-align: left;"> Rcpp, RcppEigen</td>
</tr>
<tr>
 <td style="text-align: left;">
NeedsCompilation: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;">
Repository: </td><td style="text-align: left;"> CRAN</td>
</tr>
<tr>
 <td style="text-align: left;">
Packaged: </td><td style="text-align: left;"> 2025-02-11 23:15:44 UTC; xinyifang</td>
</tr>
<tr>
 <td style="text-align: left;">
RoxygenNote: </td><td style="text-align: left;"> 7.3.2</td>
</tr>
<tr>
 <td style="text-align: left;">
Date/Publication: </td><td style="text-align: left;"> 2024-04-25 23:20:08 UTC</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>


<p>Index of help topics:
</p>
<pre>
FastGaSP-package        Fast and Exact Computation of Gaussian
                        Stochastic Process
extract_time_window     Extract time window from particle data
fgasp                   Setting up the Fast GaSP model
fgasp-class             Fast GaSP class
fit                     Fit Particle Interaction Models
fit.fmou                The fast EM algorithm of multivariate
                        Ornstein-Uhlenbeck processes
fit.gppca               Parameter estimation for generalized
                        probabilistic principal component analysis of
                        correlated data.
fit.particle.data       Fit method for particle data
fmou                    Setting up the FMOU model
fmou-class              FMOU class
gppca                   Setting up the GPPCA model
gppca-class             GPPCA class
log_lik                 Natural logarithm of profile likelihood by the
                        fast computing algorithm
particle.data-class     Particle trajectory data class
particle.est-class      Particle interaction estimation class
predict                 Prediction and uncertainty quantification on
                        the testing input using a GaSP model.
predict.fmou            Prediction and uncertainty quantification on
                        the future observations using a FMOU model.
predict.gppca           Prediction and uncertainty quantification on
                        the future observations using GPPCA.
predictobj.fgasp-class
                        Predictive results for the Fast GaSP class
show,fgasp-method       Show an 'fgasp' object.
show,particle.est-method
                        Show method for particle estimation class
show.particle.data      Show method for particle data class
simulate_particle       Simulate particle trajectories
trajectory_data         Convert experimental particle tracking data to
                        particle.data object
</pre>
<p>Fast computational algorithms for Gaussian stochastic process with Matern kernels 
by the forward filtering and backward smoothing algorithm.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>,  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu and Y. Xu (2020), <em>Nonseparable Gaussian stochastic process: a unified
view and computational strategy</em>, <em>Fast Nonseparable Gaussian Stochastic Process With Application to Methylation Level Interpolation</em>, <em>Journal of Computational and Graphical Statistics</em>, <b>29</b>, 250-260.
</p>
<p>M. Gu and W. Shen (2020), <em>Generalized probabilistic principal component analysis of correlated data</em>, <em>Journal of Machine Learning Research</em>, <b>21</b>, 13-1.
</p>
<p>M. Gu, X. Wang and J.O. Berger (2018), <em>Robust Gaussian Stochastic Process Emulation</em>, <em>Annals of Statistics</em>, <b>46</b>, 3038-3066.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FastGaSP">FastGaSP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(FastGaSP)

#------------------------------------------------------------------------------
# Example 1 : fast computation algorithm for noisy data
#------------------------------------------------------------------------------

y_R&lt;-function(x){
  sin(2*pi*x)
}

###let's test for 2000 observations
set.seed(1)
num_obs=2000
input=runif(num_obs)
output=y_R(input)+rnorm(num_obs,mean=0,sd=0.1)

##constucting the fgasp.model
fgasp.model=fgasp(input, output)

##range and noise-variance ratio (nugget) parameters 
param=c( log(1),log(.02))
## the log lik
log_lik(param,fgasp.model)
##time cost to compute the likelihood
time_cost=system.time(log_lik(param,fgasp.model))
time_cost[1]

##consider a nonparametric regression setting 
##estimate the parameter by maximum likelihood estimation

est_all&lt;-optim(c(log(1),log(.02)),log_lik,object=fgasp.model,method="L-BFGS-B",
              control = list(fnscale=-1))  

##estimated log inverse range parameter and log nugget
est_all$par

##estimate variance 
est.var=Get_log_det_S2(est_all$par,fgasp.model@have_noise,fgasp.model@delta_x,
                          fgasp.model@output,fgasp.model@kernel_type)[[2]]/fgasp.model@num_obs
est.var

###1. Do some interpolation test 
num_test=5000
testing_input=runif(num_test) ##there are the input where you don't have observations
pred.model=predict(param=est_all$par,object=fgasp.model,testing_input=testing_input)

lb=pred.model@mean+qnorm(0.025)*sqrt(pred.model@var)
ub=pred.model@mean+qnorm(0.975)*sqrt(pred.model@var)

## calculate lb for the mean function
pred.model2=predict(param=est_all$par,object=fgasp.model,testing_input=testing_input,var_data=FALSE)
lb_mean_funct=pred.model2@mean+qnorm(0.025)*sqrt(pred.model2@var)
ub_mean_funct=pred.model2@mean+qnorm(0.975)*sqrt(pred.model2@var)

## plot the prediction
min_val=min(lb,output)
max_val=max(ub,output)

plot(pred.model@testing_input,pred.model@mean,type='l',col='blue',
     ylim=c(min_val,max_val),
     xlab='x',ylab='y')
polygon(c(pred.model@testing_input,rev(pred.model@testing_input)),
        c(lb,rev(ub)),col = "grey80", border = FALSE)
lines(pred.model@testing_input,pred.model@mean,type='l',col='blue')
lines(pred.model@testing_input,y_R(pred.model@testing_input),type='l',col='black')
lines(pred.model2@testing_input,lb_mean_funct,col='blue',lty=2)
lines(pred.model2@testing_input,ub_mean_funct,col='blue',lty=2)
lines(input,output,type='p',pch=16,col='black',cex=0.4) #one can plot data

legend("bottomleft", legend=c("predictive mean","95% predictive interval","truth"),
       col=c("blue","blue","black"), lty=c(1,2,1), cex=.8)

#--------------------------------------------------------------
# Example 2: example that one does not have a noise in the data
#--------------------------------------------------------------
## Here is a function in the Sobolev Space with order 3
y_R&lt;-function(x){
  j_seq=seq(1,200,1)
  record_y_R=0
  for(i_j in 1:200){
    record_y_R=record_y_R+2*j_seq[i_j]^{-2*3}*sin(j_seq[i_j])*cos(pi*(j_seq[i_j]-0.5)*x)

  }
  record_y_R
}


##generate some data without noise
num_obs=50
input=seq(0,1,1/(num_obs-1))

output=y_R(input)


##constucting the fgasp.model
fgasp.model=fgasp(input, output,have_noise=FALSE)

##range and noise-variance ratio (nugget) parameters 
param=c( log(1))
## the log lik
log_lik(param,fgasp.model)



#if one does not have noise one may need to give a lower bound or use a penalty 
#(e.g. induced by a prior) to make the estimation more robust
est_all&lt;-optimize(log_lik,interval=c(0,10),maximum=TRUE,fgasp.model)
  
##Do some interpolation test for comparison
num_test=1000
testing_input=runif(num_test) ##there are the input where you don't have observations

pred.model=predict(param=est_all$maximum,object=fgasp.model,testing_input=testing_input)



#This is the 95 posterior credible interval for the outcomes which contain the estimated 
#variance of the noise
#sometimes there are numerical instability is one does not have noise or error
lb=pred.model@mean+qnorm(0.025)*sqrt(abs(pred.model@var))
ub=pred.model@mean+qnorm(0.975)*sqrt(abs(pred.model@var))

## plot the prediction
min_val=min(lb,output)
max_val=max(ub,output)

plot(pred.model@testing_input,pred.model@mean,type='l',col='blue',
     ylim=c(min_val,max_val),
     xlab='x',ylab='y')
polygon( c(pred.model@testing_input,rev(pred.model@testing_input)),
         c(lb,rev(ub)),col = "grey80", border = FALSE)
lines(pred.model@testing_input,pred.model@mean,type='l',col='blue')
lines(pred.model@testing_input,y_R(pred.model@testing_input),type='l',col='black')
lines(input,output,type='p',pch=16,col='black')
legend("bottomleft", legend=c("predictive mean","95% predictive interval","truth"),
       col=c("blue","blue","black"), lty=c(1,2,1), cex=.8)


##mean square error for all inputs
mean((pred.model@mean- y_R(pred.model@testing_input))^2)

</code></pre>

<hr>
<h2 id='A_t_times_x_particle'>Transpose matrix-vector multiplication for particle systems</h2><span id='topic+A_t_times_x_particle'></span>

<h3>Description</h3>

<p>Performs the matrix-vector multiplication A^T*x for particle systems, where A is a sparse matrix stored only the non-zero entries
</p>


<h3>Usage</h3>

<pre><code class='language-R'>A_t_times_x_particle(output, A_all_v, num_neighbors_vec, D_y, N_tilde)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="A_t_times_x_particle_+3A_output">output</code></td>
<td>
<p>A numeric vector containing the input vector for multiplication.</p>
</td></tr>
<tr><td><code id="A_t_times_x_particle_+3A_a_all_v">A_all_v</code></td>
<td>
<p>A numeric vector containing the interaction matrices in vectorized form.</p>
</td></tr>
<tr><td><code id="A_t_times_x_particle_+3A_num_neighbors_vec">num_neighbors_vec</code></td>
<td>
<p>An integer vector specifying the number of neighbors for each particle.</p>
</td></tr>
<tr><td><code id="A_t_times_x_particle_+3A_d_y">D_y</code></td>
<td>
<p>An integer specifying the dimension of the output vector per particle.</p>
</td></tr>
<tr><td><code id="A_t_times_x_particle_+3A_n_tilde">N_tilde</code></td>
<td>
<p>An integer specifying the total dimension of the output vector.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a numeric vector containing the result of the matrix-vector multiplication.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Fang, X., &amp; Gu, M. (2024). <em>The inverse Kalman filter</em>. arXiv:2407.10089.
</p>

<hr>
<h2 id='A_times_x_particle'>Matrix-vector multiplication for particle systems</h2><span id='topic+A_times_x_particle'></span>

<h3>Description</h3>

<p>Performs the matrix-vector multiplication A*x for particle systems, where A is a sparse matrix stored only the non-zero entries
</p>


<h3>Usage</h3>

<pre><code class='language-R'>A_times_x_particle(output, A_all_v, num_neighbors_vec, D, N)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="A_times_x_particle_+3A_output">output</code></td>
<td>
<p>A numeric vector containing the input vector for multiplication.</p>
</td></tr>
<tr><td><code id="A_times_x_particle_+3A_a_all_v">A_all_v</code></td>
<td>
<p>A numeric vector containing the interaction matrices in vectorized form.</p>
</td></tr>
<tr><td><code id="A_times_x_particle_+3A_num_neighbors_vec">num_neighbors_vec</code></td>
<td>
<p>An integer vector specifying the number of neighbors for each particle.</p>
</td></tr>
<tr><td><code id="A_times_x_particle_+3A_d">D</code></td>
<td>
<p>An integer specifying the dimension of the output vector per particle.</p>
</td></tr>
<tr><td><code id="A_times_x_particle_+3A_n">N</code></td>
<td>
<p>An integer specifying the total dimension of the output vector.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a numeric vector containing the result of the matrix-vector multiplication.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Fang, X., &amp; Gu, M. (2024). <em>The inverse Kalman filter</em>. arXiv:2407.10089.
</p>

<hr>
<h2 id='Construct_G_exp'> 
The coefficient matrix in the dynamic linear model when kernel is the exponential covariance
</h2><span id='topic+Construct_G_exp'></span>

<h3>Description</h3>

<p>The coefficient matrix in the dynamic linear model when kernel is the exponential covariance. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Construct_G_exp(delta_x,lambda)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Construct_G_exp_+3A_delta_x">delta_x</code></td>
<td>
<p>the distance between the sorted input.
</p>
</td></tr>
<tr><td><code id="Construct_G_exp_+3A_lambda">lambda</code></td>
<td>
<p>the transformed range parameter. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>GG matrix. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>.  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2019), <em>fast nonseparable Gaussian stochastic process with application to methylation level interpolation</em>.  <em>Journal of Computational and Graphical Statistics</em>, In Press, arXiv:1711.11501.
</p>
<p>Campagnoli P, Petris G, Petrone S. (2009), <em>Dynamic linear model with R</em>. Springer-Verlag New York.
</p>

<hr>
<h2 id='Construct_G_matern_5_2'> 
The coefficient matrix in the dynamic linear model when kernel is the Matern covariance with roughness parameter 2.5. 
</h2><span id='topic+Construct_G_matern_5_2'></span>

<h3>Description</h3>

<p>The coefficient matrix in the dynamic linear model when kernel is the Matern covariance with roughness parameter 2.5. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Construct_G_matern_5_2(delta_x,lambda)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Construct_G_matern_5_2_+3A_delta_x">delta_x</code></td>
<td>
<p>The distance between the sorted input.
</p>
</td></tr>
<tr><td><code id="Construct_G_matern_5_2_+3A_lambda">lambda</code></td>
<td>
<p>the transformed range parameter. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>GG matrix. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>.  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2019), <em>fast nonseparable Gaussian stochastic process with application to methylation level interpolation</em>.  <em>Journal of Computational and Graphical Statistics</em>, In Press, arXiv:1711.11501.
</p>
<p>Campagnoli P, Petris G, Petrone S. (2009), <em>Dynamic linear model with R</em>. Springer-Verlag New York.
</p>

<hr>
<h2 id='Construct_W_exp'> 
The conditional covariance matrix of the state in the dynamic linear model when kernel is the exponential covariance
</h2><span id='topic+Construct_W_exp'></span>

<h3>Description</h3>

<p>The conditional covariance matrix of the state in the dynamic linear model when kernel is the exponential covariance. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Construct_W_exp(sigma2,delta_x,lambda,W0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Construct_W_exp_+3A_sigma2">sigma2</code></td>
<td>
<p>the variance parameter.
</p>
</td></tr>
<tr><td><code id="Construct_W_exp_+3A_delta_x">delta_x</code></td>
<td>
<p>the distance between the sorted input.
</p>
</td></tr>
<tr><td><code id="Construct_W_exp_+3A_lambda">lambda</code></td>
<td>
<p>the transformed range parameter. </p>
</td></tr>
<tr><td><code id="Construct_W_exp_+3A_w0">W0</code></td>
<td>
<p>the covariance matrix of the stationary distribution of the state. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>W matrix. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>.  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2019), <em>fast nonseparable Gaussian stochastic process with application to methylation level interpolation</em>.  <em>Journal of Computational and Graphical Statistics</em>, In Press, arXiv:1711.11501.
</p>
<p>Campagnoli P, Petris G, Petrone S. (2009), <em>Dynamic linear model with R</em>. Springer-Verlag New York.
</p>

<hr>
<h2 id='Construct_W_matern_5_2'> 
The conditional covariance matrix for  matern covariance with roughness parameter 2.5
</h2><span id='topic+Construct_W_matern_5_2'></span>

<h3>Description</h3>

<p>The conditional covariance matrix of the state in the dynamic linear model when kernel is the matern covariance with roughness parameter 2.5.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Construct_W_matern_5_2(sigma2,delta_x,lambda,W0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Construct_W_matern_5_2_+3A_sigma2">sigma2</code></td>
<td>
<p>the variance parameter.
</p>
</td></tr>
<tr><td><code id="Construct_W_matern_5_2_+3A_delta_x">delta_x</code></td>
<td>
<p>the distance between the sorted input.
</p>
</td></tr>
<tr><td><code id="Construct_W_matern_5_2_+3A_lambda">lambda</code></td>
<td>
<p>the transformed range parameter. </p>
</td></tr>
<tr><td><code id="Construct_W_matern_5_2_+3A_w0">W0</code></td>
<td>
<p>the covariance matrix of the stationary distribution of the state. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>W matrix. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>.  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2019), <em>fast nonseparable Gaussian stochastic process with application to methylation level interpolation</em>.  <em>Journal of Computational and Graphical Statistics</em>, In Press, arXiv:1711.11501.
</p>
<p>Campagnoli P, Petris G, Petrone S. (2009), <em>Dynamic linear model with R</em>. Springer-Verlag New York.
</p>

<hr>
<h2 id='Construct_W0_exp'> 
covariance of the stationary distribution of the state when kernel is the exponential covariance.
</h2><span id='topic+Construct_W0_exp'></span>

<h3>Description</h3>

<p>This function computes the covariance of the stationary distribution of the state when kernel is the exponential covariance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Construct_W0_exp(sigma2,lambda)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Construct_W0_exp_+3A_sigma2">sigma2</code></td>
<td>
<p>the variance parameter.
</p>
</td></tr>
<tr><td><code id="Construct_W0_exp_+3A_lambda">lambda</code></td>
<td>
<p>the transformed range parameter. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>W0 matrix. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>.  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2019), <em>fast nonseparable Gaussian stochastic process with application to methylation level interpolation</em>.  <em>Journal of Computational and Graphical Statistics</em>, In Press, arXiv:1711.11501.
</p>
<p>Campagnoli P, Petris G, Petrone S. (2009), <em>Dynamic linear model with R</em>. Springer-Verlag New York.
</p>

<hr>
<h2 id='Construct_W0_matern_5_2'> 
covariance of the stationary distribution of the state when kernel is the Matern covariance with roughness parameter 2.5. 
</h2><span id='topic+Construct_W0_matern_5_2'></span>

<h3>Description</h3>

<p>This function computes covariance of the stationary distribution of the state when kernel is the Matern covariance with roughness parameter 2.5. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Construct_W0_matern_5_2(sigma2,lambda)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Construct_W0_matern_5_2_+3A_sigma2">sigma2</code></td>
<td>
<p>the variance parameter.
</p>
</td></tr>
<tr><td><code id="Construct_W0_matern_5_2_+3A_lambda">lambda</code></td>
<td>
<p>the transformed range parameter. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>W0 matrix. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>.  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2019), <em>fast nonseparable Gaussian stochastic process with application to methylation level interpolation</em>.  <em>Journal of Computational and Graphical Statistics</em>, In Press, arXiv:1711.11501.
</p>
<p>Campagnoli P, Petris G, Petrone S. (2009), <em>Dynamic linear model with R</em>. Springer-Verlag New York.
</p>

<hr>
<h2 id='extract_time_window'>Extract time window from particle data</h2><span id='topic+extract_time_window'></span>

<h3>Description</h3>

<p>Extracts a specified time window from a particle.data object while preserving all relevant tracking information and parameters. Works with both simulation and experimental data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_time_window(data_obj, first_frame, last_frame)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="extract_time_window_+3A_data_obj">data_obj</code></td>
<td>
<p>An object of class <code><a href="#topic+particle.data-class">particle.data</a></code>.</p>
</td></tr>
<tr><td><code id="extract_time_window_+3A_first_frame">first_frame</code></td>
<td>
<p>Integer specifying the first frame to include (must be &gt;= 1).</p>
</td></tr>
<tr><td><code id="extract_time_window_+3A_last_frame">last_frame</code></td>
<td>
<p>Integer specifying the last frame to include (must be less than the total number of frames).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a new <code><a href="#topic+particle.data-class">particle.data</a></code> object containing:
</p>

<dl>
<dt>px_list, py_list</dt><dd><p>Position data for the selected time window</p>
</dd>
<dt>vx_list, vy_list</dt><dd><p>Velocity data for the selected time window</p>
</dd>
<dt>theta_list</dt><dd><p>Angle data if present in original object</p>
</dd>
<dt>particle_tracking</dt><dd><p>Tracking information for the selected frames (experimental data)</p>
</dd>
<dt>data_type</dt><dd><p>Original data type (&quot;simulation&quot; or &quot;experiment&quot;)</p>
</dd>
<dt>n_particles</dt><dd><p>Particle counts (constant for simulation, time series for experimental)</p>
</dd>
<dt>T_time</dt><dd><p>Number of time steps in the extracted window</p>
</dd>
<dt>model, sigma_0, radius</dt><dd><p>Original model parameters (for simulation data)</p>
</dd>
<dt>D_y</dt><dd><p>Original dimension of the output space</p>
</dd>
</dl>


<hr>
<h2 id='f_Vicsek_variation'>Modified Vicsek Interaction Function</h2><span id='topic+f_Vicsek_variation'></span>

<h3>Description</h3>

<p>A modified interaction function for the Vicsek model that defines the interaction strength between particles based on their distance. This function is used as one of the interactions in the two-interaction Vicsek model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f_Vicsek_variation(r, a = 0.02, b = 1, r_min = 0.01, r_max = 0.8, beta = 20)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="f_Vicsek_variation_+3A_r">r</code></td>
<td>
<p>numeric, the distance between particles</p>
</td></tr>
<tr><td><code id="f_Vicsek_variation_+3A_a">a</code></td>
<td>
<p>numeric, strength parameter for the short-range interaction term. Default is 0.02</p>
</td></tr>
<tr><td><code id="f_Vicsek_variation_+3A_b">b</code></td>
<td>
<p>numeric, strength parameter for the linear term. Default is 1</p>
</td></tr>
<tr><td><code id="f_Vicsek_variation_+3A_r_min">r_min</code></td>
<td>
<p>numeric, minimum distance parameter for the interaction term. Default is 0.01</p>
</td></tr>
<tr><td><code id="f_Vicsek_variation_+3A_r_max">r_max</code></td>
<td>
<p>numeric, maximum distance parameter. Default is 0.8</p>
</td></tr>
<tr><td><code id="f_Vicsek_variation_+3A_beta">beta</code></td>
<td>
<p>numeric, scaling parameter for the overall interaction. Default is 20</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function implements a modified Vicsek interaction with three components:
</p>

<ul>
<li><p> A short-range term (-a/(r+r_min))
</p>
</li>
<li><p> A linear term (-b*(r-r_max))
</p>
</li>
<li><p> A constant offset term (a/r_max)
</p>
</li></ul>

<p>The final value is scaled by the beta parameter.
</p>


<h3>Value</h3>

<p>Returns a numeric value representing the interaction strength at distance r.
</p>


<h3>References</h3>

<p>Chat'e, H., Ginelli, F., Gr'egoire, G., Peruani, F., &amp; Raynaud, F. (2008). <em>Modeling collective motion: variations on the Vicsek model</em>, <em>The European Physical Journal B</em>, <b>64</b>(3), 451-456.
</p>
<p>Fang, X., &amp; Gu, M. (2024). <em>The inverse Kalman filter</em>. arXiv:2407.10089.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Calculate interaction values at various distances
r_seq &lt;- seq(0.01, 1, by = 0.01)
interaction &lt;- f_Vicsek_variation(r_seq)
plot(r_seq, interaction, type = "l", 
     xlab = "Distance", ylab = "Interaction Strength",
     main = "Vicsek Variation Interaction Function")
</code></pre>

<hr>
<h2 id='fgasp'> Setting up the Fast GaSP model
</h2><span id='topic+fgasp'></span><span id='topic+fgasp-method'></span>

<h3>Description</h3>

<p>Creating an <code>fgasp</code> class for a GaSP model with matern covariance. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  fgasp(input, output, have_noise=TRUE, kernel_type='matern_5_2')
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fgasp_+3A_input">input</code></td>
<td>
<p>a vector with dimension num_obs x 1 for the sorted input locations.</p>
</td></tr>
<tr><td><code id="fgasp_+3A_output">output</code></td>
<td>
<p>a  vector with dimension n x 1 for the observations at the sorted input locations.</p>
</td></tr>
<tr><td><code id="fgasp_+3A_have_noise">have_noise</code></td>
<td>
<p>a bool value. If it is true, it means the model contains a noise. 
</p>
</td></tr>
<tr><td><code id="fgasp_+3A_kernel_type">kernel_type</code></td>
<td>
<p>a <code>character</code> to specify the type of kernel to use. The current version supports kernel_type to be &quot;matern_5_2&quot; or &quot;exp&quot;, meaning that the matern kernel with roughness parameter being 2.5 or 0.5 (exponent kernel), respectively. 
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>fgasp</code> returns an S4 object of class <code>fgasp</code> (see <code>fgasp</code>).
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>,  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2017), <em>Nonseparable Gaussian stochastic process: a unified
view and computational strategy</em>, arXiv:1711.11501.
</p>
<p>M. Gu, X. Wang and J.O. Berger (2018), <em>Robust Gaussian Stochastic Process Emulation</em>, <em>Annals of Statistics</em>, <b>46</b>, 3038-3066.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(FastGaSP)

#-------------------------------------
# Example 1: a simple example with noise 
#-------------------------------------

y_R&lt;-function(x){
  cos(2*pi*x)
}

###let's test for 2000 observations
set.seed(1)
num_obs=2000
input=runif(num_obs)

output=y_R(input)+rnorm(num_obs,mean=0,sd=0.1)

##constucting the fgasp.model
fgasp.model=fgasp(input, output)
show(fgasp.model)

#------------------------------------------
# Example 2: a simple example with no noise 
#------------------------------------------

y_R&lt;-function(x){
  sin(2*pi*x)
}


##generate some data without noise
num_obs=50
input=seq(0,1,1/(num_obs-1))

output=y_R(input)


##constucting the fgasp.model
fgasp.model=fgasp(input, output,have_noise=FALSE)

show(fgasp.model)

</code></pre>

<hr>
<h2 id='fgasp-class'>Fast GaSP class</h2><span id='topic+fgasp-class'></span>

<h3>Description</h3>

<p>S4 class for fast computation of the Gaussian stochastic process (GaSP) model with the Matern kernel function with or without a noise.</p>


<h3>Objects from the Class</h3>

<p>Objects of this class are created and initialized with the function <code><a href="#topic+fgasp">fgasp</a></code> that computes the calculations needed for setting up the estimation and prediction.</p>


<h3>Slots</h3>


<dl>
<dt><code>num_obs</code>:</dt><dd><p>object of class <code>integer</code>. The number of experimental observations.</p>
</dd>
<dt><code>have_noise</code>:</dt><dd><p>object of class <code>logical</code> to specify whether the the model has a noise or not. &quot;TRUE&quot; means the model contains a noise and &quot;FALSE&quot; means the model does not contain a noise.</p>
</dd>
<dt><code>kernel_type</code>:</dt><dd><p>a <code>character</code> to specify the type of kernel to use.The current version supports kernel_type to be &quot;matern_5_2&quot; or &quot;exp&quot;, meaning that the matern kernel with roughness parameter being 2.5 or 0.5 (exponent kernel), respectively. </p>
</dd>
<dt><code>input</code>:</dt><dd><p>object of class <code>vector</code> with dimension num_obs x 1 for the sorted input locations.</p>
</dd>
<dt><code>delta_x</code>:</dt><dd><p>object of class <code>vector</code> with dimension (num_obs-1) x 1 for the differences between the sorted input locations.</p>
</dd>
<dt><code>output</code>:</dt><dd><p>object of class <code>vector</code> with dimension num_obs x 1 for the observations at the sorted input locations.</p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>show</dt><dd><p>Prints the main slots of the object. </p>
</dd>
<dt>predict</dt><dd><p>See <code><a href="#topic+predict.fgasp">predict</a></code>.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>,  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2017), <em>Nonseparable Gaussian stochastic process: a unified
view and computational strategy</em>, arXiv:1711.11501.
</p>
<p>M. Gu, X. Wang and J.O. Berger (2018), <em>Robust Gaussian Stochastic Process Emulation</em>, <em>Annals of Statistics</em>, <b>46</b>, 3038-3066.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+fgasp">fgasp</a></code> for more details about how to create a <code>fgasp</code> object.
</p>

<hr>
<h2 id='fit'>Fit Particle Interaction Models</h2><span id='topic+fit'></span>

<h3>Description</h3>

<p>Generic function for fitting particle interaction models to data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fit(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fit_+3A_object">object</code></td>
<td>
<p>Object containing data to be fit</p>
</td></tr>
<tr><td><code id="fit_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to fitting methods</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+fit.particle.data">fit.particle.data</a></code> for fitting particle data models
</p>

<hr>
<h2 id='fit.fmou'> 
The fast EM algorithm of multivariate Ornstein-Uhlenbeck processes</h2><span id='topic+fit.fmou'></span><span id='topic+fit.fmou+2Cfmou-method'></span>

<h3>Description</h3>

<p>This function implements an efficient EM algorithm to estimate the parameters in the FMOU model, a latent factor model with a fixed or estimated orthogonal factor loading matrix, where each latent factor is modeled as an O-U (Ornstein-Uhlenbeck) process.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'fmou'
fit.fmou(object, M=50, threshold=1e-4,
         track_iterations=FALSE,track_neg_log_lik=FALSE,
         U_init=NULL, rho_init=NULL, sigma2_init=NULL, d_ub=NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fit.fmou_+3A_object">object</code></td>
<td>
<p>an objecft of  class <code>fmou.</code></p>
</td></tr>
<tr><td><code id="fit.fmou_+3A_m">M</code></td>
<td>
<p>number of iterations in the EM algorithm, default is 50.</p>
</td></tr>
<tr><td><code id="fit.fmou_+3A_threshold">threshold</code></td>
<td>
<p>stopping criteria with respect to predictive mean of observations, default is 1e-4.</p>
</td></tr>
<tr><td><code id="fit.fmou_+3A_track_iterations">track_iterations</code></td>
<td>
<p>a bool value, default is <code>FALSE</code>. If <code>TRUE</code>, the estimations in each EM iteration will be recorded and returned. </p>
</td></tr>
<tr><td><code id="fit.fmou_+3A_track_neg_log_lik">track_neg_log_lik</code></td>
<td>
<p>a bool value, default is <code>FALSE</code>. If <code>TRUE</code>, the negative log marginal likelihood of the output in each EM iteration will be recorded and returned.</p>
</td></tr>
<tr><td><code id="fit.fmou_+3A_u_init">U_init</code></td>
<td>
<p>user-specified initial factor loading matrix in the EM algorithm. Default is <code>NULL</code>. The dimension is k*d, where k is the length of observations at each time step and d is the number of latent factors.</p>
</td></tr>
<tr><td><code id="fit.fmou_+3A_rho_init">rho_init</code></td>
<td>
<p>user-specified initial correlation parameters in the EM algorithm. Default is <code>NULL</code>. The length is equal to the number of latent factors.</p>
</td></tr>
<tr><td><code id="fit.fmou_+3A_sigma2_init">sigma2_init</code></td>
<td>
<p>user-specified initial variance parameters in the EM algorithm. Default is <code>NULL</code>. The length is equal to the number of latent factors.</p>
</td></tr>
<tr><td><code id="fit.fmou_+3A_d_ub">d_ub</code></td>
<td>
<p>upper bound of d when d is estimated. Default is null.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>output</code></td>
<td>
<p>the observation matrix.</p>
</td></tr>
<tr><td><code>U</code></td>
<td>
<p>the estimated (or fixed) factor loading matrix.</p>
</td></tr>
<tr><td><code>post_z_mean</code></td>
<td>
<p>the posterior mean of latent factors.</p>
</td></tr>
<tr><td><code>post_z_var</code></td>
<td>
<p>the posterior variance of latent factors.</p>
</td></tr>
<tr><td><code>post_z_cov</code></td>
<td>
<p>the posterior covariance between two consecutive time steps of a latent process.</p>
</td></tr>
<tr><td><code>mean_obs</code></td>
<td>
<p>the predictive mean of the observations.</p>
</td></tr>
<tr><td><code>mean_obs_95lb</code></td>
<td>
<p>the lower bound of the 95% posterior credible intervals of predictive mean.</p>
</td></tr>
<tr><td><code>mean_obs_95ub</code></td>
<td>
<p>the upper bound of the 95% posterior credible intervals of predictive mean.</p>
</td></tr>
<tr><td><code>sigma0_2</code></td>
<td>
<p>estimated variance of noise.</p>
</td></tr>
<tr><td><code>rho</code></td>
<td>
<p>estimated correlation parameters.</p>
</td></tr>
<tr><td><code>sigma2</code></td>
<td>
<p>estimated variance parameters</p>
</td></tr>
<tr><td><code>num_iterations</code></td>
<td>
<p>number of iterations in the EM algorithm.</p>
</td></tr>
<tr><td><code>d</code></td>
<td>
<p>the estimated (or fixed) number of latent factors.</p>
</td></tr>
<tr><td><code>record_sigma0_2</code></td>
<td>
<p>estimated variance of noise in each iteration.</p>
</td></tr>
<tr><td><code>record_rho</code></td>
<td>
<p>estimated correlation parameters in each iteration.</p>
</td></tr>
<tr><td><code>record_sigma2</code></td>
<td>
<p>estimation variance parameters in each iteration.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Lin, Y., Liu, X., Segall, P., &amp; Gu, M. (2025). Fast data inversion for high-dimensional dynamical systems from noisy measurements. arXiv preprint arXiv:2501.01324.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate simulated data
library(FastGaSP)
library(rstiefel)

d = 5  # number of latent factors
k = 20 # length of observation at each time step
n = 500 # number time step
noise_level = 1 # variance of noise

U = rustiefel(k, k) # factor loading matrix
z = matrix(NA, d, n)
sigma_2 = runif(d, 0.5, 1)
rho = runif(d, 0.95, 1)
for(l in 1:d){
  R = matrix(NA, n, n)
  diag(R) = 1
  for(ir in 1:n){
    for(ic in 1:n){
      R[ir, ic] = rho[l]^(abs(ir-ic)) * R[ir, ir]
    }
  }
  R = (sigma_2[l]/(1-rho[l]^2) )* R
  z[l, ] = t(chol(R)) %*% rnorm(n)
}

signal = U[,1:d] %*% z
y = signal + matrix(rnorm(n*k,mean=0,sd=sqrt(noise_level)),k,n)

##constucting the fmou.model
fmou.model=fmou(output=y, d=d, est_U0=TRUE, est_sigma0_2=TRUE)

## estimate the parameters
em_alg &lt;- fit.fmou(fmou.model, M=500)

## root mean square error (RMSE) of predictive mean of observations
sqrt(mean((em_alg$mean_obs-signal)^2))

## standard deviation of (truth) mean of observations
sd(signal)

## estimated variance of noise
em_alg$sigma0_2

</code></pre>

<hr>
<h2 id='fit.gppca'> 
Parameter estimation for generalized probabilistic principal component analysis of correlated data.</h2><span id='topic+fit.gppca'></span><span id='topic+fit.gppca+2Cgppca-method'></span>

<h3>Description</h3>

<p>This function estimates the parameters for generalized probabilistic principal component analysis of correlated data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'gppca'
fit.gppca(object, sigma0_2=NULL, d_ub=NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fit.gppca_+3A_object">object</code></td>
<td>
<p>an object of  class <code>gppca.</code></p>
</td></tr>
<tr><td><code id="fit.gppca_+3A_sigma0_2">sigma0_2</code></td>
<td>
<p>variance of noise. Default is <code>NULL</code>. User should specify a value when it is known in real data.</p>
</td></tr>
<tr><td><code id="fit.gppca_+3A_d_ub">d_ub</code></td>
<td>
<p>upper bound of d when d is estimated. Default is <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>est_A</code></td>
<td>
<p>the estimated factor loading matrix.</p>
</td></tr>
<tr><td><code>est_beta</code></td>
<td>
<p>the estimated inversed range parameter.</p>
</td></tr>
<tr><td><code>est_sigma0_2</code></td>
<td>
<p>the estimated variance of noise.</p>
</td></tr>
<tr><td><code>est_sigma2</code></td>
<td>
<p>the estimated variance parameter.</p>
</td></tr>
<tr><td><code>mean_obs</code></td>
<td>
<p>the predictive mean of the observations.</p>
</td></tr>
<tr><td><code>mean_obs_95lb</code></td>
<td>
<p>the lower bound of the 95% posterior credible intervals of predictive mean.</p>
</td></tr>
<tr><td><code>mean_obs_95ub</code></td>
<td>
<p>the upper bound of the 95% posterior credible intervals of predictive mean.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Gu, M., &amp; Shen, W. (2020), Generalized probabilistic principal component analysis of correlated data, <em>Journal of Machine Learning Research, 21</em>(13), 1-41.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(FastGaSP)
library(rstiefel)

matern_5_2_funct &lt;- function(d, beta_i) {
  cnst &lt;- sqrt(5.0)
  matOnes &lt;- matrix(1, nrow = nrow(d), ncol = ncol(d))
  result &lt;- cnst * beta_i * d
  res &lt;- (matOnes + result + (result^2) / 3) * exp(-result)
  return(res)
}

n=200
k=8
d=4

beta_real=0.01
sigma_real=1
sigma_0_real=sqrt(.01)
input=seq(1,n,1)
R0_00=as.matrix(abs(outer(input,input,'-')))
R_r = matern_5_2_funct(R0_00, beta_real)
L_sample = t(chol(R_r))
kernel_type='matern_5_2'


input=sort(input)
delta_x=input[2:length(input)]-input[1:(length(input)-1)]
A=rustiefel(k, d)  ##sample from Stiefel manifold
Factor=matrix(0,d,n)
for(i in 1: d){
  Factor[i,]=sigma_real^2*L_sample%*%rnorm(n)
}
output=A%*%Factor+matrix(rnorm(n*k,mean=0,sd=sigma_0_real),k,n)
  
##constucting the gppca.model
gppca_obj &lt;- gppca(input, output, d, shared_params = TRUE,est_d=FALSE)
## estimate the parameters
gppca_fit &lt;- fit.gppca(gppca_obj)

## MSE between predictive mean of observations and true mean
Y_mean=A%*%Factor
mean((gppca_fit$mean_obs-Y_mean)^2)
sd(Y_mean)

## coverage of 95% posterior credible intervals
sum(gppca_fit$mean_obs_95lb&lt;=Y_mean &amp; gppca_fit$mean_obs_95ub&gt;=Y_mean)/(n*k)

## length of 95% posterior credible intervals
mean(abs(gppca_fit$mean_obs_95ub-gppca_fit$mean_obs_95lb))

</code></pre>

<hr>
<h2 id='fit.particle.data'>Fit method for particle data</h2><span id='topic+fit.particle.data'></span><span id='topic+fit+2Cparticle.data-method'></span>

<h3>Description</h3>

<p>Estimates interaction parameters for particle systems using trajectory data with the IKF-CG (Inverse Kalman Filter - Conjugate Gradient) approach. Supports both simulation and experimental data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  ## S4 method for signature 'particle.data'
fit(
    object, param, cut_r_max=1, est_param = TRUE, nx=NULL, ny=NULL,
    kernel_type = "matern_5_2", tilde_nu = 0.1, tol = 1e-6,
    maxIte = 1000, output = NULL, ho_output = NULL, 
    testing_inputs=NULL, compute_CI = TRUE, num_interaction = (length(param)-1)/2,
    data_type = object@data_type, model = object@model, 
    apolar_vicsek = FALSE, direction = NULL
  )
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fit.particle.data_+3A_object">object</code></td>
<td>
<p>An object of class <code>particle.data</code> containing the trajectory data.</p>
</td></tr>
<tr><td><code id="fit.particle.data_+3A_param">param</code></td>
<td>
<p>Numeric vector of parameters. Should contain 2*num_interaction + 1 elements: first num_interaction elements are log of inverse range parameters (beta), next num_interaction elements are log of variance-noise ratios (tau), and the final element is log(radius/(cut_r_max-radius)) where radius is the interaction radius.</p>
</td></tr>
<tr><td><code id="fit.particle.data_+3A_cut_r_max">cut_r_max</code></td>
<td>
<p>Numeric value specifying the maximum interaction radius to consider during estimation (default: 1).</p>
</td></tr>
<tr><td><code id="fit.particle.data_+3A_est_param">est_param</code></td>
<td>
<p>If TRUE, param is used as initial values for parameter optimization. If FALSE, param is treated as fixed parameters for prediction (default: TRUE).</p>
</td></tr>
<tr><td><code id="fit.particle.data_+3A_nx">nx</code></td>
<td>
<p>An integer specifying the number of grid points along the x-axis (horizontal direction). If NULL, automatically calculated as floor((px_max-px_min)/cut_r_max), where px_max and px_min represent the maximum and minimum x-coordinates of all particles.</p>
</td></tr>  
<tr><td><code id="fit.particle.data_+3A_ny">ny</code></td>
<td>
<p>An integer specifying the number of grid points along the y-axis (vertical direction). If NULL, automatically calculated as floor((py_max-py_min)/cut_r_max), where py_max and py_min represent the maximum and minimum y-coordinates of all particles.</p>
</td></tr>
<tr><td><code id="fit.particle.data_+3A_kernel_type">kernel_type</code></td>
<td>
<p>Character string specifying the kernel type: 'matern_5_2' (default) or 'exp'.</p>
</td></tr>
<tr><td><code id="fit.particle.data_+3A_tilde_nu">tilde_nu</code></td>
<td>
<p>Numeric value for stabilizing the IKF computation (default: 0.1).</p>
</td></tr>
<tr><td><code id="fit.particle.data_+3A_tol">tol</code></td>
<td>
<p>Numeric value specifying convergence tolerance for the conjugate gradient algorithm (default: 1e-6).</p>
</td></tr>
<tr><td><code id="fit.particle.data_+3A_maxite">maxIte</code></td>
<td>
<p>Integer specifying maximum iterations for the conjugate gradient algorithm (default: 1000).</p>
</td></tr>
<tr><td><code id="fit.particle.data_+3A_output">output</code></td>
<td>
<p>Numerical vector (default = NULL). Used for residual bootstrap when different outputs but same inputs are needed.</p>
</td></tr>
<tr><td><code id="fit.particle.data_+3A_ho_output">ho_output</code></td>
<td>
<p>Numerical vector (default = NULL). Used for residual bootstrap when different hold-out outputs but same inputs are needed.</p>
</td></tr>
<tr><td><code id="fit.particle.data_+3A_testing_inputs">testing_inputs</code></td>
<td>
<p>Matrix of inputs for prediction (NULL if only performing parameter estimation). Each column represents testing inputs for one interaction.</p>
</td></tr>
<tr><td><code id="fit.particle.data_+3A_compute_ci">compute_CI</code></td>
<td>
<p>When TRUE, computes the 95% credible interval for testing_inputs (default: TRUE).</p>
</td></tr>
<tr><td><code id="fit.particle.data_+3A_num_interaction">num_interaction</code></td>
<td>
<p>Integer specifying number of interactions to predict (default: (length(param_ini)-1)/2).</p>
</td></tr>
<tr><td><code id="fit.particle.data_+3A_data_type">data_type</code></td>
<td>
<p>Character string indicating data type (&quot;simulation&quot; or &quot;experiment&quot;).</p>
</td></tr>
<tr><td><code id="fit.particle.data_+3A_model">model</code></td>
<td>
<p>Character string specifying the model type (e.g., &quot;Vicsek&quot;).</p>
</td></tr>
<tr><td><code id="fit.particle.data_+3A_apolar_vicsek">apolar_vicsek</code></td>
<td>
<p>When TRUE, considers only neighboring particles moving in the same direction (default: FALSE).</p>
</td></tr>
<tr><td><code id="fit.particle.data_+3A_direction">direction</code></td>
<td>
<p>Modeling direction ('x' or 'y') for experimental data analysis.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an object of class <code><a href="#topic+particle.est-class">particle.est</a></code>. See <code><a href="#topic+particle.est-class">particle.est-class</a></code> for details.
</p>


<h3>References</h3>

<p>Fang, X., &amp; Gu, M. (2024). <em>The inverse Kalman filter</em>. arXiv:2407.10089.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simulate data
vx_abs &lt;- 0.5
vy_abs &lt;- 0.5
v_abs &lt;- sqrt(vx_abs^2+vy_abs^2)
sim &lt;- simulate_particle(v_abs=v_abs)
show(sim)

# Set up testing inputs and initial parameters
testing_n &lt;- 200
testing_inputs &lt;- matrix(as.numeric(seq(-pi,pi,(2*pi)/(testing_n-1))),nr=1)
cut_r_max=1.5
param_ini &lt;- log(c(0.3,1000,0.3/(cut_r_max-0.3)))  # Initial parameter values

# Fit model to simulation data
est &lt;- fit(sim,param=param_ini,cut_r_max=1.5, testing_inputs = testing_inputs)
show(est)
</code></pre>

<hr>
<h2 id='fmou'> 
Setting up the FMOU model</h2><span id='topic+fmou'></span><span id='topic+fmou-method'></span>

<h3>Description</h3>

<p>Creating an <code>fmou</code> class for fmou, a latent factor model with a fixed or estimated orthogonal factor loading matrix, where each latent factor is modeled as an O-U (Ornstein-Uhlenbeck) process.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  fmou(output, d, est_d=FALSE, est_U0=TRUE, est_sigma0_2=TRUE, U0=NULL, sigma0_2=NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fmou_+3A_output">output</code></td>
<td>
<p>a k*n observation matrix, where k is the length of observations at each time step and n is the number of time steps.</p>
</td></tr>
<tr><td><code id="fmou_+3A_d">d</code></td>
<td>
<p>number of latent factors.</p>
</td></tr>
<tr><td><code id="fmou_+3A_est_d">est_d</code></td>
<td>
<p>a bool value, default is <code>FALSE</code>. If <code>TRUE</code>, d will be estimated by either variance matching (when noise level is given) or information criteria (when noise level is unknown). Otherwise, d is fixed, and users must assign a value to argument d.</p>
</td></tr>
<tr><td><code id="fmou_+3A_est_u0">est_U0</code></td>
<td>
<p>a bool value, default is <code>TRUE</code>. If <code>TRUE</code>, the factor loading matrix (U0) will be estimated. Otherwise, U0 is fixed.</p>
</td></tr>
<tr><td><code id="fmou_+3A_est_sigma0_2">est_sigma0_2</code></td>
<td>
<p>a bool value, default is <code>TRUE</code> . If <code>TRUE</code>, the variance of the noise will be estimated. Otherwise, it is fixed.</p>
</td></tr>
<tr><td><code id="fmou_+3A_u0">U0</code></td>
<td>
<p>the fixed factor loading matrix. Users should assign a k*d matrix to it when <code>est_U0=FALSE</code>. </p>
</td></tr>
<tr><td><code id="fmou_+3A_sigma0_2">sigma0_2</code></td>
<td>
<p>variance of noise. User should assign a value to it when <code>est_sigma0_2=FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>fmou</code> returns an S4 object of class <code>fmou</code>.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Lin, Y., Liu, X., Segall, P., &amp; Gu, M. (2025). Fast data inversion for high-dimensional dynamical systems from noisy measurements. arXiv preprint arXiv:2501.01324.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


## generate simulated data
library(FastGaSP)
library(rstiefel)

d = 5  # number of latent factors
k = 20 # length of observation at each time step
n = 100 # number time step
noise_level = 1 # variance of noise

U = rustiefel(k, k) # factor loading matrix
z = matrix(NA, d, n)
sigma_2 = runif(d, 0.5, 1)
rho = runif(d, 0.95, 1)
for(l in 1:d){
  R = matrix(NA, n, n)
  diag(R) = 1
  for(ir in 1:n){
    for(ic in 1:n){
      R[ir, ic] = rho[l]^(abs(ir-ic)) * R[ir, ir]
    }
  }
  R = (sigma_2[l]/(1-rho[l]^2) )* R
  z[l, ] = t(chol(R)) %*% rnorm(n)
}

signal = U[,1:d] %*% z
y = signal + matrix(rnorm(n*k,mean=0,sd=sqrt(noise_level)),k,n)

##constucting the fmou.model
fmou.model=fmou(output=y, d=d, est_U0=TRUE, est_sigma0_2=TRUE)
</code></pre>

<hr>
<h2 id='fmou-class'> FMOU class</h2><span id='topic+fmou-class'></span>

<h3>Description</h3>

<p>An S4 class for fast parameter estimation in the FMOU model, a latent factor model with a fixed or estimated orthogonal factor loading matrix, where each latent factor is modeled as an O-U (Ornstein-Uhlenbeck) process.
</p>


<h3>Objects from the Class</h3>

<p>Objects of this class are created and initialized using the <code><a href="#topic+fmou">fmou</a></code> function to set up the estimation.
</p>


<h3>Slots</h3>


<dl>
<dt><code>output</code>:</dt><dd><p>object of class <code>matrix</code>. The observation matrix.</p>
</dd>
<dt><code>d</code></dt><dd><p>object of class <code>integer</code> to specify the number of latent factors.</p>
</dd>
<dt><code>est_d</code></dt><dd><p>object of class <code>logical</code>, default is <code>FALSE</code>. If <code>TRUE</code>, d will be estimated by either variance matching (when noise level is given) or information criteria (when noise level is unknown). Otherwise, d is fixed, and users must assign a value to <code>d</code>.</p>
</dd>
<dt><code>est_U0</code></dt><dd><p>object of class <code>logical</code>, default is <code>TRUE</code>. If <code>TRUE</code>, the factor loading matrix (U0) will be estimated. Otherwise, U0 is fixed.</p>
</dd>
<dt><code>est_sigma0_2</code></dt><dd><p>object of class <code>logical</code>, default is <code>TRUE</code> . If <code>TRUE</code>, the variance of the noise will be estimated. Otherwise, it is fixed.</p>
</dd>
<dt><code>U0</code></dt><dd><p>object of class <code>matrix</code>. The fixed factor loading matrix. Users should assign a k*d matrix to it when <code>est_U0=False</code>. Here k is the length of observations at each time step.</p>
</dd>
<dt><code>sigma0_2</code></dt><dd><p>object of class <code>numeric</code>. Variance of noise. User should assign a value to it when <code>est_sigma0_2=False</code>.</p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>fit.fmou</dt><dd><p>See <code><a href="#topic+fit.fmou">fit.fmou</a></code>.</p>
</dd>
<dt>predict.fmou</dt><dd><p>See <code><a href="#topic+predict.fmou">predict.fmou</a></code>.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Lin, Y., Liu, X., Segall, P., &amp; Gu, M. (2025). Fast data inversion for high-dimensional dynamical systems from noisy measurements. arXiv preprint arXiv:2501.01324.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+fmou">fmou</a></code> for more details about how to create a <code>fmou</code> object.</p>

<hr>
<h2 id='Get_C_R_K_Q'> 
matrices and vectors for the inverse covariance in the predictive distribution
</h2><span id='topic+Get_C_R_K_Q'></span>

<h3>Description</h3>

<p>This function computes the required values for the inverse covariance matrix. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Get_C_R_K_Q(index,GG,W,C0,VV)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Get_C_R_K_Q_+3A_index">index</code></td>
<td>
<p>a vector of integer of 0 and 1. 0 means no observation at that input and 1 means there is observations at that input.
</p>
</td></tr>
<tr><td><code id="Get_C_R_K_Q_+3A_gg">GG</code></td>
<td>
<p>a list of matrices defined in the dynamic linear model.</p>
</td></tr>
<tr><td><code id="Get_C_R_K_Q_+3A_w">W</code></td>
<td>
<p>a list of matrices defined in the dynamic linear model.</p>
</td></tr>
<tr><td><code id="Get_C_R_K_Q_+3A_c0">C0</code></td>
<td>
<p>a matrix defined in the dynamic linear model.</p>
</td></tr>
<tr><td><code id="Get_C_R_K_Q_+3A_vv">VV</code></td>
<td>
<p>a numerical value for the nugget.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of 4 items for C, R, K and Q. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>.  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2019), <em>fast nonseparable gaussian stochastic process with application to methylation level interpolation</em>.  <em>Journal of Computational and Graphical Statistics</em>, In Press, arXiv:1711.11501.
</p>
<p>Campagnoli P, Petris G, Petrone S. (2009), <em>Dynamic linear model with R</em>. Springer-Verlag New York.
</p>

<hr>
<h2 id='get_consecutive_data'>Extract consecutive time steps data from particle trajectories</h2><span id='topic+get_consecutive_data'></span>

<h3>Description</h3>

<p>Extracts paired data from consecutive time steps for a specified variable in particle trajectory data. This function is particularly useful for experimental data, where it uses particle tracking information to maintain particle identity between frames.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_consecutive_data(data_obj, variable = c("vx", "vy", "px", "py", "theta"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_consecutive_data_+3A_data_obj">data_obj</code></td>
<td>
<p>An object of class <code><a href="#topic+particle.data-class">particle.data</a></code> containing particle trajectory data.</p>
</td></tr>
<tr><td><code id="get_consecutive_data_+3A_variable">variable</code></td>
<td>
<p>A character string specifying which variable to extract. Must be one of &quot;vx&quot; (x-velocity), &quot;vy&quot; (y-velocity), &quot;px&quot; (x-position), &quot;py&quot; (y-position), or &quot;theta&quot; (angle).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with two components:
</p>
<table role = "presentation">
<tr><td><code>start</code></td>
<td>
<p>A list of length T_time containing the data at each time step t.</p>
</td></tr>
<tr><td><code>end</code></td>
<td>
<p>A list of length T_time containing the corresponding data at time step t+1.</p>
</td></tr>
</table>
<p>For each time t, start[[t]] and end[[t]] contain paired measurements for the same particles at consecutive time steps.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
vx_pairs = get_consecutive_data(data_obj, "vx")
vx_list = vx_pairs$start
vx_end_list = vx_pairs$end

## End(Not run)
</code></pre>

<hr>
<h2 id='Get_L_inv_y'> 
The multiplication of the inverse of L with y
</h2><span id='topic+Get_L_inv_y'></span>

<h3>Description</h3>

<p>This function computes the product of the inverse of the L matrix and the output vector, where the L matrix is the Cholesky decomposition of the correlation matrix. Instead of computing the Cholesky matrix, we compute it using the forward filtering algorithm. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Get_L_inv_y(GG,Q,K,output)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Get_L_inv_y_+3A_gg">GG</code></td>
<td>
<p>a list of matrices defined in the dynamic linear model. 
</p>
</td></tr>
<tr><td><code id="Get_L_inv_y_+3A_q">Q</code></td>
<td>
<p>a vector defined in the dynamic linear model.    </p>
</td></tr>
<tr><td><code id="Get_L_inv_y_+3A_k">K</code></td>
<td>
<p>a matrix defined in the filtering algorithm for the dynamic linear model.  </p>
</td></tr>
<tr><td><code id="Get_L_inv_y_+3A_output">output</code></td>
<td>
<p>a vector of output.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector representing the product of the inverse of the L matrix and the output vector, where the L matrix is the Cholesky decomposition of the correlation matrix. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>.  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2019), <em>fast nonseparable gaussian stochastic process with application to methylation level interpolation</em>.  <em>Journal of Computational and Graphical Statistics</em>, In Press, arXiv:1711.11501.
</p>
<p>Campagnoli P, Petris G, Petrone S. (2009), <em>Dynamic linear model with R</em>. Springer-Verlag New York.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Get_Q_K">Get_Q_K</a></code> for more details about <code class="reqn">Q</code> vector and <code class="reqn">K</code> matrix,
<code><a href="#topic+Get_L_t_y">Get_L_t_y</a></code> for <code class="reqn">L^T y</code> computation, 
<code><a href="#topic+Get_L_y">Get_L_y</a></code> for <code class="reqn">L y</code> computation, 
<code><a href="#topic+Get_L_t_inv_y">Get_L_t_inv_y</a></code> for <code class="reqn">(L^T)^{-1}y</code> computation.
</p>

<hr>
<h2 id='Get_L_t_inv_y'> 
The multiplication of the inverse of the transpose of L with y
</h2><span id='topic+Get_L_t_inv_y'></span>

<h3>Description</h3>

<p>This function computes the product of the inverse of the transpose of the L matrix and the output vector, where L is the Cholesky decomposition of the correlation matrix R. Instead of explicitly forming the Cholesky matrix, this function uses the dynamic linear model (DLM) forward filtering algorithm for efficient computation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Get_L_t_inv_y(GG, Q, K, output)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Get_L_t_inv_y_+3A_gg">GG</code></td>
<td>
<p>a list of matrices defined in the dynamic linear model.</p>
</td></tr>
<tr><td><code id="Get_L_t_inv_y_+3A_q">Q</code></td>
<td>
<p>a vector defined in the dynamic linear model.</p>
</td></tr> 
<tr><td><code id="Get_L_t_inv_y_+3A_k">K</code></td>
<td>
<p>a matrix defined in the filtering algorithm for the dynamic linear model.</p>
</td></tr>
<tr><td><code id="Get_L_t_inv_y_+3A_output">output</code></td>
<td>
<p>a vector of observations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector representing the product of the inverse of the transpose of the L matrix and the output vector, where L is the Cholesky decomposition of the correlation matrix.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal Gaussian process regression models</em>. <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>, 379-384.
</p>
<p>Fang, X., &amp; Gu, M. (2024). <em>The inverse Kalman filter</em>. arXiv:2407.10089.
</p>
<p>M. Gu, Y. Xu (2019), <em>Fast nonseparable Gaussian stochastic process with application to methylation level interpolation</em>. <em>Journal of Computational and Graphical Statistics</em>, In Press, arXiv:1711.11501.
</p>
<p>Campagnoli P, Petris G, Petrone S. (2009), <em>Dynamic linear model with R</em>. Springer-Verlag New York.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Get_Q_K">Get_Q_K</a></code> for more details on <code class="reqn">K</code> and <code class="reqn">Q</code> matrices,
<code><a href="#topic+Get_L_inv_y">Get_L_inv_y</a></code> for <code class="reqn">L^{-1}y</code> computation, 
<code><a href="#topic+Get_L_t_y">Get_L_t_y</a></code> for <code class="reqn">L^T y</code> computation, 
<code><a href="#topic+Get_L_y">Get_L_y</a></code> for <code class="reqn">L y</code> computation.
</p>

<hr>
<h2 id='Get_L_t_y'> 
The multiplication of the transpose of L with y
</h2><span id='topic+Get_L_t_y'></span>

<h3>Description</h3>

<p>This function computes the product of the transpose of the L matrix and the output vector, where L is the Cholesky decomposition of the correlation matrix R. Instead of explicitly forming the Cholesky matrix, this function uses the dynamic linear model (DLM) forward filtering algorithm for efficient computation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Get_L_t_y(GG, Q, K, output)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Get_L_t_y_+3A_gg">GG</code></td>
<td>
<p>a list of matrices defined in the dynamic linear model.</p>
</td></tr>
<tr><td><code id="Get_L_t_y_+3A_q">Q</code></td>
<td>
<p>a vector defined in the dynamic linear model.</p>
</td></tr> 
<tr><td><code id="Get_L_t_y_+3A_k">K</code></td>
<td>
<p>a matrix defined in the filtering algorithm for the dynamic linear model.</p>
</td></tr>
<tr><td><code id="Get_L_t_y_+3A_output">output</code></td>
<td>
<p>a vector of observations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector representing the product of the transpose of the L matrix and the output vector, where L is the Cholesky decomposition of the correlation matrix.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal Gaussian process regression models</em>. <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>, 379-384.
</p>
<p>Fang, X., &amp; Gu, M. (2024). <em>The inverse Kalman filter</em>. arXiv:2407.10089.
</p>
<p>M. Gu, Y. Xu (2019), <em>Fast nonseparable Gaussian stochastic process with application to methylation level interpolation</em>. <em>Journal of Computational and Graphical Statistics</em>, In Press, arXiv:1711.11501.
</p>
<p>Campagnoli P, Petris G, Petrone S. (2009), <em>Dynamic linear model with R</em>. Springer-Verlag New York.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Get_Q_K">Get_Q_K</a></code> for more details about <code class="reqn">Q</code> vector and <code class="reqn">K</code> matrix,
<code><a href="#topic+Get_L_inv_y">Get_L_inv_y</a></code> for <code class="reqn">L^{-1}y</code> computation, 
<code><a href="#topic+Get_L_y">Get_L_y</a></code> for <code class="reqn">L y</code> computation, 
<code><a href="#topic+Get_L_t_inv_y">Get_L_t_inv_y</a></code> for <code class="reqn">(L^T)^{-1}y</code> computation.
</p>

<hr>
<h2 id='Get_L_y'> 
The multiplication of L with y
</h2><span id='topic+Get_L_y'></span>

<h3>Description</h3>

<p>This function computes the product of the L matrix and the output vector, where L is the Cholesky decomposition of the correlation matrix R. Instead of explicitly forming the Cholesky matrix, this function uses the dynamic linear model (DLM) forward filtering algorithm for efficient computation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Get_L_y(GG, Q, K, output)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Get_L_y_+3A_gg">GG</code></td>
<td>
<p>a list of matrices defined in the dynamic linear model.</p>
</td></tr>
<tr><td><code id="Get_L_y_+3A_q">Q</code></td>
<td>
<p>a vector defined in the dynamic linear model.</p>
</td></tr> 
<tr><td><code id="Get_L_y_+3A_k">K</code></td>
<td>
<p>a matrix defined in the filtering algorithm for the dynamic linear model.</p>
</td></tr>
<tr><td><code id="Get_L_y_+3A_output">output</code></td>
<td>
<p>a vector of observations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector representing the product of the L matrix and the output vector, where L is the Cholesky decomposition of the correlation matrix.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal Gaussian process regression models</em>. <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>, 379-384.
</p>
<p>Fang, X., &amp; Gu, M. (2024). <em>The inverse Kalman filter</em>. arXiv:2407.10089.
</p>
<p>M. Gu, Y. Xu (2019), <em>Fast nonseparable Gaussian stochastic process with application to methylation level interpolation</em>. <em>Journal of Computational and Graphical Statistics</em>, In Press, arXiv:1711.11501.
</p>
<p>Campagnoli P, Petris G, Petrone S. (2009), <em>Dynamic linear model with R</em>. Springer-Verlag New York.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Get_Q_K">Get_Q_K</a></code> for more details on <code class="reqn">K</code> and <code class="reqn">Q</code> matrices,
<code><a href="#topic+Get_L_inv_y">Get_L_inv_y</a></code> for <code class="reqn">L^{-1}y</code> computation, 
<code><a href="#topic+Get_L_t_y">Get_L_t_y</a></code> for <code class="reqn">L^T y</code> computation, 
<code><a href="#topic+Get_L_t_inv_y">Get_L_t_inv_y</a></code> for <code class="reqn">(L^T)^{-1}y</code> computation.
</p>

<hr>
<h2 id='Get_log_det_S2'> 
the natural logarithm of the determinant of the correlation matrix and the estimated sum of squares in the exponent of the profile likelihood
</h2><span id='topic+Get_log_det_S2'></span>

<h3>Description</h3>

<p>This function computes the natural logarithm of the determinant of the correlation matrix and the estimated sum of squares for computing the profile likelihood. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Get_log_det_S2(param,have_noise,delta_x,output,kernel_type)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Get_log_det_S2_+3A_param">param</code></td>
<td>
<p>a vector of parameters. The first parameter is the natural logarithm of the inverse range parameter in the kernel function. If the data contain noise, the second parameter is the logarithm of the nugget-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="Get_log_det_S2_+3A_have_noise">have_noise</code></td>
<td>
<p>a bool value. If it is true, it means the model contains a noise. </p>
</td></tr>
<tr><td><code id="Get_log_det_S2_+3A_delta_x">delta_x</code></td>
<td>
<p>a vector with dimension (num_obs-1) x 1 for the differences between the sorted input locations.</p>
</td></tr>
<tr><td><code id="Get_log_det_S2_+3A_output">output</code></td>
<td>
<p>a  vector with dimension num_obs x 1 for the observations at the sorted input locations.</p>
</td></tr>
<tr><td><code id="Get_log_det_S2_+3A_kernel_type">kernel_type</code></td>
<td>
<p>A <code>character</code> specifying the type of kernel.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list where the first value is the natural logarithm of the determinant of the correlation matrix and the second value is the estimated sum of squares.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>,  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2017), <em>Nonseparable Gaussian stochastic process: a unified
view and computational strategy</em>, arXiv:1711.11501.
</p>
<p>M. Gu, X. Wang and J.O. Berger (2018), <em>Robust Gaussian Stochastic Process Emulation</em>, <em>Annals of Statistics</em>, <b>46</b>, 3038-3066.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+log_lik">log_lik</a></code> for more details about the profile likelihood.
</p>

<hr>
<h2 id='Get_Q_K'> 
one-step-ahead predictive variance and Kalman gain
</h2><span id='topic+Get_Q_K'></span>

<h3>Description</h3>

<p>This function computes the one-step-ahead predictive variance and Kalman gain.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Get_Q_K(GG,W,C0,VV)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Get_Q_K_+3A_gg">GG</code></td>
<td>
<p>a list of matrices defined in the dynamic linear model.</p>
</td></tr>
<tr><td><code id="Get_Q_K_+3A_w">W</code></td>
<td>
<p>a list of matrices defined in the dynamic linear model.</p>
</td></tr>
<tr><td><code id="Get_Q_K_+3A_c0">C0</code></td>
<td>
<p>a matrix defined in the dynamic linear model.</p>
</td></tr>
<tr><td><code id="Get_Q_K_+3A_vv">VV</code></td>
<td>
<p>a numerical value for the nugget.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of 2 items for Q and K. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>.  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2019), <em>fast nonseparable gaussian stochastic process with application to methylation level interpolation</em>.  <em>Journal of Computational and Graphical Statistics</em>, In Press, arXiv:1711.11501.
</p>
<p>Campagnoli P, Petris G, Petrone S. (2009), <em>Dynamic linear model with R</em>. Springer-Verlag New York.
</p>

<hr>
<h2 id='Get_R_y'> 
The multiplication of R with y
</h2><span id='topic+Get_R_y'></span>

<h3>Description</h3>

<p>This function computes the product of the R matrix and the output vector, where R is the correlation matrix for a dynamic linear model (DLM). Instead of explicitly forming the Cholesky decomposition of R, this function computes the product as <code class="reqn">L (L^T y)</code>, where <code class="reqn">L</code> is the Cholesky decomposition of R. This is achieved using the forward filtering algorithm for efficient computation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Get_R_y(GG, Q, K, output)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Get_R_y_+3A_gg">GG</code></td>
<td>
<p>a list of matrices defined in the dynamic linear model.</p>
</td></tr>
<tr><td><code id="Get_R_y_+3A_q">Q</code></td>
<td>
<p>a vector defined in the dynamic linear model.</p>
</td></tr> 
<tr><td><code id="Get_R_y_+3A_k">K</code></td>
<td>
<p>a matrix defined in the filtering algorithm for the dynamic linear model.</p>
</td></tr>
<tr><td><code id="Get_R_y_+3A_output">output</code></td>
<td>
<p>a vector of observations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector representing the product of the R matrix and the output vector, where <code class="reqn">R</code> is the correlation matrix for a dynamic linear model.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal Gaussian process regression models</em>. <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>, 379-384.
</p>
<p>Fang, X., &amp; Gu, M. (2024). <em>The inverse Kalman filter</em>. arXiv:2407.10089.
</p>
<p>M. Gu, Y. Xu (2019), <em>Fast nonseparable Gaussian stochastic process with application to methylation level interpolation</em>. <em>Journal of Computational and Graphical Statistics</em>, In Press, arXiv:1711.11501.
</p>
<p>Campagnoli P, Petris G, Petrone S. (2009), <em>Dynamic linear model with R</em>. Springer-Verlag New York.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Get_Q_K">Get_Q_K</a></code> for more details on <code class="reqn">K</code> and <code class="reqn">Q</code> matrices, 
<code><a href="#topic+Get_L_inv_y">Get_L_inv_y</a></code> for <code class="reqn">L^{-1}y</code> computation, 
<code><a href="#topic+Get_L_t_y">Get_L_t_y</a></code> for <code class="reqn">L^T y</code> computation, 
<code><a href="#topic+Get_L_y">Get_L_y</a></code> for <code class="reqn">L y</code> computation, 
<code><a href="#topic+Get_L_t_inv_y">Get_L_t_inv_y</a></code> for <code class="reqn">(L^T)^{-1}y</code> computation.
</p>

<hr>
<h2 id='gppca'>Setting up the GPPCA model</h2><span id='topic+gppca'></span><span id='topic+gppca-method'></span>

<h3>Description</h3>

<p>Creating an <code>gppca</code> class for generalized probabilistic 
principal component analysis of correlated data.</p>


<h3>Usage</h3>

<pre><code class='language-R'>gppca(input,output, d, est_d=FALSE, shared_params=TRUE, kernel_type="matern_5_2")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gppca_+3A_input">input</code></td>
<td>
<p>a vector for the sorted input locations. The length is equal to the number of observations.</p>
</td></tr>
<tr><td><code id="gppca_+3A_output">output</code></td>
<td>
<p>a k*d matrix for the observations at the sorted input locations. Here k is the number of locations and n is the number of observations.</p>
</td></tr>
<tr><td><code id="gppca_+3A_d">d</code></td>
<td>
<p>number of latent factors.</p>
</td></tr>
<tr><td><code id="gppca_+3A_est_d">est_d</code></td>
<td>
<p>a bool value, default is <code>FALSE</code>. If <code>TRUE</code>, d will be estimated by either variance matching (when noise level is given) or information criteria (when noise level is unknown). Otherwise, d is fixed, and users must assign a value to argument d.</p>
</td></tr>
<tr><td><code id="gppca_+3A_shared_params">shared_params</code></td>
<td>
<p>a bool value, default is <code>TRUE</code>. If <code>TRUE</code>, the latent processes share the correlation and variance parameters. Otherwise, each latent process has distinct parameters.</p>
</td></tr>
<tr><td><code id="gppca_+3A_kernel_type">kernel_type</code></td>
<td>
<p>a <code>character</code> to specify the type of kernel to use. The current version supports kernel_type to be &quot;matern_5_2&quot; or &quot;exponential&quot;, meaning that the matern kernel with roughness parameter being 2.5 or 0.5 (exponent kernel), respectively. </p>
</td></tr>

</table>


<h3>Value</h3>

<p><code>gppca</code> returns an S4 object of class <code>gppca</code>.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Gu, M., &amp; Shen, W. (2020), Generalized probabilistic principal component analysis of correlated data, <em>Journal of Machine Learning Research, 21(13)</em>, 1-41.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(FastGaSP)
library(rstiefel)

matern_5_2_funct &lt;- function(d, beta_i) {
  cnst &lt;- sqrt(5.0)
  matOnes &lt;- matrix(1, nrow = nrow(d), ncol = ncol(d))
  result &lt;- cnst * beta_i * d
  res &lt;- (matOnes + result + (result^2) / 3) * exp(-result)
  return(res)
}

n=200
k=8
d=4

beta_real=0.01
sigma_real=1
sigma_0_real=sqrt(.01)
input=seq(1,n,1)
R0_00=as.matrix(abs(outer(input,input,'-')))
R_r = matern_5_2_funct(R0_00, beta_real)
L_sample = t(chol(R_r))
kernel_type='matern_5_2'


input=sort(input)
delta_x=input[2:length(input)]-input[1:(length(input)-1)]
A=rustiefel(k, d)  ##sample from Stiefel manifold
Factor=matrix(0,d,n)
for(i in 1: d){
  Factor[i,]=sigma_real^2*L_sample%*%rnorm(n)
}
output=A
  
##constucting the gppca.model
gppca_obj &lt;- gppca(input, output, d, shared_params = TRUE,est_d=FALSE)

</code></pre>

<hr>
<h2 id='gppca-class'>GPPCA class</h2><span id='topic+gppca-class'></span>

<h3>Description</h3>

<p>An S4 class for generalized probabilistic principal component analysis of correlated data.
</p>


<h3>Objects from the Class</h3>

<p>Objects of this class are created and initialized using the <code><a href="#topic+gppca">gppca</a></code> function to set up the estimation.
</p>


<h3>Slots</h3>


<dl>
<dt><code>input</code>:</dt><dd><p>object of class <code>vector</code>, the length is equivalent to the number of observations.</p>
</dd>
<dt><code>output</code>:</dt><dd><p>object of class <code>matrix</code>. The observation matrix.</p>
</dd>
<dt><code>d</code>:</dt><dd><p>object of class <code>integer</code> to specify the number of latent factors.</p>
</dd>
<dt><code>est_d</code>:</dt><dd><p>object of class <code>logical</code>, default is <code>FALSE</code>. If <code>TRUE</code>, d will be estimated by either variance matching (when noise level is given) or information criteria (when noise level is unknown). Otherwise, d is fixed, and users must assign a value to <code>d</code>.</p>
</dd>
<dt><code>shared_params</code>:</dt><dd><p>object of class <code>logical</code>, default is <code>TRUE</code>. If <code>TRUE</code>, the latent processes share the correlation and variance parameters. Otherwise, each latent process has distinct parameters.</p>
</dd>
<dt><code>kernel_type</code>:</dt><dd><p>a <code>character</code> to specify the type of kernel to use. The current version supports kernel_type to be &quot;matern_5_2&quot; or &quot;exponential&quot;, meaning that the matern kernel with roughness parameter being 2.5 or 0.5 (exponent kernel), respectively. </p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>fit.gppca</dt><dd><p>See <code><a href="#topic+fit.gppca">fit.gppca</a></code> for details.</p>
</dd>
<dt>predict.gppca</dt><dd><p>See <code><a href="#topic+predict.gppca">predict.gppca</a></code> for details.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Gu, M., &amp; Shen, W. (2020), Generalized probabilistic principal component analysis of correlated data, <em>Journal of Machine Learning Research, 21</em>(13), 1-41.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+gppca">gppca</a></code> for more details about how to create a <code>gppca</code> object.</p>

<hr>
<h2 id='IKF_CG_particle'>IKF-CG algorithm for one-interaction physical model with 1D output</h2><span id='topic+IKF_CG_particle'></span>

<h3>Description</h3>

<p>Implements the IKF-CG (Inverse Kalman Filter - Conjugate Gradient) algorithm for the one-interaction physical model with 1D output. This function provides an efficient computational method for large-scale particle systems.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>IKF_CG_particle(param, kernel_type, delta_x_all, output, A_all_v, 
                sort_d_all_ix, num_neighbors_vec, tilde_nu,
                D, N, tol = 1e-6, maxIte = 1000)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="IKF_CG_particle_+3A_param">param</code></td>
<td>
<p>A numeric vector containing model parameters. The first element is the log of beta (inverse range parameter), and the second element is the log of tau (variance ratio parameter).</p>
</td></tr>
<tr><td><code id="IKF_CG_particle_+3A_kernel_type">kernel_type</code></td>
<td>
<p>A string specifying the kernel type. Must be either &quot;matern_5_2&quot; or &quot;exp&quot;.</p>
</td></tr>
<tr><td><code id="IKF_CG_particle_+3A_delta_x_all">delta_x_all</code></td>
<td>
<p>A numeric vector of successive differences of sorted inputs.</p>
</td></tr>
<tr><td><code id="IKF_CG_particle_+3A_output">output</code></td>
<td>
<p>A numeric vector representing the output values.</p>
</td></tr>
<tr><td><code id="IKF_CG_particle_+3A_a_all_v">A_all_v</code></td>
<td>
<p>A numeric vector containing the non-zero entries in matrix A.</p>
</td></tr>
<tr><td><code id="IKF_CG_particle_+3A_sort_d_all_ix">sort_d_all_ix</code></td>
<td>
<p>An integer vector containing sorted indices for distances.</p>
</td></tr>
<tr><td><code id="IKF_CG_particle_+3A_num_neighbors_vec">num_neighbors_vec</code></td>
<td>
<p>An integer vector specifying the number of neighbors for each particle.</p>
</td></tr>
<tr><td><code id="IKF_CG_particle_+3A_tilde_nu">tilde_nu</code></td>
<td>
<p>A numeric value representing the stabilizing parameter.</p>
</td></tr>
<tr><td><code id="IKF_CG_particle_+3A_d">D</code></td>
<td>
<p>An integer specifying the dimension of the output vector per particle.</p>
</td></tr>
<tr><td><code id="IKF_CG_particle_+3A_n">N</code></td>
<td>
<p>An integer specifying the total dimension of the output vector.</p>
</td></tr>
<tr><td><code id="IKF_CG_particle_+3A_tol">tol</code></td>
<td>
<p>A numeric value specifying the convergence tolerance. Default is 1e-6.</p>
</td></tr>
<tr><td><code id="IKF_CG_particle_+3A_maxite">maxIte</code></td>
<td>
<p>An integer specifying the maximum number of iterations. Default is 1000.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list containing three elements:
</p>
<table role = "presentation">
<tr><td><code>x</code></td>
<td>
<p>A numeric vector containing the estimated state variables.</p>
</td></tr>
<tr><td><code>resid</code></td>
<td>
<p>A numeric vector containing the residuals at each iteration.</p>
</td></tr>
<tr><td><code>ite</code></td>
<td>
<p>An integer indicating the number of iterations performed.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Fang, X., &amp; Gu, M. (2024). <em>The inverse Kalman filter</em>. arXiv:2407.10089.
</p>

<hr>
<h2 id='IKF_CG_particle_cell'>Inverse Kalman Filter with Conjugate Gradient for Particle Systems</h2><span id='topic+IKF_CG_particle_cell'></span>

<h3>Description</h3>

<p>Implements the IKF-CG (Inverse Kalman Filter - Conjugate Gradient) algorithm for the one-interaction physical model with 1D output, different particle numbers for different time, and non-identity diagonal noise. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>IKF_CG_particle_cell(param, kernel_type, delta_x_all, output, A_all_v, 
                     sort_d_all_ix, sigma_2_vec, num_neighbors_vec, 
                     tilde_nu, D, n_t_record, tol = 1e-6, maxIte = 1000)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="IKF_CG_particle_cell_+3A_param">param</code></td>
<td>
<p>A numeric vector containing model parameters. The first element is the log of beta (inverse range parameter), and the second element is the log of tau (variance ratio parameter).</p>
</td></tr>
<tr><td><code id="IKF_CG_particle_cell_+3A_kernel_type">kernel_type</code></td>
<td>
<p>A string specifying the covariance kernel type: &quot;matern_5_2&quot; or &quot;exp&quot;.</p>
</td></tr>
<tr><td><code id="IKF_CG_particle_cell_+3A_delta_x_all">delta_x_all</code></td>
<td>
<p>A numeric vector of successive differences of sorted inputs.</p>
</td></tr>
<tr><td><code id="IKF_CG_particle_cell_+3A_output">output</code></td>
<td>
<p>A numeric vector of observations.</p>
</td></tr>
<tr><td><code id="IKF_CG_particle_cell_+3A_a_all_v">A_all_v</code></td>
<td>
<p>A numeric vector containing the non-zero entries in matrix A.</p>
</td></tr>
<tr><td><code id="IKF_CG_particle_cell_+3A_sort_d_all_ix">sort_d_all_ix</code></td>
<td>
<p>An integer vector of indices for sorting distances.</p>
</td></tr>
<tr><td><code id="IKF_CG_particle_cell_+3A_sigma_2_vec">sigma_2_vec</code></td>
<td>
<p>A numeric vector of variances for each time point.</p>
</td></tr>
<tr><td><code id="IKF_CG_particle_cell_+3A_num_neighbors_vec">num_neighbors_vec</code></td>
<td>
<p>An integer vector specifying number of neighbors for each observation.</p>
</td></tr>
<tr><td><code id="IKF_CG_particle_cell_+3A_tilde_nu">tilde_nu</code></td>
<td>
<p>A numeric value for numerical stabilization.</p>
</td></tr>
<tr><td><code id="IKF_CG_particle_cell_+3A_d">D</code></td>
<td>
<p>An integer specifying the dimension of observations.</p>
</td></tr>
<tr><td><code id="IKF_CG_particle_cell_+3A_n_t_record">n_t_record</code></td>
<td>
<p>An integer vector containing number of particles at each time point.</p>
</td></tr>
<tr><td><code id="IKF_CG_particle_cell_+3A_tol">tol</code></td>
<td>
<p>A numeric value specifying convergence tolerance (default: 1e-6).</p>
</td></tr>
<tr><td><code id="IKF_CG_particle_cell_+3A_maxite">maxIte</code></td>
<td>
<p>An integer specifying maximum number of iterations (default: 1000).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list containing:
</p>
<table role = "presentation">
<tr><td><code>x</code></td>
<td>
<p>A numeric vector of solved coefficients.</p>
</td></tr>
<tr><td><code>resid</code></td>
<td>
<p>A numeric vector of residuals at each iteration.</p>
</td></tr>
<tr><td><code>ite</code></td>
<td>
<p>An integer indicating the number of iterations performed.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Fang, X., &amp; Gu, M. (2024). <em>The inverse Kalman filter</em>. arXiv:2407.10089.
</p>

<hr>
<h2 id='Kalman_smoother'> 
the predictive mean and predictive variance by Kalman Smoother
</h2><span id='topic+Kalman_smoother'></span>

<h3>Description</h3>

<p>This function computes the predictive mean and predictive variance on the sorted input and testing input  by the Kalman Smoother.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Kalman_smoother(param,have_noise,index_obs,delta_x_all,output,sigma_2_hat,kernel_type)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Kalman_smoother_+3A_param">param</code></td>
<td>
<p>a vector of parameters. The first parameter is the natural logarithm of the inverse range parameter in the kernel function. If the data contain noise, the second parameter is the logarithm of the nugget-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="Kalman_smoother_+3A_have_noise">have_noise</code></td>
<td>
<p>a bool value. If it is true, it means the model contains a noise. 
</p>
</td></tr>
<tr><td><code id="Kalman_smoother_+3A_index_obs">index_obs</code></td>
<td>
<p>a vector  where the entries with 1 have observations and entries with 0 have no observation.</p>
</td></tr>
<tr><td><code id="Kalman_smoother_+3A_delta_x_all">delta_x_all</code></td>
<td>
<p>a vector for the differences between the sorted input and testing input locations.</p>
</td></tr>
<tr><td><code id="Kalman_smoother_+3A_output">output</code></td>
<td>
<p>a  vector with dimension num_obs x 1 for the observations at the sorted input locations.</p>
</td></tr>
<tr><td><code id="Kalman_smoother_+3A_sigma_2_hat">sigma_2_hat</code></td>
<td>
<p>a numerical value of variance parameter of the covariance function.</p>
</td></tr>
<tr><td><code id="Kalman_smoother_+3A_kernel_type">kernel_type</code></td>
<td>
<p>A <code>character</code> specifying the type of kernel.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list where the first item is the the predictive mean and the second item is predictive variance on the sorted input and testing input  by the Kalman Smoother.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>,  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2017), <em>Nonseparable Gaussian stochastic process: a unified
view and computational strategy</em>, arXiv:1711.11501.
</p>
<p>M. Gu, X. Wang and J.O. Berger (2018), <em>Robust Gaussian Stochastic Process Emulation</em>, <em>Annals of Statistics</em>, <b>46</b>, 3038-3066.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict">predict</a></code> for more details about the prediction on the testing input by the Fast GaSP class.
</p>

<hr>
<h2 id='log_lik'> 
Natural logarithm of profile likelihood by the fast computing algorithm</h2><span id='topic+log_lik'></span>

<h3>Description</h3>

<p>This function computes the natural logarithm of the profile likelihood for the range and nugget parameter (if there is one) after plugging the closed form maximum likelihood estimator for the variance parameter. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_lik(param, object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="log_lik_+3A_param">param</code></td>
<td>
<p>a vector of parameters. The first parameter is the natural logarithm of the inverse range parameter in the kernel function. If the data contain noise, the second parameter is the logarithm of the nugget-variance ratio parameter.</p>
</td></tr>
<tr><td><code id="log_lik_+3A_object">object</code></td>
<td>
<p>an object of  class <code>fgasp</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The numerical value of natural logarithm of the profile likelihood.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>,  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2017), <em>Nonseparable Gaussian stochastic process: a unified
view and computational strategy</em>, arXiv:1711.11501.
</p>
<p>M. Gu, X. Wang and J.O. Berger (2018), <em>Robust Gaussian Stochastic Process Emulation</em>, <em>Annals of Statistics</em>, <b>46</b>, 3038-3066.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(FastGaSP)
#--------------------------------------------------------------------------
# Example 1: comparing the fast and slow algorithms to compute the likelihood 
# of the Gaussian stochastic process for data with noises
#--------------------------------------------------------------------------

y_R&lt;-function(x){
  sin(2*pi*x)
}

###let's test for 1000 observations
set.seed(1)
num_obs=1000
input=runif(num_obs)
output=y_R(input)+rnorm(num_obs,mean=0,sd=0.1)

##constucting the fgasp.model
fgasp.model=fgasp(input, output)

##range and noise-variance ratio (nugget) parameters 
param=c( log(1),log(.02))
## the log lik
log_lik(param,fgasp.model)
##time cost to compute the likelihood
time_cost=system.time(log_lik(param,fgasp.model))
time_cost[1]

##now I am comparing to the one with straightforward inversion

matern_5_2_kernel&lt;-function(d,beta){  
  res=(1+sqrt(5)*beta*d + 5*beta^2*d^2/3 )*exp(-sqrt(5)*beta*d)
  res
}

##A function for computing the likelihood by the GaSP in a straightforward way
log_lik_GaSP_slow&lt;-function(param,have_noise=TRUE,input,output){
  n=length(output)
  beta=exp(param[1])
  eta=0
  if(have_noise){
    eta=exp(param[2])
  }
  R00=abs(outer(input,input,'-'))
  R=matern_5_2_kernel(R00,beta=beta)
  R_tilde=R+eta*diag(n)
  #use Cholesky and one backsolver and one forward solver so it is more stable
  L=t(chol(R_tilde))
  output_t_R.inv= t(backsolve(t(L),forwardsolve(L,output )))
  S_2=output_t_R.inv%*%output
  
  -sum(log(diag(L)))-n/2*log(S_2)
}



##range and noise-variance ratio (nugget) parameters 
param=c( log(1),log(.02))
## the log lik
log_lik(param,fgasp.model)
log_lik_GaSP_slow(param,have_noise=TRUE,input=input,output=output)

##time cost to compute the likelihood
##More number of inputs mean larger differences
time_cost=system.time(log_lik(param,fgasp.model))
time_cost

time_cost_slow=system.time(log_lik_GaSP_slow(param,have_noise=TRUE,input=input,output=output))
time_cost_slow


#--------------------------------------------------------------------------
# Example 2: comparing the fast and slow algorithms to compute the likelihood 
# of the Gaussian stochastic process for data without a noise
#--------------------------------------------------------------------------
## Here is a function in the Sobolev Space with order 3
y_R&lt;-function(x){
  j_seq=seq(1,200,1)
  record_y_R=0
  for(i_j in 1:200){
    record_y_R=record_y_R+2*j_seq[i_j]^{-2*3}*sin(j_seq[i_j])*cos(pi*(j_seq[i_j]-0.5)*x)

  }
  record_y_R
}


##generate some data without noise
num_obs=50
input=seq(0,1,1/(num_obs-1))

output=y_R(input)


##constucting the fgasp.model
fgasp.model=fgasp(input, output,have_noise=FALSE)

##range and noise-variance ratio (nugget) parameters 
param=c( log(1))
## the log lik
log_lik(param,fgasp.model)
log_lik_GaSP_slow(param,have_noise=FALSE,input=input,output=output)


</code></pre>

<hr>
<h2 id='particle.data-class'>Particle trajectory data class</h2><span id='topic+particle.data-class'></span><span id='topic+particle.data'></span>

<h3>Description</h3>

<p>S4 class for storing and analyzing particle trajectory data from both simulations and experimental observations. This class supports different models including Vicsek and can handle both position and velocity data along with optional angle information and particle tracking capabilities for experimental data.
</p>


<h3>Objects from the Class</h3>

<p>Objects of this class can be created in two ways:
</p>

<ul>
<li><p> For simulation data: Using <code><a href="#topic+simulate_particle">simulate_particle</a></code> that computes particle trajectories under physical models
</p>
</li>
<li><p> For experimental data: Using <code><a href="#topic+trajectory_data">trajectory_data</a></code> to save particle trajectories while handling varying numbers of particles between time steps
</p>
</li></ul>



<h3>Slots</h3>


<dl>
<dt><code>px_list</code>:</dt><dd><p>Object of class <code>list</code>. List of x-positions at each time step.</p>
</dd>
<dt><code>py_list</code>:</dt><dd><p>Object of class <code>list</code>. List of y-positions at each time step.</p>
</dd>
<dt><code>vx_list</code>:</dt><dd><p>Object of class <code>list</code>. List of x-velocities at each time step.</p>
</dd>
<dt><code>vy_list</code>:</dt><dd><p>Object of class <code>list</code>. List of y-velocities at each time step.</p>
</dd>
<dt><code>theta_list</code>:</dt><dd><p>Object of class <code>listOrNULL</code>. Optional list of particle velocity angles at each time step.</p>
</dd>
<dt><code>particle_tracking</code>:</dt><dd><p>Object of class <code>listOrNULL</code>. List of data frames containing particle mappings between consecutive frames (primarily for experimental data).</p>
</dd>
<dt><code>data_type</code>:</dt><dd><p>Object of class <code>character</code>. Type of data: either &quot;simulation&quot; or &quot;experiment&quot;.</p>
</dd>
<dt><code>n_particles</code>:</dt><dd><p>Object of class <code>numeric</code>. Number of particles (constant for simulation data, or a vector recording the number of particles at each time step for experimental data).</p>
</dd>
<dt><code>T_time</code>:</dt><dd><p>Object of class <code>numeric</code>. Total number of time steps.</p>
</dd>
<dt><code>D_y</code>:</dt><dd><p>Object of class <code>numeric</code>. Dimension of the output space.</p>
</dd>
<dt><code>model</code>:</dt><dd><p>Object of class <code>characterOrNULL</code>. Type of particle interaction model (e.g., &quot;Vicsek&quot;). NULL for experimental data.</p>
</dd>
<dt><code>sigma_0</code>:</dt><dd><p>Object of class <code>numericOrNULL</code>. Noise variance parameter used in the model. NULL for experimental data.</p>
</dd>
<dt><code>radius</code>:</dt><dd><p>Object of class <code>numericOrNULL</code>. Interaction radius between particles. NULL for experimental data.</p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>show:</dt><dd><p>Method for displaying summary information about the particle.data object.</p>
</dd>
<dt>fit:</dt><dd><p>Method for fitting the latent factor model to data using the IKF-CG algorithm, which returns a <code>particle.est</code> object containing estimated parameters and predictions. See <code><a href="#topic+fit.particle.data">fit.particle.data</a></code> for detailed documentation.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Vicsek, T., Czirok, A., Ben-Jacob, E., Cohen, I., &amp; Shochet, O. (1995). <em>Novel type of phase transition in a system of self-driven particles</em>, <em>Physical Review Letters</em>, <b>75</b>(6), 1226.
</p>
<p>Chat'e, H., Ginelli, F., Gr'egoire, G., Peruani, F., &amp; Raynaud, F. (2008). <em>Modeling collective motion: variations on the Vicsek model</em>, <em>The European Physical Journal B</em>, <b>64</b>(3), 451-456.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+simulate_particle">simulate_particle</a></code> for simulating particle trajectories,
<code><a href="#topic+trajectory_data">trajectory_data</a></code> for saving experimantal particle trajectories,
<code><a href="#topic+fit.particle.data">fit.particle.data</a></code> for model fitting methods
</p>

<hr>
<h2 id='particle.est-class'>Particle interaction estimation class</h2><span id='topic+particle.est-class'></span><span id='topic+particle.est'></span>

<h3>Description</h3>

<p>S4 class for storing estimated parameters and predictions for particle interaction models. </p>


<h3>Objects from the Class</h3>

<p>Objects of this class are created by the <code><a href="#topic+fit.particle.data">fit.particle.data</a></code> (via <code>fit</code>) method when applied to <code><a href="#topic+particle.data-class">particle.data</a></code> objects to estimate interaction parameters and make predictions.
</p>


<h3>Slots</h3>


<dl>
<dt><code>data_type</code>:</dt><dd><p>Object of class <code>character</code>. Specifies the type of data (&quot;simulation&quot; or &quot;experiment&quot;).</p>
</dd>
<dt><code>model</code>:</dt><dd><p>Object of class <code>characterOrNULL</code>. Specifies the model type for simulation data (e.g., &quot;Vicsek&quot; or &quot;two_interactions_Vicsek&quot;). NULL for experimental data.</p>
</dd>
<dt><code>D_y</code>:</dt><dd><p>Object of class <code>numeric</code>. Dimension of the output space.</p>
</dd>
<dt><code>num_interaction</code>:</dt><dd><p>Object of class <code>numeric</code>. Number of interactions.</p>
</dd>
<dt><code>parameters</code>:</dt><dd><p>Object of class <code>numeric</code>. Vector of estimated parameters with length 2*D_y + 1:
</p>

<ul>
<li><p> First D_y elements: beta (inverse range parameters)
</p>
</li>
<li><p> Next D_y elements: tau (variance-noise ratios)
</p>
</li>
<li><p> Last element: interaction radius
</p>
</li></ul>

</dd>
<dt><code>sigma_2_0_est</code>:</dt><dd><p>Object of class <code>numeric</code>. Estimated noise variance.</p>
</dd>
<dt><code>predictions</code>:</dt><dd><p>object of class <code>listOrNULL</code>. Contains predicted means and 95% confidence intervals (lower and upper bounds) for the particle interactions if testing inputs are given.</p>
</dd>
<dt><code>training_data</code>:</dt><dd><p>Object of class <code>list</code>. Contains the training data used in the GP model, obtained using the estimated interaction radius.</p>
</dd>
<dt><code>gp_weights</code>:</dt><dd><p>Object of class <code>matrix</code>. Contains the weights from the GP computation (A^T_j Sigma_y^(-1) y) used for prediction, with each column corresponding to a type of interaction j.</p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>show:</dt><dd><p>Method for displaying summary information about the estimated parameters.</p>
</dd>
</dl>



<h3>References</h3>

<p>Fang, X., &amp; Gu, M. (2024). <em>The inverse Kalman filter</em>. arXiv:2407.10089.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fit.particle.data">fit.particle.data</a></code> for more details about how to create a <code>particle.est</code> object.
<code><a href="#topic+particle.data-class">particle.data-class</a></code> for the input data structure
</p>

<hr>
<h2 id='predict'> 
Prediction and uncertainty quantification on the testing input using a GaSP model.
</h2><span id='topic+predict'></span><span id='topic+predict.fgasp'></span><span id='topic+predict+2Cfgasp-method'></span>

<h3>Description</h3>

<p>This function computes the predictive mean and variance on the given testing input using a GaSP model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'fgasp'
predict(param,object, testing_input, var_data=TRUE, sigma_2=NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict_+3A_param">param</code></td>
<td>
<p>a vector of parameters. The first parameter is the natural logarithm of the inverse range parameter in the kernel function. If the data contain noise, the second parameter is the logarithm of the nugget-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="predict_+3A_object">object</code></td>
<td>
<p>an object of  class <code>fgasp</code>.
</p>
</td></tr>
<tr><td><code id="predict_+3A_testing_input">testing_input</code></td>
<td>
<p>a vector of testing input for prediction.
</p>
</td></tr>
<tr><td><code id="predict_+3A_var_data">var_data</code></td>
<td>
<p>a bool valueto decide whether the noise of the data is included for computing the posterior predictive variance.
</p>
</td></tr>
<tr><td><code id="predict_+3A_sigma_2">sigma_2</code></td>
<td>
<p>a numerical value specifying the variance of the kernel function. If given, the package uses this parameter for prediction.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The returned value is a S4 Class <code>predictobj.fgasp</code>. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>,  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2017), <em>Nonseparable Gaussian stochastic process: a unified
view and computational strategy</em>, arXiv:1711.11501.
</p>
<p>M. Gu, X. Wang and J.O. Berger (2018), <em>Robust Gaussian Stochastic Process Emulation</em>, <em>Annals of Statistics</em>, <b>46</b>, 3038-3066.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(FastGaSP)

#------------------------------------------------------------------------------
# Example 1: a simple example with noise to show fast computation algorithm 
#------------------------------------------------------------------------------

y_R&lt;-function(x){
  cos(2*pi*x)
}

###let's test for 2000 observations
set.seed(1)
num_obs=2000
input=runif(num_obs)

output=y_R(input)+rnorm(num_obs,mean=0,sd=0.1)

##constucting the fgasp.model
fgasp.model=fgasp(input, output)

##range and noise-variance ratio (nugget) parameters 
param=c( log(1),log(.02))
## the log lik
log_lik(param,fgasp.model)
##time cost to compute the likelihood
time_cost=system.time(log_lik(param,fgasp.model))
time_cost[1]

##consider a nonparametric regression setting 
##estimate the parameter by maximum likelihood estimation

est_all&lt;-optim(c(log(1),log(.02)),log_lik,object=fgasp.model,method="L-BFGS-B",
              control = list(fnscale=-1))  

##estimated log inverse range parameter and log nugget
est_all$par

##estimate variance 
est.var=Get_log_det_S2(est_all$par,fgasp.model@have_noise,fgasp.model@delta_x,
                          fgasp.model@output,fgasp.model@kernel_type)[[2]]/fgasp.model@num_obs
est.var

###1. Do some interpolation test 
num_test=5000
testing_input=runif(num_test) ##there are the input where you don't have observations
pred.model=predict(param=est_all$par,object=fgasp.model,testing_input=testing_input)

lb=pred.model@mean+qnorm(0.025)*sqrt(pred.model@var)
ub=pred.model@mean+qnorm(0.975)*sqrt(pred.model@var)

## calculate lb for the mean function
pred.model2=predict(param=est_all$par,object=fgasp.model,testing_input=testing_input,var_data=FALSE)
lb_mean_funct=pred.model2@mean+qnorm(0.025)*sqrt(pred.model2@var)
ub_mean_funct=pred.model2@mean+qnorm(0.975)*sqrt(pred.model2@var)

## plot the prediction
min_val=min(lb,output)
max_val=max(ub,output)

plot(pred.model@testing_input,pred.model@mean,type='l',col='blue',
     ylim=c(min_val,max_val),
     xlab='x',ylab='y')
polygon( c(pred.model@testing_input,rev(pred.model@testing_input)),
    c(lb,rev(ub)),col = "grey80", border = FALSE)
lines(pred.model@testing_input,pred.model@mean,type='l',col='blue')
lines(pred.model@testing_input,y_R(pred.model@testing_input),type='l',col='black')
lines(pred.model2@testing_input,lb_mean_funct,col='blue',lty=2)
lines(pred.model2@testing_input,ub_mean_funct,col='blue',lty=2)
lines(input,output,type='p',pch=16,col='black',cex=0.4) #one can plot data

legend("bottomleft", legend=c("predictive mean","95% predictive interval","truth"),
       col=c("blue","blue","black"), lty=c(1,2,1), cex=.8)


##mean square error for all inputs
mean((pred.model@mean- y_R(pred.model@testing_input))^2)
##coverage for the mean
length(which(y_R(pred.model@testing_input)&gt;lb_mean_funct &amp;
               y_R(pred.model@testing_input)&lt;ub_mean_funct))/pred.model@num_testing
##average length of the invertal for the mean
mean(abs(ub_mean_funct-lb_mean_funct))
##average length of the invertal for the data
mean(abs(ub-lb))

#---------------------------------------------------------------------------------
# Example 2: numerical comparison with the GaSP by inverting the covariance matrix
#---------------------------------------------------------------------------------
##matern correlation with smoothness parameter being 2.5
matern_5_2_kernel&lt;-function(d,beta){  
  res=(1+sqrt(5)*beta*d + 5*beta^2*d^2/3 )*exp(-sqrt(5)*beta*d)
  res
}

##A function for computing the likelihood by the GaSP in a straightforward way
log_lik_GaSP_slow&lt;-function(param,have_noise=TRUE,input,output){
  n=length(output)
  beta=exp(param[1])
  eta=0
  if(have_noise){
    eta=exp(param[2])
  }
  R00=abs(outer(input,input,'-'))
  R=matern_5_2_kernel(R00,beta=beta)
  R_tilde=R+eta*diag(n)
  #use Cholesky and one backsolver and one forward solver so it is more stable
  L=t(chol(R_tilde))
  output_t_R.inv= t(backsolve(t(L),forwardsolve(L,output )))
  S_2=output_t_R.inv%*%output
  
  -sum(log(diag(L)))-n/2*log(S_2)
}


pred_GaSP_slow&lt;-function(param,have_noise=TRUE,input,output,testing_input){
  beta=exp(param[1])
  R00=abs(outer(input,input,'-'))
  eta=0
  if(have_noise){
    eta=exp(param[2])
  }
  R=matern_5_2_kernel(R00,beta=beta)
  R_tilde=R+eta*diag(length(output))
  
  ##I invert it here but one can still use cholesky to make it more stable
  R_tilde_inv=solve(R_tilde)

  r0=abs(outer(input,testing_input,'-'))
  r=matern_5_2_kernel(r0,beta=beta)

  S_2=t(output)%*%(R_tilde_inv%*%output)

  sigma_2_hat=as.numeric(S_2/num_obs)

  pred_mean=t(r)%*%(R_tilde_inv%*%output)
  pred_var=rep(0,length(testing_input))
  
  
  for(i in 1:length(testing_input)){
    pred_var[i]=-t(r[,i])%*%R_tilde_inv%*%r[,i]
  }
  pred_var=pred_var+1+eta
  list=list()
  list$mean=pred_mean
  list$var=pred_var*sigma_2_hat
  list
}

##let's test sin function
y_R&lt;-function(x){
  sin(2*pi*x)
}


###let's test for 800 observations
set.seed(1)
num_obs=800
input=runif(num_obs)
output=y_R(input)+rnorm(num_obs,mean=0,sd=0.1)


##constucting the fgasp.model
fgasp.model=fgasp(input, output)

##range and noise-variance ratio (nugget) parameters 
param=c( log(1),log(.02))
## the log lik
log_lik(param,fgasp.model)
log_lik_GaSP_slow(param,have_noise=TRUE,input=input,output=output)

##time cost to compute the likelihood
##More number of inputs mean larger differences
time_cost=system.time(log_lik(param,fgasp.model))
time_cost

time_cost_slow=system.time(log_lik_GaSP_slow(param,have_noise=TRUE,input=input,output=output))
time_cost_slow

est_all&lt;-optim(c(log(1),log(.02)),log_lik,object=fgasp.model,method="L-BFGS-B",
              control = list(fnscale=-1))  
##estimated log inverse range parameter and log nugget
est_all$par


##Do some interpolation test for comparison
num_test=200
testing_input=runif(num_test) ##there are the input where you don't have observations

##one may sort the testing_input or not, and the prediction will be on the sorted testing_input
##testing_input=sort(testing_input)

## two ways of prediction
pred.model=predict(param=est_all$par,object=fgasp.model,testing_input=testing_input)
pred_slow=pred_GaSP_slow(param=est_all$par,have_noise=TRUE,input,output,sort(testing_input))
## look at the difference
max(abs(pred.model@mean-pred_slow$mean))
max(abs(pred.model@var-pred_slow$var))


</code></pre>

<hr>
<h2 id='predict.fmou'> 
Prediction and uncertainty quantification on the future observations using a FMOU model.</h2><span id='topic+predict.fmou'></span><span id='topic+predict.fmou+2Cfmou-method'></span>

<h3>Description</h3>

<p>This function predicts the future observations using a FMOU model. Uncertainty quantification is available. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'fmou'
predict.fmou(object, param_est, step=1, interval=FALSE, interval_data=TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.fmou_+3A_object">object</code></td>
<td>
<p>an objecft of  class <code>fmou.</code></p>
</td></tr>
<tr><td><code id="predict.fmou_+3A_param_est">param_est</code></td>
<td>
<p>estimated parameters in the FMOU model. Obtained by the results of <code>fit.fmou()</code>. </p>
</td></tr>
<tr><td><code id="predict.fmou_+3A_step">step</code></td>
<td>
<p>a vector. Number of steps to be predicted. Default is 1. </p>
</td></tr> 
<tr><td><code id="predict.fmou_+3A_interval">interval</code></td>
<td>
<p>a bool value, default is <code>FALSE</code>. If <code>TRUE</code>, the 95% predictive intervals are computed.</p>
</td></tr>
<tr><td><code id="predict.fmou_+3A_interval_data">interval_data</code></td>
<td>
<p>a bool value, default is <code>TRUE</code>. If <code>TRUE</code>, the 95% predictive intervals of the observations are computed. Otherwise, the 95% predictive intervals of the mean of the observation are computed.</p>
</td></tr> 
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>pred_mean</code></td>
<td>
<p>the predictive mean. </p>
</td></tr>
<tr><td><code>pred_interval_95lb</code></td>
<td>
<p>the 95% lower bound of the interval.</p>
</td></tr>
<tr><td><code>pred_interval_95ub</code></td>
<td>
<p>the 95% upper bound of the interval.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Lin, Y., Liu, X., Segall, P., &amp; Gu, M. (2025). Fast data inversion for high-dimensional dynamical systems from noisy measurements. arXiv preprint arXiv:2501.01324.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## generate simulated data
library(FastGaSP)
library(rstiefel)

d = 5  # number of latent factors
k = 20 # length of observation at each time step
n = 500 # number time step
noise_level = 1 # variance of noise

U = rustiefel(k, k) # factor loading matrix
z = matrix(NA, d, n)
sigma_2 = runif(d, 0.5, 1)
rho = runif(d, 0.95, 1)
for(l in 1:d){
  R = matrix(NA, n, n)
  diag(R) = 1
  for(ir in 1:n){
    for(ic in 1:n){
      R[ir, ic] = rho[l]^(abs(ir-ic)) * R[ir, ir]
    }
  }
  R = (sigma_2[l]/(1-rho[l]^2) )* R
  z[l, ] = t(chol(R)) %*% rnorm(n)
}

signal = U[,1:d] %*% z
y = signal + matrix(rnorm(n*k,mean=0,sd=sqrt(noise_level)),k,n)

##constucting the fmou.model
fmou.model=fmou(output=y, d=d, est_U0=TRUE, est_sigma0_2=TRUE)

## estimate the parameters
em_alg &lt;- fit.fmou(fmou.model, M=500)

## two-step-ahead prediction
pred_2step &lt;- predict.fmou(fmou.model,em_alg, step=c(1:2))

</code></pre>

<hr>
<h2 id='predict.gppca'> 
Prediction and uncertainty quantification on the future observations using GPPCA.</h2><span id='topic+predict.gppca'></span><span id='topic+predict.gppca+2Cgppca-method'></span>

<h3>Description</h3>

<p>This function predicts the future observations using a GPPCA model. Uncertainty quantification is available. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'gppca'
predict.gppca(object, param, A_hat, step=1, interval=FALSE, interval_data=TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.gppca_+3A_object">object</code></td>
<td>
<p>an object of  class <code>gppca.</code></p>
</td></tr>
<tr><td><code id="predict.gppca_+3A_param">param</code></td>
<td>
<p>estimated parameters (beta, sigma2, sigma0_2).</p>
</td></tr>
<tr><td><code id="predict.gppca_+3A_a_hat">A_hat</code></td>
<td>
<p>estimated factor loading matrix.</p>
</td></tr>
<tr><td><code id="predict.gppca_+3A_step">step</code></td>
<td>
<p>a vector. Number of steps to be predicted. Default is 1. </p>
</td></tr> 
<tr><td><code id="predict.gppca_+3A_interval">interval</code></td>
<td>
<p>a bool value, default is <code>FALSE</code>. If <code>TRUE</code>, the 95% predictive intervals are computed.</p>
</td></tr>
<tr><td><code id="predict.gppca_+3A_interval_data">interval_data</code></td>
<td>
<p>a bool value, default is <code>TRUE</code>. If <code>TRUE</code>, the 95% predictive intervals of the observations are computed. Otherwise, the 95% predictive intervals of the mean of the observation are computed.</p>
</td></tr> 
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>pred_mean</code></td>
<td>
<p>the predictive mean. </p>
</td></tr>
<tr><td><code>pred_interval_95lb</code></td>
<td>
<p>the 95% lower bound of the interval.</p>
</td></tr>
<tr><td><code>pred_interval_95ub</code></td>
<td>
<p>the 95% upper bound of the interval.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Gu, M., &amp; Shen, W. (2020), Generalized probabilistic principal component analysis of correlated data, <em>Journal of Machine Learning Research, 21</em>(13), 1-41.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(FastGaSP)
library(rstiefel)

matern_5_2_funct &lt;- function(d, beta_i) {
  cnst &lt;- sqrt(5.0)
  matOnes &lt;- matrix(1, nrow = nrow(d), ncol = ncol(d))
  result &lt;- cnst * beta_i * d
  res &lt;- (matOnes + result + (result^2) / 3) * exp(-result)
  return(res)
}

n=200
k=8
d=4

beta_real=0.01
sigma_real=1
sigma_0_real=sqrt(.01)
input=seq(1,n,1)
R0_00=as.matrix(abs(outer(input,input,'-')))
R_r = matern_5_2_funct(R0_00, beta_real)
L_sample = t(chol(R_r))
kernel_type='matern_5_2'


input=sort(input)
delta_x=input[2:length(input)]-input[1:(length(input)-1)]
A=rustiefel(k, d)  ##sample from Stiefel manifold
Factor=matrix(0,d,n)
for(i in 1: d){
  Factor[i,]=sigma_real^2*L_sample%*%rnorm(n)
}
output=A%*%Factor+matrix(rnorm(n*k,mean=0,sd=sigma_0_real),k,n)
  
##constucting the gppca.model
gppca_obj &lt;- gppca(input, output, d, shared_params = TRUE,est_d=FALSE)
## estimate the parameters
gppca_fit &lt;- fit.gppca(gppca_obj)

## two-step-ahead prediction
param &lt;- c(gppca_fit$est_beta, gppca_fit$est_sigma2, gppca_fit$est_sigma0_2)
gppca_pred &lt;- predict.gppca(gppca_obj, param, gppca_fit$est_A, step=1:3)
gppca_pred$pred_mean

</code></pre>

<hr>
<h2 id='predictobj.fgasp-class'>Predictive results for the Fast GaSP class </h2><span id='topic+predictobj.fgasp-class'></span><span id='topic+predictobj.fgasp'></span>

<h3>Description</h3>

<p>S4 class for prediction for a Fast GaSP model with or without a noise.</p>


<h3>Objects from the Class</h3>

<p>Objects of this class are created and initialized with the function <code><a href="#topic+predict">predict</a></code> that computes the prediction and the uncertainty quantification.</p>


<h3>Slots</h3>


<dl>
<dt><code>num_testing</code>:</dt><dd><p>object of class <code>integer</code>. Number of testing inputs.</p>
</dd>
<dt><code>testing_input</code>:</dt><dd><p>object of class <code>vector</code>. The testing input locations.</p>
</dd>
<dt>param</dt><dd><p>a vector of parameters. The first parameter is the natural logarithm of the inverse range parameter in the kernel function. If the data contain noise, the second parameter is the logarithm of the nugget-variance ratio parameter.
</p>
</dd>
<dt><code>mean</code>:</dt><dd><p>object of class <code>vector</code>. The predictive mean at testing inputs.</p>
</dd>
<dt><code>var</code>:</dt><dd><p>object of class <code>vector</code>. The predictive variance at testing inputs. If the <code>var_data</code> is true, the predictive variance of the data is calculated. Otherwise, the predictive variance of the mean is calculated. </p>
</dd>
<dt><code>var_data</code>:</dt><dd><p>object of class <code>logical</code>. If the <code>var_data</code> is true, the predictive variance of the data is calculated for <code>var</code>. Otherwise, the predictive variance of the mean is calculated for <code>var</code>. </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>,  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2017), <em>Nonseparable Gaussian stochastic process: a unified
view and computational strategy</em>, arXiv:1711.11501.
</p>
<p>M. Gu, X. Wang and J.O. Berger (2018), <em>Robust Gaussian Stochastic Process Emulation</em>, <em>Annals of Statistics</em>, <b>46</b>, 3038-3066.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.fgasp">predict.fgasp</a></code> for more details about how to do prediction for a <code>fgasp</code> object.
</p>

<hr>
<h2 id='Sample_KF'> 
Sample the prior process using a dynamic linear model  
</h2><span id='topic+Sample_KF'></span>

<h3>Description</h3>

<p>This function samples the piror process using a dynamic liner model. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Sample_KF(GG,W,C0,VV,kernel_type,sample_type)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Sample_KF_+3A_gg">GG</code></td>
<td>
<p>a list of matrices defined in the dynamic linear model. 
</p>
</td></tr>
<tr><td><code id="Sample_KF_+3A_w">W</code></td>
<td>
<p>a list of coefficient matrices defined in the dynamic linear model. 
</p>
</td></tr>
<tr><td><code id="Sample_KF_+3A_c0">C0</code></td>
<td>
<p>the covariance matrix of the stationary distribution defined in the dynamic linear model. 
</p>
</td></tr>
<tr><td><code id="Sample_KF_+3A_vv">VV</code></td>
<td>
<p>a numerical value of the variance of the nugget parameter. </p>
</td></tr>
<tr><td><code id="Sample_KF_+3A_kernel_type">kernel_type</code></td>
<td>
<p>a <code>character</code> to specify the type of kernel to use. The current version supports kernel_type to be &quot;matern_5_2&quot; or &quot;exp&quot;, meaning that the matern kernel with roughness parameter being 2.5 or 0.5 (exponent kernel), respectively. </p>
</td></tr>
<tr><td><code id="Sample_KF_+3A_sample_type">sample_type</code></td>
<td>
<p>a integer to specify the type of sample we need. 0 means the states. 1 means the first value of each state vector. 2 means the noisy observations. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of the samples. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>.  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2019), <em>fast nonseparable gaussian stochastic process with application to methylation level interpolation</em>.  <em>Journal of Computational and Graphical Statistics</em>, In Press, arXiv:1711.11501.
</p>
<p>Campagnoli P, Petris G, Petrone S. (2009), <em>Dynamic linear model with R</em>. Springer-Verlag New York.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Sample_KF_post">Sample_KF_post</a></code> for more details about sampling from the posterior distribution.
</p>

<hr>
<h2 id='Sample_KF_post'> 
Sample the posterior distribution of the process using the backward smoothing algorithm  
</h2><span id='topic+Sample_KF_post'></span>

<h3>Description</h3>

<p>This function samples the posterior distribution of the process using the backward smoothing algorithm. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Sample_KF_post(index_obs, C_R_K_Q,W0,GG,W,VV,output,kernel_type,sample_type)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Sample_KF_post_+3A_index_obs">index_obs</code></td>
<td>
<p>a vector  where the entries with 1 have observations and entries with 0 have no observation.</p>
</td></tr>
<tr><td><code id="Sample_KF_post_+3A_c_r_k_q">C_R_K_Q</code></td>
<td>
<p>a list of matrices to compute the inverse covariance matrix in the dynamic linear model.</p>
</td></tr>
<tr><td><code id="Sample_KF_post_+3A_gg">GG</code></td>
<td>
<p>a list of matrices defined in the dynamic linear model. 
</p>
</td></tr>
<tr><td><code id="Sample_KF_post_+3A_w">W</code></td>
<td>
<p>a list of coefficient matrices defined in the dynamic linear model. 
</p>
</td></tr>
<tr><td><code id="Sample_KF_post_+3A_vv">VV</code></td>
<td>
<p>a numerical value of the variance of the nugget parameter. </p>
</td></tr>
<tr><td><code id="Sample_KF_post_+3A_output">output</code></td>
<td>
<p>a vector of the output. </p>
</td></tr>
<tr><td><code id="Sample_KF_post_+3A_kernel_type">kernel_type</code></td>
<td>
<p>a <code>character</code> to specify the type of kernel to use. The current version supports kernel_type to be &quot;matern_5_2&quot; or &quot;exp&quot;, meaning that the matern kernel with roughness parameter being 2.5 or 0.5 (exponent kernel), respectively. </p>
</td></tr>
<tr><td><code id="Sample_KF_post_+3A_sample_type">sample_type</code></td>
<td>
<p>a integer to specify the type of sample we need. 0 means the states. 1 means the first value of each state vector. 2 means the noisy observations. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of the posterior samples. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>.  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2019), <em>fast nonseparable gaussian stochastic process with application to methylation level interpolation</em>.  <em>Journal of Computational and Graphical Statistics</em>, In Press, arXiv:1711.11501.
</p>
<p>Campagnoli P, Petris G, Petrone S. (2009), <em>Dynamic linear model with R</em>. Springer-Verlag New York.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Sample_KF_post">Sample_KF_post</a></code> for more details about sampling from the posterior distribution.
</p>

<hr>
<h2 id='show.fgasp'>
Show an <code>fgasp</code> object.
</h2><span id='topic+show+2Cfgasp-method'></span>

<h3>Description</h3>

<p>Function to print the <code>fgasp</code> object. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'fgasp'
show(object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="show.fgasp_+3A_object">object</code></td>
<td>
<p>an object of  class <code>fgasp</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Xinyi Fang [aut],
  Yizi Lin [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>,  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2017), <em>Nonseparable Gaussian stochastic process: a unified
view and computational strategy</em>, arXiv:1711.11501.
</p>
<p>M. Gu, X. Wang and J.O. Berger (2018), <em>Robust Gaussian Stochastic Process Emulation</em>, <em>Annals of Statistics</em>, <b>46</b>, 3038-3066.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#-------------------------------------
# Example: a simple example with noise 
#-------------------------------------

y_R&lt;-function(x){
  cos(2*pi*x)
}

###let's test for 2000 observations
set.seed(1)
num_obs=2000
input=runif(num_obs)

output=y_R(input)+rnorm(num_obs,mean=0,sd=0.1)

##constucting the fgasp.model
fgasp.model=fgasp(input, output)
show(fgasp.model)


</code></pre>

<hr>
<h2 id='show.particle.data'>Show method for particle data class</h2><span id='topic+show.particle.data'></span><span id='topic+show+2Cparticle.data-method'></span>

<h3>Description</h3>

<p>Display method for objects of class <code>particle.data</code>, which prints a summary of the key parameters and characteristics of the particle system based on its data type (simulation or experimental).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'particle.data'
show(object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="show.particle.data_+3A_object">object</code></td>
<td>
<p>An object of class <code>particle.data</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method displays essential information about the particle system. The output varies based on the data type:
</p>
<p>For simulation data:
</p>

<ul>
<li><p> Type of particle interaction model
</p>
</li>
<li><p> Number of time steps
</p>
</li>
<li><p> Number of particles in the system
</p>
</li>
<li><p> Noise variance parameter (sigma_0)
</p>
</li>
<li><p> Interaction radius between particles
</p>
</li></ul>

<p>For experimental data:
</p>

<ul>
<li><p> Number of time steps
</p>
</li>
<li><p> Average number of particles across all time steps
</p>
</li></ul>



<h3>References</h3>

<p>Vicsek, T., Czirok, A., Ben-Jacob, E., Cohen, I., &amp; Shochet, O. (1995). <em>Novel type of phase transition in a system of self-driven particles</em>, <em>Physical Review Letters</em>, <b>75</b>(6), 1226.
</p>
<p>Chat'e, H., Ginelli, F., Gr'egoire, G., Peruani, F., &amp; Raynaud, F. (2008). <em>Modeling collective motion: variations on the Vicsek model</em>, <em>The European Physical Journal B</em>, <b>64</b>(3), 451-456.
</p>
<p>Fang, X., &amp; Gu, M. (2024). <em>The inverse Kalman filter</em>. arXiv:2407.10089.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+particle.data-class">particle.data-class</a></code> for the full class description,
<code><a href="#topic+simulate_particle">simulate_particle</a></code> for creating simulation data objects,
<code><a href="#topic+trajectory_data">trajectory_data</a></code> for creating experimental data objects
</p>

<hr>
<h2 id='show.particle.est'>Show method for particle estimation class</h2><span id='topic+show+2Cparticle.est-method'></span>

<h3>Description</h3>

<p>Display method for objects of class <code>particle.est</code>, which prints a summary of the estimated parameters for the particle interaction model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  ## S4 method for signature 'particle.est'
show(object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="show.particle.est_+3A_object">object</code></td>
<td>
<p>An object of class <code>particle.est</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method displays essential information about the estimated model parameters, including:
</p>

<ul>
<li><p> Data type (simulation or experiment)
</p>
</li>
<li><p> Model type (for simulation data only)
</p>
</li>
<li><p> Dimension of output space
</p>
</li>
<li><p> Estimated parameters:
</p>

<ul>
<li><p> beta parameters (inverse range)
</p>
</li>
<li><p> tau parameters (variance-noise ratio)
</p>
</li>
<li><p> interaction radius
</p>
</li></ul>

</li></ul>



<h3>References</h3>

<p>Fang, X., &amp; Gu, M. (2024). <em>The inverse Kalman filter</em>. arXiv:2407.10089.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+particle.est-class">particle.est-class</a></code> for details of the particle estimation class
</p>

<hr>
<h2 id='simulate_particle'>Simulate particle trajectories</h2><span id='topic+simulate_particle'></span>

<h3>Description</h3>

<p>Simulates particle trajectories using either the standard Vicsek model or a two-interaction variation of the Vicsek model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  simulate_particle(v_abs, n_t = 100, T_sim = 5, h = 0.1,
    cut_r = 0.5, sigma_0 = 0.1,
    noise_type = "Gaussian", model = "Vicsek")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="simulate_particle_+3A_v_abs">v_abs</code></td>
<td>
<p>Absolute velocity magnitude for all particles.</p>
</td></tr>
<tr><td><code id="simulate_particle_+3A_n_t">n_t</code></td>
<td>
<p>Number of particles (default: 100).</p>
</td></tr>
<tr><td><code id="simulate_particle_+3A_t_sim">T_sim</code></td>
<td>
<p>Total simulation time steps (default: 5).</p>
</td></tr>
<tr><td><code id="simulate_particle_+3A_h">h</code></td>
<td>
<p>Time step size for numerical integration (default: 0.1).</p>
</td></tr>
<tr><td><code id="simulate_particle_+3A_cut_r">cut_r</code></td>
<td>
<p>Radius of interaction between particles (default: 0.5).</p>
</td></tr>
<tr><td><code id="simulate_particle_+3A_sigma_0">sigma_0</code></td>
<td>
<p>Standard deviation of noise (default: 0.1).</p>
</td></tr>
<tr><td><code id="simulate_particle_+3A_noise_type">noise_type</code></td>
<td>
<p>Distribution of noise: &quot;Gaussian&quot; or &quot;Uniform&quot; (default: &quot;Gaussian&quot;).</p>
</td></tr>
<tr><td><code id="simulate_particle_+3A_model">model</code></td>
<td>
<p>Type of interaction model: &quot;Vicsek&quot; or &quot;two_interactions_Vicsek&quot; (default: &quot;Vicsek&quot;).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an S4 object of class <code><a href="#topic+particle.data-class">particle.data</a></code>. See <code><a href="#topic+particle.data-class">particle.data-class</a></code> for details of the returned object structure.
</p>


<h3>References</h3>

<p>Vicsek, T., Czirok, A., Ben-Jacob, E., Cohen, I., &amp; Shochet, O. (1995). <em>Novel type of phase transition in a system of self-driven particles</em>, <em>Physical Review Letters</em>, <b>75</b>(6), 1226.
</p>
<p>Chat'e, H., Ginelli, F., Gr'egoire, G., Peruani, F., &amp; Raynaud, F. (2008). <em>Modeling collective motion: variations on the Vicsek model</em>, <em>The European Physical Journal B</em>, <b>64</b>(3), 451-456.
</p>
<p>Fang, X., &amp; Gu, M. (2024). <em>The inverse Kalman filter</em>. arXiv:2407.10089.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+particle.data-class">particle.data-class</a></code> for details on the particle.data class structure
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#--------------------------------------------------
# Example: Simulate using standard Vicsek model
#--------------------------------------------------
vx_abs=0.5
vy_abs=0.5
v_abs=sqrt(vx_abs^2+vy_abs^2)
sim1 &lt;- simulate_particle(v_abs=v_abs)

#--------------------------------------------------
# Example: Simulate using two-interaction variation
#--------------------------------------------------
sim2 &lt;- simulate_particle(v_abs=v_abs, model = 'two_interactions_Vicsek')
</code></pre>

<hr>
<h2 id='trajectory_data'>Convert experimental particle tracking data to particle.data object</h2><span id='topic+trajectory_data'></span>

<h3>Description</h3>

<p>Processes experimental particle tracking data and creates a standardized particle.data object. This function handles time series data with varying numbers of particles across time steps and maintains particle identity tracking between consecutive frames.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trajectory_data(particle_data)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="trajectory_data_+3A_particle_data">particle_data</code></td>
<td>
<p>A data frame containing particle tracking data with the following required columns:
</p>

<ul>
<li> <p><code>px</code>, <code>py</code>: Particle positions in x and y coordinates
</p>
</li>
<li> <p><code>vx</code>, <code>vy</code>: Particle velocities in x and y directions
</p>
</li>
<li> <p><code>time</code>: Time step identifier (integer). Must be consecutive integers starting from the minimum time value
</p>
</li>
<li> <p><code>particleID</code>: Unique particle identifier for tracking across frames
</p>
</li></ul>

</td></tr>
</table>


<h3>Value</h3>

<p>Returns an S4 object of class <code><a href="#topic+particle.data-class">particle.data</a></code> with:
</p>

<dl>
<dt>px_list, py_list</dt><dd><p>Lists of particle x and y positions at each time step</p>
</dd>
<dt>vx_list, vy_list</dt><dd><p>Lists of particle x and y velocities at each time step</p>
</dd>
<dt>theta_list</dt><dd><p>List of particle angles computed from velocities</p>
</dd>
<dt>particle_tracking</dt><dd><p>List of data frames containing particle mappings between consecutive frames</p>
</dd>
<dt>data_type</dt><dd><p>&quot;experiment&quot;</p>
</dd>
<dt>n_particles</dt><dd><p>Vector recording number of particles at each time step</p>
</dd>
<dt>T_time</dt><dd><p>Total number of time steps</p>
</dd>
<dt>D_y</dt><dd><p>Dimension of the output space (set to 1)</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+particle.data-class">particle.data-class</a></code> for details on the particle.data class structure
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create sample tracking data
sample_data &lt;- data.frame(time = rep(1:3, each = 5),
                          particleID = rep(1:5, 3),
                          px = rnorm(15),py = rnorm(15),
                          vx = rnorm(15),vy = rnorm(15)
)
# Convert to particle.data object
traj &lt;- trajectory_data(sample_data)
# Display summary
show(traj)
</code></pre>

<hr>
<h2 id='Vicsek'>Vicsek Model Simulation</h2><span id='topic+Vicsek'></span>

<h3>Description</h3>

<p>Simulates particle movement according to the Vicsek model, where particles align their velocities with neighboring particles within a specified radius, subject to noise. The model implements both Gaussian and uniform noise options.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Vicsek(p0, v0, theta0, v_abs, n_t, T_sim, h, cut_r, 
       sigma_0, noise_type = "Gaussian")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Vicsek_+3A_p0">p0</code></td>
<td>
<p>A numeric vector of initial positions for all particles, structured as (x1, y1, x2, y2, ..., xn, yn).</p>
</td></tr>
<tr><td><code id="Vicsek_+3A_v0">v0</code></td>
<td>
<p>A numeric vector of initial velocities for all particles, structured similarly to p0.</p>
</td></tr>
<tr><td><code id="Vicsek_+3A_theta0">theta0</code></td>
<td>
<p>A numeric vector of initial angles for all particles.</p>
</td></tr>
<tr><td><code id="Vicsek_+3A_v_abs">v_abs</code></td>
<td>
<p>A numeric value specifying the absolute velocity (speed) of particles.</p>
</td></tr>
<tr><td><code id="Vicsek_+3A_n_t">n_t</code></td>
<td>
<p>An integer specifying the number of particles.</p>
</td></tr>
<tr><td><code id="Vicsek_+3A_t_sim">T_sim</code></td>
<td>
<p>An integer specifying the number of time steps to simulate.</p>
</td></tr>
<tr><td><code id="Vicsek_+3A_h">h</code></td>
<td>
<p>A numeric value specifying the time step size.</p>
</td></tr>
<tr><td><code id="Vicsek_+3A_cut_r">cut_r</code></td>
<td>
<p>A numeric value specifying the interaction radius within which particles align.</p>
</td></tr>
<tr><td><code id="Vicsek_+3A_sigma_0">sigma_0</code></td>
<td>
<p>A numeric value specifying the noise strength.</p>
</td></tr>
<tr><td><code id="Vicsek_+3A_noise_type">noise_type</code></td>
<td>
<p>A character string specifying the type of noise: either &quot;Gaussian&quot; (default) or &quot;Uniform&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with three components:
</p>
<table role = "presentation">
<tr><td><code>pos</code></td>
<td>
<p>A matrix of dimension (2*n_t) x (T_sim+1) containing particle positions at each time step.</p>
</td></tr>
<tr><td><code>v</code></td>
<td>
<p>A matrix of dimension (2*n_t) x (T_sim+1) containing particle velocities at each time step.</p>
</td></tr>
<tr><td><code>theta</code></td>
<td>
<p>A matrix of dimension n_t x (T_sim+1) containing particle angles at each time step.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Vicsek, T., Czirok, A., Ben-Jacob, E., Cohen, I., &amp; Shochet, O. (1995). <em>Novel type of phase transition in a system of self-driven particles</em>, <em>Physical Review Letters</em>, <b>75</b>(6), 1226.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
