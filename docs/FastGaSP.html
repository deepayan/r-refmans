<!DOCTYPE html><html><head><title>Help for package FastGaSP</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {FastGaSP}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Construct_G_exp'>
<p>The coefficient matrix in the dynamic linear model when kernel is the exponential covariance</p></a></li>
<li><a href='#Construct_G_matern_5_2'>
<p>The coefficient matrix in the dynamic linear model when kernel is the Matern covariance with roughness parameter 2.5.</p></a></li>
<li><a href='#Construct_W_exp'>
<p>The conditional covariance matrix of the state in the dynamic linear model when kernel is the exponential covariance</p></a></li>
<li><a href='#Construct_W_matern_5_2'>
<p>The conditional covariance matrix for  matern covariance with roughness parameter 2.5</p></a></li>
<li><a href='#Construct_W0_exp'>
<p>covariance of the stationary distribution of the state when kernel is the exponential covariance.</p></a></li>
<li><a href='#Construct_W0_matern_5_2'>
<p>covariance of the stationary distribution of the state when kernel is the Matern covariance with roughness parameter 2.5.</p></a></li>
<li><a href='#FastGaSP-package'>
<p>Fast and Exact Computation of Gaussian Stochastic Process</p></a></li>
<li><a href='#fgasp'><p> Setting up the Fast GaSP model</p>
</a></li>
<li><a href='#fgasp-class'><p>Fast GaSP class</p></a></li>
<li><a href='#Get_C_R_K_Q'>
<p>matrices and vectors for the inverse covariance in the predictive distribution</p></a></li>
<li><a href='#Get_L_inv_y'>
<p>vector of half of the sum of squares</p></a></li>
<li><a href='#Get_log_det_S2'>
<p>the natural logarithm of the determinant of the correlation matrix and the estimated sum of squares in the exponent of the profile likelihood</p></a></li>
<li><a href='#Kalman_smoother'>
<p>the predictive mean and predictive variance by Kalman Smoother</p></a></li>
<li><a href='#log_lik'>
<p>Natural logarithm of profile likelihood by the fast computing algorithm</p></a></li>
<li><a href='#predict'>
<p>Prediction and uncertainty quantification on the testing input using a GaSP model.</p></a></li>
<li><a href='#predictobj.fgasp-class'><p>Predictive results for the Fast GaSP class</p></a></li>
<li><a href='#Sample_KF'>
<p>Sample the prior process using a dynamic linear model</p></a></li>
<li><a href='#Sample_KF_post'>
<p>Sample the posterior distribution of the process using the backward smoothing algorithm</p></a></li>
<li><a href='#show'>

<p>Show an <code>fgasp</code> object.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Fast and Exact Computation of Gaussian Stochastic Process</td>
</tr>
<tr>
<td>Version:</td>
<td>0.5.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-08-28</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;</td>
</tr>
<tr>
<td>Author:</td>
<td>Mengyang Gu [aut, cre]</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements fast and exact computation of Gaussian stochastic process with the Matern kernel using forward filtering and backward smoothing algorithm. It allows for the cases with or without a noise.  See the reference: Mengyang Gu and Yanxun Xu (2017),  &lt;<a href="https://arxiv.org/abs/1711.11501">arXiv:1711.11501</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Depends:</td>
<td>methods</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, RobustGaSP</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppEigen</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-08-29 04:38:10 UTC; mengyanggu</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>5.0.1</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-09-02 23:20:08 UTC</td>
</tr>
</table>
<hr>
<h2 id='Construct_G_exp'> 
The coefficient matrix in the dynamic linear model when kernel is the exponential covariance
</h2><span id='topic+Construct_G_exp'></span>

<h3>Description</h3>

<p>The coefficient matrix in the dynamic linear model when kernel is the exponential covariance. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Construct_G_exp(delta_x,lambda)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Construct_G_exp_+3A_delta_x">delta_x</code></td>
<td>
<p>the distance between the sorted input.
</p>
</td></tr>
<tr><td><code id="Construct_G_exp_+3A_lambda">lambda</code></td>
<td>
<p>the transformed range parameter. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>GG matrix. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>


<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>.  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2019), <em>fast nonseparable Gaussian stochastic process with application to methylation level interpolation</em>.  <em>Journal of Computational and Graphical Statistics</em>, In Press, arXiv:1711.11501.
</p>
<p>Campagnoli P, Petris G, Petrone S. (2009), <em>Dynamic linear model with R</em>. Springer-Verlag New York.
</p>

<hr>
<h2 id='Construct_G_matern_5_2'> 
The coefficient matrix in the dynamic linear model when kernel is the Matern covariance with roughness parameter 2.5. 
</h2><span id='topic+Construct_G_matern_5_2'></span>

<h3>Description</h3>

<p>The coefficient matrix in the dynamic linear model when kernel is the Matern covariance with roughness parameter 2.5. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Construct_G_matern_5_2(delta_x,lambda)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Construct_G_matern_5_2_+3A_delta_x">delta_x</code></td>
<td>
<p>The distance between the sorted input.
</p>
</td></tr>
<tr><td><code id="Construct_G_matern_5_2_+3A_lambda">lambda</code></td>
<td>
<p>the transformed range parameter. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>GG matrix. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>


<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>.  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2019), <em>fast nonseparable Gaussian stochastic process with application to methylation level interpolation</em>.  <em>Journal of Computational and Graphical Statistics</em>, In Press, arXiv:1711.11501.
</p>
<p>Campagnoli P, Petris G, Petrone S. (2009), <em>Dynamic linear model with R</em>. Springer-Verlag New York.
</p>

<hr>
<h2 id='Construct_W_exp'> 
The conditional covariance matrix of the state in the dynamic linear model when kernel is the exponential covariance
</h2><span id='topic+Construct_W_exp'></span>

<h3>Description</h3>

<p>The conditional covariance matrix of the state in the dynamic linear model when kernel is the exponential covariance. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Construct_W_exp(sigma2,delta_x,lambda,W0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Construct_W_exp_+3A_sigma2">sigma2</code></td>
<td>
<p>the variance parameter.
</p>
</td></tr>
<tr><td><code id="Construct_W_exp_+3A_delta_x">delta_x</code></td>
<td>
<p>the distance between the sorted input.
</p>
</td></tr>
<tr><td><code id="Construct_W_exp_+3A_lambda">lambda</code></td>
<td>
<p>the transformed range parameter. </p>
</td></tr>
<tr><td><code id="Construct_W_exp_+3A_w0">W0</code></td>
<td>
<p>the covariance matrix of the stationary distribution of the state. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>W matrix. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>


<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>.  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2019), <em>fast nonseparable Gaussian stochastic process with application to methylation level interpolation</em>.  <em>Journal of Computational and Graphical Statistics</em>, In Press, arXiv:1711.11501.
</p>
<p>Campagnoli P, Petris G, Petrone S. (2009), <em>Dynamic linear model with R</em>. Springer-Verlag New York.
</p>

<hr>
<h2 id='Construct_W_matern_5_2'> 
The conditional covariance matrix for  matern covariance with roughness parameter 2.5
</h2><span id='topic+Construct_W_matern_5_2'></span>

<h3>Description</h3>

<p>The conditional covariance matrix of the state in the dynamic linear model when kernel is the matern covariance with roughness parameter 2.5.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Construct_W_matern_5_2(sigma2,delta_x,lambda,W0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Construct_W_matern_5_2_+3A_sigma2">sigma2</code></td>
<td>
<p>the variance parameter.
</p>
</td></tr>
<tr><td><code id="Construct_W_matern_5_2_+3A_delta_x">delta_x</code></td>
<td>
<p>the distance between the sorted input.
</p>
</td></tr>
<tr><td><code id="Construct_W_matern_5_2_+3A_lambda">lambda</code></td>
<td>
<p>the transformed range parameter. </p>
</td></tr>
<tr><td><code id="Construct_W_matern_5_2_+3A_w0">W0</code></td>
<td>
<p>the covariance matrix of the stationary distribution of the state. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>W matrix. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>


<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>.  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2019), <em>fast nonseparable Gaussian stochastic process with application to methylation level interpolation</em>.  <em>Journal of Computational and Graphical Statistics</em>, In Press, arXiv:1711.11501.
</p>
<p>Campagnoli P, Petris G, Petrone S. (2009), <em>Dynamic linear model with R</em>. Springer-Verlag New York.
</p>

<hr>
<h2 id='Construct_W0_exp'> 
covariance of the stationary distribution of the state when kernel is the exponential covariance.
</h2><span id='topic+Construct_W0_exp'></span>

<h3>Description</h3>

<p>This function computes the covariance of the stationary distribution of the state when kernel is the exponential covariance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Construct_W0_exp(sigma2,lambda)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Construct_W0_exp_+3A_sigma2">sigma2</code></td>
<td>
<p>the variance parameter.
</p>
</td></tr>
<tr><td><code id="Construct_W0_exp_+3A_lambda">lambda</code></td>
<td>
<p>the transformed range parameter. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>W0 matrix. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>


<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>.  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2019), <em>fast nonseparable Gaussian stochastic process with application to methylation level interpolation</em>.  <em>Journal of Computational and Graphical Statistics</em>, In Press, arXiv:1711.11501.
</p>
<p>Campagnoli P, Petris G, Petrone S. (2009), <em>Dynamic linear model with R</em>. Springer-Verlag New York.
</p>

<hr>
<h2 id='Construct_W0_matern_5_2'> 
covariance of the stationary distribution of the state when kernel is the Matern covariance with roughness parameter 2.5. 
</h2><span id='topic+Construct_W0_matern_5_2'></span>

<h3>Description</h3>

<p>This function computes covariance of the stationary distribution of the state when kernel is the Matern covariance with roughness parameter 2.5. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Construct_W0_matern_5_2(sigma2,lambda)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Construct_W0_matern_5_2_+3A_sigma2">sigma2</code></td>
<td>
<p>the variance parameter.
</p>
</td></tr>
<tr><td><code id="Construct_W0_matern_5_2_+3A_lambda">lambda</code></td>
<td>
<p>the transformed range parameter. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>W0 matrix. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>


<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>.  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2019), <em>fast nonseparable Gaussian stochastic process with application to methylation level interpolation</em>.  <em>Journal of Computational and Graphical Statistics</em>, In Press, arXiv:1711.11501.
</p>
<p>Campagnoli P, Petris G, Petrone S. (2009), <em>Dynamic linear model with R</em>. Springer-Verlag New York.
</p>

<hr>
<h2 id='FastGaSP-package'>
Fast and Exact Computation of Gaussian Stochastic Process
</h2><span id='topic+FastGaSP-package'></span><span id='topic+FastGaSP'></span>

<h3>Description</h3>

<p>Implements fast and exact computation of Gaussian stochastic process with the Matern kernel using forward filtering and backward smoothing algorithm. It allows for the cases with or without a noise.  See the reference: Mengyang Gu and Yanxun Xu (2017),  &lt;arXiv:1711.11501&gt;.
</p>


<h3>Details</h3>

<p>The DESCRIPTION file:
</p>

<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> FastGaSP</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Title: </td><td style="text-align: left;"> Fast and Exact Computation of Gaussian Stochastic Process</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 0.5.2</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2021-08-28</td>
</tr>
<tr>
 <td style="text-align: left;">
Authors@R: </td><td style="text-align: left;"> c(person(given="Mengyang",family="Gu",role=c("aut","cre"),email="mengyang@pstat.ucsb.edu"))</td>
</tr>
<tr>
 <td style="text-align: left;">
Maintainer: </td><td style="text-align: left;"> Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;</td>
</tr>
<tr>
 <td style="text-align: left;">
Author: </td><td style="text-align: left;"> Mengyang Gu [aut, cre]</td>
</tr>
<tr>
 <td style="text-align: left;">
Description: </td><td style="text-align: left;"> Implements fast and exact computation of Gaussian stochastic process with the Matern kernel using forward filtering and backward smoothing algorithm. It allows for the cases with or without a noise.  See the reference: Mengyang Gu and Yanxun Xu (2017),  &lt;arXiv:1711.11501&gt;.</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;= 2)</td>
</tr>
<tr>
 <td style="text-align: left;">
Depends: </td><td style="text-align: left;"> methods</td>
</tr>
<tr>
 <td style="text-align: left;">
Imports: </td><td style="text-align: left;"> Rcpp, RobustGaSP</td>
</tr>
<tr>
 <td style="text-align: left;">
LinkingTo: </td><td style="text-align: left;"> Rcpp, RcppEigen</td>
</tr>
<tr>
 <td style="text-align: left;">
NeedsCompilation: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;">
Repository: </td><td style="text-align: left;"> CRAN</td>
</tr>
<tr>
 <td style="text-align: left;">
Packaged: </td><td style="text-align: left;"> 2021-08-28 19:14:39 UTC; gumengyang</td>
</tr>
<tr>
 <td style="text-align: left;">
RoxygenNote: </td><td style="text-align: left;"> 5.0.1</td>
</tr>
<tr>
 <td style="text-align: left;">
Date/Publication: </td><td style="text-align: left;"> 2018-12-11 14:41:24 UTC</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>


<p>Index of help topics:
</p>
<pre>
FastGaSP-package        Fast and Exact Computation of Gaussian
                        Stochastic Process
fgasp                   Setting up the Fast GaSP model
fgasp-class             Fast GaSP class
log_lik                 Natural logarithm of profile likelihood by the
                        fast computing algorithm
predict                 Prediction and uncertainty quantification on
                        the testing input using a GaSP model.
predictobj.fgasp-class
                        Predictive results for the Fast GaSP class
show                    Show an 'fgasp' object.
</pre>
<p>Fast computational algorithms for Gaussian stochastic process with Matern kernels 
by the forward filtering and backward smoothing algorithm.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>,  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu and Y. Xu (2020), <em>Nonseparable Gaussian stochastic process: a unified
view and computational strategy</em>, <em>Fast Nonseparable Gaussian Stochastic Process With Application to Methylation Level Interpolation</em>, <em>Journal of Computational and Graphical Statistics</em>, <b>29</b>, 250-260.
</p>
<p>M. Gu and W. Shen (2020), <em>Generalized probabilistic principal component analysis of correlated data</em>, <em>Journal of Machine Learning Research</em>, <b>21</b>, 13-1.
</p>
<p>M. Gu, X. Wang and J.O. Berger (2018), <em>Robust Gaussian Stochastic Process Emulation</em>, <em>Annals of Statistics</em>, <b>46</b>, 3038-3066.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FastGaSP">FastGaSP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(FastGaSP)

#------------------------------------------------------------------------------
# Example 1 : fast computation algorithm for noisy data
#------------------------------------------------------------------------------

y_R&lt;-function(x){
  sin(2*pi*x)
}

###let's test for 2000 observations
set.seed(1)
num_obs=2000
input=runif(num_obs)
output=y_R(input)+rnorm(num_obs,mean=0,sd=0.1)

##constucting the fgasp.model
fgasp.model=fgasp(input, output)

##range and noise-variance ratio (nugget) parameters 
param=c( log(1),log(.02))
## the log lik
log_lik(param,fgasp.model)
##time cost to compute the likelihood
time_cost=system.time(log_lik(param,fgasp.model))
time_cost[1]

##consider a nonparametric regression setting 
##estimate the parameter by maximum likelihood estimation

est_all&lt;-optim(c(log(1),log(.02)),log_lik,object=fgasp.model,method="L-BFGS-B",
              control = list(fnscale=-1))  

##estimated log inverse range parameter and log nugget
est_all$par

##estimate variance 
est.var=Get_log_det_S2(est_all$par,fgasp.model@have_noise,fgasp.model@delta_x,
                          fgasp.model@output,fgasp.model@kernel_type)[[2]]/fgasp.model@num_obs
est.var

###1. Do some interpolation test 
num_test=5000
testing_input=runif(num_test) ##there are the input where you don't have observations
pred.model=predict(param=est_all$par,object=fgasp.model,testing_input=testing_input)

lb=pred.model@mean+qnorm(0.025)*sqrt(pred.model@var)
ub=pred.model@mean+qnorm(0.975)*sqrt(pred.model@var)

## calculate lb for the mean function
pred.model2=predict(param=est_all$par,object=fgasp.model,testing_input=testing_input,var_data=FALSE)
lb_mean_funct=pred.model2@mean+qnorm(0.025)*sqrt(pred.model2@var)
ub_mean_funct=pred.model2@mean+qnorm(0.975)*sqrt(pred.model2@var)

## plot the prediction
min_val=min(lb,output)
max_val=max(ub,output)

plot(pred.model@testing_input,pred.model@mean,type='l',col='blue',
     ylim=c(min_val,max_val),
     xlab='x',ylab='y')
polygon(c(pred.model@testing_input,rev(pred.model@testing_input)),
        c(lb,rev(ub)),col = "grey80", border = FALSE)
lines(pred.model@testing_input,pred.model@mean,type='l',col='blue')
lines(pred.model@testing_input,y_R(pred.model@testing_input),type='l',col='black')
lines(pred.model2@testing_input,lb_mean_funct,col='blue',lty=2)
lines(pred.model2@testing_input,ub_mean_funct,col='blue',lty=2)
lines(input,output,type='p',pch=16,col='black',cex=0.4) #one can plot data

legend("bottomleft", legend=c("predictive mean","95% predictive interval","truth"),
       col=c("blue","blue","black"), lty=c(1,2,1), cex=.8)

#--------------------------------------------------------------
# Example 2: example that one does not have a noise in the data
#--------------------------------------------------------------
## Here is a function in the Sobolev Space with order 3
y_R&lt;-function(x){
  j_seq=seq(1,200,1)
  record_y_R=0
  for(i_j in 1:200){
    record_y_R=record_y_R+2*j_seq[i_j]^{-2*3}*sin(j_seq[i_j])*cos(pi*(j_seq[i_j]-0.5)*x)

  }
  record_y_R
}


##generate some data without noise
num_obs=50
input=seq(0,1,1/(num_obs-1))

output=y_R(input)


##constucting the fgasp.model
fgasp.model=fgasp(input, output,have_noise=FALSE)

##range and noise-variance ratio (nugget) parameters 
param=c( log(1))
## the log lik
log_lik(param,fgasp.model)



#if one does not have noise one may need to give a lower bound or use a penalty 
#(e.g. induced by a prior) to make the estimation more robust
est_all&lt;-optimize(log_lik,interval=c(0,10),maximum=TRUE,fgasp.model)
  
##Do some interpolation test for comparison
num_test=1000
testing_input=runif(num_test) ##there are the input where you don't have observations

pred.model=predict(param=est_all$maximum,object=fgasp.model,testing_input=testing_input)



#This is the 95 posterior credible interval for the outcomes which contain the estimated 
#variance of the noise
#sometimes there are numerical instability is one does not have noise or error
lb=pred.model@mean+qnorm(0.025)*sqrt(abs(pred.model@var))
ub=pred.model@mean+qnorm(0.975)*sqrt(abs(pred.model@var))

## plot the prediction
min_val=min(lb,output)
max_val=max(ub,output)

plot(pred.model@testing_input,pred.model@mean,type='l',col='blue',
     ylim=c(min_val,max_val),
     xlab='x',ylab='y')
polygon( c(pred.model@testing_input,rev(pred.model@testing_input)),
         c(lb,rev(ub)),col = "grey80", border = FALSE)
lines(pred.model@testing_input,pred.model@mean,type='l',col='blue')
lines(pred.model@testing_input,y_R(pred.model@testing_input),type='l',col='black')
lines(input,output,type='p',pch=16,col='black')
legend("bottomleft", legend=c("predictive mean","95% predictive interval","truth"),
       col=c("blue","blue","black"), lty=c(1,2,1), cex=.8)


##mean square error for all inputs
mean((pred.model@mean- y_R(pred.model@testing_input))^2)

</code></pre>

<hr>
<h2 id='fgasp'> Setting up the Fast GaSP model

</h2><span id='topic+fgasp'></span><span id='topic+fgasp-method'></span>

<h3>Description</h3>


<p>Creating an <code>fgasp</code> class for a GaSP model with matern covariance. 

</p>


<h3>Usage</h3>

<pre><code class='language-R'>  fgasp(input, output, have_noise=TRUE, kernel_type='matern_5_2')
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fgasp_+3A_input">input</code></td>
<td>
<p>a vector with dimension num_obs x 1 for the sorted input locations.</p>
</td></tr>
<tr><td><code id="fgasp_+3A_output">output</code></td>
<td>
<p>a  vector with dimension n x 1 for the observations at the sorted input locations.</p>
</td></tr>
<tr><td><code id="fgasp_+3A_have_noise">have_noise</code></td>
<td>
<p>a bool value. If it is true, it means the model contains a noise. 

</p>
</td></tr>
<tr><td><code id="fgasp_+3A_kernel_type">kernel_type</code></td>
<td>
<p>a <code>character</code> to specify the type of kernel to use. The current version supports kernel_type to be &quot;matern_5_2&quot; or &quot;exp&quot;, meaning that the matern kernel with roughness parameter being 2.5 or 0.5 (exponent kernel), respectively. 

</p>
</td></tr>
</table>


<h3>Value</h3>






<p><code>fgasp</code> returns an S4 object of class <code>fgasp</code> (see <code>fgasp</code>).
</p>





<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>


<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>,  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2017), <em>Nonseparable Gaussian stochastic process: a unified
view and computational strategy</em>, arXiv:1711.11501.
</p>
<p>M. Gu, X. Wang and J.O. Berger (2018), <em>Robust Gaussian Stochastic Process Emulation</em>, <em>Annals of Statistics</em>, <b>46</b>, 3038-3066.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(FastGaSP)

#-------------------------------------
# Example 1: a simple example with noise 
#-------------------------------------

y_R&lt;-function(x){
  cos(2*pi*x)
}

###let's test for 2000 observations
set.seed(1)
num_obs=2000
input=runif(num_obs)

output=y_R(input)+rnorm(num_obs,mean=0,sd=0.1)

##constucting the fgasp.model
fgasp.model=fgasp(input, output)
show(fgasp.model)

#------------------------------------------
# Example 2: a simple example with no noise 
#------------------------------------------

y_R&lt;-function(x){
  sin(2*pi*x)
}


##generate some data without noise
num_obs=50
input=seq(0,1,1/(num_obs-1))

output=y_R(input)


##constucting the fgasp.model
fgasp.model=fgasp(input, output,have_noise=FALSE)

show(fgasp.model)

</code></pre>

<hr>
<h2 id='fgasp-class'>Fast GaSP class</h2><span id='topic+fgasp-class'></span>

<h3>Description</h3>

<p>S4 class for fast computation of the Gaussian stochastic process (GaSP) model with the Matern kernel function with or without a noise.</p>


<h3>Objects from the Class</h3>

<p>Objects of this class are created and initialized with the function <code><a href="#topic+fgasp">fgasp</a></code> that computes the calculations needed for setting up the estimation and prediction.</p>


<h3>Slots</h3>


<dl>
<dt><code>num_obs</code>:</dt><dd><p>object of class <code>integer</code>. The number of experimental observations.</p>
</dd>
<dt><code>have_noise</code>:</dt><dd><p>object of class <code>logical</code> to specify whether the the model has a noise or not. &quot;TRUE&quot; means the model contains a noise and &quot;FALSE&quot; means the model does not contain a noise.</p>
</dd>
<dt><code>kernel_type</code>:</dt><dd><p>a <code>character</code> to specify the type of kernel to use.The current version supports kernel_type to be &quot;matern_5_2&quot; or &quot;exp&quot;, meaning that the matern kernel with roughness parameter being 2.5 or 0.5 (exponent kernel), respectively. </p>
</dd>
<dt><code>input</code>:</dt><dd><p>object of class <code>vector</code> with dimension num_obs x 1 for the sorted input locations.</p>
</dd>
<dt><code>delta_x</code>:</dt><dd><p>object of class <code>vector</code> with dimension (num_obs-1) x 1 for the differences between the sorted input locations.</p>
</dd>
<dt><code>output</code>:</dt><dd><p>object of class <code>vector</code> with dimension num_obs x 1 for the observations at the sorted input locations.</p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>show</dt><dd><p>Prints the main slots of the object. </p>
</dd>
<dt>predict</dt><dd><p>See <code><a href="#topic+predict.fgasp">predict</a></code>.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>,  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2017), <em>Nonseparable Gaussian stochastic process: a unified
view and computational strategy</em>, arXiv:1711.11501.
</p>
<p>M. Gu, X. Wang and J.O. Berger (2018), <em>Robust Gaussian Stochastic Process Emulation</em>, <em>Annals of Statistics</em>, <b>46</b>, 3038-3066.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+fgasp">fgasp</a></code> for more details about how to create a <code>fgasp</code> object.

</p>

<hr>
<h2 id='Get_C_R_K_Q'> 
matrices and vectors for the inverse covariance in the predictive distribution
</h2><span id='topic+Get_C_R_K_Q'></span>

<h3>Description</h3>

<p>This function computes the required values for the inverse covariance matrix. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Get_C_R_K_Q(index,GG,W,C0,VV)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Get_C_R_K_Q_+3A_index">index</code></td>
<td>
<p>a vector of integer of 0 and 1. 0 means no observation at that input and 1 means there is observations at that input.
</p>
</td></tr>
<tr><td><code id="Get_C_R_K_Q_+3A_have_noise">have_noise</code></td>
<td>
<p>a bool value. If it is true, it means the model contains a noise. </p>
</td></tr>

<tr><td><code id="Get_C_R_K_Q_+3A_gg">GG</code></td>
<td>
<p>a list of matrices defined in the dynamic linear model.</p>
</td></tr>
<tr><td><code id="Get_C_R_K_Q_+3A_w">W</code></td>
<td>
<p>a list of matrices defined in the dynamic linear model.</p>
</td></tr>
<tr><td><code id="Get_C_R_K_Q_+3A_c0">C0</code></td>
<td>
<p>a matrix defined in the dynamic linear model.</p>
</td></tr>
<tr><td><code id="Get_C_R_K_Q_+3A_vv">VV</code></td>
<td>
<p>a numerical value for the nugget.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of 4 items for C, R, K and Q. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>


<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>.  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2019), <em>fast nonseparable gaussian stochastic process with application to methylation level interpolation</em>.  <em>Journal of Computational and Graphical Statistics</em>, In Press, arXiv:1711.11501.
</p>
<p>Campagnoli P, Petris G, Petrone S. (2009), <em>Dynamic linear model with R</em>. Springer-Verlag New York.
</p>

<hr>
<h2 id='Get_L_inv_y'> 
vector of half of the sum of squares 
</h2><span id='topic+Get_L_inv_y'></span>

<h3>Description</h3>

<p>This function computes the inverse of L matrix multiples the output vector, where the L matrix is the Cholesky decomposition of the correlation matrix. Instead of computing the Cholesky matrix, we compute it using the forward filtering algorithm. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Get_L_inv_y(GG,VV,Q,K,output)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Get_L_inv_y_+3A_gg">GG</code></td>
<td>
<p>a list of matrices defined in the dynamic linear model. 
</p>
</td></tr>
<tr><td><code id="Get_L_inv_y_+3A_vv">VV</code></td>
<td>
<p>a numerical value of the variance of the nugget parameter. </p>
</td></tr>

<tr><td><code id="Get_L_inv_y_+3A_q">Q</code></td>
<td>
<p>a vector defined in the dynamic linear model.    </p>
</td></tr>
<tr><td><code id="Get_L_inv_y_+3A_k">K</code></td>
<td>
<p>a matrix defined in the filtering algorithm for the dynamic linear model.  </p>
</td></tr>
<tr><td><code id="Get_L_inv_y_+3A_output">output</code></td>
<td>
<p>a vector of output.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of the inverse of L matrix multiples the output vector, where the L matrix is the Cholesky decomposition of the correlation matrix. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>


<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>.  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2019), <em>fast nonseparable gaussian stochastic process with application to methylation level interpolation</em>.  <em>Journal of Computational and Graphical Statistics</em>, In Press, arXiv:1711.11501.
</p>
<p>Campagnoli P, Petris G, Petrone S. (2009), <em>Dynamic linear model with R</em>. Springer-Verlag New York.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Get_C_R_K_Q">Get_C_R_K_Q</a></code> for more details about Q vector and K matrix.

</p>

<hr>
<h2 id='Get_log_det_S2'> 
the natural logarithm of the determinant of the correlation matrix and the estimated sum of squares in the exponent of the profile likelihood
</h2><span id='topic+Get_log_det_S2'></span>

<h3>Description</h3>

<p>This function computes the natural logarithm of the determinant of the correlation matrix and the estimated sum of squares for computing the profile likelihood. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Get_log_det_S2(param,have_noise,delta_x,output,kernel_type)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Get_log_det_S2_+3A_param">param</code></td>
<td>
<p>a vector of parameters. The first parameter is the natural logarithm of the inverse range parameter in the kernel function. If the data contain noise, the second parameter is the logarithm of the nugget-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="Get_log_det_S2_+3A_have_noise">have_noise</code></td>
<td>
<p>a bool value. If it is true, it means the model contains a noise. </p>
</td></tr>

<tr><td><code id="Get_log_det_S2_+3A_delta_x">delta_x</code></td>
<td>
<p>a vector with dimension (num_obs-1) x 1 for the differences between the sorted input locations.</p>
</td></tr>
<tr><td><code id="Get_log_det_S2_+3A_output">output</code></td>
<td>
<p>a  vector with dimension num_obs x 1 for the observations at the sorted input locations.</p>
</td></tr>
<tr><td><code id="Get_log_det_S2_+3A_kernel_type">kernel_type</code></td>
<td>
<p>A <code>character</code> specifying the type of kernel.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list where the first value is the natural logarithm of the determinant of the correlation matrix and the second value is the estimated sum of squares.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>


<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>,  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2017), <em>Nonseparable Gaussian stochastic process: a unified
view and computational strategy</em>, arXiv:1711.11501.
</p>
<p>M. Gu, X. Wang and J.O. Berger (2018), <em>Robust Gaussian Stochastic Process Emulation</em>, <em>Annals of Statistics</em>, <b>46</b>, 3038-3066.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+log_lik">log_lik</a></code> for more details about the profile likelihood.

</p>

<hr>
<h2 id='Kalman_smoother'> 
the predictive mean and predictive variance by Kalman Smoother
</h2><span id='topic+Kalman_smoother'></span>

<h3>Description</h3>

<p>This function computes the predictive mean and predictive variance on the sorted input and testing input  by the Kalman Smoother.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Kalman_smoother(param,have_noise,index_obs,delta_x_all,output,sigma_2_hat,kernel_type)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Kalman_smoother_+3A_param">param</code></td>
<td>
<p>a vector of parameters. The first parameter is the natural logarithm of the inverse range parameter in the kernel function. If the data contain noise, the second parameter is the logarithm of the nugget-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="Kalman_smoother_+3A_have_noise">have_noise</code></td>
<td>
<p>a bool value. If it is true, it means the model contains a noise. 

</p>
</td></tr>
<tr><td><code id="Kalman_smoother_+3A_index_obs">index_obs</code></td>
<td>
<p>a vector  where the entries with 1 have observations and entries with 0 have no observation.</p>
</td></tr>
<tr><td><code id="Kalman_smoother_+3A_delta_x_all">delta_x_all</code></td>
<td>
<p>a vector for the differences between the sorted input and testing input locations.</p>
</td></tr>
<tr><td><code id="Kalman_smoother_+3A_output">output</code></td>
<td>
<p>a  vector with dimension num_obs x 1 for the observations at the sorted input locations.</p>
</td></tr>
<tr><td><code id="Kalman_smoother_+3A_sigma_2_hat">sigma_2_hat</code></td>
<td>
<p>a numerical value of variance parameter of the covariance function.</p>
</td></tr>
<tr><td><code id="Kalman_smoother_+3A_kernel_type">kernel_type</code></td>
<td>
<p>A <code>character</code> specifying the type of kernel.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list where the first item is the the predictive mean and the second item is predictive variance on the sorted input and testing input  by the Kalman Smoother.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>


<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>,  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2017), <em>Nonseparable Gaussian stochastic process: a unified
view and computational strategy</em>, arXiv:1711.11501.
</p>
<p>M. Gu, X. Wang and J.O. Berger (2018), <em>Robust Gaussian Stochastic Process Emulation</em>, <em>Annals of Statistics</em>, <b>46</b>, 3038-3066.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict">predict</a></code> for more details about the prediction on the testing input by the Fast GaSP class.

</p>

<hr>
<h2 id='log_lik'> 
Natural logarithm of profile likelihood by the fast computing algorithm
</h2><span id='topic+log_lik'></span>

<h3>Description</h3>

<p>This function computes the natural logarithm of the profile likelihood for the range and nugget parameter (if there is one) after plugging the closed form maximum likelihood estimator for the variance parameter. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_lik(param, object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="log_lik_+3A_param">param</code></td>
<td>
<p>a vector of parameters. The first parameter is the natural logarithm of the inverse range parameter in the kernel function. If the data contain noise, the second parameter is the logarithm of the nugget-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="log_lik_+3A_object">object</code></td>
<td>
<p>an object of  class <code>fgasp</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The numerical value of natural logarithm of the profile likelihood.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>


<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>,  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2017), <em>Nonseparable Gaussian stochastic process: a unified
view and computational strategy</em>, arXiv:1711.11501.
</p>
<p>M. Gu, X. Wang and J.O. Berger (2018), <em>Robust Gaussian Stochastic Process Emulation</em>, <em>Annals of Statistics</em>, <b>46</b>, 3038-3066.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(FastGaSP)
#--------------------------------------------------------------------------
# Example 1: comparing the fast and slow algorithms to compute the likelihood 
# of the Gaussian stochastic process for data with noises
#--------------------------------------------------------------------------

y_R&lt;-function(x){
  sin(2*pi*x)
}

###let's test for 1000 observations
set.seed(1)
num_obs=1000
input=runif(num_obs)
output=y_R(input)+rnorm(num_obs,mean=0,sd=0.1)

##constucting the fgasp.model
fgasp.model=fgasp(input, output)

##range and noise-variance ratio (nugget) parameters 
param=c( log(1),log(.02))
## the log lik
log_lik(param,fgasp.model)
##time cost to compute the likelihood
time_cost=system.time(log_lik(param,fgasp.model))
time_cost[1]

##now I am comparing to the one with straightforward inversion

matern_5_2_kernel&lt;-function(d,beta){  
  res=(1+sqrt(5)*beta*d + 5*beta^2*d^2/3 )*exp(-sqrt(5)*beta*d)
  res
}

##A function for computing the likelihood by the GaSP in a straightforward way
log_lik_GaSP_slow&lt;-function(param,have_noise=TRUE,input,output){
  n=length(output)
  beta=exp(param[1])
  eta=0
  if(have_noise){
    eta=exp(param[2])
  }
  R00=abs(outer(input,input,'-'))
  R=matern_5_2_kernel(R00,beta=beta)
  R_tilde=R+eta*diag(n)
  #use Cholesky and one backsolver and one forward solver so it is more stable
  L=t(chol(R_tilde))
  output_t_R.inv= t(backsolve(t(L),forwardsolve(L,output )))
  S_2=output_t_R.inv%*%output
  
  -sum(log(diag(L)))-n/2*log(S_2)
}



##range and noise-variance ratio (nugget) parameters 
param=c( log(1),log(.02))
## the log lik
log_lik(param,fgasp.model)
log_lik_GaSP_slow(param,have_noise=TRUE,input=input,output=output)

##time cost to compute the likelihood
##More number of inputs mean larger differences
time_cost=system.time(log_lik(param,fgasp.model))
time_cost

time_cost_slow=system.time(log_lik_GaSP_slow(param,have_noise=TRUE,input=input,output=output))
time_cost_slow


#--------------------------------------------------------------------------
# Example 2: comparing the fast and slow algorithms to compute the likelihood 
# of the Gaussian stochastic process for data without a noise
#--------------------------------------------------------------------------
## Here is a function in the Sobolev Space with order 3
y_R&lt;-function(x){
  j_seq=seq(1,200,1)
  record_y_R=0
  for(i_j in 1:200){
    record_y_R=record_y_R+2*j_seq[i_j]^{-2*3}*sin(j_seq[i_j])*cos(pi*(j_seq[i_j]-0.5)*x)

  }
  record_y_R
}


##generate some data without noise
num_obs=50
input=seq(0,1,1/(num_obs-1))

output=y_R(input)


##constucting the fgasp.model
fgasp.model=fgasp(input, output,have_noise=FALSE)

##range and noise-variance ratio (nugget) parameters 
param=c( log(1))
## the log lik
log_lik(param,fgasp.model)
log_lik_GaSP_slow(param,have_noise=FALSE,input=input,output=output)


</code></pre>

<hr>
<h2 id='predict'> 
Prediction and uncertainty quantification on the testing input using a GaSP model.
</h2><span id='topic+predict'></span><span id='topic+predict.fgasp'></span><span id='topic+predict+2Cfgasp-method'></span>

<h3>Description</h3>

<p>This function computes the predictive mean and variance on the given testing input using a GaSP model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'fgasp'
predict(param,object, testing_input, var_data=TRUE, sigma_2=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_+3A_param">param</code></td>
<td>
<p>a vector of parameters. The first parameter is the natural logarithm of the inverse range parameter in the kernel function. If the data contain noise, the second parameter is the logarithm of the nugget-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="predict_+3A_object">object</code></td>
<td>
<p>an object of  class <code>fgasp</code>.
</p>
</td></tr>
<tr><td><code id="predict_+3A_testing_input">testing_input</code></td>
<td>
<p>a vector of testing input for prediction.
</p>
</td></tr>
<tr><td><code id="predict_+3A_var_data">var_data</code></td>
<td>
<p>a bool valueto decide whether the noise of the data is included for computing the posterior predictive variance.
</p>
</td></tr>
<tr><td><code id="predict_+3A_sigma_2">sigma_2</code></td>
<td>
<p>a numerical value specifying the variance of the kernel function. If given, the package uses this parameter for prediction.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The returned value is a S4 Class <code>predictobj.fgasp</code>. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>


<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>,  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2017), <em>Nonseparable Gaussian stochastic process: a unified
view and computational strategy</em>, arXiv:1711.11501.
</p>
<p>M. Gu, X. Wang and J.O. Berger (2018), <em>Robust Gaussian Stochastic Process Emulation</em>, <em>Annals of Statistics</em>, <b>46</b>, 3038-3066.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(FastGaSP)

#------------------------------------------------------------------------------
# Example 1: a simple example with noise to show fast computation algorithm 
#------------------------------------------------------------------------------

y_R&lt;-function(x){
  cos(2*pi*x)
}

###let's test for 2000 observations
set.seed(1)
num_obs=2000
input=runif(num_obs)

output=y_R(input)+rnorm(num_obs,mean=0,sd=0.1)

##constucting the fgasp.model
fgasp.model=fgasp(input, output)

##range and noise-variance ratio (nugget) parameters 
param=c( log(1),log(.02))
## the log lik
log_lik(param,fgasp.model)
##time cost to compute the likelihood
time_cost=system.time(log_lik(param,fgasp.model))
time_cost[1]

##consider a nonparametric regression setting 
##estimate the parameter by maximum likelihood estimation

est_all&lt;-optim(c(log(1),log(.02)),log_lik,object=fgasp.model,method="L-BFGS-B",
              control = list(fnscale=-1))  

##estimated log inverse range parameter and log nugget
est_all$par

##estimate variance 
est.var=Get_log_det_S2(est_all$par,fgasp.model@have_noise,fgasp.model@delta_x,
                          fgasp.model@output,fgasp.model@kernel_type)[[2]]/fgasp.model@num_obs
est.var

###1. Do some interpolation test 
num_test=5000
testing_input=runif(num_test) ##there are the input where you don't have observations
pred.model=predict(param=est_all$par,object=fgasp.model,testing_input=testing_input)

lb=pred.model@mean+qnorm(0.025)*sqrt(pred.model@var)
ub=pred.model@mean+qnorm(0.975)*sqrt(pred.model@var)

## calculate lb for the mean function
pred.model2=predict(param=est_all$par,object=fgasp.model,testing_input=testing_input,var_data=FALSE)
lb_mean_funct=pred.model2@mean+qnorm(0.025)*sqrt(pred.model2@var)
ub_mean_funct=pred.model2@mean+qnorm(0.975)*sqrt(pred.model2@var)

## plot the prediction
min_val=min(lb,output)
max_val=max(ub,output)

plot(pred.model@testing_input,pred.model@mean,type='l',col='blue',
     ylim=c(min_val,max_val),
     xlab='x',ylab='y')
polygon( c(pred.model@testing_input,rev(pred.model@testing_input)),
    c(lb,rev(ub)),col = "grey80", border = FALSE)
lines(pred.model@testing_input,pred.model@mean,type='l',col='blue')
lines(pred.model@testing_input,y_R(pred.model@testing_input),type='l',col='black')
lines(pred.model2@testing_input,lb_mean_funct,col='blue',lty=2)
lines(pred.model2@testing_input,ub_mean_funct,col='blue',lty=2)
lines(input,output,type='p',pch=16,col='black',cex=0.4) #one can plot data

legend("bottomleft", legend=c("predictive mean","95% predictive interval","truth"),
       col=c("blue","blue","black"), lty=c(1,2,1), cex=.8)


##mean square error for all inputs
mean((pred.model@mean- y_R(pred.model@testing_input))^2)
##coverage for the mean
length(which(y_R(pred.model@testing_input)&gt;lb_mean_funct &amp;
               y_R(pred.model@testing_input)&lt;ub_mean_funct))/pred.model@num_testing
##average length of the invertal for the mean
mean(abs(ub_mean_funct-lb_mean_funct))
##average length of the invertal for the data
mean(abs(ub-lb))

#---------------------------------------------------------------------------------
# Example 2: numerical comparison with the GaSP by inverting the covariance matrix
#---------------------------------------------------------------------------------
##matern correlation with smoothness parameter being 2.5
matern_5_2_kernel&lt;-function(d,beta){  
  res=(1+sqrt(5)*beta*d + 5*beta^2*d^2/3 )*exp(-sqrt(5)*beta*d)
  res
}

##A function for computing the likelihood by the GaSP in a straightforward way
log_lik_GaSP_slow&lt;-function(param,have_noise=TRUE,input,output){
  n=length(output)
  beta=exp(param[1])
  eta=0
  if(have_noise){
    eta=exp(param[2])
  }
  R00=abs(outer(input,input,'-'))
  R=matern_5_2_kernel(R00,beta=beta)
  R_tilde=R+eta*diag(n)
  #use Cholesky and one backsolver and one forward solver so it is more stable
  L=t(chol(R_tilde))
  output_t_R.inv= t(backsolve(t(L),forwardsolve(L,output )))
  S_2=output_t_R.inv%*%output
  
  -sum(log(diag(L)))-n/2*log(S_2)
}


pred_GaSP_slow&lt;-function(param,have_noise=TRUE,input,output,testing_input){
  beta=exp(param[1])
  R00=abs(outer(input,input,'-'))
  eta=0
  if(have_noise){
    eta=exp(param[2])
  }
  R=matern_5_2_kernel(R00,beta=beta)
  R_tilde=R+eta*diag(length(output))
  
  ##I invert it here but one can still use cholesky to make it more stable
  R_tilde_inv=solve(R_tilde)

  r0=abs(outer(input,testing_input,'-'))
  r=matern_5_2_kernel(r0,beta=beta)

  S_2=t(output)%*%(R_tilde_inv%*%output)

  sigma_2_hat=as.numeric(S_2/num_obs)

  pred_mean=t(r)%*%(R_tilde_inv%*%output)
  pred_var=rep(0,length(testing_input))
  
  
  for(i in 1:length(testing_input)){
    pred_var[i]=-t(r[,i])%*%R_tilde_inv%*%r[,i]
  }
  pred_var=pred_var+1+eta
  list=list()
  list$mean=pred_mean
  list$var=pred_var*sigma_2_hat
  list
}

##let's test sin function
y_R&lt;-function(x){
  sin(2*pi*x)
}


###let's test for 800 observations
set.seed(1)
num_obs=800
input=runif(num_obs)
output=y_R(input)+rnorm(num_obs,mean=0,sd=0.1)


##constucting the fgasp.model
fgasp.model=fgasp(input, output)

##range and noise-variance ratio (nugget) parameters 
param=c( log(1),log(.02))
## the log lik
log_lik(param,fgasp.model)
log_lik_GaSP_slow(param,have_noise=TRUE,input=input,output=output)

##time cost to compute the likelihood
##More number of inputs mean larger differences
time_cost=system.time(log_lik(param,fgasp.model))
time_cost

time_cost_slow=system.time(log_lik_GaSP_slow(param,have_noise=TRUE,input=input,output=output))
time_cost_slow

est_all&lt;-optim(c(log(1),log(.02)),log_lik,object=fgasp.model,method="L-BFGS-B",
              control = list(fnscale=-1))  
##estimated log inverse range parameter and log nugget
est_all$par


##Do some interpolation test for comparison
num_test=200
testing_input=runif(num_test) ##there are the input where you don't have observations

##one may sort the testing_input or not, and the prediction will be on the sorted testing_input
##testing_input=sort(testing_input)

## two ways of prediction
pred.model=predict(param=est_all$par,object=fgasp.model,testing_input=testing_input)
pred_slow=pred_GaSP_slow(param=est_all$par,have_noise=TRUE,input,output,sort(testing_input))
## look at the difference
max(abs(pred.model@mean-pred_slow$mean))
max(abs(pred.model@var-pred_slow$var))


</code></pre>

<hr>
<h2 id='predictobj.fgasp-class'>Predictive results for the Fast GaSP class </h2><span id='topic+predictobj.fgasp-class'></span><span id='topic+predictobj.fgasp'></span>

<h3>Description</h3>

<p>S4 class for prediction for a Fast GaSP model with or without a noise.</p>


<h3>Objects from the Class</h3>

<p>Objects of this class are created and initialized with the function <code><a href="#topic+predict">predict</a></code> that computes the prediction and the uncertainty quantification.</p>


<h3>Slots</h3>


<dl>
<dt><code>num_testing</code>:</dt><dd><p>object of class <code>integer</code>. Number of testing inputs.</p>
</dd>
<dt><code>testing_input</code>:</dt><dd><p>object of class <code>vector</code>. The testing input locations.</p>
</dd>
<dt>param</dt><dd><p>a vector of parameters. The first parameter is the natural logarithm of the inverse range parameter in the kernel function. If the data contain noise, the second parameter is the logarithm of the nugget-variance ratio parameter.
</p>
</dd>
<dt><code>mean</code>:</dt><dd><p>object of class <code>vector</code>. The predictive mean at testing inputs.</p>
</dd>
<dt><code>var</code>:</dt><dd><p>object of class <code>vector</code>. The predictive variance at testing inputs. If the <code>var_data</code> is true, the predictive variance of the data is calculated. Otherwise, the predictive variance of the mean is calculated. </p>
</dd>
<dt><code>var_data</code>:</dt><dd><p>object of class <code>logical</code>. If the <code>var_data</code> is true, the predictive variance of the data is calculated for <code>var</code>. Otherwise, the predictive variance of the mean is calculated for <code>var</code>. </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>,  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2017), <em>Nonseparable Gaussian stochastic process: a unified
view and computational strategy</em>, arXiv:1711.11501.
</p>
<p>M. Gu, X. Wang and J.O. Berger (2018), <em>Robust Gaussian Stochastic Process Emulation</em>, <em>Annals of Statistics</em>, <b>46</b>, 3038-3066.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.fgasp">predict.fgasp</a></code> for more details about how to do prediction for a <code>fgasp</code> object.

</p>

<hr>
<h2 id='Sample_KF'> 
Sample the prior process using a dynamic linear model  
</h2><span id='topic+Sample_KF'></span>

<h3>Description</h3>

<p>This function samples the piror process using a dynamic liner model. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Sample_KF(GG,W,C0,VV,kernel_type,sample_type)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Sample_KF_+3A_gg">GG</code></td>
<td>
<p>a list of matrices defined in the dynamic linear model. 
</p>
</td></tr>
<tr><td><code id="Sample_KF_+3A_w">W</code></td>
<td>
<p>a list of coefficient matrices defined in the dynamic linear model. 
</p>
</td></tr>
<tr><td><code id="Sample_KF_+3A_c0">C0</code></td>
<td>
<p>the covariance matrix of the stationary distribution defined in the dynamic linear model. 
</p>
</td></tr>
<tr><td><code id="Sample_KF_+3A_vv">VV</code></td>
<td>
<p>a numerical value of the variance of the nugget parameter. </p>
</td></tr>

<tr><td><code id="Sample_KF_+3A_kernel_type">kernel_type</code></td>
<td>
<p>a <code>character</code> to specify the type of kernel to use. The current version supports kernel_type to be &quot;matern_5_2&quot; or &quot;exp&quot;, meaning that the matern kernel with roughness parameter being 2.5 or 0.5 (exponent kernel), respectively. </p>
</td></tr>
<tr><td><code id="Sample_KF_+3A_sample_type">sample_type</code></td>
<td>
<p>a integer to specify the type of sample we need. 0 means the states. 1 means the first value of each state vector. 2 means the noisy observations. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of the samples. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>


<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>.  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2019), <em>fast nonseparable gaussian stochastic process with application to methylation level interpolation</em>.  <em>Journal of Computational and Graphical Statistics</em>, In Press, arXiv:1711.11501.
</p>
<p>Campagnoli P, Petris G, Petrone S. (2009), <em>Dynamic linear model with R</em>. Springer-Verlag New York.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Sample_KF_post">Sample_KF_post</a></code> for more details about sampling from the posterior distribution.

</p>

<hr>
<h2 id='Sample_KF_post'> 
Sample the posterior distribution of the process using the backward smoothing algorithm  
</h2><span id='topic+Sample_KF_post'></span>

<h3>Description</h3>

<p>This function samples the posterior distribution of the process using the backward smoothing algorithm. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Sample_KF_post(index_obs, C_R_K_Q,W0,GG,W,VV,output,kernel_type,sample_type)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Sample_KF_post_+3A_index_obs">index_obs</code></td>
<td>
<p>a vector  where the entries with 1 have observations and entries with 0 have no observation.</p>
</td></tr>
<tr><td><code id="Sample_KF_post_+3A_c_r_k_q">C_R_K_Q</code></td>
<td>
<p>a list of matrices to compute the inverse covariance matrix in the dynamic linear model.</p>
</td></tr>
<tr><td><code id="Sample_KF_post_+3A_gg">GG</code></td>
<td>
<p>a list of matrices defined in the dynamic linear model. 
</p>
</td></tr>
<tr><td><code id="Sample_KF_post_+3A_w">W</code></td>
<td>
<p>a list of coefficient matrices defined in the dynamic linear model. 
</p>
</td></tr>
<tr><td><code id="Sample_KF_post_+3A_c0">C0</code></td>
<td>
<p>the covariance matrix of the stationary distribution defined in the dynamic linear model. 
</p>
</td></tr>
<tr><td><code id="Sample_KF_post_+3A_vv">VV</code></td>
<td>
<p>a numerical value of the variance of the nugget parameter. </p>
</td></tr>
<tr><td><code id="Sample_KF_post_+3A_output">output</code></td>
<td>
<p>a vector of the output. </p>
</td></tr>
<tr><td><code id="Sample_KF_post_+3A_kernel_type">kernel_type</code></td>
<td>
<p>a <code>character</code> to specify the type of kernel to use. The current version supports kernel_type to be &quot;matern_5_2&quot; or &quot;exp&quot;, meaning that the matern kernel with roughness parameter being 2.5 or 0.5 (exponent kernel), respectively. </p>
</td></tr>
<tr><td><code id="Sample_KF_post_+3A_sample_type">sample_type</code></td>
<td>
<p>a integer to specify the type of sample we need. 0 means the states. 1 means the first value of each state vector. 2 means the noisy observations. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of the posterior samples. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>


<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>.  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2019), <em>fast nonseparable gaussian stochastic process with application to methylation level interpolation</em>.  <em>Journal of Computational and Graphical Statistics</em>, In Press, arXiv:1711.11501.
</p>
<p>Campagnoli P, Petris G, Petrone S. (2009), <em>Dynamic linear model with R</em>. Springer-Verlag New York.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Sample_KF_post">Sample_KF_post</a></code> for more details about sampling from the posterior distribution.

</p>

<hr>
<h2 id='show'>

Show an <code>fgasp</code> object.
</h2><span id='topic+show'></span><span id='topic+show.fgasp'></span><span id='topic+show.fgasp-class'></span><span id='topic+show+2Cfgasp-method'></span>

<h3>Description</h3>


<p>Function to print the codefgasp object. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'fgasp'
show(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="show_+3A_object">object</code></td>
<td>
<p>an object of  class <code>fgasp</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>


<p>Mengyang Gu [aut, cre]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Hartikainen, J. and Sarkka, S. (2010). <em>Kalman filtering and smoothing solutions to temporal gaussian process regression models</em>,  <em>Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop</em>,  379-384.
</p>
<p>M. Gu, Y. Xu (2017), <em>Nonseparable Gaussian stochastic process: a unified
view and computational strategy</em>, arXiv:1711.11501.
</p>
<p>M. Gu, X. Wang and J.O. Berger (2018), <em>Robust Gaussian Stochastic Process Emulation</em>, <em>Annals of Statistics</em>, <b>46</b>, 3038-3066.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#-------------------------------------
# Example: a simple example with noise 
#-------------------------------------

y_R&lt;-function(x){
  cos(2*pi*x)
}

###let's test for 2000 observations
set.seed(1)
num_obs=2000
input=runif(num_obs)

output=y_R(input)+rnorm(num_obs,mean=0,sd=0.1)

##constucting the fgasp.model
fgasp.model=fgasp(input, output)
show(fgasp.model)


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
