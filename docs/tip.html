<!DOCTYPE html><html><head><title>Help for package tip</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tip}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bcm-class'><p>Bayesian Clustering Model (bcm) S4 class.</p></a></li>
<li><a href='#get_cpt_neighbors'><p>Estimate the number of similar subjects</p></a></li>
<li><a href='#ggnet2_network_plot'><p>Visualize the posterior similarity matrix (i.e., posterior probability matrix)</p></a></li>
<li><a href='#ggplot_line_point'><p>Plot connected points using ggplot2</p></a></li>
<li><a href='#ggplot_number_of_clusters_hist'><p>Plot the posterior distribution of the number of clusters.</p></a></li>
<li><a href='#ggplot_number_of_clusters_trace'><p>Plot the trace plot of the posterior number of clusters</p></a></li>
<li><a href='#partition_undirected_graph'><p>Partition an undirected graph</p></a></li>
<li><a href='#plot,bcm,missing-method'><p>Generate plots from a Bayesian Clustering Model (bcm) object</p></a></li>
<li><a href='#tip'><p>Bayesian Clustering with the Table Invitation Prior</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Bayesian Clustering Using the Table Invitation Prior (TIP)</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Cluster data without specifying the number of clusters using the Table Invitation Prior (TIP) introduced in the paper "Clustering Gene Expression Using the Table Invitation Prior" by Charles W. Harrison, Qing He, and Hsin-Hsiung Huang (2022) &lt;<a href="https://doi.org/10.3390%2Fgenes13112036">doi:10.3390/genes13112036</a>&gt;. TIP is a Bayesian prior that uses pairwise distance and similarity information to cluster vectors, matrices, or tensors.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>rlang, igraph, network, ggplot2, GGally, LaplacesDemon,
changepoint, parallel, doParallel, foreach, methods, mniw</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, sna, mcclust, SMFilter, rmarkdown, spelling, testthat
(&ge; 3.0.0)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-11-12 01:56:07 UTC; CharlesHarrison</td>
</tr>
<tr>
<td>Author:</td>
<td>Charles W. Harrison [cre, aut, cph],
  Qing He [aut, cph],
  Hsin-Hsiung Huang [aut, cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Charles W. Harrison &lt;charleswharrison@knights.ucf.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-11-14 17:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='bcm-class'>Bayesian Clustering Model (bcm) S4 class.</h2><span id='topic+bcm-class'></span>

<h3>Description</h3>

<p>An S4 class to store the results of the Gibbs sampler.
</p>


<h3>Value</h3>

<p>An object of class bcm.
</p>


<h3>Slots</h3>


<dl>
<dt><code>n</code></dt><dd><p>Positive integer: the sample size (i.e., the number of subjects).</p>
</dd>
<dt><code>burn</code></dt><dd><p>Non-negative integer: the number of burn-in iterations in the Gibbs sampler.</p>
</dd>
<dt><code>samples</code></dt><dd><p>Positive integer: the number of sampling iterations in the Gibbs sampler.</p>
</dd>
<dt><code>posterior_assignments</code></dt><dd><p>List of vectors of positive integers: a list of vectors of cluster assignments (i.e., positive integers) for each sampling iteration in the Gibbs sampler.</p>
</dd>
<dt><code>posterior_similarity_matrix</code></dt><dd><p>Matrix: a matrix where the (i,j)th element is the posterior probability that subject i and subject j belong to the same cluster.</p>
</dd>
<dt><code>posterior_number_of_clusters</code></dt><dd><p>Vector of positive integers: each vector element is the number of clusters after posterior sampling for each sampling iteration in the Gibbs sampler.</p>
</dd>
<dt><code>prior_name</code></dt><dd><p>Character: the name of the prior used.</p>
</dd>
<dt><code>likelihood_name</code></dt><dd><p>Character: the name of the likelihood used.</p>
</dd>
</dl>

<hr>
<h2 id='get_cpt_neighbors'>Estimate the number of similar subjects</h2><span id='topic+get_cpt_neighbors'></span>

<h3>Description</h3>

<p>Estimate the number of similar subjects using univariate multiple change point detection (i.e., binary segmentation in the changepoint package).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_cpt_neighbors(.distance_matrix)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_cpt_neighbors_+3A_.distance_matrix">.distance_matrix</code></td>
<td>
<p>Matrix: a symmetric n x n matrix of distance values.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of positive integers: a vector of positive integers where the (i)th integer corresponds to the number of subjects (observations) that are similar to the (i)th subject.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Import the tip library
library(tip)

# Choose an arbitrary random seed
set.seed(007)

# Generate some data (i.e., 20 subjects described by a 5 x 1 vector)
X &lt;- matrix(rnorm(10*10),nrow=20,ncol=5)

# Compute the pairwise distances between the subjects
distance_matrix &lt;- data.matrix(dist(X))

# For each subject, find the estimate for the number of similar subjects
get_cpt_neighbors(.distance_matrix = distance_matrix)
</code></pre>

<hr>
<h2 id='ggnet2_network_plot'>Visualize the posterior similarity matrix (i.e., posterior probability matrix)</h2><span id='topic+ggnet2_network_plot'></span>

<h3>Description</h3>

<p>A function that produces a ggnet2 network plot to visualize the posterior similarity matrix (i.e., the matrix of posterior probabilities).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ggnet2_network_plot(
  .matrix_graph,
  .subject_names = vector(),
  .subject_class_names = vector(),
  .class_colors,
  .class_shapes,
  .random_seed = 7,
  .node_size = 6,
  .add_node_labels = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ggnet2_network_plot_+3A_.matrix_graph">.matrix_graph</code></td>
<td>
<p>Matrix: a matrix M where each element Mij corresponds to the posterior
probability that the (i)th subject and the (j)th subject are in the same cluster.</p>
</td></tr>
<tr><td><code id="ggnet2_network_plot_+3A_.subject_names">.subject_names</code></td>
<td>
<p>Vector of characters: an optional vector of subject names that will appear in the graph plot.</p>
</td></tr>
<tr><td><code id="ggnet2_network_plot_+3A_.subject_class_names">.subject_class_names</code></td>
<td>
<p>Vector of characters: an optional vector of class names corresponding to each subject (i.e. vertex in the graph)
which influences each vertex's color and shape. For example, the subject class names can be the true label
(for the purpose of research) or it can be any other label that analyst chooses.</p>
</td></tr>
<tr><td><code id="ggnet2_network_plot_+3A_.class_colors">.class_colors</code></td>
<td>
<p>Named vector of characters: an optional named vector of colors that
correspond to each unique value in <code>.subject_class_names</code>. The vector names are
required to be the unique .subject_class_names whereas the vector values are required to be the colors.</p>
</td></tr>
<tr><td><code id="ggnet2_network_plot_+3A_.class_shapes">.class_shapes</code></td>
<td>
<p>Named vector of integers: an optional named vector of shapes that correspond
to each unique value in the <code>.subject_class_names</code>. The vector names are required to be
the unique <code>.subject_class_names</code> whereas the vector values are required to be positive integers
(i.e., pch values like 15, 16, 17, and so on).</p>
</td></tr>
<tr><td><code id="ggnet2_network_plot_+3A_.random_seed">.random_seed</code></td>
<td>
<p>Numeric: the plot uses the random layout, so set a seed for reproducibility.</p>
</td></tr>
<tr><td><code id="ggnet2_network_plot_+3A_.node_size">.node_size</code></td>
<td>
<p>Positive integer: the size of each node (i.e., vertex) in the graph plot.</p>
</td></tr>
<tr><td><code id="ggnet2_network_plot_+3A_.add_node_labels">.add_node_labels</code></td>
<td>
<p>Boolean (i.e., TRUE or FALSE): should individual node labels be added to each node (i.e., vertex) in the graph plot?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggnet2 network plot: a network plot with respect to the undirected network given by .matrix_graph. This is used to visualize the posterior similarity matrix.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# Import the tip library
library(tip)

# Choose an arbitrary random seed to generate the data
set.seed(4*8*15*16*23*42)

# Generate a symmetric posterior probability matrix
# Each element is the probability that the two subjects belong
# to the same cluster
n1 &lt;- 10
posterior_prob_matrix &lt;- matrix(NA, nrow = n1, ncol = n1)
for(i in 1:n1){
  for(j in i:n1){
    if(i != j){
      posterior_prob_matrix[i,j] &lt;- runif(n=1,min=0,max=1)
      posterior_prob_matrix[j,i] &lt;- posterior_prob_matrix[i,j]
    }else{
      posterior_prob_matrix[i,j] &lt;- 1.0
    }
  }
}

# --- BEGIN GRAPH PLOT 1: NO COLORS OR NODE LABELS ---

# Set an arbitrary random seed
random_seed &lt;- 815

# Set add_node_labels to FALSE
add_node_labels &lt;- FALSE

# Set the node size
node_size &lt;- 6

# Construct the graph plot
ggnet2_network_plot(.matrix_graph = posterior_prob_matrix,
                    .subject_names = vector(),
                    .subject_class_names = vector(),
                    .class_colors = vector(),
                    .class_shapes = vector(),
                    .random_seed = random_seed,
                    .node_size = node_size,
                    .add_node_labels = add_node_labels)

# --- END GRAPH PLOT 1: NO COLORS OR NODE LABELS ---

# --- BEGIN GRAPH PLOT 2: NO COLORS, BUT ADD NODE LABELS ---

# Set an arbitrary random seed
random_seed &lt;- 815

# Add node labels to the plot
add_node_labels &lt;- TRUE

# Set graph nodes (i.e. subject identifier) a larger size
node_size &lt;- 6

# Add subject names to the plot
subject_names &lt;- paste("S", 1:n1, sep = "")

# Construct the graph plot
ggnet2_network_plot(.matrix_graph = posterior_prob_matrix,
                    .subject_names = subject_names,
                    .subject_class_names = vector(),
                    .class_colors = vector(),
                    .class_shapes = vector(),
                    .random_seed = random_seed,
                    .node_size = 6,
                    .add_node_labels = TRUE)

# --- END GRAPH PLOT 2: NO COLORS, BUT ADD NODE LABELS ---


# --- BEGIN GRAPH PLOT 3: ADD COLORS AND NODE LABELS ---

# Set an arbitrary random seed
random_seed &lt;- 815

# Add node labels to the plot
add_node_labels &lt;- TRUE

# Set graph nodes (i.e. subject identifier) a larger size
node_size &lt;- 10

# Add subject names to the plot
subject_names &lt;- paste("S", 1:n1, sep = "")

# Create a vector of class labels
subject_class_names &lt;- c("Class2","Class2","Class1","Class2","Class1",
                              "Class1","Class2","Class1","Class1","Class2")

# Assign a color to each class; this can be a character value or a hex value
class_colors &lt;- c("Class1" = "skyblue", "Class2" = "#FF9999")

# Assign a pch integer value to each class
class_shapes &lt;- c("Class1" = 16, "Class2" = 17)

# Generate the plot
ggnet2_network_plot(.matrix_graph = posterior_prob_matrix,
                    .subject_names = subject_names,
                    .subject_class_names = subject_class_names,
                    .class_colors = class_colors,
                    .class_shapes = class_shapes,
                    .random_seed = random_seed,
                    .node_size = node_size,
                    .add_node_labels = add_node_labels)

# --- END GRAPH PLOT 3: ADD COLORS AND NODE LABELS ---

</code></pre>

<hr>
<h2 id='ggplot_line_point'>Plot connected points using ggplot2</h2><span id='topic+ggplot_line_point'></span>

<h3>Description</h3>

<p>A function to that produces a ggplot2 plot of .y versus .x
where points are added via geom_point() and the points are connected via geom_line().
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ggplot_line_point(.x, .y, .xlab = "", .ylab = "")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ggplot_line_point_+3A_.x">.x</code></td>
<td>
<p>The variable on the horizontal axis.</p>
</td></tr>
<tr><td><code id="ggplot_line_point_+3A_.y">.y</code></td>
<td>
<p>The variable on the vertical axis.</p>
</td></tr>
<tr><td><code id="ggplot_line_point_+3A_.xlab">.xlab</code></td>
<td>
<p>Character: the label on the horizontal axis.</p>
</td></tr>
<tr><td><code id="ggplot_line_point_+3A_.ylab">.ylab</code></td>
<td>
<p>Character: the label on the vertical axis.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot2 geom_line + geom_point plot: a ggplot2 plot of .y versus .x with a label .xlab on the horizontal axis and label .ylab on the vertical axis.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Import the tip library
library(tip)

# Create the variable that appears on the horizontal axis
x &lt;- 1:10

# Create the variable that appears on the vertical axis
y &lt;- rnorm(n = length(x), mean = 3, sd = 1)

# Create a label that appears on the horizontal axis
xlab &lt;- "x"

# Create a label that appears on the vertical axis
ylab &lt;- "y"

# Create the plot of y versus x with
ggplot_line_point(.x = x, .y = y, .xlab = xlab, .ylab = ylab)
</code></pre>

<hr>
<h2 id='ggplot_number_of_clusters_hist'>Plot the posterior distribution of the number of clusters.</h2><span id='topic+ggplot_number_of_clusters_hist'></span>

<h3>Description</h3>

<p>A function that produces a ggplot bar chart (i.e., geom_bar) that corresponds
to the posterior number of clusters. The vertical axis is normalized so that it displays
the posterior probability.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ggplot_number_of_clusters_hist(.posterior_number_of_clusters)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ggplot_number_of_clusters_hist_+3A_.posterior_number_of_clusters">.posterior_number_of_clusters</code></td>
<td>
<p>Vector of positive integers: each integer corresponds to the
number of clusters after posterior sampling for a given sampling iteration in the Gibbs sampler.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot2 geom_bar plot: a plot of the distribution of the posterior number of clusters
computed after each sampling iteration in the Gibbs sampler.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Import the tip library
library(tip)

# Generate a vector of positive integers
# Example: the posterior number of clusters computed after posterior
# sampling in each sampling iteration of the Gibbs sampler.
num_clusters &lt;- c(1,2,2,2,2,3,3,1,2,3,3,3,1,3)

# Generate the plot of the posterior number of clusters versus the
# sampling iteration number in the Gibbs sampler.
ggplot_number_of_clusters_hist(.posterior_number_of_clusters = num_clusters)
</code></pre>

<hr>
<h2 id='ggplot_number_of_clusters_trace'>Plot the trace plot of the posterior number of clusters</h2><span id='topic+ggplot_number_of_clusters_trace'></span>

<h3>Description</h3>

<p>A function that produces a ggplot2 trace plot (i.e., geom_line)
with respect to the posterior number of clusters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ggplot_number_of_clusters_trace(.posterior_number_of_clusters)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ggplot_number_of_clusters_trace_+3A_.posterior_number_of_clusters">.posterior_number_of_clusters</code></td>
<td>
<p>Vector of positive integers: each (s)th element denotes the number of clusters after posterior sampling
for each iteration s = 1, 2, ..., <code>samples</code> + <code>burn</code> in the Gibbs sampler.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot2 geom_line plot: a plot of the posterior number of clusters in each Gibbs sampling iteration versus the Gibbs sampling iteration number.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Import the tip library
library(tip)

# Generate a vector of positive integers
# Example: the posterior number of clusters computed after posterior
# sampling in each sampling iteration of the Gibbs sampler.
num_clusters &lt;- c(1,2,2,2,2,3,3,1,2,3,3,3,1,3)

# Generate the plot of the posterior number of clusters versus the
# sampling iteration number in the Gibbs sampler.
ggplot_number_of_clusters_trace(.posterior_number_of_clusters = num_clusters)
</code></pre>

<hr>
<h2 id='partition_undirected_graph'>Partition an undirected graph</h2><span id='topic+partition_undirected_graph'></span>

<h3>Description</h3>

<p>A function that iteratively applies the transformation max(0, .graph_matrix - cutoff) until
there are &lt;.num_components&gt; graph components where cutoff = cutoff + .step_size. This is used to generate the one-cluster graph and plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>partition_undirected_graph(.graph_matrix, .num_components, .step_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="partition_undirected_graph_+3A_.graph_matrix">.graph_matrix</code></td>
<td>
<p>Matrix: a symmetric matrix that the analyst wishes to decompose into &lt;.num_components&gt; components.</p>
</td></tr>
<tr><td><code id="partition_undirected_graph_+3A_.num_components">.num_components</code></td>
<td>
<p>Positive integer: the number of components that the analyst wishes to decompose &lt;.graph_matrix&gt; into.</p>
</td></tr>
<tr><td><code id="partition_undirected_graph_+3A_.step_size">.step_size</code></td>
<td>
<p>Positive numeric: the size of the update for the cutoff in the transformation max(0, .graph_matrix - cutoff)
where cutoff = cutoff + .step_size.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List with three elements:
</p>
<table>
<tr><td><code>graph_component_members</code></td>
<td>
<p>Vector. A vector of positive integers: the (i)th element is the graph component assignment for the (i)th subject.</p>
</td></tr>
<tr><td><code>cutoff</code></td>
<td>
<p>Numeric. The value max(0, g_i,j - cutoff) so that there are &lt;<code>.num_components</code>&gt; components in the graph.</p>
</td></tr>
<tr><td><code>partitioned_graph_matrix</code></td>
<td>
<p>Matrix. The graph with &lt;<code>.num_components</code>&gt; components (parts).</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># Import the tip library
library(tip)

# Choose an arbitrary random seed to generate the data
set.seed(4*8*15*16*23*42)

# Generate a symmetric posterior probability matrix
# Each element is the probability that the two subjects belong
# to the same cluster
n1 &lt;- 10
posterior_prob_matrix &lt;- matrix(NA, nrow = n1, ncol = n1)
for(i in 1:n1){
  for(j in i:n1){
    if(i != j){
      posterior_prob_matrix[i,j] &lt;- runif(n=1,min=0,max=1)
      posterior_prob_matrix[j,i] &lt;- posterior_prob_matrix[i,j]
    }else{
      posterior_prob_matrix[i,j] &lt;- 1.0
    }
  }
}

# Generate a one-cluster graph (i.e., partitioned_graph_matrix)
partition_undirected_graph(.graph_matrix = posterior_prob_matrix,
                           .num_components = 1,
                           .step_size = 0.001)
</code></pre>

<hr>
<h2 id='plot+2Cbcm+2Cmissing-method'>Generate plots from a Bayesian Clustering Model (bcm) object</h2><span id='topic+plot+2Cbcm+2Cmissing-method'></span><span id='topic+plot'></span>

<h3>Description</h3>

<p>Generate plots from a Bayesian Clustering Model (bcm) object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'bcm,missing'
plot(x, y, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot+2B2Cbcm+2B2Cmissing-method_+3A_x">x</code></td>
<td>
<p>bcm object: a Bayesian Clustering Model (bcm) object fit to a dataset</p>
</td></tr>
<tr><td><code id="plot+2B2Cbcm+2B2Cmissing-method_+3A_y">y</code></td>
<td>
<p>Not used.</p>
</td></tr>
<tr><td><code id="plot+2B2Cbcm+2B2Cmissing-method_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List: a list of two ggplot2 plots that are constructed using the information contained in an object of class bcm (Bayesian Clustering Model). A bcm object contains the clustering results from a clustering model that uses the TIP prior.
</p>
<table>
<tr><td><code>trace_plot_posterior_number_of_clusters</code></td>
<td>
<p>ggplot2 Plot: a plot of the posterior number of clusters (sampling iterations only) versus the corresponding sampling iteration number from the Gibbs sampler.</p>
</td></tr>
<tr><td><code>histogram_posterior_number_of_clusters</code></td>
<td>
<p>ggplot2 Plot: a bar plot of the posterior number of clusters (sampling iterations only) from the Gibbs sampler.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>

  # Import the tip library
  library(tip)

  # Import the iris dataset
  data(iris)

  # The first 4 columns are the data whereas
  # the 5th column refers to the true labels
  X &lt;- data.matrix(iris[,c("Sepal.Length",
                           "Sepal.Width",
                           "Petal.Length",
                           "Petal.Width")])

  # Extract the true labels (optional)
  # True labels are only necessary for constructing network
  # graphs that incorporate the true labels; this is often
  # for research.
  true_labels &lt;- iris[,"Species"]

  # Compute the distance matrix
  distance_matrix &lt;- data.matrix(dist(X))

  # Compute the temperature parameter estiamte
  temperature &lt;- 1/median(distance_matrix[upper.tri(distance_matrix)])

  # For each subject, compute the point estimate for the number of similar
  # subjects using  univariate multiple change point detection (i.e.)
  init_num_neighbors = get_cpt_neighbors(.distance_matrix = distance_matrix)

  # Set the number of burn-in iterations in the Gibbs samlper
  # RECOMMENDATION: burn &gt;= 1000
  burn &lt;- 1000

  # Set the number of sampling iterations in the Gibbs sampler
  # RECOMMENDATION: burn &gt;= 1000
  samples &lt;- 1000

  # Set the subject names
  names_subjects &lt;- paste(1:dim(iris)[1])

  # Run TIP clustering using only the prior
  # --&gt; That is, the likelihood function is constant
  tip1 &lt;- tip(.data = data.matrix(X),
              .burn = burn,
              .samples = samples,
              .similarity_matrix = exp(-1.0*temperature*distance_matrix),
              .init_num_neighbors = init_num_neighbors,
              .likelihood_model = "NIW",
              .subject_names = names_subjects,
              .num_cores = 1)

  # Produce plots for the Bayesian Clustering Model
  tip_plots &lt;- plot(tip1)

</code></pre>

<hr>
<h2 id='tip'>Bayesian Clustering with the Table Invitation Prior</h2><span id='topic+tip'></span>

<h3>Description</h3>

<p>Bayesian clustering with the Table Invitation Prior (TIP) and optional likelihood functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tip(
  .data = list(),
  .burn = 1000,
  .samples = 1000,
  .similarity_matrix,
  .init_num_neighbors,
  .likelihood_model = "CONSTANT",
  .subject_names = vector(),
  .num_cores = 1,
  .step_size = 0.001
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tip_+3A_.data">.data</code></td>
<td>
<p>Data frame (vectors comprise a row in a data frame; NIW only) or a list of matrices (MNIW only) that the analyst wishes to cluster. Note: if .likelihood_model = &quot;CONSTANT&quot;, then the .data argument has no effect.</p>
</td></tr>
<tr><td><code id="tip_+3A_.burn">.burn</code></td>
<td>
<p>Non-negative integer: the number of burn-in iterations in the Gibbs sampler.</p>
</td></tr>
<tr><td><code id="tip_+3A_.samples">.samples</code></td>
<td>
<p>Positive integer: the number of sampling iterations in the Gibbs sampler.</p>
</td></tr>
<tr><td><code id="tip_+3A_.similarity_matrix">.similarity_matrix</code></td>
<td>
<p>Matrix: an n x n matrix of similarity values.</p>
</td></tr>
<tr><td><code id="tip_+3A_.init_num_neighbors">.init_num_neighbors</code></td>
<td>
<p>Vector of positive integers: each (i)th positive integer corresponds to the estimate of the number of subjects that are similar to the (i)th subject.</p>
</td></tr>
<tr><td><code id="tip_+3A_.likelihood_model">.likelihood_model</code></td>
<td>
<p>Character: the name of the likelihood model used to compute the posterior probabilities. Options: &quot;NIW&quot; (vectors; .data is a dataframe), &quot;MNIW&quot; (matrices; .data is a list of matrices), or &quot;CONSTANT&quot; (vector, matrices, and tensors; .data is an empty list)</p>
</td></tr>
<tr><td><code id="tip_+3A_.subject_names">.subject_names</code></td>
<td>
<p>Vector of characters: an optional vector of names for the individual subjects. This is useful for the plotting function.</p>
</td></tr>
<tr><td><code id="tip_+3A_.num_cores">.num_cores</code></td>
<td>
<p>Positive integer: the number of cores to use.</p>
</td></tr>
<tr><td><code id="tip_+3A_.step_size">.step_size</code></td>
<td>
<p>Positive numeric: A parameter used to ensure matrices are invertible. A small number is iteratively added to a matrix diagonal (if necessary) until the matrix is invertible.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Object of class bcm: bcm denotes a &quot;Bayesian Clustering Model&quot; object that contains the results from a clustering model that uses the TIP prior.
</p>
<table>
<tr><td><code>n</code></td>
<td>
<p>Positive integer: the sample size or number of subjects.</p>
</td></tr>
<tr><td><code>burn</code></td>
<td>
<p>Non-negative integer: the number of burn-in iterations in the Gibbs sampler.</p>
</td></tr>
<tr><td><code>samples</code></td>
<td>
<p>Positive integer: the number of sampling iterations in the Gibbs sampler.</p>
</td></tr>
<tr><td><code>posterior_assignments</code></td>
<td>
<p>List: a list of &lt;<code>samples</code>&gt; vectors where the (i)th element of each vector is the posterior cluster assignment for the (i)th subject.</p>
</td></tr>
<tr><td><code>posterior_similarity_matrix</code></td>
<td>
<p>Matrix: an <code>n</code> x <code>n</code> matrix where the (i,j)th element is the posterior probability that subject i and subject j are in the same cluster.</p>
</td></tr>
<tr><td><code>posterior_number_of_clusters</code></td>
<td>
<p>Vector of positive integers: a vector where the jth element is the number of clusters after posterior sampling (i.e., the posterior number of clusters).</p>
</td></tr>
<tr><td><code>prior_name</code></td>
<td>
<p>Character: the name of the prior used.</p>
</td></tr>
<tr><td><code>likelihood_name</code></td>
<td>
<p>Character: the name of the likelihood used.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>

  ##### BEGIN EXAMPLE 1: Clustering the Iris Dataset (vector clustering) #####
  ##### Prior Distribution: Table Invitation Prior (TIP)
  ##### Likelihood Model: Normal Inverse Wishart (NIW)

  # Import the tip library
  library(tip)

  # Import the iris dataset
  data(iris)

  # The first 4 columns are the data whereas
  # the 5th column refers to the true labels
  X &lt;- data.matrix(iris[,c("Sepal.Length",
                           "Sepal.Width",
                           "Petal.Length",
                           "Petal.Width")])

  # Extract the true labels (optional)
  # True labels are only necessary for constructing network
  # graphs that incorporate the true labels; this is often
  # for research.
  true_labels &lt;- iris[,"Species"]

  # Compute the distance matrix
  distance_matrix &lt;- data.matrix(dist(X))

  # Compute the temperature parameter estiamte
  temperature &lt;- 1/median(distance_matrix[upper.tri(distance_matrix)])

  # For each subject, compute the point estimate for the number of similar
  # subjects using  univariate multiple change point detection (i.e.)
  init_num_neighbors = get_cpt_neighbors(.distance_matrix = distance_matrix)

  # Set the number of burn-in iterations in the Gibbs samlper
  # RECOMMENDATION: burn &gt;= 1000
  burn &lt;- 1000

  # Set the number of sampling iterations in the Gibbs sampler
  # RECOMMENDATION: samples &gt;= 1000
  samples &lt;- 1000

  # Set the subject names
  names_subjects &lt;- paste(1:dim(iris)[1])

  # Run TIP clustering using only the prior
  # --&gt; That is, the likelihood function is constant
  tip1 &lt;- tip(.data = data.matrix(X),
              .burn = burn,
              .samples = samples,
              .similarity_matrix = exp(-1.0*temperature*distance_matrix),
              .init_num_neighbors = init_num_neighbors,
              .likelihood_model = "NIW",
              .subject_names = names_subjects,
              .num_cores = 1)

  # Produce plots for the Bayesian Clustering Model
  tip_plots &lt;- plot(tip1)

  # View the posterior distribution of the number of clusters
  tip_plots$histogram_posterior_number_of_clusters

  # View the trace plot with respect to the posterior number of clusters
  tip_plots$trace_plot_posterior_number_of_clusters

  # Extract posterior cluster assignments using the Posterior Expected Adjusted Rand (PEAR) index
  cluster_assignments &lt;- mcclust::maxpear(psm = tip1@posterior_similarity_matrix)$cl

  # If the true labels are available, then show the cluster result via a contingency table
  table(data.frame(true_label = true_labels,
                   cluster_assignment = cluster_assignments))

  # Create the one component graph with minimum entropy
  partition_list &lt;- partition_undirected_graph(.graph_matrix = tip1@posterior_similarity_matrix,
                                               .num_components = 1,
                                               .step_size = 0.001)

  # Associate class labels and colors for the plot
  class_palette_colors &lt;- c("setosa" = "blue",
                            "versicolor" = 'green',
                            "virginica" = "orange")

  # Associate class labels and shapes for the plot
  class_palette_shapes &lt;- c("setosa" = 19,
                            "versicolor" = 18,
                            "virginica" = 17)

  # Visualize the posterior similarity matrix by constructing a graph plot of
  # the one-cluster graph. The true labels are used here (below they are not).
  ggnet2_network_plot(.matrix_graph = partition_list$partitioned_graph_matrix,
                      .subject_names = NA,
                      .subject_class_names = true_labels,
                      .class_colors = class_palette_colors,
                      .class_shapes = class_palette_shapes,
                      .node_size = 2,
                      .add_node_labels = FALSE)

  # If true labels are not available, then construct a network plot
  # of the one-cluster graph without any class labels.
  ggnet2_network_plot(.matrix_graph = partition_list$partitioned_graph_matrix,
                      .subject_names = names_subjects,
                      .node_size = 2,
                      .add_node_labels = TRUE)

  # If true labels are not available, then construct a network plot
  # of the one-cluster graph without any class labels. Also, suppress
  # the subject labels.
  ggnet2_network_plot(.matrix_graph = partition_list$partitioned_graph_matrix,
                      .subject_names = names_subjects,
                      .node_size = 2,
                      .add_node_labels = FALSE)
  ##### END EXAMPLE 1: Vector Clustering (NIW) #####



  ##### BEGIN EXAMPLE 2: Clustering the US Arrests Dataset (vector clustering) #####
  ##### Prior Distribution: Table Invitation Prior (TIP)
  ##### Likelihood Model: Normal Inverse Wishart (NIW)

  # Import the TIP library
  library(tip)

  # Import the US Arrests dataset
  data(USArrests)

  # Convert the data to a matrix
  X &lt;- data.matrix(USArrests)

  # Compute the distance matrix
  distance_matrix &lt;- data.matrix(dist(X))

  # Compute the temperature parameter estiamte
  temperature &lt;- 1/median(distance_matrix[upper.tri(distance_matrix)])

  # For each subject, compute the point estimate for the number of similar
  # subjects using  univariate multiple change point detection (i.e.)
  init_num_neighbors = get_cpt_neighbors(.distance_matrix = distance_matrix)

  # Set the number of burn-in iterations in the Gibbs samlper
  # RECOMENDATION: *** burn &gt;= 1000 ***
  burn &lt;- 1000

  # Set the number of sampling iterations in the Gibbs sampler
  # RECOMENDATION: *** samples &gt;= 1000 ***
  samples &lt;- 1000

  # Extract the state names
  names_subjects &lt;- rownames(USArrests)

  # Run TIP clustering using only the prior
  # --&gt; That is, the likelihood function is constant
  tip1 &lt;- tip(.data = data.matrix(X),
              .burn = burn,
              .samples = samples,
              .similarity_matrix = exp(-1.0*temperature*distance_matrix),
              .init_num_neighbors = init_num_neighbors,
              .likelihood_model = "NIW",
              .subject_names = names_subjects,
              .num_cores = 1)

  # Produce plots for the Bayesian Clustering Model
  tip_plots &lt;- plot(tip1)

  # View the posterior distribution of the number of clusters
  tip_plots$trace_plot_posterior_number_of_clusters

  # View the trace plot with respect to the posterior number of clusters
  tip_plots$trace_plot_posterior_number_of_clusters

  # Extract posterior cluster assignments using the Posterior Expected Adjusted Rand (PEAR) index
  cluster_assignments &lt;- mcclust::maxpear(psm = tip1@posterior_similarity_matrix)$cl

  # Create a list where each element stores the cluster assignments
  cluster_assignment_list &lt;- list()
  for(k in 1:length(unique(cluster_assignments))){
    cluster_assignment_list[[k]] &lt;- names_subjects[which(cluster_assignments == k)]
  }
  cluster_assignment_list

  # Create the one component graph with minimum entropy
  partition_list &lt;- partition_undirected_graph(.graph_matrix = tip1@posterior_similarity_matrix,
                                               .num_components = 1,
                                               .step_size = 0.001)

  # View the state names
  # names_subjects

  # Create a vector of true region labels to see if there is a pattern
  true_region &lt;- c("Southeast", "West", "Southwest", "Southeast", "West", "West",
                   "Northeast", "Northeast", "Southeast", "Southeast", "West", "West",
                   "Midwest", "Midwest", "Midwest", "Midwest", "Southeast", "Southeast",
                   "Northeast", "Northeast", "Northeast", "Midwest", "Midwest", "Southeast",
                   "Midwest", "West", "Midwest", "West", "Northeast", "Northeast",
                   "Southwest", "Northeast", "Southeast", "Midwest", "Midwest", "Southwest",
                   "West", "Northeast", "Northeast", "Southeast", "Midwest", "Southeast",
                   "Southwest", "West", "Northeast", "Southeast", "West", "Southeast",
                   "Midwest", "West")

  names_subjects

  # Associate class labels and colors for the plot
  class_palette_colors &lt;- c("Northeast" = "blue",
                            "Southeast" = "red",
                            "Midwest" = "purple",
                            "Southwest" = "orange",
                            "West" = "green")

  # Associate class labels and shapes for the plot
  class_palette_shapes &lt;- c("Northeast" = 15,
                            "Southeast" = 16,
                            "Midwest" = 17,
                            "Southwest" = 18,
                            "West" = 19)

  # Visualize the posterior similarity matrix by constructing a graph plot of
  # the one-cluster graph. The true labels are used here (below they are not).
  ggnet2_network_plot(.matrix_graph = partition_list$partitioned_graph_matrix,
                      .subject_names = names_subjects,
                      .subject_class_names = true_region,
                      .class_colors = class_palette_colors,
                      .class_shapes = class_palette_shapes,
                      .node_size = 2,
                      .add_node_labels = TRUE)


  # Visualize the posterior similarity matrix by constructing a graph plot of
  # the one-cluster graph. The true labels are used here (below they are not).
  # Remove the subject names with .add_node_labels = FALSE
  ggnet2_network_plot(.matrix_graph = partition_list$partitioned_graph_matrix,
                      .subject_names = names_subjects,
                      .subject_class_names = true_region,
                      .class_colors = class_palette_colors,
                      .class_shapes = class_palette_shapes,
                      .node_size = 2,
                      .add_node_labels = FALSE)

  # Construct a network plot without class labels
  ggnet2_network_plot(.matrix_graph = partition_list$partitioned_graph_matrix,
                      .subject_names = names_subjects,
                      .node_size = 2,
                      .add_node_labels = TRUE)

  # Construct a network plot without class labels. Also, suppress
  # the subject labels.
  ggnet2_network_plot(.matrix_graph = partition_list$partitioned_graph_matrix,
                      .subject_names = names_subjects,
                      .node_size = 2,
                      .add_node_labels = FALSE)
  ##### END EXAMPLE 2: Clustering the US Arrests Dataset (vector clustering) #####


## Not run: 

  ##### BEGIN EXAMPLE 3: Clustering gene expression data (vector clustering) #####
  ##### Prior Distribution: Table Invitation Prior (TIP)
  ##### Likelihood Model: Normal Inverse Wishart (NIW)

  # Import the TIP library
  library(tip)

  # ----- Dataset information -----
  # The data were accessed from the UCI machine learning repository
  # Original link: https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq
  # Source: Samuele Fiorini, samuele.fiorini '@' dibris.unige.it,
  # University of Genoa, redistributed under Creative Commons license
  # (http://creativecommons.org/licenses/by/3.0/legalcode)
  # from https://www.synapse.org/#!Synapse:syn4301332.
  # Data Set Information: Samples (instances) are stored row-wise. Variables
  # (attributes) of each sample are RNA-Seq gene expression levels measured by
  # illumina HiSeq platform.
  # Relevant Papers: Weinstein, John N., et al. 'The cancer genome atlas pan-cancer
  # analysis project.' Nature genetics 45.10 (2013): 1113-1120.
  # -------------------------------

  # Import the data (see the provided link above)
  X &lt;- read.csv("data.csv")
  true_labels &lt;- read.csv("labels.csv")

  # Extract the true indices
  subject_names &lt;- true_labels$X

  # Extract the true classes
  true_labels &lt;- true_labels$Class

  # Convert the dataset into a matrix
  X &lt;- data.matrix(X)

  ##### BEGIN: Apply PCA to the dataset #####

  # Step 1: perform Prinicpal Components Analysis (PCA) on the dataset
  pca1 &lt;- prcomp(X)

  # Step 2: compute summary information
  summary1 &lt;- summary(pca1)

  # Step 3: plot the cumulative percentage of the variance
  # explained against the number of principal components
  tip::ggplot_line_point(.x = 1:length(summary1$importance['Cumulative Proportion',]),
                         .y = summary1$importance['Cumulative Proportion',],
                         .xlab = "Number of Principal Components",
                         .ylab = "Cumulative Percentage of the Variance Explained")

  # The number of principal components chosen is 7, and
  # the 7 principal components explain roughly 80% of
  # the variance
  num_principal_components &lt;- which(summary1$importance['Cumulative Proportion',] &lt;= 0.8)

  # The clustering is applied to the principal component dataset
  X &lt;- pca1$x[,as.numeric(num_principal_components)]
  ##### END: Apply PCA to the dataset #####

  # Compute the distance matrix
  distance_matrix &lt;- data.matrix(dist(X))

  # Compute the temperature parameter estiamte
  temperature &lt;- 1/median(distance_matrix[upper.tri(distance_matrix)])

  # For each subject, compute the point estimate for the number of similar
  # subjects using  univariate multiple change point detection (i.e.)
  init_num_neighbors = get_cpt_neighbors(.distance_matrix = distance_matrix)

  # Set the number of burn-in iterations in the Gibbs samlper
  # RECOMENDATION: *** burn &gt;= 1000 ***
  burn &lt;- 1000

  # Set the number of sampling iterations in the Gibbs sampler
  # RECOMENDATION: *** samples &gt;= 1000 ***
  samples &lt;- 1000

  # Run TIP clustering using only the prior
  # --&gt; That is, the likelihood function is constant
  tip1 &lt;- tip(.data = data.matrix(X),
              .burn = burn,
              .samples = samples,
              .similarity_matrix = exp(-1.0*temperature*distance_matrix),
              .init_num_neighbors = init_num_neighbors,
              .likelihood_model = "NIW",
              .subject_names = c(),
              .num_cores = 1)

  # Produce plots for the Bayesian Clustering Model
  tip_plots &lt;- plot(tip1)

  # View the posterior distribution of the number of clusters
  tip_plots$trace_plot_posterior_number_of_clusters

  # View the trace plot with respect to the posterior number of clusters
  tip_plots$trace_plot_posterior_number_of_clusters

  # Extract posterior cluster assignments using the Posterior Expected Adjusted Rand (PEAR) index
  cluster_assignments &lt;- mcclust::maxpear(psm = tip1@posterior_similarity_matrix)$cl

  # Create a list where each element stores the cluster assignments
  cluster_assignment_list &lt;- list()
  for(k in 1:length(unique(cluster_assignments))){
    cluster_assignment_list[[k]] &lt;- true_labels[which(cluster_assignments == k)]
  }
  cluster_assignment_list

  # Create the one component graph with minimum entropy
  partition_list &lt;- partition_undirected_graph(.graph_matrix = tip1@posterior_similarity_matrix,
                                               .num_components = 1,
                                               .step_size = 0.001)

  # Associate class labels and shapes for the plot
  class_palette_shapes &lt;- c("PRAD" = 19,
                            "BRCA" = 18,
                            "KIRC" = 17,
                            "LUAD" = 16,
                            "COAD" = 15)

  # Associate class labels and colors for the plot
  class_palette_colors &lt;- c("PRAD" = "blue",
                            "BRCA" = "red",
                            "KIRC" = "black",
                            "LUAD" = "green",
                            "COAD" = "orange")

  # Visualize the posterior similarity matrix by constructing a graph plot of
  # the one-cluster graph. The true labels are used here (below they are not).
  ggnet2_network_plot(.matrix_graph = partition_list$partitioned_graph_matrix,
                      .subject_names = subject_names,
                      .subject_class_names = true_labels,
                      .class_colors = class_palette_colors,
                      .class_shapes = class_palette_shapes,
                      .node_size = 2,
                      .add_node_labels = TRUE)

  # Visualize the posterior similarity matrix by constructing a graph plot of
  # the one-cluster graph. The true labels are used here (below they are not).
  # Remove the subject names with .add_node_labels = FALSE
  ggnet2_network_plot(.matrix_graph = partition_list$partitioned_graph_matrix,
                      .subject_names = subject_names,
                      .subject_class_names = true_labels,
                      .class_colors = class_palette_colors,
                      .class_shapes = class_palette_shapes,
                      .node_size = 2,
                      .add_node_labels = FALSE)

  # Construct a network plot without class labels but with subject labels
  ggnet2_network_plot(.matrix_graph = partition_list$partitioned_graph_matrix,
                      .subject_names = subject_names,
                      .node_size = 2,
                      .add_node_labels = TRUE)

  # Construct a network plot without class labels and subject labels
  ggnet2_network_plot(.matrix_graph = partition_list$partitioned_graph_matrix,
                      .subject_names = subject_names,
                      .node_size = 2,
                      .add_node_labels = FALSE)
  ##### END EXAMPLE 3: Clustering gene expression data (vector clustering) #####

## End(Not run)



  ##### BEGIN EXAMPLE 4: Matrix Clustering (MNIW) #####
  ##### Prior Distribution: Table Invitation Prior
  ##### Likelihood Model: Matrix Normal Inverse Wishart (MNIW)

  # Import the tip library
  library(tip)

  # A function to generate random matrices from a matrix normal distribution
  random_mat_normal &lt;- function(mu, num_rows, num_cols){
    LaplacesDemon::rmatrixnorm(M = matrix(mu,
                                          nrow = num_rows,
                                          ncol = num_cols),
                               U = diag(num_rows),
                               V = diag(num_cols))
  }

  # Generate 3 clusters of matrices
  p &lt;- 5
  m &lt;- 3
  c1 &lt;- lapply(1:20, function(x) random_mat_normal(mu = 0, num_rows = m, num_cols = p))
  c2 &lt;- lapply(1:25, function(x) random_mat_normal(mu = 5, num_rows = m, num_cols = p))
  c3 &lt;- lapply(1:30, function(x) random_mat_normal(mu = -5, num_rows = m, num_cols = p))

  # Put all the data into a list
  data_list &lt;- c(c1,c2,c3)

  # Create a vector of true labels. True labels are only necessary
  # for constructing network graphs that incorporate the true labels;
  # this is often useful for research.
  true_labels &lt;- c(rep("Cluster 1", 20),
                   rep("Cluster 2", 25),
                   rep("Cluster 3", 30))

  distance_matrix &lt;- matrix(NA,
                            nrow = length(true_labels),
                            ncol = length(true_labels))
  # Distance matrix
  for(i in 1:length(true_labels)){
    for(j in i:length(true_labels)){
      distance_matrix[i,j] &lt;- SMFilter::FDist2(mX = data_list[[i]],
                                               mY = data_list[[j]])
      distance_matrix[j,i] &lt;- distance_matrix[i,j]
    }
  }

  # Compute the temperature parameter estiamte
  temperature &lt;- 1/median(distance_matrix[upper.tri(distance_matrix)])

  # For each subject, compute the point estimate for the number of similar
  # subjects using  univariate multiple change point detection (i.e.)
  init_num_neighbors = get_cpt_neighbors(.distance_matrix = distance_matrix)

  # Set the number of burn-in iterations in the Gibbs samlper
  # RECOMMENDATION: burn &gt;= 1000
  burn &lt;- 1000

  # Set the number of sampling iterations in the Gibbs sampler
  # RECOMMENDATION: samples &gt;= 1000
  samples &lt;- 1000

  # Set the subject names
  names_subjects &lt;- paste(1:dim(distance_matrix)[1])

  # Run TIP clustering using only the prior
  # --&gt; That is, the likelihood function is constant
  tip1 &lt;- tip(.data = data_list,
              .burn = burn,
              .samples = samples,
              .similarity_matrix = exp(-1.0*temperature*distance_matrix),
              .init_num_neighbors = init_num_neighbors,
              .likelihood_model = "MNIW",
              .subject_names = names_subjects,
              .num_cores = 1)

  # Produce plots for the Bayesian Clustering Model
  tip_plots &lt;- plot(tip1)

  # View the posterior distribution of the number of clusters
  tip_plots$histogram_posterior_number_of_clusters

  # View the trace plot with respect to the posterior number of clusters
  tip_plots$trace_plot_posterior_number_of_clusters

  # Extract posterior cluster assignments using the Posterior Expected Adjusted Rand (PEAR) index
  cluster_assignments &lt;- mcclust::maxpear(psm = tip1@posterior_similarity_matrix)$cl

  # If the true labels are available, then show the cluster result via a contigency table
  table(data.frame(true_label = true_labels,
                   cluster_assignment = cluster_assignments))

  # Create the one component graph with minimum entropy
  partition_list &lt;- partition_undirected_graph(.graph_matrix = tip1@posterior_similarity_matrix,
                                               .num_components = 1,
                                               .step_size = 0.001)

  # Associate class labels and colors for the plot
  class_palette_colors &lt;- c("Cluster 1" = "blue",
                            "Cluster 2" = 'green',
                            "Cluster 3" = "red")

  # Associate class labels and shapes for the plot
  class_palette_shapes &lt;- c("Cluster 1" = 19,
                            "Cluster 2" = 18,
                            "Cluster 3" = 17)

  # Visualize the posterior similarity matrix by constructing a graph plot of
  # the one-cluster graph. The true labels are used here (below they are not).
  ggnet2_network_plot(.matrix_graph = partition_list$partitioned_graph_matrix,
                      .subject_names = NA,
                      .subject_class_names = true_labels,
                      .class_colors = class_palette_colors,
                      .class_shapes = class_palette_shapes,
                      .node_size = 2,
                      .add_node_labels = FALSE)

  # If true labels are not available, then construct a network plot
  # of the one-cluster graph without any class labels.
  ggnet2_network_plot(.matrix_graph = partition_list$partitioned_graph_matrix,
                      .subject_names = names_subjects,
                      .node_size = 2,
                      .add_node_labels = TRUE)

  # If true labels are not available, then construct a network plot
  # of the one-cluster graph without any class labels. Also, suppress the
  # subject labels.
  ggnet2_network_plot(.matrix_graph = partition_list$partitioned_graph_matrix,
                      .subject_names = names_subjects,
                      .node_size = 2,
                      .add_node_labels = FALSE)

  ##### END EXAMPLE 4: Matrix Clustering (MNIW) #####

  ##### BEGIN EXAMPLE 5: Tensor Clustering #####
  ##### Prior Distribution: Table Invitation Prior
  ##### Likelihood Model: CONSTANT

  # Import the tip library
  library(tip)

  # ################## NOTE ##################
  # Order 3 Tensor dimension: c(d1,d2,d3)
  # d1: number of rows
  # d2: number of columns
  # d3: number of slices
  ############################################

  # Set a random seed for reproducibility
  set.seed(007)

  # A function to generate an order-3 tensor
  generate_gaussian_tensor &lt;- function(.tensor_dimension, .mean = 0, .sd = 1){
    array(data = c(rnorm(n = prod(.tensor_dimension),
                         mean = .mean,
                         sd = .sd)),
          dim = .tensor_dimension)
  }

  # Define the tensor dimension
  tensor_dimension &lt;- c(256,256,3)

  # Generate clusters of tensors
  c1 &lt;- lapply(1:30, function(x) generate_gaussian_tensor(.tensor_dimension = tensor_dimension,
                                                          .mean = 0,
                                                          .sd = 1))


  # Generate clusters of tensors
  c2 &lt;- lapply(1:40, function(x) generate_gaussian_tensor(.tensor_dimension = tensor_dimension,
                                                          .mean = 5,
                                                          .sd = 1))


  # Generate clusters of tensors
  c3 &lt;- lapply(1:50, function(x) generate_gaussian_tensor(.tensor_dimension = tensor_dimension,
                                                          .mean = -5,
                                                          .sd = 1))

  # Make a list of tensors
  X &lt;- c(c1, c2, c3)

  # Compute the number of subjects for each cluster
  n1 &lt;- length(c1)
  n2 &lt;- length(c2)
  n3 &lt;- length(c3)

  # Create a vector of true labels. True labels are only necessary
  # for constructing network graphs that incorporate the true labels;
  # this is often useful for research.
  true_labels &lt;- c(rep("Cluster 1", n1),
                   rep("Cluster 2", n2),
                   rep("Cluster 3", n3))

  # Compute the total number of subjects
  n &lt;- length(X)

  # Construct the distance matrix
  distance_matrix &lt;- matrix(data = NA, nrow = n, ncol = n)
  for(i in 1:n){
    for(j in i:n){
      distance_matrix[i,j] &lt;- sqrt(sum((X[[i]] - X[[j]])^2))
      distance_matrix[j,i] &lt;- distance_matrix[i,j]
    }
  }

  # Compute the temperature parameter estiamte
  temperature &lt;- 1/median(distance_matrix[upper.tri(distance_matrix)])

  # For each subject, compute the point estimate for the number of similar
  # subjects using  univariate multiple change point detection (i.e.)
  init_num_neighbors = get_cpt_neighbors(.distance_matrix = distance_matrix)

  # Set the number of burn-in iterations in the Gibbs samlper
  # RECOMMENDATION: burn &gt;= 1000
  burn &lt;- 1000

  # Set the number of sampling iterations in the Gibbs sampler
  # RECOMMENDATION: samples &gt;= 1000
  samples &lt;- 1000

  # Set the subject names
  names_subjects &lt;- paste(1:n)

  # Run TIP clustering using only the prior
  # --&gt; That is, the likelihood function is constant
  tip1 &lt;- tip(.data = list(),
              .burn = burn,
              .samples = samples,
              .similarity_matrix = exp(-1.0*temperature*distance_matrix),
              .init_num_neighbors = init_num_neighbors,
              .likelihood_model = "CONSTANT",
              .subject_names = names_subjects,
              .num_cores = 1)

  # Produce plots for the Bayesian Clustering Model
  tip_plots &lt;- plot(tip1)

  # View the posterior distribution of the number of clusters
  tip_plots$histogram_posterior_number_of_clusters

  # View the trace plot with respect to the posterior number of clusters
  tip_plots$trace_plot_posterior_number_of_clusters

  # Extract posterior cluster assignments using the Posterior Expected Adjusted Rand (PEAR) index
  cluster_assignments &lt;- mcclust::maxpear(psm = tip1@posterior_similarity_matrix)$cl

  # If the true labels are available, then show the cluster result via a contigency table
  table(data.frame(true_label = true_labels,
                   cluster_assignment = cluster_assignments))

  # Create the one component graph with minimum entropy
  partition_list &lt;- partition_undirected_graph(.graph_matrix = tip1@posterior_similarity_matrix,
                                               .num_components = 1,
                                               .step_size = 0.001)

  # Associate class labels and colors for the plot
  class_palette_colors &lt;- c("Cluster 1" = "blue",
                            "Cluster 2" = 'green',
                            "Cluster 3" = "red")

  # Associate class labels and shapes for the plot
  class_palette_shapes &lt;- c("Cluster 1" = 19,
                            "Cluster 2" = 18,
                            "Cluster 3" = 17)

  # Visualize the posterior similarity matrix by constructing a graph plot of
  # the one-cluster graph. The true labels are used here (below they are not).
  ggnet2_network_plot(.matrix_graph = partition_list$partitioned_graph_matrix,
                      .subject_names = NA,
                      .subject_class_names = true_labels,
                      .class_colors = class_palette_colors,
                      .class_shapes = class_palette_shapes,
                      .node_size = 2,
                      .add_node_labels = FALSE)

  # If true labels are not available, then construct a network plot
  # of the one-cluster graph without any class labels.
  ggnet2_network_plot(.matrix_graph = partition_list$partitioned_graph_matrix,
                      .subject_names = names_subjects,
                      .node_size = 2,
                      .add_node_labels = TRUE)

  # If true labels are not available, then construct a network plot
  # of the one-cluster graph without any class labels. Also, suppress
  # the subject labels.
  ggnet2_network_plot(.matrix_graph = partition_list$partitioned_graph_matrix,
                      .subject_names = names_subjects,
                      .node_size = 2,
                      .add_node_labels = FALSE)

  ##### END EXAMPLE 5: Tensor Clustering #####

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
