<!DOCTYPE html><html><head><title>Help for package sarsop</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sarsop}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#alphas_from_log'><p>alphas_from_log</p></a></li>
<li><a href='#assert_has_appl'><p>test the APPL binaries</p></a></li>
<li><a href='#compute_policy'><p>compute_policy</p></a></li>
<li><a href='#f_from_log'><p>f from log</p></a></li>
<li><a href='#fisheries_matrices'><p>fisheries_matrices</p></a></li>
<li><a href='#hindcast_pomdp'><p>hindcast_pomdp</p></a></li>
<li><a href='#meta_from_log'><p>meta from log</p></a></li>
<li><a href='#models_from_log'><p>model from log</p></a></li>
<li><a href='#pomdpsol'><p>APPL wrappers</p></a></li>
<li><a href='#read_policyx'><p>read_policyx</p></a></li>
<li><a href='#sarsop'><p>sarsop</p></a></li>
<li><a href='#sim_pomdp'><p>simulate a POMDP</p></a></li>
<li><a href='#write_pomdpx'><p>write pomdpx files</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Approximate POMDP Planning Software</td>
</tr>
<tr>
<td>Version:</td>
<td>0.6.14</td>
</tr>
<tr>
<td>Description:</td>
<td>A toolkit for Partially Observed Markov Decision Processes (POMDP). Provides
    bindings to C++ libraries implementing the algorithm SARSOP (Successive Approximations
    of the Reachable Space under Optimal Policies) and described in Kurniawati et al (2008),
    &lt;<a href="https://doi.org/10.15607%2FRSS.2008.IV.009">doi:10.15607/RSS.2008.IV.009</a>&gt;.  This package also provides a high-level interface
    for generating, solving and simulating POMDP problems and their solutions.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/boettiger-lab/sarsop">https://github.com/boettiger-lab/sarsop</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/boettiger-lab/sarsop/issues">https://github.com/boettiger-lab/sarsop/issues</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>xml2, parallel, processx, digest, Matrix</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, roxygen2, knitr, covr, spelling</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>BH</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>mallinfo, hence Linux, MacOS or Windows</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-01-09 17:08:10 UTC; cboettig</td>
</tr>
<tr>
<td>Author:</td>
<td>Carl Boettiger <a href="https://orcid.org/0000-0002-1642-628X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [cre, aut, cph],
  Jeroen Ooms [aut],
  Milad Memarzadeh [aut],
  Hanna Kurniawati [ctb, cph],
  David Hsu [ctb, cph],
  Hanna Kurniawati [ctb, cph],
  Wee Sun Lee [ctb, cph],
  Yanzhu Du [ctb],
  Xan Huang [ctb],
  Trey Smith [ctb, cph],
  Tony Cassandra [ctb, cph],
  Lee Thomason [ctb, cph],
  Carl Kindman [ctb, cph],
  Le Trong Dao [ctb, cph],
  Amit Jain [ctb, cph],
  Rong Nan [ctb, cph],
  Ulrich Drepper [ctb],
  Free Software Foundation [cph],
  Tyge Lovset [ctb, cph],
  Yves Berquin [ctb, cph],
  Benjamin Gr√ºdelbach [ctb],
  RSA Data Security, Inc. [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Carl Boettiger &lt;cboettig@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-01-10 09:10:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='alphas_from_log'>alphas_from_log</h2><span id='topic+alphas_from_log'></span>

<h3>Description</h3>

<p>Read alpha vectors from a log file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>alphas_from_log(meta, log_dir = ".")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="alphas_from_log_+3A_meta">meta</code></td>
<td>
<p>a data frame containing the log metadata
for each set of alpha vectors desired, see
<code><a href="#topic+meta_from_log">meta_from_log</a></code></p>
</td></tr>
<tr><td><code id="alphas_from_log_+3A_log_dir">log_dir</code></td>
<td>
<p>path to log directory</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with a matrix of alpha vectors for each
entry in the provided metadata (as returned by <code><a href="#topic+sarsop">sarsop</a></code>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'> # takes &gt; 5s

source(system.file("examples/fisheries-ex.R", package = "sarsop"))
log = tempfile()
alpha &lt;- sarsop(transition, observation, reward, discount, precision = 10,
                log_dir = log)


</code></pre>

<hr>
<h2 id='assert_has_appl'>test the APPL binaries</h2><span id='topic+assert_has_appl'></span>

<h3>Description</h3>

<p>Asserts that the C++ binaries for appl have been compiled successfully
</p>


<h3>Usage</h3>

<pre><code class='language-R'>assert_has_appl()
</code></pre>


<h3>Value</h3>

<p>Will return TRUE if binaries are installed and can be located
and executed, and FALSE otherwise.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>assert_has_appl()

</code></pre>

<hr>
<h2 id='compute_policy'>compute_policy</h2><span id='topic+compute_policy'></span>

<h3>Description</h3>

<p>Derive the corresponding policy function from the alpha vectors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_policy(
  alpha,
  transition,
  observation,
  reward,
  state_prior = rep(1, dim(observation)[[1]])/dim(observation)[[1]],
  a_0 = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_policy_+3A_alpha">alpha</code></td>
<td>
<p>the matrix of alpha vectors returned by <code><a href="#topic+sarsop">sarsop</a></code></p>
</td></tr>
<tr><td><code id="compute_policy_+3A_transition">transition</code></td>
<td>
<p>Transition matrix, dimension n_s x n_s x n_a</p>
</td></tr>
<tr><td><code id="compute_policy_+3A_observation">observation</code></td>
<td>
<p>Observation matrix, dimension n_s x n_z x n_a</p>
</td></tr>
<tr><td><code id="compute_policy_+3A_reward">reward</code></td>
<td>
<p>reward matrix, dimension n_s x n_a</p>
</td></tr>
<tr><td><code id="compute_policy_+3A_state_prior">state_prior</code></td>
<td>
<p>initial belief state, optional, defaults to uniform
over states</p>
</td></tr>
<tr><td><code id="compute_policy_+3A_a_0">a_0</code></td>
<td>
<p>previous action. Belief in state depends not only on observation, but on prior belief of the state and subsequent action that had been taken.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data frame providing the optimal policy (choice of action) and corresponding value of the action for each possible belief state
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
m &lt;- fisheries_matrices()
 ## Takes &gt; 5s
if(assert_has_appl()){
alpha &lt;- sarsop(m$transition, m$observation, m$reward, 0.95, precision = 10)
compute_policy(alpha, m$transition, m$observation, m$reward)
}


</code></pre>

<hr>
<h2 id='f_from_log'>f from log</h2><span id='topic+f_from_log'></span>

<h3>Description</h3>

<p>Read transition function from log
</p>


<h3>Usage</h3>

<pre><code class='language-R'>f_from_log(meta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="f_from_log_+3A_meta">meta</code></td>
<td>
<p>a data frame containing the log metadata
for each set of alpha vectors desired, see
<code><a href="#topic+meta_from_log">meta_from_log</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>note this function is unique to the fisheries example problem and assumes that
sarsop call is run with logging specifying a column &quot;model&quot; that contains either the string
&quot;ricker&quot; (corresponding to a Ricker-type growth function) or &quot;allen&quot; (corresponding to an Allen-type.)
</p>


<h3>Value</h3>

<p>the growth function associated with the model indicated.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> # takes &gt; 5s

source(system.file("examples/fisheries-ex.R", package = "sarsop"))
log = tempfile()
alpha &lt;- sarsop(transition, observation, reward, discount, precision = 10,
                log_dir = log)


</code></pre>

<hr>
<h2 id='fisheries_matrices'>fisheries_matrices</h2><span id='topic+fisheries_matrices'></span>

<h3>Description</h3>

<p>Initialize the transition, observation, and reward matrices given
a transition function, reward function, and state space
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fisheries_matrices(
  states = 0:20,
  actions = states,
  observed_states = states,
  reward_fn = function(x, a) pmin(x, a),
  f = ricker(1, 15),
  sigma_g = 0.1,
  sigma_m = 0.1,
  noise = c("rescaled-lognormal", "lognormal", "uniform", "normal")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fisheries_matrices_+3A_states">states</code></td>
<td>
<p>sequence of possible states</p>
</td></tr>
<tr><td><code id="fisheries_matrices_+3A_actions">actions</code></td>
<td>
<p>sequence of possible actions</p>
</td></tr>
<tr><td><code id="fisheries_matrices_+3A_observed_states">observed_states</code></td>
<td>
<p>sequence of possible observations</p>
</td></tr>
<tr><td><code id="fisheries_matrices_+3A_reward_fn">reward_fn</code></td>
<td>
<p>function of x and a that gives reward for tacking action a when state is x</p>
</td></tr>
<tr><td><code id="fisheries_matrices_+3A_f">f</code></td>
<td>
<p>transition function of state x and action a.</p>
</td></tr>
<tr><td><code id="fisheries_matrices_+3A_sigma_g">sigma_g</code></td>
<td>
<p>half-width of uniform shock or equivalent variance for log-normal</p>
</td></tr>
<tr><td><code id="fisheries_matrices_+3A_sigma_m">sigma_m</code></td>
<td>
<p>half-width of uniform shock or equivalent variance for log-normal</p>
</td></tr>
<tr><td><code id="fisheries_matrices_+3A_noise">noise</code></td>
<td>
<p>distribution for noise, &quot;lognormal&quot; or &quot;uniform&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>assumes log-normally distributed observation errors and process errors
</p>


<h3>Value</h3>

<p>list of transition matrix, observation matrix, and reward matrix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>m &lt;- fisheries_matrices()
</code></pre>

<hr>
<h2 id='hindcast_pomdp'>hindcast_pomdp</h2><span id='topic+hindcast_pomdp'></span><span id='topic+compare_pomdp'></span>

<h3>Description</h3>

<p>Compare historical actions to what pomdp recommendation would have been.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hindcast_pomdp(
  transition,
  observation,
  reward,
  discount,
  obs,
  action,
  state_prior = rep(1, dim(observation)[[1]])/dim(observation)[[1]],
  alpha = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hindcast_pomdp_+3A_transition">transition</code></td>
<td>
<p>Transition matrix, dimension n_s x n_s x n_a</p>
</td></tr>
<tr><td><code id="hindcast_pomdp_+3A_observation">observation</code></td>
<td>
<p>Observation matrix, dimension n_s x n_z x n_a</p>
</td></tr>
<tr><td><code id="hindcast_pomdp_+3A_reward">reward</code></td>
<td>
<p>reward matrix, dimension n_s x n_a</p>
</td></tr>
<tr><td><code id="hindcast_pomdp_+3A_discount">discount</code></td>
<td>
<p>the discount factor</p>
</td></tr>
<tr><td><code id="hindcast_pomdp_+3A_obs">obs</code></td>
<td>
<p>a given sequence of observations</p>
</td></tr>
<tr><td><code id="hindcast_pomdp_+3A_action">action</code></td>
<td>
<p>the corresponding sequence of actions</p>
</td></tr>
<tr><td><code id="hindcast_pomdp_+3A_state_prior">state_prior</code></td>
<td>
<p>initial belief state, optional, defaults to uniform
over states</p>
</td></tr>
<tr><td><code id="hindcast_pomdp_+3A_alpha">alpha</code></td>
<td>
<p>the matrix of alpha vectors returned by <code><a href="#topic+sarsop">sarsop</a></code></p>
</td></tr>
<tr><td><code id="hindcast_pomdp_+3A_...">...</code></td>
<td>
<p>additional arguments to <code><a href="#topic+appl">appl</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list, containing: a data frame with columns for time, obs, action, and optimal action,
and an array containing the posterior belief distribution at each time t
</p>


<h3>Examples</h3>

<pre><code class='language-R'>m &lt;- fisheries_matrices()
 ## Takes &gt; 5s
if(assert_has_appl()){
alpha &lt;- sarsop(m$transition, m$observation, m$reward, 0.95, precision = 10)
sim &lt;- hindcast_pomdp(m$transition, m$observation, m$reward, 0.95,
                     obs = rnorm(21, 15, .1), action = rep(1, 21),
                     alpha = alpha)

}
</code></pre>

<hr>
<h2 id='meta_from_log'>meta from log</h2><span id='topic+meta_from_log'></span>

<h3>Description</h3>

<p>load metadata from a log file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>meta_from_log(
  parameters,
  log_dir = ".",
  metafile = paste0(log_dir, "/meta.csv")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="meta_from_log_+3A_parameters">parameters</code></td>
<td>
<p>a data.frame with the desired parameter values as given in metafile</p>
</td></tr>
<tr><td><code id="meta_from_log_+3A_log_dir">log_dir</code></td>
<td>
<p>path to log directory</p>
</td></tr>
<tr><td><code id="meta_from_log_+3A_metafile">metafile</code></td>
<td>
<p>path to metafile index, assumed to be meta.csv in log_dir</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame with the rows of the matching metadata.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> # takes &gt; 5s

source(system.file("examples/fisheries-ex.R", package = "sarsop"))
log = tempfile()
alpha &lt;- sarsop(transition, observation, reward, discount, precision = 10,
                log_dir = log)


</code></pre>

<hr>
<h2 id='models_from_log'>model from log</h2><span id='topic+models_from_log'></span>

<h3>Description</h3>

<p>Read model details from log file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>models_from_log(meta, reward_fn = function(x, h) pmin(x, h))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="models_from_log_+3A_meta">meta</code></td>
<td>
<p>a data frame containing the log metadata
for each set of alpha vectors desired, see
<code><a href="#topic+meta_from_log">meta_from_log</a></code></p>
</td></tr>
<tr><td><code id="models_from_log_+3A_reward_fn">reward_fn</code></td>
<td>
<p>a function f(x,a) giving the reward for taking action
a given a system in state x.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>assumes transition can be determined by the f_from_log function,
which is specific to the fisheries example
</p>


<h3>Value</h3>

<p>a list with an element for each row in the requested meta data frame,
which itself is a list of the three matrices: transition, observation, and
reward, defining the pomdp problem.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> # takes &gt; 5s

source(system.file("examples/fisheries-ex.R", package = "sarsop"))
log = tempfile()
alpha &lt;- sarsop(transition, observation, reward, discount, precision = 10,
                log_dir = log)


</code></pre>

<hr>
<h2 id='pomdpsol'>APPL wrappers</h2><span id='topic+pomdpsol'></span><span id='topic+appl'></span><span id='topic+SARSOP'></span><span id='topic+polgraph'></span><span id='topic+pomdpsim'></span><span id='topic+pomdpeval'></span><span id='topic+pomdpconvert'></span>

<h3>Description</h3>

<p>Wrappers for the APPL executables. The <code>pomdpsol</code> function solves a model
file and returns the path to the output policy file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pomdpsol(
  model,
  output = tempfile(),
  precision = 0.001,
  timeout = NULL,
  fast = FALSE,
  randomization = FALSE,
  memory = NULL,
  improvementConstant = NULL,
  timeInterval = NULL,
  stdout = tempfile(),
  stderr = tempfile(),
  spinner = TRUE
)

polgraph(
  model,
  policy,
  output = tempfile(),
  max_depth = 3,
  max_branches = 10,
  min_prob = 0.001,
  stdout = "",
  spinner = TRUE
)

pomdpsim(
  model,
  policy,
  output = tempfile(),
  steps = 100,
  simulations = 3,
  stdout = "",
  spinner = TRUE
)

pomdpeval(
  model,
  policy,
  output = tempfile(),
  steps = 100,
  simulations = 3,
  stdout = "",
  spinner = TRUE
)

pomdpconvert(model, stdout = "", spinner = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pomdpsol_+3A_model">model</code></td>
<td>
<p>file/path to the <code>pomdp</code> model file</p>
</td></tr>
<tr><td><code id="pomdpsol_+3A_output">output</code></td>
<td>
<p>file/path of the output policy file. This is also returned
by the function.</p>
</td></tr>
<tr><td><code id="pomdpsol_+3A_precision">precision</code></td>
<td>
<p>targetPrecision. Set targetPrecision as the target precision
in solution quality; run ends when target precision is reached. The target
precision is 1e-3 by default.</p>
</td></tr>
<tr><td><code id="pomdpsol_+3A_timeout">timeout</code></td>
<td>
<p>Use timeLimit as the timeout in seconds.  If running time
exceeds the specified value, pomdpsol writes out a policy and terminates.
There is no time limit by default.</p>
</td></tr>
<tr><td><code id="pomdpsol_+3A_fast">fast</code></td>
<td>
<p>logical, default FALSE. use fast (but very picky) alternate
parser for .pomdp files.</p>
</td></tr>
<tr><td><code id="pomdpsol_+3A_randomization">randomization</code></td>
<td>
<p>logical, default FALSE. Turn on randomization for the
sampling algorithm.</p>
</td></tr>
<tr><td><code id="pomdpsol_+3A_memory">memory</code></td>
<td>
<p>Use memoryLimit as the memory limit in MB. No memory limit
by default.  If memory usage exceeds the specified value, pomdpsol writes
out a policy and terminates. Set the value to be less than physical memory
to avoid swapping.</p>
</td></tr>
<tr><td><code id="pomdpsol_+3A_improvementconstant">improvementConstant</code></td>
<td>
<p>Use improvementConstant as the trial improvement
factor in the sampling algorithm. At the default of 0.5, a trial terminates
at a belief when the gap between its upper and lower bound is 0.5 of the
current precision at the initial belief.</p>
</td></tr>
<tr><td><code id="pomdpsol_+3A_timeinterval">timeInterval</code></td>
<td>
<p>Use timeInterval as the time interval between two
consecutive write-out of policy files. If this is not specified, pomdpsol
only writes out a policy file upon termination.</p>
</td></tr>
<tr><td><code id="pomdpsol_+3A_stdout">stdout</code></td>
<td>
<p>a filename where pomdp run statistics will be stored</p>
</td></tr>
<tr><td><code id="pomdpsol_+3A_stderr">stderr</code></td>
<td>
<p>currently ignored.</p>
</td></tr>
<tr><td><code id="pomdpsol_+3A_spinner">spinner</code></td>
<td>
<p>should we show a spinner while sarsop is running?</p>
</td></tr>
<tr><td><code id="pomdpsol_+3A_policy">policy</code></td>
<td>
<p>file/path to the policy file</p>
</td></tr>
<tr><td><code id="pomdpsol_+3A_max_depth">max_depth</code></td>
<td>
<p>the maximum horizon of the generated policy graph</p>
</td></tr>
<tr><td><code id="pomdpsol_+3A_max_branches">max_branches</code></td>
<td>
<p>maximum number of branches to show in the policy graph</p>
</td></tr>
<tr><td><code id="pomdpsol_+3A_min_prob">min_prob</code></td>
<td>
<p>the minimum probability threshold for a branch to be shown in the policy graph</p>
</td></tr>
<tr><td><code id="pomdpsol_+3A_steps">steps</code></td>
<td>
<p>number of steps for each simulation run</p>
</td></tr>
<tr><td><code id="pomdpsol_+3A_simulations">simulations</code></td>
<td>
<p>as the number of simulation runs</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>

if(assert_has_appl()){
  model &lt;- system.file("models", "example.pomdp", package = "sarsop")
  policy &lt;- tempfile(fileext = ".policyx")
  pomdpsol(model, output = policy, timeout = 1)

# Other tools
  evaluation &lt;- pomdpeval(model, policy, stdout = FALSE)
  graph &lt;- polgraph(model, policy, stdout = FALSE)
  simulations &lt;- pomdpsim(model, policy, stdout = FALSE)
}


</code></pre>

<hr>
<h2 id='read_policyx'>read_policyx</h2><span id='topic+read_policyx'></span>

<h3>Description</h3>

<p>read a .policyx file created by SARSOP and return alpha vectors and associated actions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_policyx(file = "output.policyx")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read_policyx_+3A_file">file</code></td>
<td>
<p>name of the policyx file to be read.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list, first element &quot;vectors&quot; is an n_states x n_vectors array of alpha vectors,
second element is a numeric vector &quot;action&quot; of length n_vectors whose i'th element indicates
the action corresponding to the i'th alpha vector (column) in the vectors array.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>f &lt;- system.file("extdata", "out.policy", package="sarsop", mustWork = TRUE)
policy &lt;- read_policyx(f)

</code></pre>

<hr>
<h2 id='sarsop'>sarsop</h2><span id='topic+sarsop'></span>

<h3>Description</h3>

<p>sarsop wraps the tasks of writing the pomdpx file defining the problem,
running the pomdsol (SARSOP) algorithm in C++, and then reading the
resulting policy file back into R.  The returned alpha vectors and
alpha_action information is then transformed into a more generic,
user-friendly representation as a matrix whose columns correspond
to actions and rows to states.  This function can thus be used at
the heart of most pomdp applications.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sarsop(
  transition,
  observation,
  reward,
  discount,
  state_prior = rep(1, dim(observation)[[1]])/dim(observation)[[1]],
  verbose = TRUE,
  log_dir = tempdir(),
  log_data = NULL,
  cache = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sarsop_+3A_transition">transition</code></td>
<td>
<p>Transition matrix, dimension n_s x n_s x n_a</p>
</td></tr>
<tr><td><code id="sarsop_+3A_observation">observation</code></td>
<td>
<p>Observation matrix, dimension n_s x n_z x n_a</p>
</td></tr>
<tr><td><code id="sarsop_+3A_reward">reward</code></td>
<td>
<p>reward matrix, dimension n_s x n_a</p>
</td></tr>
<tr><td><code id="sarsop_+3A_discount">discount</code></td>
<td>
<p>the discount factor</p>
</td></tr>
<tr><td><code id="sarsop_+3A_state_prior">state_prior</code></td>
<td>
<p>initial belief state, optional, defaults to uniform
over states</p>
</td></tr>
<tr><td><code id="sarsop_+3A_verbose">verbose</code></td>
<td>
<p>logical, should the function include a message with pomdp
diagnostics (timings, final precision, end condition)</p>
</td></tr>
<tr><td><code id="sarsop_+3A_log_dir">log_dir</code></td>
<td>
<p>pomdpx and policyx files will be saved here, along with
a metadata file</p>
</td></tr>
<tr><td><code id="sarsop_+3A_log_data">log_data</code></td>
<td>
<p>a data.frame of additional columns to include in the log,
such as model parameters. A unique id value for each run can be provided
as one of the columns, otherwise, a globally unique id will be generated.</p>
</td></tr>
<tr><td><code id="sarsop_+3A_cache">cache</code></td>
<td>
<p>should results from the log directory be cached? Default TRUE.
Identical functional calls will quickly return previously cached alpha
vectors from file rather than re-running.</p>
</td></tr>
<tr><td><code id="sarsop_+3A_...">...</code></td>
<td>
<p>additional arguments to <code><a href="#topic+appl">appl</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix of alpha vectors. Column index indicates action associated
with the alpha vector, (1:n_actions), rows indicate system state, x.
Actions for which no alpha vector was found are included as all -Inf,
since such actions are not optimal regardless of belief, and thus have no
corresponding alpha vectors in alpha_action list.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> ## Takes &gt; 5s
## Use example code to generate matrices for pomdp problem:
source(system.file("examples/fisheries-ex.R", package = "sarsop"))
alpha &lt;- sarsop(transition, observation, reward, discount, precision = 10)
compute_policy(alpha, transition, observation, reward)


</code></pre>

<hr>
<h2 id='sim_pomdp'>simulate a POMDP</h2><span id='topic+sim_pomdp'></span>

<h3>Description</h3>

<p>Simulate a POMDP given the appropriate matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sim_pomdp(
  transition,
  observation,
  reward,
  discount,
  state_prior = rep(1, dim(observation)[[1]])/dim(observation)[[1]],
  x0,
  a0 = 1,
  Tmax = 20,
  policy = NULL,
  alpha = NULL,
  reps = 1,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sim_pomdp_+3A_transition">transition</code></td>
<td>
<p>Transition matrix, dimension n_s x n_s x n_a</p>
</td></tr>
<tr><td><code id="sim_pomdp_+3A_observation">observation</code></td>
<td>
<p>Observation matrix, dimension n_s x n_z x n_a</p>
</td></tr>
<tr><td><code id="sim_pomdp_+3A_reward">reward</code></td>
<td>
<p>reward matrix, dimension n_s x n_a</p>
</td></tr>
<tr><td><code id="sim_pomdp_+3A_discount">discount</code></td>
<td>
<p>the discount factor</p>
</td></tr>
<tr><td><code id="sim_pomdp_+3A_state_prior">state_prior</code></td>
<td>
<p>initial belief state, optional, defaults to uniform
over states</p>
</td></tr>
<tr><td><code id="sim_pomdp_+3A_x0">x0</code></td>
<td>
<p>initial state</p>
</td></tr>
<tr><td><code id="sim_pomdp_+3A_a0">a0</code></td>
<td>
<p>initial action (default is action 1, e.g. can be arbitrary
if the observation process is independent of the action taken)</p>
</td></tr>
<tr><td><code id="sim_pomdp_+3A_tmax">Tmax</code></td>
<td>
<p>duration of simulation</p>
</td></tr>
<tr><td><code id="sim_pomdp_+3A_policy">policy</code></td>
<td>
<p>Simulate using a pre-computed policy (e.g. MDP policy) instead of POMDP</p>
</td></tr>
<tr><td><code id="sim_pomdp_+3A_alpha">alpha</code></td>
<td>
<p>the matrix of alpha vectors returned by <code><a href="#topic+sarsop">sarsop</a></code></p>
</td></tr>
<tr><td><code id="sim_pomdp_+3A_reps">reps</code></td>
<td>
<p>number of replicate simulations to compute</p>
</td></tr>
<tr><td><code id="sim_pomdp_+3A_...">...</code></td>
<td>
<p>additional arguments to mclapply</p>
</td></tr>
</table>


<h3>Details</h3>

<p>simulation assumes the following order of updating: For system in state[t] at
time t, an observation of the system obs[t] is made, and then action[t] is based on that
observation and the given policy, returning (discounted) reward[t].
</p>


<h3>Value</h3>

<p>a data frame with columns for time, state, obs, action, and (discounted) value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>m &lt;- fisheries_matrices()
discount &lt;- 0.95
 ## Takes &gt; 5s
if(assert_has_appl()){
alpha &lt;- sarsop(m$transition, m$observation, m$reward, discount, precision = 10)
sim &lt;- sim_pomdp(m$transition, m$observation, m$reward, discount,
                 x0 = 5, Tmax = 20, alpha = alpha)

}

</code></pre>

<hr>
<h2 id='write_pomdpx'>write pomdpx files</h2><span id='topic+write_pomdpx'></span>

<h3>Description</h3>

<p>A POMDPX file specifies a POMDP problem in terms of the transition,
observation, and reward matrices, the discount factor, and the initial
belief.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write_pomdpx(
  P,
  O,
  R,
  gamma,
  b = rep(1/dim(O)[1], dim(O)[1]),
  file = "input.pomdpx",
  digits = 12,
  digits2 = 12,
  format = "f"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="write_pomdpx_+3A_p">P</code></td>
<td>
<p>transition matrix</p>
</td></tr>
<tr><td><code id="write_pomdpx_+3A_o">O</code></td>
<td>
<p>observation matrix</p>
</td></tr>
<tr><td><code id="write_pomdpx_+3A_r">R</code></td>
<td>
<p>reward</p>
</td></tr>
<tr><td><code id="write_pomdpx_+3A_gamma">gamma</code></td>
<td>
<p>discount factor</p>
</td></tr>
<tr><td><code id="write_pomdpx_+3A_b">b</code></td>
<td>
<p>initial belief</p>
</td></tr>
<tr><td><code id="write_pomdpx_+3A_file">file</code></td>
<td>
<p>pomdpx file to create</p>
</td></tr>
<tr><td><code id="write_pomdpx_+3A_digits">digits</code></td>
<td>
<p>precision to round to before normalizing.
Leave at 4 since sarsop seems unable to do more?</p>
</td></tr>
<tr><td><code id="write_pomdpx_+3A_digits2">digits2</code></td>
<td>
<p>precision to write solution to. Leave at 10,
since normalizing requires additional precision</p>
</td></tr>
<tr><td><code id="write_pomdpx_+3A_format">format</code></td>
<td>
<p>floating point format, because sarsop parser
doesn't seem to know scientific notation</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>m &lt;- fisheries_matrices()
f &lt;- tempfile()
write_pomdpx(m$transition, m$observation, m$reward, 0.95,
             file = f)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
