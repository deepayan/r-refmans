<!DOCTYPE html><html><head><title>Help for package PGRdup</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {PGRdup}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#AddProbDup'><p>Add probable duplicate sets fields to the PGR passport database</p></a></li>
<li><a href='#DataClean'><p>Clean PGR passport data</p></a></li>
<li><a href='#DisProbDup'><p>Get disjoint probable duplicate sets</p></a></li>
<li><a href='#DoubleMetaphone'><p>'Double Metaphone' phonetic algorithm</p></a></li>
<li><a href='#GN1000'><p>Sample groundnut PGR passport data</p></a></li>
<li><a href='#KWCounts'><p>Generate keyword counts</p></a></li>
<li><a href='#KWIC'><p>Create a KWIC index</p></a></li>
<li><a href='#MergeKW'><p>Merge keyword strings</p></a></li>
<li><a href='#MergeProbDup'><p>Merge two objects of class <code>ProbDup</code></p></a></li>
<li><a href='#ParseProbDup'><p>Parse an object of class <code>ProbDup</code> to a data frame.</p></a></li>
<li><a href='#PGRdup-package'><p>The PGRdup Package</p></a></li>
<li><a href='#print.KWIC'><p>Prints summary of <code>KWIC</code> object.</p></a></li>
<li><a href='#print.ProbDup'><p>Prints summary of <code>ProbDup</code> object.</p></a></li>
<li><a href='#ProbDup'><p>Identify probable duplicates of accessions</p></a></li>
<li><a href='#read.genesys'><p>Convert 'Darwin Core - Germplasm' zip archive to a flat file</p></a></li>
<li><a href='#ReconstructProbDup'><p>Reconstruct an object of class ProbDup</p></a></li>
<li><a href='#ReviewProbDup'><p>Retrieve probable duplicate set information from PGR passport database for</p>
review</a></li>
<li><a href='#SplitProbDup'><p>Split an object of class <code>ProbDup</code></p></a></li>
<li><a href='#ValidatePrimKey'><p>Validate if a data frame column confirms to primary key/ID constraints</p></a></li>
<li><a href='#ViewProbDup'><p>Visualize the probable duplicate sets retrieved in a <code>ProbDup</code> object</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Discover Probable Duplicates in Plant Genetic Resources
Collections</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.3.9</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides functions to aid the identification of probable/possible
    duplicates in Plant Genetic Resources (PGR) collections using
    'passport databases' comprising of information records of each constituent
    sample. These include methods for cleaning the data, creation of a
    searchable Key Word in Context (KWIC) index of keywords associated with
    sample records and the identification of nearly identical records with
    similar information by fuzzy, phonetic and semantic matching of keywords.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.2)</td>
</tr>
<tr>
<td>Imports:</td>
<td>data.table (&ge; 1.9.3), igraph, stringdist (&ge; 0.9.4), stringi,
ggplot2, grid, gridExtra, methods, utils, stats</td>
</tr>
<tr>
<td>Suggests:</td>
<td>diagram, wordcloud, microbenchmark, XML, httr, RCurl, knitr,
rmarkdown, pander</td>
</tr>
<tr>
<td>Copyright:</td>
<td>2014-2023, ICAR-NBPGR</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://cran.r-project.org/package=PGRdup">https://cran.r-project.org/package=PGRdup</a>,
<a href="https://github.com/aravind-j/PGRdup">https://github.com/aravind-j/PGRdup</a>,
<a href="https://doi.org/10.5281/zenodo.841963">https://doi.org/10.5281/zenodo.841963</a>,
<a href="https://aravind-j.github.io/PGRdup/">https://aravind-j.github.io/PGRdup/</a>,
<a href="https://www.rdocumentation.org/packages/PGRdup">https://www.rdocumentation.org/packages/PGRdup</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/aravind-j/PGRdup/issues">https://github.com/aravind-j/PGRdup/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-08-31 17:00:05 UTC; J. Aravind</td>
</tr>
<tr>
<td>Author:</td>
<td>J. Aravind <a href="https://orcid.org/0000-0002-4791-442X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  J. Radhamani [aut],
  Kalyani Srinivasan [aut],
  B. Ananda Subhash [aut],
  Rishi Kumar Tyagi [aut],
  ICAR-NBGPR [cph] (www.nbpgr.ernet.in),
  Maurice Aubrey [ctb] (Double Metaphone),
  Kevin Atkinson [ctb] (Double Metaphone),
  Lawrence Philips [ctb] (Double Metaphone)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>J. Aravind &lt;j.aravind@icar.gov.in&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-08-31 22:10:16 UTC</td>
</tr>
</table>
<hr>
<h2 id='AddProbDup'>Add probable duplicate sets fields to the PGR passport database</h2><span id='topic+AddProbDup'></span>

<h3>Description</h3>

<p><code>AddProbDup</code> adds the fuzzy, phonetic and semantic probable duplicates 
sets data fields from an object of class <code>ProbDup</code> to the original PGR 
passport database.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AddProbDup(pdup, db, addto = c("I", "II"), max.count = 30)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="AddProbDup_+3A_pdup">pdup</code></td>
<td>
<p>An object of class <code>ProbDup</code>.</p>
</td></tr>
<tr><td><code id="AddProbDup_+3A_db">db</code></td>
<td>
<p>A data frame of the PGR passport database.</p>
</td></tr>
<tr><td><code id="AddProbDup_+3A_addto">addto</code></td>
<td>
<p>Either <code>"I"</code> or <code>"II"</code> indicating the database to 
which the data.fields are to be added (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="AddProbDup_+3A_max.count">max.count</code></td>
<td>
<p>The maximum count of probable duplicate sets whose 
information is to be retrieved.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function helps to add information associated with identified fuzzy, 
phonetic and semantic probable duplicate sets using the 
<code><a href="#topic+ProbDup">ProbDup</a></code> function to the original PGR passport database.
Associated data fields such as <code>SET_NO</code>, <code>ID</code> and <code>IDKW</code> are 
added based on the <code>PRIM_ID</code> field(column).
</p>
<p>In case more than one KWIC index was used to generate the object of class 
<code>ProbDup</code>, the argument <code>addto</code> can be used to specify to which 
database the data fields are to be added. The default <code>"I"</code> indicates 
the database from which the first KWIC index was created and <code>"II"</code> 
indicates the database from which the second index was created.
</p>


<h3>Value</h3>

<p>A data frame of the PGR passport database with the probable duplicate
sets fields added.
</p>


<h3>Note</h3>

<p>When any primary ID/key records in the fuzzy, phonetic or semantic 
duplicate sets are found to be missing from the original database 
<code>db</code>, then they are ignored and only the matching records are 
considered for adding the information with a warning.
</p>
<p>This may be due to data standardization of the primary ID/key field using 
the function <code><a href="#topic+DataClean">DataClean</a></code> before creation of the KWIC 
index and subsequent identification of probable duplicate sets. In such a 
case, it is recommended to use an identical data standardization operation 
on the database <code>db</code> before running this function.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+DataClean">DataClean</a></code>, <code><a href="#topic+KWIC">KWIC</a></code>,
<code><a href="#topic+ProbDup">ProbDup</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


## Not run: 

#' # Load PGR passport database
GN &lt;- GN1000

# Specify as a vector the database fields to be used
GNfields &lt;- c("NationalID", "CollNo", "DonorID", "OtherID1", "OtherID2")

# Clean the data
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) DataClean(x))
y1 &lt;- list(c("Gujarat", "Dwarf"), c("Castle", "Cary"), c("Small", "Japan"),
c("Big", "Japan"), c("Mani", "Blanco"), c("Uganda", "Erect"),
c("Mota", "Company"))
y2 &lt;- c("Dark", "Light", "Small", "Improved", "Punjab", "SAM")
y3 &lt;- c("Local", "Bold", "Cary", "Mutant", "Runner", "Giant", "No.",
        "Bunch", "Peanut")
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergeKW(x, y1, delim = c("space", "dash")))
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergePrefix(x, y2, delim = c("space", "dash")))
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergeSuffix(x, y3, delim = c("space", "dash")))

# Generate KWIC index
GNKWIC &lt;- KWIC(GN, GNfields)

# Specify the exceptions as a vector
exep &lt;- c("A", "B", "BIG", "BOLD", "BUNCH", "C", "COMPANY", "CULTURE", 
         "DARK", "E", "EARLY", "EC", "ERECT", "EXOTIC", "FLESH", "GROUNDNUT", 
         "GUTHUKAI", "IMPROVED", "K", "KUTHUKADAL", "KUTHUKAI", "LARGE", 
         "LIGHT", "LOCAL", "OF", "OVERO", "P", "PEANUT", "PURPLE", "R", 
         "RED", "RUNNER", "S1", "SAM", "SMALL", "SPANISH", "TAN", "TYPE", 
         "U", "VALENCIA", "VIRGINIA", "WHITE")
          
# Specify the synsets as a list
syn &lt;- list(c("CHANDRA", "AH114"), c("TG1", "VIKRAM"))

# Fetch probable duplicate sets
GNdup &lt;- ProbDup(kwic1 = GNKWIC, method = "a", excep = exep, fuzzy = TRUE,
                 phonetic = TRUE, encoding = "primary", 
                 semantic = TRUE, syn = syn)

# Add the duplicates sets to the original database                 
GNwithdup &lt;-  AddProbDup(pdup = GNdup, db = GN1000, addto = "I")                  


## End(Not run)



</code></pre>

<hr>
<h2 id='DataClean'>Clean PGR passport data</h2><span id='topic+DataClean'></span>

<h3>Description</h3>

<p><code>DataClean</code> cleans the data in a character vector according to the 
conditions in the arguments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DataClean(
  x,
  fix.comma = TRUE,
  fix.semcol = TRUE,
  fix.col = TRUE,
  fix.bracket = TRUE,
  fix.punct = TRUE,
  fix.space = TRUE,
  fix.sep = TRUE,
  fix.leadzero = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DataClean_+3A_x">x</code></td>
<td>
<p>A character vector. If not, coerced to character by 
<code>as.character</code>.</p>
</td></tr>
<tr><td><code id="DataClean_+3A_fix.comma">fix.comma</code></td>
<td>
<p>logical. If <code>TRUE</code>, all the commas are replaced by 
space (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="DataClean_+3A_fix.semcol">fix.semcol</code></td>
<td>
<p>logical. If <code>TRUE</code>, all the semicolons are replaced by
space (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="DataClean_+3A_fix.col">fix.col</code></td>
<td>
<p>logical. If <code>TRUE</code>, all the colons are replaced by space 
(see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="DataClean_+3A_fix.bracket">fix.bracket</code></td>
<td>
<p>logical. If <code>TRUE</code>, all the brackets are replaced by 
space (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="DataClean_+3A_fix.punct">fix.punct</code></td>
<td>
<p>logical. If <code>TRUE</code>, all punctuation characters are 
removed (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="DataClean_+3A_fix.space">fix.space</code></td>
<td>
<p>logical. If <code>TRUE</code>, all space characters are replaced 
by space and multiple spaces are converted to single space (see 
<strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="DataClean_+3A_fix.sep">fix.sep</code></td>
<td>
<p>logical. If <code>TRUE</code>, space between alphabetic characters 
followed by digits is removed (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="DataClean_+3A_fix.leadzero">fix.leadzero</code></td>
<td>
<p>logical. If <code>TRUE</code>, leading zeros are removed (see 
<strong>Details</strong>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function aids in standardization and preparation of the PGR passport 
data for creation of a KWIC index with <code><a href="#topic+KWIC">KWIC</a></code> function 
and the identification of probable duplicate accessions by the 
<code><a href="#topic+ProbDup">ProbDup</a></code> function. It cleans the character strings in 
passport data fields(columns) specified as the input character vector 
<code>x</code> according to the conditions in the arguments in the same order. If 
the input vector <code>x</code> is not of type character, it is coerced to a 
character vector.
</p>
<p>This function is designed particularly for use with fields corresponding to 
accession names such as accession ids, collection numbers, accession names 
etc. It is essentially a wrapper around the <code><a href="base.html#topic+grep">gsub</a></code> base 
function with <code><a href="base.html#topic+regex">regex</a></code> arguments. It also converts all 
strings to upper case and removes leading and trailing spaces.
</p>
<p>Commas, semicolons and colons which are sometimes used to separate multiple 
strings or names within the same field can be replaced with a single space 
using the logical arguments <code>fix.comma</code>, <code>fix.semcol</code> and 
<code>fix.col</code> respectively.
</p>
<p>Similarly the logical argument <code>fix.bracket</code> can be used to replace all 
brackets including parenthesis, square brackets and curly brackets with
space.
</p>
<p>The logical argument <code>fix.punct</code> can be used to remove all punctuation 
from the data.
</p>
<p><code>fix.space</code> can be used to convert all space characters such as tab, 
newline, vertical tab, form feed and carriage return to spaces and finally 
convert multiple spaces to single space.
</p>
<p><code>fix.sep</code> can be used to merge together accession identifiers 
composed of alphabetic characters separated from as series of digits by a 
space character. For example IR 64, PUSA 256 etc.
</p>
<p><code>fix.leadzero</code> can be used to remove leading zeros from accession name 
fields to facilitate matching to identify probable duplicates. e.g. IR0064 -&gt; 
IR64
</p>


<h3>Value</h3>

<p>A character vector with the cleaned data converted to upper case. 
<code>NAs</code> if any are converted to blank strings.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+grep">gsub</a></code>, <code><a href="base.html#topic+regex">regex</a></code>, 
<code><a href="#topic+MergeKW">MergeKW</a></code>, <code><a href="#topic+KWIC">KWIC</a></code>, 
<code><a href="#topic+ProbDup">ProbDup</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>names &lt;- c("S7-12-6", "ICG-3505", "U 4-47-18;EC 21127", "AH 6481", "RS   1",
           "AK 12-24", "2-5 (NRCG-4053)", "T78, Mwitunde", "ICG 3410",
           "#648-4 (Gwalior)", "TG4;U/4/47/13", "EC0021003")
DataClean(names)
</code></pre>

<hr>
<h2 id='DisProbDup'>Get disjoint probable duplicate sets</h2><span id='topic+DisProbDup'></span>

<h3>Description</h3>

<p><code>DisProbDup</code> finds and joins intersecting sets in an object of class
<code>ProbDup</code> to get disjoint probable duplicate sets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DisProbDup(pdup, combine = c("F", "P", "S"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DisProbDup_+3A_pdup">pdup</code></td>
<td>
<p>An object of class <code>ProbDup</code>.</p>
</td></tr>
<tr><td><code id="DisProbDup_+3A_combine">combine</code></td>
<td>
<p>A character vector indicating the type of sets to be
considered together for retrieving disjoint sets. If <code>NULL</code>, then
disjoint sets within each type are retrieved (see <strong>Details</strong>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function considers the accession primary keys/IDs for finding
intersecting sets and subsequently joins them to retrieve disjoint sets.
These operations are implemented utilizing the
<code><a href="igraph.html#topic+aaa-igraph-package">igraph</a></code> package functions.
</p>
<p>Disjoint sets are retrieved either individually for each type of probable
duplicate sets or considering all type of sets simultaneously. In case of the
latter, the disjoint of all the type of sets alone are returned in the output
as an additional data frame <code>DisjointDuplicates</code> in an object of class
<code>ProbDup</code>
</p>


<h3>Value</h3>

<p>Returns an object of class <code>ProbDup</code> with either the disjoint
sets within each type - <code>FuzzyDuplicates</code>, <code>PhoneticDuplicates</code>
and <code>SemanticDuplicates</code> when <code>combine = NULL</code> or the combined
disjoint duplicate sets as an additional element <code>DisjointDupicates</code>
according to the choice specified in the argument <code>combine</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ProbDup">ProbDup</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


## Not run: 

# Load PGR passport database
GN &lt;- GN1000

# Specify as a vector the database fields to be used
GNfields &lt;- c("NationalID", "CollNo", "DonorID", "OtherID1", "OtherID2")

# Clean the data
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) DataClean(x))
y1 &lt;- list(c("Gujarat", "Dwarf"), c("Castle", "Cary"), c("Small", "Japan"),
c("Big", "Japan"), c("Mani", "Blanco"), c("Uganda", "Erect"),
c("Mota", "Company"))
y2 &lt;- c("Dark", "Light", "Small", "Improved", "Punjab", "SAM")
y3 &lt;- c("Local", "Bold", "Cary", "Mutant", "Runner", "Giant", "No.",
        "Bunch", "Peanut")
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergeKW(x, y1, delim = c("space", "dash")))
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergePrefix(x, y2, delim = c("space", "dash")))
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergeSuffix(x, y3, delim = c("space", "dash")))

# Generate KWIC index
GNKWIC &lt;- KWIC(GN, GNfields)

# Specify the exceptions as a vector
exep &lt;- c("A", "B", "BIG", "BOLD", "BUNCH", "C", "COMPANY", "CULTURE",
         "DARK", "E", "EARLY", "EC", "ERECT", "EXOTIC", "FLESH", "GROUNDNUT",
         "GUTHUKAI", "IMPROVED", "K", "KUTHUKADAL", "KUTHUKAI", "LARGE",
         "LIGHT", "LOCAL", "OF", "OVERO", "P", "PEANUT", "PURPLE", "R",
         "RED", "RUNNER", "S1", "SAM", "SMALL", "SPANISH", "TAN", "TYPE",
         "U", "VALENCIA", "VIRGINIA", "WHITE")

# Specify the synsets as a list
syn &lt;- list(c("CHANDRA", "AH114"), c("TG1", "VIKRAM"))

# Fetch probable duplicate sets
GNdup &lt;- ProbDup(kwic1 = GNKWIC, method = "a", excep = exep, fuzzy = TRUE,
                 phonetic = TRUE, encoding = "primary",
                 semantic = TRUE, syn = syn)
lapply(GNdup, dim)

# Get disjoint probable duplicate sets of each kind
disGNdup1 &lt;- DisProbDup(GNdup, combine = NULL)
lapply(disGNdup1, nrow)

# Get disjoint probable duplicate sets combining all the kinds of sets
disGNdup2 &lt;- DisProbDup(GNdup, combine = c("F", "P", "S"))
lapply(disGNdup2, nrow)


## End(Not run)



</code></pre>

<hr>
<h2 id='DoubleMetaphone'>'Double Metaphone' phonetic algorithm</h2><span id='topic+DoubleMetaphone'></span>

<h3>Description</h3>

<p><code>DoubleMetaphone</code> converts strings to double metaphone phonetic codes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DoubleMetaphone(str)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DoubleMetaphone_+3A_str">str</code></td>
<td>
<p>A character vector whose strings are to be encoded by double
metaphone algorithm.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An implementation of the Double Metaphone phonetic algorithm in <code>R</code>. If
non-ASCII characters encountered in the input character vector <code>str</code>, a
warning is issued and they are transliterated so that the accented characters
are converted to their ASCII unaccented versions.
</p>


<h3>Value</h3>

<p>Returns a list with two character vectors of the same length as the
input vector. The first character vector contains the primary double
metaphone encodings, while the second character vector contains the
alternate encodings.
</p>


<h3>Acknowledgement</h3>

<p>The <code>C</code> code for the double metaphone
algorithm was adapted from Maurice Aubrey's perl module hosted at the
<strong>gitpan/Text-DoubleMetaphone</strong>
<a href="https://github.com/gitpan/Text-DoubleMetaphone/blob/master/double_metaphone.c">public
github library</a> along with the corresponding
<a href="https://github.com/gitpan/Text-DoubleMetaphone/blob/master/README">license
information</a>.
</p>


<h3>Note</h3>

<p>In case of non-ASCII characters in strings, a warning is issued and
accented characters are converted to their ASCII unaccented versions.
</p>


<h3>References</h3>

<p>Philips, Lawrence. 2000. &quot;The Double Metaphone Search Algorithm.&quot;
<em>C/C++ Users Journal</em> 18 (6): 38-43.
<a href="https://dl.acm.org/doi/10.5555/349124.349132">https://dl.acm.org/doi/10.5555/349124.349132</a>.
</p>


<h3>See Also</h3>

<p><code><a href="stringdist.html#topic+phonetic">phonetic</a></code>,
<a href="https://cran.r-project.org/package=RecordLinkage"><code>phonetics</code></a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Return the primary and secondary Double Metaphone encodings for a character vector.
str1 &lt;- c("Jyothi", "Jyoti")
str2 &lt;- c("POLLACHI", "BOLLACHI")
DoubleMetaphone(str1)
DoubleMetaphone(str2)
## Not run: 
# Issue a warning in case of non-ASCII characters.
str3 &lt;- c("J\xf5geva", "Jogeva")
DoubleMetaphone(str3) 
## End(Not run)
</code></pre>

<hr>
<h2 id='GN1000'>Sample groundnut PGR passport data</h2><span id='topic+GN1000'></span>

<h3>Description</h3>

<p>Sample PGR passport data of 1000 groundnut accessions held in the Indian 
National Genebank at National Bureau of Plant Genetic Resources (NBPGR), New 
Delhi.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(GN1000)
</code></pre>


<h3>Format</h3>

<p>A data frame having 1000 records with the following 10 
columns(fields): </p>
 <ul>
<li><p>CommonName : Common name 
</p>
</li>
<li><p>BotanicalName : Botanical name </p>
</li>
<li><p>NationalID : NBPGR 
National identifier </p>
</li>
<li><p>CollNo : Collector number </p>
</li>
<li><p>DonorID :
Donor ID </p>
</li>
<li><p>OtherID1 : Other ID field 1 </p>
</li>
<li><p>OtherID2 : 
Other ID field 2 </p>
</li>
<li><p>BioStatus : Biological status
</p>
</li>
<li><p>SourceCountry : Country of origin </p>
</li>
<li><p>TransferYear : Year
of transfer </p>
</li></ul>



<h3>See Also</h3>

<p><a href="http://www.nbpgr.ernet.in:8080/PGRPortal/">http://www.nbpgr.ernet.in:8080/PGRPortal/</a>
</p>

<hr>
<h2 id='KWCounts'>Generate keyword counts</h2><span id='topic+KWCounts'></span>

<h3>Description</h3>

<p><code>KWCounts</code> generates keyword counts from PGR passport database 
fields(columns).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KWCounts(x, fields, excep)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KWCounts_+3A_x">x</code></td>
<td>
<p>A data frame.</p>
</td></tr>
<tr><td><code id="KWCounts_+3A_fields">fields</code></td>
<td>
<p>A character vector with the names of fields(columns) of the 
data frame from which KWIC index is to be generated. The first field is 
considered as the primary key or identifier (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="KWCounts_+3A_excep">excep</code></td>
<td>
<p>A vector of the keywords not to be considered for the counts
(see <strong>Details</strong>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes the keyword counts from PGR passport database 
fields(columns) specified in the <code>fields</code> argument. The first field is 
considered as the primary key or identifier and is not used for counting the 
keywords. Any strings given in the <code>excep</code> argument are ignored for 
generating the counts.
</p>
<p>The keyword counts can give a rough indication of the completeness of the 
data in the database fields being used for identification of probable 
duplicates.
</p>


<h3>Value</h3>

<p>A data frame with the keyword counts for each record.
</p>


<h3>Note</h3>

<p>For large number of exceptions and/or large data.frame computation of
keyword counts may take some time.
</p>


<h3>See Also</h3>

<p><code><a href="stringi.html#topic+stri_count">stri_count</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


# Load PGR passport database
GN &lt;- GN1000

# Specify database fields to use as a vector
GNfields &lt;- c("NationalID", "CollNo", "DonorID", "OtherID1", "OtherID2")

# Specify the exceptions as a vector
exep &lt;- c("A", "B", "BIG", "BOLD", "BUNCH", "C", "COMPANY", "CULTURE", 
         "DARK", "E", "EARLY", "EC", "ERECT", "EXOTIC", "FLESH", "GROUNDNUT", 
         "GUTHUKAI", "IMPROVED", "K", "KUTHUKADAL", "KUTHUKAI", "LARGE", 
         "LIGHT", "LOCAL", "OF", "OVERO", "P", "PEANUT", "PURPLE", "R", 
         "RED", "RUNNER", "S1", "SAM", "SMALL", "SPANISH", "TAN", "TYPE", 
         "U", "VALENCIA", "VIRGINIA", "WHITE")
         
# Compute the keyword counts
GNKWCouts &lt;- KWCounts(GN, GNfields, exep)

# Plot the keyword counts
bp &lt;- barplot(table(GNKWCouts$COUNT),
              xlab = "Word count", ylab = "Frequency", col = "#CD5555")
text(bp, 0, table(GNKWCouts$COUNT),cex=1,pos=3)



</code></pre>

<hr>
<h2 id='KWIC'>Create a KWIC index</h2><span id='topic+KWIC'></span>

<h3>Description</h3>

<p><code>KWIC</code> creates a Keyword in Context index from PGR passport database
fields.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KWIC(x, fields, min.freq = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KWIC_+3A_x">x</code></td>
<td>
<p>A data frame from which KWIC index is to be generated.</p>
</td></tr>
<tr><td><code id="KWIC_+3A_fields">fields</code></td>
<td>
<p>A character vector with the names of fields(columns) of the
data frame from which KWIC index is to be generated. The first field is
considered as the primary key or identifier (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="KWIC_+3A_min.freq">min.freq</code></td>
<td>
<p>Frequency of keywords are not computed if below
<code>min.freq</code>. Default is 10.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function generates a Keyword in Context index from a data frame of a PGR
passport database based on the fields(columns) stated in the arguments, using
<code><a href="data.table.html#topic+data.table">data.table</a></code> package.
</p>
<p>The first element of vector <code>fields</code> is considered as the primary key or
identifier which uniquely identifies all rows in the data frame.
</p>
<p>Cleaning of the data the input fields(columns) using the
<code><a href="#topic+DataClean">DataClean</a></code> function with appropriate arguments is
suggested before running this function.
</p>


<h3>Value</h3>

<p>A list of class <code>KWIC</code> containing the following components:
</p>

<table>
<tr>
 <td style="text-align: left;"> <code>KWIC</code> </td><td style="text-align: left;"> The KWIC index in the form of a data frame.
  </td>
</tr>
<tr>
 <td style="text-align: left;"> <code>KeywordFreq</code> </td><td style="text-align: left;"> A data frame of the keywords detected with
  frequency greater than <code>min.freq</code>. </td>
</tr>
<tr>
 <td style="text-align: left;"> <code>Fields</code> </td><td style="text-align: left;"> A character
  vector with the names of the PGR database fields from which the keywords
  were extracted. </td>
</tr>
<tr>
 <td style="text-align: left;"> </td>
</tr>

</table>



<h3>References</h3>

<p>Knüpffer, H. 1988. &quot;The European Barley Database of the ECP/GR:
An Introduction.&quot; <em>Die Kulturpflanze</em> 36 (1): 135-62.
<a href="https://doi.org/10.1007/BF01986957">doi:10.1007/BF01986957</a>.
</p>
<p>Knüpffer, H., L. Frese, and M. W. M. Jongen. 1997. &quot;Using Central Crop
Databases: Searching for Duplicates and Gaps.&quot; In <em>Central Crop
Databases: Tools for Plant Genetic Resources Management. Report of a
Workshop, Budapest, Hungary, 13-16 October 1996</em>, edited by E. Lipman, M.
W. M. Jongen, T. J. L. van Hintum, T. Gass, and L. Maggioni, 67-77. Rome,
Italy and Wageningen, The Netherlands: International Plant Genetic
Resources Institute and Centre for Genetic Resources.
</p>


<h3>See Also</h3>

<p><code><a href="data.table.html#topic+data.table">data.table</a></code>,
<code><a href="#topic+print.KWIC">print.KWIC</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


# Load PGR passport database
GN &lt;- GN1000

# Specify as a vector the database fields to be used
GNfields &lt;- c("NationalID", "CollNo", "DonorID", "OtherID1", "OtherID2")

# Clean the data
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) DataClean(x))

## Not run: 

# Generate KWIC index
GNKWIC &lt;- KWIC(GN, GNfields)
GNKWIC

# Retrieve the KWIC index from the KWIC object
KWIC &lt;- GNKWIC[[1]]

# Retrieve the keyword frequencies from the KWIC object
KeywordFreq &lt;- GNKWIC[[2]]

# Show error in case of duplicates and NULL values
# in the primary key/ID field "NationalID"
GN[1001:1005,] &lt;- GN[1:5,]
GN[1001,3] &lt;- ""
GNKWIC &lt;- KWIC(GN, GNfields)

## End(Not run)



</code></pre>

<hr>
<h2 id='MergeKW'>Merge keyword strings</h2><span id='topic+MergeKW'></span><span id='topic+MergePrefix'></span><span id='topic+MergeSuffix'></span>

<h3>Description</h3>

<p>These functions merge keyword strings separated by delimiters such as space, 
period or dash in a character vector into single keyword strings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MergeKW(x, y, delim = c("space", "dash", "period"))

MergePrefix(x, y, delim = c("space", "dash", "period"))

MergeSuffix(x, y, delim = c("space", "dash", "period"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MergeKW_+3A_x">x</code></td>
<td>
<p>A character vector. If not, coerced to character by 
<code>as.character</code>.</p>
</td></tr>
<tr><td><code id="MergeKW_+3A_y">y</code></td>
<td>
<p>A list of character vectors with pairs of strings that are to be 
merged (for <code>MergeKW</code>) or a character vector of strings which are to 
be merged to succeeding string (for <code>MergePrefix</code>) or the preceding 
string (for <code>MergeSuffix</code>). If not of type character, coerced by 
<code>as.character</code>.</p>
</td></tr>
<tr><td><code id="MergeKW_+3A_delim">delim</code></td>
<td>
<p>Delimiting characters to be removed between keywords.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions aid in standardization of relevant data fields(columns) in 
PGR passport data for creation of a KWIC index with 
<code><a href="#topic+KWIC">KWIC</a></code> function and subsequent identification of probable
duplicate accessions by the <code><a href="#topic+ProbDup">ProbDup</a></code> function.
</p>
<p>It is recommended to run this function before using the 
<code><a href="#topic+DataClean">DataClean</a></code> function on the relevant data fields(columns)
of PGR passport databases.
</p>
<p><code>MergeKW</code> merges together pairs of strings specified as a list in 
argument <code>y</code> wherever they exist in a character vector. The second 
string in the pair is merged even when it is followed by a number.
</p>
<p><code>MergePrefix</code> merges prefix strings specified as a character vector in 
argument <code>y</code> to the succeeding root word, wherever they exist in a 
character vector.
</p>
<p><code>MergeSuffix</code> merges suffix strings specified as a character vector in 
argument <code>y</code> to the preceding root word, wherever they exist in a 
character vector. The suffix strings which are followed by numbers are also 
merged.
</p>


<h3>Value</h3>

<p>A character vector of the same length as <code>x</code> with the required 
keyword strings merged.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+DataClean">DataClean</a></code>, <code><a href="#topic+KWIC">KWIC</a></code>, 
<code><a href="#topic+ProbDup">ProbDup</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>names &lt;- c("Punjab Bold", "Gujarat- Dwarf", "Nagpur.local", "SAM COL 144",
           "SAM COL--280", "NIZAMABAD-LOCAL", "Dark Green Mutant",
           "Dixie-Giant", "Georgia- Bunch", "Uganda-erect", "Small Japan",
           "Castle  Cary", "Punjab erect", "Improved small japan",
           "Dark Purple")

# Merge pairs of strings
y1 &lt;- list(c("Gujarat", "Dwarf"), c("Castle", "Cary"), c("Small", "Japan"),
           c("Big", "Japan"), c("Mani", "Blanco"), c("Uganda", "Erect"),
           c("Mota", "Company"))
names &lt;- MergeKW(names, y1, delim = c("space", "dash", "period"))

# Merge prefix strings
y2 &lt;- c("Light", "Small", "Improved", "Punjab", "SAM")
names &lt;- MergePrefix(names, y2, delim = c("space", "dash", "period"))

# Merge suffix strings
y3 &lt;- c("Local", "Bold", "Cary", "Mutant", "Runner", "Giant", "No.",
        "Bunch", "Peanut")
names &lt;- MergeSuffix(names, y3, delim = c("space", "dash", "period"))
</code></pre>

<hr>
<h2 id='MergeProbDup'>Merge two objects of class <code>ProbDup</code></h2><span id='topic+MergeProbDup'></span>

<h3>Description</h3>

<p><code>MergeProbDup</code> merges two objects of class <code>ProbDup</code> into a single
one.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MergeProbDup(pdup1, pdup2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MergeProbDup_+3A_pdup1">pdup1</code></td>
<td>
<p>An object of class <code>ProbDup</code>.</p>
</td></tr>
<tr><td><code id="MergeProbDup_+3A_pdup2">pdup2</code></td>
<td>
<p>An object of class <code>ProbDup</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>ProbDup</code> with the merged list of  fuzzy, phonetic
and semantic probable duplicate sets.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ProbDup">ProbDup</a></code>, <code><a href="#topic+SplitProbDup">SplitProbDup</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


## Not run: 
#' # Load PGR passport database
GN &lt;- GN1000

# Specify as a vector the database fields to be used
GNfields &lt;- c("NationalID", "CollNo", "DonorID", "OtherID1", "OtherID2")

# Clean the data
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) DataClean(x))
y1 &lt;- list(c("Gujarat", "Dwarf"), c("Castle", "Cary"), c("Small", "Japan"),
c("Big", "Japan"), c("Mani", "Blanco"), c("Uganda", "Erect"),
c("Mota", "Company"))
y2 &lt;- c("Dark", "Light", "Small", "Improved", "Punjab", "SAM")
y3 &lt;- c("Local", "Bold", "Cary", "Mutant", "Runner", "Giant", "No.",
        "Bunch", "Peanut")
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergeKW(x, y1, delim = c("space", "dash")))
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergePrefix(x, y2, delim = c("space", "dash")))
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergeSuffix(x, y3, delim = c("space", "dash")))

# Generate KWIC index
GNKWIC &lt;- KWIC(GN, GNfields)

# Specify the exceptions as a vector
exep &lt;- c("A", "B", "BIG", "BOLD", "BUNCH", "C", "COMPANY", "CULTURE", 
         "DARK", "E", "EARLY", "EC", "ERECT", "EXOTIC", "FLESH", "GROUNDNUT", 
         "GUTHUKAI", "IMPROVED", "K", "KUTHUKADAL", "KUTHUKAI", "LARGE", 
         "LIGHT", "LOCAL", "OF", "OVERO", "P", "PEANUT", "PURPLE", "R", 
         "RED", "RUNNER", "S1", "SAM", "SMALL", "SPANISH", "TAN", "TYPE", 
         "U", "VALENCIA", "VIRGINIA", "WHITE")
          
# Specify the synsets as a list
syn &lt;- list(c("CHANDRA", "AH114"), c("TG1", "VIKRAM"))

# Fetch probable duplicate sets
GNdup &lt;- ProbDup(kwic1 = GNKWIC, method = "a", excep = exep, fuzzy = TRUE,
                 phonetic = TRUE, encoding = "primary", 
                 semantic = TRUE, syn = syn)
                 
# Split the probable duplicate sets
GNdupSplit &lt;- SplitProbDup(GNdup, splitat = c(10, 10, 10))

# Merge the split sets
GNdupMerged &lt;- MergeProbDup(GNdupSplit[[1]], GNdupSplit[[3]])


## End(Not run)



</code></pre>

<hr>
<h2 id='ParseProbDup'>Parse an object of class <code>ProbDup</code> to a data frame.</h2><span id='topic+ParseProbDup'></span>

<h3>Description</h3>

<p><code>ParseProbDup</code> converts an object of class <code>ProbDup</code> to a data frame for export.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ParseProbDup(pdup, max.count = 30, insert.blanks = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ParseProbDup_+3A_pdup">pdup</code></td>
<td>
<p>An object of class <code>ProbDup</code>.</p>
</td></tr>
<tr><td><code id="ParseProbDup_+3A_max.count">max.count</code></td>
<td>
<p>The maximum count of probable duplicate sets which are to be parsed to a data frame.</p>
</td></tr>
<tr><td><code id="ParseProbDup_+3A_insert.blanks">insert.blanks</code></td>
<td>
<p>logical. If <code>TRUE</code>, inserts a row of <code>NAs</code> 
after each set.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame of the long/narrow form of the probable duplicate sets 
data with the following core columns: </p>

<table>
<tr>
 <td style="text-align: left;"> 
  <code>SET_NO</code> </td><td style="text-align: left;"> The set number. </td>
</tr>
<tr>
 <td style="text-align: left;"> <code>TYPE</code> </td><td style="text-align: left;"> The type of 
  probable duplicate set. 'F' for fuzzy, 'P' for phonetic and 'S' for 
  semantic matching sets. </td>
</tr>
<tr>
 <td style="text-align: left;"> <code>K</code> </td><td style="text-align: left;"> The KWIC index or database of 
  origin of the record. The <code>method</code> is specified within the square 
  brackets in the column name.  </td>
</tr>
<tr>
 <td style="text-align: left;"> <code>PRIM_ID</code> </td><td style="text-align: left;"> The primary ID of the
  accession record from which the set could be identified. </td>
</tr>
<tr>
 <td style="text-align: left;"> <code>IDKW</code> 
  </td><td style="text-align: left;"> The 'matching' keywords along with the IDs. </td>
</tr>
<tr>
 <td style="text-align: left;"> <code>COUNT</code> </td><td style="text-align: left;"> The number of elements in a set. </td>
</tr>
<tr>
 <td style="text-align: left;"> </td>
</tr>

</table>
<p> For the 
retrieved columns(fields) the prefix <code>K*</code> indicates the KWIC index of 
origin.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ProbDup">ProbDup</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


## Not run: 

#' # Load PGR passport database
GN &lt;- GN1000

# Specify as a vector the database fields to be used
GNfields &lt;- c("NationalID", "CollNo", "DonorID", "OtherID1", "OtherID2")

# Clean the data
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) DataClean(x))
y1 &lt;- list(c("Gujarat", "Dwarf"), c("Castle", "Cary"), c("Small", "Japan"),
c("Big", "Japan"), c("Mani", "Blanco"), c("Uganda", "Erect"),
c("Mota", "Company"))
y2 &lt;- c("Dark", "Light", "Small", "Improved", "Punjab", "SAM")
y3 &lt;- c("Local", "Bold", "Cary", "Mutant", "Runner", "Giant", "No.",
        "Bunch", "Peanut")
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergeKW(x, y1, delim = c("space", "dash")))
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergePrefix(x, y2, delim = c("space", "dash")))
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergeSuffix(x, y3, delim = c("space", "dash")))

# Generate KWIC index
GNKWIC &lt;- KWIC(GN, GNfields)

# Specify the exceptions as a vector
exep &lt;- c("A", "B", "BIG", "BOLD", "BUNCH", "C", "COMPANY", "CULTURE", 
         "DARK", "E", "EARLY", "EC", "ERECT", "EXOTIC", "FLESH", "GROUNDNUT", 
         "GUTHUKAI", "IMPROVED", "K", "KUTHUKADAL", "KUTHUKAI", "LARGE", 
         "LIGHT", "LOCAL", "OF", "OVERO", "P", "PEANUT", "PURPLE", "R", 
         "RED", "RUNNER", "S1", "SAM", "SMALL", "SPANISH", "TAN", "TYPE", 
         "U", "VALENCIA", "VIRGINIA", "WHITE")
          
# Specify the synsets as a list
syn &lt;- list(c("CHANDRA", "AH114"), c("TG1", "VIKRAM"))

# Fetch probable duplicate sets
GNdup &lt;- ProbDup(kwic1 = GNKWIC, method = "a", excep = exep, fuzzy = TRUE,
                 phonetic = TRUE, encoding = "primary", 
                 semantic = TRUE, syn = syn)
                 
# Convert to data frame of sets               
GNdupParsed &lt;- ParseProbDup(GNdup)


## End(Not run)



</code></pre>

<hr>
<h2 id='PGRdup-package'>The PGRdup Package</h2><span id='topic+PGRdup-package'></span><span id='topic+PGRdup'></span>

<h3>Description</h3>

<p>Functions to facilitate genebank managers in the identification of probable 
duplicate accessions from plant genetic resources (PGR) passport databases.
</p>


<h3>Author(s)</h3>

<p>J Aravind <a href="mailto:aravindj@nbpgr.ernet.in">aravindj@nbpgr.ernet.in</a> <br /> J Radhamani 
<a href="mailto:radhamani@nbpgr.ernet.in">radhamani@nbpgr.ernet.in</a> <br /> Kalyani Srinivasan 
<a href="mailto:kalyani@nbpgr.ernet.in">kalyani@nbpgr.ernet.in</a> <br /> B Ananda Subhash 
<a href="mailto:anandasubhash@gmail.com">anandasubhash@gmail.com</a> <br /> RK Tyagi
<a href="mailto:rktyagi@nbpgr.ernet.in">rktyagi@nbpgr.ernet.in</a>
</p>

<hr>
<h2 id='print.KWIC'>Prints summary of <code>KWIC</code> object.</h2><span id='topic+print.KWIC'></span>

<h3>Description</h3>

<p><code>print.KWIC</code> prints to console the summary of an object of class 
<code>KWIC</code> including the database fields(columns) used, the total number of
keywords and the number of distinct keywords in the index.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'KWIC'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.KWIC_+3A_x">x</code></td>
<td>
<p>An object of class <code>KWIC</code>.</p>
</td></tr>
<tr><td><code id="print.KWIC_+3A_...">...</code></td>
<td>
<p>Unused</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+KWIC">KWIC</a></code>
</p>

<hr>
<h2 id='print.ProbDup'>Prints summary of <code>ProbDup</code> object.</h2><span id='topic+print.ProbDup'></span>

<h3>Description</h3>

<p><code>print.ProbDup</code> prints to console the summary of an object of class 
<code>ProbDup</code> including the method used (&quot;a&quot;, &quot;b&quot; or &quot;c&quot;), the database
fields(columns) considered, the number of probable duplicate sets of each
kind along with the corresponding number of records.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ProbDup'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.ProbDup_+3A_x">x</code></td>
<td>
<p>An object of class <code>ProbDup</code>.</p>
</td></tr>
<tr><td><code id="print.ProbDup_+3A_...">...</code></td>
<td>
<p>Unused</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+ProbDup">ProbDup</a></code>
</p>

<hr>
<h2 id='ProbDup'>Identify probable duplicates of accessions</h2><span id='topic+ProbDup'></span>

<h3>Description</h3>

<p><code>ProbDup</code> identifies probable duplicates of germplasm accessions in KWIC
indexes created from PGR passport databases using fuzzy, phonetic and
semantic matching strategies.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ProbDup(
  kwic1,
  kwic2 = NULL,
  method = c("a", "b", "c"),
  excep = NULL,
  chunksize = 1000,
  useBytes = TRUE,
  fuzzy = TRUE,
  max.dist = 3,
  force.exact = TRUE,
  max.alpha = 4,
  max.digit = Inf,
  phonetic = TRUE,
  encoding = c("primary", "alternate"),
  phon.min.alpha = 5,
  min.enc = 3,
  semantic = FALSE,
  syn = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ProbDup_+3A_kwic1">kwic1</code></td>
<td>
<p>An object of class <code>KWIC</code>.</p>
</td></tr>
<tr><td><code id="ProbDup_+3A_kwic2">kwic2</code></td>
<td>
<p>An object of class <code>KWIC</code>. Required for <code>method</code>
<code>"b"</code> and <code>"c"</code> only (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="ProbDup_+3A_method">method</code></td>
<td>
<p>The method to be followed for identification of probable
duplicates. Either <code>"a"</code>, <code>"b"</code> or <code>"c"</code>. (see
<strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="ProbDup_+3A_excep">excep</code></td>
<td>
<p>A vector of the keywords in KWIC not to be used for probable
duplicate search (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="ProbDup_+3A_chunksize">chunksize</code></td>
<td>
<p>A value indicating the size of KWIC index keyword block to
be used for searching for matches at a time in case of large number of
keywords(see <strong>Note</strong>).</p>
</td></tr>
<tr><td><code id="ProbDup_+3A_usebytes">useBytes</code></td>
<td>
<p>logical. If <code>TRUE</code>, performs byte-wise comparison
instead of character-wise comparison (see <strong>Note</strong>).</p>
</td></tr>
<tr><td><code id="ProbDup_+3A_fuzzy">fuzzy</code></td>
<td>
<p>logical. If <code>TRUE</code> identifies probable duplicates based on
fuzzy matching.</p>
</td></tr>
<tr><td><code id="ProbDup_+3A_max.dist">max.dist</code></td>
<td>
<p>The maximum levenshtein distance between keyword strings
allowed for a match. Default is 3 (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="ProbDup_+3A_force.exact">force.exact</code></td>
<td>
<p>logical. If <code>TRUE</code>, enforces exact matching instead
of fuzzy matching for keyword strings which match the criteria specified in
arguments <code>max.alpha</code> and <code>max.digit</code> (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="ProbDup_+3A_max.alpha">max.alpha</code></td>
<td>
<p>Maximum number of alphabet characters present in a keyword
string up to which exact matching is enforced rather than fuzzy matching.
Default is 4 (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="ProbDup_+3A_max.digit">max.digit</code></td>
<td>
<p>Maximum number of numeric characters present in a keyword
string up to which exact matching is enforced rather than fuzzy matching.
Default is Inf (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="ProbDup_+3A_phonetic">phonetic</code></td>
<td>
<p>logical. If <code>TRUE</code> identifies probable duplicates based
on phonetic matching.</p>
</td></tr>
<tr><td><code id="ProbDup_+3A_encoding">encoding</code></td>
<td>
<p>Double metaphone encoding for phonetic matching. The default
is <code>"primary"</code> (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="ProbDup_+3A_phon.min.alpha">phon.min.alpha</code></td>
<td>
<p>Minimum number of alphabet characters to be present in
a keyword string for phonetic matching (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="ProbDup_+3A_min.enc">min.enc</code></td>
<td>
<p>Minimum number of characters to be be present in double
metaphone encoding of a keyword string for phonetic matching (see
<strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="ProbDup_+3A_semantic">semantic</code></td>
<td>
<p>logical. If <code>TRUE</code> identifies probable duplicates based
on semantic matching.</p>
</td></tr>
<tr><td><code id="ProbDup_+3A_syn">syn</code></td>
<td>
<p>A list with character vectors of synsets (see <strong>Details</strong>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function performs fuzzy, phonetic and semantic matching of keywords in
KWIC indexes of PGR passport databases (created using
<code><a href="#topic+KWIC">KWIC</a></code> function) to identify probable duplicates of
germplasm accessions. The function can execute matching according to either
of the following three methods as specified by the <code>method</code> argument.
</p>
 <dl>
<dt>Method <code>a</code>:</dt><dd><p>Perform string matching of keywords in a
single KWIC index to identify probable duplicates of accessions in a single
PGR passport database.</p>
</dd> <dt>Method <code>b</code>:</dt><dd><p>Perform string matching of
keywords in the first KWIC index (query) with that of the keywords in the
second index (source) to identify probable duplicates of accessions of the
first PGR passport database among the accessions in the second database.</p>
</dd>
<dt>Method <code>c</code>:</dt><dd><p>Perform string matching of keywords in two different
KWIC indexes jointly to identify probable duplicates of accessions from among
two PGR passport databases.</p>
</dd></dl>

<p><strong>Fuzzy matching</strong> or approximate string matching of keywords is carried
out by computing the generalized levenshtein (edit) distance between them.
This distance measure  counts the number of deletions, insertions and
substitutions necessary to turn one string to the another. A distance of up
to <code>max.dist</code> are considered for a match.
</p>
<p>Exact matching will be enforced when the argument <code>force.exact</code> is
<code>TRUE</code>. It can be used to avoid fuzzy matching when the number of
alphabet characters in keywords is lesser than a critical value
(<code>max.alpha</code>). Similarly, the value of <code>max.digit</code> can also be set
according to the requirements. The default value of <code>Inf</code> avoids fuzzy
matching and enforces exact matching for all keywords having any numerical
characters. If <code>max.digit</code> and <code>max.alpha</code> are both set to
<code>Inf</code>, exact matching will be enforced for all the keywords.
</p>
<p>When exact matching is enforced, for keywords having both alphabet and
numeric characters and with the number of alphabet characters greater than
<code>max.digit</code>, matching will be carried out separately for alphabet and
numeric characters present.
</p>
<p><strong>Phonetic matching</strong> of keywords is carried out using the Double
Metaphone phonetic algorithm (<code><a href="#topic+DoubleMetaphone">DoubleMetaphone</a></code>) to
identify keywords that have the similar pronunciation. Either the
<code>primary</code> or <code>alternate</code> encodings can be used by specifying the
<code>encoding</code> argument. The argument <code>phon.min.alpha</code> sets the limits
for the number of alphabet characters to be present in a string for executing
phonetic matching. Similarly <code>min.enc</code> sets the limits for the number of
characters to be present in the encoding of a keyword for phonetic matching.
</p>
<p><strong>Semantic matching</strong> matches keywords based on a list of accession name
synonyms supplied as list with character vectors of synonym sets (synsets) to
the <code>syn</code> argument. Synonyms in this context refers to interchangeable
identifiers or names by which an accession is recognized. Multiple keywords
specified as members of the same synset in <code>syn</code> are merged together. To
facilitate accurate identification of synonyms from the KWIC index, identical
data standardization operations using the <code><a href="#topic+MergeKW">MergeKW</a></code> and
<code><a href="#topic+DataClean">DataClean</a></code> functions for both the original database
fields and the synset list are recommended.
</p>
<p>The probable duplicate sets identified initially here may be intersecting
with other sets. To get the disjoint sets after the union of all the
intersecting sets use the <code><a href="#topic+DisProbDup">DisProbDup</a></code> function.
</p>
<p>The function <code><a href="#topic+AddProbDup">AddProbDup</a></code> can be used to add the
information associated with the identified sets in an object of class
<code>ProbDup</code> as fields(columns) to the original PGR passport database.
</p>
<p>All of the string matching operations here are executed through the
<code><a href="stringdist.html#topic+stringdist-package">stringdist-package</a></code> functions.
</p>


<h3>Value</h3>

<p>A list of class <code>ProbDup</code> containing the following data frames
of probable duplicate sets identified along with the corresponding keywords
and set counts: </p>
 <ol>
<li> <p><code>FuzzyDuplicates</code> </p>
</li>
<li>
<p><code>PhoneticDuplicates</code> </p>
</li>
<li> <p><code>SemanticDuplicates</code> </p>
</li></ol>
<p> Each data frame
has the following columns: </p>

<table>
<tr>
 <td style="text-align: left;"> <code>SET_NO</code> </td><td style="text-align: left;"> The set number.
  </td>
</tr>
<tr>
 <td style="text-align: left;"> <code>TYPE</code> </td><td style="text-align: left;"> The type of probable duplicate set. 'F' for fuzzy, 'P'
  for phonetic and 'S' for semantic matching sets. </td>
</tr>
<tr>
 <td style="text-align: left;"> <code>ID</code> </td><td style="text-align: left;"> The
  primary IDs of records of accessions comprising a set. </td>
</tr>
<tr>
 <td style="text-align: left;"> <code>ID:KW</code>
  </td><td style="text-align: left;"> The 'matching' keywords along with the IDs. </td>
</tr>
<tr>
 <td style="text-align: left;"> <code>COUNT</code> </td><td style="text-align: left;"> The
  number of elements in a set. </td>
</tr>
<tr>
 <td style="text-align: left;"> </td>
</tr>

</table>

<p>The prefix <code>[K*]</code> indicates the KWIC index of origin of the KEYWORD or
PRIM_ID.
</p>


<h3>Note</h3>

<p>As the number of keywords in the KWIC indexes increases, the memory
consumption by the function also increases. For string matching, this
function relies upon creation of a <code class="reqn">n</code>*<code class="reqn">m</code> matrix of all possible
keyword pairs for comparison, where <code class="reqn">n</code> and <code class="reqn">m</code> are the number of
keywords in the query and source indexes respectively. This can lead to
<code>cannot allocate vector of size</code> errors in case very large KWIC
indexes where the comparison matrix is too large to reside in memory. In
such a case, try to adjust the <code>chunksize</code> argument to get the
appropriate size of the KWIC index keyword block to be used for searching
for matches at a time. However a smaller chunksize may lead to longer
computation time due to the memory-time trade-off.
</p>
<p>The progress of matching is displayed in the console as number of blocks
completed out of total (e.g. 6 / 30), the percentage of achievement (e.g.
30%) and a text-based progress bar.
</p>
<p>In case of multi-byte characters in keywords, the matching speed is further
dependent upon the <code>useBytes</code> argument as described in
<strong>Encoding issues</strong> for the <code><a href="stringdist.html#topic+stringdist">stringdist</a></code>
function, which is made use of here for string matching.
</p>


<h3>References</h3>

<p>van der Loo, M. P. J. 2014. &quot;The Stringdist Package for
Approximate String Matching.&quot; <em>R Journal</em> 6 (1):111-22.
<a href="https://journal.r-project.org/archive/2014/RJ-2014-011/index.html">https://journal.r-project.org/archive/2014/RJ-2014-011/index.html</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+KWIC">KWIC</a></code>, <code><a href="#topic+DoubleMetaphone">DoubleMetaphone</a></code>
<code><a href="stringdist.html#topic+stringdist">stringdistmatrix</a></code>,
<code><a href="utils.html#topic+adist">adist</a></code>, <code><a href="#topic+print.ProbDup">print.ProbDup</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


## Not run: 

# Method "a"
#===========

# Load PGR passport database
GN &lt;- GN1000

# Specify as a vector the database fields to be used
GNfields &lt;- c("NationalID", "CollNo", "DonorID", "OtherID1", "OtherID2")

# Clean the data
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) DataClean(x))
y1 &lt;- list(c("Gujarat", "Dwarf"), c("Castle", "Cary"), c("Small", "Japan"),
c("Big", "Japan"), c("Mani", "Blanco"), c("Uganda", "Erect"),
c("Mota", "Company"))
y2 &lt;- c("Dark", "Light", "Small", "Improved", "Punjab", "SAM")
y3 &lt;- c("Local", "Bold", "Cary", "Mutant", "Runner", "Giant", "No.",
        "Bunch", "Peanut")
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergeKW(x, y1, delim = c("space", "dash")))
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergePrefix(x, y2, delim = c("space", "dash")))
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergeSuffix(x, y3, delim = c("space", "dash")))

# Generate KWIC index
GNKWIC &lt;- KWIC(GN, GNfields)

# Specify the exceptions as a vector
exep &lt;- c("A", "B", "BIG", "BOLD", "BUNCH", "C", "COMPANY", "CULTURE",
         "DARK", "E", "EARLY", "EC", "ERECT", "EXOTIC", "FLESH", "GROUNDNUT",
         "GUTHUKAI", "IMPROVED", "K", "KUTHUKADAL", "KUTHUKAI", "LARGE",
         "LIGHT", "LOCAL", "OF", "OVERO", "P", "PEANUT", "PURPLE", "R",
         "RED", "RUNNER", "S1", "SAM", "SMALL", "SPANISH", "TAN", "TYPE",
         "U", "VALENCIA", "VIRGINIA", "WHITE")

# Specify the synsets as a list
syn &lt;- list(c("CHANDRA", "AH114"), c("TG1", "VIKRAM"))

# Fetch probable duplicate sets
GNdup &lt;- ProbDup(kwic1 = GNKWIC, method = "a", excep = exep, fuzzy = TRUE,
                 phonetic = TRUE, encoding = "primary",
                 semantic = TRUE, syn = syn)
GNdup

# Method "b and c"
#=================

# Load PGR passport databases
GN1 &lt;- GN1000[!grepl("^ICG", GN1000$DonorID), ]
GN1$DonorID &lt;- NULL
GN2 &lt;- GN1000[grepl("^ICG", GN1000$DonorID), ]
GN2 &lt;- GN2[!grepl("S", GN2$DonorID), ]
GN2$NationalID &lt;- NULL

# Specify as a vector the database fields to be used
GN1fields &lt;- c("NationalID", "CollNo", "OtherID1", "OtherID2")
GN2fields &lt;- c("DonorID", "CollNo", "OtherID1", "OtherID2")

# Clean the data
GN1[GN1fields] &lt;- lapply(GN1[GN1fields], function(x) DataClean(x))
GN2[GN2fields] &lt;- lapply(GN2[GN2fields], function(x) DataClean(x))
y1 &lt;- list(c("Gujarat", "Dwarf"), c("Castle", "Cary"), c("Small", "Japan"),
c("Big", "Japan"), c("Mani", "Blanco"), c("Uganda", "Erect"),
c("Mota", "Company"))
y2 &lt;- c("Dark", "Light", "Small", "Improved", "Punjab", "SAM")
y3 &lt;- c("Local", "Bold", "Cary", "Mutant", "Runner", "Giant", "No.",
        "Bunch", "Peanut")
GN1[GN1fields] &lt;- lapply(GN1[GN1fields], function(x) MergeKW(x, y1, delim = c("space", "dash")))
GN1[GN1fields] &lt;- lapply(GN1[GN1fields], function(x) MergePrefix(x, y2, delim = c("space", "dash")))
GN1[GN1fields] &lt;- lapply(GN1[GN1fields], function(x) MergeSuffix(x, y3, delim = c("space", "dash")))
GN2[GN2fields] &lt;- lapply(GN2[GN2fields], function(x) MergeKW(x, y1, delim = c("space", "dash")))
GN2[GN2fields] &lt;- lapply(GN2[GN2fields], function(x) MergePrefix(x, y2, delim = c("space", "dash")))
GN2[GN2fields] &lt;- lapply(GN2[GN2fields], function(x) MergeSuffix(x, y3, delim = c("space", "dash")))

# Remove duplicated DonorID records in GN2
GN2 &lt;- GN2[!duplicated(GN2$DonorID), ]

# Generate KWIC index
GN1KWIC &lt;- KWIC(GN1, GN1fields)
GN2KWIC &lt;- KWIC(GN2, GN2fields)

# Specify the exceptions as a vector
exep &lt;- c("A", "B", "BIG", "BOLD", "BUNCH", "C", "COMPANY", "CULTURE",
         "DARK", "E", "EARLY", "EC", "ERECT", "EXOTIC", "FLESH", "GROUNDNUT",
         "GUTHUKAI", "IMPROVED", "K", "KUTHUKADAL", "KUTHUKAI", "LARGE",
         "LIGHT", "LOCAL", "OF", "OVERO", "P", "PEANUT", "PURPLE", "R",
         "RED", "RUNNER", "S1", "SAM", "SMALL", "SPANISH", "TAN", "TYPE",
         "U", "VALENCIA", "VIRGINIA", "WHITE")

# Specify the synsets as a list
syn &lt;- list(c("CHANDRA", "AH114"), c("TG1", "VIKRAM"))

# Fetch probable duplicate sets
GNdupb &lt;- ProbDup(kwic1 = GN1KWIC, kwic2 = GN2KWIC, method = "b",
                  excep = exep, fuzzy = TRUE, phonetic = TRUE,
                  encoding = "primary", semantic = TRUE, syn = syn)
GNdupb

GNdupc &lt;- ProbDup(kwic1 = GN1KWIC, kwic2 = GN2KWIC, method = "c",
                  excep = exep, fuzzy = TRUE, phonetic = TRUE,
                  encoding = "primary", semantic = TRUE, syn = syn)
GNdupc


## End(Not run)



</code></pre>

<hr>
<h2 id='read.genesys'>Convert 'Darwin Core - Germplasm' zip archive to a flat file</h2><span id='topic+read.genesys'></span>

<h3>Description</h3>

<p><code>read.genesys</code> reads PGR data in a Darwin Core - germplasm zip 
archive downloaded from genesys database and creates a flat file
<code>data.frame</code> from it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read.genesys(zip.genesys, scrub.names.space = TRUE, readme = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read.genesys_+3A_zip.genesys">zip.genesys</code></td>
<td>
<p>A character vector giving the file path to the downloaded 
zip file from Genesys.</p>
</td></tr>
<tr><td><code id="read.genesys_+3A_scrub.names.space">scrub.names.space</code></td>
<td>
<p>logical. If <code>TRUE</code>, all space characters are 
removed from name field in names extension (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="read.genesys_+3A_readme">readme</code></td>
<td>
<p>logical. If <code>TRUE</code>, the genesys zip file readme is printed
to console.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function helps to import to R environment, the PGR data 
downloaded from genesys database <a href="https://www.genesys-pgr.org/">https://www.genesys-pgr.org/</a> as a 
Darwin Core - germplasm (DwC-germplasm) zip archive. The different csv files 
in the archive are merged as a flat file into a single <code>data.frame</code>.
</p>
<p>All the space characters can be removed from the fields corresponding to 
accession names such as acceNumb, collNumb, ACCENAME, COLLNUMB, DONORNUMB and
OTHERNUMB using the argument <code>scrub.names.space</code> to facilitate creation 
of KWIC index with <code><a href="#topic+KWIC">KWIC</a></code> function and subsequent
matching operations to identify probable duplicates with
<code><a href="#topic+ProbDup">ProbDup</a></code> function.
</p>
<p>The argument <code>readme</code> can be used to print the readme file in the 
archive to console, if required.
</p>


<h3>Value</h3>

<p>A data.frame with the flat file form of the genesys data.
</p>


<h3>See Also</h3>

<p><code><a href="data.table.html#topic+data.table">data.table</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


## Not run: 
# Import the DwC-Germplasm zip archive "genesys-accessions-filtered.zip"
PGRgenesys &lt;- read.genesys("genesys-accessions-filtered.zip",
                           scrub.names.space = TRUE, readme = TRUE)

## End(Not run)



</code></pre>

<hr>
<h2 id='ReconstructProbDup'>Reconstruct an object of class ProbDup</h2><span id='topic+ReconstructProbDup'></span>

<h3>Description</h3>

<p><code>ReconstructProbDup</code> reconstructs a data frame of probable duplicate 
sets created using the function <code>ReviewProbDup</code> and subjected to manual 
clerical review, back into an object of class <code>ProbDup</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ReconstructProbDup(rev)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ReconstructProbDup_+3A_rev">rev</code></td>
<td>
<p>A data frame with the the core columns(fields) <code>SET_NO</code>, 
<code>TYPE</code>, <code>K</code>, <code>PRIM_ID</code>, <code>DEL</code>, <code>SPLIT</code>, 
<code>COUNT</code> and <code>IDKW</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>A data frame created using the function <code><a href="#topic+ReviewProbDup">ReviewProbDup</a></code> 
from an object of class <code>ProbDup</code> for manual clerical review of 
identified probable duplicate sets can be reconstituted back to the same 
object after the review using this function. The instructions for modifying 
the sets entered in the appropriate format in the columns <code>DEL</code> and 
<code>SPLIT</code> during clerical review are taken into account for reconstituting
the probable duplicate sets.
</p>
<p>Any records with <code>Y</code> in column <code>DEL</code> are deleted and records with 
identical integers in the column <code>SPLIT</code> other than the default <code>0</code>
are reassembled into a new set.
</p>


<h3>Value</h3>

<p>An object of class <code>ProbDup</code> with the modified fuzzy,
phonetic and semantic probable duplicate sets according to the instructions
specified under clerical review.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ProbDup">ProbDup</a></code>, <code><a href="#topic+ReviewProbDup">ReviewProbDup</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


## Not run: 

# Load PGR passport database
GN &lt;- GN1000

# Specify as a vector the database fields to be used
GNfields &lt;- c("NationalID", "CollNo", "DonorID", "OtherID1", "OtherID2")

# Clean the data
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) DataClean(x))
y1 &lt;- list(c("Gujarat", "Dwarf"), c("Castle", "Cary"), c("Small", "Japan"),
c("Big", "Japan"), c("Mani", "Blanco"), c("Uganda", "Erect"),
c("Mota", "Company"))
y2 &lt;- c("Dark", "Light", "Small", "Improved", "Punjab", "SAM")
y3 &lt;- c("Local", "Bold", "Cary", "Mutant", "Runner", "Giant", "No.",
        "Bunch", "Peanut")
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergeKW(x, y1, delim = c("space", "dash")))
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergePrefix(x, y2, delim = c("space", "dash")))
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergeSuffix(x, y3, delim = c("space", "dash")))

# Generate KWIC index
GNKWIC &lt;- KWIC(GN, GNfields)

# Specify the exceptions as a vector
exep &lt;- c("A", "B", "BIG", "BOLD", "BUNCH", "C", "COMPANY", "CULTURE", 
         "DARK", "E", "EARLY", "EC", "ERECT", "EXOTIC", "FLESH", "GROUNDNUT", 
         "GUTHUKAI", "IMPROVED", "K", "KUTHUKADAL", "KUTHUKAI", "LARGE", 
         "LIGHT", "LOCAL", "OF", "OVERO", "P", "PEANUT", "PURPLE", "R", 
         "RED", "RUNNER", "S1", "SAM", "SMALL", "SPANISH", "TAN", "TYPE", 
         "U", "VALENCIA", "VIRGINIA", "WHITE")
          
# Specify the synsets as a list
syn &lt;- list(c("CHANDRA", "AH114"), c("TG1", "VIKRAM"))

# Fetch probable duplicate sets
GNdup &lt;- ProbDup(kwic1 = GNKWIC, method = "a", excep = exep, fuzzy = TRUE,
                 phonetic = TRUE, encoding = "primary", 
                 semantic = TRUE, syn = syn)

# Get disjoint probable duplicate sets of each kind
disGNdup &lt;- DisProbDup(GNdup, combine = NULL)

# Get the data frame for reviewing the duplicate sets identified
RevGNdup &lt;- ReviewProbDup(pdup = disGNdup, db1 = GN1000,
                          extra.db1 = c("SourceCountry", "TransferYear"), 
                          max.count = 30, insert.blanks = TRUE)
# Examine and review the duplicate sets using edit function
RevGNdup &lt;- edit(RevGNdup)

# Examine and make changes to a set
subset(RevGNdup, SET_NO==12 &amp; TYPE=="P", select= c(IDKW, DEL, SPLIT))
RevGNdup[c(110, 112, 114, 118, 121, 122, 124), 6] &lt;- "Y"
RevGNdup[c(111, 115, 128), 7] &lt;- 1
RevGNdup[c(113, 117, 120), 7] &lt;- 2
RevGNdup[c(116, 119), 7] &lt;- 3
RevGNdup[c(123, 125), 7] &lt;- 4
RevGNdup[c(126, 127), 7] &lt;- 5
subset(RevGNdup, SET_NO==12 &amp; TYPE=="P", select= c(IDKW, DEL, SPLIT))

# Reconstruct ProDup object
GNdup2 &lt;- ReconstructProbDup(RevGNdup)
lapply(disGNdup, nrow)
lapply(GNdup2, nrow)


## End(Not run)



</code></pre>

<hr>
<h2 id='ReviewProbDup'>Retrieve probable duplicate set information from PGR passport database for
review</h2><span id='topic+ReviewProbDup'></span>

<h3>Description</h3>

<p><code>ReviewProbDup</code> retrieves information associated with the probable
duplicate sets from the original PGR passport database(s) from which they
were identified in order to facilitate manual clerical review.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ReviewProbDup(
  pdup,
  db1,
  db2 = NULL,
  extra.db1 = NULL,
  extra.db2 = NULL,
  max.count = 30,
  insert.blanks = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ReviewProbDup_+3A_pdup">pdup</code></td>
<td>
<p>An object of class <code>ProbDup</code>.</p>
</td></tr>
<tr><td><code id="ReviewProbDup_+3A_db1">db1</code></td>
<td>
<p>A data frame of the PGR passport database.</p>
</td></tr>
<tr><td><code id="ReviewProbDup_+3A_db2">db2</code></td>
<td>
<p>A data frame of the PGR passport database. Required when
<code>pdup</code> was created using more than one KWIC Index.</p>
</td></tr>
<tr><td><code id="ReviewProbDup_+3A_extra.db1">extra.db1</code></td>
<td>
<p>A character vector of extra <code>db1</code> column names to be
retrieved.</p>
</td></tr>
<tr><td><code id="ReviewProbDup_+3A_extra.db2">extra.db2</code></td>
<td>
<p>A character vector of extra <code>db2</code> column names to be
retrieved.</p>
</td></tr>
<tr><td><code id="ReviewProbDup_+3A_max.count">max.count</code></td>
<td>
<p>The maximum count of probable duplicate sets whose
information is to be retrieved.</p>
</td></tr>
<tr><td><code id="ReviewProbDup_+3A_insert.blanks">insert.blanks</code></td>
<td>
<p>logical. If <code>TRUE</code>, inserts a row of /codeNAs
after each set.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function helps to retrieve PGR passport information associated with
fuzzy, phonetic or semantic probable duplicate sets in an object of class
<code>ProbDup</code> from the original databases(s) from which they were
identified. The original information of accessions comprising a set, which
have not been subjected to data standardization can be compared under manual
clerical review for the validation of the set.
</p>
<p>By default only the fields(columns) which were used initially for creation of
the KWIC indexes using the <code><a href="#topic+KWIC">KWIC</a></code> function are retrieved.
Additional fields(columns) if necessary can be specified using the
<code>extra.db1</code> and <code>extra.db2</code> arguments.
</p>
<p>The output data frame can be subjected to clerical review either after
exporting into an external spreadsheet using
<code><a href="utils.html#topic+write.table">write.csv</a></code> function or by using the
<code><a href="utils.html#topic+edit">edit</a></code> function.
</p>
<p>The column <code>DEL</code> can be used to indicate whether a record has to be
deleted from a set or not. <code>Y</code> indicates &quot;Yes&quot;, and the default <code>N</code>
indicates &quot;No&quot;.
</p>
<p>The column <code>SPLIT</code> similarly can be used to indicate whether a record in
a set has to be branched into a new set. A set of identical integers in this
column other than the default <code>0</code> can be used to indicate that they are
to be removed and assembled into a new set.
</p>


<h3>Value</h3>

<p>A data frame of the long/narrow form of the probable duplicate sets
data along with associated fields from the original database(s). The core
columns in the resulting data frame are as follows: </p>

<table>
<tr>
 <td style="text-align: left;">
  <code>SET_NO</code> </td><td style="text-align: left;"> The set number. </td>
</tr>
<tr>
 <td style="text-align: left;"> <code>TYPE</code> </td><td style="text-align: left;"> The type of
  probable duplicate set. 'F' for fuzzy, 'P' for phonetic and 'S' for
  semantic matching sets. </td>
</tr>
<tr>
 <td style="text-align: left;"> <code>K[*]</code> </td><td style="text-align: left;"> The KWIC index or database of
  origin of the record. The <code>method</code> is specified within the square
  brackets in the column name.  </td>
</tr>
<tr>
 <td style="text-align: left;"> <code>PRIM_ID</code> </td><td style="text-align: left;"> The primary ID of the
  accession record from which the set could be identified. </td>
</tr>
<tr>
 <td style="text-align: left;"> <code>IDKW</code>
  </td><td style="text-align: left;"> The 'matching' keywords along with the IDs. </td>
</tr>
<tr>
 <td style="text-align: left;"> <code>DEL</code> </td><td style="text-align: left;"> Column
  to indicate whether record has to be deleted or not. </td>
</tr>
<tr>
 <td style="text-align: left;"> <code>SPLIT</code> </td><td style="text-align: left;">
  Column to indicate whether record has to be branched and assembled into new
  set. </td>
</tr>
<tr>
 <td style="text-align: left;"> <code>COUNT</code> </td><td style="text-align: left;"> The number of elements in a set. </td>
</tr>
<tr>
 <td style="text-align: left;"> </td>
</tr>

</table>
<p> For the
retrieved columns(fields) the prefix <code>K*</code> indicates the KWIC index of
origin.
</p>


<h3>Note</h3>

<p>When any primary ID/key records in the fuzzy, phonetic or semantic
duplicate sets are found to be missing from the original databases
<code>db1</code> and <code>db2</code>, then they are ignored and only the matching
records are considered for retrieving the information with a warning.
</p>
<p>This may be due to data standardization of the primary ID/key field using
the function <code><a href="#topic+DataClean">DataClean</a></code> before creation of the KWIC
index and subsequent identification of probable duplicate sets. In such a
case, it is recommended to use an identical data standardization operation
on the databases <code>db1</code> and <code>db2</code> before running this function.
</p>
<p>With <code>R</code> &lt;= v3.0.2, due to copying of named objects by <code>list()</code>,
<code>Invalid .internal.selfref detected and fixed...</code> warning can appear,
which may be safely ignored.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+DataClean">DataClean</a></code>, <code><a href="#topic+KWIC">KWIC</a></code>,
<code><a href="#topic+ProbDup">ProbDup</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


## Not run: 

# Load PGR passport database
GN &lt;- GN1000

# Specify as a vector the database fields to be used
GNfields &lt;- c("NationalID", "CollNo", "DonorID", "OtherID1", "OtherID2")

# Clean the data
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) DataClean(x))
y1 &lt;- list(c("Gujarat", "Dwarf"), c("Castle", "Cary"), c("Small", "Japan"),
c("Big", "Japan"), c("Mani", "Blanco"), c("Uganda", "Erect"),
c("Mota", "Company"))
y2 &lt;- c("Dark", "Light", "Small", "Improved", "Punjab", "SAM")
y3 &lt;- c("Local", "Bold", "Cary", "Mutant", "Runner", "Giant", "No.",
        "Bunch", "Peanut")
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergeKW(x, y1, delim = c("space", "dash")))
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergePrefix(x, y2, delim = c("space", "dash")))
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergeSuffix(x, y3, delim = c("space", "dash")))

# Generate KWIC index
GNKWIC &lt;- KWIC(GN, GNfields)

# Specify the exceptions as a vector
exep &lt;- c("A", "B", "BIG", "BOLD", "BUNCH", "C", "COMPANY", "CULTURE",
         "DARK", "E", "EARLY", "EC", "ERECT", "EXOTIC", "FLESH", "GROUNDNUT",
         "GUTHUKAI", "IMPROVED", "K", "KUTHUKADAL", "KUTHUKAI", "LARGE",
         "LIGHT", "LOCAL", "OF", "OVERO", "P", "PEANUT", "PURPLE", "R",
         "RED", "RUNNER", "S1", "SAM", "SMALL", "SPANISH", "TAN", "TYPE",
         "U", "VALENCIA", "VIRGINIA", "WHITE")

# Specify the synsets as a list
syn &lt;- list(c("CHANDRA", "AH114"), c("TG1", "VIKRAM"))

# Fetch probable duplicate sets
GNdup &lt;- ProbDup(kwic1 = GNKWIC, method = "a", excep = exep, fuzzy = TRUE,
                 phonetic = TRUE, encoding = "primary",
                 semantic = TRUE, syn = syn)

# Get disjoint probable duplicate sets of each kind
disGNdup &lt;- DisProbDup(GNdup, combine = NULL)

# Get the data frame for reviewing the duplicate sets identified
RevGNdup &lt;- ReviewProbDup(pdup = disGNdup, db1 = GN1000,
                          extra.db1 = c("SourceCountry", "TransferYear"),
                          max.count = 30, insert.blanks = TRUE)
# Examine and review the duplicate sets using edit function
RevGNdup &lt;- edit(RevGNdup)

# OR examine and review the duplicate sets after exporting them as a csv file
write.csv(file="Duplicate sets for review.csv", x=RevGNdup)


## End(Not run)



</code></pre>

<hr>
<h2 id='SplitProbDup'>Split an object of class <code>ProbDup</code></h2><span id='topic+SplitProbDup'></span>

<h3>Description</h3>

<p><code>SplitProbDup</code> splits an object of class <code>ProbDup</code> into two on the 
basis of set counts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SplitProbDup(pdup, splitat = c(30, 30, 30))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SplitProbDup_+3A_pdup">pdup</code></td>
<td>
<p>An object of class <code>ProbDup</code>.</p>
</td></tr>
<tr><td><code id="SplitProbDup_+3A_splitat">splitat</code></td>
<td>
<p>A vector of 3 integers indicating the set count at which 
Fuzzy, Phonetic and Semantic duplicate sets in <code>pdup</code> are to be split.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the the divided objects of class <code>ProbDup</code> 
(<code>pdup1</code> and <code>pdup2</code>) along with the corresponding lists of
accessions present in each (<code>list1</code> and <code>list2</code>).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ProbDup">ProbDup</a></code>, <code><a href="#topic+MergeProbDup">MergeProbDup</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


## Not run: 
# Load PGR passport database
GN &lt;- GN1000

# Specify as a vector the database fields to be used
GNfields &lt;- c("NationalID", "CollNo", "DonorID", "OtherID1", "OtherID2")

# Clean the data
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) DataClean(x))
y1 &lt;- list(c("Gujarat", "Dwarf"), c("Castle", "Cary"), c("Small", "Japan"),
c("Big", "Japan"), c("Mani", "Blanco"), c("Uganda", "Erect"),
c("Mota", "Company"))
y2 &lt;- c("Dark", "Light", "Small", "Improved", "Punjab", "SAM")
y3 &lt;- c("Local", "Bold", "Cary", "Mutant", "Runner", "Giant", "No.",
        "Bunch", "Peanut")
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergeKW(x, y1, delim = c("space", "dash")))
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergePrefix(x, y2, delim = c("space", "dash")))
GN[GNfields] &lt;- lapply(GN[GNfields], function(x) MergeSuffix(x, y3, delim = c("space", "dash")))

# Generate KWIC index
GNKWIC &lt;- KWIC(GN, GNfields)

# Specify the exceptions as a vector
exep &lt;- c("A", "B", "BIG", "BOLD", "BUNCH", "C", "COMPANY", "CULTURE", 
         "DARK", "E", "EARLY", "EC", "ERECT", "EXOTIC", "FLESH", "GROUNDNUT", 
         "GUTHUKAI", "IMPROVED", "K", "KUTHUKADAL", "KUTHUKAI", "LARGE", 
         "LIGHT", "LOCAL", "OF", "OVERO", "P", "PEANUT", "PURPLE", "R", 
         "RED", "RUNNER", "S1", "SAM", "SMALL", "SPANISH", "TAN", "TYPE", 
         "U", "VALENCIA", "VIRGINIA", "WHITE")
          
# Specify the synsets as a list
syn &lt;- list(c("CHANDRA", "AH114"), c("TG1", "VIKRAM"))

# Fetch probable duplicate sets
GNdup &lt;- ProbDup(kwic1 = GNKWIC, method = "a", excep = exep, fuzzy = TRUE,
                 phonetic = TRUE, encoding = "primary", 
                 semantic = TRUE, syn = syn)
                 
# Split the probable duplicate sets
GNdupSplit &lt;- SplitProbDup(GNdup, splitat = c(10, 10, 10))


## End(Not run)



</code></pre>

<hr>
<h2 id='ValidatePrimKey'>Validate if a data frame column confirms to primary key/ID constraints</h2><span id='topic+ValidatePrimKey'></span>

<h3>Description</h3>

<p><code>ValidatePrimKey</code> checks if a column in a data frame confirms to the 
primary key/ID constraints of absence of duplicates and NULL values. Aberrant
records if encountered are returned in the output list.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ValidatePrimKey(x, prim.key)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ValidatePrimKey_+3A_x">x</code></td>
<td>
<p>A data frame.</p>
</td></tr>
<tr><td><code id="ValidatePrimKey_+3A_prim.key">prim.key</code></td>
<td>
<p>A character vector indicating the name of the data frame 
column to be validated for primary key/ID constraints (see 
<strong>Details</strong>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function checks whether a field(column) in a data frame of PGR passport 
database confirms to the primary key/ID constraints of absence of duplicates 
and NULL values. If records with nonconforming values in the column are 
encountered, they are returned in the output list for rectification.
</p>
<p>If multiple fields(columns) are given as a character vector in 
<code>prim.key</code> field, only the first element will be considered as the 
primary key/ID field(column).
</p>
<p>Cleaning of the data in the input field(column) using the 
<code><a href="#topic+DataClean">DataClean</a></code> function with appropriate arguments is 
suggested before running this function.
</p>
<p>It is recommended to run this function and rectify aberrant records in a PGR 
passport database before creating a KWIC index using the 
<code><a href="#topic+KWIC">KWIC</a></code> function.
</p>


<h3>Value</h3>

<p>A list with containing the following components: </p>

<table>
<tr>
 <td style="text-align: left;"> 
  <code>message1</code> </td><td style="text-align: left;"> Indicates whether duplicated values were encountered 
  in <code>prim.key</code> field(column) of data frame <code>x</code> or not. </td>
</tr>
<tr>
 <td style="text-align: left;"> 
  <code>Duplicates</code> </td><td style="text-align: left;"> A data frame of the records with duplicated prim.key
  values if they were encountered. </td>
</tr>
<tr>
 <td style="text-align: left;"> <code>message2</code> </td><td style="text-align: left;"> Indicates whether
  NULL values were encountered in <code>prim.key</code> field(column) of data frame
  <code>x</code> or not. </td>
</tr>
<tr>
 <td style="text-align: left;"> <code>NullRecords</code> </td><td style="text-align: left;"> A data frame of the records 
  with NULL prim.key values if they were encountered. </td>
</tr>
<tr>
 <td style="text-align: left;"> </td>
</tr>

</table>



<h3>See Also</h3>

<p><code><a href="#topic+DataClean">DataClean</a></code>, <code><a href="#topic+KWIC">KWIC</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>GN &lt;- GN1000
ValidatePrimKey(x=GN, prim.key="NationalID")
## Not run: 
# Show error in case of duplicates and NULL values 
# in the primary key/ID field "NationalID"
GN[1001:1005,] &lt;- GN[1:5,]
GN[1001,3] &lt;- ""
ValidatePrimKey(x=GN, prim.key="NationalID")
## End(Not run)
</code></pre>

<hr>
<h2 id='ViewProbDup'>Visualize the probable duplicate sets retrieved in a <code>ProbDup</code> object</h2><span id='topic+ViewProbDup'></span>

<h3>Description</h3>

<p><code>ViewProbDup</code> plots summary visualizations of accessions within the
probable duplicate sets retrieved in a <code>ProbDup</code> object according to a 
grouping factor field(column) in the original database(s).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ViewProbDup(
  pdup,
  db1,
  db2 = NULL,
  factor.db1,
  factor.db2 = NULL,
  max.count = 30,
  select,
  order = "type",
  main = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ViewProbDup_+3A_pdup">pdup</code></td>
<td>
<p>An object of class <code>ProbDup</code>.</p>
</td></tr>
<tr><td><code id="ViewProbDup_+3A_db1">db1</code></td>
<td>
<p>A data frame of the PGR passport database.</p>
</td></tr>
<tr><td><code id="ViewProbDup_+3A_db2">db2</code></td>
<td>
<p>A data frame of the PGR passport database. Required when 
<code>pdup</code> was created using more than one KWIC Index.</p>
</td></tr>
<tr><td><code id="ViewProbDup_+3A_factor.db1">factor.db1</code></td>
<td>
<p>The <code>db1</code> column to be considered for grouping the 
accessions. Should be of class character or factor.</p>
</td></tr>
<tr><td><code id="ViewProbDup_+3A_factor.db2">factor.db2</code></td>
<td>
<p>The <code>db2</code> column to be considered for grouping the 
accessions. Should be of class character or factor. retrieved.</p>
</td></tr>
<tr><td><code id="ViewProbDup_+3A_max.count">max.count</code></td>
<td>
<p>The maximum count of probable duplicate sets whose 
information is to be plotted (see <strong>Note</strong>).</p>
</td></tr>
<tr><td><code id="ViewProbDup_+3A_select">select</code></td>
<td>
<p>A character vector of factor names in <code>factor.db1</code> and/or 
<code>factor.db2</code> to be considered for grouping accessions (see 
<strong>Note</strong>).</p>
</td></tr>
<tr><td><code id="ViewProbDup_+3A_order">order</code></td>
<td>
<p>The order of the type of sets retrieved in the plot. The default
is <code>"type"</code> (see <strong>Details</strong>).</p>
</td></tr>
<tr><td><code id="ViewProbDup_+3A_main">main</code></td>
<td>
<p>The title of the plot.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the following objects: </p>

<table>
<tr>
 <td style="text-align: left;"> 
  <code>Summary1</code> </td><td style="text-align: left;"> The summary <code>data.frame</code> of number of accessions 
  per factor level. </td>
</tr>
<tr>
 <td style="text-align: left;"> <code>Summary2</code> </td><td style="text-align: left;"> The summary <code>data.frame</code> of
  number of accessions and sets per each type of sets classified according to
  factor levels. </td>
</tr>
<tr>
 <td style="text-align: left;"> <code>SummaryGrob</code> </td><td style="text-align: left;"> A grid graphical object (Grob) 
  of the summary visualization plot. Can be plotted using the <code>grid.arrange</code> function </td>
</tr>
<tr>
 <td style="text-align: left;"> </td>
</tr>

</table>



<h3>Note</h3>

<p>When any primary ID/key records in the fuzzy, phonetic or semantic 
duplicate sets are found to be missing from the original databases 
<code>db1</code> and <code>db2</code>, then they are ignored and only the matching 
records are considered for visualization.
</p>
<p>This may be due to data standardization of the primary ID/key field using 
the function <code><a href="#topic+DataClean">DataClean</a></code> before creation of the KWIC 
index and subsequent identification of probable duplicate sets. In such a 
case, it is recommended to use an identical data standardization operation 
on the databases <code>db1</code> and <code>db2</code> before running this function. 
For summary and visualization of the set information in the object of class
<code>ProbDup</code> by <code>ViewProbDup</code>, the disjoint of the retrieved sets 
are made use of, as they are more meaningful than the raw sets retrieved. 
So it is recommended that the disjoint of sets obtained using the 
<code>DisProbDup</code> be used as the input <code>pdup</code>.
</p>
<p>All the accession records in sets with count &gt; <code>max.count</code> will be 
considered as being unique.
</p>
<p>The factor levels in the <code>factor.db1</code> and/or <code>factor.db2</code> columns
corresponding to those mentioned in <code>select</code> argument alone will be 
considered for visualization. All other factor levels will be grouped 
together to a single level named &quot;Others&quot;.
</p>
<p>The argument <code>order</code> can be used to specify the order in which the 
type of sets retrieved are to be plotted in the visualization. The default 
<code>"type"</code> will order according to the kind of sets, <code>"sets"</code> will 
order according to the number of sets in each kind and <code>"acc"</code> will 
order according to the number of accessions in each kind.
</p>
<p>The individual plots are made using <code><a href="ggplot2.html#topic+ggplot">ggplot</a></code> and then 
grouped together using <code><a href="gridExtra.html#topic+gridExtra-package">gridExtra-package</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ProbDup">ProbDup</a></code>, <code><a href="#topic+DisProbDup">DisProbDup</a></code>, 
<code><a href="#topic+DataClean">DataClean</a></code>, <code><a href="ggplot2.html#topic+ggplot">ggplot</a></code>, 
<code><a href="gridExtra.html#topic+gridExtra-package">gridExtra-package</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>


## Not run: 

# Method "b and c"
#=================

# Load PGR passport databases
GN1 &lt;- GN1000[!grepl("^ICG", GN1000$DonorID), ]
GN1$DonorID &lt;- NULL
GN2 &lt;- GN1000[grepl("^ICG", GN1000$DonorID), ]
GN2 &lt;- GN2[!grepl("S", GN2$DonorID), ]
GN2$NationalID &lt;- NULL

GN1$SourceCountry &lt;- toupper(GN1$SourceCountry)
GN2$SourceCountry &lt;- toupper(GN2$SourceCountry)

GN1$SourceCountry &lt;- gsub("UNITED STATES OF AMERICA", "USA", GN1$SourceCountry)
GN2$SourceCountry &lt;- gsub("UNITED STATES OF AMERICA", "USA", GN2$SourceCountry)

# Specify as a vector the database fields to be used
GN1fields &lt;- c("NationalID", "CollNo", "OtherID1", "OtherID2")
GN2fields &lt;- c("DonorID", "CollNo", "OtherID1", "OtherID2")

# Clean the data
GN1[GN1fields] &lt;- lapply(GN1[GN1fields], function(x) DataClean(x))
GN2[GN2fields] &lt;- lapply(GN2[GN2fields], function(x) DataClean(x))
y1 &lt;- list(c("Gujarat", "Dwarf"), c("Castle", "Cary"), c("Small", "Japan"),
           c("Big", "Japan"), c("Mani", "Blanco"), c("Uganda", "Erect"),
           c("Mota", "Company"))
y2 &lt;- c("Dark", "Light", "Small", "Improved", "Punjab", "SAM")
y3 &lt;- c("Local", "Bold", "Cary", "Mutant", "Runner", "Giant", "No.",
        "Bunch", "Peanut")
GN1[GN1fields] &lt;- lapply(GN1[GN1fields], function(x) MergeKW(x, y1, delim = c("space", "dash")))
GN1[GN1fields] &lt;- lapply(GN1[GN1fields], function(x) MergePrefix(x, y2, delim = c("space", "dash")))
GN1[GN1fields] &lt;- lapply(GN1[GN1fields], function(x) MergeSuffix(x, y3, delim = c("space", "dash")))
GN2[GN2fields] &lt;- lapply(GN2[GN2fields], function(x) MergeKW(x, y1, delim = c("space", "dash")))
GN2[GN2fields] &lt;- lapply(GN2[GN2fields], function(x) MergePrefix(x, y2, delim = c("space", "dash")))
GN2[GN2fields] &lt;- lapply(GN2[GN2fields], function(x) MergeSuffix(x, y3, delim = c("space", "dash")))

# Remove duplicated DonorID records in GN2
GN2 &lt;- GN2[!duplicated(GN2$DonorID), ]

# Generate KWIC index
GN1KWIC &lt;- KWIC(GN1, GN1fields)
GN2KWIC &lt;- KWIC(GN2, GN2fields)

# Specify the exceptions as a vector
exep &lt;- c("A", "B", "BIG", "BOLD", "BUNCH", "C", "COMPANY", "CULTURE",
          "DARK", "E", "EARLY", "EC", "ERECT", "EXOTIC", "FLESH", "GROUNDNUT",
          "GUTHUKAI", "IMPROVED", "K", "KUTHUKADAL", "KUTHUKAI", "LARGE",
          "LIGHT", "LOCAL", "OF", "OVERO", "P", "PEANUT", "PURPLE", "R",
          "RED", "RUNNER", "S1", "SAM", "SMALL", "SPANISH", "TAN", "TYPE",
          "U", "VALENCIA", "VIRGINIA", "WHITE")

# Specify the synsets as a list
syn &lt;- list(c("CHANDRA", "AH114"), c("TG1", "VIKRAM"))

GNdupc &lt;- ProbDup(kwic1 = GN1KWIC, kwic2 = GN2KWIC, method = "c",
                  excep = exep, fuzzy = TRUE, phonetic = TRUE,
                  encoding = "primary", semantic = TRUE, syn = syn)

GNdupcView &lt;- ViewProbDup(GNdupc, GN1, GN2, "SourceCountry", "SourceCountry",
                         max.count = 30, select = c("INDIA", "USA"), order = "type",
                         main = "Groundnut Probable Duplicates")

library(gridExtra)                                                    
grid.arrange(GNdupcView$SummaryGrob)                          


## End(Not run)   


    
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
