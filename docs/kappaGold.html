<!DOCTYPE html><html lang="en"><head><title>Help for package kappaGold</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {kappaGold}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#kappaGold'><p>kappaGold package</p></a></li>
<li><a href='#agreem_binary'><p>Three reliability studies for some binary rating</p></a></li>
<li><a href='#depression'><p>Depression screening</p></a></li>
<li><a href='#diagnoses'><p>Psychiatric diagnoses</p></a></li>
<li><a href='#kappa_test'><p>Significance test for homogeneity of kappa coefficients in independent groups</p></a></li>
<li><a href='#kappa_test_corr'><p>Test for homogeneity of kappa in correlated groups</p></a></li>
<li><a href='#kappa2'><p>Cohen's kappa for nominal data</p></a></li>
<li><a href='#kappam_fleiss'><p>Fleiss' kappa for multiple nominal-scale raters</p></a></li>
<li><a href='#kappam_gold'><p>Agreement of a group of nominal-scale raters with a gold standard</p></a></li>
<li><a href='#kappam_vanbelle'><p>Agreement between two groups of raters</p></a></li>
<li><a href='#SC_test'><p>Script concordance test (SCT).</p></a></li>
<li><a href='#simulKappa'><p>Simulate rating data and calculate agreement with gold standard</p></a></li>
<li><a href='#stagingData'><p>Staging of colorectal carcinoma</p></a></li>
<li><a href='#victorinox'><p>delete-1 jackknife estimator</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Agreement of Nominal Scale Raters (with a Gold Standard)</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-12-09</td>
</tr>
<tr>
<td>Description:</td>
<td>Estimate agreement of a group of raters with a gold standard rating
    on a nominal scale. For a single gold standard rater the average pairwise
    agreement of raters with this gold standard is provided. For a group of (gold
    standard) raters the approach of S. Vanbelle, A. Albert (2009)
    &lt;<a href="https://doi.org/10.1007%2Fs11336-009-9116-1">doi:10.1007/s11336-009-9116-1</a>&gt; is implemented. Bias and standard error are
    estimated via delete-1 jackknife.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Imports:</td>
<td>future.apply (&ge; 1.6), purrr (&ge; 1.0), rlang (&ge; 1.0), stats,
tibble, tidyr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>dplyr, irr, knitr, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0)</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-12-09 22:40:42 UTC; kuhnmat</td>
</tr>
<tr>
<td>Author:</td>
<td>Matthias Kuhn <a href="https://orcid.org/0000-0003-2868-5155"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Jonas Breidenstein [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Matthias Kuhn &lt;matthias.kuhn@tu-dresden.de&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-12-09 22:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='kappaGold'>kappaGold package</h2><span id='topic+kappaGold-package'></span><span id='topic+kappaGold'></span>

<h3>Description</h3>

<p>Estimate agreement with a gold-standard rating for nominal categories.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Matthias Kuhn <a href="mailto:matthias.kuhn@tu-dresden.de">matthias.kuhn@tu-dresden.de</a> (<a href="https://orcid.org/0000-0003-2868-5155">ORCID</a>)
</p>
<p>Authors:
</p>

<ul>
<li><p> Jonas Breidenstein <a href="mailto:jonas.breidenstein@tu-dresden.de">jonas.breidenstein@tu-dresden.de</a>
</p>
</li></ul>


<hr>
<h2 id='agreem_binary'>Three reliability studies for some binary rating</h2><span id='topic+agreem_binary'></span>

<h3>Description</h3>

<p>The data are reported in a textbook from Fleiss, probably it is fictitious.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>agreem_binary
</code></pre>


<h3>Format</h3>

<p>A list that contains three matrices. Each matrix contains the result of a
study involving two raters. It is a binary rating scale (&quot;+&quot; and &quot;-&quot;).
</p>


<h3>Source</h3>

<p>Chapter 18, Problems 18.3
</p>


<h3>References</h3>

<p>Fleiss, J. L., Levin, B., &amp; Paik, M. C. Statistical
Methods for Rates and Proportions, 3rd edition, 2003, ISBN 0-471-52629-0
</p>

<hr>
<h2 id='depression'>Depression screening</h2><span id='topic+depression'></span>

<h3>Description</h3>

<p>Fifty general hospital patients, admitted to the Monash Medical Centre in
Melbourne, were randomly drawn from a larger sample described by Clarke et
al. (1993). Agreement between two different screening tests and a diagnosis
of depression was compared. Definition of depression included DSM-III-R Major
Depression, Dysthymia, Adjustment Disorderwith Depressed Mood, and Depression
NOS. Depression was determined empirically using the Cutoff (McKenzie &amp;
Clarke, 1992) program.
The screening tests consisted of
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depression
</code></pre>


<h3>Format</h3>

<p>A matrix with 50 observations and 3 variables:
</p>

<dl>
<dt>depression</dt><dd><p>diagnoses as determined by the Cutoff program</p>
</dd>
<dt>BDI</dt><dd><p>Beck Depression Inventory</p>
</dd>
<dt>GHQ</dt><dd><p>General Health Questionnaire</p>
</dd>
</dl>



<h3>Details</h3>


<ol>
<li><p> the Beck Depression Inventory (BDI) (Beck et al., 1961) and
</p>
</li>
<li><p> the General Health Questionnaire (GHQ) (Goldberg &amp; Williams, 1988)
</p>
</li></ol>



<h3>References</h3>

<p>McKenzie, D. P. et al., Comparing Correlated Kappas by Resampling:
Is One Level of Agreement Significantly Different from Another?
J. psychiat. Res, Vol. 30, 1996. <a href="https://doi.org/10.1016/S0022-3956%2896%2900033-7">doi:10.1016/S0022-3956(96)00033-7</a>
</p>

<hr>
<h2 id='diagnoses'>Psychiatric diagnoses</h2><span id='topic+diagnoses'></span>

<h3>Description</h3>

<p>N = 30 patients were given one of k = 5 diagnoses by some n = 6
psychiatrists out of 43 psychiatrists in total. The diagnoses are
</p>

<ol>
<li><p> Depression
</p>
</li>
<li><p> PD (=Personality Disorder)
</p>
</li>
<li><p> Schizophrenia
</p>
</li>
<li><p> Neurosis
</p>
</li>
<li><p> Other
</p>
</li></ol>



<h3>Usage</h3>

<pre><code class='language-R'>diagnoses
</code></pre>


<h3>Format</h3>



<h4><code>diagnoses</code></h4>

<p>A matrix with 30 rows and 6 columns:
</p>

<dl>
<dt>rater1</dt><dd><p>1st rating of some six raters</p>
</dd>
<dt>rater2</dt><dd><p>2nd rating of some six raters</p>
</dd>
<dt>rater3</dt><dd><p>3rd rating of some six raters</p>
</dd>
<dt>rater4</dt><dd><p>4th rating of some six raters</p>
</dd>
<dt>rater5</dt><dd><p>5th rating of some six raters</p>
</dd>
<dt>rater6</dt><dd><p>6th rating of some six raters</p>
</dd>
</dl>




<h3>Details</h3>

<p>A total of 43 psychiatrists provided diagnoses. In the actual
study (Sandifer, Hordern, Timbury, &amp; Green, 1968), between 6 and 10
psychiatrists from the pool of 43 were unsystematically selected to diagnose
a subject. Fleiss randomly selected six diagnoses per subject to bring the
number of assignments per patient down to a constant of six.
</p>
<p>As there is not a fixed set of six raters the ratings from the same column
are not related to each other. Therefore, compared to the dataset with the
same name in package <code>irr</code>, we applied a permutation of the six ratings.
</p>


<h3>References</h3>

<p>Sandifer, M. G., Hordern, A., Timbury, G. C., &amp; Green, L. M.
Psychiatric diagnosis: A comparative study in North Carolina, London and
Glasgow. British Journal of Psychiatry, 1968, 114, 1-9.
</p>
<p>Fleiss, J. L. Measuring nominal scale agreement among many
raters. Psychological Bulletin, 1971, 76(5), 378–382.
<a href="https://doi.org/10.1037/h0031619">doi:10.1037/h0031619</a>
</p>


<h3>See Also</h3>

<p>This dataset is also available as <code>diagnoses</code> in the irr-package on
CRAN.
</p>

<hr>
<h2 id='kappa_test'>Significance test for homogeneity of kappa coefficients in independent groups</h2><span id='topic+kappa_test'></span>

<h3>Description</h3>

<p>The null hypothesis states that the kappas for all involved groups are the
same (&quot;homogeneous&quot;). A prerequisite is that the groups are independent of
each other, this means the groups are comprised of different subjects and
each group has different raters. Each rater employs a nominal scale. The test
requires estimates of kappa and its standard error per group.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kappa_test(kappas, val = "value0", se = "se0", conf.level = 0.95)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kappa_test_+3A_kappas">kappas</code></td>
<td>
<p>list of kappas from different groups. It uses the kappa
estimate and its standard error.</p>
</td></tr>
<tr><td><code id="kappa_test_+3A_val">val</code></td>
<td>
<p>character. Name of field to extract kappa coefficient estimate.</p>
</td></tr>
<tr><td><code id="kappa_test_+3A_se">se</code></td>
<td>
<p>character. Name of field to extract standard error of kappa.</p>
</td></tr>
<tr><td><code id="kappa_test_+3A_conf.level">conf.level</code></td>
<td>
<p>numeric. confidence level of confidence interval for
overall kappa</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A common overall kappa coefficient across groups is estimated. The test
statistic assesses the weighted squared deviance of the individual kappas
from the overall kappa estimate. The weights depend on the provided standard
errors. Under H0, the test statistics is chi-square distributed.
</p>


<h3>Value</h3>

<p>list containing the test results, including the entries <code>statistic</code>
and <code>p.value</code> (class <code>htest</code>)
</p>


<h3>References</h3>

<p>Joseph L. Fleiss, Statistical Methods for Rates and Proportions,
3rd ed., 2003, section 18.1
</p>


<h3>Examples</h3>

<pre><code class='language-R'># three independent agreement studies (different raters, different subjects)
# each study involves two raters that employ a binary rating scale
k2_studies &lt;- lapply(agreem_binary, kappa2)

# combined estimate and test for homogeneity of kappa
kappa_test(kappas = k2_studies, val = "value", se = "se")


</code></pre>

<hr>
<h2 id='kappa_test_corr'>Test for homogeneity of kappa in correlated groups</h2><span id='topic+kappa_test_corr'></span>

<h3>Description</h3>

<p>Bootstrap test on kappa based on data with common subjects. The differences
in kappa between all groups (but first) relative to first group (e.g., Group
2 - Group 1) are considered.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kappa_test_corr(
  ratings,
  grpIdx,
  kappaF,
  kappaF_args = list(),
  B = 100,
  alternative = "two.sided",
  conf.level = 0.95
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kappa_test_corr_+3A_ratings">ratings</code></td>
<td>
<p>matrix. ratings as sbj x raters, including the multiple groups
to be tested</p>
</td></tr>
<tr><td><code id="kappa_test_corr_+3A_grpidx">grpIdx</code></td>
<td>
<p>list. Comprises numeric index vectors per group. Each group is
defined as set of raters (i.e., columns)</p>
</td></tr>
<tr><td><code id="kappa_test_corr_+3A_kappaf">kappaF</code></td>
<td>
<p>function or list of functions. kappa function to apply on each group.</p>
</td></tr>
<tr><td><code id="kappa_test_corr_+3A_kappaf_args">kappaF_args</code></td>
<td>
<p>list. Further arguments for the kappa function. By
default, these settings apply to all groups, but the settings can be
specified per group (as list of lists).</p>
</td></tr>
<tr><td><code id="kappa_test_corr_+3A_b">B</code></td>
<td>
<p>numeric. number of bootstrap samples. At least 1000 are recommended
for stable results.</p>
</td></tr>
<tr><td><code id="kappa_test_corr_+3A_alternative">alternative</code></td>
<td>
<p>character. Direction of alternative. Currently only
<code>'two.sided'</code> is supported.</p>
</td></tr>
<tr><td><code id="kappa_test_corr_+3A_conf.level">conf.level</code></td>
<td>
<p>numeric. confidence level for confidence intervals</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list. test results as class <code>htest</code>. The confidence interval shown
by <code>print</code> refers to the 1st difference <code>k1-k2</code>.
</p>


<h3>Note</h3>

<p>Due to limitations of the <code>htest</code> print method the confidence interval shown
by <code>print</code> refers to the 1st difference <code>k1-k2</code>. If there are more than 2
groups access all confidence intervals via entry <code>conf.int</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Compare Fleiss kappa between students and expert raters
# For real analyses use more bootstrap samples (B &gt;= 1000)
kappa_test_corr(ratings = SC_test, grpIdx = list(S=1:39, E=40:50), B = 125,
                kappaF = kappam_fleiss,
                kappaF_args = list(variant = "fleiss", ratingScale=-2:2))

</code></pre>

<hr>
<h2 id='kappa2'>Cohen's kappa for nominal data</h2><span id='topic+kappa2'></span>

<h3>Description</h3>

<p>Cohen's kappa is the classical agreement measure when two raters provide
ratings for subjects on a nominal scale.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kappa2(ratings, robust = FALSE, ratingScale = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kappa2_+3A_ratings">ratings</code></td>
<td>
<p>matrix (dimension nx2), containing the ratings as subjects by
raters</p>
</td></tr>
<tr><td><code id="kappa2_+3A_robust">robust</code></td>
<td>
<p>flag. Use robust estimate for random chance of agreement by
Brennan-Prediger?</p>
</td></tr>
<tr><td><code id="kappa2_+3A_ratingscale">ratingScale</code></td>
<td>
<p>Possible levels for the rating. Or <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data of ratings must be stored in a two column object, each rater is a
column and the subjects are in the rows. Every rating category is used and
the levels are sorted. Weighting of categories is currently not implemented.
</p>


<h3>Value</h3>

<p>list containing Cohen's kappa agreement measure (value) or <code>NULL</code> if
no valid subjects
</p>


<h3>See Also</h3>

<p><code><a href="irr.html#topic+kappa2">irr::kappa2()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 2 raters have assessed 4 subjects into categories "A", "B" or "C"
# organize ratings as two column matrix, one row per subject rated
m &lt;- rbind(sj1 = c("A", "A"),
           sj2 = c("C", "B"),
           sj3 = c("B", "C"),
           sj4 = c("C", "C"))
           
# Cohen's kappa -----
kappa2(ratings = m)

# robust variant ---------
kappa2(ratings = m, robust = TRUE)

</code></pre>

<hr>
<h2 id='kappam_fleiss'>Fleiss' kappa for multiple nominal-scale raters</h2><span id='topic+kappam_fleiss'></span>

<h3>Description</h3>

<p>When multiple raters judge subjects on a nominal scale we can assess their agreement with Fleiss' kappa.
It is a generalization of Cohen's Kappa for two raters and there are different variants how to assess chance agreement.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kappam_fleiss(
  ratings,
  variant = c("fleiss", "conger", "robust", "uniform"),
  detail = FALSE,
  ratingScale = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kappam_fleiss_+3A_ratings">ratings</code></td>
<td>
<p>matrix (subjects by raters), containing the ratings</p>
</td></tr>
<tr><td><code id="kappam_fleiss_+3A_variant">variant</code></td>
<td>
<p>Which variant of kappa? Default is Fleiss (1971). Other options are Conger (1980) or robust variant.</p>
</td></tr>
<tr><td><code id="kappam_fleiss_+3A_detail">detail</code></td>
<td>
<p>Should category-wise Kappas be computed? Only available for the Fleiss (1971) variant.</p>
</td></tr>
<tr><td><code id="kappam_fleiss_+3A_ratingscale">ratingScale</code></td>
<td>
<p>Specify possible levels for the rating. Default <code>NULL</code> means to use all unique levels from the sample.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Different <strong>variants</strong> of Fleiss' kappa are implemented.
By default (<code>variant="fleiss"</code>), the original Fleiss Kappa (1971) is calculated, together with an asymptotic standard error and test for kappa=0.
It assumes that the raters involved are not assumed to be the same (one-way ANOVA setting).
The marginal category proportions determine the chance agreement.
Setting <code>variant="conger"</code> gives the variant of Conger (1980) that reduces to Cohen's kappa when m=2 raters.
It assumes identical raters for the different subjects (two-way ANOVA setting).
The chance agreement is based on the category proportions of each rater separately.
Typically, the Conger variant yields slightly higher values than Fleiss kappa.
<code>variant="robust"</code> assumes a chance agreement of two raters to be simply 1/q, where q is the number of categories (uniform model).
</p>


<h3>Value</h3>

<p>list containing Fleiss's kappa agreement measure (value) or <code>NULL</code> if no subjects
</p>


<h3>See Also</h3>

<p><code><a href="irr.html#topic+kappam.fleiss">irr::kappam.fleiss()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 4 subjects were rated by 3 raters in categories "1", "2" or "3"
# organize ratings as matrix with subjects in rows and raters in columns
m &lt;- matrix(c("3", "2", "3",
              "2", "2", "1",
              "1", "3", "1",
              "2", "2", "3"), ncol = 3, byrow = TRUE)
kappam_fleiss(m)

# show category-wise kappas -----
kappam_fleiss(m, detail = TRUE)

</code></pre>

<hr>
<h2 id='kappam_gold'>Agreement of a group of nominal-scale raters with a gold standard</h2><span id='topic+kappam_gold'></span>

<h3>Description</h3>

<p>First, Cohen's kappa is calculated between each rater against the gold
standard which is taken from the 1st column by default. The average of these
kappas is returned as 'kappam_gold0'. The variant setting (<code style="white-space: pre;">&#8288;robust=&#8288;</code>) is
forwarded to Cohen's kappa. A bias-corrected version 'kappam_gold' and a
corresponding confidence interval are provided as well via the jackknife
method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kappam_gold(
  ratings,
  refIdx = 1,
  robust = FALSE,
  ratingScale = NULL,
  conf.level = 0.95
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kappam_gold_+3A_ratings">ratings</code></td>
<td>
<p>matrix. subjects by raters</p>
</td></tr>
<tr><td><code id="kappam_gold_+3A_refidx">refIdx</code></td>
<td>
<p>numeric. index of reference gold-standard raters. Currently,
only a single gold-standard rater is supported. By default, it is the 1st
rater.</p>
</td></tr>
<tr><td><code id="kappam_gold_+3A_robust">robust</code></td>
<td>
<p>flag. Use robust estimate for random chance of agreement by
Brennan-Prediger?</p>
</td></tr>
<tr><td><code id="kappam_gold_+3A_ratingscale">ratingScale</code></td>
<td>
<p>Possible levels for the rating. Or <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="kappam_gold_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level for confidence interval</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list. agreement measures (raw and bias-corrected) kappa with
confidence interval. Entry <code>raters</code> refers to the number of tested raters,
not counting the reference rater
</p>


<h3>Examples</h3>

<pre><code class='language-R'># matrix with subjects in rows and raters in columns.
# 1st column is taken as gold-standard
m &lt;- matrix(c("O", "G", "O",
              "G", "G", "R",
              "R", "R", "R",
              "G", "G", "O"), ncol = 3, byrow = TRUE)
kappam_gold(m)

</code></pre>

<hr>
<h2 id='kappam_vanbelle'>Agreement between two groups of raters</h2><span id='topic+kappam_vanbelle'></span>

<h3>Description</h3>

<p>This function expands upon Cohen's and Fleiss' Kappa as measures for
interrater agreement while taking into account the heterogeneity within each
group.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kappam_vanbelle(
  ratings,
  refIdx,
  ratingScale = NULL,
  weights = c("unweighted", "linear", "quadratic"),
  conf.level = 0.95
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kappam_vanbelle_+3A_ratings">ratings</code></td>
<td>
<p>matrix of subjects x raters for both groups of raters</p>
</td></tr>
<tr><td><code id="kappam_vanbelle_+3A_refidx">refIdx</code></td>
<td>
<p>numeric. indices of raters that constitute the reference group.
Can also be all negative to define rater group by exclusion.</p>
</td></tr>
<tr><td><code id="kappam_vanbelle_+3A_ratingscale">ratingScale</code></td>
<td>
<p>character vector of the levels for the rating. Or <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="kappam_vanbelle_+3A_weights">weights</code></td>
<td>
<p>optional weighting schemes: <code>"unweighted"</code>,
<code>"linear"</code>,<code>"quadratic"</code></p>
</td></tr>
<tr><td><code id="kappam_vanbelle_+3A_conf.level">conf.level</code></td>
<td>
<p>confidence level for interval estimation</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Data need to be stored with raters in columns.
</p>


<h3>Value</h3>

<p>list. kappa agreement between two groups of raters
</p>


<h3>References</h3>

<p>Vanbelle, S., Albert, A. Agreement between Two Independent Groups
of Raters. Psychometrika 74, 477–491 (2009).
<a href="https://doi.org/10.1007/s11336-009-9116-1">doi:10.1007/s11336-009-9116-1</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># compare student ratings with ratings of 11 experts
kappam_vanbelle(SC_test, refIdx = 40:50)

</code></pre>

<hr>
<h2 id='SC_test'>Script concordance test (SCT).</h2><span id='topic+SC_test'></span>

<h3>Description</h3>

<p>In medical education, the script concordance test (SCT) (Charlin, Gagnon,
Sibert, &amp; Van der Vleuten, 2002) is used to score physicians or medical
students in their ability to solve clinical situations as compared to answers
given by experts. The test consists of a number of items to be evaluated on a
5-point Likert scale.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SC_test
</code></pre>


<h3>Format</h3>

<p>A matrix with 34 rows and 50 columns. Columns 1 to 39 are student
raters, columns 40 to 50 are experts. Each rater applies to each clinical
situation one of five levels ranging from -2 to 2 with the following
meaning:
</p>

<dl>
<dt>-2</dt><dd><p>The assumption is practically eliminated;</p>
</dd>
<dt>-1</dt><dd><p>The assumption becomes less likely;</p>
</dd>
<dt>0</dt><dd><p>The information has no effect on the assumption;</p>
</dd>
<dt>+1</dt><dd><p>The assumption becomes more likely;</p>
</dd>
<dt>+2</dt><dd><p>The assumption is virtually the only possible one.</p>
</dd>
</dl>



<h3>Details</h3>

<p>Each item represents a clinical situation (called an 'assumption') likely to
be encountered in the physician’s practice. The situation has to be unclear,
even for an expert. The task of the subjects being evaluated is to consider
the effect of new information on the assumption to solve the situation. The
data incorporates 50 raters, 39 students and 11 experts.
</p>
<p>Each rater judges the same 34 assumptions.
</p>


<h3>Source</h3>

<p>Sophie Vanbelle (personal communication, 2021)
</p>


<h3>References</h3>

<p>Vanbelle, S., Albert, A. Agreement between Two Independent Groups
of Raters. Psychometrika 74, 477–491 (2009).
<a href="https://doi.org/10.1007/s11336-009-9116-1">doi:10.1007/s11336-009-9116-1</a>
</p>

<hr>
<h2 id='simulKappa'>Simulate rating data and calculate agreement with gold standard</h2><span id='topic+simulKappa'></span>

<h3>Description</h3>

<p>The function generates simulation data according to given categories and probabilities.
and can repeatedly apply function <code><a href="#topic+kappam_gold">kappam_gold()</a></code>.
Currently, there is no variation in probabilities from rater to rater,
only sampling variability from multinomial distribution is at work.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulKappa(nRater, cats, nSubj, probs, mcSim = 10, simOnly = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="simulKappa_+3A_nrater">nRater</code></td>
<td>
<p>numeric. number of raters.</p>
</td></tr>
<tr><td><code id="simulKappa_+3A_cats">cats</code></td>
<td>
<p>categories specified either as character vector or just the
numbers of categories.</p>
</td></tr>
<tr><td><code id="simulKappa_+3A_nsubj">nSubj</code></td>
<td>
<p>numeric. number of subjects per gold standard category. Either a
single number or as vector of numbers per category, e.g. for non-balanced
situation.</p>
</td></tr>
<tr><td><code id="simulKappa_+3A_probs">probs</code></td>
<td>
<p>numeric square matrix (nCat x nCat) with classification
probabilities. Row <code>i</code> has probabilities of rater categorization for
subjects of category <code>i</code> (gold standard).</p>
</td></tr>
<tr><td><code id="simulKappa_+3A_mcsim">mcSim</code></td>
<td>
<p>numeric. Number of Monte-Carlo simulations.</p>
</td></tr>
<tr><td><code id="simulKappa_+3A_simonly">simOnly</code></td>
<td>
<p>logical. Need only simulation data? Default is <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is future-aware for the repeated evaluation of <code><a href="#topic+kappam_gold">kappam_gold()</a></code>
that is triggered by this function.
</p>


<h3>Value</h3>

<p>dataframe of kappa-gold on the simulated datasets or (when
<code>simOnly=TRUE</code>) list of length <code>mcSim</code> with each element a simulated data
set with goldrating in first column and then the raters.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># repeatedly estimate agreement with goldstandard for simulated data
simulKappa(nRater = 8, cats = 3, nSubj = 11,
           # assumed prob for classification by raters
           probs = matrix(c(.6, .2, .1, # subjects of cat 1
                            .3, .4, .3, # subjects of cat 2
                            .1, .4, .5  # subjects of cat 3
           ), nrow = 3, byrow = TRUE))


</code></pre>

<hr>
<h2 id='stagingData'>Staging of colorectal carcinoma</h2><span id='topic+stagingData'></span>

<h3>Description</h3>

<p>Staging of carcinoma is done by different medical professions. Gold standard
is the (histo-)pathological rating of a tissue sample but this information
typically only becomes available late, after surgery. However prior to
surgery the carcinoma is also staged by radiologists in the clinical setting
on the basis of MRI scans.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stagingData
</code></pre>


<h3>Format</h3>

<p>A data frame with 21 observations and 6 variables:
</p>

<dl>
<dt>patho</dt><dd><p>the (histo-)pathological staging (gold standard) with categories I, II or III</p>
</dd>
<dt>rad1</dt><dd><p>the clinical staging with categories I, II or III by radiologist 1</p>
</dd>
<dt>rad2</dt><dd><p>the clinical staging with categories I, II or III by radiologist 2</p>
</dd>
<dt>rad3</dt><dd><p>the clinical staging with categories I, II or III by radiologist 3</p>
</dd>
<dt>rad4</dt><dd><p>the clinical staging with categories I, II or III by radiologist 4</p>
</dd>
<dt>rad5</dt><dd><p>the clinical staging with categories I, II or III by radiologist 5</p>
</dd>
</dl>



<h3>Details</h3>

<p>These fictitious data were inspired by the OCUM trial. The simulation uses
the following two assumptions: over-staging occurs more frequently than
under-staging and an error by two categories is less likely than an error by
only one category.
</p>
<p>Stages conform to the UICC classification according to the TNM
classification. Note that cases in stage IV do not appear in this data set
and that the following description of stages is simplified.
</p>

<ol>
<li> <p><strong>I</strong> Until T2, N0, M0
</p>
</li>
<li> <p><strong>II</strong> From T3, N0, M0
</p>
</li>
<li> <p><strong>III</strong> Any T, N1/N2, M0
</p>
</li></ol>



<h3>Source</h3>

<p>simulated data
</p>


<h3>References</h3>

<p>Kreis, M. E. et al., MRI-Based Use of Neoadjuvant
Chemoradiotherapy in Rectal Carcinoma: Surgical Quality and
Histopathological Outcome of the OCUM Trial
<a href="https://doi.org/10.1245/s10434-019-07696-y">doi:10.1245/s10434-019-07696-y</a>
</p>

<hr>
<h2 id='victorinox'>delete-1 jackknife estimator</h2><span id='topic+victorinox'></span>

<h3>Description</h3>

<p>Quick simple jackknife routine to estimate bias and standard error of an
estimator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>victorinox(est, idx)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="victorinox_+3A_est">est</code></td>
<td>
<p>estimator function</p>
</td></tr>
<tr><td><code id="victorinox_+3A_idx">idx</code></td>
<td>
<p>maximal index vector for data of estimator</p>
</td></tr>
</table>


<h3>Value</h3>

<p>list with jackknife information, bias and SE
</p>


<h3>References</h3>

<p>https://de.wikipedia.org/wiki/Jackknife-Methode
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
