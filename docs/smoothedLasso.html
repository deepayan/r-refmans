<!DOCTYPE html><html><head><title>Help for package smoothedLasso</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {smoothedLasso}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#crossvalidation'><p>Perform cross validation to select the regularization parameter.</p></a></li>
<li><a href='#elasticNet'><p>Auxiliary function which returns the objective, penalty, and dependence structure among regression coefficients of the elastic net.</p></a></li>
<li><a href='#fusedLasso'><p>Auxiliary function which returns the objective, penalty, and dependence structure among regression coefficients of the fused Lasso.</p></a></li>
<li><a href='#graphicalLasso'><p>Auxiliary function which returns the objective, penalty, and dependence structure among regression coefficients of the graphical Lasso.</p></a></li>
<li><a href='#minimizeFunction'><p>Minimize the objective function of an unsmoothed or smoothed regression operator with respect to <code class="reqn">betavector</code> using BFGS.</p></a></li>
<li><a href='#minimizeSmoothedSequence'><p>Minimize the objective function of a smoothed regression operator with respect to <code class="reqn">betavector</code> using the progressive smoothing algorithm.</p></a></li>
<li><a href='#objFunction'><p>Auxiliary function to define the objective function of an L1 penalized regression operator.</p></a></li>
<li><a href='#objFunctionGradient'><p>Auxiliary function which computes the (non-smooth) gradient of an L1 penalized regression operator.</p></a></li>
<li><a href='#objFunctionSmooth'><p>Auxiliary function to define the objective function of the smoothed L1 penalized regression operator.</p></a></li>
<li><a href='#objFunctionSmoothGradient'><p>Auxiliary function which computes the gradient of the smoothed L1 penalized regression operator.</p></a></li>
<li><a href='#prsLasso'><p>Auxiliary function which returns the objective, penalty, and dependence structure among regression coefficients of the Lasso for polygenic risk scores (prs).</p></a></li>
<li><a href='#standardLasso'><p>Auxiliary function which returns the objective, penalty, and dependence structure among regression coefficients of the Lasso.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>A Framework to Smooth L1 Penalized Regression Operators using
Nesterov Smoothing</td>
</tr>
<tr>
<td>Version:</td>
<td>1.6</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-03-18</td>
</tr>
<tr>
<td>Author:</td>
<td>Georg Hahn [aut,cre], Sharon M. Lutz [ctb], Nilanjana Laha [ctb], Christoph Lange [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Georg Hahn &lt;ghahn@hsph.harvard.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>We provide full functionality to smooth L1 penalized regression operators and to compute regression estimates thereof. For this, the objective function of a user-specified regression operator is first smoothed using Nesterov smoothing (see Y. Nesterov (2005) &lt;<a href="https://doi.org/10.1007%2Fs10107-004-0552-5">doi:10.1007/s10107-004-0552-5</a>&gt;), resulting in a modified objective function with explicit gradients everywhere. The smoothed objective function and its gradient are minimized via BFGS, and the obtained minimizer is returned. Using Nesterov smoothing, the smoothed objective function can be made arbitrarily close to the original (unsmoothed) one. In particular, the Nesterov approach has the advantage that it comes with explicit accuracy bounds, both on the L1/L2 difference of the unsmoothed to the smoothed objective functions as well as on their respective minimizers (see G. Hahn, S.M. Lutz, N. Laha, C. Lange (2020) &lt;<a href="https://doi.org/10.1101%2F2020.09.17.301788">doi:10.1101/2020.09.17.301788</a>&gt;). A progressive smoothing approach is provided which iteratively smoothes the objective function, resulting in more stable regression estimates. A function to perform cross validation for selection of the regularization parameter is provided.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rdpack, Matrix</td>
</tr>
<tr>
<td>RdMacros:</td>
<td>Rdpack</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-03-18 15:34:10 UTC; acer</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-03-21 07:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='crossvalidation'>Perform cross validation to select the regularization parameter.</h2><span id='topic+crossvalidation'></span>

<h3>Description</h3>

<p>Perform cross validation to select the regularization parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crossvalidation(auxfun, X, y, param, K = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crossvalidation_+3A_auxfun">auxfun</code></td>
<td>
<p>A complete fitting function which takes as arguments a data matrix <code class="reqn">X</code>, a response vector <code class="reqn">y</code>, and some parameter <code class="reqn">p</code> (to be tuned), and returns the estimator (<code class="reqn">betavector</code>) (minimizer) of the regression operator under investigation.</p>
</td></tr>
<tr><td><code id="crossvalidation_+3A_x">X</code></td>
<td>
<p>The design matrix.</p>
</td></tr>
<tr><td><code id="crossvalidation_+3A_y">y</code></td>
<td>
<p>The response vector.</p>
</td></tr>
<tr><td><code id="crossvalidation_+3A_param">param</code></td>
<td>
<p>A vector of regularization parameters which are to be evaluated via cross validation.</p>
</td></tr>
<tr><td><code id="crossvalidation_+3A_k">K</code></td>
<td>
<p>The number of folds for cross validation (should divide the number of rows of <code class="reqn">X</code>). The default is <code class="reqn">10</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of average errors over all folds. The entries in the returned vector correspond to the entries in the vector <code class="reqn">param</code> in the same order.
</p>


<h3>References</h3>

<p>Hahn, G., Lutz, S., Laha, N., and Lange, C. (2020). A framework to efficiently smooth L1 penalties for linear regression. bioRxiv:2020.09.17.301788.
</p>
<p>Tibshirani, R. (2013). Model selection and validation 1: Cross-validation. https://www.stat.cmu.edu/~ryantibs/datamining/lectures/18-val1.pdf
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(smoothedLasso)
n &lt;- 1000
p &lt;- 100
betavector &lt;- runif(p)
X &lt;- matrix(runif(n*p),nrow=n,ncol=p)
y &lt;- X %*% betavector
auxfun &lt;- function(X,y,lambda) {
		temp &lt;- standardLasso(X,y,lambda)
	obj &lt;- function(z) objFunction(z,temp$u,temp$v,temp$w)
		objgrad &lt;- function(z) objFunctionGradient(z,temp$w,temp$du,temp$dv,temp$dw)
	return(minimizeFunction(p,obj,objgrad))
}
lambdaVector &lt;- seq(0,1,by=0.1)
print(crossvalidation(auxfun,X,y,lambdaVector,10))

</code></pre>

<hr>
<h2 id='elasticNet'>Auxiliary function which returns the objective, penalty, and dependence structure among regression coefficients of the elastic net.</h2><span id='topic+elasticNet'></span>

<h3>Description</h3>

<p>Auxiliary function which returns the objective, penalty, and dependence structure among regression coefficients of the elastic net.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>elasticNet(X, y, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="elasticNet_+3A_x">X</code></td>
<td>
<p>The design matrix.</p>
</td></tr>
<tr><td><code id="elasticNet_+3A_y">y</code></td>
<td>
<p>The response vector.</p>
</td></tr>
<tr><td><code id="elasticNet_+3A_alpha">alpha</code></td>
<td>
<p>The regularization parameter of the elastic net.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with six functions, precisely the objective <code class="reqn">u</code>, penalty <code class="reqn">v</code>, and dependence structure <code class="reqn">w</code>, as well as their derivatives <code class="reqn">du</code>, <code class="reqn">dv</code>, and <code class="reqn">dw</code>.
</p>


<h3>References</h3>

<p>Zou, H. and Hastie, T. (2005). Regularization and variable selection via the elastic net. J Roy Stat Soc B Met, 67(2):301-320.
</p>
<p>Friedman, J., Hastie, T., Tibshirani, R., Narasimhan, B., Tay, K., Simon, N., and Qian, J. (2020). glmnet: Lasso and Elastic-Net Regularized Generalized Linear Models. R-package version 4.0.
</p>
<p>Hahn, G., Lutz, S., Laha, N., and Lange, C. (2020). A framework to efficiently smooth L1 penalties for linear regression. bioRxiv:2020.09.17.301788.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(smoothedLasso)
n &lt;- 100
p &lt;- 500
betavector &lt;- runif(p)
X &lt;- matrix(runif(n*p),nrow=n,ncol=p)
y &lt;- X %*% betavector
alpha &lt;- 0.5
temp &lt;- elasticNet(X,y,alpha)

</code></pre>

<hr>
<h2 id='fusedLasso'>Auxiliary function which returns the objective, penalty, and dependence structure among regression coefficients of the fused Lasso.</h2><span id='topic+fusedLasso'></span>

<h3>Description</h3>

<p>Auxiliary function which returns the objective, penalty, and dependence structure among regression coefficients of the fused Lasso.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fusedLasso(X, y, E, lambda, gamma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fusedLasso_+3A_x">X</code></td>
<td>
<p>The design matrix.</p>
</td></tr>
<tr><td><code id="fusedLasso_+3A_y">y</code></td>
<td>
<p>The response vector.</p>
</td></tr>
<tr><td><code id="fusedLasso_+3A_e">E</code></td>
<td>
<p>The adjacency matrix which encodes with a one in position <code class="reqn">(i,j)</code> the presence of an edge between variables <code class="reqn">i</code> and <code class="reqn">j</code>. Note that only the upper triangle of <code class="reqn">E</code> is read.</p>
</td></tr>
<tr><td><code id="fusedLasso_+3A_lambda">lambda</code></td>
<td>
<p>The first regularization parameter of the fused Lasso.</p>
</td></tr>
<tr><td><code id="fusedLasso_+3A_gamma">gamma</code></td>
<td>
<p>The second regularization parameter of the fused Lasso.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with six functions, precisely the objective <code class="reqn">u</code>, penalty <code class="reqn">v</code>, and dependence structure <code class="reqn">w</code>, as well as their derivatives <code class="reqn">du</code>, <code class="reqn">dv</code>, and <code class="reqn">dw</code>.
</p>


<h3>References</h3>

<p>Tibshirani, R., Saunders, M., Rosset, S., Zhu, J., and Knight, K. (2005). Sparsity and Smoothness via the Fused Lasso. J Roy Stat Soc B Met, 67(1):91-108.
</p>
<p>Arnold, T.B. and Tibshirani, R.J. (2020). genlasso: Path Algorithm for Generalized Lasso Problems. R package version 1.5.
</p>
<p>Hahn, G., Lutz, S., Laha, N., and Lange, C. (2020). A framework to efficiently smooth L1 penalties for linear regression. bioRxiv:2020.09.17.301788.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(smoothedLasso)
n &lt;- 100
p &lt;- 500
betavector &lt;- runif(p)
X &lt;- matrix(runif(n*p),nrow=n,ncol=p)
y &lt;- X %*% betavector
E &lt;- matrix(sample(c(TRUE,FALSE),p*p,replace=TRUE),p)
lambda &lt;- 1
gamma &lt;- 0.5
temp &lt;- fusedLasso(X,y,E,lambda,gamma)

</code></pre>

<hr>
<h2 id='graphicalLasso'>Auxiliary function which returns the objective, penalty, and dependence structure among regression coefficients of the graphical Lasso.</h2><span id='topic+graphicalLasso'></span>

<h3>Description</h3>

<p>Auxiliary function which returns the objective, penalty, and dependence structure among regression coefficients of the graphical Lasso.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>graphicalLasso(S, lambda)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="graphicalLasso_+3A_s">S</code></td>
<td>
<p>The sample covariance matrix.</p>
</td></tr>
<tr><td><code id="graphicalLasso_+3A_lambda">lambda</code></td>
<td>
<p>The regularization parameter of the graphical Lasso.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with three functions, precisely the objective <code class="reqn">u</code>, penalty <code class="reqn">v</code>, and dependence structure <code class="reqn">w</code>. Not all derivatives are available in closed form, and thus computing the numerical derivative of the entire objective function is recommended.
</p>


<h3>References</h3>

<p>Friedman, J., Hastie, T., and Tibshirani, R. (2008). Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9(3):432-441.
</p>
<p>Friedman, J., Hastie, T., and Tibshirani, R. (2019). glasso: Graphical Lasso: Estimation of Gaussian Graphical Models. R package version 1.11.
</p>
<p>Hahn, G., Lutz, S., Laha, N., and Lange, C. (2020). A framework to efficiently smooth L1 penalties for linear regression. bioRxiv:2020.09.17.301788.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(smoothedLasso)
p &lt;- 30
S &lt;- matrix(rWishart(1,p,diag(p)),p)
lambda &lt;- 1
temp &lt;- graphicalLasso(S,lambda)

</code></pre>

<hr>
<h2 id='minimizeFunction'>Minimize the objective function of an unsmoothed or smoothed regression operator with respect to <code class="reqn">betavector</code> using BFGS.</h2><span id='topic+minimizeFunction'></span>

<h3>Description</h3>

<p>Minimize the objective function of an unsmoothed or smoothed regression operator with respect to <code class="reqn">betavector</code> using BFGS.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>minimizeFunction(p, obj, objgrad)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="minimizeFunction_+3A_p">p</code></td>
<td>
<p>The dimension of the unknown parameters (regression coefficients).</p>
</td></tr>
<tr><td><code id="minimizeFunction_+3A_obj">obj</code></td>
<td>
<p>The objective function of the regression operator as a function of <code class="reqn">betavector</code>.</p>
</td></tr>
<tr><td><code id="minimizeFunction_+3A_objgrad">objgrad</code></td>
<td>
<p>The gradient function of the regression operator as a function of <code class="reqn">betavector</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The estimator <code class="reqn">betavector</code> (minimizer) of the regression operator.
</p>


<h3>References</h3>

<p>Hahn, G., Lutz, S., Laha, N., and Lange, C. (2020). A framework to efficiently smooth L1 penalties for linear regression. bioRxiv:2020.09.17.301788.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(smoothedLasso)
n &lt;- 100
p &lt;- 500
betavector &lt;- runif(p)
X &lt;- matrix(runif(n*p),nrow=n,ncol=p)
y &lt;- X %*% betavector
lambda &lt;- 1
temp &lt;- standardLasso(X,y,lambda)
obj &lt;- function(z) objFunctionSmooth(z,temp$u,temp$v,temp$w,mu=0.1)
objgrad &lt;- function(z) objFunctionSmoothGradient(z,temp$w,temp$du,temp$dv,temp$dw,mu=0.1)
print(minimizeFunction(p,obj,objgrad))

</code></pre>

<hr>
<h2 id='minimizeSmoothedSequence'>Minimize the objective function of a smoothed regression operator with respect to <code class="reqn">betavector</code> using the progressive smoothing algorithm.</h2><span id='topic+minimizeSmoothedSequence'></span>

<h3>Description</h3>

<p>Minimize the objective function of a smoothed regression operator with respect to <code class="reqn">betavector</code> using the progressive smoothing algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>minimizeSmoothedSequence(p, obj, objgrad, muSeq = 2^seq(3, -6))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="minimizeSmoothedSequence_+3A_p">p</code></td>
<td>
<p>The dimension of the unknown parameters (regression coefficients).</p>
</td></tr>
<tr><td><code id="minimizeSmoothedSequence_+3A_obj">obj</code></td>
<td>
<p>The objective function of the regression operator. Note that in the case of the progressive smoothing algorithm, the objective function must be a function of both <code class="reqn">betavector</code> and <code class="reqn">mu</code>.</p>
</td></tr>
<tr><td><code id="minimizeSmoothedSequence_+3A_objgrad">objgrad</code></td>
<td>
<p>The gradient function of the regression operator. Note that in the case of the progressive smoothing algorithm, the gradient must be a function of both <code class="reqn">betavector</code> and <code class="reqn">mu</code>.</p>
</td></tr>
<tr><td><code id="minimizeSmoothedSequence_+3A_museq">muSeq</code></td>
<td>
<p>The sequence of Nesterov smoothing parameters. The default is <code class="reqn">2^{-n}</code> for <code class="reqn">n \in \{-3,\ldots,6\}</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The estimator <code class="reqn">betavector</code> (minimizer) of the regression operator.
</p>


<h3>References</h3>

<p>Hahn, G., Lutz, S., Laha, N., and Lange, C. (2020). A framework to efficiently smooth L1 penalties for linear regression. bioRxiv:2020.09.17.301788.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(smoothedLasso)
n &lt;- 100
p &lt;- 500
betavector &lt;- runif(p)
X &lt;- matrix(runif(n*p),nrow=n,ncol=p)
y &lt;- X %*% betavector
lambda &lt;- 1
temp &lt;- standardLasso(X,y,lambda)
obj &lt;- function(z,m) objFunctionSmooth(z,temp$u,temp$v,temp$w,mu=m)
objgrad &lt;- function(z,m) objFunctionSmoothGradient(z,temp$w,temp$du,temp$dv,temp$dw,mu=m)
print(minimizeSmoothedSequence(p,obj,objgrad))

</code></pre>

<hr>
<h2 id='objFunction'>Auxiliary function to define the objective function of an L1 penalized regression operator.</h2><span id='topic+objFunction'></span>

<h3>Description</h3>

<p>Auxiliary function to define the objective function of an L1 penalized regression operator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>objFunction(betavector, u, v, w)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="objFunction_+3A_betavector">betavector</code></td>
<td>
<p>The vector of regression coefficients.</p>
</td></tr>
<tr><td><code id="objFunction_+3A_u">u</code></td>
<td>
<p>The function encoding the objective of the regression operator.</p>
</td></tr>
<tr><td><code id="objFunction_+3A_v">v</code></td>
<td>
<p>The function encoding the penalty of the regression operator.</p>
</td></tr>
<tr><td><code id="objFunction_+3A_w">w</code></td>
<td>
<p>The function encoding the dependence structure among the regression coefficients.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The value of the L1 penalized regression operator for the input <code class="reqn">betavector</code>.
</p>


<h3>References</h3>

<p>Hahn, G., Lutz, S., Laha, N., and Lange, C. (2020). A framework to efficiently smooth L1 penalties for linear regression. bioRxiv:2020.09.17.301788.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(smoothedLasso)
n &lt;- 100
p &lt;- 500
betavector &lt;- runif(p)
X &lt;- matrix(runif(n*p),nrow=n,ncol=p)
y &lt;- X %*% betavector
lambda &lt;- 1
temp &lt;- standardLasso(X,y,lambda)
print(objFunction(betavector,temp$u,temp$v,temp$w))

</code></pre>

<hr>
<h2 id='objFunctionGradient'>Auxiliary function which computes the (non-smooth) gradient of an L1 penalized regression operator.</h2><span id='topic+objFunctionGradient'></span>

<h3>Description</h3>

<p>Auxiliary function which computes the (non-smooth) gradient of an L1 penalized regression operator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>objFunctionGradient(betavector, w, du, dv, dw)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="objFunctionGradient_+3A_betavector">betavector</code></td>
<td>
<p>The vector of regression coefficients.</p>
</td></tr>
<tr><td><code id="objFunctionGradient_+3A_w">w</code></td>
<td>
<p>The function encoding the dependence structure among the regression coefficients.</p>
</td></tr>
<tr><td><code id="objFunctionGradient_+3A_du">du</code></td>
<td>
<p>The derivative (gradient) of the objective of the regression operator.</p>
</td></tr>
<tr><td><code id="objFunctionGradient_+3A_dv">dv</code></td>
<td>
<p>The derivative (gradient) of the penalty of the regression operator.</p>
</td></tr>
<tr><td><code id="objFunctionGradient_+3A_dw">dw</code></td>
<td>
<p>The derivative (Jacobian matrix) of the function encoding the dependence structure among the regression coefficients.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The value of the gradient for the input <code class="reqn">betavector</code>.
</p>


<h3>References</h3>

<p>Hahn, G., Lutz, S., Laha, N., and Lange, C. (2020). A framework to efficiently smooth L1 penalties for linear regression. bioRxiv:2020.09.17.301788.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(smoothedLasso)
n &lt;- 100
p &lt;- 500
betavector &lt;- runif(p)
X &lt;- matrix(runif(n*p),nrow=n,ncol=p)
y &lt;- X %*% betavector
lambda &lt;- 1
temp &lt;- standardLasso(X,y,lambda)
print(objFunctionGradient(betavector,temp$w,temp$du,temp$dv,temp$dw))

</code></pre>

<hr>
<h2 id='objFunctionSmooth'>Auxiliary function to define the objective function of the smoothed L1 penalized regression operator.</h2><span id='topic+objFunctionSmooth'></span>

<h3>Description</h3>

<p>Auxiliary function to define the objective function of the smoothed L1 penalized regression operator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>objFunctionSmooth(betavector, u, v, w, mu, entropy = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="objFunctionSmooth_+3A_betavector">betavector</code></td>
<td>
<p>The vector of regression coefficients.</p>
</td></tr>
<tr><td><code id="objFunctionSmooth_+3A_u">u</code></td>
<td>
<p>The function encoding the objective of the regression operator.</p>
</td></tr>
<tr><td><code id="objFunctionSmooth_+3A_v">v</code></td>
<td>
<p>The function encoding the penalty of the regression operator.</p>
</td></tr>
<tr><td><code id="objFunctionSmooth_+3A_w">w</code></td>
<td>
<p>The function encoding the dependence structure among the regression coefficients.</p>
</td></tr>
<tr><td><code id="objFunctionSmooth_+3A_mu">mu</code></td>
<td>
<p>The Nesterov smoothing parameter.</p>
</td></tr>
<tr><td><code id="objFunctionSmooth_+3A_entropy">entropy</code></td>
<td>
<p>A boolean switch to select the entropy prox function (default) or the squared error prox function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The value of the smoothed regression operator for the input <code class="reqn">betavector</code>.
</p>


<h3>References</h3>

<p>Hahn, G., Lutz, S., Laha, N., and Lange, C. (2020). A framework to efficiently smooth L1 penalties for linear regression. bioRxiv:2020.09.17.301788.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(smoothedLasso)
n &lt;- 100
p &lt;- 500
betavector &lt;- runif(p)
X &lt;- matrix(runif(n*p),nrow=n,ncol=p)
y &lt;- X %*% betavector
lambda &lt;- 1
temp &lt;- standardLasso(X,y,lambda)
print(objFunctionSmooth(betavector,temp$u,temp$v,temp$w,mu=0.1))

</code></pre>

<hr>
<h2 id='objFunctionSmoothGradient'>Auxiliary function which computes the gradient of the smoothed L1 penalized regression operator.</h2><span id='topic+objFunctionSmoothGradient'></span>

<h3>Description</h3>

<p>Auxiliary function which computes the gradient of the smoothed L1 penalized regression operator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>objFunctionSmoothGradient(betavector, w, du, dv, dw, mu, entropy = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="objFunctionSmoothGradient_+3A_betavector">betavector</code></td>
<td>
<p>The vector of regression coefficients.</p>
</td></tr>
<tr><td><code id="objFunctionSmoothGradient_+3A_w">w</code></td>
<td>
<p>The function encoding the dependence structure among the regression coefficients.</p>
</td></tr>
<tr><td><code id="objFunctionSmoothGradient_+3A_du">du</code></td>
<td>
<p>The derivative (gradient) of the objective of the regression operator.</p>
</td></tr>
<tr><td><code id="objFunctionSmoothGradient_+3A_dv">dv</code></td>
<td>
<p>The derivative (gradient) of the penalty of the regression operator.</p>
</td></tr>
<tr><td><code id="objFunctionSmoothGradient_+3A_dw">dw</code></td>
<td>
<p>The derivative (Jacobian matrix) of the function encoding the dependence structure among the regression coefficients.</p>
</td></tr>
<tr><td><code id="objFunctionSmoothGradient_+3A_mu">mu</code></td>
<td>
<p>The Nesterov smoothing parameter.</p>
</td></tr>
<tr><td><code id="objFunctionSmoothGradient_+3A_entropy">entropy</code></td>
<td>
<p>A boolean switch to select the entropy prox function (default) or the squared error prox function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The value of the gradient for the input <code class="reqn">betavector</code>.
</p>


<h3>References</h3>

<p>Hahn, G., Lutz, S., Laha, N., and Lange, C. (2020). A framework to efficiently smooth L1 penalties for linear regression. bioRxiv:2020.09.17.301788.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(smoothedLasso)
n &lt;- 100
p &lt;- 500
betavector &lt;- runif(p)
X &lt;- matrix(runif(n*p),nrow=n,ncol=p)
y &lt;- X %*% betavector
lambda &lt;- 1
temp &lt;- standardLasso(X,y,lambda)
print(objFunctionSmoothGradient(betavector,temp$w,temp$du,temp$dv,temp$dw,mu=0.1))

</code></pre>

<hr>
<h2 id='prsLasso'>Auxiliary function which returns the objective, penalty, and dependence structure among regression coefficients of the Lasso for polygenic risk scores (prs).</h2><span id='topic+prsLasso'></span>

<h3>Description</h3>

<p>Auxiliary function which returns the objective, penalty, and dependence structure among regression coefficients of the Lasso for polygenic risk scores (prs).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prsLasso(X, y, s, lambda)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prsLasso_+3A_x">X</code></td>
<td>
<p>The design matrix.</p>
</td></tr>
<tr><td><code id="prsLasso_+3A_y">y</code></td>
<td>
<p>The response vector.</p>
</td></tr>
<tr><td><code id="prsLasso_+3A_s">s</code></td>
<td>
<p>The shrinkage parameter used to regularize the design matrix.</p>
</td></tr>
<tr><td><code id="prsLasso_+3A_lambda">lambda</code></td>
<td>
<p>The regularization parameter of the prs Lasso.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with six functions, precisely the objective <code class="reqn">u</code>, penalty <code class="reqn">v</code>, and dependence structure <code class="reqn">w</code>, as well as their derivatives <code class="reqn">du</code>, <code class="reqn">dv</code>, and <code class="reqn">dw</code>.
</p>


<h3>References</h3>

<p>Mak, T.S., Porsch, R.M., Choi, S.W., Zhou, X., and Sham, P.C. (2017). Polygenic scores via penalized regression on summary statistics. Genet Epidemiol, 41(6):469-480.
</p>
<p>Mak, T.S. and Porsch, R.M. (2020). lassosum: LASSO with summary statistics and a reference panel. R package version 0.4.5.
</p>
<p>Hahn, G., Lutz, S., Laha, N., and Lange, C. (2020). A framework to efficiently smooth L1 penalties for linear regression. bioRxiv:2020.09.17.301788.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(smoothedLasso)
n &lt;- 100
p &lt;- 500
betavector &lt;- runif(p)
X &lt;- matrix(runif(n*p),nrow=n,ncol=p)
y &lt;- X %*% betavector
s &lt;- 0.5
lambda &lt;- 1
temp &lt;- prsLasso(X,y,s,lambda)

</code></pre>

<hr>
<h2 id='standardLasso'>Auxiliary function which returns the objective, penalty, and dependence structure among regression coefficients of the Lasso.</h2><span id='topic+standardLasso'></span>

<h3>Description</h3>

<p>Auxiliary function which returns the objective, penalty, and dependence structure among regression coefficients of the Lasso.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>standardLasso(X, y, lambda)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="standardLasso_+3A_x">X</code></td>
<td>
<p>The design matrix.</p>
</td></tr>
<tr><td><code id="standardLasso_+3A_y">y</code></td>
<td>
<p>The response vector.</p>
</td></tr>
<tr><td><code id="standardLasso_+3A_lambda">lambda</code></td>
<td>
<p>The Lasso regularization parameter.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with six functions, precisely the objective <code class="reqn">u</code>, penalty <code class="reqn">v</code>, and dependence structure <code class="reqn">w</code>, as well as their derivatives <code class="reqn">du</code>, <code class="reqn">dv</code>, and <code class="reqn">dw</code>.
</p>


<h3>References</h3>

<p>Tibshirani, R. (1996). Regression Shrinkage and Selection Via the Lasso. J Roy Stat Soc B Met, 58(1):267-288.
</p>
<p>Hahn, G., Lutz, S., Laha, N., and Lange, C. (2020). A framework to efficiently smooth L1 penalties for linear regression. bioRxiv:2020.09.17.301788.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(smoothedLasso)
n &lt;- 100
p &lt;- 500
betavector &lt;- runif(p)
X &lt;- matrix(runif(n*p),nrow=n,ncol=p)
y &lt;- X %*% betavector
lambda &lt;- 1
temp &lt;- standardLasso(X,y,lambda)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
