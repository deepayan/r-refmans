<!DOCTYPE html><html lang="en"><head><title>Help for package BTM</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {BTM}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#BTM'><p>Construct a Biterm Topic Model on Short Text</p></a></li>
<li><a href='#logLik.BTM'><p>Get the likelihood of biterms in a BTM model</p></a></li>
<li><a href='#predict.BTM'><p>Predict function for a Biterm Topic Model</p></a></li>
<li><a href='#terms.BTM'><p>Get highest token probabilities for each topic or get biterms used in the model</p></a></li>
<li><a href='#terms.data.frame'><p>Get the set of Biterms from a tokenised data frame</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Biterm Topic Models for Short Text</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.7</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jan Wijffels &lt;jwijffels@bnosac.be&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Biterm Topic Models find topics in collections of short texts. 
    It is a word co-occurrence based topic model that learns topics by modeling word-word co-occurrences patterns which are called biterms.
    This in contrast to traditional topic models like Latent Dirichlet Allocation and Probabilistic Latent Semantic Analysis 
    which are word-document co-occurrence topic models.
    A biterm consists of two words co-occurring in the same short text window.  
    This context window can for example be a twitter message, a short answer on a survey, a sentence of a text or a document identifier. 
    The techniques are explained in detail in the paper 'A Biterm Topic Model For Short Text' by Xiaohui Yan, Jiafeng Guo, Yanyan Lan, Xueqi Cheng (2013) <a href="https://github.com/xiaohuiyan/xiaohuiyan.github.io/blob/master/paper/BTM-WWW13.pdf">https://github.com/xiaohuiyan/xiaohuiyan.github.io/blob/master/paper/BTM-WWW13.pdf</a>.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License 2.0</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/bnosac/BTM">https://github.com/bnosac/BTM</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>udpipe, data.table</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-02-11 14:22:06 UTC; jwijf</td>
</tr>
<tr>
<td>Author:</td>
<td>Jan Wijffels [aut, cre, cph] (R wrapper),
  BNOSAC [cph] (R wrapper),
  Xiaohui Yan [ctb, cph] (BTM C++ library)</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-02-11 14:40:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='BTM'>Construct a Biterm Topic Model on Short Text</h2><span id='topic+BTM'></span>

<h3>Description</h3>

<p>The Biterm Topic Model (BTM) is a word co-occurrence based topic model that learns topics by modeling word-word co-occurrences patterns (e.g., biterms)
</p>

<ul>
<li><p> A biterm consists of two words co-occurring in the same context, for example, in the same short text window. 
</p>
</li>
<li><p> BTM models the biterm occurrences in a corpus (unlike LDA models which model the word occurrences in a document). 
</p>
</li>
<li><p> It's a generative model. In the generation procedure, a biterm is generated by drawing two words independently from a same topic z. 
In other words, the distribution of a biterm <code class="reqn">b=(wi,wj)</code> is defined as: <code class="reqn">P(b) = \sum_k{P(wi|z)*P(wj|z)*P(z)}</code> 
where k is the number of topics you want to extract.
</p>
</li>
<li><p> Estimation of the topic model is done with the Gibbs sampling algorithm. Where estimates are provided for <code class="reqn">P(w|k)=phi</code> and <code class="reqn">P(z)=theta</code>.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>BTM(
  data,
  k = 5,
  alpha = 50/k,
  beta = 0.01,
  iter = 1000,
  window = 15,
  background = FALSE,
  trace = FALSE,
  biterms,
  detailed = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="BTM_+3A_data">data</code></td>
<td>
<p>a tokenised data frame containing one row per token with 2 columns 
</p>

<ul>
<li><p> the first column is a context identifier (e.g. a tweet id, a document id, a sentence id, an identifier of a survey answer, an identifier of a part of a text)
</p>
</li>
<li><p> the second column is a column called of type character containing the sequence of words occurring within the context identifier 
</p>
</li></ul>
</td></tr>
<tr><td><code id="BTM_+3A_k">k</code></td>
<td>
<p>integer with the number of topics to identify</p>
</td></tr>
<tr><td><code id="BTM_+3A_alpha">alpha</code></td>
<td>
<p>numeric, indicating the symmetric dirichlet prior probability of a topic P(z). Defaults to 50/k.</p>
</td></tr>
<tr><td><code id="BTM_+3A_beta">beta</code></td>
<td>
<p>numeric, indicating the symmetric dirichlet prior probability of a word given the topic P(w|z). Defaults to 0.01.</p>
</td></tr>
<tr><td><code id="BTM_+3A_iter">iter</code></td>
<td>
<p>integer with the number of iterations of Gibbs sampling</p>
</td></tr>
<tr><td><code id="BTM_+3A_window">window</code></td>
<td>
<p>integer with the window size for biterm extraction. Defaults to 15.</p>
</td></tr>
<tr><td><code id="BTM_+3A_background">background</code></td>
<td>
<p>logical if set to <code>TRUE</code>, the first topic is set to a background topic that 
equals to the empirical word distribution. This can be used to filter out common words. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="BTM_+3A_trace">trace</code></td>
<td>
<p>logical indicating to print out evolution of the Gibbs sampling iterations. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="BTM_+3A_biterms">biterms</code></td>
<td>
<p>optionally, your own set of biterms to use for modelling.<br /> 
This argument should be a data.frame with column names doc_id, term1, term2 and cooc, indicating how many times each biterm (as indicated by terms term1 and term2) 
is occurring within a certain doc_id. The field cooc indicates how many times this biterm happens with the doc_id. <br />
Note that doc_id's which are not in <code>data</code> are not allowed, as well as terms (in term1 and term2) which are not also in <code>data</code>.
See the examples.<br /> 
If provided, the <code>window</code> argument is ignored and the <code>data</code> argument will only be used to calculate the background word frequency distribution.</p>
</td></tr>
<tr><td><code id="BTM_+3A_detailed">detailed</code></td>
<td>
<p>logical indicating to return detailed output containing as well the vocabulary and the biterms used to construct the model. Defaults to FALSE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an object of class BTM which is a list containing
</p>

<ul>
<li><p>model: a pointer to the C++ BTM model
</p>
</li>
<li><p>K: the number of topics
</p>
</li>
<li><p>W: the number of tokens in the data
</p>
</li>
<li><p>alpha: the symmetric dirichlet prior probability of a topic P(z)
</p>
</li>
<li><p>beta: the symmetric dirichlet prior probability of a word given the topic P(w|z)
</p>
</li>
<li><p>iter: the number of iterations of Gibbs sampling
</p>
</li>
<li><p>background: indicator if the first topic is set to the background topic that equals the empirical word distribution.
</p>
</li>
<li><p>theta: a vector with the topic probability p(z) which is determinated by the overall proportions of biterms in it
</p>
</li>
<li><p>phi: a matrix of dimension W x K with one row for each token in the data. This matrix contains the probability of the token given the topic P(w|z).
the rownames of the matrix indicate the token w
</p>
</li>
<li><p>vocab: a data.frame with columns token and freq indicating the frequency of occurrence of the tokens in <code>data</code>. Only provided in case argument <code>detailed</code> is set to <code>TRUE</code>
</p>
</li>
<li><p>biterms: the result of a call to <code>terms</code> with type set to biterms, containing all the biterms used in the model. Only provided in case argument <code>detailed</code> is set to <code>TRUE</code>
</p>
</li></ul>



<h3>Note</h3>

<p>A biterm is defined as a pair of words co-occurring in the same text window. 
If you have as an example a document with sequence of words <code>'A B C B'</code>, and assuming the window size is set to 3, 
that implies there are two text windows which can generate biterms namely 
text window <code>'A B C'</code> with biterms <code>'A B', 'B C', 'A C'</code> and text window <code>'B C B'</code> with biterms <code>'B C', 'C B', 'B B'</code>
A biterm is an unorder word pair where <code>'B C' = 'C B'</code>. Thus, the document <code>'A B C B'</code> will have the following biterm frequencies: <br />
</p>

<ul>
<li><p> 'A B': 1 
</p>
</li>
<li><p> 'B C': 3
</p>
</li>
<li><p> 'A C': 1
</p>
</li>
<li><p> 'B B': 1
</p>
</li></ul>

<p>These biterms are used to create the model.
</p>


<h3>References</h3>

<p>Xiaohui Yan, Jiafeng Guo, Yanyan Lan, Xueqi Cheng. A Biterm Topic Model For Short Text. WWW2013,
<a href="https://github.com/xiaohuiyan/BTM">https://github.com/xiaohuiyan/BTM</a>, <a href="https://github.com/xiaohuiyan/xiaohuiyan.github.io/blob/master/paper/BTM-WWW13.pdf">https://github.com/xiaohuiyan/xiaohuiyan.github.io/blob/master/paper/BTM-WWW13.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.BTM">predict.BTM</a></code>, <code><a href="#topic+terms.BTM">terms.BTM</a></code>, <code><a href="#topic+logLik.BTM">logLik.BTM</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(udpipe)
data("brussels_reviews_anno", package = "udpipe")
x &lt;- subset(brussels_reviews_anno, language == "nl")
x &lt;- subset(x, xpos %in% c("NN", "NNP", "NNS"))
x &lt;- x[, c("doc_id", "lemma")]
model  &lt;- BTM(x, k = 5, alpha = 1, beta = 0.01, iter = 10, trace = TRUE)
model
terms(model)
scores &lt;- predict(model, newdata = x)

## Another small run with first topic the background word distribution
set.seed(123456)
model &lt;- BTM(x, k = 5, beta = 0.01, iter = 10, background = TRUE)
model
terms(model)

##
## You can also provide your own set of biterms to cluster upon
## Example: cluster nouns and adjectives in the neighbourhood of one another
##
library(data.table)
library(udpipe)
x &lt;- subset(brussels_reviews_anno, language == "nl")
x &lt;- head(x, 5500) # take a sample to speed things up on CRAN
biterms &lt;- as.data.table(x)
biterms &lt;- biterms[, cooccurrence(x = lemma, 
                                  relevant = xpos %in% c("NN", "NNP", "NNS", "JJ"),
                                  skipgram = 2), 
                   by = list(doc_id)]
head(biterms)
set.seed(123456)
x &lt;- subset(x, xpos %in% c("NN", "NNP", "NNS", "JJ"))
x &lt;- x[, c("doc_id", "lemma")]
model &lt;- BTM(x, k = 5, beta = 0.01, iter = 10, background = TRUE, 
             biterms = biterms, trace = 10, detailed = TRUE)
model
terms(model)
bitermset &lt;- terms(model, "biterms")
head(bitermset$biterms, 100)

bitermset$n
sum(biterms$cooc)


## Not run: 
##
## Visualisation either using the textplot or the LDAvis package
##
library(textplot)
library(ggraph)
library(concaveman)
plot(model, top_n = 4)

library(LDAvis)
docsize &lt;- table(x$doc_id)
scores  &lt;- predict(model, x)
scores  &lt;- scores[names(docsize), ]
json &lt;- createJSON(
  phi = t(model$phi), 
  theta = scores, 
  doc.length = as.integer(docsize),
  vocab = model$vocabulary$token, 
  term.frequency = model$vocabulary$freq)
serVis(json)

## End(Not run)
</code></pre>

<hr>
<h2 id='logLik.BTM'>Get the likelihood of biterms in a BTM model</h2><span id='topic+logLik.BTM'></span>

<h3>Description</h3>

<p>Get the likelihood how good biterms are fit by the BTM model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'BTM'
logLik(object, data = terms.BTM(object, type = "biterms")$biterms, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="logLik.BTM_+3A_object">object</code></td>
<td>
<p>an object of class BTM as returned by <code><a href="#topic+BTM">BTM</a></code></p>
</td></tr>
<tr><td><code id="logLik.BTM_+3A_data">data</code></td>
<td>
<p>a data.frame with 2 columns term1 and term2 containing biterms. Defaults to the 
biterms used to construct the model.</p>
</td></tr>
<tr><td><code id="logLik.BTM_+3A_...">...</code></td>
<td>
<p>other arguments not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list with elements
</p>

<ul>
<li><p> likelihood: a vector with the same number of rows as <code>data</code> containing the likelihood
of the biterms alongside the BTM model. Calculated as <code>sum(phi[term1, ] * phi[term2, ] * theta)</code>.
</p>
</li>
<li> <p><code>ll</code> the sum of the log of the biterm likelihoods 
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+BTM">BTM</a></code>, <code><a href="#topic+predict.BTM">predict.BTM</a></code>, <code><a href="#topic+terms.BTM">terms.BTM</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(udpipe)
data("brussels_reviews_anno", package = "udpipe")
x &lt;- subset(brussels_reviews_anno, language == "nl")
x &lt;- subset(x, xpos %in% c("NN", "NNP", "NNS"))
x &lt;- x[, c("doc_id", "lemma")]

model  &lt;- BTM(x, k = 5, iter = 5, trace = TRUE, detailed = TRUE)
fit &lt;- logLik(model)
fit$ll

</code></pre>

<hr>
<h2 id='predict.BTM'>Predict function for a Biterm Topic Model</h2><span id='topic+predict.BTM'></span>

<h3>Description</h3>

<p>Classify new text alongside the biterm topic model.<br />
</p>
<p>To infer the topics in a document, it is assumed that the topic proportions of a document 
is driven by the expectation of the topic proportions of biterms generated from the document.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'BTM'
predict(object, newdata, type = c("sum_b", "sub_w", "mix"), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.BTM_+3A_object">object</code></td>
<td>
<p>an object of class BTM as returned by <code><a href="#topic+BTM">BTM</a></code></p>
</td></tr>
<tr><td><code id="predict.BTM_+3A_newdata">newdata</code></td>
<td>
<p>a tokenised data frame containing one row per token with 2 columns 
</p>

<ul>
<li><p> the first column is a context identifier (e.g. a tweet id, a document id, a sentence id, an identifier of a survey answer, an identifier of a part of a text)
</p>
</li>
<li><p> the second column is a column called of type character containing the sequence of words occurring within the context identifier 
</p>
</li></ul>
</td></tr>
<tr><td><code id="predict.BTM_+3A_type">type</code></td>
<td>
<p>character string with the type of prediction. 
Either one of 'sum_b', 'sub_w' or 'mix'. Default is set to 'sum_b' as indicated in the paper, 
indicating to sum over the the expectation of the topic proportions of biterms generated from the document. For the other approaches, please inspect the paper.</p>
</td></tr>
<tr><td><code id="predict.BTM_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix containing containing P(z|d) - the probability of the topic given the biterms.<br />
The matrix has one row for each unique doc_id (context identifier)
which contains words part of the dictionary of the BTM model and has K columns, 
one for each topic.
</p>


<h3>References</h3>

<p>Xiaohui Yan, Jiafeng Guo, Yanyan Lan, Xueqi Cheng. A Biterm Topic Model For Short Text. WWW2013,
<a href="https://github.com/xiaohuiyan/BTM">https://github.com/xiaohuiyan/BTM</a>, <a href="https://github.com/xiaohuiyan/xiaohuiyan.github.io/blob/master/paper/BTM-WWW13.pdf">https://github.com/xiaohuiyan/xiaohuiyan.github.io/blob/master/paper/BTM-WWW13.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BTM">BTM</a></code>, <code><a href="#topic+terms.BTM">terms.BTM</a></code>, <code><a href="#topic+logLik.BTM">logLik.BTM</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(udpipe)
data("brussels_reviews_anno", package = "udpipe")
x &lt;- subset(brussels_reviews_anno, language == "nl")
x &lt;- subset(x, xpos %in% c("NN", "NNP", "NNS"))
x &lt;- x[, c("doc_id", "lemma")]
model  &lt;- BTM(x, k = 5, iter = 5, trace = TRUE)
scores &lt;- predict(model, newdata = x, type = "sum_b")
scores &lt;- predict(model, newdata = x, type = "sub_w")
scores &lt;- predict(model, newdata = x, type = "mix")
head(scores)

</code></pre>

<hr>
<h2 id='terms.BTM'>Get highest token probabilities for each topic or get biterms used in the model</h2><span id='topic+terms.BTM'></span>

<h3>Description</h3>

<p>Get highest token probabilities for each topic or get biterms used in the model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'BTM'
terms(x, type = c("tokens", "biterms"), threshold = 0, top_n = 5, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="terms.BTM_+3A_x">x</code></td>
<td>
<p>an object of class BTM as returned by <code><a href="#topic+BTM">BTM</a></code></p>
</td></tr>
<tr><td><code id="terms.BTM_+3A_type">type</code></td>
<td>
<p>a character string, either 'tokens' or 'biterms'. Defaults to 'tokens'.</p>
</td></tr>
<tr><td><code id="terms.BTM_+3A_threshold">threshold</code></td>
<td>
<p>threshold in 0-1 range. Only the terms which are more likely than the threshold are returned for each topic. Only used in case type = 'tokens'.</p>
</td></tr>
<tr><td><code id="terms.BTM_+3A_top_n">top_n</code></td>
<td>
<p>integer indicating to return the top n tokens for each topic only. Only used in case type = 'tokens'.</p>
</td></tr>
<tr><td><code id="terms.BTM_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Depending if type is set to 'tokens' or 'biterms' the following is returned:
</p>

<ul>
<li><p>If <code>type='tokens'</code>: Get the probability of the token given the topic P(w|z). 
It returns a list of data.frames (one for each topic) where each data.frame contains columns token and probability ordered from high to low.
The list is the same length as the number of topics.
</p>
</li>
<li><p>If <code>type='biterms'</code>: a list containing 2 elements: 
</p>

<ul>
<li> <p><code>n</code> which indicates the number of biterms used to train the model
</p>
</li>
<li> <p><code>biterms</code> which is a data.frame with columns term1, term2 and topic, 
indicating for all biterms found in the data the topic to which the biterm is assigned to
</p>
</li></ul>

<p>Note that a biterm is unordered, in the output of <code>type='biterms'</code> term1 is always smaller than or equal to term2.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+BTM">BTM</a></code>, <code><a href="#topic+predict.BTM">predict.BTM</a></code>, <code><a href="#topic+logLik.BTM">logLik.BTM</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(udpipe)
data("brussels_reviews_anno", package = "udpipe")
x &lt;- subset(brussels_reviews_anno, language == "nl")
x &lt;- subset(x, xpos %in% c("NN", "NNP", "NNS"))
x &lt;- x[, c("doc_id", "lemma")]
model  &lt;- BTM(x, k = 5, iter = 5, trace = TRUE)
terms(model)
terms(model, top_n = 10)
terms(model, threshold = 0.01, top_n = +Inf)
bi &lt;- terms(model, type = "biterms")
str(bi)

</code></pre>

<hr>
<h2 id='terms.data.frame'>Get the set of Biterms from a tokenised data frame</h2><span id='topic+terms.data.frame'></span>

<h3>Description</h3>

<p>This extracts words occurring in the neighbourhood of one another, within a certain window range.
The default setting provides the biterms used when fitting <code><a href="#topic+BTM">BTM</a></code> with the default window parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'data.frame'
terms(x, type = c("tokens", "biterms"), window = 15, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="terms.data.frame_+3A_x">x</code></td>
<td>
<p>a tokenised data frame containing one row per token with 2 columns 
</p>

<ul>
<li><p> the first column is a context identifier (e.g. a tweet id, a document id, a sentence id, an identifier of a survey answer, an identifier of a part of a text)
</p>
</li>
<li><p> the second column is a column called of type character containing the sequence of words occurring within the context identifier 
</p>
</li></ul>
</td></tr>
<tr><td><code id="terms.data.frame_+3A_type">type</code></td>
<td>
<p>a character string, either 'tokens' or 'biterms'. Defaults to 'tokens'.</p>
</td></tr>
<tr><td><code id="terms.data.frame_+3A_window">window</code></td>
<td>
<p>integer with the window size for biterm extraction. Defaults to 15.</p>
</td></tr>
<tr><td><code id="terms.data.frame_+3A_...">...</code></td>
<td>
<p>not used</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Depending if type is set to 'tokens' or 'biterms' the following is returned:
</p>

<ul>
<li><p>If <code>type='tokens'</code>: a list containing 2 elements: 
</p>

<ul>
<li> <p><code>n</code> which indicates the number of tokens
</p>
</li>
<li> <p><code>tokens</code> which is a data.frame with columns id, token and freq, 
indicating for all tokens found in the data the frequency of occurrence
</p>
</li></ul>


</li>
<li><p>If <code>type='biterms'</code>: a list containing 2 elements: 
</p>

<ul>
<li> <p><code>n</code> which indicates the number of biterms used to train the model
</p>
</li>
<li> <p><code>biterms</code> which is a data.frame with columns term1 and term2, 
indicating all biterms found in the data. The same biterm combination can occur several times.
</p>
</li></ul>

<p>Note that a biterm is unordered, in the output of <code>type='biterms'</code> term1 is always smaller than or equal to term2.
</p>
</li></ul>



<h3>Note</h3>

<p>If <code>x</code> is a data.frame which has an attribute called 'terms', it just returns that <code>'terms'</code> attribute
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BTM">BTM</a></code>, <code><a href="#topic+predict.BTM">predict.BTM</a></code>, <code><a href="#topic+logLik.BTM">logLik.BTM</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(udpipe)
data("brussels_reviews_anno", package = "udpipe")
x &lt;- subset(brussels_reviews_anno, language == "nl")
x &lt;- subset(x, xpos %in% c("NN", "NNP", "NNS"))
x &lt;- x[, c("doc_id", "lemma")]
biterms &lt;- terms(x, window = 15, type = "biterms")
str(biterms)
tokens &lt;- terms(x, type = "tokens")
str(tokens)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
