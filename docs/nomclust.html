<!DOCTYPE html><html><head><title>Help for package nomclust</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {nomclust}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#anderberg'><p>Anderberg (AN) Measure</p></a></li>
<li><a href='#as.agnes'><p>Convert Objects to Class agnes, twins</p></a></li>
<li><a href='#burnaby'><p>Burnaby (BU) Measure</p></a></li>
<li><a href='#CA.methods'><p>Selected clustering algorithms</p></a></li>
<li><a href='#data20'><p>Artificial nominal dataset</p></a></li>
<li><a href='#dend.plot'><p>Visualization of Cluster Hierarchy using a Dendrogram</p></a></li>
<li><a href='#eskin'><p>Eskin (ES) Measure</p></a></li>
<li><a href='#eval.plot'><p>Visualization of Evaluation Criteria</p></a></li>
<li><a href='#evalclust'><p>Cluster Quality Evaluation of Nominal Data Hierarchical Clustering</p></a></li>
<li><a href='#gambaryan'><p>Gambaryan (GA) Measure</p></a></li>
<li><a href='#goodall1'><p>Goodall 1 (G1) Measure</p></a></li>
<li><a href='#goodall2'><p>Goodall 2 (G2) Measure</p></a></li>
<li><a href='#goodall3'><p>Goodall 3 (G3) Measure</p></a></li>
<li><a href='#goodall4'><p>Goodall 4 (G4) Measure</p></a></li>
<li><a href='#iof'><p>Inverse Occurence Frequency (IOF) Measure</p></a></li>
<li><a href='#lin'><p>Lin (LIN) Measure</p></a></li>
<li><a href='#lin1'><p>Lin 1 (LIN1) Measure</p></a></li>
<li><a href='#nomclust'><p>Hierarchical Clustering of Nominal Data</p></a></li>
<li><a href='#nomprox'><p>Hierarchical Clustering of Nominal Data Based on a Proximity Matrix</p></a></li>
<li><a href='#of'><p>Occurence Frequency (OF) Measure</p></a></li>
<li><a href='#sm'><p>Simple Matching Coefficient (SM)</p></a></li>
<li><a href='#smirnov'><p>Smirnov (SV) Measure</p></a></li>
<li><a href='#ve'><p>Variable Entropy (VE) Measure</p></a></li>
<li><a href='#vm'><p>Variable Mutability (VM) measure</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Hierarchical Cluster Analysis of Nominal Data</td>
</tr>
<tr>
<td>Author:</td>
<td>Zdenek Sulc [aut, cre],
  Jana Cibulkova [aut],
  Hana Rezankova [aut],
  Jaroslav Hornicek [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Zdenek Sulc &lt;zdenek.sulc@vse.cz&gt;</td>
</tr>
<tr>
<td>Version:</td>
<td>2.8.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-8-18</td>
</tr>
<tr>
<td>Description:</td>
<td>Similarity measures for hierarchical clustering of objects characterized by
    nominal (categorical) variables. Evaluation criteria for nominal data clustering.</td>
</tr>
<tr>
<td>Depends:</td>
<td>cluster, methods, clValid</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-08-18 09:34:22 UTC; zdenek</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.11.0)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-08-18 10:12:38 UTC</td>
</tr>
</table>
<hr>
<h2 id='anderberg'>Anderberg (AN) Measure</h2><span id='topic+anderberg'></span>

<h3>Description</h3>

<p>The function calculates a dissimilarity matrix based on the AN similarity measure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>anderberg(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="anderberg_+3A_data">data</code></td>
<td>
<p>A data.frame or a matrix with cases in rows and variables in columns.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Anderberg similarity measure was presented in (Anderberg, 1973).
The measure assigns higher weights to infrequent matches and mismatches. 
It takes on values from zero to one. The minimum similarity is attained when there are no matches and vice versa, see (Borian et al., 2008).
</p>


<h3>Value</h3>

<p>The function returns an object of the class &quot;dist&quot;.
<br />
</p>


<h3>Author(s)</h3>

<p>Zdenek Sulc. <br /> Contact: <a href="mailto:zdenek.sulc@vse.cz">zdenek.sulc@vse.cz</a>
</p>


<h3>References</h3>

<p>Andergerg M.R. (1973). Cluster analysis for applications. Academic Press, New York.
<br />
<br />
Boriah S., Chandola V., Kumar V. (2008). Similarity measures for categorical data: A comparative evaluation.
In: Proceedings of the 8th SIAM International Conference on Data Mining, SIAM, p. 243-254.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+burnaby">burnaby</a></code>,
<code><a href="#topic+eskin">eskin</a></code>,
<code><a href="#topic+gambaryan">gambaryan</a></code>,
<code><a href="#topic+goodall1">goodall1</a></code>,
<code><a href="#topic+goodall2">goodall2</a></code>,
<code><a href="#topic+goodall3">goodall3</a></code>,
<code><a href="#topic+goodall4">goodall4</a></code>,
<code><a href="#topic+iof">iof</a></code>,
<code><a href="#topic+lin">lin</a></code>,
<code><a href="#topic+lin1">lin1</a></code>,
<code><a href="#topic+of">of</a></code>,
<code><a href="#topic+sm">sm</a></code>,
<code><a href="#topic+smirnov">smirnov</a></code>,
<code><a href="#topic+ve">ve</a></code>,
<code><a href="#topic+vm">vm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># sample data
data(data20)

# dissimilarity matrix calculation
prox.anderberg &lt;- anderberg(data20)

</code></pre>

<hr>
<h2 id='as.agnes'>Convert Objects to Class agnes, twins</h2><span id='topic+as.agnes'></span>

<h3>Description</h3>

<p>Converts objects of the class &quot;nomclust&quot; to the class &quot;agnes, twins&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.agnes(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.agnes_+3A_x">x</code></td>
<td>
<p>The &quot;nomclust&quot; object containing components &quot;dend&quot; and &quot;prox&quot;.</p>
</td></tr>
<tr><td><code id="as.agnes_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns an object of class &quot;agnes, twins&quot;.
<br />
</p>


<h3>Author(s)</h3>

<p>Zdenek Sulc. <br /> Contact: <a href="mailto:zdenek.sulc@vse.cz">zdenek.sulc@vse.cz</a>
</p>


<h3>See Also</h3>

<p><code><a href="cluster.html#topic+agnes">agnes</a></code>, <code><a href="stats.html#topic+as.hclust">as.hclust</a></code> and <code><a href="stats.html#topic+hclust">hclust</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># sample data
data(data20)

# creating an object with results of hierarchical clustering of 
hca.object &lt;- nomclust(data20, measure = "lin", method = "average",
 clu.high = 5, prox = TRUE)

# nomclust plot
plot(hca.object)

# obtaining the agnes, twins object
hca.object.agnes &lt;- as.agnes(hca.object)

# agnes plot
plot(hca.object.agnes)

# obtaining the hclust object
hca.object.hclust &lt;- as.hclust(hca.object)

# hclust plot
plot(hca.object.hclust)

</code></pre>

<hr>
<h2 id='burnaby'>Burnaby (BU) Measure</h2><span id='topic+burnaby'></span>

<h3>Description</h3>

<p>The function calculates a dissimilarity matrix based on the BU similarity measure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>burnaby(data, var.weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="burnaby_+3A_data">data</code></td>
<td>
<p>A data.frame or a matrix with cases in rows and variables in columns.</p>
</td></tr>
<tr><td><code id="burnaby_+3A_var.weights">var.weights</code></td>
<td>
<p>A numeric vector setting weights to the used variables. One can choose the real numbers from zero to one.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Burnaby similarity measure was presented in (Burnaby, 1970).
The measure assigns low similarity to mismatches on rare values and high similarity to mismatches on frequent values, see (Borian et al., 2008).
</p>


<h3>Value</h3>

<p>The function returns an object of the class &quot;dist&quot;.
<br />
</p>


<h3>Author(s)</h3>

<p>Zdenek Sulc. <br /> Contact: <a href="mailto:zdenek.sulc@vse.cz">zdenek.sulc@vse.cz</a>
</p>


<h3>References</h3>

<p>Burnaby T. (1970). On a method for character weighting a similarity coefficient, employing the concept of information.
Mathematical Geology, 2(1), 25-38.
<br />
<br />
Boriah S., Chandola V., Kumar V. (2008). Similarity measures for categorical data: A comparative evaluation.
In: Proceedings of the 8th SIAM International Conference on Data Mining, SIAM, p. 243-254.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+anderberg">anderberg</a></code>,
<code><a href="#topic+eskin">eskin</a></code>,
<code><a href="#topic+gambaryan">gambaryan</a></code>,
<code><a href="#topic+goodall1">goodall1</a></code>,
<code><a href="#topic+goodall2">goodall2</a></code>,
<code><a href="#topic+goodall3">goodall3</a></code>,
<code><a href="#topic+goodall4">goodall4</a></code>,
<code><a href="#topic+iof">iof</a></code>,
<code><a href="#topic+lin">lin</a></code>,
<code><a href="#topic+lin1">lin1</a></code>,
<code><a href="#topic+of">of</a></code>,
<code><a href="#topic+sm">sm</a></code>,
<code><a href="#topic+smirnov">smirnov</a></code>,
<code><a href="#topic+ve">ve</a></code>,
<code><a href="#topic+vm">vm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># sample data
data(data20)

# dissimilarity matrix calculation
prox.burnaby &lt;- burnaby(data20)

# dissimilarity matrix calculation with variable weights
weights.burnaby &lt;- burnaby(data20, var.weights = c(0.7, 1, 0.9, 0.5, 0))

</code></pre>

<hr>
<h2 id='CA.methods'>Selected clustering algorithms</h2><span id='topic+CA.methods'></span>

<h3>Description</h3>

<p>The dataset contains five different characteristics of 24 clustering algorithms. The &quot;Type&quot; variable expresses the principle on which the clustering is based. There are five possible categories: density, grid, hierarchical, model-based, and partitioning. The binary variable &quot;OptClu&quot; indicates if the clustering algorithm offers the optimal number of clusters. The variable &quot;Large&quot; indicates if the clustering algorithm was designed to cluster large datasets. The &quot;TypicalType&quot; variable presents the typical data type for which the clustering algorithm was determined. There are three possible categories: categorical, mixed, and quantitative. Since some clustering algorithms support more data types, the binary variable &quot;MoreTypes&quot; indicates this support.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("CA.methods")</code></pre>


<h3>Format</h3>

<p>A data frame containing 5 variables and 24 cases.</p>


<h3>Source</h3>

<p>created by the authors of the nomclust package</p>

<hr>
<h2 id='data20'>Artificial nominal dataset</h2><span id='topic+data20'></span>

<h3>Description</h3>

<p>This dataset consists of 5 nominal variables and 20 cases.
Its main aim is to demonstrate the desired entry data structure for the
nomclust package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(data20)</code></pre>


<h3>Format</h3>

<p>A data frame containing 5 variables and 20 cases.</p>


<h3>Source</h3>

<p>created by the authors of the nomclust package</p>

<hr>
<h2 id='dend.plot'>Visualization of Cluster Hierarchy using a Dendrogram</h2><span id='topic+dend.plot'></span>

<h3>Description</h3>

<p>The function <code>dend.plot()</code> visualizes the hierarchy of clusters using a dendrogram. The function also enables a user to mark the individual clusters with colors. 
The number of displayed clusters can be defined either by a user or by one of the five evaluation criteria.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dend.plot(
  x,
  clusters = "BIC",
  style = "greys",
  colorful = TRUE,
  clu.col = NA,
  main = "Dendrogram",
  ac = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dend.plot_+3A_x">x</code></td>
<td>
<p>An output of the <code>nomclust()</code> or <code>nomprox()</code> functions containing the <code>dend</code> component.</p>
</td></tr>
<tr><td><code id="dend.plot_+3A_clusters">clusters</code></td>
<td>
<p>Either a <em>numeric</em> value or a <em>character</em> string with the name of the evaluation criterion expressing the number of displayed clusters in a dendrogram. The following evaluation criteria can be used: <code>"AIC"</code>, <code>"BIC"</code>, <code>"BK"</code>, <code>"PSFE"</code> and <code>"PSFM"</code>.</p>
</td></tr>
<tr><td><code id="dend.plot_+3A_style">style</code></td>
<td>
<p>A <em>character</em> string or a <em>vector</em> of colors defines a graphical style of the produced plots. There are two predefined styles in the <b>nomclust</b> package, namely <code>"greys"</code> and <code>"dark"</code>, but a custom color scheme can be set by a user as a vector of a length four.</p>
</td></tr>
<tr><td><code id="dend.plot_+3A_colorful">colorful</code></td>
<td>
<p>A <em>logical</em> argument specifying if the output will be colorful or black and white.</p>
</td></tr>
<tr><td><code id="dend.plot_+3A_clu.col">clu.col</code></td>
<td>
<p>An optional <em>vector</em> of colors which allows a researcher to apply user-defined colors for displayed (marked) clusters in a dendrogram.</p>
</td></tr>
<tr><td><code id="dend.plot_+3A_main">main</code></td>
<td>
<p>A <em>character</em> string with the chart title.</p>
</td></tr>
<tr><td><code id="dend.plot_+3A_ac">ac</code></td>
<td>
<p>A <em>logical</em> argument indicating if an agglomerative coefficient will be present in the output.</p>
</td></tr>
<tr><td><code id="dend.plot_+3A_...">...</code></td>
<td>
<p>Other graphical arguments compatible with the generic <code>plot()</code> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function can be applied to a <code>nomclust()</code> or <code>nomprox()</code> output containing the <code>dend</code> component. This component is not available when the optimization process is used.
</p>


<h3>Value</h3>

<p>The function returns a dendrogram describing the hierarchy of clusters that can help to identify the optimal number of clusters.
<br />
</p>


<h3>Author(s)</h3>

<p>Jana Cibulkova and Zdenek Sulc. <br /> Contact: <a href="mailto:jana.cibulkova@vse.cz">jana.cibulkova@vse.cz</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+eval.plot">eval.plot</a></code>, <code><a href="#topic+nomclust">nomclust</a></code>, <code><a href="#topic+nomprox">nomprox</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># sample data
data(data20)

# creating an object with results of hierarchical clustering 
hca.object &lt;- nomclust(data20, measure = "iof", eval = TRUE)

# a basic plot
dend.plot(hca.object)

# a dendrogram with color-coded clusters according to the BIC index
dend.plot(hca.object, clusters = "BIC", colorful = TRUE)

# using a dark style and specifying own colors in a solution with three clusters
dend.plot(hca.object, clusters = 3, style = "dark", clu.col = c("blue", "red", "green"))

# a black and white dendrogram
dend.plot(hca.object, clusters = 3, style = "dark", colorful = FALSE)

</code></pre>

<hr>
<h2 id='eskin'>Eskin (ES) Measure</h2><span id='topic+eskin'></span>

<h3>Description</h3>

<p>The function calculates a dissimilarity matrix based on the ES similarity measure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>eskin(data, var.weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="eskin_+3A_data">data</code></td>
<td>
<p>A data.frame or a matrix with cases in rows and variables in columns.</p>
</td></tr>
<tr><td><code id="eskin_+3A_var.weights">var.weights</code></td>
<td>
<p>A numeric vector setting weights to the used variables. One can choose the real numbers from zero to one.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Eskin similarity measure was proposed by Eskin et al. (2002) and examined by Boriah et al., (2008). It is constructed to assign
higher weights to mismatches on variables with more categories.
</p>


<h3>Value</h3>

<p>The function returns an object of the class &quot;dist&quot;.
<br />
</p>


<h3>Author(s)</h3>

<p>Zdenek Sulc. <br /> Contact: <a href="mailto:zdenek.sulc@vse.cz">zdenek.sulc@vse.cz</a>
</p>


<h3>References</h3>

<p>Boriah S., Chandola V., Kumar V. (2008). Similarity measures for categorical data: A comparative evaluation.
In: Proceedings of the 8th SIAM International Conference on Data Mining, SIAM, p. 243-254.
<br />
<br />
Eskin E., Arnold A., Prerau M., Portnoy L. and Stolfo S. (2002). A geometric framework for unsupervised anomaly detection.
In D. Barbara and S. Jajodia (Eds): Applications of Data Mining in Computer Security, p. 78-100. Norwell: Kluwer Academic Publishers.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+anderberg">anderberg</a></code>,
<code><a href="#topic+burnaby">burnaby</a></code>,
<code><a href="#topic+gambaryan">gambaryan</a></code>,
<code><a href="#topic+goodall1">goodall1</a></code>,
<code><a href="#topic+goodall2">goodall2</a></code>,
<code><a href="#topic+goodall3">goodall3</a></code>,
<code><a href="#topic+goodall4">goodall4</a></code>,
<code><a href="#topic+iof">iof</a></code>,
<code><a href="#topic+lin">lin</a></code>,
<code><a href="#topic+lin1">lin1</a></code>,
<code><a href="#topic+of">of</a></code>,
<code><a href="#topic+sm">sm</a></code>,
<code><a href="#topic+smirnov">smirnov</a></code>,
<code><a href="#topic+ve">ve</a></code>,
<code><a href="#topic+vm">vm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># sample data
data(data20)

# dissimilarity matrix calculation
prox.eskin &lt;- eskin(data20)

# dissimilarity matrix calculation with variable weights
weights.eskin &lt;- eskin(data20, var.weights = c(0.7, 1, 0.9, 0.5, 0))

</code></pre>

<hr>
<h2 id='eval.plot'>Visualization of Evaluation Criteria</h2><span id='topic+eval.plot'></span>

<h3>Description</h3>

<p>The function visualizes the values of up to eight evaluation criteria for the range of cluster solutions defined by the user in the <b>nomclust</b>, <b>evalclust</b> or <b>nomprox</b> functions.
It also indicates the optimal number of clusters determined by these criteria. The charts for the evaluation criteria in the <b>nomclust</b> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>eval.plot(
  x,
  criteria = "all",
  style = "greys",
  opt.col = "red",
  main = "Cluster Evaluation",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="eval.plot_+3A_x">x</code></td>
<td>
<p>An output of the &quot;nomclust&quot; object containing the <code>eval</code> and <code>opt</code> components.</p>
</td></tr>
<tr><td><code id="eval.plot_+3A_criteria">criteria</code></td>
<td>
<p>A character string or character vector specifying the criteria that are going to be visualized. It can be selected one particular criterion, a vector of criteria, or all the available criteria by typing <code>"all"</code>.</p>
</td></tr>
<tr><td><code id="eval.plot_+3A_style">style</code></td>
<td>
<p>A character string or a vector of colors defines the graphical style of the produced plots. There are two predefined styles in the <b>nomclust</b> package, namely <code>"greys"</code> and <code>"dark"</code>, but a custom color scheme can be set by a user as a vector of a length four.</p>
</td></tr>
<tr><td><code id="eval.plot_+3A_opt.col">opt.col</code></td>
<td>
<p>An argument specifying a color that is used for the optimal number of clusters identification.</p>
</td></tr>
<tr><td><code id="eval.plot_+3A_main">main</code></td>
<td>
<p>A character string with the chart title.</p>
</td></tr>
<tr><td><code id="eval.plot_+3A_...">...</code></td>
<td>
<p>Other graphical arguments compatible with the generic <code>plot()</code> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function can display up to eight evaluation criteria. Namely, Within-cluster mutability coefficient (WCM), Within-cluster entropy coefficient (WCE),
Pseudo F Indices based on the mutability (PSFM) and the entropy (PSFE), Bayesian (BIC), and Akaike (AIC) information criteria for categorical data, the BK index, and the silhouette index (SI).
<br />
</p>


<h3>Value</h3>

<p>The function returns a series of up to eight plots with evaluation criteria values and the graphical indication of the optimal numbers of clusters (for AIC, BIC, BK, PSFE, PSFM, SI).
</p>


<h3>Author(s)</h3>

<p>Jana Cibulkova and Zdenek Sulc. <br /> Contact: <a href="mailto:jana.cibulkova@vse.cz">jana.cibulkova@vse.cz</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dend.plot">dend.plot</a></code>, <code><a href="#topic+nomclust">nomclust</a></code>, <code><a href="#topic+evalclust">evalclust</a></code>, <code><a href="#topic+nomprox">nomprox</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># sample data
data(data20)

# creating an object with results of hierarchical clustering 
hca.object &lt;- nomclust(data20, measure = "iof", eval = TRUE)

# a default series of plots
eval.plot(hca.object)

# changing the color indicating the optimum number of clusters
eval.plot(hca.object, opt.col= "darkorange")

# selecting only AIC and BIC criteria with the dark style
eval.plot(hca.object, criteria = c("AIC", "BIC"), style = "dark")

# selecting only SI
eval.plot(hca.object, criteria = "SI")

</code></pre>

<hr>
<h2 id='evalclust'>Cluster Quality Evaluation of Nominal Data Hierarchical Clustering</h2><span id='topic+evalclust'></span>

<h3>Description</h3>

<p>The function evaluates clustering results by a set of evaluation criteria (cluster validity indices).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evalclust(data, clusters, diss = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="evalclust_+3A_data">data</code></td>
<td>
<p>A data.frame or a matrix with cases in rows and variables in columns.</p>
</td></tr>
<tr><td><code id="evalclust_+3A_clusters">clusters</code></td>
<td>
<p>A data.frame or a list of cluster memberships obtained based on the dataset defined in the parameter <code>data</code> in the form of a sequence from the two-cluster solution to the maximal-cluster solution.</p>
</td></tr>
<tr><td><code id="evalclust_+3A_diss">diss</code></td>
<td>
<p>An optional parameter. A matrix or a dist object containing dissimilarities calculated based on the dataset defined in the parameter <code>data</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function calculates a set of evaluation criteria if the original dataset and the cluster membership variables are provided. 
The function calculates up to 13 evaluation criteria described by (Sulc et al., 2018) and (Corter and Gluck, 1992) and provides the optimal number of clusters based on these criteria. 
It is primarily focused on evaluating hierarchical clustering results obtained by similarity measures different from those that occur in the nomclust package. 
Thus, it can serve for the comparison of various similarity measures for categorical data.
</p>


<h3>Value</h3>

<p>The function returns a list with three components.
<br />
<br />
The <code>eval</code> component contains up to 13 evaluation criteria as vectors in a list. Namely, Within-cluster mutability coefficient (WCM), Within-cluster entropy coefficient (WCE),
Pseudo F Indices based on the mutability (PSFM) and the entropy (PSFE), Bayesian (BIC), and Akaike (AIC) information criteria for categorical data, the BK index, Category Utility (CU), Category Information (CI), Hartigan Mutability (HM), Hartigan Entropy (HE) and, if the prox component is present, the silhouette index (SI) and the Dunn index (DI).
<br />
<br />
The <code>opt</code> component is present in the output together with the <code>eval</code> component. It displays the optimal number of clusters for the evaluation criteria from the <code>eval</code> component, except for WCM and WCE, where the optimal number of clusters is based on the elbow method.
<br />
<br />
The <code>call</code> component contains the function call.
</p>


<h3>Author(s)</h3>

<p>Zdenek Sulc. <br /> Contact: <a href="mailto:zdenek.sulc@vse.cz">zdenek.sulc@vse.cz</a>
</p>


<h3>References</h3>

<p>Corter J.E., Gluck M.A. (1992). Explaining basic categories: Feature predictability and information. Psychological Bulletin 111(2), p. 291–303.
<br />
<br />
Sulc Z., Cibulkova J., Prochazka J., Rezankova H. (2018). Internal Evaluation Criteria for Categorical Data in Hierarchical Clustering: Optimal Number of Clusters Determination, Metodoloski Zveski, 15(2), p. 1-20.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nomclust">nomclust</a></code>, <code><a href="#topic+nomprox">nomprox</a></code>, <code><a href="#topic+eval.plot">eval.plot</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># sample data
data(data20)

# creating an object with results of hierarchical clustering
hca.object &lt;- nomclust(data20, measure = "iof", method = "average", clu.high = 7)

# the cluster memberships
data20.clu &lt;- hca.object$mem

# obtaining evaluation criteria for the provided dataset and cluster memberships
data20.eval &lt;- evalclust(data20, clusters = data20.clu)

# visualization of the evaluation criteria
eval.plot(data20.eval)

# silhouette index can be calculated if the dissimilarity matrix is provided
data20.eval &lt;- evalclust(data20, clusters = data20.clu, diss = hca.object$prox)
eval.plot(data20.eval, criteria = "SI")

</code></pre>

<hr>
<h2 id='gambaryan'>Gambaryan (GA) Measure</h2><span id='topic+gambaryan'></span>

<h3>Description</h3>

<p>The function calculates a dissimilarity matrix based on the GA similarity measure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gambaryan(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gambaryan_+3A_data">data</code></td>
<td>
<p>A data.frame or a matrix with cases in rows and variables in columns.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Gambaryan similarity measure was presented in (Gambaryan, 1964).
The measure assigns low weight to matches where the matching value occurs in about half the dataset, i.e., in between being frequent and rare, see (Borian et al., 2008).
</p>


<h3>Value</h3>

<p>The function returns an object of the class &quot;dist&quot;.
<br />
</p>


<h3>Author(s)</h3>

<p>Zdenek Sulc. <br /> Contact: <a href="mailto:zdenek.sulc@vse.cz">zdenek.sulc@vse.cz</a>
</p>


<h3>References</h3>

<p>Gambaryan P. (1964). A mathematical model of taxonomy. 
SSR, 17(12), 47-53.
<br />
<br />
Boriah S., Chandola V., Kumar V. (2008). Similarity measures for categorical data: A comparative evaluation.
In: Proceedings of the 8th SIAM International Conference on Data Mining, SIAM, p. 243-254.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+anderberg">anderberg</a></code>,
<code><a href="#topic+burnaby">burnaby</a></code>,
<code><a href="#topic+eskin">eskin</a></code>,
<code><a href="#topic+goodall1">goodall1</a></code>,
<code><a href="#topic+goodall2">goodall2</a></code>,
<code><a href="#topic+goodall3">goodall3</a></code>,
<code><a href="#topic+goodall4">goodall4</a></code>,
<code><a href="#topic+iof">iof</a></code>,
<code><a href="#topic+lin">lin</a></code>,
<code><a href="#topic+lin1">lin1</a></code>,
<code><a href="#topic+of">of</a></code>,
<code><a href="#topic+sm">sm</a></code>,
<code><a href="#topic+smirnov">smirnov</a></code>,
<code><a href="#topic+ve">ve</a></code>,
<code><a href="#topic+vm">vm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># sample data
data(data20)

# dissimilarity matrix calculation
prox.gambaryan &lt;- gambaryan(data20)

</code></pre>

<hr>
<h2 id='goodall1'>Goodall 1 (G1) Measure</h2><span id='topic+goodall1'></span>

<h3>Description</h3>

<p>The function calculates a dissimilarity matrix based on the G1 similarity measure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>goodall1(data, var.weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="goodall1_+3A_data">data</code></td>
<td>
<p>A data.frame or a matrix with cases in rows and variables in column.</p>
</td></tr>
<tr><td><code id="goodall1_+3A_var.weights">var.weights</code></td>
<td>
<p>A numeric vector setting weights to the used variables. One can choose the real numbers from zero to one.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Goodall 1 similarity measure was presented in (Boriah et al., 2008).
It is a simple modification of the original Goodall measure (Goodall, 1966). The measure assigns higher weights to infrequent matches.
</p>


<h3>Value</h3>

<p>The function returns an object of the class &quot;dist&quot;.
<br />
</p>


<h3>Author(s)</h3>

<p>Zdenek Sulc. <br /> Contact: <a href="mailto:zdenek.sulc@vse.cz">zdenek.sulc@vse.cz</a>
</p>


<h3>References</h3>

<p>Boriah S., Chandola V., Kumar V. (2008). Similarity measures for categorical data: A comparative evaluation.
In: Proceedings of the 8th SIAM International Conference on Data Mining, SIAM, p. 243-254.
<br />
<br />
Goodall V.D. (1966). A new similarity index based on probability. Biometrics, 22(4), p. 882.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+anderberg">anderberg</a></code>,
<code><a href="#topic+burnaby">burnaby</a></code>,
<code><a href="#topic+eskin">eskin</a></code>,
<code><a href="#topic+gambaryan">gambaryan</a></code>,
<code><a href="#topic+goodall2">goodall2</a></code>,
<code><a href="#topic+goodall3">goodall3</a></code>,
<code><a href="#topic+goodall4">goodall4</a></code>,
<code><a href="#topic+iof">iof</a></code>,
<code><a href="#topic+lin">lin</a></code>,
<code><a href="#topic+lin1">lin1</a></code>,
<code><a href="#topic+of">of</a></code>,
<code><a href="#topic+sm">sm</a></code>,
<code><a href="#topic+smirnov">smirnov</a></code>,
<code><a href="#topic+ve">ve</a></code>,
<code><a href="#topic+vm">vm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># sample data
data(data20)

# dissimilarity matrix calculation
prox.goodall1 &lt;- goodall1(data20)

# dissimilarity matrix calculation with variable weights
weights.goodall1 &lt;- goodall1(data20, var.weights = c(0.7, 1, 0.9, 0.5, 0))

</code></pre>

<hr>
<h2 id='goodall2'>Goodall 2 (G2) Measure</h2><span id='topic+goodall2'></span>

<h3>Description</h3>

<p>The function calculates a dissimilarity matrix based on the G2 similarity measure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>goodall2(data, var.weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="goodall2_+3A_data">data</code></td>
<td>
<p>A data.frame or a matrix with cases in rows and variables in columns.</p>
</td></tr>
<tr><td><code id="goodall2_+3A_var.weights">var.weights</code></td>
<td>
<p>A numeric vector setting weights to the used variables. One can choose the real numbers from zero to one.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Goodall 2 similarity measure was presented in (Boriah et al., 2008). It is a simple modification of the original Goodall measure (Goodall, 1966).                         
The measure assigns weight to infrequent matches under the condition that there are also other categories, which are even less frequent than the examined one.
</p>


<h3>Value</h3>

<p>The function returns an object of the class &quot;dist&quot;.
<br />
</p>


<h3>Author(s)</h3>

<p>Zdenek Sulc. <br /> Contact: <a href="mailto:zdenek.sulc@vse.cz">zdenek.sulc@vse.cz</a>
</p>


<h3>References</h3>

<p>Boriah S., Chandola V., Kumar V. (2008). Similarity measures for categorical data: A comparative evaluation.
In: Proceedings of the 8th SIAM International Conference on Data Mining, SIAM, p. 243-254.
<br />
<br />
Goodall V.D. (1966). A new similarity index based on probability. Biometrics, 22(4), p. 882.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+anderberg">anderberg</a></code>,
<code><a href="#topic+burnaby">burnaby</a></code>,
<code><a href="#topic+eskin">eskin</a></code>,
<code><a href="#topic+gambaryan">gambaryan</a></code>,
<code><a href="#topic+goodall1">goodall1</a></code>,
<code><a href="#topic+goodall3">goodall3</a></code>,
<code><a href="#topic+goodall4">goodall4</a></code>,
<code><a href="#topic+iof">iof</a></code>,
<code><a href="#topic+lin">lin</a></code>,
<code><a href="#topic+lin1">lin1</a></code>,
<code><a href="#topic+of">of</a></code>,
<code><a href="#topic+sm">sm</a></code>,
<code><a href="#topic+smirnov">smirnov</a></code>,
<code><a href="#topic+ve">ve</a></code>,
<code><a href="#topic+vm">vm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># sample data
data(data20)

# dissimilarity matrix calculation
prox.goodall2 &lt;- goodall2(data20)

# dissimilarity matrix calculation with variable weights
weights.goodall2 &lt;- goodall2(data20, var.weights = c(0.7, 1, 0.9, 0.5, 0))

</code></pre>

<hr>
<h2 id='goodall3'>Goodall 3 (G3) Measure</h2><span id='topic+goodall3'></span>

<h3>Description</h3>

<p>The function calculates a dissimilarity matrix based on the G3 similarity measure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>goodall3(data, var.weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="goodall3_+3A_data">data</code></td>
<td>
<p>A data.frame or a matrix with cases in rows and variables in columns.</p>
</td></tr>
<tr><td><code id="goodall3_+3A_var.weights">var.weights</code></td>
<td>
<p>A numeric vector setting weights to the used variables. One can choose the real numbers from zero to one.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Goodall 3 similarity measure was presented in (Boriah et al., 2008). It is a simple modification of the original Goodall measure (Goodall, 1966).           
The measure assigns higher weight if the infrequent categories match
regardless on frequencies of other categories.
</p>


<h3>Value</h3>

<p>The function returns an object of the class &quot;dist&quot;.
<br />
</p>


<h3>Author(s)</h3>

<p>Zdenek Sulc. <br /> Contact: <a href="mailto:zdenek.sulc@vse.cz">zdenek.sulc@vse.cz</a>
</p>


<h3>References</h3>

<p>Boriah S., Chandola V., Kumar V. (2008). Similarity measures for categorical data: A comparative evaluation.
In: Proceedings of the 8th SIAM International Conference on Data Mining, SIAM, p. 243-254.
<br />
<br />
Goodall V.D. (1966). A new similarity index based on probability. Biometrics, 22(4), p. 882.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+anderberg">anderberg</a></code>,
<code><a href="#topic+burnaby">burnaby</a></code>,
<code><a href="#topic+eskin">eskin</a></code>,
<code><a href="#topic+gambaryan">gambaryan</a></code>,
<code><a href="#topic+goodall1">goodall1</a></code>,
<code><a href="#topic+goodall2">goodall2</a></code>,
<code><a href="#topic+goodall4">goodall4</a></code>,
<code><a href="#topic+iof">iof</a></code>,
<code><a href="#topic+lin">lin</a></code>,
<code><a href="#topic+lin1">lin1</a></code>,
<code><a href="#topic+of">of</a></code>,
<code><a href="#topic+sm">sm</a></code>,
<code><a href="#topic+smirnov">smirnov</a></code>,
<code><a href="#topic+ve">ve</a></code>,
<code><a href="#topic+vm">vm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># sample data
data(data20)

# dissimilarity matrix calculation
prox.goodall3 &lt;- goodall3(data20)

# dissimilarity matrix calculation with variable weights
weights.goodall3 &lt;- goodall3(data20, var.weights = c(0.7, 1, 0.9, 0.5, 0))

</code></pre>

<hr>
<h2 id='goodall4'>Goodall 4 (G4) Measure</h2><span id='topic+goodall4'></span>

<h3>Description</h3>

<p>The function calculates a dissimilarity matrix based on the G4 similarity measure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>goodall4(data, var.weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="goodall4_+3A_data">data</code></td>
<td>
<p>A data.frame or a matrix with cases in rows and variables in columns.</p>
</td></tr>
<tr><td><code id="goodall4_+3A_var.weights">var.weights</code></td>
<td>
<p>A numeric vector setting weights to the used variables. One can choose the real numbers from zero to one.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Goodall 4 similarity measure was presented in (Boriah et al., 2008). It is a simple modification of the original Goodall measure (Goodall, 1966).                                  
It assigns higher weights to the frequent categories matches.
</p>


<h3>Value</h3>

<p>The function returns an object of the class &quot;dist&quot;.
<br />
</p>


<h3>Author(s)</h3>

<p>Zdenek Sulc. <br /> Contact: <a href="mailto:zdenek.sulc@vse.cz">zdenek.sulc@vse.cz</a>
</p>


<h3>References</h3>

<p>Boriah S., Chandola V., Kumar V. (2008). Similarity measures for categorical data: A comparative evaluation.
In: Proceedings of the 8th SIAM International Conference on Data Mining, SIAM, p. 243-254.
<br />
<br />
Goodall V.D. (1966). A new similarity index based on probability. Biometrics, 22(4), p. 882.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+anderberg">anderberg</a></code>,
<code><a href="#topic+burnaby">burnaby</a></code>,
<code><a href="#topic+eskin">eskin</a></code>,
<code><a href="#topic+gambaryan">gambaryan</a></code>,
<code><a href="#topic+goodall1">goodall1</a></code>,
<code><a href="#topic+goodall2">goodall2</a></code>,
<code><a href="#topic+goodall3">goodall3</a></code>,
<code><a href="#topic+iof">iof</a></code>,
<code><a href="#topic+lin">lin</a></code>,
<code><a href="#topic+lin1">lin1</a></code>,
<code><a href="#topic+of">of</a></code>,
<code><a href="#topic+sm">sm</a></code>,
<code><a href="#topic+smirnov">smirnov</a></code>,
<code><a href="#topic+ve">ve</a></code>,
<code><a href="#topic+vm">vm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># sample data
data(data20)

# dissimilarity matrix calculation
prox.goodall4 &lt;- goodall4(data20)

# dissimilarity matrix calculation with variable weights
weights.goodall4 &lt;- goodall4(data20, var.weights = c(0.7, 1, 0.9, 0.5, 0))

</code></pre>

<hr>
<h2 id='iof'>Inverse Occurence Frequency (IOF) Measure</h2><span id='topic+iof'></span>

<h3>Description</h3>

<p>The function calculates a dissimilarity matrix based on the IOF similarity measure.
<br />
</p>


<h3>Usage</h3>

<pre><code class='language-R'>iof(data, var.weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="iof_+3A_data">data</code></td>
<td>
<p>A data.frame or a matrix with cases in rows and variables in columns.</p>
</td></tr>
<tr><td><code id="iof_+3A_var.weights">var.weights</code></td>
<td>
<p>A numeric vector setting weights to the used variables. One can choose the real numbers from zero to one.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The IOF (Inverse Occurrence Frequency) measure was originally constructed for the text mining tasks,
see (Sparck-Jones, 1972), later, it was adjusted for categorical variables, see (Boriah et al., 2008).
The measure assigns higher weight to mismatches on less frequent values and vice versa.
</p>


<h3>Value</h3>

<p>The function returns an object of the class &quot;dist&quot;.
<br />
</p>


<h3>Author(s)</h3>

<p>Zdenek Sulc. <br /> Contact: <a href="mailto:zdenek.sulc@vse.cz">zdenek.sulc@vse.cz</a>
</p>


<h3>References</h3>

<p>Boriah S., Chandola V., Kumar V. (2008). Similarity measures for categorical data: A comparative evaluation.
In: Proceedings of the 8th SIAM International Conference on Data Mining, SIAM, p. 243-254.
<br />
<br />
Spark-Jones K. (1972). A statistical interpretation of term specificity and its application in retrieval.
In Journal of Documentation, 28(1), 11-21. Later: Journal of Documentation, 60(5) (2002), 493-502.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+anderberg">anderberg</a></code>,
<code><a href="#topic+burnaby">burnaby</a></code>,
<code><a href="#topic+eskin">eskin</a></code>,
<code><a href="#topic+gambaryan">gambaryan</a></code>,
<code><a href="#topic+goodall1">goodall1</a></code>,
<code><a href="#topic+goodall2">goodall2</a></code>,
<code><a href="#topic+goodall3">goodall3</a></code>,
<code><a href="#topic+goodall4">goodall4</a></code>,
<code><a href="#topic+lin">lin</a></code>,
<code><a href="#topic+lin1">lin1</a></code>,
<code><a href="#topic+of">of</a></code>,
<code><a href="#topic+sm">sm</a></code>,
<code><a href="#topic+smirnov">smirnov</a></code>,
<code><a href="#topic+ve">ve</a></code>,
<code><a href="#topic+vm">vm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># sample data
data(data20)

# dissimilarity matrix calculation
prox.iof &lt;- iof(data20)

# dissimilarity matrix calculation with variable weights
weights.iof &lt;- iof(data20, var.weights = c(0.7, 1, 0.9, 0.5, 0))

</code></pre>

<hr>
<h2 id='lin'>Lin (LIN) Measure</h2><span id='topic+lin'></span>

<h3>Description</h3>

<p>The function calculates a dissimilarity matrix based on the LIN similarity measure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lin(data, var.weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lin_+3A_data">data</code></td>
<td>
<p>A data.frame or a matrix with cases in rows and variables in columns.</p>
</td></tr>
<tr><td><code id="lin_+3A_var.weights">var.weights</code></td>
<td>
<p>A numeric vector setting weights to the used variables. One can choose the real numbers from zero to one.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Lin measure was introduced by Lin (1998) and presented in (Boriah et al., 2008).
The measure assigns higher weights to more frequent categories in case of matches
and lower weights to less frequent categories in case of mismatches.
</p>


<h3>Value</h3>

<p>The function returns an object of the class &quot;dist&quot;.
<br />
</p>


<h3>Author(s)</h3>

<p>Zdenek Sulc. <br /> Contact: <a href="mailto:zdenek.sulc@vse.cz">zdenek.sulc@vse.cz</a>
</p>


<h3>References</h3>

<p>Boriah S., Chandola V., Kumar V. (2008). Similarity measures for categorical data: A comparative evaluation.
In: Proceedings of the 8th SIAM International Conference on Data Mining, SIAM, p. 243-254.
<br />
<br />
Lin D. (1998). An information-theoretic definition of similarity.
In: ICML '98: Proceedings of the 15th International Conference on Machine Learning. San Francisco, p. 296-304.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+anderberg">anderberg</a></code>,
<code><a href="#topic+burnaby">burnaby</a></code>,
<code><a href="#topic+eskin">eskin</a></code>,
<code><a href="#topic+gambaryan">gambaryan</a></code>,
<code><a href="#topic+goodall1">goodall1</a></code>,
<code><a href="#topic+goodall2">goodall2</a></code>,
<code><a href="#topic+goodall3">goodall3</a></code>,
<code><a href="#topic+goodall4">goodall4</a></code>,
<code><a href="#topic+iof">iof</a></code>,
<code><a href="#topic+lin1">lin1</a></code>,
<code><a href="#topic+of">of</a></code>,
<code><a href="#topic+sm">sm</a></code>,
<code><a href="#topic+smirnov">smirnov</a></code>,
<code><a href="#topic+ve">ve</a></code>,
<code><a href="#topic+vm">vm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># sample data
data(data20)

# dissimilarity matrix calculation
prox.lin &lt;- lin(data20)

# dissimilarity matrix calculation with variable weights
weights.lin&lt;- lin(data20, var.weights = c(0.7, 1, 0.9, 0.5, 0))

</code></pre>

<hr>
<h2 id='lin1'>Lin 1 (LIN1) Measure</h2><span id='topic+lin1'></span>

<h3>Description</h3>

<p>The function calculates a dissimilarity matrix based on the LIN1 similarity measure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lin1(data, var.weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lin1_+3A_data">data</code></td>
<td>
<p>A data.frame or a matrix with cases in rows and variables in columns.</p>
</td></tr>
<tr><td><code id="lin1_+3A_var.weights">var.weights</code></td>
<td>
<p>A numeric vector setting weights to the used variables. One can choose the real numbers from zero to one.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Lin 1 similarity measure was introduced in (Boriah et al., 2008) as a modification of the original Lin measure (Lin, 1998). In has
a complex system of weights. In case of mismatch, lower similarity is assigned if either
the mismatching values are very frequent or their relative frequency is in between the relative
frequencies of mismatching values. Higher similarity is assigned if the mismatched categories
are infrequent and there are a few other infrequent categories. In case of match,
lower similarity is given for matches on frequent categories or matches on categories
that have many other values of the same frequency. Higher similarity is given to matches
on infrequent categories.
</p>


<h3>Value</h3>

<p>The function returns an object of the class &quot;dist&quot;.
<br />
</p>


<h3>Author(s)</h3>

<p>Zdenek Sulc. <br /> Contact: <a href="mailto:zdenek.sulc@vse.cz">zdenek.sulc@vse.cz</a>
</p>


<h3>References</h3>

<p>Boriah S., Chandola V., Kumar V. (2008). Similarity measures for categorical data: A comparative evaluation.
In: Proceedings of the 8th SIAM International Conference on Data Mining, SIAM, p. 243-254.
<br />
<br />
Lin D. (1998). An information-theoretic definition of similarity.
In: ICML '98: Proceedings of the 15th International Conference on Machine Learning. San Francisco, p. 296-304.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+anderberg">anderberg</a></code>,
<code><a href="#topic+burnaby">burnaby</a></code>,
<code><a href="#topic+eskin">eskin</a></code>,
<code><a href="#topic+gambaryan">gambaryan</a></code>,
<code><a href="#topic+goodall1">goodall1</a></code>,
<code><a href="#topic+goodall2">goodall2</a></code>,
<code><a href="#topic+goodall3">goodall3</a></code>,
<code><a href="#topic+goodall4">goodall4</a></code>,
<code><a href="#topic+iof">iof</a></code>,
<code><a href="#topic+lin">lin</a></code>,
<code><a href="#topic+of">of</a></code>,
<code><a href="#topic+sm">sm</a></code>,
<code><a href="#topic+smirnov">smirnov</a></code>,
<code><a href="#topic+ve">ve</a></code>,
<code><a href="#topic+vm">vm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># sample data
data(data20)

# dissimilarity matrix calculation
prox.lin1 &lt;- lin1(data20)

# dissimilarity matrix calculation with variable weights
weights.lin1 &lt;- lin1(data20, var.weights = c(0.7, 1, 0.9, 0.5, 0))

</code></pre>

<hr>
<h2 id='nomclust'>Hierarchical Clustering of Nominal Data</h2><span id='topic+nomclust'></span>

<h3>Description</h3>

<p>The function performs and evaluates hierarchical cluster analysis of nominal data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nomclust(
  data,
  measure = "lin",
  method = "average",
  clu.high = 6,
  eval = TRUE,
  prox = 100,
  var.weights = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nomclust_+3A_data">data</code></td>
<td>
<p>A data.frame or a matrix with cases in rows and variables in columns.</p>
</td></tr>
<tr><td><code id="nomclust_+3A_measure">measure</code></td>
<td>
<p>A character string defining the similarity measure used for computation of proximity matrix in HCA:
<code>"anderberg"</code>, <code>"burnaby"</code>, <code>"eskin"</code>, <code>"gambaryan"</code>, <code>"goodall1"</code>, <code>"goodall2"</code>, <code>"goodall3"</code>, <code>"goodall4"</code>, <code>"iof"</code>, <code>"lin"</code>, <code>"lin1"</code>, <code>"of"</code>, <code>"sm"</code>, <code>"smirnov"</code>, <code>"ve"</code>, <code>"vm"</code>.</p>
</td></tr>
<tr><td><code id="nomclust_+3A_method">method</code></td>
<td>
<p>A character string defining the clustering method. The following methods can be used: <code>"average"</code>, <code>"complete"</code>, <code>"single"</code>.</p>
</td></tr>
<tr><td><code id="nomclust_+3A_clu.high">clu.high</code></td>
<td>
<p>A numeric value expressing the maximal number of cluster for which the cluster memberships variables are produced.</p>
</td></tr>
<tr><td><code id="nomclust_+3A_eval">eval</code></td>
<td>
<p>A logical operator; if TRUE, evaluation of the clustering results is performed.</p>
</td></tr>
<tr><td><code id="nomclust_+3A_prox">prox</code></td>
<td>
<p>A logical operator or a numeric value. If a logical value TRUE indicates that the proximity matrix is a part of the output. A numeric value (integer) of this argument indicates the maximal number of cases in a dataset for which a proximity matrix will occur in the output.</p>
</td></tr>
<tr><td><code id="nomclust_+3A_var.weights">var.weights</code></td>
<td>
<p>A numeric vector setting weights to the used variables. One can choose the real numbers from zero to one.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function runs hierarchical cluster analysis (HCA) with objects characterized by nominal variables (without natural order of categories).
It completely covers the clustering process, from the dissimilarity matrix calculation to the cluster quality evaluation. The function enables a user to choose from the similarity measures for nominal data summarized by (Boriah et al., 2008) and by (Sulc and Rezankova, 2019). 
Next, it offers to choose from three linkage methods that can be used for categorical data. It is also possible to assign user-defined variable weights. The obtained clusters can be evaluated by up to 13 evaluation criteria (Sulc et al., 2018) and (Corter and Gluck, 1992). The output of the nomclust() function may serve as an input for the visualization functions <em>dend.plot</em> and <em>eval.plot</em> in the nomclust package.
</p>


<h3>Value</h3>

<p>The function returns a list with up to six components.
<br />
<br />
The <code>mem</code> component contains cluster membership partitions for the selected numbers of clusters in the form of a list.
<br />
<br />
The <code>eval</code> component contains up to 13 evaluation criteria as vectors in a list. Namely, Within-cluster mutability coefficient (WCM), Within-cluster entropy coefficient (WCE),
Pseudo F Indices based on the mutability (PSFM) and the entropy (PSFE), Bayesian (BIC), and Akaike (AIC) information criteria for categorical data, the BK index, Category Utility (CU), Category Information (CI), Hartigan Mutability (HM), Hartigan Entropy (HE) and, if the prox component is present, the silhouette index (SI) and the Dunn index (DI).
<br />
<br />
The <code>opt</code> component is present in the output together with the <code>eval</code> component. It displays the optimal number of clusters for the evaluation criteria from the <code>eval</code> component, except for WCM and WCE, where the optimal number of clusters is based on the elbow method.
<br />
<br />
The <code>dend</code> component can be found in the output together with the <code>prox</code> component. It contains all the necessary information for dendrogram creation.
<br />
<br />
The <code>prox</code> component contains the dissimilarity matrix in the form of the &quot;dist&quot; object.
<br />
<br />
The <code>call</code> component contains the function call.
</p>


<h3>Author(s)</h3>

<p>Zdenek Sulc. <br /> Contact: <a href="mailto:zdenek.sulc@vse.cz">zdenek.sulc@vse.cz</a>
</p>


<h3>References</h3>

<p>Boriah S., Chandola V. and Kumar, V. (2008). Similarity measures for categorical data: A comparative evaluation.
In: Proceedings of the 8th SIAM International Conference on Data Mining, SIAM, p. 243-254.
<br />
<br />
Corter J.E., Gluck M.A. (1992). Explaining basic categories: Feature predictability and information. Psychological Bulletin 111(2), p. 291–303.
<br />
<br />
Sulc Z., Cibulkova J., Prochazka J., Rezankova H. (2018). Internal Evaluation Criteria for Categorical Data in Hierarchical Clustering: Optimal Number of Clusters Determination, Metodoloski Zveski, 15(2), p. 1-20.
<br />
<br />
Sulc Z. and Rezankova H. (2019). Comparison of Similarity Measures for Categorical Data in Hierarchical Clustering. Journal of Classification, 35(1), p. 58-72. DOI: 10.1007/s00357-019-09317-5.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+evalclust">evalclust</a></code>, <code><a href="#topic+nomprox">nomprox</a></code>, <code><a href="#topic+eval.plot">eval.plot</a></code>, <code><a href="#topic+dend.plot">dend.plot</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># sample data
data(data20)

# creating an object with results of hierarchical clustering of 
hca.object &lt;- nomclust(data20, measure = "lin", method = "average",
 clu.high = 5, prox = TRUE)

# assigning variable weights
hca.weights &lt;- nomclust(data20, measure = "lin", method = "average",
 clu.high = 5, prox = TRUE, var.weights = c(0.7, 1, 0.9, 0.5, 0))

# quick clustering summary
summary(hca.object)

# quick cluster quality evaluation
print(hca.object)

# visualization of the evaluation criteria
eval.plot(hca.object)

# a quick dendrogram
plot(hca.object)

# a dendrogram with three designated clusters
dend.plot(hca.object, clusters = 3)

# obtaining values of evaluation indices as a data.frame
data20.eval &lt;- as.data.frame(hca.object$eval)

# getting the optimal numbers of clusters as a data.frame
data20.opt &lt;- as.data.frame(hca.object$opt)

# extracting cluster membership variables as a data.frame
data20.mem &lt;- as.data.frame(hca.object$mem)

# obtaining a proximity matrix
data20.prox &lt;- as.matrix(hca.object$prox)

# setting the maximal number of objects for which a proximity matrix is provided in the output to 30
hca.object &lt;- nomclust(data20, measure = "iof", method = "complete",
 clu.high = 5, prox = 30)
 
# transforming the nomclust object to the class "hclust"
hca.object.hclust &lt;- as.hclust(hca.object)

# transforming the nomclust object to the class "agnes, twins"
hca.object.agnes &lt;- as.agnes(hca.object)


</code></pre>

<hr>
<h2 id='nomprox'>Hierarchical Clustering of Nominal Data Based on a Proximity Matrix</h2><span id='topic+nomprox'></span>

<h3>Description</h3>

<p>The function performs hierarchical cluster analysis based on a dissimilarity matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nomprox(
  diss,
  data = NULL,
  method = "average",
  clu.high = 6,
  eval = TRUE,
  prox = 100
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nomprox_+3A_diss">diss</code></td>
<td>
<p>A proximity matrix or a dist object calculated based on the dataset defined in a parameter <code>data</code>.</p>
</td></tr>
<tr><td><code id="nomprox_+3A_data">data</code></td>
<td>
<p>A data.frame or a matrix with cases in rows and variables in columns.</p>
</td></tr>
<tr><td><code id="nomprox_+3A_method">method</code></td>
<td>
<p>A character string defining the clustering method. The following methods can be used: <code>"average"</code>, <code>"complete"</code>, <code>"single"</code>.</p>
</td></tr>
<tr><td><code id="nomprox_+3A_clu.high">clu.high</code></td>
<td>
<p>A numeric value that expresses the maximal number of clusters for which the cluster membership variables are produced.</p>
</td></tr>
<tr><td><code id="nomprox_+3A_eval">eval</code></td>
<td>
<p>A logical operator; if TRUE, evaluation of clustering results is performed.</p>
</td></tr>
<tr><td><code id="nomprox_+3A_prox">prox</code></td>
<td>
<p>A logical operator or a numeric value. If a logical value TRUE indicates that the proximity matrix is a part of the output. A numeric value (integer) of this argument indicates the maximal number of cases in a dataset for which a proximity matrix will occur in the output.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function performs hierarchical cluster analysis in situations when the proximity (dissimilarity) matrix was calculated externally. For instance, in a different R package, in an own-created function, or in other software.
It offers three linkage methods that can be used for categorical data. The obtained clusters can be evaluated by up to 13 evaluation criteria (Sulc et al., 2018) and (Corter and Gluck, 1992).
</p>


<h3>Value</h3>

<p>The function returns a list with up to six components:
<br />
<br />
The <code>mem</code> component contains cluster membership partitions for the selected numbers of clusters in the form of a list.
<br />
<br />
The <code>eval</code> component contains up to 13 evaluation criteria as vectors in a list. Namely, Within-cluster mutability coefficient (WCM), Within-cluster entropy coefficient (WCE),
Pseudo F Indices based on the mutability (PSFM) and the entropy (PSFE), Bayesian (BIC), and Akaike (AIC) information criteria for categorical data, the BK index, Category Utility (CU), Category Information (CI), Hartigan Mutability (HM), Hartigan Entropy (HE) and, if the prox component is present, the silhouette index (SI) and the Dunn index (DI).
<br />
<br />
The <code>opt</code> component is present in the output together with the <code>eval</code> component. It displays the optimal number of clusters for the evaluation criteria from the <code>eval</code> component, except for WCM and WCE, where the optimal number of clusters is based on the elbow method.
<br />
<br />
The <code>dend</code> component can be found in the output only together with the <code>prox</code> component. It contains all the necessary information for dendrogram creation.
<br />
<br />
The <code>prox</code> component contains the dissimilarity matrix in the form of the &quot;dist&quot; object.
<br />
<br />
The <code>call</code> component contains the function call.
</p>


<h3>Author(s)</h3>

<p>Zdenek Sulc. <br /> Contact: <a href="mailto:zdenek.sulc@vse.cz">zdenek.sulc@vse.cz</a>
</p>


<h3>References</h3>

<p>Corter J.E., Gluck M.A. (1992). Explaining basic categories: Feature predictability and information. Psychological Bulletin 111(2), p. 291–303.
<br />
<br />
Sulc Z., Cibulkova J., Prochazka J., Rezankova H. (2018). Internal Evaluation Criteria for Categorical Data in Hierarchical Clustering: Optimal Number of Clusters Determination, Metodoloski Zveski, 15(2), p. 1-20.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nomclust">nomclust</a></code>, <code><a href="#topic+evalclust">evalclust</a></code>, <code><a href="#topic+eval.plot">eval.plot</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># sample data
data(data20)

# computation of a dissimilarity matrix using the iof similarity measure
diss.matrix &lt;- iof(data20)

# creating an object with results of hierarchical clustering 
hca.object &lt;- nomprox(diss = diss.matrix, data = data20, method = "complete",
 clu.high = 5, eval = TRUE, prox = FALSE)
 
# quick clustering summary
summary(hca.object)

# quick cluster quality evaluation
print(hca.object)

# visualization of the evaluation criteria
eval.plot(hca.object)

# a dendrogram can be displayed if the object contains the prox component
hca.object &lt;- nomprox(diss = diss.matrix, data = data20, method = "complete",
 clu.high = 5, eval = TRUE, prox = TRUE)

# a quick dendrogram
plot(hca.object)

# a dendrogram with three designated clusters
dend.plot(hca.object, clusters = 3)


</code></pre>

<hr>
<h2 id='of'>Occurence Frequency (OF) Measure</h2><span id='topic+of'></span>

<h3>Description</h3>

<p>The function calculates a dissimilarity matrix based on the OF similarity measure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>of(data, var.weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="of_+3A_data">data</code></td>
<td>
<p>A data.frame or a matrix with cases in rows and variables in columns.</p>
</td></tr>
<tr><td><code id="of_+3A_var.weights">var.weights</code></td>
<td>
<p>A numeric vector setting weights to the used variables. One can choose the real numbers from zero to one.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The OF (Occurrence Frequency) measure was originally constructed for the text mining tasks,
see (Sparck-Jones, 1972), later, it was adjusted for categorical variables, see (Boriah et al., 2008)
It assigns higher weight to mismatches on less frequent values and otherwise.
</p>


<h3>Value</h3>

<p>The function returns an object of the class &quot;dist&quot;.
<br />
</p>


<h3>Author(s)</h3>

<p>Zdenek Sulc. <br /> Contact: <a href="mailto:zdenek.sulc@vse.cz">zdenek.sulc@vse.cz</a>
</p>


<h3>References</h3>

<p>Boriah S., Chandola V., Kumar V. (2008). Similarity measures for categorical data: A comparative evaluation.
In: Proceedings of the 8th SIAM International Conference on Data Mining, SIAM, p. 243-254.
<br />
<br />
Spark-Jones K. (1972). A statistical interpretation of term specificity and its application in retrieval.
In Journal of Documentation, 28(1), p. 11-21. Later: Journal of Documentation, 60(5) (2002), p. 493-502.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+anderberg">anderberg</a></code>,
<code><a href="#topic+burnaby">burnaby</a></code>,
<code><a href="#topic+eskin">eskin</a></code>,
<code><a href="#topic+gambaryan">gambaryan</a></code>,
<code><a href="#topic+goodall1">goodall1</a></code>,
<code><a href="#topic+goodall2">goodall2</a></code>,
<code><a href="#topic+goodall3">goodall3</a></code>,
<code><a href="#topic+goodall4">goodall4</a></code>,
<code><a href="#topic+iof">iof</a></code>,
<code><a href="#topic+lin">lin</a></code>,
<code><a href="#topic+lin1">lin1</a></code>,
<code><a href="#topic+sm">sm</a></code>,
<code><a href="#topic+smirnov">smirnov</a></code>,
<code><a href="#topic+ve">ve</a></code>,
<code><a href="#topic+vm">vm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># sample data
data(data20)

# dissimilarity matrix calculation
prox.of &lt;- of(data20)

# dissimilarity matrix calculation with variable weights
weights.of &lt;- of(data20, var.weights = c(0.7, 1, 0.9, 0.5, 0))

</code></pre>

<hr>
<h2 id='sm'>Simple Matching Coefficient (SM)</h2><span id='topic+sm'></span>

<h3>Description</h3>

<p>The function calculates a dissimilarity matrix based on the SM similarity measure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sm(data, var.weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sm_+3A_data">data</code></td>
<td>
<p>A data.frame or a matrix with cases in rows and variables in columns.</p>
</td></tr>
<tr><td><code id="sm_+3A_var.weights">var.weights</code></td>
<td>
<p>A numeric vector setting weights to the used variables. One can choose the real numbers from zero to one.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The simple matching coefficient (Sokal, 1958) represents the simplest way of measuring similarity. It does not impose any weights.
By a given variable, it assigns the value 1 in case of match and value 0 otherwise.
</p>


<h3>Value</h3>

<p>The function returns an object of the class &quot;dist&quot;.
<br />
</p>


<h3>Author(s)</h3>

<p>Zdenek Sulc. <br /> Contact: <a href="mailto:zdenek.sulc@vse.cz">zdenek.sulc@vse.cz</a>
</p>


<h3>References</h3>

<p>Boriah S., Chandola V., Kumar V. (2008). Similarity measures for categorical data: A comparative evaluation.
In: Proceedings of the 8th SIAM International Conference on Data Mining, SIAM, p. 243-254.
<br />
<br />
Sokal R., Michener C. (1958). A statistical method for evaluating systematic relationships. In: Science bulletin, 38(22),
The University of Kansas.  
<br />
</p>


<h3>See Also</h3>

<p><code><a href="#topic+anderberg">anderberg</a></code>,
<code><a href="#topic+burnaby">burnaby</a></code>,
<code><a href="#topic+eskin">eskin</a></code>,
<code><a href="#topic+gambaryan">gambaryan</a></code>,
<code><a href="#topic+goodall1">goodall1</a></code>,
<code><a href="#topic+goodall2">goodall2</a></code>,
<code><a href="#topic+goodall3">goodall3</a></code>,
<code><a href="#topic+goodall4">goodall4</a></code>,
<code><a href="#topic+iof">iof</a></code>,
<code><a href="#topic+lin">lin</a></code>,
<code><a href="#topic+lin1">lin1</a></code>,
<code><a href="#topic+of">of</a></code>,
<code><a href="#topic+smirnov">smirnov</a></code>,
<code><a href="#topic+ve">ve</a></code>,
<code><a href="#topic+vm">vm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># sample data
data(data20)

# dissimilarity matrix calculation
prox.sm &lt;- sm(data20)

# dissimilarity matrix calculation with variable weights
weights.sm &lt;- sm(data20, var.weights = c(0.7, 1, 0.9, 0.5, 0))

</code></pre>

<hr>
<h2 id='smirnov'>Smirnov (SV) Measure</h2><span id='topic+smirnov'></span>

<h3>Description</h3>

<p>The function calculates a dissimilarity matrix based on the SV similarity measure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>smirnov(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="smirnov_+3A_data">data</code></td>
<td>
<p>A data.frame or a matrix with cases in rows and variables in columns.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Smirnov similarity measure was presented in (Smirnov, 1968).
The measure assigns high similarity to matches when the frequency of the matching value is low, and the other values occur frequently, see (Borian et al., 2008).
</p>


<h3>Value</h3>

<p>The function returns an object of the class &quot;dist&quot;.
<br />
</p>


<h3>Author(s)</h3>

<p>Zdenek Sulc. <br /> Contact: <a href="mailto:zdenek.sulc@vse.cz">zdenek.sulc@vse.cz</a>
</p>


<h3>References</h3>

<p>Smirnov E.S. (1968). On exact methods in systematics. 
Systematic Zoology, 17(1), 1-13.
<br />
<br />
Boriah S., Chandola V., Kumar V. (2008). Similarity measures for categorical data: A comparative evaluation.
In: Proceedings of the 8th SIAM International Conference on Data Mining, SIAM, p. 243-254.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+anderberg">anderberg</a></code>,
<code><a href="#topic+burnaby">burnaby</a></code>,
<code><a href="#topic+eskin">eskin</a></code>,
<code><a href="#topic+gambaryan">gambaryan</a></code>,
<code><a href="#topic+goodall1">goodall1</a></code>,
<code><a href="#topic+goodall2">goodall2</a></code>,
<code><a href="#topic+goodall3">goodall3</a></code>,
<code><a href="#topic+goodall4">goodall4</a></code>,
<code><a href="#topic+iof">iof</a></code>,
<code><a href="#topic+lin">lin</a></code>,
<code><a href="#topic+lin1">lin1</a></code>,
<code><a href="#topic+of">of</a></code>,
<code><a href="#topic+sm">sm</a></code>,
<code><a href="#topic+ve">ve</a></code>,
<code><a href="#topic+vm">vm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># sample data
data(data20)

# dissimilarity matrix calculation
prox.smirnov &lt;- smirnov(data20)

</code></pre>

<hr>
<h2 id='ve'>Variable Entropy (VE) Measure</h2><span id='topic+ve'></span>

<h3>Description</h3>

<p>The function calculates a dissimilarity matrix based on the VE similarity measure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ve(data, var.weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ve_+3A_data">data</code></td>
<td>
<p>A data.frame or a matrix with cases in rows and variables in columns.</p>
</td></tr>
<tr><td><code id="ve_+3A_var.weights">var.weights</code></td>
<td>
<p>A numeric vector setting weights to the used variables. One can choose the real numbers from zero to one.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Variable Entropy similarity measure was introduced in (Sulc and Rezankova, 2019). It treats
the similarity between two categories based on the within-cluster variability expressed by the normalized entropy.
The measure assigns higher weights to rare categories.
</p>


<h3>Value</h3>

<p>The function returns an object of the class &quot;dist&quot;.
<br />
</p>


<h3>Author(s)</h3>

<p>Zdenek Sulc. <br /> Contact: <a href="mailto:zdenek.sulc@vse.cz">zdenek.sulc@vse.cz</a>
</p>


<h3>References</h3>

<p>Boriah S., Chandola V., Kumar V. (2008). Similarity measures for categorical data: A comparative evaluation.
In: Proceedings of the 8th SIAM International Conference on Data Mining, SIAM, p. 243-254.
<br />
<br />
Sulc Z. and Rezankova H. (2019). Comparison of Similarity Measures for Categorical Data in Hierarchical Clustering. Journal of Classification. 2019, 35(1), p. 58-72. DOI: 10.1007/s00357-019-09317-5.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+anderberg">anderberg</a></code>,
<code><a href="#topic+burnaby">burnaby</a></code>,
<code><a href="#topic+eskin">eskin</a></code>,
<code><a href="#topic+gambaryan">gambaryan</a></code>,
<code><a href="#topic+goodall1">goodall1</a></code>,
<code><a href="#topic+goodall2">goodall2</a></code>,
<code><a href="#topic+goodall3">goodall3</a></code>,
<code><a href="#topic+goodall4">goodall4</a></code>,
<code><a href="#topic+iof">iof</a></code>,
<code><a href="#topic+lin">lin</a></code>,
<code><a href="#topic+lin1">lin1</a></code>,
<code><a href="#topic+of">of</a></code>,
<code><a href="#topic+sm">sm</a></code>,
<code><a href="#topic+smirnov">smirnov</a></code>,
<code><a href="#topic+vm">vm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># sample data
data(data20)

# dissimilarity matrix calculation
prox.ve &lt;- ve(data20)

# dissimilarity matrix calculation with variable weighting
prox.ve.2 &lt;- ve(data20, var.weights = c(1, 0.8, 0.6, 0.4, 0.2))

# dissimilarity matrix calculation with variable weights
weights.ve &lt;- ve(data20, var.weights = c(0.7, 1, 0.9, 0.5, 0))

</code></pre>

<hr>
<h2 id='vm'>Variable Mutability (VM) measure</h2><span id='topic+vm'></span>

<h3>Description</h3>

<p>The function calculates a dissimilarity matrix based on the VM similarity measure.
<br />
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vm(data, var.weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vm_+3A_data">data</code></td>
<td>
<p>A data.frame or a matrix with cases in rows and variables in columns.</p>
</td></tr>
<tr><td><code id="vm_+3A_var.weights">var.weights</code></td>
<td>
<p>A numeric vector setting weights to the used variables. One can choose the real numbers from zero to one.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Variable Mutability similarity measure was introduced in (Sulc and Rezankova, 2019).
It treats the similarity between two categories based on the within-cluster variability expressed by the normalized mutability. The measure assigns higher weights to rarer categories.
</p>


<h3>Value</h3>

<p>The function returns an object of the class &quot;dist&quot;.
<br />
</p>


<h3>Author(s)</h3>

<p>Zdenek Sulc. <br /> Contact: <a href="mailto:zdenek.sulc@vse.cz">zdenek.sulc@vse.cz</a>
</p>


<h3>References</h3>

<p>Sulc Z. and Rezankova H. (2019). Comparison of Similarity Measures for Categorical Data in Hierarchical Clustering. Journal of Classification. 2019, 35(1), p. 58-72. DOI: 10.1007/s00357-019-09317-5.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+anderberg">anderberg</a></code>,
<code><a href="#topic+burnaby">burnaby</a></code>,
<code><a href="#topic+eskin">eskin</a></code>,
<code><a href="#topic+gambaryan">gambaryan</a></code>,
<code><a href="#topic+goodall1">goodall1</a></code>,
<code><a href="#topic+goodall2">goodall2</a></code>,
<code><a href="#topic+goodall3">goodall3</a></code>,
<code><a href="#topic+goodall4">goodall4</a></code>,
<code><a href="#topic+iof">iof</a></code>,
<code><a href="#topic+lin">lin</a></code>,
<code><a href="#topic+lin1">lin1</a></code>,
<code><a href="#topic+of">of</a></code>,
<code><a href="#topic+sm">sm</a></code>,
<code><a href="#topic+smirnov">smirnov</a></code>,
<code><a href="#topic+ve">ve</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#sample data
data(data20)

# dissimilarity matrix calculation
prox.vm &lt;- vm(data20)

# dissimilarity matrix calculation with variable weights
weights.vm &lt;- vm(data20, var.weights = c(0.7, 1, 0.9, 0.5, 0)) 

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
