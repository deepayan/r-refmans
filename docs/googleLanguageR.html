<!DOCTYPE html><html><head><title>Help for package googleLanguageR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {googleLanguageR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#gl_auth'><p>Authenticate with Google language API services</p></a></li>
<li><a href='#gl_nlp'><p>Perform Natural Language Analysis</p></a></li>
<li><a href='#gl_speech'><p>Call Google Speech API</p></a></li>
<li><a href='#gl_speech_op'><p>Get a speech operation</p></a></li>
<li><a href='#gl_talk'><p>Perform text to speech</p></a></li>
<li><a href='#gl_talk_languages'><p>Get a list of voices available for text to speech</p></a></li>
<li><a href='#gl_talk_player'><p>Play audio in a browser</p></a></li>
<li><a href='#gl_talk_shiny'><p>Speak in Shiny module (server)</p></a></li>
<li><a href='#gl_talk_shinyUI'><p>Speak in Shiny module (ui)</p></a></li>
<li><a href='#gl_translate'><p>Translate the language of text within a request</p></a></li>
<li><a href='#gl_translate_detect'><p>Detect the language of text within a request</p></a></li>
<li><a href='#gl_translate_languages'><p>Lists languages from Google Translate API</p></a></li>
<li><a href='#googleLanguageR'><p>googleLanguageR</p></a></li>
<li><a href='#is.NullOb'><p>A helper function that tests whether an object is either NULL _or_</p>
a list of NULLs</a></li>
<li><a href='#rmNullObs'><p>Recursively step down into list, removing all such objects</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Call Google's 'Natural Language' API, 'Cloud Translation' API,
'Cloud Speech' API and 'Cloud Text-to-Speech' API</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Call 'Google Cloud' machine learning APIs for text and speech tasks.
  Call the 'Cloud Translation' API <a href="https://cloud.google.com/translate/">https://cloud.google.com/translate/</a> for detection 
  and translation of text, the 'Natural Language' API <a href="https://cloud.google.com/natural-language/">https://cloud.google.com/natural-language/</a> to 
  analyse text for sentiment, entities or syntax, the 'Cloud Speech' API 
  <a href="https://cloud.google.com/speech/">https://cloud.google.com/speech/</a> to transcribe sound files to text and 
  the 'Cloud Text-to-Speech' API <a href="https://cloud.google.com/text-to-speech/">https://cloud.google.com/text-to-speech/</a> to turn text 
  into sound files.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://code.markedmondson.me/googleLanguageR/">http://code.markedmondson.me/googleLanguageR/</a>,
<a href="https://github.com/ropensci/googleLanguageR">https://github.com/ropensci/googleLanguageR</a>,
<a href="https://docs.ropensci.org/googleLanguageR/">https://docs.ropensci.org/googleLanguageR/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ropensci/googleLanguageR/issues">https://github.com/ropensci/googleLanguageR/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.3)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.0</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Imports:</td>
<td>assertthat, base64enc, googleAuthR (&ge; 1.1.1), jsonlite,
magrittr, purrr (&ge; 0.2.4), stats, tibble, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>cld2, testthat, knitr, rmarkdown, rvest, shiny, shinyjs,
stringdist, tidyr, tuneR, xml2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-04-19 13:14:01 UTC; mark</td>
</tr>
<tr>
<td>Author:</td>
<td>Mark Edmondson [aut, cre],
  John Muschelli [ctb],
  Neal Richardson [rev] (Neal reviewed the package for ropensci, see
    &lt;https://github.com/ropensci/onboarding/issues/127&gt;),
  Julia Gustavsen [rev] (Julia reviewed the package for ropensci, see
    &lt;https://github.com/ropensci/onboarding/issues/127&gt;)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Mark Edmondson &lt;r@sunholo.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-04-19 13:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='gl_auth'>Authenticate with Google language API services</h2><span id='topic+gl_auth'></span><span id='topic+gl_auto_auth'></span>

<h3>Description</h3>

<p>Authenticate with Google language API services
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gl_auth(json_file)

gl_auto_auth(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gl_auth_+3A_json_file">json_file</code></td>
<td>
<p>Authentication json file you have downloaded from your Google Project</p>
</td></tr>
<tr><td><code id="gl_auth_+3A_...">...</code></td>
<td>
<p>additional argument to
pass to <code><a href="googleAuthR.html#topic+gar_attach_auto_auth">gar_attach_auto_auth</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The best way to authenticate is to use an environment argument pointing at your authentication file.
</p>
<p>Set the file location of your download Google Project JSON file in a <code>GL_AUTH</code> argument
</p>
<p>Then, when you load the library you should auto-authenticate
</p>
<p>However, you can authenticate directly using this function pointing at your JSON auth file.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
library(googleLanguageR)
gl_auth("location_of_json_file.json")

## End(Not run)

## Not run: 
library(googleLanguageR)
gl_auto_auth()
gl_auto_auth(environment_var = "GAR_AUTH_FILE")

## End(Not run)
</code></pre>

<hr>
<h2 id='gl_nlp'>Perform Natural Language Analysis</h2><span id='topic+gl_nlp'></span>

<h3>Description</h3>

<p>Analyse text entities, sentiment, syntax and categorisation using the Google Natural Language API
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gl_nlp(
  string,
  nlp_type = c("annotateText", "analyzeEntities", "analyzeSentiment", "analyzeSyntax",
    "analyzeEntitySentiment", "classifyText"),
  type = c("PLAIN_TEXT", "HTML"),
  language = c("en", "zh", "zh-Hant", "fr", "de", "it", "ja", "ko", "pt", "es"),
  encodingType = c("UTF8", "UTF16", "UTF32", "NONE")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gl_nlp_+3A_string">string</code></td>
<td>
<p>A vector of text to detect language for, or Google Cloud Storage URI(s)</p>
</td></tr>
<tr><td><code id="gl_nlp_+3A_nlp_type">nlp_type</code></td>
<td>
<p>The type of Natural Language Analysis to perform.  The default <code>annotateText</code> will perform all features in one call.</p>
</td></tr>
<tr><td><code id="gl_nlp_+3A_type">type</code></td>
<td>
<p>Whether input text is plain text or a HTML page</p>
</td></tr>
<tr><td><code id="gl_nlp_+3A_language">language</code></td>
<td>
<p>Language of source, must be supported by API.</p>
</td></tr>
<tr><td><code id="gl_nlp_+3A_encodingtype">encodingType</code></td>
<td>
<p>Text encoding that the caller uses to process the output</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>string</code> can be a character vector, or a location of a file content on Google cloud Storage.
This URI must be of the form <code>gs://bucket_name/object_name</code>
</p>
<p>Encoding type can usually be left at default <code>UTF8</code>.
<a href="https://cloud.google.com/natural-language/docs/reference/rest/v1/EncodingType">Read more here</a>
</p>
<p>The current language support is available <a href="https://cloud.google.com/natural-language/docs/languages">here</a>
</p>


<h3>Value</h3>

<p>A list of the following objects, if those fields are asked for via <code>nlp_type</code>:
</p>

<ul>
<li><p>sentences - <a href="https://cloud.google.com/natural-language/docs/reference/rest/v1/Sentence">Sentences in the input document</a>
</p>
</li>
<li><p>tokens - <a href="https://cloud.google.com/natural-language/docs/reference/rest/v1/Token">Tokens, along with their syntactic information, in the input document</a>
</p>
</li>
<li><p>entities - <a href="https://cloud.google.com/natural-language/docs/reference/rest/v1/Entity">Entities, along with their semantic information, in the input document</a>
</p>
</li>
<li><p>documentSentiment - <a href="https://cloud.google.com/natural-language/docs/reference/rest/v1/Sentiment">The overall sentiment for the document</a>
</p>
</li>
<li><p>classifyText -<a href="https://cloud.google.com/natural-language/docs/classifying-text">Classification of the document</a>
</p>
</li>
<li><p>language - The language of the text, which will be the same as the language specified in the request or, if not specified, the automatically-detected language
</p>
</li>
<li><p>text - The original text passed into the API. <code>NA</code> if not passed due to being zero-length etc. 
</p>
</li></ul>



<h3>See Also</h3>

<p><a href="https://cloud.google.com/natural-language/docs/reference/rest/v1/documents">https://cloud.google.com/natural-language/docs/reference/rest/v1/documents</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

text &lt;- "to administer medicince to animals is frequently a very difficult matter,
  and yet sometimes it's necessary to do so"
nlp &lt;- gl_nlp(text)

nlp$sentences

nlp$tokens

nlp$entities

nlp$documentSentiment

## vectorised input
texts &lt;- c("The cat sat one the mat", "oh no it didn't you fool")
nlp_results &lt;- gl_nlp(texts)




## End(Not run)

</code></pre>

<hr>
<h2 id='gl_speech'>Call Google Speech API</h2><span id='topic+gl_speech'></span>

<h3>Description</h3>

<p>Turn audio into text
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gl_speech(
  audio_source,
  encoding = c("LINEAR16", "FLAC", "MULAW", "AMR", "AMR_WB", "OGG_OPUS",
    "SPEEX_WITH_HEADER_BYTE"),
  sampleRateHertz = NULL,
  languageCode = "en-US",
  maxAlternatives = 1L,
  profanityFilter = FALSE,
  speechContexts = NULL,
  asynch = FALSE,
  customConfig = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gl_speech_+3A_audio_source">audio_source</code></td>
<td>
<p>File location of audio data, or Google Cloud Storage URI</p>
</td></tr>
<tr><td><code id="gl_speech_+3A_encoding">encoding</code></td>
<td>
<p>Encoding of audio data sent</p>
</td></tr>
<tr><td><code id="gl_speech_+3A_sampleratehertz">sampleRateHertz</code></td>
<td>
<p>Sample rate in Hertz of audio data. Valid values <code>8000-48000</code>. Optimal and default if left <code>NULL</code> is <code>16000</code></p>
</td></tr>
<tr><td><code id="gl_speech_+3A_languagecode">languageCode</code></td>
<td>
<p>Language of the supplied audio as a <code>BCP-47</code> language tag</p>
</td></tr>
<tr><td><code id="gl_speech_+3A_maxalternatives">maxAlternatives</code></td>
<td>
<p>Maximum number of recognition hypotheses to be returned. <code>0-30</code></p>
</td></tr>
<tr><td><code id="gl_speech_+3A_profanityfilter">profanityFilter</code></td>
<td>
<p>If <code>TRUE</code> will attempt to filter out profanities</p>
</td></tr>
<tr><td><code id="gl_speech_+3A_speechcontexts">speechContexts</code></td>
<td>
<p>An optional character vector of context to assist the speech recognition</p>
</td></tr>
<tr><td><code id="gl_speech_+3A_asynch">asynch</code></td>
<td>
<p>If your <code>audio_source</code> is greater than 60 seconds, set this to TRUE to return an asynchronous call</p>
</td></tr>
<tr><td><code id="gl_speech_+3A_customconfig">customConfig</code></td>
<td>
<p>[optional] A <code>RecognitionConfig</code> object that will be converted from a list to JSON via <code><a href="jsonlite.html#topic+toJSON">toJSON</a></code> - see <a href="https://cloud.google.com/speech-to-text/docs/reference/rest/v1p1beta1/RecognitionConfig">RecognitionConfig documentation</a>. The <code>languageCode</code> will be taken from this functions arguments if not present since it is required.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Google Cloud Speech API enables developers to convert audio to text by applying powerful
neural network models in an easy to use API.
The API recognizes over 80 languages and variants, to support your global user base.
You can transcribe the text of users dictating to an application’s microphone,
enable command-and-control through voice, or transcribe audio files, among many other use cases.
Recognize audio uploaded in the request, and integrate with your audio storage on Google Cloud Storage,
by using the same technology Google uses to power its own products.
</p>


<h3>Value</h3>

<p>A list of two tibbles:  <code>$transcript</code>, a tibble of the <code>transcript</code> with a <code>confidence</code>; <code>$timings</code>, a tibble that contains <code>startTime</code>, <code>endTime</code> per <code>word</code>.  If maxAlternatives is greater than 1, then the transcript will return near-duplicate rows with other interpretations of the text.
If <code>asynch</code> is TRUE, then an operation you will need to pass to <a href="#topic+gl_speech_op">gl_speech_op</a> to get the finished result.
</p>


<h3>AudioEncoding</h3>

<p>Audio encoding of the data sent in the audio message. All encodings support only 1 channel (mono) audio.
Only FLAC and WAV include a header that describes the bytes of audio that follow the header.
The other encodings are raw audio bytes with no header.
For best results, the audio source should be captured and transmitted using a
lossless encoding (FLAC or LINEAR16).
Recognition accuracy may be reduced if lossy codecs, which include the other codecs listed in this section,
are used to capture or transmit the audio, particularly if background noise is present.
</p>
<p>Read more on audio encodings here <a href="https://cloud.google.com/speech/docs/encoding">https://cloud.google.com/speech/docs/encoding</a>
</p>


<h3>WordInfo</h3>

<p><code>startTime</code> - Time offset relative to the beginning of the audio, and corresponding to the start of the spoken word.
</p>
<p><code>endTime</code> - Time offset relative to the beginning of the audio, and corresponding to the end of the spoken word.
</p>
<p><code>word</code> - The word corresponding to this set of information.
</p>


<h3>See Also</h3>

<p><a href="https://cloud.google.com/speech/reference/rest/v1/speech/recognize">https://cloud.google.com/speech/reference/rest/v1/speech/recognize</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

test_audio &lt;- system.file("woman1_wb.wav", package = "googleLanguageR")
result &lt;- gl_speech(test_audio)

result$transcript
result$timings

result2 &lt;- gl_speech(test_audio, maxAlternatives = 2L)
result2$transcript

result_brit &lt;- gl_speech(test_audio, languageCode = "en-GB")


## make an asynchronous API request (mandatory for sound files over 60 seconds)
asynch &lt;- gl_speech(test_audio, asynch = TRUE)

## Send to gl_speech_op() for status or finished result
gl_speech_op(asynch)

## Upload to GCS bucket for long files &gt; 60 seconds
test_gcs &lt;- "gs://mark-edmondson-public-files/googleLanguageR/a-dream-mono.wav"
gcs &lt;- gl_speech(test_gcs, sampleRateHertz = 44100L, asynch = TRUE)
gl_speech_op(gcs)

## Use a custom configuration
my_config &lt;- list(encoding = "LINEAR16",
                  diarizationConfig = list(
                    enableSpeakerDiarization = TRUE,
                    minSpeakerCount = 2,
                    maxSpeakCount = 3
                    ))

# languageCode is required, so will be added if not in your custom config
gl_speech(my_audio, languageCode = "en-US", customConfig = my_config)


## End(Not run)



</code></pre>

<hr>
<h2 id='gl_speech_op'>Get a speech operation</h2><span id='topic+gl_speech_op'></span>

<h3>Description</h3>

<p>For asynchronous calls of audio over 60 seconds, this returns the finished job
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gl_speech_op(operation = .Last.value)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gl_speech_op_+3A_operation">operation</code></td>
<td>
<p>A speech operation object from <a href="#topic+gl_speech">gl_speech</a> when <code>asynch = TRUE</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>If the operation is still running, another operation object.  If done, the result as per <a href="#topic+gl_speech">gl_speech</a>
</p>


<h3>See Also</h3>

<p><a href="#topic+gl_speech">gl_speech</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

test_audio &lt;- system.file("woman1_wb.wav", package = "googleLanguageR")

## make an asynchronous API request (mandatory for sound files over 60 seconds)
asynch &lt;- gl_speech(test_audio, asynch = TRUE)

## Send to gl_speech_op() for status or finished result
gl_speech_op(asynch)


## End(Not run)

</code></pre>

<hr>
<h2 id='gl_talk'>Perform text to speech</h2><span id='topic+gl_talk'></span>

<h3>Description</h3>

<p>Synthesizes speech synchronously: receive results after all text input has been processed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gl_talk(
  input,
  output = "output.wav",
  languageCode = "en",
  gender = c("SSML_VOICE_GENDER_UNSPECIFIED", "MALE", "FEMALE", "NEUTRAL"),
  name = NULL,
  audioEncoding = c("LINEAR16", "MP3", "OGG_OPUS"),
  speakingRate = 1,
  pitch = 0,
  volumeGainDb = 0,
  sampleRateHertz = NULL,
  inputType = c("text", "ssml"),
  effectsProfileIds = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gl_talk_+3A_input">input</code></td>
<td>
<p>The text to turn into speech</p>
</td></tr>
<tr><td><code id="gl_talk_+3A_output">output</code></td>
<td>
<p>Where to save the speech audio file</p>
</td></tr>
<tr><td><code id="gl_talk_+3A_languagecode">languageCode</code></td>
<td>
<p>The language of the voice as a <code>BCP-47</code> language code</p>
</td></tr>
<tr><td><code id="gl_talk_+3A_gender">gender</code></td>
<td>
<p>The gender of the voice, if available</p>
</td></tr>
<tr><td><code id="gl_talk_+3A_name">name</code></td>
<td>
<p>Name of the voice, see list via <a href="#topic+gl_talk_languages">gl_talk_languages</a> for supported voices.  Set to <code>NULL</code> to make the service choose a voice based on <code>languageCode</code> and <code>gender</code>.</p>
</td></tr>
<tr><td><code id="gl_talk_+3A_audioencoding">audioEncoding</code></td>
<td>
<p>Format of the requested audio stream</p>
</td></tr>
<tr><td><code id="gl_talk_+3A_speakingrate">speakingRate</code></td>
<td>
<p>Speaking rate/speed between <code>0.25</code> and <code>4.0</code></p>
</td></tr>
<tr><td><code id="gl_talk_+3A_pitch">pitch</code></td>
<td>
<p>Speaking pitch between <code>-20.0</code> and <code>20.0</code> in semitones.</p>
</td></tr>
<tr><td><code id="gl_talk_+3A_volumegaindb">volumeGainDb</code></td>
<td>
<p>Volumne gain in dB</p>
</td></tr>
<tr><td><code id="gl_talk_+3A_sampleratehertz">sampleRateHertz</code></td>
<td>
<p>Sample rate for returned audio</p>
</td></tr>
<tr><td><code id="gl_talk_+3A_inputtype">inputType</code></td>
<td>
<p>Choose between <code>text</code> (the default) or SSML markup. The <code>input</code> text must be SSML markup if you choose <code>ssml</code></p>
</td></tr>
<tr><td><code id="gl_talk_+3A_effectsprofileids">effectsProfileIds</code></td>
<td>
<p>Optional. An identifier which selects 'audio effects' profiles that are applied on (post synthesized) text to speech. Effects are applied on top of each other in the order they are given</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Requires the Cloud Text-To-Speech API to be activated for your Google Cloud project.
</p>
<p>Supported voices are here <a href="https://cloud.google.com/text-to-speech/docs/voices">https://cloud.google.com/text-to-speech/docs/voices</a> and can be imported into R via <a href="#topic+gl_talk_languages">gl_talk_languages</a>
</p>
<p>To play the audio in code via a browser see <a href="#topic+gl_talk_player">gl_talk_player</a>
</p>
<p>To use Speech Synthesis Markup Language (SSML) select <code>inputType=ssml</code> - more details on using this to insert pauses, sounds and breaks in your audio can be found here: <a href="https://cloud.google.com/text-to-speech/docs/ssml">https://cloud.google.com/text-to-speech/docs/ssml</a>
</p>
<p>To use audio profiles, supply a character vector of the available audio profiles listed here: <a href="https://cloud.google.com/text-to-speech/docs/audio-profiles">https://cloud.google.com/text-to-speech/docs/audio-profiles</a> - the audio profiles are applied in the order given.  For instance <code>effectsProfileIds="wearable-class-device"</code> will optimise output for smart watches, <code>effectsProfileIds=c("wearable-class-device","telephony-class-application")</code> will apply sound filters optimised for smart watches, then telephonic devices.
</p>


<h3>Value</h3>

<p>The file output name you supplied as <code>output</code>
</p>


<h3>See Also</h3>

<p><a href="https://cloud.google.com/text-to-speech/docs/">https://cloud.google.com/text-to-speech/docs/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
library(magrittr)
gl_talk("The rain in spain falls mainly in the plain",
        output = "output.wav")

gl_talk("Testing my new audio player") %&gt;% gl_talk_player()

# using SSML
gl_talk('&lt;speak&gt;The &lt;say-as interpret-as=\"characters\"&gt;SSML&lt;/say-as&gt;
  standard &lt;break time=\"1s\"/&gt;is defined by the
  &lt;sub alias=\"World Wide Web Consortium\"&gt;W3C&lt;/sub&gt;.&lt;/speak&gt;',
  inputType =  "ssml")

# using effects profiles
gl_talk("This sounds great on headphones",
        effectsProfileIds = "headphone-class-device")


## End(Not run)

</code></pre>

<hr>
<h2 id='gl_talk_languages'>Get a list of voices available for text to speech</h2><span id='topic+gl_talk_languages'></span>

<h3>Description</h3>

<p>Returns a list of voices supported for synthesis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gl_talk_languages(languageCode = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gl_talk_languages_+3A_languagecode">languageCode</code></td>
<td>
<p>A <code>BCP-47</code> language tag.  If specified, will only return voices that can be used to synthesize this languageCode</p>
</td></tr>
</table>

<hr>
<h2 id='gl_talk_player'>Play audio in a browser</h2><span id='topic+gl_talk_player'></span>

<h3>Description</h3>

<p>This uses HTML5 audio tags to play audio in your browser
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gl_talk_player(audio = "output.wav", html = "player.html")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gl_talk_player_+3A_audio">audio</code></td>
<td>
<p>The file location of the audio file.  Must be supported by HTML5</p>
</td></tr>
<tr><td><code id="gl_talk_player_+3A_html">html</code></td>
<td>
<p>The html file location that will be created host the audio</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A platform neutral way to play audio is not easy, so this uses your browser to play it instead.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

gl_talk("Testing my new audio player") %&gt;% gl_talk_player()


## End(Not run)

</code></pre>

<hr>
<h2 id='gl_talk_shiny'>Speak in Shiny module (server)</h2><span id='topic+gl_talk_shiny'></span>

<h3>Description</h3>

<p>Call via <code>shiny::callModule(gl_talk_shiny, "your_id")</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gl_talk_shiny(
  input,
  output,
  session,
  transcript,
  ...,
  autoplay = TRUE,
  controls = TRUE,
  loop = FALSE,
  keep_wav = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gl_talk_shiny_+3A_input">input</code></td>
<td>
<p>shiny input</p>
</td></tr>
<tr><td><code id="gl_talk_shiny_+3A_output">output</code></td>
<td>
<p>shiny output</p>
</td></tr>
<tr><td><code id="gl_talk_shiny_+3A_session">session</code></td>
<td>
<p>shiny session</p>
</td></tr>
<tr><td><code id="gl_talk_shiny_+3A_transcript">transcript</code></td>
<td>
<p>The (reactive) text to talk</p>
</td></tr>
<tr><td><code id="gl_talk_shiny_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+gl_talk">gl_talk</a></code>
</p>

<dl>
<dt><code>languageCode</code></dt><dd><p>The language of the voice as a <code>BCP-47</code> language code</p>
</dd>
<dt><code>name</code></dt><dd><p>Name of the voice, see list via <a href="#topic+gl_talk_languages">gl_talk_languages</a> for supported voices.  Set to <code>NULL</code> to make the service choose a voice based on <code>languageCode</code> and <code>gender</code>.</p>
</dd>
<dt><code>gender</code></dt><dd><p>The gender of the voice, if available</p>
</dd>
<dt><code>audioEncoding</code></dt><dd><p>Format of the requested audio stream</p>
</dd>
<dt><code>speakingRate</code></dt><dd><p>Speaking rate/speed between <code>0.25</code> and <code>4.0</code></p>
</dd>
<dt><code>pitch</code></dt><dd><p>Speaking pitch between <code>-20.0</code> and <code>20.0</code> in semitones.</p>
</dd>
<dt><code>volumeGainDb</code></dt><dd><p>Volumne gain in dB</p>
</dd>
<dt><code>sampleRateHertz</code></dt><dd><p>Sample rate for returned audio</p>
</dd>
<dt><code>inputType</code></dt><dd><p>Choose between <code>text</code> (the default) or SSML markup. The <code>input</code> text must be SSML markup if you choose <code>ssml</code></p>
</dd>
<dt><code>effectsProfileIds</code></dt><dd><p>Optional. An identifier which selects 'audio effects' profiles that are applied on (post synthesized) text to speech. Effects are applied on top of each other in the order they are given</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="gl_talk_shiny_+3A_autoplay">autoplay</code></td>
<td>
<p>passed to the HTML audio player - default <code>TRUE</code> plays on load</p>
</td></tr>
<tr><td><code id="gl_talk_shiny_+3A_controls">controls</code></td>
<td>
<p>passed to the HTML audio player - default <code>TRUE</code> shows controls</p>
</td></tr>
<tr><td><code id="gl_talk_shiny_+3A_loop">loop</code></td>
<td>
<p>passed to the HTML audio player - default <code>FALSE</code> does not loop</p>
</td></tr>
<tr><td><code id="gl_talk_shiny_+3A_keep_wav">keep_wav</code></td>
<td>
<p>keep the generated wav files if TRUE.</p>
</td></tr>
</table>

<hr>
<h2 id='gl_talk_shinyUI'>Speak in Shiny module (ui)</h2><span id='topic+gl_talk_shinyUI'></span>

<h3>Description</h3>

<p>Speak in Shiny module (ui)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gl_talk_shinyUI(id)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gl_talk_shinyUI_+3A_id">id</code></td>
<td>
<p>The Shiny id</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Shiny Module for use with <a href="#topic+gl_talk_shiny">gl_talk_shiny</a>.
</p>

<hr>
<h2 id='gl_translate'>Translate the language of text within a request</h2><span id='topic+gl_translate'></span>

<h3>Description</h3>

<p>Translate character vectors via the Google Translate API
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gl_translate(
  t_string,
  target = "en",
  format = c("text", "html"),
  source = "",
  model = c("nmt", "base")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gl_translate_+3A_t_string">t_string</code></td>
<td>
<p>A character vector of text to detect language for</p>
</td></tr>
<tr><td><code id="gl_translate_+3A_target">target</code></td>
<td>
<p>The target language</p>
</td></tr>
<tr><td><code id="gl_translate_+3A_format">format</code></td>
<td>
<p>Whether the text is plain or HTML</p>
</td></tr>
<tr><td><code id="gl_translate_+3A_source">source</code></td>
<td>
<p>Specify the language to translate from. Will detect it if left default</p>
</td></tr>
<tr><td><code id="gl_translate_+3A_model">model</code></td>
<td>
<p>What translation model to use</p>
</td></tr>
</table>


<h3>Details</h3>

<p>You can translate a vector of strings, although if too many for one call then it will be
broken up into one API call per element.
This is the same cost as charging is per character translated, but will take longer.
</p>
<p>If translating HTML set the <code>format = "html"</code>.
Consider removing anything not needed to be translated first,
such as JavaScript and CSS scripts. See example on how to do this with <code>rvest</code>
</p>
<p>The API limits in three ways: characters per day, characters per 100 seconds,
and API requests per 100 seconds.
All can be set in the API manager
<a href="https://console.developers.google.com/apis/api/translate.googleapis.com/quotas">https://console.developers.google.com/apis/api/translate.googleapis.com/quotas</a>
</p>


<h3>Value</h3>

<p>A tibble of <code>translatedText</code> and <code>detectedSourceLanguage</code>
and <code>text</code> of length equal to the vector of text you passed in.
</p>


<h3>See Also</h3>

<p><a href="https://cloud.google.com/translate/docs/reference/translate">https://cloud.google.com/translate/docs/reference/translate</a>
</p>
<p>Other translations: 
<code><a href="#topic+gl_translate_detect">gl_translate_detect</a>()</code>,
<code><a href="#topic+gl_translate_languages">gl_translate_languages</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

text &lt;- "to administer medicine to animals is frequently a very difficult matter,
  and yet sometimes it's necessary to do so"

gl_translate(text, target = "ja")

# translate webpages using rvest to process beforehand
library(rvest)
library(googleLanguageR)

# translate webpages

# dr.dk article
my_url &lt;- "http://bit.ly/2yhrmrH"

## in this case the content to translate is in css selector '.wcms-article-content'
read_html(my_url) %&gt;%
  html_node(css = ".wcms-article-content") %&gt;%
  html_text %&gt;%
  gl_translate(format = "html")


## End(Not run)

</code></pre>

<hr>
<h2 id='gl_translate_detect'>Detect the language of text within a request</h2><span id='topic+gl_translate_detect'></span>

<h3>Description</h3>

<p>Detect the language of text within a request
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gl_translate_detect(string)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gl_translate_detect_+3A_string">string</code></td>
<td>
<p>A character vector of text to detect language for</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Consider using <code>library(cld2)</code> and <code>cld2::detect_language</code> instead offline,
since that is free and local without needing a paid API call.
</p>
<p><a href="#topic+gl_translate">gl_translate</a> also returns a detection of the language,
so you could also wish to do it in one step via that function.
</p>


<h3>Value</h3>

<p>A tibble of the detected languages with columns <code>confidence</code>, <code>isReliable</code>, <code>language</code>, and <code>text</code> of length equal to the vector of text you passed in.
</p>


<h3>See Also</h3>

<p><a href="https://cloud.google.com/translate/docs/reference/detect">https://cloud.google.com/translate/docs/reference/detect</a>
</p>
<p>Other translations: 
<code><a href="#topic+gl_translate_languages">gl_translate_languages</a>()</code>,
<code><a href="#topic+gl_translate">gl_translate</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

gl_translate_detect("katten sidder på måtten")
# Detecting language: 39 characters - katten sidder på måtten...
# confidence isReliable language                    text
# 1   0.536223      FALSE       da katten sidder på måtten



## End(Not run)

</code></pre>

<hr>
<h2 id='gl_translate_languages'>Lists languages from Google Translate API</h2><span id='topic+gl_translate_languages'></span>

<h3>Description</h3>

<p>Returns a list of supported languages for translation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gl_translate_languages(target = "en")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gl_translate_languages_+3A_target">target</code></td>
<td>
<p>If specified, language names are localized in target language</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Supported language codes, generally consisting of its ISO 639-1 identifier. (E.g. <code>'en', 'ja'</code>).
In certain cases, BCP-47 codes including language + region identifiers are returned (e.g. <code>'zh-TW', 'zh-CH'</code>)
</p>


<h3>Value</h3>

<p>A tibble of supported languages
</p>


<h3>See Also</h3>

<p><a href="https://cloud.google.com/translate/docs/reference/languages">https://cloud.google.com/translate/docs/reference/languages</a>
</p>
<p>Other translations: 
<code><a href="#topic+gl_translate_detect">gl_translate_detect</a>()</code>,
<code><a href="#topic+gl_translate">gl_translate</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 

# default english names of languages supported
gl_translate_languages()

# specify a language code to get other names, such as Danish
gl_translate_languages("da")


## End(Not run)
</code></pre>

<hr>
<h2 id='googleLanguageR'>googleLanguageR</h2><span id='topic+googleLanguageR'></span>

<h3>Description</h3>

<p>This package contains functions for analysing language through the
Google Cloud Machine Learning APIs
</p>


<h3>Details</h3>

<p>For examples and documentation see the vignettes and the website:
</p>
<p><a href="http://code.markedmondson.me/googleLanguageR/">http://code.markedmondson.me/googleLanguageR/</a>
</p>


<h3>See Also</h3>

<p><a href="https://cloud.google.com/products/machine-learning/">https://cloud.google.com/products/machine-learning/</a>
</p>

<hr>
<h2 id='is.NullOb'>A helper function that tests whether an object is either NULL _or_
a list of NULLs</h2><span id='topic+is.NullOb'></span>

<h3>Description</h3>

<p>A helper function that tests whether an object is either NULL _or_
a list of NULLs
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.NullOb(x)
</code></pre>

<hr>
<h2 id='rmNullObs'>Recursively step down into list, removing all such objects</h2><span id='topic+rmNullObs'></span>

<h3>Description</h3>

<p>Recursively step down into list, removing all such objects
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmNullObs(x)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
