<!DOCTYPE html><html><head><title>Help for package agtboost</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {agtboost}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#agtboost'><p>Adaptive and automatic gradient boosting computations.</p></a></li>
<li><a href='#caravan.train'><p>The Insurance Company (TIC) Benchmark</p>
</a></li>
<li><a href='#gbt.complexity'><p>Return complexity of model in terms of hyperparameters.</p></a></li>
<li><a href='#gbt.convergence'><p>Convergence of agtboost model.</p></a></li>
<li><a href='#gbt.importance'><p>Importance of features in a model.</p></a></li>
<li><a href='#gbt.ksval'><p>Kolmogorov-Smirnov validation of model</p></a></li>
<li><a href='#gbt.load'><p>Load an aGTBoost Model</p></a></li>
<li><a href='#gbt.save'><p>Save an aGTBoost Model</p></a></li>
<li><a href='#gbt.train'><p>aGTBoost Training.</p></a></li>
<li><a href='#predict.Rcpp_ENSEMBLE'><p>aGTBoost Prediction</p></a></li>
<li><a href='#predict.Rcpp_GBT_COUNT_AUTO'><p>aGTBoost Count-Regression Auto Prediction</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Adaptive and Automatic Gradient Boosting Computations</td>
</tr>
<tr>
<td>Version:</td>
<td>0.9.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-11-23</td>
</tr>
<tr>
<td>Author:</td>
<td>Berent Ånund Strømnes Lunde</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Berent Ånund Strømnes Lunde &lt;lundeberent@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Fast and automatic gradient tree boosting designed
    to avoid manual tuning and cross-validation by utilizing an information
    theoretic approach. This makes the algorithm adaptive to the dataset at
    hand; it is completely automatic, and with minimal worries of overfitting.
    Consequently, the speed-ups relative to state-of-the-art implementations
    can be in the thousands while mathematical and technical knowledge required
    on the user are minimized.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, Rcpp (&ge; 1.0.1)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppEigen</td>
</tr>
<tr>
<td>RcppModules:</td>
<td>aGTBModule</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-11-23 20:53:03 UTC; berentanundstromneslunde</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-11-23 21:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='agtboost'>Adaptive and automatic gradient boosting computations.</h2><span id='topic+agtboost'></span>

<h3>Description</h3>

<p>Adaptive and Automatic Gradient Boosting Computations
</p>


<h3>Details</h3>

<p><span class="pkg">agtboost</span> is a lightning fast gradient boosting library designed to avoid 
manual tuning and cross-validation by utilizing an information theoretic 
approach. This makes the algorithm adaptive to the dataset at hand; it is 
completely automatic, and with minimal worries of overfitting. 
Consequently, the speed-ups relative to state-of-the-art implementations 
are in the thousands while mathematical and technical knowledge required 
on the user are minimized.
</p>
<p>Important functions:
</p>

<ul>
<li> <p><code><a href="#topic+gbt.train">gbt.train</a></code>: function for training an <span class="pkg">agtboost</span> ensemble
</p>
</li>
<li> <p><code><a href="#topic+predict.Rcpp_ENSEMBLE">predict.Rcpp_ENSEMBLE</a></code>: function for predicting from an <span class="pkg">agtboost</span> ensemble
</p>
</li></ul>

<p>See individual function documentation for usage.
</p>


<h3>Author(s)</h3>

<p>Berent Ånund Strømnes Lunde
</p>

<hr>
<h2 id='caravan.train'>The Insurance Company (TIC) Benchmark

</h2><span id='topic+caravan.train'></span><span id='topic+caravan.test'></span>

<h3>Description</h3>

<p><code>caravan.train</code> and <code>caravan.test</code> both contain a design
matrix with 85 columns and a response vector. The train set consists
of 70% of the data, with 4075 rows. The test set consists of the
remaining 30% with 1747 rows. The following references the documentation
within the <span class="pkg">ISLR</span> package: 
The original data contains 5822 real customer records. Each record
consists of 86 variables, containing sociodemographic data (variables
1-43) and product ownership (variables 44-86). The sociodemographic
data is derived from zip codes. All customers living in areas with the
same zip code have the same sociodemographic attributes. Variable 86
(<code>Purchase</code>) indicates whether the customer purchased a caravan
insurance policy. Further information on the individual variables can
be obtained at  http://www.liacs.nl/~putten/library/cc2000/data.html

</p>


<h3>Usage</h3>

<pre><code class='language-R'>caravan.train; caravan.test</code></pre>


<h3>Format</h3>

<p>Lists with a design matrix <code>x</code> and response <code>y</code>
</p>


<h3>Source</h3>

<p>The data was originally supplied by Sentient Machine Research
and was used in the CoIL Challenge 2000.
</p>


<h3>References</h3>

<p>P. van der Putten and M. van Someren (eds) . CoIL Challenge
2000: The Insurance Company Case.  Published by Sentient Machine
Research, Amsterdam. Also a Leiden Institute of Advanced Computer
Science Technical Report 2000-09. June 22, 2000. See
http://www.liacs.nl/~putten/library/cc2000/<br />
P. van der Putten and M. van Someren. A Bias-Variance Analysis of a Real World Learning Problem: The CoIL Challenge 2000. Machine Learning, October 2004, vol. 57, iss. 1-2, pp. 177-195, Kluwer Academic Publishers<br />
James, G., Witten, D., Hastie, T., and Tibshirani, R. (2013)
<em>An Introduction to Statistical Learning with applications in R</em>,
<a href="https://trevorhastie.github.io/ISLR/">https://trevorhastie.github.io/ISLR/</a>,
Springer-Verlag, New York
</p>


<h3>Examples</h3>

<pre><code class='language-R'>summary(caravan.train)
summary(caravan.test)
</code></pre>

<hr>
<h2 id='gbt.complexity'>Return complexity of model in terms of hyperparameters.</h2><span id='topic+gbt.complexity'></span>

<h3>Description</h3>

<p><code>gbt.complexity</code> creates a list of hyperparameters from a model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gbt.complexity(model, type)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gbt.complexity_+3A_model">model</code></td>
<td>
<p>object or pointer to object of class <code>ENSEMBLE</code></p>
</td></tr>
<tr><td><code id="gbt.complexity_+3A_type">type</code></td>
<td>
<p>currently supports &quot;xgboost&quot; or &quot;lightgbm&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Returns the complexity of <code>model</code> in terms of hyperparameters associated
to model <code>type</code>.
</p>


<h3>Value</h3>

<p><code>list</code> with <code>type</code> hyperparameters.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(123)
library(agtboost)
n &lt;- 10000
xtr &lt;- as.matrix(runif(n, 0, 4))
ytr &lt;- rnorm(n, xtr, 1)
xte &lt;- as.matrix(runif(n, 0, 4))
yte &lt;- rnorm(n, xte, 1)
model &lt;- gbt.train(ytr, xtr, learning_rate = 0.1)
gbt.complexity(model, type="xgboost")
gbt.complexity(model, type="lightgbm")
## See demo(topic="gbt-complexity", package="agtboost")


</code></pre>

<hr>
<h2 id='gbt.convergence'>Convergence of agtboost model.</h2><span id='topic+gbt.convergence'></span>

<h3>Description</h3>

<p><code>gbt.convergence</code> calculates loss of data over iterations in the model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gbt.convergence(object, y, x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gbt.convergence_+3A_object">object</code></td>
<td>
<p>Object or pointer to object of class <code>ENSEMBLE</code></p>
</td></tr>
<tr><td><code id="gbt.convergence_+3A_y">y</code></td>
<td>
<p>response vector</p>
</td></tr>
<tr><td><code id="gbt.convergence_+3A_x">x</code></td>
<td>
<p>design matrix for training. Must be of type <code>matrix</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the loss on supplied data at each boosting iterations of the model passed as object.
This may be used to visually test for overfitting on test data, or the converce, to check for underfitting
or non-convergence.
</p>


<h3>Value</h3>

<p><code>vector</code> with $K+1$ elements with loss at each boosting iteration and at the first constant prediction
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Gaussian regression:
x_tr &lt;- as.matrix(runif(500, 0, 4))
y_tr &lt;- rnorm(500, x_tr, 1)
x_te &lt;- as.matrix(runif(500, 0, 4))
y_te &lt;- rnorm(500, x_te, 1)
mod &lt;- gbt.train(y_tr, x_tr)
convergence &lt;- gbt.convergence(mod, y_te, x_te)
which.min(convergence) # Should be fairly similar to boosting iterations + 1
mod$get_num_trees() +1 # num_trees does not include initial prediction

</code></pre>

<hr>
<h2 id='gbt.importance'>Importance of features in a model.</h2><span id='topic+gbt.importance'></span>

<h3>Description</h3>

<p><code>gbt.importance</code> creates a <code>data.frame</code> of feature importance in a model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gbt.importance(feature_names, object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gbt.importance_+3A_feature_names">feature_names</code></td>
<td>
<p>character vector of feature names</p>
</td></tr>
<tr><td><code id="gbt.importance_+3A_object">object</code></td>
<td>
<p>object or pointer to object of class <code>ENSEMBLE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Sums up &quot;expected reduction&quot; in generalization loss (scaled using <code>learning_rate</code>) 
at each node for each tree in the model, and attributes it to 
the feature the node is split on. Returns result in terms of percents.
</p>


<h3>Value</h3>

<p><code>data.frame</code> with percentwise reduction in loss of total attributed to each feature.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Load data
data(caravan.train, package = "agtboost")
train &lt;- caravan.train
mod &lt;- gbt.train(train$y, train$x, loss_function = "logloss", verbose=10)
feature_names &lt;- colnames(train$x)
imp &lt;- gbt.importance(feature_names, mod)
imp


</code></pre>

<hr>
<h2 id='gbt.ksval'>Kolmogorov-Smirnov validation of model</h2><span id='topic+gbt.ksval'></span>

<h3>Description</h3>

<p><code>gbt.ksval</code> transforms observations to U(0,1) if the model
is correct and performs a Kolmogorov-Smirnov test for uniformity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gbt.ksval(object, y, x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gbt.ksval_+3A_object">object</code></td>
<td>
<p>Object or pointer to object of class <code>ENSEMBLE</code></p>
</td></tr>
<tr><td><code id="gbt.ksval_+3A_y">y</code></td>
<td>
<p>Observations to be tested</p>
</td></tr>
<tr><td><code id="gbt.ksval_+3A_x">x</code></td>
<td>
<p>design matrix for training. Must be of type <code>matrix</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Model validation of model passed as <code>object</code> using observations <code>y</code>.
Assuming the loss is a negative log-likelihood and thus a probabilistic model, 
the transformation 
</p>
<p style="text-align: center;"><code class="reqn">u = F_Y(y;x,\theta) \sim U(0,1),</code>
</p>

<p>is usually valid. 
One parameter, <code class="reqn">\mu=g^{-1}(f(x))</code>, is given by the model. Remaining parameters 
are estimated globally over feature space, assuming they are constant.
This then allow the above transformation to be exploited, so that the 
Kolmogorov-Smirnov test for uniformity can be performed.
</p>
<p>If the response is a count model (<code>poisson</code> or <code>negbinom</code>), the transformation
</p>
<p style="text-align: center;"><code class="reqn">u_i = F_Y(y_i-1;x,\theta) + Uf_Y(y_i,x,\theta), ~ U \sim U(0,1)</code>
</p>

<p>is used to obtain a continuous transformation to the unit interval, which, if the model is
correct, will give standard uniform random variables.
</p>


<h3>Value</h3>

<p>Kolmogorov-Smirnov test of model
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Gaussian regression:
x_tr &lt;- as.matrix(runif(500, 0, 4))
y_tr &lt;- rnorm(500, x_tr, 1)
x_te &lt;- as.matrix(runif(500, 0, 4))
y_te &lt;- rnorm(500, x_te, 1)
mod &lt;- gbt.train(y_tr, x_tr)
gbt.ksval(mod, y_te, x_te)

</code></pre>

<hr>
<h2 id='gbt.load'>Load an aGTBoost Model</h2><span id='topic+gbt.load'></span>

<h3>Description</h3>

<p><code>gbt.load</code> is an interface for loading a <span class="pkg">agtboost</span> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gbt.load(file)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gbt.load_+3A_file">file</code></td>
<td>
<p>Valid file-path to a stored aGTBoost model</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The load function for <span class="pkg">agtboost</span>.
Loades a GTB model from a txt file.
</p>


<h3>Value</h3>

<p>Trained aGTBoost model.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gbt.save">gbt.save</a></code>
</p>

<hr>
<h2 id='gbt.save'>Save an aGTBoost Model</h2><span id='topic+gbt.save'></span>

<h3>Description</h3>

<p><code>gbt.save</code> is an interface for storing a <span class="pkg">agtboost</span> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gbt.save(gbt_model, file)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gbt.save_+3A_gbt_model">gbt_model</code></td>
<td>
<p>Model object or pointer to object of class <code>ENSEMBLE</code></p>
</td></tr>
<tr><td><code id="gbt.save_+3A_file">file</code></td>
<td>
<p>Valid file-path</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The model-storage function for <span class="pkg">agtboost</span>.
Saves a GTB model as a txt file. Might be retrieved using <code>gbt.load</code>
</p>


<h3>Value</h3>

<p>Txt file that can be loaded using <code>gbt.load</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gbt.load">gbt.load</a></code>
</p>

<hr>
<h2 id='gbt.train'>aGTBoost Training.</h2><span id='topic+gbt.train'></span>

<h3>Description</h3>

<p><code>gbt.train</code> is an interface for training an <span class="pkg">agtboost</span> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gbt.train(
  y,
  x,
  learning_rate = 0.01,
  loss_function = "mse",
  nrounds = 50000,
  verbose = 0,
  gsub_compare,
  algorithm = "global_subset",
  previous_pred = NULL,
  weights = NULL,
  force_continued_learning = FALSE,
  offset = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gbt.train_+3A_y">y</code></td>
<td>
<p>response vector for training. Must correspond to the design matrix <code>x</code>.</p>
</td></tr>
<tr><td><code id="gbt.train_+3A_x">x</code></td>
<td>
<p>design matrix for training. Must be of type <code>matrix</code>.</p>
</td></tr>
<tr><td><code id="gbt.train_+3A_learning_rate">learning_rate</code></td>
<td>
<p>control the learning rate: scale the contribution of each tree by a factor of <code>0 &lt; learning_rate &lt; 1</code> when it is added to the current approximation. Lower value for <code>learning_rate</code> implies an increase in the number of boosting iterations: low <code>learning_rate</code> value means model more robust to overfitting but slower to compute. Default: 0.01</p>
</td></tr>
<tr><td><code id="gbt.train_+3A_loss_function">loss_function</code></td>
<td>
<p>specify the learning objective (loss function). Only pre-specified loss functions are currently supported.
</p>

<ul>
<li> <p><code>mse</code> regression with squared error loss (Default).
</p>
</li>
<li> <p><code>logloss</code> logistic regression for binary classification, output score before logistic transformation.
</p>
</li>
<li> <p><code>poisson</code> Poisson regression for count data using a log-link, output score before natural transformation.
</p>
</li>
<li> <p><code>gamma::neginv</code> gamma regression using the canonical negative inverse link. Scaling independent of y.
</p>
</li>
<li> <p><code>gamma::log</code> gamma regression using the log-link. Constant information parametrisation. 
</p>
</li>
<li> <p><code>negbinom</code> Negative binomial regression for count data with overdispersion. Log-link.
</p>
</li>
<li> <p><code>count::auto</code> Chooses automatically between Poisson or negative binomial regression.
</p>
</li></ul>
</td></tr>
<tr><td><code id="gbt.train_+3A_nrounds">nrounds</code></td>
<td>
<p>a just-in-case max number of boosting iterations. Default: 50000</p>
</td></tr>
<tr><td><code id="gbt.train_+3A_verbose">verbose</code></td>
<td>
<p>Enable boosting tracing information at i-th iteration? Default: <code>0</code>.</p>
</td></tr>
<tr><td><code id="gbt.train_+3A_gsub_compare">gsub_compare</code></td>
<td>
<p>Deprecated. Boolean: Global-subset comparisons. <code>FALSE</code> means standard GTB, <code>TRUE</code> compare subset-splits with global splits (next root split). Default: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="gbt.train_+3A_algorithm">algorithm</code></td>
<td>
<p>specify the algorithm used for gradient tree boosting.
</p>

<ul>
<li> <p><code>vanilla</code> ordinary gradient tree boosting. Trees are optimized as if they were the last tree.
</p>
</li>
<li> <p><code>global_subset</code> function-change to target maximized reduction in generalization loss for individual datapoints
</p>
</li></ul>
</td></tr>
<tr><td><code id="gbt.train_+3A_previous_pred">previous_pred</code></td>
<td>
<p>prediction vector for training. Boosted training given predictions from another model.</p>
</td></tr>
<tr><td><code id="gbt.train_+3A_weights">weights</code></td>
<td>
<p>weights vector for scaling contributions of individual observations. Default <code>NULL</code> (the unit vector).</p>
</td></tr>
<tr><td><code id="gbt.train_+3A_force_continued_learning">force_continued_learning</code></td>
<td>
<p>Boolean: <code>FALSE</code> (default) stops at information stopping criterion, <code>TRUE</code> stops at <code>nround</code> iterations.</p>
</td></tr>
<tr><td><code id="gbt.train_+3A_offset">offset</code></td>
<td>
<p>add offset to the model g(mu) = offset + F(x).</p>
</td></tr>
<tr><td><code id="gbt.train_+3A_...">...</code></td>
<td>
<p>additional parameters passed.
</p>

<ul>
<li><p> if loss_function is 'negbinom', dispersion must be provided in <code>...</code>
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>These are the training functions for an <span class="pkg">agtboost</span>.
</p>
<p>Explain the philosophy and the algorithm and a little math
</p>
<p><code>gbt.train</code> learn trees with adaptive complexity given by an information criterion, 
until the same (but scaled) information criterion tells the algorithm to stop. The data used 
for training at each boosting iteration stems from a second order Taylor expansion to the loss 
function, evaluated at predictions given by ensemble at the previous boosting iteration.
</p>


<h3>Value</h3>

<p>An object of class <code>ENSEMBLE</code> with some or all of the following elements:
</p>

<ul>
<li> <p><code>handle</code> a handle (pointer) to the <span class="pkg">agtboost</span> model in memory.
</p>
</li>
<li> <p><code>initialPred</code> a field containing the initial prediction of the ensemble.
</p>
</li>
<li> <p><code>set_param</code> function for changing the parameters of the ensemble.
</p>
</li>
<li> <p><code>train</code> function for re-training (or from scratch) the ensemble directly on vector <code>y</code> and design matrix <code>x</code>.
</p>
</li>
<li> <p><code>predict</code> function for predicting observations given a design matrix
</p>
</li>
<li> <p><code>predict2</code> function as above, but takes a parameter max number of boosting ensemble iterations.
</p>
</li>
<li> <p><code>estimate_generalization_loss</code> function for calculating the (approximate) optimism of the ensemble.
</p>
</li>
<li> <p><code>get_num_trees</code> function returning the number of trees in the ensemble.
</p>
</li></ul>



<h3>References</h3>

<p>Berent Ånund Strømnes Lunde, Tore Selland Kleppe and Hans Julius Skaug,
&quot;An Information Criterion for Automatic Gradient Tree Boosting&quot;, 2020, 
<a href="https://arxiv.org/abs/2008.05926">https://arxiv.org/abs/2008.05926</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.Rcpp_ENSEMBLE">predict.Rcpp_ENSEMBLE</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## A simple gtb.train example with linear regression:
x &lt;- runif(500, 0, 4)
y &lt;- rnorm(500, x, 1)
x.test &lt;- runif(500, 0, 4)
y.test &lt;- rnorm(500, x.test, 1)

mod &lt;- gbt.train(y, as.matrix(x))
y.pred &lt;- predict( mod, as.matrix( x.test ) )

plot(x.test, y.test)
points(x.test, y.pred, col="red")


</code></pre>

<hr>
<h2 id='predict.Rcpp_ENSEMBLE'>aGTBoost Prediction</h2><span id='topic+predict.Rcpp_ENSEMBLE'></span>

<h3>Description</h3>

<p><code>predict</code> is an interface for predicting from a <span class="pkg">agtboost</span> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Rcpp_ENSEMBLE'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.Rcpp_ENSEMBLE_+3A_object">object</code></td>
<td>
<p>Object or pointer to object of class <code>ENSEMBLE</code></p>
</td></tr>
<tr><td><code id="predict.Rcpp_ENSEMBLE_+3A_newdata">newdata</code></td>
<td>
<p>Design matrix of data to be predicted. Type <code>matrix</code></p>
</td></tr>
<tr><td><code id="predict.Rcpp_ENSEMBLE_+3A_...">...</code></td>
<td>
<p>additional parameters passed. Currently not in use.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The prediction function for <span class="pkg">agtboost</span>.
Using the generic <code>predict</code> function in R is also possible, using the same arguments.
</p>


<h3>Value</h3>

<p>For regression or binary classification, it returns a vector of length <code>nrows(newdata)</code>.
</p>


<h3>References</h3>

<p>Berent Ånund Strømnes Lunde, Tore Selland Kleppe and Hans Julius Skaug,
&quot;An Information Criterion for Automatic Gradient Tree Boosting&quot;, 2020, 
<a href="https://arxiv.org/abs/2008.05926">https://arxiv.org/abs/2008.05926</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gbt.train">gbt.train</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## A simple gtb.train example with linear regression:
x &lt;- runif(500, 0, 4)
y &lt;- rnorm(500, x, 1)
x.test &lt;- runif(500, 0, 4)
y.test &lt;- rnorm(500, x.test, 1)

mod &lt;- gbt.train(y, as.matrix(x))

## predict is overloaded
y.pred &lt;- predict( mod, as.matrix( x.test ) )

plot(x.test, y.test)
points(x.test, y.pred, col="red")


</code></pre>

<hr>
<h2 id='predict.Rcpp_GBT_COUNT_AUTO'>aGTBoost Count-Regression Auto Prediction</h2><span id='topic+predict.Rcpp_GBT_COUNT_AUTO'></span>

<h3>Description</h3>

<p><code>predict</code> is an interface for predicting from a <code>agtboost</code> model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Rcpp_GBT_COUNT_AUTO'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.Rcpp_GBT_COUNT_AUTO_+3A_object">object</code></td>
<td>
<p>Object or pointer to object of class <code>GBT_ZI_MIX</code></p>
</td></tr>
<tr><td><code id="predict.Rcpp_GBT_COUNT_AUTO_+3A_newdata">newdata</code></td>
<td>
<p>Design matrix of data to be predicted. Type <code>matrix</code></p>
</td></tr>
<tr><td><code id="predict.Rcpp_GBT_COUNT_AUTO_+3A_...">...</code></td>
<td>
<p>additional parameters passed. Currently not in use.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The prediction function for <code>agtboost</code>.
Using the generic <code>predict</code> function in R is also possible, using the same arguments.
</p>


<h3>Value</h3>

<p>For regression or binary classification, it returns a vector of length <code>nrows(newdata)</code>.
</p>


<h3>References</h3>

<p>Berent Ånund Strømnes Lunde, Tore Selland Kleppe and Hans Julius Skaug,
&quot;An Information Criterion for Automatic Gradient Tree Boosting&quot;, 2020, 
<a href="https://arxiv.org/abs/2008.05926">https://arxiv.org/abs/2008.05926</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gbt.train">gbt.train</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## A simple gtb.train example with linear regression:
## Random generation of zero-inflated poisson
2+2

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
