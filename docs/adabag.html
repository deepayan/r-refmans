<!DOCTYPE html><html><head><title>Help for package adabag</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {adabag}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#adabag-package'>
<p>Applies Multiclass AdaBoost.M1, SAMME and Bagging</p></a></li>
<li><a href='#adabag-internal'><p>Internal <code>adabag</code> functions</p></a></li>
<li><a href='#autoprune'>
<p>Builds automatically a pruned tree of class <code>rpart</code></p></a></li>
<li><a href='#bagging'><p> Applies the Bagging algorithm to a data set</p></a></li>
<li><a href='#bagging.cv'><p>Runs v-fold cross validation with Bagging</p></a></li>
<li><a href='#boosting'><p> Applies the AdaBoost.M1 and SAMME algorithms to a data set</p></a></li>
<li><a href='#boosting.cv'><p> Runs v-fold cross validation with AdaBoost.M1 or SAMME</p></a></li>
<li><a href='#Ensemble_ranking_IW'><p>Ensemble methods for ranking data: Item-Weighted Boosting and Bagging Algorithms</p></a></li>
<li><a href='#errorevol'><p> Shows the error evolution of the ensemble</p></a></li>
<li><a href='#errorevol_ranking_vector_IW'><p>Calculate the error evolution and final predictions of an item-weighted ensemble for rankings</p></a></li>
<li><a href='#importanceplot'>
<p>Plots the variables relative importance</p></a></li>
<li><a href='#MarginOrderedPruning.Bagging'><p>MarginOrderedPruning.Bagging</p></a></li>
<li><a href='#margins'><p> Calculates the margins</p></a></li>
<li><a href='#plot.errorevol'>
<p>Plots the error evolution of the ensemble</p></a></li>
<li><a href='#plot.margins'>
<p>Plots the margins of the ensemble</p></a></li>
<li><a href='#predict.bagging'><p>Predicts from a fitted bagging object</p></a></li>
<li><a href='#predict.boosting'><p> Predicts from a fitted boosting object</p></a></li>
<li><a href='#prep_data'><p>Prepare Ranking Data for Item-Weighted Ensemble Algorithm</p></a></li>
<li><a href='#simulatedRankingData'>
<p>Simulated ranking data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Applies Multiclass AdaBoost.M1, SAMME and Bagging</td>
</tr>
<tr>
<td>Version:</td>
<td>5.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-05-30</td>
</tr>
<tr>
<td>Author:</td>
<td>Alfaro, Esteban; Gamez, Matias and Garcia, Noelia; with contributions from L. Guo, A. Albano, M. Sciandra and A. Plaia </td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Esteban Alfaro &lt;Esteban.Alfaro@uclm.es&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>rpart, caret, foreach, doParallel, R (&ge; 4.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, tidyr, dplyr, ConsRank (&ge; 2.1.3)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>mlbench</td>
</tr>
<tr>
<td>Description:</td>
<td>It implements Freund and Schapire's Adaboost.M1 algorithm and Breiman's Bagging
	algorithm using classification trees as individual classifiers. Once these classifiers have been
	trained, they can be used to predict on new data. Also, cross validation estimation of the error can
	be done. Since version 2.0 the function margins() is available to calculate the margins for these
	classifiers. Also a higher flexibility is achieved giving access to the rpart.control() argument
	of 'rpart'. Four important new features were introduced on version 3.0, AdaBoost-SAMME (Zhu 
	et al., 2009) is implemented and a new function errorevol() shows the error of the ensembles as
	a function of the number of iterations. In addition, the ensembles can be pruned using the option 
	'newmfinal' in the predict.bagging() and predict.boosting() functions and the posterior probability of
	each class for observations can be obtained. Version 3.1 modifies the relative importance measure
	to take into account the gain of the Gini index given by a variable in each tree and the weights of 
	these trees. Version 4.0 includes the margin-based ordered aggregation for Bagging pruning (Guo
	and Boukir, 2013) and a function to auto prune the 'rpart' tree. Moreover, three new plots are also 
	available importanceplot(), plot.errorevol() and plot.margins(). Version 4.1 allows to predict on 
	unlabeled data. Version 4.2 includes the parallel computation option for some of the functions. 
	Version 5.0 includes the Boosting and Bagging algorithms for label ranking (Albano, Sciandra
	and Plaia, 2023).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-05-30 10:47:01 UTC; Esteban.Alfaro</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-05-31 17:00:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='adabag-package'>
Applies Multiclass AdaBoost.M1, SAMME and Bagging
</h2><span id='topic+adabag-package'></span><span id='topic+adabag'></span>

<h3>Description</h3>

<p>It implements Freund and Schapire's Adaboost.M1 algorithm and Breiman's Bagging
algorithm using classification trees as individual classifiers. Once these classifiers have been trained, they can be used to predict on new data. Also, cross validation estimation of the error can 	be done. Since version 2.0 the function margins() is available to calculate the margins for these classifiers. Also a higher flexibility is achieved giving access to the rpart.control() argument of 'rpart'. Four important new features were introduced on version 3.0, AdaBoost-SAMME (Zhu et al., 2009) is implemented and a new function errorevol() shows the error of the ensembles as a function of the number of iterations. In addition, the ensembles can be pruned using the option 'newmfinal' in the predict.bagging() and predict.boosting() functions and the posterior probability of
each class for observations can be obtained. Version 3.1 modifies the relative importance measure to take into account the gain of the Gini index given by a variable in each tree and the weights of these trees. Version 4.0 includes the margin-based ordered aggregation for Bagging pruning (Guo and Boukir, 2013) and a function to auto prune the 'rpart' tree. Moreover, three new plots are also
available importanceplot(), plot.errorevol() and plot.margins(). Version 4.1 allows to predict on unlabeled data. Version 4.2 includes the parallel computation option for some of the functions. Version 5.0 includes the Boosting and Bagging algorithms for label ranking (Albano, Sciandra and Plaia, 2023).
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> adabag</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 5.0</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2023-05-4</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL(&gt;= 2)</td>
</tr>
<tr>
 <td style="text-align: left;">
LazyLoad: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Author: Esteban Alfaro-Cortes, Matias Gamez-Martinez and Noelia Garcia-Rubio,<br />
with contributions from L. Guo, A. Albano, M. Sciandra and A. Plaia <br />
Maintainer: Esteban Alfaro-Cortes &lt;Esteban.Alfaro@uclm.es&gt;
</p>


<h3>References</h3>

<p>Albano, A., Sciandra, M., and Plaia, A. (2023): &ldquo;A weighted distance-based approach with boosted decision trees for label ranking&rdquo;. Expert Systems with Applications.
</p>
<p>Alfaro, E., Gamez, M. and Garcia, N. (2013): &ldquo;adabag: An R Package for Classification with Boosting and Bagging&rdquo;. Journal of Statistical Software, 54(2), 1&ndash;35.
</p>
<p>Alfaro, E., Garcia, N., Gamez, M. and Elizondo, D. (2008): &ldquo;Bankruptcy forecasting: An empirical comparison of AdaBoost and neural networks&rdquo;. Decision Support Systems, 45, 110&ndash;122.
</p>
<p>Breiman, L. (1998): &ldquo;Arcing classifiers&rdquo;. The Annals of Statistics, 26(3), 801&ndash;849.
</p>
<p>Freund, Y. and Schapire, R.E. (1996): &ldquo;Experiments with a new boosting algorithm&rdquo;. In Proceedings of the Thirteenth International Conference on Machine Learning, 148&ndash;156, Morgan Kaufmann.
</p>
<p>Guo, L. and Boukir, S. (2013): &quot;Margin-based ordered aggregation for ensemble pruning&quot;. Pattern Recognition Letters, 34(6), 603-609.
</p>
<p>Zhu, J., Zou, H., Rosset, S. and Hastie, T. (2009): &ldquo;Multi-class AdaBoost&rdquo;. Statistics and Its Interface, 2, 349&ndash;360.
</p>


<h4>Reverse cites</h4>

<p>To the best of our knowledge this package has been cited by:
</p>
<p>Andriyas, S. and McKee, M. (2013). Recursive partitioning techniques for modeling irrigation behavior.
Environmental Modelling &amp; Software, 47, 207&ndash;217.
</p>
<p>Chan, J. C. W. and Paelinckx, D. (2008). Evaluation of Random Forest and Adaboost tree-based ensemble classification and spectral band selection for ecotope mapping using airborne hyperspectral imagery.
Remote Sensing of Environment, 112(6), 2999&ndash;3011.
</p>
<p>Chrzanowska, M., Alfaro, E., and Witkowska, D. (2009). The individual borrowers recognition: Single and ensemble trees.
Expert Systems with Applications, 36(2), 6409&ndash;6414.
</p>
<p>De Bock, K. W., Coussement, K., and Van den Poel, D. (2010). Ensemble classification based on generalized additive models.
Computational Statistics &amp; Data Analysis, 54(6), 1535&ndash;1546.
</p>
<p>De Bock, K. W. and Van den Poel, D. (2011). An empirical evaluation of rotation-based ensemble classifiers for customer churn prediction.
Expert Systems with Applications, 38(10), 12293&ndash;12301.
</p>
<p>Fan, Y., Murphy, T.B., William, R. and Watson G. (2009). digeR: GUI tool for analyzing 2D DIGE data.
R package version 1.2.
</p>
<p>Garcia-Perez-de-Lema, D., Alfaro-Cortes, E., Manzaneque-Lizano, M. and Banegas-Ochovo, R. (2012). Strategy, competitive factors and performance in small and medium enterprise (SMEs).
African Journal of Business Management, 6(26), 7714&ndash;7726.
</p>
<p>Gonzalez-Rufino, E., Carrion, P., Cernadas, E., Fernandez-Delgado, M. and Dominguez-Petit, R. (2013). Exhaustive comparison of colour texture
features and classification methods to discriminate cells categories in histological images of fish ovary. Pattern Recognition, 46, 2391&ndash;2407.
</p>
<p>Krempl, G. and Hofer, V. (2008). Partitioner trees: combining boosting and arbitrating.
In: Okun, O., Valentini, G. (eds.) Proc. 2nd Workshop Supervised and Unsupervised Ensemble Methods and Their Applications, Patras, Greece, 61&ndash;66.
</p>
<p>Maindonald, J. and Braun, J. (2010). Data Analysis and Graphics Using R - An Example-Based Approach.
3rd ed, Cambridge University Press (p. 373)
</p>
<p>Murphy, T. B., Dean, N. and Raftery, A. E. (2010). Variable selection and updating in model-based discriminant analysis for high dimensional data with food authenticity applications.
The annals of applied statistics, 4(1), 396&ndash;421.
</p>
<p>Stewart, B.M. and Zhukov, Y.M. (2009). Use of force and civil-military relations in Russia: An automated content analysis.
Small Wars &amp; Insurgencies, 20(2), 319&ndash;343.
</p>
<p>Torgo, L.  (2010). Data Mining with R: Learning with Case Studies. Series: Chapman &amp; Hall/CRC Data Mining
and Knowledge Discovery.
</p>
<p>If you know any other work where this package is cited, please send us an email
</p>



<h3>See Also</h3>

<p><code><a href="#topic+autoprune">autoprune</a></code>,
<code><a href="#topic+bagging">bagging</a></code>,
<code><a href="#topic+bagging.cv">bagging.cv</a></code>,
<code><a href="#topic+boosting">boosting</a></code>,
<code><a href="#topic+boosting.cv">boosting.cv</a></code>,
<code><a href="#topic+errorevol">errorevol</a></code>,
<code><a href="#topic+importanceplot">importanceplot</a></code>,
<code><a href="#topic+margins">margins</a></code>,
<code><a href="#topic+MarginOrderedPruning.Bagging">MarginOrderedPruning.Bagging</a></code>,
<code><a href="#topic+plot.errorevol">plot.errorevol</a></code>,
<code><a href="#topic+plot.margins">plot.margins</a></code>,
<code><a href="#topic+predict.bagging">predict.bagging</a></code>,
<code><a href="#topic+predict.boosting">predict.boosting</a></code>,
<code><a href="#topic+Ensemble_ranking_IW">Ensemble_ranking_IW</a></code>,
<code><a href="#topic+errorevol_ranking_vector_IW">errorevol_ranking_vector_IW</a></code>,
<code><a href="#topic+prep_data">prep_data</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## rpart library should be loaded
data(iris)
iris.adaboost &lt;- boosting(Species~., data=iris, boos=TRUE,
	mfinal=3)
importanceplot(iris.adaboost)

sub &lt;- c(sample(1:50, 35), sample(51:100, 35), sample(101:150, 35))
iris.bagging &lt;- bagging(Species ~ ., data=iris[sub,], mfinal=3)
#Predicting with labeled data
iris.predbagging&lt;-predict.bagging(iris.bagging, newdata=iris[-sub,])
iris.predbagging
#Predicting with unlabeled data
iris.predbagging&lt;- predict.bagging(iris.bagging, newdata=iris[-sub,-5])
iris.predbagging
</code></pre>

<hr>
<h2 id='adabag-internal'>Internal <code>adabag</code> functions</h2><span id='topic+adabag-internal'></span><span id='topic+entropyEachTree.bagging'></span><span id='topic+Margin.vote'></span><span id='topic+OOBIndex'></span><span id='topic+predictOrderedAggregation.bagging'></span><span id='topic+vote.bagging'></span><span id='topic+select'></span><span id='topic+internal1'></span><span id='topic+internal2'></span><span id='topic+itemp_IW'></span><span id='topic+stemp_IW'></span><span id='topic+etemp_IW'></span>

<h3>Description</h3>

<p>Internal <code>adabag</code> functions</p>


<h3>Usage</h3>

<pre><code class='language-R'>entropyEachTree.bagging(object, trainingdata, OOB=TRUE, marginType="unsupervised",

	newmfinal=length(object$trees), doTrace=TRUE)

Margin.vote(vote,type="unsupervised",observed)

OOBIndex(mySample)

predictOrderedAggregation.bagging(object, newdata, myorder,doTrace=TRUE)

vote.bagging(object, newdata,OOB=TRUE, myTreeIndex=seq(1,length(object$trees)))

select(fila, vardep.summary, ...)

internal1(x,item)

internal2(riga, namecol, item=item)

itemp_IW(y, offset, parms, wt)

stemp_IW(y, wt, x, parms, continuous)

etemp_IW(y, wt, parms)


</code></pre>


<h3>Details</h3>

<p>These are not to be called by the user</p>

<hr>
<h2 id='autoprune'>
Builds automatically a pruned tree of class <code>rpart</code>
</h2><span id='topic+autoprune'></span>

<h3>Description</h3>

<p>Builds automatically a pruned tree of class <code>rpart</code> looking in the
cptable for the minimum cross validation error plus a standard deviation 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>autoprune(formula, data, subset=1:length(data[,1]), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="autoprune_+3A_formula">formula</code></td>
<td>
<p> a formula, as in the <code>lm</code> function.  </p>
</td></tr>
<tr><td><code id="autoprune_+3A_data">data</code></td>
<td>
<p>a data frame in which to interpret the variables named in the <code>formula</code>. </p>
</td></tr>
<tr><td><code id="autoprune_+3A_subset">subset</code></td>
<td>
   
<p>optional expression saying that only a subset of the rows of the data should be used in the fit, as in the <code>rpart</code> function.</p>
</td></tr>
<tr><td><code id="autoprune_+3A_...">...</code></td>
<td>
<p> further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The cross validation estimation of the error (xerror) has a random component. To avoid this randomness
the 1-SE rule (or 1-SD rule) selects the simplest model with a xerror equal or less than
the minimum xerror plus the standard deviation of the minimum xerror.
</p>


<h3>Value</h3>

<p>An object of class <code>rpart</code>
</p>


<h3>Author(s)</h3>

<p>Esteban Alfaro-Cortes <a href="mailto:Esteban.Alfaro@uclm.es">Esteban.Alfaro@uclm.es</a>, Matias Gamez-Martinez <a href="mailto:Matias.Gamez@uclm.es">Matias.Gamez@uclm.es</a> and Noelia Garcia-Rubio <a href="mailto:Noelia.Garcia@uclm.es">Noelia.Garcia@uclm.es</a> </p>


<h3>References</h3>

<p>Breiman, L., Friedman, J.H., Olshen, R. and Stone, C.J.  (1984): &quot;Classification and Regression Trees&quot;. Wadsworth International Group. Belmont
</p>
<p>Therneau, T., Atkinson, B. and Ripley, B. (2014). rpart: Recursive Partitioning and Regression Trees. R package version 4.1-5
</p>


<h3>See Also</h3>

<p><code><a href="rpart.html#topic+rpart">rpart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## rpart library should be loaded
library(rpart)
data(iris)
iris.prune&lt;-autoprune(Species~., data=iris)
iris.prune

## Comparing the test error of rpart and autoprune
library(mlbench)
data(BreastCancer)
l &lt;- length(BreastCancer[,1])
sub &lt;- sample(1:l,2*l/3)

BC.rpart &lt;- rpart(Class~.,data=BreastCancer[sub,-1],cp=-1, maxdepth=5)
BC.rpart.pred &lt;- predict(BC.rpart,newdata=BreastCancer[-sub,-1],type="class")
tb &lt;-table(BC.rpart.pred,BreastCancer$Class[-sub])
tb
1-(sum(diag(tb))/sum(tb))


BC.prune&lt;-autoprune(Class~.,data=BreastCancer[,-1],subset=sub)
BC.rpart.pred &lt;- predict(BC.prune,newdata=BreastCancer[-sub,-1],type="class")
tb &lt;-table(BC.rpart.pred,BreastCancer$Class[-sub])
tb
1-(sum(diag(tb))/sum(tb))



</code></pre>

<hr>
<h2 id='bagging'> Applies the Bagging algorithm to a data set </h2><span id='topic+bagging'></span>

<h3>Description</h3>

<p>Fits the Bagging algorithm proposed by Breiman 
in 1996 using classification trees as single classifiers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bagging(formula, data, mfinal = 100, control, par=FALSE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bagging_+3A_formula">formula</code></td>
<td>
<p> a formula, as in the <code>lm</code> function.  </p>
</td></tr>
<tr><td><code id="bagging_+3A_data">data</code></td>
<td>
<p>a data frame in which to interpret the variables named in the <code>formula</code> </p>
</td></tr>
<tr><td><code id="bagging_+3A_mfinal">mfinal</code></td>
<td>
<p>an integer, the number of iterations for which boosting is run 
or the number of trees to use. Defaults to <code>mfinal=100</code> iterations.</p>
</td></tr>
<tr><td><code id="bagging_+3A_control">control</code></td>
<td>
<p>options that control details of the rpart algorithm. See rpart.control for more details.  </p>
</td></tr>
<tr><td><code id="bagging_+3A_par">par</code></td>
<td>
<p> if <code>TRUE</code>, the cross validation process is runned in parallel. If <code>FALSE</code> (by default),
the function runs without parallelization.  </p>
</td></tr>
<tr><td><code id="bagging_+3A_...">...</code></td>
<td>
<p> further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Unlike boosting, individual classifiers are independent among them in bagging
</p>


<h3>Value</h3>

<p>An object of class <code>bagging</code>, which is a list with the following components:
</p>
<table>
<tr><td><code>formula</code></td>
<td>
<p>the formula used.</p>
</td></tr>
<tr><td><code>trees</code></td>
<td>
<p>the trees grown along the iterations.</p>
</td></tr>
<tr><td><code>votes</code></td>
<td>
<p>a matrix describing, for each observation, the number of trees that assigned it to each class.</p>
</td></tr>
<tr><td><code>prob</code></td>
<td>
<p>a matrix describing, for each observation, the posterior probability or degree of support of each class. 
These probabilities are calculated using the proportion of votes in the final ensemble.</p>
</td></tr>
<tr><td><code>class</code></td>
<td>
<p>the class predicted by the ensemble classifier.</p>
</td></tr>
<tr><td><code>samples</code></td>
<td>
<p> the bootstrap samples used along the iterations.</p>
</td></tr>
<tr><td><code>importance</code></td>
<td>
<p>returns the relative importance of each variable in the classification task.
This measure takes into account the gain of the Gini index given by a variable in each tree.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Esteban Alfaro-Cortes <a href="mailto:Esteban.Alfaro@uclm.es">Esteban.Alfaro@uclm.es</a>, Matias Gamez-Martinez <a href="mailto:Matias.Gamez@uclm.es">Matias.Gamez@uclm.es</a> and Noelia Garcia-Rubio <a href="mailto:Noelia.Garcia@uclm.es">Noelia.Garcia@uclm.es</a> </p>


<h3>References</h3>

<p>Alfaro, E., Gamez, M. and Garcia, N. (2013): &ldquo;<code>adabag</code>: An R Package for Classification with Boosting and Bagging&rdquo;. Journal of Statistical Software, Vol 54, 2, pp. 1&ndash;35.
</p>
<p>Alfaro, E., Garcia, N., Gamez, M. and Elizondo, D. (2008): &ldquo;Bankruptcy forecasting: An empirical comparison of AdaBoost and neural networks&rdquo;. Decision Support Systems, 45, pp. 110&ndash;122.
</p>
<p>Breiman, L. (1996): &quot;Bagging predictors&quot;. Machine Learning, Vol 24, 2, pp.123&ndash;140.
</p>
<p>Breiman, L. (1998): &quot;Arcing classifiers&quot;. The Annals of Statistics, Vol 26, 3, pp. 801&ndash;849. </p>


<h3>See Also</h3>

 
<p><code><a href="#topic+predict.bagging">predict.bagging</a></code>,
<code><a href="#topic+bagging.cv">bagging.cv</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## rpart library should be loaded
#This example has been hidden to fulfill execution time &lt;5s
#library(rpart)
#data(iris)
#iris.bagging &lt;- bagging(Species~., data=iris, mfinal=10)

# Data Vehicle (four classes)
library(rpart)
library(mlbench)
data(Vehicle)
l &lt;- length(Vehicle[,1])
sub &lt;- sample(1:l,2*l/3)
Vehicle.bagging &lt;- bagging(Class ~.,data=Vehicle[sub, ],mfinal=5, 
	control=rpart.control(maxdepth=5, minsplit=15))
#Using the pruning option
Vehicle.bagging.pred &lt;- predict.bagging(Vehicle.bagging,newdata=Vehicle[-sub, ], newmfinal=3)
Vehicle.bagging.pred$confusion
Vehicle.bagging.pred$error


</code></pre>

<hr>
<h2 id='bagging.cv'>Runs v-fold cross validation with Bagging</h2><span id='topic+bagging.cv'></span>

<h3>Description</h3>

<p>The data are divided into <code>v</code> non-overlapping subsets of roughly equal size. Then, <code>bagging</code>
is applied on <code>(v-1)</code> of the subsets. Finally, predictions are made for the left out subsets,
and the process is repeated for each of the <code>v</code> subsets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bagging.cv(formula, data, v = 10, mfinal = 100, control, par=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bagging.cv_+3A_formula">formula</code></td>
<td>
<p> a formula, as in the <code>lm</code> function. </p>
</td></tr>
<tr><td><code id="bagging.cv_+3A_data">data</code></td>
<td>
<p>a data frame in which to interpret the variables named in <code>formula</code> </p>
</td></tr>
<tr><td><code id="bagging.cv_+3A_v">v</code></td>
<td>
<p>An integer, specifying the type of v-fold cross validation. Defaults to 10.
If <code>v</code> is set as the number of observations, leave-one-out cross validation is carried out. 
Besides this, every value between two and the number of observations is valid and means
that roughly every v-th observation is left out.</p>
</td></tr>
<tr><td><code id="bagging.cv_+3A_mfinal">mfinal</code></td>
<td>
<p>an integer, the number of iterations for which boosting is run 
or the number of trees to use. Defaults to <code>mfinal=100</code> iterations.</p>
</td></tr>
<tr><td><code id="bagging.cv_+3A_control">control</code></td>
<td>
<p>options that control details of the rpart algorithm. See rpart.control for more details.  </p>
</td></tr>
<tr><td><code id="bagging.cv_+3A_par">par</code></td>
<td>
<p> if <code>TRUE</code>, the cross validation process is runned in parallel. If <code>FALSE</code> (by default),
the function runs without parallelization.  </p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>bagging.cv</code>, which is a list with the following components:
</p>
<table>
<tr><td><code>class</code></td>
<td>
<p>the class predicted by the ensemble classifier.</p>
</td></tr>
<tr><td><code>confusion</code></td>
<td>
<p>the confusion matrix which compares the real class with the predicted one.</p>
</td></tr>
<tr><td><code>error</code></td>
<td>
<p>returns the average error.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Esteban Alfaro-Cortes <a href="mailto:Esteban.Alfaro@uclm.es">Esteban.Alfaro@uclm.es</a>, Matias Gamez-Martinez <a href="mailto:Matias.Gamez@uclm.es">Matias.Gamez@uclm.es</a> and Noelia Garcia-Rubio <a href="mailto:Noelia.Garcia@uclm.es">Noelia.Garcia@uclm.es</a> </p>


<h3>References</h3>

<p>Alfaro, E., Gamez, M. and Garcia, N. (2013): &ldquo;adabag: An R Package for Classification with Boosting and Bagging&rdquo;. Journal of Statistical Software, Vol 54, 2, pp. 1&ndash;35.
</p>
<p>Alfaro, E., Garcia, N., Gamez, M. and Elizondo, D. (2008): &ldquo;Bankruptcy forecasting: An empirical comparison of AdaBoost and neural networks&rdquo;. Decision Support Systems, 45, pp. 110&ndash;122.
</p>
<p>Breiman, L. (1996): &quot;Bagging predictors&quot;. Machine Learning, Vol 24, 2, pp. 123&ndash;140.
</p>
<p>Breiman, L. (1998). &quot;Arcing classifiers&quot;. The Annals of Statistics, Vol 26, 3, pp. 801&ndash;849. </p>


<h3>See Also</h3>

 
<p><code><a href="#topic+bagging">bagging</a></code>, 
<code><a href="#topic+predict.bagging">predict.bagging</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## rpart library should be loaded
library(rpart)
data(iris)
iris.baggingcv &lt;- bagging.cv(Species ~ ., v=2, data=iris, mfinal=3,
control=rpart.control(cp=0.01))
iris.baggingcv[-1]


## rpart and mlbench libraries should be loaded
## Data Vehicle (four classes) 
#This example has been hidden to keep execution time &lt;5s
#data(Vehicle)
#Vehicle.bagging.cv &lt;- bagging.cv(Class ~.,data=Vehicle,v=5,mfinal=10, 
#control=rpart.control(maxdepth=5))
#Vehicle.bagging.cv[-1]

</code></pre>

<hr>
<h2 id='boosting'> Applies the AdaBoost.M1 and SAMME algorithms to a data set </h2><span id='topic+boosting'></span><span id='topic+adaboost.M1'></span>

<h3>Description</h3>

<p> Fits the AdaBoost.M1 (Freund and Schapire, 1996) and SAMME (Zhu et al., 2009) algorithms 
using classification trees as single classifiers.  </p>


<h3>Usage</h3>

<pre><code class='language-R'>  boosting(formula, data, boos = TRUE, mfinal = 100, coeflearn = 'Breiman', 
	control,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="boosting_+3A_formula">formula</code></td>
<td>
<p> a formula, as in the <code>lm</code> function.  </p>
</td></tr>
<tr><td><code id="boosting_+3A_data">data</code></td>
<td>
<p>a data frame in which to interpret the variables named in <code>formula</code>.  </p>
</td></tr>
<tr><td><code id="boosting_+3A_boos">boos</code></td>
<td>
<p> if <code>TRUE</code> (by default), a bootstrap sample of the training set is drawn using 
the weights for each observation on that iteration. If <code>FALSE</code>, every observation
is used with its weights.  </p>
</td></tr>
<tr><td><code id="boosting_+3A_mfinal">mfinal</code></td>
<td>
<p>an integer, the number of iterations for which boosting is run 
or the number of trees to use. Defaults to <code>mfinal=100</code> iterations.  </p>
</td></tr>
<tr><td><code id="boosting_+3A_coeflearn">coeflearn</code></td>
<td>
<p> if 'Breiman'(by default), <code>alpha=1/2ln((1-err)/err)</code> is used. 
If 'Freund' <code>alpha=ln((1-err)/err)</code> is used. In both cases the AdaBoost.M1 algorithm is used
and <code>alpha</code> is the weight updating coefficient. On the other hand, if coeflearn is 'Zhu' the SAMME algorithm 
is implemented with <code>alpha=ln((1-err)/err)+</code> <code>ln(nclasses-1)</code>.</p>
</td></tr>
<tr><td><code id="boosting_+3A_control">control</code></td>
<td>
<p>options that control details of the rpart algorithm. See rpart.control for more details.  </p>
</td></tr>
<tr><td><code id="boosting_+3A_...">...</code></td>
<td>
<p> further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>AdaBoost.M1 and SAMME are simple generalizations of AdaBoost for more than two classes. In AdaBoost-SAMME 
the individual trees are required to have an error lower than 1-1/nclasses instead of 1/2 of the AdaBoost.M1
</p>


<h3>Value</h3>

<p>An object of class <code>boosting</code>, which is a list with the following components:  
</p>
<table>
<tr><td><code>formula</code></td>
<td>
<p>the formula used.  </p>
</td></tr>
<tr><td><code>trees</code></td>
<td>
<p>the trees grown along the iterations.  </p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>a vector with the weighting of the trees of all iterations.  </p>
</td></tr>
<tr><td><code>votes</code></td>
<td>
<p>a matrix describing, for each observation, the number of trees that assigned it to each class, weighting each tree by its <code>alpha</code> coefficient.  </p>
</td></tr>
<tr><td><code>prob</code></td>
<td>
<p>a matrix describing, for each observation, the posterior probability or degree of support of each class. 
These probabilities are calculated using the proportion of votes in the final ensemble.</p>
</td></tr>
<tr><td><code>class</code></td>
<td>
<p>the class predicted by the ensemble classifier.  </p>
</td></tr>
<tr><td><code>importance</code></td>
<td>
<p>returns the relative importance of each variable in the classification task.
This measure takes into account the gain of the Gini index given by a variable in a tree and the weight of this tree.  </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Esteban Alfaro-Cortes <a href="mailto:Esteban.Alfaro@uclm.es">Esteban.Alfaro@uclm.es</a>, Matias Gamez-Martinez <a href="mailto:Matias.Gamez@uclm.es">Matias.Gamez@uclm.es</a> and Noelia Garcia-Rubio <a href="mailto:Noelia.Garcia@uclm.es">Noelia.Garcia@uclm.es</a> </p>


<h3>References</h3>

<p>Alfaro, E., Gamez, M. and Garcia, N. (2013): &ldquo;adabag: An R Package for Classification with Boosting and Bagging&rdquo;. Journal of Statistical Software, Vol 54, 2, pp. 1&ndash;35.
</p>
<p>Alfaro, E., Garcia, N., Gamez, M. and Elizondo, D. (2008): &ldquo;Bankruptcy forecasting: An empirical comparison of AdaBoost and neural networks&rdquo;. Decision Support Systems, 45, pp. 110&ndash;122.
</p>
<p>Breiman, L. (1998): &ldquo;Arcing classifiers&rdquo;. The Annals of Statistics, Vol 26, 3, pp. 801&ndash;849.
</p>
<p>Freund, Y. and Schapire, R.E. (1996): &ldquo;Experiments with a new boosting algorithm&rdquo;. In Proceedings of the Thirteenth International Conference on Machine Learning, pp. 148&ndash;156, Morgan Kaufmann. 
</p>
<p>Zhu, J., Zou, H., Rosset, S. and Hastie, T. (2009): &ldquo;Multi-class AdaBoost&rdquo;. Statistics and Its Interface, 2, pp. 349&ndash;360. 
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+predict.boosting">predict.boosting</a></code>,
<code><a href="#topic+boosting.cv">boosting.cv</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>
## rpart library should be loaded
data(iris)
iris.adaboost &lt;- boosting(Species~., data=iris, boos=TRUE, mfinal=3)
iris.adaboost


## Data Vehicle (four classes) 
library(mlbench)
data(Vehicle)
l &lt;- length(Vehicle[,1])
sub &lt;- sample(1:l,2*l/3)
mfinal &lt;- 3 
maxdepth &lt;- 5

Vehicle.rpart &lt;- rpart(Class~.,data=Vehicle[sub,],maxdepth=maxdepth)
Vehicle.rpart.pred &lt;- predict(Vehicle.rpart,newdata=Vehicle[-sub, ],type="class")
tb &lt;- table(Vehicle.rpart.pred,Vehicle$Class[-sub])
error.rpart &lt;- 1-(sum(diag(tb))/sum(tb))
tb
error.rpart

Vehicle.adaboost &lt;- boosting(Class ~.,data=Vehicle[sub, ],mfinal=mfinal, coeflearn="Zhu",
	control=rpart.control(maxdepth=maxdepth))
Vehicle.adaboost.pred &lt;- predict.boosting(Vehicle.adaboost,newdata=Vehicle[-sub, ])
Vehicle.adaboost.pred$confusion
Vehicle.adaboost.pred$error

#comparing error evolution in training and test set
errorevol(Vehicle.adaboost,newdata=Vehicle[sub, ])-&gt;evol.train
errorevol(Vehicle.adaboost,newdata=Vehicle[-sub, ])-&gt;evol.test

plot.errorevol(evol.test,evol.train)


</code></pre>

<hr>
<h2 id='boosting.cv'> Runs v-fold cross validation with AdaBoost.M1 or SAMME</h2><span id='topic+boosting.cv'></span>

<h3>Description</h3>

<p>The data are divided into <code>v</code> non-overlapping subsets of roughly equal size. Then, <code>boosting</code>
is applied on <code>(v-1)</code> of the subsets. Finally, predictions are made for the left out subsets,
and the process is repeated for each of the <code>v</code> subsets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>boosting.cv(formula, data, v = 10, boos = TRUE, mfinal = 100, 
 coeflearn = "Breiman", control, par=FALSE) 
</code></pre>


<h3>Arguments</h3>

  
<table>
<tr><td><code id="boosting.cv_+3A_formula">formula</code></td>
<td>
<p> a formula, as in the <code>lm</code> function.  </p>
</td></tr>
<tr><td><code id="boosting.cv_+3A_data">data</code></td>
<td>
<p>a data frame in which to interpret the variables named in <code>formula</code> </p>
</td></tr>
<tr><td><code id="boosting.cv_+3A_boos">boos</code></td>
<td>
<p> if <code>TRUE</code> (by default), a bootstrap sample of the training set is drawn using 
the weights for each observation on that iteration. If <code>FALSE</code>, every observation
is used with its weights. </p>
</td></tr>
<tr><td><code id="boosting.cv_+3A_v">v</code></td>
<td>
<p>An integer, specifying the type of v-fold cross validation. Defaults to 10.
If <code>v</code> is set as the number of observations, leave-one-out cross validation is carried out. 
Besides this, every value between two and the number of observations is valid and means
that roughly every v-th observation is left out.</p>
</td></tr>
<tr><td><code id="boosting.cv_+3A_mfinal">mfinal</code></td>
<td>
<p>an integer, the number of iterations for which boosting is run 
or the number of trees to use. Defaults to <code>mfinal=100</code> iterations.</p>
</td></tr>
<tr><td><code id="boosting.cv_+3A_coeflearn">coeflearn</code></td>
<td>
<p> if 'Breiman'(by default), <code>alpha=1/2ln((1-err)/err)</code> is used. 
If 'Freund' <code>alpha=ln((1-err)/err)</code> is used. In both cases the AdaBoost.M1 algorithm is used
and <code>alpha</code> is the weight updating coefficient. On the other hand, if coeflearn is 'Zhu' the SAMME algorithm 
is implemented with <code>alpha=ln((1-err)/err)+</code> <code>ln(nclasses-1)</code>.</p>
</td></tr>
<tr><td><code id="boosting.cv_+3A_control">control</code></td>
<td>
<p>options that control details of the rpart algorithm. See rpart.control for more details.  </p>
</td></tr>
<tr><td><code id="boosting.cv_+3A_par">par</code></td>
<td>
<p> if <code>TRUE</code>, the cross validation process is runned in parallel. If <code>FALSE</code> (by default),
the function runs without parallelization.  </p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>boosting.cv</code>, which is a list with the following components:
</p>
<table>
<tr><td><code>class</code></td>
<td>
<p>the class predicted by the ensemble classifier.</p>
</td></tr>
<tr><td><code>confusion</code></td>
<td>
<p>the confusion matrix which compares the real class with the predicted one.</p>
</td></tr>
<tr><td><code>error</code></td>
<td>
<p>returns the average error.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Esteban Alfaro-Cortes <a href="mailto:Esteban.Alfaro@uclm.es">Esteban.Alfaro@uclm.es</a>, Matias Gamez-Martinez <a href="mailto:Matias.Gamez@uclm.es">Matias.Gamez@uclm.es</a> and Noelia Garcia-Rubio <a href="mailto:Noelia.Garcia@uclm.es">Noelia.Garcia@uclm.es</a> </p>


<h3>References</h3>

<p>Alfaro, E., Gamez, M. and Garcia, N. (2013): &ldquo;adabag: An R Package for Classification with Boosting and Bagging&rdquo;. Journal of Statistical Software, Vol 54, 2, pp. 1&ndash;35.
</p>
<p>Alfaro, E., Garcia, N., Gamez, M. and Elizondo, D. (2008): &ldquo;Bankruptcy forecasting: An empirical comparison of AdaBoost and neural networks&rdquo;. Decision Support Systems, 45, pp. 110&ndash;122.
</p>
<p>Breiman, L. (1998): &quot;Arcing classifiers&quot;. The Annals of Statistics, Vol 26, 3, pp. 801&ndash;849. 
</p>
<p>Freund, Y. and Schapire, R.E. (1996): &quot;Experiments with a new boosting algorithm&quot;. In Proceedings of the Thirteenth International Conference on Machine Learning, pp. 148&ndash;156, Morgan Kaufmann. 
</p>
<p>Zhu, J., Zou, H., Rosset, S. and Hastie, T. (2009): &ldquo;Multi-class AdaBoost&rdquo;. Statistics and Its Interface, 2, pp. 349&ndash;360. 
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+boosting">boosting</a></code>,
<code><a href="#topic+predict.boosting">predict.boosting</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>
## rpart library should be loaded
data(iris)
iris.boostcv &lt;- boosting.cv(Species ~ ., v=2, data=iris, mfinal=5, 
control=rpart.control(cp=0.01))
iris.boostcv[-1]

## rpart and mlbench libraries should be loaded
## Data Vehicle (four classes) 
#This example has been hidden to fulfill execution time &lt;5s 
#data(Vehicle)
#Vehicle.boost.cv &lt;- boosting.cv(Class ~.,data=Vehicle,v=5, mfinal=10, coeflearn="Zhu",
#control=rpart.control(maxdepth=5))
#Vehicle.boost.cv[-1]



</code></pre>

<hr>
<h2 id='Ensemble_ranking_IW'>Ensemble methods for ranking data: Item-Weighted Boosting and Bagging Algorithms</h2><span id='topic+Ensemble_ranking_IW'></span>

<h3>Description</h3>

<p>The <code>Ensemble_ranking_IW</code> function applies the item-weighted Boosting and Bagging algorithms to ranking data (Albano et al., 2023). These algorithms utilize classification trees as base classifiers to perform item-weighted ensemble methods for rankings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Ensemble_ranking_IW(formula, data, iw, algo = "boosting", 
  mfinal = 100, coeflearn = "Breiman", control, bin = FALSE, 
  trace= TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Ensemble_ranking_IW_+3A_formula">formula</code></td>
<td>
<p>a formula specifying the response ranking variable and predictors, similar to the <code>lm</code> function. The response variable must be the &quot;Label&quot; column of the object generated by the <code>prep_data</code> function.</p>
</td></tr>
<tr><td><code id="Ensemble_ranking_IW_+3A_data">data</code></td>
<td>
<p>An N by (K+1) data frame containing the prepared item-weighted ranking data. The column &quot;Label&quot; should contain the transformed ranking responses, and the remaining columns should contain the predictors. Continuous variables are allowed, while the dummy coding should be used for categorical variables. The data frame must be the output of the <code>prep_data</code> function.</p>
</td></tr>
<tr><td><code id="Ensemble_ranking_IW_+3A_iw">iw</code></td>
<td>
<p>a vector or matrix representing the item weights or dissimilarities for the ranking data. For a vector, it should be a row vector of length M, where M is the number of items. For a matrix, it should be a symmetric M by M matrix representing item dissimilarities. For coherence, <code>iw</code> should be the same vector/matrix used in <code>prep_data(...)</code>.</p>
</td></tr>
<tr><td><code id="Ensemble_ranking_IW_+3A_algo">algo</code></td>
<td>
<p>the ensemble method to use. Possible values are &quot;bagging&quot; or &quot;boosting&quot;. Defaults to &quot;boosting&quot;.</p>
</td></tr>
<tr><td><code id="Ensemble_ranking_IW_+3A_mfinal">mfinal</code></td>
<td>
<p>the number of trees to use for  boosting or bagging. Defaults to 100 iterations.</p>
</td></tr>
<tr><td><code id="Ensemble_ranking_IW_+3A_coeflearn">coeflearn</code></td>
<td>
<p>the coefficient learning method to use. Possible values are &quot;Breiman&quot;, &quot;Freund&quot;, or &quot;Zhu&quot;. Defaults to &quot;Breiman&quot;.</p>
</td></tr>
<tr><td><code id="Ensemble_ranking_IW_+3A_control">control</code></td>
<td>
<p>an optional argument to control details of the classification tree algorithm. See <code>rpart.control</code> for more information.</p>
</td></tr>
<tr><td><code id="Ensemble_ranking_IW_+3A_bin">bin</code></td>
<td>
<p>a logical value indicating whether to use the binary logarithm function for updating weights at each iteration. Defaults to <code>FALSE</code>. When set to <code>TRUE</code>, it corresponds to utilizing the AdaBoost.R.M2 algorithm as defined by Albano et al. (2023).</p>
</td></tr>
<tr><td><code id="Ensemble_ranking_IW_+3A_trace">trace</code></td>
<td>
<p>a logical value controling the display of additional information ( the number of trees and the average weighted tau_x) during execution. Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="Ensemble_ranking_IW_+3A_...">...</code></td>
<td>
<p>additional arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>Ensemble_ranking_IW</code> function extends the Boosting and Bagging algorithms to handle item-weighted ranking data. It allows for the application of these ensemble methods to improve ranking predicting performance using classification trees as base classifiers.
</p>


<h3>Value</h3>

<p>An object of class <code>boosting</code> or <code>bagging</code>, which is a list with the following components:
</p>
<table>
<tr><td><code>formula</code></td>
<td>
<p>the used formula.</p>
</td></tr>
<tr><td><code>trees</code></td>
<td>
<p>the trees grown during the iterations.</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>a vector of weights for each tree in all iterations.</p>
</td></tr>
<tr><td><code>importance</code></td>
<td>
<p>a measure of the relative importance of each predictor in the ranking task, taking into account the weighted gain of the variable's contribution in each tree.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Alessandro Albano <a href="mailto:alessandro.albano@unipa.it">alessandro.albano@unipa.it</a>, Mariangela Sciandra <a href="mailto:mariangela.sciandra@unipa.it">mariangela.sciandra@unipa.it</a>, and Antonella Plaia <a href="mailto:antonella.plaia@unipa.it">antonella.plaia@unipa.it</a></p>


<h3>References</h3>

<p>Albano, A., Sciandra, M., and Plaia, A. (2023): &quot;A weighted distance-based approach with boosted decision trees for label ranking.&quot; <em>Expert Systems with Applications</em>.
</p>
<p>Alfaro, E., Gamez, M., and Garcia, N. (2013): &quot;adabag: An R Package for Classification with Boosting and Bagging.&quot; <em>Journal of Statistical Software</em>, Vol. 54, 2, pp. 1&ndash;35.
</p>
<p>Breiman, L. (1998): &quot;Arcing classifiers.&quot; <em>The Annals of Statistics</em>, Vol. 26, 3, pp. 801&ndash;849.
</p>
<p>D'Ambrosio, A.[aut, cre], Amodio, S. [ctb], Mazzeo, G. [ctb], Albano, A. [ctb], Plaia, A. [ctb] (2023). ConsRank: Compute the Median Ranking(s) According to the Kemeny's Axiomatic Approach. R package version 2.1.3, https://cran.r-project.org/package=ConsRank.
</p>
<p>Freund, Y., and Schapire, R.E. (1996): &quot;Experiments with a new boosting algorithm.&quot; In <em>Proceedings of the Thirteenth International Conference on Machine Learning</em>, pp. 148&ndash;156, Morgan Kaufmann.
</p>
<p>Plaia, A., Buscemi, S., Furnkranz, J., and Mencıa, E.L. (2021): &quot;Comparing boosting and bagging for decision trees of rankings.&quot; <em>Journal of Classification</em>, pages 1–22.
</p>
<p>Zhu, J., Zou, H., Rosset, S., and Hastie, T. (2009): &quot;Multi-class AdaBoost.&quot; <em>Statistics and Its Interface</em>, 2, pp. 349&ndash;360.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  # Load simulated ranking data
  data(simulatedRankingData)
  x &lt;- simulatedRankingData$x
  y &lt;- simulatedRankingData$y

  # Prepare the data with item weights
  dati &lt;- prep_data(y, x, iw = c(2, 5, 5, 2))

  # Divide the data into training and test sets
  set.seed(12345)
  samp &lt;- sample(nrow(dati))
  l &lt;- length(dati[, 1])
  sub &lt;- sample(1:l, 2 * l / 3)
  data_sub1 &lt;- dati[sub, ]
  data_test1 &lt;- dati[-sub, ]

  # Apply ensemble ranking with AdaBoost.M1
  boosting_1 &lt;- Ensemble_ranking_IW(
    Label ~ .,
    data = data_sub1,
    iw = c(2, 5, 5, 2),
    mfinal = 3,
    coeflearn = "Breiman",
    control = rpart.control(maxdepth = 4, cp = -1),
    algo = "boosting",
    bin = FALSE
  )

  # Evaluate the performance
  test_boosting1 &lt;- errorevol_ranking_vector_IW(boosting_1, 
    newdata = data_test1, iw=c(2,5,5,2), squared = FALSE)
  test_boosting1.1 &lt;- errorevol_ranking_vector_IW(boosting_1, 
    newdata = data_sub1, iw=c(2,5,5,2), squared = FALSE)

  # Plot the error evolution
  plot.errorevol(test_boosting1, test_boosting1.1)
  
## End(Not run)
</code></pre>

<hr>
<h2 id='errorevol'> Shows the error evolution of the ensemble </h2><span id='topic+errorevol'></span>

<h3>Description</h3>

<p>Calculates the error evolution of an AdaBoost.M1, AdaBoost-SAMME or Bagging classifier for a data frame
as the ensemble size grows  
</p>


<h3>Usage</h3>

<pre><code class='language-R'> errorevol(object, newdata)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="errorevol_+3A_object">object</code></td>
<td>

<p>This object must be the output of one of the functions <code>bagging</code> or <code>boosting</code>.
This is assumed to be the result of some function that produces an object with two components named <code>formula</code>  and <code>trees</code>,  as those 
returned for instance by the <code>bagging</code> function.
</p>
</td></tr>
<tr><td><code id="errorevol_+3A_newdata">newdata</code></td>
<td>
<p> Could be the same data frame used in <code>object</code> or a new one</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This can be useful to see how fast <code>Bagging</code>,  <code>boosting</code> reduce the error of the ensemble. in addition,
it can detect the presence of overfitting and, therefore, the convenience of pruning the ensemble using <code>predict.bagging</code> or <code>predict.boosting</code>.
</p>


<h3>Value</h3>

<p>An object of class <code>errorevol</code>, which is a list with only one component:  
</p>
<table>
<tr><td><code>error</code></td>
<td>
<p>a vector with the error evolution.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Esteban Alfaro-Cortes <a href="mailto:Esteban.Alfaro@uclm.es">Esteban.Alfaro@uclm.es</a>, Matias Gamez-Martinez <a href="mailto:Matias.Gamez@uclm.es">Matias.Gamez@uclm.es</a> and Noelia Garcia-Rubio <a href="mailto:Noelia.Garcia@uclm.es">Noelia.Garcia@uclm.es</a> </p>


<h3>References</h3>

<p>Alfaro, E., Gamez, M. and Garcia, N. (2013): &ldquo;adabag: An R Package for Classification with Boosting and Bagging&rdquo;. Journal of Statistical Software, Vol 54, 2, pp. 1&ndash;35.
</p>
<p>Alfaro, E., Garcia, N., Gamez, M. and Elizondo, D. (2008): &ldquo;Bankruptcy forecasting: An empirical comparison of AdaBoost and neural networks&rdquo;. Decision Support Systems, 45, pp. 110&ndash;122.
</p>
<p>Breiman, L. (1996): &ldquo;Bagging predictors&rdquo;. Machine Learning, Vol 24, 2, pp.123&ndash;140.
</p>
<p>Freund, Y. and Schapire, R.E. (1996): &ldquo;Experiments with a new boosting algorithm&rdquo;. In Proceedings of the Thirteenth International Conference on Machine Learning, pp. 148&ndash;156, Morgan Kaufmann. 
</p>
<p>Zhu, J., Zou, H., Rosset, S. and Hastie, T. (2009): &ldquo;Multi-class AdaBoost&rdquo;. Statistics and Its Interface, 2, pp. 349&ndash;360. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+boosting">boosting</a></code>,
<code><a href="#topic+predict.boosting">predict.boosting</a></code>,
<code><a href="#topic+bagging">bagging</a></code>,
<code><a href="#topic+predict.bagging">predict.bagging</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(mlbench)
data(BreastCancer)
l &lt;- length(BreastCancer[,1])
sub &lt;- sample(1:l,2*l/3)
cntrl &lt;- rpart.control(maxdepth = 3, minsplit = 0,  cp = -1)

BC.adaboost &lt;- boosting(Class ~.,data=BreastCancer[sub,-1],mfinal=5, control=cntrl)
BC.adaboost.pred &lt;- predict.boosting(BC.adaboost,newdata=BreastCancer[-sub,-1])

errorevol(BC.adaboost,newdata=BreastCancer[-sub,-1])-&gt;evol.test
errorevol(BC.adaboost,newdata=BreastCancer[sub,-1])-&gt;evol.train

plot.errorevol(evol.test,evol.train)
abline(h=min(evol.test[[1]]), col="red",lty=2,lwd=2)
abline(h=min(evol.train[[1]]), col="blue",lty=2,lwd=2)


</code></pre>

<hr>
<h2 id='errorevol_ranking_vector_IW'>Calculate the error evolution and final predictions of an item-weighted ensemble for rankings</h2><span id='topic+errorevol_ranking_vector_IW'></span>

<h3>Description</h3>

<p>This function calculates the error evolution and final predictions of an item-weigthed ensemble method for ranking data (Albano et al., 2023). </p>


<h3>Usage</h3>

<pre><code class='language-R'>  errorevol_ranking_vector_IW(object, newdata, iw, squared = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="errorevol_ranking_vector_IW_+3A_object">object</code></td>
<td>
<p>an object of class 'bagging' or 'boosting' generated by the <code>Ensemble_ranking_IW</code> function.</p>
</td></tr>
<tr><td><code id="errorevol_ranking_vector_IW_+3A_newdata">newdata</code></td>
<td>
<p>a data frame that can be the same as the one used in the  <code>object</code> or a new one. Continuous  variables are allowed, while the dummy coding should be used for categorical variables. It must be the output of the <code>prep_data</code> function.</p>
</td></tr>
<tr><td><code id="errorevol_ranking_vector_IW_+3A_iw">iw</code></td>
<td>
<p>a weighting vector or matrix. For coherence, <code>iw</code> should be the same vector/matrix used in <code>Ensemble_ranking_IW(...)</code>.</p>
</td></tr>
<tr><td><code id="errorevol_ranking_vector_IW_+3A_squared">squared</code></td>
<td>
<p>logical value indicating whether squared weighting should be used in the final prediction. Default is <code>FALSE</code>. When set to <code>TRUE</code>, it corresponds to utilizing the AdaBoost.R.M3 algorithm defined by Albano et al. (2023).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes the error and final predictions for a boosting or bagging ranking model using item weighting.</p>


<h3>Value</h3>

<p>An object of class 'errorevol'. It has two components:
</p>
<table>
<tr><td><code>error</code></td>
<td>
<p> a vector with the error values at each ensemble iteration</p>
</td></tr>
<tr><td><code>final_prediction</code></td>
<td>
<p> a data frame of final predictions for each observation in <code>newdata</code>.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Albano, A., Sciandra, M., and Plaia, A. (2023): &quot;A weighted distance-based approach with boosted decision trees for label ranking.&quot; <em>Expert Systems with Applications</em>.
</p>
<p>Alfaro, E., Gamez, M., and Garcia, N. (2013): &quot;adabag: An R Package for Classification with Boosting and Bagging.&quot; <em>Journal of Statistical Software</em>, Vol. 54, 2, pp. 1&ndash;35.
</p>
<p>Breiman, L. (1998): &quot;Arcing classifiers.&quot; <em>The Annals of Statistics</em>, Vol. 26, 3, pp. 801&ndash;849.
</p>
<p>D'Ambrosio, A.[aut, cre], Amodio, S. [ctb], Mazzeo, G. [ctb], Albano, A. [ctb], Plaia, A. [ctb] (2023). ConsRank: Compute the Median Ranking(s) According to the Kemeny's Axiomatic Approach. R package version 2.1.3, https://cran.r-project.org/package=ConsRank.
</p>
<p>Freund, Y., and Schapire, R.E. (1996): &quot;Experiments with a new boosting algorithm.&quot; In <em>Proceedings of the Thirteenth International Conference on Machine Learning</em>, pp. 148&ndash;156, Morgan Kaufmann.
</p>
<p>Plaia, A., Buscemi, S., Furnkranz, J., and Mencıa, E.L. (2021): &quot;Comparing boosting and bagging for decision trees of rankings.&quot; <em>Journal of Classification</em>, pages 1–22.
</p>
<p>Zhu, J., Zou, H., Rosset, S., and Hastie, T. (2009): &quot;Multi-class AdaBoost.&quot; <em>Statistics and Its Interface</em>, 2, pp. 349&ndash;360.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  # Load simulated ranking data
  data(simulatedRankingData)
  x &lt;- simulatedRankingData$x
  y &lt;- simulatedRankingData$y

  # Prepare the data with item weights
  dati &lt;- prep_data(y, x, iw = c(2, 5, 5, 2))

  # Divide the data into training and test sets
  set.seed(12345)
  samp &lt;- sample(nrow(dati))
  l &lt;- length(dati[, 1])
  sub &lt;- sample(1:l, 2 * l / 3)
  data_sub1 &lt;- dati[sub, ]
  data_test1 &lt;- dati[-sub, ]

  # Apply ensemble ranking with AdaBoost.M1
  boosting_1 &lt;- Ensemble_ranking_IW(
    Label ~ .,
    data = data_sub1,
    iw = c(2, 5, 5, 2),
    mfinal = 3,
    coeflearn = "Breiman",
    control = rpart.control(maxdepth = 4, cp = -1),
    algo = "boosting",
    bin = FALSE
  )

  # Evaluate the performance
  test_boosting1 &lt;- errorevol_ranking_vector_IW(boosting_1, 
    newdata = data_test1, iw=c(2,5,5,2), squared = FALSE)
  test_boosting1.1 &lt;- errorevol_ranking_vector_IW(boosting_1, 
    newdata = data_sub1, iw=c(2,5,5,2), squared = FALSE)

  # Plot the error evolution
  plot.errorevol(test_boosting1, test_boosting1.1)
  
## End(Not run)
</code></pre>

<hr>
<h2 id='importanceplot'>
Plots the variables relative importance 
</h2><span id='topic+importanceplot'></span>

<h3>Description</h3>

<p>Plots the relative importance of each variable in the classification task.
This measure takes into account the gain of the Gini index given by a variable in a tree and, in the boosting case, the weight of this tree. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>importanceplot(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="importanceplot_+3A_object">object</code></td>
<td>

<p>fitted model object of class <code>boosting</code> or <code>bagging</code>. This is assumed to be the result
of some function that produces an object with a component named <code>importance</code> as that 
returned by the <code>boosting</code> and <code>bagging</code> functions.
</p>
</td></tr>
<tr><td><code id="importanceplot_+3A_...">...</code></td>
<td>

<p>further arguments passed to or from other methods.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For this goal, the <code>varImp</code> function of the <code>caret</code> package is used to get 
the gain of the Gini index of the variables in each tree.
</p>


<h3>Value</h3>

<p>A labeled plot is produced on the current graphics device (one being opened if needed). 
</p>


<h3>Author(s)</h3>

<p>Esteban Alfaro-Cortes <a href="mailto:Esteban.Alfaro@uclm.es">Esteban.Alfaro@uclm.es</a>, Matias Gamez-Martinez <a href="mailto:Matias.Gamez@uclm.es">Matias.Gamez@uclm.es</a> and Noelia Garcia-Rubio <a href="mailto:Noelia.Garcia@uclm.es">Noelia.Garcia@uclm.es</a> </p>


<h3>References</h3>

<p>Alfaro, E., Gamez, M. and Garcia, N. (2013): &ldquo;adabag: An R Package for Classification with Boosting and Bagging&rdquo;. Journal of Statistical Software, Vol 54, 2, pp. 1&ndash;35.
</p>
<p>Alfaro, E., Garcia, N., Gamez, M. and Elizondo, D. (2008): &ldquo;Bankruptcy forecasting: An empirical comparison of AdaBoost and neural networks&rdquo;. Decision Support Systems, 45, pp. 110&ndash;122.
</p>
<p>Breiman, L. (1996): &ldquo;Bagging predictors&rdquo;. Machine Learning, Vol 24, 2, pp.123&ndash;140.
</p>
<p>Freund, Y. and Schapire, R.E. (1996): &ldquo;Experiments with a new boosting algorithm&rdquo;. In Proceedings of the Thirteenth International Conference on Machine Learning, pp. 148&ndash;156, Morgan Kaufmann. 
</p>
<p>Zhu, J., Zou, H., Rosset, S. and Hastie, T. (2009): &ldquo;Multi-class AdaBoost&rdquo;. Statistics and Its Interface, 2, pp. 349&ndash;360. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+boosting">boosting</a></code>,
<code><a href="#topic+bagging">bagging</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Examples
#Iris example
library(rpart)
data(iris)
sub &lt;- c(sample(1:50, 25), sample(51:100, 25), sample(101:150, 25))
iris.adaboost &lt;- boosting(Species ~ ., data=iris[sub,], mfinal=3)
importanceplot(iris.adaboost)

#Examples with bagging
#iris.bagging &lt;- bagging(Species ~ ., data=iris[sub,], mfinal=5)
#importanceplot(iris.bagging, horiz=TRUE, cex.names=.6)

</code></pre>

<hr>
<h2 id='MarginOrderedPruning.Bagging'>MarginOrderedPruning.Bagging</h2><span id='topic+MarginOrderedPruning.Bagging'></span>

<h3>Description</h3>

<p>Margin-based ordered aggregation for bagging pruning</p>


<h3>Usage</h3>

<pre><code class='language-R'>MarginOrderedPruning.Bagging(baggingObject, trainingset, pruningset, 
	marginType = "unsupervised", doTrace = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MarginOrderedPruning.Bagging_+3A_baggingobject">baggingObject</code></td>
<td>

<p>fitted model object of class <code>bagging</code> 
</p>
</td></tr>
<tr><td><code id="MarginOrderedPruning.Bagging_+3A_trainingset">trainingset</code></td>
<td>

<p>the training set of the <code>bagging</code> object
</p>
</td></tr>
<tr><td><code id="MarginOrderedPruning.Bagging_+3A_pruningset">pruningset</code></td>
<td>

<p>a set aside dataset for <code>bagging</code> pruning
</p>
</td></tr>
<tr><td><code id="MarginOrderedPruning.Bagging_+3A_margintype">marginType</code></td>
<td>

<p>if &quot;unsupervised&quot; (by default) the margin is the difference between the proportions of votes
of the first and second most popular classes. Else the margin is calculated as the 
difference between the proportion of votes of the correct class and the most popular among the other classes
</p>
</td></tr>
<tr><td><code id="MarginOrderedPruning.Bagging_+3A_dotrace">doTrace</code></td>
<td>

<p>If set to <code>TRUE</code>, give a more verbose output as <code>MarginOrderedPruning.Bagging</code> is running
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with the following components:
</p>
<table>
<tr><td><code>prunedBagging</code></td>
<td>
<p>a pruned <code>bagging</code> object</p>
</td></tr>
<tr><td><code>AccuracyOrderedEnsemblePruningSet</code></td>
<td>
<p>Accuracy of each ordered ensemble 
on pruning set</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Questions about this function should be sent to Li Guo
</p>


<h3>Author(s)</h3>

<p>Li Guo <a href="mailto:guoli84@hotmail.com">guoli84@hotmail.com</a>
</p>


<h3>References</h3>

<p>Guo, L. and Boukir, S. (2013): &quot;Margin-based ordered aggregation for ensemble pruning&quot;. Pattern Recognition
Letters, 34(6), 603-609.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bagging">bagging</a></code>,
<code><a href="#topic+predict.bagging">predict.bagging</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## mlbench package should be loaded
library(mlbench)
data(Satellite)
## Separate data into 3 parts: training set, pruning set and test set
ind &lt;- sample(3, nrow(Satellite), replace = TRUE, prob=c(0.3, 0.2,0.5))

## create bagging with training set
#increase mfinal in your own execution of this example to see 
#the real usefulness of this function
Satellite.bagging&lt;-bagging(classes~.,data=Satellite[ind==1,],mfinal=3)
#Satellite.bagging.pred&lt;-predict(Satellite.bagging,Satellite[ind==3,])

##pruning bagging
Satellite.bagging.pruning&lt;-MarginOrderedPruning.Bagging(Satellite.bagging,
Satellite[ind==1,],Satellite[ind==2,])
#Satellite.bagging.pruning.pred&lt;-predict(Satellite.bagging.pruning$prunedBagging,
#Satellite[ind==3,])

## create bagging with training and pruning set
#This example has been hidden to fulfill execution time &lt;5s 
#Satellite.bagging2&lt;-bagging(classes~.,data=Satellite[ind!=3,],25)
#Satellite.bagging2.pred&lt;-predict(Satellite.bagging2,Satellite[ind==3,])

</code></pre>

<hr>
<h2 id='margins'> Calculates the margins </h2><span id='topic+margins'></span>

<h3>Description</h3>

<p> Calculates the margins of an AdaBoost.M1, AdaBoost-SAMME or Bagging classifier for a data frame</p>


<h3>Usage</h3>

<pre><code class='language-R'> margins(object, newdata)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="margins_+3A_object">object</code></td>
<td>
<p>This object must be the output of one of the functions <code>bagging</code>, <code>boosting</code>, <code>predict.bagging</code> or <code>predict.boosting</code>.
This is assumed to be the result of some function that produces an object with two components named <code>formula</code>  and <code>class</code>,  as those 
returned for instance by the <code>bagging</code> function. </p>
</td></tr>
<tr><td><code id="margins_+3A_newdata">newdata</code></td>
<td>
<p> The same data frame used for building the <code>object</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Intuitively, the margin for an observation is related to the certainty of its classification. It is calculated as the difference between
the support of the correct class and the maximum support of an incorrect class</p>


<h3>Value</h3>

<p>An object of class <code>margins</code>, which is a list with only one component:  
</p>
<table>
<tr><td><code>margins</code></td>
<td>
<p>a vector with the margins. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Esteban Alfaro-Cortes <a href="mailto:Esteban.Alfaro@uclm.es">Esteban.Alfaro@uclm.es</a>, Matias Gamez-Martinez <a href="mailto:Matias.Gamez@uclm.es">Matias.Gamez@uclm.es</a> and Noelia Garcia-Rubio <a href="mailto:Noelia.Garcia@uclm.es">Noelia.Garcia@uclm.es</a> </p>


<h3>References</h3>

<p>Alfaro, E., Gamez, M. and Garcia, N. (2013): &ldquo;adabag: An R Package for Classification with Boosting and Bagging&rdquo;. Journal of Statistical Software, Vol 54, 2, pp. 1&ndash;35.
</p>
<p>Alfaro, E., Garcia, N., Gamez, M. and Elizondo, D. (2008): &ldquo;Bankruptcy forecasting: An empirical comparison of AdaBoost and neural networks&rdquo;. Decision Support Systems, 45, pp. 110&ndash;122.
</p>
<p>Schapire, R.E., Freund, Y., Bartlett, P. and Lee, W.S. (1998): &ldquo;Boosting the margin: A new explanation for the effectiveness of voting methods&rdquo;. The Annals of Statistics, vol 26, 5, pp. 1651&ndash;1686.
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+bagging">bagging</a></code>,
<code><a href="#topic+boosting">boosting</a></code>,
<code><a href="#topic+plot.margins">plot.margins</a></code>,
<code><a href="#topic+predict.boosting">predict.boosting</a></code>,
<code><a href="#topic+predict.bagging">predict.bagging</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#Iris example
library(rpart)
data(iris)
sub &lt;- c(sample(1:50, 25), sample(51:100, 25), sample(101:150, 25))
iris.adaboost &lt;- boosting(Species ~ ., data=iris[sub,], mfinal=3)
margins(iris.adaboost,iris[sub,])-&gt;iris.margins # training set
plot.margins(iris.margins)

# test set
iris.predboosting&lt;- predict.boosting(iris.adaboost, newdata=iris[-sub,])
margins(iris.predboosting,iris[-sub,])-&gt;iris.predmargins 
plot.margins(iris.predmargins,iris.margins)

#Examples with bagging
iris.bagging &lt;- bagging(Species ~ ., data=iris[sub,], mfinal=3)
margins(iris.bagging,iris[sub,])-&gt;iris.bagging.margins # training set

iris.predbagging&lt;- predict.bagging(iris.bagging, newdata=iris[-sub,])
margins(iris.predbagging,iris[-sub,])-&gt;iris.bagging.predmargins # test set
par(bg="lightyellow")
plot.margins(iris.bagging.predmargins,iris.bagging.margins)


</code></pre>

<hr>
<h2 id='plot.errorevol'>
Plots the error evolution of the ensemble
</h2><span id='topic+plot.errorevol'></span>

<h3>Description</h3>

<p>Plots the previously calculated error evolution of an AdaBoost.M1, AdaBoost-SAMME or Bagging classifier for a data frame
as the ensemble size grows  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'errorevol'
plot(x, y = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.errorevol_+3A_x">x</code></td>
<td>
<p>An object of class <code>errorevol</code>. This is assumed to be the result
of some function that produces an object with a component named <code>error</code> as that 
returned by the <code>errorevol</code> function. </p>
</td></tr>
<tr><td><code id="plot.errorevol_+3A_y">y</code></td>
<td>

<p>This argument can be used to represent in the same plot
the evolution of the test and train errors, <code>x</code> and <code>y</code>, respectively.
Should be <code>NULL</code> (by default) or an object of class <code>errorevol</code>.
</p>
</td></tr>
<tr><td><code id="plot.errorevol_+3A_...">...</code></td>
<td>

<p>further arguments passed to or from other methods.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This can be useful to see how fast <code>bagging</code> or  <code>boosting</code> reduce the error of the ensemble. in addition,
it can detect the presence of overfitting and, therefore, the convenience of pruning the ensemble using <code>predict.bagging</code> or <code>predict.boosting</code>.
</p>


<h3>Value</h3>

<p>A labeled plot is produced on the current graphics device (one being opened if needed). 
</p>


<h3>Author(s)</h3>

<p>Esteban Alfaro-Cortes <a href="mailto:Esteban.Alfaro@uclm.es">Esteban.Alfaro@uclm.es</a>, Matias Gamez-Martinez <a href="mailto:Matias.Gamez@uclm.es">Matias.Gamez@uclm.es</a> and Noelia Garcia-Rubio <a href="mailto:Noelia.Garcia@uclm.es">Noelia.Garcia@uclm.es</a> </p>


<h3>References</h3>

<p>Alfaro, E., Gamez, M. and Garcia, N. (2013): &ldquo;adabag: An R Package for Classification with Boosting and Bagging&rdquo;. Journal of Statistical Software, Vol 54, 2, pp. 1&ndash;35.
</p>
<p>Alfaro, E., Garcia, N., Gamez, M. and Elizondo, D. (2008): &ldquo;Bankruptcy forecasting: An empirical comparison of AdaBoost and neural networks&rdquo;. Decision Support Systems, 45, pp. 110&ndash;122.
</p>
<p>Breiman, L. (1996): &ldquo;Bagging predictors&rdquo;. Machine Learning, Vol 24, 2, pp.123&ndash;140.
</p>
<p>Freund, Y. and Schapire, R.E. (1996): &ldquo;Experiments with a new boosting algorithm&rdquo;. In Proceedings of the Thirteenth International Conference on Machine Learning, pp. 148&ndash;156, Morgan Kaufmann. 
</p>
<p>Zhu, J., Zou, H., Rosset, S. and Hastie, T. (2009): &ldquo;Multi-class AdaBoost&rdquo;. Statistics and Its Interface, 2, pp. 349&ndash;360. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+boosting">boosting</a></code>,
<code><a href="#topic+predict.boosting">predict.boosting</a></code>,
<code><a href="#topic+bagging">bagging</a></code>,
<code><a href="#topic+predict.bagging">predict.bagging</a></code>,
<code><a href="#topic+errorevol">errorevol</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
train &lt;- c(sample(1:50, 25), sample(51:100, 25), sample(101:150, 25))

cntrl&lt;-rpart.control(maxdepth=1)
#increase mfinal in your own execution of this example to see 
#the real usefulness of this function
iris.adaboost &lt;- boosting(Species ~ ., data=iris[train,], mfinal=10, control=cntrl)

#Error evolution along the iterations in training set 
errorevol(iris.adaboost,iris[train,])-&gt;evol.train
plot.errorevol(evol.train)

#comparing error evolution in training and test set
errorevol(iris.adaboost,iris[-train,])-&gt;evol.test
plot.errorevol(evol.test, evol.train)

# See the help of the functions error evolution and boosting 
# for more examples of the use of the error evolution

</code></pre>

<hr>
<h2 id='plot.margins'>
Plots the margins of the ensemble
</h2><span id='topic+plot.margins'></span>

<h3>Description</h3>

<p> Plots the previously calculated margins of an AdaBoost.M1, AdaBoost-SAMME or Bagging classifier for a data frame</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'margins'
plot(x, y = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.margins_+3A_x">x</code></td>
<td>

<p>An object of class <code>margins</code>. This is assumed to be the result
of some function that produces an object with a component named <code>margins</code> as that 
returned by the <code>margins</code> function. 
</p>
</td></tr>
<tr><td><code id="plot.margins_+3A_y">y</code></td>
<td>

<p>This argument can be used to represent in the same plot
the margins in the test and train sets, <code>x</code> and <code>y</code>, respectively.
Should be <code>NULL</code> (by default) or an object of class <code>margins</code>.
</p>
</td></tr>
<tr><td><code id="plot.margins_+3A_...">...</code></td>
<td>

<p>further arguments passed to or from other methods.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Intuitively, the margin for an observation is related to the certainty of its classification. It is calculated as the difference between
the support of the correct class and the maximum support of an incorrect class
</p>


<h3>Value</h3>

<p>A labeled plot is produced on the current graphics device (one being opened if needed). 
</p>


<h3>Author(s)</h3>

<p>Esteban Alfaro-Cortes <a href="mailto:Esteban.Alfaro@uclm.es">Esteban.Alfaro@uclm.es</a>, Matias Gamez-Martinez <a href="mailto:Matias.Gamez@uclm.es">Matias.Gamez@uclm.es</a> and Noelia Garcia-Rubio <a href="mailto:Noelia.Garcia@uclm.es">Noelia.Garcia@uclm.es</a> </p>


<h3>References</h3>

<p>Alfaro, E., Gamez, M. and Garcia, N. (2013): &ldquo;adabag: An R Package for Classification with Boosting and Bagging&rdquo;. Journal of Statistical Software, Vol 54, 2, pp. 1&ndash;35.
</p>
<p>Alfaro, E., Garcia, N., Gamez, M. and Elizondo, D. (2008): &ldquo;Bankruptcy forecasting: An empirical comparison of AdaBoost and neural networks&rdquo;. Decision Support Systems, 45, pp. 110&ndash;122.
</p>
<p>Schapire, R.E., Freund, Y., Bartlett, P. and Lee, W.S. (1998): &ldquo;Boosting the margin: A new explanation for the effectiveness of voting methods&rdquo;. The Annals of Statistics, vol 26, 5, pp. 1651&ndash;1686.
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+margins">margins</a></code>,
<code><a href="#topic+boosting">boosting</a></code>,
<code><a href="#topic+predict.boosting">predict.boosting</a></code>,
<code><a href="#topic+bagging">bagging</a></code>,
<code><a href="#topic+predict.bagging">predict.bagging</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mlbench)
data(BreastCancer)
l &lt;- length(BreastCancer[,1])
sub &lt;- sample(1:l,2*l/3)
cntrl &lt;- rpart.control(maxdepth = 3, minsplit = 0,  cp = -1)

BC.adaboost &lt;- boosting(Class ~.,data=BreastCancer[sub,-1],mfinal=5, control=cntrl)
BC.adaboost.pred &lt;- predict.boosting(BC.adaboost,newdata=BreastCancer[-sub,-1])

BC.margins&lt;-margins(BC.adaboost,BreastCancer[sub,-1]) # training set
BC.predmargins&lt;-margins(BC.adaboost.pred,BreastCancer[-sub,-1]) # test set
plot.margins(BC.predmargins,BC.margins)


</code></pre>

<hr>
<h2 id='predict.bagging'>Predicts from a fitted bagging object</h2><span id='topic+predict.bagging'></span>

<h3>Description</h3>

<p>Classifies a dataframe using a fitted bagging object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bagging'
predict(object, newdata, newmfinal=length(object$trees), ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.bagging_+3A_object">object</code></td>
<td>
<p>fitted model object of class <code>bagging</code>. This is assumed to be the result
of some function that produces an object with the same named components as that 
returned by the <code>bagging</code> function. </p>
</td></tr>
<tr><td><code id="predict.bagging_+3A_newdata">newdata</code></td>
<td>
<p>data frame containing the values at which predictions are required. The predictors referred 
to in the right side of <code>formula(object)</code> must be present by name in <code>newdata</code>.</p>
</td></tr>
<tr><td><code id="predict.bagging_+3A_newmfinal">newmfinal</code></td>
<td>
<p>The number of trees of the bagging object to be used in the prediction.
This argument allows the user to prune the ensemble. By default all the trees 
in the bagging object are used </p>
</td></tr>
<tr><td><code id="predict.bagging_+3A_...">...</code></td>
<td>
<p> further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>predict.bagging</code>, which is a list with the following components:
</p>
<table>
<tr><td><code>formula</code></td>
<td>
<p>the formula used.</p>
</td></tr>
<tr><td><code>votes</code></td>
<td>
<p>a matrix describing, for each observation, the number of trees that assigned it to each class.</p>
</td></tr>
<tr><td><code>prob</code></td>
<td>
<p>a matrix describing, for each observation, the posterior probability or degree of support of each class. 
These probabilities are calculated using the proportion of votes in the final ensemble.</p>
</td></tr>
<tr><td><code>class</code></td>
<td>
<p>the class predicted by the ensemble classifier.</p>
</td></tr>
<tr><td><code>confusion</code></td>
<td>
<p>the confusion matrix which compares the real class with the predicted one.</p>
</td></tr>
<tr><td><code>error</code></td>
<td>
<p>returns the average error.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Esteban Alfaro-Cortes <a href="mailto:Esteban.Alfaro@uclm.es">Esteban.Alfaro@uclm.es</a>, Matias Gamez-Martinez <a href="mailto:Matias.Gamez@uclm.es">Matias.Gamez@uclm.es</a> and Noelia Garcia-Rubio <a href="mailto:Noelia.Garcia@uclm.es">Noelia.Garcia@uclm.es</a> </p>


<h3>References</h3>

<p>Alfaro, E., Gamez, M. and Garcia, N. (2013): &ldquo;adabag: An R Package for Classification with Boosting and Bagging&rdquo;. Journal of Statistical Software, Vol 54, 2, pp. 1&ndash;35.
</p>
<p>Alfaro, E., Garcia, N., Gamez, M. and Elizondo, D. (2008): &ldquo;Bankruptcy forecasting: An empirical comparison of AdaBoost and neural networks&rdquo;. Decision Support Systems, 45, pp. 110&ndash;122.
</p>
<p>Breiman, L. (1996): &quot;Bagging predictors&quot;. Machine Learning, Vol 24, 2, pp. 123&ndash;140.
</p>
<p>Breiman, L. (1998). &quot;Arcing classifiers&quot;. The Annals of Statistics, Vol 26, 3, pp. 801&ndash;849. </p>


<h3>See Also</h3>

 
<p><code><a href="#topic+bagging">bagging</a></code>,
<code><a href="#topic+bagging.cv">bagging.cv</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>#library(rpart)
#data(iris)
#sub &lt;- c(sample(1:50, 25), sample(51:100, 25), sample(101:150, 25))
#iris.bagging &lt;- bagging(Species ~ ., data=iris[sub,], mfinal=5)
#iris.predbagging&lt;- predict.bagging(iris.bagging, newdata=iris[-sub,])
#iris.predbagging

## rpart and mlbench libraries should be loaded
library(rpart)
library(mlbench)
data(BreastCancer)
l &lt;- length(BreastCancer[,1])
sub &lt;- sample(1:l,2*l/3)
BC.bagging &lt;- bagging(Class ~.,data=BreastCancer[,-1],mfinal=5, 
control=rpart.control(maxdepth=3))
BC.bagging.pred &lt;- predict.bagging(BC.bagging,newdata=BreastCancer[-sub,-1])
BC.bagging.pred$prob
BC.bagging.pred$confusion
BC.bagging.pred$error


</code></pre>

<hr>
<h2 id='predict.boosting'> Predicts from a fitted boosting object </h2><span id='topic+predict.boosting'></span>

<h3>Description</h3>

<p>Classifies a dataframe using a fitted boosting object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'boosting'
predict(object, newdata, newmfinal=length(object$trees), ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.boosting_+3A_object">object</code></td>
<td>
<p>fitted model object of class <code>boosting</code>. This is assumed to be the result
of some function that produces an object with the same named components as that 
returned by the <code>boosting</code> function. </p>
</td></tr>
<tr><td><code id="predict.boosting_+3A_newdata">newdata</code></td>
<td>
<p>data frame containing the values at which predictions are required. The predictors referred 
to in the right side of <code>formula(object)</code> must be present by name in newdata.</p>
</td></tr>
<tr><td><code id="predict.boosting_+3A_newmfinal">newmfinal</code></td>
<td>
<p>The number of trees of the boosting object to be used in the prediction.
This argument allows the user to prune the ensemble. By default all the trees 
in <code>object</code> are used </p>
</td></tr>
<tr><td><code id="predict.boosting_+3A_...">...</code></td>
<td>
<p> further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class predict.boosting, which is a list with the following components:
</p>
<table>
<tr><td><code>formula</code></td>
<td>
<p>the formula used.  </p>
</td></tr>
<tr><td><code>votes</code></td>
<td>
<p>a matrix describing, for each observation, the number of trees that assigned it to each class, weighting each tree by its <code>alpha</code> coefficient.  </p>
</td></tr>
<tr><td><code>prob</code></td>
<td>
<p>a matrix describing, for each observation, the posterior probability or degree of support of each class. 
These probabilities are calculated using the proportion of votes in the final ensemble.</p>
</td></tr>
<tr><td><code>class</code></td>
<td>
<p>the class predicted by the ensemble classifier.  </p>
</td></tr>
<tr><td><code>confusion</code></td>
<td>
<p>the confusion matrix which compares the real class with the predicted one.</p>
</td></tr>
<tr><td><code>error</code></td>
<td>
<p>returns the average error.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Esteban Alfaro-Cortes <a href="mailto:Esteban.Alfaro@uclm.es">Esteban.Alfaro@uclm.es</a>, Matias Gamez-Martinez <a href="mailto:Matias.Gamez@uclm.es">Matias.Gamez@uclm.es</a> and Noelia Garcia-Rubio <a href="mailto:Noelia.Garcia@uclm.es">Noelia.Garcia@uclm.es</a> </p>


<h3>References</h3>

<p>Alfaro, E., Gamez, M. and Garcia, N. (2013): &ldquo;adabag: An R Package for Classification with Boosting and Bagging&rdquo;. Journal of Statistical Software, Vol 54, 2, pp. 1&ndash;35.
</p>
<p>Alfaro, E., Garcia, N., Gamez, M. and Elizondo, D. (2008): &ldquo;Bankruptcy forecasting: An empirical comparison of AdaBoost and neural networks&rdquo;. Decision Support Systems, 45, pp. 110&ndash;122.
</p>
<p>Breiman, L. (1998): &quot;Arcing classifiers&quot;. The Annals of Statistics, Vol 26, 3, pp. 801&ndash;849. 
</p>
<p>Freund, Y. and Schapire, R.E. (1996): &quot;Experiments with a new boosting algorithm&quot;. En Proceedings of the Thirteenth International Conference on Machine Learning, pp. 148&ndash;156, Morgan Kaufmann. 
</p>
<p>Zhu, J., Zou, H., Rosset, S. and Hastie, T. (2009): &ldquo;Multi-class AdaBoost&rdquo;. Statistics and Its Interface, 2, pp. 349&ndash;360. 
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+boosting">boosting</a></code>,
<code><a href="#topic+boosting.cv">boosting.cv</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>## rpart library should be loaded
#This example has been hidden to fulfill execution time &lt;5s 
#library(rpart)
#data(iris)
#sub &lt;- c(sample(1:50, 25), sample(51:100, 25), sample(101:150, 25))
#iris.adaboost &lt;- boosting(Species ~ ., data=iris[sub,], mfinal=10)
#iris.predboosting&lt;- predict.boosting(iris.adaboost, newdata=iris[-sub,])
#iris.predboosting$prob

## rpart and mlbench libraries should be loaded
## Comparing the test error of rpart and adaboost.M1
library(rpart)
library(mlbench)
data(BreastCancer)
l &lt;- length(BreastCancer[,1])
sub &lt;- sample(1:l,2*l/3)

BC.rpart &lt;- rpart(Class~.,data=BreastCancer[sub,-1], maxdepth=3)
BC.rpart.pred &lt;- predict(BC.rpart,newdata=BreastCancer[-sub,-1],type="class")
tb &lt;-table(BC.rpart.pred,BreastCancer$Class[-sub])
error.rpart &lt;- 1-(sum(diag(tb))/sum(tb))
tb
error.rpart

BC.adaboost &lt;- boosting(Class ~.,data=BreastCancer[,-1],mfinal=10, coeflearn="Freund", 
boos=FALSE , control=rpart.control(maxdepth=3))

#Using the pruning option
BC.adaboost.pred &lt;- predict.boosting(BC.adaboost,newdata=BreastCancer[-sub,-1], newmfinal=10)
BC.adaboost.pred$confusion
BC.adaboost.pred$error




</code></pre>

<hr>
<h2 id='prep_data'>Prepare Ranking Data for Item-Weighted Ensemble Algorithm</h2><span id='topic+prep_data'></span>

<h3>Description</h3>

<p>The <code>prep_data</code> function prepares item-weighted ranking data for further analysis. It takes a ranking matrix, predictors matrix, and weighting vector or matrix, and returns a data frame suitable for item-weighted ensemble algorithms for rankings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  prep_data(y, x, iw)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prep_data_+3A_y">y</code></td>
<td>
<p>an N by M matrix or data frame representing the ranking responses, where N is the number of individuals and M is the number of items. Each row corresponds to a ranking, ties are allowed.</p>
</td></tr>
<tr><td><code id="prep_data_+3A_x">x</code></td>
<td>
<p>an N by K matrix or data frame containing the K predictors associated with each individual ranking. Continuous variables are allowed, while the dummy coding should be used for categorical variables.</p>
</td></tr>
<tr><td><code id="prep_data_+3A_iw">iw</code></td>
<td>
<p>a vector or matrix representing the item weights or dissimilarities for the ranking data. For a vector, it should be a row vector of length M. For a matrix, it should be a symmetric M by M matrix representing item dissimilarities.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>prep_data</code> function performs the following steps:
Check the dimensions of the weighting vector or matrix to ensure compatibility with the ranking data.
Adjust the ranking matrix  <em>y</em> using the &quot;min&quot; method for ties.
Convert the ranked matrix into a data frame.
Generate the universe of rankings using the <span class="pkg">ConsRank::</span><code>univranks</code> function. Match the ranking matrix  <em>y</em> with the whole universe of rankings to obtain a label for each ranking. Combine the Label column with the predictor matrix.
Remove rows with missing values. The function then returns the prepared data frame for ensemble ranking. It also create the internal objects: <code>item</code>, <code>perm_tab_complete_up</code>, <code>perm</code>, <code>mat.dist</code> that are employed in the <code>Ensemble_ranking_IW</code> function.
</p>


<h3>Value</h3>

<p>An N by (K+1) data frame containing the prepared item-weighted ranking data. The first column &quot;Label&quot; contains the transformed ranking responses, and the remaining columns contain the predictors.
</p>


<h3>References</h3>

<p>Albano, A., Sciandra, M., and Plaia, A. (2023): &quot;A Weighted Distance-Based Approach with Boosted Decision Trees for Label Ranking.&quot; <em>Expert Systems with Applications</em>.
</p>
<p>D'Ambrosio, A.[aut, cre], Amodio, S. [ctb], Mazzeo, G. [ctb], Albano, A. [ctb], Plaia, A. [ctb] (2023). &quot;ConsRank: Compute the Median Ranking(s) According to the Kemeny's Axiomatic Approach. R package version 2.1.3&quot;, https://cran.r-project.org/package=ConsRank.
</p>
<p>Plaia, A., Buscemi, S., Furnkranz, J., and Mencıa, E.L. (2021): &quot;Comparing Boosting and Bagging for Decision Trees of Rankings.&quot; <em>Journal of Classification</em>, pages 1–22.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  # Prepare item-weighted ranking data
  y &lt;- matrix(c(1, 2, 3, 4, 2, 3, 1, 4, 4, 1, 3, 2, 2, 3, 1, 4), nrow = 4, ncol = 4, byrow = TRUE)
  x &lt;- matrix(c(0.5, 0.8, 1.2, 0.7, 1.1, 0.9, 0.6, 1.3, 0.4, 1.5, 0.7, 0.9), nrow = 4, ncol = 3)
  iw &lt;- c(2, 5, 5, 2)
  dati &lt;- prep_data(y, x, iw)
</code></pre>

<hr>
<h2 id='simulatedRankingData'>
Simulated ranking data
</h2><span id='topic+simulatedRankingData'></span>

<h3>Description</h3>

<p>The <code>simulatedRankingData</code> dataset is a list that includes the following components: 
</p>
<p>The ranking matrix, <code>y</code>, contains the ranking matrix. It consists of 500 rows and 4 columns, indicating the ranking positions. Each element in the matrix represents the rank assigned to an individual for a particular item.
</p>
<p>The predictor matrix <code>x</code> in the dataset consists of 20 continuous explanatory variables. These variables are used for predicting the rankings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(simulatedRankingData)</code></pre>


<h3>References</h3>

<p>Albano, A., Sciandra, M., and Plaia, A. (2023): &quot;A weighted distance-based approach with boosted decision trees for label ranking.&quot; <em>Expert Systems with Applications</em>.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
