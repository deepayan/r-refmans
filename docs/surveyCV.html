<!DOCTYPE html><html lang="en"><head><title>Help for package surveyCV</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {surveyCV}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#+25+26gt+3B+25'><p>Pipe operator</p></a></li>
<li><a href='#cv.svy'><p>CV for survey data</p></a></li>
<li><a href='#cv.svydesign'><p>CV for <code>svydesign</code> objects</p></a></li>
<li><a href='#cv.svyglm'><p>CV for <code>svyglm</code> objects</p></a></li>
<li><a href='#folds.svy'><p>Creating CV folds based on the survey design</p></a></li>
<li><a href='#folds.svydesign'><p>Creating CV folds based on the <code>svydesign</code> object</p></a></li>
<li><a href='#NSFG_data'><p>Subset of the 2015-2017 National Survey of Family Growth (NSFG): one birth per respondent.</p></a></li>
<li><a href='#NSFG_data_everypreg'><p>Subset of the 2015-2017 National Survey of Family Growth (NSFG): all live births per respondent.</p></a></li>
<li><a href='#surveyCV'><p>surveyCV: Cross Validation Based on Survey Design</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Cross Validation Based on Survey Design</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-03-14</td>
</tr>
<tr>
<td>Description:</td>
<td>Functions to generate K-fold cross validation (CV) folds
    and CV test error estimates that take into account
    how a survey dataset's sampling design was constructed
    (SRS, clustering, stratification, and/or unequal sampling weights).
    You can input linear and logistic regression models, along with data and a 
    type of survey design in order to get an output that can help you determine
    which model best fits the data using K-fold cross validation.
    Our paper on "K-Fold Cross-Validation for Complex Sample Surveys"
    by Wieczorek, Guerin, and McMahon (2022)
    &lt;<a href="https://doi.org/10.1002%2Fsta4.454">doi:10.1002/sta4.454</a>&gt;
    explains why differing how we take folds based on survey design is useful.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>survey (&ge; 4.1), magrittr (&ge; 2.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>dplyr (&ge; 1.0), ggplot2 (&ge; 3.3), grid (&ge; 4.0), gridExtra
(&ge; 2.3), ISLR (&ge; 1.2), knitr (&ge; 1.29), rmarkdown (&ge; 2.2),
rpms (&ge; 0.5), splines (&ge; 4.0), testthat (&ge; 3.1)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/ColbyStatSvyRsch/surveyCV/">https://github.com/ColbyStatSvyRsch/surveyCV/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ColbyStatSvyRsch/surveyCV/issues">https://github.com/ColbyStatSvyRsch/surveyCV/issues</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-03-15 02:52:15 UTC; jawieczo</td>
</tr>
<tr>
<td>Author:</td>
<td>Cole Guerin [aut],
  Thomas McMahon [aut],
  Jerzy Wieczorek <a href="https://orcid.org/0000-0002-2859-6534"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [cre, aut],
  Hunter Ratliff [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jerzy Wieczorek &lt;jawieczo@colby.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-03-15 08:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='+25+26gt+3B+25'>Pipe operator</h2><span id='topic++25+3E+25'></span>

<h3>Description</h3>

<p>See <code>magrittr::<a href="magrittr.html#topic+pipe">%&gt;%</a></code> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lhs %&gt;% rhs
</code></pre>

<hr>
<h2 id='cv.svy'>CV for survey data</h2><span id='topic+cv.svy'></span>

<h3>Description</h3>

<p>This is a cross validation function designed for survey samples taken using a SRS,
stratified, clustered, or clustered-and-stratified sampling design.
Returns survey CV estimates of the mean loss for each model
(MSE for linear models, or binary cross-entropy for logistic models).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.svy(
  Data,
  formulae,
  nfolds = 5,
  strataID = NULL,
  clusterID = NULL,
  nest = FALSE,
  fpcID = NULL,
  method = c("linear", "logistic"),
  weightsID = NULL,
  useSvyForFolds = TRUE,
  useSvyForFits = TRUE,
  useSvyForLoss = TRUE,
  na.rm = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cv.svy_+3A_data">Data</code></td>
<td>
<p>Dataframe of dataset to be used for CV</p>
</td></tr>
<tr><td><code id="cv.svy_+3A_formulae">formulae</code></td>
<td>
<p>Vector of formulas (as strings) for the GLMs to be compared in
cross validation</p>
</td></tr>
<tr><td><code id="cv.svy_+3A_nfolds">nfolds</code></td>
<td>
<p>Number of folds to be used during cross validation, defaults to
5</p>
</td></tr>
<tr><td><code id="cv.svy_+3A_strataid">strataID</code></td>
<td>
<p>String of the variable name used to stratify during sampling, must
be the same as in the dataset used</p>
</td></tr>
<tr><td><code id="cv.svy_+3A_clusterid">clusterID</code></td>
<td>
<p>String of the variable name used to cluster during sampling, must
be the same as in the dataset used</p>
</td></tr>
<tr><td><code id="cv.svy_+3A_nest">nest</code></td>
<td>
<p>Specify nest = TRUE if clusters are nested within strata, defaults to FALSE</p>
</td></tr>
<tr><td><code id="cv.svy_+3A_fpcid">fpcID</code></td>
<td>
<p>String of the variable name used for finite population corrections, must
be the same as in the dataset used, see <code><a href="survey.html#topic+svydesign">svydesign</a></code> for details</p>
</td></tr>
<tr><td><code id="cv.svy_+3A_method">method</code></td>
<td>
<p>String, must be either &quot;linear&quot; or &quot;logistic&quot;, determines type of
model fit during cross validation, defaults to linear</p>
</td></tr>
<tr><td><code id="cv.svy_+3A_weightsid">weightsID</code></td>
<td>
<p>String of the variable name in the dataset that contains sampling weights</p>
</td></tr>
<tr><td><code id="cv.svy_+3A_usesvyforfolds">useSvyForFolds</code></td>
<td>
<p>Specify useSvyForFolds = TRUE (default) to take svydesign into account when making folds;
should not be set FALSE except for running simulations to understand the properties of surveyCV</p>
</td></tr>
<tr><td><code id="cv.svy_+3A_usesvyforfits">useSvyForFits</code></td>
<td>
<p>Specify useSvyForFits = TRUE (default) to take svydesign into account when fitting models on training sets;
should not be set FALSE except for running simulations to understand the properties of surveyCV</p>
</td></tr>
<tr><td><code id="cv.svy_+3A_usesvyforloss">useSvyForLoss</code></td>
<td>
<p>Specify useSvyForLoss = TRUE (default) to take svydesign into account when calculating loss over test sets;
should not be set FALSE except for running simulations to understand the properties of surveyCV</p>
</td></tr>
<tr><td><code id="cv.svy_+3A_na.rm">na.rm</code></td>
<td>
<p>Whether to drop cases with missing values when taking 'svymean'
of test losses</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If you have already created a <code>svydesign</code> object or fitted a <code>svyglm</code>,
you will probably prefer the convenience wrapper functions
<code><a href="#topic+cv.svydesign">cv.svydesign</a></code> or <code><a href="#topic+cv.svyglm">cv.svyglm</a></code>.
</p>
<p>For models other than linear or logistic regression,
you can use <code><a href="#topic+folds.svy">folds.svy</a></code> or <code><a href="#topic+folds.svydesign">folds.svydesign</a></code> to generate
CV fold IDs that respect any stratification or clustering in the survey design.
You can then carry out K-fold CV as usual,
taking care to also use the survey design features and survey weights
when fitting models in each training set
and also when evaluating models against each test set.
</p>


<h3>Value</h3>

<p>Object of class <code>svystat</code>, which is a named vector of survey CV estimates of the mean loss
(MSE for linear models, or binary cross-entropy for logistic models) for each model,
with names &quot;.Model_1&quot;, &quot;.Model_2&quot;, etc. corresponding to the models provided in <code>formulae</code>;
and with a <code>var</code> attribute giving the variances.
See <code><a href="survey.html#topic+surveysummary">surveysummary</a></code> for details.
</p>


<h3>See Also</h3>

<p><code><a href="survey.html#topic+surveysummary">surveysummary</a></code>, <code><a href="survey.html#topic+svydesign">svydesign</a></code>
</p>
<p><code><a href="#topic+cv.svydesign">cv.svydesign</a></code> for a wrapper to use with a <code>svydesign</code> object,
or <code><a href="#topic+cv.svyglm">cv.svyglm</a></code> for a wrapper to use with a <code>svyglm</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Compare CV MSEs and their SEs under 3 linear models
# for a stratified sample and a one-stage cluster sample,
# using data from the `survey` package
library(survey)
data("api", package = "survey")
# stratified sample
cv.svy(apistrat, c("api00~ell",
                   "api00~ell+meals",
                   "api00~ell+meals+mobility"),
       nfolds = 5, strataID = "stype", weightsID = "pw", fpcID = "fpc")
# one-stage cluster sample
cv.svy(apiclus1, c("api00~ell",
                   "api00~ell+meals",
                   "api00~ell+meals+mobility"),
       nfolds = 5, clusterID = "dnum", weightsID = "pw", fpcID = "fpc")

# Compare CV MSEs and their SEs under 3 linear models
# for a stratified cluster sample with clusters nested within strata
data(NSFG_data)
library(splines)
cv.svy(NSFG_data, c("income ~ ns(age, df = 2)",
                    "income ~ ns(age, df = 3)",
                    "income ~ ns(age, df = 4)"),
       nfolds = 4,
       strataID = "strata", clusterID = "SECU",
       nest = TRUE, weightsID = "wgt")

# Logistic regression example, using the same stratified cluster sample;
# instead of CV MSE, we calculate CV binary cross-entropy loss,
# where (as with MSE) lower values indicate better fitting models
# (NOTE: na.rm=TRUE is not usually ideal;
#  it's used below purely for convenience, to keep the example short,
#  but a thorough analysis would look for better ways to handle the missing data)
cv.svy(NSFG_data, c("KnowPreg ~ ns(age, df = 1)",
                    "KnowPreg ~ ns(age, df = 2)",
                    "KnowPreg ~ ns(age, df = 3)"),
       method = "logistic", nfolds = 4,
       strataID = "strata", clusterID = "SECU",
       nest = TRUE, weightsID = "wgt",
       na.rm = TRUE)
</code></pre>

<hr>
<h2 id='cv.svydesign'>CV for <code>svydesign</code> objects</h2><span id='topic+cv.svydesign'></span>

<h3>Description</h3>

<p>Wrapper function which takes a <code><a href="survey.html#topic+svydesign">svydesign</a></code> object
and a vector of model formulas (as strings),
and passes it into <code><a href="#topic+cv.svy">cv.svy</a></code>.
Returns survey CV estimates of the mean loss for each model
(MSE for linear models, or binary cross-entropy for logistic models).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.svydesign(
  design_object,
  formulae,
  nfolds = 5,
  method = c("linear", "logistic"),
  na.rm = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cv.svydesign_+3A_design_object">design_object</code></td>
<td>
<p>Name of a <code>svydesign</code> object created using the <code>survey</code>
package. We do not yet support use of <code>probs</code> or <code>pps</code>.</p>
</td></tr>
<tr><td><code id="cv.svydesign_+3A_formulae">formulae</code></td>
<td>
<p>Vector of formulas (as strings) for the GLMs to be compared in
cross validation</p>
</td></tr>
<tr><td><code id="cv.svydesign_+3A_nfolds">nfolds</code></td>
<td>
<p>Number of folds to be used during cross validation, defaults to
5</p>
</td></tr>
<tr><td><code id="cv.svydesign_+3A_method">method</code></td>
<td>
<p>String, must be either &quot;linear&quot; or &quot;logistic&quot;, determines type of
model fit during cross validation, defaults to linear</p>
</td></tr>
<tr><td><code id="cv.svydesign_+3A_na.rm">na.rm</code></td>
<td>
<p>Whether to drop cases with missing values when taking 'svymean'
of test losses</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If you have already fitted a <code>svyglm</code>,
you may prefer the convenience wrapper function
<code><a href="#topic+cv.svyglm">cv.svyglm</a></code>.
</p>
<p>For models other than linear or logistic regression,
you can use <code><a href="#topic+folds.svy">folds.svy</a></code> or <code><a href="#topic+folds.svydesign">folds.svydesign</a></code> to generate
CV fold IDs that respect any stratification or clustering in the survey design.
You can then carry out K-fold CV as usual,
taking care to also use the survey design features and survey weights
when fitting models in each training set
and also when evaluating models against each test set.
</p>


<h3>Value</h3>

<p>Object of class <code>svystat</code>, which is a named vector of survey CV estimates of the mean loss
(MSE for linear models, or binary cross-entropy for logistic models) for each model,
with names &quot;.Model_1&quot;, &quot;.Model_2&quot;, etc. corresponding to the models provided in <code>formulae</code>;
and with a <code>var</code> attribute giving the variances.
See <code><a href="survey.html#topic+surveysummary">surveysummary</a></code> for details.
</p>


<h3>See Also</h3>

<p><code><a href="survey.html#topic+surveysummary">surveysummary</a></code>, <code><a href="survey.html#topic+svydesign">svydesign</a></code>
</p>
<p><code><a href="#topic+cv.svyglm">cv.svyglm</a></code> for a wrapper to use with a <code>svyglm</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Compare CV MSEs and their SEs under 3 linear models
# for a stratified sample and a one-stage cluster sample,
# using data from the `survey` package
library(survey)
data("api", package = "survey")
# stratified sample
dstrat &lt;- svydesign(id = ~1, strata = ~stype, weights = ~pw, data = apistrat,
                    fpc = ~fpc)
cv.svydesign(formulae = c("api00~ell",
                          "api00~ell+meals",
                          "api00~ell+meals+mobility"),
             design_object = dstrat, nfolds = 5)
# one-stage cluster sample
dclus1 &lt;- svydesign(id = ~dnum, weights = ~pw, data = apiclus1, fpc = ~fpc)
cv.svydesign(formulae = c("api00~ell",
                          "api00~ell+meals",
                          "api00~ell+meals+mobility"),
             design_object = dclus1, nfolds = 5)

# Compare CV MSEs and their SEs under 3 linear models
# for a stratified cluster sample with clusters nested within strata
data(NSFG_data)
library(splines)
NSFG.svydes &lt;- svydesign(id = ~SECU, strata = ~strata, nest = TRUE,
                         weights = ~wgt, data = NSFG_data)
cv.svydesign(formulae = c("income ~ ns(age, df = 2)",
                          "income ~ ns(age, df = 3)",
                          "income ~ ns(age, df = 4)"),
             design_object = NSFG.svydes, nfolds = 4)

# Logistic regression example, using the same stratified cluster sample;
# instead of CV MSE, we calculate CV binary cross-entropy loss,
# where (as with MSE) lower values indicate better fitting models
# (NOTE: na.rm=TRUE is not usually ideal;
#  it's used below purely for convenience, to keep the example short,
#  but a thorough analysis would look for better ways to handle the missing data)
cv.svydesign(formulae = c("KnowPreg ~ ns(age, df = 1)",
                          "KnowPreg ~ ns(age, df = 2)",
                          "KnowPreg ~ ns(age, df = 3)"),
             design_object = NSFG.svydes, nfolds = 4,
             method = "logistic", na.rm = TRUE)
</code></pre>

<hr>
<h2 id='cv.svyglm'>CV for <code>svyglm</code> objects</h2><span id='topic+cv.svyglm'></span>

<h3>Description</h3>

<p>Wrapper function which takes a <code><a href="survey.html#topic+svyglm">svyglm</a></code> object
(which itself contains a <code>svydesign</code> object)
and passes it through <code><a href="#topic+cv.svydesign">cv.svydesign</a></code> to <code><a href="#topic+cv.svy">cv.svy</a></code>.
Chooses linear or logistic regression based on the <code>svyglm</code> object's value of <code>family</code>.
Returns survey CV estimates of the mean loss for each model
(MSE for linear models, or binary cross-entropy for logistic models).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.svyglm(glm_object, nfolds = 5, na.rm = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cv.svyglm_+3A_glm_object">glm_object</code></td>
<td>
<p>Name of a <code>svyglm</code> object created from the <code>survey</code> package</p>
</td></tr>
<tr><td><code id="cv.svyglm_+3A_nfolds">nfolds</code></td>
<td>
<p>Number of folds to be used during cross validation, defaults to
5</p>
</td></tr>
<tr><td><code id="cv.svyglm_+3A_na.rm">na.rm</code></td>
<td>
<p>Whether to drop cases with missing values when taking 'svymean'
of test losses</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If you have created a <code>svydesign</code> object and want to compare several <code>svyglm</code> models,
you may prefer the function <code><a href="#topic+cv.svydesign">cv.svydesign</a></code>.
</p>
<p>For models other than linear or logistic regression,
you can use <code><a href="#topic+folds.svy">folds.svy</a></code> or <code><a href="#topic+folds.svydesign">folds.svydesign</a></code> to generate
CV fold IDs that respect any stratification or clustering in the survey design.
You can then carry out K-fold CV as usual,
taking care to also use the survey design features and survey weights
when fitting models in each training set
and also when evaluating models against each test set.
</p>


<h3>Value</h3>

<p>Object of class <code>svystat</code>, which is a named vector with the survey CV estimate of the mean loss
(MSE for linear models, or binary cross-entropy for logistic models)
for the model in the <code>svyglm</code> object provided to <code>glm_object</code>;
and with a <code>var</code> attribute giving the variance.
See <code><a href="survey.html#topic+surveysummary">surveysummary</a></code> for details.
</p>


<h3>See Also</h3>

<p><code><a href="survey.html#topic+surveysummary">surveysummary</a></code>, <code><a href="survey.html#topic+svydesign">svydesign</a></code>, <code><a href="survey.html#topic+svyglm">svyglm</a></code>
</p>
<p><code><a href="#topic+cv.svydesign">cv.svydesign</a></code> to use with a <code>svydesign</code> object for comparing several <code>svyglm</code> models
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Calculate CV MSE and its SE under one `svyglm` linear model
# for a stratified sample and a one-stage cluster sample,
# using data from the `survey` package
library(survey)
data("api", package = "survey")
# stratified sample
dstrat &lt;- svydesign(id = ~1, strata = ~stype, weights = ~pw, data = apistrat,
                    fpc = ~fpc)
glmstrat &lt;- svyglm(api00 ~ ell+meals+mobility, design = dstrat)
cv.svyglm(glmstrat, nfolds = 5)
# one-stage cluster sample
dclus1 &lt;- svydesign(id = ~dnum, weights = ~pw, data = apiclus1, fpc = ~fpc)
glmclus1 &lt;- svyglm(api00 ~ ell+meals+mobility, design = dclus1)
cv.svyglm(glmclus1, nfolds = 5)

# Calculate CV MSE and its SE under one `svyglm` linear model
# for a stratified cluster sample with clusters nested within strata
data(NSFG_data)
library(splines)
NSFG.svydes &lt;- svydesign(id = ~SECU, strata = ~strata, nest = TRUE,
                         weights = ~wgt, data = NSFG_data)
NSFG.svyglm &lt;- svyglm(income ~ ns(age, df = 3), design = NSFG.svydes)
cv.svyglm(glm_object = NSFG.svyglm, nfolds = 4)

# Logistic regression example, using the same stratified cluster sample;
# instead of CV MSE, we calculate CV binary cross-entropy loss,
# where (as with MSE) lower values indicate better fitting models
# (NOTE: na.rm=TRUE is not usually ideal;
#  it's used below purely for convenience, to keep the example short,
#  but a thorough analysis would look for better ways to handle the missing data)
NSFG.svyglm.logreg &lt;- svyglm(KnowPreg ~ ns(age, df = 2),
                             design = NSFG.svydes, family = quasibinomial())
cv.svyglm(glm_object = NSFG.svyglm.logreg, nfolds = 4, na.rm = TRUE)
</code></pre>

<hr>
<h2 id='folds.svy'>Creating CV folds based on the survey design</h2><span id='topic+folds.svy'></span>

<h3>Description</h3>

<p>This function creates a fold ID for each row in the dataset,
to be used for carrying out cross validation on survey samples taken using a
SRS, stratified, clustered, or clustered-and-stratified sampling design.
Returns a vector of fold IDs, which in most cases you will want to append
to your dataset using <code>cbind</code> or similar (see Examples below).
These fold IDs respect any stratification or clustering in the survey design.
You can then carry out K-fold CV as usual,
taking care to also use the survey design features and survey weights
when fitting models in each training set
and also when evaluating models against each test set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>folds.svy(Data, nfolds, strataID = NULL, clusterID = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="folds.svy_+3A_data">Data</code></td>
<td>
<p>Dataframe of dataset</p>
</td></tr>
<tr><td><code id="folds.svy_+3A_nfolds">nfolds</code></td>
<td>
<p>Number of folds to be used during cross validation</p>
</td></tr>
<tr><td><code id="folds.svy_+3A_strataid">strataID</code></td>
<td>
<p>String of the variable name used to stratify during sampling, must
be the same as in the dataset used</p>
</td></tr>
<tr><td><code id="folds.svy_+3A_clusterid">clusterID</code></td>
<td>
<p>String of the variable name used to cluster during sampling, must
be the same as in the dataset used</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If you have already created a <code>svydesign</code> object,
you will probably prefer the convenience wrapper function
<code><a href="#topic+folds.svydesign">folds.svydesign</a></code>.
</p>
<p>For the special cases of linear or logistic GLMs, use instead
<code><a href="#topic+cv.svy">cv.svy</a></code>, <code><a href="#topic+cv.svydesign">cv.svydesign</a></code>, or <code><a href="#topic+cv.svyglm">cv.svyglm</a></code>
which will automate the whole CV process for you.
</p>


<h3>Value</h3>

<p>Integer vector of fold IDs with length <code>nrow(Data)</code>.
Most likely you will want to append the returned vector to your dataset,
for instance with <code>cbind</code> (see Examples below).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+folds.svydesign">folds.svydesign</a></code> for a wrapper to use with a <code>svydesign</code> object
</p>
<p><code><a href="#topic+cv.svy">cv.svy</a></code>, <code><a href="#topic+cv.svydesign">cv.svydesign</a></code>, or <code><a href="#topic+cv.svyglm">cv.svyglm</a></code>
to carry out the whole CV process (not just forming folds but also training
and testing your models) for linear or logistic regression models
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Set up CV folds for a stratified sample and a one-stage cluster sample,
# using data from the `survey` package
library(survey)
data("api", package = "survey")
# stratified sample
apistrat &lt;- cbind(apistrat,
                  .foldID = folds.svy(apistrat, nfolds = 5, strataID = "stype"))
# Each fold will have observations from every stratum
with(apistrat, table(stype, .foldID))
# Fold sizes should be roughly equal
table(apistrat$.foldID)
#
# one-stage cluster sample
apiclus1 &lt;- cbind(apiclus1,
                  .foldID = folds.svy(apiclus1, nfolds = 5, clusterID = "dnum"))
# For any given cluster, all its observations will be in the same fold;
# and each fold should contain roughly the same number of clusters
with(apiclus1, table(dnum, .foldID))
# But if cluster sizes are unequal,
# the number of individuals per fold will also vary
table(apiclus1$.foldID)
# See the end of `intro` vignette for an example of using such folds
# as part of a custom loop over CV folds
# to tune parameters in a design-consistent random forest model
</code></pre>

<hr>
<h2 id='folds.svydesign'>Creating CV folds based on the <code>svydesign</code> object</h2><span id='topic+folds.svydesign'></span>

<h3>Description</h3>

<p>Wrapper function which takes a <code><a href="survey.html#topic+svydesign">svydesign</a></code> object
and desired number of CV folds,
and passes it into <code><a href="#topic+folds.svy">folds.svy</a></code>.
Returns a vector of fold IDs, which in most cases you will want to append
to your <code>svydesign</code> object using <code>update.svydesign</code>
(see Examples below).
These fold IDs respect any stratification or clustering in the survey design.
You can then carry out K-fold CV as usual,
taking care to also use the survey design features and survey weights
when fitting models in each training set
and also when evaluating models against each test set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>folds.svydesign(design_object, nfolds)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="folds.svydesign_+3A_design_object">design_object</code></td>
<td>
<p>Name of a <code>svydesign</code> object created using the <code>survey</code>
package. The arguments <code>id</code> and <code>strata</code> (if used)
must be specified as formulas, e.g. <code>svydesign(ids = ~MyPSUs, ...)</code>.</p>
</td></tr>
<tr><td><code id="folds.svydesign_+3A_nfolds">nfolds</code></td>
<td>
<p>Number of folds to be used during cross validation</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the special cases of linear or logistic GLMs, use instead
<code><a href="#topic+cv.svydesign">cv.svydesign</a></code> or <code><a href="#topic+cv.svyglm">cv.svyglm</a></code>
which will automate the whole CV process for you.
</p>


<h3>Value</h3>

<p>Integer vector of fold IDs with length <code>nrow(Data)</code>.
Most likely you will want to append the returned vector
to the <code>svydesign</code> object,
for instance with <code>update.svydesign</code> (see Examples below).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+folds.svy">folds.svy</a></code>
</p>
<p><code><a href="#topic+cv.svy">cv.svy</a></code>, <code><a href="#topic+cv.svydesign">cv.svydesign</a></code>, or <code><a href="#topic+cv.svyglm">cv.svyglm</a></code>
to carry out the whole CV process (not just forming folds but also training
and testing your models) for linear or logistic regression models
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Set up CV folds for a stratified sample and a one-stage cluster sample,
# using data from the `survey` package
library(survey)
data("api", package = "survey")
# stratified sample
dstrat &lt;- svydesign(id = ~1, strata = ~stype, weights = ~pw, data = apistrat,
                    fpc = ~fpc)
dstrat &lt;- update(dstrat, .foldID = folds.svydesign(dstrat, nfolds = 5))
# Each fold will have observations from every stratum
with(dstrat$variables, table(stype, .foldID))
# Fold sizes should be roughly equal
table(dstrat$variables$.foldID)
#
# one-stage cluster sample
dclus1 &lt;- svydesign(id = ~dnum, weights = ~pw, data = apiclus1, fpc = ~fpc)
dclus1 &lt;- update(dclus1, .foldID = folds.svydesign(dclus1, nfolds = 5))
# For any given cluster, all its observations will be in the same fold;
# and each fold should contain roughly the same number of clusters
with(dclus1$variables, table(dnum, .foldID))
# But if cluster sizes are unequal,
# the number of individuals per fold will also vary
table(dclus1$variables$.foldID)
# See the end of `intro` vignette for an example of using such folds
# as part of a custom loop over CV folds
# to tune parameters in a design-consistent random forest model
</code></pre>

<hr>
<h2 id='NSFG_data'>Subset of the 2015-2017 National Survey of Family Growth (NSFG): one birth per respondent.</h2><span id='topic+NSFG_data'></span>

<h3>Description</h3>

<p>We downloaded this data from the NSFG website and cleaned it following
an approach posted to RPubs by Hunter Ratliff.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NSFG_data
</code></pre>


<h3>Format</h3>

<p>A data frame with 2801 rows and 17 variables:
</p>

<dl>
<dt>CASEID</dt><dd><p>Respondent ID number (per respondent, not per pregnancy)</p>
</dd>
<dt>LBW</dt><dd><p>(originally LBW1) Low birthweight (TRUE/FALSE) for the 1st baby from this pregnancy</p>
</dd>
<dt>PreMe</dt><dd><p>(recode of WKSGEST) Whether gestational age was premature (below 37 weeks) or full term</p>
</dd>
<dt>gotPNcare</dt><dd><p>(recode of BGNPRENA) Whether or not respondent got prenatal care in first trimester (before 13 weeks)</p>
</dd>
<dt>KnowPreg</dt><dd><p>(recode of KNEWPREG) Whether or not respondent learned she was pregnant by 6 weeks</p>
</dd>
<dt>age</dt><dd><p>(originally AGECON) Age at time of conception</p>
</dd>
<dt>income</dt><dd><p>(originally POVERTY) Income as percent of poverty level, so that 100 = income is at the poverty line; topcoded at 500</p>
</dd>
<dt>YrEdu</dt><dd><p>(originally EDUCAT) Education (number of years of schooling)</p>
</dd>
<dt>race</dt><dd><p>(originally HISPRACE) Race &amp; Hispanic origin of respondent</p>
</dd>
<dt>BMI</dt><dd><p>Body Mass Index</p>
</dd>
<dt>PregNum</dt><dd><p>(originally PREGNUM) Respondent's total number of pregnancies</p>
</dd>
<dt>eduCat</dt><dd><p>(originally HIEDUC) Highest completed year of school or highest degree received</p>
</dd>
<dt>GA</dt><dd><p>(originally WKSGEST) Gestational length of completed pregnancy (in weeks)</p>
</dd>
<dt>Wanted</dt><dd><p>(recode of NEWWANTR) Whether or not pregnancy came at right time according to respondent (rather than too soon, too late, or unwanted)</p>
</dd>
<dt>wgt</dt><dd><p>(originally WGT2015_2017) Final weight for the 2015-2017 NSFG (at the respondent level, not pregnancy level)</p>
</dd>
<dt>SECU</dt><dd><p>Randomized version of cluster ID, or &quot;sampling error computational unit&quot; &ndash; these are nested within strata</p>
</dd>
<dt>strata</dt><dd><p>(originally SEST) Randomized version of stratum ID</p>
</dd>
</dl>



<h3>Details</h3>

<p>Note that these data were filtered down to include only:
</p>
<p>- live births,
- with gestational ages below 45 weeks,
- born to mothers who were aged 20-40 years old at time of conception;
</p>
<p>...then filtered further down to only the *first* such birth per respondent.
</p>
<p>Also note that SECUs = Sampling Error Computation Units are effectively
pseudo-PSUs, nested within (pseudo-)strata. See page 35 of the NSFG
2011-2013 sample design documentation for details.
</p>


<h3>Source</h3>

<p><a href="https://www.cdc.gov/nchs/nsfg/nsfg_2015_2017_puf.htm">https://www.cdc.gov/nchs/nsfg/nsfg_2015_2017_puf.htm</a>
</p>
<p><a href="https://rpubs.com/HunterRatliff1/NSFG_Wrangle">https://rpubs.com/HunterRatliff1/NSFG_Wrangle</a>
</p>
<p><a href="https://www.cdc.gov/nchs/data/nsfg/nsfg_2011_2013_sampledesign.pdf">https://www.cdc.gov/nchs/data/nsfg/nsfg_2011_2013_sampledesign.pdf</a>
</p>

<hr>
<h2 id='NSFG_data_everypreg'>Subset of the 2015-2017 National Survey of Family Growth (NSFG): all live births per respondent.</h2><span id='topic+NSFG_data_everypreg'></span>

<h3>Description</h3>

<p>Same as 'NSFG_data' but using *every* birth, not just the *first* birth,
out of the initial subset there
(live births with gestational age &lt; 45 weeks
for mothers aged 20 to 40 at time of conception).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NSFG_data_everypreg
</code></pre>


<h3>Format</h3>

<p>A data frame with 5089 rows and 17 variables
</p>

<hr>
<h2 id='surveyCV'>surveyCV: Cross Validation Based on Survey Design</h2><span id='topic+surveyCV'></span>

<h3>Description</h3>

<p>Functions to generate K-fold cross validation (CV) folds
and CV test error estimates that take into account
how a survey dataset's sampling design was constructed
(SRS, clustering, stratification, and/or unequal sampling weights).
You can input linear and logistic regression models, along with data and a
type of survey design in order to get an output that can help you determine
which model best fits the data using K-fold cross validation.
Our paper on &quot;K-Fold Cross-Validation for Complex Sample Surveys&quot;
by Wieczorek, Guerin, and McMahon (2022)
&lt;doi: <a href="https://doi.org/10.1002/sta4.454">10.1002/sta4.454</a>&gt;
explains why differing how we take folds based on survey design is useful.
</p>


<h3>Details</h3>

<p>The code for this package seeks to create an alternative for the
<code><a href="boot.html#topic+cv.glm">boot::cv.glm</a></code>
function, so that results correctly account for survey designs during
K-fold cross validation.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
