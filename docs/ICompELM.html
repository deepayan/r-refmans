<!DOCTYPE html><html><head><title>Help for package ICompELM</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ICompELM}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ica.elm_forecast'><p>Forecasting from ICA based ELM model</p></a></li>
<li><a href='#ica.elm_train'><p>Training of ICA based ELM model for time series forecasting</p></a></li>
<li><a href='#pca.elm_forecast'><p>Forecasting from PCA based ELM model</p></a></li>
<li><a href='#pca.elm_train'><p>Training of PCA based ELM model for time series forecasting</p></a></li>
<li><a href='#price'><p>Aggregate gram price data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Independent Component Analysis Based Extreme Learning Machine</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Single Layer Feed-forward Neural networks (SLFNs) have many applications in various fields of statistical modelling, especially for time-series forecasting. However, there are some major disadvantages of training such networks via the widely accepted 'gradient-based backpropagation' algorithm, such as convergence to local minima, dependencies on learning rate and large training time. These concerns were addressed by Huang et al. (2006) &lt;<a href="https://doi.org/10.1016%2Fj.neucom.2005.12.126">doi:10.1016/j.neucom.2005.12.126</a>&gt;, wherein they introduced the Extreme Learning Machine (ELM), an extremely fast learning algorithm for SLFNs which randomly chooses the weights connecting input and hidden nodes and analytically determines the output weights of SLFNs. It shows good generalized performance, but is still subject to a high degree of randomness. To mitigate this issue, this package uses a dimensionality reduction technique given in Hyvarinen (1999) &lt;<a href="https://doi.org/10.1109%2F72.761722">doi:10.1109/72.761722</a>&gt;, namely, the Independent Component Analysis (ICA) to determine the input-hidden connections and thus, remove any sort of randomness from the algorithm. This leads to a robust, fast and stable ELM model. Using functions within this package, the proposed model can also be compared with an existing alternative based on the Principal Component Analysis (PCA) algorithm given by Pearson (1901) &lt;<a href="https://doi.org/10.1080%2F14786440109462720">doi:10.1080/14786440109462720</a>&gt;, i.e., the PCA based ELM model given by Castano et al. (2013) &lt;<a href="https://doi.org/10.1007%2Fs11063-012-9253-x">doi:10.1007/s11063-012-9253-x</a>&gt;, from which the implemented ICA based algorithm is greatly inspired.</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, tsutils, ica</td>
</tr>
<tr>
<td>Suggests:</td>
<td>forecast</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-06-07 19:53:01 UTC; Saikath</td>
</tr>
<tr>
<td>Author:</td>
<td>Saikath Das [aut, cre],
  Ranjit Kumar Paul [aut],
  Md Yeasin [aut],
  Amrit Kumar Paul [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Saikath Das &lt;saikathdas007@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-06-10 17:00:14 UTC</td>
</tr>
</table>
<hr>
<h2 id='ica.elm_forecast'>Forecasting from ICA based ELM model</h2><span id='topic+ica.elm_forecast'></span>

<h3>Description</h3>

<p>Forecasts are generated recursively from a trained Extreme Learning Machine
built using Independent Component Analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ica.elm_forecast(ica.elm_model, h = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ica.elm_forecast_+3A_ica.elm_model">ica.elm_model</code></td>
<td>
<p>A trained ICA based ELM model.</p>
</td></tr>
<tr><td><code id="ica.elm_forecast_+3A_h">h</code></td>
<td>
<p>Number of periods for forecasting. Defaults to one-step
ahead forecast.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of point forecasts.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ica.elm_train">ica.elm_train()</a></code> for training an ICA based ELM model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>train_set &lt;- head(price, 12*12)
test_set &lt;- tail(price, 12)
ica.model &lt;- ica.elm_train(train_data = train_set, lags = 12)
y_hat &lt;- ica.elm_forecast(ica.elm_model = ica.model, h = length(test_set))
# Evaluation of the forecasts
if(require("forecast")) forecast::accuracy(y_hat, test_set)
</code></pre>

<hr>
<h2 id='ica.elm_train'>Training of ICA based ELM model for time series forecasting</h2><span id='topic+ica.elm_train'></span>

<h3>Description</h3>

<p>An Extreme Learning Machine is trained by utilizing the concept of
Independent Component Analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ica.elm_train(train_data, lags, comps = lags, bias = TRUE, actfun = "sig")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ica.elm_train_+3A_train_data">train_data</code></td>
<td>
<p>A univariate time series data.</p>
</td></tr>
<tr><td><code id="ica.elm_train_+3A_lags">lags</code></td>
<td>
<p>Number of lags to be considered.</p>
</td></tr>
<tr><td><code id="ica.elm_train_+3A_comps">comps</code></td>
<td>
<p>Number of independent components to be considered. Corresponds
to number of hidden nodes. Defaults to maximum value, i.e., <code>lags</code>.</p>
</td></tr>
<tr><td><code id="ica.elm_train_+3A_bias">bias</code></td>
<td>
<p>Whether to include bias term while computing output weights.
Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="ica.elm_train_+3A_actfun">actfun</code></td>
<td>
<p>Activation function for the hidden layer. Defaults to
<code>sig</code>. See <code style="white-space: pre;">&#8288;Activation functions&#8288;</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An Extreme Learning Machine (ELM) is trained wherein the weights connecting
the input layer and hidden layer are obtained using Independent Component
Analysis (ICA), instead of being chosen randomly. The number of hidden
nodes is determined by the number of independent components.
</p>


<h3>Value</h3>

<p>A list containing the trained ICA-ELM model with the following
components.
</p>
<table>
<tr><td><code>inp_weights</code></td>
<td>
<p>Weights connecting the input layer to hidden layer,
obtained from the unmixing matrix <code class="reqn">W</code> of ICA. The
columns represent the hidden nodes while rows represent
input nodes.</p>
</td></tr>
<tr><td><code>out_weights</code></td>
<td>
<p>Weights connecting the hidden layer to output layer.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Fitted values of the model.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Residuals of the model.</p>
</td></tr>
<tr><td><code>h.out</code></td>
<td>
<p>A data frame containing the hidden layer outputs (activation
function applied) with columns representing hidden nodes and
rows representing observations.</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>The univariate <code>ts</code> data used for training the model.</p>
</td></tr>
<tr><td><code>lags</code></td>
<td>
<p>Number of lags used during training.</p>
</td></tr>
<tr><td><code>comps</code></td>
<td>
<p>Number of independent components considered for training. It
determines the number of hidden nodes.</p>
</td></tr>
<tr><td><code>bias</code></td>
<td>
<p>Whether bias node was included during training.</p>
</td></tr>
<tr><td><code>actfun</code></td>
<td>
<p>Activation function for the hidden layer.
See <code style="white-space: pre;">&#8288;Activation functions&#8288;</code>.</p>
</td></tr>
</table>


<h3>Activation functions</h3>

<p>The activation function for the hidden layer must be one of the following.
</p>

<dl>
<dt><code>sig</code></dt><dd><p>Sigmoid function: <code class="reqn">(1 + e^{-x})^{-1}</code></p>
</dd>
<dt><code>radbas</code></dt><dd><p>Radial basis function: <code class="reqn">e^{-x^2}</code></p>
</dd>
<dt><code>hardlim</code></dt><dd><p>Hard-limit function: <code class="reqn">\begin{cases} 1, &amp; if\:x
        \geq 0 \\ 0, &amp; if\:x&lt;0 \end{cases}</code></p>
</dd>
<dt><code>hardlims</code></dt><dd><p>Symmetric hard-limit function: <code class="reqn">\begin{cases}1,
              &amp; if\:x \geq 0 \\ -1, &amp; if\:x&lt;0 \end{cases}</code></p>
</dd>
<dt><code>satlins</code></dt><dd><p>Symmetric saturating linear function: <code class="reqn">
        \begin{cases}1, &amp; if\:x \geq 1 \\ x, &amp; if\:-1&lt;x&lt;1 \\ -1, &amp; if\:x
        \leq -1 \end{cases}</code></p>
</dd>
<dt><code>tansig</code></dt><dd><p>Tan-sigmoid function: <code class="reqn">2(1 + e^{-2x})^{-1}-1</code></p>
</dd>
<dt><code>tribas</code></dt><dd><p>Triangular basis function: <code class="reqn">\begin{cases} 1-|x|,
        &amp; if \: -1 \leq x \leq 1 \\ 0, &amp; otherwise \end{cases}</code></p>
</dd>
<dt><code>poslin</code></dt><dd><p>Postive linear function: <code class="reqn">\begin{cases} x,
        &amp; if\: x \geq 0 \\ 0, &amp; otherwise \end{cases}</code></p>
</dd>
</dl>



<h3>References</h3>

<p>Huang, G. B., Zhu, Q. Y., &amp; Siew, C. K. (2006). Extreme learning
machine: theory and applications. Neurocomputing, 70(1-3), 489-501.
<a href="doi:10.1016/j.neucom.2005.12.126">doi:10.1016/j.neucom.2005.12.126</a>.
</p>
<p>Hyvarinen, A. (1999). Fast and robust fixed-point algorithms for independent
component analysis. IEEE transactions on Neural Networks, 10(3), 626-634.
<a href="doi:10.1109/72.761722">doi:10.1109/72.761722</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ica.elm_forecast">ica.elm_forecast()</a></code> for forecasting from trained ICA based ELM
model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>train_set &lt;- head(price, 12*12)
ica.model &lt;- ica.elm_train(train_data = train_set, lags = 12)
</code></pre>

<hr>
<h2 id='pca.elm_forecast'>Forecasting from PCA based ELM model</h2><span id='topic+pca.elm_forecast'></span>

<h3>Description</h3>

<p>Forecasts are generated recursively from a trained Extreme Learning Machine
built using Principal Component Analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pca.elm_forecast(pca.elm_model, h = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pca.elm_forecast_+3A_pca.elm_model">pca.elm_model</code></td>
<td>
<p>A trained PCA based ELM model.</p>
</td></tr>
<tr><td><code id="pca.elm_forecast_+3A_h">h</code></td>
<td>
<p>Number of periods for forecasting. Defaults to one-step
ahead forecast.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of point forecasts.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pca.elm_train">pca.elm_train()</a></code> for training an ICA based ELM model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>train_set &lt;- head(price, 12*12)
test_set &lt;- tail(price, 12)
pca.model &lt;- pca.elm_train(train_data = train_set, lags = 12)
y_hat &lt;- pca.elm_forecast(pca.elm_model = pca.model, h = length(test_set))
# Evaluation of the forecasts
if(require("forecast")) forecast::accuracy(y_hat, test_set)
</code></pre>

<hr>
<h2 id='pca.elm_train'>Training of PCA based ELM model for time series forecasting</h2><span id='topic+pca.elm_train'></span>

<h3>Description</h3>

<p>An Extreme Learning Machine is trained by utilizing the concept of
Principal Component Analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pca.elm_train(
  train_data,
  lags,
  comps = lags,
  center = TRUE,
  scale = TRUE,
  bias = TRUE,
  actfun = "sig"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pca.elm_train_+3A_train_data">train_data</code></td>
<td>
<p>A univariate time series data.</p>
</td></tr>
<tr><td><code id="pca.elm_train_+3A_lags">lags</code></td>
<td>
<p>Number of lags to be considered.</p>
</td></tr>
<tr><td><code id="pca.elm_train_+3A_comps">comps</code></td>
<td>
<p>Number of independent components to be considered. Corresponds
to number of hidden nodes. Defaults to maximum value, i.e., <code>lags</code>.</p>
</td></tr>
<tr><td><code id="pca.elm_train_+3A_center">center</code></td>
<td>
<p>Whether to compute PCA on mean-adjusted data.</p>
</td></tr>
<tr><td><code id="pca.elm_train_+3A_scale">scale</code></td>
<td>
<p>Whether to compute PCA on variance-adjusted data.</p>
</td></tr>
<tr><td><code id="pca.elm_train_+3A_bias">bias</code></td>
<td>
<p>Whether to include bias term while computing output weights.
Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="pca.elm_train_+3A_actfun">actfun</code></td>
<td>
<p>Activation function for the hidden layer. Defaults to
<code>sig</code>. See <code style="white-space: pre;">&#8288;Activation functions&#8288;</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An Extreme Learning Machine (ELM) is trained wherein the weights connecting
the input layer and hidden layer are obtained using Principal Component
Analysis (PCA), instead of being chosen randomly. The number of hidden
nodes is determined by the number of principal components.
</p>


<h3>Value</h3>

<p>A list containing the trained ICA-ELM model with the following
components.
</p>
<table>
<tr><td><code>inp_weights</code></td>
<td>
<p>Weights connecting the input layer to hidden layer,
obtained from the unmixing matrix <code class="reqn">W</code> of ICA. The
columns represent the hidden nodes while rows represent
input nodes.</p>
</td></tr>
<tr><td><code>out_weights</code></td>
<td>
<p>Weights connecting the hidden layer to output layer.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Fitted values of the model.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Residuals of the model.</p>
</td></tr>
<tr><td><code>h.out</code></td>
<td>
<p>A data frame containing the hidden layer outputs (activation
function applied) with columns representing hidden nodes and
rows representing observations.</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>The univariate <code>ts</code> data used for training the model.</p>
</td></tr>
<tr><td><code>lags</code></td>
<td>
<p>Number of lags used during training.</p>
</td></tr>
<tr><td><code>comps</code></td>
<td>
<p>Number of independent components considered for training. It
determines the number of hidden nodes.</p>
</td></tr>
<tr><td><code>center</code></td>
<td>
<p>Whether the input data was mean-adjusted during training.</p>
</td></tr>
<tr><td><code>scale</code></td>
<td>
<p>Whether the input data was variance-adjusted during training.</p>
</td></tr>
<tr><td><code>bias</code></td>
<td>
<p>Whether bias node was included during training.</p>
</td></tr>
<tr><td><code>actfun</code></td>
<td>
<p>Activation function for the hidden layer.
See <code style="white-space: pre;">&#8288;Activation functions&#8288;</code>.</p>
</td></tr>
</table>


<h3>Activation functions</h3>

<p>The activation function for the hidden layer must be one of the following.
</p>

<dl>
<dt><code>sig</code></dt><dd><p>Sigmoid function: <code class="reqn">(1 + e^{-x})^{-1}</code></p>
</dd>
<dt><code>radbas</code></dt><dd><p>Radial basis function: <code class="reqn">e^{-x^2}</code></p>
</dd>
<dt><code>hardlim</code></dt><dd><p>Hard-limit function: <code class="reqn">\begin{cases} 1, &amp; if\:x
        \geq 0 \\ 0, &amp; if\:x&lt;0 \end{cases}</code></p>
</dd>
<dt><code>hardlims</code></dt><dd><p>Symmetric hard-limit function: <code class="reqn">\begin{cases}1,
              &amp; if\:x \geq 0 \\ -1, &amp; if\:x&lt;0 \end{cases}</code></p>
</dd>
<dt><code>satlins</code></dt><dd><p>Symmetric saturating linear function: <code class="reqn">
        \begin{cases}1, &amp; if\:x \geq 1 \\ x, &amp; if\:-1&lt;x&lt;1 \\ -1, &amp; if\:x
        \leq -1 \end{cases}</code></p>
</dd>
<dt><code>tansig</code></dt><dd><p>Tan-sigmoid function: <code class="reqn">2(1 + e^{-2x})^{-1}-1</code></p>
</dd>
<dt><code>tribas</code></dt><dd><p>Triangular basis function: <code class="reqn">\begin{cases} 1-|x|,
        &amp; if \: -1 \leq x \leq 1 \\ 0, &amp; otherwise \end{cases}</code></p>
</dd>
<dt><code>poslin</code></dt><dd><p>Postive linear function: <code class="reqn">\begin{cases} x,
        &amp; if\: x \geq 0 \\ 0, &amp; otherwise \end{cases}</code></p>
</dd>
</dl>



<h3>References</h3>

<p>Pearson, K. (1901). LIII. On lines and planes of closest fit to
systems of points in space. The London, Edinburgh, and Dublin
philosophical magazine and journal of science, 2(11), 559-572.
<a href="doi:10.1080/14786440109462720">doi:10.1080/14786440109462720</a>.
</p>
<p>Castaño, A., Fernández-Navarro, F., &amp; Hervás-Martínez, C. (2013).
PCA-ELM: a robust and pruned extreme learning machine approach based on
principal component analysis. Neural processing letters, 37, 377-392.
<a href="doi:10.1007/s11063-012-9253-x">doi:10.1007/s11063-012-9253-x</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pca.elm_forecast">pca.elm_forecast()</a></code> for forecasing from trained PCA based ELM
model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>train_set &lt;- head(price, 12*12)
pca.model &lt;- pca.elm_train(train_data = train_set, lags = 12)
</code></pre>

<hr>
<h2 id='price'>Aggregate gram price data</h2><span id='topic+price'></span>

<h3>Description</h3>

<p>National aggregate price of gram from Indian markets, which is a major pulse
in the country. The observations range from January, 2010 upto December,
2023.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>price
</code></pre>


<h3>Format</h3>

<p>A <code>ts</code> object with 156 observations.
</p>


<h3>Source</h3>

<p><a href="https://www.agmarknet.gov.in/">https://www.agmarknet.gov.in/</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>plot(price, xlab = "Year", ylab = "Aggregate price of Gram (Rs./Bag)")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
