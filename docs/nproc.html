<!DOCTYPE html><html><head><title>Help for package nproc</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {nproc}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#compare'><p>Compare two NP classification methods at different type I error upper bounds.</p></a></li>
<li><a href='#lines.nproc'><p>Add NP-ROC curves to the current plot object.</p></a></li>
<li><a href='#npc'><p>Construct a Neyman-Pearson Classifier from a sample of class 0 and class 1.</p></a></li>
<li><a href='#nproc'><p>Calculate the Neyman-Pearson Receiver Operating Characteristics</p></a></li>
<li><a href='#plot.nproc'><p>Plot the nproc band(s).</p></a></li>
<li><a href='#predict.npc'><p>Predicting the outcome of a set of new observations using the fitted npc</p>
object.</a></li>
<li><a href='#print.npc'><p>Print the npc object.</p></a></li>
<li><a href='#print.nproc'><p>Print the nproc object.</p></a></li>
<li><a href='#rocCV'><p>Calculate the Receiver Operating Characteristics with Cross-validation or Subsampling</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Neyman-Pearson (NP) Classification Algorithms and NP Receiver
Operating Characteristic (NP-ROC) Curves</td>
</tr>
<tr>
<td>Version:</td>
<td>2.1.5</td>
</tr>
<tr>
<td>Date:</td>
<td>2020-01-13</td>
</tr>
<tr>
<td>Imports:</td>
<td>glmnet, e1071, randomForest, naivebayes, MASS, parallel, ada,
stats, graphics, ROCR, tree</td>
</tr>
<tr>
<td>Description:</td>
<td>In many binary classification applications, such as disease
    diagnosis and spam detection, practitioners commonly face the need to limit
    type I error (i.e., the conditional probability of misclassifying a class 0
    observation as class 1) so that it remains below a desired threshold. To address
    this need, the Neyman-Pearson (NP) classification paradigm is a natural choice;
    it minimizes type II error (i.e., the conditional probability of misclassifying
    a class 1 observation as class 0) while enforcing an upper bound, alpha, on the
    type I error. Although the NP paradigm has a century-long history in hypothesis
    testing, it has not been well recognized and implemented in classification
    schemes. Common practices that directly limit the empirical type I error to
    no more than alpha do not satisfy the type I error control objective because
    the resulting classifiers are still likely to have type I errors much larger
    than alpha. As a result, the NP paradigm has not been properly implemented
    for many classification scenarios in practice. In this work, we develop the
    first umbrella algorithm that implements the NP paradigm for all scoring-type
    classification methods, including popular methods such as logistic regression,
    support vector machines and random forests. Powered by this umbrella algorithm,
    we propose a novel graphical tool for NP classification methods: NP receiver
    operating characteristic (NP-ROC) bands, motivated by the popular receiver
    operating characteristic (ROC) curves. NP-ROC bands will help choose in a data
    adaptive way and compare different NP classifiers. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.0</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://advances.sciencemag.org/content/4/2/eaao1659">http://advances.sciencemag.org/content/4/2/eaao1659</a></td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-01-13 16:01:14 UTC; yangfeng</td>
</tr>
<tr>
<td>Author:</td>
<td>Yang Feng [aut, cre],
  Jessica Li [aut],
  Xin Tong [aut],
  Ye Tian [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Yang Feng &lt;yangfengstat@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-01-13 19:20:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='compare'>Compare two NP classification methods at different type I error upper bounds.</h2><span id='topic+compare'></span>

<h3>Description</h3>

<p><code>compare</code> compares NP classification methods and provides the regions where one method is better than the other.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compare(roc1, roc2, plot = TRUE, col1 = "black", col2 = "red")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compare_+3A_roc1">roc1</code></td>
<td>
<p>the first nproc object.</p>
</td></tr>
<tr><td><code id="compare_+3A_roc2">roc2</code></td>
<td>
<p>the second nproc object.</p>
</td></tr>
<tr><td><code id="compare_+3A_plot">plot</code></td>
<td>
<p>whether to generate the two NP-ROC plots and mark the area of significant difference. Default = 'TRUE'.</p>
</td></tr>
<tr><td><code id="compare_+3A_col1">col1</code></td>
<td>
<p>the color of the region where roc1 is significantly better than roc2. Default = 'black'.</p>
</td></tr>
<tr><td><code id="compare_+3A_col2">col2</code></td>
<td>
<p>the color of the region where roc2 is significantly better than roc1. Default = 'red'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following items.
</p>
<table>
<tr><td><code>alpha1</code></td>
<td>
<p>the alpha values where roc1 is significantly better than roc2. </p>
</td></tr>
<tr><td><code>alpha2</code></td>
<td>
<p>the alpha values where roc2 is significantly better than roc1. </p>
</td></tr>
<tr><td><code>alpha3</code></td>
<td>
<p>the alpha values where roc1 and roc2 are not significantly different.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Xin Tong, Yang Feng, and Jingyi Jessica Li (2018), Neyman-Pearson (NP) classification algorithms and NP receiver operating characteristic (NP-ROC), <em>Science Advances</em>, <b>4</b>, 2, eaao1659.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npc">npc</a></code>, <code><a href="#topic+nproc">nproc</a></code>, <code><a href="#topic+predict.npc">predict.npc</a></code> and <code><a href="#topic+plot.nproc">plot.nproc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n = 1000
set.seed(1)
x1 = c(rnorm(n), rnorm(n) + 1)
x2 = c(rnorm(n), rnorm(n)*sqrt(6) + 1)
y = c(rep(0,n), rep(1,n))
fit1 = nproc(x1, y, method = 'lda')
fit2 = nproc(x2, y, method = 'lda')
v = compare(fit1, fit2)
legend('topleft',legend=c('x1','x2'),col=1:2,lty=c(1,1))

</code></pre>

<hr>
<h2 id='lines.nproc'>Add NP-ROC curves to the current plot object.</h2><span id='topic+lines.nproc'></span>

<h3>Description</h3>

<p>Add NP-ROC curves to the current plot object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nproc'
lines(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lines.nproc_+3A_x">x</code></td>
<td>
<p>fitted NP-ROC object using <code>nproc</code>.</p>
</td></tr>
<tr><td><code id="lines.nproc_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+npc">npc</a></code>, <code><a href="#topic+nproc">nproc</a></code> and <code><a href="#topic+plot.nproc">plot.nproc</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n = 1000
x = matrix(rnorm(n*2),n,2)
c = 1+3*x[,1]
y = rbinom(n,1,1/(1+exp(-c)))
fit = nproc(x, y, method = 'nb')
plot(fit)
fit2 = nproc(x, y, method = 'lda')
lines(fit2, col = 2)
</code></pre>

<hr>
<h2 id='npc'>Construct a Neyman-Pearson Classifier from a sample of class 0 and class 1.</h2><span id='topic+npc'></span>

<h3>Description</h3>

<p>Given a type I error upper bound alpha and a violation upper bound delta, <code>npc</code> calculates the Neyman-Pearson Classifier
which controls the type I error under alpha with probability at least 1-delta.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>npc(x = NULL, y, method = c("logistic", "penlog", "svm", "randomforest",
  "lda", "slda", "nb", "nnb", "ada", "tree"), alpha = 0.05, delta = 0.05,
  split = 1, split.ratio = 0.5, n.cores = 1, band = FALSE,
  nfolds = 10, randSeed = 0, warning = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="npc_+3A_x">x</code></td>
<td>
<p>n * p observation matrix. n observations, p covariates.</p>
</td></tr>
<tr><td><code id="npc_+3A_y">y</code></td>
<td>
<p>n 0/1 observatons.</p>
</td></tr>
<tr><td><code id="npc_+3A_method">method</code></td>
<td>
<p>base classification method.
</p>

<ul>
<li><p> logistic: Logistic regression. <a href="stats.html#topic+glm">glm</a> function with family = 'binomial'
</p>
</li>
<li><p> penlog: Penalized logistic regression with LASSO penalty. <code><a href="glmnet.html#topic+glmnet">glmnet</a></code> in <code>glmnet</code> package
</p>
</li>
<li><p> svm: Support Vector Machines. <code><a href="e1071.html#topic+svm">svm</a></code> in <code>e1071</code> package
</p>
</li>
<li><p> randomforest: Random Forest. <code><a href="randomForest.html#topic+randomForest">randomForest</a></code> in <code>randomForest</code> package
</p>
</li>
<li><p> lda: Linear Discriminant Analysis. <code><a href="MASS.html#topic+lda">lda</a></code> in <code>MASS</code> package
</p>
</li>
<li><p> slda: Sparse Linear Discriminant Analysis with LASSO penalty.
</p>
</li>
<li><p> nb: Naive Bayes. <code><a href="e1071.html#topic+naiveBayes">naiveBayes</a></code> in <code>e1071</code> package
</p>
</li>
<li><p> nnb: Nonparametric Naive Bayes. <code><a href="naivebayes.html#topic+naive_bayes">naive_bayes</a></code> in <code>naivebayes</code> package
</p>
</li>
<li><p> ada: Ada-Boost. <code><a href="ada.html#topic+ada">ada</a></code> in <code>ada</code> package
</p>
</li></ul>
</td></tr>
<tr><td><code id="npc_+3A_alpha">alpha</code></td>
<td>
<p>the desirable upper bound on type I error. Default = 0.05.</p>
</td></tr>
<tr><td><code id="npc_+3A_delta">delta</code></td>
<td>
<p>the violation rate of the type I error. Default = 0.05.</p>
</td></tr>
<tr><td><code id="npc_+3A_split">split</code></td>
<td>
<p>the number of splits for the class 0 sample. Default = 1. For ensemble
version, choose split &gt; 1.</p>
</td></tr>
<tr><td><code id="npc_+3A_split.ratio">split.ratio</code></td>
<td>
<p>the ratio of splits used for the class 0 sample to train the
base classifier. The rest are used to estimate the threshold. Can also be set to be &quot;adaptive&quot;, which will be determined using a data-driven method implemented in <code>find.optim.split</code>. Default = 0.5.</p>
</td></tr>
<tr><td><code id="npc_+3A_n.cores">n.cores</code></td>
<td>
<p>number of cores used for parallel computing. Default = 1. WARNING:
windows machine is not supported.</p>
</td></tr>
<tr><td><code id="npc_+3A_band">band</code></td>
<td>
<p>whether to generate both lower and upper bounds of type II error. Default = FALSE.</p>
</td></tr>
<tr><td><code id="npc_+3A_nfolds">nfolds</code></td>
<td>
<p>number of folds for performing adaptive split ratio selection. Default = 10.</p>
</td></tr>
<tr><td><code id="npc_+3A_randseed">randSeed</code></td>
<td>
<p>the random seed used in the algorithm.</p>
</td></tr>
<tr><td><code id="npc_+3A_warning">warning</code></td>
<td>
<p>whether to show various warnings in the program. Default = TRUE.</p>
</td></tr>
<tr><td><code id="npc_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object with S3 class npc.
</p>
<table>
<tr><td><code>fits</code></td>
<td>
<p>a list of length max(1,split), represents the fit during each split.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the base classification method.</p>
</td></tr>
<tr><td><code>split</code></td>
<td>
<p>the number of splits used.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Xin Tong, Yang Feng, and Jingyi Jessica Li (2018), Neyman-Pearson (NP) classification algorithms and NP receiver operating characteristic (NP-ROC), <em>Science Advances</em>, <b>4</b>, 2, eaao1659.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nproc">nproc</a></code> and <code><a href="#topic+predict.npc">predict.npc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
n = 1000
x = matrix(rnorm(n*2),n,2)
c = 1+3*x[,1]
y = rbinom(n,1,1/(1+exp(-c)))
xtest = matrix(rnorm(n*2),n,2)
ctest = 1+3*xtest[,1]
ytest = rbinom(n,1,1/(1+exp(-ctest)))

##Use lda classifier and the default type I error control with alpha=0.05, delta=0.05
fit = npc(x, y, method = 'lda')
pred = predict(fit,xtest)
fit.score = predict(fit,x)
accuracy = mean(pred$pred.label==ytest)
cat('Overall Accuracy: ',  accuracy,'\n')
ind0 = which(ytest==0)
typeI = mean(pred$pred.label[ind0]!=ytest[ind0]) #type I error on test set
cat('Type I error: ', typeI, '\n')

## Not run: 
##Ensembled lda classifier with split = 11,  alpha=0.05, delta=0.05
fit = npc(x, y, method = 'lda', split = 11)
pred = predict(fit,xtest)
accuracy = mean(pred$pred.label==ytest)
cat('Overall Accuracy: ',  accuracy,'\n')
ind0 = which(ytest==0)
typeI = mean(pred$pred.label[ind0]!=ytest[ind0]) #type I error on test set
cat('Type I error: ', typeI, '\n')

##Now, change the method to logistic regression and change alpha to 0.1
fit = npc(x, y, method = 'logistic', alpha = 0.1)
pred = predict(fit,xtest)
accuracy = mean(pred$pred.label==ytest)
cat('Overall Accuracy: ',  accuracy,'\n')
ind0 = which(ytest==0)
typeI = mean(pred$pred.label[ind0]!=ytest[ind0]) #type I error on test set
cat('Type I error: ', typeI, '\n')

##Now, change the method to adaboost
fit = npc(x, y, method = 'ada', alpha = 0.1)
pred = predict(fit,xtest)
accuracy = mean(pred$pred.label==ytest)
cat('Overall Accuracy: ',  accuracy,'\n')
ind0 = which(ytest==0)
typeI = mean(pred$pred.label[ind0]!=ytest[ind0]) #type I error on test set
cat('Type I error: ', typeI, '\n')

##Now, try the adaptive splitting ratio
fit = npc(x, y, method = 'ada', alpha = 0.1, split.ratio = 'adaptive')
pred = predict(fit,xtest)
accuracy = mean(pred$pred.label==ytest)
cat('Overall Accuracy: ',  accuracy,'\n')
ind0 = which(ytest==0)
typeI = mean(pred$pred.label[ind0]!=ytest[ind0]) #type I error on test set
cat('Type I error: ', typeI, '\n')
cat('Splitting ratio:', fit$split.ratio)

## End(Not run)
</code></pre>

<hr>
<h2 id='nproc'>Calculate the Neyman-Pearson Receiver Operating Characteristics</h2><span id='topic+nproc'></span>

<h3>Description</h3>

<p><code>nproc</code> calculates the Neyman-Pearson Receiver Operating Characteristics
band for a given sequence of type I error values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nproc(x = NULL, y, method = c("logistic", "penlog", "svm", "randomforest",
  "lda", "nb", "nnb", "ada", "tree"), delta = 0.05, split = 1,
  split.ratio = 0.5, n.cores = 1, randSeed = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nproc_+3A_x">x</code></td>
<td>
<p>n * p observation matrix. n observations, p covariates.</p>
</td></tr>
<tr><td><code id="nproc_+3A_y">y</code></td>
<td>
<p>n 0/1 observatons.</p>
</td></tr>
<tr><td><code id="nproc_+3A_method">method</code></td>
<td>
<p>base classification method(s).
</p>

<ul>
<li><p> logistic: Logistic regression. <a href="stats.html#topic+glm">glm</a> function with family = 'binomial'
</p>
</li>
<li><p> penlog: Penalized logistic regression with LASSO penalty. <code><a href="glmnet.html#topic+glmnet">glmnet</a></code> in <code>glmnet</code> package
</p>
</li>
<li><p> svm: Support Vector Machines. <code><a href="e1071.html#topic+svm">svm</a></code> in <code>e1071</code> package
</p>
</li>
<li><p> randomforest: Random Forest. <code><a href="randomForest.html#topic+randomForest">randomForest</a></code> in <code>randomForest</code> package
</p>
</li>
<li><p> Linear Discriminant Analysis. lda: <code><a href="MASS.html#topic+lda">lda</a></code> in <code>MASS</code> package
</p>
</li>
<li><p> nb: Naive Bayes. <code><a href="e1071.html#topic+naiveBayes">naiveBayes</a></code> in <code>e1071</code> package
</p>
</li>
<li><p> nnb: Nonparametric Naive Bayes. <code><a href="naivebayes.html#topic+naive_bayes">naive_bayes</a></code> in <code>naivebayes</code> package
</p>
</li>
<li><p> ada: Ada-Boost. <code><a href="ada.html#topic+ada">ada</a></code> in <code>ada</code> package
</p>
</li></ul>
</td></tr>
<tr><td><code id="nproc_+3A_delta">delta</code></td>
<td>
<p>the violation rate of the type I error. Default = 0.05.</p>
</td></tr>
<tr><td><code id="nproc_+3A_split">split</code></td>
<td>
<p>the number of splits for the class 0 sample. Default = 1. For ensemble
version, choose split &gt; 1.</p>
</td></tr>
<tr><td><code id="nproc_+3A_split.ratio">split.ratio</code></td>
<td>
<p>the ratio of splits used for the class 0 sample to train the
classifier. Default = 0.5.</p>
</td></tr>
<tr><td><code id="nproc_+3A_n.cores">n.cores</code></td>
<td>
<p>number of cores used for parallel computing. Default = 1.</p>
</td></tr>
<tr><td><code id="nproc_+3A_randseed">randSeed</code></td>
<td>
<p>the random seed used in the algorithm.</p>
</td></tr>
<tr><td><code id="nproc_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object with S3 class nproc.
</p>
<table>
<tr><td><code>typeI.u</code></td>
<td>
<p>sequence of upper bound of type I error.</p>
</td></tr>
<tr><td><code>typeII.l</code></td>
<td>
<p>sequence of lower bound of type II error.</p>
</td></tr>
<tr><td><code>typeII.u</code></td>
<td>
<p>sequence of upper bound of type II error.</p>
</td></tr>
<tr><td><code>auc.l</code></td>
<td>
<p>the auc value of the lower NP-ROC curve.</p>
</td></tr>
<tr><td><code>auc.u</code></td>
<td>
<p>the auc value of the upper NP-ROC curve.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the base classification method implemented.</p>
</td></tr>
<tr><td><code>delta</code></td>
<td>
<p>the violation rate.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Xin Tong, Yang Feng, and Jingyi Jessica Li (2018), Neyman-Pearson (NP) classification algorithms and NP receiver operating characteristic (NP-ROC), <em>Science Advances</em>, <b>4</b>, 2, eaao1659.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+npc">npc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n = 200
x = matrix(rnorm(n*2),n,2)
c = 1 - 3*x[,1]
y = rbinom(n,1,1/(1+exp(-c)))
#fit = nproc(x, y, method = 'svm')
fit2 = nproc(x, y, method = 'penlog')
##Plot the nproc curve
plot(fit2)

## Not run: 
fit3 = nproc(x, y, method = 'penlog',  n.cores = 2)
#In practice, replace 2 by the number of cores available 'detectCores()'
fit4 = nproc(x, y, method = 'penlog', n.cores = detectCores())

#Confidence nproc curves
fit6 = nproc(x, y, method = 'lda')
plot(fit6)
nproc ensembled version
fit7 = nproc(x, y, method = 'lda', split = 11)
plot(fit7)

## End(Not run)

</code></pre>

<hr>
<h2 id='plot.nproc'>Plot the nproc band(s).</h2><span id='topic+plot.nproc'></span>

<h3>Description</h3>

<p>Plot the nproc band(s).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nproc'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.nproc_+3A_x">x</code></td>
<td>
<p>fitted nproc object using <code>nproc</code>.</p>
</td></tr>
<tr><td><code id="plot.nproc_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+npc">npc</a></code>, <code><a href="#topic+nproc">nproc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n = 1000
x = matrix(rnorm(n*2),n,2)
c = 1+3*x[,1]
y = rbinom(n,1,1/(1+exp(-c)))
fit = nproc(x, y, method = 'lda')
plot(fit)
</code></pre>

<hr>
<h2 id='predict.npc'>Predicting the outcome of a set of new observations using the fitted npc
object.</h2><span id='topic+predict.npc'></span>

<h3>Description</h3>

<p>Predicting the outcome of a set of new observations using the fitted npc
object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'npc'
predict(object, newx = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.npc_+3A_object">object</code></td>
<td>
<p>fitted npc object using <code>npc</code>.</p>
</td></tr>
<tr><td><code id="predict.npc_+3A_newx">newx</code></td>
<td>
<p>a set of new observations.</p>
</td></tr>
<tr><td><code id="predict.npc_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the predicted label and score.
</p>
<table>
<tr><td><code>pred.label</code></td>
<td>
<p>Predicted label vector.</p>
</td></tr>
<tr><td><code>pred.score</code></td>
<td>
<p>Predicted score vector.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+npc">npc</a></code> and <code><a href="#topic+nproc">nproc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n = 1000
x = matrix(rnorm(n*2),n,2)
c = 1+3*x[,1]
y = rbinom(n,1,1/(1+exp(-c)))
xtest = matrix(rnorm(n*2),n,2)
ctest = 1+3*xtest[,1]
ytest = rbinom(n,1,1/(1+exp(-ctest)))


## Not run: 
##Use logistic classifier and the default type I error control with alpha=0.05
fit = npc(x, y, method = 'logistic')
pred = predict(fit,xtest)
fit.score = predict(fit,x)
accuracy = mean(pred$pred.label==ytest)
cat('Overall Accuracy: ',  accuracy,'\n')
ind0 = which(ytest==0)
ind1 = which(ytest==1)
typeI = mean(pred$pred.label[ind0]!=ytest[ind0]) #type I error on test set
cat('Type I error: ', typeI, '\n')
typeII = mean(pred$pred.label[ind1]!=ytest[ind1]) #type II error on test set
cat('Type II error: ', typeII, '\n')

## End(Not run)
</code></pre>

<hr>
<h2 id='print.npc'>Print the npc object.</h2><span id='topic+print.npc'></span>

<h3>Description</h3>

<p>Print the npc object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'npc'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.npc_+3A_x">x</code></td>
<td>
<p>fitted npc object using <code>npc</code>.</p>
</td></tr>
<tr><td><code id="print.npc_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+npc">npc</a></code>, <code><a href="#topic+nproc">nproc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n = 1000
x = matrix(rnorm(n*2),n,2)
c = 1+3*x[,1]
y = rbinom(n,1,1/(1+exp(-c)))
fit = npc(x, y, method = 'lda')
print(fit)
</code></pre>

<hr>
<h2 id='print.nproc'>Print the nproc object.</h2><span id='topic+print.nproc'></span>

<h3>Description</h3>

<p>Print the nproc object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nproc'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.nproc_+3A_x">x</code></td>
<td>
<p>fitted nproc object using <code>nproc</code>.</p>
</td></tr>
<tr><td><code id="print.nproc_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+npc">npc</a></code>, <code><a href="#topic+nproc">nproc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n = 1000
x = matrix(rnorm(n*2),n,2)
c = 1+3*x[,1]
y = rbinom(n,1,1/(1+exp(-c)))
fit = nproc(x, y, method = 'lda')
print(fit)
</code></pre>

<hr>
<h2 id='rocCV'>Calculate the Receiver Operating Characteristics with Cross-validation or Subsampling</h2><span id='topic+rocCV'></span>

<h3>Description</h3>

<p><code>rocCV</code> calculates the receiver operating characterisitc with cross-validation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rocCV(x = NULL, y, method = c("logistic", "penlog", "svm", "randomforest",
  "lda", "nb", "ada", "tree"), metric = "CV", n.folds = 5,
  train.frac = 0.5, n.cores = 1, randSeed = 0, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rocCV_+3A_x">x</code></td>
<td>
<p>n * p observation matrix. n observations, p covariates.</p>
</td></tr>
<tr><td><code id="rocCV_+3A_y">y</code></td>
<td>
<p>n 0/1 observatons.</p>
</td></tr>
<tr><td><code id="rocCV_+3A_method">method</code></td>
<td>
<p>classification method(s).
</p>

<ul>
<li><p> logistic: Logistic regression. <a href="stats.html#topic+glm">glm</a> function with family = 'binomial'
</p>
</li>
<li><p> penlog: Penalized logistic regression with LASSO penalty. <code><a href="glmnet.html#topic+glmnet">glmnet</a></code> in <code>glmnet</code> package
</p>
</li>
<li><p> svm: Support Vector Machines. <code><a href="e1071.html#topic+svm">svm</a></code> in <code>e1071</code> package
</p>
</li>
<li><p> randomforest: Random Forest. <code><a href="randomForest.html#topic+randomForest">randomForest</a></code> in <code>randomForest</code> package
</p>
</li>
<li><p> Linear Discriminant Analysis. lda: <code><a href="MASS.html#topic+lda">lda</a></code> in <code>MASS</code> package
</p>
</li>
<li><p> nb: Naive Bayes. <code><a href="e1071.html#topic+naiveBayes">naiveBayes</a></code> in <code>e1071</code> package
</p>
</li>
<li><p> ada: Ada-Boost. <code><a href="ada.html#topic+ada">ada</a></code> in <code>ada</code> package
</p>
</li></ul>
</td></tr>
<tr><td><code id="rocCV_+3A_metric">metric</code></td>
<td>
<p>metric used for averging performance. Includes 'CV' and 'SS' as options. Default = 'CV'.</p>
</td></tr>
<tr><td><code id="rocCV_+3A_n.folds">n.folds</code></td>
<td>
<p>number of folds used for cross-validation or the number of splits in the subsampling. Default = 5.</p>
</td></tr>
<tr><td><code id="rocCV_+3A_train.frac">train.frac</code></td>
<td>
<p>fraction of training data in the subsampling process. Default = 0.5.</p>
</td></tr>
<tr><td><code id="rocCV_+3A_n.cores">n.cores</code></td>
<td>
<p>number of cores used for parallel computing. Default = 1.</p>
</td></tr>
<tr><td><code id="rocCV_+3A_randseed">randSeed</code></td>
<td>
<p>the random seed used in the algorithm. Default = 0.</p>
</td></tr>
<tr><td><code id="rocCV_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list.
</p>
<table>
<tr><td><code>fpr</code></td>
<td>
<p>sequence of false positive rate.</p>
</td></tr>
<tr><td><code>tpr</code></td>
<td>
<p>sequence of true positive rate.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Xin Tong, Yang Feng, and Jingyi Jessica Li (2018), Neyman-Pearson (NP) classification algorithms and NP receiver operating characteristic (NP-ROC), <em>Science Advances</em>, <b>4</b>, 2, eaao1659.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nproc">nproc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n = 200
x = matrix(rnorm(n*2),n,2)
c = 1 - 3*x[,1]
y = rbinom(n,1,1/(1+exp(-c)))
fit = rocCV(x, y, method = 'svm')
fit2 = rocCV(x, y, method = 'penlog')
fit3 = rocCV(x, y, method = 'penlog', metric = 'SS')
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
