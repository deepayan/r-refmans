<!DOCTYPE html><html><head><title>Help for package BayesMallows</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {BayesMallows}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#assess_convergence'><p>Trace Plots from Metropolis-Hastings Algorithm</p></a></li>
<li><a href='#assign_cluster'><p>Assign Assessors to Clusters</p></a></li>
<li><a href='#asymptotic_partition_function'><p>Asymptotic Approximation of Partition Function</p></a></li>
<li><a href='#BayesMallows-package'><p>BayesMallows: Bayesian Preference Learning with the Mallows Rank Model</p></a></li>
<li><a href='#beach_preferences'><p>Beach preferences</p></a></li>
<li><a href='#bernoulli_data'><p>Simulated intransitive pairwise preferences</p></a></li>
<li><a href='#burnin'><p>See the burnin</p></a></li>
<li><a href='#burnin&lt;-'><p>Set the burnin</p></a></li>
<li><a href='#cluster_data'><p>Simulated clustering data</p></a></li>
<li><a href='#compute_consensus'><p>Compute Consensus Ranking</p></a></li>
<li><a href='#compute_expected_distance'><p>Expected value of metrics under a Mallows rank model</p></a></li>
<li><a href='#compute_mallows'><p>Preference Learning with the Mallows Rank Model</p></a></li>
<li><a href='#compute_mallows_mixtures'><p>Compute Mixtures of Mallows Models</p></a></li>
<li><a href='#compute_mallows_sequentially'><p>Estimate the Bayesian Mallows Model Sequentially</p></a></li>
<li><a href='#compute_observation_frequency'><p>Frequency distribution of the ranking sequences</p></a></li>
<li><a href='#compute_posterior_intervals'><p>Compute Posterior Intervals</p></a></li>
<li><a href='#compute_rank_distance'><p>Distance between a set of rankings and a given rank sequence</p></a></li>
<li><a href='#create_ranking'><p>Convert between ranking and ordering.</p></a></li>
<li><a href='#estimate_partition_function'><p>Estimate Partition Function</p></a></li>
<li><a href='#get_acceptance_ratios'><p>Get Acceptance Ratios</p></a></li>
<li><a href='#get_cardinalities'><p>Get cardinalities for each distance</p></a></li>
<li><a href='#get_mallows_loglik'><p>Likelihood and log-likelihood evaluation for a Mallows mixture model</p></a></li>
<li><a href='#get_transitive_closure'><p>Get transitive closure</p></a></li>
<li><a href='#heat_plot'><p>Heat plot of posterior probabilities</p></a></li>
<li><a href='#plot_elbow'><p>Plot Within-Cluster Sum of Distances</p></a></li>
<li><a href='#plot_top_k'><p>Plot Top-k Rankings with Pairwise Preferences</p></a></li>
<li><a href='#plot.BayesMallows'><p>Plot Posterior Distributions</p></a></li>
<li><a href='#plot.SMCMallows'><p>Plot SMC Posterior Distributions</p></a></li>
<li><a href='#potato_true_ranking'><p>True ranking of the weights of 20 potatoes.</p></a></li>
<li><a href='#potato_visual'><p>Potato weights assessed visually</p></a></li>
<li><a href='#potato_weighing'><p>Potato weights assessed by hand</p></a></li>
<li><a href='#predict_top_k'><p>Predict Top-k Rankings with Pairwise Preferences</p></a></li>
<li><a href='#print.BayesMallows'><p>Print Method for BayesMallows Objects</p></a></li>
<li><a href='#rmallows'><p>Sample from the Mallows distribution.</p></a></li>
<li><a href='#sample_mallows'><p>Random Samples from the Mallows Rank Model</p></a></li>
<li><a href='#sample_prior'><p>Sample from prior distribution</p></a></li>
<li><a href='#set_compute_options'><p>Specify options for computation</p></a></li>
<li><a href='#set_initial_values'><p>Set initial values of scale parameter and modal ranking</p></a></li>
<li><a href='#set_model_options'><p>Set options for Bayesian Mallows model</p></a></li>
<li><a href='#set_priors'><p>Set prior parameters for Bayesian Mallows model</p></a></li>
<li><a href='#set_smc_options'><p>Set SMC compute options</p></a></li>
<li><a href='#setup_rank_data'><p>Setup rank data</p></a></li>
<li><a href='#sushi_rankings'><p>Sushi rankings</p></a></li>
<li><a href='#update_mallows'><p>Update a Bayesian Mallows model with new users</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Bayesian Preference Learning with the Mallows Rank Model</td>
</tr>
<tr>
<td>Version:</td>
<td>2.1.1</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Oystein Sorensen &lt;oystein.sorensen.1985@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>An implementation of the Bayesian version of the Mallows rank model
    (Vitelli et al., Journal of Machine Learning Research, 2018 <a href="https://jmlr.org/papers/v18/15-481.html">https://jmlr.org/papers/v18/15-481.html</a>;
    Crispino et al., Annals of Applied Statistics, 2019 &lt;<a href="https://doi.org/10.1214%2F18-AOAS1203">doi:10.1214/18-AOAS1203</a>&gt;;
    Sorensen et al., R Journal, 2020 &lt;<a href="https://doi.org/10.32614%2FRJ-2020-026">doi:10.32614/RJ-2020-026</a>&gt;;
    Stein, PhD Thesis, 2023 <a href="https://eprints.lancs.ac.uk/id/eprint/195759">https://eprints.lancs.ac.uk/id/eprint/195759</a>). Both Metropolis-Hastings
    and sequential Monte Carlo algorithms for estimating the models are available. Cayley, footrule,
    Hamming, Kendall, Spearman, and Ulam distances are supported in the models. The rank data to be
    analyzed can be in the form of complete rankings, top-k rankings, partially missing rankings, as well
    as consistent and inconsistent pairwise preferences. Several functions for plotting and studying the
    posterior distributions of parameters are provided. The package also provides functions for estimating
    the partition function (normalizing constant) of the Mallows rank model, both with the importance
    sampling algorithm of Vitelli et al. and asymptotic approximation with the IPFP algorithm
    (Mukherjee, Annals of Statistics, 2016 &lt;<a href="https://doi.org/10.1214%2F15-AOS1389">doi:10.1214/15-AOS1389</a>&gt;).</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/ocbe-uio/BayesMallows">https://github.com/ocbe-uio/BayesMallows</a>,
<a href="https://ocbe-uio.github.io/BayesMallows/">https://ocbe-uio.github.io/BayesMallows/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ocbe-uio/BayesMallows/issues">https://github.com/ocbe-uio/BayesMallows/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 1.0.0), ggplot2 (&ge; 3.1.0), Rdpack (&ge; 1.0), igraph
(&ge; 1.2.5), sets (&ge; 1.0-18), relations (&ge; 0.6-8), rlang (&ge;
0.3.1)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo, testthat</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, testthat (&ge; 3.0.0), label.switching (&ge; 1.7),
rmarkdown, covr, parallel (&ge; 3.5.1)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>RdMacros:</td>
<td>Rdpack</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-14 15:26:53 UTC; oyss</td>
</tr>
<tr>
<td>Author:</td>
<td>Oystein Sorensen <a href="https://orcid.org/0000-0003-0724-3542"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Waldir Leoncio [aut],
  Valeria Vitelli <a href="https://orcid.org/0000-0002-6746-0453"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Marta Crispino [aut],
  Qinghua Liu [aut],
  Cristina Mollica [aut],
  Luca Tardella [aut],
  Anja Stein [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-15 12:30:11 UTC</td>
</tr>
</table>
<hr>
<h2 id='assess_convergence'>Trace Plots from Metropolis-Hastings Algorithm</h2><span id='topic+assess_convergence'></span><span id='topic+assess_convergence.BayesMallows'></span><span id='topic+assess_convergence.BayesMallowsMixtures'></span>

<h3>Description</h3>

<p><code>assess_convergence</code> provides trace plots for the parameters of the Mallows
Rank model, in order to study the convergence of the Metropolis-Hastings
algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>assess_convergence(model_fit, ...)

## S3 method for class 'BayesMallows'
assess_convergence(
  model_fit,
  parameter = c("alpha", "rho", "Rtilde", "cluster_probs", "theta"),
  items = NULL,
  assessors = NULL,
  ...
)

## S3 method for class 'BayesMallowsMixtures'
assess_convergence(
  model_fit,
  parameter = c("alpha", "cluster_probs"),
  items = NULL,
  assessors = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="assess_convergence_+3A_model_fit">model_fit</code></td>
<td>
<p>A fitted model object of class <code>BayesMallows</code> returned from
<code><a href="#topic+compute_mallows">compute_mallows()</a></code> or an object of class <code>BayesMallowsMixtures</code> returned
from <code><a href="#topic+compute_mallows_mixtures">compute_mallows_mixtures()</a></code>.</p>
</td></tr>
<tr><td><code id="assess_convergence_+3A_...">...</code></td>
<td>
<p>Other arguments passed on to other methods. Currently not used.</p>
</td></tr>
<tr><td><code id="assess_convergence_+3A_parameter">parameter</code></td>
<td>
<p>Character string specifying which parameter to plot.
Available options are <code>"alpha"</code>, <code>"rho"</code>, <code>"Rtilde"</code>, <code>"cluster_probs"</code>, or
<code>"theta"</code>.</p>
</td></tr>
<tr><td><code id="assess_convergence_+3A_items">items</code></td>
<td>
<p>The items to study in the diagnostic plot for <code>rho</code>. Either a
vector of item names, corresponding to <code>model_fit$data$items</code> or a vector of
indices. If NULL, five items are selected randomly. Only used when
<code>parameter = "rho"</code> or <code>parameter = "Rtilde"</code>.</p>
</td></tr>
<tr><td><code id="assess_convergence_+3A_assessors">assessors</code></td>
<td>
<p>Numeric vector specifying the assessors to study in the
diagnostic plot for <code>"Rtilde"</code>.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
# Fit a model on the potato_visual data
mod &lt;- compute_mallows(setup_rank_data(potato_visual))
# Check for convergence
assess_convergence(mod)
assess_convergence(mod, parameter = "rho", items = 1:20)
</code></pre>

<hr>
<h2 id='assign_cluster'>Assign Assessors to Clusters</h2><span id='topic+assign_cluster'></span>

<h3>Description</h3>

<p>Assign assessors to clusters by finding the cluster with highest
posterior probability.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>assign_cluster(model_fit, soft = TRUE, expand = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="assign_cluster_+3A_model_fit">model_fit</code></td>
<td>
<p>An object of type <code>BayesMallows</code>, returned from
<code><a href="#topic+compute_mallows">compute_mallows()</a></code>.</p>
</td></tr>
<tr><td><code id="assign_cluster_+3A_soft">soft</code></td>
<td>
<p>A logical specifying whether to perform soft or hard clustering.
If <code>soft=TRUE</code>, all cluster probabilities are returned, whereas if
<code>soft=FALSE</code>, only the maximum a posterior (MAP) cluster probability is
returned, per assessor. In the case of a tie between two or more cluster
assignments, a random cluster is taken as MAP estimate.</p>
</td></tr>
<tr><td><code id="assign_cluster_+3A_expand">expand</code></td>
<td>
<p>A logical specifying whether or not to expand the rowset of
each assessor to also include clusters for which the assessor has 0 a
posterior assignment probability. Only used when <code>soft = TRUE</code>. Defaults to
<code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataframe. If <code>soft = FALSE</code>, it has one row per assessor, and
columns <code>assessor</code>, <code>probability</code> and <code>map_cluster</code>. If <code>soft = TRUE</code>, it
has <code>n_cluster</code> rows per assessor, and the additional column <code>cluster</code>.
</p>


<h3>See Also</h3>

<p>Other posterior quantities: 
<code><a href="#topic+compute_consensus">compute_consensus</a>()</code>,
<code><a href="#topic+compute_posterior_intervals">compute_posterior_intervals</a>()</code>,
<code><a href="#topic+get_acceptance_ratios">get_acceptance_ratios</a>()</code>,
<code><a href="#topic+heat_plot">heat_plot</a>()</code>,
<code><a href="#topic+plot.BayesMallows">plot.BayesMallows</a>()</code>,
<code><a href="#topic+plot.SMCMallows">plot.SMCMallows</a>()</code>,
<code><a href="#topic+plot_elbow">plot_elbow</a>()</code>,
<code><a href="#topic+plot_top_k">plot_top_k</a>()</code>,
<code><a href="#topic+predict_top_k">predict_top_k</a>()</code>,
<code><a href="#topic+print.BayesMallows">print.BayesMallows</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Fit a model with three clusters to the simulated example data
set.seed(1)
mixture_model &lt;- compute_mallows(
  data = setup_rank_data(cluster_data),
  model_options = set_model_options(n_clusters = 3),
  compute_options = set_compute_options(nmc = 5000, burnin = 1000)
)

head(assign_cluster(mixture_model))
head(assign_cluster(mixture_model, soft = FALSE))

</code></pre>

<hr>
<h2 id='asymptotic_partition_function'>Asymptotic Approximation of Partition Function</h2><span id='topic+asymptotic_partition_function'></span>

<h3>Description</h3>

<p>Compute the asymptotic approximation of the logarithm of the partition function,
using the iteration algorithm of Mukherjee (2016).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>asymptotic_partition_function(
  alpha_vector,
  n_items,
  metric,
  K,
  n_iterations = 1000L,
  tol = 1e-09
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="asymptotic_partition_function_+3A_alpha_vector">alpha_vector</code></td>
<td>
<p>A numeric vector of alpha values.</p>
</td></tr>
<tr><td><code id="asymptotic_partition_function_+3A_n_items">n_items</code></td>
<td>
<p>Integer specifying the number of items.</p>
</td></tr>
<tr><td><code id="asymptotic_partition_function_+3A_metric">metric</code></td>
<td>
<p>One of <code>"footrule"</code> and <code>"spearman"</code>.</p>
</td></tr>
<tr><td><code id="asymptotic_partition_function_+3A_k">K</code></td>
<td>
<p>Integer.</p>
</td></tr>
<tr><td><code id="asymptotic_partition_function_+3A_n_iterations">n_iterations</code></td>
<td>
<p>Integer specifying the number of iterations.</p>
</td></tr>
<tr><td><code id="asymptotic_partition_function_+3A_tol">tol</code></td>
<td>
<p>Stopping criterion for algorithm. The previous matrix is subtracted
from the updated, and if the maximum absolute relative difference is below <code>tol</code>,
the iteration stops.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector, containing the partition function at each value of alpha.
</p>


<h3>References</h3>

<p>Mukherjee S (2016).
&ldquo;Estimation in exponential families on permutations.&rdquo;
<em>The Annals of Statistics</em>, <b>44</b>(2), 853&ndash;875.
<a href="https://doi.org/10.1214/15-aos1389">doi:10.1214/15-aos1389</a>.
</p>

<hr>
<h2 id='BayesMallows-package'>BayesMallows: Bayesian Preference Learning with the Mallows Rank Model</h2><span id='topic+BayesMallows'></span><span id='topic+BayesMallows-package'></span>

<h3>Description</h3>

<p>An implementation of the Bayesian version of the Mallows rank model (Vitelli et al., Journal of Machine Learning Research, 2018 <a href="https://jmlr.org/papers/v18/15-481.html">https://jmlr.org/papers/v18/15-481.html</a>; Crispino et al., Annals of Applied Statistics, 2019 <a href="https://doi.org/10.1214/18-AOAS1203">doi:10.1214/18-AOAS1203</a>; Sorensen et al., R Journal, 2020 <a href="https://doi.org/10.32614/RJ-2020-026">doi:10.32614/RJ-2020-026</a>; Stein, PhD Thesis, 2023 <a href="https://eprints.lancs.ac.uk/id/eprint/195759">https://eprints.lancs.ac.uk/id/eprint/195759</a>). Both Metropolis-Hastings and sequential Monte Carlo algorithms for estimating the models are available. Cayley, footrule, Hamming, Kendall, Spearman, and Ulam distances are supported in the models. The rank data to be analyzed can be in the form of complete rankings, top-k rankings, partially missing rankings, as well as consistent and inconsistent pairwise preferences. Several functions for plotting and studying the posterior distributions of parameters are provided. The package also provides functions for estimating the partition function (normalizing constant) of the Mallows rank model, both with the importance sampling algorithm of Vitelli et al. and asymptotic approximation with the IPFP algorithm (Mukherjee, Annals of Statistics, 2016 <a href="https://doi.org/10.1214/15-AOS1389">doi:10.1214/15-AOS1389</a>).
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Oystein Sorensen <a href="mailto:oystein.sorensen.1985@gmail.com">oystein.sorensen.1985@gmail.com</a> (<a href="https://orcid.org/0000-0003-0724-3542">ORCID</a>)
</p>
<p>Authors:
</p>

<ul>
<li><p> Waldir Leoncio <a href="mailto:w.l.netto@medisin.uio.no">w.l.netto@medisin.uio.no</a>
</p>
</li>
<li><p> Valeria Vitelli <a href="mailto:valeria.vitelli@medisin.uio.no">valeria.vitelli@medisin.uio.no</a> (<a href="https://orcid.org/0000-0002-6746-0453">ORCID</a>)
</p>
</li>
<li><p> Marta Crispino <a href="mailto:crispino.marta8@gmail.com">crispino.marta8@gmail.com</a>
</p>
</li>
<li><p> Qinghua Liu <a href="mailto:qinghual@math.uio.no">qinghual@math.uio.no</a>
</p>
</li>
<li><p> Cristina Mollica <a href="mailto:cristina.mollica@uniroma1.it">cristina.mollica@uniroma1.it</a>
</p>
</li>
<li><p> Luca Tardella
</p>
</li>
<li><p> Anja Stein
</p>
</li></ul>



<h3>References</h3>

<p>Sørensen Ø, Crispino M, Liu Q, Vitelli V (2020).
&ldquo;BayesMallows: An R Package for the Bayesian Mallows Model.&rdquo;
<em>The R Journal</em>, <b>12</b>(1), 324&ndash;342.
<a href="https://doi.org/10.32614/RJ-2020-026">doi:10.32614/RJ-2020-026</a>.
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/ocbe-uio/BayesMallows">https://github.com/ocbe-uio/BayesMallows</a>
</p>
</li>
<li> <p><a href="https://ocbe-uio.github.io/BayesMallows/">https://ocbe-uio.github.io/BayesMallows/</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/ocbe-uio/BayesMallows/issues">https://github.com/ocbe-uio/BayesMallows/issues</a>
</p>
</li></ul>


<hr>
<h2 id='beach_preferences'>Beach preferences</h2><span id='topic+beach_preferences'></span>

<h3>Description</h3>

<p>Example dataset from (Vitelli et al. 2018), Section 6.2.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>beach_preferences
</code></pre>


<h3>Format</h3>

<p>An object of class <code>data.frame</code> with 1442 rows and 3 columns.
</p>


<h3>References</h3>

<p>Vitelli V, Sørensen, Crispino M, Arjas E, Frigessi A (2018).
&ldquo;Probabilistic Preference Learning with the Mallows Rank Model.&rdquo;
<em>Journal of Machine Learning Research</em>, <b>18</b>(1), 1&ndash;49.
<a href="https://jmlr.org/papers/v18/15-481.html">https://jmlr.org/papers/v18/15-481.html</a>.
</p>


<h3>See Also</h3>

<p>Other datasets: 
<code><a href="#topic+bernoulli_data">bernoulli_data</a></code>,
<code><a href="#topic+cluster_data">cluster_data</a></code>,
<code><a href="#topic+potato_true_ranking">potato_true_ranking</a></code>,
<code><a href="#topic+potato_visual">potato_visual</a></code>,
<code><a href="#topic+potato_weighing">potato_weighing</a></code>,
<code><a href="#topic+sushi_rankings">sushi_rankings</a></code>
</p>

<hr>
<h2 id='bernoulli_data'>Simulated intransitive pairwise preferences</h2><span id='topic+bernoulli_data'></span>

<h3>Description</h3>

<p>Simulated dataset based on the <a href="#topic+potato_visual">potato_visual</a> data. Based on
the rankings in <a href="#topic+potato_visual">potato_visual</a>, all n-choose-2 = 190 pairs of items were
sampled from each assessor. With probability .9, the pairwise
preference was in agreement with <a href="#topic+potato_visual">potato_visual</a>, and with probability .1,
they were in disagreement. Hence, the data generating mechanism was a
Bernoulli error model (Crispino et al. 2019) with
<code class="reqn">\theta=0.1</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bernoulli_data
</code></pre>


<h3>Format</h3>

<p>An object of class <code>data.frame</code> with 2280 rows and 3 columns.
</p>


<h3>See Also</h3>

<p>Other datasets: 
<code><a href="#topic+beach_preferences">beach_preferences</a></code>,
<code><a href="#topic+cluster_data">cluster_data</a></code>,
<code><a href="#topic+potato_true_ranking">potato_true_ranking</a></code>,
<code><a href="#topic+potato_visual">potato_visual</a></code>,
<code><a href="#topic+potato_weighing">potato_weighing</a></code>,
<code><a href="#topic+sushi_rankings">sushi_rankings</a></code>
</p>

<hr>
<h2 id='burnin'>See the burnin</h2><span id='topic+burnin'></span><span id='topic+burnin.BayesMallows'></span><span id='topic+burnin.BayesMallowsMixtures'></span><span id='topic+burnin.SMCMallows'></span>

<h3>Description</h3>

<p>See the current burnin value of the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>burnin(model, ...)

## S3 method for class 'BayesMallows'
burnin(model, ...)

## S3 method for class 'BayesMallowsMixtures'
burnin(model, ...)

## S3 method for class 'SMCMallows'
burnin(model, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="burnin_+3A_model">model</code></td>
<td>
<p>A model object.</p>
</td></tr>
<tr><td><code id="burnin_+3A_...">...</code></td>
<td>
<p>Optional arguments passed on to other methods. Currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An integer specifying the burnin, if it exists. Otherwise <code>NULL</code>.
</p>


<h3>See Also</h3>

<p>Other modeling: 
<code><a href="#topic+burnin+3C-">burnin&lt;-</a>()</code>,
<code><a href="#topic+compute_mallows">compute_mallows</a>()</code>,
<code><a href="#topic+compute_mallows_mixtures">compute_mallows_mixtures</a>()</code>,
<code><a href="#topic+compute_mallows_sequentially">compute_mallows_sequentially</a>()</code>,
<code><a href="#topic+sample_prior">sample_prior</a>()</code>,
<code><a href="#topic+update_mallows">update_mallows</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(445)
mod &lt;- compute_mallows(setup_rank_data(potato_visual))
assess_convergence(mod)
burnin(mod)
burnin(mod) &lt;- 1500
burnin(mod)
plot(mod)
#'
models &lt;- compute_mallows_mixtures(
  data = setup_rank_data(cluster_data),
  n_clusters = 1:3)
burnin(models)
burnin(models) &lt;- 100
burnin(models)
burnin(models) &lt;- c(100, 300, 200)
burnin(models)
</code></pre>

<hr>
<h2 id='burnin+26lt+3B-'>Set the burnin</h2><span id='topic+burnin+3C-'></span><span id='topic+burnin+3C-.BayesMallows'></span><span id='topic+burnin+3C-.BayesMallowsMixtures'></span>

<h3>Description</h3>

<p>Set or update the burnin of a model
computed using Metropolis-Hastings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>burnin(model, ...) &lt;- value

## S3 replacement method for class 'BayesMallows'
burnin(model, ...) &lt;- value

## S3 replacement method for class 'BayesMallowsMixtures'
burnin(model, ...) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="burnin+2B26lt+2B3B-_+3A_model">model</code></td>
<td>
<p>An object of class <code>BayesMallows</code> returned from
<code><a href="#topic+compute_mallows">compute_mallows()</a></code> or an object of class <code>BayesMallowsMixtures</code> returned
from <code><a href="#topic+compute_mallows_mixtures">compute_mallows_mixtures()</a></code>.</p>
</td></tr>
<tr><td><code id="burnin+2B26lt+2B3B-_+3A_...">...</code></td>
<td>
<p>Optional arguments passed on to other methods. Currently not used.</p>
</td></tr>
<tr><td><code id="burnin+2B26lt+2B3B-_+3A_value">value</code></td>
<td>
<p>An integer specifying the burnin. If <code>model</code> is of class
<code>BayesMallowsMixtures</code>, a single value will be assumed to be the burnin
for each model element. Alternatively, <code>value</code> can be specified as an
integer vector of the same length as <code>model</code>, and hence a separate burnin
can be set for each number of mixture components.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>BayesMallows</code> with burnin set.
</p>


<h3>See Also</h3>

<p>Other modeling: 
<code><a href="#topic+burnin">burnin</a>()</code>,
<code><a href="#topic+compute_mallows">compute_mallows</a>()</code>,
<code><a href="#topic+compute_mallows_mixtures">compute_mallows_mixtures</a>()</code>,
<code><a href="#topic+compute_mallows_sequentially">compute_mallows_sequentially</a>()</code>,
<code><a href="#topic+sample_prior">sample_prior</a>()</code>,
<code><a href="#topic+update_mallows">update_mallows</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(445)
mod &lt;- compute_mallows(setup_rank_data(potato_visual))
assess_convergence(mod)
burnin(mod)
burnin(mod) &lt;- 1500
burnin(mod)
plot(mod)
#'
models &lt;- compute_mallows_mixtures(
  data = setup_rank_data(cluster_data),
  n_clusters = 1:3)
burnin(models)
burnin(models) &lt;- 100
burnin(models)
burnin(models) &lt;- c(100, 300, 200)
burnin(models)
</code></pre>

<hr>
<h2 id='cluster_data'>Simulated clustering data</h2><span id='topic+cluster_data'></span>

<h3>Description</h3>

<p>Simulated dataset of 60 complete rankings of five items, with three
different clusters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster_data
</code></pre>


<h3>Format</h3>

<p>An object of class <code>matrix</code> (inherits from <code>array</code>) with 60 rows and 5 columns.
</p>


<h3>See Also</h3>

<p>Other datasets: 
<code><a href="#topic+beach_preferences">beach_preferences</a></code>,
<code><a href="#topic+bernoulli_data">bernoulli_data</a></code>,
<code><a href="#topic+potato_true_ranking">potato_true_ranking</a></code>,
<code><a href="#topic+potato_visual">potato_visual</a></code>,
<code><a href="#topic+potato_weighing">potato_weighing</a></code>,
<code><a href="#topic+sushi_rankings">sushi_rankings</a></code>
</p>

<hr>
<h2 id='compute_consensus'>Compute Consensus Ranking</h2><span id='topic+compute_consensus'></span><span id='topic+compute_consensus.BayesMallows'></span><span id='topic+compute_consensus.SMCMallows'></span>

<h3>Description</h3>

<p>Compute the consensus ranking using either cumulative
probability (CP) or maximum a posteriori (MAP) consensus
(Vitelli et al. 2018). For mixture models, the consensus
is given for each mixture. Consensus of augmented ranks can also be
computed for each assessor, by setting <code>parameter = "Rtilde"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_consensus(model_fit, ...)

## S3 method for class 'BayesMallows'
compute_consensus(
  model_fit,
  type = c("CP", "MAP"),
  parameter = c("rho", "Rtilde"),
  assessors = 1L,
  ...
)

## S3 method for class 'SMCMallows'
compute_consensus(model_fit, type = c("CP", "MAP"), parameter = "rho", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_consensus_+3A_model_fit">model_fit</code></td>
<td>
<p>A model fit.</p>
</td></tr>
<tr><td><code id="compute_consensus_+3A_...">...</code></td>
<td>
<p>Other arguments passed on to other methods. Currently not used.</p>
</td></tr>
<tr><td><code id="compute_consensus_+3A_type">type</code></td>
<td>
<p>Character string specifying which consensus to compute. Either
<code>"CP"</code> or <code>"MAP"</code>. Defaults to <code>"CP"</code>.</p>
</td></tr>
<tr><td><code id="compute_consensus_+3A_parameter">parameter</code></td>
<td>
<p>Character string defining the parameter for which to compute
the consensus. Defaults to <code>"rho"</code>. Available options are <code>"rho"</code> and
<code>"Rtilde"</code>, with the latter giving consensus rankings for augmented ranks.</p>
</td></tr>
<tr><td><code id="compute_consensus_+3A_assessors">assessors</code></td>
<td>
<p>When <code>parameter = "rho"</code>, this integer vector is used to
define the assessors for which to compute the augmented ranking. Defaults
to <code>1L</code>, which yields augmented rankings for assessor 1.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Vitelli V, Sørensen, Crispino M, Arjas E, Frigessi A (2018).
&ldquo;Probabilistic Preference Learning with the Mallows Rank Model.&rdquo;
<em>Journal of Machine Learning Research</em>, <b>18</b>(1), 1&ndash;49.
<a href="https://jmlr.org/papers/v18/15-481.html">https://jmlr.org/papers/v18/15-481.html</a>.
</p>


<h3>See Also</h3>

<p>Other posterior quantities: 
<code><a href="#topic+assign_cluster">assign_cluster</a>()</code>,
<code><a href="#topic+compute_posterior_intervals">compute_posterior_intervals</a>()</code>,
<code><a href="#topic+get_acceptance_ratios">get_acceptance_ratios</a>()</code>,
<code><a href="#topic+heat_plot">heat_plot</a>()</code>,
<code><a href="#topic+plot.BayesMallows">plot.BayesMallows</a>()</code>,
<code><a href="#topic+plot.SMCMallows">plot.SMCMallows</a>()</code>,
<code><a href="#topic+plot_elbow">plot_elbow</a>()</code>,
<code><a href="#topic+plot_top_k">plot_top_k</a>()</code>,
<code><a href="#topic+predict_top_k">predict_top_k</a>()</code>,
<code><a href="#topic+print.BayesMallows">print.BayesMallows</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The example datasets potato_visual and potato_weighing contain complete
# rankings of 20 items, by 12 assessors. We first analyse these using the
# Mallows model:
model_fit &lt;- compute_mallows(setup_rank_data(potato_visual))

# Se the documentation to compute_mallows for how to assess the convergence of
# the algorithm. Having chosen burin = 1000, we compute posterior intervals
burnin(model_fit) &lt;- 1000
# We then compute the CP consensus.
compute_consensus(model_fit, type = "CP")
# And we compute the MAP consensus
compute_consensus(model_fit, type = "MAP")

## Not run: 
  # CLUSTERWISE CONSENSUS
  # We can run a mixture of Mallows models, using the n_clusters argument
  # We use the sushi example data. See the documentation of compute_mallows for
  # a more elaborate example
  model_fit &lt;- compute_mallows(
    setup_rank_data(sushi_rankings),
    model_options = set_model_options(n_clusters = 5))
  # Keeping the burnin at 1000, we can compute the consensus ranking per cluster
  burnin(model_fit) &lt;- 1000
  cp_consensus_df &lt;- compute_consensus(model_fit, type = "CP")
  # We can now make a table which shows the ranking in each cluster:
  cp_consensus_df$cumprob &lt;- NULL
  stats::reshape(cp_consensus_df, direction = "wide", idvar = "ranking",
                 timevar = "cluster",
                 varying = list(sort(unique(cp_consensus_df$cluster))))

## End(Not run)

## Not run: 
  # MAP CONSENSUS FOR PAIRWISE PREFENCE DATA
  # We use the example dataset with beach preferences.
  model_fit &lt;- compute_mallows(setup_rank_data(preferences = beach_preferences))
  # We set burnin = 1000
  burnin(model_fit) &lt;- 1000
  # We now compute the MAP consensus
  map_consensus_df &lt;- compute_consensus(model_fit, type = "MAP")

  # CP CONSENSUS FOR AUGMENTED RANKINGS
  # We use the example dataset with beach preferences.
  model_fit &lt;- compute_mallows(
    setup_rank_data(preferences = beach_preferences),
    compute_options = set_compute_options(save_aug = TRUE, aug_thinning = 2))
  # We set burnin = 1000
  burnin(model_fit) &lt;- 1000
  # We now compute the CP consensus of augmented ranks for assessors 1 and 3
  cp_consensus_df &lt;- compute_consensus(
    model_fit, type = "CP", parameter = "Rtilde", assessors = c(1L, 3L))
  # We can also compute the MAP consensus for assessor 2
  map_consensus_df &lt;- compute_consensus(
    model_fit, type = "MAP", parameter = "Rtilde", assessors = 2L)

  # Caution!
  # With very sparse data or with too few iterations, there may be ties in the
  # MAP consensus. This is illustrated below for the case of only 5 post-burnin
  # iterations. Two MAP rankings are equally likely in this case (and for this
  # seed).
  model_fit &lt;- compute_mallows(
    setup_rank_data(preferences = beach_preferences),
    compute_options = set_compute_options(
      nmc = 1005, save_aug = TRUE, aug_thinning = 1))
  burnin(model_fit) &lt;- 1000
  compute_consensus(model_fit, type = "MAP", parameter = "Rtilde",
                    assessors = 2L)

## End(Not run)
</code></pre>

<hr>
<h2 id='compute_expected_distance'>Expected value of metrics under a Mallows rank model</h2><span id='topic+compute_expected_distance'></span>

<h3>Description</h3>

<p>Compute the expectation of several metrics under the Mallows
rank model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_expected_distance(
  alpha,
  n_items,
  metric = c("footrule", "spearman", "cayley", "hamming", "kendall", "ulam")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_expected_distance_+3A_alpha">alpha</code></td>
<td>
<p>Non-negative scalar specifying the scale (precision) parameter
in the Mallows rank model.</p>
</td></tr>
<tr><td><code id="compute_expected_distance_+3A_n_items">n_items</code></td>
<td>
<p>Integer specifying the number of items.</p>
</td></tr>
<tr><td><code id="compute_expected_distance_+3A_metric">metric</code></td>
<td>
<p>Character string specifying the distance measure to use.
Available options are <code>"kendall"</code>, <code>"cayley"</code>, <code>"hamming"</code>, <code>"ulam"</code>,
<code>"footrule"</code>, and <code>"spearman"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A scalar providing the expected value of the <code>metric</code> under the
Mallows rank model with distance specified by the <code>metric</code> argument.
</p>


<h3>See Also</h3>

<p>Other rank functions: 
<code><a href="#topic+compute_observation_frequency">compute_observation_frequency</a>()</code>,
<code><a href="#topic+compute_rank_distance">compute_rank_distance</a>()</code>,
<code><a href="#topic+create_ranking">create_ranking</a>()</code>,
<code><a href="#topic+get_mallows_loglik">get_mallows_loglik</a>()</code>,
<code><a href="#topic+sample_mallows">sample_mallows</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>compute_expected_distance(1, 5, metric = "kendall")
compute_expected_distance(2, 6, metric = "cayley")
compute_expected_distance(1.5, 7, metric = "hamming")
compute_expected_distance(5, 30, "ulam")
compute_expected_distance(3.5, 45, "footrule")
compute_expected_distance(4, 10, "spearman")
</code></pre>

<hr>
<h2 id='compute_mallows'>Preference Learning with the Mallows Rank Model</h2><span id='topic+compute_mallows'></span>

<h3>Description</h3>

<p>Compute the posterior distributions of the parameters of the
Bayesian Mallows Rank Model, given rankings or preferences stated by a set
of assessors.
</p>
<p>The <code>BayesMallows</code> package uses the following parametrization of the
Mallows rank model (Mallows 1957):
</p>
<p style="text-align: center;"><code class="reqn">p(r|\alpha,\rho) = \frac{1}{Z_{n}(\alpha)} \exp\left\{\frac{-\alpha}{n}
  d(r,\rho)\right\}</code>
</p>

<p>where <code class="reqn">r</code> is a ranking, <code class="reqn">\alpha</code> is a scale parameter, <code class="reqn">\rho</code>
is the latent consensus ranking, <code class="reqn">Z_{n}(\alpha)</code> is the partition
function (normalizing constant), and <code class="reqn">d(r,\rho)</code> is a distance function
measuring the distance between <code class="reqn">r</code> and <code class="reqn">\rho</code>. We refer to
Vitelli et al. (2018) for further details of the Bayesian
Mallows model.
</p>
<p><code>compute_mallows</code> always returns posterior distributions of the latent
consensus ranking <code class="reqn">\rho</code> and the scale parameter <code class="reqn">\alpha</code>. Several
distance measures are supported, and the preferences can take the form of
complete or incomplete rankings, as well as pairwise preferences.
<code>compute_mallows</code> can also compute mixtures of Mallows models, for
clustering of assessors with similar preferences.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_mallows(
  data,
  model_options = set_model_options(),
  compute_options = set_compute_options(),
  priors = set_priors(),
  initial_values = set_initial_values(),
  pfun_estimate = NULL,
  verbose = FALSE,
  cl = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_mallows_+3A_data">data</code></td>
<td>
<p>An object of class &quot;BayesMallowsData&quot; returned from
<code><a href="#topic+setup_rank_data">setup_rank_data()</a></code>.</p>
</td></tr>
<tr><td><code id="compute_mallows_+3A_model_options">model_options</code></td>
<td>
<p>An object of class &quot;BayesMallowsModelOptions&quot; returned
from <code><a href="#topic+set_model_options">set_model_options()</a></code>.</p>
</td></tr>
<tr><td><code id="compute_mallows_+3A_compute_options">compute_options</code></td>
<td>
<p>An object of class &quot;BayesMallowsComputeOptions&quot;
returned from <code><a href="#topic+set_compute_options">set_compute_options()</a></code>.</p>
</td></tr>
<tr><td><code id="compute_mallows_+3A_priors">priors</code></td>
<td>
<p>An object of class &quot;BayesMallowsPriors&quot; returned from
<code><a href="#topic+set_priors">set_priors()</a></code>.</p>
</td></tr>
<tr><td><code id="compute_mallows_+3A_initial_values">initial_values</code></td>
<td>
<p>An object of class &quot;BayesMallowsInitialValues&quot; returned
from <code><a href="#topic+set_initial_values">set_initial_values()</a></code>.</p>
</td></tr>
<tr><td><code id="compute_mallows_+3A_pfun_estimate">pfun_estimate</code></td>
<td>
<p>Object returned from <code><a href="#topic+estimate_partition_function">estimate_partition_function()</a></code>.
Defaults to <code>NULL</code>, and will only be used for footrule, Spearman, or
Ulam distances when the cardinalities are not available, cf.
<code><a href="#topic+get_cardinalities">get_cardinalities()</a></code>.</p>
</td></tr>
<tr><td><code id="compute_mallows_+3A_verbose">verbose</code></td>
<td>
<p>Logical specifying whether to print out the progress of the
Metropolis-Hastings algorithm. If <code>TRUE</code>, a notification is printed
every 1000th iteration. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="compute_mallows_+3A_cl">cl</code></td>
<td>
<p>Optional cluster returned from <code><a href="parallel.html#topic+makeCluster">parallel::makeCluster()</a></code>. If
provided, chains will be run in parallel, one on each node of <code>cl</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class BayesMallows.
</p>


<h3>References</h3>

<p>Mallows CL (1957).
&ldquo;Non-Null Ranking Models. I.&rdquo;
<em>Biometrika</em>, <b>44</b>(1/2), 114&ndash;130.<br /><br /> Vitelli V, Sørensen, Crispino M, Arjas E, Frigessi A (2018).
&ldquo;Probabilistic Preference Learning with the Mallows Rank Model.&rdquo;
<em>Journal of Machine Learning Research</em>, <b>18</b>(1), 1&ndash;49.
<a href="https://jmlr.org/papers/v18/15-481.html">https://jmlr.org/papers/v18/15-481.html</a>.
</p>


<h3>See Also</h3>

<p>Other modeling: 
<code><a href="#topic+burnin">burnin</a>()</code>,
<code><a href="#topic+burnin+3C-">burnin&lt;-</a>()</code>,
<code><a href="#topic+compute_mallows_mixtures">compute_mallows_mixtures</a>()</code>,
<code><a href="#topic+compute_mallows_sequentially">compute_mallows_sequentially</a>()</code>,
<code><a href="#topic+sample_prior">sample_prior</a>()</code>,
<code><a href="#topic+update_mallows">update_mallows</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># ANALYSIS OF COMPLETE RANKINGS
# The example datasets potato_visual and potato_weighing contain complete
# rankings of 20 items, by 12 assessors. We first analyse these using the Mallows
# model:
set.seed(1)
model_fit &lt;- compute_mallows(
  data = setup_rank_data(rankings = potato_visual),
  compute_options = set_compute_options(nmc = 2000)
  )

# We study the trace plot of the parameters
assess_convergence(model_fit, parameter = "alpha")
assess_convergence(model_fit, parameter = "rho", items = 1:4)

# Based on these plots, we set burnin = 1000.
burnin(model_fit) &lt;- 1000
# Next, we use the generic plot function to study the posterior distributions
# of alpha and rho
plot(model_fit, parameter = "alpha")
plot(model_fit, parameter = "rho", items = 10:15)

# We can also compute the CP consensus posterior ranking
compute_consensus(model_fit, type = "CP")

# And we can compute the posterior intervals:
# First we compute the interval for alpha
compute_posterior_intervals(model_fit, parameter = "alpha")
# Then we compute the interval for all the items
compute_posterior_intervals(model_fit, parameter = "rho")

# ANALYSIS OF PAIRWISE PREFERENCES
# The example dataset beach_preferences contains pairwise
# preferences between beaches stated by 60 assessors. There
# is a total of 15 beaches in the dataset.
beach_data &lt;- setup_rank_data(
  preferences = beach_preferences
)
# We then run the Bayesian Mallows rank model
# We save the augmented data for diagnostics purposes.
model_fit &lt;- compute_mallows(
  data = beach_data,
  compute_options = set_compute_options(save_aug = TRUE),
  verbose = TRUE)
# We can assess the convergence of the scale parameter
assess_convergence(model_fit)
# We can assess the convergence of latent rankings. Here we
# show beaches 1-5.
assess_convergence(model_fit, parameter = "rho", items = 1:5)
# We can also look at the convergence of the augmented rankings for
# each assessor.
assess_convergence(model_fit, parameter = "Rtilde",
                   items = c(2, 4), assessors = c(1, 2))
# Notice how, for assessor 1, the lines cross each other, while
# beach 2 consistently has a higher rank value (lower preference) for
# assessor 2. We can see why by looking at the implied orderings in
# beach_tc
subset(get_transitive_closure(beach_data), assessor %in% c(1, 2) &amp;
         bottom_item %in% c(2, 4) &amp; top_item %in% c(2, 4))
# Assessor 1 has no implied ordering between beach 2 and beach 4,
# while assessor 2 has the implied ordering that beach 4 is preferred
# to beach 2. This is reflected in the trace plots.


# CLUSTERING OF ASSESSORS WITH SIMILAR PREFERENCES
## Not run: 
  # The example dataset sushi_rankings contains 5000 complete
  # rankings of 10 types of sushi
  # We start with computing a 3-cluster solution
  model_fit &lt;- compute_mallows(
    data = setup_rank_data(sushi_rankings),
    model_options = set_model_options(n_clusters = 3),
    compute_options = set_compute_options(nmc = 10000),
    verbose = TRUE)
  # We then assess convergence of the scale parameter alpha
  assess_convergence(model_fit)
  # Next, we assess convergence of the cluster probabilities
  assess_convergence(model_fit, parameter = "cluster_probs")
  # Based on this, we set burnin = 1000
  # We now plot the posterior density of the scale parameters alpha in
  # each mixture:
  burnin(model_fit) &lt;- 1000
  plot(model_fit, parameter = "alpha")
  # We can also compute the posterior density of the cluster probabilities
  plot(model_fit, parameter = "cluster_probs")
  # We can also plot the posterior cluster assignment. In this case,
  # the assessors are sorted according to their maximum a posteriori cluster estimate.
  plot(model_fit, parameter = "cluster_assignment")
  # We can also assign each assessor to a cluster
  cluster_assignments &lt;- assign_cluster(model_fit, soft = FALSE)
  
## End(Not run)

# DETERMINING THE NUMBER OF CLUSTERS
## Not run: 
  # Continuing with the sushi data, we can determine the number of cluster
  # Let us look at any number of clusters from 1 to 10
  # We use the convenience function compute_mallows_mixtures
  n_clusters &lt;- seq(from = 1, to = 10)
  models &lt;- compute_mallows_mixtures(
    n_clusters = n_clusters,
    data = setup_rank_data(rankings = sushi_rankings),
    compute_options = set_compute_options(
      nmc = 6000, alpha_jump = 10, include_wcd = TRUE)
    )
  # models is a list in which each element is an object of class BayesMallows,
  # returned from compute_mallows
  # We can create an elbow plot
  burnin(models) &lt;- 1000
  plot_elbow(models)
  # We then select the number of cluster at a point where this plot has
  # an "elbow", e.g., at 6 clusters.

## End(Not run)

# SPEEDING UP COMPUTION WITH OBSERVATION FREQUENCIES With a large number of
# assessors taking on a relatively low number of unique rankings, the
# observation_frequency argument allows providing a rankings matrix with the
# unique set of rankings, and the observation_frequency vector giving the number
# of assessors with each ranking. This is illustrated here for the potato_visual
# dataset
#
# assume each row of potato_visual corresponds to between 1 and 5 assessors, as
# given by the observation_frequency vector
## Not run: 
  set.seed(1234)
  observation_frequency &lt;- sample.int(n = 5, size = nrow(potato_visual), replace = TRUE)
  m &lt;- compute_mallows(
    setup_rank_data(rankings = potato_visual, observation_frequency = observation_frequency))

  # INTRANSITIVE PAIRWISE PREFERENCES
  set.seed(1234)
  mod &lt;- compute_mallows(
    setup_rank_data(preferences = bernoulli_data),
    compute_options = set_compute_options(nmc = 5000),
    priors = set_priors(kappa = c(1, 10)),
    model_options = set_model_options(error_model = "bernoulli")
  )

  assess_convergence(mod)
  assess_convergence(mod, parameter = "theta")
  burnin(mod) &lt;- 3000

  plot(mod)
  plot(mod, parameter = "theta")

## End(Not run)
# CHEKING FOR LABEL SWITCHING
## Not run: 
  # This example shows how to assess if label switching happens in BayesMallows
  # We start by creating a directory in which csv files with individual
  # cluster probabilities should be saved in each step of the MCMC algorithm
  # NOTE: For computational efficiency, we use much fewer MCMC iterations than one
  # would normally do.
  dir.create("./test_label_switch")
  # Next, we go into this directory
  setwd("./test_label_switch/")
  # For comparison, we run compute_mallows with and without saving the cluster
  # probabilities The purpose of this is to assess the time it takes to save
  # the cluster probabilites.
  system.time(m &lt;- compute_mallows(
    setup_rank_data(rankings = sushi_rankings),
    model_options = set_model_options(n_clusters = 3),
    compute_options = set_compute_options(nmc = 500, save_ind_clus = FALSE),
    verbose = TRUE))
  # With this options, compute_mallows will save cluster_probs2.csv,
  # cluster_probs3.csv, ..., cluster_probs[nmc].csv.
  system.time(m &lt;- compute_mallows(
    setup_rank_data(rankings = sushi_rankings),
    model_options = set_model_options(n_clusters = 3),
    compute_options = set_compute_options(nmc = 500, save_ind_clus = TRUE),
    verbose = TRUE))

  # Next, we check convergence of alpha
  assess_convergence(m)

  # We set the burnin to 200
  burnin &lt;- 200

  # Find all files that were saved. Note that the first file saved is
  # cluster_probs2.csv
  cluster_files &lt;- list.files(pattern = "cluster\\_probs[[:digit:]]+\\.csv")

  # Check the size of the files that were saved.
  paste(sum(do.call(file.size, list(cluster_files))) * 1e-6, "MB")

  # Find the iteration each file corresponds to, by extracting its number
  iteration_number &lt;- as.integer(
    regmatches(x = cluster_files,m = regexpr(pattern = "[0-9]+", cluster_files)
               ))
  # Remove all files before burnin
  file.remove(cluster_files[iteration_number &lt;= burnin])
  # Update the vector of files, after the deletion
  cluster_files &lt;- list.files(pattern = "cluster\\_probs[[:digit:]]+\\.csv")
  # Create 3d array, with dimensions (iterations, assessors, clusters)
  prob_array &lt;- array(
    dim = c(length(cluster_files), m$data$n_assessors, m$n_clusters))
  # Read each file, adding to the right element of the array
  for(i in seq_along(cluster_files)){
    prob_array[i, , ] &lt;- as.matrix(
      read.csv(cluster_files[[i]], header = FALSE))
  }

  # Create an integer array of latent allocations, as this is required by
  # label.switching
  z &lt;- subset(m$cluster_assignment, iteration &gt; burnin)
  z$value &lt;- as.integer(gsub("Cluster ", "", z$value))
  z$chain &lt;- NULL
  z &lt;- reshape(z, direction = "wide", idvar = "iteration", timevar = "assessor")
  z$iteration &lt;- NULL
  z &lt;- as.matrix(z)

  # Now apply Stephen's algorithm
  library(label.switching)
  switch_check &lt;- label.switching("STEPHENS", z = z,
                                  K = m$n_clusters, p = prob_array)

  # Check the proportion of cluster assignments that were switched
  mean(apply(switch_check$permutations$STEPHENS, 1, function(x) {
    !all(x == seq(1, m$n_clusters, by = 1))
  }))

  # Remove the rest of the csv files
  file.remove(cluster_files)
  # Move up one directory
  setwd("..")
  # Remove the directory in which the csv files were saved
  file.remove("./test_label_switch/")

## End(Not run)
</code></pre>

<hr>
<h2 id='compute_mallows_mixtures'>Compute Mixtures of Mallows Models</h2><span id='topic+compute_mallows_mixtures'></span>

<h3>Description</h3>

<p>Convenience function for computing Mallows models with varying numbers of
mixtures. This is useful for deciding the number of mixtures to use in the
final model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_mallows_mixtures(
  n_clusters,
  data,
  model_options = set_model_options(),
  compute_options = set_compute_options(),
  priors = set_priors(),
  initial_values = set_initial_values(),
  pfun_estimate = NULL,
  verbose = FALSE,
  cl = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_mallows_mixtures_+3A_n_clusters">n_clusters</code></td>
<td>
<p>Integer vector specifying the number of clusters to use.</p>
</td></tr>
<tr><td><code id="compute_mallows_mixtures_+3A_data">data</code></td>
<td>
<p>An object of class &quot;BayesMallowsData&quot; returned from
<code><a href="#topic+setup_rank_data">setup_rank_data()</a></code>.</p>
</td></tr>
<tr><td><code id="compute_mallows_mixtures_+3A_model_options">model_options</code></td>
<td>
<p>An object of class &quot;BayesMallowsModelOptions&quot; returned
from <code><a href="#topic+set_model_options">set_model_options()</a></code>.</p>
</td></tr>
<tr><td><code id="compute_mallows_mixtures_+3A_compute_options">compute_options</code></td>
<td>
<p>An object of class &quot;BayesMallowsComputeOptions&quot;
returned from <code><a href="#topic+set_compute_options">set_compute_options()</a></code>.</p>
</td></tr>
<tr><td><code id="compute_mallows_mixtures_+3A_priors">priors</code></td>
<td>
<p>An object of class &quot;BayesMallowsPriors&quot; returned from
<code><a href="#topic+set_priors">set_priors()</a></code>.</p>
</td></tr>
<tr><td><code id="compute_mallows_mixtures_+3A_initial_values">initial_values</code></td>
<td>
<p>An object of class &quot;BayesMallowsInitialValues&quot; returned
from <code><a href="#topic+set_initial_values">set_initial_values()</a></code>.</p>
</td></tr>
<tr><td><code id="compute_mallows_mixtures_+3A_pfun_estimate">pfun_estimate</code></td>
<td>
<p>Object returned from <code><a href="#topic+estimate_partition_function">estimate_partition_function()</a></code>.
Defaults to <code>NULL</code>, and will only be used for footrule, Spearman, or
Ulam distances when the cardinalities are not available, cf.
<code><a href="#topic+get_cardinalities">get_cardinalities()</a></code>.</p>
</td></tr>
<tr><td><code id="compute_mallows_mixtures_+3A_verbose">verbose</code></td>
<td>
<p>Logical specifying whether to print out the progress of the
Metropolis-Hastings algorithm. If <code>TRUE</code>, a notification is printed
every 1000th iteration. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="compute_mallows_mixtures_+3A_cl">cl</code></td>
<td>
<p>Optional cluster returned from <code><a href="parallel.html#topic+makeCluster">parallel::makeCluster()</a></code>. If
provided, chains will be run in parallel, one on each node of <code>cl</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>n_clusters</code> argument to <code><a href="#topic+set_model_options">set_model_options()</a></code> is ignored
when calling <code>compute_mallows_mixtures</code>.
</p>


<h3>Value</h3>

<p>A list of Mallows models of class <code>BayesMallowsMixtures</code>, with
one element for each number of mixtures that was computed. This object can
be studied with <code><a href="#topic+plot_elbow">plot_elbow()</a></code>.
</p>


<h3>See Also</h3>

<p>Other modeling: 
<code><a href="#topic+burnin">burnin</a>()</code>,
<code><a href="#topic+burnin+3C-">burnin&lt;-</a>()</code>,
<code><a href="#topic+compute_mallows">compute_mallows</a>()</code>,
<code><a href="#topic+compute_mallows_sequentially">compute_mallows_sequentially</a>()</code>,
<code><a href="#topic+sample_prior">sample_prior</a>()</code>,
<code><a href="#topic+update_mallows">update_mallows</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># SIMULATED CLUSTER DATA
set.seed(1)
n_clusters &lt;- seq(from = 1, to = 5)
models &lt;- compute_mallows_mixtures(
  n_clusters = n_clusters, data = setup_rank_data(cluster_data),
  compute_options = set_compute_options(nmc = 2000, include_wcd = TRUE))

# There is good convergence for 1, 2, and 3 cluster, but not for 5.
# Also note that there seems to be label switching around the 7000th iteration
# for the 2-cluster solution.
assess_convergence(models)
# We can create an elbow plot, suggesting that there are three clusters, exactly
# as simulated.
burnin(models) &lt;- 1000
plot_elbow(models)

# We now fit a model with three clusters
mixture_model &lt;- compute_mallows(
  data = setup_rank_data(cluster_data),
  model_options = set_model_options(n_clusters = 3),
  compute_options = set_compute_options(nmc = 2000))

# The trace plot for this model looks good. It seems to converge quickly.
assess_convergence(mixture_model)
# We set the burnin to 500
burnin(mixture_model) &lt;- 500

# We can now look at posterior quantities
# Posterior of scale parameter alpha
plot(mixture_model)
plot(mixture_model, parameter = "rho", items = 4:5)
# There is around 33 % probability of being in each cluster, in agreemeent
# with the data simulating mechanism
plot(mixture_model, parameter = "cluster_probs")
# We can also look at a cluster assignment plot
plot(mixture_model, parameter = "cluster_assignment")

# DETERMINING THE NUMBER OF CLUSTERS IN THE SUSHI EXAMPLE DATA
## Not run: 
  # Let us look at any number of clusters from 1 to 10
  # We use the convenience function compute_mallows_mixtures
  n_clusters &lt;- seq(from = 1, to = 10)
  models &lt;- compute_mallows_mixtures(
    n_clusters = n_clusters, data = setup_rank_data(sushi_rankings),
    compute_options = set_compute_options(include_wcd = TRUE))
  # models is a list in which each element is an object of class BayesMallows,
  # returned from compute_mallows
  # We can create an elbow plot
  burnin(models) &lt;- 1000
  plot_elbow(models)
  # We then select the number of cluster at a point where this plot has
  # an "elbow", e.g., n_clusters = 5.

  # Having chosen the number of clusters, we can now study the final model
  # Rerun with 5 clusters
  mixture_model &lt;- compute_mallows(
    rankings = sushi_rankings,
    model_options = set_model_options(n_clusters = 5),
    compute_options = set_compute_options(include_wcd = TRUE))
  # Delete the models object to free some memory
  rm(models)
  # Set the burnin
  burnin(mixture_model) &lt;- 1000
  # Plot the posterior distributions of alpha per cluster
  plot(mixture_model)
  # Compute the posterior interval of alpha per cluster
  compute_posterior_intervals(mixture_model, parameter = "alpha")
  # Plot the posterior distributions of cluster probabilities
  plot(mixture_model, parameter = "cluster_probs")
  # Plot the posterior probability of cluster assignment
  plot(mixture_model, parameter = "cluster_assignment")
  # Plot the posterior distribution of "tuna roll" in each cluster
  plot(mixture_model, parameter = "rho", items = "tuna roll")
  # Compute the cluster-wise CP consensus, and show one column per cluster
  cp &lt;- compute_consensus(mixture_model, type = "CP")
  cp$cumprob &lt;- NULL
  stats::reshape(cp, direction = "wide", idvar = "ranking",
                 timevar = "cluster", varying = list(as.character(unique(cp$cluster))))

  # Compute the MAP consensus, and show one column per cluster
  map &lt;- compute_consensus(mixture_model, type = "MAP")
  map$probability &lt;- NULL
  stats::reshape(map, direction = "wide", idvar = "map_ranking",
                 timevar = "cluster", varying = list(as.character(unique(map$cluster))))

  # RUNNING IN PARALLEL
  # Computing Mallows models with different number of mixtures in parallel leads to
  # considerably speedup
  library(parallel)
  cl &lt;- makeCluster(detectCores() - 1)
  n_clusters &lt;- seq(from = 1, to = 10)
  models &lt;- compute_mallows_mixtures(
    n_clusters = n_clusters,
    rankings = sushi_rankings,
    compute_options = set_compute_options(include_wcd = TRUE),
    cl = cl)
  stopCluster(cl)

## End(Not run)



</code></pre>

<hr>
<h2 id='compute_mallows_sequentially'>Estimate the Bayesian Mallows Model Sequentially</h2><span id='topic+compute_mallows_sequentially'></span>

<h3>Description</h3>

<p>Compute the posterior distributions of the parameters of the
Bayesian Mallows model using sequential Monte Carlo. This is based on the
algorithms developed in
Stein (2023).
This function differs from <code><a href="#topic+update_mallows">update_mallows()</a></code> in that it takes all the data
at once, and uses SMC to fit the model step-by-step. Used in this way, SMC
is an alternative to Metropolis-Hastings, which may work better in some
settings. In addition, it allows visualization of the learning process.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_mallows_sequentially(
  data,
  initial_values,
  model_options = set_model_options(),
  smc_options = set_smc_options(),
  compute_options = set_compute_options(),
  priors = set_priors(),
  pfun_estimate = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_mallows_sequentially_+3A_data">data</code></td>
<td>
<p>A list of objects of class &quot;BayesMallowsData&quot; returned from
<code><a href="#topic+setup_rank_data">setup_rank_data()</a></code>. Each list element is interpreted as the data belonging
to a given timepoint.</p>
</td></tr>
<tr><td><code id="compute_mallows_sequentially_+3A_initial_values">initial_values</code></td>
<td>
<p>An object of class &quot;BayesMallowsPriorSamples&quot; returned
from <code><a href="#topic+sample_prior">sample_prior()</a></code>.</p>
</td></tr>
<tr><td><code id="compute_mallows_sequentially_+3A_model_options">model_options</code></td>
<td>
<p>An object of class &quot;BayesMallowsModelOptions&quot; returned
from <code><a href="#topic+set_model_options">set_model_options()</a></code>.</p>
</td></tr>
<tr><td><code id="compute_mallows_sequentially_+3A_smc_options">smc_options</code></td>
<td>
<p>An object of class &quot;SMCOptions&quot; returned from
<code><a href="#topic+set_smc_options">set_smc_options()</a></code>.</p>
</td></tr>
<tr><td><code id="compute_mallows_sequentially_+3A_compute_options">compute_options</code></td>
<td>
<p>An object of class &quot;BayesMallowsComputeOptions&quot;
returned from <code><a href="#topic+set_compute_options">set_compute_options()</a></code>.</p>
</td></tr>
<tr><td><code id="compute_mallows_sequentially_+3A_priors">priors</code></td>
<td>
<p>An object of class &quot;BayesMallowsPriors&quot; returned from
<code><a href="#topic+set_priors">set_priors()</a></code>.</p>
</td></tr>
<tr><td><code id="compute_mallows_sequentially_+3A_pfun_estimate">pfun_estimate</code></td>
<td>
<p>Object returned from <code><a href="#topic+estimate_partition_function">estimate_partition_function()</a></code>.
Defaults to <code>NULL</code>, and will only be used for footrule, Spearman, or
Ulam distances when the cardinalities are not available, cf.
<code><a href="#topic+get_cardinalities">get_cardinalities()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is very new, and plotting functions and other tools
for visualizing the posterior distribution do not yet work. See the examples
for some workarounds.
</p>


<h3>Value</h3>

<p>An object of class BayesMallowsSequential.
</p>


<h3>References</h3>

<p>Stein A (2023).
<em>Sequential Inference with the Mallows Model</em>.
Ph.D. thesis, Lancaster University.
</p>


<h3>See Also</h3>

<p>Other modeling: 
<code><a href="#topic+burnin">burnin</a>()</code>,
<code><a href="#topic+burnin+3C-">burnin&lt;-</a>()</code>,
<code><a href="#topic+compute_mallows">compute_mallows</a>()</code>,
<code><a href="#topic+compute_mallows_mixtures">compute_mallows_mixtures</a>()</code>,
<code><a href="#topic+sample_prior">sample_prior</a>()</code>,
<code><a href="#topic+update_mallows">update_mallows</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Observe one ranking at each of 12 timepoints
library(ggplot2)
data &lt;- lapply(seq_len(nrow(potato_visual)), function(i) {
  setup_rank_data(potato_visual[i, ], user_ids = i)
})

initial_values &lt;- sample_prior(
  n = 200, n_items = 20,
  priors = set_priors(gamma = 3, lambda = .1))

mod &lt;- compute_mallows_sequentially(
  data = data,
  initial_values = initial_values,
  smc_options = set_smc_options(n_particles = 500, mcmc_steps = 20))

# We can see the acceptance ratio of the move step for each timepoint:
get_acceptance_ratios(mod)

plot_dat &lt;- data.frame(
  n_obs = seq_along(data),
  alpha_mean = apply(mod$alpha_samples, 2, mean),
  alpha_sd = apply(mod$alpha_samples, 2, sd)
)

# Visualize how the dispersion parameter is being learned as more data arrive
ggplot(plot_dat, aes(x = n_obs, y = alpha_mean, ymin = alpha_mean - alpha_sd,
                     ymax = alpha_mean + alpha_sd)) +
  geom_line() +
  geom_ribbon(alpha = .1) +
  ylab(expression(alpha)) +
  xlab("Observations") +
  theme_classic() +
  scale_x_continuous(
    breaks = seq(min(plot_dat$n_obs), max(plot_dat$n_obs), by = 1))

# Visualize the learning of the rank for a given item (item 1 in this example)
plot_dat &lt;- data.frame(
  n_obs = seq_along(data),
  rank_mean = apply(mod$rho_samples[1, , ], 2, mean),
  rank_sd = apply(mod$rho_samples[1, , ], 2, sd)
)

ggplot(plot_dat, aes(x = n_obs, y = rank_mean, ymin = rank_mean - rank_sd,
                     ymax = rank_mean + rank_sd)) +
  geom_line() +
  geom_ribbon(alpha = .1) +
  xlab("Observations") +
  ylab(expression(rho[1])) +
  theme_classic() +
  scale_x_continuous(
    breaks = seq(min(plot_dat$n_obs), max(plot_dat$n_obs), by = 1))
</code></pre>

<hr>
<h2 id='compute_observation_frequency'>Frequency distribution of the ranking sequences</h2><span id='topic+compute_observation_frequency'></span>

<h3>Description</h3>

<p>Construct the frequency distribution of the distinct ranking
sequences from the dataset of the individual rankings. This can be of
interest in itself, but also used to speed up computation by providing
the <code>observation_frequency</code> argument to <code><a href="#topic+compute_mallows">compute_mallows()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_observation_frequency(rankings)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_observation_frequency_+3A_rankings">rankings</code></td>
<td>
<p>A matrix with the individual rankings in each row.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric matrix with the distinct rankings in each row and the
corresponding frequencies indicated in the last <code>(n_items+1)</code>-th
column.
</p>


<h3>See Also</h3>

<p>Other rank functions: 
<code><a href="#topic+compute_expected_distance">compute_expected_distance</a>()</code>,
<code><a href="#topic+compute_rank_distance">compute_rank_distance</a>()</code>,
<code><a href="#topic+create_ranking">create_ranking</a>()</code>,
<code><a href="#topic+get_mallows_loglik">get_mallows_loglik</a>()</code>,
<code><a href="#topic+sample_mallows">sample_mallows</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create example data. We set the burn-in and thinning very low
# for the sampling to go fast
data0 &lt;- sample_mallows(rho0 = 1:5, alpha = 10, n_samples = 1000,
                        burnin = 10, thinning = 1)
# Find the frequency distribution
compute_observation_frequency(rankings = data0)

# The function also works when the data have missing values
rankings &lt;- matrix(c(1, 2, 3, 4,
                     1, 2, 4, NA,
                     1, 2, 4, NA,
                     3, 2, 1, 4,
                     NA, NA, 2, 1,
                     NA, NA, 2, 1,
                     NA, NA, 2, 1,
                     2, NA, 1, NA), ncol = 4, byrow = TRUE)

compute_observation_frequency(rankings)
</code></pre>

<hr>
<h2 id='compute_posterior_intervals'>Compute Posterior Intervals</h2><span id='topic+compute_posterior_intervals'></span><span id='topic+compute_posterior_intervals.BayesMallows'></span><span id='topic+compute_posterior_intervals.SMCMallows'></span>

<h3>Description</h3>

<p>Compute posterior intervals of parameters of interest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_posterior_intervals(model_fit, ...)

## S3 method for class 'BayesMallows'
compute_posterior_intervals(
  model_fit,
  parameter = c("alpha", "rho", "cluster_probs"),
  level = 0.95,
  decimals = 3L,
  ...
)

## S3 method for class 'SMCMallows'
compute_posterior_intervals(
  model_fit,
  parameter = c("alpha", "rho"),
  level = 0.95,
  decimals = 3L,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_posterior_intervals_+3A_model_fit">model_fit</code></td>
<td>
<p>A model object.</p>
</td></tr>
<tr><td><code id="compute_posterior_intervals_+3A_...">...</code></td>
<td>
<p>Other arguments. Currently not used.</p>
</td></tr>
<tr><td><code id="compute_posterior_intervals_+3A_parameter">parameter</code></td>
<td>
<p>Character string defining which parameter to compute
posterior intervals for. One of <code>"alpha"</code>, <code>"rho"</code>, or
<code>"cluster_probs"</code>. Default is <code>"alpha"</code>.</p>
</td></tr>
<tr><td><code id="compute_posterior_intervals_+3A_level">level</code></td>
<td>
<p>Decimal number in <code class="reqn">[0,1]</code> specifying the confidence level.
Defaults to <code>0.95</code>.</p>
</td></tr>
<tr><td><code id="compute_posterior_intervals_+3A_decimals">decimals</code></td>
<td>
<p>Integer specifying the number of decimals to include in
posterior intervals and the mean and median. Defaults to <code>3</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes both the Highest Posterior Density Interval (HPDI),
which may be discontinuous for bimodal distributions, and
the central posterior interval, which is simply defined by the quantiles of the posterior
distribution.
</p>


<h3>References</h3>

<p>There are no references for Rd macro <code style="white-space: pre;">&#8288;\insertAllCites&#8288;</code> on this help page.
</p>


<h3>See Also</h3>

<p>Other posterior quantities: 
<code><a href="#topic+assign_cluster">assign_cluster</a>()</code>,
<code><a href="#topic+compute_consensus">compute_consensus</a>()</code>,
<code><a href="#topic+get_acceptance_ratios">get_acceptance_ratios</a>()</code>,
<code><a href="#topic+heat_plot">heat_plot</a>()</code>,
<code><a href="#topic+plot.BayesMallows">plot.BayesMallows</a>()</code>,
<code><a href="#topic+plot.SMCMallows">plot.SMCMallows</a>()</code>,
<code><a href="#topic+plot_elbow">plot_elbow</a>()</code>,
<code><a href="#topic+plot_top_k">plot_top_k</a>()</code>,
<code><a href="#topic+predict_top_k">predict_top_k</a>()</code>,
<code><a href="#topic+print.BayesMallows">print.BayesMallows</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
model_fit &lt;- compute_mallows(
  setup_rank_data(potato_visual),
  compute_options = set_compute_options(nmc = 3000, burnin = 1000))

# First we compute the interval for alpha
compute_posterior_intervals(model_fit, parameter = "alpha")
# We can reduce the number decimals
compute_posterior_intervals(model_fit, parameter = "alpha", decimals = 2)
# By default, we get a 95 % interval. We can change that to 99 %.
compute_posterior_intervals(model_fit, parameter = "alpha", level = 0.99)
# We can also compute the posterior interval for the latent ranks rho
compute_posterior_intervals(model_fit, parameter = "rho")

## Not run: 
  # Posterior intervals of cluster probabilities
  model_fit &lt;- compute_mallows(
    setup_rank_data(sushi_rankings),
    model_options = set_model_options(n_clusters = 5))
  burnin(model_fit) &lt;- 1000

  compute_posterior_intervals(model_fit, parameter = "alpha")

  compute_posterior_intervals(model_fit, parameter = "cluster_probs")

## End(Not run)


</code></pre>

<hr>
<h2 id='compute_rank_distance'>Distance between a set of rankings and a given rank sequence</h2><span id='topic+compute_rank_distance'></span>

<h3>Description</h3>

<p>Compute the distance between a matrix of rankings and a rank
sequence.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute_rank_distance(
  rankings,
  rho,
  metric = c("footrule", "spearman", "cayley", "hamming", "kendall", "ulam"),
  observation_frequency = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_rank_distance_+3A_rankings">rankings</code></td>
<td>
<p>A matrix of size <code class="reqn">N \times n_{items}</code> of
rankings in each row. Alternatively, if <code class="reqn">N</code> equals 1, <code>rankings</code>
can be a vector.</p>
</td></tr>
<tr><td><code id="compute_rank_distance_+3A_rho">rho</code></td>
<td>
<p>A ranking sequence.</p>
</td></tr>
<tr><td><code id="compute_rank_distance_+3A_metric">metric</code></td>
<td>
<p>Character string specifying the distance measure to use.
Available options are <code>"kendall"</code>, <code>"cayley"</code>, <code>"hamming"</code>,
<code>"ulam"</code>, <code>"footrule"</code> and <code>"spearman"</code>.</p>
</td></tr>
<tr><td><code id="compute_rank_distance_+3A_observation_frequency">observation_frequency</code></td>
<td>
<p>Vector of observation frequencies of length <code class="reqn">N</code>, or of length 1,
which means that all ranks are given the same weight. Defaults to 1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The implementation of Cayley distance is based on a <code style="white-space: pre;">&#8288;C++&#8288;</code>
translation of <code>Rankcluster::distCayley()</code> (Grimonprez and Jacques 2016).
</p>


<h3>Value</h3>

<p>A vector of distances according to the given <code>metric</code>.
</p>


<h3>References</h3>

<p>Grimonprez Q, Jacques J (2016).
<em>Rankcluster: Model-Based Clustering for Multivariate Partial Ranking Data</em>.
R package version 0.94, <a href="https://CRAN.R-project.org/package=Rankcluster">https://CRAN.R-project.org/package=Rankcluster</a>.
</p>


<h3>See Also</h3>

<p>Other rank functions: 
<code><a href="#topic+compute_expected_distance">compute_expected_distance</a>()</code>,
<code><a href="#topic+compute_observation_frequency">compute_observation_frequency</a>()</code>,
<code><a href="#topic+create_ranking">create_ranking</a>()</code>,
<code><a href="#topic+get_mallows_loglik">get_mallows_loglik</a>()</code>,
<code><a href="#topic+sample_mallows">sample_mallows</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Distance between two vectors of rankings:
compute_rank_distance(1:5, 5:1, metric = "kendall")
compute_rank_distance(c(2, 4, 3, 6, 1, 7, 5), c(3, 5, 4, 7, 6, 2, 1), metric = "cayley")
compute_rank_distance(c(4, 2, 3, 1), c(3, 4, 1, 2), metric = "hamming")
compute_rank_distance(c(1, 3, 5, 7, 9, 8, 6, 4, 2), c(1, 2, 3, 4, 9, 8, 7, 6, 5), "ulam")
compute_rank_distance(c(8, 7, 1, 2, 6, 5, 3, 4), c(1, 2, 8, 7, 3, 4, 6, 5), "footrule")
compute_rank_distance(c(1, 6, 2, 5, 3, 4), c(4, 3, 5, 2, 6, 1), "spearman")

# Difference between a metric and a vector
# We set the burn-in and thinning too low for the example to run fast
data0 &lt;- sample_mallows(rho0 = 1:10, alpha = 20, n_samples = 1000,
                        burnin = 10, thinning = 1)

compute_rank_distance(rankings = data0, rho = 1:10, metric = "kendall")
</code></pre>

<hr>
<h2 id='create_ranking'>Convert between ranking and ordering.</h2><span id='topic+create_ranking'></span><span id='topic+create_ordering'></span>

<h3>Description</h3>

<p><code>create_ranking</code> takes a vector or matrix of ordered items <code>orderings</code> and
returns a corresponding vector or matrix of ranked items.
<code>create_ordering</code> takes a vector or matrix of rankings <code>rankings</code> and
returns a corresponding vector or matrix of ordered items.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_ranking(orderings)

create_ordering(rankings)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_ranking_+3A_orderings">orderings</code></td>
<td>
<p>A vector or matrix of ordered items. If a matrix, it should be of
size N times n, where N is the number of samples and n is the number of
items.</p>
</td></tr>
<tr><td><code id="create_ranking_+3A_rankings">rankings</code></td>
<td>
<p>A vector or matrix of ranked items. If a matrix, it should be N
times n, where N is the number of samples and n is the number of items.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector or matrix of rankings. Missing orderings coded as <code>NA</code> are propagated into corresponding missing ranks and vice versa.
</p>


<h3>See Also</h3>

<p>Other rank functions: 
<code><a href="#topic+compute_expected_distance">compute_expected_distance</a>()</code>,
<code><a href="#topic+compute_observation_frequency">compute_observation_frequency</a>()</code>,
<code><a href="#topic+compute_rank_distance">compute_rank_distance</a>()</code>,
<code><a href="#topic+get_mallows_loglik">get_mallows_loglik</a>()</code>,
<code><a href="#topic+sample_mallows">sample_mallows</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># A vector of ordered items.
orderings &lt;- c(5, 1, 2, 4, 3)
# Get ranks
rankings &lt;- create_ranking(orderings)
# rankings is c(2, 3, 5, 4, 1)
# Finally we convert it backed to an ordering.
orderings_2 &lt;- create_ordering(rankings)
# Confirm that we get back what we had
all.equal(orderings, orderings_2)

# Next, we have a matrix with N = 19 samples
# and n = 4 items
set.seed(21)
N &lt;- 10
n &lt;- 4
orderings &lt;- t(replicate(N, sample.int(n)))
# Convert the ordering to ranking
rankings &lt;- create_ranking(orderings)
# Now we try to convert it back to an ordering.
orderings_2 &lt;- create_ordering(rankings)
# Confirm that we get back what we had
all.equal(orderings, orderings_2)
</code></pre>

<hr>
<h2 id='estimate_partition_function'>Estimate Partition Function</h2><span id='topic+estimate_partition_function'></span>

<h3>Description</h3>

<p>Estimate the logarithm of the partition function of the Mallows rank model.
Choose between the importance sampling algorithm described in
(Vitelli et al. 2018) and the IPFP algorithm for computing
an asymptotic approximation described in
(Mukherjee 2016). Note that exact partition functions
can be computed efficiently for Cayley, Hamming and Kendall distances with
any number of items, for footrule distances with up to 50 items, Spearman
distance with up to 20 items, and Ulam distance with up to 60 items. This
function is thus intended for the complement of these cases. See
<code><a href="#topic+get_cardinalities">get_cardinalities()</a></code> for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimate_partition_function(
  method = c("importance_sampling", "asymptotic"),
  alpha_vector,
  n_items,
  metric,
  n_iterations,
  K = 20,
  cl = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate_partition_function_+3A_method">method</code></td>
<td>
<p>Character string specifying the method to use in order to
estimate the logarithm of the partition function. Available options are
<code>"importance_sampling"</code> and <code>"asymptotic"</code>.</p>
</td></tr>
<tr><td><code id="estimate_partition_function_+3A_alpha_vector">alpha_vector</code></td>
<td>
<p>Numeric vector of <code class="reqn">\alpha</code> values over which to
compute the importance sampling estimate.</p>
</td></tr>
<tr><td><code id="estimate_partition_function_+3A_n_items">n_items</code></td>
<td>
<p>Integer specifying the number of items.</p>
</td></tr>
<tr><td><code id="estimate_partition_function_+3A_metric">metric</code></td>
<td>
<p>Character string specifying the distance measure to use.
Available options are <code>"footrule"</code> and <code>"spearman"</code> when <code>method = "asymptotic"</code> and in addition <code>"cayley"</code>, <code>"hamming"</code>, <code>"kendall"</code>, and
<code>"ulam"</code> when <code>method = "importance_sampling"</code>.</p>
</td></tr>
<tr><td><code id="estimate_partition_function_+3A_n_iterations">n_iterations</code></td>
<td>
<p>Integer specifying the number of iterations to use. When
<code>method = "importance_sampling"</code>, this is the number of Monte Carlo samples
to generate. When <code>method = "asymptotic"</code>, on the other hand, it represents
the number of iterations of the IPFP algorithm.</p>
</td></tr>
<tr><td><code id="estimate_partition_function_+3A_k">K</code></td>
<td>
<p>Integer specifying the parameter <code class="reqn">K</code> in the asymptotic
approximation of the partition function. Only used when <code>method = "asymptotic"</code>. Defaults to 20.</p>
</td></tr>
<tr><td><code id="estimate_partition_function_+3A_cl">cl</code></td>
<td>
<p>Optional computing cluster used for parallelization, returned from
<code><a href="parallel.html#topic+makeCluster">parallel::makeCluster()</a></code>. Defaults to <code>NULL</code>. Only used when <code>method = "importance_sampling"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix with two column and number of rows equal the degree of the
fitted polynomial approximating the partition function. The matrix can be
supplied to the <code>pfun_estimate</code> argument of <code><a href="#topic+compute_mallows">compute_mallows()</a></code>.
</p>


<h3>References</h3>

<p>Mukherjee S (2016).
&ldquo;Estimation in exponential families on permutations.&rdquo;
<em>The Annals of Statistics</em>, <b>44</b>(2), 853&ndash;875.
<a href="https://doi.org/10.1214/15-aos1389">doi:10.1214/15-aos1389</a>.<br /><br /> Vitelli V, Sørensen, Crispino M, Arjas E, Frigessi A (2018).
&ldquo;Probabilistic Preference Learning with the Mallows Rank Model.&rdquo;
<em>Journal of Machine Learning Research</em>, <b>18</b>(1), 1&ndash;49.
<a href="https://jmlr.org/papers/v18/15-481.html">https://jmlr.org/papers/v18/15-481.html</a>.
</p>


<h3>See Also</h3>

<p>Other partition function: 
<code><a href="#topic+get_cardinalities">get_cardinalities</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># IMPORTANCE SAMPLING
# Let us estimate logZ(alpha) for 20 items with Spearman distance
# We create a grid of alpha values from 0 to 10
alpha_vector &lt;- seq(from = 0, to = 10, by = 0.5)
n_items &lt;- 20
metric &lt;- "spearman"

# We start with 1e3 Monte Carlo samples
fit1 &lt;- estimate_partition_function(
  method = "importance_sampling", alpha_vector = alpha_vector,
  n_items = n_items, metric = metric, n_iterations = 1e3)
# A matrix containing powers of alpha and regression coefficients is returned
fit1
# The approximated partition function can hence be obtained:
estimate1 &lt;-
  vapply(alpha_vector, function(a) sum(a^fit1[, 1] * fit1[, 2]), numeric(1))

# Now let us recompute with 2e3 Monte Carlo samples
fit2 &lt;- estimate_partition_function(
  method = "importance_sampling", alpha_vector = alpha_vector,
  n_items = n_items, metric = metric, n_iterations = 2e3)
estimate2 &lt;-
  vapply(alpha_vector, function(a) sum(a^fit2[, 1] * fit2[, 2]), numeric(1))

# ASYMPTOTIC APPROXIMATION
# We can also compute an estimate using the asymptotic approximation
fit3 &lt;- estimate_partition_function(
  method = "asymptotic", alpha_vector = alpha_vector,
  n_items = n_items, metric = metric, n_iterations = 50)
estimate3 &lt;-
  vapply(alpha_vector, function(a) sum(a^fit3[, 1] * fit3[, 2]), numeric(1))

# We can now plot the estimates side-by-side
plot(alpha_vector, estimate1, type = "l", xlab = expression(alpha),
     ylab = expression(log(Z(alpha))))
lines(alpha_vector, estimate2, col = 2)
lines(alpha_vector, estimate3, col = 3)
legend(x = 7, y = 40, legend = c("IS,1e3", "IS,2e3", "IPFP"),
       col = 1:3, lty = 1)

# We see that the two importance sampling estimates, which are unbiased,
# overlap. The asymptotic approximation seems a bit off. It can be worthwhile
# to try different values of n_iterations and K.

# When we are happy, we can provide the coefficient vector in the
# pfun_estimate argument to compute_mallows
# Say we choose to use the importance sampling estimate with 1e4 Monte Carlo samples:
model_fit &lt;- compute_mallows(
  setup_rank_data(potato_visual),
  model_options = set_model_options(metric = "spearman"),
  compute_options = set_compute_options(nmc = 200),
  pfun_estimate = fit2)

</code></pre>

<hr>
<h2 id='get_acceptance_ratios'>Get Acceptance Ratios</h2><span id='topic+get_acceptance_ratios'></span><span id='topic+get_acceptance_ratios.BayesMallows'></span><span id='topic+get_acceptance_ratios.SMCMallows'></span>

<h3>Description</h3>

<p>Extract acceptance ratio from Metropolis-Hastings
algorithm used by <code><a href="#topic+compute_mallows">compute_mallows()</a></code> or by the move step in
<code><a href="#topic+update_mallows">update_mallows()</a></code> and <code><a href="#topic+compute_mallows_sequentially">compute_mallows_sequentially()</a></code>. Currently the
function only returns the values, but it will be refined in the future. If
burnin is not set in the call to <code><a href="#topic+compute_mallows">compute_mallows()</a></code>, the acceptance ratio
for all iterations will be reported. Otherwise the post burnin acceptance
ratio is reported. For the SMC method the acceptance ratios apply to all
iterations, since no burnin is needed in here.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_acceptance_ratios(model_fit, ...)

## S3 method for class 'BayesMallows'
get_acceptance_ratios(model_fit, ...)

## S3 method for class 'SMCMallows'
get_acceptance_ratios(model_fit, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_acceptance_ratios_+3A_model_fit">model_fit</code></td>
<td>
<p>A model fit.</p>
</td></tr>
<tr><td><code id="get_acceptance_ratios_+3A_...">...</code></td>
<td>
<p>Other arguments passed on to other methods. Currently not used.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other posterior quantities: 
<code><a href="#topic+assign_cluster">assign_cluster</a>()</code>,
<code><a href="#topic+compute_consensus">compute_consensus</a>()</code>,
<code><a href="#topic+compute_posterior_intervals">compute_posterior_intervals</a>()</code>,
<code><a href="#topic+heat_plot">heat_plot</a>()</code>,
<code><a href="#topic+plot.BayesMallows">plot.BayesMallows</a>()</code>,
<code><a href="#topic+plot.SMCMallows">plot.SMCMallows</a>()</code>,
<code><a href="#topic+plot_elbow">plot_elbow</a>()</code>,
<code><a href="#topic+plot_top_k">plot_top_k</a>()</code>,
<code><a href="#topic+predict_top_k">predict_top_k</a>()</code>,
<code><a href="#topic+print.BayesMallows">print.BayesMallows</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
mod &lt;- compute_mallows(
  data = setup_rank_data(potato_visual),
  compute_options = set_compute_options(burnin = 200)
)

get_acceptance_ratios(mod)
</code></pre>

<hr>
<h2 id='get_cardinalities'>Get cardinalities for each distance</h2><span id='topic+get_cardinalities'></span>

<h3>Description</h3>

<p>The partition function for the Mallows model can be defined in a
computationally efficient manner as
</p>
<p style="text-align: center;"><code class="reqn">Z_{n}(\alpha) = \sum_{d_{n} \in
  \mathcal{D}_{n}} N_{m,n} e^{-(\alpha/n) d_{m}}</code>
</p>
<p>.
In this equation, <code class="reqn">\mathcal{D}_{n}</code> a set containing all possible
distances at the given number of items, and <code class="reqn">d_{m}</code> is on element of
this set. Finally, <code class="reqn">N_{m,n}</code> is the number of possible configurations
of the items that give the particular distance. See
Irurozki et al. (2016),
Vitelli et al. (2018), and
Crispino et al. (2023) for details.
</p>
<p>For footrule distance, the cardinalities come from entry A062869 in the
On-Line Encyclopedia of Integer Sequences (OEIS)
(Sloane and Inc. 2020). For Spearman distance, they come from
entry A175929, and for Ulam distance from entry A126065.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_cardinalities(n_items, metric = c("footrule", "spearman", "ulam"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_cardinalities_+3A_n_items">n_items</code></td>
<td>
<p>Number of items.</p>
</td></tr>
<tr><td><code id="get_cardinalities_+3A_metric">metric</code></td>
<td>
<p>Distance function, one of &quot;footrule&quot;, &quot;spearman&quot;, or &quot;ulam&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataframe with two columns, <code>distance</code> which contains each distance
in the support set at the current number of items, i.e., <code class="reqn">d_{m}</code>, and
<code>value</code> which contains the number of values at this particular distances,
i.e., <code class="reqn">N_{m,n}</code>.
</p>


<h3>References</h3>

<p>Crispino M, Mollica C, Astuti V, Tardella L (2023).
&ldquo;Efficient and accurate inference for mixtures of Mallows models with Spearman distance.&rdquo;
<em>Statistics and Computing</em>, <b>33</b>(5).
ISSN 1573-1375, <a href="https://doi.org/10.1007/s11222-023-10266-8">doi:10.1007/s11222-023-10266-8</a>, <a href="http://dx.doi.org/10.1007/s11222-023-10266-8">http://dx.doi.org/10.1007/s11222-023-10266-8</a>.<br /><br /> Irurozki E, Calvo B, Lozano JA (2016).
&ldquo;PerMallows: An R Package for Mallows and Generalized Mallows Models.&rdquo;
<em>Journal of Statistical Software</em>, <b>71</b>(12), 1&ndash;30.
<a href="https://doi.org/10.18637/jss.v071.i12">doi:10.18637/jss.v071.i12</a>.<br /><br /> Sloane NJA, Inc. TOF (2020).
&ldquo;The on-line encyclopedia of integer sequences.&rdquo;
<a href="https://oeis.org/">https://oeis.org/</a>.<br /><br /> Vitelli V, Sørensen, Crispino M, Arjas E, Frigessi A (2018).
&ldquo;Probabilistic Preference Learning with the Mallows Rank Model.&rdquo;
<em>Journal of Machine Learning Research</em>, <b>18</b>(1), 1&ndash;49.
<a href="https://jmlr.org/papers/v18/15-481.html">https://jmlr.org/papers/v18/15-481.html</a>.
</p>


<h3>See Also</h3>

<p>Other partition function: 
<code><a href="#topic+estimate_partition_function">estimate_partition_function</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Extract the cardinalities for four items with footrule distance
n_items &lt;- 4
dat &lt;- get_cardinalities(n_items)
# Compute the partition function at alpha = 2
alpha &lt;- 2
sum(dat$value * exp(-alpha / n_items * dat$distance))
#'
# We can confirm that it is correct by enumerating all possible combinations
all &lt;- expand.grid(1:4, 1:4, 1:4, 1:4)
perms &lt;- all[apply(all, 1, function(x) length(unique(x)) == 4), ]
sum(apply(perms, 1, function(x) exp(-alpha / n_items * sum(abs(x - 1:4)))))

# We do the same for the Spearman distance
dat &lt;- get_cardinalities(n_items, metric = "spearman")
sum(dat$value * exp(-alpha / n_items * dat$distance))
#'
# We can confirm that it is correct by enumerating all possible combinations
sum(apply(perms, 1, function(x) exp(-alpha / n_items * sum((x - 1:4)^2))))
</code></pre>

<hr>
<h2 id='get_mallows_loglik'>Likelihood and log-likelihood evaluation for a Mallows mixture model</h2><span id='topic+get_mallows_loglik'></span>

<h3>Description</h3>

<p>Compute either the likelihood or the log-likelihood value of the
Mallows mixture model parameters for a dataset of complete rankings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_mallows_loglik(
  rho,
  alpha,
  weights,
  metric = c("footrule", "spearman", "cayley", "hamming", "kendall", "ulam"),
  rankings,
  observation_frequency = NULL,
  log = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_mallows_loglik_+3A_rho">rho</code></td>
<td>
<p>A matrix of size <code style="white-space: pre;">&#8288;n_clusters x n_items&#8288;</code> whose rows are
permutations of the first n_items integers corresponding to the modal
rankings of the Mallows mixture components.</p>
</td></tr>
<tr><td><code id="get_mallows_loglik_+3A_alpha">alpha</code></td>
<td>
<p>A vector of <code>n_clusters</code> non-negative scalar specifying the
scale (precision) parameters of the Mallows mixture components.</p>
</td></tr>
<tr><td><code id="get_mallows_loglik_+3A_weights">weights</code></td>
<td>
<p>A vector of <code>n_clusters</code> non-negative scalars specifying
the mixture weights.</p>
</td></tr>
<tr><td><code id="get_mallows_loglik_+3A_metric">metric</code></td>
<td>
<p>Character string specifying the distance measure to use.
Available options are <code>"kendall"</code>, <code>"cayley"</code>, <code>"hamming"</code>,
<code>"ulam"</code>, <code>"footrule"</code>, and <code>"spearman"</code>.</p>
</td></tr>
<tr><td><code id="get_mallows_loglik_+3A_rankings">rankings</code></td>
<td>
<p>A matrix with observed rankings in each row.</p>
</td></tr>
<tr><td><code id="get_mallows_loglik_+3A_observation_frequency">observation_frequency</code></td>
<td>
<p>A vector of observation frequencies (weights) to apply to
each row in <code>rankings</code>. This can speed up computation if a large
number of assessors share the same rank pattern. Defaults to <code>NULL</code>,
which means that each row of <code>rankings</code> is multiplied by 1. If
provided, <code>observation_frequency</code> must have the same number of elements as there
are rows in <code>rankings</code>, and <code>rankings</code> cannot be <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="get_mallows_loglik_+3A_log">log</code></td>
<td>
<p>A logical; if TRUE, the log-likelihood value is returned,
otherwise its exponential. Default is <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The likelihood or the log-likelihood value corresponding to one or
more observed complete rankings under the Mallows mixture rank model with
distance specified by the <code>metric</code> argument.
</p>


<h3>See Also</h3>

<p>Other rank functions: 
<code><a href="#topic+compute_expected_distance">compute_expected_distance</a>()</code>,
<code><a href="#topic+compute_observation_frequency">compute_observation_frequency</a>()</code>,
<code><a href="#topic+compute_rank_distance">compute_rank_distance</a>()</code>,
<code><a href="#topic+create_ranking">create_ranking</a>()</code>,
<code><a href="#topic+sample_mallows">sample_mallows</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simulate a sample from a Mallows model with the Kendall distance

n_items &lt;- 5
mydata &lt;- sample_mallows(
  n_samples = 100,
  rho0 = 1:n_items,
  alpha0 = 10,
  metric = "kendall")

# Compute the likelihood and log-likelihood values under the true model...
get_mallows_loglik(
  rho = rbind(1:n_items, 1:n_items),
  alpha = c(10, 10),
  weights = c(0.5, 0.5),
  metric = "kendall",
  rankings = mydata,
  log = FALSE
  )

get_mallows_loglik(
  rho = rbind(1:n_items, 1:n_items),
  alpha = c(10, 10),
  weights = c(0.5, 0.5),
  metric = "kendall",
  rankings = mydata,
  log = TRUE
  )

# or equivalently, by using the frequency distribution
freq_distr &lt;- compute_observation_frequency(mydata)
get_mallows_loglik(
  rho = rbind(1:n_items, 1:n_items),
  alpha = c(10, 10),
  weights = c(0.5, 0.5),
  metric = "kendall",
  rankings = freq_distr[, 1:n_items],
  observation_frequency = freq_distr[, n_items + 1],
  log = FALSE
  )

get_mallows_loglik(
  rho = rbind(1:n_items, 1:n_items),
  alpha = c(10, 10),
  weights = c(0.5, 0.5),
  metric = "kendall",
  rankings = freq_distr[, 1:n_items],
  observation_frequency = freq_distr[, n_items + 1],
  log = TRUE
  )
</code></pre>

<hr>
<h2 id='get_transitive_closure'>Get transitive closure</h2><span id='topic+get_transitive_closure'></span>

<h3>Description</h3>

<p>A simple method for showing any transitive closure computed by
<code><a href="#topic+setup_rank_data">setup_rank_data()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_transitive_closure(rank_data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_transitive_closure_+3A_rank_data">rank_data</code></td>
<td>
<p>An object of class <code>"BayesMallowsData"</code> returned from
<a href="#topic+setup_rank_data">setup_rank_data</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataframe with transitive closure, if there is any.
</p>


<h3>See Also</h3>

<p>Other preprocessing: 
<code><a href="#topic+set_compute_options">set_compute_options</a>()</code>,
<code><a href="#topic+set_initial_values">set_initial_values</a>()</code>,
<code><a href="#topic+set_model_options">set_model_options</a>()</code>,
<code><a href="#topic+set_priors">set_priors</a>()</code>,
<code><a href="#topic+set_smc_options">set_smc_options</a>()</code>,
<code><a href="#topic+setup_rank_data">setup_rank_data</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Original beach preferences
head(beach_preferences)
dim(beach_preferences)
# We then create a rank data object
dat &lt;- setup_rank_data(preferences = beach_preferences)
# The transitive closure contains additional filled-in preferences implied
# by the stated preferences.
head(get_transitive_closure(dat))
dim(get_transitive_closure(dat))

</code></pre>

<hr>
<h2 id='heat_plot'>Heat plot of posterior probabilities</h2><span id='topic+heat_plot'></span>

<h3>Description</h3>

<p>Generates a heat plot with items in their consensus ordering along the
horizontal axis and ranking along the vertical axis. The color denotes
posterior probability.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>heat_plot(model_fit, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="heat_plot_+3A_model_fit">model_fit</code></td>
<td>
<p>An object of type <code>BayesMallows</code>, returned from
<code><a href="#topic+compute_mallows">compute_mallows()</a></code>.</p>
</td></tr>
<tr><td><code id="heat_plot_+3A_...">...</code></td>
<td>
<p>Additional arguments passed on to other methods. In particular,
<code>type = "CP"</code> or <code>type = "MAP"</code> can be passed on to
<code><a href="#topic+compute_consensus">compute_consensus()</a></code> to determine the order of items along the
horizontal axis.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object.
</p>


<h3>See Also</h3>

<p>Other posterior quantities: 
<code><a href="#topic+assign_cluster">assign_cluster</a>()</code>,
<code><a href="#topic+compute_consensus">compute_consensus</a>()</code>,
<code><a href="#topic+compute_posterior_intervals">compute_posterior_intervals</a>()</code>,
<code><a href="#topic+get_acceptance_ratios">get_acceptance_ratios</a>()</code>,
<code><a href="#topic+plot.BayesMallows">plot.BayesMallows</a>()</code>,
<code><a href="#topic+plot.SMCMallows">plot.SMCMallows</a>()</code>,
<code><a href="#topic+plot_elbow">plot_elbow</a>()</code>,
<code><a href="#topic+plot_top_k">plot_top_k</a>()</code>,
<code><a href="#topic+predict_top_k">predict_top_k</a>()</code>,
<code><a href="#topic+print.BayesMallows">print.BayesMallows</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
model_fit &lt;- compute_mallows(
  setup_rank_data(potato_visual),
  compute_options = set_compute_options(nmc = 2000, burnin = 500))

heat_plot(model_fit)
heat_plot(model_fit, type = "MAP")
</code></pre>

<hr>
<h2 id='plot_elbow'>Plot Within-Cluster Sum of Distances</h2><span id='topic+plot_elbow'></span>

<h3>Description</h3>

<p>Plot the within-cluster sum of distances from the corresponding cluster
consensus for different number of clusters. This function is useful for
selecting the number of mixture.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_elbow(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_elbow_+3A_...">...</code></td>
<td>
<p>One or more objects returned from <code><a href="#topic+compute_mallows">compute_mallows()</a></code>, separated
by comma, or a list of such objects. Typically, each object has been run
with a different number of mixtures, as specified in the <code>n_clusters</code>
argument to <code><a href="#topic+compute_mallows">compute_mallows()</a></code>. Alternatively an object returned from
<code><a href="#topic+compute_mallows_mixtures">compute_mallows_mixtures()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A boxplot with the number of clusters on the horizontal axis and the
with-cluster sum of distances on the vertical axis.
</p>


<h3>See Also</h3>

<p>Other posterior quantities: 
<code><a href="#topic+assign_cluster">assign_cluster</a>()</code>,
<code><a href="#topic+compute_consensus">compute_consensus</a>()</code>,
<code><a href="#topic+compute_posterior_intervals">compute_posterior_intervals</a>()</code>,
<code><a href="#topic+get_acceptance_ratios">get_acceptance_ratios</a>()</code>,
<code><a href="#topic+heat_plot">heat_plot</a>()</code>,
<code><a href="#topic+plot.BayesMallows">plot.BayesMallows</a>()</code>,
<code><a href="#topic+plot.SMCMallows">plot.SMCMallows</a>()</code>,
<code><a href="#topic+plot_top_k">plot_top_k</a>()</code>,
<code><a href="#topic+predict_top_k">predict_top_k</a>()</code>,
<code><a href="#topic+print.BayesMallows">print.BayesMallows</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># SIMULATED CLUSTER DATA
set.seed(1)
n_clusters &lt;- seq(from = 1, to = 5)
models &lt;- compute_mallows_mixtures(
  n_clusters = n_clusters, data = setup_rank_data(cluster_data),
  compute_options = set_compute_options(nmc = 2000, include_wcd = TRUE))

# There is good convergence for 1, 2, and 3 cluster, but not for 5.
# Also note that there seems to be label switching around the 7000th iteration
# for the 2-cluster solution.
assess_convergence(models)
# We can create an elbow plot, suggesting that there are three clusters, exactly
# as simulated.
burnin(models) &lt;- 1000
plot_elbow(models)

# We now fit a model with three clusters
mixture_model &lt;- compute_mallows(
  data = setup_rank_data(cluster_data),
  model_options = set_model_options(n_clusters = 3),
  compute_options = set_compute_options(nmc = 2000))

# The trace plot for this model looks good. It seems to converge quickly.
assess_convergence(mixture_model)
# We set the burnin to 500
burnin(mixture_model) &lt;- 500

# We can now look at posterior quantities
# Posterior of scale parameter alpha
plot(mixture_model)
plot(mixture_model, parameter = "rho", items = 4:5)
# There is around 33 % probability of being in each cluster, in agreemeent
# with the data simulating mechanism
plot(mixture_model, parameter = "cluster_probs")
# We can also look at a cluster assignment plot
plot(mixture_model, parameter = "cluster_assignment")

# DETERMINING THE NUMBER OF CLUSTERS IN THE SUSHI EXAMPLE DATA
## Not run: 
  # Let us look at any number of clusters from 1 to 10
  # We use the convenience function compute_mallows_mixtures
  n_clusters &lt;- seq(from = 1, to = 10)
  models &lt;- compute_mallows_mixtures(
    n_clusters = n_clusters, data = setup_rank_data(sushi_rankings),
    compute_options = set_compute_options(include_wcd = TRUE))
  # models is a list in which each element is an object of class BayesMallows,
  # returned from compute_mallows
  # We can create an elbow plot
  burnin(models) &lt;- 1000
  plot_elbow(models)
  # We then select the number of cluster at a point where this plot has
  # an "elbow", e.g., n_clusters = 5.

  # Having chosen the number of clusters, we can now study the final model
  # Rerun with 5 clusters
  mixture_model &lt;- compute_mallows(
    rankings = sushi_rankings,
    model_options = set_model_options(n_clusters = 5),
    compute_options = set_compute_options(include_wcd = TRUE))
  # Delete the models object to free some memory
  rm(models)
  # Set the burnin
  burnin(mixture_model) &lt;- 1000
  # Plot the posterior distributions of alpha per cluster
  plot(mixture_model)
  # Compute the posterior interval of alpha per cluster
  compute_posterior_intervals(mixture_model, parameter = "alpha")
  # Plot the posterior distributions of cluster probabilities
  plot(mixture_model, parameter = "cluster_probs")
  # Plot the posterior probability of cluster assignment
  plot(mixture_model, parameter = "cluster_assignment")
  # Plot the posterior distribution of "tuna roll" in each cluster
  plot(mixture_model, parameter = "rho", items = "tuna roll")
  # Compute the cluster-wise CP consensus, and show one column per cluster
  cp &lt;- compute_consensus(mixture_model, type = "CP")
  cp$cumprob &lt;- NULL
  stats::reshape(cp, direction = "wide", idvar = "ranking",
                 timevar = "cluster", varying = list(as.character(unique(cp$cluster))))

  # Compute the MAP consensus, and show one column per cluster
  map &lt;- compute_consensus(mixture_model, type = "MAP")
  map$probability &lt;- NULL
  stats::reshape(map, direction = "wide", idvar = "map_ranking",
                 timevar = "cluster", varying = list(as.character(unique(map$cluster))))

  # RUNNING IN PARALLEL
  # Computing Mallows models with different number of mixtures in parallel leads to
  # considerably speedup
  library(parallel)
  cl &lt;- makeCluster(detectCores() - 1)
  n_clusters &lt;- seq(from = 1, to = 10)
  models &lt;- compute_mallows_mixtures(
    n_clusters = n_clusters,
    rankings = sushi_rankings,
    compute_options = set_compute_options(include_wcd = TRUE),
    cl = cl)
  stopCluster(cl)

## End(Not run)



</code></pre>

<hr>
<h2 id='plot_top_k'>Plot Top-k Rankings with Pairwise Preferences</h2><span id='topic+plot_top_k'></span>

<h3>Description</h3>

<p>Plot the posterior probability, per item, of being ranked among the
top-<code class="reqn">k</code> for each assessor. This plot is useful when the data take the
form of pairwise preferences.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_top_k(model_fit, k = 3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_top_k_+3A_model_fit">model_fit</code></td>
<td>
<p>An object of type <code>BayesMallows</code>, returned from
<code><a href="#topic+compute_mallows">compute_mallows()</a></code>.</p>
</td></tr>
<tr><td><code id="plot_top_k_+3A_k">k</code></td>
<td>
<p>Integer specifying the k in top-<code class="reqn">k</code>.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other posterior quantities: 
<code><a href="#topic+assign_cluster">assign_cluster</a>()</code>,
<code><a href="#topic+compute_consensus">compute_consensus</a>()</code>,
<code><a href="#topic+compute_posterior_intervals">compute_posterior_intervals</a>()</code>,
<code><a href="#topic+get_acceptance_ratios">get_acceptance_ratios</a>()</code>,
<code><a href="#topic+heat_plot">heat_plot</a>()</code>,
<code><a href="#topic+plot.BayesMallows">plot.BayesMallows</a>()</code>,
<code><a href="#topic+plot.SMCMallows">plot.SMCMallows</a>()</code>,
<code><a href="#topic+plot_elbow">plot_elbow</a>()</code>,
<code><a href="#topic+predict_top_k">predict_top_k</a>()</code>,
<code><a href="#topic+print.BayesMallows">print.BayesMallows</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
# We use the example dataset with beach preferences. Se the documentation to
# compute_mallows for how to assess the convergence of the algorithm
# We need to save the augmented data, so setting this option to TRUE
model_fit &lt;- compute_mallows(
  data = setup_rank_data(preferences = beach_preferences),
  compute_options = set_compute_options(
    nmc = 1000, burnin = 500, save_aug = TRUE))
# By default, the probability of being top-3 is plotted
# The default plot gives the probability for each assessor
plot_top_k(model_fit)
# We can also plot the probability of being top-5, for each item
plot_top_k(model_fit, k = 5)
# We get the underlying numbers with predict_top_k
probs &lt;- predict_top_k(model_fit)
# To find all items ranked top-3 by assessors 1-3 with probability more than 80 %,
# we do
subset(probs, assessor %in% 1:3 &amp; prob &gt; 0.8)

# We can also plot for clusters
model_fit &lt;- compute_mallows(
  data = setup_rank_data(preferences = beach_preferences),
  model_options = set_model_options(n_clusters = 3),
  compute_options = set_compute_options(
    nmc = 1000, burnin = 500, save_aug = TRUE)
  )
# The modal ranking in general differs between clusters, but the plot still
# represents the posterior distribution of each user's augmented rankings.
plot_top_k(model_fit)
</code></pre>

<hr>
<h2 id='plot.BayesMallows'>Plot Posterior Distributions</h2><span id='topic+plot.BayesMallows'></span>

<h3>Description</h3>

<p>Plot posterior distributions of the parameters of the Mallows Rank model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'BayesMallows'
plot(x, parameter = "alpha", items = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.BayesMallows_+3A_x">x</code></td>
<td>
<p>An object of type <code>BayesMallows</code>, returned from
<code><a href="#topic+compute_mallows">compute_mallows()</a></code>.</p>
</td></tr>
<tr><td><code id="plot.BayesMallows_+3A_parameter">parameter</code></td>
<td>
<p>Character string defining the parameter to plot. Available
options are <code>"alpha"</code>, <code>"rho"</code>, <code>"cluster_probs"</code>,
<code>"cluster_assignment"</code>, and <code>"theta"</code>.</p>
</td></tr>
<tr><td><code id="plot.BayesMallows_+3A_items">items</code></td>
<td>
<p>The items to study in the diagnostic plot for <code>rho</code>. Either
a vector of item names, corresponding to <code>x$data$items</code> or a
vector of indices. If NULL, five items are selected randomly.
Only used when <code>parameter = "rho"</code>.</p>
</td></tr>
<tr><td><code id="plot.BayesMallows_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <code>plot</code> (not used).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other posterior quantities: 
<code><a href="#topic+assign_cluster">assign_cluster</a>()</code>,
<code><a href="#topic+compute_consensus">compute_consensus</a>()</code>,
<code><a href="#topic+compute_posterior_intervals">compute_posterior_intervals</a>()</code>,
<code><a href="#topic+get_acceptance_ratios">get_acceptance_ratios</a>()</code>,
<code><a href="#topic+heat_plot">heat_plot</a>()</code>,
<code><a href="#topic+plot.SMCMallows">plot.SMCMallows</a>()</code>,
<code><a href="#topic+plot_elbow">plot_elbow</a>()</code>,
<code><a href="#topic+plot_top_k">plot_top_k</a>()</code>,
<code><a href="#topic+predict_top_k">predict_top_k</a>()</code>,
<code><a href="#topic+print.BayesMallows">print.BayesMallows</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model_fit &lt;- compute_mallows(setup_rank_data(potato_visual))
burnin(model_fit) &lt;- 1000

# By default, the scale parameter "alpha" is plotted
plot(model_fit)
# We can also plot the latent rankings "rho"
plot(model_fit, parameter = "rho")
# By default, a random subset of 5 items are plotted
# Specify which items to plot in the items argument.
plot(model_fit, parameter = "rho",
     items = c(2, 4, 6, 9, 10, 20))
# When the ranking matrix has column names, we can also
# specify these in the items argument.
# In this case, we have the following names:
colnames(potato_visual)
# We can therefore get the same plot with the following call:
plot(model_fit, parameter = "rho",
     items = c("P2", "P4", "P6", "P9", "P10", "P20"))

## Not run: 
  # Plots of mixture parameters:
  model_fit &lt;- compute_mallows(
    setup_rank_data(sushi_rankings),
    model_options = set_model_options(n_clusters = 5))
  burnin(model_fit) &lt;- 1000
  # Posterior distributions of the cluster probabilities
  plot(model_fit, parameter = "cluster_probs")
  # Cluster assignment plot. Color shows the probability of belonging to each
  # cluster.
  plot(model_fit, parameter = "cluster_assignment")

## End(Not run)




</code></pre>

<hr>
<h2 id='plot.SMCMallows'>Plot SMC Posterior Distributions</h2><span id='topic+plot.SMCMallows'></span>

<h3>Description</h3>

<p>Plot posterior distributions of SMC-Mallow parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SMCMallows'
plot(x, parameter = "alpha", items = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.SMCMallows_+3A_x">x</code></td>
<td>
<p>An object of type <code>SMC-Mallows</code>.</p>
</td></tr>
<tr><td><code id="plot.SMCMallows_+3A_parameter">parameter</code></td>
<td>
<p>Character string defining the parameter to plot. Available
options are <code>"alpha"</code> and <code>"rho"</code>.</p>
</td></tr>
<tr><td><code id="plot.SMCMallows_+3A_items">items</code></td>
<td>
<p>Either a vector of item names, or a vector of indices. If NULL,
five items are selected randomly.</p>
</td></tr>
<tr><td><code id="plot.SMCMallows_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <code><a href="base.html#topic+plot">plot</a></code> (not used).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A plot of the posterior distributions
</p>


<h3>See Also</h3>

<p>Other posterior quantities: 
<code><a href="#topic+assign_cluster">assign_cluster</a>()</code>,
<code><a href="#topic+compute_consensus">compute_consensus</a>()</code>,
<code><a href="#topic+compute_posterior_intervals">compute_posterior_intervals</a>()</code>,
<code><a href="#topic+get_acceptance_ratios">get_acceptance_ratios</a>()</code>,
<code><a href="#topic+heat_plot">heat_plot</a>()</code>,
<code><a href="#topic+plot.BayesMallows">plot.BayesMallows</a>()</code>,
<code><a href="#topic+plot_elbow">plot_elbow</a>()</code>,
<code><a href="#topic+plot_top_k">plot_top_k</a>()</code>,
<code><a href="#topic+predict_top_k">predict_top_k</a>()</code>,
<code><a href="#topic+print.BayesMallows">print.BayesMallows</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
# UPDATING A MALLOWS MODEL WITH NEW COMPLETE RANKINGS
# Assume we first only observe the first four rankings in the potato_visual
# dataset
data_first_batch &lt;- potato_visual[1:4, ]

# We start by fitting a model using Metropolis-Hastings
mod_init &lt;- compute_mallows(
  data = setup_rank_data(data_first_batch),
  compute_options = set_compute_options(nmc = 10000))

# Convergence seems good after no more than 2000 iterations
assess_convergence(mod_init)
burnin(mod_init) &lt;- 2000

# Next, assume we receive four more observations
data_second_batch &lt;- potato_visual[5:8, ]

# We can now update the model using sequential Monte Carlo
mod_second &lt;- update_mallows(
  model = mod_init,
  new_data = setup_rank_data(rankings = data_second_batch),
  smc_options = set_smc_options(resampler = "systematic")
  )

# This model now has a collection of particles approximating the posterior
# distribution after the first and second batch
# We can use all the posterior summary functions as we do for the model
# based on compute_mallows():
plot(mod_second)
plot(mod_second, parameter = "rho", items = 1:4)
compute_posterior_intervals(mod_second)

# Next, assume we receive the third and final batch of data. We can update
# the model again
data_third_batch &lt;- potato_visual[9:12, ]
mod_final &lt;- update_mallows(
  model = mod_second, new_data = setup_rank_data(rankings = data_third_batch))

# We can plot the same things as before
plot(mod_final)
compute_consensus(mod_final)

# UPDATING A MALLOWS MODEL WITH NEW OR UPDATED PARTIAL RANKINGS
# The sequential Monte Carlo algorithm works for data with missing ranks as
# well. This both includes the case where new users arrive with partial ranks,
# and when previously seen users arrive with more complete data than they had
# previously.
# We illustrate for top-k rankings of the first 10 users in potato_visual
potato_top_10 &lt;- ifelse(potato_visual[1:10, ] &gt; 10, NA_real_,
                        potato_visual[1:10, ])
potato_top_12 &lt;- ifelse(potato_visual[1:10, ] &gt; 12, NA_real_,
                        potato_visual[1:10, ])
potato_top_14 &lt;- ifelse(potato_visual[1:10, ] &gt; 14, NA_real_,
                        potato_visual[1:10, ])

# We need the rownames as user IDs
(user_ids &lt;- 1:10)

# First, users provide top-10 rankings
mod_init &lt;- compute_mallows(
  data = setup_rank_data(rankings = potato_top_10, user_ids = user_ids),
  compute_options = set_compute_options(nmc = 10000))

# Convergence seems fine. We set the burnin to 2000.
assess_convergence(mod_init)
burnin(mod_init) &lt;- 2000

# Next assume the users update their rankings, so we have top-12 instead.
mod1 &lt;- update_mallows(
  model = mod_init,
  new_data = setup_rank_data(rankings = potato_top_12, user_ids = user_ids),
  smc_options = set_smc_options(resampler = "stratified")
)

plot(mod1)

# Then, assume we get even more data, this time top-14 rankings:
mod2 &lt;- update_mallows(
  model = mod1,
  new_data = setup_rank_data(rankings = potato_top_14, user_ids = user_ids)
)

plot(mod2)

# Finally, assume a set of new users arrive, who have complete rankings.
potato_new &lt;- potato_visual[11:12, ]
# We need to update the user IDs, to show that these users are different
(user_ids &lt;- 11:12)

mod_final &lt;- update_mallows(
  model = mod2,
  new_data = setup_rank_data(rankings = potato_new, user_ids = user_ids)
)

plot(mod_final)

# We can also update models with pairwise preferences
# We here start by running MCMC on the first 20 assessors of the beach data
# A realistic application should run a larger number of iterations than we
# do in this example.
set.seed(3)
dat &lt;- subset(beach_preferences, assessor &lt;= 20)
mod &lt;- compute_mallows(
  data = setup_rank_data(
    preferences = beach_preferences),
  compute_options = set_compute_options(nmc = 3000, burnin = 1000)
)

# Next we provide assessors 21 to 24 one at a time.
for(i in 21:24){
  mod &lt;- update_mallows(
    model = mod,
    new_data = setup_rank_data(
      preferences = subset(beach_preferences, assessor == i),
      user_ids = i, shuffle_unranked = TRUE),
    smc_options = set_smc_options(latent_sampling_lag = 0)
  )
}

# Compared to running full MCMC, there is a downward bias in the scale
# parameter. This can be alleviated by increasing the number of particles,
# MCMC steps, and the latent sampling lag.
plot(mod)
compute_consensus(mod)
</code></pre>

<hr>
<h2 id='potato_true_ranking'>True ranking of the weights of 20 potatoes.</h2><span id='topic+potato_true_ranking'></span>

<h3>Description</h3>

<p>True ranking of the weights of 20 potatoes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>potato_true_ranking
</code></pre>


<h3>Format</h3>

<p>An object of class <code>numeric</code> of length 20.
</p>


<h3>References</h3>

<p>Liu Q, Crispino M, Scheel I, Vitelli V, Frigessi A (2019).
&ldquo;Model-Based Learning from Preference Data.&rdquo;
<em>Annual Review of Statistics and Its Application</em>, <b>6</b>(1).
<a href="https://doi.org/10.1146/annurev-statistics-031017-100213">doi:10.1146/annurev-statistics-031017-100213</a>.
</p>


<h3>See Also</h3>

<p>Other datasets: 
<code><a href="#topic+beach_preferences">beach_preferences</a></code>,
<code><a href="#topic+bernoulli_data">bernoulli_data</a></code>,
<code><a href="#topic+cluster_data">cluster_data</a></code>,
<code><a href="#topic+potato_visual">potato_visual</a></code>,
<code><a href="#topic+potato_weighing">potato_weighing</a></code>,
<code><a href="#topic+sushi_rankings">sushi_rankings</a></code>
</p>

<hr>
<h2 id='potato_visual'>Potato weights assessed visually</h2><span id='topic+potato_visual'></span>

<h3>Description</h3>

<p>Result of ranking potatoes by weight, where the assessors were only allowed
to inspected the potatoes visually. 12 assessors ranked 20 potatoes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>potato_visual
</code></pre>


<h3>Format</h3>

<p>An object of class <code>matrix</code> (inherits from <code>array</code>) with 12 rows and 20 columns.
</p>


<h3>References</h3>

<p>Liu Q, Crispino M, Scheel I, Vitelli V, Frigessi A (2019).
&ldquo;Model-Based Learning from Preference Data.&rdquo;
<em>Annual Review of Statistics and Its Application</em>, <b>6</b>(1).
<a href="https://doi.org/10.1146/annurev-statistics-031017-100213">doi:10.1146/annurev-statistics-031017-100213</a>.
</p>


<h3>See Also</h3>

<p>Other datasets: 
<code><a href="#topic+beach_preferences">beach_preferences</a></code>,
<code><a href="#topic+bernoulli_data">bernoulli_data</a></code>,
<code><a href="#topic+cluster_data">cluster_data</a></code>,
<code><a href="#topic+potato_true_ranking">potato_true_ranking</a></code>,
<code><a href="#topic+potato_weighing">potato_weighing</a></code>,
<code><a href="#topic+sushi_rankings">sushi_rankings</a></code>
</p>

<hr>
<h2 id='potato_weighing'>Potato weights assessed by hand</h2><span id='topic+potato_weighing'></span>

<h3>Description</h3>

<p>Result of ranking potatoes by weight, where the assessors were
allowed to lift the potatoes. 12 assessors ranked 20 potatoes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>potato_weighing
</code></pre>


<h3>Format</h3>

<p>An object of class <code>matrix</code> (inherits from <code>array</code>) with 12 rows and 20 columns.
</p>


<h3>References</h3>

<p>Liu Q, Crispino M, Scheel I, Vitelli V, Frigessi A (2019).
&ldquo;Model-Based Learning from Preference Data.&rdquo;
<em>Annual Review of Statistics and Its Application</em>, <b>6</b>(1).
<a href="https://doi.org/10.1146/annurev-statistics-031017-100213">doi:10.1146/annurev-statistics-031017-100213</a>.
</p>


<h3>See Also</h3>

<p>Other datasets: 
<code><a href="#topic+beach_preferences">beach_preferences</a></code>,
<code><a href="#topic+bernoulli_data">bernoulli_data</a></code>,
<code><a href="#topic+cluster_data">cluster_data</a></code>,
<code><a href="#topic+potato_true_ranking">potato_true_ranking</a></code>,
<code><a href="#topic+potato_visual">potato_visual</a></code>,
<code><a href="#topic+sushi_rankings">sushi_rankings</a></code>
</p>

<hr>
<h2 id='predict_top_k'>Predict Top-k Rankings with Pairwise Preferences</h2><span id='topic+predict_top_k'></span>

<h3>Description</h3>

<p>Predict the posterior probability, per item, of being ranked among the
top-<code class="reqn">k</code> for each assessor. This is useful when the data take the form of
pairwise preferences.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_top_k(model_fit, k = 3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_top_k_+3A_model_fit">model_fit</code></td>
<td>
<p>An object of type <code>BayesMallows</code>, returned from
<code><a href="#topic+compute_mallows">compute_mallows()</a></code>.</p>
</td></tr>
<tr><td><code id="predict_top_k_+3A_k">k</code></td>
<td>
<p>Integer specifying the k in top-<code class="reqn">k</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataframe with columns <code>assessor</code>, <code>item</code>, and
<code>prob</code>, where each row states the probability that the given assessor
rates the given item among top-<code class="reqn">k</code>.
</p>


<h3>See Also</h3>

<p>Other posterior quantities: 
<code><a href="#topic+assign_cluster">assign_cluster</a>()</code>,
<code><a href="#topic+compute_consensus">compute_consensus</a>()</code>,
<code><a href="#topic+compute_posterior_intervals">compute_posterior_intervals</a>()</code>,
<code><a href="#topic+get_acceptance_ratios">get_acceptance_ratios</a>()</code>,
<code><a href="#topic+heat_plot">heat_plot</a>()</code>,
<code><a href="#topic+plot.BayesMallows">plot.BayesMallows</a>()</code>,
<code><a href="#topic+plot.SMCMallows">plot.SMCMallows</a>()</code>,
<code><a href="#topic+plot_elbow">plot_elbow</a>()</code>,
<code><a href="#topic+plot_top_k">plot_top_k</a>()</code>,
<code><a href="#topic+print.BayesMallows">print.BayesMallows</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
# We use the example dataset with beach preferences. Se the documentation to
# compute_mallows for how to assess the convergence of the algorithm
# We need to save the augmented data, so setting this option to TRUE
model_fit &lt;- compute_mallows(
  data = setup_rank_data(preferences = beach_preferences),
  compute_options = set_compute_options(
    nmc = 1000, burnin = 500, save_aug = TRUE))
# By default, the probability of being top-3 is plotted
# The default plot gives the probability for each assessor
plot_top_k(model_fit)
# We can also plot the probability of being top-5, for each item
plot_top_k(model_fit, k = 5)
# We get the underlying numbers with predict_top_k
probs &lt;- predict_top_k(model_fit)
# To find all items ranked top-3 by assessors 1-3 with probability more than 80 %,
# we do
subset(probs, assessor %in% 1:3 &amp; prob &gt; 0.8)

# We can also plot for clusters
model_fit &lt;- compute_mallows(
  data = setup_rank_data(preferences = beach_preferences),
  model_options = set_model_options(n_clusters = 3),
  compute_options = set_compute_options(
    nmc = 1000, burnin = 500, save_aug = TRUE)
  )
# The modal ranking in general differs between clusters, but the plot still
# represents the posterior distribution of each user's augmented rankings.
plot_top_k(model_fit)
</code></pre>

<hr>
<h2 id='print.BayesMallows'>Print Method for BayesMallows Objects</h2><span id='topic+print.BayesMallows'></span><span id='topic+print.BayesMallowsMixtures'></span><span id='topic+print.SMCMallows'></span>

<h3>Description</h3>

<p>The default print method for a <code>BayesMallows</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'BayesMallows'
print(x, ...)

## S3 method for class 'BayesMallowsMixtures'
print(x, ...)

## S3 method for class 'SMCMallows'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.BayesMallows_+3A_x">x</code></td>
<td>
<p>An object of type <code>BayesMallows</code>, returned from
<code><a href="#topic+compute_mallows">compute_mallows()</a></code>.</p>
</td></tr>
<tr><td><code id="print.BayesMallows_+3A_...">...</code></td>
<td>
<p>Other arguments passed to <code>print</code> (not used).</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other posterior quantities: 
<code><a href="#topic+assign_cluster">assign_cluster</a>()</code>,
<code><a href="#topic+compute_consensus">compute_consensus</a>()</code>,
<code><a href="#topic+compute_posterior_intervals">compute_posterior_intervals</a>()</code>,
<code><a href="#topic+get_acceptance_ratios">get_acceptance_ratios</a>()</code>,
<code><a href="#topic+heat_plot">heat_plot</a>()</code>,
<code><a href="#topic+plot.BayesMallows">plot.BayesMallows</a>()</code>,
<code><a href="#topic+plot.SMCMallows">plot.SMCMallows</a>()</code>,
<code><a href="#topic+plot_elbow">plot_elbow</a>()</code>,
<code><a href="#topic+plot_top_k">plot_top_k</a>()</code>,
<code><a href="#topic+predict_top_k">predict_top_k</a>()</code>
</p>

<hr>
<h2 id='rmallows'>Sample from the Mallows distribution.</h2><span id='topic+rmallows'></span>

<h3>Description</h3>

<p>Sample from the Mallows distribution with arbitrary distance metric using
a Metropolis-Hastings algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmallows(
  rho0,
  alpha0,
  n_samples,
  burnin,
  thinning,
  leap_size = 1L,
  metric = "footrule"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmallows_+3A_rho0">rho0</code></td>
<td>
<p>Vector specifying the latent consensus ranking.</p>
</td></tr>
<tr><td><code id="rmallows_+3A_alpha0">alpha0</code></td>
<td>
<p>Scalar specifying the scale parameter.</p>
</td></tr>
<tr><td><code id="rmallows_+3A_n_samples">n_samples</code></td>
<td>
<p>Integer specifying the number of random samples to generate.</p>
</td></tr>
<tr><td><code id="rmallows_+3A_burnin">burnin</code></td>
<td>
<p>Integer specifying the number of iterations to discard as burn-in.</p>
</td></tr>
<tr><td><code id="rmallows_+3A_thinning">thinning</code></td>
<td>
<p>Integer specifying the number of MCMC iterations to perform
between each time a random rank vector is sampled.</p>
</td></tr>
<tr><td><code id="rmallows_+3A_leap_size">leap_size</code></td>
<td>
<p>Integer specifying the step size of the leap-and-shift proposal distribution.</p>
</td></tr>
<tr><td><code id="rmallows_+3A_metric">metric</code></td>
<td>
<p>Character string specifying the distance measure to use. Available
options are <code>"footrule"</code> (default), <code>"spearman"</code>, <code>"cayley"</code>, <code>"hamming"</code>,
<code>"kendall"</code>, and <code>"ulam"</code>.</p>
</td></tr>
</table>


<h3>References</h3>

<p>There are no references for Rd macro <code style="white-space: pre;">&#8288;\insertAllCites&#8288;</code> on this help page.
</p>

<hr>
<h2 id='sample_mallows'>Random Samples from the Mallows Rank Model</h2><span id='topic+sample_mallows'></span>

<h3>Description</h3>

<p>Generate random samples from the Mallows Rank Model
(Mallows 1957) with consensus ranking <code class="reqn">\rho</code> and
scale parameter <code class="reqn">\alpha</code>. The samples are obtained by running the
Metropolis-Hastings algorithm described in Appendix C of
Vitelli et al. (2018).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_mallows(
  rho0,
  alpha0,
  n_samples,
  leap_size = max(1L, floor(n_items/5)),
  metric = "footrule",
  diagnostic = FALSE,
  burnin = ifelse(diagnostic, 0, 1000),
  thinning = ifelse(diagnostic, 1, 1000),
  items_to_plot = NULL,
  max_lag = 1000L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample_mallows_+3A_rho0">rho0</code></td>
<td>
<p>Vector specifying the latent consensus ranking in the Mallows
rank model.</p>
</td></tr>
<tr><td><code id="sample_mallows_+3A_alpha0">alpha0</code></td>
<td>
<p>Scalar specifying the scale parameter in the Mallows rank
model.</p>
</td></tr>
<tr><td><code id="sample_mallows_+3A_n_samples">n_samples</code></td>
<td>
<p>Integer specifying the number of random samples to generate.
When <code>diagnostic = TRUE</code>, this number must be larger than 1.</p>
</td></tr>
<tr><td><code id="sample_mallows_+3A_leap_size">leap_size</code></td>
<td>
<p>Integer specifying the step size of the leap-and-shift
proposal distribution.</p>
</td></tr>
<tr><td><code id="sample_mallows_+3A_metric">metric</code></td>
<td>
<p>Character string specifying the distance measure to use.
Available options are <code>"footrule"</code> (default), <code>"spearman"</code>,
<code>"cayley"</code>, <code>"hamming"</code>, <code>"kendall"</code>, and <code>"ulam"</code>.
See also the <code>rmm</code> function in the <code>PerMallows</code> package
(Irurozki et al. 2016) for sampling from the Mallows
model with Cayley, Hamming, Kendall, and Ulam distances.</p>
</td></tr>
<tr><td><code id="sample_mallows_+3A_diagnostic">diagnostic</code></td>
<td>
<p>Logical specifying whether to output convergence
diagnostics. If <code>TRUE</code>, a diagnostic plot is printed, together with
the returned samples.</p>
</td></tr>
<tr><td><code id="sample_mallows_+3A_burnin">burnin</code></td>
<td>
<p>Integer specifying the number of iterations to discard as
burn-in. Defaults to 1000 when <code>diagnostic = FALSE</code>, else to 0.</p>
</td></tr>
<tr><td><code id="sample_mallows_+3A_thinning">thinning</code></td>
<td>
<p>Integer specifying the number of MCMC iterations to perform
between each time a random rank vector is sampled. Defaults to 1000 when
<code>diagnostic = FALSE</code>, else to 1.</p>
</td></tr>
<tr><td><code id="sample_mallows_+3A_items_to_plot">items_to_plot</code></td>
<td>
<p>Integer vector used if <code>diagnostic = TRUE</code>, in
order to specify the items to plot in the diagnostic output. If not
provided, 5 items are picked at random.</p>
</td></tr>
<tr><td><code id="sample_mallows_+3A_max_lag">max_lag</code></td>
<td>
<p>Integer specifying the maximum lag to use in the computation
of autocorrelation. Defaults to 1000L. This argument is passed to
<code>stats::acf</code>. Only used when <code>diagnostic = TRUE</code>.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Irurozki E, Calvo B, Lozano JA (2016).
&ldquo;PerMallows: An R Package for Mallows and Generalized Mallows Models.&rdquo;
<em>Journal of Statistical Software</em>, <b>71</b>(12), 1&ndash;30.
<a href="https://doi.org/10.18637/jss.v071.i12">doi:10.18637/jss.v071.i12</a>.<br /><br /> Mallows CL (1957).
&ldquo;Non-Null Ranking Models. I.&rdquo;
<em>Biometrika</em>, <b>44</b>(1/2), 114&ndash;130.<br /><br /> Vitelli V, Sørensen, Crispino M, Arjas E, Frigessi A (2018).
&ldquo;Probabilistic Preference Learning with the Mallows Rank Model.&rdquo;
<em>Journal of Machine Learning Research</em>, <b>18</b>(1), 1&ndash;49.
<a href="https://jmlr.org/papers/v18/15-481.html">https://jmlr.org/papers/v18/15-481.html</a>.
</p>


<h3>See Also</h3>

<p>Other rank functions: 
<code><a href="#topic+compute_expected_distance">compute_expected_distance</a>()</code>,
<code><a href="#topic+compute_observation_frequency">compute_observation_frequency</a>()</code>,
<code><a href="#topic+compute_rank_distance">compute_rank_distance</a>()</code>,
<code><a href="#topic+create_ranking">create_ranking</a>()</code>,
<code><a href="#topic+get_mallows_loglik">get_mallows_loglik</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Sample 100 random rankings from a Mallows distribution with footrule distance
set.seed(1)
# Number of items
n_items &lt;- 15
# Set the consensus ranking
rho0 &lt;- seq(from = 1, to = n_items, by = 1)
# Set the scale
alpha0 &lt;- 10
# Number of samples
n_samples &lt;- 100
# We first do a diagnostic run, to find the thinning and burnin to use
# We set n_samples to 1000, in order to run 1000 diagnostic iterations.
test &lt;- sample_mallows(rho0 = rho0, alpha0 = alpha0, diagnostic = TRUE,
                       n_samples = 1000, burnin = 1, thinning = 1)
# When items_to_plot is not set, 5 items are picked at random. We can change this.
# We can also reduce the number of lags computed in the autocorrelation plots
test &lt;- sample_mallows(rho0 = rho0, alpha0 = alpha0, diagnostic = TRUE,
                       n_samples = 1000, burnin = 1, thinning = 1,
                       items_to_plot = c(1:3, 10, 15), max_lag = 500)
# From the autocorrelation plot, it looks like we should use
# a thinning of at least 200. We set thinning = 1000 to be safe,
# since the algorithm in any case is fast. The Markov Chain
# seems to mix quickly, but we set the burnin to 1000 to be safe.
# We now run sample_mallows again, to get the 100 samples we want:
samples &lt;- sample_mallows(rho0 = rho0, alpha0 = alpha0, n_samples = 100,
                          burnin = 1000, thinning = 1000)
# The samples matrix now contains 100 rows with rankings of 15 items.
# A good diagnostic, in order to confirm that burnin and thinning are set high
# enough, is to run compute_mallows on the samples
model_fit &lt;- compute_mallows(
  setup_rank_data(samples),
  compute_options = set_compute_options(nmc = 10000))
# The highest posterior density interval covers alpha0 = 10.
burnin(model_fit) &lt;- 2000
compute_posterior_intervals(model_fit, parameter = "alpha")

</code></pre>

<hr>
<h2 id='sample_prior'>Sample from prior distribution</h2><span id='topic+sample_prior'></span>

<h3>Description</h3>

<p>Function to obtain samples from the prior distributions of the Bayesian
Mallows model. Intended to be given to <code><a href="#topic+update_mallows">update_mallows()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_prior(n, n_items, priors = set_priors())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample_prior_+3A_n">n</code></td>
<td>
<p>An integer specifying the number of samples to take.</p>
</td></tr>
<tr><td><code id="sample_prior_+3A_n_items">n_items</code></td>
<td>
<p>An integer specifying the number of items to be ranked.</p>
</td></tr>
<tr><td><code id="sample_prior_+3A_priors">priors</code></td>
<td>
<p>An object of class &quot;BayesMallowsPriors&quot; returned from
<code><a href="#topic+set_priors">set_priors()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class &quot;BayesMallowsPriorSample&quot;, containing <code>n</code>
independent samples of <code class="reqn">\alpha</code> and <code class="reqn">\rho</code>.
</p>


<h3>See Also</h3>

<p>Other modeling: 
<code><a href="#topic+burnin">burnin</a>()</code>,
<code><a href="#topic+burnin+3C-">burnin&lt;-</a>()</code>,
<code><a href="#topic+compute_mallows">compute_mallows</a>()</code>,
<code><a href="#topic+compute_mallows_mixtures">compute_mallows_mixtures</a>()</code>,
<code><a href="#topic+compute_mallows_sequentially">compute_mallows_sequentially</a>()</code>,
<code><a href="#topic+update_mallows">update_mallows</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># We can use a collection of particles from the prior distribution as
# initial values for the sequential Monte Carlo algorithm.
# Here we start by drawing 1000 particles from the priors, using default
# parameters.
prior_samples &lt;- sample_prior(1000, ncol(sushi_rankings))
# Next, we provide the prior samples to update_mallws(), together
# with the first five rows of the sushi dataset
model1 &lt;- update_mallows(
  model = prior_samples,
  new_data = setup_rank_data(sushi_rankings[1:5, ]))
plot(model1)

# We keep adding more data
model2 &lt;- update_mallows(
  model = model1,
  new_data = setup_rank_data(sushi_rankings[6:10, ]))
plot(model2)

model3 &lt;- update_mallows(
  model = model2,
  new_data = setup_rank_data(sushi_rankings[11:15, ]))
plot(model3)
</code></pre>

<hr>
<h2 id='set_compute_options'>Specify options for computation</h2><span id='topic+set_compute_options'></span>

<h3>Description</h3>

<p>Set parameters related to the Metropolis-Hastings algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_compute_options(
  nmc = 2000,
  burnin = NULL,
  alpha_prop_sd = 0.1,
  rho_proposal = c("ls", "swap"),
  leap_size = 1,
  aug_method = c("uniform", "pseudo"),
  pseudo_aug_metric = c("footrule", "spearman"),
  swap_leap = 1,
  alpha_jump = 1,
  aug_thinning = 1,
  clus_thinning = 1,
  rho_thinning = 1,
  include_wcd = FALSE,
  save_aug = FALSE,
  save_ind_clus = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_compute_options_+3A_nmc">nmc</code></td>
<td>
<p>Integer specifying the number of iteration of the
Metropolis-Hastings algorithm to run. Defaults to <code>2000</code>. See
<code><a href="#topic+assess_convergence">assess_convergence()</a></code> for tools to check convergence of the Markov chain.</p>
</td></tr>
<tr><td><code id="set_compute_options_+3A_burnin">burnin</code></td>
<td>
<p>Integer defining the number of samples to discard. Defaults to
<code>NULL</code>, which means that burn-in is not set.</p>
</td></tr>
<tr><td><code id="set_compute_options_+3A_alpha_prop_sd">alpha_prop_sd</code></td>
<td>
<p>Numeric value specifying the <code class="reqn">\sigma</code> parameter of
the lognormal proposal distribution used for <code class="reqn">\alpha</code> in the
Metropolis-Hastings algorithm. The logarithm of the proposed samples will
have standard deviation given by <code>alpha_prop_sd</code>. Defaults to <code>0.1</code>.</p>
</td></tr>
<tr><td><code id="set_compute_options_+3A_rho_proposal">rho_proposal</code></td>
<td>
<p>Character string specifying the proposal distribution of
modal ranking <code class="reqn">\rho</code>. Defaults to &quot;ls&quot;, which means that the
leap-and-shift algorithm of Vitelli et al. (2018)
is used. The other option is &quot;swap&quot;, which means that the swap proposal of
Crispino et al. (2019) is used instead.</p>
</td></tr>
<tr><td><code id="set_compute_options_+3A_leap_size">leap_size</code></td>
<td>
<p>Integer specifying the step size of the distribution defined
in <code>rho_proposal</code> for proposing new latent ranks <code class="reqn">rho</code>. Defaults to 1.</p>
</td></tr>
<tr><td><code id="set_compute_options_+3A_aug_method">aug_method</code></td>
<td>
<p>Augmentation proposal for use with missing data. One of
&quot;pseudo&quot; and &quot;uniform&quot;. Defaults to &quot;uniform&quot;, which means that new
augmented rankings are proposed by sampling uniformly from the set of
available ranks, see Section 4 in
Vitelli et al. (2018). Setting the argument to
&quot;pseudo&quot; instead, means that the pseudo-likelihood proposal defined in
Chapter 5 of
Stein (2023) is
used instead.</p>
</td></tr>
<tr><td><code id="set_compute_options_+3A_pseudo_aug_metric">pseudo_aug_metric</code></td>
<td>
<p>String defining the metric to be used in the
pseudo-likelihood proposal. Only used if <code>aug_method = "pseudo"</code>. Can be
either &quot;footrule&quot; or &quot;spearman&quot;, and defaults to &quot;footrule&quot;.</p>
</td></tr>
<tr><td><code id="set_compute_options_+3A_swap_leap">swap_leap</code></td>
<td>
<p>Integer specifying the leap size for the swap proposal used
for proposing latent ranks in the case of non-transitive pairwise
preference data. Note that leap size for the swap proposal when used for
proposal the modal ranking <code class="reqn">\rho</code> is given by the <code>leap_size</code>
argument above.</p>
</td></tr>
<tr><td><code id="set_compute_options_+3A_alpha_jump">alpha_jump</code></td>
<td>
<p>Integer specifying how many times to sample <code class="reqn">\rho</code>
between each sampling of <code class="reqn">\alpha</code>. In other words, how many times to
jump over <code class="reqn">\alpha</code> while sampling <code class="reqn">\rho</code>, and possibly other
parameters like augmented ranks <code class="reqn">\tilde{R}</code> or cluster assignments
<code class="reqn">z</code>. Setting <code>alpha_jump</code> to a high number can speed up computation
time, by reducing the number of times the partition function for the
Mallows model needs to be computed. Defaults to <code>1</code>.</p>
</td></tr>
<tr><td><code id="set_compute_options_+3A_aug_thinning">aug_thinning</code></td>
<td>
<p>Integer specifying the thinning for saving augmented
data. Only used when <code>save_aug = TRUE</code>. Defaults to <code>1</code>.</p>
</td></tr>
<tr><td><code id="set_compute_options_+3A_clus_thinning">clus_thinning</code></td>
<td>
<p>Integer specifying the thinning to be applied to cluster
assignments and cluster probabilities. Defaults to <code>1</code>.</p>
</td></tr>
<tr><td><code id="set_compute_options_+3A_rho_thinning">rho_thinning</code></td>
<td>
<p>Integer specifying the thinning of <code>rho</code> to be performed
in the Metropolis- Hastings algorithm. Defaults to <code>1</code>. <code>compute_mallows</code>
save every <code>rho_thinning</code>th value of <code class="reqn">\rho</code>.</p>
</td></tr>
<tr><td><code id="set_compute_options_+3A_include_wcd">include_wcd</code></td>
<td>
<p>Logical indicating whether to store the within-cluster
distances computed during the Metropolis-Hastings algorithm. Defaults to
<code>FALSE</code>. Setting <code>include_wcd = TRUE</code> is useful when deciding the number of
mixture components to include, and is required by <code><a href="#topic+plot_elbow">plot_elbow()</a></code>.</p>
</td></tr>
<tr><td><code id="set_compute_options_+3A_save_aug">save_aug</code></td>
<td>
<p>Logical specifying whether or not to save the augmented
rankings every <code>aug_thinning</code>th iteration, for the case of missing data or
pairwise preferences. Defaults to <code>FALSE</code>. Saving augmented data is useful
for predicting the rankings each assessor would give to the items not yet
ranked, and is required by <code><a href="#topic+plot_top_k">plot_top_k()</a></code>.</p>
</td></tr>
<tr><td><code id="set_compute_options_+3A_save_ind_clus">save_ind_clus</code></td>
<td>
<p>Whether or not to save the individual cluster
probabilities in each step. This results in csv files <code>cluster_probs1.csv</code>,
<code>cluster_probs2.csv</code>, ..., being saved in the calling directory. This
option may slow down the code considerably, but is necessary for detecting
label switching using Stephen's algorithm.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"BayesMallowsComputeOptions"</code>, to be provided in
the <code>compute_options</code> argument to <code><a href="#topic+compute_mallows">compute_mallows()</a></code>,
<code><a href="#topic+compute_mallows_mixtures">compute_mallows_mixtures()</a></code>, or <code><a href="#topic+update_mallows">update_mallows()</a></code>.
</p>


<h3>References</h3>

<p>Crispino M, Arjas E, Vitelli V, Barrett N, Frigessi A (2019).
&ldquo;A Bayesian Mallows approach to nontransitive pair comparison data: How human are sounds?&rdquo;
<em>The Annals of Applied Statistics</em>, <b>13</b>(1), 492&ndash;519.
<a href="https://doi.org/10.1214/18-aoas1203">doi:10.1214/18-aoas1203</a>.<br /><br /> Stein A (2023).
<em>Sequential Inference with the Mallows Model</em>.
Ph.D. thesis, Lancaster University.<br /><br /> Vitelli V, Sørensen, Crispino M, Arjas E, Frigessi A (2018).
&ldquo;Probabilistic Preference Learning with the Mallows Rank Model.&rdquo;
<em>Journal of Machine Learning Research</em>, <b>18</b>(1), 1&ndash;49.
<a href="https://jmlr.org/papers/v18/15-481.html">https://jmlr.org/papers/v18/15-481.html</a>.
</p>


<h3>See Also</h3>

<p>Other preprocessing: 
<code><a href="#topic+get_transitive_closure">get_transitive_closure</a>()</code>,
<code><a href="#topic+set_initial_values">set_initial_values</a>()</code>,
<code><a href="#topic+set_model_options">set_model_options</a>()</code>,
<code><a href="#topic+set_priors">set_priors</a>()</code>,
<code><a href="#topic+set_smc_options">set_smc_options</a>()</code>,
<code><a href="#topic+setup_rank_data">setup_rank_data</a>()</code>
</p>

<hr>
<h2 id='set_initial_values'>Set initial values of scale parameter and modal ranking</h2><span id='topic+set_initial_values'></span>

<h3>Description</h3>

<p>Set initial values used by the Metropolis-Hastings algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_initial_values(rho_init = NULL, alpha_init = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_initial_values_+3A_rho_init">rho_init</code></td>
<td>
<p>Numeric vector specifying the initial value of the latent
consensus ranking <code class="reqn">\rho</code>. Defaults to NULL, which means that the
initial value is set randomly. If <code>rho_init</code> is provided when
<code>n_clusters &gt; 1</code>, each mixture component <code class="reqn">\rho_{c}</code> gets the same
initial value.</p>
</td></tr>
<tr><td><code id="set_initial_values_+3A_alpha_init">alpha_init</code></td>
<td>
<p>Numeric value specifying the initial value of the scale
parameter <code class="reqn">\alpha</code>. Defaults to <code>1</code>. When <code>n_clusters &gt; 1</code>,
each mixture component <code class="reqn">\alpha_{c}</code> gets the same initial value. When
chains are run in parallel, by providing an argument <code>cl = cl</code>, then
<code>alpha_init</code> can be a vector of of length <code>length(cl)</code>, each
element of which becomes an initial value for the given chain.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"BayesMallowsInitialValues"</code>, to be
provided to the <code>initial_values</code> argument of <code><a href="#topic+compute_mallows">compute_mallows()</a></code> or
<code><a href="#topic+compute_mallows_mixtures">compute_mallows_mixtures()</a></code>.
</p>


<h3>See Also</h3>

<p>Other preprocessing: 
<code><a href="#topic+get_transitive_closure">get_transitive_closure</a>()</code>,
<code><a href="#topic+set_compute_options">set_compute_options</a>()</code>,
<code><a href="#topic+set_model_options">set_model_options</a>()</code>,
<code><a href="#topic+set_priors">set_priors</a>()</code>,
<code><a href="#topic+set_smc_options">set_smc_options</a>()</code>,
<code><a href="#topic+setup_rank_data">setup_rank_data</a>()</code>
</p>

<hr>
<h2 id='set_model_options'>Set options for Bayesian Mallows model</h2><span id='topic+set_model_options'></span>

<h3>Description</h3>

<p>Specify various model options for the Bayesian Mallows model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_model_options(
  metric = c("footrule", "spearman", "cayley", "hamming", "kendall", "ulam"),
  n_clusters = 1,
  error_model = c("none", "bernoulli")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_model_options_+3A_metric">metric</code></td>
<td>
<p>A character string specifying the distance metric to use in the
Bayesian Mallows Model. Available options are <code>"footrule"</code>, <code>"spearman"</code>,
<code>"cayley"</code>, <code>"hamming"</code>, <code>"kendall"</code>, and <code>"ulam"</code>. The distance given by
<code>metric</code> is also used to compute within-cluster distances, when
<code>include_wcd = TRUE</code>.</p>
</td></tr>
<tr><td><code id="set_model_options_+3A_n_clusters">n_clusters</code></td>
<td>
<p>Integer specifying the number of clusters, i.e., the number
of mixture components to use. Defaults to <code>1L</code>, which means no clustering
is performed. See <code><a href="#topic+compute_mallows_mixtures">compute_mallows_mixtures()</a></code> for a convenience function
for computing several models with varying numbers of mixtures.</p>
</td></tr>
<tr><td><code id="set_model_options_+3A_error_model">error_model</code></td>
<td>
<p>Character string specifying which model to use for
inconsistent rankings. Defaults to <code>"none"</code>, which means that inconsistent
rankings are not allowed. At the moment, the only available other option is
<code>"bernoulli"</code>, which means that the Bernoulli error model is used. See
Crispino et al. (2019) for a definition of the
Bernoulli model.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"BayesMallowsModelOptions"</code>, to be provided in
the <code>model_options</code> argument to <code><a href="#topic+compute_mallows">compute_mallows()</a></code>,
<code><a href="#topic+compute_mallows_mixtures">compute_mallows_mixtures()</a></code>, or <code><a href="#topic+update_mallows">update_mallows()</a></code>.
</p>


<h3>References</h3>

<p>Crispino M, Arjas E, Vitelli V, Barrett N, Frigessi A (2019).
&ldquo;A Bayesian Mallows approach to nontransitive pair comparison data: How human are sounds?&rdquo;
<em>The Annals of Applied Statistics</em>, <b>13</b>(1), 492&ndash;519.
<a href="https://doi.org/10.1214/18-aoas1203">doi:10.1214/18-aoas1203</a>.
</p>


<h3>See Also</h3>

<p>Other preprocessing: 
<code><a href="#topic+get_transitive_closure">get_transitive_closure</a>()</code>,
<code><a href="#topic+set_compute_options">set_compute_options</a>()</code>,
<code><a href="#topic+set_initial_values">set_initial_values</a>()</code>,
<code><a href="#topic+set_priors">set_priors</a>()</code>,
<code><a href="#topic+set_smc_options">set_smc_options</a>()</code>,
<code><a href="#topic+setup_rank_data">setup_rank_data</a>()</code>
</p>

<hr>
<h2 id='set_priors'>Set prior parameters for Bayesian Mallows model</h2><span id='topic+set_priors'></span>

<h3>Description</h3>

<p>Set values related to the prior distributions for the Bayesian
Mallows model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_priors(gamma = 1, lambda = 0.001, psi = 10, kappa = c(1, 3))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_priors_+3A_gamma">gamma</code></td>
<td>
<p>Strictly positive numeric value specifying the shape parameter
of the gamma prior distribution of <code class="reqn">\alpha</code>. Defaults to <code>1</code>, thus
recovering the exponential prior distribution used by
(Vitelli et al. 2018).</p>
</td></tr>
<tr><td><code id="set_priors_+3A_lambda">lambda</code></td>
<td>
<p>Strictly positive numeric value specifying the rate parameter
of the gamma prior distribution of <code class="reqn">\alpha</code>. Defaults
to <code>0.001</code>. When <code>n_cluster &gt; 1</code>, each mixture component <code class="reqn">\alpha_{c}</code>
has the same prior distribution.</p>
</td></tr>
<tr><td><code id="set_priors_+3A_psi">psi</code></td>
<td>
<p>Positive integer specifying the concentration parameter <code class="reqn">\psi</code>
of the Dirichlet prior distribution used for the cluster probabilities
<code class="reqn">\tau_{1}, \tau_{2}, \dots, \tau_{C}</code>, where <code class="reqn">C</code> is the value of
<code>n_clusters</code>. Defaults to <code>10L</code>. When <code>n_clusters = 1</code>, this argument is
not used.</p>
</td></tr>
<tr><td><code id="set_priors_+3A_kappa">kappa</code></td>
<td>
<p>Hyperparameters of the truncated Beta prior used for error
probability <code class="reqn">\theta</code> in the Bernoulli error model. The prior has the
form <code class="reqn">\pi(\theta) = \theta^{\kappa_{1}} (1 - \theta)^{\kappa_{2}}</code>.
Defaults to <code>c(1, 3)</code>, which means that the <code class="reqn">\theta</code> is a priori
expected to be closer to zero than to 0.5. See
(Crispino et al. 2019) for details.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"BayesMallowsPriors"</code>, to be provided in the
<code>priors</code> argument to <code><a href="#topic+compute_mallows">compute_mallows()</a></code>, <code><a href="#topic+compute_mallows_mixtures">compute_mallows_mixtures()</a></code>, or
<code><a href="#topic+update_mallows">update_mallows()</a></code>.
</p>


<h3>References</h3>

<p>Crispino M, Arjas E, Vitelli V, Barrett N, Frigessi A (2019).
&ldquo;A Bayesian Mallows approach to nontransitive pair comparison data: How human are sounds?&rdquo;
<em>The Annals of Applied Statistics</em>, <b>13</b>(1), 492&ndash;519.
<a href="https://doi.org/10.1214/18-aoas1203">doi:10.1214/18-aoas1203</a>.<br /><br /> Vitelli V, Sørensen, Crispino M, Arjas E, Frigessi A (2018).
&ldquo;Probabilistic Preference Learning with the Mallows Rank Model.&rdquo;
<em>Journal of Machine Learning Research</em>, <b>18</b>(1), 1&ndash;49.
<a href="https://jmlr.org/papers/v18/15-481.html">https://jmlr.org/papers/v18/15-481.html</a>.
</p>


<h3>See Also</h3>

<p>Other preprocessing: 
<code><a href="#topic+get_transitive_closure">get_transitive_closure</a>()</code>,
<code><a href="#topic+set_compute_options">set_compute_options</a>()</code>,
<code><a href="#topic+set_initial_values">set_initial_values</a>()</code>,
<code><a href="#topic+set_model_options">set_model_options</a>()</code>,
<code><a href="#topic+set_smc_options">set_smc_options</a>()</code>,
<code><a href="#topic+setup_rank_data">setup_rank_data</a>()</code>
</p>

<hr>
<h2 id='set_smc_options'>Set SMC compute options</h2><span id='topic+set_smc_options'></span>

<h3>Description</h3>

<p>Sets the SMC compute options to be used in
<code><a href="#topic+update_mallows.BayesMallows">update_mallows.BayesMallows()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_smc_options(
  n_particles = 1000,
  mcmc_steps = 5,
  resampler = c("stratified", "systematic", "residual", "multinomial"),
  latent_sampling_lag = NA_integer_
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_smc_options_+3A_n_particles">n_particles</code></td>
<td>
<p>Integer specifying the number of particles.</p>
</td></tr>
<tr><td><code id="set_smc_options_+3A_mcmc_steps">mcmc_steps</code></td>
<td>
<p>Number of MCMC steps to be applied in the resample-move
step.</p>
</td></tr>
<tr><td><code id="set_smc_options_+3A_resampler">resampler</code></td>
<td>
<p>Character string defining the resampling method to use. One
of &quot;stratified&quot;, &quot;systematic&quot;, &quot;residual&quot;, and &quot;multinomial&quot;. Defaults to
&quot;stratified&quot;. While multinomial resampling was used in
Stein (2023),
stratified, systematic, or residual resampling typically give lower Monte
Carlo error (Douc and Cappe 2005; Hol et al. 2006; Naesseth et al. 2019).</p>
</td></tr>
<tr><td><code id="set_smc_options_+3A_latent_sampling_lag">latent_sampling_lag</code></td>
<td>
<p>Parameter specifying the number of timesteps to go
back when resampling the latent ranks in the move step. See Section 6.2.3
of (Kantas et al. 2015) for details. The <code class="reqn">L</code> in their
notation corresponds to <code>latent_sampling_lag</code>. See more under Details.
Defaults to <code>NA</code>, which means that all latent ranks from previous timesteps
are moved. If set to <code>0</code>, no move step is applied to the latent ranks.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class &quot;SMCOptions&quot;.
</p>


<h3>Lag parameter in move step</h3>

<p>The parameter <code>latent_sampling_lag</code> corresponds to <code class="reqn">L</code> in
(Kantas et al. 2015). Its use in this package is can be
explained in terms of Algorithm 12 in
(Stein 2023). The
relevant line of the algorithm is:
</p>
<p><strong>for</strong> <code class="reqn">j = 1 : M_{t}</code> <strong>do</strong> <br />
<strong>M-H step:</strong> update <code class="reqn">\tilde{\mathbf{R}}_{j}^{(i)}</code> with proposal
<code class="reqn">\tilde{\mathbf{R}}_{j}' \sim q(\tilde{\mathbf{R}}_{j}^{(i)} |
  \mathbf{R}_{j}, \boldsymbol{\rho}_{t}^{(i)}, \alpha_{t}^{(i)})</code>.<br />
<strong>end</strong>
</p>
<p>Let <code class="reqn">L</code> denote the value of <code>latent_sampling_lag</code>. With this parameter,
we modify for algorithm so it becomes
</p>
<p><strong>for</strong> <code class="reqn">j = M_{t-L+1} : M_{t}</code> <strong>do</strong> <br />
<strong>M-H step:</strong> update <code class="reqn">\tilde{\mathbf{R}}_{j}^{(i)}</code> with proposal
<code class="reqn">\tilde{\mathbf{R}}_{j}' \sim q(\tilde{\mathbf{R}}_{j}^{(i)} |
  \mathbf{R}_{j}, \boldsymbol{\rho}_{t}^{(i)}, \alpha_{t}^{(i)})</code>.<br />
<strong>end</strong>
</p>
<p>This means that with <code class="reqn">L=0</code> no move step is performed on any latent
ranks, whereas <code class="reqn">L=1</code> means that the move step is only applied to the
parameters entering at the given timestep. The default,
<code>latent_sampling_lag = NA</code> means that <code class="reqn">L=t</code> at each timestep, and hence
all latent ranks are part of the move step at each timestep.
</p>


<h3>References</h3>

<p>Douc R, Cappe O (2005).
&ldquo;Comparison of resampling schemes for particle filtering.&rdquo;
In <em>ISPA 2005. Proceedings of the 4th International Symposium on Image and Signal Processing and Analysis,  2005.</em>.
<a href="https://doi.org/10.1109/ispa.2005.195385">doi:10.1109/ispa.2005.195385</a>, <a href="http://dx.doi.org/10.1109/ISPA.2005.195385">http://dx.doi.org/10.1109/ISPA.2005.195385</a>.<br /><br /> Hol JD, Schon TB, Gustafsson F (2006).
&ldquo;On Resampling Algorithms for Particle Filters.&rdquo;
In <em>2006 IEEE Nonlinear Statistical Signal Processing Workshop</em>.
<a href="https://doi.org/10.1109/nsspw.2006.4378824">doi:10.1109/nsspw.2006.4378824</a>, <a href="http://dx.doi.org/10.1109/NSSPW.2006.4378824">http://dx.doi.org/10.1109/NSSPW.2006.4378824</a>.<br /><br /> Kantas N, Doucet A, Singh SS, Maciejowski J, Chopin N (2015).
&ldquo;On Particle Methods for Parameter Estimation in State-Space Models.&rdquo;
<em>Statistical Science</em>, <b>30</b>(3).
ISSN 0883-4237, <a href="https://doi.org/10.1214/14-sts511">doi:10.1214/14-sts511</a>, <a href="http://dx.doi.org/10.1214/14-STS511">http://dx.doi.org/10.1214/14-STS511</a>.<br /><br /> Naesseth CA, Lindsten F, Schön TB (2019).
&ldquo;Elements of Sequential Monte Carlo.&rdquo;
<em>Foundations and Trends® in Machine Learning</em>, <b>12</b>(3), 187–306.
ISSN 1935-8245, <a href="https://doi.org/10.1561/2200000074">doi:10.1561/2200000074</a>, <a href="http://dx.doi.org/10.1561/2200000074">http://dx.doi.org/10.1561/2200000074</a>.<br /><br /> Stein A (2023).
<em>Sequential Inference with the Mallows Model</em>.
Ph.D. thesis, Lancaster University.
</p>


<h3>See Also</h3>

<p>Other preprocessing: 
<code><a href="#topic+get_transitive_closure">get_transitive_closure</a>()</code>,
<code><a href="#topic+set_compute_options">set_compute_options</a>()</code>,
<code><a href="#topic+set_initial_values">set_initial_values</a>()</code>,
<code><a href="#topic+set_model_options">set_model_options</a>()</code>,
<code><a href="#topic+set_priors">set_priors</a>()</code>,
<code><a href="#topic+setup_rank_data">setup_rank_data</a>()</code>
</p>

<hr>
<h2 id='setup_rank_data'>Setup rank data</h2><span id='topic+setup_rank_data'></span>

<h3>Description</h3>

<p>Prepare rank or preference data for further analyses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setup_rank_data(
  rankings = NULL,
  preferences = NULL,
  user_ids = numeric(),
  observation_frequency = NULL,
  validate_rankings = TRUE,
  na_action = c("augment", "fail", "omit"),
  cl = NULL,
  shuffle_unranked = FALSE,
  random = FALSE,
  random_limit = 8L,
  timepoint = NULL,
  n_items = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="setup_rank_data_+3A_rankings">rankings</code></td>
<td>
<p>A matrix of ranked items, of size <code style="white-space: pre;">&#8288;n_assessors x n_items&#8288;</code>.
See <code><a href="#topic+create_ranking">create_ranking()</a></code> if you have an ordered set of items that need to be
converted to rankings. If <code>preferences</code> is provided, <code>rankings</code> is an
optional initial value of the rankings. If <code>rankings</code> has column names,
these are assumed to be the names of the items. <code>NA</code> values in rankings are
treated as missing data and automatically augmented; to change this
behavior, see the <code>na_action</code> argument to <code><a href="#topic+set_model_options">set_model_options()</a></code>. A vector
length <code>n_items</code> is silently converted to a matrix of length <code style="white-space: pre;">&#8288;1 x n_items&#8288;</code>,
and names (if any), are used as column names.</p>
</td></tr>
<tr><td><code id="setup_rank_data_+3A_preferences">preferences</code></td>
<td>
<p>A data frame with one row per pairwise comparison, and
columns <code>assessor</code>, <code>top_item</code>, and <code>bottom_item</code>. Each column contains the
following:
</p>

<ul>
<li> <p><code>assessor</code> is a numeric vector containing the assessor index.
</p>
</li>
<li> <p><code>bottom_item</code> is a numeric vector containing the index of the item that
was disfavored in each pairwise comparison.
</p>
</li>
<li> <p><code>top_item</code> is a numeric vector containing the index of the item that was
preferred in each pairwise comparison.
</p>
</li></ul>

<p>So if we have two assessors and five items, and assessor 1 prefers item 1
to item 2 and item 1 to item 5, while assessor 2 prefers item 3 to item 5,
we have the following <code>df</code>:
</p>

<table>
<tr>
 <td style="text-align: right;">
<strong>assessor</strong> </td><td style="text-align: right;"> <strong>bottom_item</strong> </td><td style="text-align: right;"> <strong>top_item</strong></td>
</tr>
<tr>
 <td style="text-align: right;">
1 </td><td style="text-align: right;"> 2 </td><td style="text-align: right;"> 1</td>
</tr>
<tr>
 <td style="text-align: right;">
1 </td><td style="text-align: right;"> 5 </td><td style="text-align: right;"> 1</td>
</tr>
<tr>
 <td style="text-align: right;">
2 </td><td style="text-align: right;"> 5 </td><td style="text-align: right;"> 3</td>
</tr>
<tr>
 <td style="text-align: right;">
</td>
</tr>

</table>
</td></tr>
<tr><td><code id="setup_rank_data_+3A_user_ids">user_ids</code></td>
<td>
<p>Optional <code>numeric</code> vector of user IDs. Only only used by
<code><a href="#topic+update_mallows">update_mallows()</a></code>. If provided, new data can consist of updated partial
rankings from users already in the dataset, as described in Section 6 of
Stein (2023).</p>
</td></tr>
<tr><td><code id="setup_rank_data_+3A_observation_frequency">observation_frequency</code></td>
<td>
<p>A vector of observation frequencies (weights) to
apply do each row in <code>rankings</code>. This can speed up computation if a large
number of assessors share the same rank pattern. Defaults to <code>NULL</code>, which
means that each row of <code>rankings</code> is multiplied by 1. If provided,
<code>observation_frequency</code> must have the same number of elements as there are
rows in <code>rankings</code>, and <code>rankings</code> cannot be <code>NULL</code>. See
<code><a href="#topic+compute_observation_frequency">compute_observation_frequency()</a></code> for a convenience function for computing
it.</p>
</td></tr>
<tr><td><code id="setup_rank_data_+3A_validate_rankings">validate_rankings</code></td>
<td>
<p>Logical specifying whether the rankings provided (or
generated from <code>preferences</code>) should be validated. Defaults to <code>TRUE</code>.
Turning off this check will reduce computing time with a large number of
items or assessors.</p>
</td></tr>
<tr><td><code id="setup_rank_data_+3A_na_action">na_action</code></td>
<td>
<p>Character specifying how to deal with <code>NA</code> values in the
<code>rankings</code> matrix, if provided. Defaults to <code>"augment"</code>, which means that
missing values are automatically filled in using the Bayesian data
augmentation scheme described in
Vitelli et al. (2018). The other options for this
argument are <code>"fail"</code>, which means that an error message is printed and the
algorithm stops if there are <code>NA</code>s in <code>rankings</code>, and <code>"omit"</code> which simply
deletes rows with <code>NA</code>s in them.</p>
</td></tr>
<tr><td><code id="setup_rank_data_+3A_cl">cl</code></td>
<td>
<p>Optional computing cluster used for parallelization when generating
transitive closure based on preferences, returned from
<code><a href="parallel.html#topic+makeCluster">parallel::makeCluster()</a></code>. Defaults to <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="setup_rank_data_+3A_shuffle_unranked">shuffle_unranked</code></td>
<td>
<p>Logical specifying whether or not to randomly permute
unranked items in the initial ranking. When <code>shuffle_unranked=TRUE</code> and
<code>random=FALSE</code>, all unranked items for each assessor are randomly permuted.
Otherwise, the first ordering returned by <code>igraph::topo_sort()</code> is
returned.</p>
</td></tr>
<tr><td><code id="setup_rank_data_+3A_random">random</code></td>
<td>
<p>Logical specifying whether or not to use a random initial
ranking. Defaults to <code>FALSE</code>. Setting this to <code>TRUE</code> means that all
possible orderings consistent with the stated pairwise preferences are
generated for each assessor, and one of them is picked at random.</p>
</td></tr>
<tr><td><code id="setup_rank_data_+3A_random_limit">random_limit</code></td>
<td>
<p>Integer specifying the maximum number of items allowed
when all possible orderings are computed, i.e., when <code>random=TRUE</code>.
Defaults to <code>8L</code>.</p>
</td></tr>
<tr><td><code id="setup_rank_data_+3A_timepoint">timepoint</code></td>
<td>
<p>Integer vector specifying the timepoint. Defaults to <code>NULL</code>,
which means that a vector of ones, one for each observation, is generated.
Used by <code><a href="#topic+update_mallows">update_mallows()</a></code> to identify data with a given iteration of the
sequential Monte Carlo algorithm. If not <code>NULL</code>, must contain one integer
for each row in <code>rankings</code>.</p>
</td></tr>
<tr><td><code id="setup_rank_data_+3A_n_items">n_items</code></td>
<td>
<p>Integer specifying the number of items. Defaults to <code>NULL</code>,
which means that the number of items is inferred from <code>rankings</code> or from
<code>preferences</code>. Setting <code>n_items</code> manually can be useful with pairwise
preference data in the SMC algorithm, i.e., when <code>rankings</code> is <code>NULL</code> and
<code>preferences</code> is non-<code>NULL</code>, and contains a small number of pairwise
preferences for a subset of users and items.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>"BayesMallowsData"</code>, to be provided in the <code>data</code>
argument to <code><a href="#topic+compute_mallows">compute_mallows()</a></code>.
</p>


<h3>Note</h3>

<p>Setting <code>random=TRUE</code> means that all possible orderings of each
assessor's preferences are generated, and one of them is picked at random.
This can be useful when experiencing convergence issues, e.g., if the MCMC
algorithm does not mix properly. However, finding all possible orderings is
a combinatorial problem, which may be computationally very hard. The result
may not even be possible to fit in memory, which may cause the R session to
crash. When using this option, please try to increase the size of the
problem incrementally, by starting with smaller subsets of the complete
data. An example is given below.
</p>
<p>It is assumed that the items are labeled starting from 1. For example, if a
single comparison of the following form is provided, it is assumed that
there is a total of 30 items (<code>n_items=30</code>), and the initial ranking is a
permutation of these 30 items consistent with the preference 29&lt;30.
</p>

<table>
<tr>
 <td style="text-align: right;">
<strong>assessor</strong> </td><td style="text-align: right;"> <strong>bottom_item</strong> </td><td style="text-align: right;"> <strong>top_item</strong></td>
</tr>
<tr>
 <td style="text-align: right;">
1 </td><td style="text-align: right;"> 29 </td><td style="text-align: right;"> 30</td>
</tr>
<tr>
 <td style="text-align: right;">
</td>
</tr>

</table>

<p>If in reality there are only two items, they should be relabeled to 1 and
2, as follows:
</p>

<table>
<tr>
 <td style="text-align: right;">
<strong>assessor</strong> </td><td style="text-align: right;"> <strong>bottom_item</strong> </td><td style="text-align: right;"> <strong>top_item</strong></td>
</tr>
<tr>
 <td style="text-align: right;">
1 </td><td style="text-align: right;"> 1 </td><td style="text-align: right;"> 2</td>
</tr>
<tr>
 <td style="text-align: right;">
</td>
</tr>

</table>



<h3>References</h3>

<p>Stein A (2023).
<em>Sequential Inference with the Mallows Model</em>.
Ph.D. thesis, Lancaster University.<br /><br /> Vitelli V, Sørensen, Crispino M, Arjas E, Frigessi A (2018).
&ldquo;Probabilistic Preference Learning with the Mallows Rank Model.&rdquo;
<em>Journal of Machine Learning Research</em>, <b>18</b>(1), 1&ndash;49.
<a href="https://jmlr.org/papers/v18/15-481.html">https://jmlr.org/papers/v18/15-481.html</a>.
</p>


<h3>See Also</h3>

<p>Other preprocessing: 
<code><a href="#topic+get_transitive_closure">get_transitive_closure</a>()</code>,
<code><a href="#topic+set_compute_options">set_compute_options</a>()</code>,
<code><a href="#topic+set_initial_values">set_initial_values</a>()</code>,
<code><a href="#topic+set_model_options">set_model_options</a>()</code>,
<code><a href="#topic+set_priors">set_priors</a>()</code>,
<code><a href="#topic+set_smc_options">set_smc_options</a>()</code>
</p>

<hr>
<h2 id='sushi_rankings'>Sushi rankings</h2><span id='topic+sushi_rankings'></span>

<h3>Description</h3>

<p>Complete rankings of 10 types of sushi from 5000 assessors
(Kamishima 2003).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sushi_rankings
</code></pre>


<h3>Format</h3>

<p>An object of class <code>matrix</code> (inherits from <code>array</code>) with 5000 rows and 10 columns.
</p>


<h3>References</h3>

<p>Kamishima T (2003).
&ldquo;Nantonac Collaborative Filtering: Recommendation Based on Order Responses.&rdquo;
In <em>Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 583&ndash;588.
</p>


<h3>See Also</h3>

<p>Other datasets: 
<code><a href="#topic+beach_preferences">beach_preferences</a></code>,
<code><a href="#topic+bernoulli_data">bernoulli_data</a></code>,
<code><a href="#topic+cluster_data">cluster_data</a></code>,
<code><a href="#topic+potato_true_ranking">potato_true_ranking</a></code>,
<code><a href="#topic+potato_visual">potato_visual</a></code>,
<code><a href="#topic+potato_weighing">potato_weighing</a></code>
</p>

<hr>
<h2 id='update_mallows'>Update a Bayesian Mallows model with new users</h2><span id='topic+update_mallows'></span><span id='topic+update_mallows.BayesMallowsPriorSamples'></span><span id='topic+update_mallows.BayesMallows'></span><span id='topic+update_mallows.SMCMallows'></span>

<h3>Description</h3>

<p>Update a Bayesian Mallows model estimated using the Metropolis-Hastings
algorithm in <code><a href="#topic+compute_mallows">compute_mallows()</a></code> using the sequential Monte Carlo algorithm
described in
Stein (2023).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>update_mallows(model, new_data, ...)

## S3 method for class 'BayesMallowsPriorSamples'
update_mallows(
  model,
  new_data,
  model_options = set_model_options(),
  smc_options = set_smc_options(),
  compute_options = set_compute_options(),
  priors = model$priors,
  pfun_estimate = NULL,
  ...
)

## S3 method for class 'BayesMallows'
update_mallows(
  model,
  new_data,
  model_options = set_model_options(),
  smc_options = set_smc_options(),
  compute_options = set_compute_options(),
  priors = model$priors,
  ...
)

## S3 method for class 'SMCMallows'
update_mallows(model, new_data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="update_mallows_+3A_model">model</code></td>
<td>
<p>A model object of class &quot;BayesMallows&quot; returned from
<code><a href="#topic+compute_mallows">compute_mallows()</a></code>, an object of class &quot;SMCMallows&quot; returned from this
function, or an object of class &quot;BayesMallowsPriorSamples&quot; returned from
<code><a href="#topic+sample_prior">sample_prior()</a></code>.</p>
</td></tr>
<tr><td><code id="update_mallows_+3A_new_data">new_data</code></td>
<td>
<p>An object of class &quot;BayesMallowsData&quot; returned from
<code><a href="#topic+setup_rank_data">setup_rank_data()</a></code>. The object should contain the new data being provided.</p>
</td></tr>
<tr><td><code id="update_mallows_+3A_...">...</code></td>
<td>
<p>Optional arguments. Currently not used.</p>
</td></tr>
<tr><td><code id="update_mallows_+3A_model_options">model_options</code></td>
<td>
<p>An object of class &quot;BayesMallowsModelOptions&quot; returned
from <code><a href="#topic+set_model_options">set_model_options()</a></code>.</p>
</td></tr>
<tr><td><code id="update_mallows_+3A_smc_options">smc_options</code></td>
<td>
<p>An object of class &quot;SMCOptions&quot; returned from
<code><a href="#topic+set_smc_options">set_smc_options()</a></code>.</p>
</td></tr>
<tr><td><code id="update_mallows_+3A_compute_options">compute_options</code></td>
<td>
<p>An object of class &quot;BayesMallowsComputeOptions&quot;
returned from <code><a href="#topic+set_compute_options">set_compute_options()</a></code>.</p>
</td></tr>
<tr><td><code id="update_mallows_+3A_priors">priors</code></td>
<td>
<p>An object of class &quot;BayesMallowsPriors&quot; returned from
<code><a href="#topic+set_priors">set_priors()</a></code>. Defaults to the priors used in <code>model</code>.</p>
</td></tr>
<tr><td><code id="update_mallows_+3A_pfun_estimate">pfun_estimate</code></td>
<td>
<p>Object returned from <code><a href="#topic+estimate_partition_function">estimate_partition_function()</a></code>.
Defaults to <code>NULL</code>, and will only be used for footrule, Spearman, or
Ulam distances when the cardinalities are not available, cf.
<code><a href="#topic+get_cardinalities">get_cardinalities()</a></code>. Only used by the specialization for objects of type
&quot;BayesMallowsPriorSamples&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An updated model, of class &quot;SMCMallows&quot;.
</p>


<h3>See Also</h3>

<p>Other modeling: 
<code><a href="#topic+burnin">burnin</a>()</code>,
<code><a href="#topic+burnin+3C-">burnin&lt;-</a>()</code>,
<code><a href="#topic+compute_mallows">compute_mallows</a>()</code>,
<code><a href="#topic+compute_mallows_mixtures">compute_mallows_mixtures</a>()</code>,
<code><a href="#topic+compute_mallows_sequentially">compute_mallows_sequentially</a>()</code>,
<code><a href="#topic+sample_prior">sample_prior</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
# UPDATING A MALLOWS MODEL WITH NEW COMPLETE RANKINGS
# Assume we first only observe the first four rankings in the potato_visual
# dataset
data_first_batch &lt;- potato_visual[1:4, ]

# We start by fitting a model using Metropolis-Hastings
mod_init &lt;- compute_mallows(
  data = setup_rank_data(data_first_batch),
  compute_options = set_compute_options(nmc = 10000))

# Convergence seems good after no more than 2000 iterations
assess_convergence(mod_init)
burnin(mod_init) &lt;- 2000

# Next, assume we receive four more observations
data_second_batch &lt;- potato_visual[5:8, ]

# We can now update the model using sequential Monte Carlo
mod_second &lt;- update_mallows(
  model = mod_init,
  new_data = setup_rank_data(rankings = data_second_batch),
  smc_options = set_smc_options(resampler = "systematic")
  )

# This model now has a collection of particles approximating the posterior
# distribution after the first and second batch
# We can use all the posterior summary functions as we do for the model
# based on compute_mallows():
plot(mod_second)
plot(mod_second, parameter = "rho", items = 1:4)
compute_posterior_intervals(mod_second)

# Next, assume we receive the third and final batch of data. We can update
# the model again
data_third_batch &lt;- potato_visual[9:12, ]
mod_final &lt;- update_mallows(
  model = mod_second, new_data = setup_rank_data(rankings = data_third_batch))

# We can plot the same things as before
plot(mod_final)
compute_consensus(mod_final)

# UPDATING A MALLOWS MODEL WITH NEW OR UPDATED PARTIAL RANKINGS
# The sequential Monte Carlo algorithm works for data with missing ranks as
# well. This both includes the case where new users arrive with partial ranks,
# and when previously seen users arrive with more complete data than they had
# previously.
# We illustrate for top-k rankings of the first 10 users in potato_visual
potato_top_10 &lt;- ifelse(potato_visual[1:10, ] &gt; 10, NA_real_,
                        potato_visual[1:10, ])
potato_top_12 &lt;- ifelse(potato_visual[1:10, ] &gt; 12, NA_real_,
                        potato_visual[1:10, ])
potato_top_14 &lt;- ifelse(potato_visual[1:10, ] &gt; 14, NA_real_,
                        potato_visual[1:10, ])

# We need the rownames as user IDs
(user_ids &lt;- 1:10)

# First, users provide top-10 rankings
mod_init &lt;- compute_mallows(
  data = setup_rank_data(rankings = potato_top_10, user_ids = user_ids),
  compute_options = set_compute_options(nmc = 10000))

# Convergence seems fine. We set the burnin to 2000.
assess_convergence(mod_init)
burnin(mod_init) &lt;- 2000

# Next assume the users update their rankings, so we have top-12 instead.
mod1 &lt;- update_mallows(
  model = mod_init,
  new_data = setup_rank_data(rankings = potato_top_12, user_ids = user_ids),
  smc_options = set_smc_options(resampler = "stratified")
)

plot(mod1)

# Then, assume we get even more data, this time top-14 rankings:
mod2 &lt;- update_mallows(
  model = mod1,
  new_data = setup_rank_data(rankings = potato_top_14, user_ids = user_ids)
)

plot(mod2)

# Finally, assume a set of new users arrive, who have complete rankings.
potato_new &lt;- potato_visual[11:12, ]
# We need to update the user IDs, to show that these users are different
(user_ids &lt;- 11:12)

mod_final &lt;- update_mallows(
  model = mod2,
  new_data = setup_rank_data(rankings = potato_new, user_ids = user_ids)
)

plot(mod_final)

# We can also update models with pairwise preferences
# We here start by running MCMC on the first 20 assessors of the beach data
# A realistic application should run a larger number of iterations than we
# do in this example.
set.seed(3)
dat &lt;- subset(beach_preferences, assessor &lt;= 20)
mod &lt;- compute_mallows(
  data = setup_rank_data(
    preferences = beach_preferences),
  compute_options = set_compute_options(nmc = 3000, burnin = 1000)
)

# Next we provide assessors 21 to 24 one at a time.
for(i in 21:24){
  mod &lt;- update_mallows(
    model = mod,
    new_data = setup_rank_data(
      preferences = subset(beach_preferences, assessor == i),
      user_ids = i, shuffle_unranked = TRUE),
    smc_options = set_smc_options(latent_sampling_lag = 0)
  )
}

# Compared to running full MCMC, there is a downward bias in the scale
# parameter. This can be alleviated by increasing the number of particles,
# MCMC steps, and the latent sampling lag.
plot(mod)
compute_consensus(mod)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
