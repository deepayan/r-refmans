<!DOCTYPE html><html lang="en"><head><title>Help for package kldest</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {kldest}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#kldest-package'><p>kldest: Sample-Based Estimation of Kullback-Leibler Divergence</p></a></li>
<li><a href='#combinations'><p>Combinations of input arguments</p></a></li>
<li><a href='#constDiagMatrix'><p>Constant plus diagonal matrix</p></a></li>
<li><a href='#convergence_rate'><p>Empirical convergence rate of a KL divergence estimator</p></a></li>
<li><a href='#is_two_sample'><p>Detect if a one- or two-sample problem is specified</p></a></li>
<li><a href='#kld_ci_bootstrap'><p>Uncertainty of KL divergence estimate using Efron's bootstrap.</p></a></li>
<li><a href='#kld_ci_subsampling'><p>Uncertainty of KL divergence estimate using Politis/Romano's subsampling bootstrap.</p></a></li>
<li><a href='#kld_discrete'><p>Analytical KL divergence for two discrete distributions</p></a></li>
<li><a href='#kld_est'><p>Kullback-Leibler divergence estimator for discrete, continuous or mixed data.</p></a></li>
<li><a href='#kld_est_brnn'><p>Bias-reduced generalized k-nearest-neighbour KL divergence estimation</p></a></li>
<li><a href='#kld_est_discrete'><p>Plug-in KL divergence estimator for samples from discrete distributions</p></a></li>
<li><a href='#kld_est_kde'><p>Kernel density-based Kullback-Leibler divergence estimation in any dimension</p></a></li>
<li><a href='#kld_est_kde1'><p>1-D kernel density-based estimation of Kullback-Leibler divergence</p></a></li>
<li><a href='#kld_est_kde2'><p>2-D kernel density-based estimation of Kullback-Leibler divergence</p></a></li>
<li><a href='#kld_est_nn'><p>k-nearest neighbour KL divergence estimator</p></a></li>
<li><a href='#kld_exponential'><p>Analytical KL divergence for two univariate exponential distributions</p></a></li>
<li><a href='#kld_gaussian'><p>Analytical KL divergence for two uni- or multivariate Gaussian distributions</p></a></li>
<li><a href='#kld_uniform'><p>Analytical KL divergence for two uniform distributions</p></a></li>
<li><a href='#kld_uniform_gaussian'><p>Analytical KL divergence between a uniform and a Gaussian distribution</p></a></li>
<li><a href='#mvdnorm'><p>Probability density function of multivariate Gaussian distribution</p></a></li>
<li><a href='#to_uniform_scale'><p>Transform samples to uniform scale</p></a></li>
<li><a href='#tr'><p>Matrix trace operator</p></a></li>
<li><a href='#trapz'><p>Trapezoidal integration in 1 or 2 dimensions</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Sample-Based Estimation of Kullback-Leibler Divergence</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Niklas Hartung &lt;niklas.hartung@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Estimation algorithms for Kullback-Leibler divergence between two probability
    distributions, based on one or two samples, and including uncertainty quantification.
    Distributions can be uni- or multivariate and continuous, discrete or mixed.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, RANN</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, KernSmooth, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Config/Needs/website:</td>
<td>ggplot2, reshape2, MASS</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://niklhart.github.io/kldest/">https://niklhart.github.io/kldest/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/niklhart/kldest/issues">https://github.com/niklhart/kldest/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-04-08 13:15:50 UTC; niklashartung</td>
</tr>
<tr>
<td>Author:</td>
<td>Niklas Hartung <a href="https://orcid.org/0000-0002-4000-6525"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre, cph]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-04-09 08:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='kldest-package'>kldest: Sample-Based Estimation of Kullback-Leibler Divergence</h2><span id='topic+kldest'></span><span id='topic+kldest-package'></span>

<h3>Description</h3>

<p>Estimation algorithms for Kullback-Leibler divergence between two probability distributions, based on one or two samples, and including uncertainty quantification. Distributions can be uni- or multivariate and continuous, discrete or mixed.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Niklas Hartung <a href="mailto:niklas.hartung@gmail.com">niklas.hartung@gmail.com</a> (<a href="https://orcid.org/0000-0002-4000-6525">ORCID</a>) [copyright holder]
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://niklhart.github.io/kldest/">https://niklhart.github.io/kldest/</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/niklhart/kldest/issues">https://github.com/niklhart/kldest/issues</a>
</p>
</li></ul>


<hr>
<h2 id='combinations'>Combinations of input arguments</h2><span id='topic+combinations'></span>

<h3>Description</h3>

<p>Combinations of input arguments
</p>


<h3>Usage</h3>

<pre><code class='language-R'>combinations(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="combinations_+3A_...">...</code></td>
<td>
<p>Any number of atomic vectors.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with columns named as the inputs, containing all input
combinations.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>combinations(a = 1:2, b = letters[1:3], c = LETTERS[1:2])
</code></pre>

<hr>
<h2 id='constDiagMatrix'>Constant plus diagonal matrix</h2><span id='topic+constDiagMatrix'></span>

<h3>Description</h3>

<p>Specify a matrix with constant values on the diagonal and on the off-diagonals.
Such matrices can be used to vary the degree of dependency in covariate matrices,
for example when evaluating accuracy of KL-divergence estimation algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>constDiagMatrix(dim = 1, diag = 1, offDiag = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="constDiagMatrix_+3A_dim">dim</code></td>
<td>
<p>Dimension</p>
</td></tr>
<tr><td><code id="constDiagMatrix_+3A_diag">diag</code></td>
<td>
<p>Value at the diagonal</p>
</td></tr>
<tr><td><code id="constDiagMatrix_+3A_offdiag">offDiag</code></td>
<td>
<p>Value at off-diagonals</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>dim</code>-by-<code>dim</code> matrix
</p>


<h3>Examples</h3>

<pre><code class='language-R'>constDiagMatrix(dim = 3, diag = 1, offDiag = 0.9)

</code></pre>

<hr>
<h2 id='convergence_rate'>Empirical convergence rate of a KL divergence estimator</h2><span id='topic+convergence_rate'></span>

<h3>Description</h3>

<p>Subsampling-based confidence intervals computed by <code>kld_ci_subsampling()</code>
require the convergence rate of the KL divergence estimator as an input. The
default rate of <code>0.5</code> assumes that the variance term dominates the bias term.
For high-dimensional problems, depending on the data, the convergence rate
might be lower. This function allows to empirically derive the convergence
rate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>convergence_rate(
  estimator,
  X,
  Y = NULL,
  q = NULL,
  n.sizes = 4,
  spacing.factor = 1.5,
  typical.subsample = function(n) sqrt(n),
  B = 500L,
  plot = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="convergence_rate_+3A_estimator">estimator</code></td>
<td>
<p>A KL divergence estimator.</p>
</td></tr>
<tr><td><code id="convergence_rate_+3A_x">X</code>, <code id="convergence_rate_+3A_y">Y</code></td>
<td>
<p><code>n</code>-by-<code>d</code> and <code>m</code>-by-<code>d</code> data frames or matrices (multivariate
samples), or numeric/character vectors (univariate samples, i.e. <code>d = 1</code>),
representing <code>n</code> samples from the true distribution <code class="reqn">P</code> and <code>m</code>
samples from the approximate distribution <code class="reqn">Q</code> in <code>d</code> dimensions.
<code>Y</code> can be left blank if <code>q</code> is specified (see below).</p>
</td></tr>
<tr><td><code id="convergence_rate_+3A_q">q</code></td>
<td>
<p>The density function of the approximate distribution <code class="reqn">Q</code>. Either
<code>Y</code> or <code>q</code> must be specified. If the distributions are all continuous or
all discrete, <code>q</code> can be directly specified as the probability density/mass
function. However, for mixed continuous/discrete distributions, <code>q</code> must
be given in decomposed form, <code class="reqn">q(y_c,y_d)=q_{c|d}(y_c|y_d)q_d(y_d)</code>,
specified as a named list with field <code>cond</code> for the conditional density
<code class="reqn">q_{c|d}(y_c|y_d)</code> (a function that expects two arguments <code>y_c</code> and
<code>y_d</code>) and <code>disc</code> for the discrete marginal density <code class="reqn">q_d(y_d)</code> (a
function that expects one argument <code>y_d</code>). If such a decomposition is not
available, it may be preferable to instead simulate a large sample from
<code class="reqn">Q</code> and use the two-sample syntax.</p>
</td></tr>
<tr><td><code id="convergence_rate_+3A_n.sizes">n.sizes</code></td>
<td>
<p>Number of different subsample sizes to use (default: <code>4</code>).</p>
</td></tr>
<tr><td><code id="convergence_rate_+3A_spacing.factor">spacing.factor</code></td>
<td>
<p>Multiplicative factor controlling the spacing of sample
sizes (default: <code>1.5</code>).</p>
</td></tr>
<tr><td><code id="convergence_rate_+3A_typical.subsample">typical.subsample</code></td>
<td>
<p>A function that produces a typical subsample size,
used as the geometric mean of subsample sizes (default: <code>sqrt(n)</code>).</p>
</td></tr>
<tr><td><code id="convergence_rate_+3A_b">B</code></td>
<td>
<p>Number of subsamples to draw per subsample size.</p>
</td></tr>
<tr><td><code id="convergence_rate_+3A_plot">plot</code></td>
<td>
<p>A boolean (default: <code>FALSE</code>) controlling whether to produce a
diagnostic plot visualizing the fit.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>References:
</p>
<p>Politis, Romano and Wolf, &quot;Subsampling&quot;, Chapter 8 (1999), for theory.
</p>
<p>The implementation has been adapted from lecture notes by C. J. Geyer,
https://www.stat.umn.edu/geyer/5601/notes/sub.pdf
</p>


<h3>Value</h3>

<p>A scalar, the parameter <code class="reqn">\beta</code> in the empirical convergence
rate <code class="reqn">n^-\beta</code> of the <code>estimator</code> to the true KL divergence.
It can be used in the <code>convergence.rate</code> argument of <code>kld_ci_subsampling()</code>
as <code>convergence.rate = function(n) n^beta</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    # NN method usually has a convergence rate around 0.5:
    set.seed(0)
    convergence_rate(kld_est_nn, X = rnorm(1000), Y = rnorm(1000, mean = 1, sd = 2))

</code></pre>

<hr>
<h2 id='is_two_sample'>Detect if a one- or two-sample problem is specified</h2><span id='topic+is_two_sample'></span>

<h3>Description</h3>

<p>Detect if a one- or two-sample problem is specified
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_two_sample(Y, q)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is_two_sample_+3A_y">Y</code></td>
<td>
<p>A vector, matrix, data frame or <code>NULL</code></p>
</td></tr>
<tr><td><code id="is_two_sample_+3A_q">q</code></td>
<td>
<p>A function or <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> for a two-sample problem (i.e., <code>Y</code> non-null and <code>q = NULL</code>)
and <code>FALSE</code> for a one-sample problem (i.e., <code>Y = NULL</code> and <code>q</code> non-null).
</p>

<hr>
<h2 id='kld_ci_bootstrap'>Uncertainty of KL divergence estimate using Efron's bootstrap.</h2><span id='topic+kld_ci_bootstrap'></span>

<h3>Description</h3>

<p>This function computes a confidence interval for KL divergence based on Efron's
bootstrap. The approach only works for kernel density-based estimators since
nearest neighbour-based estimators cannot deal with the ties produced when
sampling with replacement.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kld_ci_bootstrap(
  X,
  Y,
  estimator = kld_est_kde1,
  B = 500L,
  alpha = 0.05,
  method = c("quantile", "se"),
  include.boot = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kld_ci_bootstrap_+3A_x">X</code>, <code id="kld_ci_bootstrap_+3A_y">Y</code></td>
<td>
<p><code>n</code>-by-<code>d</code> and <code>m</code>-by-<code>d</code> matrices, representing <code>n</code> samples from
the true distribution <code class="reqn">P</code> and <code>m</code> samples from the approximate distribution
<code class="reqn">Q</code>, both in <code>d</code> dimensions. Vector input is treated as a column matrix.</p>
</td></tr>
<tr><td><code id="kld_ci_bootstrap_+3A_estimator">estimator</code></td>
<td>
<p>A function expecting two inputs <code>X</code> and <code>Y</code>, the
Kullback-Leibler divergence estimation method. Defaults to <code>kld_est_kde1</code>,
which can only deal with one-dimensional two-sample problems (i.e.,
<code>d = 1</code> and <code>q = NULL</code>).</p>
</td></tr>
<tr><td><code id="kld_ci_bootstrap_+3A_b">B</code></td>
<td>
<p>Number of bootstrap replicates (default: <code>500</code>), the larger, the
more accurate, but also more computationally expensive.</p>
</td></tr>
<tr><td><code id="kld_ci_bootstrap_+3A_alpha">alpha</code></td>
<td>
<p>Error level, defaults to <code>0.05</code>.</p>
</td></tr>
<tr><td><code id="kld_ci_bootstrap_+3A_method">method</code></td>
<td>
<p>Either <code>"quantile"</code> (the default), also known as the reverse
percentile method, or <code>"se"</code> for a normal approximation of the KL
divergence estimator using the standard error of the subsamples.</p>
</td></tr>
<tr><td><code id="kld_ci_bootstrap_+3A_include.boot">include.boot</code></td>
<td>
<p>Boolean, <code>TRUE</code> means KL divergene estimates on bootstrap
samples are included in the returned list.</p>
</td></tr>
<tr><td><code id="kld_ci_bootstrap_+3A_...">...</code></td>
<td>
<p>Arguments passed on to <code>estimator</code>, i.e. as <code>estimator(X, Y, ...)</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Reference:
</p>
<p>Efron, &quot;Bootstrap Methods: Another Look at the Jackknife&quot;, The Annals of
Statistics, Vol. 7, No. 1 (1979).
</p>


<h3>Value</h3>

<p>A list with the following fields:
</p>

<ul>
<li> <p><code>"est"</code> (the estimated KL divergence),
</p>
</li>
<li> <p><code>"boot"</code> (a length <code>B</code> numeric vector with KL divergence estimates on
the bootstrap subsamples), only included if <code>include.boot = TRUE</code>,
</p>
</li>
<li> <p><code>"ci"</code> (a length <code>2</code> vector containing the lower and upper limits of the
estimated confidence interval).
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># 1D Gaussian, two samples
set.seed(0)
X &lt;- rnorm(100)
Y &lt;- rnorm(100, mean = 1, sd = 2)
kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2)
kld_est_kde1(X, Y)
kld_ci_bootstrap(X, Y)

</code></pre>

<hr>
<h2 id='kld_ci_subsampling'>Uncertainty of KL divergence estimate using Politis/Romano's subsampling bootstrap.</h2><span id='topic+kld_ci_subsampling'></span>

<h3>Description</h3>

<p>This function computes a confidence interval for KL divergence based on the
subsampling bootstrap introduced by Politis and Romano. See <strong>Details</strong> for
theoretical properties of this method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kld_ci_subsampling(
  X,
  Y = NULL,
  q = NULL,
  estimator = kld_est_nn,
  B = 500L,
  alpha = 0.05,
  subsample.size = function(x) x^(2/3),
  convergence.rate = sqrt,
  method = c("quantile", "se"),
  include.boot = FALSE,
  n.cores = 1L,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kld_ci_subsampling_+3A_x">X</code>, <code id="kld_ci_subsampling_+3A_y">Y</code></td>
<td>
<p><code>n</code>-by-<code>d</code> and <code>m</code>-by-<code>d</code> data frames or matrices (multivariate
samples), or numeric/character vectors (univariate samples, i.e. <code>d = 1</code>),
representing <code>n</code> samples from the true distribution <code class="reqn">P</code> and <code>m</code>
samples from the approximate distribution <code class="reqn">Q</code> in <code>d</code> dimensions.
<code>Y</code> can be left blank if <code>q</code> is specified (see below).</p>
</td></tr>
<tr><td><code id="kld_ci_subsampling_+3A_q">q</code></td>
<td>
<p>The density function of the approximate distribution <code class="reqn">Q</code>. Either
<code>Y</code> or <code>q</code> must be specified. If the distributions are all continuous or
all discrete, <code>q</code> can be directly specified as the probability density/mass
function. However, for mixed continuous/discrete distributions, <code>q</code> must
be given in decomposed form, <code class="reqn">q(y_c,y_d)=q_{c|d}(y_c|y_d)q_d(y_d)</code>,
specified as a named list with field <code>cond</code> for the conditional density
<code class="reqn">q_{c|d}(y_c|y_d)</code> (a function that expects two arguments <code>y_c</code> and
<code>y_d</code>) and <code>disc</code> for the discrete marginal density <code class="reqn">q_d(y_d)</code> (a
function that expects one argument <code>y_d</code>). If such a decomposition is not
available, it may be preferable to instead simulate a large sample from
<code class="reqn">Q</code> and use the two-sample syntax.</p>
</td></tr>
<tr><td><code id="kld_ci_subsampling_+3A_estimator">estimator</code></td>
<td>
<p>The Kullback-Leibler divergence estimation method; a
function expecting two inputs (<code>X</code> and <code>Y</code> or <code>q</code>, depending on arguments
provided). Defaults to <code>kld_est_nn</code>.</p>
</td></tr>
<tr><td><code id="kld_ci_subsampling_+3A_b">B</code></td>
<td>
<p>Number of bootstrap replicates (default: <code>500</code>), the larger, the
more accurate, but also more computationally expensive.</p>
</td></tr>
<tr><td><code id="kld_ci_subsampling_+3A_alpha">alpha</code></td>
<td>
<p>Error level, defaults to <code>0.05</code>.</p>
</td></tr>
<tr><td><code id="kld_ci_subsampling_+3A_subsample.size">subsample.size</code></td>
<td>
<p>A function specifying the size of the subsamples,
defaults to <code class="reqn">f(x) = x^{2/3}</code>.</p>
</td></tr>
<tr><td><code id="kld_ci_subsampling_+3A_convergence.rate">convergence.rate</code></td>
<td>
<p>A function computing the convergence rate of the
estimator as a function of sample sizes. Defaults to <code class="reqn">f(x) = x^{1/2}</code>.
If <code>convergence.rate</code> is <code>NULL</code>, it is estimated empirically from the
sample(s) using <code>kldest::convergence_rate()</code>.</p>
</td></tr>
<tr><td><code id="kld_ci_subsampling_+3A_method">method</code></td>
<td>
<p>Either <code>"quantile"</code> (the default), also known as the reverse
percentile method, or <code>"se"</code> for a normal approximation of the KL
divergence estimator using the standard error of the subsamples.</p>
</td></tr>
<tr><td><code id="kld_ci_subsampling_+3A_include.boot">include.boot</code></td>
<td>
<p>Boolean, <code>TRUE</code> means KL divergence estimates on subsamples
are included in the returned list. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="kld_ci_subsampling_+3A_n.cores">n.cores</code></td>
<td>
<p>Number of cores to use in parallel computing (defaults to <code>1</code>,
which means that no parallel computing is used).
To use this option, the <code>parallel</code> package must be installed and the OS
must be of UNIX type (i.e., not Windows). Otherwise, <code>n.cores</code> will be
reset to <code>1</code>, with a message.</p>
</td></tr>
<tr><td><code id="kld_ci_subsampling_+3A_...">...</code></td>
<td>
<p>Arguments passed on to <code>estimator</code>, i.e. via the call
<code>estimator(X, Y = Y, ...)</code> or <code>estimator(X, q = q, ...)</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In general terms, tetting <code class="reqn">b_n</code> be the subsample size for a sample of
size <code class="reqn">n</code>, and <code class="reqn">\tau_n</code> the convergence rate of the estimator, a
confidence interval calculated by subsampling has asymptotic coverage
<code class="reqn">1 - \alpha</code> as long as <code class="reqn">b_n/n\rightarrow 0</code>,
<code class="reqn">b_n\rightarrow\infty</code> and <code class="reqn">\frac{\tau_{b_n}}{\tau_n}\rightarrow 0</code>.
</p>
<p>In many cases, the convergence rate of the nearest-neighbour based KL
divergence estimator is <code class="reqn">\tau_n = \sqrt{n}</code> and the condition on the
subsample size reduces to <code class="reqn">b_n/n\rightarrow 0</code> and <code class="reqn">b_n\rightarrow\infty</code>.
By default, <code class="reqn">b_n = n^{2/3}</code>. In a two-sample problem, <code class="reqn">n</code> and <code class="reqn">b_n</code>
are replaced by effective sample sizes <code class="reqn">n_\text{eff} = \min(n,m)</code> and
<code class="reqn">b_{n,\text{eff}} = \min(b_n,b_m)</code>.
</p>
<p>Reference:
</p>
<p>Politis and Romano, &quot;Large sample confidence regions based on subsamples under
minimal assumptions&quot;, The Annals of Statistics, Vol. 22, No. 4 (1994).
</p>


<h3>Value</h3>

<p>A list with the following fields:
</p>

<ul>
<li> <p><code>"est"</code> (the estimated KL divergence),
</p>
</li>
<li> <p><code>"ci"</code> (a length <code>2</code> vector containing the lower and upper limits of the
estimated confidence interval).
</p>
</li>
<li> <p><code>"boot"</code> (a length <code>B</code> numeric vector with KL divergence estimates on
the bootstrap subsamples), only included if <code>include.boot = TRUE</code>,
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># 1D Gaussian (one- and two-sample problems)
set.seed(0)
X &lt;- rnorm(100)
Y &lt;- rnorm(100, mean = 1, sd = 2)
q &lt;- function(x) dnorm(x, mean =1, sd = 2)
kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2)
kld_est_nn(X, Y = Y)
kld_est_nn(X, q = q)
kld_ci_subsampling(X, Y)$ci
kld_ci_subsampling(X, q = q)$ci

</code></pre>

<hr>
<h2 id='kld_discrete'>Analytical KL divergence for two discrete distributions</h2><span id='topic+kld_discrete'></span>

<h3>Description</h3>

<p>Analytical KL divergence for two discrete distributions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kld_discrete(P, Q)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kld_discrete_+3A_p">P</code>, <code id="kld_discrete_+3A_q">Q</code></td>
<td>
<p>Numerical arrays with the same dimensions, representing discrete
probability distributions</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A scalar (the Kullback-Leibler divergence)
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 1-D example
P &lt;- 1:4/10
Q &lt;- rep(0.25,4)
kld_discrete(P,Q)

# The above example in 2-D
P &lt;- matrix(1:4/10,nrow=2)
Q &lt;- matrix(0.25,nrow=2,ncol=2)
kld_discrete(P,Q)

</code></pre>

<hr>
<h2 id='kld_est'>Kullback-Leibler divergence estimator for discrete, continuous or mixed data.</h2><span id='topic+kld_est'></span>

<h3>Description</h3>

<p>For two mixed continuous/discrete distributions with densities <code class="reqn">p</code> and
<code class="reqn">q</code>, and denoting <code class="reqn">x = (x_\text{c},x_\text{d})</code>, the Kullback-Leibler
divergence <code class="reqn">D_{KL}(p||q)</code> is given as
</p>
<p style="text-align: center;"><code class="reqn">D_{KL}(p||q) = \sum_{x_d} \int p(x_c,x_d) \log\left(\frac{p(x_c,x_d)}{q(x_c,x_d)}\right)dx_c.</code>
</p>

<p>Conditioning on the discrete variables <code class="reqn">x_d</code>, this can be re-written as
</p>
<p style="text-align: center;"><code class="reqn">D_{KL}(p||q) = \sum_{x_d} p(x_d) D_{KL}\big(p(\cdot|x_d)||q(\cdot|x_d)\big) +
D_{KL}\big(p_{x_d}||q_{x_d}\big).</code>
</p>

<p>Here, the terms
</p>
<p style="text-align: center;"><code class="reqn">D_{KL}\big(p(\cdot|x_d)||q(\cdot|x_d)\big)</code>
</p>

<p>are approximated via nearest neighbour- or kernel-based density estimates on
the datasets <code>X</code> and <code>Y</code> stratified by the discrete variables, and
</p>
<p style="text-align: center;"><code class="reqn">D_{KL}\big(p_{x_d}||q_{x_d}\big)</code>
</p>

<p>is approximated using relative frequencies.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kld_est(
  X,
  Y = NULL,
  q = NULL,
  estimator.continuous = kld_est_nn,
  estimator.discrete = kld_est_discrete,
  vartype = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kld_est_+3A_x">X</code>, <code id="kld_est_+3A_y">Y</code></td>
<td>
<p><code>n</code>-by-<code>d</code> and <code>m</code>-by-<code>d</code> data frames or matrices (multivariate
samples), or numeric/character vectors (univariate samples, i.e. <code>d = 1</code>),
representing <code>n</code> samples from the true distribution <code class="reqn">P</code> and <code>m</code>
samples from the approximate distribution <code class="reqn">Q</code> in <code>d</code> dimensions.
<code>Y</code> can be left blank if <code>q</code> is specified (see below).</p>
</td></tr>
<tr><td><code id="kld_est_+3A_q">q</code></td>
<td>
<p>The density function of the approximate distribution <code class="reqn">Q</code>. Either
<code>Y</code> or <code>q</code> must be specified. If the distributions are all continuous or
all discrete, <code>q</code> can be directly specified as the probability density/mass
function. However, for mixed continuous/discrete distributions, <code>q</code> must
be given in decomposed form, <code class="reqn">q(y_c,y_d)=q_{c|d}(y_c|y_d)q_d(y_d)</code>,
specified as a named list with field <code>cond</code> for the conditional density
<code class="reqn">q_{c|d}(y_c|y_d)</code> (a function that expects two arguments <code>y_c</code> and
<code>y_d</code>) and <code>disc</code> for the discrete marginal density <code class="reqn">q_d(y_d)</code> (a
function that expects one argument <code>y_d</code>). If such a decomposition is not
available, it may be preferable to instead simulate a large sample from
<code class="reqn">Q</code> and use the two-sample syntax.</p>
</td></tr>
<tr><td><code id="kld_est_+3A_estimator.continuous">estimator.continuous</code>, <code id="kld_est_+3A_estimator.discrete">estimator.discrete</code></td>
<td>
<p>KL divergence estimators for
continuous and discrete data, respectively. Both are functions with two
arguments <code>X</code> and <code>Y</code> or <code>X</code> and <code>q</code>, depending on whether a two-sample or
one-sample problem is considered. Defaults are <code>kld_est_nn</code> and
<code>kld_est_discrete</code>, respectively.</p>
</td></tr>
<tr><td><code id="kld_est_+3A_vartype">vartype</code></td>
<td>
<p>A length <code>d</code> character vector, with <code>vartype[i] = "c"</code> meaning
the <code>i</code>-th variable is continuous, and <code>vartype[i] = "d"</code> meaning it is
discrete. If unspecified, <code>vartype</code> is <code>"c"</code> for numeric columns and <code>"d"</code>
for character or factor columns. This default will mostly work, except if
levels of discrete variables are encoded using numbers (e.g., <code>0</code> for
females and <code>1</code> for males) or for count data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A scalar, the estimated Kullback-Leibler divergence <code class="reqn">\hat D_{KL}(P||Q)</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 2D example, two samples
set.seed(0)
X &lt;- data.frame(cont  = rnorm(10),
                discr = c(rep('a',4),rep('b',6)))
Y &lt;- data.frame(cont  = c(rnorm(5), rnorm(5, sd = 2)),
                discr = c(rep('a',5),rep('b',5)))
kld_est(X, Y)

# 2D example, one sample
set.seed(0)
X &lt;- data.frame(cont  = rnorm(10),
                discr = c(rep(0,4),rep(1,6)))
q &lt;- list(cond = function(xc,xd) dnorm(xc, mean = xd, sd = 1),
          disc = function(xd) dbinom(xd, size = 1, prob = 0.5))
kld_est(X, q = q, vartype = c("c","d"))
</code></pre>

<hr>
<h2 id='kld_est_brnn'>Bias-reduced generalized k-nearest-neighbour KL divergence estimation</h2><span id='topic+kld_est_brnn'></span>

<h3>Description</h3>

<p>This is the bias-reduced generalized k-NN based KL divergence estimator from
Wang et al. (2009) specified in Eq.(29).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kld_est_brnn(X, Y, max.k = 100, warn.max.k = TRUE, eps = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kld_est_brnn_+3A_x">X</code>, <code id="kld_est_brnn_+3A_y">Y</code></td>
<td>
<p><code>n</code>-by-<code>d</code> and <code>m</code>-by-<code>d</code> matrices, representing <code>n</code> samples from
the true distribution <code class="reqn">P</code> and <code>m</code> samples from the approximate distribution
<code class="reqn">Q</code>, both in <code>d</code> dimensions. Vector input is treated as a column matrix.
<code>Y</code> can be left blank if <code>q</code> is specified (see below).</p>
</td></tr>
<tr><td><code id="kld_est_brnn_+3A_max.k">max.k</code></td>
<td>
<p>Maximum numbers of nearest neighbours to compute (default: <code>100</code>).
A larger <code>max.k</code> may yield a more accurate KL-D estimate (see <code>warn.max.k</code>),
but will always increase the computational cost.</p>
</td></tr>
<tr><td><code id="kld_est_brnn_+3A_warn.max.k">warn.max.k</code></td>
<td>
<p>If <code>TRUE</code> (the default), warns if <code>max.k</code> is such that more
than <code>max.k</code> neighbours are within the neighbourhood <code class="reqn">\delta</code> for some
data point(s). In this case, only the first <code>max.k</code> neighbours are counted.
As a consequence, <code>max.k</code> may required to be increased.</p>
</td></tr>
<tr><td><code id="kld_est_brnn_+3A_eps">eps</code></td>
<td>
<p>Error bound in the nearest neighbour search. A value of <code>eps = 0</code>
(the default) implies an exact nearest neighbour search, for <code>eps &gt; 0</code>
approximate nearest neighbours are sought, which may be somewhat faster for
high-dimensional problems.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Finite sample bias reduction is achieved by an adaptive choice of the number
of nearest neighbours. Fixing the number of nearest neighbours upfront, as
done in <code><a href="#topic+kld_est_nn">kld_est_nn()</a></code>, may result in very different distances
<code class="reqn">\rho^l_i,\nu^k_i</code> of a datapoint <code class="reqn">x_i</code> to its <code class="reqn">l</code>-th nearest
neighbours in <code class="reqn">X</code> and <code class="reqn">k</code>-th nearest neighbours in <code class="reqn">Y</code>,
respectively, which may lead to unequal biases in NN density estimation,
especially in a high-dimensional setting.
To overcome this issue, the number of neighbours <code class="reqn">l,k</code> are here chosen
in a way to render <code class="reqn">\rho^l_i,\nu^k_i</code> comparable, by taking the largest
possible number of neighbours <code class="reqn">l_i,k_i</code> smaller than
<code class="reqn">\delta_i:=\max(\rho^1_i,\nu^1_i)</code>.
</p>
<p>Since the bias reduction explicitly uses both samples <code>X</code> and <code>Y</code>, one-sample
estimation is not possible using this method.
</p>
<p>Reference:
Wang, Kulkarni and Verdú, &quot;Divergence Estimation for Multidimensional
Densities Via k-Nearest-Neighbor Distances&quot;, IEEE Transactions on Information
Theory, Vol. 55, No. 5 (2009). DOI: https://doi.org/10.1109/TIT.2009.2016060
</p>


<h3>Value</h3>

<p>A scalar, the estimated Kullback-Leibler divergence <code class="reqn">\hat D_{KL}(P||Q)</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># KL-D between one or two samples from 1-D Gaussians:
set.seed(0)
X &lt;- rnorm(100)
Y &lt;- rnorm(100, mean = 1, sd = 2)
q &lt;- function(x) dnorm(x, mean = 1, sd =2)
kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2)
kld_est_nn(X, Y)
kld_est_nn(X, q = q)
kld_est_nn(X, Y, k = 5)
kld_est_nn(X, q = q, k = 5)
kld_est_brnn(X, Y)


# KL-D between two samples from 2-D Gaussians:
set.seed(0)
X1 &lt;- rnorm(100)
X2 &lt;- rnorm(100)
Y1 &lt;- rnorm(100)
Y2 &lt;- Y1 + rnorm(100)
X &lt;- cbind(X1,X2)
Y &lt;- cbind(Y1,Y2)
kld_gaussian(mu1 = rep(0,2), sigma1 = diag(2),
             mu2 = rep(0,2), sigma2 = matrix(c(1,1,1,2),nrow=2))
kld_est_nn(X, Y)
kld_est_nn(X, Y, k = 5)
kld_est_brnn(X, Y)
</code></pre>

<hr>
<h2 id='kld_est_discrete'>Plug-in KL divergence estimator for samples from discrete distributions</h2><span id='topic+kld_est_discrete'></span>

<h3>Description</h3>

<p>Plug-in KL divergence estimator for samples from discrete distributions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kld_est_discrete(X, Y = NULL, q = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kld_est_discrete_+3A_x">X</code>, <code id="kld_est_discrete_+3A_y">Y</code></td>
<td>
<p><code>n</code>-by-<code>d</code> and <code>m</code>-by-<code>d</code> matrices or data frames, representing
<code>n</code> samples from the true discrete distribution <code class="reqn">P</code> and <code>m</code> samples from
the approximate discrete distribution <code class="reqn">Q</code>, both in <code>d</code> dimensions.
Vector input is treated as a column matrix. Argument <code>Y</code> can be omitted if
argument <code>q</code> is given (see below).</p>
</td></tr>
<tr><td><code id="kld_est_discrete_+3A_q">q</code></td>
<td>
<p>The probability mass function of the approximate distribution
<code class="reqn">Q</code>. Currently, the one-sample problem is only implemented for <code>d=1</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A scalar, the estimated Kullback-Leibler divergence <code class="reqn">\hat D_{KL}(P||Q)</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 1D example, two samples
X &lt;- c(rep('M',5),rep('F',5))
Y &lt;- c(rep('M',6),rep('F',4))
kld_est_discrete(X, Y)

# 1D example, one sample
X &lt;- c(rep(0,4),rep(1,6))
q &lt;- function(x) dbinom(x, size = 1, prob = 0.5)
kld_est_discrete(X, q = q)

</code></pre>

<hr>
<h2 id='kld_est_kde'>Kernel density-based Kullback-Leibler divergence estimation in any dimension</h2><span id='topic+kld_est_kde'></span>

<h3>Description</h3>

<p>Disclaimer: this function doesn't use binning and/or the fast Fourier transform
and hence, it is extremely slow even for moderate datasets. For this reason,
it is not exported currently.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kld_est_kde(X, Y, hX = NULL, hY = NULL, rule = c("Silverman", "Scott"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kld_est_kde_+3A_x">X</code>, <code id="kld_est_kde_+3A_y">Y</code></td>
<td>
<p><code>n</code>-by-<code>d</code> and <code>m</code>-by-<code>d</code> matrices, representing <code>n</code> samples from
the true distribution <code class="reqn">P</code> and <code>m</code> samples from the approximate distribution
<code class="reqn">Q</code>, both in <code>d</code> dimensions. Vector input is treated as a column matrix.</p>
</td></tr>
<tr><td><code id="kld_est_kde_+3A_hx">hX</code>, <code id="kld_est_kde_+3A_hy">hY</code></td>
<td>
<p>Positive scalars or length <code>d</code> vectors, representing bandwidth
parameters (possibly different in each component) for the density estimates
of <code class="reqn">P</code> and <code class="reqn">Q</code>, respectively. If unspecified, a heurestic specified
via the <code>rule</code> argument is used.</p>
</td></tr>
<tr><td><code id="kld_est_kde_+3A_rule">rule</code></td>
<td>
<p>A heuristic for computing arguments <code>hX</code> and/or <code>hY</code>. The default
<code>"silverman"</code> is Silverman's rule
</p>
<p style="text-align: center;"><code class="reqn">h_i = \sigma_i\left(\frac{4}{(2+d)n}\right)^{1/(d+4)}.</code>
</p>

<p>As an alternative, Scott's rule <code>"scott"</code> can be used,
</p>
<p style="text-align: center;"><code class="reqn">h_i = \frac{\sigma_i}{n^{1/(d+4)}}.</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This estimation method approximates the densities of the unknown distributions
<code class="reqn">P</code> and <code class="reqn">Q</code> by kernel density estimates, using a sample size- and
dimension-dependent bandwidth parameter and a Gaussian kernel. It works for
any number of dimensions but is very slow.
</p>


<h3>Value</h3>

<p>A scalar, the estimated Kullback-Leibler divergence <code class="reqn">\hat D_{KL}(P||Q)</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># KL-D between two samples from 1-D Gaussians:
set.seed(0)
X &lt;- rnorm(100)
Y &lt;- rnorm(100, mean = 1, sd = 2)
kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2)
kld_est_kde1(X, Y)
kld_est_nn(X, Y)
kld_est_brnn(X, Y)

# KL-D between two samples from 2-D Gaussians:
set.seed(0)
X1 &lt;- rnorm(100)
X2 &lt;- rnorm(100)
Y1 &lt;- rnorm(100)
Y2 &lt;- Y1 + rnorm(100)
X &lt;- cbind(X1,X2)
Y &lt;- cbind(Y1,Y2)
kld_gaussian(mu1 = rep(0,2), sigma1 = diag(2),
             mu2 = rep(0,2), sigma2 = matrix(c(1,1,1,2),nrow=2))
kld_est_kde2(X, Y)
kld_est_nn(X, Y)
kld_est_brnn(X, Y)
</code></pre>

<hr>
<h2 id='kld_est_kde1'>1-D kernel density-based estimation of Kullback-Leibler divergence</h2><span id='topic+kld_est_kde1'></span>

<h3>Description</h3>

<p>This estimation method approximates the densities of the unknown distributions
<code class="reqn">P</code> and <code class="reqn">Q</code> by a kernel density estimate using function 'density' from
package 'stats'. Only the two-sample, not the one-sample problem is implemented.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kld_est_kde1(X, Y, MC = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kld_est_kde1_+3A_x">X</code>, <code id="kld_est_kde1_+3A_y">Y</code></td>
<td>
<p>Numeric vectors or single-column matrices, representing samples
from the true distribution <code class="reqn">P</code> and the approximate distribution
<code class="reqn">Q</code>, respectively.</p>
</td></tr>
<tr><td><code id="kld_est_kde1_+3A_mc">MC</code></td>
<td>
<p>A boolean: use a Monte Carlo approximation instead of numerical
integration via the trapezoidal rule (default: <code>FALSE</code>)?</p>
</td></tr>
<tr><td><code id="kld_est_kde1_+3A_...">...</code></td>
<td>
<p>Further parameters to passed on to <code>stats::density</code> (e.g.,
argument <code>bw</code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A scalar, the estimated Kullback-Leibler divergence <code class="reqn">\hat D_{KL}(P||Q)</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># KL-D between two samples from 1D Gaussians:
set.seed(0)
X &lt;- rnorm(100)
Y &lt;- rnorm(100, mean = 1, sd = 2)
kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2)
kld_est_kde1(X,Y)
kld_est_kde1(X,Y, MC = TRUE)
</code></pre>

<hr>
<h2 id='kld_est_kde2'>2-D kernel density-based estimation of Kullback-Leibler divergence</h2><span id='topic+kld_est_kde2'></span>

<h3>Description</h3>

<p>This estimation method approximates the densities of the unknown bivariate
distributions <code class="reqn">P</code> and <code class="reqn">Q</code> by kernel density estimates using function
'bkde' from package 'KernSmooth'. If 'KernSmooth' is not installed, a message
is issued and the (much) slower function 'kld_est_kde' is used instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kld_est_kde2(
  X,
  Y,
  MC = FALSE,
  hX = NULL,
  hY = NULL,
  rule = c("Silverman", "Scott"),
  eps = 1e-05
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kld_est_kde2_+3A_x">X</code>, <code id="kld_est_kde2_+3A_y">Y</code></td>
<td>
<p><code>n</code>-by-<code>2</code> and <code>m</code>-by-<code>2</code> matrices, representing <code>n</code> samples from
the bivariate true distribution <code class="reqn">P</code> and <code>m</code> samples from the approximate
distribution <code class="reqn">Q</code>, respectively.</p>
</td></tr>
<tr><td><code id="kld_est_kde2_+3A_mc">MC</code></td>
<td>
<p>A boolean: use a Monte Carlo approximation instead of numerical
integration via the trapezoidal rule (default: <code>FALSE</code>)? Currently, this
option is not implemented, i.e. a value of <code>TRUE</code> results in an error.</p>
</td></tr>
<tr><td><code id="kld_est_kde2_+3A_hx">hX</code>, <code id="kld_est_kde2_+3A_hy">hY</code></td>
<td>
<p>Bandwidths for the kernel density estimates of <code class="reqn">P</code> and <code class="reqn">Q</code>,
respectively. The default <code>NULL</code> means they are determined by argument <code>rule</code>.</p>
</td></tr>
<tr><td><code id="kld_est_kde2_+3A_rule">rule</code></td>
<td>
<p>A heuristic to derive parameters <code>hX</code> and <code>hY</code>, default is
<code style="white-space: pre;">&#8288;"Silverman", which means that &#8288;</code></p>
<p style="text-align: center;"><code class="reqn">h_i = \sigma_i\left(\frac{4}{(2+d)n}\right)^{1/(d+4)}.</code>
</p>
</td></tr>
<tr><td><code id="kld_est_kde2_+3A_eps">eps</code></td>
<td>
<p>A nonnegative scalar; if <code>eps &gt; 0</code>, <code class="reqn">Q</code> is estimated as a mixture
between the kernel density estimate and a uniform distribution on the computational
grid. The weight of the uniform component is <code>eps</code> times the maximum density
estimate of <code class="reqn">Q</code>. This increases the robustness of the estimator at the
expense of an additional bias. Defaults to <code>eps = 1e-5</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A scalar, the estimated Kullback-Leibler divergence <code class="reqn">\hat D_{KL}(P||Q)</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># KL-D between two samples from 2-D Gaussians:
set.seed(0)
X1 &lt;- rnorm(1000)
X2 &lt;- rnorm(1000)
Y1 &lt;- rnorm(1000)
Y2 &lt;- Y1 + rnorm(1000)
X &lt;- cbind(X1,X2)
Y &lt;- cbind(Y1,Y2)
kld_gaussian(mu1 = rep(0,2), sigma1 = diag(2),
             mu2 = rep(0,2), sigma2 = matrix(c(1,1,1,2),nrow=2))
kld_est_kde2(X,Y)
</code></pre>

<hr>
<h2 id='kld_est_nn'>k-nearest neighbour KL divergence estimator</h2><span id='topic+kld_est_nn'></span>

<h3>Description</h3>

<p>This function estimates Kullback-Leibler divergence <code class="reqn">D_{KL}(P||Q)</code> between
two continuous distributions <code class="reqn">P</code> and <code class="reqn">Q</code> using nearest-neighbour (NN)
density estimation in a Monte Carlo approximation of <code class="reqn">D_{KL}(P||Q)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kld_est_nn(X, Y = NULL, q = NULL, k = 1L, eps = 0, log.q = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kld_est_nn_+3A_x">X</code>, <code id="kld_est_nn_+3A_y">Y</code></td>
<td>
<p><code>n</code>-by-<code>d</code> and <code>m</code>-by-<code>d</code> matrices, representing <code>n</code> samples from
the true distribution <code class="reqn">P</code> and <code>m</code> samples from the approximate distribution
<code class="reqn">Q</code>, both in <code>d</code> dimensions. Vector input is treated as a column matrix.
<code>Y</code> can be left blank if <code>q</code> is specified (see below).</p>
</td></tr>
<tr><td><code id="kld_est_nn_+3A_q">q</code></td>
<td>
<p>The density function of the approximate distribution <code class="reqn">Q</code>. Either
<code>Y</code> or <code>q</code> must be specified.</p>
</td></tr>
<tr><td><code id="kld_est_nn_+3A_k">k</code></td>
<td>
<p>The number of nearest neighbours to consider for NN density estimation.
Larger values for <code>k</code> generally increase bias, but decrease variance of the
estimator. Defaults to <code>k = 1</code>.</p>
</td></tr>
<tr><td><code id="kld_est_nn_+3A_eps">eps</code></td>
<td>
<p>Error bound in the nearest neighbour search. A value of <code>eps = 0</code>
(the default) implies an exact nearest neighbour search, for <code>eps &gt; 0</code>
approximate nearest neighbours are sought, which may be somewhat faster for
high-dimensional problems.</p>
</td></tr>
<tr><td><code id="kld_est_nn_+3A_log.q">log.q</code></td>
<td>
<p>If <code>TRUE</code>, function <code>q</code> is the log-density rather than the density
of the approximate distribution <code class="reqn">Q</code> (default: <code>log.q = FALSE</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Input for estimation is a sample <code>X</code> from <code class="reqn">P</code> and either the density
function <code>q</code> of <code class="reqn">Q</code> (one-sample problem) or a sample <code>Y</code> of <code class="reqn">Q</code>
(two-sample problem). In the two-sample problem, it is the estimator in Eq.(5)
of Wang et al. (2009). In the one-sample problem, the asymptotic bias (the
expectation of a Gamma distribution) is substracted, see Pérez-Cruz (2008),
Eq.(18).
</p>
<p>References:
</p>
<p>Wang, Kulkarni and Verdú, &quot;Divergence Estimation for Multidimensional
Densities Via k-Nearest-Neighbor Distances&quot;, IEEE Transactions on Information
Theory, Vol. 55, No. 5 (2009).
</p>
<p>Pérez-Cruz, &quot;Kullback-Leibler Divergence Estimation of Continuous
Distributions&quot;, IEEE International Symposium on Information Theory (2008).
</p>


<h3>Value</h3>

<p>A scalar, the estimated Kullback-Leibler divergence <code class="reqn">\hat D_{KL}(P||Q)</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># KL-D between one or two samples from 1-D Gaussians:
set.seed(0)
X &lt;- rnorm(100)
Y &lt;- rnorm(100, mean = 1, sd = 2)
q &lt;- function(x) dnorm(x, mean = 1, sd =2)
kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2)
kld_est_nn(X, Y)
kld_est_nn(X, q = q)
kld_est_nn(X, Y, k = 5)
kld_est_nn(X, q = q, k = 5)
kld_est_brnn(X, Y)


# KL-D between two samples from 2-D Gaussians:
set.seed(0)
X1 &lt;- rnorm(100)
X2 &lt;- rnorm(100)
Y1 &lt;- rnorm(100)
Y2 &lt;- Y1 + rnorm(100)
X &lt;- cbind(X1,X2)
Y &lt;- cbind(Y1,Y2)
kld_gaussian(mu1 = rep(0,2), sigma1 = diag(2),
             mu2 = rep(0,2), sigma2 = matrix(c(1,1,1,2),nrow=2))
kld_est_nn(X, Y)
kld_est_nn(X, Y, k = 5)
kld_est_brnn(X, Y)
</code></pre>

<hr>
<h2 id='kld_exponential'>Analytical KL divergence for two univariate exponential distributions</h2><span id='topic+kld_exponential'></span>

<h3>Description</h3>

<p>This function computes <code class="reqn">D_{KL}(p||q)</code>, where <code class="reqn">p\sim \text{Exp}(\lambda_1)</code>
and <code class="reqn">q\sim \text{Exp}(\lambda_2)</code>, in rate parametrization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kld_exponential(lambda1, lambda2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kld_exponential_+3A_lambda1">lambda1</code></td>
<td>
<p>A scalar (rate parameter of true exponential distribution)</p>
</td></tr>
<tr><td><code id="kld_exponential_+3A_lambda2">lambda2</code></td>
<td>
<p>A scalar (rate parameter of approximate exponential distribution)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A scalar (the Kullback-Leibler divergence)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>kld_exponential(lambda1 = 1, lambda2 = 2)
</code></pre>

<hr>
<h2 id='kld_gaussian'>Analytical KL divergence for two uni- or multivariate Gaussian distributions</h2><span id='topic+kld_gaussian'></span>

<h3>Description</h3>

<p>This function computes <code class="reqn">D_{KL}(p||q)</code>, where <code class="reqn">p\sim \mathcal{N}(\mu_1,\Sigma_1)</code>
and <code class="reqn">q\sim \mathcal{N}(\mu_2,\Sigma_2)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kld_gaussian(mu1, sigma1, mu2, sigma2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kld_gaussian_+3A_mu1">mu1</code></td>
<td>
<p>A numeric vector (mean of true Gaussian)</p>
</td></tr>
<tr><td><code id="kld_gaussian_+3A_sigma1">sigma1</code></td>
<td>
<p>A s.p.d. matrix (Covariance matrix of true Gaussian)</p>
</td></tr>
<tr><td><code id="kld_gaussian_+3A_mu2">mu2</code></td>
<td>
<p>A numeric vector (mean of approximate Gaussian)</p>
</td></tr>
<tr><td><code id="kld_gaussian_+3A_sigma2">sigma2</code></td>
<td>
<p>A s.p.d. matrix  (Covariance matrix of approximate Gaussian)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A scalar (the Kullback-Leibler divergence)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>kld_gaussian(mu1 = 1, sigma1 = 1, mu2 = 1, sigma2 = 2^2)
kld_gaussian(mu1 = rep(0,2), sigma1 = diag(2),
                mu2 = rep(1,2), sigma2 = matrix(c(1,0.5,0.5,1), nrow = 2))
</code></pre>

<hr>
<h2 id='kld_uniform'>Analytical KL divergence for two uniform distributions</h2><span id='topic+kld_uniform'></span>

<h3>Description</h3>

<p>This function computes <code class="reqn">D_{KL}(p||q)</code>, where <code class="reqn">p\sim \text{U}(a_1,b_1)</code>
and <code class="reqn">q\sim \text{U}(a_2,b_2)</code>, with <code class="reqn">a_2&lt;a_1&lt;b_1&lt;b_2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kld_uniform(a1, b1, a2, b2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kld_uniform_+3A_a1">a1</code>, <code id="kld_uniform_+3A_b1">b1</code></td>
<td>
<p>Range of true uniform distribution</p>
</td></tr>
<tr><td><code id="kld_uniform_+3A_a2">a2</code>, <code id="kld_uniform_+3A_b2">b2</code></td>
<td>
<p>Range of approximate uniform distribution</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A scalar (the Kullback-Leibler divergence)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>kld_uniform(a1 = 0, b1 = 1, a2 = 0, b2 = 2)
</code></pre>

<hr>
<h2 id='kld_uniform_gaussian'>Analytical KL divergence between a uniform and a Gaussian distribution</h2><span id='topic+kld_uniform_gaussian'></span>

<h3>Description</h3>

<p>This function computes <code class="reqn">D_{KL}(p||q)</code>, where <code class="reqn">p\sim \text{U}(a,b)</code>
and <code class="reqn">q\sim \mathcal{N}(\mu,\sigma^2)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kld_uniform_gaussian(a = 0, b = 1, mu = 0, sigma2 = 1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kld_uniform_gaussian_+3A_a">a</code>, <code id="kld_uniform_gaussian_+3A_b">b</code></td>
<td>
<p>Parameters of uniform (true) distribution</p>
</td></tr>
<tr><td><code id="kld_uniform_gaussian_+3A_mu">mu</code>, <code id="kld_uniform_gaussian_+3A_sigma2">sigma2</code></td>
<td>
<p>Parameters of Gaussian (approximate) distribution</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A scalar (the Kullback-Leibler divergence)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>kld_uniform_gaussian(a = 0, b = 1, mu = 0, sigma2 = 1)
</code></pre>

<hr>
<h2 id='mvdnorm'>Probability density function of multivariate Gaussian distribution</h2><span id='topic+mvdnorm'></span>

<h3>Description</h3>

<p>Probability density function of multivariate Gaussian distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mvdnorm(x, mu, Sigma)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mvdnorm_+3A_x">x</code></td>
<td>
<p>A vector of length <code>d</code> at which Gaussian density is evaluated.</p>
</td></tr>
<tr><td><code id="mvdnorm_+3A_mu">mu</code></td>
<td>
<p>A vector of length <code>d</code>, mean of Gaussian distribution.</p>
</td></tr>
<tr><td><code id="mvdnorm_+3A_sigma">Sigma</code></td>
<td>
<p>A <code>d</code>-by-<code>d</code> matrix, covariance matrix of Gaussian distribution.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The probability density of <code class="reqn">N(\mu,\Sigma)</code> evaluated at <code>x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 1D example
mvdnorm(x = 2, mu = 1, Sigma = 2)
dnorm(x = 2, mean = 1, sd = sqrt(2))
# Independent 2D example
mvdnorm(x = c(2,2), mu = c(1,1), Sigma = diag(1:2))
prod(dnorm(x = c(2,2), mean = c(1,1), sd = sqrt(1:2)))
# Correlated 2D example
mvdnorm(x = c(2,2), mu = c(1,1), Sigma = matrix(c(2,1,1,2),nrow=2))
</code></pre>

<hr>
<h2 id='to_uniform_scale'>Transform samples to uniform scale</h2><span id='topic+to_uniform_scale'></span>

<h3>Description</h3>

<p>Since Kullback-Leibler divergence is scale-invariant, its sample-based
approximations can be computed on a conveniently chosen scale. This helper
functions transforms each variable in a way that all marginal distributions
of the joint dataset <code class="reqn">(X,Y)</code> are uniform. In this way, the scales of
different variables are rendered comparable, with the idea of a better
performance of neighbour-based methods in this situation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>to_uniform_scale(X, Y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="to_uniform_scale_+3A_x">X</code>, <code id="to_uniform_scale_+3A_y">Y</code></td>
<td>
<p><code>n</code>-by-<code>d</code> and <code>m</code>-by-<code>d</code> matrices, representing <code>n</code> samples from
the true distribution <code class="reqn">P</code> and <code>m</code> samples from the approximate distribution
<code class="reqn">Q</code>, both in <code>d</code> dimensions. Vector input is treated as a column matrix.
<code>Y</code> can be left blank if <code>q</code> is specified (see below).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with fields <code>X</code> and <code>Y</code>, containing the transformed samples.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 2D example
n &lt;- 10L
X &lt;- cbind(rnorm(n, mean = 0, sd = 3),
           rnorm(n, mean = 1, sd = 2))
Y &lt;- cbind(rnorm(n, mean = 1, sd = 2),
           rnorm(n, mean = 0, sd = 2))
to_uniform_scale(X, Y)
</code></pre>

<hr>
<h2 id='tr'>Matrix trace operator</h2><span id='topic+tr'></span>

<h3>Description</h3>

<p>Matrix trace operator
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tr(M)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tr_+3A_m">M</code></td>
<td>
<p>A square matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The matrix trace (a scalar)
</p>

<hr>
<h2 id='trapz'>Trapezoidal integration in 1 or 2 dimensions</h2><span id='topic+trapz'></span>

<h3>Description</h3>

<p>Trapezoidal integration in 1 or 2 dimensions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trapz(h, fx)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="trapz_+3A_h">h</code></td>
<td>
<p>A length <code>d</code> numeric vector of grid widths.</p>
</td></tr>
<tr><td><code id="trapz_+3A_fx">fx</code></td>
<td>
<p>A <code>d</code>-dimensional array (or a vector, if <code>d=1</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The trapezoidal approximation of the integral.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 1D example
trapz(h = 1, fx = 1:10)
# 2D example
trapz(h = c(1,1), fx = matrix(1:10, nrow = 2))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
