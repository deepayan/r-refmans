<!DOCTYPE html><html><head><title>Help for package marqLevAlg</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {marqLevAlg}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#marqLevAlg-package'><p>A parallelized general-purpose optimization based on Marquardt-Levenberg algorithm</p></a></li>
<li><a href='#dataEx'><p>Simulated dataset</p></a></li>
<li><a href='#deriva'><p>Numerical derivatives</p></a></li>
<li><a href='#deriva_grad'><p>Numerical derivatives of the gradient function</p></a></li>
<li><a href='#gradLMM'><p>Gradient of the log-likelihood of a linear mixed model with random intercept</p></a></li>
<li><a href='#loglikLMM'><p>Log-likelihood of a linear mixed model with random intercept</p></a></li>
<li><a href='#marqLevAlg'><p>A parallelized general-purpose optimization based on Marquardt-Levenberg algorithm</p></a></li>
<li><a href='#print.marqLevAlg'><p>Summary of a <code>marqLevAlg</code> object</p></a></li>
<li><a href='#summary.marqLevAlg'><p>Summary of optimization</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>A Parallelized General-Purpose Optimization Based on
Marquardt-Levenberg Algorithm</td>
</tr>
<tr>
<td>Version:</td>
<td>2.0.8</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-03-22</td>
</tr>
<tr>
<td>Author:</td>
<td>Viviane Philipps, Cecile Proust-Lima, Melanie Prague, Boris Hejblum, Daniel Commenges, Amadou Diakite</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Viviane Philipps &lt;viviane.philipps@u-bordeaux.fr&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>microbenchmark,knitr,rmarkdown,ggplot2,viridis,patchwork,xtable</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Description:</td>
<td>This algorithm provides a numerical solution to the
        problem of unconstrained local minimization (or maximization). It is particularly suited for complex problems and more efficient than
        the Gauss-Newton-like algorithm when starting from points very
        far from the final minimum (or maximum). Each iteration is parallelized and convergence relies on a stringent stopping criterion based on the first and second derivatives. See Philipps et al, 2021 &lt;<a href="https://doi.org/10.32614%2FRJ-2021-089">doi:10.32614/RJ-2021-089</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2.0)]</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/VivianePhilipps/marqLevAlgParallel/issues">https://github.com/VivianePhilipps/marqLevAlgParallel/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>doParallel, foreach</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-03-22 10:39:53 UTC; vp3</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-03-22 14:00:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='marqLevAlg-package'>A parallelized general-purpose optimization based on Marquardt-Levenberg algorithm</h2><span id='topic+marqLevAlg-package'></span>

<h3>Description</h3>

<p>This algorithm provides a numerical solution to the problem of unconstrained local
minimization/maximization. This is more efficient than the
Gauss-Newton-like algorithm when starting from points very far from the final
minimum/maximum. A new convergence test
is implemented (RDM) in addition to the usual stopping criterion : stopping
rule is when the gradients are small enough in the parameters metric
(GH^-1G).
</p>


<h3>Details</h3>



<table>
<tr>
 <td style="text-align: left;"> Package: </td><td style="text-align: left;"> marqLevAlg</td>
</tr>
<tr>
 <td style="text-align: left;"> Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 2.0.8 </td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2023-03-22 </td>
</tr>
<tr>
 <td style="text-align: left;"> License: </td><td style="text-align: left;"> GPL (&gt;= 2.0)</td>
</tr>
<tr>
 <td style="text-align: left;">
LazyLoad: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;"> </td>
</tr>

</table>
<p> This algorithm provides a numerical solution to the
problem of optimizing a function. This is more efficient than the
Gauss-Newton-like algorithm when starting from points very far from the final
maximum. A new convergence test is implemented (RDM) in addition to the
usual stopping criterion : stopping rule is when the gradients are small
enough in the parameters metric (GH-1G).
</p>


<h3>Author(s)</h3>

<p>Viviane Philipps, Cecile Proust-Lima, Boris Hejblum, Melanie Prague, Daniel Commenges, Amadou Diakite
</p>


<h3>References</h3>

<p><em>marqLevAlg Algorithm</em>
</p>
<p>Philipps V. Hejblum B.P. Prague M. Commenge D. Proust-Lima C.
Robust and Efficient Optimization Using a Marquardt-Levenberg Algorithm
with R Package marqLevAlg. The R Journal (2021).
</p>
<p>Donald W. marquardt An algorithm for Least-Squares Estimation of Nonlinear
Parameters. Journal of the Society for Industrial and Applied Mathematics,
Vol. 11, No. 2. (Jun, 1963), pp. 431-441.
</p>
<p><em>Convergence criteria : Relative distance to Maximum</em>
</p>
<p>Commenges D. Jacqmin-Gadda H. Proust C. Guedj J. A Newton-like algorithm for
likelihood maximization : the robust-variance scoring algorithm
arxiv:math/0610402v2 (2006)
</p>

<hr>
<h2 id='dataEx'>Simulated dataset</h2><span id='topic+dataEx'></span>

<h3>Description</h3>

<p>Sample of 500 subjects simulated according to a linear mixed model. 
The fixed part of the model included an intercept, 2 natural cubic splines on time 
and their interactions with time-independent covariates X1, X2 and X3. 
A random intercept and a independant error term were also inclued.
</p>


<h3>Format</h3>

<p>A data frame with 2429 observations on the following 6 variables.
</p>
 <dl>
<dt>i</dt><dd><p>subject identification number</p>
</dd>
<dt>t</dt><dd><p>time of measurement</p>
</dd> <dt>X1</dt><dd><p>binary covariate</p>
</dd> 
<dt>X2</dt><dd><p>continous Gaussian standard variable</p>
</dd> 
<dt>X3</dt><dd><p>continous Gaussian variable</p>
</dd> 
<dt>Y</dt><dd><p>longitudinal outcome</p>
</dd></dl>


<hr>
<h2 id='deriva'>Numerical derivatives</h2><span id='topic+deriva'></span>

<h3>Description</h3>

<p>The function computes the first derivates and the information score matrix.
Central finite-differences and forward finite-differences are used for the first
and second derivatives respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deriva(nproc = 1, b, funcpa, .packages = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deriva_+3A_nproc">nproc</code></td>
<td>
<p>number of processors for parallel computing</p>
</td></tr>
<tr><td><code id="deriva_+3A_b">b</code></td>
<td>
<p>value of parameters to be optimized over</p>
</td></tr>
<tr><td><code id="deriva_+3A_funcpa">funcpa</code></td>
<td>
<p>function to be minimized (or maximized), with argument the vector
of parameters over which minimization isto take place.
It should return a scalar result.</p>
</td></tr>
<tr><td><code id="deriva_+3A_.packages">.packages</code></td>
<td>
<p>character vector of packages that funcpa depends on</p>
</td></tr>
<tr><td><code id="deriva_+3A_...">...</code></td>
<td>
<p>other arguments of the funcpa function</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>v</code></td>
<td>
<p>vector containing the upper part of the information score
matrix and the first derivatives</p>
</td></tr> <tr><td><code>rl</code></td>
<td>
<p>the value of the funcpa function
at point b</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Viviane Philipps, Boris Hejblum, Cecile Proust-Lima, Daniel Commenges
</p>


<h3>References</h3>

<p>Donald W. Marquardt An algorithm for Least-Squares Estimation of Nonlinear Parameters. Journal of the Society for Industrial and Applied Mathematics, Vol. 11, No. 2. (Jun, 1963), pp. 431-441.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>b &lt;- 0.1
f &lt;- function(b){return((2*b[1]**2+3*b[1]))}
d &lt;- deriva(b=b,funcpa=f)

</code></pre>

<hr>
<h2 id='deriva_grad'>Numerical derivatives of the gradient function</h2><span id='topic+deriva_grad'></span>

<h3>Description</h3>

<p>The function computes the information score matrix in the case where the first
derivatives of the function to optimize are analytically known. Therefore,
minus the derivatives of the gradient are computed by central finite differences.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deriva_grad(nproc = 1, b, grad, .packages = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deriva_grad_+3A_nproc">nproc</code></td>
<td>
<p>number of processors for parallel computing</p>
</td></tr>
<tr><td><code id="deriva_grad_+3A_b">b</code></td>
<td>
<p>value of parameters to be optimized over</p>
</td></tr>
<tr><td><code id="deriva_grad_+3A_grad">grad</code></td>
<td>
<p>the gradient of the function to be minimized (or maximized)</p>
</td></tr>
<tr><td><code id="deriva_grad_+3A_.packages">.packages</code></td>
<td>
<p>character vector of packages that grad depends on</p>
</td></tr>
<tr><td><code id="deriva_grad_+3A_...">...</code></td>
<td>
<p>other arguments of the grad function</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>hessian</code></td>
<td>
<p>vector containing the upper part of the information score matrix</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Viviane Philipps, Boris Hejblum, Cecile Proust-Lima, Daniel Commenges
</p>

<hr>
<h2 id='gradLMM'>Gradient of the log-likelihood of a linear mixed model with random intercept</h2><span id='topic+gradLMM'></span>

<h3>Description</h3>

<p>Gradient of the log-likelihood of a linear mixed model with random intercept
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gradLMM(b, Y, X, ni)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gradLMM_+3A_b">b</code></td>
<td>
<p>numeric vector specifying the parameter's values in the
following order : first the fixed effects and then the standard
deviation of the random intercept and of the independent error</p>
</td></tr>
<tr><td><code id="gradLMM_+3A_y">Y</code></td>
<td>
<p>numeric vector including the dependent outcome vector ordered by subject</p>
</td></tr>
<tr><td><code id="gradLMM_+3A_x">X</code></td>
<td>
<p>numeric matrix including the covariates</p>
</td></tr>
<tr><td><code id="gradLMM_+3A_ni">ni</code></td>
<td>
<p>interger vector giving the number of repeated measures for each subject</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector containing the gradient of the log-likelihood of the linear mixed model at point b
</p>

<hr>
<h2 id='loglikLMM'>Log-likelihood of a linear mixed model with random intercept</h2><span id='topic+loglikLMM'></span>

<h3>Description</h3>

<p>Log-likelihood of a linear mixed model with random intercept
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loglikLMM(b, Y, X, ni)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loglikLMM_+3A_b">b</code></td>
<td>
<p>numeric vector specifying the parameter's values in the
following order : first the fixed effects and then the standard
deviation of the random intercept and of the independent error</p>
</td></tr>
<tr><td><code id="loglikLMM_+3A_y">Y</code></td>
<td>
<p>numeric vector including the dependent outcome vector ordered by subject</p>
</td></tr>
<tr><td><code id="loglikLMM_+3A_x">X</code></td>
<td>
<p>numeric matrix including the covariates</p>
</td></tr>
<tr><td><code id="loglikLMM_+3A_ni">ni</code></td>
<td>
<p>interger vector giving the number of repeated measures for each subject</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the log-likelihood of the linear mixed model at point b
</p>

<hr>
<h2 id='marqLevAlg'>A parallelized general-purpose optimization based on Marquardt-Levenberg algorithm</h2><span id='topic+marqLevAlg'></span><span id='topic+mla'></span>

<h3>Description</h3>

<p>This algorithm provides a numerical solution to the problem of unconstrained local
optimization. This is more efficient than the Gauss-Newton-like algorithm when
starting from points very far from the final maximum. A new convergence test
is implemented (RDM) in addition to the usual stopping criterion : stopping
rule is when the gradients are small enough in the parameters metric
(GH-1G).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>marqLevAlg(
  b,
  m = FALSE,
  fn,
  gr = NULL,
  hess = NULL,
  maxiter = 500,
  epsa = 1e-04,
  epsb = 1e-04,
  epsd = 1e-04,
  partialH = NULL,
  digits = 8,
  print.info = FALSE,
  blinding = TRUE,
  multipleTry = 25,
  nproc = 1,
  clustertype = NULL,
  file = "",
  .packages = NULL,
  minimize = TRUE,
  ...
)

mla(
  b,
  m = FALSE,
  fn,
  gr = NULL,
  hess = NULL,
  maxiter = 500,
  epsa = 1e-04,
  epsb = 1e-04,
  epsd = 1e-04,
  partialH = NULL,
  digits = 8,
  print.info = FALSE,
  blinding = TRUE,
  multipleTry = 25,
  nproc = 1,
  clustertype = NULL,
  file = "",
  .packages = NULL,
  minimize = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="marqLevAlg_+3A_b">b</code></td>
<td>
<p>an optional vector containing the initial values for the
parameters. Default is 0.1 for every parameter.</p>
</td></tr>
<tr><td><code id="marqLevAlg_+3A_m">m</code></td>
<td>
<p>number of parameters. Optional if b is specified.</p>
</td></tr>
<tr><td><code id="marqLevAlg_+3A_fn">fn</code></td>
<td>
<p>the function to be optimized, with first argument the
vector of parameters over which optimization is to take place (argument b).
It should return a scalar result.</p>
</td></tr>
<tr><td><code id="marqLevAlg_+3A_gr">gr</code></td>
<td>
<p>a function to return the gradient value for a specific point.
If missing, finite-difference approximation will be used.</p>
</td></tr>
<tr><td><code id="marqLevAlg_+3A_hess">hess</code></td>
<td>
<p>a function to return the hessian matrix for a specific point.
If missing, finite-difference approximation will be used.</p>
</td></tr>
<tr><td><code id="marqLevAlg_+3A_maxiter">maxiter</code></td>
<td>
<p>optional maximum number of iterations for the marqLevAlg
iterative algorithm. Default is 500.</p>
</td></tr>
<tr><td><code id="marqLevAlg_+3A_epsa">epsa</code></td>
<td>
<p>optional threshold for the convergence criterion based on the
parameter stability. Default is 0.0001.</p>
</td></tr>
<tr><td><code id="marqLevAlg_+3A_epsb">epsb</code></td>
<td>
<p>optional threshold for the convergence criterion based on the
objective function stability. Default is 0.0001.</p>
</td></tr>
<tr><td><code id="marqLevAlg_+3A_epsd">epsd</code></td>
<td>
<p>optional threshold for the relative distance to maximum. This
criterion has the nice interpretation of estimating the ratio of the
approximation error over the statistical error, thus it can be used for
stopping the iterative process whathever the problem. Default is 0.0001.</p>
</td></tr>
<tr><td><code id="marqLevAlg_+3A_partialh">partialH</code></td>
<td>
<p>optional vector giving the indexes of the parameters to be dropped from
the Hessian matrix to define the relative distance to maximum/minimum. If specified,
this option will only be considered at iterations where the two first convergence
criteria are satisfied (epsa and epsb) and if the total Hessian is not invertible.
By default, no partial Hessian is defined.</p>
</td></tr>
<tr><td><code id="marqLevAlg_+3A_digits">digits</code></td>
<td>
<p>number of digits to print in outputs. Default value is 8.</p>
</td></tr>
<tr><td><code id="marqLevAlg_+3A_print.info">print.info</code></td>
<td>
<p>logical indicating if information about computation should be
reported at each iteration.
Default value is FALSE.</p>
</td></tr>
<tr><td><code id="marqLevAlg_+3A_blinding">blinding</code></td>
<td>
<p>logical. Equals to TRUE if the algorithm is allowed to go on
in case of an infinite or not definite value of function. Default value is
FALSE.</p>
</td></tr>
<tr><td><code id="marqLevAlg_+3A_multipletry">multipleTry</code></td>
<td>
<p>integer, different from 1 if the algorithm is allowed to
go for the first iteration in case of an infinite or not definite value of
gradients or hessian. As many tries as requested in multipleTry will be done by
changing the starting point of the algorithm. Default value is 25.</p>
</td></tr>
<tr><td><code id="marqLevAlg_+3A_nproc">nproc</code></td>
<td>
<p>number of processors for parallel computing</p>
</td></tr>
<tr><td><code id="marqLevAlg_+3A_clustertype">clustertype</code></td>
<td>
<p>one of the supported types from <code><a href="parallel.html#topic+makeCluster">makeCluster</a></code></p>
</td></tr>
<tr><td><code id="marqLevAlg_+3A_file">file</code></td>
<td>
<p>optional character giving the name of the file where the outputs
of each iteration should be written (if print.info=TRUE).</p>
</td></tr>
<tr><td><code id="marqLevAlg_+3A_.packages">.packages</code></td>
<td>
<p>for parallel setting only, packages used in the fn function</p>
</td></tr>
<tr><td><code id="marqLevAlg_+3A_minimize">minimize</code></td>
<td>
<p>logical indicating if the fn function should be minimized or maximized. By default minimize=TRUE, the function is minimized.</p>
</td></tr>
<tr><td><code id="marqLevAlg_+3A_...">...</code></td>
<td>
<p>other arguments of the fn, gr and hess functions</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Convergence criteria are very strict as they are based on derivatives of the
objective function in addition to the parameter and objective function stability. In
some cases, the program may not converge and reach the maximum number of
iterations fixed at 500.  In this case, the user should check that parameter
estimates at the last iteration are not on the boundaries of the parameter
space.  If the parameters are on the boundaries of the parameter space, the
identifiability of the model should be assessed.  If not, the program should
be run again with other initial values, with a higher maximum number of
iterations or less strict convergence tolerances. An alternative is to remove some
parameters from the Hessian matrix.
</p>


<h3>Value</h3>

<table>
<tr><td><code>cl</code></td>
<td>
<p> summary of the call to the function marqLevAlg.  </p>
</td></tr>
<tr><td><code>ni</code></td>
<td>
<p> number of marqLevAlg iterations before reaching stopping
criterion.  </p>
</td></tr> <tr><td><code>istop</code></td>
<td>
<p> status of convergence: =1 if the convergence
criteria were satisfied, =2 if the maximum number of iterations was reached,
=3 if convergence criteria with partial Hessian matrix were satisfied,
=4 if the algorithm encountered a problem in the function computation.  </p>
</td></tr>
<tr><td><code>v</code></td>
<td>
<p>if istop=1 or istop=3, vector containing the upper triangle matrix of variance-covariance
estimates at the stopping point. Otherwise v contains the second derivatives
of the fn function with respect to the parameters.</p>
</td></tr> <tr><td><code>grad</code></td>
<td>
<p>vector containing the gradient
at the stopping point.</p>
</td></tr> <tr><td><code>fn.value</code></td>
<td>
<p> function evaluation at
the stopping point.  </p>
</td></tr> <tr><td><code>b</code></td>
<td>
<p> stopping point value.  </p>
</td></tr> <tr><td><code>ca</code></td>
<td>

<p>convergence criteria for parameters stabilisation.  </p>
</td></tr> <tr><td><code>cb</code></td>
<td>
<p> convergence
criteria for function stabilisation.  </p>
</td></tr> <tr><td><code>rdm</code></td>
<td>
<p> convergence criteria on
the relative distance to minimum (or maximum).  </p>
</td></tr> <tr><td><code>time</code></td>
<td>
<p> a running time.  </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Melanie Prague, Viviane Philipps, Cecile Proust-Lima, Boris Hejblum, Daniel Commenges, Amadou Diakite
</p>


<h3>References</h3>

<p><em>marqLevAlg Algorithm</em>
</p>
<p>Donald W. marquardt An algorithm for Least-Squares Estimation of Nonlinear
Parameters. Journal of the Society for Industrial and Applied Mathematics,
Vol. 11, No. 2. (Jun, 1963), pp. 431-441.
</p>
<p><em>Convergence criteria : Relative distance to Minimim (or Maximum)</em>
</p>
<p>Commenges D. Jacqmin-Gadda H. Proust C. Guedj J. A Newton-like algorithm for
likelihood maximization the robust-variance scoring algorithm
arxiv:math/0610402v2 (2006)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

### example 1
### initial values
b &lt;- c(8,9)
### your function
f1 &lt;- function(b){
	return(-4*(b[1]-5)^2-(b[2]-6)^2)
}
### gradient
g1 &lt;- function(b){
     return(c(-8*(b[1]-5),-2*(b[2]-6)))
}
## Call
test1 &lt;- mla(b=b, fn=f1, minimize=FALSE)

## Not run: 
microbenchmark::microbenchmark(mla(b=b, fn=f1, minimize=FALSE),
                              mla(b=b, fn=f1, minimize=FALSE, nproc=2),
                              mla(b=b, fn=f1, gr=g1, minimize=FALSE),
                              mla(b=b, fn=f1, gr=g1, minimize=FALSE, nproc=2),
                              times=10)
        
## End(Not run)



### example 2
## initial values
b &lt;- c(3,-1,0,1)
## your function
f2 &lt;- function(b){
	return(-((b[1]+10*b[2])^2+5*(b[3]-b[4])^2+(b[2]-2*b[3])^4+10*(b[1]-b[4])^4))
}
## Call
test2 &lt;- mla(b=b, fn=f2, minimize=FALSE)
test2$b

test2_par &lt;- mla(b=b, fn=f2, minimize=FALSE, nproc=2)
test2_par$b

## Not run: 
microbenchmark::microbenchmark(mla(b=b, fn=f2, minimize=FALSE),
                              mla(b=b, fn=f2, minimize=FALSE, nproc=2),
                              times=10)
        
## End(Not run)


## Not run: 
### example 3 : a linear mixed model
## the log-likelihood is implemented in the loglikLMM function
## the gradient is implemented in the gradLMM function

## data
Y &lt;- dataEx$Y
X &lt;- as.matrix(cbind(1,dataEx[,c("t","X1","X3")],dataEx$t*dataEx$X1))
ni &lt;- as.numeric(table(dataEx$i))

## initial values
binit &lt;- c(0,0,0,0,0,1,1)

## estimation in sequential mode, with numeric derivatives
estim &lt;- marqLevAlg(b=binit, fn=loglikLMM, minimize=FALSE, X=X, Y=Y, ni=ni)
## estimation in parallel mode, with numeric derivatives
estim2 &lt;- marqLevAlg(b=binit, fn=loglikLMM, minimize=FALSE, X=X, Y=Y, ni=ni, 
nproc=2, clustertype="FORK")
## estimation in sequential mode, with analytic gradient
estim3 &lt;- marqLevAlg(b=binit, fn=loglikLMM, gr=gradLMM, minimize=FALSE, X=X, Y=Y, ni=ni)
## estimation in parallel mode, with analytic gradient
estim4 &lt;- marqLevAlg(b=binit, fn=loglikLMM, gr=gradLMM, minimize=FALSE, X=X, Y=Y, ni=ni, 
nproc=2, clustertype="FORK")

## End(Not run)
</code></pre>

<hr>
<h2 id='print.marqLevAlg'>Summary of a <code>marqLevAlg</code> object</h2><span id='topic+print.marqLevAlg'></span>

<h3>Description</h3>

<p>The function provides a summary of a <code>marqLevAlg</code> optimisation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'marqLevAlg'
print(x, digits = 8, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.marqLevAlg_+3A_x">x</code></td>
<td>
<p>a marqLevAlg object.</p>
</td></tr>
<tr><td><code id="print.marqLevAlg_+3A_digits">digits</code></td>
<td>
<p>Number of digits to print in outputs. Default value is 8.</p>
</td></tr>
<tr><td><code id="print.marqLevAlg_+3A_...">...</code></td>
<td>
<p>other (unusued) arguments.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>V. Philipps, C. Proust-Lima, B. Hejblum, D. Commenges, M. Prague, A. Diakite
</p>


<h3>See Also</h3>

<p><code>link{summary.marqLevAlg}</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
f1 &lt;- function(b){	
return(4*(b[1]-5)^2+(b[2]-6)^2)	
}
test.marq &lt;- marqLevAlg(b=c(8,9),m=2,maxiter=100,epsa=0.001,epsb=0.001,
epsd=0.001,fn=f1)

test.marq

</code></pre>

<hr>
<h2 id='summary.marqLevAlg'>Summary of optimization</h2><span id='topic+summary.marqLevAlg'></span>

<h3>Description</h3>

<p>A short summary of parameters estimates by marqLevAlg algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'marqLevAlg'
summary(object, digits = 8, loglik = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.marqLevAlg_+3A_object">object</code></td>
<td>
<p>a marqLevAlg object</p>
</td></tr>
<tr><td><code id="summary.marqLevAlg_+3A_digits">digits</code></td>
<td>
<p>Number of digits to print in outputs. Default value is 8.</p>
</td></tr>
<tr><td><code id="summary.marqLevAlg_+3A_loglik">loglik</code></td>
<td>
<p>Logical indicating if the objective function is a log-likelihood. By
default, loglik=FALSE.</p>
</td></tr>
<tr><td><code id="summary.marqLevAlg_+3A_...">...</code></td>
<td>
<p>other (unsued) arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame containing as many rows as estimated parameters. If
loglik=FALSE, it includes one column containing the estimated
parameters values. If loglik=TRUE, it includes 6 columns : the
estimated parameters, their standard errors, the corresponding Wald
statistic, the associated p-value and the boundaries of the 95% confidence
interval.
</p>


<h3>Author(s)</h3>

<p>V. Philipps, C. Proust-Lima, B. Hejblum, D. Commenges, M. Prague, A. Diakite
</p>


<h3>See Also</h3>

<p><code><a href="#topic+marqLevAlg">marqLevAlg</a></code>, <code><a href="#topic+print.marqLevAlg">print.marqLevAlg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
f1 &lt;- function(b){	
return(4*(b[1]-5)^2+(b[2]-6)^2)	
}
test.marq &lt;- marqLevAlg(b=c(8,9),m=2,maxiter=100,epsa=0.001,epsb=0.001,
epsd=0.001,fn=f1)

summary(test.marq)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
