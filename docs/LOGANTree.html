<!DOCTYPE html><html><head><title>Help for package LOGANTree</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {LOGANTree}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ChiSquarePlot'><p>Plot for Chi-square Statistics</p></a></li>
<li><a href='#ChiSquareTable'><p>Chi-square Statistics Table</p></a></li>
<li><a href='#ComputeChisquared'><p>Compute the chi-square scores of features</p></a></li>
<li><a href='#cp025q01.features'><p>Data for PISA 2012, CP025, Q01 (selected countries)</p></a></li>
<li><a href='#cp025q01.wgt'><p>Treated data for PISA 2012, CP025, Q01 (selected countries)</p></a></li>
<li><a href='#DataPartition'><p>Data Partition</p></a></li>
<li><a href='#DtResult'><p>Decision Tree Result in Text View and Plot</p></a></li>
<li><a href='#LOGANTree'><p>LOGANTree: Tree-based models for the analysis of log files from computer-based assessments</p></a></li>
<li><a href='#NearZeroVariance'><p>Flag the features that have (near) zero variance</p></a></li>
<li><a href='#PartialDependencePlot'><p>Partial Dependence Plot</p></a></li>
<li><a href='#PerformanceMetrics'><p>Report table with the performance metrics for tree-based learning methods</p></a></li>
<li><a href='#RocPlot'><p>ROC Curves Plot</p></a></li>
<li><a href='#testing'><p>PISA 2012, CP025, Q01 (selected countries) Testing Data Set</p></a></li>
<li><a href='#training'><p>PISA 2012, CP025, Q01 (selected countries) Training Data Set</p></a></li>
<li><a href='#TreeModels'><p>Tree-based Model Training</p></a></li>
<li><a href='#TreeModelsAllSteps'><p>Data Partition and Tree-based Model Training</p></a></li>
<li><a href='#VariableImportancePlot'><p>Barplot comparing the feature importance across different learning methods.</p></a></li>
<li><a href='#VariableImportanceTable'><p>Table comparing the feature importance for tree-based learning methods.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Tree-Based Models for the Analysis of Log Files from
Computer-Based Assessments</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Enables researchers to model log-file data from computer-based assessments using machine-learning techniques. It allows researchers to generate new knowledge by comparing the performance of three tree-based classification models (i.e., decision trees, random forest, and gradient boosting) to predict student's outcome. It also contains a set of handful functions for the analysis of the features' influence on the modeling. Data from the Climate control item from the 2012 Programme for International Student Assessment (PISA, <a href="https://www.oecd.org/pisa/">https://www.oecd.org/pisa/</a>) is available for an illustration of the package's capability. He, Q., &amp; von Davier, M. (2015) &lt;<a href="https://doi.org/10.1007%2F978-3-319-19977-1_13">doi:10.1007/978-3-319-19977-1_13</a>&gt; Boehmke, B., &amp; Greenwell, B. M. (2019) &lt;<a href="https://doi.org/10.1201%2F9780367816377">doi:10.1201/9780367816377</a>&gt; .</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>ROCR, caret, caretEnsemble, dplyr, ggplot2, rpart.plot,
tibble, gbm, stats</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-06-22 19:16:42 UTC; na.</td>
</tr>
<tr>
<td>Author:</td>
<td>Denise Reis Costa [aut, ths],
  Qi Qin [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Qi Qin &lt;logantreeqq@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-06-22 22:20:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='ChiSquarePlot'>Plot for Chi-square Statistics</h2><span id='topic+ChiSquarePlot'></span>

<h3>Description</h3>

<p>Plot for Chi-square Statistics
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ChiSquarePlot(
  trainingdata = NULL,
  nfeatureNames = NULL,
  outcome = NULL,
  level = NULL,
  ModelObject = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ChiSquarePlot_+3A_trainingdata">trainingdata</code></td>
<td>
<p>A data set used for training</p>
</td></tr>
<tr><td><code id="ChiSquarePlot_+3A_nfeaturenames">nfeatureNames</code></td>
<td>
<p>A vector of feature names that will be used for computing chi-square statistics</p>
</td></tr>
<tr><td><code id="ChiSquarePlot_+3A_outcome">outcome</code></td>
<td>
<p>A character string with the name of the binary outcome variable.</p>
</td></tr>
<tr><td><code id="ChiSquarePlot_+3A_level">level</code></td>
<td>
<p>A numerical value indicating the number of categories that the outcome contains</p>
</td></tr>
<tr><td><code id="ChiSquarePlot_+3A_modelobject">ModelObject</code></td>
<td>
<p>A model object containing tree-based models</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function returns a barplot of scaled chi-square statistics for the study’s features. These measures were computed as described by He &amp; von Davier (2015).
</p>


<h3>References</h3>

<p>He, Q., &amp; von Davier, M. (2015). Identifying feature sequences from process data in problem-solving items with N-grams. In Quantitative Psychology Research: The 79th Annual Meeting of the Psychometric Society (pp. 173–190). Madison, Wisconsin: Springer International Publishing.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
colnames(training)[14] &lt;- "perf"
ensemblist &lt;- TreeModels(traindata = training,
methodlist = c("dt", "gbm"),checkprogress = TRUE)

ChiSquarePlot(trainingdata = training,
nfeatureNames = colnames(training[,7:13]),
outcome = "perf", level = 2, ModelObject = ensemblist$ModelObject)

</code></pre>

<hr>
<h2 id='ChiSquareTable'>Chi-square Statistics Table</h2><span id='topic+ChiSquareTable'></span>

<h3>Description</h3>

<p>Chi-square Statistics Table
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ChiSquareTable(
  trainingdata = NULL,
  nfeatureNames = NULL,
  outcome = NULL,
  level = NULL,
  ModelObject = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ChiSquareTable_+3A_trainingdata">trainingdata</code></td>
<td>
<p>A data set used for training</p>
</td></tr>
<tr><td><code id="ChiSquareTable_+3A_nfeaturenames">nfeatureNames</code></td>
<td>
<p>A vector of feature names that will be used for computing chi-square statistics</p>
</td></tr>
<tr><td><code id="ChiSquareTable_+3A_outcome">outcome</code></td>
<td>
<p>A character string with the name of the binary outcome variable.</p>
</td></tr>
<tr><td><code id="ChiSquareTable_+3A_level">level</code></td>
<td>
<p>A numerical value indicating the number of categories that the outcome contains</p>
</td></tr>
<tr><td><code id="ChiSquareTable_+3A_modelobject">ModelObject</code></td>
<td>
<p>A model object containing tree-based models</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function returns a table with five columns. The chi-square statistics were computed as described by He &amp; von Davier (2015).
</p>
<p>Feature: Features names
</p>
<p>CvAverageChisq: Average chisquare statistics computed from 10-fold cross validation samples
</p>
<p>Rank.CvAverageChisq: Ordem of the feature importance from the CvAverageChisq measures#'
</p>
<p>OverallChisq: chisquare scores computed from the whole training sample
</p>
<p>Rank.OverallChisq: Ordem of the feature importance from the OverallChisq measures
</p>


<h3>References</h3>

<p>He, Q., &amp; von Davier, M. (2015). Identifying feature sequences from process data in problem-solving items with N-grams. In Quantitative Psychology Research: The 79th Annual Meeting of the Psychometric Society (pp. 173–190). Madison, Wisconsin: Springer International Publishing.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
colnames(training)[14] &lt;- "perf"
ensemblist &lt;- TreeModels(traindata = training,
methodlist = c("dt", "gbm"),checkprogress = TRUE)

ChiSquareTable(trainingdata=training,
nfeatureNames=colnames(training[,7:13]),
outcome = "perf",level = 2, ModelObject = ensemblist$ModelObject)

</code></pre>

<hr>
<h2 id='ComputeChisquared'>Compute the chi-square scores of features</h2><span id='topic+ComputeChisquared'></span>

<h3>Description</h3>

<p>Compute the chi-square scores of features
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ComputeChisquared(data, outcome, level, weight = FALSE, ctable = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ComputeChisquared_+3A_data">data</code></td>
<td>
<p>A dataset containing an outcome variable and action features with either raw frequencies or weighted frequencies.</p>
</td></tr>
<tr><td><code id="ComputeChisquared_+3A_outcome">outcome</code></td>
<td>
<p>Name of the outcome variable.</p>
</td></tr>
<tr><td><code id="ComputeChisquared_+3A_level">level</code></td>
<td>
<p>The level of outcome. e.g. correct/incorrect would be of 2 levels; 0/1/2 would be 3 levels</p>
</td></tr>
<tr><td><code id="ComputeChisquared_+3A_weight">weight</code></td>
<td>
<p>If weight = TRUE, the weighted frequencies will be computed and then be utilized for the chi-square scores ; If weight = F, returning the chisquare scores computed from the raw feature frequencies.</p>
</td></tr>
<tr><td><code id="ComputeChisquared_+3A_ctable">ctable</code></td>
<td>
<p>If ctable = TRUE, returning the contingency tables instead of the chi-square scores.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function returns a data frame with ranked chi-scores or contingency tables for each feature.
</p>
<p>To get the weighted frequencies solely, please run WeightedFeatures() in LOGAN package.
</p>


<h3>References</h3>

<p>He Q., von Davier M. (2015) Identifying Feature Sequences from Process Data in Problem-Solving Items with N-Grams. In: van der Ark L., Bolt D., Wang WC., Douglas J., Chow SM. (eds) Quantitative Psychology Research. Springer Proceedings in Mathematics &amp; Statistics, vol 140. Springer, Cham. https://doi-org.ezproxy.uio.no/10.1007/978-3-319-19977-1_13
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ComputeChisquared(data = cp025q01.wgt[,c(7:13,15)],
outcome = "outcome", level = 2, weight = FALSE, ctable = FALSE)

ComputeChisquared(data = training[,7:14],
outcome = "outcome", level = 2, weight = FALSE, ctable = TRUE)

</code></pre>

<hr>
<h2 id='cp025q01.features'>Data for PISA 2012, CP025, Q01 (selected countries)</h2><span id='topic+cp025q01.features'></span>

<h3>Description</h3>

<p>A dataset containing the original features generated from 2012 PISA Climate Control CP025Q01 task
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cp025q01.features
</code></pre>


<h3>Format</h3>

<p>A data frame with 1456 rows and 16 variables.
</p>


<h3>Source</h3>

<p><a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02461/full">https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02461/full</a>
</p>

<hr>
<h2 id='cp025q01.wgt'>Treated data for PISA 2012, CP025, Q01 (selected countries)</h2><span id='topic+cp025q01.wgt'></span>

<h3>Description</h3>

<p>A dataset containing the weighted features generated from 2012 PISA Climate Control CP025Q01 task
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cp025q01.wgt
</code></pre>


<h3>Format</h3>

<p>A data frame with 1456 rows and 15 variables.
</p>


<h3>Source</h3>

<p><a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02461/full">https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02461/full</a>
</p>

<hr>
<h2 id='DataPartition'>Data Partition</h2><span id='topic+DataPartition'></span>

<h3>Description</h3>

<p>Data Partition
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DataPartition(data = NULL, outcome = NULL, proportion = 0.7, seed = 2022)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DataPartition_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> that contains the study’s features and the outcome variable.</p>
</td></tr>
<tr><td><code id="DataPartition_+3A_outcome">outcome</code></td>
<td>
<p>A character string with the name of the outcome variable from the data.</p>
</td></tr>
<tr><td><code id="DataPartition_+3A_proportion">proportion</code></td>
<td>
<p>A numeric value for the proportion of data to be put into model training. Default is set to 0.7.</p>
</td></tr>
<tr><td><code id="DataPartition_+3A_seed">seed</code></td>
<td>
<p>A numeric value for set.seed. It is set to be 2022 by default.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function returns a list with training and testing data sets using a stratified selection by the outcome variable as performed by the createDataPartition function from the caret package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dp &lt;- DataPartition(data = cp025q01.wgt, outcome = "outcome")
</code></pre>

<hr>
<h2 id='DtResult'>Decision Tree Result in Text View and Plot</h2><span id='topic+DtResult'></span>

<h3>Description</h3>

<p>Decision Tree Result in Text View and Plot
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DtResult(ModelObject)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DtResult_+3A_modelobject">ModelObject</code></td>
<td>
<p>A fitted model object from TreeModels() or TreeModelsAllSteps() functions.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function returns the structure of the decision tree final model as a text view, and a plot of the rpart model object as displayed by the rpart.plot package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
colnames(training)[14] &lt;- "perf"
ensemblist &lt;- TreeModels(traindata = training,
methodlist = "dt",checkprogress = TRUE)

DtResult(ensemblist$ModelObject)

</code></pre>

<hr>
<h2 id='LOGANTree'>LOGANTree: Tree-based models for the analysis of log files from computer-based assessments</h2><span id='topic+LOGANTree'></span>

<h3>Description</h3>

<p>This package enables users to model log-file data from computer-based assessments using machine-learning techniques. It allows researchers to generate new knowledge by comparing the performance of three tree-based classification models (i.e., decision trees, random forest, and gradient boosting) to predict student’s outcome. It also contains a set of handful functions for the analysis of the features’ influence on the modeling. Data from the Climate control item from the 2012 Programme for International Student Assessment (PISA, &lt;https://www.oecd.org/pisa/&gt;) is available for an illustration of the package’s capability. An application of the package functions for a math item in PISA 2012 is described in Qin (2022).
</p>


<h3>LOGANTree functions</h3>

<p>The LOGANTree functions can be categorized in two types: (a) tree-based modeling and (b) features’ analysis. While the first one provides tools for the specification and the evaluation of the three classification models, the second category is devoted to a careful analysis of the data features and their influence on the model’s results. We use the caret package to perform most of the analyses and we provide summary reports and data visualization tools to better compare the three classifiers.
</p>
<p>What follows is a list of functions organized per category:
</p>
<p>Tree-based modeling:
</p>

<ul>
<li><p>TreeModels
</p>
</li>
<li><p>DataPartition
</p>
</li>
<li><p>TreeModelsAllSteps
</p>
</li>
<li><p>PerformanceMatrics
</p>
</li>
<li><p>RocPlot
</p>
</li></ul>

<p>Features’ analysis:
</p>

<ul>
<li><p>NearZeroVariance
</p>
</li>
<li><p>DtResult(
</p>
</li>
<li><p>VariableImportanceTable
</p>
</li>
<li><p>VariableImportancePlot
</p>
</li>
<li><p>ChisquareTable
</p>
</li>
<li><p>ChisquarePlot
</p>
</li>
<li><p>PartialDependencePlot
</p>
</li></ul>



<h3>Author(s)</h3>

 <ul>
<li><p>Qi Qin [aut, cre], </p>
</li>
<li><p>Denise Reis Costa [aut, ths]</p>
</li></ul>



<h3>References</h3>

<p>Qin, Q. (2022). Application of tree-based data mining techniques to examine log file data from a 2012 PISA computer-based Mathematics item. [Unpublished thesis]. University of Oslo.
</p>

<hr>
<h2 id='NearZeroVariance'>Flag the features that have (near) zero variance</h2><span id='topic+NearZeroVariance'></span>

<h3>Description</h3>

<p>Flag the features that have (near) zero variance
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NearZeroVariance(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NearZeroVariance_+3A_data">data</code></td>
<td>
<p>A dataset containing the study’s features.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function returns a dataframe with feature names and their frequency ratio, percentage of the unique value and logic values indicating whether the feature is zero variance or has near zero variance.
</p>
<p>feature : name of the features.
</p>
<p>flag.zv (Flag Zero Variance) : True/False, flagging zero variance.
</p>
<p>fr (Frequency Ratio) : the ratio of the value with the highest frequency over the value with the second highest frequency.
</p>
<p>puv (Percentage of Unique Values) : number of the unique values divided by the total number of samples.
</p>
<p>flag.nzv (Flag Near Zero Variance) : True/False, flagging near zero variance.
</p>


<h3>References</h3>

<p>Boehmke, B., &amp; Greenwell, B. M. (2019). Hands-on machine learning with R. CRC Press.p.52-55. https://doi-org.ezproxy.uio.no/10.1201/9780367816377
</p>


<h3>Examples</h3>

<pre><code class='language-R'>NearZeroVariance(training)
</code></pre>

<hr>
<h2 id='PartialDependencePlot'>Partial Dependence Plot</h2><span id='topic+PartialDependencePlot'></span>

<h3>Description</h3>

<p>Partial Dependence Plot
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PartialDependencePlot(
  data = NULL,
  FeatureNames = NULL,
  FittedModelObject = NULL,
  j = 20
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PartialDependencePlot_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> that contains the study’s features and the outcome.</p>
</td></tr>
<tr><td><code id="PartialDependencePlot_+3A_featurenames">FeatureNames</code></td>
<td>
<p>A vector with the names of features to plot.</p>
</td></tr>
<tr><td><code id="PartialDependencePlot_+3A_fittedmodelobject">FittedModelObject</code></td>
<td>
<p>A fitted model object.</p>
</td></tr>
<tr><td><code id="PartialDependencePlot_+3A_j">j</code></td>
<td>
<p>A numerical value that indicates the size of the equally spaced values for the feature of interest.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function returns a plot where X axis presents the values for each feature and Y axis illustrates the predicted proportion of correct answer to the item.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
colnames(training)[14] &lt;- "perf"
ensemblist &lt;- TreeModels(traindata = training,
methodlist = c("dt","rf"),checkprogress = TRUE)

PartialDependencePlot(data = training,
FeatureNames = colnames(training[-c(4,14)]),
FittedModelObject = ensemblist$ModelObject$rpart, j = 30)

PartialDependencePlot(data = training,
FeatureNames = colnames(training[-c(4,14)]),
FittedModelObject = ensemblist$ModelObject$ranger, j = 20)

</code></pre>

<hr>
<h2 id='PerformanceMetrics'>Report table with the performance metrics for tree-based learning methods</h2><span id='topic+PerformanceMetrics'></span>

<h3>Description</h3>

<p>Report table with the performance metrics for tree-based learning methods
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PerformanceMetrics(
  testdata,
  DT = NULL,
  RF = NULL,
  GBM = NULL,
  outcome,
  reflevel
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PerformanceMetrics_+3A_testdata">testdata</code></td>
<td>
<p>A test dataset that contains the study’s features and the outcome variable.</p>
</td></tr>
<tr><td><code id="PerformanceMetrics_+3A_dt">DT</code></td>
<td>
<p>A fitted decision tree model object</p>
</td></tr>
<tr><td><code id="PerformanceMetrics_+3A_rf">RF</code></td>
<td>
<p>A fitted random forest model object</p>
</td></tr>
<tr><td><code id="PerformanceMetrics_+3A_gbm">GBM</code></td>
<td>
<p>A fitted gradient boosting model object</p>
</td></tr>
<tr><td><code id="PerformanceMetrics_+3A_outcome">outcome</code></td>
<td>
<p>A factor variable with the outcome levels.</p>
</td></tr>
<tr><td><code id="PerformanceMetrics_+3A_reflevel">reflevel</code></td>
<td>
<p>A character string with the quoted reference level of outcome.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function returns a <code>data.frame</code> with a table that compares five performance metrics from different tree-based machine learning methods. The metrics are: Accuracy, Kappa, Sensitivity, Specificity, and Precision. The results are derived from the confusionMatrix function from the caret package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
colnames(training)[14] &lt;- "perf"
ensemblist &lt;- TreeModels(traindata = training,
methodlist = c("dt", "rf","gbm"),checkprogress = TRUE)

PerformanceMetrics(testdata = testing, RF = ensemblist$ModelObject$ranger,
outcome = "outcome", reflevel = "correct")

PerformanceMetrics(testdata = testing, RF = ensemblist$ModelObject$ranger,
GBM = ensemblist$ModelObject$gbm,
outcome = "outcome", reflevel = "correct")

PerformanceMetrics(testdata = testing, DT = ensemblist$ModelObject$rpart,
RF = ensemblist$ModelObject$ranger, GBM = ensemblist$ModelObject$gbm,
outcome = "outcome", reflevel = "correct")

</code></pre>

<hr>
<h2 id='RocPlot'>ROC Curves Plot</h2><span id='topic+RocPlot'></span>

<h3>Description</h3>

<p>ROC Curves Plot
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RocPlot(ModelObject, testdata, outcome, reflevel)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RocPlot_+3A_modelobject">ModelObject</code></td>
<td>
<p>An object obtained from TreeModels() or TreeModelsAllSteps() functions.</p>
</td></tr>
<tr><td><code id="RocPlot_+3A_testdata">testdata</code></td>
<td>
<p>A testing dataset.</p>
</td></tr>
<tr><td><code id="RocPlot_+3A_outcome">outcome</code></td>
<td>
<p>A character string with the name of the binary outcome variable.</p>
</td></tr>
<tr><td><code id="RocPlot_+3A_reflevel">reflevel</code></td>
<td>
<p>A character string with the quoted reference level of outcome.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function returns a plot with ROC curves for the selected tree-based models (i.e., decision tree, random forest, or gradient boosting).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
colnames(training)[14] &lt;- "perf"
colnames(testing)[14] &lt;- "perf"
ensemblist &lt;- TreeModels(traindata = training,
methodlist = c("dt", "gbm","rf"),checkprogress = TRUE)

RocPlot(ModelObject = ensemblist$ModelObject, testdata = testing,
outcome = "perf", reflevel = "incorrect")

</code></pre>

<hr>
<h2 id='testing'>PISA 2012, CP025, Q01 (selected countries) Testing Data Set</h2><span id='topic+testing'></span>

<h3>Description</h3>

<p>A testing set partitioned from the cp025q01.wgt dataset with 30
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testing
</code></pre>


<h3>Format</h3>

<p>A data frame with 436 rows and 14 variables.
</p>


<h3>Source</h3>

<p><a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02461/full">https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02461/full</a>
</p>

<hr>
<h2 id='training'>PISA 2012, CP025, Q01 (selected countries) Training Data Set</h2><span id='topic+training'></span>

<h3>Description</h3>

<p>A training set partitioned from the cp025q01.wgt dataset with 70
</p>


<h3>Usage</h3>

<pre><code class='language-R'>training
</code></pre>


<h3>Format</h3>

<p>A data frame with 1020 rows and 14 variables.
</p>


<h3>Source</h3>

<p><a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02461/full">https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02461/full</a>
</p>

<hr>
<h2 id='TreeModels'>Tree-based Model Training</h2><span id='topic+TreeModels'></span>

<h3>Description</h3>

<p>Tree-based Model Training
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TreeModels(
  traindata = NULL,
  seed = 2022,
  methodlist = c("dt", "rf", "gbm"),
  iternumber = 10,
  dt.gridsearch = NULL,
  rf.gridsearch = NULL,
  gbm.gridsearch = NULL,
  checkprogress = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TreeModels_+3A_traindata">traindata</code></td>
<td>
<p>A <code>data.frame</code> with the training data set. Please name the outcome variable as &quot;perf&quot;.</p>
</td></tr>
<tr><td><code id="TreeModels_+3A_seed">seed</code></td>
<td>
<p>A numeric value for set.seed. It is set to be 2022 by default.</p>
</td></tr>
<tr><td><code id="TreeModels_+3A_methodlist">methodlist</code></td>
<td>
<p>A list of the tree-based methods to model. The default is methodlist = c(&quot;dt&quot;, &quot;rf&quot;, &quot;gbm&quot;).</p>
</td></tr>
<tr><td><code id="TreeModels_+3A_iternumber">iternumber</code></td>
<td>
<p>Number of resampling iterations/Number of folds for the  cross-validation scheme.</p>
</td></tr>
<tr><td><code id="TreeModels_+3A_dt.gridsearch">dt.gridsearch</code></td>
<td>
<p>A <code>data.frame</code> of the tuning grid,  which allows for specifying parameters for decision tree model.</p>
</td></tr>
<tr><td><code id="TreeModels_+3A_rf.gridsearch">rf.gridsearch</code></td>
<td>
<p>A <code>data.frame</code> of the tuning grid,  which allows for specifying parameters for random forest model.</p>
</td></tr>
<tr><td><code id="TreeModels_+3A_gbm.gridsearch">gbm.gridsearch</code></td>
<td>
<p>A <code>data.frame</code> of the tuning grid,  which allows for specifying parameters for gradient boosting model.</p>
</td></tr>
<tr><td><code id="TreeModels_+3A_checkprogress">checkprogress</code></td>
<td>
<p>Logical. Print the modeling progress if it is TRUE. The default is FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function performs the modeling step of a predictive analysis. The selected classifiers are used for modeling the provided training dataset under a cross-validation scheme. Users have the possibility to choose which model they want to compare by specifying it on the <code>methodlist</code> argument. The caretEnsemble package is used in the modeling process to ensure that all models follow the same resampling procedures. ROC is used to select the optimal model for each tree-based method using the largest value. Finally, a summary report is displayed.
</p>


<h3>Value</h3>

<p>This function returns two lists:
</p>
<p>ModelObject An object with results from selected models
</p>
<p>SummaryReport A <code>data.frame</code> with the summary of model parameters. The summary report is shown automatically in the output.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
colnames(training)[14] &lt;- "perf"
ensemblist &lt;- TreeModels(traindata = training,
methodlist = c("rf","gbm","dt"),checkprogress = TRUE)

ensemblist &lt;- TreeModels(traindata = training,
methodlist = c("rf"),
rf.gridsearch = data.frame(mtry = 2, splitrule = "gini", min.node.size = 1))

</code></pre>

<hr>
<h2 id='TreeModelsAllSteps'>Data Partition and Tree-based Model Training</h2><span id='topic+TreeModelsAllSteps'></span>

<h3>Description</h3>

<p>Data Partition and Tree-based Model Training
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TreeModelsAllSteps(
  data = NULL,
  proportion = 0.7,
  seed = 2022,
  methodlist = c("dt", "rf", "gbm"),
  iternumber = 10,
  dt.gridsearch = NULL,
  rf.gridsearch = NULL,
  gbm.gridsearch = NULL,
  checkprogress = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TreeModelsAllSteps_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> that contains the study’s features and the outcome variable.
Please name the outcome variable as &quot;perf&quot;.</p>
</td></tr>
<tr><td><code id="TreeModelsAllSteps_+3A_proportion">proportion</code></td>
<td>
<p>A numeric value for the proportion of data to be put into model training. Default is set to 0.7.</p>
</td></tr>
<tr><td><code id="TreeModelsAllSteps_+3A_seed">seed</code></td>
<td>
<p>A numeric value for set.seed. It is set to be 2022 by default.</p>
</td></tr>
<tr><td><code id="TreeModelsAllSteps_+3A_methodlist">methodlist</code></td>
<td>
<p>A list of the tree-based methods to model. The default is methodlist = c(&quot;dt&quot;, &quot;rf&quot;, &quot;gbm&quot;).</p>
</td></tr>
<tr><td><code id="TreeModelsAllSteps_+3A_iternumber">iternumber</code></td>
<td>
<p>A numeric value for the number of resampling iterations/number of folds for the  cross-validation scheme.</p>
</td></tr>
<tr><td><code id="TreeModelsAllSteps_+3A_dt.gridsearch">dt.gridsearch</code></td>
<td>
<p>A <code>data.frame</code> of the tuning grid, which allows for specifying parameters for decision tree model.</p>
</td></tr>
<tr><td><code id="TreeModelsAllSteps_+3A_rf.gridsearch">rf.gridsearch</code></td>
<td>
<p>A <code>data.frame</code> of the tuning grid, which allows for specifying parameters for random forest model.</p>
</td></tr>
<tr><td><code id="TreeModelsAllSteps_+3A_gbm.gridsearch">gbm.gridsearch</code></td>
<td>
<p>A <code>data.frame</code> of the tuning grid, which allows for specifying parameters for gradient boosting model.</p>
</td></tr>
<tr><td><code id="TreeModelsAllSteps_+3A_checkprogress">checkprogress</code></td>
<td>
<p>Logical. Print the modeling progress if it is TRUE. The default is FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function performs all the steps of a predictive analysis. First, the data is partitioned in the training and testing datasets using a stratified selection by the outcome variable as performed by the createDataPartition function from the caret package. Then, the selected classifiers are used for modeling the training dataset under a cross-validation scheme. Users have the possibility to choose which model they want to compare by specifying it on the <code>methodlist</code> argument. The caretEnsemble package is used in the modeling process to ensure that all models follow the same resampling procedures. ROC is used to select the optimal model for each tree-based method using the largest value. Finally, a summary report is displayed.
</p>


<h3>Value</h3>

<p>This function returns three lists:
</p>
<p>DataPartition The partitioned datasets: training (cv_train) and testing (cv_test).
</p>
<p>ModelObject An object with results from selected models
</p>
<p>SummaryReport A <code>data.frame</code> with the summary of model parameters. The summary report is shown automatically in the output.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
cp025q01.wgt &lt;- cp025q01.wgt[,-14]
colnames(cp025q01.wgt)[14] &lt;- "perf"

ensemblist &lt;- TreeModelsAllSteps(data = cp025q01.wgt,
checkprogress = TRUE)

ensemblist &lt;- TreeModelsAllSteps(data = cp025q01.wgt,
methodlist = c("dt", "gbm"), checkprogress = TRUE)

ensemblist &lt;- TreeModelsAllSteps(data = cp025q01.wgt,
methodlist = c("rf"),
rf.gridsearch = data.frame(mtry = 2, splitrule = "gini", min.node.size = 1),
checkprogress = TRUE)

</code></pre>

<hr>
<h2 id='VariableImportancePlot'>Barplot comparing the feature importance across different learning methods.</h2><span id='topic+VariableImportancePlot'></span>

<h3>Description</h3>

<p>Barplot comparing the feature importance across different learning methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>VariableImportancePlot(DT = NULL, RF = NULL, GBM = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="VariableImportancePlot_+3A_dt">DT</code></td>
<td>
<p>A fitted decision tree model object</p>
</td></tr>
<tr><td><code id="VariableImportancePlot_+3A_rf">RF</code></td>
<td>
<p>A fitted random forest model object</p>
</td></tr>
<tr><td><code id="VariableImportancePlot_+3A_gbm">GBM</code></td>
<td>
<p>A fitted gradient boosting model object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function returns a barplot that compares the standardized feature importance across different tree-based machine learning methods. These measures are computed via the caret package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(gbm)
colnames(training)[14] &lt;- "perf"
ensemblist &lt;- TreeModels(traindata = training,
methodlist = c("dt", "rf","gbm"),checkprogress = TRUE)

VariableImportancePlot(DT = ensemblist$ModelObject$rpart,
RF = ensemblist$ModelObject$ranger,GBM = ensemblist$ModelObject$gbm)

VariableImportancePlot(RF = ensemblist$ModelObject$ranger,
GBM = ensemblist$ModelObject$gbm)

VariableImportancePlot(DT = ensemblist$ModelObject$rpart)

</code></pre>

<hr>
<h2 id='VariableImportanceTable'>Table comparing the feature importance for tree-based learning methods.</h2><span id='topic+VariableImportanceTable'></span>

<h3>Description</h3>

<p>Table comparing the feature importance for tree-based learning methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>VariableImportanceTable(DT = NULL, RF = NULL, GBM = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="VariableImportanceTable_+3A_dt">DT</code></td>
<td>
<p>A fitted decision tree model object</p>
</td></tr>
<tr><td><code id="VariableImportanceTable_+3A_rf">RF</code></td>
<td>
<p>A fitted random forest model object</p>
</td></tr>
<tr><td><code id="VariableImportanceTable_+3A_gbm">GBM</code></td>
<td>
<p>A fitted gradient boosting model object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function returns a data frame that compares the feature importance from different tree-based machine learning methods. These measures are computed via the caret package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(gbm)
colnames(training)[14] &lt;- "perf"
ensemblist &lt;- TreeModels(traindata = training,
methodlist = c("dt", "rf","gbm"),checkprogress = TRUE)

VariableImportanceTable(DT = ensemblist$ModelObject$rpart,
RF = ensemblist$ModelObject$ranger,GBM = ensemblist$ModelObject$gbm)

VariableImportanceTable(DT = ensemblist$ModelObject$rpart,
RF = ensemblist$ModelObject$ranger)

VariableImportanceTable(DT = ensemblist$ModelObject$rpart)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
