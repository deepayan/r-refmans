<!DOCTYPE html><html><head><title>Help for package npbr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {npbr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#air'><p>European air controllers</p></a></li>
<li><a href='#cub_spline_est'>
<p>Cubic spline fitting</p></a></li>
<li><a href='#cub_spline_kn'>
<p>AIC and BIC criteria for choosing the number of inter-knot segments in cubic spline fits</p></a></li>
<li><a href='#dea_est'>
<p>DEA, FDH and linearized FDH estimators.</p></a></li>
<li><a href='#dfs_momt'>
<p>Moment frontier estimator</p></a></li>
<li><a href='#dfs_pick'>
<p>Pickands frontier estimator</p></a></li>
<li><a href='#dfs_pwm'>
<p>Probability-weighted moment frontier estimator</p></a></li>
<li><a href='#green'>
<p>American electric utility companies</p></a></li>
<li><a href='#kern_smooth'>
<p>Frontier estimation via kernel smoothing</p></a></li>
<li><a href='#kern_smooth_bw'>
<p>Bandwidth selection for kernel smoothing frontier estimators</p></a></li>
<li><a href='#kopt_momt_pick'>
<p>Optimal <code class="reqn">k</code> in moment and Pickands frontier estimators</p></a></li>
<li><a href='#loc_est'>
<p>Local linear frontier estimator</p></a></li>
<li><a href='#loc_est_bw'>
<p>Bandwidth selection for the local linear frontier estimator</p></a></li>
<li><a href='#loc_max'>
<p>Local maximum frontier estimators</p></a></li>
<li><a href='#mopt_pwm'>
<p>Threshold selection for the PWM frontier estimator</p></a></li>
<li><a href='#npbr-package'><p>Nonparametric boundary regression</p></a></li>
<li><a href='#nuclear'><p>Reliability programs of nuclear reactors</p></a></li>
<li><a href='#pick_est'>
<p>Local Pickands' frontier estimator</p></a></li>
<li><a href='#poly_degree'>
<p>AIC and BIC criteria for choosing the optimal degree of the polynomial frontier estimator</p></a></li>
<li><a href='#poly_est'>
<p>Polynomial frontier estimators</p></a></li>
<li><a href='#post'><p>European postal services</p></a></li>
<li><a href='#quad_spline_est'>
<p>Quadratic spline frontiers</p></a></li>
<li><a href='#quad_spline_kn'>
<p>AIC and BIC criteria for choosing the optimal number of inter-knot segments in quadratic spline fits</p></a></li>
<li><a href='#records'><p>Annual sport records</p></a></li>
<li><a href='#rho_momt_pick'>
<p>Optimal rho for moment and Pickands frontier estimator</p></a></li>
<li><a href='#rho_pwm'>
<p>Probability-weighted moment frontier estimator</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Nonparametric Boundary Regression</td>
</tr>
<tr>
<td>Version:</td>
<td>1.8</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-03-21</td>
</tr>
<tr>
<td>Author:</td>
<td>Abdelaati Daouia &lt;Abdelaati.Daouia@tse-fr.eu&gt;, Thibault Laurent &lt;thibault.laurent@univ-tlse1.fr&gt;, Hohsuk Noh &lt;word5810@gmail.com&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Thibault Laurent &lt;thibault.laurent@univ-tlse1.fr&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0), graphics, stats, utils</td>
</tr>
<tr>
<td>Imports:</td>
<td>Benchmarking, np, quadprog, Rglpk (&ge; 0.6-2), splines</td>
</tr>
<tr>
<td>Description:</td>
<td>A variety of functions for the best known and most innovative approaches to nonparametric boundary estimation. The selected methods are concerned with empirical, smoothed, unrestricted as well as constrained fits under both separate and multiple shape constraints. They cover robust approaches to outliers  as well as data envelopment techniques based on piecewise polynomials, splines, local linear fitting, extreme values and kernel smoothing. The package also seamlessly allows for Monte Carlo comparisons among these different estimation methods.  Its use is illustrated via a number of empirical applications and simulated examples.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-03-21 16:06:46 UTC; laurent</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-03-22 09:00:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='air'>European air controllers 
</h2><span id='topic+air'></span>

<h3>Description</h3>

<p>The dataset is concerned with the assessment of the efficiency of 37 European Air Controllers. The performance of each controller can be measured by its &ldquo;distance&rdquo; 
from the upper support boundary, or equivalently, the set of the most efficient controllers. This dataset is taken from Mouchart and Simar (2002). 
Here, the activity of the controllers is described by one input (an aggregate factor of different kind of labor) 
and one output (an aggregate factor of the activity produced, based on the number of  controlled air movements, the number of  controlled flight hours, etc.). 
See also Daouia, Florens and Simar (2008). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(air)</code></pre>


<h3>Format</h3>

<p>A data frame with 37 observations on the following 2 variables.
</p>

<dl>
<dt><code>xtab</code></dt><dd><p>an input.</p>
</dd>
<dt><code>ytab</code></dt><dd><p>an output.</p>
</dd>
</dl>



<h3>References</h3>

<p>Daouia, A., Florens, J.-P. and Simar, L. (2008). Functional Convergence of Quantile-type Frontiers with Application to Parametric Approximations. <em>Journal of Statistical Planning and Inference</em>, 138, 708-725.
</p>
<p>Mouchart, M. and L. Simar (2002). Efficiency analysis of Air Controllers:  first insights, Consulting report 0202, Institut de Statistique, UCL, Belgium.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("air")
</code></pre>

<hr>
<h2 id='cub_spline_est'>
Cubic spline fitting 
</h2><span id='topic+cub_spline_est'></span>

<h3>Description</h3>

<p>The function cub_spline_est is an implementation of the (un)constrained cubic spline estimates proposed by Daouia, Noh and Park (2016).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cub_spline_est(xtab, ytab, x, kn = ceiling((length(xtab))^(1/4)), method= "u",
               all.dea=FALSE, control = list("tm_limit" = 700))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cub_spline_est_+3A_xtab">xtab</code></td>
<td>
<p>a numeric vector containing the observed inputs  <code class="reqn">x_1,\ldots,x_n</code>.</p>
</td></tr>
<tr><td><code id="cub_spline_est_+3A_ytab">ytab</code></td>
<td>
<p>a numeric vector of the same length as <code>xtab</code> containing the observed outputs <code class="reqn">y_1,\ldots,y_n</code>.</p>
</td></tr>
<tr><td><code id="cub_spline_est_+3A_x">x</code></td>
<td>
<p>a numeric vector of evaluation points in which the estimator is to be computed.</p>
</td></tr>
<tr><td><code id="cub_spline_est_+3A_kn">kn</code></td>
<td>
<p>an integer specifying the number of inter-knot segments used in the computation of the spline estimate.</p>
</td></tr>
<tr><td><code id="cub_spline_est_+3A_method">method</code></td>
<td>
<p>a character equal to &quot;u&quot; (unconstrained estimator), &quot;m&quot; (under the monotonicity constraint) or &quot;mc&quot; (under simultaneous monotonicity and concavity constraints).</p>
</td></tr>
<tr><td><code id="cub_spline_est_+3A_all.dea">all.dea</code></td>
<td>
<p>a boolean.</p>
</td></tr>
<tr><td><code id="cub_spline_est_+3A_control">control</code></td>
<td>
<p>a list of parameters to the GLPK solver. See *Details* of help(Rglpk_solve_LP).</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>Let <code class="reqn">a</code> and <code class="reqn">b</code> be, respectively, the minimum and maximum of the design points <code class="reqn">x_1,\ldots,x_n</code>.  
Denote a partition of <code class="reqn">[a,b]</code> by <code class="reqn">a=t_0&lt;t_1&lt;\cdots&lt;t_{k_n}=b</code> (see below the selection process).
Let <code class="reqn">N=k_n+3</code> and <code class="reqn">\pi(x)=(\pi_0(x),\ldots,\pi_{N-1}(x))^T</code> 
be the vector of normalized B-splines of order 4 based on the knot mesh <code class="reqn">\{t_j\}</code> (see Daouia et al. (2015)). The
unconstrained (option <code>method="u"</code>) cubic spline estimate of the frontier <code class="reqn">\varphi(x)</code> is then
defined by <code class="reqn">\tilde\varphi_n(x)=\pi(x)^T \tilde\alpha</code>, where <code class="reqn">\tilde\alpha</code> minimizes
</p>
<p style="text-align: center;"><code class="reqn">\int_{0}^1\pi(x)^T \alpha \,dx = \sum_{j=1}^N \alpha_j \int_{0}^1\pi_j(x) \,dx</code>
</p>

<p>over <code class="reqn">\alpha\in\R^N</code> subject to the envelopment constraints
<code class="reqn">\pi(x_i)^T \alpha\geq y_i</code>,  <code class="reqn">i=1,\ldots,n</code>.
A simple way of choosing the knot mesh in this unconstrained setting is by considering the  
<code class="reqn">j/k_n</code>th quantiles <code class="reqn">t_j = x_{[j n/k_n]}</code>  of the distinct values of <code class="reqn">x_i</code> for <code class="reqn">j=1,\ldots,k_n-1</code>. 
The number of inter-knot segments <code class="reqn">k_n</code> is obtained by calling the function <code><a href="#topic+cub_spline_kn">cub_spline_kn</a></code> (option <code>method="u"</code>).
</p>
<p>In what concerns the monotonicity constraint, we use the following suffcient condtion for the monotonicity:
</p>
<p style="text-align: center;"><code class="reqn">\alpha_0 \leq \alpha_1 \leq \cdots \leq \alpha_{N-1}</code>
</p>
<p>.
This condition was previously used in Lu et al. (2007) and Pya and Wood (2015). Note that since the condition corresponds to linear constraints on <code class="reqn">\alpha</code>, the estimator satisfying the monotonocity can be obtained via linear programming.  
</p>
<p>When the estimate is required to be both monotone and concave, we use the function <code>cub_spline_est</code> with the option <code>method="mc"</code>. Such estimate is obtained as the cubic spline function which minimizes the same linear objective function as the unconstrained estimate subject to the same linear envelopment constraints, the monotonicity constraint above and the additional linear concavity constraints <code class="reqn">\pi''(t_j)^T \alpha\leq , j=0,1,\ldots,k_n</code>, where the second derivative <code class="reqn">\pi''</code> is a linear spline. Regarding the choice of knots, we just apply the same scheme as for the unconstrained cubic spline estimate. 
</p>


<h3>Value</h3>

<p>Returns a numeric vector with the same length as <code>x</code>. Returns a vector of NA if no solution has been found by the solver (GLPK). 
</p>


<h3>Author(s)</h3>

<p>Hohsuk Noh
</p>


<h3>References</h3>

<p>Daouia, A., Noh, H. and Park, B.U. (2016). Data Envelope fitting with constrained polynomial splines. <em>Journal of the Royal Statistical Society: Series B</em>, <b>78</b>(1), 3-30. doi:10.1111/rssb.12098.
</p>
<p>Lu, M., Zhang, Y. and Huang, J. (2007). Estimation of the mean function with panel count data using monotone polynomial splines. <em>Biometrika</em>, <b>94</b>, 705-718.
</p>
<p>Pya, N. and Wood, S. N. (2015). Shape constrained additive models. <em>Statistical Computing</em>, to appear.
</p>
<p>Schumaker, L.L. (2007). <em>Spline Functions: Basic Theory</em>, 3rd edition, Cambridge University Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cub_spline_kn">cub_spline_kn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("air")
x.air &lt;- seq(min(air$xtab), max(air$xtab), length.out=101)
 
# 1. Unconstrained cubic spline fits
# Optimal number of inter-knot segments via the BIC criterion
(kn.bic.air.u&lt;-cub_spline_kn(air$xtab, air$ytab, 
 method="u", type="BIC"))
# Unconstrained spline estimate
y.cub.air.u&lt;-cub_spline_est(air$xtab, air$ytab, 
 x.air, kn=kn.bic.air.u, method="u")

# 2. Monotonicity constraint
# Optimal number of inter-knot segments via the BIC criterion
(kn.bic.air.m&lt;-cub_spline_kn(air$xtab, air$ytab, 
 method="m", type="BIC")) 
# Monotonic splines estimate
y.cub.air.m&lt;-cub_spline_est(air$xtab, air$ytab, 
 x.air, kn=kn.bic.air.m, method="m")
   
# 3. Monotonicity and Concavity constraints
# Optimal number of inter-knot segments via the BIC criterion
(kn.bic.air.mc&lt;-cub_spline_kn(air$xtab, air$ytab, 
 method="mc", type="BIC"))
# Monotonic/Concave splines estimate 
y.cub.air.mc&lt;-cub_spline_est(air$xtab, air$ytab, 
 x.air, kn=kn.bic.air.mc, method="mc", all.dea=TRUE)

# Representation 
plot(x.air, y.cub.air.u, lty=1, lwd=4, col="green", 
 type="l", xlab="log(COST)", ylab="log(OUTPUT)")   
lines(x.air, y.cub.air.m, lty=2, lwd=4, col="cyan")
lines(x.air, y.cub.air.mc, lty=3, lwd=4, col="magenta")   
points(ytab~xtab, data=air)
legend("topleft", col=c("green","cyan","magenta"), 
lty=c(1,2,3), legend=c("unconstrained", "monotone", 
 "monotone + concave"), lwd=4, cex=0.8)    
</code></pre>

<hr>
<h2 id='cub_spline_kn'>
AIC and BIC criteria for choosing the number of inter-knot segments in cubic spline fits  
</h2><span id='topic+cub_spline_kn'></span>

<h3>Description</h3>

<p>Computes the optimal number of inter-knot segments for the (un)constrained cubic spline fit proposed by Daouia, Noh and Park (2016).</p>


<h3>Usage</h3>

<pre><code class='language-R'>cub_spline_kn(xtab, ytab, method, krange = 1:20, type = "AIC", 
 control = list("tm_limit" = 700))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cub_spline_kn_+3A_xtab">xtab</code></td>
<td>
<p>a numeric vector containing the observed inputs  <code class="reqn">x_1,\ldots,x_n</code>.</p>
</td></tr>
<tr><td><code id="cub_spline_kn_+3A_ytab">ytab</code></td>
<td>
<p>a numeric vector of the same length as <code>xtab</code> containing the observed outputs <code class="reqn">y_1,\ldots,y_n</code>.</p>
</td></tr>
<tr><td><code id="cub_spline_kn_+3A_method">method</code></td>
<td>
<p>a character equal to &quot;u&quot; (unconstrained estimator), &quot;m&quot; (under the monotonicity constraint) or &quot;mc&quot; (under simultaneous monotonicity and concavity constraints).</p>
</td></tr>
<tr><td><code id="cub_spline_kn_+3A_krange">krange</code></td>
<td>
<p>a vector of integers specifying the range in which the optimal number of inter-knot segments is to be selected.</p>
</td></tr>
<tr><td><code id="cub_spline_kn_+3A_type">type</code></td>
<td>
<p>a character equal to &quot;AIC&quot; or &quot;BIC&quot;.</p>
</td></tr>
<tr><td><code id="cub_spline_kn_+3A_control">control</code></td>
<td>
<p>a list of parameters to the GLPK solver. See *Details* of help(Rglpk_solve_LP).</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>The implementation of the unconstrained cubic spline smoother <code class="reqn">\tilde\varphi_n</code> (see <code><a href="#topic+cub_spline_est">cub_spline_est</a></code>) 
is based on the knot mesh <code class="reqn">\{t_j\}</code>, with <code class="reqn">t_j = x_{[j n/k_n]}</code> being the <code class="reqn">j/k_n</code>th quantile 
of the distinct values of <code class="reqn">x_i</code> for <code class="reqn">j=1,\ldots,k_n-1</code>. 
Because the number of knots <code class="reqn">k_n</code> determines the complexity of the spline approximation,
its choice may then be viewed as model selection through the minimization of the following two information criteria:
</p>
<p style="text-align: center;"><code class="reqn">
AIC(k) = \log \left( \sum_{i=1}^{n} (\tilde \varphi_n(x_i)- y_i) \right) + (k+2)/n,</code>
</p>

<p style="text-align: center;"><code class="reqn">BIC(k) = \log \left( \sum_{i=1}^{n} (\tilde \varphi_n(x_i) - y_i) \right) + \log n \cdot (k+2)/2n.</code>
</p>

<p>The first one (option <code>type = "AIC"</code>) is similar to the famous Akaike information criterion (Akaike, 1973) and the second one
(option <code>type = "BIC"</code>) to the Bayesian information criterion (Schwartz, 1978).
For the implementation of the concave cubic spline estimator, just apply the same scheme as for the unconstrained version.
</p>


<h3>Value</h3>

<p>Returns an integer.
</p>


<h3>Author(s)</h3>

<p>Hohsuk Noh.
</p>


<h3>References</h3>

<p>Akaike, H. (1973).  Information theory and an extension of the maximum likelihood principle, in <em>Second International Symposium of Information Theory</em>, eds. B. N. Petrov and F. Csaki, Budapest: Akademia Kiado, 267&ndash;281.  
</p>
<p>Daouia, A., Noh, H. and Park, B.U. (2016). Data Envelope fitting with constrained polynomial splines. <em>Journal of the Royal Statistical Society: Series B</em>, <b>78</b>(1), 3-30. doi:10.1111/rssb.12098.
</p>
<p>Schwartz, G. (1978). Estimating the dimension of a model, <em>Annals of Statistics</em>, 6, 461&ndash;464.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cub_spline_est">cub_spline_est</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("air")
# a. Unconstrained cubic spline fits
(kn.bic.air.u&lt;-cub_spline_kn(air$xtab, air$ytab, 
 method="u", type="BIC"))
# b. Monotone cubic spline smoother
(kn.bic.air.m&lt;-cub_spline_kn(air$xtab, air$ytab, 
 method="m", type="BIC")) 
# c. Monotone and Concave cubic spline smoother
(kn.bic.air.mc&lt;-cub_spline_kn(air$xtab, air$ytab, 
 method="mc", type="BIC"))
</code></pre>

<hr>
<h2 id='dea_est'>
DEA, FDH and linearized FDH estimators.  
</h2><span id='topic+dea_est'></span>

<h3>Description</h3>

<p>The function implements the empirical FDH (free disposal hull), LFDH (linearized FDH) and DEA (data envelopment analysis) frontier estimators.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dea_est(xtab, ytab, x, type = "dea")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dea_est_+3A_xtab">xtab</code></td>
<td>
<p>a numeric vector containing the observed inputs  <code class="reqn">x_1,\ldots,x_n</code>.</p>
</td></tr>
<tr><td><code id="dea_est_+3A_ytab">ytab</code></td>
<td>
<p>a numeric vector of the same length as <code>xtab</code> containing the observed outputs <code class="reqn">y_1,\ldots,y_n</code>.</p>
</td></tr>
<tr><td><code id="dea_est_+3A_x">x</code></td>
<td>
<p>a numeric vector of evaluation points in which the estimator is to be computed.</p>
</td></tr>
<tr><td><code id="dea_est_+3A_type">type</code></td>
<td>
<p>a character equal to &quot;dea&quot;, &quot;fdh&quot; or &quot;lfdh&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are mainly two usual frontier estimation methods for preserving monotonicity: the free disposal hull (FDH) introduced by Deprins <em>et al.</em> (1984)  
and the data envelopment analysis (DEA) initiated by Farrell (1957). The FDH boundary is the lowest &ldquo;stair-case&rdquo; monotone curve covering all the data points
</p>
<p style="text-align: center;"><code class="reqn">\varphi_n(x):=\max\{y_i,\,i:x_i\leq x\}.</code>
</p>

<p>An improved version of this estimator, referred to as the linearized FDH (LFDH), is obtained by drawing the polygonal line smoothing the staircase FDH curve. 
It has been considered in Hall and Park (2002) and Jeong and Simar (2006).
When the joint support of data is in addition convex, the DEA estimator is defined as the least concave majorant of the FDH frontier
(see also Gijbels <em>et al.</em> (1999)). We employ the function <code>DEA</code> from the package <span class="pkg">Benchmarking</span> to implement the function <code>dea_est</code>.
</p>


<h3>Value</h3>

<p>Returns a numeric vector with the same length as <code>x</code>.
</p>


<h3>Author(s)</h3>

<p>Hohsuk Noh.
</p>


<h3>References</h3>

<p>Bogetoft, P. and Otto, L. (2011), <em>Benchmarking with DEA, SFA and R</em>, Springer-Verlag.   
</p>
<p>Deprins, D., Simar, L. and H. Tulkens (1984).  Measuring labor
efficiency in post offices, in <em>The performance of Public Enterprises: Concepts and Measurements</em> (M. Marchand, P. Pestieau and H. Tulkens Eds), North-Holland, Amsterdam, 243&ndash;267. 
</p>
<p>Farrell, M.J. (1957). The measurement of productive efficiency.
<em>Journal of the Royal Statistical Society: Series A</em>, 120, 253&ndash;281. 
</p>
<p>Gijbels, I., Mammen, E., Park, B.U. and Simar, L. (1999). On estimation of monotone and concave frontier functions, <em>Journal of American Statistical Association</em>, 94, 220&ndash;228.   
</p>
<p>Hall, P. and Park, B.U. (2002). New methods for bias correction at endpoints and boundaries, <em>Annals of Statistics</em>, 30, 1460-1479.  
</p>
<p>Jeong, S.-O. and Simar, L. (2006). Linearly interpolated FDH efficiency score for nonconvex frontiers,
<em>Journal of Multivariate Analysis</em>, 97, 2141&ndash;2161.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+quad_spline_est">quad_spline_est</a></code>, <code><a href="#topic+cub_spline_est">cub_spline_est</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("green")
plot(OUTPUT~COST, data=green)
x &lt;- seq(min(green$COST), max(green$COST), length.out=1001)
# We compute and represent the DEA, FDH and LFDH estimates
lines(x, dea_est(green$COST, green$OUTPUT, x, type="dea"),
 lty=4, lwd=4, col="cyan")  
lines(x, dea_est(green$COST, green$OUTPUT, x, type="fdh"),
 lty=1, lwd=4, col="green")
lines(x, dea_est(green$COST, green$OUTPUT, x, type="lfdh"), 
 lty=2, lwd=4, col="magenta")   
legend("topleft",legend=c("dea","fdh","lfdh"), 
 col=c("cyan","green","magenta"), lty=c(4,1,2), lwd=4)
</code></pre>

<hr>
<h2 id='dfs_momt'>
Moment frontier estimator
</h2><span id='topic+dfs_momt'></span>

<h3>Description</h3>

<p>This function is an implementation of the moment-type estimator developed by Daouia, Florens and Simar (2010).    
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfs_momt(xtab, ytab, x, rho, k, ci=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dfs_momt_+3A_xtab">xtab</code></td>
<td>
<p>a numeric vector containing the observed inputs  <code class="reqn">x_1,\ldots,x_n</code>.</p>
</td></tr>
<tr><td><code id="dfs_momt_+3A_ytab">ytab</code></td>
<td>
<p>a numeric vector of the same length as <code>xtab</code> containing the observed outputs <code class="reqn">y_1,\ldots,y_n</code>.</p>
</td></tr>
<tr><td><code id="dfs_momt_+3A_x">x</code></td>
<td>
<p>a numeric vector of evaluation points in which the estimator is to be computed.</p>
</td></tr>
<tr><td><code id="dfs_momt_+3A_rho">rho</code></td>
<td>
<p>a numeric vector of the same length as <code>x</code> or a scalar, which determines the values of rho.</p>
</td></tr>  
<tr><td><code id="dfs_momt_+3A_k">k</code></td>
<td>
<p>a numeric vector of the same length as <code>x</code> or a scalar, which determines the thresholds at which the moment estimator will be computed.</p>
</td></tr>
<tr><td><code id="dfs_momt_+3A_ci">ci</code></td>
<td>
<p>a boolean, TRUE for computing the confidence interval.</p>
</td></tr> 
</table>


<h3>Details</h3>

<p>Combining ideas from Dekkers, Einmahl and de Haan (1989) with the dimensionless 
transformation <code class="reqn">\{z^{x}_i := y_i\mathbf{1}_{\{x_i\le x\}}, \,i=1,\cdots,n\}</code>  of 
the observed sample <code class="reqn">\{(x_i,y_i), \,i=1,\cdots,n\}</code>, the authors
estimate the conditional endpoint <code class="reqn">\varphi(x)</code> by 
</p>
<p style="text-align: center;"><code class="reqn">\tilde\varphi_{momt}(x) =   z^{x}_{(n-k)}  +  z^{x}_{(n-k)}  M^{(1)}_n \left\{1 + \rho_x  \right\}</code>
</p>
  
<p>where <code class="reqn">M^{(1)}_n = (1/k)\sum_{i=0}^{k-1}\left(\log  z^x_{(n-i)}- \log   z^x_{(n-k)}\right)</code>, 
<code class="reqn">z^{x}_{(1)}\leq \cdots\leq  z^{x}_{(n)}</code> are the ascending order statistics  
corresponding to the transformed sample <code class="reqn">\{z^{x}_i, \,i=1,\cdots,n\}</code>
and <code class="reqn">\rho_x&gt;0</code>  is referred to as the extreme-value index and has the following interpretation:
when  <code class="reqn">\rho_x&gt;2</code>, the joint density of data decays smoothly to zero at a speed of power <code class="reqn">\rho_x -2</code> of the distance from the frontier; 
when <code class="reqn">\rho_x=2</code>,  the density has sudden jumps at the frontier; when <code class="reqn">\rho_x&lt;2</code>, the density increases toward infinity at a speed of power <code class="reqn">\rho_x -2</code> 
of the distance from the frontier. 
Most of the contributions to econometric literature on frontier analysis assume that the joint density is strictly positive at its support boundary, or equivalently, <code class="reqn">\rho_x=2</code> for all <code class="reqn">x</code>.
When <code class="reqn">\rho_x</code> is unknown, Daouia et al. (2010) suggest to use the following two-step estimator: 
First, estimate <code class="reqn">\rho_x</code> by the moment estimator <code class="reqn">\tilde\rho_x</code> implemented in the function <code><a href="#topic+rho_momt_pick">rho_momt_pick</a></code> by utilizing the option <code>method="moment"</code>,  
or by the Pickands estimator <code class="reqn">\hat\rho_x</code> by using the option <code>method="pickands"</code>.
Second, use the estimator <code class="reqn">\tilde\varphi_{momt}(x)</code>, as if <code class="reqn">\rho_x</code> were known, by substituting the estimated value <code class="reqn">\tilde\rho_x</code> 
or <code class="reqn">\hat\rho_x</code> in place of <code class="reqn">\rho_x</code>.
The <code class="reqn">95\%</code> confidence interval of <code class="reqn">\varphi(x)</code> derived from the asymptotic normality of <code class="reqn">\tilde\varphi_{momt}(x)</code> is given by
</p>
<p style="text-align: center;"><code class="reqn">[\tilde\varphi_{momt}(x)  \pm   1.96 \sqrt{V(\rho_x) / k}  z^{x}_{(n-k)}  M^{(1)}_n (1 + 1/\rho_x) ]</code>
</p>

<p>where <code class="reqn">V(\rho_x) = \rho^2_x  (1+2/\rho_x)^{-1}</code>.
The sample fraction <code class="reqn">k=k_n(x)</code> plays here the role of the smoothing parameter and varies between 1 and <code class="reqn">N_x-1</code>, with <code class="reqn">N_x=\sum_{i=1}^n\mathbf{1}_{\{x_i\le x\}}</code> 
being the number of observations <code class="reqn">(x_i,y_i)</code> with <code class="reqn">x_i \leq x</code>. See <code><a href="#topic+kopt_momt_pick">kopt_momt_pick</a></code> for an automatic data-driven rule for selecting <code class="reqn">k</code>. 
</p>


<h3>Value</h3>

<p>Returns a numeric vector with the same length as <code>x</code>.
</p>


<h3>Note</h3>

<p>As it is common in extreme-value theory,  good results require a large sample size <code class="reqn">N_x</code> at each evaluation point <code class="reqn">x</code>.  
See also the note in <code><a href="#topic+kopt_momt_pick">kopt_momt_pick</a></code>.</p>


<h3>Author(s)</h3>

<p>Abdelaati Daouia and Thibault Laurent (converted from Leopold Simar's Matlab code). 
</p>


<h3>References</h3>

<p>Daouia, A., Florens, J.P. and Simar, L. (2010). Frontier Estimation and Extreme Value Theory, <em>Bernoulli</em>, 16, 1039-1063.
</p>
<p>Dekkers, A.L.M., Einmahl, J.H.J. and L. de Haan (1989), A moment estimator for the index of an extreme-value distribution, <em>nnals of Statistics</em>, 17, 1833-1855.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dfs_pick">dfs_pick</a></code>, <code><a href="#topic+kopt_momt_pick">kopt_momt_pick</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("post")
x.post &lt;- seq(post$xinput[100], max(post$xinput), 
 length.out = 100) 
# 1. When rho[x] is known and equal to 2, we set:
rho &lt;- 2
# To determine the sample fraction k=k[n](x) 
# in tilde(varphi[momt])(x).
best_kn.1 &lt;- kopt_momt_pick(post$xinput, post$yprod, 
 x.post, rho = rho)
# To compute the frontier estimates and confidence intervals:  
res.momt.1 &lt;- dfs_momt(post$xinput, post$yprod, x.post, 
 rho = rho, k = best_kn.1)
# Representation
plot(yprod~xinput, data = post, xlab = "Quantity of labor", 
 ylab = "Volume of delivered mail")
lines(x.post, res.momt.1[,1], lty = 1, col = "cyan")  
lines(x.post, res.momt.1[,2], lty = 3, col = "magenta")  
lines(x.post, res.momt.1[,3], lty = 3, col = "magenta")  

## Not run: 
# 2. rho[x] is unknown and estimated by 
# the Pickands estimator tilde(rho[x])
rho_momt &lt;- rho_momt_pick(post$xinput, post$yprod, 
 x.post)
best_kn.2 &lt;- kopt_momt_pick(post$xinput, post$yprod,
  x.post, rho = rho_momt)
res.momt.2 &lt;- dfs_momt(post$xinput, post$yprod, x.post, 
 rho = rho_momt, k = best_kn.2)  
# 3. rho[x] is unknown independent of x and estimated
# by the (trimmed) mean of tilde(rho[x])
rho_trimmean &lt;- mean(rho_momt, trim=0.00)
best_kn.3 &lt;- kopt_momt_pick(post$xinput, post$yprod,
  x.post, rho = rho_trimmean)   
res.momt.3 &lt;- dfs_momt(post$xinput, post$yprod, x.post, 
 rho = rho_trimmean, k = best_kn.3)  

# Representation 
plot(yprod~xinput, data = post, col = "grey", 
 xlab = "Quantity of labor", ylab = "Volume of delivered mail")
lines(x.post, res.momt.2[,1], lty = 1, lwd = 2, col = "cyan")  
lines(x.post, res.momt.2[,2], lty = 3, lwd = 4, col = "magenta")  
lines(x.post, res.momt.2[,3], lty = 3, lwd = 4, col = "magenta")  
plot(yprod~xinput, data = post, col = "grey", 
 xlab = "Quantity of labor", ylab = "Volume of delivered mail")
lines(x.post, res.momt.3[,1], lty = 1, lwd = 2, col = "cyan")  
lines(x.post, res.momt.3[,2], lty = 3, lwd = 4, col = "magenta")  
lines(x.post, res.momt.3[,3], lty = 3, lwd = 4, col = "magenta") 

## End(Not run)
</code></pre>

<hr>
<h2 id='dfs_pick'>
Pickands frontier estimator
</h2><span id='topic+dfs_pick'></span>

<h3>Description</h3>

<p>This function is an implementation of the Pickands type estimator developed by Daouia, Florens and Simar (2010).    
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfs_pick(xtab, ytab, x, k, rho, ci=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dfs_pick_+3A_xtab">xtab</code></td>
<td>
<p>a numeric vector containing the observed inputs  <code class="reqn">x_1,\ldots,x_n</code>.</p>
</td></tr>
<tr><td><code id="dfs_pick_+3A_ytab">ytab</code></td>
<td>
<p>a numeric vector of the same length as <code>xtab</code> containing the observed outputs <code class="reqn">y_1,\ldots,y_n</code>.</p>
</td></tr>
<tr><td><code id="dfs_pick_+3A_x">x</code></td>
<td>
<p>a numeric vector of evaluation points in which the estimator is to be computed.</p>
</td></tr>  
<tr><td><code id="dfs_pick_+3A_k">k</code></td>
<td>
<p>a numeric vector of the same length as <code>x</code> or a scalar, which determines the thresholds at which the Pickands estimator will be computed.</p>
</td></tr>
<tr><td><code id="dfs_pick_+3A_rho">rho</code></td>
<td>
<p>a numeric vector of the same length as <code>x</code> or a scalar, which determines the values of rho.</p>
</td></tr>
<tr><td><code id="dfs_pick_+3A_ci">ci</code></td>
<td>
<p>a boolean, TRUE for computing the confidence interval.</p>
</td></tr>    
</table>


<h3>Details</h3>

<p>Built on the ideas of Dekkers and de Haan (1989), Daouia et al. (2010) propose to estimate the frontier point <code class="reqn">\varphi(x)</code> by
</p>
<p style="text-align: center;"><code class="reqn">\hat\varphi_{pick}(x) =   \frac{z^x_{(n-k+1)} - z^x_{(n-2k+1)}}{2^{1/\rho_x} - 1} + z^x_{(n-k+1)}</code>
</p>

<p>from the transformed data <code class="reqn">\{z^{x}_i, \,i=1,\cdots,n\}</code> described in <code><a href="#topic+dfs_momt">dfs_momt</a></code>, where <code class="reqn">\rho_x&gt;0</code> is the same tail-index as in <code><a href="#topic+dfs_momt">dfs_momt</a></code>.
If <code class="reqn">\rho_x</code>  is known (typically equal to 2 if the joint density of data is believed to have sudden jumps at the frontier), then one can use the estimator <code class="reqn">\hat\varphi_{pick}(x)</code>
in conjunction with the function <code><a href="#topic+kopt_momt_pick">kopt_momt_pick</a></code> which implements an automatic data-driven method for selecting the threshold <code class="reqn">k</code>. 
In contrast, if <code class="reqn">\rho_x</code>  is unknown, one could consider using the following two-step estimator:
First, estimate <code class="reqn">\rho_x</code> by the Pickands estimator <code class="reqn">\hat\rho_x</code>  implemented in the function <code><a href="#topic+rho_momt_pick">rho_momt_pick</a></code> by using the option <code>method="pickands"</code>, 
or by the moment estimator <code class="reqn">\tilde\rho_x</code> by utilizing the option <code>method="moment"</code>. 
Second, use the estimator <code class="reqn">\hat\varphi_{pick}(x)</code>, as if <code class="reqn">\rho_x</code> were known, by substituting the estimated value <code class="reqn">\hat\rho_x</code> or <code class="reqn">\tilde\rho_x</code> 
in place of <code class="reqn">\rho_x</code>.
The pointwise <code class="reqn">95\%</code> confidence interval of the frontier function obtained from the asymptotic normality of <code class="reqn">\hat\varphi_{pick}(x)</code> is given by
</p>
<p style="text-align: center;"><code class="reqn">[\hat\varphi_{pick}(x)  \pm   1.96 \sqrt{v(\rho_x) / (2 k)}    ( z^x_{(n-k+1)} - z^x_{(n-2k+1)})]</code>
</p>

<p>where <code class="reqn">v(\rho_x) =\rho^{-2}_x  2^{-2/\rho_x}/(2^{-1/\rho_x} -1)^4</code>.
Finally, to select the threshold <code class="reqn">k=k_n(x)</code>, one could use the automatic data-driven method of Daouia et al. (2010) implemented in the function 
<code><a href="#topic+kopt_momt_pick">kopt_momt_pick</a></code> (option <code>method="pickands"</code>).
</p>


<h3>Value</h3>

<p>Returns a numeric vector with the same length as <code>x</code>.
</p>


<h3>Note</h3>

<p>As it is common in extreme-value theory,  good results require a large sample size <code class="reqn">N_x</code> at each evaluation point <code class="reqn">x</code>. See also the note in <code><a href="#topic+kopt_momt_pick">kopt_momt_pick</a></code>.</p>


<h3>Author(s)</h3>

<p>Abdelaati Daouia and Thibault Laurent (converted from Leopold Simar's Matlab code). 
</p>


<h3>References</h3>

<p>Daouia, A., Florens, J.P. and Simar, L. (2010). Frontier Estimation and Extreme Value Theory, <em>Bernoulli</em>, 16, 1039-1063.
</p>
<p>Dekkers, A.L.M., Einmahl, J.H.J. and L. de Haan (1989), A moment estimator for the index of an extreme-value distribution, <em>Annals of Statistics</em>, 17, 1833-1855.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dfs_momt">dfs_momt</a></code>, <code><a href="#topic+kopt_momt_pick">kopt_momt_pick</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("post")
x.post&lt;- seq(post$xinput[100],max(post$xinput), 
 length.out=100) 
# 1. When rho[x] is known and equal to 2, we set:
rho&lt;-2
# To determine the sample fraction k=k[n](x) 
# in hat(varphi[pick])(x).
best_kn.1&lt;-kopt_momt_pick(post$xinput, post$yprod, 
 x.post, method="pickands", rho=rho)
# To compute the frontier estimates and confidence intervals:  
res.pick.1&lt;-dfs_pick(post$xinput, post$yprod, x.post, 
 rho=rho, k=best_kn.1)
# Representation
plot(yprod~xinput, data=post, xlab="Quantity of labor", 
 ylab="Volume of delivered mail")
lines(x.post, res.pick.1[,1], lty=1, col="cyan")  
lines(x.post, res.pick.1[,2], lty=3, col="magenta")  
lines(x.post, res.pick.1[,3], lty=3, col="magenta")  

## Not run: 
# 2. rho[x] is unknown and estimated by 
# the Pickands estimator hat(rho[x])
rho_pick&lt;-rho_momt_pick(post$xinput, post$yprod, 
 x.post, method="pickands")
best_kn.2&lt;-kopt_momt_pick(post$xinput, post$yprod,
  x.post, method="pickands", rho=rho_pick)
res.pick.2&lt;-dfs_pick(post$xinput, post$yprod, x.post, 
 rho=rho_pick, k=best_kn.2)  
# 3. rho[x] is unknown independent of x and estimated
# by the (trimmed) mean of hat(rho[x])
rho_trimmean&lt;-mean(rho_pick, trim=0.00)
best_kn.3&lt;-kopt_momt_pick(post$xinput, post$yprod,
  x.post, rho=rho_trimmean, method="pickands")   
res.pick.3&lt;-dfs_pick(post$xinput, post$yprod, x.post, 
 rho=rho_trimmean, k=best_kn.3)  

# Representation 
plot(yprod~xinput, data=post, col="grey", xlab="Quantity of labor", 
 ylab="Volume of delivered mail")
lines(x.post, res.pick.2[,1], lty=1, lwd=2, col="cyan")  
lines(x.post, res.pick.2[,2], lty=3, lwd=4, col="magenta")  
lines(x.post, res.pick.2[,3], lty=3, lwd=4, col="magenta")  
plot(yprod~xinput, data=post, col="grey", xlab="Quantity of labor", 
 ylab="Volume of delivered mail")
lines(x.post, res.pick.3[,1], lty=1, lwd=2, col="cyan")  
lines(x.post, res.pick.3[,2], lty=3, lwd=4, col="magenta")  
lines(x.post, res.pick.3[,3], lty=3, lwd=4, col="magenta") 

## End(Not run)
</code></pre>

<hr>
<h2 id='dfs_pwm'>
Probability-weighted moment frontier estimator
</h2><span id='topic+dfs_pwm'></span>

<h3>Description</h3>

<p>This function is an implementation of the probability-weighted moment frontier estimator developed by Daouia, Florens and Simar (2012).    
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dfs_pwm(xtab, ytab, x, coefm, a=2, rho, ci=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dfs_pwm_+3A_xtab">xtab</code></td>
<td>
<p>a numeric vector containing the observed inputs  <code class="reqn">x_1,\ldots,x_n</code>.</p>
</td></tr>
<tr><td><code id="dfs_pwm_+3A_ytab">ytab</code></td>
<td>
<p>a numeric vector of the same length as <code>xtab</code> containing the observed outputs <code class="reqn">y_1,\ldots,y_n</code>.</p>
</td></tr>
<tr><td><code id="dfs_pwm_+3A_x">x</code></td>
<td>
<p>a numeric vector of evaluation points in which the estimator is to be computed.</p>
</td></tr>
<tr><td><code id="dfs_pwm_+3A_coefm">coefm</code></td>
<td>
<p>a tuning parameter (integer) larger than or equal to 1.</p>
</td></tr>  
<tr><td><code id="dfs_pwm_+3A_a">a</code></td>
<td>
<p>a smoothing parameter (integer) larger than or equal to 2.</p>
</td></tr>
<tr><td><code id="dfs_pwm_+3A_rho">rho</code></td>
<td>
<p>a numeric vector of the same length as <code>x</code> or a scalar, which determines the values of rho.</p>
</td></tr>
<tr><td><code id="dfs_pwm_+3A_ci">ci</code></td>
<td>
<p>a boolean, TRUE for computing the confidence interval.</p>
</td></tr> 
</table>


<h3>Details</h3>

<p>The regularized frontier estimator introduced by Daouia et al. (2012) is based 
on the unregularized probability-weighted moment estimator
</p>
<p style="text-align: center;"><code class="reqn">\hat{\varphi}_m(x) = \varphi_{fdh}(x) - \int_{0}^{\varphi_{fdh}(x)} \hat{F}^m(y|x)dy</code>
</p>

<p>where the trimming order <code class="reqn">m\geq 1</code> is an integer such that <code class="reqn">m=m_n\to\infty</code> as <code class="reqn">n\to\infty</code>, 
and <code class="reqn">\hat{F}(y|x)=\sum_{i=1}^n1_{(x_i\leq x,y_i\leq y)}/\sum_{i=1}^n1_{(x_i\leq x)}</code>.
The implemented estimator of <code class="reqn">\varphi(x)</code> is then defined as 
</p>
<p style="text-align: center;"><code class="reqn">\tilde{\varphi}_m(x) = \hat{\varphi}_m(x) + \Gamma\left(1 + 1/\bar\rho_x\right)\left( 1/m\,\hat\ell_x\right)^{1/\bar\rho_x}</code>
</p>

<p>where
</p>
<p style="text-align: center;"><code class="reqn">\bar{\rho}_x    =   \log (a)\left\{ \log\Big( \frac{\hat\varphi_{m}(x)-\hat\varphi_{am}(x)}{\hat\varphi_{am}(x)-\hat\varphi_{a^2m}(x)}
\Big) \right\}^{-1} ,  \quad
\hat{\ell}_x     =  \frac {1}{m}\left[\frac{\Gamma(1+ 1/\bar\rho_x)\big(1-a^{-1/\bar\rho_x}\big)}{\hat\varphi_{m}(x)-\hat\varphi_{am}(x)}\right]^{\bar\rho_x},</code>
</p>

<p>with <code class="reqn">a\geq 2</code> being a fixed integer. If the true tail-index <code class="reqn">\rho_x=\beta_x+2</code> is known, we set <code class="reqn">\bar{\rho}_x=\rho_x</code> in the expressions above.
The two smoothing parameters <code class="reqn">m</code> and <code class="reqn">a</code> have to be fixed by the user in the 4th and 5th arguments of the function.
</p>
<p>The pointwise <code class="reqn">95\%</code> confidence interval of <code class="reqn">\varphi(x)</code> derived from the asymptotic normality of <code class="reqn">\tilde\varphi_{m}(x)</code> 
is given by <code class="reqn">[\tilde{\varphi}_m(x)  \pm   1.96 \, \hat\sigma(m,x)/\sqrt{n}]</code>
where 
</p>
<p style="text-align: center;"><code class="reqn"> 
 \hat\sigma^2(m,x)= \frac{2m^2}{ \hat F_X(x)}\int_0^{\varphi_{fdh}(x)}  
\int_0^{\varphi_{fdh}(x)}  \hat F^{m}(y|x)\hat F^{m-1}(u|x)(1-\hat F(u|x))
1_{(y\le u)}\, dy\,du,</code>
</p>

<p>with <code class="reqn">\hat F_X(x) =(1/n)\sum_{i=1}^n1_{(x_i\leq x)}</code>. 
Note that the standard deviation <code class="reqn">\sigma(m,x)/\sqrt{n}</code> of the bias-corrected estimator <code class="reqn">\tilde{\varphi}_m(x)</code> is adjusted by a bootstrap estimator 
in the numerical illustrations of Daouia et al. (2012), whereas the exact estimate <code class="reqn">\hat\sigma(m,x)/\sqrt{n}</code> is utilized in the implemented function.
A practical choice of <code class="reqn">m</code> that Daouia et al. (2012) have employed is the simple rule of thumb 
<code class="reqn">m=coefm \times N^{1/3}_x</code>, where <code class="reqn">N_x=\sum_{i=1}^n1_{\{x_i\le x\}}</code>, 
and the integer <code>coefm</code> as well as the second smoothing parameter <code>a</code> are to be tuned by the user to avoid numerical instabilities 
in the pointwise estimates of the tail-index <code class="reqn">\rho_x</code> and the frontier function <code class="reqn">\varphi(x)</code>. 
The user may start with the values <code>coefm=5</code> and <code>a=2</code> [respectively, <code>coefm=10</code> and <code>a=20</code>] 
for computing the estimator <code class="reqn">\tilde{\varphi}_m(x)</code>  [respectively, <code class="reqn">\bar{\rho}_x</code>]. Note that tail-index estimation and frontier estimation are conducted separately. 
</p>


<h3>Value</h3>

<p>Returns a numeric vector with the same length as <code>x</code>.
</p>


<h3>Note</h3>

<p>The computational burden here is demanding, so be forewarned. 
</p>


<h3>Author(s)</h3>

<p>Abdelaati Daouia and Thibault Laurent (converted from Abdelaati Daouia's Matlab code). 
</p>


<h3>References</h3>

<p>Daouia, A., Florens, J.-P. and Simar, L. (2012). Regularization of Nonparametric Frontier Estimators. <em>Journal of Econometrics</em>, 168, 285-299.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rho_pwm">rho_pwm</a></code>, <code><a href="#topic+mopt_pwm">mopt_pwm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("post")
x.post&lt;- seq(post$xinput[100],max(post$xinput), 
 length.out=100) 
## Not run: 
# 1. When rho[x] is known and equal to 2, we set:
rho&lt;-2
res.pwm.1&lt;- dfs_pwm(post$xinput, post$yprod, x.post, coefm=5,
 a=2, rho, ci=TRUE)
# 2. When rho[x] is unknown and dependent of x, 
# its estimate hat(rho[x]) is obtained via:
rho_pwm &lt;- rho_pwm(post$xinput, post$yprod, x.post, coefm=10, a=20)
# and the corresponding frontier estimator via: 
res.pwm.2&lt;- dfs_pwm(post$xinput, post$yprod, x.post, coefm=5,
 a=2, rho_pwm, ci=TRUE)
# 3. When rho[x] is unknown but independent of x, 
# a robust estimation strategy is by using the (trimmed) mean 
# over the estimates hat(rho[x]): 
rho_trimmean&lt;-mean(rho_pwm, trim=0.00)
res.pwm.3&lt;- dfs_pwm(post$xinput, post$yprod, x.post, coefm=5,
 a=2, rho_trimmean, ci=TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='green'>
American electric utility companies
</h2><span id='topic+green'></span>

<h3>Description</h3>

<p>The dataset consists of 123 American electric utility companies. As in the set-up of Gijbels <em>et al.</em> (1999), 
we used the measurements of the variables <code class="reqn">y_i = \log(q_i)</code> and 
<code class="reqn">x_i = \log(c_i)</code>, where <code class="reqn">q_i</code> is the production output 
of the company <code class="reqn">i</code> and <code class="reqn">c_i</code> is the total cost involved in the production.  
For a detailed description and analysis of these data see, <em>e.g.</em>, Christensen and Greene (1976) and Greene (1990).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(green)</code></pre>


<h3>Format</h3>

<p>A data frame with 123 observations on the following 2 variables.
</p>

<dl>
<dt><code>COST</code></dt><dd><p>a numeric vector.</p>
</dd>
<dt><code>OUTPUT</code></dt><dd><p>a numeric vector.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Gijbels <em>et al.</em> (1999).
</p>


<h3>References</h3>

<p>Christensen, L.R. and Greene, W.H. (1976). Economies of Scale in U.S. Electric Power Generation, <em>Journal of Political Economy</em>, University of Chicago Press, 84, 655-76.
</p>
<p>Gijbels, I., Mammen, E., Park, B.U. and Simar, L. (1999). On estimation of monotone and
concave frontier functions. <em>Journal of American Statistical Association</em>, 94, 220-228.    
</p>
<p>Greene, W.H. (1990). A Gamma-distributed stochastic frontier model, <em>Journal of Econometrics</em>, 46, 141-163.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("green")
</code></pre>

<hr>
<h2 id='kern_smooth'>
Frontier estimation via kernel smoothing  
</h2><span id='topic+kern_smooth'></span>

<h3>Description</h3>

<p>The function <code>kern_smooth</code> implements two frontier estimators based on kernel smoothing techniques. One is from Noh (2014) and the other is from Parmeter and Racine (2013).  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kern_smooth(xtab, ytab, x, h, method="u", technique="noh", 
 control = list("tm_limit" = 700))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kern_smooth_+3A_xtab">xtab</code></td>
<td>
<p>a numeric vector containing the observed inputs  <code class="reqn">x_1,\ldots,x_n</code>.</p>
</td></tr>
<tr><td><code id="kern_smooth_+3A_ytab">ytab</code></td>
<td>
<p>a numeric vector of the same length as <code>xtab</code> containing the observed outputs <code class="reqn">y_1,\ldots,y_n</code>.</p>
</td></tr>
<tr><td><code id="kern_smooth_+3A_x">x</code></td>
<td>
<p>a numeric vector of evaluation points in which the estimator is to be computed.</p>
</td></tr>
<tr><td><code id="kern_smooth_+3A_h">h</code></td>
<td>
<p>determines the bandwidth at which the smoothed kernel estimate will be computed.</p>
</td></tr>
<tr><td><code id="kern_smooth_+3A_method">method</code></td>
<td>
<p>a character equal to &quot;u&quot; (unconstrained estimator), &quot;m&quot; (under the monotonicity constraint) or &quot;mc&quot; (under simultaneous monotonicity and concavity constraints).</p>
</td></tr>  
<tr><td><code id="kern_smooth_+3A_technique">technique</code></td>
<td>
<p>which estimation method to use. &quot;Noh&quot;&quot; specifies the use of the method in Noh (2014) and &quot;pr&quot; is for the method in Parameter and Racine (2013).</p>
</td></tr>
<tr><td><code id="kern_smooth_+3A_control">control</code></td>
<td>
<p>a list of parameters to the GLPK solver. See *Details* of help(Rglpk_solve_LP).</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>To estimate the frontier function, Parameter and Racine (2013) considered the following generalization of linear regression smoothers 
</p>
<p style="text-align: center;"><code class="reqn">\hat \varphi(x|p) = \sum_{i=1}^n p_i A_i(x)y_i,</code>
</p>
<p> where <code class="reqn">A_i(x)</code> is the kernel weight function of <code class="reqn">x</code> for the <code class="reqn">i</code>th data depending on <code class="reqn">x_i</code>'s and the sort of linear smoothers. For example, the Nadaraya-Watson kernel weights are <code class="reqn">A_i(x) = K_i(x)/(\sum_{j=1}^n K_j(x)),</code> where <code class="reqn">K_i(x) = h^{-1} K\{ (x-x_i)/h\}</code>, with the kernel function <code class="reqn">K</code> being a bounded and symmetric probability density, and <code class="reqn">h</code> is a bandwidth. Then, the weight vector <code class="reqn">p=(p_1,\ldots,p_n)^T</code> is chosen to minimize the distance <code class="reqn">D(p)=(p-p_u)^T(p-p_u)</code> subject to the envelopment constraints and the choice of the shape constraints, where <code class="reqn">p_u</code> is an <code class="reqn">n</code>-dimensional vector with all elements being one. The envelopement and shape constraints are 
</p>
<p style="text-align: center;"><code class="reqn">
\begin{array}{rcl}
\hat \varphi(x_i|p) - y_i &amp;=&amp; \sum_{i=1}^n p_i A_i(x_i)y_i - y_i \geq 0,~i=1,\ldots,n;~~~{\sf (envelopment~constraints)} 
\\ \hat \varphi^{(1)}(x|p) &amp;=&amp; \sum_{i=1}^n p_i A_i^{(1)}(x)y_i \geq 0,~x \in \mathcal{M};~~~{\sf (monotonocity~constraints)} 
\\  \hat \varphi^{(2)}(x|p) &amp;=&amp; \sum_{i=1}^n p_i A_i^{(2)}(x)y_i \leq 0,~x \in \mathcal{C},~~~{\sf (concavity~constraints)} 
\end{array}</code>
</p>

<p>where <code class="reqn">\hat \varphi^{(s)}(x|p) = \sum_{i=1}^n p_i A_i^{(s)}(x) y_i</code> is the <code class="reqn">s</code>th derivative of <code class="reqn">\hat \varphi(x|p)</code>, with <code class="reqn">\mathcal{M}</code> and <code class="reqn">\mathcal{C}</code> being the collections of points where monotonicity and concavity are imposed, respectively. In our implementation of the estimator, we simply take the entire dataset <code class="reqn"> \{(x_i,y_i),~i=1,\ldots,n\}</code> to be <code class="reqn">\mathcal{M}</code> and <code class="reqn">\mathcal{C}</code> and, in case of small samples, we augment the sample points by an equispaced grid of length 201 over the observed support <code class="reqn">[\min_i x_i,\max_i x_i]</code> of <code class="reqn">X</code>. For the weight <code class="reqn">A_i(x)</code>, we use the Nadaraya-Watson weights.
</p>
<p>Noh (2014) considered the same generalization of linear smoothers <code class="reqn">\hat \varphi(x|p)</code> for frontier estimation, but with a difference choice of the weight <code class="reqn">p</code>. Using the same envelopment and shape constraints as Parmeter and Racine (2013), the weight vector <code class="reqn">p</code> is chosen to minimize the area under the fitted curve <code class="reqn">\hat \varphi(x|p)</code>, that is <code class="reqn">A(p) = \int_a^b\hat \varphi(x|p) dx = \sum_{i=1}^n p_i y_i \left( \int_a^b A_i(x) dx \right)</code>, where <code class="reqn">[a,b]</code> is the true support of <code class="reqn">X</code>. In practice, we integrate over the observed support <code class="reqn">[\min_i x_i,\max_i x_i]</code> since the theoretic one is unknown. In what concerns the kernel weights <code class="reqn">A_i(x)</code>, we use the Priestley-Chao weights 
</p>
<p style="text-align: center;"><code class="reqn">
A_i(x) = \left\{ \begin{array}{cc} 0 &amp;,~i=1 \\ (x_i - x_{i-1}) K_i(x) &amp;,~i \neq 1 \end{array} \right.,
</code>
</p>

<p>where it is assumed that the pairs <code class="reqn">(x_i,y_i)</code> have been ordered so that <code class="reqn">x_1 \leq \cdots \leq x_n</code>. The choice of such weights is motivated by their convenience for the evaluation of the integral <code class="reqn">\int A_i(x) dx</code>.
</p>


<h3>Value</h3>

<p>Returns a numeric vector with the same length as <code>x</code>. Returns a vector of NA if no solution has been found by the solver (GLPK). 
</p>


<h3>Author(s)</h3>

<p>Hohsuk Noh
</p>


<h3>References</h3>

<p>Noh, H. (2014). Frontier estimation using kernel smoothing estimators with data transformation. <em>Journal of the Korean Statistical Society</em>, 43, 503-512.
</p>
<p>Parmeter, C.F. and Racine, J.S. (2013). Smooth constrained frontier analysis in <em>Recent Advances and Future Directions in Causality, Prediction, and Specification Analysis</em>, Springer-Verlag, New York, 463-488.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kern_smooth_bw">kern_smooth_bw</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data("green")
x.green &lt;- seq(min(log(green$COST)), max(log(green$COST)), 
 length.out = 101)
options(np.tree=TRUE, crs.messages=FALSE, np.messages=FALSE)
# 1. Unconstrained 
(h.bic.green.u &lt;- kern_smooth_bw(log(green$COST), 
 log(green$OUTPUT), method = "u", technique = "noh", 
 bw_method = "bic"))
y.ks.green.u &lt;- kern_smooth(log(green$COST), 
 log(green$OUTPUT), x.green, h = h.bic.green.u, 
 method = "u", technique = "noh")

# 2. Monotonicity constraint
(h.bic.green.m &lt;- kern_smooth_bw(log(green$COST),
 log(green$OUTPUT), method = "m", technique = "noh",
 bw_method = "bic"))
y.ks.green.m &lt;- kern_smooth(log(green$COST), 
 log(green$OUTPUT), x.green, h = h.bic.green.m, 
 method = "m", technique = "noh")

# 3. Monotonicity and Concavity constraints
(h.bic.green.mc&lt;-kern_smooth_bw(log(green$COST), log(green$OUTPUT), 
 method="mc", technique="noh", bw_method="bic"))
y.ks.green.mc&lt;-kern_smooth(log(green$COST), 
 log(green$OUTPUT), x.green, h=h.bic.green.mc, method="mc", 
 technique="noh")

# Representation 
plot(log(OUTPUT)~log(COST), data=green, xlab="log(COST)", 
 ylab="log(OUTPUT)") 
lines(x.green, y.ks.green.u, lty=1, lwd=4, col="green")
lines(x.green, y.ks.green.m, lty=2, lwd=4, col="cyan")
lines(x.green, y.ks.green.mc, lty=3, lwd=4, col="magenta")   
legend("topleft", col=c("green","cyan","magenta"), 
lty=c(1,2,3), legend=c("unconstrained", "monotone", 
 "monotone + concave"), lwd=4, cex=0.8)

## End(Not run)    
</code></pre>

<hr>
<h2 id='kern_smooth_bw'>
Bandwidth selection for kernel smoothing frontier estimators
</h2><span id='topic+kern_smooth_bw'></span>

<h3>Description</h3>

<p>The function <code>kern_smooth_bw</code> provides two bandwidth selection methods. One is the least squares cross-validation developed by Parmeter and Racine (2013). The other is the BIC developed in Noh (2014).    
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kern_smooth_bw(xtab, ytab, method="u", technique="noh", bw_method="bic", 
 control = list("tm_limit" = 700))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kern_smooth_bw_+3A_xtab">xtab</code></td>
<td>
<p>a numeric vector containing the observed inputs  <code class="reqn">x_1,\ldots,x_n</code>.</p>
</td></tr>
<tr><td><code id="kern_smooth_bw_+3A_ytab">ytab</code></td>
<td>
<p>a numeric vector of the same length as <code>xtab</code> containing the observed outputs <code class="reqn">y_1,\ldots,y_n</code>.</p>
</td></tr>
<tr><td><code id="kern_smooth_bw_+3A_method">method</code></td>
<td>
<p>a character equal to &quot;u&quot; (unconstrained estimator), &quot;m&quot; (under the monotonicity constraint) or &quot;mc&quot; (under simultaneous monotonicity and concavity constraints).</p>
</td></tr>  
<tr><td><code id="kern_smooth_bw_+3A_technique">technique</code></td>
<td>
<p>which estimation technique to use: &quot;Noh&quot; specifies the use of the method in Noh (2014), while &quot;pr&quot; is for the method in Parameter and Racine (2013).</p>
</td></tr>  
<tr><td><code id="kern_smooth_bw_+3A_bw_method">bw_method</code></td>
<td>
<p>which bandwidth selection method to use: &quot;cv&quot; returns the bandwidth that minimizes the least squares cross-validation criterion, and &quot;bic&quot; returns the bandwidth minimizing the BIC.</p>
</td></tr>
<tr><td><code id="kern_smooth_bw_+3A_control">control</code></td>
<td>
<p>a list of parameters to the GLPK solver. See *Details* of help(Rglpk_solve_LP).</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>As with any smoothed techniques, the bandwidth selection is critical to the quality of the frontier estimator. Parmeter and Racine (2013)'s recommendation is to use the least squares cross-validation method implemented with <code>bw\_method="cv"</code> in the function <code>kern\_smooth\_bw</code>.
Instead, Noh (2014) proposed to select the bandwidth which minimizes the following criterion: 
</p>
<p style="text-align: center;"><code class="reqn"> 
BIC(h) = \log \left( \sum_{i=1}^n (\hat \varphi(x_i|\hat p(h))-y_i)\right)+\frac {\log n \cdot tr(S(h))}{2n},</code>
</p>

<p>where <code class="reqn">\hat p(h)</code> is the chosen weight vector associated to the bandwidth <code class="reqn">h</code>, and <code class="reqn">tr(S(h))</code> is the trace of the smoothing matrix 
</p>
<p style="text-align: center;"><code class="reqn">
S(h) = \left( \begin{array}{ccc} A_1(x_1) &amp; \cdots &amp; A_n(x_1)
\\ \vdots &amp; \ddots&amp; \vdots
\\ A_1(x_n) &amp; \cdots &amp; A_n(x_n) \end{array} \right).
</code>
</p>

<p>The function <code>kern\_smooth\_bw</code> computes the optimal bandwidth from this criterion with option <code>bw\_method="bic"</code>.
</p>


<h3>Value</h3>

<p>Returns an optimal bandwidth depending on the specified selection method. 
</p>


<h3>Author(s)</h3>

<p>Hohsuk Noh
</p>


<h3>References</h3>

<p>Noh, H. (2014). Frontier estimation using kernel smoothing estimators with data transformation. <em>Journal of the Korean Statistical Society</em>, 43, 503-512.
</p>
<p>Parmeter, C.F. and Racine, J.S. (2013). Smooth constrained frontier analysis in <em>Recent Advances and Future Directions in Causality, Prediction, and Specification Analysis</em>, Springer-Verlag, New York, 463-488.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kern_smooth">kern_smooth</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data("green")
x.green &lt;- seq(min(log(green$COST)), max(log(green$COST)),length.out=101)
options(np.tree=TRUE,crs.messages=FALSE,np.messages=FALSE)
h.pr.green.m&lt;-kern_smooth_bw(log(green$COST),log(green$OUTPUT), method="m", 
 technique="pr", bw_method="cv")
h.noh.green.m&lt;-kern_smooth_bw(log(green$COST),log(green$OUTPUT), method="m", 
 technique="noh", bw_method="bic")
y.pr.green.m&lt;-kern_smooth(log(green$COST),log(green$OUTPUT), x.green, 
 h=h.pr.green.m, method="m", technique="pr")
y.noh.green.m&lt;-kern_smooth(log(green$COST),log(green$OUTPUT), x.green, 
 h=h.noh.green.m, method="m", technique="noh")
plot(log(OUTPUT)~log(COST), data=green, xlab="log(COST)",ylab="log(OUTPUT)") 
lines(x.green, y.pr.green.m, lwd=4, lty=3, col="red") 
lines(x.green, y.noh.green.m, lwd=4, lty=3, col="blue")  
legend("topleft", col=c("blue","red"),lty=3, legend=c("noh","pr"), 
 lwd=4, cex=0.8)

## End(Not run) 
</code></pre>

<hr>
<h2 id='kopt_momt_pick'>
Optimal <code class="reqn">k</code> in moment and Pickands frontier estimators
</h2><span id='topic+kopt_momt_pick'></span>

<h3>Description</h3>

<p>This function gives the optimal sample fraction k in the moment and Pickands type of estimators introduced by Daouia, Florens and Simar (2010).    
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kopt_momt_pick(xtab, ytab, x, rho, method="moment", wind.coef=0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kopt_momt_pick_+3A_xtab">xtab</code></td>
<td>
<p>a numeric vector containing the observed inputs  <code class="reqn">x_1,\ldots,x_n</code>.</p>
</td></tr>
<tr><td><code id="kopt_momt_pick_+3A_ytab">ytab</code></td>
<td>
<p>a numeric vector of the same length as <code>xtab</code> containing the observed outputs <code class="reqn">y_1,\ldots,y_n</code>.</p>
</td></tr>
<tr><td><code id="kopt_momt_pick_+3A_x">x</code></td>
<td>
<p>a numeric vector of evaluation points in which the estimator is to be computed.</p>
</td></tr>
<tr><td><code id="kopt_momt_pick_+3A_rho">rho</code></td>
<td>
<p>a numeric vector of the same length as <code>x</code> or a scalar, which determines the values of rho.</p>
</td></tr>
<tr><td><code id="kopt_momt_pick_+3A_method">method</code></td>
<td>
<p>a character equal to &quot;moment&quot; or &quot;pickands&quot;.</p>
</td></tr>  
<tr><td><code id="kopt_momt_pick_+3A_wind.coef">wind.coef</code></td>
<td>
<p>a scalar coefficient to be selected in the interval (0,1].</p>
</td></tr>    
</table>


<h3>Details</h3>

<p>This function is an implementation of an experimental method by Daouia et al. (2010) for the 
automated threshold selection (choice of <code class="reqn">k=k_n(x)</code>) for the moment frontier estimator 
<code class="reqn">\tilde\varphi_{momt}(x)</code>  [see <code><a href="#topic+dfs_momt">dfs_momt</a></code>] in case  <code>method="moment"</code> and for the Pickands 
frontier estimator <code class="reqn">\hat\varphi_{pick}(x)</code> [see <code><a href="#topic+dfs_pick">dfs_pick</a></code>] in case <code>method="pickands"</code>.
The idea is to select first (for each <code class="reqn">x</code>) a grid of values for the sample fraction <code class="reqn">k_n(x)</code> given by <code class="reqn">k = 1, \cdots, [\sqrt{N_x}]</code>, 
where <code class="reqn">[\sqrt{N_x}]</code> stands for the integer part of <code class="reqn">\sqrt{N_x}</code> with <code class="reqn">N_x=\sum_{i=1}^n1_{\{x_i\le x\}}</code>, 
and then select the <code class="reqn">k</code> where the variation of the results is the smallest. To achieve this here, 
Daouia et al. (2010) compute the standard deviations of <code class="reqn">\tilde\varphi_{momt}(x)</code> [option <code>method="moment"</code>] or <code class="reqn">\hat\varphi_{pick}(x)</code> [option <code>method="pickands"</code>]
over a &ldquo;window&rdquo; of size <code class="reqn">\max(3, [ wind.coef  \times \sqrt{N_x} /2])</code>, where the coefficient <code>wind.coef</code> should be selected in the interval <code class="reqn">(0,1]</code> in such 
a way to avoid numerical instabilities. 
The default option <code>wind.coef=0.1</code> corresponds to having a window large enough to cover around <code class="reqn">10\%</code> of the possible values of <code class="reqn">k</code> in the selected range of values for <code class="reqn">k_n(x)</code>. 
The value of <code class="reqn">k</code> where the standard deviation is minimal defines the desired sample fraction <code class="reqn">k_n(x)</code>. 
</p>


<h3>Value</h3>

<p>Returns a numeric vector with the same length as <code>x</code>.
</p>


<h3>Note</h3>

<p>In order to choose a raisonable estimate  <code class="reqn">\tilde\varphi_{momt}(x)=\tilde\varphi_{momt}(x,k)</code> [see <code><a href="#topic+dfs_momt">dfs_momt</a></code>] and 
<code class="reqn">\hat\varphi_{pick}(x)=\hat\varphi_{pick}(x,k)</code> [see <code><a href="#topic+dfs_pick">dfs_pick</a></code>] of the frontier function  <code class="reqn">\varphi(x)</code>, 
for each fixed <code class="reqn">x</code>, one can construct the plot of the estimator of interest, consisting of the points <code class="reqn">\{(k,\tilde\varphi_{momt}(x,k))\}_k</code> or
<code class="reqn">\{(k,\hat\varphi_{pick}(x,k))\}_k</code>, and select a value of the estimate at which the obtained graph looks stable. This is this kind of idea
which guides the proposed automatic data-driven rule for a chosen grid of values of <code class="reqn">x</code>. The main difficulty with such a method is that the plots of
<code class="reqn">\tilde\varphi_{momt}(x,k)</code> or <code class="reqn">\hat\varphi_{pick}(x,k)</code> as functions of <code class="reqn">k</code>, for each <code class="reqn">x</code>, may be so unstable that reasonable values of
<code class="reqn">k</code> [which would correspond to the true value of <code class="reqn">\varphi(x)</code>] may be hidden in the graphs. In result, the obtained frontier estimator may exhibit considerable volatility as
a function of <code class="reqn">x</code>. One way to avoid such instabilities is by tuning the choice of the parameter <code>wind.coef</code> in the interval (0,1]. Note that the default value is <code>wind.coef=0.1</code>.
The user can also improve appreciably the estimation of  <code class="reqn">\varphi(x)</code> by refining the estimation of the extreme-value index <code class="reqn">\rho_x</code> (see <code><a href="#topic+rho_momt_pick">rho_momt_pick</a></code> for details).  
</p>


<h3>Author(s)</h3>

<p>Abdelaati Daouia and Thibault Laurent (converted from Leopold Simar's Matlab code). 
</p>


<h3>References</h3>

<p>Daouia, A., Florens, J.P. and Simar, L. (2010). Frontier Estimation and Extreme Value Theory, <em>Bernoulli</em>, 16, 1039-1063.
</p>
<p>Dekkers, A.L.M., Einmahl, J.H.J. and L. de Haan (1989), A moment estimator for the index of an extreme-value distribution, <em>Annals of Statistics</em>, 17, 1833-1855.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dfs_momt">dfs_momt</a></code>, <code><a href="#topic+dfs_pick">dfs_pick</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("post")
x.post&lt;- seq(post$xinput[100],max(post$xinput), 
 length.out=100) 
# When rho[x] is known and equal to 2, we set:
rho&lt;-2
# a. Optimal k in Pickands frontier estimators
best_kn.pick&lt;-kopt_momt_pick(post$xinput, post$yprod, 
 x.post, method="pickands", rho=rho)
# b. Optimal k in moment frontier estimators
## Not run: 
best_kn.momt&lt;-kopt_momt_pick(post$xinput, post$yprod, 
 x.post, rho=rho)

## End(Not run)
</code></pre>

<hr>
<h2 id='loc_est'>
Local linear frontier estimator
</h2><span id='topic+loc_est'></span>

<h3>Description</h3>

<p>Computes the local linear smoothing frontier estimator of Hall, Park and Stern (1998) and Hall and Park (2004).    
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loc_est(xtab, ytab, x, h, method="u", control = list("tm_limit" = 700))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loc_est_+3A_xtab">xtab</code></td>
<td>
<p>a numeric vector containing the observed inputs  <code class="reqn">x_1,\ldots,x_n</code>.</p>
</td></tr>
<tr><td><code id="loc_est_+3A_ytab">ytab</code></td>
<td>
<p>a numeric vector of the same length as <code>xtab</code> containing the observed outputs <code class="reqn">y_1,\ldots,y_n</code>.</p>
</td></tr>
<tr><td><code id="loc_est_+3A_x">x</code></td>
<td>
<p>a numeric vector of evaluation points in which the estimator is to be computed.</p>
</td></tr>
<tr><td><code id="loc_est_+3A_h">h</code></td>
<td>
<p>determines the bandwidth at which the local linear estimate will be computed.</p>
</td></tr>
<tr><td><code id="loc_est_+3A_method">method</code></td>
<td>
<p>a character equal to &quot;u&quot; (unconstrained estimator) or &quot;m&quot; (improved version of the unconstrained estimator).</p>
</td></tr> 
<tr><td><code id="loc_est_+3A_control">control</code></td>
<td>
<p>a list of parameters to the GLPK solver. See *Details* of help(Rglpk_solve_LP).</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>In the unconstrained case (option <code>method="u"</code>), the implemented estimator of <code class="reqn">\varphi(x)</code> is defined by
</p>
<p style="text-align: center;"><code class="reqn">
\hat \varphi_{n,LL}(x) = \min  \Big\{ z : {\rm there~exists~} \theta ~{\rm such~that~} y_i \leq z + \theta(x_i - x)</code>
</p>

<p style="text-align: center;"><code class="reqn">{\rm for~all}~i~{\rm such~that~}x_i \in (x-h,x+h) \Big\},</code>
</p>

<p>where the bandwidth <code class="reqn">h</code> has to be fixed by the user in the 4th argument of the function.
This estimator may lack of smoothness in case of small samples and has no guarantee of being monotone  even if the true frontier is so.  
Following the curvature of the monotone frontier <code class="reqn">\varphi</code>, the unconstrained  estimator <code class="reqn">\hat \varphi_{n,LL}</code>  is likely to exhibit substantial bias, especially at the sample boundaries 
(see Daouia et al (2016) for numerical illustrations). A simple way to remedy to this drawback is by imposing the extra condition <code class="reqn">\theta \geq 0</code> in the definition of <code class="reqn">\hat \varphi_{n,LL}(x)</code> to get 
</p>
<p style="text-align: center;"><code class="reqn">
\tilde \varphi_{n,LL}(x) = \min  \Big\{ z : {\rm there~exists~} \theta\geq 0 ~{\rm such~that~} y_i \leq z + \theta(x_i - x)</code>
</p>

<p style="text-align: center;"><code class="reqn">{\rm for~all}~i~{\rm such~that~}x_i \in (x-h,x+h) \Big\}.</code>
</p>

<p>As shown in Daouia et al (2016), this version only reduces the vexing bias and border defects of the original estimator when the true frontier is monotone. 
The option <code>method="m"</code> indicates that the improved fit <code class="reqn">\tilde \varphi_{n,LL}(x)</code> should be utilized in place of <code class="reqn">\hat \varphi_{n,LL}(x)</code>. 
Hall and Park (2004) proposed a bootstrap procedure for selecting the optimal 
bandwidth <code class="reqn">h</code> in  <code class="reqn">\hat \varphi_{n,LL}(x)</code> and <code class="reqn">\tilde \varphi_{n,LL}(x)</code> (see the function <code><a href="#topic+loc_est_bw">loc_est_bw</a></code>).
</p>


<h3>Value</h3>

<p>Returns a numeric vector with the same length as <code>x</code>. Returns a vector of NA if no solution has been found by the solver (GLPK). 
</p>


<h3>Author(s)</h3>

<p>Hohsuk Noh.
</p>


<h3>References</h3>

<p>Daouia, A., Noh, H. and Park, B.U. (2016). Data Envelope fitting with constrained polynomial splines. <em>Journal of the Royal Statistical Society: Series B</em>, <b>78</b>(1), 3-30. doi:10.1111/rssb.12098.
</p>
<p>Hall, P. and Park, B.U. (2004). Bandwidth choice for local polynomial estimation of smooth boundaries. <em>Journal of Multivariate Analysis</em>, 91, 240-261. 
</p>
<p>Hall, P., Park, B.U. and Stern, S.E. (1998). On polynomial estimators of frontiers and boundaries. <em>Journal of Multivariate Analysis</em>, 66, 71-98.  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+loc_est_bw">loc_est_bw</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("nuclear")
x.nucl &lt;- seq(min(nuclear$xtab), max(nuclear$xtab), 
 length.out=101) 
# 1. Unconstrained estimator
# Optimal bandwidths over 100 bootstrap replications
## Not run: 
h.nucl.u &lt;- loc_est_bw(nuclear$xtab, nuclear$ytab, 
 x.nucl, h=40, B=100, method="u")

## End(Not run)
(h.nucl.u&lt;-79.11877)
y.nucl.u&lt;-loc_est(nuclear$xtab, nuclear$ytab, x.nucl, 
 h=h.nucl.u, method="u")

# 2. improved version of the estimator
# Optimal bandwidths over 100 bootstrap replications
## Not run:  
h.nucl.m &lt;- loc_est_bw(nuclear$xtab, nuclear$ytab, 
 x.nucl, h=40, B=100, method="m")

## End(Not run) 
(h.nucl.m&lt;-79.12)
y.nucl.m&lt;-loc_est(nuclear$xtab, nuclear$ytab, x.nucl, 
 h=h.nucl.m, method="m") 

# 3. Representation 
plot(x.nucl, y.nucl.u, lty=1, lwd=4, col="magenta", type="l")
lines(x.nucl, y.nucl.m, lty=2, lwd=4, col="cyan") 
points(ytab~xtab, data=nuclear)
legend("topleft",legend=c("unconstrained", "improved"), 
 col=c("magenta","cyan"), lwd=4, lty=c(1,2))
</code></pre>

<hr>
<h2 id='loc_est_bw'>
Bandwidth selection for the local linear frontier estimator
</h2><span id='topic+loc_est_bw'></span>

<h3>Description</h3>

<p>Computes the optimal bootstrap bandwidth proposed by Hall and Park (2004) for the local linear frontier estimator. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loc_est_bw(xtab, ytab, x, hini, B = 5, method = "u", 
 fix.seed = FALSE, control = list("tm_limit" = 700))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loc_est_bw_+3A_xtab">xtab</code></td>
<td>
<p>a numeric vector containing the observed inputs  <code class="reqn">x_1,\ldots,x_n</code>.</p>
</td></tr>
<tr><td><code id="loc_est_bw_+3A_ytab">ytab</code></td>
<td>
<p>a numeric vector of the same length as <code>xtab</code> containing the observed outputs <code class="reqn">y_1,\ldots,y_n</code>.</p>
</td></tr>
<tr><td><code id="loc_est_bw_+3A_x">x</code></td>
<td>
<p>a numeric vector of evaluation points in which the estimator is to be computed.</p>
</td></tr>
<tr><td><code id="loc_est_bw_+3A_hini">hini</code></td>
<td>
<p>the initial bandwidth at which the local linear estimate will be computed.</p>
</td></tr>
<tr><td><code id="loc_est_bw_+3A_b">B</code></td>
<td>
<p>number of bootstrap replications.</p>
</td></tr>
<tr><td><code id="loc_est_bw_+3A_method">method</code></td>
<td>
<p>a character equal to &quot;u&quot; (unconstrained estimator) or &quot;m&quot; (improved version of the unconstrained estimator).</p>
</td></tr>   
<tr><td><code id="loc_est_bw_+3A_fix.seed">fix.seed</code></td>
<td>
<p>a boolean equal to TRUE for fixing the seed (bootstrap sampling).</p>
</td></tr> 
<tr><td><code id="loc_est_bw_+3A_control">control</code></td>
<td>
<p>a list of parameters to the GLPK solver. See *Details* of help(Rglpk_solve_LP).</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>For a detailed description of the bootstrap procedure, see Hall and Park (2004).
</p>


<h3>Value</h3>

<p>Returns the optimal bootstrap bandwidth.
</p>


<h3>Note</h3>

<p>The computational burden here is very demanding, so be forewarned. 
</p>


<h3>Author(s)</h3>

<p>Hohsuk Noh.
</p>


<h3>References</h3>

<p>Hall, P. and Park, B.U. (2004). Bandwidth choice for local polynomial estimation of smooth boundaries. <em>Journal of Multivariate Analysis</em>, 91, 240-261.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+loc_est">loc_est</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data("nuclear")
x.nucl &lt;- seq(min(nuclear$xtab), max(nuclear$xtab), 
 length.out = 101) 
# 1. Unconstrained case 
# Optimal bandwidths over 100 bootstrap replications
system.time(
h.nucl.u &lt;- loc_est_bw(nuclear$xtab, nuclear$ytab, 
 x.nucl, hini = 40, B = 1, method = "u")
)
# result is 79.11877

# 2. Monotonicity constraint
# Optimal bandwidths over 100 bootstrap replications
h.nucl.m &lt;- loc_est_bw(nuclear$xtab, nuclear$ytab, 
 x.nucl, hini = 40, B = 100, method = "m") 
# result is 79.12

## End(Not run)
</code></pre>

<hr>
<h2 id='loc_max'>
Local maximum frontier estimators
</h2><span id='topic+loc_max'></span>

<h3>Description</h3>

<p>Computes the local constant and local DEA  boundary estimates  proposed by  Gijbels and Peng (2000).   
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loc_max(xtab, ytab, x, h, type="one-stage")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="loc_max_+3A_xtab">xtab</code></td>
<td>
<p>a numeric vector containing the observed inputs  <code class="reqn">x_1,\ldots,x_n</code>.</p>
</td></tr>
<tr><td><code id="loc_max_+3A_ytab">ytab</code></td>
<td>
<p>a numeric vector of the same length as <code>xtab</code> containing the observed outputs <code class="reqn">y_1,\ldots,y_n</code>.</p>
</td></tr>
<tr><td><code id="loc_max_+3A_x">x</code></td>
<td>
<p>a numeric vector of evaluation points in which the estimator is to be computed.</p>
</td></tr>
<tr><td><code id="loc_max_+3A_h">h</code></td>
<td>
<p>determines the bandwidth at which the estimate will be computed.</p>
</td></tr>
<tr><td><code id="loc_max_+3A_type">type</code></td>
<td>
<p>a character equal to &quot;one-stage&quot; or &quot;two-stage&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When estimating <code class="reqn">\varphi(x)</code>, for a given point <code class="reqn">x\in\R</code>, 
the methodology of Gijbels and Peng consists of considering a strip around <code class="reqn">x</code> of width <code class="reqn">2h</code>,
where <code class="reqn">h=h_n\to 0</code> with <code class="reqn">nh_n\to\infty</code>
as <code class="reqn">n\to\infty</code>, and focusing then on the <code class="reqn">y_i</code> observations falling into this strip.
More precisely, they consider the transformend variables <code class="reqn">z^{xh}_i = y_i\mathbf{1}_{(|x_i-x|\leq h)}</code>, 
<code class="reqn">i=1,\ldots,n</code>, and the corresponding order statistics <code class="reqn">z^{xh}_{(1)}\le\cdots\le z^{xh}_{(n)}</code>.
</p>
<p>The simple maximum <code class="reqn">z^{xh}_{(n)}=\max_{i=1,\ldots,n}z^{xh}_i</code> defines then the local constant estimator of the
frontier point <code class="reqn">\varphi(x)</code> [option <code>type="one-stage"</code>].
This opens a way to a  two-stage estimation procedure as follows.
In a first stage, Gijbels and Peng calculate the maximum <code class="reqn">z^{xh}_{(n)}</code>.
Then,  they  suggest to replace each observation <code class="reqn">y_i</code>  in  the strip of width <code class="reqn">2h</code> around <code class="reqn">x</code> by  this maximum,  leaving all observations outside the strip unchanged.
More precisely, they define
<code class="reqn">\tilde{y}_i= y_i</code> if <code class="reqn">|x_i-x| &gt; h</code> and <code class="reqn">\tilde{y}_i= z^{xh}_{(n)}</code> if <code class="reqn">|x_i-x| \leq h</code> either.
Then, they apply the DEA estimator (see the function <code><a href="#topic+dea_est">dea_est</a></code>) to these transformed data <code class="reqn">(x_i,\tilde{y}_i)</code>,
giving  the local DEA  estimator  (option  <code>type="two-stage"</code>). 
An <em>ad hoc</em> way of selecting <code class="reqn">h</code> is by using for instance the function <code>npcdistbw</code> from the <span class="pkg">np</span> package (see Daouia et al. (2016) for details).
</p>


<h3>Value</h3>

<p>Returns a numeric vector with the same length as <code>x</code>.
</p>


<h3>Author(s)</h3>

<p>Abdelaati Daouia and Thibault Laurent.
</p>


<h3>References</h3>

<p>Daouia, A., Laurent, T. and Noh, H. (2017). npbr: A Package for Nonparametric Boundary Regression in R. <em>Journal of Statistical Software</em>, <b>79</b>(9), 1-43. doi:10.18637/jss.v079.i09.
</p>
<p>Gijbels, I. and Peng, L. (2000). Estimation of a support curve  via order statistics, <em>Extremes</em>, 3,  251&ndash;277. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dea_est">dea_est</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("green")
x.green &lt;- seq(min(log(green$COST)), max(log(green$COST)), 
 length.out=101)
# Local maximum frontier estimates
# a. Local constant estimator
loc_max_1stage&lt;-loc_max(log(green$COST), log(green$OUTPUT), 
 x.green, h=0.5, type="one-stage")
# b. Local DEA estimator
loc_max_2stage&lt;-loc_max(log(green$COST), log(green$OUTPUT), 
 x.green, h=0.5, type="two-stage")  
# Representation 
plot(log(OUTPUT)~log(COST), data=green)
lines(x.green, loc_max_1stage, lty=1, col="magenta")
lines(x.green, loc_max_2stage, lty=2, col="cyan")
legend("topleft",legend=c("one-stage", "two-stage"), 
 col=c("magenta","cyan"), lty=c(1,2))
</code></pre>

<hr>
<h2 id='mopt_pwm'>
Threshold selection for the PWM frontier estimator
</h2><span id='topic+mopt_pwm'></span>

<h3>Description</h3>

<p>This function implements the optimal smoothing parameter 
<code>coefm</code> involved in the probability-weighted moment frontier estimator of Daouia, Florens and Simar (2012).    
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mopt_pwm(xtab, ytab, x, a=2, rho, wind.coef=0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mopt_pwm_+3A_xtab">xtab</code></td>
<td>
<p>a numeric vector containing the observed inputs  <code class="reqn">x_1,\ldots,x_n</code>.</p>
</td></tr>
<tr><td><code id="mopt_pwm_+3A_ytab">ytab</code></td>
<td>
<p>a numeric vector of the same length as <code>xtab</code> containing the observed outputs <code class="reqn">y_1,\ldots,y_n</code>.</p>
</td></tr>
<tr><td><code id="mopt_pwm_+3A_x">x</code></td>
<td>
<p>a numeric vector of evaluation points in which the estimator is to be computed.</p>
</td></tr> 
<tr><td><code id="mopt_pwm_+3A_a">a</code></td>
<td>
<p>a smoothing parameter (integer) larger than or equal to 2 (2 by default).</p>
</td></tr>
<tr><td><code id="mopt_pwm_+3A_rho">rho</code></td>
<td>
<p>a numeric vector of the same length as <code>x</code> or a scalar, which determines the values of rho.</p>
</td></tr>  
<tr><td><code id="mopt_pwm_+3A_wind.coef">wind.coef</code></td>
<td>
<p>a scalar coefficient to be selected in the interval (0,1].</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>This is an implementation of an automated selection of the parameter <code>coefm</code>
involved in the probability-weighted moment (PWM) estimator <code class="reqn">\tilde\varphi_{pwm}(x)</code>  [see <code><a href="#topic+dfs_pwm">dfs_pwm</a></code>].
It is an adaptation of the experimental method <code><a href="#topic+kopt_momt_pick">kopt_momt_pick</a></code> by Daouia et al. (2010).
The idea is to select first (for each <code class="reqn">x</code>) a grid of values for the parameter <code>coefm</code> given by 
<code class="reqn">c = 1, \cdots, \min(10,[\sqrt{N_x}])</code>, where <code class="reqn">N_x=\sum_{i=1}^n1_{\{x_i\le x\}}</code>,  
and then select the <code class="reqn">c</code> where the variation of the results is the smallest. 
To achieve this, we compute the standard deviations of <code class="reqn">\tilde\varphi_{pwm}(x)</code> over a &ldquo;window&rdquo; of size 
<code class="reqn">wind.coef  \times \min(10,[\sqrt{N_x}])</code>, where the coefficient <code>wind.coef</code> should be selected 
in the interval <code class="reqn">(0,1]</code> in such a way to avoid numerical instabilities. 
The default option <code>wind.coef=0.1</code> corresponds to having a window large enough to cover around <code class="reqn">10\%</code> 
of the possible values of <code class="reqn">c</code> in the selected range of values for <code>coefm</code>. 
The value of <code class="reqn">c</code> where the standard deviation is minimal defines the desired <code>coefm</code>.   
</p>


<h3>Value</h3>

<p>Returns a numeric vector with the same length as <code>x</code>.
</p>


<h3>Author(s)</h3>

<p>Abdelaati Daouia and Thibault Laurent. 
</p>


<h3>References</h3>

<p>Daouia, A., Florens, J.-P. and Simar, L. (2010). Frontier estimation and extreme value theory. <em>Bernoulli</em>, 16, 1039-1063.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dfs_pwm">dfs_pwm</a></code>, <code><a href="#topic+kopt_momt_pick">kopt_momt_pick</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("post")
x.post&lt;- seq(post$xinput[100],max(post$xinput), 
 length.out=100) 
## Not run: 
# When rho[x] is known and equal to 2:
best_cm.1&lt;- mopt_pwm(post$xinput, post$yprod, 
 x.post, a=2, rho=2)

## End(Not run)
</code></pre>

<hr>
<h2 id='npbr-package'>Nonparametric boundary regression</h2><span id='topic+npbr-package'></span><span id='topic+npbr'></span>

<h3>Description</h3>

<p>The package npbr (Daouia et al., 2017) is the first free specialized software for data edge and frontier analysis in the statistical literature.
It provides a variety of  functions for the best known and most innovative approaches to nonparametric boundary estimation. 
The selected methods are concerned with empirical, smoothed, unrestricted  as well  as constrained fits under both separate and multiple shape constraints.  
They also cover data envelopment techniques as well as robust approaches to outliers. 
The routines included in <span class="pkg">npbr</span>  are  user  friendly and afford a large degree of flexibility in the estimation specifications.  
They provide smoothing parameter selection for the modern local linear and polynomial spline methods and for some promising extreme value techniques. 
Also, they seamlessly allow for Monte Carlo comparisons among the implemented estimation procedures.  This package will be very useful for statisticians and applied researchers interested 
in employing nonparametric boundary regression models. Its use is illustrated with a number of empirical applications and simulated examples.
</p>


<h3>Details</h3>

<p>Suppose that we have <code class="reqn">n</code> pairs of observations
<code class="reqn">(x_i,y_i),~i=1,\ldots,n</code>,
from a bivariate distribution with a density <code class="reqn">f(x,y)</code> in <code class="reqn">R^2</code>. The support <code class="reqn">\Psi</code> of <code class="reqn">f</code> is assumed to be of the form
</p>
<p style="text-align: center;"><code class="reqn">
 \Psi = \{ (x,y) | y \leq \varphi(x) \}   \supseteq   \{ (x,y) | f(x,y) &gt; 0 \} </code>
</p>

<p style="text-align: center;"><code class="reqn">\{ (x,y) | y &gt; \varphi(x) \}   \subseteq \{ (x,y) | f(x,y) = 0 \},
  </code>
</p>
  
<p>where the graph of <code class="reqn">\varphi</code> corresponds to the locus of the curve above which the density <code class="reqn">f</code> is zero. We consider the estimation of the frontier function 
<code class="reqn">\varphi</code> based on the sample <code class="reqn">\{ (x_i,y_i),~i=1,\ldots,n\}</code> in the general setting where the density <code class="reqn">f</code> may have sudden jumps at the frontier, 
decay to zero or rise up to infinity as it approaches its support boundary.  
</p>
<p>The overall objective of the present package is to provide a large variety of functions for the best known approaches to nonparametric boundary regression, 
including the vast class of methods employed in both Monte Carlo 
comparisons of  Daouia et al. (2016) and Noh (2014) as well as other promising nonparametric devices, namely the extreme-value techniques 
of Gijbels and Peng (2000), Daouia et al. (2010) and Daouia et al. (2012).   
The various functions in the <span class="pkg">npbr</span> package are summarized in the table below.
We are not aware of any other existing set of statistical routines more adapted to data envelope fitting and robust frontier estimation.  
Only the classical nonsmooth FDH and DEA methods can be found in some available packages dedicated to the economic literature on measurements of  the production performance of enterprises, such as the programs 
<span class="pkg">Benchmarking</span> by  Bogetoft and Otto (2011) and  <span class="pkg">FEAR</span> by  Wilson (2008). Other contributions to the econometric literature on frontier analysis by Parmeter and Racine (2013) can be found at <a href="https://socialsciences.mcmaster.ca/racinej/Gallery/Home.html">https://socialsciences.mcmaster.ca/racinej/Gallery/Home.html</a>.
The package <span class="pkg">npbr</span> is actually the first free specialized software for the statistical literature on nonparametric frontier analysis.  The routines included in <span class="pkg">npbr</span> are  user 
friendly and highly flexible  in terms of estimation specifications.  They allow the user to locate the boundary from data 
by making use of both empirical and smooth fits as well as (un)constrained  estimates under single and multiple shape constraints.    
They also integrate smoothing parameter selection for the innovative methods based on local linear techniques, polynomial splines, extreme values and kernel smoothing,  
though the proposed selection procedures can be computationally demanding.
</p>
<p>In addition, the package will be extremely useful for researchers and practitioners interested in employing nonparametric boundary regression methods. 
On one hand, such methods are very appealing because they rely on very few assumptions and benefit from their modeling flexibility, function approximation 
power and ability to detect the boundary structure of data without recourse to any <em>a priori</em> parametric restrictions on the shape of the frontier and/or the distribution of noise. 
On the other hand, the package offers <span class="rlang"><b>R</b></span> users and statisticians in this active area of research simple functions to compute the empirical mean integrated squared error, 
the empirical integrated squared bias and the empirical integrated variance of the implemented frontier estimators. 
This seamlessly allows  the interested researcher  to reproduce the Monte Carlo estimates obtained in the original articles and, perhaps most importantly,  
to easily compare the quality of any new proposal with the competitive existing methods.
</p>

<table>
<tr>
 <td style="text-align: left;">
Function                              </td><td style="text-align: left;">  Description                           </td><td style="text-align: left;"> Reference </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+dea_est">dea_est</a></code>                 </td><td style="text-align: left;">  DEA, FDH                              </td><td style="text-align: left;">  Farrell (1957) </td>
</tr>
<tr>
 <td style="text-align: left;">
                                      </td><td style="text-align: left;">                                        </td><td style="text-align: left;">  Deprins et al. (1984), </td>
</tr>
<tr>
 <td style="text-align: left;">
                                      </td><td style="text-align: left;">  and linearized FDH                    </td><td style="text-align: left;"> Hall and Park (2002) </td>
</tr>
<tr>
 <td style="text-align: left;">
                                      </td><td style="text-align: left;">                                        </td><td style="text-align: left;"> Jeong and Simar (2006)</td>
</tr>
<tr>
 <td style="text-align: left;">            
<code><a href="#topic+loc_est">loc_est</a></code>                 </td><td style="text-align: left;">  Local linear fitting                  </td><td style="text-align: left;">  Hall et al. (1998), </td>
</tr>
<tr>
 <td style="text-align: left;">
                                      </td><td style="text-align: left;">                                        </td><td style="text-align: left;">  Hall and Park (2004)  </td>
</tr>
<tr>
 <td style="text-align: left;">   
<code><a href="#topic+loc_est_bw">loc_est_bw</a></code>              </td><td style="text-align: left;">  Bandwidth choice                      </td><td style="text-align: left;">  Hall and Park (2004) </td>
</tr>
<tr>
 <td style="text-align: left;"> 
                                      </td><td style="text-align: left;">  for local linear fitting              </td><td style="text-align: left;">    </td>
</tr>
<tr>
 <td style="text-align: left;"> 
<code><a href="#topic+poly_est">poly_est</a></code>                </td><td style="text-align: left;">  Polynomial estimation                 </td><td style="text-align: left;">  Hall et al. (1998)  </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+poly_degree">poly_degree</a></code>             </td><td style="text-align: left;"> Optimal polynomial                     </td><td style="text-align: left;">  Daouia et al. (2015)  </td>
</tr>
<tr>
 <td style="text-align: left;">
                                      </td><td style="text-align: left;">  degree selection                      </td><td style="text-align: left;">   </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+dfs_momt">dfs_momt</a></code>                </td><td style="text-align: left;">  Moment type estimation                </td><td style="text-align: left;">  Daouia et al. (2010),  </td>
</tr>
<tr>
 <td style="text-align: left;">
                                      </td><td style="text-align: left;">                                        </td><td style="text-align: left;">  Dekkers et al. (1989)  </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+dfs_pick">dfs_pick</a></code>                </td><td style="text-align: left;">  Pickands type estimation              </td><td style="text-align: left;"> Daouia et al. (2010),  </td>
</tr>
<tr>
 <td style="text-align: left;">
                                      </td><td style="text-align: left;">                                        </td><td style="text-align: left;">  Dekkers and de Haan (1989) </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+rho_momt_pick">rho_momt_pick</a></code>           </td><td style="text-align: left;">  Conditional tail                      </td><td style="text-align: left;"> Daouia et al. (2010),   </td>
</tr>
<tr>
 <td style="text-align: left;">
                                      </td><td style="text-align: left;"> index estimation                       </td><td style="text-align: left;"> Dekkers et al. (1989),  </td>
</tr>
<tr>
 <td style="text-align: left;">
                                      </td><td style="text-align: left;">                                        </td><td style="text-align: left;">  Dekkers and de Haan (1989) </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+kopt_momt_pick">kopt_momt_pick</a></code>          </td><td style="text-align: left;"> Threshold selection for                </td><td style="text-align: left;"> Daouia et al. (2010)  </td>
</tr>
<tr>
 <td style="text-align: left;">
                                      </td><td style="text-align: left;"> moment/Pickands frontiers              </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+dfs_pwm">dfs_pwm</a></code>                </td><td style="text-align: left;"> Nonparametric frontier                 </td><td style="text-align: left;"> Daouia et al. (2012) </td>
</tr>
<tr>
 <td style="text-align: left;">
                                      </td><td style="text-align: left;"> regularization                         </td><td style="text-align: left;">  </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+loc_max">loc_max</a></code>                 </td><td style="text-align: left;">  Local constant estimation             </td><td style="text-align: left;"> Gijbels and Peng (2000) </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+pick_est">pick_est</a></code>                </td><td style="text-align: left;">  Local extreme-value estimation        </td><td style="text-align: left;">  Gijbels and Peng (2000) </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+quad_spline_est">quad_spline_est</a></code>         </td><td style="text-align: left;">  Quadratic spline fitting              </td><td style="text-align: left;"> Daouia et al. (2015)  </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+quad_spline_kn">quad_spline_kn</a></code>          </td><td style="text-align: left;">  Knot selection for                    </td><td style="text-align: left;">  Daouia et al. (2015)   </td>
</tr>
<tr>
 <td style="text-align: left;">
                                      </td><td style="text-align: left;">  quadratic spline fitting              </td><td style="text-align: left;">    </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+cub_spline_est">cub_spline_est</a></code>          </td><td style="text-align: left;">  Cubic spline fitting                  </td><td style="text-align: left;"> Daouia et al. (2015) </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+cub_spline_kn">cub_spline_kn</a></code>           </td><td style="text-align: left;">  Knot selection for                    </td><td style="text-align: left;">  Daouia et al. (2015)  </td>
</tr>
<tr>
 <td style="text-align: left;">
                                      </td><td style="text-align: left;">  cubic spline fitting                  </td><td style="text-align: left;">     </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+kern_smooth">kern_smooth</a></code>                   </td><td style="text-align: left;"> Nonparametric kernel                   </td><td style="text-align: left;">  Parmeter and Racine (2013), </td>
</tr>
<tr>
 <td style="text-align: left;">
                                      </td><td style="text-align: left;"> boundary regression                    </td><td style="text-align: left;">  Noh (2014) </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+kern_smooth_bw">kern_smooth_bw</a></code>               </td><td style="text-align: left;"> Bandwidth choice for                   </td><td style="text-align: left;">  Parmeter and Racine (2013),    </td>
</tr>
<tr>
 <td style="text-align: left;">
                                      </td><td style="text-align: left;"> kernel boundary regression             </td><td style="text-align: left;"> Noh (2014)   </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Abdelaati Daouia &lt;Abdelaati.Daouia@tse-fr.eu&gt;, Thibault Laurent &lt;thibault.laurent@univ-tlse1.fr&gt;, Hohsuk Noh &lt;word5810@gmail.com&gt;
</p>
<p>Maintainer: Thibault Laurent &lt;thibault.laurent@univ-tlse1.fr&gt;
</p>


<h3>References</h3>

<p>Daouia, A., Florens, J.-P. and Simar, L. (2010). Frontier estimation and extreme value theory. <em>Bernoulli</em>, <b>16</b>, 1039-1063.
</p>
<p>Daouia, A., Florens, J.-P. and Simar, L. (2012). Regularization of Nonparametric Frontier Estimators. <em>Journal of Econometrics</em>, <b>168</b>, 285-299.
</p>
<p>Daouia, A., Laurent, T. and Noh, H. (2017). npbr: A Package for Nonparametric Boundary Regression in R. <em>Journal of Statistical Software</em>, <b>79</b>(9), 1-43. doi:10.18637/jss.v079.i09.
</p>
<p>Daouia, A., Noh, H. and Park, B.U. (2016). Data Envelope fitting with constrained polynomial splines. <em>Journal of the Royal Statistical Society: Series B</em>, <b>78</b>(1), 3-30. doi:10.1111/rssb.12098.
</p>
<p>Dekkers, A.L.M. and L. de Haan (1989). On the estimation of extreme-value index and large quantiles estimation. <em>Annals of Statistics</em>, <b>17</b>, 1795-1832.
</p>
<p>Dekkers, A.L.M., Einmahl, J.H.J. and L. de Haan (1989). A moment estimator for the index of an extreme-value distribution. <em>Annals of Statistics</em>, <b>17</b>, 1833-1855.
</p>
<p>Deprins, D., Simar, L. and Tulkens H. (1984). Measuring labor efficiency in post offices, in: M. Marchand, P. Pestieau and H. Tulkens (Eds), The performance of Public Enterprises: Concepts and Measurements. North-Holland, Amsterdam, 243-267.
</p>
<p>Farrell, M.J. (1957). The measurement of productive efficiency. <em>Journal of the Royal Statistical Society, Series A</em>, <b>120</b>, 253-281.
</p>
<p>Gijbels, I. and Peng, L. (2000). Estimation of a support curve via order statistics. <em>Extremes</em>, <b>3</b>, 251-277.
</p>
<p>Hall, P., Park, B.U. and Stern, S.E. (1998). On polynomial estimators of frontiers and boundaries. <em>Journal of Multivariate Analysis</em>, <b>66</b>, 71-98.
</p>
<p>Hall, P. and Park, B.U. (2004). Bandwidth choice for local polynomial estimation of smooth boundaries. <em>Journal of Multivariate Analysis</em>, <b>91</b>, 240-261.
</p>
<p>Jeong, S.-O. and Simar, L. (2006). Linearly interpolated FDH efficiency score for nonconvex frontiers. <em>Journal of Multivariate Analysis</em>, <b>97</b>, 2141-2161.
</p>
<p>Noh, H. (2014). Frontier Estimation using Kernel Smoothing with Data Transformation. <em>Journal of the Korean Statistical Society</em>, <b>43</b>, 503-512.
</p>
<p>Parmeter, C. and Racine, J.S. (2013). Smooth Constrained Frontier Analysis. In <em>Recent Advances and Future Directions in Causality, Prediction, and Specification Analysis: Essays in Honor of Halbert L. White, Jr.</em>, Springer Verlag, (X. Chen and N.R. Swanson Eds), 463-488.
</p>

<hr>
<h2 id='nuclear'>Reliability programs of nuclear reactors
</h2><span id='topic+nuclear'></span>

<h3>Description</h3>

<p>The dataset from the US Electric Power Research Institute (EPRI) consists of 254 toughness results obtained from
non-irradiated representative steels. For each steel <code class="reqn">i</code>, fracture toughness <code class="reqn">y_i</code> and temperature <code class="reqn">x_i</code>
were measured.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(nuclear)</code></pre>


<h3>Format</h3>

<p>A data frame with 254 observations on the following 2 variables.
</p>

<dl>
<dt><code>xtab</code></dt><dd><p>Temperature.</p>
</dd>
<dt><code>ytab</code></dt><dd><p>Fracture toughness of each material.</p>
</dd>
</dl>



<h3>Source</h3>

<p>US Electric Power Research Institute (EPRI).
</p>


<h3>References</h3>

<p>Daouia, A., Girard, S. and Guillou, A. (2014). A Gamma-moment approach to monotonic boundary estimation. <em>Journal of Econometrics</em>, 78, 727-740. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("nuclear")
</code></pre>

<hr>
<h2 id='pick_est'>
Local Pickands' frontier estimator
</h2><span id='topic+pick_est'></span>

<h3>Description</h3>

<p>Computes the Pickands type of  estimator introduced by Gijbels and Peng (2000).  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pick_est(xtab, ytab, x, h, k, type="one-stage")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pick_est_+3A_xtab">xtab</code></td>
<td>
<p>a numeric vector containing the observed inputs  <code class="reqn">x_1,\ldots,x_n</code>.</p>
</td></tr>
<tr><td><code id="pick_est_+3A_ytab">ytab</code></td>
<td>
<p>a numeric vector of the same length as <code>xtab</code> containing the observed outputs <code class="reqn">y_1,\ldots,y_n</code>.</p>
</td></tr>
<tr><td><code id="pick_est_+3A_x">x</code></td>
<td>
<p>a numeric vector of evaluation points in which the estimator is to be computed.</p>
</td></tr>
<tr><td><code id="pick_est_+3A_h">h</code></td>
<td>
<p>determines the bandwidth at which the estimate will be computed.</p>
</td></tr>
<tr><td><code id="pick_est_+3A_k">k</code></td>
<td>
<p>a numeric vector of the same length as <code>x</code>, which determines the thresholds at which the Pickands' estimator will be computed.</p>
</td></tr>
<tr><td><code id="pick_est_+3A_type">type</code></td>
<td>
<p>a character equal to &quot;one-stage&quot; or &quot;two-stage&quot;.</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>The local Pickands' frontier estimator (option <code>type="one-stage"</code>), obtained by applying the well-known approach of Dekkers and de Haan (1989) 
in conjunction with the transformed sample of <code class="reqn">z^{xh}_i</code>'s described in the function <code><a href="#topic+loc_max">loc_max</a></code>, is defined as
</p>
<p style="text-align: center;"><code class="reqn">
z^{xh}_{(n-k)} + \left(z^{xh}_{(n-k)}-z^{xh}_{(n-2k)}\right)\{2^{-\log\frac{z^{xh}_{(n-k)}-z^{xh}_{(n-2k)}}{z^{xh}_{(n-2k)}-z^{xh}_{(n-4k)}}/\log 2}-1\}^{-1}.
</code>
</p>

<p>It is based on three upper order statistics <code class="reqn">z^{xh}_{(n-k)}</code>, <code class="reqn">z^{xh}_{(n-2k)}</code>, <code class="reqn">z^{xh}_{(n-4k)}</code>, and depends on <code class="reqn">h</code> (see <code><a href="#topic+loc_max">loc_max</a></code>)  
as well as an intermediate sequence <code class="reqn">k=k(x,n)\to\infty</code> with <code class="reqn">k/n\to 0</code> as <code class="reqn">n\to\infty</code>. 
The two smoothing parameters <code class="reqn">h</code> and <code class="reqn">k</code> have to be fixed in the 4th and 5th arguments of the function.
</p>
<p>Also, the user can replace each observation <code class="reqn">y_i</code> in  the strip of width <code class="reqn">2h</code> around <code class="reqn">x</code> by the resulting local Pickands', leaving all observations outside the strip unchanged.
Then, one may apply the DEA estimator (see the function <code><a href="#topic+dea_est">dea_est</a></code>) to the obtained transformed data,
giving the local DEA  estimator  (option  <code>type="two-stage"</code>). 
</p>


<h3>Value</h3>

<p>Returns a numeric vector with the same length as <code>x</code>.
</p>


<h3>Author(s)</h3>

<p>Abdelaati Daouia and Thibault Laurent.
</p>


<h3>References</h3>

<p>Dekkers, A.L.M. and L. de Haan (1989). On the estimation of extreme-value index and large quantiles estimation, <em>Annals of Statistics</em>, 17, 1795-1832.
</p>
<p>Gijbels, I. and Peng, L. (2000). Estimation of a support curve  via order statistics, <em>Extremes</em>, 3,  251-277. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dea_est">dea_est</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data("green")
plot(log(OUTPUT)~log(COST), data=green)
x &lt;- seq(min(log(green$COST)), max(log(green$COST)), length.out=101)
h=0.5
nx&lt;-unlist(lapply(x,function(y) length(which(abs(log(green$COST)-y)&lt;=h))))
k&lt;-trunc(nx^0.1)
lines(x, pick_est(log(green$COST), log(green$OUTPUT), x, h=h, k=k), lty=1, col="red")

## End(Not run)
</code></pre>

<hr>
<h2 id='poly_degree'>
AIC and BIC criteria for choosing the optimal degree of the polynomial frontier estimator
</h2><span id='topic+poly_degree'></span>

<h3>Description</h3>

<p>Computes the optimal degree of the unconstrained polynomial frontier estimator proposed by Hall, Park and Stern (1998).</p>


<h3>Usage</h3>

<pre><code class='language-R'>poly_degree(xtab, ytab, prange=0:20, type="AIC", 
 control = list("tm_limit" = 700))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="poly_degree_+3A_xtab">xtab</code></td>
<td>
<p>a numeric vector containing the observed inputs  <code class="reqn">x_1,\ldots,x_n</code>.</p>
</td></tr>
<tr><td><code id="poly_degree_+3A_ytab">ytab</code></td>
<td>
<p>a numeric vector of the same length as <code>xtab</code> containing the observed outputs <code class="reqn">y_1,\ldots,y_n</code>.</p>
</td></tr>
<tr><td><code id="poly_degree_+3A_prange">prange</code></td>
<td>
<p>a vector of integers specifying the range in which the optimal degree of the polynomial frontier estimator is to be selected.</p>
</td></tr>
<tr><td><code id="poly_degree_+3A_type">type</code></td>
<td>
<p>a character equal to &quot;AIC&quot; or &quot;BIC&quot;.</p>
</td></tr>
<tr><td><code id="poly_degree_+3A_control">control</code></td>
<td>
<p>a list of parameters to the GLPK solver. See *Details* of help(Rglpk_solve_LP).</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>As the degree <code class="reqn">p</code> of the polynomial estimator <code class="reqn">\hat \varphi_{n,p}</code> (see <code><a href="#topic+poly_est">poly_est</a></code>) determines the dimensionality of the approximating function, we may view the problem of choosing p as model selection.
By analogy to the information criteria proposed by Daouia et al. (2016) in the boundary regression context, we obtain the optimal polynomial degree by minimizing
</p>
<p style="text-align: center;"><code class="reqn">
AIC(p) = \log \left( \sum_{i=1}^{n} (\hat \varphi_{n,p}(x_i)-y_i)\right) + (p+1)/n ,</code>
</p>

<p style="text-align: center;"><code class="reqn">BIC(p) = \log \left( \sum_{i=1}^{n} (\hat \varphi_{n,p}(x_i)-y_i)\right) + \log n (p+1)/(2n).</code>
</p>

<p>The first one (option <code>type = "AIC"</code>) is similar to the famous Akaike information criterion Akaike (1973) and the second one
(option <code>type = "BIC"</code>) to the Bayesian information criterion Schwartz (1978). 
</p>


<h3>Value</h3>

<p>Returns an integer.
</p>


<h3>Author(s)</h3>

<p>Hohsuk Noh.
</p>


<h3>References</h3>

<p>Akaike, H. (1973).  Information theory and an extension of the maximum likelihood principle, in <em>Second International Symposium of Information Theory</em>, eds. B. N. Petrov and F. Csaki, Budapest: Akademia Kiado, 267&ndash;281.  
</p>
<p>Daouia, A., Noh, H. and Park, B.U. (2016). Data Envelope fitting with constrained polynomial splines. <em>Journal of the Royal Statistical Society: Series B</em>, <b>78</b>(1), 3-30. doi:10.1111/rssb.12098.
</p>
<p>Hall, P., Park, B.U. and Stern, S.E. (1998). On polynomial estimators of frontiers and boundaries. <em>Journal of Multivariate Analysis</em>, 66, 71-98.
</p>
<p>Schwartz, G. (1978). Estimating the dimension of a model, <em>Annals of Statistics</em>, 6, 461&ndash;464.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+poly_est">poly_est</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("air")
x.air &lt;- seq(min(air$xtab), max(air$xtab), 
 length.out = 101)
# Optimal polynomial degrees via the AIC criterion
(p.aic.air &lt;- poly_degree(air$xtab, air$ytab, 
 type = "AIC"))
# Optimal polynomial degrees via the BIC criterion  
(p.bic.air &lt;- poly_degree(air$xtab, air$ytab, 
 type = "BIC"))
</code></pre>

<hr>
<h2 id='poly_est'>
Polynomial frontier estimators</h2><span id='topic+poly_est'></span>

<h3>Description</h3>

<p>Computes the polynomial-type estimators of frontiers and boundaries proposed by Hall, Park and Stern (1998).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>poly_est(xtab, ytab, x, deg, control = list("tm_limit" = 700))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="poly_est_+3A_xtab">xtab</code></td>
<td>
<p>a numeric vector containing the observed inputs  <code class="reqn">x_1,\ldots,x_n</code>.</p>
</td></tr>
<tr><td><code id="poly_est_+3A_ytab">ytab</code></td>
<td>
<p>a numeric vector of the same length as <code>xtab</code> containing the observed outputs <code class="reqn">y_1,\ldots,y_n</code>.</p>
</td></tr>
<tr><td><code id="poly_est_+3A_x">x</code></td>
<td>
<p>a numeric vector of evaluation points in which the estimator is to be computed.</p>
</td></tr>
<tr><td><code id="poly_est_+3A_deg">deg</code></td>
<td>
<p>an integer (polynomial degree).</p>
</td></tr>
<tr><td><code id="poly_est_+3A_control">control</code></td>
<td>
<p>a list of parameters to the GLPK solver. See *Details* of help(Rglpk_solve_LP).</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>The data edge is modeled by a single polynomial <code class="reqn">\varphi_{\theta}(x) = \theta_0+\theta_1 x+\cdots+\theta_p x^p</code> 
of known degree <code class="reqn">p</code> that envelopes the full data and minimizes the area under its graph for <code class="reqn">x\in[a,b]</code>, with <code class="reqn">a</code> and <code class="reqn">b</code> 
being respectively the lower and upper endpoints of the design points <code class="reqn">x_1,\ldots,x_n</code>.
The implemented function is the estimate <code class="reqn">\hat\varphi_{n,p}(x) = \hat\theta_0+\hat\theta_1 x+\cdots+\hat\theta_p x^p</code>
of <code class="reqn">\varphi(x)</code>, where  <code class="reqn">\hat\theta=(\hat\theta_0,\hat\theta_1,\cdots,\hat\theta_p)^T</code>
minimizes
<code class="reqn">\int_{a}^b \varphi_{\theta}(x) \,dx</code>
over <code class="reqn">\theta\in\R^{p+1}</code> subject to the envelopment constraints
<code class="reqn">\varphi_{\theta}(x_i)\geq y_i</code>,  <code class="reqn">i=1,\ldots,n</code>.
</p>


<h3>Value</h3>

<p>Returns a numeric vector with the same length as <code>x</code>. Returns a vector of NA if no solution has been found by the solver (GLPK). 
</p>


<h3>Author(s)</h3>

<p>Hohsuk Noh.
</p>


<h3>References</h3>

<p>Hall, P., Park, B.U. and Stern, S.E. (1998). On polynomial estimators of frontiers and boundaries. <em>Journal of Multivariate Analysis</em>, 66, 71-98. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+loc_est">loc_est</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("air")
x.air &lt;- seq(min(air$xtab), max(air$xtab), 
 length.out = 101)
# Optimal polynomial degrees via the AIC criterion
(p.aic.air &lt;- poly_degree(air$xtab, air$ytab, 
 type = "AIC"))
# Polynomial boundaries estimate 
y.poly.air&lt;-poly_est(air$xtab, air$ytab, x.air, 
 deg = p.aic.air)
# Representation
plot(x.air, y.poly.air, lty = 1, lwd = 4, 
 col = "magenta", type = "l")
points(ytab~xtab, data = air)  
legend("topleft",legend = paste("degree =", p.aic.air), 
 col = "magenta", lwd = 4, lty = 1)  
</code></pre>

<hr>
<h2 id='post'>European postal services</h2><span id='topic+post'></span>

<h3>Description</h3>

<p>The dataset <code>post</code> about the cost of the delivery activity of the postal services in Europe was first analyzed by Cazals, Florens and Simar (2002). 
There are 4,000 post offices observed in 1994. For each post office <code class="reqn">i</code>, the input <code class="reqn">x_i</code> is the labor cost measured by the quantity of labor, 
which represents more than <code class="reqn">80\%</code> of the total cost of the delivery activity. The output <code class="reqn">y_i</code> is defined as the volume of delivered mail (in number of objects). It should be noted that noise has been added to the original data.</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(post)</code></pre>


<h3>Format</h3>

<p>A data frame with 4000 observations on the following 2 variables.
</p>

<dl>
<dt><code>xinput</code></dt><dd><p>a numeric vector.</p>
</dd>
<dt><code>yprod</code></dt><dd><p>a numeric vector.</p>
</dd>
</dl>



<h3>References</h3>

<p>Cazals, C., Florens, J.-P., Simar, L. (2002), Nonparametric frontier estimation: a robust approach, <em>Journal of Econometrics</em>, 106, 1-25.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("post")
</code></pre>

<hr>
<h2 id='quad_spline_est'>
Quadratic spline frontiers 
</h2><span id='topic+quad_spline_est'></span>

<h3>Description</h3>

<p>This function is an implementation of the (un)constrained quadratic spline smoother proposed by Daouia, Noh and Park (2016).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quad_spline_est(xtab, ytab, x, kn = ceiling((length(xtab))^(1/4)), method= "u", 
 all.dea = FALSE, control = list("tm_limit" = 700))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="quad_spline_est_+3A_xtab">xtab</code></td>
<td>
<p>a numeric vector containing the observed inputs  <code class="reqn">x_1,\ldots,x_n</code>.</p>
</td></tr>
<tr><td><code id="quad_spline_est_+3A_ytab">ytab</code></td>
<td>
<p>a numeric vector of the same length as <code>xtab</code> containing the observed outputs <code class="reqn">y_1,\ldots,y_n</code>.</p>
</td></tr>
<tr><td><code id="quad_spline_est_+3A_x">x</code></td>
<td>
<p>a numeric vector of evaluation points in which the estimator is to be computed.</p>
</td></tr>
<tr><td><code id="quad_spline_est_+3A_kn">kn</code></td>
<td>
<p>an integer specifying the number of inter-knot segments used in the computation of the spline estimate.</p>
</td></tr>
<tr><td><code id="quad_spline_est_+3A_method">method</code></td>
<td>
<p>a character equal to &quot;u&quot; (unconstrained estimator), &quot;m&quot; (under the monotonicity constraint) or &quot;mc&quot; (under simultaneous monotonicity and concavity constraints).</p>
</td></tr>
<tr><td><code id="quad_spline_est_+3A_all.dea">all.dea</code></td>
<td>
<p>a boolean.</p>
</td></tr>
<tr><td><code id="quad_spline_est_+3A_control">control</code></td>
<td>
<p>a list of parameters to the GLPK solver. See *Details* of help(Rglpk_solve_LP).</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>Let <code class="reqn">a</code> and <code class="reqn">b</code> be, respectively, the minimum and maximum of the design points <code class="reqn">x_1,\ldots,x_n</code>.  
Denote a partition of <code class="reqn">[a,b]</code> by <code class="reqn">a=t_0&lt;t_1&lt;\cdots&lt;t_{k_n}=b</code> (see below the selection process).
Let <code class="reqn">N=k_n+1</code> and <code class="reqn">\pi(x)=(\pi_1(x),\ldots,\pi_N(x))^T</code> be the vector of normalized 
B-splines of order 3 based on the knot mesh <code class="reqn">\{t_j\}</code> (see, <em>e.g.</em>, Schumaker (2007)).
When the true frontier <code class="reqn">\varphi(x)</code> is known or required to be monotone nondecreasing (option <code>cv=0</code>), 
its constrained quadratic spline estimate is defined by  <code class="reqn">\hat\varphi_n(x)=\pi(x)^T \hat\alpha</code>, where <code class="reqn">\hat\alpha</code> minimizes
</p>
<p style="text-align: center;"><code class="reqn">\int_{0}^1\pi(x)^T \alpha \,dx = \sum_{j=1}^N \alpha_j \int_{0}^1\pi_j(x) \,dx</code>
</p>

<p>over <code class="reqn">\alpha\in\R^N</code> subject to the envelopment and monotonicity constraints
<code class="reqn">\pi(x_i)^T \alpha\geq y_i</code>,  <code class="reqn">i=1,\ldots,n</code>, and <code class="reqn">\pi'(t_j)^T \alpha\geq 0</code>, <code class="reqn">j=0,1,\ldots,k_n</code>, 
with <code class="reqn">\pi'</code> being the derivative of <code class="reqn">\pi</code>.
</p>
<p>Considering the special connection of the spline smoother <code class="reqn">\hat \varphi_n</code> with the traditional FDH frontier
<code class="reqn">\varphi_n</code> (see the function <code><a href="#topic+dea_est">dea_est</a></code>),
Daouia et al. (2015) propose an easy way of choosing the knot mesh.
Let <code class="reqn">(\mathcal{X}_1,\mathcal{Y}_1),\ldots, (\mathcal{X}_\mathcal{N},\mathcal{Y}_\mathcal{N})</code> 
be the observations <code class="reqn">(x_i,y_i)</code> lying on the FDH boundary (<em>i.e.</em> <code class="reqn">y_i=\varphi_n(x_i)</code>).
The basic idea is to pick out a set of knots equally spaced in percentile ranks 
among the <code class="reqn">\mathcal{N}</code> FDH points <code class="reqn">(\mathcal{X}_{\ell},\mathcal{Y}_{\ell})</code> by
taking <code class="reqn">t_j = {\mathcal{X}_{[j \mathcal{N}/k_n]}}</code>, the <code class="reqn">j/k_n</code>th quantile 
of the values of <code class="reqn">\mathcal{X}_{\ell}</code> for <code class="reqn">j=1,\ldots,k_n-1</code>.
The choice of the number of internal knots is then viewed as model selection 
through the minimization of the AIC and BIC information criteria (see the function <code><a href="#topic+quad_spline_kn">quad_spline_kn</a></code>).
</p>
<p>When the monotone boundary <code class="reqn">\varphi(x)</code>  is also believed to be concave (option <code>cv=1</code>),
its constrained fit is defined as <code class="reqn">\hat\varphi^{\star}_n(x)=\pi(x)^T \hat\alpha^{\star}</code>, where <code class="reqn">\hat\alpha^{\star}\in\R^N</code> 
minimizes the same objective function as <code class="reqn">\hat\alpha</code> subject to the same envelopment 
and monotonicity constraints and the additional concavity constraints
<code class="reqn">\pi''(t^*_j)^T \alpha\leq 0</code>, <code class="reqn">j=1,\ldots,k_n,</code>
where <code class="reqn">\pi''</code> is the constant second derivative of <code class="reqn">\pi</code> on each inter-knot interval  and <code class="reqn">t^*_j</code> is the midpoint of <code class="reqn">(t_{j-1},t_j]</code>.
</p>
<p>Regarding the choice of knots, the same scheme as for <code class="reqn">\hat\varphi_n</code> can be applied by replacing 
the FDH points <code class="reqn">(\mathcal{X}_1,\mathcal{Y}_1),\ldots, (\mathcal{X}_\mathcal{N},\mathcal{Y}_\mathcal{N})</code>   
with the DEA points <code class="reqn">(\mathcal{X}^*_1,\mathcal{Y}^*_1),\ldots, (\mathcal{X}^*_\mathcal{M},\mathcal{Y}^*_\mathcal{M})</code>, that is, 
the observations <code class="reqn">(x_i,y_i)</code> lying on the piecewise linear DEA frontier  (see the function <code><a href="#topic+dea_est">dea_est</a></code>). 
Alternatively, the strategy of just using all the DEA points as knots is also 
working quite well for datasets of modest size as shown in Daouia et al. (2016). 
In this case, the user has to choose the option <code>all.dea=TRUE</code>.
</p>


<h3>Value</h3>

<p>Returns a numeric vector with the same length as <code>x</code>. Returns a vector of NA if no solution has been found by the solver (GLPK). 
</p>


<h3>Author(s)</h3>

<p>Hohsuk Noh.
</p>


<h3>References</h3>

<p>Daouia, A., Noh, H. and Park, B.U. (2016). Data Envelope fitting with constrained polynomial splines. <em>Journal of the Royal Statistical Society: Series B</em>, <b>78</b>(1), 3-30. doi:10.1111/rssb.12098.
</p>
<p>Schumaker, L.L. (2007). <em>Spline Functions: Basic Theory</em>, 3rd edition, Cambridge University Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+quad_spline_kn">quad_spline_kn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data("green")
x.green &lt;- seq(min(log(green$COST)), max(log(green$COST)), length.out=101)
# 1. Unconstrained quadratic spline fits
# Optimal number of inter-knot segments via the BIC criterion
(kn.bic.green.u&lt;-quad_spline_kn(log(green$COST), 
 log(green$OUTPUT), method="u", type="BIC"))
# Unconstrained spline estimate
y.quad.green.u&lt;-quad_spline_est(log(green$COST), 
 log(green$OUTPUT), x.green, kn=kn.bic.green.u, method="u")

# 2. Monotonicity constraint
# Optimal number of inter-knot segments via the BIC criterion
(kn.bic.green.m&lt;-quad_spline_kn(log(green$COST), 
 log(green$OUTPUT), method="m", type="BIC"))
# Monotonic splines estimate
y.quad.green.m&lt;-quad_spline_est(log(green$COST), 
 log(green$OUTPUT), x.green, kn=kn.bic.green.m, method="m") 
   
# 3. Monotonicity and Concavity constraints
# Optimal number of inter-knot segments via the BIC criterion
(kn.bic.green.mc&lt;-quad_spline_kn(log(green$COST), 
 log(green$OUTPUT), method="mc", type="BIC"))
# Monotonic/Concave splines estimate 
y.quad.green.mc&lt;-quad_spline_est(log(green$COST), 
 log(green$OUTPUT), x.green, kn=kn.bic.green.mc, 
 method="mc", all.dea=TRUE)

# Representation 
plot(x.green, y.quad.green.u, lty=1, lwd=4, col="green", 
 type="l", xlab="log(COST)", ylab="log(OUTPUT)")   
lines(x.green, y.quad.green.m, lty=2, lwd=4, col="cyan")
lines(x.green, y.quad.green.mc, lwd=4, lty=3, col="magenta")   
points(log(OUTPUT)~log(COST), data=green)
legend("topleft", col=c("green","cyan","magenta"), 
lty=c(1,2,3), legend=c("unconstrained", "monotone", 
 "monotone + concave"), lwd=4, cex=0.8) 

## End(Not run)   
</code></pre>

<hr>
<h2 id='quad_spline_kn'>
AIC and BIC criteria for choosing the optimal number of inter-knot segments in quadratic spline fits  
</h2><span id='topic+quad_spline_kn'></span>

<h3>Description</h3>

<p>Computes the optimal number <code class="reqn">k_n</code> of inter-knot segments in the quadratic spline fits proposed by Daouia, Noh and Park (2016).</p>


<h3>Usage</h3>

<pre><code class='language-R'>quad_spline_kn(xtab, ytab, method, krange = 1:20, type = "AIC", 
 control = list("tm_limit" = 700))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="quad_spline_kn_+3A_xtab">xtab</code></td>
<td>
<p>a numeric vector containing the observed inputs  <code class="reqn">x_1,\ldots,x_n</code>.</p>
</td></tr>
<tr><td><code id="quad_spline_kn_+3A_ytab">ytab</code></td>
<td>
<p>a numeric vector of the same length as <code>xtab</code> containing the observed outputs <code class="reqn">y_1,\ldots,y_n</code>.</p>
</td></tr>
<tr><td><code id="quad_spline_kn_+3A_method">method</code></td>
<td>
<p>a character equal to &quot;u&quot; (unconstrained estimator), &quot;m&quot; (under the monotonicity constraint) or &quot;mc&quot; (under simultaneous monotonicity and concavity constraints).</p>
</td></tr>
<tr><td><code id="quad_spline_kn_+3A_krange">krange</code></td>
<td>
<p>a vector of integers specifying the range in which the optimal number of inter-knot segments is to be selected.</p>
</td></tr>
<tr><td><code id="quad_spline_kn_+3A_type">type</code></td>
<td>
<p>a character equal to &quot;AIC&quot; or &quot;BIC&quot;.</p>
</td></tr>
<tr><td><code id="quad_spline_kn_+3A_control">control</code></td>
<td>
<p>a list of parameters to the GLPK solver. See *Details* of help(Rglpk_solve_LP).</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>For the implementation of the unconstrained quadratic spline smoother 
<code class="reqn">\tilde\varphi_n</code> (see <code><a href="#topic+quad_spline_est">quad_spline_est</a></code>), based on the knot mesh 
<code class="reqn">\{t_j = x_{[j n/k_n]}: j=1,\ldots,k_n-1\}</code>,
the user has to employ the option <code>method="u"</code>. 
Since the number <code class="reqn">k_n</code> determines the complexity of the spline approximation, 
its choice may be viewed as model selection via the minimization of the following Akaike (option <code>type="AIC"</code>) 
or Bayesian (option <code>type="BIC"</code>) information criteria:
</p>
<p style="text-align: center;"><code class="reqn">
A\tilde{I}C(k) = \log \left( \sum_{i=1}^{n} (\tilde \varphi_n(x_i)- y_i) \right) + (k+2)/n,</code>
</p>

<p style="text-align: center;"><code class="reqn">B\tilde{I}C(k) = \log \left( \sum_{i=1}^{n} (\tilde \varphi_n(x_i) - y_i) \right) + \log n \cdot (k+2)/2n.</code>
</p>

<p>For the implementation of the monotone (option <code>method="m"</code>) quadratic spline smoother <code class="reqn">\hat\varphi_n</code> (see <code><a href="#topic+quad_spline_est">quad_spline_est</a></code>),
the authors first suggest using the set of knots <code class="reqn">\{ t_j = {\mathcal{X}_{[j \mathcal{N}/k_n]}},~j=1,\ldots,k_n-1 \}</code> 
among the FDH points <code class="reqn">(\mathcal{X}_{\ell},\mathcal{Y}_{\ell})</code>, <code class="reqn">\ell=1,\ldots,\mathcal{N}</code>
(function <code><a href="#topic+quad_spline_est">quad_spline_est</a></code>).
Then, they propose to choose <code class="reqn">k_n</code> by minimizing the following AIC (option <code>type="AIC"</code>) or BIC (option <code>type="BIC"</code>) information criteria:
</p>
<p style="text-align: center;"><code class="reqn">
A\hat{I}C(k) = \log \left( \sum_{i=1}^{n} (\hat \varphi_n(x_i)- y_i) \right) + (k+2)/n,</code>
</p>

<p style="text-align: center;"><code class="reqn">B\hat{I}C(k) = \log \left( \sum_{i=1}^{n} (\hat \varphi_n(x_i) - y_i) \right) + \log n \cdot (k+2)/2n.</code>
</p>

<p>A small number of knots is typically needed as elucidated by the asymptotic theory. 
</p>
<p>For the implementation of the monotone and concave (option <code>method="mc"</code>) spline estimator <code class="reqn">\hat\varphi^{\star}_n</code>, 
just apply the same scheme as above by replacing the FDH points <code class="reqn">(\mathcal{X}_{\ell},\mathcal{Y}_{\ell})</code>  
with the DEA points <code class="reqn">(\mathcal{X}^*_{\ell},\mathcal{Y}^*_{\ell})</code>  (see <code><a href="#topic+dea_est">dea_est</a></code>). 
</p>


<h3>Value</h3>

<p>Returns an integer.
</p>


<h3>Author(s)</h3>

<p>Hohsuk Noh.
</p>


<h3>References</h3>

<p>Akaike, H. (1973).  Information theory and an extension of the maximum likelihood principle, in <em>Second International Symposium of Information Theory</em>, eds. B. N. Petrov and F. Csaki, Budapest: Akademia Kiado, 267&ndash;281.  
</p>
<p>Daouia, A., Noh, H. and Park, B.U. (2016). Data Envelope fitting with constrained polynomial splines. <em>Journal of the Royal Statistical Society: Series B</em>, <b>78</b>(1), 3-30. doi:10.1111/rssb.12098.
</p>
<p>Schwartz, G. (1978). Estimating the dimension of a model, <em>Annals of Statistics</em>, 6, 461&ndash;464.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+quad_spline_est">quad_spline_est</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("green")
## Not run: 
# BIC criteria for choosing the optimal number of 
# inter-knot segments in:   
# a. Unconstrained quadratic spline fits
(kn.bic.green.u &lt;- quad_spline_kn(log(green$COST), 
 log(green$OUTPUT), method = "u", type = "BIC"))
# b. Monotone quadratic spline smoother
(kn.bic.green.m &lt;- quad_spline_kn(log(green$COST), 
 log(green$OUTPUT), method = "m", type = "BIC"))  
# c. Monotone and concave quadratic spline smoother
(kn.bic.green.mc&lt;-quad_spline_kn(log(green$COST), 
 log(green$OUTPUT), method = "mc", type = "BIC"))

## End(Not run)
</code></pre>

<hr>
<h2 id='records'>Annual sport records 
</h2><span id='topic+records'></span>

<h3>Description</h3>

<p>The dataset <code>records</code> is concerned with the yearly best men's outdoor 1500m times starting from 1966. Following Jirak, Meister and Reiss (2014), the lower boundary can be 
interpreted as the best possible time for a given year. This boundary is not believed to be shape constrained and can be estimated by any unconstrained shape nonparametric method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(records)</code></pre>


<h3>Format</h3>

<p>A data frame with 46 observations on the following 2 variables.
</p>

<dl>
<dt><code>year</code></dt><dd><p>year.</p>
</dd>
<dt><code>result</code></dt><dd><p>1500m record in seconds.</p>
</dd>
</dl>



<h3>References</h3>

<p>Jirak, M.,  Meister, A.  and M. Reiss (2014), Optimal adaptive estimation in nonparametric regression with one-sided errors. <em>Annals of Statistics</em>, 42, 1970&ndash;2002.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("records")
</code></pre>

<hr>
<h2 id='rho_momt_pick'>
Optimal rho for moment and Pickands frontier estimator
</h2><span id='topic+rho_momt_pick'></span>

<h3>Description</h3>

<p>This function gives the optimal rho involved in the moment and Pickands estimators of Daouia, Florens and Simar (2010).    
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rho_momt_pick(xtab, ytab, x, method="moment", lrho=1, urho=Inf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rho_momt_pick_+3A_xtab">xtab</code></td>
<td>
<p>a numeric vector containing the observed inputs  <code class="reqn">x_1,\ldots,x_n</code>.</p>
</td></tr>
<tr><td><code id="rho_momt_pick_+3A_ytab">ytab</code></td>
<td>
<p>a numeric vector of the same length as <code>xtab</code> containing the observed outputs <code class="reqn">y_1,\ldots,y_n</code>.</p>
</td></tr>
<tr><td><code id="rho_momt_pick_+3A_x">x</code></td>
<td>
<p>a numeric vector of evaluation points in which the estimator is to be computed.</p>
</td></tr>
<tr><td><code id="rho_momt_pick_+3A_method">method</code></td>
<td>
<p>a character equal to &quot;moment&quot; or &quot;pickands&quot;.</p>
</td></tr>
<tr><td><code id="rho_momt_pick_+3A_lrho">lrho</code></td>
<td>
<p>a scalar, minimum rho threshold value.</p>
</td></tr>  
<tr><td><code id="rho_momt_pick_+3A_urho">urho</code></td>
<td>
<p>a scalar, maximum rho threshold value.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes the moment and Pickands estimates of the extreme-value index 
<code class="reqn">\rho_x</code> involved in the frontier estimators <code class="reqn">\tilde\varphi_{momt}(x)</code>  [see <code><a href="#topic+dfs_momt">dfs_momt</a></code>] and 
<code class="reqn">\hat\varphi_{pick}(x)</code> [see <code><a href="#topic+dfs_pick">dfs_pick</a></code>].
In case <code>method="moment"</code>, the estimator of <code class="reqn">\rho_x</code> defined as
</p>
<p style="text-align: center;"><code class="reqn">\tilde{\rho}_x = -\left(M^{(1)}_n + 1 -\frac{1}{2}\left[1-(M^{(1)}_n)^2/M^{(2)}_n\right]^{-1}\right)^{-1}</code>
</p>

<p>is based on the moments <code class="reqn">M^{(j)}_n = (1/k)\sum_{i=0}^{k-1}\left(\log  z^x_{(n-i)}- \log   z^x_{(n-k)}\right)^j</code>   
for <code class="reqn">j=1,2</code>, with <code class="reqn">z^{x}_{(1)}\leq \cdots\leq  z^{x}_{(n)}</code> are the ascending order statistics  
corresponding to the transformed sample <code class="reqn">\{z^{x}_i := y_i\mathbf{1}_{\{x_i\le x\}}, \,i=1,\cdots,n\}</code>
In case <code>method="pickands"</code>, the estimator of <code class="reqn">\rho_x</code> is given by
</p>
<p style="text-align: center;"><code class="reqn">\hat{\rho}_x = - \log 2/\log\{(z^x_{(n-k+1)} - z^x_{(n-2k+1)})/(z^x_{(n-2k+1)} - z^x_{(n-4k+1)})\}.</code>
</p>

<p>To select the threshold <code class="reqn">k=k_n(x)</code> in <code class="reqn">\tilde{\rho}_x</code> and <code class="reqn">\hat{\rho}_x</code>, Daouia et al. (2010) have suggested to use the following data driven method for each 
<code class="reqn">x</code>: They first select a grid of values for <code class="reqn">k=k_n(x)</code>.
For the Pickands estimator <code class="reqn">\hat{\rho}_x</code>, they choose <code class="reqn">k_n(x) = [N_x /4] - k + 1</code>, where <code class="reqn">k</code> is an integer varying between 1 
and the integer part <code class="reqn">[N_x/4]</code> of <code class="reqn">N_x/4</code>, with <code class="reqn">N_x=\sum_{i=1}^n1_{\{x_i\le x\}}</code>.
For the moment estimator <code class="reqn">\tilde{\rho}_x</code>, they choose <code class="reqn">k_n(x) = N_x - k</code>, where <code class="reqn">k</code> is an integer varying between 1 and <code class="reqn">N_x -1</code>.
Then, they evaluate the estimator <code class="reqn">\hat{\rho}_x(k)</code>  (respectively, <code class="reqn">\tilde{\rho}_x(k)</code>) and select the k where the variation of the results is the smallest. 
They achieve this by computing the standard deviation of <code class="reqn">\hat{\rho}_x(k)</code> (respectively, <code class="reqn">\tilde{\rho}_x(k)</code>) over a &ldquo;window&rdquo; of 
<code class="reqn">\max([\sqrt{N_x /4}],3)</code> (respectively, <code class="reqn">\max([\sqrt{N_x-1}],3)</code>) 
successive values of <code class="reqn">k</code>. The value of <code class="reqn">k</code> where this standard deviation is minimal defines the value of <code class="reqn">k_n(x)</code>.
The user can also appreciably improve the estimation of <code class="reqn">\rho_x</code> and <code class="reqn">\varphi(x)</code> itself by tuning the choice of the lower limit (default option <code>lrho=1</code>) 
and upper limit (default option <code>urho=Inf</code>).
</p>


<h3>Value</h3>

<p>Returns a numeric vector with the same length as <code>x</code>.
</p>


<h3>Note</h3>

<p>In order to choose a raisonable estimate <code class="reqn">\tilde\rho_x=\tilde\rho_x(k)</code> and 
<code class="reqn">\hat\rho_x=\hat\rho_x(k)</code> of the extreme-value index <code class="reqn">\rho_x</code>, 
for each fixed <code class="reqn">x</code>, one can construct the plot of the estimator of interest, consisting of the points <code class="reqn">\{(k,\tilde\rho_x(k))\}_k</code> or
<code class="reqn">\{(k,\hat\rho_x(k))\}_k</code>, and select a value of the estimate at which the obtained graph looks stable. This is this kind of idea
which guides the propoed automatic data-driven rule for a chosen grid of values of <code class="reqn">x</code>. The main difficulty with such a method is that the plots of
<code class="reqn">\tilde\rho_x(k)</code> or <code class="reqn">\hat\rho_x(k)</code> as functions of <code class="reqn">k</code>, for each <code class="reqn">x</code>, may be so unstable that reasonable values of
<code class="reqn">k</code> [which would correspond to the true value of <code class="reqn">\rho_x</code>] may be hidden in the graphs. In results, the obtained extreme-value index estimator and the frontier estimator itself may 
exhibits considerable volatility as functions of <code class="reqn">x</code>. The user can appreciably improve the estimation of <code class="reqn">\rho_x</code> and <code class="reqn">\varphi(x)</code> 
by tuning the choice of the lower limit (default option <code>lrho=1</code>) and upper limit (default option <code>urho=Inf</code>). 
</p>


<h3>Author(s)</h3>

<p>Abdelaati Daouia and Thibault Laurent (codes converted from Matlab's Leopold Simar code). 
</p>


<h3>References</h3>

<p>Daouia, A., Florens, J.P. and Simar, L. (2010). Frontier Estimation and Extreme Value Theory, <em>Bernoulli</em>, 16, 1039-1063.
</p>
<p>Dekkers, A.L.M., Einmahl, J.H.J. and L. de Haan (1989), A moment estimator for the index of an extreme-value distribution, <em>The Annals of Statistics</em>, 17(4), 1833-1855.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dfs_momt">dfs_momt</a></code>, <code><a href="#topic+dfs_pick">dfs_pick</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("post")
x.post&lt;- seq(post$xinput[100],max(post$xinput), 
 length.out=100) 
## Not run: 
# a. Optimal rho for Pickands frontier estimator
rho_pick&lt;-rho_momt_pick(post$xinput, post$yprod, 
 x.post, method="pickands")
# b. Optimal rho for moment frontier estimator
rho_momt&lt;-rho_momt_pick(post$xinput, post$yprod, 
 x.post, method="moment")

## End(Not run)
</code></pre>

<hr>
<h2 id='rho_pwm'>
Probability-weighted moment frontier estimator
</h2><span id='topic+rho_pwm'></span>

<h3>Description</h3>

<p>This function is an implementation of the Probability-weighted moment frontier estimator developed by Daouia, Florens and Simar (2012).    
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rho_pwm(xtab, ytab, x, a=2, lrho=1, urho=Inf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rho_pwm_+3A_xtab">xtab</code></td>
<td>
<p>a numeric vector containing the observed inputs  <code class="reqn">x_1,\ldots,x_n</code>.</p>
</td></tr>
<tr><td><code id="rho_pwm_+3A_ytab">ytab</code></td>
<td>
<p>a numeric vector of the same length as <code>xtab</code> containing the observed outputs <code class="reqn">y_1,\ldots,y_n</code>.</p>
</td></tr>
<tr><td><code id="rho_pwm_+3A_x">x</code></td>
<td>
<p>a numeric vector of evaluation points in which the estimator is to be computed.</p>
</td></tr>
<tr><td><code id="rho_pwm_+3A_a">a</code></td>
<td>
<p>a smoothing parameter (integer) larger than or equal to 2.</p>
</td></tr>
<tr><td><code id="rho_pwm_+3A_lrho">lrho</code></td>
<td>
<p>a scalar, minimum rho threshold value.</p>
</td></tr>  
<tr><td><code id="rho_pwm_+3A_urho">urho</code></td>
<td>
<p>a scalar, maximum rho threshold value.</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>The function computes the probability-weighted moment (PWM) estimator <code class="reqn">\bar\rho_x</code> utilized in the frontier estimate 
<code class="reqn">\tilde\varphi_{pwm}(x)</code>[see <code><a href="#topic+dfs_pwm">dfs_pwm</a></code>]. 
This estimator depends on the smoothing parameters <code class="reqn">a</code> and <code class="reqn">m</code>. A simple selection rule of thumb that Daouia et al. (2012) have employed is 
<code class="reqn">a=2</code> 
[default option in the 4th argument of the function] 
and <code class="reqn">m=coefm \times N^{1/3}_x</code>, where <code class="reqn">N_x=\sum_{i=1}^n1_{\{x_i\le x\}}</code> 
and the integer <code>coefm</code> is to be tuned by the user. 
To choose this parameter in an optimal way for each <code class="reqn">x</code>, we adapt the automated threshold selection method of Daouia et al. (2010) as follows:  
We first evaluate the estimator <code class="reqn">\bar\rho_x</code> over a grid of values of <code>coefm</code> given by 
<code class="reqn">c = 1, \cdots, 150</code>. 
Then, we select the <code class="reqn">c</code> where the variation of the results is the smallest. This is achieved by computing the standard deviation of  the estimates <code class="reqn">\bar\rho_x</code> over a &ldquo;window&rdquo; of 
<code class="reqn">\max([\sqrt{150}],3)</code> successive values of <code class="reqn">c</code>. The value of <code class="reqn">c</code> where this standard deviation is minimal defines the value of <code>coefm</code>.
The user can also appreciably improve the estimation of the extreme-value index <code class="reqn">\rho_x</code> and the frontier function <code class="reqn">\varphi_x</code> itself by tuning the choice of the lower limit 
(default option <code>lrho=1</code>) and upper limit (default option <code>urho=Inf</code>).
</p>


<h3>Value</h3>

<p>Returns a numeric vector with the same length as <code>x</code>.
</p>


<h3>Note</h3>

<p>The computational burden here is demanding, so be forewarned. 
</p>


<h3>Author(s)</h3>

<p>Abdelaati Daouia and Thibault Laurent. 
</p>


<h3>References</h3>

<p>Daouia, A., Florens, J.-P. and Simar, L. (2010). Frontier estimation and extreme value theory. <em>Bernoulli</em>, 16, 1039-1063.
</p>
<p>Daouia, A., Florens, J.-P. and Simar, L. (2012). Regularization of Nonparametric Frontier Estimators. <em>Journal of Econometrics</em>, 168, 285-299.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dfs_pwm">dfs_pwm</a></code>, <code><a href="#topic+mopt_pwm">mopt_pwm</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("post")
x.post&lt;- seq(post$xinput[100],max(post$xinput), 
 length.out=100) 
## Not run: 
# When rho[x] is unknown and dependent of x, 
# its estimate hat(rho[x]) is obtained via:
rho_pwm &lt;- rho_pwm(post$xinput, post$yprod, x.post,  a=20)

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
