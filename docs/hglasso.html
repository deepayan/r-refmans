<!DOCTYPE html><html lang="en"><head><title>Help for package hglasso</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {hglasso}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#hglasso-package'>
<p>Fit the hub graphical lasso, hub covariance graph, and hub binary network</p></a></li>
<li><a href='#binaryMCMC'>
<p>Generate samples using Gibbs sampling for binary network specified by the parameter Theta</p></a></li>
<li><a href='#hbn'>
<p>Hub binary network</p></a></li>
<li><a href='#hcov'>
<p>Hub covariance graph</p></a></li>
<li><a href='#hglasso'>
<p>Hub graphical lasso</p></a></li>
<li><a href='#hglassoBIC'>
<p>BIC-type criterion for <code>hglasso</code></p></a></li>
<li><a href='#HubNetwork'>
<p>Hub network generation</p></a></li>
<li><a href='#image.hglasso'>
<p>Image plot of an object of class <code>hglasso</code>, <code>hcov</code>, or <code>hbn</code></p></a></li>
<li><a href='#plot.hglasso'>
<p>Plot an object of class <code>hglasso</code>, <code>hcov</code>, or <code>hbn</code></p></a></li>
<li><a href='#summary.hglasso'>
<p>Plot an object of class <code>hglasso</code>, <code>hcov</code>, or <code>hbn</code></p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Learning Graphical Models with Hubs</td>
</tr>
<tr>
<td>Version:</td>
<td>1.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-05-13</td>
</tr>
<tr>
<td>Author:</td>
<td>Kean Ming Tan</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kean Ming Tan &lt;keanming@umich.edu&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>glasso, mvtnorm, igraph</td>
</tr>
<tr>
<td>Imports:</td>
<td>fields</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements the hub graphical lasso and hub covariance graph proposal by Tan, KM., London, P., Mohan, K., Lee, S-I., Fazel, M., and Witten, D. (2014). Learning graphical models with hubs. Journal of Machine Learning Research 15(Oct):3297-3331.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-05-13 04:58:47 UTC; keanmingtan</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-05-13 08:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='hglasso-package'>
Fit the hub graphical lasso, hub covariance graph, and hub binary network
</h2><span id='topic+hglasso-package'></span>

<h3>Description</h3>

<p>This package is called hglasso, for &quot;hub graphical lasso&quot;.  It implements three methods:hub graphical lasso, hub covariance graph, and hub binary network.  All are described in the paper &quot;Learning graphical models with hubs&quot;, by Tan et al. (2014).
</p>
<p>The main functions are as follows:
(1) hglasso
(2) hcov
(3) hbn
</p>
<p>The first function, hglasso, performs hub graphical lasso.  The second function, hcov, performs hub covariance graph estimation.  The third function, hbn, performs hub binary network estimation.   
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> hglasso</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.2</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2014-08-09</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;=2.0) </td>
</tr>
<tr>
 <td style="text-align: left;">
LazyLoad: </td><td style="text-align: left;"> yes </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>The package includes the following functinos:
</p>

<table>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+hglasso">hglasso</a></code>: </td><td style="text-align: left;"> Performs hub graphical lasso</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+hcov">hcov</a></code>:</td><td style="text-align: left;"> Performs hub covariance graph estimation </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+hbn">hbn</a></code>:</td><td style="text-align: left;"> Performs hub binary network estimation </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+HubNetwork">HubNetwork</a></code>:</td><td style="text-align: left;"> Generates inverse covariance matrix or covariance matrix with hubs </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+binaryMCMC">binaryMCMC</a></code>:</td><td style="text-align: left;"> Generates samples for binary Ising model via Gibbs sampling </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+image.hglasso">image.hglasso</a></code>: </td><td style="text-align: left;"> Creates image plot of the matrix V and Z</td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+plot.hglasso">plot.hglasso</a></code>: </td><td style="text-align: left;"> Creates a graphical representation of the estimated matrix Theta </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+summary.hglasso">summary.hglasso</a></code>: </td><td style="text-align: left;"> Provides summary for the matrix Theta, Z, and V  </td>
</tr>
<tr>
 <td style="text-align: left;">
<code><a href="#topic+hglassoBIC">hglassoBIC</a></code>: </td><td style="text-align: left;"> Calculate BIC-type criterion for <code><a href="#topic+hglasso">hglasso</a></code> </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Kean Ming Tan and Karthik Mohan
</p>
<p>Karthik Mohan implemented the Barzilai-Borwein method for <code><a href="#topic+hbn">hbn</a></code>
</p>
<p>Maintainer: Kean Ming Tan &lt;keanming@uw.edu&gt;
</p>


<h3>References</h3>

<p>Tan, KM., London, P., Mohan, K., Lee, S-I., Fazel, M., and Witten, D. (2014). Learning graphical models with hubs. Journal of Machine Learning Research 15(Oct):3297-3331.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hglasso">hglasso</a></code>
<code><a href="#topic+hcov">hcov</a></code>
<code><a href="#topic+hbn">hbn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##############################################
# Example from Figure 1 in the manuscript
# A toy example to illustrate the results from 
# Hub Graphical Lasso
##############################################
#library(mvtnorm)
#set.seed(1)
#n=100
#p=100

# A network with 4 hubs
#network&lt;-HubNetwork(p,0.99,4,0.1)
#Theta &lt;- network$Theta
#truehub &lt;- network$hubcol
# The four hub nodes have indices 14, 42, 45, 78
#print(truehub)

# Generate data matrix x
#x &lt;- rmvnorm(n,rep(0,p),solve(Theta))
#x &lt;- scale(x)

# Run Hub Graphical Lasso to estimate the inverse covariance matrix
# res1&lt;-hglasso(cov(x),0.3,0.3,1.5)

# print out a summary of the object hglasso
#summary(res1)
# we see that the estimated hub nodes have indices 14, 42, 45, 78
# We successfully recover the 4 hub nodes

# Plot the matrices V and Z 
#image(res1)
#dev.off()
# Plot a graphical representation of the estimated inverse
# covariance matrix --- conditional independence graph
#plot(res1,main="Conditional Independence Graph")

</code></pre>

<hr>
<h2 id='binaryMCMC'>
Generate samples using Gibbs sampling for binary network specified by the parameter Theta 
</h2><span id='topic+binaryMCMC'></span>

<h3>Description</h3>

<p>Sampling from the binary Ising model using Gibbs sampling.  This function is not efficient and is only intended to be used in the examples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>binaryMCMC(n, Theta, burnin, skip,trace=FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="binaryMCMC_+3A_n">n</code></td>
<td>

<p>The number of samples.
</p>
</td></tr>
<tr><td><code id="binaryMCMC_+3A_theta">Theta</code></td>
<td>

<p>A symmetric parameter matrix for the model from which the data is being generated.</p>
</td></tr>
<tr><td><code id="binaryMCMC_+3A_burnin">burnin</code></td>
<td>

<p>The number of samples to discard as burn in.
</p>
</td></tr>
<tr><td><code id="binaryMCMC_+3A_skip">skip</code></td>
<td>

<p>The number of samples to discard in-between returned samples.
</p>
</td></tr>
<tr><td><code id="binaryMCMC_+3A_trace">trace</code></td>
<td>

<p>Default value of trace=FALSE. If trace=TRUE, the progress of Gibbs sampling is printed when each observation is sampled.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>X</code></td>
<td>
<p>An n x p matrix of samples generated from the binary network specified by Theta.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kean Ming Tan</p>


<h3>References</h3>

<p>Tan et al. (2014). Learning graphical models with hubs. To appear in Journal of Machine Learning Research. arXiv.org/pdf/1402.7349.pdf.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+HubNetwork">HubNetwork</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># generate Theta that specified the structure of a binary Ising model with p=10 variables and 2 hubs
#p&lt;-10
#n&lt;-50
#Theta &lt;- HubNetwork(p,0.95,2,0.3,type="binary")$Theta

# generate samples using Gibbs sampling
#X &lt;- binaryMCMC(n,Theta,burnin=1000,skip=500)

</code></pre>

<hr>
<h2 id='hbn'>
Hub binary network</h2><span id='topic+hbn'></span>

<h3>Description</h3>

<p>Estimates a binary network with hub nodes using a Lasso penalty and a sparse group Lasso penalty.  The estimated Theta matrix can be decomposed as Theta = Z + V + t(V), where Z is a sparse matrix and V is a matrix that contains hub nodes.  The details are given in Tan et al. (2014).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hbn(X, lambda1, lambda2=100000, lambda3=100000, convergence = 1e-8
, maxiter = 1000, start = "cold", var.init = NULL, trace=FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hbn_+3A_x">X</code></td>
<td>

<p>An n by p data matrix.  Cannot contain missing values.       
</p>
</td></tr>
<tr><td><code id="hbn_+3A_lambda1">lambda1</code></td>
<td>

<p>Non-negative regularization parameter for lasso on the matrix Z.  lambda=0 means no regularization.
</p>
</td></tr>
<tr><td><code id="hbn_+3A_lambda2">lambda2</code></td>
<td>

<p>Non-negative regularization parameter for lasso on the matrix V.  lambda2=0 means no regularization.  The default value is lambda2=100000, encouraging V to be a zero matrix.
</p>
</td></tr>
<tr><td><code id="hbn_+3A_lambda3">lambda3</code></td>
<td>

<p>Non-negative regularization parameter for group lasso on the matrix V.  lambda3=0 means no regularization.  The default value is lambda3=100000, encouraging V to be a zero matrix.
</p>
</td></tr>
<tr><td><code id="hbn_+3A_convergence">convergence</code></td>
<td>

<p>Threshold for convergence.  Devault value is 1e-8. 
</p>
</td></tr>
<tr><td><code id="hbn_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations of ADMM algorithm.  Default is 1000 iterations.
</p>
</td></tr>
<tr><td><code id="hbn_+3A_start">start</code></td>
<td>

<p>Type of start.  cold start is the default.  Using warm start, one can provide starting values for the parameters using object from hbn.
</p>
</td></tr>
<tr><td><code id="hbn_+3A_var.init">var.init</code></td>
<td>

<p>Object from hbn that provides starting values for all the parameters when start=&quot;warm&quot; is specified.
</p>
</td></tr>
<tr><td><code id="hbn_+3A_trace">trace</code></td>
<td>

<p>Default value of trace=FALSE.  If trace=TRUE, every 10 iterations of the ADMM algorithm is printed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This implements hub binary network using ADMM algorithm (see Algorithm 1 and Section 5) in Tan et al. (2014).  The estimated Theta matrix can be decomposed into Z + V + t(V): Z is a sparse matrix and V is a matrix that contains dense columns, each column corresponding to a hub node.  
</p>
<p>The default value of lambda2=100000 and lambda3=100000 will yield the sparse binary network model estimate as in Hofling and Tibshirani  (2009) 'Estimation of sparse binary pairwise Markov networks using pseudo-likelihoods'.   
</p>
<p>The tuning parameters lambda1 determines the sparsity of the matrix Z, lambda2 determines the sparsity of the selected hub nodes, and lambda3 determines the selection of hub nodes.  
</p>
<p>Within each iteration of the ADMM algorithm, we need to perform an iterative procedure to obtain an update for the matrix Theta since there is no closed form solution for Theta.  The Barzilai-Borwein method is used for this purpose (Barzilai and Borwein, 1988).  For details, see Algorithm 2 in Appendix F in Tan et al. (2014).    
</p>
<p>Note: we recommend using this function for moderate size network. For instance, network with 50-100 variables.  
</p>


<h3>Value</h3>

<p>an object of class hbn.  
</p>
<p>Among some internal variables, this object includes the elements 
</p>
<table role = "presentation">
<tr><td><code>Theta</code></td>
<td>
<p>Theta is the estimated inverse covariance matrix. Note that Theta = Z + V + t(V).</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>V is the estimated matrix that contains hub nodes used to compute Theta.</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>
<p>Z is the estimated sparse matrix used to compute Theta.</p>
</td></tr>
<tr><td><code>hubind</code></td>
<td>
<p>Indices for features that are estimated to be hub nodes</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kean Ming Tan and Karthik Mohan
</p>


<h3>References</h3>

<p>Tan et al. (2014). Learning graphical models with hubs. To appear in Journal of Machine Learning Research. arXiv.org/pdf/1402.7349.pdf.
</p>
<p>Hofling, H. and Tibshirani, R. (2009). Estimation of sparse binary pairwise Markov networks using pseudo-likelihoods. Journal of Machine Learning Research, 10:883-906. 
</p>
<p>Barzilai, J. and Borwein, J. (1988).  Two-point step size gradient methods. IMA Journal of Numerical Analysis, 8:141-148.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+image.hglasso">image.hglasso</a></code>
<code><a href="#topic+plot.hglasso">plot.hglasso</a></code>
<code><a href="#topic+summary.hglasso">summary.hglasso</a></code>
<code><a href="#topic+binaryMCMC">binaryMCMC</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##############################################
# An implementation of Hub Binary Network
##############################################
#set.seed(1000)
#n=50
#p=5

# A network with 2 hubs
#network&lt;-HubNetwork(p,0.95,2,0.1,type="binary")
#Theta &lt;- network$Theta
#truehub &lt;- network$hubcol
# The four hub nodes have indices 4,5
#print(truehub)

# Generate data matrix x
#X &lt;- binaryMCMC(n,Theta,burnin=500,skip=100)

# Run Hub Binary Network to estimate Theta
#res1 &lt;- hbn(X,2,1,3,trace=TRUE)

# print out a summary of the object hbn
#summary(res1)

# We see that the estimated hub nodes have indices 1,5
# We successfully recover the hub nodes

# Plot the resulting network
# plot(res1) 
</code></pre>

<hr>
<h2 id='hcov'>
Hub covariance graph
</h2><span id='topic+hcov'></span>

<h3>Description</h3>

<p>Estimates a sparse covariance matrix with hub nodes using a Lasso penalty and a sparse group Lasso penalty.  The estimated covariance matrix Sigma can be decomposed as Sigma = Z + V + t(V).  The details are given in Section 4 in Tan et al. (2014).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hcov(S, lambda1, lambda2=100000, lambda3=100000, convergence = 1e-10, 
maxiter = 1000, start = "cold", var.init = NULL,trace=FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hcov_+3A_s">S</code></td>
<td>

<p>A p by p correlation/covariance matrix.  Cannot contain missing values.       
</p>
</td></tr>
<tr><td><code id="hcov_+3A_lambda1">lambda1</code></td>
<td>

<p>Non-negative regularization parameter for lasso on the matrix Z.  lambda=0 means no regularization.
</p>
</td></tr>
<tr><td><code id="hcov_+3A_lambda2">lambda2</code></td>
<td>

<p>Non-negative regularization parameter for lasso on the matrix V.  lambda2=0 means no regularization.  The default value is lambda2=100000, encouraging V to be a zero matrix.
</p>
</td></tr>
<tr><td><code id="hcov_+3A_lambda3">lambda3</code></td>
<td>

<p>Non-negative regularization parameter for group lasso on the matrix V.  lambda3=0 means no regularization.  The default value is lambda3=100000, encouraging V to be a zero matrix.
</p>
</td></tr>
<tr><td><code id="hcov_+3A_convergence">convergence</code></td>
<td>

<p>Threshold for convergence.  Devault value is 1e-10. 
</p>
</td></tr>
<tr><td><code id="hcov_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations of ADMM algorithm.  Default is 1000 iterations.
</p>
</td></tr>
<tr><td><code id="hcov_+3A_start">start</code></td>
<td>

<p>Type of start.  cold start is the default.  Using warm start, one can provide starting values for the parameters using object from hcov.
</p>
</td></tr>
<tr><td><code id="hcov_+3A_var.init">var.init</code></td>
<td>

<p>Object from hcov that provides starting values for all the parameters when start=&quot;warm&quot; is specified.
</p>
</td></tr>
<tr><td><code id="hcov_+3A_trace">trace</code></td>
<td>

<p>Default value of trace=FALSE.  If trace=TRUE, every 10 iterations of the ADMM algorithm is printed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This implements hub covariance graph estimation procedure using ADMM algorithm described in Section 4 in Tan et al. (2014), which estimates a sparse covariance matrix with hub nodes.  The estimated covariance matrix can be decomposed into Z + V + t(V): Z is a sparse matrix and V is a matrix that contains dense columns, each column corresponding to a hub node.  For the positive definite constraint Sigma &gt;= epsilon*I that appears in the optimization problem, we choose epsilon to be 0.001 by default.  
</p>
<p>The default value of lambda2=100000 and lambda3=100000 will yield  the estimator proposed by Xue et al. (2012).
</p>
<p>Note that tuning parameters lambda1 determines the sparsity of the matrix Z, lambda2 determines the sparsity of the selected hub nodes, and lambda3 determines the selection of hub nodes.  
</p>


<h3>Value</h3>

<p>an object of class hcov.
</p>
<p>Among some internal variables, this object includes the elements
</p>
<table role = "presentation">
<tr><td><code>Sigma</code></td>
<td>
<p>Sigma is the estimated covariance matrix. Note that Sigma = Z + V + t(V).</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>V is the estimated matrix that contain hub nodes used to compute Sigma.</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>
<p>Z is the estimated sparse matrix used to compute Sigma.</p>
</td></tr>
<tr><td><code>objective</code></td>
<td>
<p>Objective is the minimized objective value of the loss-function considered in Section 4 of Tan et al. (2014).</p>
</td></tr>
<tr><td><code>iteration</code></td>
<td>
<p>The number of iterations of the ADMM algorithm until convergence.</p>
</td></tr>
<tr><td><code>hubind</code></td>
<td>
<p>Indices for features that are estimated to be hub nodes</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kean Ming Tan
</p>


<h3>References</h3>

<p>Tan et al. (2014). Learning graphical models with hubs. To appear in Journal of Machine Learning Research.  arXiv.org/pdf/1402.7349.pdf.
</p>
<p>Xue et al. (2012).  Positive-definite l1-penalized estimation of large covariance matrices.  Journal of the American Staitstical Association, 107:1480-1491.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+image.hcov">image.hcov</a></code>
<code><a href="#topic+plot.hcov">plot.hcov</a></code>
<code><a href="#topic+summary.hcov">summary.hcov</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#############################################
# Example for estimating covariance matrix
# with hubs
##############################################
library(mvtnorm)
set.seed(1)
n=100
p=100

# a covariance with 4 hubs

network &lt;- HubNetwork(p,0.95,4,0.1,type="covariance")
Sigma &lt;- network$Theta
hubind &lt;- network$hubcol
x &lt;- rmvnorm(n,rep(0,p),Sigma)
x &lt;- scale(x)

# Estimate the covariance matrix
res1&lt;-hcov(cov(x),0.3,0.2,1.2)
summary(res1)
# correctly identified two of the hub nodes

# Plot the matrices V and Z 
image(res1)
dev.off()
# Plot a graphical representation of the estimated covariance matrix --- covariance graph
plot(res1)

# Xue et al cannot identified any hub nodes
res2 &lt;- hcov(cov(x),0.3)
summary(res2)
plot(res2)

</code></pre>

<hr>
<h2 id='hglasso'>
Hub graphical lasso</h2><span id='topic+hglasso'></span>

<h3>Description</h3>

<p>Estimates a sparse inverse covariance matrix with hub nodes using a Lasso penalty and a sparse group Lasso penalty.  The estimated inverse covariance matrix Theta can be decomposed as Theta = Z + V + t(V), where Z is a sparse matrix and V is a matrix that contains hub nodes.  The details are given in Tan et al. (2014).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hglasso(S, lambda1, lambda2=100000, lambda3=100000, convergence = 1e-10
, maxiter = 1000, start = "cold", var.init = NULL, trace=FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hglasso_+3A_s">S</code></td>
<td>

<p>A p by p correlation/covariance matrix.  Cannot contain missing values.       
</p>
</td></tr>
<tr><td><code id="hglasso_+3A_lambda1">lambda1</code></td>
<td>

<p>Non-negative regularization parameter for lasso on the matrix Z.  lambda=0 means no regularization.
</p>
</td></tr>
<tr><td><code id="hglasso_+3A_lambda2">lambda2</code></td>
<td>

<p>Non-negative regularization parameter for lasso on the matrix V.  lambda2=0 means no regularization.  The default value is lambda2=100000, encouraging V to be a zero matrix.
</p>
</td></tr>
<tr><td><code id="hglasso_+3A_lambda3">lambda3</code></td>
<td>

<p>Non-negative regularization parameter for group lasso on the matrix V.  lambda3=0 means no regularization.  The default value is lambda3=100000, encouraging V to be a zero matrix.
</p>
</td></tr>
<tr><td><code id="hglasso_+3A_convergence">convergence</code></td>
<td>

<p>Threshold for convergence.  Devault value is 1e-10. 
</p>
</td></tr>
<tr><td><code id="hglasso_+3A_maxiter">maxiter</code></td>
<td>

<p>Maximum number of iterations of ADMM algorithm.  Default is 1000 iterations.
</p>
</td></tr>
<tr><td><code id="hglasso_+3A_start">start</code></td>
<td>

<p>Type of start.  cold start is the default.  Using warm start, one can provide starting values for the parameters using object from hglasso.
</p>
</td></tr>
<tr><td><code id="hglasso_+3A_var.init">var.init</code></td>
<td>

<p>Object from hglasso that provides starting values for all the parameters when start=&quot;warm&quot; is specified.
</p>
</td></tr>
<tr><td><code id="hglasso_+3A_trace">trace</code></td>
<td>

<p>Default value of trace=FALSE.  If trace=TRUE, every 10 iterations of the ADMM algorithm is printed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This implements hub graphical lasso using ADMM Algorithm (see Algorithm 1) described in Tan et al. (2014), which estimates a sparse inverse covariance matrix with hub nodes.  The estimated inverse covariance matrix can be decomposed into Z + V + t(V): Z is a sparse matrix and V is a matrix that contains dense columns, each column corresponding to a hub node.  
</p>
<p>The default value of lambda2=100000 and lambda3=100000 will yield the graphical lasso estimate as in Friedman et al. (2007) 'Sparse inverse covariance estimation with lasso'.   
</p>
<p>Note that tuning parameters lambda1 determines the sparsity of the matrix Z, lambda2 determines the sparsity of the selected hub nodes, and lambda3 determines the selection of hub nodes.  
</p>
<p>This algorithm uses a block diagonal screening rule to speed up computations considerably.  Details are given in Theorem 1 in Tan et al. (2014) 'Learning graphical models with hubs'.  The idea is as follow: we first check whether the solution to the hglasso problem will be block diagonal, for a given set of tuning parameters, using Theorem 1.  If so, then one can simply apply hglasso to each block separately, leading to massive speed improvements.  Similar idea has been used to obtain a sparse inverse covariance matrix as in Friedman et al. (2007) in the glasso package.
</p>


<h3>Value</h3>

<p>an object of class hglasso.  
</p>
<p>Amog some internal variables, this object include the elements 
</p>
<table role = "presentation">
<tr><td><code>Theta</code></td>
<td>
<p>Theta is the estimated inverse covariance matrix. Note that Theta = Z + V + t(V).</p>
</td></tr>
<tr><td><code>V</code></td>
<td>
<p>V is the estimated matrix that contains hub nodes used to compute Theta.</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>
<p>Z is the estimated sparse matrix used to compute Theta.</p>
</td></tr>
<tr><td><code>objective</code></td>
<td>
<p>Objective is the minimized objective value of the loss-function considered in Section 3 of Tan et al. (2014).</p>
</td></tr>
<tr><td><code>hubind</code></td>
<td>
<p>Indices for features that are estimated to be hub nodes</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kean Ming Tan
</p>


<h3>References</h3>

<p>Tan et al. (2014). Learning graphical models with hubs. To appear in Journal of Machine Learning Research. arXiv.org/pdf/1402.7349.pdf.
</p>
<p>Friedman et al. (2007). Sparse inverse covariance estimation with the lasso. Biostatistics, 9(3):432-441.
</p>
<p>Witten et al. (2011). New insights and faster computations for the graphical lasso.  Journal of Computational and Graphical Statistics, 20(4):892-900.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+image.hglasso">image.hglasso</a></code>
<code><a href="#topic+plot.hglasso">plot.hglasso</a></code>
<code><a href="#topic+summary.hglasso">summary.hglasso</a></code>
<code><a href="#topic+hglassoBIC">hglassoBIC</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##############################################
# Example from Figure 1 in the manuscript
# A toy example to illustrate the results from 
# Hub Graphical Lasso
##############################################
library(mvtnorm)
library(glasso)
set.seed(1)
n=100
p=100

# A network with 4 hubs
network&lt;-HubNetwork(p,0.99,4,0.1)
Theta &lt;- network$Theta
truehub &lt;- network$hubcol
# The four hub nodes have indices 14, 42, 45, 78
print(truehub)

# Generate data matrix x
x &lt;- rmvnorm(n,rep(0,p),solve(Theta))
x &lt;- scale(x)

# Run Hub Graphical Lasso to estimate the inverse covariance matrix
res1 &lt;- hglasso(cov(x),0.3,0.3,1.5)

# print out a summary of the object hglasso
summary(res1)
# we see that the estimated hub nodes have indices 14, 42, 45, 78
# We successfully recover the 4 hub nodes

# Run hglasso using with and without warm start.  
# system.time(hglasso(cov(x),0.31,0.3,1.5))

# system.time(hglasso(cov(x),0.31,0.3,1.5,start="warm",var.init=res1))

# Run hglasso with larger lambda2, encouraging the hub nodes to be more sparse
res2 &lt;- hglasso(cov(x),0.3,0.35,1.5)

# Run hglasso with lambda2=lambda3=100000, the solution is the 
# same as the graphical lasso solution obtain from glasso package
res3 &lt;- hglasso(cov(x),0.3)
res4 &lt;- glasso(cov(x),0.3,penalize.diagonal=FALSE)
# print the frobenius norm of the difference between the two estimates
print(sum((res3$Theta-res4$wi)^2))

</code></pre>

<hr>
<h2 id='hglassoBIC'>
BIC-type criterion for <code>hglasso</code>
</h2><span id='topic+hglassoBIC'></span>

<h3>Description</h3>

<p>This function calculates the BIC-type criterion for tuning parameter selection for <code><a href="#topic+hglasso">hglasso</a></code> proposed in Section 3.4 in Tan et al. (2014)  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hglassoBIC(x, S, c=0.2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hglassoBIC_+3A_x">x</code></td>
<td>

<p>An object of class <code><a href="#topic+hglasso">hglasso</a></code>.</p>
</td></tr>
<tr><td><code id="hglassoBIC_+3A_s">S</code></td>
<td>

<p>A p by p correlation/covariance matrix.  Cannot contain missing values.</p>
</td></tr>
<tr><td><code id="hglassoBIC_+3A_c">c</code></td>
<td>

<p>A constant between 0 and 1.  When c is small, the BIC-type criterion will favor more hub nodes. The default value is c=0.2.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>BIC</code></td>
<td>
<p>The calculated BIC-type criterion in Section 3.4 in Tan et al. (2014).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kean Ming Tan</p>


<h3>References</h3>

<p>Tan et al. (2014). Learning graphical models with hubs. To appear in Journal of Machine Learning Research. arXiv.org/pdf/1402.7349.pdf.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hglasso">hglasso</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#library(mvtnorm)
#library(glasso)
#set.seed(1)
#n=100
#p=100

# A network with 4 hubs
#network&lt;-HubNetwork(p,0.99,4,0.1)
#Theta &lt;- network$Theta
#truehub &lt;- network$hubcol
# The four hub nodes have indices 14, 42, 45, 78
#print(truehub)

# Generate data matrix x
#x &lt;- rmvnorm(n,rep(0,p),solve(Theta))
#x &lt;- scale(x)
#S &lt;- cov(x)
# Run Hub Graphical Lasso with different tuning parameters
#lambdas2 &lt;- seq(0,0.5,by=0.05)
#BICcriterion &lt;- NULL
#for(lambda2 in lambdas2){
#res1 &lt;- hglasso(S,0.3,lambda2,1.5)
#BICcriterion &lt;- c(BICcriterion,hglassoBIC(res1,S)$BIC)
#}
#lambda2 &lt;- lambdas2[which(BICcriterion==min(BICcriterion))]
</code></pre>

<hr>
<h2 id='HubNetwork'>
Hub network generation
</h2><span id='topic+HubNetwork'></span>

<h3>Description</h3>

<p>Generate an inverse covariance matrix, covariance matrix, or binary network with hub structure
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HubNetwork(p, sparsity, hubnumber, hubsparsity, type = "Gaussian")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="HubNetwork_+3A_p">p</code></td>
<td>

<p>The number of features
</p>
</td></tr>
<tr><td><code id="HubNetwork_+3A_sparsity">sparsity</code></td>
<td>

<p>Sparsity of the network
</p>
</td></tr>
<tr><td><code id="HubNetwork_+3A_hubnumber">hubnumber</code></td>
<td>

<p>The number of hubs in the network
</p>
</td></tr>
<tr><td><code id="HubNetwork_+3A_hubsparsity">hubsparsity</code></td>
<td>

<p>Sparsity level within each hub
</p>
</td></tr>
<tr><td><code id="HubNetwork_+3A_type">type</code></td>
<td>

<p>Type of network. The default value type=&quot;Gaussian&quot; generates an inverse covariance matrix.  type=&quot;covariance&quot; generates a covariance matrix with hubs.  type=&quot;binary&quot; generates a binary network with hubs.  
</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>Theta</code></td>
<td>
<p>Theta is the generated inverse covariance matrix, covariance matrix, or binary network.</p>
</td></tr>
<tr><td><code>hubcol</code></td>
<td>
<p>hubcol contains indices for features that are hubs.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Kean Ming Tan</p>


<h3>References</h3>

<p>Tan et al. (2014). Learning graphical models with hubs. To appear in Journal of Machine Learning Research.  arXiv.org/pdf/1402.7349.pdf.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Generate inverse covariance matrix with 5 hubs
# 30% of the elements within a hub are zero 
# 95% of the elements that are not within hub nodes are zero
p &lt;- 100
Theta &lt;- HubNetwork(p,0.95,5,0.3)$Theta

# Generate covariance matrix with 5 hubs with similar structure
Sigma &lt;- HubNetwork(p,0.95,5,0.3,type="covariance")$Theta

# Generate binary network with 2 hubs with p=10
Theta &lt;- HubNetwork(p=10,0.95,2,0.3,type="binary")$Theta
</code></pre>

<hr>
<h2 id='image.hglasso'>
Image plot of an object of class <code>hglasso</code>, <code>hcov</code>, or <code>hbn</code>
</h2><span id='topic+image.hglasso'></span><span id='topic+image.hcov'></span><span id='topic+image.hbn'></span>

<h3>Description</h3>

<p>This function plots a hglasso or hcov &mdash; the estimated matrix V and Z from <code><a href="#topic+hglasso">hglasso</a></code>, <code><a href="#topic+hcov">hcov</a></code>, or <code><a href="#topic+hbn">hbn</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hglasso'
image(x, ...) 
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="image.hglasso_+3A_x">x</code></td>
<td>

<p>an object of class hglasso, hcov, or hbn.
</p>
</td></tr>
<tr><td><code id="image.hglasso_+3A_...">...</code></td>
<td>

<p>additional parameters to be passed to <code><a href="graphics.html#topic+image">image</a></code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The estimated inverse covariance matrix from <code><a href="#topic+hglasso">hglasso</a></code>,  covariance matrix from <code><a href="#topic+hcov">hcov</a></code>, and estimated binary network <code><a href="#topic+hbn">hbn</a></code> can be decomposed as Z + V + t(V), where V is a matrix that contains hub nodes.  This function creates image plots of Z and V.   
</p>


<h3>Author(s)</h3>

<p>Kean Ming Tan
</p>


<h3>References</h3>

<p>Tan et al. (2014). Learning graphical models with hubs. To appear in Journal of Machine Learning Research. arXiv.org/pdf/1402.7349.pdf.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.hglasso">plot.hglasso</a></code>
<code><a href="#topic+summary.hglasso">summary.hglasso</a></code>
<code><a href="#topic+hglasso">hglasso</a></code>
<code><a href="#topic+hcov">hcov</a></code>
<code><a href="#topic+hbn">hbn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##############################################
# Example from Figure 1 in the manuscript
# A toy example to illustrate the results from 
# Hub Graphical Lasso
##############################################
library(mvtnorm)
set.seed(1)
n=100
p=100

# A network with 4 hubs
Theta&lt;-HubNetwork(p,0.99,4,0.1)$Theta

# Generate data matrix x
x &lt;- rmvnorm(n,rep(0,p),solve(Theta))
x &lt;- scale(x)

# Run Hub Graphical Lasso to estimate the inverse covariance matrix
res1 &lt;- hglasso(cov(x),0.3,0.2,2)

# image plots for the matrix V and Z
image(res1)
dev.off()

</code></pre>

<hr>
<h2 id='plot.hglasso'>
Plot an object of class <code>hglasso</code>, <code>hcov</code>, or <code>hbn</code>
</h2><span id='topic+plot.hglasso'></span><span id='topic+plot.hcov'></span><span id='topic+plot.hbn'></span>

<h3>Description</h3>

<p>This function plots an object hglasso or hcov &mdash; graphical representation of the estimated inverse covariance matrix from <code><a href="#topic+hglasso">hglasso</a></code>, covariance matrix from <code><a href="#topic+hcov">hcov</a></code>, or binary network from <code><a href="#topic+hbn">hbn</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hglasso'
plot(x, layout=NULL,...) 
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.hglasso_+3A_x">x</code></td>
<td>

<p>an object of class <code><a href="#topic+hglasso">hglasso</a></code>, <code><a href="#topic+hcov">hcov</a></code>, or <code><a href="#topic+hbn">hbn</a></code>.
</p>
</td></tr>
<tr><td><code id="plot.hglasso_+3A_layout">layout</code></td>
<td>

<p>the layout of the graph to use.  If not specified, <code>layout.kamada.kawai</code> is used.
</p>
</td></tr>
<tr><td><code id="plot.hglasso_+3A_...">...</code></td>
<td>

<p>additional parameters to be passed to <code>plot.igraph</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function plots a graphical representation of the estimated inverse covariance matrix or covariance matrix.  The hubs are colored in red and has a large vertex size. Features indices for hubs are shown.     
</p>


<h3>Author(s)</h3>

<p>Kean Ming Tan 
</p>


<h3>References</h3>

<p>Tan et al. (2014). Learning graphical models with hubs. To appear in Journal of Machine Learning Research. arXiv.org/pdf/1402.7349.pdf.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+image.hglasso">image.hglasso</a></code>
<code><a href="#topic+summary.hglasso">summary.hglasso</a></code>
<code><a href="#topic+hglasso">hglasso</a></code>
<code><a href="#topic+hcov">hcov</a></code>
<code><a href="#topic+hbn">hbn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##############################################
# Example from Figure 1 in the manuscript
# A toy example to illustrate the results from 
# Hub Graphical Lasso
##############################################
library(mvtnorm)
set.seed(1)
n=100
p=100

# A network with 4 hubs
Theta&lt;-HubNetwork(p,0.99,4,0.1)$Theta

# Generate data matrix x
x &lt;- rmvnorm(n,rep(0,p),solve(Theta))
x &lt;- scale(x)

# Run Hub Graphical Lasso to estimate the inverse covariance matrix
res1 &lt;- hglasso(cov(x),0.3,0.3,1.5)

# Graphical representation of the estimated Theta
plot(res1,main="conditional independence graph")

</code></pre>

<hr>
<h2 id='summary.hglasso'>
Plot an object of class <code>hglasso</code>, <code>hcov</code>, or <code><a href="#topic+hbn">hbn</a></code>
</h2><span id='topic+summary.hglasso'></span><span id='topic+summary.hcov'></span><span id='topic+summary.hbn'></span>

<h3>Description</h3>

<p>This function provides some information for an object <code><a href="#topic+hglasso">hglasso</a></code>, <code><a href="#topic+hcov">hcov</a></code>, or <code><a href="#topic+hbn">hbn</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'hglasso'
summary(object, ...) 
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.hglasso_+3A_object">object</code></td>
<td>

<p>an object of class <code><a href="#topic+hglasso">hglasso</a></code>, <code><a href="#topic+hcov">hcov</a></code>, or <code><a href="#topic+hbn">hbn</a></code>.
</p>
</td></tr>
<tr><td><code id="summary.hglasso_+3A_...">...</code></td>
<td>

<p>any other arguments passed to <code>print</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Some information for an object hglasso, hcov, or hbn: (1) The number of observations n and the number of features p.  (2) The number of edges in Theta, V, and Z. (3) The indices for hub nodes, and also the number of edges within each hub node.</p>


<h3>Author(s)</h3>

<p>Kean Ming Tan 
</p>


<h3>References</h3>

<p>Tan et al. (2014). Learning graphical models with hubs. Journal of Machine Learning Research 15(Oct):3297-3331.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+image.hglasso">image.hglasso</a></code>
<code><a href="#topic+plot.hglasso">plot.hglasso</a></code>
<code><a href="#topic+hglasso">hglasso</a></code>
<code><a href="#topic+hcov">hcov</a></code>
<code><a href="#topic+hbn">hbn</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See example in hglasso, hcov, or hbn.
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
