<!DOCTYPE html><html lang="en"><head><title>Help for package promptr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {promptr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#complete_chat'><p>Complete an LLM Chat</p></a></li>
<li><a href='#complete_prompt'><p>Complete an LLM Prompt</p></a></li>
<li><a href='#format_chat'><p>Format a Chat Prompt</p></a></li>
<li><a href='#format_prompt'><p>Format an LLM prompt</p></a></li>
<li><a href='#occupations'><p>Occupations</p></a></li>
<li><a href='#occupations_examples'><p>Labelled Occupations</p></a></li>
<li><a href='#openai_api_key'><p>Install an OPENAI API KEY in Your <code>.Renviron</code> File for Repeated Use</p></a></li>
<li><a href='#scotus_tweets'><p>Tweets About The Supreme Court of the United States</p></a></li>
<li><a href='#scotus_tweets_examples'><p>Labelled Example Tweets About The Supreme Court of the United States</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Format and Complete Few-Shot LLM Prompts</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Format and submit few-shot prompts to OpenAI's Large Language Models (LLMs). Designed to be particularly useful for text classification problems in the social sciences. Methods are described in Ornstein, Blasingame, and Truscott (2024) <a href="https://joeornstein.github.io/publications/ornstein-blasingame-truscott.pdf">https://joeornstein.github.io/publications/ornstein-blasingame-truscott.pdf</a>.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Imports:</td>
<td>curl, dplyr, glue, httr2, jsonlite, stringr</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/joeornstein/promptr">https://github.com/joeornstein/promptr</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/joeornstein/promptr/issues">https://github.com/joeornstein/promptr/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-08-19 14:36:41 UTC; jo22058</td>
</tr>
<tr>
<td>Author:</td>
<td>Joe Ornstein <a href="https://orcid.org/0000-0002-5704-2098"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre, cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Joe Ornstein &lt;jornstein@uga.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-08-23 11:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='complete_chat'>Complete an LLM Chat</h2><span id='topic+complete_chat'></span>

<h3>Description</h3>

<p>Submits a prompt to OpenAI's &quot;Chat&quot; API endpoint and formats the response into a string or tidy dataframe.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>complete_chat(
  prompt,
  model = "gpt-3.5-turbo",
  openai_api_key = Sys.getenv("OPENAI_API_KEY"),
  max_tokens = 1,
  temperature = 0,
  seed = NULL,
  parallel = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="complete_chat_+3A_prompt">prompt</code></td>
<td>
<p>The prompt</p>
</td></tr>
<tr><td><code id="complete_chat_+3A_model">model</code></td>
<td>
<p>Which OpenAI model to use. Defaults to 'gpt-3.5-turbo'</p>
</td></tr>
<tr><td><code id="complete_chat_+3A_openai_api_key">openai_api_key</code></td>
<td>
<p>Your API key. By default, looks for a system environment variable called &quot;OPENAI_API_KEY&quot; (recommended option). Otherwise, it will prompt you to enter the API key as an argument.</p>
</td></tr>
<tr><td><code id="complete_chat_+3A_max_tokens">max_tokens</code></td>
<td>
<p>How many tokens (roughly 4 characters of text) should the model return? Defaults to a single token (next word prediction).</p>
</td></tr>
<tr><td><code id="complete_chat_+3A_temperature">temperature</code></td>
<td>
<p>A numeric between 0 and 2 When set to zero, the model will always return the most probable next token. For values greater than zero, the model selects the next word probabilistically.</p>
</td></tr>
<tr><td><code id="complete_chat_+3A_seed">seed</code></td>
<td>
<p>An integer. If specified, the OpenAI API will &quot;make a best effort to sample deterministically&quot;.</p>
</td></tr>
<tr><td><code id="complete_chat_+3A_parallel">parallel</code></td>
<td>
<p>TRUE to submit API requests in parallel. Setting to FALSE can reduce rate limit errors at the expense of longer runtime.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If max_tokens = 1, returns a dataframe with the 5 most likely next-word responses and their probabilities. If max_tokens &gt; 1, returns a single string of text generated by the model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
format_chat('Are frogs sentient? Yes or No.') |&gt; complete_chat()
format_chat('Write a haiku about frogs.') |&gt; complete_chat(max_tokens = 100)

## End(Not run)
</code></pre>

<hr>
<h2 id='complete_prompt'>Complete an LLM Prompt</h2><span id='topic+complete_prompt'></span>

<h3>Description</h3>

<p>Submits a text prompt to OpenAI's &quot;Completion&quot; API endpoint and formats the response into a string or tidy dataframe. (Note that, as of 2024, this endpoint is considered &quot;Legacy&quot; by OpenAI and is likely to be deprecated.)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>complete_prompt(
  prompt,
  model = "gpt-3.5-turbo-instruct",
  openai_api_key = Sys.getenv("OPENAI_API_KEY"),
  max_tokens = 1,
  temperature = 0,
  seed = NULL,
  parallel = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="complete_prompt_+3A_prompt">prompt</code></td>
<td>
<p>The prompt</p>
</td></tr>
<tr><td><code id="complete_prompt_+3A_model">model</code></td>
<td>
<p>Which OpenAI model to use. Defaults to 'gpt-3.5-turbo-instruct'</p>
</td></tr>
<tr><td><code id="complete_prompt_+3A_openai_api_key">openai_api_key</code></td>
<td>
<p>Your API key. By default, looks for a system environment variable called &quot;OPENAI_API_KEY&quot; (recommended option). Otherwise, it will prompt you to enter the API key as an argument.</p>
</td></tr>
<tr><td><code id="complete_prompt_+3A_max_tokens">max_tokens</code></td>
<td>
<p>How many tokens (roughly 4 characters of text) should the model return? Defaults to a single token (next word prediction).</p>
</td></tr>
<tr><td><code id="complete_prompt_+3A_temperature">temperature</code></td>
<td>
<p>A numeric between 0 and 2 When set to zero, the model will always return the most probable next token. For values greater than zero, the model selects the next word probabilistically.</p>
</td></tr>
<tr><td><code id="complete_prompt_+3A_seed">seed</code></td>
<td>
<p>An integer. If specified, the OpenAI API will &quot;make a best effort to sample deterministically&quot;.</p>
</td></tr>
<tr><td><code id="complete_prompt_+3A_parallel">parallel</code></td>
<td>
<p>TRUE to submit API requests in parallel. Setting to FALSE can reduce rate limit errors at the expense of longer runtime.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If max_tokens = 1, returns a dataframe with the 5 most likely next words and their probabilities. If max_tokens &gt; 1, returns a single string of text generated by the model.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
complete_prompt('I feel like a')
complete_prompt('Here is my haiku about frogs:',
                max_tokens = 100)

## End(Not run)
</code></pre>

<hr>
<h2 id='format_chat'>Format a Chat Prompt</h2><span id='topic+format_chat'></span>

<h3>Description</h3>

<p>Format a chat prompt to submit to OpenAI's ChatGPT or GPT-4 (particularly useful for classification tasks).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>format_chat(
  text,
  instructions = NA,
  examples = data.frame(),
  system_message = NA
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="format_chat_+3A_text">text</code></td>
<td>
<p>The text to be classified.</p>
</td></tr>
<tr><td><code id="format_chat_+3A_instructions">instructions</code></td>
<td>
<p>Instructions to be included at the beginning of the prompt (format them like you would format instructions to a human research assistant).</p>
</td></tr>
<tr><td><code id="format_chat_+3A_examples">examples</code></td>
<td>
<p>A dataframe of &quot;few-shot&quot; examples. Must include one column called 'text' with the example text(s) and another column called &quot;label&quot; with the correct label(s).</p>
</td></tr>
<tr><td><code id="format_chat_+3A_system_message">system_message</code></td>
<td>
<p>An optional &quot;system message&quot; with high-level instructions (e.g. &quot;You are a helpful research assistant.&quot;)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a series of messages formatted as a list object, which can be used as an input for promptr::complete_chat() or openai::create_chat_completion().
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(scotus_tweets_examples)

format_chat(text = "I am disappointed with this ruling.",
                   instructions = "Decide if the statement is Positive or Negative.",
                   examples = scotus_tweets_examples)
</code></pre>

<hr>
<h2 id='format_prompt'>Format an LLM prompt</h2><span id='topic+format_prompt'></span>

<h3>Description</h3>

<p>Format a text prompt for a Large Language Model. Particularly useful for few-shot text classification tasks. Note that if you are planning to use one of OpenAI's chat models, like ChatGPT or GPT-4, you will want to use the <code>format_chat()</code> function instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>format_prompt(
  text,
  instructions = "",
  examples = data.frame(),
  template = "Text: {text}\nClassification: {label}",
  prompt_template = "{instructions}{examples}{input}",
  separator = "\n\n"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="format_prompt_+3A_text">text</code></td>
<td>
<p>The text to be classified. Can be a character vector or a single string.</p>
</td></tr>
<tr><td><code id="format_prompt_+3A_instructions">instructions</code></td>
<td>
<p>Instructions to be included in the prompt (format them like you would format instructions to a human research assistant).</p>
</td></tr>
<tr><td><code id="format_prompt_+3A_examples">examples</code></td>
<td>
<p>A dataframe of &quot;few-shot&quot; examples. Must include one column called 'text' with the example text(s) and another column called &quot;label&quot; with the correct label(s).</p>
</td></tr>
<tr><td><code id="format_prompt_+3A_template">template</code></td>
<td>
<p>The template for how examples and completions should be formatted, in <code>glue</code> syntax. If you are including few-shot examples in the prompt, this must contain the {text} and {label} placeholders.</p>
</td></tr>
<tr><td><code id="format_prompt_+3A_prompt_template">prompt_template</code></td>
<td>
<p>The template for the entire prompt. Defaults to instructions, followed by few-shot examples, followed by the input to be classified.</p>
</td></tr>
<tr><td><code id="format_prompt_+3A_separator">separator</code></td>
<td>
<p>A character that separates examples. Defaults to two carriage returns.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a formatted prompt that can be used as input for <code>complete_prompt()</code> or <code>openai::create_completion()</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(scotus_tweets_examples)

format_prompt(text = "I am disappointed with this ruling.",
              instructions = "Decide if the sentiment of this statement is Positive or Negative.",
              examples = scotus_tweets_examples,
              template = "Statement: {text}\nSentiment: {label}")

format_prompt(text = 'I am sad about the Supreme Court',
              examples = scotus_tweets_examples,
              template = '"{text}" is a {label} statement',
              separator = '\n')
</code></pre>

<hr>
<h2 id='occupations'>Occupations</h2><span id='topic+occupations'></span>

<h3>Description</h3>

<p>This dataset contains 3,948 ballot designations from municipal elections in California.
A random subset are hand-labeled as either &quot;Working Class&quot; or &quot;Not Working Class&quot; occupations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(occupations)
</code></pre>


<h3>Format</h3>

<p>A data frame with 3948 rows and 2 columns:
</p>

<dl>
<dt>baldesig</dt><dd><p>Ballot designation as it appears in the CEDA dataset</p>
</dd>
<dt>hand_coded</dt><dd><p>A hand-coded occupation classification (for a random subset)</p>
</dd>
</dl>



<h3>References</h3>

<p>California Elections Data Archive (CEDA). https://hdl.handle.net/10211.3/210187
</p>

<hr>
<h2 id='occupations_examples'>Labelled Occupations</h2><span id='topic+occupations_examples'></span>

<h3>Description</h3>

<p>This dataset contains 9 example occupations
along with a classification. These can be used as few-shot
examples for classifying occupations in the <code>occupations</code> dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(occupations_examples)
</code></pre>


<h3>Format</h3>

<p>A data frame with 9 rows and 2 columns:
</p>

<dl>
<dt>text</dt><dd><p>The text of the ballot designation</p>
</dd>
<dt>label</dt><dd><p>The hand-coded label (Working Class, Not Working Class, NA)</p>
</dd>
</dl>



<h3>References</h3>

<p>California Elections Data Archive (CEDA). https://hdl.handle.net/10211.3/210187
</p>

<hr>
<h2 id='openai_api_key'>Install an OPENAI API KEY in Your <code>.Renviron</code> File for Repeated Use</h2><span id='topic+openai_api_key'></span>

<h3>Description</h3>

<p>This function will add your OpenAI API key to your <code>.Renviron</code> file so it can be called securely without being stored
in your code. After you have installed your key, it can be called any time by typing <code>Sys.getenv("OPENAI_API_KEY")</code> and will be
automatically called in package functions. If you do not have an <code>.Renviron</code> file, the function will create one for you.
If you already have an <code>.Renviron</code> file, the function will append the key to your existing file, while making a backup of your
original file for disaster recovery purposes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>openai_api_key(key, overwrite = FALSE, install = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="openai_api_key_+3A_key">key</code></td>
<td>
<p>The API key provided to you from the OpenAI formated in quotes.</p>
</td></tr>
<tr><td><code id="openai_api_key_+3A_overwrite">overwrite</code></td>
<td>
<p>If this is set to TRUE, it will overwrite an existing OPENAI_API_KEY that you already have in your <code>.Renviron</code> file.</p>
</td></tr>
<tr><td><code id="openai_api_key_+3A_install">install</code></td>
<td>
<p>if TRUE, will install the key in your <code>.Renviron</code> file for use in future sessions.  Defaults to FALSE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value, called for side effects
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
openai_api_key("111111abc", install = TRUE)
# First time, reload your environment so you can use the key without restarting R.
readRenviron("~/.Renviron")
# You can check it with:
Sys.getenv("OPENAI_API_KEY")

## End(Not run)

## Not run: 
# If you need to overwrite an existing key:
openai_api_key("111111abc", overwrite = TRUE, install = TRUE)
# First time, reload your environment so you can use the key without restarting R.
readRenviron("~/.Renviron")
# You can check it with:
Sys.getenv("OPENAI_API_KEY")

## End(Not run)
</code></pre>

<hr>
<h2 id='scotus_tweets'>Tweets About The Supreme Court of the United States</h2><span id='topic+scotus_tweets'></span>

<h3>Description</h3>

<p>This dataset contains 945 tweets referencing the US Supreme Court.
Roughly half were collected on June 4, 2018 following the <em>Masterpiece Cakeshop</em>
ruling, and the other half were collected on July 9, 2020 following the
Court's concurrently released opinions in <em>Trump v. Mazars</em> and <em>Trump v. Vance</em>.
Each tweet includes three independent human-coded sentiment scores (-1 to +1).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(scotus_tweets)
</code></pre>


<h3>Format</h3>

<p>A data frame with 945 rows and 5 columns:
</p>

<dl>
<dt>tweet_id</dt><dd><p>A unique ID</p>
</dd>
<dt>text</dt><dd><p>The text of the tweet</p>
</dd>
<dt>case</dt><dd><p>An identifier denoting which Supreme Court ruling the tweet was collected after.</p>
</dd>
<dt>expert1, expert2, expert3</dt><dd><p>Hand-coded sentiment score (-1 = negative, 0 = neutral, 1 = positive)</p>
</dd>
</dl>



<h3>Details</h3>

<p>CONTENT WARNING: These texts come from social media, and many contain explicit
or offensive language.
</p>


<h3>References</h3>

<p>Ornstein et al. (2024). &quot;How To Train Your Stochastic Parrot&quot;
</p>

<hr>
<h2 id='scotus_tweets_examples'>Labelled Example Tweets About The Supreme Court of the United States</h2><span id='topic+scotus_tweets_examples'></span>

<h3>Description</h3>

<p>This dataset contains 12 example tweets referencing the Supreme Court
along with a sentiment label. These can be used as few-shot prompt
examples for classifying tweets in the <code>scotus_tweets</code> dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(scotus_tweets_examples)
</code></pre>


<h3>Format</h3>

<p>A data frame with 12 rows and 4 columns:
</p>

<dl>
<dt>tweet_id</dt><dd><p>A unique ID for each tweet</p>
</dd>
<dt>text</dt><dd><p>The text of the tweet</p>
</dd>
<dt>case</dt><dd><p>The case referenced in the tweet (Masterpiece Cakeshop or Trump v. Mazars)</p>
</dd>
<dt>label</dt><dd><p>The &quot;true&quot; label (Positive, Negative, or Neutral)</p>
</dd>
</dl>



<h3>References</h3>

<p>Ornstein et al. (2023). &quot;How To Train Your Stochastic Parrot&quot;
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
