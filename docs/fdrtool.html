<!DOCTYPE html><html><head><title>Help for package fdrtool</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {fdrtool}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#censored.fit'><p>Fit Null Distribution To Censored Data by Maximum Likelihood</p></a></li>
<li><a href='#dcor0'><p>Distribution of the Vanishing Correlation Coefficient (rho=0) and Related Functions</p></a></li>
<li><a href='#fdrtool'><p>Estimate (Local) False Discovery Rates For Diverse Test Statistics</p></a></li>
<li><a href='#fdrtool-internal'><p>Internal fdrtool Functions</p></a></li>
<li><a href='#gcmlcm'><p>Greatest Convex Minorant and Least Concave Majorant</p></a></li>
<li><a href='#grenader'><p>Grenander Estimator of a Decreasing or Increasing Density</p></a></li>
<li><a href='#halfnormal'><p>The Half-Normal Distribution</p></a></li>
<li><a href='#hc.score'><p>Compute Empirical Higher Criticism Scores and Corresponding Decision Threshold From p-Values</p></a></li>
<li><a href='#monoreg'><p>Monotone Regression: Isotonic Regression and Antitonic Regression</p></a></li>
<li><a href='#pval.estimate.eta0'><p>Estimate the Proportion of Null p-Values</p></a></li>
<li><a href='#pvalues'><p>Example p-Values</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.2.17</td>
</tr>
<tr>
<td>Date:</td>
<td>2021-11-13</td>
</tr>
<tr>
<td>Title:</td>
<td>Estimation of (Local) False Discovery Rates and Higher Criticism</td>
</tr>
<tr>
<td>Author:</td>
<td>Bernd Klaus and Korbinian Strimmer.</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Korbinian Strimmer &lt;strimmerlab@gmail.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.2)</td>
</tr>
<tr>
<td>Imports:</td>
<td>graphics, grDevices, stats</td>
</tr>
<tr>
<td>Description:</td>
<td>Estimates both tail area-based false 
   discovery rates (Fdr) as well as local false discovery rates (fdr) for a 
   variety of null models (p-values, z-scores, correlation coefficients,
   t-scores).  The proportion of null values and the parameters of the null 
   distribution are adaptively estimated from the data.  In addition, the package 
   contains functions for non-parametric density estimation (Grenander estimator), 
   for monotone regression (isotonic regression and antitonic regression with weights),
   for computing the greatest convex minorant (GCM) and the least concave majorant (LCM), 
   for the half-normal and correlation distributions, and for computing
   empirical higher criticism (HC) scores and the corresponding decision threshold.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://strimmerlab.github.io/software/fdrtool/">https://strimmerlab.github.io/software/fdrtool/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-11-13 19:05:08 UTC; strimmer</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-11-13 20:30:11 UTC</td>
</tr>
</table>
<hr>
<h2 id='censored.fit'>Fit Null Distribution To Censored Data by Maximum Likelihood</h2><span id='topic+censored.fit'></span><span id='topic+fndr.cutoff'></span>

<h3>Description</h3>

<p><code>censored.fit</code> fits  a null distribution 
to censored data.
</p>
<p><code>fndr.cutoff</code> finds a suitable cutoff point based on the 
(approximate) false non-discovery rate (FNDR).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>censored.fit(x, cutoff, statistic=c("normal", "correlation", "pvalue", "studentt"))
fndr.cutoff(x, statistic=c("normal", "correlation", "pvalue", "studentt"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="censored.fit_+3A_x">x</code></td>
<td>
<p>vector of test statistics.</p>
</td></tr>
<tr><td><code id="censored.fit_+3A_cutoff">cutoff</code></td>
<td>
<p>truncation point (this may a single value or a vector).</p>
</td></tr>
<tr><td><code id="censored.fit_+3A_statistic">statistic</code></td>
<td>
<p>type of statistic - normal, correlation, or student t.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>As null model truncated normal, truncated student t or a truncated
correlation density is assumed.  The truncation point is specified
by the cutoff parameter.  All data points whose absolute value
are large than the cutoff point are ignored when fitting the truncated
null model via maximum likelihood.  The total number of data points is
only used to estimate the fraction of null values eta0.
</p>


<h3>Value</h3>

<p><code>censored.fit</code> returns a matrix whose rows contain the estimated parameters and corresponding errors
for each cutoff point. 
</p>
<p><code>fndr.cutoff</code> returns a tentative cutoff point.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fdrtool">fdrtool</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load "fdrtool" library
library("fdrtool")

# simulate normal data
sd.true = 2.232
n = 5000
z = rnorm(n, sd=sd.true)
censored.fit(z, c(2,3,5), statistic="normal")


# simulate contaminated mixture of correlation distribution
r = rcor0(700, kappa=10)
u1 = runif(200, min=-1, max=-0.7)
u2 = runif(200, min=0.7, max=1)
rc = c(r, u1, u2)

censored.fit(r, 0.7, statistic="correlation")
censored.fit(rc, 0.7, statistic="correlation")

# pvalue example
data(pvalues)
co = fndr.cutoff(pvalues, statistic="pvalue")
co
censored.fit(pvalues, cutoff=co, statistic="pvalue")
</code></pre>

<hr>
<h2 id='dcor0'>Distribution of the Vanishing Correlation Coefficient (rho=0) and Related Functions</h2><span id='topic+dcor0'></span><span id='topic+pcor0'></span><span id='topic+rcor0'></span><span id='topic+qcor0'></span>

<h3>Description</h3>

<p>The above functions describe the distribution of the Pearson correlation 
coefficient <code>r</code> assuming that there is no correlation present (<code>rho = 0</code>).
</p>
<p>Note that the distribution has only a single parameter: the degree 
of freedom <code>kappa</code>, which is equal to the inverse of the variance of the distribution.
</p>
<p>The theoretical value of  <code>kappa</code> depends both on the sample size <code>n</code> and the number 
<code>p</code> of considered variables.  If a simple correlation coefficient between two
variables  (<code>p=2</code>)  is considered the degree of freedom equals <code>kappa = n-1</code>.
However, if a partial correlation coefficient is considered (conditioned on <code>p-2</code> remaining 
variables) the degree of freedom is <code>kappa = n-1-(p-2) = n-p+1</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dcor0(x, kappa, log=FALSE)
pcor0(q, kappa, lower.tail=TRUE, log.p=FALSE)
qcor0(p, kappa, lower.tail=TRUE, log.p=FALSE)
rcor0(n, kappa)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dcor0_+3A_x">x</code>, <code id="dcor0_+3A_q">q</code></td>
<td>
<p>vector of sample correlations</p>
</td></tr>
<tr><td><code id="dcor0_+3A_p">p</code></td>
<td>
<p>vector of probabilities</p>
</td></tr>
<tr><td><code id="dcor0_+3A_kappa">kappa</code></td>
<td>
<p>the degree of freedom of the distribution (= inverse variance)</p>
</td></tr>
<tr><td><code id="dcor0_+3A_n">n</code></td>
<td>
<p>number of values to generate. If n is a vector, length(n)
values will be generated</p>
</td></tr>
<tr><td><code id="dcor0_+3A_log">log</code>, <code id="dcor0_+3A_log.p">log.p</code></td>
<td>
<p>logical vector; if TRUE, probabilities p are given as log(p)</p>
</td></tr>
<tr><td><code id="dcor0_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical vector; if TRUE (default), probabilities are <code class="reqn">P[R &lt;= r]</code>,
otherwise, <code class="reqn">P[R &gt; r]</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>For density and distribution functions as well as a corresponding random number generator
of the correlation coefficient for arbitrary non-vanishing correlation <code>rho</code> please refer to the
<code>SuppDists</code> package by  Bob Wheeler <a href="mailto:bwheeler@echip.com">bwheeler@echip.com</a> (available on CRAN).
Note that the parameter <code>N</code> in his <code>dPearson</code> function corresponds to  <code>N=kappa+1</code>.
</p>


<h3>Value</h3>

<p><code>dcor0</code> gives the density,  <code>pcor0</code> 
gives the distribution function, <code>qcor0</code> gives
the quantile function, and <code>rcor0</code> generates random deviates.
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+cor">cor</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load fdrtool library
library("fdrtool")

# distribution of r for various degrees of freedom
x = seq(-1,1,0.01)
y1 = dcor0(x, kappa=7)
y2 = dcor0(x, kappa=15)
plot(x,y2,type="l", xlab="r", ylab="pdf",
  xlim=c(-1,1), ylim=c(0,2))
lines(x,y1)

# simulated data
r = rcor0(1000, kappa=7)
hist(r, freq=FALSE, 
  xlim=c(-1,1), ylim=c(0,5))
lines(x,y1,type="l")

# distribution function
pcor0(-0.2, kappa=15)
</code></pre>

<hr>
<h2 id='fdrtool'>Estimate (Local) False Discovery Rates For Diverse Test Statistics</h2><span id='topic+fdrtool'></span>

<h3>Description</h3>

<p><code>fdrtool</code> takes a vector of z-scores (or of correlations, p-values,
or t-statistics), and estimates for each case both the tail area-based Fdr
as well as the density-based fdr (=q-value resp. local false discovery rate).
The parameters of the null distribution are 
estimated adaptively from the data (except for the case of p-values where
this is not necessary).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fdrtool(x, statistic=c("normal", "correlation", "pvalue"),
  plot=TRUE, color.figure=TRUE, verbose=TRUE, 
  cutoff.method=c("fndr", "pct0", "locfdr"),
  pct0=0.75)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fdrtool_+3A_x">x</code></td>
<td>
<p>vector of the observed test statistics.</p>
</td></tr>
<tr><td><code id="fdrtool_+3A_statistic">statistic</code></td>
<td>
<p>one of &quot;normal&quot; (default), &quot;correlation&quot;, &quot;pvalue&quot;.  
This species the null model.</p>
</td></tr>
<tr><td><code id="fdrtool_+3A_plot">plot</code></td>
<td>
<p>plot a figure with estimated densities, distribution functions, 
and (local) false discovery rates.</p>
</td></tr>
<tr><td><code id="fdrtool_+3A_verbose">verbose</code></td>
<td>
<p>print out status messages.</p>
</td></tr>
<tr><td><code id="fdrtool_+3A_cutoff.method">cutoff.method</code></td>
<td>
<p>one of &quot;fndr&quot; (default), &quot;pct0&quot;, &quot;locfdr&quot;.</p>
</td></tr>
<tr><td><code id="fdrtool_+3A_pct0">pct0</code></td>
<td>
<p>fraction of data used for fitting null model - only if <code>cutoff.method</code>=&quot;pct0&quot;</p>
</td></tr>
<tr><td><code id="fdrtool_+3A_color.figure">color.figure</code></td>
<td>
<p>determines whether a color figure or a black and white
figure is produced (defaults to &quot;TRUE&quot;, i.e. to color figure).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm implemented in this function proceeds as follows:
</p>

<ol>
<li><p>  A suitable cutoff point is determined.  If <code>cutoff.method</code>
is &quot;fndr&quot; then first an approximate null model is fitted and
subsequently a cutoff point is sought with false nondiscovery
rate as small as possible (see <code><a href="#topic+fndr.cutoff">fndr.cutoff</a></code>). 
If <code>cutoff.method</code> is &quot;pct0&quot;
then a specified quantile (default value: 0.75) of the data
is used as the cutoff point.  If <code>cutoff.method</code> equals
&quot;locfdr&quot; then the heuristic of the &quot;locfdr&quot; package (version 1.1-6)
is employed to find the cutoff (z-scores and correlations only).
</p>
</li>
<li><p>  The parameters of the null model are estimated from the 
data using <code><a href="#topic+censored.fit">censored.fit</a></code>. This results
in estimates for scale parameters und and proportion
of null values (<code>eta0</code>). 
</p>
</li>
<li><p>  Subsequently the corresponding p-values are computed, and
a modified <code><a href="#topic+grenander">grenander</a></code> algorithm is employed
to obtain the overall density and distribution function 
(note that this respects the estimated <code>eta0</code>).
</p>
</li>
<li><p>  Finally, q-values and local fdr values are computed for each case.
</p>
</li></ol>

<p>The assumed null models all have (except for p-values) one free
scale parameter.  Note that the z-scores and the correlations
are assumed to have zero mean. 
</p>


<h3>Value</h3>

<p>A list with the following components:
</p>
<table>
<tr><td><code>pval</code></td>
<td>
<p>a vector with p-values for each case.</p>
</td></tr> 
<tr><td><code>qval</code></td>
<td>
<p>a vector with q-values (Fdr) for each case.</p>
</td></tr>
<tr><td><code>lfdr</code></td>
<td>
<p>a vector with local fdr values for each case.</p>
</td></tr>
<tr><td><code>statistic</code></td>
<td>
<p>the specified type of null model.</p>
</td></tr> 
<tr><td><code>param</code></td>
<td>
<p>a vector containing the estimated parameters (the null 
proportion <code>eta0</code>  and the free parameter of the null model).</p>
</td></tr>  
</table>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>References</h3>

<p>Strimmer, K. (2008a).   A unified approach to false discovery 
rate estimation. BMC Bioinformatics 9: 303.
&lt;DOI:10.1186/1471-2105-9-303&gt;
</p>
<p>Strimmer, K. (2008b). fdrtool: a versatile R package for estimating 
local and tail area- based false discovery rates.
Bioinformatics 24: 1461-1462.
&lt;DOI:10.1093/bioinformatics/btn209&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pval.estimate.eta0">pval.estimate.eta0</a></code>, <code><a href="#topic+censored.fit">censored.fit</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load "fdrtool" library and p-values
library("fdrtool")
data(pvalues)


# estimate fdr and Fdr from p-values

data(pvalues)
fdr = fdrtool(pvalues, statistic="pvalue")
fdr$qval # estimated Fdr values 
fdr$lfdr # estimated local fdr 

# the same but with black and white figure  
fdr = fdrtool(pvalues, statistic="pvalue", color.figure=FALSE)


# estimate fdr and Fdr from z-scores

sd.true = 2.232
n = 500
z = rnorm(n, sd=sd.true)
z = c(z, runif(30, 5, 10)) # add some contamination
fdr = fdrtool(z)

# you may change some parameters of the underlying functions
fdr = fdrtool(z, cutoff.method="pct0", pct0=0.9) 
</code></pre>

<hr>
<h2 id='fdrtool-internal'>Internal fdrtool Functions</h2><span id='topic+pvt.isoMean'></span><span id='topic+r2t'></span><span id='topic+t2r'></span><span id='topic+pvt.nullfunction'></span><span id='topic+pvt.plotlabels'></span><span id='topic+ecdf.pval'></span><span id='topic+num.curv'></span><span id='topic+approximate.fit'></span><span id='topic+iqr.fit'></span><span id='topic+get.nullmodel'></span><span id='topic+pvt.fit.nullmodel'></span>

<h3>Description</h3>

<p>Internal fdrtool functions.
</p>


<h3>Note</h3>

<p>These are not to be called by the user (or in some cases are just
waiting for proper documentation to be written).
</p>

<hr>
<h2 id='gcmlcm'>Greatest Convex Minorant and Least Concave Majorant</h2><span id='topic+gcmlcm'></span>

<h3>Description</h3>

<p><code>gcmlcm</code> computes the greatest convex minorant (GCM) or the
least concave majorant (LCM) of a piece-wise linear function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gcmlcm(x, y, type=c("gcm", "lcm"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gcmlcm_+3A_x">x</code>, <code id="gcmlcm_+3A_y">y</code></td>
<td>
<p>coordinate vectors of the piece-wise linear function. Note
that the x values need to be unique and be arranged in sorted order.</p>
</td></tr>
<tr><td><code id="gcmlcm_+3A_type">type</code></td>
<td>
<p>specifies whether to compute the greatest convex 
minorant (<code>type="gcm"</code>, the default) or the
least concave majorant (<code>type="lcm"</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The GCM is obtained by isotonic regression of the raw slopes,
whereas the LCM is obtained by antitonic regression.
See Robertson et al. (1988).
</p>


<h3>Value</h3>

<p>A list with the following entries:
</p>
<table>
<tr><td><code>x.knots</code></td>
<td>
<p>the x values belonging to the knots of the LCM/GCM curve</p>
</td></tr>
<tr><td><code>y.knots</code></td>
<td>
<p>the corresponding y values</p>
</td></tr>
<tr><td><code>slope.knots</code></td>
<td>
<p>the slopes of the corresponding line segments</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>References</h3>

<p>Robertson, T., F. T. Wright, and R. L. Dykstra. 1988.  Order restricted
statistical inference. John Wiley and Sons.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+monoreg">monoreg</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load "fdrtool" library
library("fdrtool")

# generate some data
x = 1:20
y = rexp(20)
plot(x, y, type="l", lty=3, main="GCM (red) and LCM (blue)")
points(x, y)

# greatest convex minorant (red)
gg = gcmlcm(x,y)
lines(gg$x.knots, gg$y.knots, col=2, lwd=2)

# least concave majorant (blue)
ll = gcmlcm(x,y, type="lcm")
lines(ll$x.knots, ll$y.knots, col=4, lwd=2)

</code></pre>

<hr>
<h2 id='grenader'>Grenander Estimator of a Decreasing or Increasing Density</h2><span id='topic+grenander'></span><span id='topic+plot.grenander'></span>

<h3>Description</h3>

<p>The function <code>grenander</code> computes the Grenander estimator
of a one-dimensional decreasing or increasing density. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>grenander(F, type=c("decreasing", "increasing"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="grenader_+3A_f">F</code></td>
<td>
<p>an <code><a href="stats.html#topic+ecdf">ecdf</a></code> containing the empirical cumulative density.</p>
</td></tr>
<tr><td><code id="grenader_+3A_type">type</code></td>
<td>
<p>specifies whether the distribution is decreasing (the default) or increasing.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Grenander (1956) density estimator is given by the slopes of the 
least concave majorant (LCM) of the empirical distribution function (ECDF).
It is a decreasing piecewise-constant function and can be shown to be the 
non-parametric maximum likelihood estimate (NPMLE) under the assumption
of a decreasing density (note that the ECDF is the NPMLE without
this assumption).  Similarly, an increasing density function is obtained
by using the greatest convex minorant (GCM) of the ECDF.
</p>


<h3>Value</h3>

<p>A list of class <code>grenander</code> with the following components:
</p>
<table>
<tr><td><code>F</code></td>
<td>
<p>the empirical distribution function specified as input.   </p>
</td></tr> 
<tr><td><code>x.knots</code></td>
<td>
<p>x locations of the knots of the least concave majorant of the ECDF.</p>
</td></tr>
<tr><td><code>F.knots</code></td>
<td>
<p>the corresponding y locations of the least concave majorant of the ECDF.</p>
</td></tr>
<tr><td><code>f.knots</code></td>
<td>
<p>the corresponding slopes (=density).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>References</h3>

<p>Grenander, U. (1956). On the theory of mortality measurement, Part II.  
<em>Skan. Aktuarietidskr</em>,
<b>39</b>, 125&ndash;153.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+ecdf">ecdf</a></code>, <code><a href="#topic+gcmlcm">gcmlcm</a></code>, <code><a href="stats.html#topic+density">density</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load "fdrtool" library
library("fdrtool")

# samples from random exponential variable 
z = rexp(30,1)
e = ecdf(z)
g = grenander(e)
g

# plot ecdf, concave cdf, and Grenander density estimator (on log scale)
plot(g, log="y") 

# for comparison the kernel density estimate
plot(density(z)) 

# area under the Grenander density estimator 
sum( g$f.knots[-length(g$f.knots)]*diff(g$x.knots) )
</code></pre>

<hr>
<h2 id='halfnormal'>The Half-Normal Distribution</h2><span id='topic+halfnormal'></span><span id='topic+dhalfnorm'></span><span id='topic+phalfnorm'></span><span id='topic+qhalfnorm'></span><span id='topic+rhalfnorm'></span><span id='topic+sd2theta'></span><span id='topic+theta2sd'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function and random
generation for the half-normal distribution with parameter <code>theta</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dhalfnorm(x, theta=sqrt(pi/2), log = FALSE)
phalfnorm(q, theta=sqrt(pi/2), lower.tail = TRUE, log.p = FALSE)
qhalfnorm(p, theta=sqrt(pi/2), lower.tail = TRUE, log.p = FALSE)
rhalfnorm(n, theta=sqrt(pi/2))
sd2theta(sd)
theta2sd(theta)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="halfnormal_+3A_x">x</code>, <code id="halfnormal_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="halfnormal_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="halfnormal_+3A_n">n</code></td>
<td>
<p>number of observations. If <code>length(n) &gt; 1</code>, the length
is taken to be the number required.</p>
</td></tr>
<tr><td><code id="halfnormal_+3A_theta">theta</code></td>
<td>
<p>parameter of half-normal distribution.</p>
</td></tr>
<tr><td><code id="halfnormal_+3A_log">log</code>, <code id="halfnormal_+3A_log.p">log.p</code></td>
<td>
<p>logical; if TRUE, probabilities p are given as log(p).</p>
</td></tr>
<tr><td><code id="halfnormal_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical; if TRUE (default), probabilities are
<code class="reqn">P[X \le x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code>.</p>
</td></tr>
<tr><td><code id="halfnormal_+3A_sd">sd</code></td>
<td>
<p>standard deviation of the zero-mean normal distribution 
that corresponds to the half-normal with parameter <code>theta</code>. </p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>x = abs(z)</code> follows a half-normal distribution with
if <code>z</code> is a normal variate with zero mean. 
The half-normal distribution has density
</p>
<p style="text-align: center;"><code class="reqn">
    f(x) =
    \frac{2 \theta}{\pi} e^{-x^2 \theta^2/\pi}</code>
</p>

<p>It has mean  <code class="reqn">E(x) = \frac{1}{\theta}</code> and variance 
<code class="reqn">Var(x) = \frac{\pi-2}{2 \theta^2}</code>.
</p>
<p>The parameter <code class="reqn">\theta</code> is related to the
standard deviation <code class="reqn">\sigma</code> of the corresponding 
zero-mean normal distribution by the equation
<code class="reqn">\theta = \sqrt{\pi/2}/\sigma</code>.
</p>
<p>If <code class="reqn">\theta</code> is not specified in the above functions 
it assumes the default values of <code class="reqn">\sqrt{\pi/2}</code>, 
corresponding to <code class="reqn">\sigma=1</code>.  
</p>


<h3>Value</h3>

<p><code>dhalfnorm</code> gives the density,
<code>phalfnorm</code> gives the distribution function,
<code>qhalfnorm</code> gives the quantile function, and
<code>rhalfnorm</code> generates random deviates.
<code>sd2theta</code> computes a <code>theta</code> parameter. 
<code>theta2sd</code> computes a <code>sd</code> parameter. 
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+Normal">Normal</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load "fdrtool" library
library("fdrtool")


## density of half-normal compared with a corresponding normal
par(mfrow=c(1,2))

sd.norm = 0.64
x  = seq(0, 5, 0.01)
x2 = seq(-5, 5, 0.01)
plot(x, dhalfnorm(x, sd2theta(sd.norm)), type="l", xlim=c(-5, 5), lwd=2,
   main="Probability Density", ylab="pdf(x)")
lines(x2, dnorm(x2, sd=sd.norm), col=8 )


plot(x, phalfnorm(x, sd2theta(sd.norm)), type="l", xlim=c(-5, 5),  lwd=2,
   main="Distribution Function", ylab="cdf(x)")
lines(x2, pnorm(x2, sd=sd.norm), col=8 )

legend("topleft", 
c("half-normal", "normal"), lwd=c(2,1),
col=c(1, 8), bty="n", cex=1.0)

par(mfrow=c(1,1))


## distribution function

integrate(dhalfnorm, 0, 1.4, theta = 1.234)
phalfnorm(1.4, theta = 1.234)

## quantile function
qhalfnorm(-1) # NaN
qhalfnorm(0)
qhalfnorm(.5)
qhalfnorm(1)
qhalfnorm(2) # NaN

## random numbers
theta = 0.72
hz = rhalfnorm(10000, theta)
hist(hz, freq=FALSE)
lines(x, dhalfnorm(x, theta))

mean(hz) 
1/theta  # theoretical mean

var(hz)
(pi-2)/(2*theta*theta) # theoretical variance


## relationship with two-sided normal p-values
z = rnorm(1000)

# two-sided p-values
pvl = 1- phalfnorm(abs(z))
pvl2 = 2 - 2*pnorm(abs(z)) 
sum(pvl-pvl2)^2 # equivalent
hist(pvl2, freq=FALSE)  # uniform distribution

# back to half-normal scores
hz = qhalfnorm(1-pvl)
hist(hz, freq=FALSE)
lines(x, dhalfnorm(x))
</code></pre>

<hr>
<h2 id='hc.score'>Compute Empirical Higher Criticism Scores and Corresponding Decision Threshold From p-Values</h2><span id='topic+hc.score'></span><span id='topic+hc.thresh'></span>

<h3>Description</h3>

<p><code>hc.score</code> computes the empirical higher criticism (HC) scores from p-values.
</p>
<p><code>hc.thresh</code> determines the HC decision threshold by searching for the p-value with the maximum HC score.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hc.score(pval)
hc.thresh(pval, alpha0=1, plot=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hc.score_+3A_pval">pval</code></td>
<td>
<p>vector of p-values.</p>
</td></tr>
<tr><td><code id="hc.score_+3A_alpha0">alpha0</code></td>
<td>
<p>look only at a fraction <code>alpha0</code> of the p-values (default: 1, i.e. all p-values).</p>
</td></tr>
<tr><td><code id="hc.score_+3A_plot">plot</code></td>
<td>
<p>show plot with HC decision threshold.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Higher Criticism (HC) provides an alternative means to determine decision thresholds for
signal identification, especially if the signal is rare and weak.
</p>
<p>See Donoho and Jin (2008) for details of this approach 
and Klaus and Strimmer (2012) for a review and connections with FDR methdology. 
</p>


<h3>Value</h3>

<p><code>hc.score</code> returns a vector with the HC score corresponding to each p-value. 
</p>
<p><code>hc.thresh</code> returns the p-value corresponding to the maximum HC score.
</p>


<h3>Author(s)</h3>

<p>Bernd Klaus and  Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>


<h3>References</h3>

<p>Donoho, D. and J. Jin. (2008). Higher criticism thresholding: optimal feature selection
when useful features are rare and weak. Proc. Natl. Acad. Sci. USA 105:14790-15795.
</p>
<p>Klaus, B., and K. Strimmer (2013). Signal identification for rare and
weak features: higher criticism or false discovery rates?
Biostatistics 14: 129-143. &lt;DOI:10.1093/biostatistics/kxs030&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fdrtool">fdrtool</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load "fdrtool" library
library("fdrtool")

# some p-values
data(pvalues)

# compute HC scores
hc.score(pvalues)

# determine HC threshold
hc.thresh(pvalues)
</code></pre>

<hr>
<h2 id='monoreg'>Monotone Regression: Isotonic Regression and Antitonic Regression</h2><span id='topic+monoreg'></span><span id='topic+plot.monoreg'></span><span id='topic+fitted.monoreg'></span><span id='topic+residuals.monoreg'></span>

<h3>Description</h3>

<p><code>monoreg</code> performs monotone regression (either isotonic
or antitonic) with weights. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>monoreg(x, y=NULL, w=rep(1, length(x)), type=c("isotonic", "antitonic"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="monoreg_+3A_x">x</code>, <code id="monoreg_+3A_y">y</code></td>
<td>
<p>coordinate vectors of the regression
points.  Alternatively a single &ldquo;plotting&rdquo; structure can be
specified: see <code><a href="grDevices.html#topic+xy.coords">xy.coords</a></code>.</p>
</td></tr>
<tr><td><code id="monoreg_+3A_w">w</code></td>
<td>
<p>data weights (default values: 1).</p>
</td></tr>
<tr><td><code id="monoreg_+3A_type">type</code></td>
<td>
<p>fit a monotonely increasing (&quot;isotonic&quot;) or 
monotonely decreasing (&quot;antitonic&quot;) function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>monoreg</code> is similar to <code><a href="stats.html#topic+isoreg">isoreg</a></code>, with the addition
that <code>monoreg</code> accepts weights.
</p>
<p>If several identical <code>x</code> values are given as input, the 
corresponding <code>y</code> values and the
weights <code>w</code> are automatically merged, and a warning is issued.
</p>
<p>The <code>plot.monoreg</code> function optionally plots the cumulative
sum diagram with the greatest convex minorant (isotonic regression)
or the least concave majorant (antitonic regression), see the
examples below. 
</p>


<h3>Value</h3>

<p>A list with the following entries:
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>the sorted and unique x values</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>the corresponding y values</p>
</td></tr>
<tr><td><code>w</code></td>
<td>
<p>the corresponding weights</p>
</td></tr>
<tr><td><code>yf</code></td>
<td>
<p>the fitted y values</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>the type of monotone regression (&quot;isotonic&quot; or &quot;antitonic&quot;</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the function call</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>
<p>Part of this function is C code that has been ported from
R code originally written by Kaspar Rufibach. 
</p>


<h3>References</h3>

<p>Robertson, T., F. T. Wright, and R. L. Dykstra. 1988.  Order restricted
statistical inference. John Wiley and Sons.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+isoreg">isoreg</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load "fdrtool" library
library("fdrtool")


# an example with weights

# Example 1.1.1. (dental study) from Robertson, Wright and Dykstra (1988)
age = c(14, 14, 8, 8, 8, 10, 10, 10, 12, 12, 12)
size = c(23.5, 25, 21, 23.5, 23, 24, 21, 25, 21.5, 22, 19)

mr = monoreg(age, size)

# sorted x values
mr$x # 8 10 12 14
# weights and merged y values
mr$w  # 3 3 3 2
mr$y #  22.50000 23.33333 20.83333 24.25000
# fitted y values
mr$yf # 22.22222 22.22222 22.22222 24.25000
fitted(mr)
residuals(mr)

plot(mr, ylim=c(18, 26))  # this shows the averaged data points
points(age, size, pch=2)  # add original data points


###

y = c(1,0,1,0,0,1,0,1,1,0,1,0)
x = 1:length(y)
mr = monoreg(y)

# plot with greatest convex minorant
plot(mr, plot.type="row.wise")  

# this is the same
mr = monoreg(x,y)
plot(mr)

# antitonic regression and least concave majorant
mr = monoreg(-y, type="a")
plot(mr, plot.type="row.wise")  

# the fit yf is independent of the location of x and y
plot(monoreg(x + runif(1, -1000, 1000), 
             y +runif(1, -1000, 1000)) )

###

y = c(0,0,2/4,1/5,2/4,1/2,4/5,5/8,7/11,10/11)
x = c(5,9,13,18,22,24,29,109,120,131)

mr = monoreg(x,y)
plot(mr, plot.type="row.wise")

# the fit (yf) only depends on the ordering of x
monoreg(1:length(y), y)$yf 
monoreg(x, y)$yf 


</code></pre>

<hr>
<h2 id='pval.estimate.eta0'>Estimate the Proportion of Null p-Values</h2><span id='topic+pval.estimate.eta0'></span>

<h3>Description</h3>

<p><code>pval.estimate.eta0</code> estimates the proportion eta0 of null p-values in a given
vector of p-values. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pval.estimate.eta0(p, method=c("smoother", "bootstrap", "conservative",
   "adaptive", "quantile"), lambda=seq(0,0.9,0.05), 
   diagnostic.plot=TRUE, q=0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pval.estimate.eta0_+3A_p">p</code></td>
<td>
<p>vector of p-values</p>
</td></tr>
<tr><td><code id="pval.estimate.eta0_+3A_method">method</code></td>
<td>
<p>algorithm used to estimate the proportion of null p-values.
Available options are
&quot;conservative&quot; , &quot;adaptive&quot;, &quot;bootstrap&quot;, quantile, and &quot;smoother&quot; (default).</p>
</td></tr>
<tr><td><code id="pval.estimate.eta0_+3A_lambda">lambda</code></td>
<td>
<p>optional tuning parameter vector needed for &quot;bootstrap&quot;
and &quot;smoothing&quot; methods (defaults to <code>seq(0,0.9,0.05)</code>)
- see Storey (2002) and Storey and Tibshirani (2003).</p>
</td></tr>
<tr><td><code id="pval.estimate.eta0_+3A_diagnostic.plot">diagnostic.plot</code></td>
<td>
<p>if <code>TRUE</code> (the default) the histogram of the p-values together
with the estimate of <code>eta0</code> null line is plotted.  This is useful to
visually check the fit of the estimated proportion of null p-values.</p>
</td></tr>  
<tr><td><code id="pval.estimate.eta0_+3A_q">q</code></td>
<td>
<p>quantile used for estimating eta0 - only if <code>method</code>=&quot;quantile&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This quantity <code>eta0</code>, i.e.  the proportion eta0 of null p-values in a given
vector of p-values, is an important parameter 
when controlling  the false discovery rate (FDR).  A conservative choice is
eta0 = 1 but a choice closer to the true value will increase efficiency
and power 
- see   Benjamini and Hochberg (1995, 2000) and Storey (2002) for details.
</p>
<p>The function <code>pval.estimate.eta0</code> provides five algorithms: the &quot;conservative&quot;
method always returns eta0 = 1  (Benjamini and Hochberg, 1995), &quot;adaptive&quot;
uses the approach suggested in Benjamini and Hochberg (2000), &quot;bootstrap&quot;
employs the method from Storey (2002), &quot;smoother&quot; uses the smoothing spline
approach in Storey and Tibshirani (2003), and &quot;quantile&quot; is a simplified version
of &quot;smoother&quot;.
</p>


<h3>Value</h3>

<p>The estimated proportion eta0 of null p-values. 
</p>


<h3>Author(s)</h3>

<p>Korbinian Strimmer (<a href="https://strimmerlab.github.io">https://strimmerlab.github.io</a>).
</p>
<p>Adapted in part from code by Y. Benjamini and J.D. Storey. 
</p>


<h3>References</h3>

<p><em>&quot;conservative&quot; procedure:</em> Benjamini, Y., and Y. Hochberg (1995)  Controlling the false
discovery rate: a practical and powerful approach to multiple testing.
<em>J. Roy. Statist. Soc. B</em>, <b>57</b>, 289&ndash;300.
</p>
<p><em>&quot;adaptive&quot; procedure:</em> Benjamini, Y., and Y. Hochberg (2000) The adaptive control
of the false discovery rate in multiple hypotheses testing with independent statistics.
<em>J. Behav. Educ. Statist.</em>, <b>25</b>, 60&ndash;83.
</p>
<p><em>&quot;bootstrap&quot; procedure:</em> Storey, J. D. (2002) A direct approach to false
discovery rates.
<em>J. Roy. Statist. Soc. B.</em>, <b>64</b>, 479&ndash;498.
</p>
<p><em>&quot;smoother&quot; procedure:</em> Storey, J. D., and R. Tibshirani (2003)
Statistical significance for genome-wide experiments. 
<em>Proc. Nat. Acad. Sci. USA</em>, <b>100</b>, 9440-9445.
</p>
<p><em>&quot;quantile&quot; procedure:</em> similar to smoother, except that the lower q
quantile of all eta0 computed in dependence of lambda is taken as overall estimate of eta0.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fdrtool">fdrtool</a></code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'># load "fdrtool" library and p-values
library("fdrtool")
data(pvalues)


# Proportion of null p-values for different methods
pval.estimate.eta0(pvalues, method="conservative")
pval.estimate.eta0(pvalues, method="adaptive")
pval.estimate.eta0(pvalues, method="bootstrap")
pval.estimate.eta0(pvalues, method="smoother")
pval.estimate.eta0(pvalues, method="quantile")
</code></pre>

<hr>
<h2 id='pvalues'>Example p-Values</h2><span id='topic+pvalues'></span>

<h3>Description</h3>

<p>This data set contains 4,289 p-values.  These data are used to
illustrate the functionality of the functions <code><a href="#topic+fdrtool">fdrtool</a></code>
and <code><a href="#topic+pval.estimate.eta0">pval.estimate.eta0</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(pvalues)
</code></pre>


<h3>Format</h3>

<p><code>pvalues</code> is a vector with 4,289 p-values. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load fdrtool library
library("fdrtool")

# load data set
data(pvalues)

# estimate density and distribution function, 
# and compute corresponding (local) false discovery rates
fdrtool(pvalues, statistic="pvalue")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
