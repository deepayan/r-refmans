<!DOCTYPE html><html><head><title>Help for package bartMachine</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {bartMachine}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#automobile'><p>Data concerning automobile prices.</p></a></li>
<li><a href='#bart_machine_get_posterior'>
<p>Get Full Posterior Distribution</p></a></li>
<li><a href='#bart_machine_num_cores'>
<p>Get Number of Cores Used by BART</p></a></li>
<li><a href='#bart_predict_for_test_data'>
<p>Predict for Test Data with Known Outcomes</p></a></li>
<li><a href='#bartMachine'><p>Build a BART Model</p></a></li>
<li><a href='#bartMachineArr'>
<p>Create an array of BART models for the same data.</p></a></li>
<li><a href='#bartMachineCV'>
<p>Build BART-CV</p></a></li>
<li><a href='#benchmark_datasets'><p>benchmark_datasets</p></a></li>
<li><a href='#calc_credible_intervals'>
<p>Calculate Credible Intervals</p></a></li>
<li><a href='#calc_prediction_intervals'>
<p>Calculate Prediction Intervals</p></a></li>
<li><a href='#check_bart_error_assumptions'>
<p>Check BART Error Assumptions</p></a></li>
<li><a href='#cov_importance_test'>
<p>Importance Test for Covariate(s) of Interest</p></a></li>
<li><a href='#destroy_bart_machine'>
<p>Destroy BART Model (deprecated &mdash; do not use!)</p></a></li>
<li><a href='#dummify_data'>
<p>Dummify Design Matrix</p></a></li>
<li><a href='#extract_raw_node_data'>
<p>Gets Raw Node data</p></a></li>
<li><a href='#get_projection_weights'>
<p>Gets Training Sample Projection / Weights</p></a></li>
<li><a href='#get_sigsqs'>
<p>Get Posterior Error Variance Estimates</p></a></li>
<li><a href='#get_var_counts_over_chain'>
<p>Get the Variable Inclusion Counts</p></a></li>
<li><a href='#get_var_props_over_chain'>
<p>Get the Variable Inclusion Proportions</p></a></li>
<li><a href='#interaction_investigator'>
<p>Explore Pairwise Interactions in BART Model</p></a></li>
<li><a href='#investigate_var_importance'>
<p>Explore Variable Inclusion Proportions in BART Model</p></a></li>
<li><a href='#k_fold_cv'>
<p>Estimate Out-of-sample Error with K-fold Cross validation</p></a></li>
<li><a href='#linearity_test'>
<p>Test of Linearity</p></a></li>
<li><a href='#node_prediction_training_data_indices'>
<p>Gets node predictions indices of the training data for new data.</p></a></li>
<li><a href='#pd_plot'>
<p>Partial Dependence Plot</p></a></li>
<li><a href='#plot_convergence_diagnostics'>
<p>Plot Convergence Diagnostics</p></a></li>
<li><a href='#plot_y_vs_yhat'>
<p>Plot the fitted Versus Actual Response</p></a></li>
<li><a href='#predict_bartMachineArr'>
<p>Make a prediction on data using a BART array object</p></a></li>
<li><a href='#predict.bartMachine'>
<p>Make a prediction on data using a BART object</p></a></li>
<li><a href='#print.bartMachine'>
<p>Summarizes information about a <code>bartMachine</code> object.</p></a></li>
<li><a href='#rmse_by_num_trees'>
<p>Assess the Out-of-sample RMSE by Number of Trees</p></a></li>
<li><a href='#set_bart_machine_num_cores'>
<p>Set the Number of Cores for BART</p></a></li>
<li><a href='#summary.bartMachine'>
<p>Summarizes information about a <code>bartMachine</code> object.</p></a></li>
<li><a href='#var_selection_by_permute'>
<p>Perform Variable Selection using Three Threshold-based Procedures</p></a></li>
<li><a href='#var_selection_by_permute_cv'>
<p>Perform Variable Selection Using Cross-validation Procedure</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Bayesian Additive Regression Trees</td>
</tr>
<tr>
<td>Version:</td>
<td>1.3.4.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-6-25</td>
</tr>
<tr>
<td>Author:</td>
<td>Adam Kapelner and Justin Bleich (R package)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Adam Kapelner &lt;kapelner@qc.cuny.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>An advanced implementation of Bayesian Additive Regression Trees with expanded features for data analysis and visualization.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.14.0), rJava (&ge; 0.9-8), bartMachineJARs (&ge; 1.2.1),
randomForest, missForest</td>
</tr>
<tr>
<td>Imports:</td>
<td>graphics, grDevices, stats</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>Java (&gt;= 8.0)</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-07-06 19:11:19 UTC; kapel</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-07-06 23:50:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='automobile'>Data concerning automobile prices.</h2><span id='topic+automobile'></span>

<h3>Description</h3>

<p>The <code>automobile</code> data frame has 201 rows and 25 columns and
concerns automobiles in the 1985 Auto Imports Database. The response 
variable, <code>price</code>, is the log selling price of the automobile. There
are 7 categorical predictors and 17 continuous / integer predictors which
are features of the automobiles. 41 automobiles have missing data in one
or more of the feature entries. This dataset is true to the original except 
with a few of the predictors dropped.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(automobile)
</code></pre>


<h3>Source</h3>

<p>K Bache and M Lichman. UCI machine learning repository, 2013. 
http://archive.ics.uci.edu/ml/datasets/Automobile
</p>

<hr>
<h2 id='bart_machine_get_posterior'>
Get Full Posterior Distribution
</h2><span id='topic+bart_machine_get_posterior'></span>

<h3>Description</h3>

<p>Generates draws from posterior distribution of <code class="reqn">\hat{f}(x)</code> for a specified set of observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bart_machine_get_posterior(bart_machine, new_data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bart_machine_get_posterior_+3A_bart_machine">bart_machine</code></td>
<td>

<p>An object of class &ldquo;bartMachine&rdquo;.
</p>
</td></tr>
<tr><td><code id="bart_machine_get_posterior_+3A_new_data">new_data</code></td>
<td>

<p>A data frame containing observations at which draws from posterior distribution of <code class="reqn">\hat{f}(x)</code> are to be obtained.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with the following components:

</p>
<table>
<tr><td><code>y_hat</code></td>
<td>
<p>Posterior mean estimates. For regression, the estimates have the same units as the response. For classification, the estimates are probabilities.</p>
</td></tr>
<tr><td><code>new_data</code></td>
<td>
<p>The data frame with rows at which the posterior draws are to be generated. Column names should match that of the training data.</p>
</td></tr>
<tr><td><code>y_hat_posterior_samples</code></td>
<td>
<p>The full set of posterior samples of size <code>num_iterations_after_burn_in</code> for each observation. For regression, the estimates have the same units as the response. For classification, the estimates are probabilities.</p>
</td></tr>

</table>


<h3>Note</h3>

<p>This function is parallelized by the number of cores set in <code><a href="#topic+set_bart_machine_num_cores">set_bart_machine_num_cores</a></code>.
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>See Also</h3>

<p><code><a href="#topic+calc_credible_intervals">calc_credible_intervals</a></code>, <code><a href="#topic+calc_prediction_intervals">calc_prediction_intervals</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#Regression example

#generate Friedman data
set.seed(11)
n  = 200 
p = 5
X = data.frame(matrix(runif(n * p), ncol = p))
y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)

##build BART regression model
bart_machine = bartMachine(X, y)

#get posterior distribution
posterior = bart_machine_get_posterior(bart_machine, X)
print(posterior$y_hat)


#Classification example

#get data and only use 2 factors
data(iris)
iris2 = iris[51:150,]
iris2$Species = factor(iris2$Species)

#build BART classification model
bart_machine = bartMachine(iris2[ ,1 : 4], iris2$Species)

#get posterior distribution
posterior = bart_machine_get_posterior(bart_machine, iris2[ ,1 : 4])
print(posterior$y_hat)

## End(Not run)


</code></pre>

<hr>
<h2 id='bart_machine_num_cores'>
Get Number of Cores Used by BART
</h2><span id='topic+bart_machine_num_cores'></span>

<h3>Description</h3>

<p>Returns number of cores used by BART
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bart_machine_num_cores()
</code></pre>


<h3>Details</h3>

<p>Returns the number of cores currently being used by parallelized BART functions
</p>


<h3>Value</h3>



<p>Number of cores currently being used by parallelized BART functions.


</p>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>See Also</h3>

<p><code><a href="#topic+set_bart_machine_num_cores">set_bart_machine_num_cores</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
bart_machine_num_cores()

## End(Not run)
</code></pre>

<hr>
<h2 id='bart_predict_for_test_data'>
Predict for Test Data with Known Outcomes
</h2><span id='topic+bart_predict_for_test_data'></span>

<h3>Description</h3>

<p>Utility wrapper function for computing out-of-sample metrics for a BART model when the test set outcomes are known.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bart_predict_for_test_data(bart_machine, Xtest, ytest, prob_rule_class = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bart_predict_for_test_data_+3A_bart_machine">bart_machine</code></td>
<td>

<p>An object of class &ldquo;bartMachine&rdquo;.
</p>
</td></tr>
<tr><td><code id="bart_predict_for_test_data_+3A_xtest">Xtest</code></td>
<td>

<p>Data frame for test data containing rows at which predictions are to be made. Colnames should match that of the training data.
</p>
</td></tr>
<tr><td><code id="bart_predict_for_test_data_+3A_ytest">ytest</code></td>
<td>

<p>Actual outcomes for test data.
</p>
</td></tr>
<tr><td><code id="bart_predict_for_test_data_+3A_prob_rule_class">prob_rule_class</code></td>
<td>

<p>Threshold for classification.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For regression models, a list with the following components is returned:
</p>
<table>
<tr><td><code>y_hat</code></td>
<td>
<p>Predictions (as posterior means) for the test observations.</p>
</td></tr>
<tr><td><code>L1_err</code></td>
<td>
<p>L1 error for predictions.</p>
</td></tr>
<tr><td><code>L2_err</code></td>
<td>
<p>L2 error for predictions.</p>
</td></tr>
<tr><td><code>rmse</code></td>
<td>
<p>RMSE for predictions.</p>
</td></tr>
</table>
<p>For classification models, a list with the following components is returned: 
</p>
<table>
<tr><td><code>y_hat</code></td>
<td>
<p>Class predictions for the test observations.</p>
</td></tr>
<tr><td><code>p_hat</code></td>
<td>
<p>Probability estimates for the test observations.</p>
</td></tr>
<tr><td><code>confusion_matrix</code></td>
<td>
<p>A confusion matrix for the test observations.</p>
</td></tr>

</table>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+predict">predict</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#generate Friedman data
set.seed(11)
n  = 250 
p = 5
X = data.frame(matrix(runif(n * p), ncol = p))
y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)

##split into train and test
train_X = X[1 : 200, ]
test_X = X[201 : 250, ]
train_y = y[1 : 200]
test_y = y[201 : 250]

##build BART regression model
bart_machine = bartMachine(train_X, train_y)

#explore performance on test data
oos_perf = bart_predict_for_test_data(bart_machine, test_X, test_y)
print(oos_perf$rmse)

## End(Not run)
</code></pre>

<hr>
<h2 id='bartMachine'>Build a BART Model</h2><span id='topic+bartMachine'></span><span id='topic+build_bart_machine'></span>

<h3>Description</h3>

<p>Builds a BART model for regression or classification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bartMachine(X = NULL, y = NULL, Xy = NULL, 
num_trees = 50, 
num_burn_in = 250, 
num_iterations_after_burn_in = 1000, 
alpha = 0.95, beta = 2, k = 2, q = 0.9, nu = 3, 
prob_rule_class = 0.5, 
mh_prob_steps = c(2.5, 2.5, 4)/9,
debug_log = FALSE, 
run_in_sample = TRUE,  
s_sq_y = "mse",
sig_sq_est = NULL,
print_tree_illustrations = FALSE,
cov_prior_vec = NULL, 
interaction_constraints = NULL,
use_missing_data = FALSE, 
covariates_to_permute = NULL,
num_rand_samps_in_library = 10000, 
use_missing_data_dummies_as_covars = FALSE, 
replace_missing_data_with_x_j_bar = FALSE,
impute_missingness_with_rf_impute = FALSE,
impute_missingness_with_x_j_bar_for_lm = TRUE,
mem_cache_for_speed = TRUE,
flush_indices_to_save_RAM = TRUE,
serialize = FALSE,
seed = NULL,
verbose = TRUE)

build_bart_machine(X = NULL, y = NULL, Xy = NULL, 
num_trees = 50, 
num_burn_in = 250, 
num_iterations_after_burn_in = 1000, 
alpha = 0.95, beta = 2, k = 2, q = 0.9, nu = 3, 
prob_rule_class = 0.5, 
mh_prob_steps = c(2.5, 2.5, 4)/9,
debug_log = FALSE, 
run_in_sample = TRUE,  
s_sq_y = "mse",
sig_sq_est = NULL,
print_tree_illustrations = FALSE,
cov_prior_vec = NULL, 
interaction_constraints = NULL,
use_missing_data = FALSE, 
covariates_to_permute = NULL,
num_rand_samps_in_library = 10000, 
use_missing_data_dummies_as_covars = FALSE, 
replace_missing_data_with_x_j_bar = FALSE,
impute_missingness_with_rf_impute = FALSE,
impute_missingness_with_x_j_bar_for_lm = TRUE,
mem_cache_for_speed = TRUE,
flush_indices_to_save_RAM = TRUE,
serialize = FALSE,
seed = NULL,
verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bartMachine_+3A_x">X</code></td>
<td>

<p>Data frame of predictors. Factors are automatically converted to dummies internally. 
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_y">y</code></td>
<td>

<p>Vector of response variable. If <code>y</code> is <code>numeric</code> or <code>integer</code>, a BART model for regression is built. If <code>y</code> is a factor with two levels, a BART model for classification is built.
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_xy">Xy</code></td>
<td>

<p>A data frame of predictors and the response. The response column must be named &ldquo;y&rdquo;. 
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_num_trees">num_trees</code></td>
<td>

<p>The number of trees to be grown in the sum-of-trees model.
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_num_burn_in">num_burn_in</code></td>
<td>

<p>Number of MCMC samples to be discarded as &ldquo;burn-in&rdquo;.
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_num_iterations_after_burn_in">num_iterations_after_burn_in</code></td>
<td>

<p>Number of MCMC samples to draw from the posterior distribution of <code class="reqn">\hat{f}(x)</code>. 
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_alpha">alpha</code></td>
<td>

<p>Base hyperparameter in tree prior for whether a node is nonterminal or not.
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_beta">beta</code></td>
<td>

<p>Power hyperparameter in tree prior for whether a node is nonterminal or not.
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_k">k</code></td>
<td>

<p>For regression, <code>k</code> determines the prior probability that <code class="reqn">E(Y|X)</code> is contained in the interval <code class="reqn">(y_{min}, y_{max})</code>, based on a normal distribution. For example, when <code class="reqn">k=2</code>, the prior probability is 95%. For classification, <code>k</code> determines the prior probability that <code class="reqn">E(Y|X)</code> is between <code class="reqn">(-3,3)</code>. Note that a larger value of <code>k</code> results in more shrinkage and a more conservative fit. 
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_q">q</code></td>
<td>

<p>Quantile of the prior on the error variance at which the data-based estimate is placed. Note that the larger the value of <code>q</code>, the more aggressive the fit as you are placing more prior weight on values lower than the data-based estimate. Not used for classification.
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_nu">nu</code></td>
<td>

<p>Degrees of freedom for the inverse <code class="reqn">\chi^2</code> prior. Not used for classification.
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_prob_rule_class">prob_rule_class</code></td>
<td>

<p>Threshold for classification. Any observation with a conditional probability greater than <code>prob_class_rule</code> is assigned the &ldquo;positive&rdquo; outcome. Note that the first level of the response is treated as the &ldquo;positive&rdquo; outcome and the second is treated as the &ldquo;negative&rdquo; outcome.  
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_mh_prob_steps">mh_prob_steps</code></td>
<td>

<p>Vector of prior probabilities for proposing changes to the tree structures: (GROW, PRUNE, CHANGE)
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_debug_log">debug_log</code></td>
<td>

<p>If TRUE, additional information about the model construction are printed to a file in the working directory.
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_run_in_sample">run_in_sample</code></td>
<td>

<p>If TRUE, in-sample statistics such as <code class="reqn">\hat{f}(x)</code>, Pseudo-<code class="reqn">R^2</code>, and RMSE are computed. Setting this to FALSE when not needed can decrease computation time. 
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_s_sq_y">s_sq_y</code></td>
<td>

<p>If &ldquo;mse&rdquo;, a data-based estimated of the error variance is computed as the MSE from ordinary least squares regression. If &ldquo;var&rdquo;., the data-based estimate is computed as the variance of the response. Not used in classification. 
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_sig_sq_est">sig_sq_est</code></td>
<td>

<p>Pass in an estimate of the maximum sig_sq of the model. This is useful to cache somewhere and then pass in during cross-validation since the default method of estimation is a linear model. In large dimensions, linear model estimation is slow.
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_print_tree_illustrations">print_tree_illustrations</code></td>
<td>

<p>For every Gibbs iteration, print out an illustration of the trees side-by-side. This is excruciatingly SLOW!
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_cov_prior_vec">cov_prior_vec</code></td>
<td>

<p>Vector assigning relative weights to how often a particular variable should be proposed as a candidate for a split. The vector is internally normalized so that the weights sum to 1. Note that the length of this vector must equal the length of the design matrix after dummification and augmentation of indicators of missingness (if used). To see what the dummified matrix looks like, use <code><a href="#topic+dummify_data">dummify_data</a></code>. See Bleich et al. (2013) for more details on when this feature is most appropriate. 
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_interaction_constraints">interaction_constraints</code></td>
<td>

<p>A list of vectors indicating where the vectors are sets of elements allowed to interact with one another. The elements in each 
vector correspond to features in the data frame <code>X</code> specified by either the column number as a numeric value or the column 
name as a string e.g. <code>list(c(1, 2), c("nox", "rm"))</code>. The elements of the vectors can be reused among components for any 
level of interaction complexity you wish. Default is <code>NULL</code> which corresponds to the vanilla modeling procedure where
all interactions are legal. For a pure generalized added model, use <code>as.list(seq(1 : p))</code> where <code>p</code> 
is the number of columns in the design matrix <code>X</code>.
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_use_missing_data">use_missing_data</code></td>
<td>

<p>If TRUE, the missing data feature is used to automatically handle missing data without imputation. See Kapelner and Bleich (2013) for details. 
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_covariates_to_permute">covariates_to_permute</code></td>
<td>

<p>Private argument for <code><a href="#topic+cov_importance_test">cov_importance_test</a></code>. Not needed by user. 
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_num_rand_samps_in_library">num_rand_samps_in_library</code></td>
<td>

<p>Before building a BART model, samples from the Standard Normal and <code class="reqn">\chi^2(\nu)</code> are drawn to be used in the MCMC steps. This parameter determines the number of samples to be taken.  
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_use_missing_data_dummies_as_covars">use_missing_data_dummies_as_covars</code></td>
<td>

<p>If TRUE, additional indicator variables for whether or not an observation in a particular column is missing are included. See Kapelner and Bleich (2013) for details.
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_replace_missing_data_with_x_j_bar">replace_missing_data_with_x_j_bar</code></td>
<td>

<p>If TRUE ,missing entries in <code>X</code> are imputed with average value or modal category.
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_impute_missingness_with_rf_impute">impute_missingness_with_rf_impute</code></td>
<td>

<p>If TRUE, missing entries are filled in using the rf.impute() function from the <code>randomForest</code> library. 
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_impute_missingness_with_x_j_bar_for_lm">impute_missingness_with_x_j_bar_for_lm</code></td>
<td>

<p>If TRUE, when computing the data-based estimate of <code class="reqn">\sigma^2</code>, missing entries are imputed with average value or modal category.
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_mem_cache_for_speed">mem_cache_for_speed</code></td>
<td>

<p>Speed enhancement that caches the predictors and the split values that are available at each node for selecting new rules. If the number
of predictors is large, the memory requirements become large. We recommend keeping this on (default) and turning it off if you experience out-of-memory errors.  
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_flush_indices_to_save_ram">flush_indices_to_save_RAM</code></td>
<td>

<p>Setting this flag to <code>TRUE</code> saves memory with the downside that you cannot use the functions <code>node_prediction_training_data_indices</code> nor <code>get_projection_weights</code>.
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_serialize">serialize</code></td>
<td>

<p>Setting this option to <code>TRUE</code> will allow serialization of bartMachine objects which allows for persistence between
R sessions if the object is saved and reloaded. Note that serialized objects can take up a large amount of memory. 
Thus, the default is <code>FALSE</code>.  
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_seed">seed</code></td>
<td>

<p>Optional: sets the seed in both R and Java. Default is <code>NULL</code> which does not set the seed in R nor Java. 
Setting the seed enforces deterministic behavior only in the case when one core is used (the default before 
<code>set_bart_machine_num_cores() was invoked</code>.
</p>
</td></tr>
<tr><td><code id="bartMachine_+3A_verbose">verbose</code></td>
<td>

<p>Prints information about progress of the algorithm to the screen. 
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an object of class &ldquo;bartMachine&rdquo;. The &ldquo;bartMachine&rdquo; object contains a list of the following components:
</p>
<table>
<tr><td><code>java_bart_machine</code></td>
<td>
<p>A pointer to the BART Java object.</p>
</td></tr>
<tr><td><code>train_data_features</code></td>
<td>
<p>The names of the variables used in the training data.</p>
</td></tr>
<tr><td><code>training_data_features_with_missing_features.</code></td>
<td>
<p>The names of the variables used in the training data. If <code>use_missing_data_dummies_as_covars = TRUE</code>, this also includes dummies for any predictors that contain at least one missing entry (named &ldquo;M_&lt;feature&gt;&rdquo;).</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>The values of the response for the training data.</p>
</td></tr>
<tr><td><code>y_levels</code></td>
<td>
<p>The levels of the response (for classification only).</p>
</td></tr>
<tr><td><code>pred_type</code></td>
<td>
<p>Whether the model was build for regression of classification.</p>
</td></tr>
<tr><td><code>model_matrix_training_data</code></td>
<td>
<p>The training data with factors converted to dummies.</p>
</td></tr>
<tr><td><code>num_cores</code></td>
<td>
<p>The number of cores used to build the BART model.</p>
</td></tr>
<tr><td><code>sig_sq_est</code></td>
<td>
<p>The data-based estimate of <code class="reqn">\sigma^2</code> used to create the prior on the error variance for the BART model.</p>
</td></tr>
<tr><td><code>time_to_build</code></td>
<td>
<p>Total time to build the BART model.</p>
</td></tr>
<tr><td><code>y_hat_train</code></td>
<td>
<p>The posterior means of <code class="reqn">\hat{f}(x)</code> for each observation. Only returned if <code>run_in_sample = TRUE</code>.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>The model residuals given by <code>y</code> - <code>y_hat_train</code>. Only returned if <code>run_in_sample = TRUE</code>.</p>
</td></tr>
<tr><td><code>L1_err_train</code></td>
<td>
<p>L1 error on the training set. Only returned if <code>run_in_sample = TRUE</code>.</p>
</td></tr>
<tr><td><code>L2_err_train</code></td>
<td>
<p>L2 error on the training set. Only returned if <code>run_in_sample = TRUE</code>.</p>
</td></tr>
<tr><td><code>PseudoRsq</code></td>
<td>
<p>Calculated as 1 - SSE / SST where SSE is the sum of square errors in the training data and SST is the sample variance of the response times <code class="reqn">n-1</code>. Only returned if <code>run_in_sample = TRUE</code>.</p>
</td></tr>
<tr><td><code>rmse_train</code></td>
<td>
<p>Root mean square error on the training set. Only returned if <code>run_in_sample = TRUE</code>.</p>
</td></tr>
</table>
<p>Additionally, the parameters passed to the function <code>bartMachine</code> are also components of the list. 
</p>


<h3>Note</h3>

<p>This function is parallelized by the number of cores set by <code><a href="#topic+set_bart_machine_num_cores">set_bart_machine_num_cores</a></code>. Each core will create an 
independent MCMC chain of size <br />
<code>num_burn_in</code> <code class="reqn">+</code> <code>num_iterations_after_burn_in / bart_machine_num_cores</code>.
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>References</h3>

<p>Adam Kapelner, Justin Bleich (2016). bartMachine: Machine Learning
with Bayesian Additive Regression Trees. Journal of Statistical
Software, 70(4), 1-40. doi:10.18637/jss.v070.i04
</p>
<p>HA Chipman, EI George, and RE McCulloch. BART: Bayesian Additive Regressive Trees.
The Annals of Applied Statistics, 4(1): 266&ndash;298, 2010.
</p>
<p>A Kapelner and J Bleich. Prediction with Missing Data via Bayesian Additive Regression
Trees. Canadian Journal of Statistics, 43(2): 224-239, 2015
</p>
<p>J Bleich, A Kapelner, ST Jensen, and EI George. Variable Selection Inference for Bayesian
Additive Regression Trees. ArXiv e-prints, 2013.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bartMachineCV">bartMachineCV</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
##regression example

##generate Friedman data
set.seed(11)
n  = 200 
p = 5
X = data.frame(matrix(runif(n * p), ncol = p))
y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)

##build BART regression model
bart_machine = bartMachine(X, y)
summary(bart_machine)

##Build another BART regression model
bart_machine = bartMachine(X,y, num_trees = 200, num_burn_in = 500,
num_iterations_after_burn_in = 1000)

##Classification example

#get data and only use 2 factors
data(iris)
iris2 = iris[51:150,]
iris2$Species = factor(iris2$Species)

#build BART classification model
bart_machine = build_bart_machine(iris2[ ,1:4], iris2$Species)

##get estimated probabilities
phat = bart_machine$p_hat_train
##look at in-sample confusion matrix
bart_machine$confusion_matrix

## End(Not run)




</code></pre>

<hr>
<h2 id='bartMachineArr'>
Create an array of BART models for the same data.
</h2><span id='topic+bartMachineArr'></span>

<h3>Description</h3>

<p>If BART creates models that are variable,
running many on the same dataset and averaging is a good strategy. 
This function is a convenience method for this procedure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bartMachineArr(bart_machine, R = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bartMachineArr_+3A_bart_machine">bart_machine</code></td>
<td>

<p>An object of class &ldquo;bartMachine&rdquo;.
</p>
</td></tr>
<tr><td><code id="bartMachineArr_+3A_r">R</code></td>
<td>

<p>The number of replicated BART models in the array.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>bartMachineArr</code> object which is just a list of the <code>R</code> bartMachine models.
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Regression example
## Not run: 
#generate Friedman data
set.seed(11)
n  = 200 
p = 5
X = data.frame(matrix(runif(n * p), ncol = p))
y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)

##build BART regression model
bart_machine = bartMachine(X, y)
bart_machine_arr = bartMachineArr(bart_machine)

#Classification example
data(iris)
iris2 = iris[51 : 150, ] #do not include the third type of flower for this example
iris2$Species = factor(iris2$Species)  
bart_machine = bartMachine(iris2[ ,1:4], iris2$Species)
bart_machine_arr = bartMachineArr(bart_machine)

## End(Not run)


</code></pre>

<hr>
<h2 id='bartMachineCV'>
Build BART-CV
</h2><span id='topic+bartMachineCV'></span><span id='topic+build_bart_machine_cv'></span>

<h3>Description</h3>

<p>Builds a BART-CV model by cross-validating over a grid of hyperparameter choices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bartMachineCV(X = NULL, y = NULL, Xy = NULL, 
num_tree_cvs = c(50, 200), k_cvs = c(2, 3, 5), 
nu_q_cvs = NULL, k_folds = 5, folds_vec = NULL, verbose = FALSE, ...)

build_bart_machine_cv(X = NULL, y = NULL, Xy = NULL, 
num_tree_cvs = c(50, 200), k_cvs = c(2, 3, 5), 
nu_q_cvs = NULL, k_folds = 5, folds_vec = NULL, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bartMachineCV_+3A_x">X</code></td>
<td>

<p>Data frame of predictors. Factors are automatically converted to dummies interally. 
</p>
</td></tr>
<tr><td><code id="bartMachineCV_+3A_y">y</code></td>
<td>

<p>Vector of response variable. If <code>y</code> is <code>numeric</code> or <code>integer</code>, a BART model for regression is built. If <code>y</code> is a factor with two levels, a BART model for classification is built.
</p>
</td></tr>
<tr><td><code id="bartMachineCV_+3A_xy">Xy</code></td>
<td>

<p>A data frame of predictors and the response. The response column must be named &ldquo;y&rdquo;. 
</p>
</td></tr>
<tr><td><code id="bartMachineCV_+3A_num_tree_cvs">num_tree_cvs</code></td>
<td>

<p>Vector of sizes for the sum-of-trees models to cross-validate over.
</p>
</td></tr>
<tr><td><code id="bartMachineCV_+3A_k_cvs">k_cvs</code></td>
<td>

<p>Vector of choices for the hyperparameter <code>k</code> to cross-validate over.
</p>
</td></tr>
<tr><td><code id="bartMachineCV_+3A_nu_q_cvs">nu_q_cvs</code></td>
<td>

<p>Only for regression. List of vectors containing (<code>nu</code>, <code>q</code>) ordered pair choices to cross-validate over. If <code>NULL</code>, then it defaults to the three values <code>list(c(3, 0.9), c(3, 0.99), c(10, 0.75))</code>.
</p>
</td></tr>
<tr><td><code id="bartMachineCV_+3A_k_folds">k_folds</code></td>
<td>

<p>Number of folds for cross-validation
</p>
</td></tr>
<tr><td><code id="bartMachineCV_+3A_folds_vec">folds_vec</code></td>
<td>

<p>An integer vector of indices specifying which fold each observation belongs to.   
</p>
</td></tr>
<tr><td><code id="bartMachineCV_+3A_verbose">verbose</code></td>
<td>

<p>Prints information about progress of the algorithm to the screen. 
</p>
</td></tr>
<tr><td><code id="bartMachineCV_+3A_...">...</code></td>
<td>

<p>Additional arguments to be passed to <code>bartMachine</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an object of class &ldquo;bartMachine&rdquo; with the set of hyperparameters chosen via cross-validation. We also return a matrix &ldquo;cv_stats&rdquo;
which contains the out-of-sample RMSE for each hyperparameter set tried and &ldquo;folds&rdquo; which gives the fold in which each observation fell across the k-folds.
</p>


<h3>Note</h3>

<p>This function may require significant run-time.
This function is parallelized by the number of cores set in <code><a href="#topic+set_bart_machine_num_cores">set_bart_machine_num_cores</a></code> via calling <code><a href="#topic+bartMachine">bartMachine</a></code>.
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>References</h3>

<p>Adam Kapelner, Justin Bleich (2016). bartMachine: Machine Learning
with Bayesian Additive Regression Trees. Journal of Statistical
Software, 70(4), 1-40. doi:10.18637/jss.v070.i04
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bartMachine">bartMachine</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#generate Friedman data
set.seed(11)
n  = 200 
p = 5
X = data.frame(matrix(runif(n * p), ncol = p))
y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)

##build BART regression model
bart_machine_cv = bartMachineCV(X, y)

#information about cross-validated model
summary(bart_machine_cv)

## End(Not run)

</code></pre>

<hr>
<h2 id='benchmark_datasets'>benchmark_datasets</h2><span id='topic+ankara'></span><span id='topic+baseball'></span><span id='topic+boston'></span><span id='topic+compactiv'></span><span id='topic+ozone'></span><span id='topic+pole'></span><span id='topic+triazine'></span><span id='topic+wine.red'></span><span id='topic+wine.white'></span>

<h3>Description</h3>

<p>Nine diverse datasets which were used for benchmarking bartMachine's out of sample performance in 
the vignette for this package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(benchmark_datasets)
</code></pre>


<h3>Source</h3>

<p>See vignette for details.
</p>

<hr>
<h2 id='calc_credible_intervals'>
Calculate Credible Intervals
</h2><span id='topic+calc_credible_intervals'></span>

<h3>Description</h3>

<p>Generates credible intervals for <code class="reqn">\hat{f}(x)</code> for a specified set of observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_credible_intervals(bart_machine, new_data, 
ci_conf = 0.95)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_credible_intervals_+3A_bart_machine">bart_machine</code></td>
<td>

<p>An object of class &ldquo;bartMachine&rdquo;.
</p>
</td></tr>
<tr><td><code id="calc_credible_intervals_+3A_new_data">new_data</code></td>
<td>

<p>A data frame containing observations at which credible intervals for <code class="reqn">\hat{f}(x)</code> are to be computed.
</p>
</td></tr>
<tr><td><code id="calc_credible_intervals_+3A_ci_conf">ci_conf</code></td>
<td>

<p>Confidence level for the credible intervals. The default is 95%.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This interval is the appropriate quantiles  based on the confidence level, <code>ci_conf</code>, of the predictions 
for each of the Gibbs samples post-burn in.
</p>


<h3>Value</h3>

<p>Returns a matrix of the lower and upper bounds of the credible intervals for each observation in <code>new_data</code>.
</p>


<h3>Note</h3>

<p>This function is parallelized by the number of cores set in <code><a href="#topic+set_bart_machine_num_cores">set_bart_machine_num_cores</a></code>.
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>See Also</h3>

<p><code><a href="#topic+calc_prediction_intervals">calc_prediction_intervals</a></code>, <code><a href="#topic+bart_machine_get_posterior">bart_machine_get_posterior</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
#generate Friedman data
set.seed(11)
n  = 200 
p = 5
X = data.frame(matrix(runif(n * p), ncol = p))
y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)

##build BART regression model
bart_machine = bartMachine(X, y)

#get credible interval
cred_int = calc_credible_intervals(bart_machine, X)
print(head(cred_int))

## End(Not run)
</code></pre>

<hr>
<h2 id='calc_prediction_intervals'>
Calculate Prediction Intervals
</h2><span id='topic+calc_prediction_intervals'></span>

<h3>Description</h3>

<p>Generates prediction intervals for <code class="reqn">\hat{y}</code> for a specified set of observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_prediction_intervals(bart_machine, new_data, 
pi_conf = 0.95, num_samples_per_data_point = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calc_prediction_intervals_+3A_bart_machine">bart_machine</code></td>
<td>

<p>An object of class &ldquo;bartMachine&rdquo;.
</p>
</td></tr>
<tr><td><code id="calc_prediction_intervals_+3A_new_data">new_data</code></td>
<td>

<p>A data frame containing observations at which prediction intervals for <code class="reqn">\hat{y}</code> are to be computed.
</p>
</td></tr>
<tr><td><code id="calc_prediction_intervals_+3A_pi_conf">pi_conf</code></td>
<td>

<p>Confidence level for the prediction intervals. The default is 95%.
</p>
</td></tr>
<tr><td><code id="calc_prediction_intervals_+3A_num_samples_per_data_point">num_samples_per_data_point</code></td>
<td>

<p>The number of samples taken from the predictive distribution. The default is 1000. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Credible intervals (see <code><a href="#topic+calc_credible_intervals">calc_credible_intervals</a></code>) are the appropriate quantiles of the prediction 
for each of the Gibbs samples post-burn in. Prediction intervals also make use of the noise estimate at each Gibbs
sample and hence are wider. For each Gibbs sample, we record the <code class="reqn">\hat{y}</code> estimate of the response and the 
<code class="reqn">\hat{\sigma^2}</code> estimate of the noise variance. We then sample <code>normal_samples_per_gibbs_sample</code> times
from a <code class="reqn">N(\hat{y}, \hat{\sigma^2})</code> random variable to simulate many possible disturbances for that Gibbs sample.
Then, all <code>normal_samples_per_gibbs_sample</code> times the number of Gibbs sample post burn-in are collected and the 
appropriate quantiles are taken based on the confidence level, <code>pi_conf</code>.
</p>


<h3>Value</h3>

<p>Returns a matrix of the lower and upper bounds of the prediction intervals for each observation in <code>new_data</code>.
</p>


<h3>Note</h3>

<p>This function is parallelized by the number of cores set in <code><a href="#topic+set_bart_machine_num_cores">set_bart_machine_num_cores</a></code>.
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>References</h3>

<p>Adam Kapelner, Justin Bleich (2016). bartMachine: Machine Learning
with Bayesian Additive Regression Trees. Journal of Statistical
Software, 70(4), 1-40. doi:10.18637/jss.v070.i04
</p>


<h3>See Also</h3>

<p><code><a href="#topic+calc_credible_intervals">calc_credible_intervals</a></code>, <code><a href="#topic+bart_machine_get_posterior">bart_machine_get_posterior</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#generate Friedman data
set.seed(11)
n  = 200 
p = 5
X = data.frame(matrix(runif(n * p), ncol = p))
y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)

##build BART regression model
bart_machine = bartMachine(X, y)

#get prediction interval
pred_int = calc_prediction_intervals(bart_machine, X)
print(head(pred_int))

## End(Not run)

</code></pre>

<hr>
<h2 id='check_bart_error_assumptions'>
Check BART Error Assumptions
</h2><span id='topic+check_bart_error_assumptions'></span>

<h3>Description</h3>

<p>Diagnostic tools to assess whether the errors of the BART model for regression are normally distributed and homoskedastic, as assumed by the model. This function generates a normal quantile plot of the residuals with a Shapiro-Wilks p-value as well as a residual plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_bart_error_assumptions(bart_machine, hetero_plot = "yhats")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_bart_error_assumptions_+3A_bart_machine">bart_machine</code></td>
<td>

<p>An object of class &ldquo;bartMachine&rdquo;.
</p>
</td></tr>
<tr><td><code id="check_bart_error_assumptions_+3A_hetero_plot">hetero_plot</code></td>
<td>

<p>If &ldquo;yhats&rdquo;, the residuals are plotted against the fitted values of the response. If &ldquo;ys&rdquo;, the residuals are plotted against the actual values of the response.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None.
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot_convergence_diagnostics">plot_convergence_diagnostics</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#generate Friedman data
set.seed(11)
n  = 300 
p = 5
X = data.frame(matrix(runif(n * p), ncol = p))
y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)

##build BART regression model
bart_machine = bartMachine(X, y)

#check error diagnostics
check_bart_error_assumptions(bart_machine)

## End(Not run)

</code></pre>

<hr>
<h2 id='cov_importance_test'>
Importance Test for Covariate(s) of Interest
</h2><span id='topic+cov_importance_test'></span>

<h3>Description</h3>

<p>This function tests the null hypothesis <code class="reqn">H_0</code>: These covariates of interest
do not affect the response under the assumptions of the BART 
model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cov_importance_test(bart_machine, covariates = NULL, 
num_permutation_samples = 100, plot = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cov_importance_test_+3A_bart_machine">bart_machine</code></td>
<td>

<p>An object of class &ldquo;bart_machine&rdquo;.
</p>
</td></tr>
<tr><td><code id="cov_importance_test_+3A_covariates">covariates</code></td>
<td>

<p>A vector of names of covariates of interest to be tested for having an effect on the response. A value of NULL
indicates an omnibus test for all covariates having an effect on the response. If the name of a covariate is a factor,
the entire factor will be permuted. We do not recommend entering the names of factor covariate dummies.
</p>
</td></tr>
<tr><td><code id="cov_importance_test_+3A_num_permutation_samples">num_permutation_samples</code></td>
<td>

<p>The number of times to permute the covariates of interest and create a corresponding new BART model (see details).
</p>
</td></tr>
<tr><td><code id="cov_importance_test_+3A_plot">plot</code></td>
<td>

<p>If <code>TRUE</code>, this produces a histogram of the Pseudo-Rsq's / total misclassifcation error rates from
the <code>num_permutations</code> BART models created with the <code>covariates</code> permuted. The plot also illustrates
the observed Pseudo-Rsq's / total misclassifcation error rate from the original training data and indicates
the test's p-value.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To test the importance of a covariate or a set of covariates of interest on the response, this function generates 
<code>num_permutations</code> BART models with the covariate(s) of interest permuted (differently each time). 
On each run, a measure of fit is recorded. For regression, the metric is Pseudo-Rsq; for classification, it is
total misclassification error.<br /> A 
p-value can then be generated as follows. For regression, the p-value is the number of 
permutation-sampled Pseudo-Rsq's greater than the observed Pseudo-Rsq divided by 
<code>num_permutations + 1</code>. For classification, the p-value is the number of permutation-sampled 
total misclassification errors less than the observed total misclassification error divided by <code>num_permutations + 1</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>permutation_samples_of_error</code></td>
<td>
<p>A vector which records the error metric of the BART models with the covariates permuted (see details).</p>
</td></tr>
<tr><td><code>observed_error_estimate</code></td>
<td>
<p>For regression, this is the Pseudo-Rsq on the original
training data set. For classification, this is the observed total misclassification error
on the original training data set.</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>The approximate p-value for this test (see details). 
</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function is parallelized by the number of cores set in <code><a href="#topic+set_bart_machine_num_cores">set_bart_machine_num_cores</a></code>.
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>References</h3>

<p>Adam Kapelner, Justin Bleich (2016). bartMachine: Machine Learning
with Bayesian Additive Regression Trees. Journal of Statistical
Software, 70(4), 1-40. doi:10.18637/jss.v070.i04
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
##regression example

##generate Friedman data
set.seed(11)
n  = 200 
p = 5
X = data.frame(matrix(runif(n * p), ncol = p))
y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)

##build BART regression model
bart_machine = bartMachine(X, y)

##now test if X[, 1] affects Y nonparametrically under the BART model assumptions
cov_importance_test(bart_machine, covariates = c(1))
## note the plot and the printed p-value


## End(Not run)

</code></pre>

<hr>
<h2 id='destroy_bart_machine'>
Destroy BART Model (deprecated &mdash; do not use!)
</h2><span id='topic+destroy_bart_machine'></span>

<h3>Description</h3>

<p>A deprecated function that previously was responsible for cleaning up the RAM 
associated with a BART model. This is now handled natively by R's garbage collection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>destroy_bart_machine(bart_machine)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="destroy_bart_machine_+3A_bart_machine">bart_machine</code></td>
<td>

<p>deprecated &mdash; do not use!
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Removing a &ldquo;bart_machine&rdquo; object from <code>R</code> previously did not free heap space from Java. 
Since BART objects can consume a large amount of RAM, it is important to remove 
these objects by calling this function if they are no longer needed or many BART 
objects are being created. This operation is now taken care of by R's garbage collection.
This function is deprecated and should not be used. However, running it is harmless.
</p>


<h3>Value</h3>

<p>None.
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>Examples</h3>

<pre><code class='language-R'>##None
</code></pre>

<hr>
<h2 id='dummify_data'>
Dummify Design Matrix
</h2><span id='topic+dummify_data'></span>

<h3>Description</h3>

<p>Create a data frame with factors converted to dummies. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dummify_data(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dummify_data_+3A_data">data</code></td>
<td>

<p>Data frame to be dummified. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The column names of the dummy variables are given by the &ldquo;FactorName_LevelName&rdquo; and are augmented to the end of the design matrix. See the example below.
</p>


<h3>Value</h3>

<p>Returns a data frame with factors converted to dummy indicator variables.
</p>


<h3>Note</h3>

<p>BART handles dummification internally. This function is provided as a utility function. 
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#generate data
set.seed(11)
x1 = rnorm(20)
x2 = as.factor(ifelse(x1 &gt; 0, "A", "B"))
x3 = runif(20)
X = data.frame(x1,x2,x3)
#dummify data
X_dummified = dummify_data(X)
print(X_dummified)

## End(Not run)
</code></pre>

<hr>
<h2 id='extract_raw_node_data'>
Gets Raw Node data
</h2><span id='topic+extract_raw_node_data'></span>

<h3>Description</h3>

<p>Returns a list object that contains all the information for all trees in a given Gibbs sample. Daughter nodes are nested
in the list structure recursively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_raw_node_data(bart_machine, g = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_raw_node_data_+3A_bart_machine">bart_machine</code></td>
<td>

<p>An object of class &ldquo;bartMachine&rdquo;.
</p>
</td></tr>
<tr><td><code id="extract_raw_node_data_+3A_g">g</code></td>
<td>

<p>The gibbs sample number. It must be a natural number between 1 and the number of iterations after burn in. Default is 1.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list object that contains all the information for all trees in a given Gibbs sample.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
options(java.parameters = "-Xmx10g")
pacman::p_load(bartMachine)

seed = 1984
set.seed(seed)
n = 100
x = rnorm(n, 0, 1)
sigma = 0.1
y = x + rnorm(n, 0, sigma)

num_trees = 200
num_iterations_after_burn_in = 1000
bart_mod = bartMachine(data.frame(x = x), y, 
	flush_indices_to_save_RAM = FALSE, 
	num_trees = num_trees, 
	num_iterations_after_burn_in = num_iterations_after_burn_in, 
	seed = seed)

raw_node_data = extract_raw_node_data(bart_mod)


## End(Not run)
</code></pre>

<hr>
<h2 id='get_projection_weights'>
Gets Training Sample Projection / Weights
</h2><span id='topic+get_projection_weights'></span>

<h3>Description</h3>

<p>Returns the matrix H where yhat is approximately equal to H y where yhat is the predicted values for <code>new_data</code>. If <code>new_data</code> is unspecified, yhat will be the in-sample fits.
If BART was the same as OLS, H would be an orthogonal projection matrix. Here it is a projection matrix, but clearly non-orthogonal. Unfortunately, I cannot get
this function to work correctly because of three possible reasons (1) BART does not work by averaging tree predictions: it is a sum of trees model where each tree sees the residuals
via backfitting (2) the prediction in each node is a bayesian posterior draw which is close to ybar of the observations contained in the node if noise is gauged to be small and 
(3) there are transformations of the original y variable. I believe I got close and I think I'm off by a constant multiple which is a function of the number of trees. I can
use regression to estimate the constant multiple and correct for it. Turn <code>regression_kludge</code> to <code>TRUE</code> for this. Note that the weights do not add up to one here.
The intuition is because due to the backfitting there is multiple counting. But I'm not entirely sure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_projection_weights(bart_machine, new_data = NULL, regression_kludge = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_projection_weights_+3A_bart_machine">bart_machine</code></td>
<td>

<p>An object of class &ldquo;bartMachine&rdquo;.
</p>
</td></tr>
<tr><td><code id="get_projection_weights_+3A_new_data">new_data</code></td>
<td>

<p>Data that you wish to investigate the training sample projection / weights. If <code>NULL</code>, the original training data is used.
</p>
</td></tr>
<tr><td><code id="get_projection_weights_+3A_regression_kludge">regression_kludge</code></td>
<td>

<p>See explanation in the description. Default is <code>FALSE</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a matrix of proportions with number of rows equal to the number of rows of <code>new_data</code> and number of columns equal to the number of rows of the original training data, n.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
options(java.parameters = "-Xmx10g")
pacman::p_load(bartMachine, tidyverse)

seed = 1984
set.seed(seed)
n = 100
x = rnorm(n, 0, 1)
sigma = 0.1
y = x + rnorm(n, 0, sigma)

num_trees = 200
num_iterations_after_burn_in = 1000
bart_mod = bartMachine(data.frame(x = x), y, 
	flush_indices_to_save_RAM = FALSE, 
	num_trees = num_trees, 
	num_iterations_after_burn_in = num_iterations_after_burn_in, 
	seed = seed)
bart_mod

n_star = 100
x_star = rnorm(n_star)
y_star = as.numeric(x_star + rnorm(n_star, 0, sigma))
yhat_star_bart = predict(bart_mod, data.frame(x = x_star))

Hstar = get_projection_weights(bart_mod, data.frame(x = x_star))
rowSums(Hstar)
yhat_star_projection = as.numeric(Hstar 

ggplot(data.frame(
	yhat_star = yhat_star_bart, 
	yhat_star_projection = yhat_star_projection, 
	y_star = y_star)) +
  geom_point(aes(x = yhat_star_bart, y = yhat_star_projection), col = "green") + 
  geom_abline(slope = 1, intercept = 0)

Hstar = get_projection_weights(bart_mod, data.frame(x = x_star), regression_kludge = TRUE)
rowSums(Hstar)
yhat_star_projection = as.numeric(Hstar 

ggplot(data.frame(
	yhat_star = yhat_star_bart, 
	yhat_star_projection = yhat_star_projection, 
	y_star = y_star)) +
  geom_point(aes(x = yhat_star_bart, y = yhat_star_projection), col = "green") + 
  geom_abline(slope = 1, intercept = 0)


## End(Not run)
</code></pre>

<hr>
<h2 id='get_sigsqs'>
Get Posterior Error Variance Estimates 
</h2><span id='topic+get_sigsqs'></span>

<h3>Description</h3>

<p>Returns the posterior estimates of the error variance from the Gibbs samples with an option to create a histogram of the posterior estimates of the error variance  with a credible interval overlaid.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_sigsqs(bart_machine, after_burn_in = T, 
plot_hist = F, plot_CI = .95, plot_sigma = F)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_sigsqs_+3A_bart_machine">bart_machine</code></td>
<td>

<p>An object of class &ldquo;bartMachine&rdquo;.
</p>
</td></tr>
<tr><td><code id="get_sigsqs_+3A_after_burn_in">after_burn_in</code></td>
<td>

<p>If TRUE, only the <code class="reqn">\sigma^2</code> draws after the burn-in period are returned.
</p>
</td></tr>
<tr><td><code id="get_sigsqs_+3A_plot_hist">plot_hist</code></td>
<td>

<p>If TRUE, a histogram of the posterior <code class="reqn">\sigma^2</code> draws is generated.
</p>
</td></tr>
<tr><td><code id="get_sigsqs_+3A_plot_ci">plot_CI</code></td>
<td>

<p>Confidence level for credible interval on histogram.
</p>
</td></tr>
<tr><td><code id="get_sigsqs_+3A_plot_sigma">plot_sigma</code></td>
<td>

<p>If TRUE, plots <code class="reqn">\sigma</code> instead of <code class="reqn">\sigma^2</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a vector of posterior <code class="reqn">\sigma^2</code> draws (with or without the burn-in samples).
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>See Also</h3>

<p><code><a href="#topic+get_sigsqs">get_sigsqs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#generate Friedman data
set.seed(11)
n  = 300 
p = 5
X = data.frame(matrix(runif(n * p), ncol = p))
y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)

##build BART regression model
bart_machine = bartMachine(X, y)

#get posterior sigma^2's after burn-in and plot
sigsqs = get_sigsqs(bart_machine, plot_hist = TRUE)

## End(Not run)

</code></pre>

<hr>
<h2 id='get_var_counts_over_chain'>
Get the Variable Inclusion Counts
</h2><span id='topic+get_var_counts_over_chain'></span>

<h3>Description</h3>

<p>Computes the variable inclusion counts for a BART model. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_var_counts_over_chain(bart_machine, type = "splits")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_var_counts_over_chain_+3A_bart_machine">bart_machine</code></td>
<td>

<p>An object of class &ldquo;bartMachine&rdquo;.
</p>
</td></tr>
<tr><td><code id="get_var_counts_over_chain_+3A_type">type</code></td>
<td>

<p>If &ldquo;splits&rdquo;, then the number of times each variable is chosen for a splitting rule is computed. If &ldquo;trees&rdquo;, then the number of times each variable appears in a tree is computed.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a matrix of counts of each predictor across all trees by Gibbs sample. Thus, the dimension is <code>num_interations_after_burn_in</code> 
by <code>p</code> (where <code>p</code> is the number of predictors after dummifying factors and adding missingness dummies if specified by <code>use_missing_data_dummies_as_covars</code>).
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>See Also</h3>

<p><code><a href="#topic+get_var_props_over_chain">get_var_props_over_chain</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

#generate Friedman data
set.seed(11)
n  = 200 
p = 10
X = data.frame(matrix(runif(n * p), ncol = p))
y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)

##build BART regression model
bart_machine = bartMachine(X, y, num_trees = 20)

#get variable inclusion counts
var_counts = get_var_counts_over_chain(bart_machine)
print(var_counts)

## End(Not run)
</code></pre>

<hr>
<h2 id='get_var_props_over_chain'>
Get the Variable Inclusion Proportions
</h2><span id='topic+get_var_props_over_chain'></span>

<h3>Description</h3>

<p>Computes the variable inclusion proportions for a BART model. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_var_props_over_chain(bart_machine, type = "splits")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_var_props_over_chain_+3A_bart_machine">bart_machine</code></td>
<td>

<p>An object of class &ldquo;bartMachine&rdquo;.
</p>
</td></tr>
<tr><td><code id="get_var_props_over_chain_+3A_type">type</code></td>
<td>

<p>If &ldquo;splits&rdquo;, then the proportion of times each variable is chosen for a splitting rule versus all splitting rules is computed. If &ldquo;trees&rdquo;, then the proportion of times each variable appears in a tree versus all appearances of variables in trees is computed.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a vector of the variable inclusion proportions.
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>See Also</h3>

<p><code><a href="#topic+get_var_counts_over_chain">get_var_counts_over_chain</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#generate Friedman data
set.seed(11)
n  = 200 
p = 10
X = data.frame(matrix(runif(n * p), ncol = p))
y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)

##build BART regression model
bart_machine = bartMachine(X, y, num_trees = 20)

#Get variable inclusion proportions
var_props = get_var_props_over_chain(bart_machine)
print(var_props)

## End(Not run)
</code></pre>

<hr>
<h2 id='interaction_investigator'>
Explore Pairwise Interactions in BART Model
</h2><span id='topic+interaction_investigator'></span>

<h3>Description</h3>

<p>Explore the pairwise interaction counts for a BART model to learn about interactions fit by the model. This function includes an option to generate a plot of the pairwise interaction counts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>interaction_investigator(bart_machine, plot = TRUE, 
num_replicates_for_avg = 5, num_trees_bottleneck = 20, 
num_var_plot = 50, cut_bottom = NULL, bottom_margin = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="interaction_investigator_+3A_bart_machine">bart_machine</code></td>
<td>

<p>An object of class &ldquo;bartMachine&rdquo;.
</p>
</td></tr>
<tr><td><code id="interaction_investigator_+3A_plot">plot</code></td>
<td>

<p>If TRUE, a plot of the pairwise interaction counts is generated. 
</p>
</td></tr>
<tr><td><code id="interaction_investigator_+3A_num_replicates_for_avg">num_replicates_for_avg</code></td>
<td>

<p>The number of replicates of BART to be used to generate pairwise interaction inclusion counts. 
Averaging across multiple BART models improves stability of the estimates.
</p>
</td></tr>
<tr><td><code id="interaction_investigator_+3A_num_trees_bottleneck">num_trees_bottleneck</code></td>
<td>

<p>Number of trees to be used in the sum-of-trees model for computing pairwise interactions counts. 
A small number of trees should be used to force the variables to compete for entry into the model.
</p>
</td></tr>
<tr><td><code id="interaction_investigator_+3A_num_var_plot">num_var_plot</code></td>
<td>

<p>Number of variables to be shown on the plot. If &ldquo;Inf,&rdquo; all variables are plotted (not recommended if
the number of predictors is large). Default is 50.
</p>
</td></tr>
<tr><td><code id="interaction_investigator_+3A_cut_bottom">cut_bottom</code></td>
<td>

<p>A display parameter between 0 and 1 that controls where the y-axis is plotted. A value of 0 would begin the y-axis at 0; a value of 1 begins 
the y-axis at the minimum of the average pairwise interaction inclusion count (the smallest bar in the bar plot). Values between 0 and 1 begin the 
y-axis as a percentage of that minimum. 
</p>
</td></tr>
<tr><td><code id="interaction_investigator_+3A_bottom_margin">bottom_margin</code></td>
<td>

<p>A display parameter that adjusts the bottom margin of the graph if labels are clipped. The scale of this parameter is the same as set with <code>par(mar = c(....))</code> in R.
Higher values allow for more space if the crossed covariate names are long. Note that making this parameter too large will prevent plotting and the plot function in R will throw an error.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An interaction between two variables is considered to occur whenever a path from any node of a tree to 
any of its terminal node contains splits using those two variables. See Kapelner and Bleich, 2013, Section 4.11.
</p>


<h3>Value</h3>

<table>
<tr><td><code>interaction_counts</code></td>
<td>
<p>For each of the <code class="reqn">p \times p</code> interactions, what is the count across all <code>num_replicates_for_avg</code>
BART model replicates' post burn-in Gibbs samples in all trees.</p>
</td></tr>
<tr><td><code>interaction_counts_avg</code></td>
<td>
<p>For each of the <code class="reqn">p \times p</code> interactions, what is the average count across all <code>num_replicates_for_avg</code>
BART model replicates' post burn-in Gibbs samples in all trees.</p>
</td></tr>
<tr><td><code>interaction_counts_sd</code></td>
<td>
<p>For each of the <code class="reqn">p \times p</code> interactions, what is the sd of the interaction counts across the <code>num_replicates_for_avg</code>
BART models replicates.</p>
</td></tr>
<tr><td><code>interaction_counts_avg_and_sd_long</code></td>
<td>
<p>For each of the <code class="reqn">p \times p</code> interactions, what is the average and sd of the interaction counts across the <code>num_replicates_for_avg</code>
BART models replicates. The output is organized as a convenient long table of class <code>data.frame</code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>In the plot, the red bars correspond to the standard error of the variable inclusion proportion estimates (since multiple replicates were used).
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>References</h3>

<p>Adam Kapelner, Justin Bleich (2016). bartMachine: Machine Learning
with Bayesian Additive Regression Trees. Journal of Statistical
Software, 70(4), 1-40. doi:10.18637/jss.v070.i04
</p>


<h3>See Also</h3>

<p><code><a href="#topic+investigate_var_importance">investigate_var_importance</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#generate Friedman data
set.seed(11)
n  = 200 
p = 10
X = data.frame(matrix(runif(n * p), ncol = p))
y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)

##build BART regression model
bart_machine = bartMachine(X, y, num_trees = 20)

#investigate interactions
interaction_investigator(bart_machine)

## End(Not run)

</code></pre>

<hr>
<h2 id='investigate_var_importance'>
Explore Variable Inclusion Proportions in BART Model
</h2><span id='topic+investigate_var_importance'></span>

<h3>Description</h3>

<p>Explore the variable inclusion proportions for a BART model to learn about the relative influence of the different covariates. This function includes an option to generate a plot of the variable inclusion proportions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>investigate_var_importance(bart_machine, type = "splits", 
plot = TRUE, num_replicates_for_avg = 5, num_trees_bottleneck = 20, 
num_var_plot = Inf, bottom_margin = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="investigate_var_importance_+3A_bart_machine">bart_machine</code></td>
<td>

<p>An object of class &ldquo;bartMachine&rdquo;.
</p>
</td></tr>
<tr><td><code id="investigate_var_importance_+3A_type">type</code></td>
<td>

<p>If &ldquo;splits&rdquo;, then the proportion of times each variable is chosen for a splitting rule is computed. If &ldquo;trees&rdquo;, then the proportion of times each variable appears in a tree is computed.
</p>
</td></tr>
<tr><td><code id="investigate_var_importance_+3A_plot">plot</code></td>
<td>

<p>If TRUE, a plot of the variable inclusion proportions is generated. 
</p>
</td></tr>
<tr><td><code id="investigate_var_importance_+3A_num_replicates_for_avg">num_replicates_for_avg</code></td>
<td>

<p>The number of replicates of BART to be used to generate variable inclusion proportions. Averaging across multiple BART models improves stability of the estimates. See Bleich et al. (2013) for more details. 
</p>
</td></tr>
<tr><td><code id="investigate_var_importance_+3A_num_trees_bottleneck">num_trees_bottleneck</code></td>
<td>

<p>Number of trees to be used in the sum-of-trees for computing the variable inclusion proportions. A small number of trees should be used to force the variables to compete for entry into the model. Chipman et al. (2010) recommend 20. See this reference for more details. 
</p>
</td></tr>
<tr><td><code id="investigate_var_importance_+3A_num_var_plot">num_var_plot</code></td>
<td>

<p>Number of variables to be shown on the plot. If &ldquo;Inf&rdquo;, all variables are plotted.
</p>
</td></tr>
<tr><td><code id="investigate_var_importance_+3A_bottom_margin">bottom_margin</code></td>
<td>

<p>A display parameter that adjusts the bottom margin of the graph if labels are clipped. The scale of this parameter is the same as set with <code>par(mar = c(....))</code> in R.
Higher values allow for more space if the covariate names are long. Note that making this parameter too large will prevent plotting and the plot function in R will throw an error.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the plot, the red bars correspond to the standard error of the variable inclusion proportion estimates.
</p>


<h3>Value</h3>

<p>Invisibly, returns a list with the following components:
</p>
<table>
<tr><td><code>avg_var_props</code></td>
<td>
<p>The average variable inclusion proportions for each variable<br /> (across <code>num_replicates_for_avg</code>)</p>
</td></tr>
<tr><td><code>sd_var_props</code></td>
<td>
<p>The standard deviation of the variable inclusion proportions for each variable (across <code>num_replicates_for_avg</code>)</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function is parallelized by the number of cores set in <code><a href="#topic+set_bart_machine_num_cores">set_bart_machine_num_cores</a></code>.
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>References</h3>

<p>Adam Kapelner, Justin Bleich (2016). bartMachine: Machine Learning
with Bayesian Additive Regression Trees. Journal of Statistical
Software, 70(4), 1-40. doi:10.18637/jss.v070.i04
</p>
<p>J Bleich, A Kapelner, ST Jensen, and EI George. Variable Selection Inference for Bayesian
Additive Regression Trees. ArXiv e-prints, 2013.
</p>
<p>HA Chipman, EI George, and RE McCulloch. BART: Bayesian Additive Regressive Trees.
The Annals of Applied Statistics, 4(1): 266&ndash;298, 2010.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+interaction_investigator">interaction_investigator</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#generate Friedman data
set.seed(11)
n  = 200 
p = 10
X = data.frame(matrix(runif(n * p), ncol = p))
y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)

##build BART regression model
bart_machine = bartMachine(X, y, num_trees = 20)

#investigate variable inclusion proportions
investigate_var_importance(bart_machine)

## End(Not run)

</code></pre>

<hr>
<h2 id='k_fold_cv'>
Estimate Out-of-sample Error with K-fold Cross validation
</h2><span id='topic+k_fold_cv'></span>

<h3>Description</h3>

<p>Builds a BART model using a specified set of arguments to <code>build_bart_machine</code> and estimates the out-of-sample performance by using k-fold cross validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>k_fold_cv(X, y, k_folds = 5, folds_vec = NULL, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="k_fold_cv_+3A_x">X</code></td>
<td>

<p>Data frame of predictors. Factors are automatically converted to dummies interally. 
</p>
</td></tr>
<tr><td><code id="k_fold_cv_+3A_y">y</code></td>
<td>

<p>Vector of response variable. If <code>y</code> is <code>numeric</code> or <code>integer</code>, a BART model for regression is built. If <code>y</code> is a factor with two levels, a BART model for classification is built.
</p>
</td></tr>
<tr><td><code id="k_fold_cv_+3A_k_folds">k_folds</code></td>
<td>

<p>Number of folds to cross-validate over. This argument is ignored if <code>folds_vec</code> is non-null.
</p>
</td></tr>
<tr><td><code id="k_fold_cv_+3A_folds_vec">folds_vec</code></td>
<td>

<p>An integer vector of indices specifying which fold each observation belongs to. 
</p>
</td></tr>
<tr><td><code id="k_fold_cv_+3A_verbose">verbose</code></td>
<td>

<p>Prints information about progress of the algorithm to the screen. 
</p>
</td></tr>
<tr><td><code id="k_fold_cv_+3A_...">...</code></td>
<td>

<p>Additional arguments to be passed to <code>build_bart_machine</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each fold, a new BART model is trained (using the same set of arguments) and its performance is evaluated on the holdout piece of that fold.
</p>


<h3>Value</h3>

<p>For regression models, a list with the following components is returned:
</p>
<table>
<tr><td><code>y_hat</code></td>
<td>
<p>Predictions for the observations computed on the fold for which the observation was omitted from the training set.</p>
</td></tr>
<tr><td><code>L1_err</code></td>
<td>
<p>Aggregate L1 error across the folds.</p>
</td></tr>
<tr><td><code>L2_err</code></td>
<td>
<p>Aggregate L1 error across the folds.</p>
</td></tr>
<tr><td><code>rmse</code></td>
<td>
<p>Aggregate RMSE across the folds.</p>
</td></tr>
<tr><td><code>folds</code></td>
<td>
<p>Vector of indices specifying which fold each observation belonged to.</p>
</td></tr>
</table>
<p>For classification models, a list with the following components is returned: 
</p>
<table>
<tr><td><code>y_hat</code></td>
<td>
<p>Class predictions for the observations computed on the fold for which the observation was omitted from the training set.</p>
</td></tr>
<tr><td><code>p_hat</code></td>
<td>
<p>Probability estimates for the observations computed on the fold for which the observation was omitted from the training set.</p>
</td></tr>
<tr><td><code>confusion_matrix</code></td>
<td>
<p>Aggregate confusion matrix across the folds.</p>
</td></tr>
<tr><td><code>misclassification_error</code></td>
<td>
<p>Total misclassification error across the folds.</p>
</td></tr>
<tr><td><code>folds</code></td>
<td>
<p>Vector of indices specifying which fold each observation belonged to.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function is parallelized by the number of cores set in <code><a href="#topic+set_bart_machine_num_cores">set_bart_machine_num_cores</a></code>.
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bartMachine">bartMachine</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#generate Friedman data
set.seed(11)
n  = 200 
p = 5
X = data.frame(matrix(runif(n * p), ncol = p))
y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)

#evaluate default BART on 5 folds
k_fold_val = k_fold_cv(X, y)
print(k_fold_val$rmse)

## End(Not run)

</code></pre>

<hr>
<h2 id='linearity_test'>
Test of Linearity
</h2><span id='topic+linearity_test'></span>

<h3>Description</h3>

<p>Test to investigate <code class="reqn">H_0:</code> the functional relationship between the response and the
regressors is linear. We fit a linear model and then test if the residuals are a function
of the regressors using the 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>linearity_test(lin_mod = NULL, X = NULL, y = NULL, 
	num_permutation_samples = 100, plot = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="linearity_test_+3A_lin_mod">lin_mod</code></td>
<td>

<p>A linear model you can pass in if you do not want to use the default which is <code>lm(y ~ X)</code>. Default is <code>NULL</code> which should be used if you pass in <code>X</code> and <code>y</code>. 
</p>
</td></tr>
<tr><td><code id="linearity_test_+3A_x">X</code></td>
<td>

<p>Data frame of predictors. Factors are automatically converted to dummies internally. Default is <code>NULL</code> which should be used if you pass in <code>lin_mode</code>.
</p>
</td></tr>
<tr><td><code id="linearity_test_+3A_y">y</code></td>
<td>

<p>Vector of response variable. If <code>y</code> is <code>numeric</code> or <code>integer</code>, a BART model for regression is built. If <code>y</code> is a factor with two levels, a BART model for classification is built.
Default is <code>NULL</code> which should be used if you pass in <code>lin_mode</code>.
</p>
</td></tr>
<tr><td><code id="linearity_test_+3A_num_permutation_samples">num_permutation_samples</code></td>
<td>

<p>This function relies on <code><a href="#topic+cov_importance_test">cov_importance_test</a></code> (see documentation there for details).
</p>
</td></tr>
<tr><td><code id="linearity_test_+3A_plot">plot</code></td>
<td>

<p>This function relies on <code><a href="#topic+cov_importance_test">cov_importance_test</a></code> (see documentation there for details).
</p>
</td></tr>
<tr><td><code id="linearity_test_+3A_...">...</code></td>
<td>

<p>Additional parameters to be passed to <code>bartMachine</code>, the model constructed on the residuals of the linear model.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>permutation_samples_of_error</code></td>
<td>
<p>	This function relies on <code><a href="#topic+cov_importance_test">cov_importance_test</a></code> (see documentation there for details).
</p>
</td></tr>
<tr><td><code>observed_error_estimate</code></td>
<td>
<p>	This function relies on <code><a href="#topic+cov_importance_test">cov_importance_test</a></code> (see documentation there for details).
</p>
</td></tr>
<tr><td><code>pval</code></td>
<td>
<p>The approximate p-value for this test. See the documentation at <code><a href="#topic+cov_importance_test">cov_importance_test</a></code>.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Adam Kapelner
</p>


<h3>See Also</h3>

<p><code><a href="#topic+cov_importance_test">cov_importance_test</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
##regression example

##generate Friedman data i.e. a nonlinear response model
set.seed(11)
n  = 200 
p = 5
X = data.frame(matrix(runif(n * p), ncol = p))
y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)

##now test if there is a nonlinear relationship between X1, ..., X5 and y.
linearity_test(X = X, y = y)
## note the plot and the printed p-value.. should be approx 0

#generate a linear response model
y = 1 * X[ ,1] + 3 * X[,2] + 5 * X[,3] + 7 * X[ ,4] + 9 * X[,5] + rnorm(n)
linearity_test(X = X, y = y)
## note the plot and the printed p-value.. should be &gt; 0.05


## End(Not run)

</code></pre>

<hr>
<h2 id='node_prediction_training_data_indices'>
Gets node predictions indices of the training data for new data.
</h2><span id='topic+node_prediction_training_data_indices'></span>

<h3>Description</h3>

<p>This returns a binary tensor for all gibbs samples after burn-in for all trees and for all training observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>node_prediction_training_data_indices(bart_machine, new_data = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="node_prediction_training_data_indices_+3A_bart_machine">bart_machine</code></td>
<td>

<p>An object of class &ldquo;bartMachine&rdquo;.
</p>
</td></tr>
<tr><td><code id="node_prediction_training_data_indices_+3A_new_data">new_data</code></td>
<td>

<p>Data that you wish to investigate the training sample weights. If <code>NULL</code>, the original training data is used.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a binary tensor indicating whether the prediction node contained a training datum or not. For each observation in new data, the size of this tensor is number of gibbs sample after burn-in
times the number of trees times the number of training data observations. This the size of the full tensor is the number of observations in the new data times the three dimensional object just explained.
</p>

<hr>
<h2 id='pd_plot'>
Partial Dependence Plot
</h2><span id='topic+pd_plot'></span>

<h3>Description</h3>

<p>Creates a partial dependence plot for a BART model for regression or classification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pd_plot(bart_machine, j, 
levs = c(0.05, seq(from = 0.1, to = 0.9, by = 0.1), 0.95), 
lower_ci = 0.025, upper_ci = 0.975, prop_data = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pd_plot_+3A_bart_machine">bart_machine</code></td>
<td>

<p>An object of class &ldquo;bartMachine&rdquo;.
</p>
</td></tr>
<tr><td><code id="pd_plot_+3A_j">j</code></td>
<td>

<p>The number or name of the column in the design matrix for which the partial dependence plot is to be created. 
</p>
</td></tr>
<tr><td><code id="pd_plot_+3A_levs">levs</code></td>
<td>

<p>Quantiles at which the partial dependence function should be evaluated. Linear extrapolation is performed between these points.
</p>
</td></tr>
<tr><td><code id="pd_plot_+3A_lower_ci">lower_ci</code></td>
<td>

<p>Lower limit for credible interval
</p>
</td></tr>
<tr><td><code id="pd_plot_+3A_upper_ci">upper_ci</code></td>
<td>

<p>Upper limit for credible interval
</p>
</td></tr>
<tr><td><code id="pd_plot_+3A_prop_data">prop_data</code></td>
<td>

<p>The proportion of the training data to use. Default is 1. Use a lower proportion for speedier pd_plots. The closer to 1, the more resolution
the PD plot will have; the closer to 0, the lower but faster.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For regression models, the units on the y-axis are the same as the units of the response. For classification models, the units on the y-axis are probits. 
</p>


<h3>Value</h3>

<p>Invisibly, returns a list with the following components:
</p>
<table>
<tr><td><code>x_j_quants</code></td>
<td>
<p>Quantiles at which the partial dependence function is evaluated</p>
</td></tr>
<tr><td><code>bart_avg_predictions_by_quantile_by_gibbs</code></td>
<td>
<p>All samples of <code class="reqn">\hat{f}(x)</code></p>
</td></tr>
<tr><td><code>bart_avg_predictions_by_quantile</code></td>
<td>
<p>Posterior means for <code class="reqn">\hat{f}(x)</code> at <code>x_j_quants</code></p>
</td></tr>
<tr><td><code>bart_avg_predictions_lower</code></td>
<td>
<p>Lower bound of the desired confidence of the credible interval of <code class="reqn">\hat{f}(x)</code></p>
</td></tr>
<tr><td><code>bart_avg_predictions_upper</code></td>
<td>
<p>Upper bound of the desired confidence of the credible interval of <code class="reqn">\hat{f}(x)</code></p>
</td></tr>
<tr><td><code>prop_data</code></td>
<td>
<p>The proportion of the training data to use as specified when this function was executed</p>
</td></tr>

</table>


<h3>Note</h3>

<p>This function is parallelized by the number of cores set in <code><a href="#topic+set_bart_machine_num_cores">set_bart_machine_num_cores</a></code>.
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>References</h3>

<p>Adam Kapelner, Justin Bleich (2016). bartMachine: Machine Learning
with Bayesian Additive Regression Trees. Journal of Statistical
Software, 70(4), 1-40. doi:10.18637/jss.v070.i04
</p>
<p>HA Chipman, EI George, and RE McCulloch. BART: Bayesian Additive Regressive Trees.
The Annals of Applied Statistics, 4(1): 266&ndash;298, 2010.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#Regression example

#generate Friedman data
set.seed(11)
n  = 200 
p = 5
X = data.frame(matrix(runif(n * p), ncol = p))
y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)

##build BART regression model
bart_machine = bartMachine(X, y)

#partial dependence plot for quadratic term
pd_plot(bart_machine, "X3")


#Classification example

#get data and only use 2 factors
data(iris)
iris2 = iris[51:150,]
iris2$Species = factor(iris2$Species)

#build BART classification model
bart_machine = bartMachine(iris2[ ,1:4], iris2$Species)

#partial dependence plot 
pd_plot(bart_machine, "Petal.Width")

## End(Not run)


</code></pre>

<hr>
<h2 id='plot_convergence_diagnostics'>
Plot Convergence Diagnostics
</h2><span id='topic+plot_convergence_diagnostics'></span>

<h3>Description</h3>

<p>A suite of plots to assess convergence diagonstics and features of the BART model. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_convergence_diagnostics(bart_machine,  
plots = c("sigsqs", "mh_acceptance", "num_nodes", "tree_depths"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_convergence_diagnostics_+3A_bart_machine">bart_machine</code></td>
<td>

<p>An object of class &ldquo;bartMachine&rdquo;.
</p>
</td></tr>
<tr><td><code id="plot_convergence_diagnostics_+3A_plots">plots</code></td>
<td>

<p>The list of plots to be displayed. The four options are: &quot;sigsqs&quot;, &quot;mh_acceptance&quot;, &quot;num_nodes&quot;, &quot;tree_depths&quot;.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The &ldquo;sigsqs&rdquo; option plots the posterior error variance estimates by the Gibbs sample number. This is a standard tool to assess convergence of MCMC algorithms. This option is not applicable to classification BART models.<br />
The &ldquo;mh_acceptance&rdquo; option plots the proportion of Metropolis-Hastings steps accepted for each Gibbs sample (number accepted divided by number of trees).<br />
The &ldquo;num_nodes&rdquo; option plots the average number of nodes across each tree in the sum-of-trees model by the Gibbs sample number (for post burn-in only). The blue line
is the average number of nodes over all trees.<br />
The &ldquo;tree_depths&rdquo; option plots the average tree depth across each tree in the sum-of-trees model by the Gibbs sample number (for post burn-in only). The blue line
is the average number of nodes over all trees.
</p>


<h3>Value</h3>

<p>None.
</p>


<h3>Note</h3>

<p>The &ldquo;sigsqs&rdquo; plot separates the burn-in <code class="reqn">\sigma^2</code>'s for the first core by post burn-in <code class="reqn">\sigma^2</code>'s estimates for all cores by grey vertical lines.
The &ldquo;mh_acceptance&rdquo; plot separates burn-in from post-burn in by a grey vertical line. Post burn-in, the different core proportions plot in different colors.
The &ldquo;num_nodes&rdquo; plot separates different core estimates by vertical lines (post burn-in only).
The 'tree_depths&rdquo; plot separates different core estimates by vertical lines (post burn-in only).
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#generate Friedman data
set.seed(11)
n  = 200 
p = 5
X = data.frame(matrix(runif(n * p), ncol = p))
y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)

##build BART regression model
bart_machine = bartMachine(X, y)

#plot convergence diagnostics
plot_convergence_diagnostics(bart_machine)

## End(Not run)

</code></pre>

<hr>
<h2 id='plot_y_vs_yhat'>
Plot the fitted Versus Actual Response
</h2><span id='topic+plot_y_vs_yhat'></span>

<h3>Description</h3>

<p>Generates a plot actual versus fitted values and corresponding credible intervals or prediction intervals for the fitted values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_y_vs_yhat(bart_machine, Xtest = NULL, ytest = NULL, 
credible_intervals = FALSE, prediction_intervals = FALSE, 
interval_confidence_level = 0.95)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_y_vs_yhat_+3A_bart_machine">bart_machine</code></td>
<td>

<p>An object of class &ldquo;bartMachine&rdquo;. 
</p>
</td></tr>
<tr><td><code id="plot_y_vs_yhat_+3A_xtest">Xtest</code></td>
<td>

<p>Optional argument for test data. If included, BART computes fitted values at the rows of <code>Xtest</code>. Else, the fitted values from the training data are used.
</p>
</td></tr>
<tr><td><code id="plot_y_vs_yhat_+3A_ytest">ytest</code></td>
<td>

<p>Optional argument for test data. Vector of observed values corresponding to the rows of <code>Xtest</code> to be plotted against the predictions for the rows of <code>Xtest</code>.
</p>
</td></tr>
<tr><td><code id="plot_y_vs_yhat_+3A_credible_intervals">credible_intervals</code></td>
<td>

<p>If TRUE, Bayesian credible intervals are computed using the quantiles of the posterior distribution of <code class="reqn">\hat{f}(x)</code>. See <code><a href="#topic+calc_credible_intervals">calc_credible_intervals</a></code> for details.
</p>
</td></tr>
<tr><td><code id="plot_y_vs_yhat_+3A_prediction_intervals">prediction_intervals</code></td>
<td>

<p>If TRUE, Bayesian predictive intervals are computed using the a draw of from <code class="reqn">\hat{f}(x)</code>. See <code><a href="#topic+calc_prediction_intervals">calc_prediction_intervals</a></code> for details.
</p>
</td></tr>
<tr><td><code id="plot_y_vs_yhat_+3A_interval_confidence_level">interval_confidence_level</code></td>
<td>

<p>Desired level of confidence for credible or prediction intervals. 
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None.
</p>


<h3>Note</h3>

<p>This function is parallelized by the number of cores set in <code><a href="#topic+set_bart_machine_num_cores">set_bart_machine_num_cores</a></code>.
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bart_machine_get_posterior">bart_machine_get_posterior</a></code>, <code><a href="#topic+calc_credible_intervals">calc_credible_intervals</a></code>, <code><a href="#topic+calc_prediction_intervals">calc_prediction_intervals</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#generate linear data
set.seed(11)
n  = 500 
p = 3
X = data.frame(matrix(runif(n * p), ncol = p))
y = 3*X[ ,1] + 2*X[ ,2] +X[ ,3] + rnorm(n)

##build BART regression model
bart_machine = bartMachine(X, y)

##generate plot
plot_y_vs_yhat(bart_machine)

#generate plot with prediction bands
plot_y_vs_yhat(bart_machine, prediction_intervals = TRUE)

## End(Not run)

</code></pre>

<hr>
<h2 id='predict_bartMachineArr'>
Make a prediction on data using a BART array object
</h2><span id='topic+predict_bartMachineArr'></span>

<h3>Description</h3>

<p>Makes a prediction on new data given an array of fitted BART model for 
regression or classification. If BART creates models that are variable,
running many and averaging is a good strategy. It is well known that the
Gibbs sampler gets locked into local modes at times. This is a way
to average over many chains.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_bartMachineArr(object, new_data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_bartMachineArr_+3A_object">object</code></td>
<td>

<p>An object of class &ldquo;bartMachineArr&rdquo;.
</p>
</td></tr>
<tr><td><code id="predict_bartMachineArr_+3A_new_data">new_data</code></td>
<td>

<p>A data frame where each row is an observation to predict. The column names
should be the same as the column names of the training data.
</p>
</td></tr>
<tr><td><code id="predict_bartMachineArr_+3A_...">...</code></td>
<td>

<p>Not supported. Note that parameters <code>type</code> and <code>prob_rule_class</code> for 
<code><a href="#topic+predict.bartMachine">predict.bartMachine</a></code> are not supported.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If regression, a numeric vector of <code>y_hat</code>, the best guess as to the response. If classification and <code>type = ``prob''</code>, 
a numeric vector of <code>p_hat</code>, the best guess as to the probability of the response class being  the &rdquo;positive&rdquo; class. If classification and 
<code>type = ''class''</code>, a character vector of the best guess of the response's class labels. 
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.bartMachine">predict.bartMachine</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Regression example
## Not run: 
#generate Friedman data
set.seed(11)
n  = 200 
p = 5
X = data.frame(matrix(runif(n * p), ncol = p))
y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)

##build BART regression model
bart_machine = bartMachine(X, y)
bart_machine_arr = bartMachineArr(bart_machine)

##make predictions on the training data
y_hat = predict(bart_machine_arr, X)

#Classification example
data(iris)
iris2 = iris[51 : 150, ] #do not include the third type of flower for this example
iris2$Species = factor(iris2$Species)  
bart_machine = bartMachine(iris2[ ,1:4], iris2$Species)
bart_machine_arr = bartMachineArr(bart_machine)

##make probability predictions on the training data
p_hat = predict_bartMachineArr(bart_machine_arr, iris2[ ,1:4])

## End(Not run)


</code></pre>

<hr>
<h2 id='predict.bartMachine'>
Make a prediction on data using a BART object
</h2><span id='topic+predict.bartMachine'></span>

<h3>Description</h3>

<p>Makes a prediction on new data given a fitted BART model for regression or classification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bartMachine'
predict(object, new_data, type = "prob", prob_rule_class = NULL, verbose = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.bartMachine_+3A_object">object</code></td>
<td>

<p>An object of class &ldquo;bartMachine&rdquo;.
</p>
</td></tr>
<tr><td><code id="predict.bartMachine_+3A_new_data">new_data</code></td>
<td>

<p>A data frame where each row is an observation to predict. The column names
should be the same as the column names of the training data.
</p>
</td></tr>
<tr><td><code id="predict.bartMachine_+3A_type">type</code></td>
<td>

<p>Only relevant if the bartMachine model is classification. The type can be &ldquo;prob&rdquo; which will
return the estimate of <code class="reqn">P(Y = 1)</code>(the &ldquo;positive&rdquo; class) or &ldquo;class&rdquo; which will return the best guess as to the
class of the object, in the original label, based on if the probability estimate is greater 
than <code>prob_rule_class</code>. Default is &ldquo;prob.&rdquo;
</p>
</td></tr>
<tr><td><code id="predict.bartMachine_+3A_prob_rule_class">prob_rule_class</code></td>
<td>

<p>The rule to determine when the class estimate is <code class="reqn">Y = 1</code> (the &ldquo;positive&rdquo; class) based on the probability estimate. This
defaults to what was originally specified in the <code>bart_machine</code> object. 
</p>
</td></tr>
<tr><td><code id="predict.bartMachine_+3A_verbose">verbose</code></td>
<td>

<p>Prints out prediction-related messages. Currently in use only for probability predictions to let the user know which class
is being predicted. Default is <code>TRUE</code>.
</p>
</td></tr>
<tr><td><code id="predict.bartMachine_+3A_...">...</code></td>
<td>

<p>Parameters that are ignored.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If regression, a numeric vector of <code>y_hat</code>, the best guess as to the response. If classification and <code>type = ``prob''</code>, 
a numeric vector of <code>p_hat</code>, the best guess as to the probability of the response class being  the &rdquo;positive&rdquo; class. If classification and 
<code>type = ''class''</code>, a character vector of the best guess of the response's class labels. 
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bart_predict_for_test_data">bart_predict_for_test_data</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Regression example
## Not run: 
#generate Friedman data
set.seed(11)
n  = 200 
p = 5
X = data.frame(matrix(runif(n * p), ncol = p))
y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)

##build BART regression model
bart_machine = bartMachine(X, y)

##make predictions on the training data
y_hat = predict(bart_machine, X)

#Classification example
data(iris)
iris2 = iris[51 : 150, ] #do not include the third type of flower for this example
iris2$Species = factor(iris2$Species)  
bart_machine = bartMachine(iris2[ ,1:4], iris2$Species)

##make probability predictions on the training data
p_hat = predict(bart_machine, X)

##make class predictions on test data
y_hat_class = predict(bart_machine, X, type = "class")

##make class predictions on test data conservatively for ''versicolor''
y_hat_class_conservative = predict(bart_machine, X, type = "class", prob_rule_class = 0.9)

## End(Not run)


</code></pre>

<hr>
<h2 id='print.bartMachine'>
Summarizes information about a <code>bartMachine</code> object.
</h2><span id='topic+print.bartMachine'></span>

<h3>Description</h3>

<p>This is an alias for the <code><a href="#topic+summary.bartMachine">summary.bartMachine</a></code> function. See description in that section.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bartMachine'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.bartMachine_+3A_x">x</code></td>
<td>

<p>An object of class &ldquo;bartMachine&rdquo;.
</p>
</td></tr>
<tr><td><code id="print.bartMachine_+3A_...">...</code></td>
<td>

<p>Parameters that are ignored.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None.
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
#Regression example

#generate Friedman data
set.seed(11)
n  = 200 
p = 5
X = data.frame(matrix(runif(n * p), ncol = p))
y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)

##build BART regression model
bart_machine = bartMachine(X, y)

##print out details
print(bart_machine)

##Also, the default print works too
bart_machine

## End(Not run)
</code></pre>

<hr>
<h2 id='rmse_by_num_trees'>
Assess the Out-of-sample RMSE by Number of Trees
</h2><span id='topic+rmse_by_num_trees'></span>

<h3>Description</h3>

<p>Assess out-of-sample RMSE of a BART model for varying numbers of trees in the sum-of-trees model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmse_by_num_trees(bart_machine, tree_list = c(5, seq(10, 50, 10), 100, 150, 200),
in_sample = FALSE, plot = TRUE, holdout_pctg = 0.3, num_replicates = 4, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmse_by_num_trees_+3A_bart_machine">bart_machine</code></td>
<td>

<p>An object of class &ldquo;bartMachine&rdquo;.
</p>
</td></tr>
<tr><td><code id="rmse_by_num_trees_+3A_tree_list">tree_list</code></td>
<td>

<p>List of sizes for the sum-of-trees models.
</p>
</td></tr>
<tr><td><code id="rmse_by_num_trees_+3A_in_sample">in_sample</code></td>
<td>

<p>If TRUE, the RMSE is computed on in-sample data rather than an out-of-sample holdout. 
</p>
</td></tr>
<tr><td><code id="rmse_by_num_trees_+3A_plot">plot</code></td>
<td>

<p>If TRUE, a plot of the RMSE by the number of trees in the ensemble is created.
</p>
</td></tr>
<tr><td><code id="rmse_by_num_trees_+3A_holdout_pctg">holdout_pctg</code></td>
<td>

<p>Percentage of the data to be treated as an out-of-sample holdout. 
</p>
</td></tr>
<tr><td><code id="rmse_by_num_trees_+3A_num_replicates">num_replicates</code></td>
<td>

<p>Number of replicates to average the results over. Each replicate uses a randomly sampled holdout of the data, (which could have overlap).
</p>
</td></tr>
<tr><td><code id="rmse_by_num_trees_+3A_...">...</code></td>
<td>

<p>Other arguments to be passed to the plot function.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly, returns the out-of-sample average RMSEs for each tree size. 
</p>


<h3>Note</h3>

<p>Since using a large number of trees can substantially increase computation time, this plot can help assess whether a smaller ensemble size is sufficient to obtain desirable predictive performance.
This function is parallelized by the number of cores set in <code><a href="#topic+set_bart_machine_num_cores">set_bart_machine_num_cores</a></code>.
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#generate Friedman data
set.seed(11)
n  = 200 
p = 10
X = data.frame(matrix(runif(n * p), ncol = p))
y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)

##build BART regression model
bart_machine = bartMachine(X, y, num_trees = 20)

#explore RMSE by number of trees
rmse_by_num_trees(bart_machine)

## End(Not run)

</code></pre>

<hr>
<h2 id='set_bart_machine_num_cores'>
Set the Number of Cores for BART
</h2><span id='topic+set_bart_machine_num_cores'></span>

<h3>Description</h3>

<p>Sets the number of cores to be used for all parallelized BART functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_bart_machine_num_cores(num_cores)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_bart_machine_num_cores_+3A_num_cores">num_cores</code></td>
<td>

<p>Number of cores to use. If the number of cores is more than 1, setting the seed during model construction
cannot be deterministic.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None.
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bart_machine_num_cores">bart_machine_num_cores</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#set all parallelized functions to use 4 cores
set_bart_machine_num_cores(4) 

## End(Not run)
</code></pre>

<hr>
<h2 id='summary.bartMachine'>
Summarizes information about a <code>bartMachine</code> object.
</h2><span id='topic+summary.bartMachine'></span>

<h3>Description</h3>

<p>Provides a quick summary of the BART model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bartMachine'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.bartMachine_+3A_object">object</code></td>
<td>

<p>An object of class &ldquo;bartMachine&rdquo;.
</p>
</td></tr>
<tr><td><code id="summary.bartMachine_+3A_...">...</code></td>
<td>

<p>Parameters that are ignored.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Gives the version number of the <code>bartMachine</code> package used to build this <code>additiveBartMachine</code> object and if the object
models either &ldquo;regression&rdquo; or &ldquo;classification.&rdquo; Gives the amount of training data and the dimension of feature space. Prints 
the amount of time it took to build the model, how many processor cores were used to during its construction, as well as the 
number of burn-in and posterior Gibbs samples were used. 
</p>
<p>If the model is for regression, it prints the estimate of <code class="reqn">\sigma^2</code> before the model was constructed as well as after so 
the user can inspect how much variance was explained.
</p>
<p>If the model was built using the <code>run_in_sample = TRUE</code> parameter in <code><a href="#topic+build_bart_machine">build_bart_machine</a></code> and is for regression, the summary L1,
L2, rmse, Pseudo-<code class="reqn">R^2</code> are printed as well as the p-value for the tests of normality and zero-mean noise. If the model is for classification, a confusion matrix is printed.
</p>


<h3>Value</h3>

<p>None.
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#Regression example

#generate Friedman data
set.seed(11)
n  = 200 
p = 5
X = data.frame(matrix(runif(n * p), ncol = p))
y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)

##build BART regression model
bart_machine = bartMachine(X, y)

##print out details
summary(bart_machine)

##Also, the default print works too
bart_machine

## End(Not run)
</code></pre>

<hr>
<h2 id='var_selection_by_permute'>
Perform Variable Selection using Three Threshold-based Procedures
</h2><span id='topic+var_selection_by_permute'></span>

<h3>Description</h3>

<p>Performs variable selection using the three thresholding methods introduced in Bleich et al. (2013). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var_selection_by_permute(bart_machine, 
num_reps_for_avg = 10, num_permute_samples = 100, 
num_trees_for_permute = 20, alpha = 0.05, 
plot = TRUE, num_var_plot = Inf, bottom_margin = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="var_selection_by_permute_+3A_bart_machine">bart_machine</code></td>
<td>

<p>An object of class &ldquo;bartMachine&rdquo;.
</p>
</td></tr>
<tr><td><code id="var_selection_by_permute_+3A_num_reps_for_avg">num_reps_for_avg</code></td>
<td>

<p>Number of replicates to over over to for the BART model's variable inclusion proportions.
</p>
</td></tr>
<tr><td><code id="var_selection_by_permute_+3A_num_permute_samples">num_permute_samples</code></td>
<td>

<p>Number of permutations of the response to be made to generate the &ldquo;null&rdquo; permutation distribution.
</p>
</td></tr>
<tr><td><code id="var_selection_by_permute_+3A_num_trees_for_permute">num_trees_for_permute</code></td>
<td>

<p>Number of trees to use in the variable selection procedure. As with <br /> <code><a href="#topic+investigate_var_importance">investigate_var_importance</a></code>, a small number of trees should be used to force variables to compete for entry into the model. Note that this number is used to estimate both the &ldquo;true&rdquo; and &ldquo;null&rdquo; variable inclusion proportions.
</p>
</td></tr>
<tr><td><code id="var_selection_by_permute_+3A_alpha">alpha</code></td>
<td>

<p>Cut-off level for the thresholds.
</p>
</td></tr>
<tr><td><code id="var_selection_by_permute_+3A_plot">plot</code></td>
<td>

<p>If TRUE, a plot showing which variables are selected by each of the procedures is generated.
</p>
</td></tr>
<tr><td><code id="var_selection_by_permute_+3A_num_var_plot">num_var_plot</code></td>
<td>

<p>Number of variables (in order of decreasing variable inclusion proportion) to be plotted.
</p>
</td></tr>
<tr><td><code id="var_selection_by_permute_+3A_bottom_margin">bottom_margin</code></td>
<td>

<p>A display parameter that adjusts the bottom margin of the graph if labels are clipped. The scale of this parameter is the same as set with <code>par(mar = c(....))</code> in R.
Higher values allow for more space if the crossed covariate names are long. Note that making this parameter too large will prevent plotting and the plot function in R will throw an error.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See Bleich et al. (2013) for a complete description of the procedures outlined above as well as the corresponding vignette for a brief summary with examples. 
</p>


<h3>Value</h3>

<p>Invisibly, returns a list with the following components:
</p>
<table>
<tr><td><code>important_vars_local_names</code></td>
<td>
<p>Names of the variables chosen by the Local procedure.</p>
</td></tr>
<tr><td><code>important_vars_global_max_names</code></td>
<td>
<p>Names of the variables chosen by the Global Max procedure.</p>
</td></tr>
<tr><td><code>important_vars_global_se_names</code></td>
<td>
<p>Names of the variables chosen by the Global SE procedure.</p>
</td></tr>
<tr><td><code>important_vars_local_col_nums</code></td>
<td>
<p>Column numbers of the variables chosen by the Local procedure.</p>
</td></tr>
<tr><td><code>important_vars_global_max_col_nums</code></td>
<td>
<p>Column numbers of the variables chosen by the Global Max procedure.</p>
</td></tr>
<tr><td><code>important_vars_global_se_col_nums</code></td>
<td>
<p>Column numbers of the variables chosen by the Global SE procedure.</p>
</td></tr>
<tr><td><code>var_true_props_avg</code></td>
<td>
<p>The variable inclusion proportions for the actual data.</p>
</td></tr>
<tr><td><code>permute_mat</code></td>
<td>
<p>The permutation distribution generated by permuting the response vector.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Although the reference only explores regression settings, this procedure is applicable to both regression and classification problems.
This function is parallelized by the number of cores set in <code><a href="#topic+set_bart_machine_num_cores">set_bart_machine_num_cores</a></code>.
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>References</h3>

<p>J Bleich, A Kapelner, ST Jensen, and EI George. Variable Selection Inference for Bayesian
Additive Regression Trees. ArXiv e-prints, 2013.
</p>
<p>Adam Kapelner, Justin Bleich (2016). bartMachine: Machine Learning
with Bayesian Additive Regression Trees. Journal of Statistical
Software, 70(4), 1-40. doi:10.18637/jss.v070.i04
</p>


<h3>See Also</h3>

<p><code><a href="#topic+var_selection_by_permute">var_selection_by_permute</a></code>, <code><a href="#topic+investigate_var_importance">investigate_var_importance</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#generate Friedman data
set.seed(11)
n  = 300 
p = 20 ##15 useless predictors 
X = data.frame(matrix(runif(n * p), ncol = p))
y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)

##build BART regression model (not actuall used in variable selection)
bart_machine = bartMachine(X, y)

#variable selection
var_sel = var_selection_by_permute(bart_machine)
print(var_sel$important_vars_local_names)
print(var_sel$important_vars_global_max_names)

## End(Not run)
  </code></pre>

<hr>
<h2 id='var_selection_by_permute_cv'>
Perform Variable Selection Using Cross-validation Procedure
</h2><span id='topic+var_selection_by_permute_cv'></span>

<h3>Description</h3>

<p>Performs variable selection by cross-validating over the three threshold-based procedures outlined in Bleich et al. (2013) and selecting the single procedure that returns the lowest cross-validation RMSE. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>var_selection_by_permute_cv(bart_machine, k_folds = 5, folds_vec = NULL, 
num_reps_for_avg = 5, num_permute_samples = 100, 
num_trees_for_permute = 20, alpha = 0.05, num_trees_pred_cv = 50)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="var_selection_by_permute_cv_+3A_bart_machine">bart_machine</code></td>
<td>

<p>An object of class &ldquo;bartMachine&rdquo;.
</p>
</td></tr>
<tr><td><code id="var_selection_by_permute_cv_+3A_k_folds">k_folds</code></td>
<td>

<p>Number of folds to be used in cross-validation.
</p>
</td></tr>
<tr><td><code id="var_selection_by_permute_cv_+3A_folds_vec">folds_vec</code></td>
<td>

<p>An integer vector of indices specifying which fold each observation belongs to.   
</p>
</td></tr>
<tr><td><code id="var_selection_by_permute_cv_+3A_num_reps_for_avg">num_reps_for_avg</code></td>
<td>

<p>Number of replicates to over over to for the BART model's variable inclusion proportions.
</p>
</td></tr>
<tr><td><code id="var_selection_by_permute_cv_+3A_num_permute_samples">num_permute_samples</code></td>
<td>

<p>Number of permutations of the response to be made to generate the &ldquo;null&rdquo; permutation distribution.
</p>
</td></tr>
<tr><td><code id="var_selection_by_permute_cv_+3A_num_trees_for_permute">num_trees_for_permute</code></td>
<td>

<p>Number of trees to use in the variable selection procedure. As with <br /> <code><a href="#topic+investigate_var_importance">investigate_var_importance</a></code>, a small number of trees should be used to force variables to compete for entry into the model. Note that this number is used to estimate both the &ldquo;true&rdquo; and &ldquo;null&rdquo; variable inclusion proportions.
</p>
</td></tr>
<tr><td><code id="var_selection_by_permute_cv_+3A_alpha">alpha</code></td>
<td>

<p>Cut-off level for the thresholds.
</p>
</td></tr>
<tr><td><code id="var_selection_by_permute_cv_+3A_num_trees_pred_cv">num_trees_pred_cv</code></td>
<td>

<p>Number of trees to use for prediction on the hold-out portion of each fold. Once variables have been selected using the training portion of each fold, a new model is built using only those variables with <code>num_trees_pred_cv</code> trees in the sum-of-trees model. Forecasts for the holdout sample are made using this model. A larger number of trees is recommended to exploit the full forecasting power of BART. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See Bleich et al. (2013) for a complete description of the procedures outlined above as well as the corresponding vignette for a brief summary with examples. 
</p>


<h3>Value</h3>

<p>Returns a list with the following components:
</p>
<table>
<tr><td><code>best_method</code></td>
<td>
<p>The name of the best variable selection procedure, as chosen via cross-validation.</p>
</td></tr>
<tr><td><code>important_vars_cv</code></td>
<td>
<p>The variables chosen by the <code>best_method</code> above.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This function can have substantial run-time. 
This function is parallelized by the number of cores set in <code><a href="#topic+set_bart_machine_num_cores">set_bart_machine_num_cores</a></code>.
</p>


<h3>Author(s)</h3>

<p>Adam Kapelner and Justin Bleich
</p>


<h3>References</h3>

<p>J Bleich, A Kapelner, ST Jensen, and EI George. Variable Selection Inference for Bayesian
Additive Regression Trees. ArXiv e-prints, 2013.
</p>
<p>Adam Kapelner, Justin Bleich (2016). bartMachine: Machine Learning
with Bayesian Additive Regression Trees. Journal of Statistical
Software, 70(4), 1-40. doi:10.18637/jss.v070.i04
</p>


<h3>See Also</h3>

<p><code><a href="#topic+var_selection_by_permute">var_selection_by_permute</a></code>, <code><a href="#topic+investigate_var_importance">investigate_var_importance</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#generate Friedman data
set.seed(11)
n  = 150 
p = 100 ##95 useless predictors 
X = data.frame(matrix(runif(n * p), ncol = p))
y = 10 * sin(pi* X[ ,1] * X[,2]) +20 * (X[,3] -.5)^2 + 10 * X[ ,4] + 5 * X[,5] + rnorm(n)

##build BART regression model (not actually used in variable selection)
bart_machine = bartMachine(X, y)

#variable selection via cross-validation
var_sel_cv = var_selection_by_permute_cv(bart_machine, k_folds = 3)
print(var_sel_cv$best_method)
print(var_sel_cv$important_vars_cv)

## End(Not run)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
