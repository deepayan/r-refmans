<!DOCTYPE html><html><head><title>Help for package bayeslm</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {bayeslm}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bayeslm-package'>
<p>Efficient sampling for Gaussian linear regression with arbitrary priors</p></a></li>
<li><a href='#bayeslm'><p>Efficient sampling for Gaussian linear model with arbitrary priors</p></a></li>
<li><a href='#hs_gibbs'><p>Gibbs sampler of horseshoe regression</p></a></li>
<li><a href='#plot.MCMC'><p>Plot posterior summary</p></a></li>
<li><a href='#predict.bayeslm.fit'><p>Predict new data</p></a></li>
<li><a href='#summary.bayeslm.fit'><p>Summarize fitted object of <code>bayeslm</code></p></a></li>
<li><a href='#summary.MCMC'><p>Summarize posterior draws</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Efficient Sampling for Gaussian Linear Regression with Arbitrary
Priors</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-6-25</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jingyu He &lt;jingyuhe@cityu.edu.hk&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Efficient sampling for Gaussian linear regression with arbitrary priors, Hahn, He and Lopes (2018) &lt;<a href="https://doi.org/10.48550/arXiv.1806.05738">doi:10.48550/arXiv.1806.05738</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-2">LGPL-2</a> | <a href="https://www.r-project.org/Licenses/LGPL-2.1">LGPL-2.1</a> | <a href="https://www.r-project.org/Licenses/LGPL-3">LGPL-3</a> [expanded from: LGPL (&ge; 2)]</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.7), stats, graphics, grDevices, coda, methods,
RcppParallel</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>GNU make</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/JingyuHe/bayeslm">https://github.com/JingyuHe/bayeslm</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo, RcppParallel</td>
</tr>
<tr>
<td>Suggests:</td>
<td>rmarkdown, knitr</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-06-27 02:09:32 UTC; jingyuhe</td>
</tr>
<tr>
<td>Author:</td>
<td>Jingyu He [aut, cre],
  P. Richard Hahn [aut],
  Hedibert Lopes [aut],
  Andrew Herren [ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-06-27 22:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='bayeslm-package'>
Efficient sampling for Gaussian linear regression with arbitrary priors
</h2><span id='topic+bayeslm-package'></span>

<h3>Description</h3>

<p>The elliptical slice sampler for Bayesian linear regression with shrinakge priors such as horseshoe, Laplace prior, ridge prior. 
</p>


<h3>Author(s)</h3>

<p>P. Richard Hahn, Jingyu He and Hedibert Lopes
</p>
<p>Maintainer: Jingyu He <a href="mailto:jingyuhe@cityu.edu.hk">jingyuhe@cityu.edu.hk</a>
</p>


<h3>References</h3>

<p>Hahn, P. Richard, Jingyu He, and Hedibert Lopes. <em>Efficient sampling for Gaussian linear regression with arbitrary priors</em> (2017).
</p>


<h3>See Also</h3>

<p>bayeslm
</p>

<hr>
<h2 id='bayeslm'>Efficient sampling for Gaussian linear model with arbitrary priors
</h2><span id='topic+bayeslm'></span><span id='topic+bayeslm.default'></span><span id='topic+bayeslm.formula'></span>

<h3>Description</h3>

<p>This package implements an efficient sampler for Gaussian Bayesian linear regression. The package uses elliptical slice sampler instead of regular Gibbs sampler. The function has several built-in priors and user can also provide their own prior function (written as a R function).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
bayeslm(Y, X = FALSE, prior = "horseshoe", penalize = NULL, 
block_vec = NULL, sigma = NULL, s2 = 1, kap2 = 1, N = 20000L, burnin = 0L, 
thinning = 1L, vglobal = 1, sampling_vglobal = TRUE, verb = FALSE, icept = TRUE, 
standardize = TRUE, singular = FALSE, scale_sigma_prior = TRUE, prior_mean = NULL, 
prob_vec = NULL, cc = NULL, lambda = NULL, ...)

## S3 method for class 'formula'
bayeslm(formula, data = list(), Y = FALSE, X = FALSE, 
prior = "horseshoe", penalize = NULL, block_vec = NULL, sigma = NULL, 
s2 = 1, kap2 = 1, N = 20000L, burnin = 0L, thinning = 1L, vglobal = 1, 
sampling_vglobal = TRUE, verb = FALSE, standardize = TRUE, singular = FALSE, 
scale_sigma_prior = TRUE, prior_mean = NULL, 
prob_vec = NULL, cc = NULL, lambda = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bayeslm_+3A_formula">formula</code></td>
<td>
<p><code>formula</code> of the model to fit.</p>
</td></tr>
<tr><td><code id="bayeslm_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables in the model.
By default the variables are taken from the environment which
<code>bayeslm</code> is called from.</p>
</td></tr>
<tr><td><code id="bayeslm_+3A_y">Y</code></td>
<td>
<p><code>data.frame</code>, <code>matrix</code>, or <code>vector</code> of inputs <code>Y</code>. Response variable. </p>
</td></tr>
<tr><td><code id="bayeslm_+3A_x">X</code></td>
<td>
<p><code>data.frame</code>, <code>matrix</code>, or <code>vector</code> of inputs <code>X</code>. Regressors. </p>
</td></tr>
<tr><td><code id="bayeslm_+3A_prior">prior</code></td>
<td>
<p>Indicating shrinkage prior to use. <code>"horseshoe"</code> for approximate horseshoe prior (default), <code>"laplace"</code> for laplace prior, <code>"ridge"</code> for ridge prior, <code>"sharkfin"</code> for &quot;sharkfin&quot; prior and <code>"nonlocal"</code> for nonlocal prior.</p>
</td></tr>
<tr><td><code id="bayeslm_+3A_block_vec">block_vec</code></td>
<td>
<p>A vector indicating number of regressors in each block. Sum of all entries should be the same as number of regressors. The default value is <code>block_vec = rep(1, p)</code>, put every regressor in its own block (slice-within-Gibbs sampler)</p>
</td></tr>
<tr><td><code id="bayeslm_+3A_penalize">penalize</code></td>
<td>
<p>A vector indicating shrink regressors or not. It's length should be the same as number of regressors. <code>1</code> indicates shrink corresponding coefficient, <code>0</code> indicates no shrinkage. The default value is <code>rep(1, p)</code>, shrink all coefficients</p>
</td></tr>
<tr><td><code id="bayeslm_+3A_sigma">sigma</code></td>
<td>
<p>Initial value of residual standard error. The default value is half of standard error of <code>Y</code>.</p>
</td></tr>
<tr><td><code id="bayeslm_+3A_s2">s2</code>, <code id="bayeslm_+3A_kap2">kap2</code></td>
<td>
<p>Parameter of prior over sigma, an inverse gamma prior with rate s2 and shape s2.</p>
</td></tr>
<tr><td><code id="bayeslm_+3A_n">N</code></td>
<td>
<p>Number of posterior samples (after burn-in).</p>
</td></tr>
<tr><td><code id="bayeslm_+3A_burnin">burnin</code></td>
<td>
<p>Number of burn-in samples. If burnin &gt; 0, the function will draw N + burnin samples and return the last N samples only.</p>
</td></tr>
<tr><td><code id="bayeslm_+3A_thinning">thinning</code></td>
<td>
<p>Number of thinnings. <code>thinning = 1</code> means no thinning.</p>
</td></tr>
<tr><td><code id="bayeslm_+3A_vglobal">vglobal</code></td>
<td>
<p>Initial value of global shrinkage parameter. Default value is 1</p>
</td></tr>
<tr><td><code id="bayeslm_+3A_sampling_vglobal">sampling_vglobal</code></td>
<td>
<p><code>Bool</code>, if <code>TRUE</code>, sampling the global shrinkage parameter by random walk Metropolis Hastings on log scale, otherwise always stay at the initial value <code>vglobal</code>.</p>
</td></tr>
<tr><td><code id="bayeslm_+3A_verb">verb</code></td>
<td>
<p>Bool, if <code>TRUE</code>, print out sampling progress.</p>
</td></tr>
<tr><td><code id="bayeslm_+3A_icept">icept</code></td>
<td>
<p>Bool, if the inputs are matrix <code>X</code> and <code>Y</code>, and <code>icept = TRUE</code>, the function will estimate intercept. Default value is <code>TRUE</code>. If the input is formula <code>Y ~ X</code>, option <code>icept</code> is useless, control intercept by formular <code>Y ~ X</code> or <code>Y ~ X - 1</code>.</p>
</td></tr>
<tr><td><code id="bayeslm_+3A_standardize">standardize</code></td>
<td>
<p>Bool, if <code>TRUE</code>, standardize X and Y before sampling.</p>
</td></tr>
<tr><td><code id="bayeslm_+3A_singular">singular</code></td>
<td>
<p>Bool, if <code>TRUE</code>, take it as a rank-deficient case such as n &lt; p or X'X is singular. See section 2.3.2 of the paper for details.</p>
</td></tr>
<tr><td><code id="bayeslm_+3A_scale_sigma_prior">scale_sigma_prior</code></td>
<td>
<p>Bool, if <code>TRUE</code>, the prior of regression coefficient <code class="reqn">\beta</code> is scaled by residual standard error <code class="reqn">\sigma</code>.</p>
</td></tr>
<tr><td><code id="bayeslm_+3A_prior_mean">prior_mean</code></td>
<td>
<p><code>vector</code>, specify prior mean of nonlocal prior for each regressor. It should have length <code>p</code> (no intercept) or <code>p + 1</code> (intercept). The default value is 1.5 for all regressors.</p>
</td></tr>
<tr><td><code id="bayeslm_+3A_prob_vec">prob_vec</code></td>
<td>
<p><code>vector</code>, specify prior mean of sharkfin prior for each regressor. It should have length <code>p</code> (no intercept) or <code>p + 1</code> (intercept). The default value is 0.25 for all regressors.</p>
</td></tr>
<tr><td><code id="bayeslm_+3A_cc">cc</code></td>
<td>
<p>Only works when <code>singular == TRUE</code>, precision parameter of ridge adjustment. It should be a vector with length $p$. If it is <code>NULL</code>, it will be set as <code>rep(10, p)</code>.</p>
</td></tr>
<tr><td><code id="bayeslm_+3A_lambda">lambda</code></td>
<td>
<p>The shrinkage parameter for Laplace prior only.</p>
</td></tr>
<tr><td><code id="bayeslm_+3A_...">...</code></td>
<td>
<p>optional parameters to be passed to the low level function <code>bayeslm.default</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For details of the approach, please see Hahn, He and Lopes (2017)
</p>


<h3>Value</h3>

<table>
<tr><td><code>loops</code></td>
<td>
<p>A <code>vector</code> of number of elliptical slice sampler loops for each posterior sample.</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>A <code>vector</code> of posterior samples of residual standard error.</p>
</td></tr>
<tr><td><code>vglobal</code></td>
<td>
<p>A <code>vector</code> of posterior samples of the global shrinkage parameter.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>A <code>matrix</code> of posterior samples of coefficients.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Fitted values of the regression model. Take posterior mean of coefficients with 20% burnin samples.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Residuals of the regression model, equals <code>y - fitted.values</code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p><code>horseshoe</code> is essentially call function <code>bayeslm</code> with <code>prior = "horseshoe"</code>. Same for <code>sharkfin</code>, <code>ridge</code>, <code>blasso</code>, <code>nonlocal</code>.
</p>


<h3>Author(s)</h3>

<p> Jingyu He <a href="mailto:jingyu.he@chicagobooth.edu">jingyu.he@chicagobooth.edu</a> </p>


<h3>References</h3>

<p>Hahn, P. Richard, Jingyu He, and Hedibert Lopes. <em>Efficient sampling for Gaussian linear regression with arbitrary priors.</em> (2017).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
p = 20
n = 100

kappa = 1.25
beta_true = c(c(1,2,3),rnorm(p-3,0,0.01))
sig_true = kappa*sqrt(sum(beta_true^2))


x = matrix(rnorm(p*n),n,p)
y = x %*% beta_true + sig_true * rnorm(n)


x = as.matrix(x)
y = as.matrix(y)
data = data.frame(x = x, y = y)

block_vec = rep(1, p) # slice-within-Gibbs sampler, put every coefficient in its own block

fitOLS = lm(y~x-1)

# call the function using formulas
fita = bayeslm(y ~ x, prior = 'horseshoe', 
        block_vec = block_vec, N = 10000, burnin = 2000)
# summary the results
summary(fita)
summary(fita$beta)


# put the first two coefficients in one elliptical sampling block
block_vec2 = c(2, rep(1, p-2))
fitb = bayeslm(y ~ x, data = data, prior = 'horseshoe', 
        block_vec = block_vec2, N = 10000, burnin = 2000)

# comparing several different priors

fit1 = bayeslm(y,x,prior = 'horseshoe', icept = FALSE, 
          block_vec = block_vec, N = 10000, burnin=2000)
beta_est1 = colMeans(fit1$beta)

fit2 = bayeslm(y,x,prior = 'laplace', icept = FALSE, 
          block_vec = block_vec, N = 10000, burnin=2000)
beta_est2 = colMeans(fit2$beta)

fit3 = bayeslm(y,x,prior = 'ridge', icept = FALSE, 
          block_vec = block_vec, N = 10000, burnin=2000)
beta_est3 = colMeans(fit3$beta)

fit4 = bayeslm(y,x,prior = 'sharkfin', icept = FALSE, 
          block_vec = block_vec, N = 10000, burnin=2000)
beta_est4 = colMeans(fit4$beta)

fit5 = bayeslm(y,x,prior = 'nonlocal', icept = FALSE, 
          block_vec = block_vec, N = 10000, burnin=2000)
beta_est5 = colMeans(fit5$beta)

plot(NULL,xlim=range(beta_true),ylim=range(beta_true), 
  xlab = "beta true", ylab = "estimation")
points(beta_true,beta_est1,pch=20)
points(beta_true,fitOLS$coef,col='red')
points(beta_true,beta_est2,pch=20,col='cyan')
points(beta_true,beta_est3,pch=20,col='orange')
points(beta_true,beta_est4,pch=20,col='pink')
points(beta_true,beta_est5,pch=20,col='lightgreen')

legend("topleft", c("OLS", "horseshoe", "laplace", "ridge", "sharkfin", 
  "nonlocal"), col = c("red", "black", "cyan", "orange", 
    "pink", "lightgreen"), pch = rep(1, 6))

abline(0,1,col='red')

rmseOLS = sqrt(sum((fitOLS$coef-beta_true)^2))
rmse1 = sqrt(sum((beta_est1-beta_true)^2))
rmse2 = sqrt(sum((beta_est2-beta_true)^2))
rmse3 = sqrt(sum((beta_est3-beta_true)^2))
rmse4 = sqrt(sum((beta_est4-beta_true)^2))
rmse5 = sqrt(sum((beta_est5-beta_true)^2))

print(cbind(ols = rmseOLS, hs = rmse1,laplace = rmse2,
ridge = rmse3,sharkfin = rmse4,nonlocal = rmse5))



</code></pre>

<hr>
<h2 id='hs_gibbs'>Gibbs sampler of horseshoe regression</h2><span id='topic+hs_gibbs'></span>

<h3>Description</h3>

<p>Standard Gibbs sampler of horseshoe regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hs_gibbs(Y, X, nsamps, a, b, scale_sigma_prior)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hs_gibbs_+3A_y">Y</code></td>
<td>
<p>Response of regression.</p>
</td></tr>
<tr><td><code id="hs_gibbs_+3A_x">X</code></td>
<td>
<p>Matrix of regressors.</p>
</td></tr>
<tr><td><code id="hs_gibbs_+3A_nsamps">nsamps</code></td>
<td>
<p>Number of posterior samples.</p>
</td></tr>
<tr><td><code id="hs_gibbs_+3A_a">a</code></td>
<td>
<p>Parameter of inverse Gamma prior on <code class="reqn">\sigma</code>.</p>
</td></tr>
<tr><td><code id="hs_gibbs_+3A_b">b</code></td>
<td>
<p>Parameter of inverse Gamma prior on <code class="reqn">\sigma</code>.</p>
</td></tr>
<tr><td><code id="hs_gibbs_+3A_scale_sigma_prior">scale_sigma_prior</code></td>
<td>
<p>Bool, if <code>TRUE</code>, use prior scaled by <code class="reqn">\sigma</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements standard Gibbs sampler of horseshoe regression. The prior is
<code class="reqn">y \mid \beta, \sigma^2, X \sim MVN(X\beta, \sigma^2 I)</code>
<code class="reqn">\beta_i \mid \tau, \lambda_i, \sigma \sim N(0, \lambda_i^2\tau^2\sigma^2)</code>
<code class="reqn">\sigma^2\sim IG(a, b)</code>
<code class="reqn">\tau \sim C^{+}(0,1)</code>
<code class="reqn">\lambda_i \sim C^{+}(0,1)</code>
</p>


<h3>Author(s)</h3>

<p>Jingyu He</p>


<h3>See Also</h3>

 <p><code><a href="coda.html#topic+summary.mcmc">summary.mcmc</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>x = matrix(rnorm(1000), 100, 10)
y = x %*% rnorm(10) + rnorm(100)
fit=hs_gibbs(y, x, 1000, 1, 1, TRUE)
summary(fit)
</code></pre>

<hr>
<h2 id='plot.MCMC'>Plot posterior summary</h2><span id='topic+plot.MCMC'></span>

<h3>Description</h3>

<p><code>plot.MCMC</code> is an S3 method to plot empirical distribution of posterior draws. The input is a <code>MCMC</code> matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'MCMC'
plot(x,names,burnin=trunc(.1*nrow(X)),tvalues,TRACEPLOT=TRUE,DEN=TRUE,INT=TRUE,
      CHECK_NDRAWS=TRUE,... )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.MCMC_+3A_x">x</code></td>
<td>
<p> A <code>MCMC</code> class matrix of posterior draws, such as <code>bayeslm\$beta</code>.</p>
</td></tr>
<tr><td><code id="plot.MCMC_+3A_names">names</code></td>
<td>
<p> an optional character vector of names for the columns of <code>X</code>.</p>
</td></tr>
<tr><td><code id="plot.MCMC_+3A_burnin">burnin</code></td>
<td>
<p> Number of draws to burn-in (default value is <code class="reqn">0.1*nrow(X)</code>).</p>
</td></tr>
<tr><td><code id="plot.MCMC_+3A_tvalues">tvalues</code></td>
<td>
<p> vector of true values.</p>
</td></tr>
<tr><td><code id="plot.MCMC_+3A_traceplot">TRACEPLOT</code></td>
<td>
<p> logical, TRUE provide sequence plots of draws and acfs (default: <code>TRUE</code>)</p>
</td></tr>
<tr><td><code id="plot.MCMC_+3A_den">DEN</code></td>
<td>
<p> logical, TRUE use density scale on histograms (default: <code>TRUE</code>)</p>
</td></tr>
<tr><td><code id="plot.MCMC_+3A_int">INT</code></td>
<td>
<p> logical, TRUE put various intervals and points on graph (default: <code>TRUE</code>)</p>
</td></tr>
<tr><td><code id="plot.MCMC_+3A_check_ndraws">CHECK_NDRAWS</code></td>
<td>
<p> logical, TRUE check that there are at least 100 draws (default: <code>TRUE</code>)</p>
</td></tr>
<tr><td><code id="plot.MCMC_+3A_...">...</code></td>
<td>
<p> optional arguments for generic function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is modified from package <code>bayesm</code> by Peter Rossi. It plots summary of posterior draws.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+summary.bayeslm.fit">summary.bayeslm.fit</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>x = matrix(rnorm(1000), 100, 10)
y = x %*% rnorm(10) + rnorm(100)
fit=bayeslm(y~x)
plot(fit$beta)
</code></pre>

<hr>
<h2 id='predict.bayeslm.fit'>Predict new data</h2><span id='topic+predict.bayeslm.fit'></span>

<h3>Description</h3>

<p><code>predict.bayeslm.fit</code> is an S3 method to predict response on new data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bayeslm.fit'
predict(object,data,burnin,X,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.bayeslm.fit_+3A_object">object</code></td>
<td>
 <p><code>object</code> is output of <code>bayeslm</code> function.</p>
</td></tr>
<tr><td><code id="predict.bayeslm.fit_+3A_data">data</code></td>
<td>
<p> A data frame or list of new data to predict.</p>
</td></tr>
<tr><td><code id="predict.bayeslm.fit_+3A_burnin">burnin</code></td>
<td>
<p> number of draws to burn-in (default value is <code class="reqn">0.1*nrow(X)</code>).</p>
</td></tr>
<tr><td><code id="predict.bayeslm.fit_+3A_x">X</code></td>
<td>
<p> If call <code>bayeslm</code> with matrices input <code>x</code> and y but not formula, pass new matrix to predict here. See example for details.</p>
</td></tr>
<tr><td><code id="predict.bayeslm.fit_+3A_...">...</code></td>
<td>
<p> optional arguments for generic function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Make prediction on new data set, users are allowed to adjust number of burn-in samples.
</p>


<h3>Author(s)</h3>

<p>Jingyu He</p>


<h3>Examples</h3>

<pre><code class='language-R'>x = matrix(rnorm(1000), 100, 10)
y = x %*% rnorm(10) + rnorm(100)
data = list(x = x, y = y)

# Train the model with formula input
fit1 = bayeslm(y ~ x, data = data)
# predict
pred1 = predict(fit1, data)

# Train the model with matrices input
fit2 = bayeslm(Y = y, X = x)
pred2 = predict(fit2, X = x)
</code></pre>

<hr>
<h2 id='summary.bayeslm.fit'>Summarize fitted object of <code>bayeslm</code></h2><span id='topic+summary.bayeslm.fit'></span>

<h3>Description</h3>

<p><code>summary.bayeslm.fit</code> is an S3 method to summarize returned object of function <code>bayeslm</code>. The input should be <code>bayeslm.fit</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'bayeslm.fit'
summary(object,names,burnin=NULL,quantiles=FALSE,trailer=TRUE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.bayeslm.fit_+3A_object">object</code></td>
<td>
 <p><code>object</code> is a fitted object, returned by function <code>bayeslm</code>.</p>
</td></tr>
<tr><td><code id="summary.bayeslm.fit_+3A_names">names</code></td>
<td>
<p> an optional character vector of names for all the coefficients.</p>
</td></tr>
<tr><td><code id="summary.bayeslm.fit_+3A_burnin">burnin</code></td>
<td>
<p> number of draws to burn-in (if it is <code>NULL</code>, will set default value as <code class="reqn">0.2*nrow(object\$beta)</code>)</p>
</td></tr>
<tr><td><code id="summary.bayeslm.fit_+3A_quantiles">quantiles</code></td>
<td>
<p> logical for should quantiles be displayed (def: <code>FALSE</code>)</p>
</td></tr>
<tr><td><code id="summary.bayeslm.fit_+3A_trailer">trailer</code></td>
<td>
<p> logical for should a trailer be displayed (def: <code>TRUE</code>)</p>
</td></tr>
<tr><td><code id="summary.bayeslm.fit_+3A_...">...</code></td>
<td>
<p> optional arguments for generic function</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function summarize returned object of function <code>bayeslm</code>. It prints mean, std Dev, effective sample size (computed by function <code>effectiveSize</code> in package <code>coda</code>) coefficients posterior samples. If <code>quantiles=TRUE</code>, quantiles of marginal distirbutions in the columns of <code class="reqn">X</code> are displayed.<br />
</p>
<p>The function also returns significance level, defined by whether the symmetric posterior quantile-based credible interval excludes zero. For example, a regression coefficient with one * has 0.025 quantile and 0.975 quantile with the same sign. Similarly, '***' denotes 0.0005 and 0.9995, '**' denotes 0.005 and 0.995, '*' denotes 0.025 and 0.975, '.' denotes 0.05 and 0.95 quantiles with the same sign.
</p>


<h3>Author(s)</h3>

<p>Jingyu He</p>


<h3>See Also</h3>

 <p><code><a href="coda.html#topic+summary.mcmc">summary.mcmc</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>x = matrix(rnorm(1000), 100, 10)
y = x %*% rnorm(10) + rnorm(100)
fit=bayeslm(y~x)
summary(fit)
</code></pre>

<hr>
<h2 id='summary.MCMC'>Summarize posterior draws</h2><span id='topic+summary.MCMC'></span>

<h3>Description</h3>

<p><code>summary.MCMC</code> is an S3 method to summarize posterior draws of the model. The input should be a matrix of draws.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'MCMC'
summary(object,names,burnin=trunc(.1*nrow(X)),quantiles=FALSE,trailer=TRUE,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.MCMC_+3A_object">object</code></td>
<td>
 <p><code>object</code> is a matrix of draws, usually an object of class <code>MCMC</code>. It's same as <code>X</code>.</p>
</td></tr>
<tr><td><code id="summary.MCMC_+3A_names">names</code></td>
<td>
<p> an optional character vector of names for the columns of <code>X</code>.</p>
</td></tr>
<tr><td><code id="summary.MCMC_+3A_burnin">burnin</code></td>
<td>
<p> number of draws to burn-in (default value is <code class="reqn">0.1*nrow(X)</code>).</p>
</td></tr>
<tr><td><code id="summary.MCMC_+3A_quantiles">quantiles</code></td>
<td>
<p> logical for should quantiles be displayed (def: <code>FALSE</code>).</p>
</td></tr>
<tr><td><code id="summary.MCMC_+3A_trailer">trailer</code></td>
<td>
<p> logical for should a trailer be displayed (def: <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="summary.MCMC_+3A_...">...</code></td>
<td>
<p> optional arguments for generic function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is modified from package <code>bayesm</code> by Peter Rossi. It summarize object <code>MCMC</code>. Mean, Std Dev, effective sample size (computed by function <code>effectiveSize</code> in package <code>coda</code>) are displayed. If <code>quantiles=TRUE</code>, quantiles of marginal distirbutions in the columns of <code class="reqn">X</code> are displayed.<br />
</p>
<p>The function also returns significance level, defined by whether the symmetric posterior quantile-based credible interval excludes zero. For example, a regression coefficient with one * has 0.025 quantile and 0.975 quantile with the same sign. Similarly, '***' denotes 0.0005 and 0.9995, '**' denotes 0.005 and 0.995, '*' denotes 0.025 and 0.975, '.' denotes 0.05 and 0.95 quantiles with the same sign.
</p>


<h3>Author(s)</h3>

<p>Peter Rossi, Anderson School, UCLA, <a href="mailto:perossichi@gmail.com">perossichi@gmail.com</a>.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+summary.bayeslm.fit">summary.bayeslm.fit</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>x = matrix(rnorm(1000), 100, 10)
y = x %*% rnorm(10) + rnorm(100)
fit=bayeslm(y~x)
summary(fit$beta)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
