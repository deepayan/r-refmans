<!DOCTYPE html><html><head><title>Help for package nftbart</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {nftbart}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bartModelMatrix'><p>Deprecated: use bMM instead</p></a></li>
<li><a href='#bMM'><p>Create a matrix out of a vector or data.frame</p></a></li>
<li><a href='#bmx'><p>NHANES 1999-2000 Body Measures and Demographics</p></a></li>
<li><a href='#CDCheight'><p>CDC height for age growth charts</p></a></li>
<li><a href='#CDimpute'><p>Cold-deck missing imputation</p></a></li>
<li><a href='#Cindex'><p>Calculate the C-index/concordance for survival analysis.</p></a></li>
<li><a href='#lung'><p>NCCTG Lung Cancer Data</p></a></li>
<li><a href='#nft2'><p>Fit NFT BART models.</p></a></li>
<li><a href='#predict.aftree'><p>Estimating the survival and the hazard for AFT BART models.</p></a></li>
<li><a href='#predict.nft2'><p>Drawing Posterior Predictive Realizations for NFT BART models.</p></a></li>
<li><a href='#tsvs2'><p>Variable selection with NFT BART models.</p></a></li>
<li><a href='#xicuts'><p>Specifying cut-points for the covariates</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Nonparametric Failure Time Bayesian Additive Regression Trees</td>
</tr>
<tr>
<td>Version:</td>
<td>2.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-11-27</td>
</tr>
<tr>
<td>Author:</td>
<td>Rodney Sparapani [aut, cre],
  Robert McCulloch [aut],
  Matthew Pratola [ctb],
  Hugh Chipman [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Rodney Sparapani &lt;rsparapa@mcw.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Nonparametric Failure Time (NFT) Bayesian Additive Regression Trees (BART): Time-to-event Machine Learning with Heteroskedastic Bayesian Additive Regression Trees (HBART) and Low Information Omnibus (LIO) Dirichlet Process Mixtures (DPM). An NFT BART model is of the form Y = mu + f(x) + sd(x) E where functions f and sd have BART and HBART priors, respectively, while E is a nonparametric error distribution due to a DPM LIO prior hierarchy. See the following for a complete description of the model at &lt;<a href="https://doi.org/10.1111%2Fbiom.13857">doi:10.1111/biom.13857</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.2.0), survival, nnet</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-11-27 19:29:08 UTC; rsparapa</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-11-28 01:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='bartModelMatrix'>Deprecated: use bMM instead</h2><span id='topic+bartModelMatrix'></span>

<h3>Description</h3>

<p>Create a matrix out of a vector or <code>data.frame</code>. The
compiled functions of this package operate on matrices in memory.
Therefore, if the user submits a vector or data.frame, then this
function converts it to a matrix.  Also, it determines the number of
cutpoints necessary for each column when asked to do so.  </p>


<h3>Usage</h3>

<pre><code class='language-R'>bartModelMatrix(X, numcut=0L, usequants=FALSE, type=7, rm.const=FALSE,
                cont=FALSE, xicuts=NULL, rm.vars=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bartModelMatrix_+3A_x">X</code></td>
<td>
<p>A vector or data.frame to create the matrix from. </p>
</td></tr>
<tr><td><code id="bartModelMatrix_+3A_numcut">numcut</code></td>
<td>
<p>The maximum number of cutpoints to consider.
If <code>numcut=0</code>, then just return a matrix; otherwise,
return a list. </p>
</td></tr>
<tr><td><code id="bartModelMatrix_+3A_usequants">usequants</code></td>
<td>
<p> If <code>usequants</code> is <code>FALSE</code>, then the
cutpoints in <code>xinfo</code> are generated uniformly; otherwise,
if <code>TRUE</code>, quantiles are used for the cutpoints. </p>
</td></tr>
<tr><td><code id="bartModelMatrix_+3A_type">type</code></td>
<td>
<p> Determines which quantile algorithm is employed.</p>
</td></tr>
<tr><td><code id="bartModelMatrix_+3A_rm.const">rm.const</code></td>
<td>
<p> Whether or not to remove constant variables.</p>
</td></tr>
<tr><td><code id="bartModelMatrix_+3A_cont">cont</code></td>
<td>
<p> Whether or not to assume all variables are continuous.</p>
</td></tr>
<tr><td><code id="bartModelMatrix_+3A_xicuts">xicuts</code></td>
<td>
<p> To specify your own cut-points, use the <code>xicuts</code>
argument.</p>
</td></tr>
<tr><td><code id="bartModelMatrix_+3A_rm.vars">rm.vars</code></td>
<td>
<p> The variables that you want removed. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>numcut==0</code> (the default), then a matrix of the
covariates is returned; otherwise, a list is returned with the
following values. 
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p>A matrix of the covariates with <code>n</code> rows and <code>p</code>
columns.</p>
</td></tr>
<tr><td><code>numcut</code></td>
<td>
<p>A vector of length <code>p</code> of the number of cut-points
for each covariate.</p>
</td></tr>
<tr><td><code>grp</code></td>
<td>
<p>A vector that corresponds to variables in the input
<code>data.frame</code> that were translated into dummy columns in the
output matrix, i.e., for each input variable in order, there is a
number in the vector corresponding to the number of output columns
created for it. </p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+bMM">bMM</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## set.seed(99)

## a &lt;- rbinom(10, 4, 0.4)

## table(a)

## x &lt;- runif(10)

## df &lt;- data.frame(a=factor(a), x=x)

## (b &lt;- bartModelMatrix(df))

## (b &lt;- bartModelMatrix(df, numcut=9))

## (b &lt;- bartModelMatrix(df, numcut=9, usequants=TRUE))

## Not run: 
    ## this is an error
    ## f &lt;- bartModelMatrix(as.character(a))

## End(Not run)
</code></pre>

<hr>
<h2 id='bMM'>Create a matrix out of a vector or data.frame</h2><span id='topic+bMM'></span>

<h3>Description</h3>

<p>Adapted from <code>bartModelMatrix()</code>.  The
compiled functions of this package operate on matrices in memory.
Therefore, if the user submits a vector or <code>data.frame</code>, then this
function converts it to a matrix.  Also, it determines the number of
cutpoints necessary for each column when asked to do so.  </p>


<h3>Usage</h3>

<pre><code class='language-R'>bMM(X, numcut=0L, usequants=FALSE, type=7, xicuts=NULL, rm.const=FALSE,
    rm.dupe=FALSE, method="spearman", use="pairwise.complete.obs")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bMM_+3A_x">X</code></td>
<td>
<p>A vector or data.frame to create the matrix from. </p>
</td></tr>
<tr><td><code id="bMM_+3A_numcut">numcut</code></td>
<td>
<p>The maximum number of cutpoints to consider.
If <code>numcut=0</code>, then just return a matrix; otherwise,
return a list. </p>
</td></tr>
<tr><td><code id="bMM_+3A_usequants">usequants</code></td>
<td>
<p> If <code>usequants</code> is <code>FALSE</code>, then the
cutpoints in <code>xinfo</code> are generated uniformly; otherwise,
if <code>TRUE</code>, quantiles are used for the cutpoints. </p>
</td></tr>
<tr><td><code id="bMM_+3A_type">type</code></td>
<td>
<p> Determines which quantile algorithm is employed.</p>
</td></tr>
<tr><td><code id="bMM_+3A_xicuts">xicuts</code></td>
<td>
<p> To specify your own cut-points, use the <code>xicuts</code>
argument.</p>
</td></tr>
<tr><td><code id="bMM_+3A_rm.const">rm.const</code></td>
<td>
<p>To remove constant variables or not.</p>
</td></tr>
<tr><td><code id="bMM_+3A_rm.dupe">rm.dupe</code></td>
<td>
<p>To remove duplicate variables or not.</p>
</td></tr>



<tr><td><code id="bMM_+3A_method">method</code>, <code id="bMM_+3A_use">use</code></td>
<td>
<p>Correlation options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>numcut==0</code> (the default), then a matrix of the
covariates is returned; otherwise, a list is returned with the
following values. 
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p>A matrix of the covariates with <code>n</code> rows and <code>p</code>
columns.</p>
</td></tr>
<tr><td><code>numcut</code></td>
<td>
<p>A vector of length <code>p</code> of the number of cut-points
for each covariate.</p>
</td></tr>
<tr><td><code>grp</code></td>
<td>
<p>A vector that corresponds to variables in the input
<code>data.frame</code> that were translated into dummy columns in the
output matrix, i.e., for each input variable in order, there is a
number in the vector corresponding to the number of output columns
created for it. </p>
</td></tr>
<tr><td><code>dummy</code></td>
<td>
<p>Corresponds to <code>grp</code> with a two row matrix
including the start column of each dummy group in row 1
and the end column in row 2.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+xicuts">xicuts</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(99)

a &lt;- rbinom(10, 4, 0.4)

table(a)

x &lt;- runif(10)

df &lt;- data.frame(a=factor(a), x=x)

(b &lt;- bMM(df))

(b &lt;- bMM(df, numcut=9))

(b &lt;- bMM(df, numcut=9, usequants=TRUE))

## Not run: 
    ## this is an error
    f &lt;- bMM(as.character(a))

## End(Not run)
</code></pre>

<hr>
<h2 id='bmx'>NHANES 1999-2000 Body Measures and Demographics</h2><span id='topic+bmx'></span>

<h3>Description</h3>

<p> This data set was created from the National Health and
Nutrition Examination Survey (NHANES) 1999-2000 Body Measures Exam and
Demographics. To create growth charts, this data is restricted to
3435 children aged 2 to 17. </p>


<h3>Usage</h3>

<pre><code class='language-R'>data(bmx)
</code></pre>


<h3>Format</h3>


<table>
<tr>
 <td style="text-align: left;">
    SEQN:</td><td style="text-align: left;"> Sequence number</td>
</tr>
<tr>
 <td style="text-align: left;">
    BMXHT:</td><td style="text-align: left;"> Height in cm</td>
</tr>
<tr>
 <td style="text-align: left;">
    RIAGENDR:</td><td style="text-align: left;"> Gender: 1=male, 2=female</td>
</tr>
<tr>
 <td style="text-align: left;">
    RIDAGEEX:</td><td style="text-align: left;"> Age in years with fractions for months</td>
</tr>
<tr>
 <td style="text-align: left;">
    RIDRETH2:</td><td style="text-align: left;"> Race/ethnicity: 1=Non-Hispanic White,
    2=Non-Hispanic Black, 3=Hispanic</td>
</tr>
<tr>
 <td style="text-align: left;">
    BMXWT:</td><td style="text-align: left;"> Weight in kg</td>
</tr>
<tr>
 <td style="text-align: left;">
  </td>
</tr>

</table>



<h3>References</h3>

<p>National Health and Nutrition Examination Survey 
1999-2000 Body Measures Exam.
<a href="https://wwwn.cdc.gov/Nchs/Nhanes/1999-2000/BMX.htm">https://wwwn.cdc.gov/Nchs/Nhanes/1999-2000/BMX.htm</a>
</p>
<p>National Health and Nutrition Examination Survey 
1999-2000 Demographics.
<a href="https://wwwn.cdc.gov/Nchs/Nhanes/1999-2000/DEMO.htm">https://wwwn.cdc.gov/Nchs/Nhanes/1999-2000/DEMO.htm</a>
</p>

<hr>
<h2 id='CDCheight'>CDC height for age growth charts</h2><span id='topic+CDCheight'></span>

<h3>Description</h3>

<p> Using the Cole and Green LMS method, here we provide
percentiles of height by age and sex based on the US National Center for Health
Statistics data for children aged 2 to 17. </p>


<h3>Usage</h3>

<pre><code class='language-R'>data(CDCheight)
</code></pre>


<h3>Format</h3>


<table>
<tr>
 <td style="text-align: left;">
    age:</td><td style="text-align: left;"> Age in years</td>
</tr>
<tr>
 <td style="text-align: left;">
    sex:</td><td style="text-align: left;"> 1=male, 2=female</td>
</tr>
<tr>
 <td style="text-align: left;">
    height.XXX:</td><td style="text-align: left;"> Height XXXth percentile in cm</td>
</tr>
<tr>
 <td style="text-align: left;">
  </td>
</tr>

</table>



<h3>References</h3>

<p>Cole, Timothy J and Green, Pamela J (1992)
Smoothing reference centile curves: the LMS method and penalized
likelihood. <em>Statistics in medicine</em>, <b>11</b>, 1305&ndash;1319.
</p>
<p>The US Centers for Disease Control and Prevention stature by age LMS parameters
<a href="https://www.cdc.gov/growthcharts/data/zscore/statage.csv">https://www.cdc.gov/growthcharts/data/zscore/statage.csv</a>
</p>

<hr>
<h2 id='CDimpute'>Cold-deck missing imputation</h2><span id='topic+CDimpute'></span>

<h3>Description</h3>

<p>This function imputes missing data.</p>


<h3>Usage</h3>

<pre><code class='language-R'>CDimpute(x.train, x.test=matrix(0, 0, 0), impute.bin=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CDimpute_+3A_x.train">x.train</code></td>
<td>
<p>The training matrix.</p>
</td></tr>
<tr><td><code id="CDimpute_+3A_x.test">x.test</code></td>
<td>
<p>The testing matrix, if given.</p>
</td></tr>
<tr><td><code id="CDimpute_+3A_impute.bin">impute.bin</code></td>
<td>
<p>An index of the columns to avoid
imputing which will be handled by BART internally.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>We call this method cold-decking in analogy to hot-decking.
Hot-decking was a method commonly employed with US Census data
in the early computing era.  For a particular respondent, missing
data was imputed by randomly selecting from the responses of their
neighbors since it is assumed that the values are likely similar.
In our case, we make no assumptions about which values may, or may
not, be nearby.  We simply take a random sample from the matrix
rows to impute the missing data.  If the training and testing
matrices are the same, then they receive the same imputation.
</p>


<h3>Value</h3>

<table>
<tr><td><code>x.train</code></td>
<td>
<p>The imputed training matrix.</p>
</td></tr>
<tr><td><code>x.test</code></td>
<td>
<p>The imputed testing matrix.</p>
</td></tr>
<tr><td><code>miss.train</code></td>
<td>
<p>A summary of the missing variables for training.</p>
</td></tr>
<tr><td><code>miss.test</code></td>
<td>
<p>A summary of the missing variables for testing.</p>
</td></tr>
<tr><td><code>impute.flag</code></td>
<td>
<p>Whether <code>impute.bin</code> columns were, or were
not, imputed.</p>
</td></tr>
<tr><td><code>same</code></td>
<td>
<p>Whether <code>x.train</code> and <code>x.test</code> are identical.</p>
</td></tr>
</table>

<hr>
<h2 id='Cindex'>Calculate the C-index/concordance for survival analysis.</h2><span id='topic+Cindex'></span><span id='topic+concordance'></span>

<h3>Description</h3>

<p> The C-index for survival analysis is the corollary of the
c statistic (the area under the Receiver Operating Characteristic curve)
for binary outcomes. As a probability, the higher is the C-index, the
better is the model discrimination vs. lesser probability
values. Similarly, the concordance is calculated like the C-index from
z-draws via the posterior predictive distribution restricted to the
horizon of the data (a la restricted mean survival time).</p>


<h3>Usage</h3>

<pre><code class='language-R'>Cindex(risk, times, delta=NULL)

concordance(draws, times, delta=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Cindex_+3A_risk">risk</code></td>
<td>
<p>A vector or prognostic risk scores. </p>
</td></tr>
<tr><td><code id="Cindex_+3A_draws">draws</code></td>
<td>
<p>A vector of draws via
the posterior predictive distribution restricted to the horizon of
the data (a la restricted mean survival time). </p>
</td></tr>
<tr><td><code id="Cindex_+3A_times">times</code></td>
<td>
<p>A vector of failure times. </p>
</td></tr>
<tr><td><code id="Cindex_+3A_delta">delta</code></td>
<td>
<p> The corresponding failure time status code: 0,
right-censored; 1, failure;
or 2, left-censored. Defaults to all failures if not specified.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The return value is the calculated C-index/concordance.
</p>


<h3>References</h3>

<p>Harrell FE, Califf RM, Pryor DB, Lee KL, Rosati RA. (1982)
Evaluating the yield of medical tests. JAMA, May 14;247(18):2543-6.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.nft">predict.nft</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(lung)
N=length(lung$status)

##lung$status: 1=censored, 2=dead
##delta: 0=censored, 1=dead
delta=lung$status-1

## this study reports time in days
times=lung$time
times=times/7  ## weeks

## matrix of covariates
x.train=cbind(lung[ , -(1:3)])
## lung$sex:        Male=1 Female=2

## Not run: 
    set.seed(99)
    post=nft(x.train, times, delta, K=0)
    pred=predict(post, x.train, XPtr=TRUE, seed=21)
    print(Cindex(pred$logt.test.mean, times, delta))

## End(Not run)

</code></pre>

<hr>
<h2 id='lung'>NCCTG Lung Cancer Data</h2><span id='topic+cancer'></span><span id='topic+lung'></span>

<h3>Description</h3>

<p> Survival for 228 patients with advanced lung cancer was
recorded up to a median of roughly one year by the North Central Cancer
Treatment Group.  Performance scores rate how well the patient can
perform usual daily activities.  </p>


<h3>Format</h3>


<table>
<tr>
 <td style="text-align: left;">
    inst:</td><td style="text-align: left;"> Institution code</td>
</tr>
<tr>
 <td style="text-align: left;">
    time:</td><td style="text-align: left;"> Survival time in days</td>
</tr>
<tr>
 <td style="text-align: left;">
    status:</td><td style="text-align: left;"> censoring status 1=censored, 2=dead</td>
</tr>
<tr>
 <td style="text-align: left;">
    age:</td><td style="text-align: left;"> Age in years</td>
</tr>
<tr>
 <td style="text-align: left;">
    sex:</td><td style="text-align: left;">  Male=1 Female=2</td>
</tr>
<tr>
 <td style="text-align: left;">
    ph.ecog:</td><td style="text-align: left;"> ECOG performance score (0=good 5=dead)</td>
</tr>
<tr>
 <td style="text-align: left;">
    ph.karno:</td><td style="text-align: left;"> Karnofsky performance score (bad=0-good=100) rated by physician</td>
</tr>
<tr>
 <td style="text-align: left;">
    pat.karno:</td><td style="text-align: left;"> Karnofsky performance score as rated by patient</td>
</tr>
<tr>
 <td style="text-align: left;">
    meal.cal:</td><td style="text-align: left;"> Calories consumed at meals</td>
</tr>
<tr>
 <td style="text-align: left;">
    wt.loss:</td><td style="text-align: left;"> Weight loss in last six months</td>
</tr>
<tr>
 <td style="text-align: left;">
  </td>
</tr>

</table>



<h3>Source</h3>

<p>Terry Therneau</p>


<h3>References</h3>

<p>Loprinzi CL. Laurie JA. Wieand HS. Krook JE. Novotny PJ.
Kugler JW. Bartel J. Law M. Bateman M. Klatt NE. et al.
Prospective evaluation of prognostic variables from patient-completed
questionnaires. North Central Cancer Treatment Group.
Journal of Clinical Oncology. 12(3):601-7, 1994. </p>


<h3>Examples</h3>

<pre><code class='language-R'>data(lung)
</code></pre>

<hr>
<h2 id='nft2'>Fit NFT BART models.</h2><span id='topic+nft2'></span><span id='topic+nft'></span>

<h3>Description</h3>

<p>The <code>nft2()/nft()</code> function is for fitting
NFT BART (Nonparametric Failure Time
Bayesian Additive Regression Tree) models
with different train/test matrices for
<code class="reqn">f</code> and <code class="reqn">sd</code> functions. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nft2(
    ## data
    xftrain, xstrain, times, delta=NULL,
    xftest=matrix(nrow=0, ncol=0),
    xstest=matrix(nrow=0, ncol=0),
    rm.const=TRUE, rm.dupe=TRUE,
    ## multi-threading
    tc=getOption("mc.cores", 1), 
    ##MCMC
    nskip=1000, ndpost=2000, nadapt=1000, adaptevery=100,
    chvf=NULL, chvs=NULL,
    method="spearman", use="pairwise.complete.obs",
    pbd=c(0.7, 0.7), pb=c(0.5, 0.5),
    stepwpert=c(0.1, 0.1), probchv=c(0.1, 0.1),
    minnumbot=c(5, 5),
    ## BART and HBART prior parameters
    ntree=c(50, 10), numcut=100, 
    xifcuts=NULL, xiscuts=NULL,
    power=c(2, 2), base=c(0.95, 0.95),
    ## f function
    fmu=NA, k=5, tau=NA, dist='weibull', 
    ## s function
    total.lambda=NA, total.nu=10, mask=NULL,
    ## survival analysis 
    K=100, events=NULL, TSVS=FALSE,
    ## DPM LIO
    drawDPM=1L, 
    alpha=1, alpha.a=1, alpha.b=0.1, alpha.draw=1,
    neal.m=2, constrain=1, 
    m0=0, k0.a=1.5, k0.b=7.5, k0=1, k0.draw=1,
    a0=3, b0.a=2, b0.b=1, b0=1, b0.draw=1,
    ## misc
    na.rm=FALSE, probs=c(0.025, 0.975), printevery=100,
    transposed=FALSE, pred=FALSE
)

nft(
    ## data
    x.train, times, delta=NULL, x.test=matrix(nrow=0, ncol=0),
    rm.const=TRUE, rm.dupe=TRUE,
    ## multi-threading
    tc=getOption("mc.cores", 1), 
    ##MCMC
    nskip=1000, ndpost=2000, nadapt=1000, adaptevery=100,
    chv=NULL,
    method="spearman", use="pairwise.complete.obs",
    pbd=c(0.7, 0.7), pb=c(0.5, 0.5),
    stepwpert=c(0.1, 0.1), probchv=c(0.1, 0.1),
    minnumbot=c(5, 5),
    ## BART and HBART prior parameters
    ntree=c(50, 10), numcut=100, xicuts=NULL,
    power=c(2, 2), base=c(0.95, 0.95),
    ## f function
    fmu=NA, k=5, tau=NA, dist='weibull', 
    ## s function
    total.lambda=NA, total.nu=10, mask=NULL,
    ## survival analysis 
    K=100, events=NULL, TSVS=FALSE,
    ## DPM LIO
    drawDPM=1L, 
    alpha=1, alpha.a=1, alpha.b=0.1, alpha.draw=1,
    neal.m=2, constrain=1, 
    m0=0, k0.a=1.5, k0.b=7.5, k0=1, k0.draw=1,
    a0=3, b0.a=2, b0.b=1, b0=1, b0.draw=1,
    ## misc
    na.rm=FALSE, probs=c(0.025, 0.975), printevery=100,
    transposed=FALSE, pred=FALSE
)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nft2_+3A_xftrain">xftrain</code></td>
<td>
<p>n x pf matrix of predictor variables for the training data.</p>
</td></tr>
<tr><td><code id="nft2_+3A_xstrain">xstrain</code></td>
<td>
<p>n x ps matrix of predictor variables for the training data.</p>
</td></tr>
<tr><td><code id="nft2_+3A_x.train">x.train</code></td>
<td>
<p>n x p matrix of predictor variables for the training data.</p>
</td></tr>
<tr><td><code id="nft2_+3A_times">times</code></td>
<td>
<p>n x 1 vector of the observed times for the training data.</p>
</td></tr>
<tr><td><code id="nft2_+3A_delta">delta</code></td>
<td>
<p>n x 1 vector of the time type for the training data:
0, for right-censoring; 1, for an event; and, 2, for left-censoring.</p>
</td></tr>
<tr><td><code id="nft2_+3A_xftest">xftest</code></td>
<td>
<p>m x pf matrix of predictor variables for the test set.</p>
</td></tr>
<tr><td><code id="nft2_+3A_xstest">xstest</code></td>
<td>
<p>m x ps matrix of predictor variables for the test set.</p>
</td></tr>
<tr><td><code id="nft2_+3A_x.test">x.test</code></td>
<td>
<p>m x p matrix of predictor variables for the test set.</p>
</td></tr>


<tr><td><code id="nft2_+3A_rm.const">rm.const</code></td>
<td>
<p>To remove constant variables or not.</p>
</td></tr>
<tr><td><code id="nft2_+3A_rm.dupe">rm.dupe</code></td>
<td>
<p>To remove duplicate variables or not.</p>
</td></tr>


<tr><td><code id="nft2_+3A_tc">tc</code></td>
<td>
<p>Number of OpenMP threads to use.</p>
</td></tr>
<tr><td><code id="nft2_+3A_nskip">nskip</code></td>
<td>
<p>Number of MCMC iterations to burn-in and discard.</p>
</td></tr>
<tr><td><code id="nft2_+3A_ndpost">ndpost</code></td>
<td>
<p>Number of MCMC iterations kept after burn-in.</p>
</td></tr>
<tr><td><code id="nft2_+3A_nadapt">nadapt</code></td>
<td>
<p>Number of MCMC iterations for adaptation prior to burn-in.</p>
</td></tr>
<tr><td><code id="nft2_+3A_adaptevery">adaptevery</code></td>
<td>
<p>Adapt MCMC proposal distributions every <code>adaptevery</code> iteration.</p>
</td></tr>
<tr><td><code id="nft2_+3A_chvf">chvf</code>, <code id="nft2_+3A_chvs">chvs</code>, <code id="nft2_+3A_chv">chv</code></td>
<td>
<p>Predictor correlation matrix used as a pre-conditioner for MCMC change-of-variable proposals.</p>
</td></tr>
<tr><td><code id="nft2_+3A_method">method</code>, <code id="nft2_+3A_use">use</code></td>
<td>
<p>Correlation options for change-of-variable proposal
pre-conditioner.</p>
</td></tr>
<tr><td><code id="nft2_+3A_pbd">pbd</code></td>
<td>
<p>Probability of performing a birth/death proposal, otherwise perform a rotate proposal.</p>
</td></tr>
<tr><td><code id="nft2_+3A_pb">pb</code></td>
<td>
<p>Probability of performing a birth proposal given that we choose to perform a birth/death proposal.</p>
</td></tr>
<tr><td><code id="nft2_+3A_stepwpert">stepwpert</code></td>
<td>
<p>Initial width of proposal distribution for peturbing cut-points.</p>
</td></tr>
<tr><td><code id="nft2_+3A_probchv">probchv</code></td>
<td>
<p>Probability of performing a change-of-variable proposal.  Otherwise, only do a perturb proposal.</p>
</td></tr>
<tr><td><code id="nft2_+3A_minnumbot">minnumbot</code></td>
<td>
<p>Minimum number of observations required in leaf (terminal) nodes.</p>
</td></tr>
<tr><td><code id="nft2_+3A_ntree">ntree</code></td>
<td>
<p>Vector of length two for the number of trees used for the mean
model and the number of trees used for the variance model.</p>
</td></tr>
<tr><td><code id="nft2_+3A_numcut">numcut</code></td>
<td>
<p>Number of cutpoints to use for each predictor variable.</p>
</td></tr>
<tr><td><code id="nft2_+3A_xifcuts">xifcuts</code>, <code id="nft2_+3A_xiscuts">xiscuts</code>, <code id="nft2_+3A_xicuts">xicuts</code></td>
<td>
<p>More detailed construction of cut-points can be specified
by the <code>xicuts</code> function and provided here.</p>
</td></tr>
<tr><td><code id="nft2_+3A_power">power</code></td>
<td>
<p>Power parameter in the tree depth penalizing prior.</p>
</td></tr>
<tr><td><code id="nft2_+3A_base">base</code></td>
<td>
<p>Base parameter in the tree depth penalizing prior.</p>
</td></tr>
<tr><td><code id="nft2_+3A_fmu">fmu</code></td>
<td>
<p>Prior parameter for the center of the mean model.</p>
</td></tr>
<tr><td><code id="nft2_+3A_k">k</code></td>
<td>
<p>Prior parameter for the mean model.</p>
</td></tr>
<tr><td><code id="nft2_+3A_tau">tau</code></td>
<td>
<p>Desired <code>SD/ntree</code> for f function leaf prior if known.</p>
</td></tr>
<tr><td><code id="nft2_+3A_dist">dist</code></td>
<td>
<p>Distribution to be passed to intercept-only AFT model to center <code>y.train</code>.</p>
</td></tr>
<tr><td><code id="nft2_+3A_total.lambda">total.lambda</code></td>
<td>
<p>A rudimentary estimate of the process standard deviation. Used in calibrating the variance prior.</p>
</td></tr>
<tr><td><code id="nft2_+3A_total.nu">total.nu</code></td>
<td>
<p>Shape parameter for the variance prior.</p>
</td></tr>
<tr><td><code id="nft2_+3A_mask">mask</code></td>
<td>
<p>If a proportion is provided, then said quantile
of <code>max.i sd(x.i)</code> is used to mask non-stationary
departures (with respect to convergence) above this threshold.</p>
</td></tr>
<tr><td><code id="nft2_+3A_k">K</code></td>
<td>
<p>Number of grid points for which to estimate survival probability.</p>
</td></tr>
<tr><td><code id="nft2_+3A_events">events</code></td>
<td>
<p>Grid points for which to estimate survival probability.</p>
</td></tr>
<tr><td><code id="nft2_+3A_tsvs">TSVS</code></td>
<td>
<p>Setting to <code>TRUE</code> will avoid unnecessary processing for
Thompson sampling variable selection, i.e., all that is needed is
the variable counts from the tree branch decision rules.</p>
</td></tr>
<tr><td><code id="nft2_+3A_drawdpm">drawDPM</code></td>
<td>
<p>Whether to utilize DPM or not.</p>
</td></tr>
<tr><td><code id="nft2_+3A_alpha">alpha</code></td>
<td>
<p>Initial value of DPM concentration parameter.</p>
</td></tr>
<tr><td><code id="nft2_+3A_alpha.a">alpha.a</code></td>
<td>
<p>Gamma prior parameter setting for DPM concentration parameter
where E[<code>alpha</code>]=<code>alpha.a</code>/<code>alpha.b</code>.</p>
</td></tr>
<tr><td><code id="nft2_+3A_alpha.b">alpha.b</code></td>
<td>
<p>See <code>alpha.a</code> above.</p>
</td></tr>
<tr><td><code id="nft2_+3A_alpha.draw">alpha.draw</code></td>
<td>
<p>Whether to draw <code>alpha</code> or it is fixed at the initial value.</p>
</td></tr>
<tr><td><code id="nft2_+3A_neal.m">neal.m</code></td>
<td>
<p>The number of additional atoms for Neal 2000 DPM algorithm 8.</p>
</td></tr>
<tr><td><code id="nft2_+3A_constrain">constrain</code></td>
<td>
<p>Whether to perform constained DPM or unconstrained.</p>
</td></tr>
<tr><td><code id="nft2_+3A_m0">m0</code></td>
<td>
<p>Center of the error distribution: defaults to zero.</p>
</td></tr>
<tr><td><code id="nft2_+3A_k0.a">k0.a</code></td>
<td>
<p>First Gamma prior argument for <code>k0</code>.</p>
</td></tr>
<tr><td><code id="nft2_+3A_k0.b">k0.b</code></td>
<td>
<p>Second Gamma prior argument for <code>k0</code>.</p>
</td></tr>
<tr><td><code id="nft2_+3A_k0">k0</code></td>
<td>
<p>Initial value of <code>k0</code>.</p>
</td></tr>
<tr><td><code id="nft2_+3A_k0.draw">k0.draw</code></td>
<td>
<p>Whether to fix k0 or draw it if from the DPM LIO prior
hierarchy: <code>k0~Gamma(k0.a, k0.b)</code>, i.e., <code>E[k0]=k0.a/k0.b</code>.</p>
</td></tr>
<tr><td><code id="nft2_+3A_a0">a0</code></td>
<td>
<p>First Gamma prior argument for <code class="reqn">tau</code>.</p>
</td></tr>
<tr><td><code id="nft2_+3A_b0.a">b0.a</code></td>
<td>
<p>First Gamma prior argument for <code>b0</code>.</p>
</td></tr>
<tr><td><code id="nft2_+3A_b0.b">b0.b</code></td>
<td>
<p>Second Gamma prior argument for <code>b0</code>.</p>
</td></tr>
<tr><td><code id="nft2_+3A_b0">b0</code></td>
<td>
<p>Initial value of <code>b0</code>.</p>
</td></tr>
<tr><td><code id="nft2_+3A_b0.draw">b0.draw</code></td>
<td>
<p>Whether to fix b0 or draw it from the DPM LIO prior 
hierarchy: <code>b0~Gamma(b0.a, b0.b)</code>, i.e.,
<code>E[b0]=b0.a/b0.b</code>.</p>
</td></tr>
<tr><td><code id="nft2_+3A_na.rm">na.rm</code></td>
<td>
<p>Value to be passed to the <code>predict</code> function.</p>
</td></tr>
<tr><td><code id="nft2_+3A_probs">probs</code></td>
<td>
<p>Value to be passed to the <code>predict</code> function.</p>
</td></tr>
<tr><td><code id="nft2_+3A_printevery">printevery</code></td>
<td>
<p>Outputs MCMC algorithm status every printevery
iterations.</p>
</td></tr>
<tr><td><code id="nft2_+3A_transposed">transposed</code></td>
<td>
<p>Specify <code>TRUE</code> if all of the pre-processing for
<code>xftrain/xstrain/xftest/xstest</code> has been conducted prior to the call (including
tranposing).</p>
</td></tr>
<tr><td><code id="nft2_+3A_pred">pred</code></td>
<td>
<p>Specify <code>TRUE</code> if you want to return the <code>pred</code>
item that is used to calculate <code>soffset</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>nft2()/nft()</code> is the function to fit time-to-event data.  The most general form of the model allowed is
<code class="reqn">Y({\bf x})=mu+f({\bf x})+sd({\bf x})Z</code>
where <code class="reqn">E</code> follows a nonparametric error distribution
by default.








The <code>nft2()/nft()</code> function returns a fit object of S3 class type
<code>nft2/nft</code> that is essentially a list containing the following items.
</p>


<h3>Value</h3>



<table>
<tr><td><code>ots</code>, <code>oid</code>, <code>ovar</code>, <code>oc</code>, <code>otheta</code></td>
<td>
<p>These are <code>XPtr</code>s to the BART
<code class="reqn">f(x)</code> objects in RAM that are only available for fits generated
in the current R session. </p>
</td></tr>
<tr><td><code>sts</code>, <code>sid</code>, <code>svar</code>, <code>sc</code>, <code>stheta</code></td>
<td>
<p>Similarly, these are <code>XPtr</code>s to the
HBART <code class="reqn">sd(x)</code> objects.</p>
</td></tr>
<tr><td><code>fmu</code></td>
<td>
<p>The constant <code class="reqn">mu</code>.</p>
</td></tr>
<tr><td><code>f.train</code>, <code>s.train</code></td>
<td>
<p>The trained <code class="reqn">f(x)</code> and <code class="reqn">sd(x)</code>
respectively: matrices with <code>ndpost</code> rows and <code class="reqn">n</code> columns.</p>
</td></tr>
<tr><td><code>f.train.mean</code>, <code>s.train.mean</code></td>
<td>
<p>The posterior mean of the trained
<code class="reqn">f(x)</code> and <code class="reqn">sd(x)</code> respectively: vectors of length <code class="reqn">n</code>.</p>
</td></tr>
<tr><td><code>f.trees</code>, <code>s.trees</code></td>
<td>
<p>Character strings representing the trained fits
of <code class="reqn">f(x)</code> and <code class="reqn">sd(x)</code> respectively to facilitate usage of the
<code>predict</code> function when <code>XPtr</code>s are unavailable.</p>
</td></tr>
<tr><td><code>dpalpha</code></td>
<td>
<p>The draws of the DPM concentration parameter
<code class="reqn">alpha</code>.</p>
</td></tr>
<tr><td><code>dpn</code>, <code>dpn.</code></td>
<td>
<p>The number of atom clusters per DPM, <code class="reqn">J</code>, for all
draws including burn-in and excluding burn-in respectively.</p>
</td></tr>
<tr><td><code>dpmu</code></td>
<td>
<p>The draws of the DPM parameter <code class="reqn">mu[i]</code> where
<code class="reqn">i=1,...,n</code> indexes subjects: a matrix with <code>ndpost</code> rows and
<code class="reqn">n</code> columns.</p>
</td></tr>
<tr><td><code>dpmu.</code></td>
<td>
<p>The draws of the DPM parameter <code class="reqn">mu[j]</code> where
<code class="reqn">j=1,...,J</code> indexes atom clusters: a matrix with <code>ndpost</code>
rows and <code class="reqn">J</code> columns.</p>
</td></tr>
<tr><td><code>dpwt.</code></td>
<td>
<p>The weights for efficient DPM calculations by atom clusters
(as opposed to subjects) for use with <code class="reqn">dpmu.</code> (and <code class="reqn">dpsd.</code>;
see below): a matrix with <code>ndpost</code> rows and <code class="reqn">J</code> columns.</p>
</td></tr>
<tr><td><code>dpsd</code>, <code>dpsd.</code></td>
<td>
<p>Similarly, the draws of the DPM parameter <code class="reqn">tau[i]</code>
transformed into the standard deviation <code class="reqn">sigma[i]</code> for
convenience.</p>
</td></tr>
<tr><td><code>dpC</code></td>
<td>
<p>The indices <code class="reqn">j</code> for each subject <code class="reqn">i</code> corresponding to
their shared atom cluster.</p>
</td></tr>
<tr><td><code>z.train</code></td>
<td>
<p>The data values/augmentation draws of <code class="reqn">log t</code>.</p>
</td></tr>
<tr><td><code>f.tmind/f.tavgd/f.tmaxd</code></td>
<td>
<p>The min/average/max tier degree of trees in the <code class="reqn">f</code> ensemble.</p>
</td></tr>
<tr><td><code>s.tmind/s.tavgd/s.tmaxd</code></td>
<td>
<p>The min/average/max tier degree of trees in the <code class="reqn">s</code> ensemble.</p>
</td></tr>
<tr><td><code>f.varcount</code>, <code>s.varcount</code></td>
<td>
<p>Variable importance counts of branch
decision rules for each <code class="reqn">x</code> of <code class="reqn">f</code> and <code class="reqn">s</code> respectively:
matrices with <code>ndpost</code> rows and <code class="reqn">p</code> columns.</p>
</td></tr>
<tr><td><code>f.varcount.mean</code>, <code>s.varcount.mean</code></td>
<td>
<p>Similarly, the posterior mean
of the variable importance counts for each <code class="reqn">x</code> of <code class="reqn">f</code> and
<code class="reqn">s</code> respectively: vectors of length <code class="reqn">p</code>.</p>
</td></tr>
<tr><td><code>f.varprob</code>, <code>s.varprob</code></td>
<td>
<p>Similarly, re-weighting the posterior mean
of the variable importance counts as sum-to-one probabilities for each
<code class="reqn">x</code> of <code class="reqn">f</code> and <code class="reqn">s</code> respectively: vectors of length
<code class="reqn">p</code>.</p>
</td></tr>
<tr><td><code>LPML</code></td>
<td>
<p>The log Pseudo-Marginal Likelihood as typically
calculated for right-/left-censoring.</p>
</td></tr>
<tr><td><code>pred</code></td>
<td>
<p>The object returned from the <code>predict</code> function where
<code>x.test=x.train</code> in order to calculate the <code>soffset</code>
item that is needed to use <code>predict</code> when <code>XPtr</code>s are not
available. </p>
</td></tr>
<tr><td><code>soffset</code></td>
<td>
<p>See <code>pred</code> above.</p>
</td></tr>
<tr><td><code>aft</code></td>
<td>
<p>The AFT model fit used to initialize NFT BART.</p>
</td></tr>
<tr><td><code>elapsed</code></td>
<td>
<p>The elapsed time of the run in seconds.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Rodney Sparapani: <a href="mailto:rsparapa@mcw.edu">rsparapa@mcw.edu</a>
</p>


<h3>References</h3>

<p>Sparapani R., Logan B., Maiers M., Laud P., McCulloch R. (2023)
Nonparametric Failure Time: Time-to-event Machine Learning with
Heteroskedastic Bayesian Additive Regression Trees and 
Low Information Omnibus Dirichlet Process Mixtures
<em>Biometrics (ahead of print)</em> &lt;doi:10.1111/biom.13857&gt;.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.nft2">predict.nft2</a></code>, <code><a href="#topic+predict.nft">predict.nft</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##library(nftbart)
data(lung)
N=length(lung$status)

##lung$status: 1=censored, 2=dead
##delta: 0=censored, 1=dead
delta=lung$status-1

## this study reports time in days rather than weeks or months
times=lung$time
times=times/7  ## weeks

## matrix of covariates
x.train=cbind(lung[ , -(1:3)])
## lung$sex:        Male=1 Female=2

## token run just to test installation
post=nft2(x.train, x.train, times, delta, K=0,
         nskip=0, ndpost=10, nadapt=4, adaptevery=1)


set.seed(99)
post=nft2(x.train, x.train, times, delta, K=0)
XPtr=TRUE

x.test = rbind(x.train, x.train)
x.test[ , 2]=rep(1:2, each=N)
K=75
events=seq(0, 150, length.out=K+1)
pred = predict(post, x.test, x.test, K=K, events=events[-1],
               XPtr=XPtr, FPD=TRUE)

plot(events, c(1, pred$surv.fpd.mean[1:K]), type='l', col=4,
     ylim=0:1, 
     xlab=expression(italic(t)), sub='weeks',
     ylab=expression(italic(S)(italic(t), italic(x))))
lines(events, c(1, pred$surv.fpd.upper[1:K]), lty=2, lwd=2, col=4)
lines(events, c(1, pred$surv.fpd.lower[1:K]), lty=2, lwd=2, col=4)
lines(events, c(1, pred$surv.fpd.mean[K+1:K]), lwd=2, col=2)
lines(events, c(1, pred$surv.fpd.upper[K+1:K]), lty=2, lwd=2, col=2)
lines(events, c(1, pred$surv.fpd.lower[K+1:K]), lty=2, lwd=2, col=2)
legend('topright', c('Adv. lung cancer\nmortality example',
                     'M', 'F'), lwd=2, col=c(0, 4, 2), lty=1)


</code></pre>

<hr>
<h2 id='predict.aftree'>Estimating the survival and the hazard for AFT BART models.</h2><span id='topic+predict.aftree'></span>

<h3>Description</h3>

<p>The function <code>predict.aftree()</code> is provided for
performing posterior inference via test data set estimates
stored in a <code>aftree</code> object returned from <code>AFTree()</code> in a similar
fashion as that of <code>predict.nft</code>. N.B.
the <code>x.test</code> matrix must be provided on the <code>AFTree()</code>
function call.  Here we are only calculating the survival function
by default, and, if requested, the hazard as well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'aftree'
predict(
            ## data
            object,
            ## predictions
            events=NULL,
            FPD=FALSE,
            probs=c(0.025, 0.975),
            take.logs=TRUE,
            seed=NULL,
            ## default settings
            ndpost=nrow(object$mix.prop),
            nclust=ncol(object$mix.prop),
            ## etc.
            ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.aftree_+3A_object">object</code></td>
<td>
<p>Object of type <code>nft</code> from a previous call to <code>nft()</code>.</p>
</td></tr>
<tr><td><code id="predict.aftree_+3A_events">events</code></td>
<td>
<p>You must specify a grid of time-points; however, they
can be a matrix with rows for each subject.</p>
</td></tr>
<tr><td><code id="predict.aftree_+3A_fpd">FPD</code></td>
<td>
<p>Whether to yield the usual predictions or marginal predictions
calculated by the partial dependence function.</p>
</td></tr>


<tr><td><code id="predict.aftree_+3A_probs">probs</code></td>
<td>
<p>A vector of length two containing the
lower and upper quantiles to be calculated for the predictions.</p>
</td></tr>
<tr><td><code id="predict.aftree_+3A_take.logs">take.logs</code></td>
<td>
<p>Whether or not to take logarithms.</p>
</td></tr>
<tr><td><code id="predict.aftree_+3A_seed">seed</code></td>
<td>
<p>If provided, then this value is used to generate random
natural logarithms of event times from the predictive distribution.</p>
</td></tr>
<tr><td><code id="predict.aftree_+3A_ndpost">ndpost</code></td>
<td>
<p>The number of MCMC samples generated.</p>
</td></tr>
<tr><td><code id="predict.aftree_+3A_nclust">nclust</code></td>
<td>
<p>The number of DPM clusters generated.</p>
</td></tr>
<tr><td><code id="predict.aftree_+3A_...">...</code></td>
<td>
<p>The et cetera objects passed to the <code>predict</code> method.
Currently, it has no functionality.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Returns a list with the following entries. If
<code>hazard=TRUE</code> is specified, then a similar set of
entries for the hazard are produced.
</p>


<h3>Value</h3>

<table>
<tr><td><code>surv.fpd</code></td>
<td>
<p>Survival function posterior draws on a grid of
time-points by the partial dependence function when requested.</p>
</td></tr>
<tr><td><code>surv.fpd.mean</code></td>
<td>
<p>Survival function estimates on a grid of
time-points by the partial dependence function when requested.</p>
</td></tr>
<tr><td><code>surv.fpd.lower</code></td>
<td>
<p>Survival function lower quantiles on a grid of
time-points by the partial dependence function when requested.</p>
</td></tr>
<tr><td><code>surv.fpd.upper</code></td>
<td>
<p>Survival function upper quantiles on a grid of
time-points by the partial dependence function when requested.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Rodney Sparapani: <a href="mailto:rsparapa@mcw.edu">rsparapa@mcw.edu</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.nft">predict.nft</a></code>
</p>

<hr>
<h2 id='predict.nft2'>Drawing Posterior Predictive Realizations for NFT BART models.</h2><span id='topic+predict.nft2'></span><span id='topic+predict.nft'></span>

<h3>Description</h3>

<p>The function <code>predict.nft2()/predict.nft()</code> is the main function for drawing posterior predictive realizations at new inputs using a fitted model stored in a <code>nft2/nft</code> object returned from <code>nft2()/nft()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nft2'
predict(
            ## data
            object,
            xftest=object$xftrain,
            xstest=object$xstrain,
            ## multi-threading
            tc=getOption("mc.cores", 1), ##OpenMP thread count
            ## current process fit vs. previous process fit
            XPtr=TRUE,
            ## predictions
            K=0,
            events=object$events,
            FPD=FALSE,
            probs=c(0.025, 0.975),
            take.logs=TRUE,
            na.rm=FALSE,
            RMST.max=NULL,
            ## default settings for NFT:BART/HBART/DPM
            fmu=object$NFT$fmu,
            soffset=object$soffset,
            drawDPM=object$drawDPM,
            ## etc.
            ...)

## S3 method for class 'nft'
predict(
            ## data
            object,
            x.test=object$x.train,
            ## multi-threading
            tc=getOption("mc.cores", 1), ##OpenMP thread count
            ## current process fit vs. previous process fit
            XPtr=TRUE,
            ## predictions
            K=0,
            events=object$events,
            FPD=FALSE,
            probs=c(0.025, 0.975),
            take.logs=TRUE,
            na.rm=FALSE,
            RMST.max=NULL,
            ## default settings for NFT:BART/HBART/DPM
            fmu=object$NFT$fmu,
            soffset=object$soffset,
            drawDPM=object$drawDPM,
            ## etc.
            ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.nft2_+3A_object">object</code></td>
<td>
<p>Object of type <code>nft2/nft</code> from a previous call to
<code>nft2()/nft()</code>.</p>
</td></tr>
<tr><td><code id="predict.nft2_+3A_xftest">xftest</code>, <code id="predict.nft2_+3A_xstest">xstest</code>, <code id="predict.nft2_+3A_x.test">x.test</code></td>
<td>
<p>New input settings in the form of a matrix at which to construct predictions.  Defaults to the training inputs.</p>
</td></tr>
<tr><td><code id="predict.nft2_+3A_tc">tc</code></td>
<td>
<p>Number of OpenMP threads to use for parallel computing.</p>
</td></tr>
<tr><td><code id="predict.nft2_+3A_xptr">XPtr</code></td>
<td>
<p>If <code>object</code> was created during the currently running
R process, then (via an Rcpp <code>XPtr</code>) the BART/HBART tree
ensemble objects can be accessed in RAM; otherwise, those objects
will need to be loaded from their string encodings.</p>
</td></tr>
<tr><td><code id="predict.nft2_+3A_k">K</code></td>
<td>
<p>The length of the grid of time-points to be used
for survival predictions.  Set to zero to avoid these calculations
which can be time-consuming for large data sets.</p>
</td></tr>
<tr><td><code id="predict.nft2_+3A_events">events</code></td>
<td>
<p>You can specify the grid of time-points; otherwise,
they are derived from quantiles of the augmented event times.</p>
</td></tr>
<tr><td><code id="predict.nft2_+3A_fpd">FPD</code></td>
<td>
<p>Whether to yield the usual predictions or marginal predictions
calculated by the partial dependence function.</p>
</td></tr>




<tr><td><code id="predict.nft2_+3A_probs">probs</code></td>
<td>
<p>A vector of length two containing the
lower and upper quantiles to be calculated for the predictions.</p>
</td></tr>
<tr><td><code id="predict.nft2_+3A_take.logs">take.logs</code></td>
<td>
<p>Whether or not to take logarithms.</p>
</td></tr>
<tr><td><code id="predict.nft2_+3A_na.rm">na.rm</code></td>
<td>
<p>Whether <code>NA</code> values should be removed from the
summaries.</p>
</td></tr>
<tr><td><code id="predict.nft2_+3A_rmst.max">RMST.max</code></td>
<td>
<p>To calculate Restricted Mean Survival Time (RMST),
we need to set a reasonable time maxima. Typically, a clinically
important time that a majority (or a large
plurality) of censored subjects have been followed through that
point or beyond.</p>
</td></tr>


<tr><td><code id="predict.nft2_+3A_fmu">fmu</code></td>
<td>
<p>BART centering parameter for the test data.  Defaults to
the value used by <code>nft2()/nft()</code> when training the model.</p>
</td></tr>
<tr><td><code id="predict.nft2_+3A_soffset">soffset</code></td>
<td>
<p>HBART centering parameter for the test data.  Defaults to the value used by <code>nft2()/nft()</code> when training the model.</p>
</td></tr>
<tr><td><code id="predict.nft2_+3A_drawdpm">drawDPM</code></td>
<td>
<p>Whether NFT BART was fit with, or without, DPM.</p>
</td></tr>
<tr><td><code id="predict.nft2_+3A_...">...</code></td>
<td>
<p>The et cetera objects passed to the <code>predict</code> method.
Currently, it has no functionality.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>predict.nft2()/predict.nft()</code> is the main function for
calculating posterior predictions and uncertainties once a model has
been fit by <code>nft2()/nft()</code>.  
</p>
<p>Returns a list with the following entries.
</p>


<h3>Value</h3>

<table>
<tr><td><code>f.test</code></td>
<td>
<p>Posterior realizations of the mean function stored in a
matrix. Omitted if partial dependence functions are performed since
these will typically be large.</p>
</td></tr>
<tr><td><code>s.test</code></td>
<td>
<p>Posterior realizations of the SD function stored in a matrix. Omitted if partial dependence functions are performed since
these will typically be large.</p>
</td></tr>
<tr><td><code>f.test.mean</code></td>
<td>
<p>Posterior predictive mean of mean function.</p>
</td></tr>
<tr><td><code>f.test.lower</code></td>
<td>
<p>Posterior predictive lower quantile of mean function.</p>
</td></tr>
<tr><td><code>f.test.upper</code></td>
<td>
<p>Posterior predictive upper quantile of mean function.</p>
</td></tr>
<tr><td><code>s.test.mean</code></td>
<td>
<p>Posterior predictive mean of SD function.</p>
</td></tr>
<tr><td><code>s.test.lower</code></td>
<td>
<p>Posterior predictive lower quantile of SD function.</p>
</td></tr>
<tr><td><code>s.test.upper</code></td>
<td>
<p>Posterior predictive upper quantile of SD
function.</p>
</td></tr>
<tr><td><code>surv.fpd</code></td>
<td>
<p>Survival function posterior draws on a grid of
time-points by the partial dependence function when requested.</p>
</td></tr>
<tr><td><code>surv.fpd.mean</code></td>
<td>
<p>Survival function estimates on a grid of
time-points by the partial dependence function when requested.</p>
</td></tr>
<tr><td><code>surv.fpd.lower</code></td>
<td>
<p>Survival function lower quantiles on a grid of
time-points by the partial dependence function when requested.</p>
</td></tr>
<tr><td><code>surv.fpd.upper</code></td>
<td>
<p>Survival function upper quantiles on a grid of
time-points by the partial dependence function when requested.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Rodney Sparapani: <a href="mailto:rsparapa@mcw.edu">rsparapa@mcw.edu</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nft2">nft2</a></code>, <code><a href="#topic+nft">nft</a></code>
</p>

<hr>
<h2 id='tsvs2'>Variable selection with NFT BART models.</h2><span id='topic+tsvs2'></span><span id='topic+tsvs'></span>

<h3>Description</h3>

<p>The <code>tsvs2()/tsvs()</code> function is for Thompson sampling
variable selection with NFT BART. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tsvs2(
               ## data
               xftrain, xstrain, times, delta=NULL, 
               rm.const=TRUE, rm.dupe=TRUE,
               ##tsvs args
               K=20, a.=1, b.=0.5, C=0.5,
               rds.file='tsvs2.rds', pdf.file='tsvs2.pdf',
               ## multi-threading
               tc=getOption("mc.cores", 1), ##OpenMP thread count
               ##MCMC
               nskip=1000, ndpost=2000, 
               nadapt=1000, adaptevery=100, 
               chvf=NULL, chvs=NULL,
               method="spearman", use="pairwise.complete.obs",
               pbd=c(0.7, 0.7), pb=c(0.5, 0.5),
               stepwpert=c(0.1, 0.1), probchv=c(0.1, 0.1),
               minnumbot=c(5, 5),
               ## BART and HBART prior parameters
               ntree=c(10, 2), numcut=100,
               xifcuts=NULL, xiscuts=NULL,
               power=c(2, 2), base=c(0.95, 0.95),
               ## f function
               fmu=NA, k=5, tau=NA, dist='weibull', 
               ## s function
               total.lambda=NA, total.nu=10, mask=0.95,
               ## survival analysis 
               ##K=100, events=NULL, 
               ## DPM LIO
               drawDPM=1L, 
               alpha=1, alpha.a=1, alpha.b=0.1, alpha.draw=1,
               neal.m=2, constrain=1, 
               m0=0, k0.a=1.5, k0.b=7.5, k0=1, k0.draw=1,
               a0=3, b0.a=2, b0.b=1, b0=1, b0.draw=1,
               ## misc
               na.rm=FALSE, probs=c(0.025, 0.975), printevery=100,
               transposed=FALSE
)

tsvs(
               ## data
               x.train, times, delta=NULL, 
               rm.const=TRUE, rm.dupe=TRUE,
               ##tsvs args
               K=20, a.=1, b.=0.5, C=0.5,
               rds.file='tsvs.rds', pdf.file='tsvs.pdf',
               ## multi-threading
               tc=getOption("mc.cores", 1), ##OpenMP thread count
               ##MCMC
               nskip=1000, ndpost=2000, 
               nadapt=1000, adaptevery=100, 
               chv=NULL,
               method="spearman", use="pairwise.complete.obs",
               pbd=c(0.7, 0.7), pb=c(0.5, 0.5),
               stepwpert=c(0.1, 0.1), probchv=c(0.1, 0.1),
               minnumbot=c(5, 5),
               ## BART and HBART prior parameters
               ntree=c(10, 2), numcut=100, xicuts=NULL,
               power=c(2, 2), base=c(0.95, 0.95),
               ## f function
               fmu=NA, k=5, tau=NA, dist='weibull', 
               ## s function
               total.lambda=NA, total.nu=10, mask=0.95,
               ## survival analysis 
               ##K=100, events=NULL, 
               ## DPM LIO
               drawDPM=1L, 
               alpha=1, alpha.a=1, alpha.b=0.1, alpha.draw=1,
               neal.m=2, constrain=1, 
               m0=0, k0.a=1.5, k0.b=7.5, k0=1, k0.draw=1,
               a0=3, b0.a=2, b0.b=1, b0=1, b0.draw=1,
               ## misc
               na.rm=FALSE, probs=c(0.025, 0.975), printevery=100,
               transposed=FALSE
)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tsvs2_+3A_xftrain">xftrain</code></td>
<td>
<p>n x pf matrix of predictor variables for the training data.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_xstrain">xstrain</code></td>
<td>
<p>n x ps matrix of predictor variables for the training data.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_x.train">x.train</code></td>
<td>
<p>n x ps matrix of predictor variables for the training data.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_times">times</code></td>
<td>
<p>nx1 vector of the observed times for the training data.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_delta">delta</code></td>
<td>
<p>nx1 vector of the time type for the training data:
0, for right-censoring; 1, for an event; and, 2, for
left-censoring.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_rm.const">rm.const</code></td>
<td>
<p>To remove constant variables or not.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_rm.dupe">rm.dupe</code></td>
<td>
<p>To remove duplicate variables or not.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_k">K</code></td>
<td>
<p>The number of Thompson sampling steps to take.
Not to be confused with the size of the time grid for survival
distribution estimation.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_a.">a.</code></td>
<td>
<p>The prior parameter for successes of a Beta distribution.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_b.">b.</code></td>
<td>
<p>The prior parameter for failures of a Beta distribution.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_c">C</code></td>
<td>
<p>The probability cut-off for variable selection.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_rds.file">rds.file</code></td>
<td>
<p>File name to store RDS object containing Thompson
sampling parameters.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_pdf.file">pdf.file</code></td>
<td>
<p>File name to store PDF graphic of variables selected.</p>
</td></tr>



<tr><td><code id="tsvs2_+3A_tc">tc</code></td>
<td>
<p>Number of OpenMP threads to use.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_nskip">nskip</code></td>
<td>
<p>Number of MCMC iterations to burn-in and discard.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_ndpost">ndpost</code></td>
<td>
<p>Number of MCMC iterations kept after burn-in.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_nadapt">nadapt</code></td>
<td>
<p>Number of MCMC iterations for adaptation prior to burn-in.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_adaptevery">adaptevery</code></td>
<td>
<p>Adapt MCMC proposal distributions every <code>adaptevery</code> iteration.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_chvf">chvf</code>, <code id="tsvs2_+3A_chvs">chvs</code>, <code id="tsvs2_+3A_chv">chv</code></td>
<td>
<p>Predictor correlation matrix used as a pre-conditioner for MCMC change-of-variable proposals.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_method">method</code>, <code id="tsvs2_+3A_use">use</code></td>
<td>
<p>Correlation options for change-of-variable proposal
pre-conditioner.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_pbd">pbd</code></td>
<td>
<p>Probability of performing a birth/death proposal, otherwise perform a rotate proposal.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_pb">pb</code></td>
<td>
<p>Probability of performing a birth proposal given that we choose to perform a birth/death proposal.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_stepwpert">stepwpert</code></td>
<td>
<p>Initial width of proposal distribution for peturbing cut-points.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_probchv">probchv</code></td>
<td>
<p>Probability of performing a change-of-variable proposal.  Otherwise, only do a perturb proposal.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_minnumbot">minnumbot</code></td>
<td>
<p>Minimum number of observations required in leaf (terminal) nodes.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_ntree">ntree</code></td>
<td>
<p>Vector of length two for the number of trees used for the mean
model and the number of trees used for the variance model.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_numcut">numcut</code></td>
<td>
<p>Number of cutpoints to use for each predictor variable.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_xifcuts">xifcuts</code>, <code id="tsvs2_+3A_xiscuts">xiscuts</code>, <code id="tsvs2_+3A_xicuts">xicuts</code></td>
<td>
<p>More detailed construction of cut-points can be specified
by the <code>xicuts</code> function and provided here.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_power">power</code></td>
<td>
<p>Power parameter in the tree depth penalizing prior.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_base">base</code></td>
<td>
<p>Base parameter in the tree depth penalizing prior.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_fmu">fmu</code></td>
<td>
<p>Prior parameter for the center of the mean model.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_k">k</code></td>
<td>
<p>Prior parameter for the mean model.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_tau">tau</code></td>
<td>
<p>Desired <code>SD/ntree</code> for f function leaf prior if known.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_dist">dist</code></td>
<td>
<p>Distribution to be passed to intercept-only AFT model to center <code>y.train</code>.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_total.lambda">total.lambda</code></td>
<td>
<p>A rudimentary estimate of the process standard deviation. Used in calibrating the variance prior.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_total.nu">total.nu</code></td>
<td>
<p>Shape parameter for the variance prior.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_mask">mask</code></td>
<td>
<p>If a proportion is provided, then said quantile
of <code>max.i sd(x.i)</code> is used to mask non-stationary
departures (with respect to convergence) above this threshold.</p>
</td></tr>





<tr><td><code id="tsvs2_+3A_drawdpm">drawDPM</code></td>
<td>
<p>Whether to utilize DPM or not.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_alpha">alpha</code></td>
<td>
<p>Initial value of DPM concentration parameter.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_alpha.a">alpha.a</code></td>
<td>
<p>Gamma prior parameter setting for DPM concentration parameter
where E[<code>alpha</code>]=<code>alpha.a</code>/<code>alpha.b</code>.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_alpha.b">alpha.b</code></td>
<td>
<p>See <code>alpha.a</code> above.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_alpha.draw">alpha.draw</code></td>
<td>
<p>Whether to draw <code>alpha</code> or it is fixed at the initial value.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_neal.m">neal.m</code></td>
<td>
<p>The number of additional atoms for Neal 2000 DPM algorithm 8.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_constrain">constrain</code></td>
<td>
<p>Whether to perform constained DPM or unconstrained.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_m0">m0</code></td>
<td>
<p>Center of the error distribution: defaults to zero.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_k0.a">k0.a</code></td>
<td>
<p>First Gamma prior argument for <code>k0</code>.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_k0.b">k0.b</code></td>
<td>
<p>Second Gamma prior argument for <code>k0</code>.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_k0">k0</code></td>
<td>
<p>Initial value of <code>k0</code>.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_k0.draw">k0.draw</code></td>
<td>
<p>Whether to fix k0 or draw it if from the DPM LIO prior
hierarchy: <code>k0~Gamma(k0.a, k0.b)</code>, i.e., <code>E[k0]=k0.a/k0.b</code>.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_a0">a0</code></td>
<td>
<p>First Gamma prior argument for <code class="reqn">tau</code>.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_b0.a">b0.a</code></td>
<td>
<p>First Gamma prior argument for <code>b0</code>.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_b0.b">b0.b</code></td>
<td>
<p>Second Gamma prior argument for <code>b0</code>.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_b0">b0</code></td>
<td>
<p>Initial value of <code>b0</code>.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_b0.draw">b0.draw</code></td>
<td>
<p>Whether to fix b0 or draw it from the DPM LIO prior 
hierarchy: <code>b0~Gamma(b0.a, b0.b)</code>, i.e.,
<code>E[b0]=b0.a/b0.b</code>.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_na.rm">na.rm</code></td>
<td>
<p>Value to be passed to the <code>predict</code> function.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_probs">probs</code></td>
<td>
<p>Value to be passed to the <code>predict</code> function.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_printevery">printevery</code></td>
<td>
<p>Outputs MCMC algorithm status every printevery
iterations.</p>
</td></tr>
<tr><td><code id="tsvs2_+3A_transposed">transposed</code></td>
<td>
<p><code>tsvs</code> handles all of the pre-processing
for <code>x.train/x.test</code> (including
tranposing) computational efficiency.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>tsvs2()/tsvs()</code> is the function to perform variable selection.








The <code>tsvs2()/tsvs()</code> function returns a fit object of S3 class type
<code>list</code> as well as storing it in <code>rds.file</code> for
sampling in progress.
</p>


<h3>Author(s)</h3>

<p>Rodney Sparapani: <a href="mailto:rsparapa@mcw.edu">rsparapa@mcw.edu</a>
</p>


<h3>References</h3>

<p>Sparapani R., Logan B., Maiers M., Laud P., McCulloch R. (2023)
Nonparametric Failure Time: Time-to-event Machine Learning with
Heteroskedastic Bayesian Additive Regression Trees and 
Low Information Omnibus Dirichlet Process Mixtures
<em>Biometrics (ahead of print)</em> &lt;doi:10.1111/biom.13857&gt;.
</p>
<p>Liu Y., Rockova V. (2021)
Variable selection via Thompson sampling.
<em>Journal of the American Statistical Association. Jun 29:1-8.</em>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tsvs">tsvs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##library(nftbart)
data(lung)
N=length(lung$status)

##lung$status: 1=censored, 2=dead
##delta: 0=censored, 1=dead
delta=lung$status-1

## this study reports time in days rather than weeks or months
times=lung$time
times=times/7  ## weeks

## matrix of covariates
x.train=cbind(lung[ , -(1:3)])
## lung$sex:        Male=1 Female=2


##vars=tsvs2(x.train, x.train, times, delta)
vars=tsvs2(x.train, x.train, times, delta, K=0) ## K=0 just returns 0

</code></pre>

<hr>
<h2 id='xicuts'>Specifying cut-points for the covariates</h2><span id='topic+xicuts'></span>

<h3>Description</h3>

<p>This function allows you to create a list that specifies
the cut-points for the covariates.</p>


<h3>Usage</h3>

<pre><code class='language-R'>xicuts(x.train, transposed=FALSE, numcut=100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="xicuts_+3A_x.train">x.train</code></td>
<td>
<p>The training matrix to derive cut-points from.</p>
</td></tr>
<tr><td><code id="xicuts_+3A_transposed">transposed</code></td>
<td>
<p>Whether or not the matrix has been tranposed yet.</p>
</td></tr>
<tr><td><code id="xicuts_+3A_numcut">numcut</code></td>
<td>
<p>The number of cut-points to create.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The cut-points are generated uniformly from min. to max., i.e.,
the distribution of the data is ignored.
</p>


<h3>Value</h3>

<p>An object is returned of type <code>BARTcutinfo</code> which is essentially a list.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
