<!DOCTYPE html><html><head><title>Help for package accSDA</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {accSDA}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#accSDA'><p>accSDA: A package for performing sparse discriminant analysis</p>
in various ways.</a></li>
<li><a href='#ADMM_EN_SMW'><p>ADMM on l1 regularized quadratic program</p></a></li>
<li><a href='#ADMM_EN2'><p>ADMM on l1 regularized quadratic program</p></a></li>
<li><a href='#APG_EN2'><p>Accelerated Proximal Gradient on l1 regularized quadratic program</p></a></li>
<li><a href='#APG_EN2bt'><p>Accelerated Proximal Gradient (with backtracking) on l1 regularized quadratic program</p></a></li>
<li><a href='#APG_EN2rr'><p>Accelerated Proximal Gradient on l1 regularized quadratic program</p></a></li>
<li><a href='#ASDA'><p>Accelerated Sparse Discriminant Analysis</p></a></li>
<li><a href='#ASDABarPlot'><p>barplot for ASDA objects</p></a></li>
<li><a href='#genDat'><p>Generate data for ordinal examples in the package</p></a></li>
<li><a href='#normalize'><p>Normalize training data</p></a></li>
<li><a href='#normalizetest'><p>Normalize training data</p></a></li>
<li><a href='#nullSp'><p>Finding null space of linear operator</p></a></li>
<li><a href='#ordASDA'><p>Ordinal Accelerated Sparse Discriminant Analysis</p></a></li>
<li><a href='#predict.ASDA'><p>Predict method for sparse discriminant analysis</p></a></li>
<li><a href='#predict.ordASDA'><p>Predict method for ordinal sparse discriminant analysis</p></a></li>
<li><a href='#print.ASDA'><p>Print method for ASDA object</p></a></li>
<li><a href='#prox_EN'><p>Accelerated Proximal Gradient on l1 regularized quadratic program</p></a></li>
<li><a href='#prox_ENbt'><p>Accelerated Proximal Gradient on l1 regularized quadratic program with backtracking</p></a></li>
<li><a href='#SDAAP'><p>Sparse Discriminant Analysis solved via Accelerated Proximal Gradient</p></a></li>
<li><a href='#SDAD'><p>Sparse Discriminant Analysis solved via ADMM</p></a></li>
<li><a href='#SDAP'><p>Sparse Discriminant Analysis solved via Proximal Gradient</p></a></li>
<li><a href='#SZVD'><p>Sparse Zero Variance Discriminant Analysis</p></a></li>
<li><a href='#SZVD_ADMM'><p>Alternating Direction Method of Multipliers for SZVD</p></a></li>
<li><a href='#SZVD_kFold_cv'><p>Cross-validation of sparse zero variance discriminant analysis</p></a></li>
<li><a href='#SZVDcv'><p>Cross-validation of sparse zero variance discriminant analysis</p></a></li>
<li><a href='#test_ZVD'><p>Classify test data using nearest centroid classification and</p>
discriminant vectors learned from the training set.</a></li>
<li><a href='#vec_shrink'><p>Softmax for SZVD ADMM iterations</p></a></li>
<li><a href='#ZVD'><p>Zero Variance Discriminant Analysis</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.1.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-03-04</td>
</tr>
<tr>
<td>Title:</td>
<td>Accelerated Sparse Discriminant Analysis</td>
</tr>
<tr>
<td>Imports:</td>
<td>MASS (&ge; 7.3.45), ggplot2 (&ge; 2.1.0), grid (&ge; 3.2.2),
gridExtra (&ge; 2.2.1)</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2)</td>
</tr>
<tr>
<td>Description:</td>
<td>Implementation of sparse linear discriminant analysis, which is a supervised
    classification method for multiple classes. Various novel optimization approaches to
    this problem are implemented including alternating direction method of multipliers ('ADMM'),
    proximal gradient (PG) and accelerated proximal gradient ('APG') (See Atkins 'et al'. &lt;<a href="https://arxiv.org/abs/1705.07194">arXiv:1705.07194</a>&gt;).
    Functions for performing cross validation are also supplied along with basic prediction
    and plotting functions.
    Sparse zero variance discriminant analysis ('SZVD') is also included in the package
    (See Ames and Hong, &lt;<a href="https://arxiv.org/abs/1401.5492">arXiv:1401.5492</a>&gt;). See the 'github' wiki for a more extended description.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/gumeo/accSDA/wiki">https://github.com/gumeo/accSDA/wiki</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/gumeo/accSDA/issues">https://github.com/gumeo/accSDA/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-06 18:34:56 UTC; gudmundureinarsson</td>
</tr>
<tr>
<td>Author:</td>
<td>Gudmundur Einarsson [aut, cre, trl],
  Line Clemmensen [aut, ths],
  Brendan Ames [aut],
  Summer Atkins [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Gudmundur Einarsson &lt;gumeo140688@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-06 18:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='accSDA'>accSDA: A package for performing sparse discriminant analysis
in various ways.</h2><span id='topic+accSDA'></span>

<h3>Description</h3>

<p>The accSDA package provides functions to perform sparse discriminant
analysis using a selection of three optimization methods,
proximal gradient (PG), accelerated proximal gradient (APG) and
alternating direction method of multipliers (ADMM). The package
is intended to extend the available tools to perform sparse
discriminant analysis in R. The three methods can be called
from the function <code><a href="#topic+ASDA">ASDA</a></code>. Cross validation is also
implemented for the L1 regularization parameter. Functions
for doing predictions, summary, printing and simple plotting
are also provided. The sparse discriminant functions perform
lda on the projected data by default, using the lda function
in the MASS package. The functions return an object of the
same class as the name of the function and provide the lda
solution, along with the projected data, thus other kinds
of classification algorithms can be employed on the projected data.
</p>

<hr>
<h2 id='ADMM_EN_SMW'>ADMM on l1 regularized quadratic program</h2><span id='topic+ADMM_EN_SMW'></span>

<h3>Description</h3>

<p>Applies Alternating Direction Method of Multipliers to the l1-regularized quadratic program
</p>
<p style="text-align: center;"><code class="reqn">f(\mathbf{x}) + g(\mathbf{x}) = \frac{1}{2}\mathbf{x}^TA\mathbf{x} - d^T\mathbf{x} + \lambda |\mathbf{x}|_1</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>ADMM_EN_SMW(Ainv, V, R, d, x0, lam, mu, maxits, tol, quiet, selector)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ADMM_EN_SMW_+3A_ainv">Ainv</code></td>
<td>
<p>Diagonal of <code class="reqn">A^{-1}</code> term in SMW formula, where A is an n by n
positive definite coefficient matrix.</p>
</td></tr>
<tr><td><code id="ADMM_EN_SMW_+3A_v">V</code></td>
<td>
<p>Matrix from SMW formula.</p>
</td></tr>
<tr><td><code id="ADMM_EN_SMW_+3A_r">R</code></td>
<td>
<p>Upper triangular matrix in Cholesky decomposition of <code class="reqn">I + UA^{-1}V</code>.</p>
</td></tr>
<tr><td><code id="ADMM_EN_SMW_+3A_d">d</code></td>
<td>
<p>nx1 dimensional column vector.</p>
</td></tr>
<tr><td><code id="ADMM_EN_SMW_+3A_lam">lam</code></td>
<td>
<p>Regularization parameter for l1 penalty, must be greater than zero.</p>
</td></tr>
<tr><td><code id="ADMM_EN_SMW_+3A_mu">mu</code></td>
<td>
<p>Augmented Lagrangian penalty parameter, must be greater than zero.</p>
</td></tr>
<tr><td><code id="ADMM_EN_SMW_+3A_maxits">maxits</code></td>
<td>
<p>Number of iterations to run</p>
</td></tr>
<tr><td><code id="ADMM_EN_SMW_+3A_tol">tol</code></td>
<td>
<p>Vector of stopping tolerances, first value is absolute, second is relative tolerance.</p>
</td></tr>
<tr><td><code id="ADMM_EN_SMW_+3A_quiet">quiet</code></td>
<td>
<p>Logical controlling display of intermediate statistics.</p>
</td></tr>
<tr><td><code id="ADMM_EN_SMW_+3A_selector">selector</code></td>
<td>
<p>Vector to choose which parameters in the discriminant vector will be used to calculate the
regularization terms. The size of the vector must be *p* the number of predictors. The
default value is a vector of all ones. This is currently only used for ordinal classification.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used by other functions and should only be called explicitly for
debugging purposes.
</p>


<h3>Value</h3>

<p><code>ADMM_EN_SMW</code> returns an object of <code><a href="base.html#topic+class">class</a></code> &quot;<code>ADMM_EN_SMW</code>&quot; including a list
with the following named components
</p>

<dl>
<dt><code>call</code></dt><dd><p>The matched call.</p>
</dd>
<dt><code>x</code></dt><dd><p>Found solution.</p>
</dd>
<dt><code>y</code></dt><dd><p>Dual solution.</p>
</dd>
<dt><code>z</code></dt><dd><p>Slack variables.</p>
</dd>
<dt><code>k</code></dt><dd><p>Number of iterations used.</p>
</dd>
</dl>



<h3>See Also</h3>

<p>Used by: <code><a href="#topic+SDAD">SDAD</a></code> and the <code>SDADcv</code> cross-validation version.
</p>

<hr>
<h2 id='ADMM_EN2'>ADMM on l1 regularized quadratic program</h2><span id='topic+ADMM_EN2'></span>

<h3>Description</h3>

<p>Applies Alternating Direction Method of Multipliers to the l1-regularized quadratic program
</p>
<p style="text-align: center;"><code class="reqn">f(\mathbf{x}) + g(\mathbf{x}) = \frac{1}{2}\mathbf{x}^TA\mathbf{x} - d^T\mathbf{x} + \lambda |\mathbf{x}|_1</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>ADMM_EN2(R, d, x0, lam, mu, maxits, tol, quiet, selector = rep(1, dim(x)[1]))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ADMM_EN2_+3A_r">R</code></td>
<td>
<p>Upper triangular matrix in Chol decomp <code class="reqn">\mu I + A = R^T R</code>.</p>
</td></tr>
<tr><td><code id="ADMM_EN2_+3A_d">d</code></td>
<td>
<p>nx1 dimensional column vector.</p>
</td></tr>
<tr><td><code id="ADMM_EN2_+3A_lam">lam</code></td>
<td>
<p>Regularization parameter for l1 penalty, must be greater than zero.</p>
</td></tr>
<tr><td><code id="ADMM_EN2_+3A_mu">mu</code></td>
<td>
<p>Augmented Lagrangian penalty parameter, must be greater than zero.</p>
</td></tr>
<tr><td><code id="ADMM_EN2_+3A_maxits">maxits</code></td>
<td>
<p>Number of iterations to run</p>
</td></tr>
<tr><td><code id="ADMM_EN2_+3A_tol">tol</code></td>
<td>
<p>Vector of stopping tolerances, first value is absolute, second is relative tolerance.</p>
</td></tr>
<tr><td><code id="ADMM_EN2_+3A_quiet">quiet</code></td>
<td>
<p>Logical controlling display of intermediate statistics.</p>
</td></tr>
<tr><td><code id="ADMM_EN2_+3A_selector">selector</code></td>
<td>
<p>Vector to choose which parameters in the discriminant vector will be used to calculate the
regularization terms. The size of the vector must be *p* the number of predictors. The
default value is a vector of all ones. This is currently only used for ordinal classification.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used by other functions and should only be called explicitly for
debugging purposes.
</p>


<h3>Value</h3>

<p><code>ADMM_EN2</code> returns an object of <code><a href="base.html#topic+class">class</a></code> &quot;<code>ADMM_EN2</code>&quot; including a list
with the following named components
</p>

<dl>
<dt><code>call</code></dt><dd><p>The matched call.</p>
</dd>
<dt><code>x</code></dt><dd><p>Found solution.</p>
</dd>
<dt><code>y</code></dt><dd><p>Dual solution.</p>
</dd>
<dt><code>z</code></dt><dd><p>Slack variables.</p>
</dd>
<dt><code>k</code></dt><dd><p>Number of iterations used.</p>
</dd>
</dl>



<h3>See Also</h3>

<p>Used by: <code><a href="#topic+SDAD">SDAD</a></code> and the <code>SDADcv</code> cross-validation version.
</p>

<hr>
<h2 id='APG_EN2'>Accelerated Proximal Gradient on l1 regularized quadratic program</h2><span id='topic+APG_EN2'></span>

<h3>Description</h3>

<p>Applies accelerated proximal gradient algorithm to the l1-regularized quadratic program
</p>
<p style="text-align: center;"><code class="reqn">f(\mathbf{x}) + g(\mathbf{x}) = \frac{1}{2}\mathbf{x}^TA\mathbf{x} - d^T\mathbf{x} + \lambda |\mathbf{x}|_1</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>APG_EN2(A, d, x0, lam, alpha, maxits, tol, selector = rep(1, dim(x0)[1]))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="APG_EN2_+3A_a">A</code></td>
<td>
<p>p by p positive definite coefficient matrix
</p>
<p style="text-align: center;"><code class="reqn">A = (\gamma Om + X^T X/n)</code>
</p>
<p>.</p>
</td></tr>
<tr><td><code id="APG_EN2_+3A_d">d</code></td>
<td>
<p>nx1 dimensional column vector.</p>
</td></tr>
<tr><td><code id="APG_EN2_+3A_lam">lam</code></td>
<td>
<p>Regularization parameter for l1 penalty, must be greater than zero.</p>
</td></tr>
<tr><td><code id="APG_EN2_+3A_alpha">alpha</code></td>
<td>
<p>Step length.</p>
</td></tr>
<tr><td><code id="APG_EN2_+3A_maxits">maxits</code></td>
<td>
<p>Number of iterations to run</p>
</td></tr>
<tr><td><code id="APG_EN2_+3A_tol">tol</code></td>
<td>
<p>Stopping tolerance for proximal gradient algorithm.</p>
</td></tr>
<tr><td><code id="APG_EN2_+3A_selector">selector</code></td>
<td>
<p>Vector to choose which parameters in the discriminant vector will be used to calculate the
regularization terms. The size of the vector must be *p* the number of predictors. The
default value is a vector of all ones. This is currently only used for ordinal classification.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used by other functions and should only be called explicitly for
debugging purposes.
</p>


<h3>Value</h3>

<p><code>APG_EN2</code> returns an object of <code><a href="base.html#topic+class">class</a></code> &quot;<code>APG_EN2</code>&quot; including a list
with the following named components
</p>

<dl>
<dt><code>call</code></dt><dd><p>The matched call.</p>
</dd>
<dt><code>x</code></dt><dd><p>Found solution.</p>
</dd>
<dt><code>k</code></dt><dd><p>Number of iterations used.</p>
</dd>
</dl>



<h3>See Also</h3>

<p>Used by: <code><a href="#topic+SDAAP">SDAAP</a></code> and the <code>SDAAPcv</code> cross-validation version.
</p>

<hr>
<h2 id='APG_EN2bt'>Accelerated Proximal Gradient (with backtracking) on l1 regularized quadratic program</h2><span id='topic+APG_EN2bt'></span>

<h3>Description</h3>

<p>Applies accelerated proximal gradient algorithm (with backtracking) to the l1-regularized quadratic program
</p>
<p style="text-align: center;"><code class="reqn">f(\mathbf{x}) + g(\mathbf{x}) = \frac{1}{2}\mathbf{x}^TA\mathbf{x} - d^T\mathbf{x} + \lambda |\mathbf{x}|_1</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>APG_EN2bt(
  A,
  Xt,
  Om,
  gamma,
  d,
  x0,
  lam,
  L,
  eta,
  maxits,
  tol,
  selector = rep(1, dim(x0)[1])
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="APG_EN2bt_+3A_a">A</code></td>
<td>
<p>p by p positive definite coefficient matrix
</p>
<p style="text-align: center;"><code class="reqn">A = (\gamma Om + X^T X/n)</code>
</p>
<p>.</p>
</td></tr>
<tr><td><code id="APG_EN2bt_+3A_xt">Xt</code></td>
<td>
<p>Same as X above, we need it to make calculations faster.</p>
</td></tr>
<tr><td><code id="APG_EN2bt_+3A_om">Om</code></td>
<td>
<p>Same reason as for the above parameter.</p>
</td></tr>
<tr><td><code id="APG_EN2bt_+3A_gamma">gamma</code></td>
<td>
<p>l2 regularizing parameter.</p>
</td></tr>
<tr><td><code id="APG_EN2bt_+3A_d">d</code></td>
<td>
<p>nx1 dimensional column vector.</p>
</td></tr>
<tr><td><code id="APG_EN2bt_+3A_lam">lam</code></td>
<td>
<p>Regularization parameter for l1 penalty, must be greater than zero.</p>
</td></tr>
<tr><td><code id="APG_EN2bt_+3A_l">L</code></td>
<td>
<p>Initial vlaue of the backtracking Lipshitz constant.</p>
</td></tr>
<tr><td><code id="APG_EN2bt_+3A_eta">eta</code></td>
<td>
<p>Backtracking scaling parameter.</p>
</td></tr>
<tr><td><code id="APG_EN2bt_+3A_maxits">maxits</code></td>
<td>
<p>Number of iterations to run</p>
</td></tr>
<tr><td><code id="APG_EN2bt_+3A_tol">tol</code></td>
<td>
<p>Stopping tolerance for proximal gradient algorithm.</p>
</td></tr>
<tr><td><code id="APG_EN2bt_+3A_selector">selector</code></td>
<td>
<p>Vector to choose which parameters in the discriminant vector will be used to calculate the
regularization terms. The size of the vector must be *p* the number of predictors. The
default value is a vector of all ones. This is currently only used for ordinal classification.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used by other functions and should only be called explicitly for
debugging purposes.
</p>


<h3>Value</h3>

<p><code>APG_EN2bt</code> returns an object of <code><a href="base.html#topic+class">class</a></code> &quot;<code>APG_EN2bt</code>&quot; including a list
with the following named components
</p>

<dl>
<dt><code>call</code></dt><dd><p>The matched call.</p>
</dd>
<dt><code>x</code></dt><dd><p>Found solution.</p>
</dd>
<dt><code>k</code></dt><dd><p>Number of iterations used.</p>
</dd>
</dl>



<h3>See Also</h3>

<p>Used by: <code><a href="#topic+SDAAP">SDAAP</a></code> and the <code>SDAAPcv</code> cross-validation version.
</p>

<hr>
<h2 id='APG_EN2rr'>Accelerated Proximal Gradient on l1 regularized quadratic program</h2><span id='topic+APG_EN2rr'></span>

<h3>Description</h3>

<p>Applies accelerated proximal gradient algorithm to the l1-regularized quadratic program
(with rank reduced Omega inside A)
</p>
<p style="text-align: center;"><code class="reqn">f(\mathbf{x}) + g(\mathbf{x}) = \frac{1}{2}\mathbf{x}^TA\mathbf{x} - d^T\mathbf{x} + \lambda |\mathbf{x}|_1</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>APG_EN2rr(A, d, x0, lam, alpha, maxits, tol, selector = rep(1, dim(x0)[1]))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="APG_EN2rr_+3A_a">A</code></td>
<td>
<p>Object containing everythign needed for calculating A,
X and Omega are factored.</p>
</td></tr>
<tr><td><code id="APG_EN2rr_+3A_d">d</code></td>
<td>
<p>nx1 dimensional column vector.</p>
</td></tr>
<tr><td><code id="APG_EN2rr_+3A_lam">lam</code></td>
<td>
<p>Regularization parameter for l1 penalty, must be greater than zero.</p>
</td></tr>
<tr><td><code id="APG_EN2rr_+3A_alpha">alpha</code></td>
<td>
<p>Step length.</p>
</td></tr>
<tr><td><code id="APG_EN2rr_+3A_maxits">maxits</code></td>
<td>
<p>Number of iterations to run</p>
</td></tr>
<tr><td><code id="APG_EN2rr_+3A_tol">tol</code></td>
<td>
<p>Stopping tolerance for proximal gradient algorithm.</p>
</td></tr>
<tr><td><code id="APG_EN2rr_+3A_selector">selector</code></td>
<td>
<p>Vector to choose which parameters in the discriminant vector will be used to calculate the
regularization terms. The size of the vector must be *p* the number of predictors. The
default value is a vector of all ones. This is currently only used for ordinal classification.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used by other functions and should only be called explicitly for
debugging purposes.
</p>


<h3>Value</h3>

<p><code>APG_EN2</code> returns an object of <code><a href="base.html#topic+class">class</a></code> &quot;<code>APG_EN2</code>&quot; including a list
with the following named components
</p>

<dl>
<dt><code>call</code></dt><dd><p>The matched call.</p>
</dd>
<dt><code>x</code></dt><dd><p>Found solution.</p>
</dd>
<dt><code>k</code></dt><dd><p>Number of iterations used.</p>
</dd>
</dl>



<h3>See Also</h3>

<p>Used by: <code><a href="#topic+SDAAP">SDAAP</a></code> and the <code>SDAAPcv</code> cross-validation version.
</p>

<hr>
<h2 id='ASDA'>Accelerated Sparse Discriminant Analysis</h2><span id='topic+ASDA'></span><span id='topic+ASDA.default'></span>

<h3>Description</h3>

<p>Applies accelerated proximal gradient algorithm, proximal gradient algorithm
or alternating direction methods of multipliers algorithm to
the optimal scoring formulation of sparse discriminant analysis proposed
by Clemmensen et al. 2011.
</p>
<p style="text-align: center;"><code class="reqn">argmin{|(Y_t\theta-X_t\beta)|_2^2 + t|\beta|_1 + \lambda|\beta|_2^2}</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>ASDA(Xt, ...)

## Default S3 method:
ASDA(
  Xt,
  Yt,
  Om = diag(p),
  gam = 0.001,
  lam = 1e-06,
  q = K - 1,
  method = "SDAAP",
  control = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ASDA_+3A_xt">Xt</code></td>
<td>
<p>n by p data matrix, (can also be a data.frame that can be coerced to a matrix)</p>
</td></tr>
<tr><td><code id="ASDA_+3A_...">...</code></td>
<td>
<p>Additional arguments for <code><a href="MASS.html#topic+lda">lda</a></code> function in package MASS.</p>
</td></tr>
<tr><td><code id="ASDA_+3A_yt">Yt</code></td>
<td>
<p>n by K matrix of indicator variables (Yij = 1 if i in class j).
This will later be changed to handle factor variables as well.
Each observation belongs in a single class, so for a given row/observation,
only one element is 1 and the rest is 0.</p>
</td></tr>
<tr><td><code id="ASDA_+3A_om">Om</code></td>
<td>
<p>p by p parameter matrix Omega in generalized elastic net penalty.</p>
</td></tr>
<tr><td><code id="ASDA_+3A_gam">gam</code></td>
<td>
<p>Regularization parameter for elastic net penalty.</p>
</td></tr>
<tr><td><code id="ASDA_+3A_lam">lam</code></td>
<td>
<p>Regularization parameter for l1 penalty, must be greater than zero.
If cross-validation is used (<code>CV = TRUE</code>) then this must be a vector
of length greater than one.</p>
</td></tr>
<tr><td><code id="ASDA_+3A_q">q</code></td>
<td>
<p>Desired number of discriminant vectors.</p>
</td></tr>
<tr><td><code id="ASDA_+3A_method">method</code></td>
<td>
<p>This parameter selects which optimization method to use.
It is specified as a character vector which can be one of the three values
</p>

<dl>
<dt><code>SDAP</code></dt><dd><p>Proximal gradient algorithm.</p>
</dd>
<dt><code>SDAAP</code></dt><dd><p>Accelerated proximal gradient algorithm.</p>
</dd>
<dt><code>SDAD</code></dt><dd><p>Alternating directions method of multipliers algorithm.</p>
</dd>
</dl>

<p>Note that further parameters are passed to the function in the argument <code>control</code>,
which is a list with named components.</p>
</td></tr>
<tr><td><code id="ASDA_+3A_control">control</code></td>
<td>
<p>List of control arguments. See Details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The control list contains the following entries to further tune the
algorithms.
</p>

<dl>
<dt><code>PGsteps</code></dt><dd><p>Maximum number if inner proximal gradient/ADMM
algorithm for finding beta. Default value is 1000.</p>
</dd>
<dt><code>PGtol</code></dt><dd><p>Stopping tolerance for inner method. If the method is <code>SDAD</code>,
then this must be a vector of two values, absolute (first element) and relative
tolerance (second element). Default value is 1e-5 for both absolute and
relative tolerances.</p>
</dd>
<dt><code>maxits</code></dt><dd><p>Number of iterations to run. Default value is 250.</p>
</dd>
<dt><code>tol</code></dt><dd><p>Stopping tolerance. Default value is 1e-3.</p>
</dd>
<dt><code>mu</code></dt><dd><p>Penalty parameter for augmented Lagrangian term,
must be greater than zero and only needs to be specified when using
method <code>SDAD</code>. Default value is 1.</p>
</dd>
<dt><code>CV</code></dt><dd><p>Logical value which is <code>TRUE</code> if cross validation is supposed to be
performed. If cross-validation is performed, then lam should be specified as
a vector containing the regularization values to be tested. Default value is <code>FALSE</code>.</p>
</dd>
<dt><code>folds</code></dt><dd><p>Integer determining the number of folds in cross-validation. Not needed
if CV is not specified. Default value is 5.</p>
</dd>
<dt><code>feat</code></dt><dd><p>Maximum fraction of nonzero features desired in validation scheme. Not needed
if CV is not specified. Default value is 0.15.</p>
</dd>
<dt><code>quiet</code></dt><dd><p>Set to <code>FALSE</code> if status updates are supposed to be printed to the R console.
Default value is <code>TRUE</code>. Note that this triggers a lot of printing to the console.</p>
</dd>
<dt><code>ordinal</code></dt><dd><p>Set to <code>TRUE</code> if the labels are ordinal. Only available for methods
<code>SDAAP</code> and <code>SDAD</code>.</p>
</dd>
<dt><code>initTheta</code></dt><dd><p>Option to set the initial theta vector, by default it is a vector of all ones
for the first theta.</p>
</dd>
<dt><code>bt</code></dt><dd><p>Logical indicating whether backtracking should be used, only applies to
the Proximal Gradient based methods. By default, backtracking is not used.</p>
</dd>
<dt><code>L</code></dt><dd><p>Initial estimate for Lipshitz constant used for backtracking. Default value is 0.25.</p>
</dd>
<dt><code>eta</code></dt><dd><p>Scalar for Lipshitz constant. Default value is 1.25.</p>
</dd>
<dt><code>rankRed</code></dt><dd><p>Boolean indicating whether Om is factorized, such that R^t*R=Om,
currently only applicable for accelerated proximal gradient.</p>
</dd>
</dl>



<h3>Value</h3>

<p><code>ASDA</code> returns an object of <code><a href="base.html#topic+class">class</a></code> &quot;<code>ASDA</code>&quot; including a list
with the following named components:
</p>

<dl>
<dt><code>call</code></dt><dd><p>The matched call.</p>
</dd>
<dt><code>B</code></dt><dd><p>p by q matrix of discriminant vectors, i.e. sparse loadings.</p>
</dd>
<dt><code>Q</code></dt><dd><p>K by q matrix of scoring vectors, i.e. optimal scores.</p>
</dd>
<dt><code>varNames</code></dt><dd><p>Names of the predictors used, i.e. column names of Xt.</p>
</dd>
<dt><code>origP</code></dt><dd><p>Number of variables in Xt.</p>
</dd>
<dt><code>fit</code></dt><dd><p>Output from function <code><a href="MASS.html#topic+lda">lda</a></code> on projected data.
This is <code>NULL</code> the trivial solution is found, i.e. B is all zeroes. Use
lower values of <code>lam</code> if that is the case.</p>
</dd>
<dt><code>classes</code></dt><dd><p>The classes in Yt.</p>
</dd>
<dt><code>lambda</code></dt><dd><p>The lambda/<code>lam</code> used, best value found by cross-
validation if <code>CV</code> is <code>TRUE</code>.</p>
</dd>
</dl>

<p><code>NULL</code>
</p>


<h3>Note</h3>

<p>The input matrix Xt should be normalized, i.e. each column corresponding to
a variable should have its mean subtracted and scaled to unit length. The functions
<code><a href="#topic+normalize">normalize</a></code> and <code><a href="#topic+normalizetest">normalizetest</a></code> are supplied for this purpose in the package.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SDAAP">SDAAP</a></code>, <code><a href="#topic+SDAP">SDAP</a></code> and <code><a href="#topic+SDAD">SDAD</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    set.seed(123)
    # Prepare training and test set
    train &lt;- c(1:40,51:90,101:140)
    Xtrain &lt;- iris[train,1:4]
    nX &lt;- normalize(Xtrain)
    Xtrain &lt;- nX$Xc
    Ytrain &lt;- iris[train,5]
    Xtest &lt;- iris[-train,1:4]
    Xtest &lt;- normalizetest(Xtest,nX)
    Ytest &lt;- iris[-train,5]

    # Define parameters for Alternating Direction Method of Multipliers (SDAD)
    Om &lt;- diag(4)+0.1*matrix(1,4,4) #elNet coef mat
    gam &lt;- 0.0001
    lam &lt;- 0.0001
    method &lt;- "SDAD"
    q &lt;- 2
    control &lt;- list(PGsteps = 100,
                    PGtol = c(1e-5,1e-5),
                    mu = 1,
                    maxits = 100,
                    tol = 1e-3,
                    quiet = FALSE)

    # Run the algorithm
    res &lt;- ASDA(Xt = Xtrain,
                Yt = Ytrain,
                Om = Om,
                gam = gam ,
                lam = lam,
                q = q,
                method = method,
                control = control)

    # Can also just use the defaults, which is Accelerated Proximal Gradient (SDAAP):
    resDef &lt;- ASDA(Xtrain,Ytrain)

    # Some example on simulated data
    # Generate Gaussian data on three classes with plenty of redundant variables

    # This example shows the basic steps on how to apply this to data, i.e.:
    #  1) Setup training data
    #  2) Normalize
    #  3) Train
    #  4) Predict
    #  5) Plot projected data
    #  6) Accuracy on test set

    P &lt;- 300 # Number of variables
    N &lt;- 50 # Number of samples per class

    # Mean for classes, they are zero everywhere except the first 3 coordinates
    m1 &lt;- rep(0,P)
    m1[1] &lt;- 3

    m2 &lt;- rep(0,P)
    m2[2] &lt;- 3

    m3 &lt;- rep(0,P)
    m3[3] &lt;- 3

    # Sample dummy data
    Xtrain &lt;- rbind(MASS::mvrnorm(n=N,mu = m1, Sigma = diag(P)),
                   MASS::mvrnorm(n=N,mu = m2, Sigma = diag(P)),
                   MASS::mvrnorm(n=N,mu = m3, Sigma = diag(P)))

    Xtest &lt;- rbind(MASS::mvrnorm(n=N,mu = m1, Sigma = diag(P)),
                   MASS::mvrnorm(n=N,mu = m2, Sigma = diag(P)),
                   MASS::mvrnorm(n=N,mu = m3, Sigma = diag(P)))

    # Generate the labels
    Ytrain &lt;- factor(rep(1:3,each=N))
    Ytest &lt;- Ytrain

    # Normalize the data
    Xt &lt;- accSDA::normalize(Xtrain)
    Xtrain &lt;- Xt$Xc # Use the centered and scaled data
    Xtest &lt;- accSDA::normalizetest(Xtest,Xt)

    # Train the classifier and increase the sparsity parameter from the default
    # so we penalize more for non-sparse solutions.
    res &lt;- accSDA::ASDA(Xtrain,Ytrain,lam=0.01)

    # Plot the projected training data, it is projected to
    # 2-dimension because we have 3 classes. The number of discriminant
    # vectors is maximum number of classes minus 1.
    XtrainProjected &lt;- Xtrain%*%res$beta

    plot(XtrainProjected[,1],XtrainProjected[,2],col=Ytrain)

    # Predict on the test data
    preds &lt;- predict(res, newdata = Xtest)

    # Plot projected test data with predicted and correct labels
    XtestProjected &lt;- Xtest%*%res$beta

    plot(XtestProjected[,1],XtestProjected[,2],col=Ytest,
         main="Projected test data with original labels")
    plot(XtestProjected[,1],XtestProjected[,2],col=preds$class,
         main="Projected test data with predicted labels")

    # Calculate accuracy
    sum(preds$class == Ytest)/(3*N) # We have N samples per class, so total 3*N

</code></pre>

<hr>
<h2 id='ASDABarPlot'>barplot for ASDA objects</h2><span id='topic+ASDABarPlot'></span>

<h3>Description</h3>

<p>This is a function to visualize the discriminant vector from the ASDA
method. The plot is constructed as a ggplot barplot and the main purpose of it is
to visually inspect the sparsity of the discriminant vectors. The main things to
look for are how many parameters are non-zero and if there is any structure in
the ones that are non-zero, but the structure is dependent on the order you specify
your variables. For time-series data, this could mean that a chunk of variables are
non-zero that are close in time, meaning that there is some particular event that is
best for discriminating between the classes that you have.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ASDABarPlot(asdaObj, numDVs = 1, xlabel, ylabel, getList = FALSE, main, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ASDABarPlot_+3A_asdaobj">asdaObj</code></td>
<td>
<p>Object from the <code>ASDA</code> function.</p>
</td></tr>
<tr><td><code id="ASDABarPlot_+3A_numdvs">numDVs</code></td>
<td>
<p>Number of discriminant vectors (DVs) to plot. This is limited by the
number of DVs outputted from the <code>ASDA</code> function or k-1 DVs where k
is the number of classes. The first 1 to numDVs are plotted.</p>
</td></tr>
<tr><td><code id="ASDABarPlot_+3A_xlabel">xlabel</code></td>
<td>
<p>Label to put under every plot</p>
</td></tr>
<tr><td><code id="ASDABarPlot_+3A_ylabel">ylabel</code></td>
<td>
<p>Vector of y-axis labels for each plot, e.g. if there are three DVs, then
<code>ylab = c('Discriminant Vector 1', 'Discriminant Vector 2', 'Discriminant Vector 3')</code>
is a valid option.</p>
</td></tr>
<tr><td><code id="ASDABarPlot_+3A_getlist">getList</code></td>
<td>
<p>Logical value indicating whether the output should be a list of the plots
or the plots stacked in one plot using the gridExtra package. By default the function
produces a single plot combining all plots of the DVs.</p>
</td></tr>
<tr><td><code id="ASDABarPlot_+3A_main">main</code></td>
<td>
<p>Main title for the plots, this is not used if getList is set to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="ASDABarPlot_+3A_...">...</code></td>
<td>
<p>Extra arguments to <code>grid.arrange</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>barplot.ASDA</code> returns either a single combined plot or a list of
individual ggplot objects.
</p>


<h3>Note</h3>

<p>This function is used as a quick diagnostics tool for the output from the ASDA function.
Feel free to look at the code to customize the plots in any way you like.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ASDA">ASDA</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    # Generate and ASDA object with your data, e.g.
    # Prepare training and test set
    # This is a very small data set, I advise you to try it on something with more
    # variables, e.g. something from this source: http://www.cs.ucr.edu/~eamonn/time_series_data/
    # or possibly run this on the Gaussian data example from the ASDA function
    train &lt;- c(1:40,51:90,101:140)
    Xtrain &lt;- iris[train,1:4]
    nX &lt;- normalize(Xtrain)
    Xtrain &lt;- nX$Xc
    Ytrain &lt;- iris[train,5]
    Xtest &lt;- iris[-train,1:4]
    Xtest &lt;- normalizetest(Xtest,nX)
    Ytest &lt;- iris[-train,5]
    # Run the method
    resIris &lt;- ASDA(Xtrain,Ytrain)

    # Look at the barplots of the DVs
    ASDABarPlot(resIris)
</code></pre>

<hr>
<h2 id='genDat'>Generate data for ordinal examples in the package</h2><span id='topic+genDat'></span>

<h3>Description</h3>

<p>Given the parameters, the function creates a dataset for testing the ordinal functionality
of the package. The data is samples from multivariate Gaussians with different means, where
the mean varies along a sinusoidal curve w.r.t. the class label.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>genDat(numClasses, numObsPerClass, mu, sigma)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="genDat_+3A_numclasses">numClasses</code></td>
<td>
<p>Positive integer specifying the number of classes for the
dataset.</p>
</td></tr>
<tr><td><code id="genDat_+3A_numobsperclass">numObsPerClass</code></td>
<td>
<p>Number of observations sampled per class.</p>
</td></tr>
<tr><td><code id="genDat_+3A_mu">mu</code></td>
<td>
<p>Mean of the first class.</p>
</td></tr>
<tr><td><code id="genDat_+3A_sigma">sigma</code></td>
<td>
<p>2 by 2 covariance matrix</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used to demonstrate the usage of the ordinal classifier.
</p>


<h3>Value</h3>

<p><code>genDat</code> Returns a list with the following attributes:
</p>

<dl>
<dt>X</dt><dd><p>A matrix with two columns and <code>numObsPerClass</code>*<code>numClasses</code> rows.</p>
</dd>
<dt>Y</dt><dd><p>Labels for the rows of <code>X</code>.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Gudmundur Einarsson
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ordASDA">ordASDA</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)

    # You can play around with these values to generate some 2D data to test one
    numClasses &lt;- 15
    sigma &lt;- matrix(c(1,-0.2,-0.2,1),2,2)
    mu &lt;- c(0,0)
    numObsPerClass &lt;- 5

    # Generate the data, can access with train$X and train$Y
    train &lt;- accSDA::genDat(numClasses,numObsPerClass,mu,sigma)
    test &lt;- accSDA::genDat(numClasses,numObsPerClass*2,mu,sigma)

    # Visualize it, only using the first variable gives very good separation
    plot(train$X[,1],train$X[,2],col = factor(train$Y),asp=1,main="Training Data")

</code></pre>

<hr>
<h2 id='normalize'>Normalize training data</h2><span id='topic+normalize'></span>

<h3>Description</h3>

<p>Normalize a vector or matrix to zero mean and unit length columns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalize(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="normalize_+3A_x">X</code></td>
<td>
<p>a matrix with the training data with observations down the rows and variables in the columns.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can e.g. be used for the training data in the <code>ASDA</code> function.
</p>


<h3>Value</h3>

<p><code>normalize</code> Returns a list with the following attributes:
</p>

<dl>
<dt>Xc</dt><dd><p>The normalized data</p>
</dd>
<dt>mx</dt><dd><p>Mean of columns of <code>X</code>.</p>
</dd>
<dt>vx</dt><dd><p>Length of columns of <code>X</code>.</p>
</dd>
<dt>Id</dt><dd><p>Logical vector indicating which variables are
included in X. If some of the columns have zero length they are omitted</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Line Clemmensen
</p>


<h3>References</h3>

<p>Clemmensen, L., Hastie, T. and Ersboell, K. (2008)
&quot;Sparse discriminant analysis&quot;, Technical report, IMM, Technical University of Denmark
</p>


<h3>See Also</h3>

<p><code><a href="#topic+normalizetest">normalizetest</a></code>, <code><a href="#topic+predict.ASDA">predict.ASDA</a></code>, <code><a href="#topic+ASDA">ASDA</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Data
X&lt;-matrix(sample(seq(3),12,replace=TRUE),nrow=3)

## Normalize data
Nm&lt;-normalize(X)
print(Nm$Xc)

## See if any variables have been removed
which(!Nm$Id)
</code></pre>

<hr>
<h2 id='normalizetest'>Normalize training data</h2><span id='topic+normalizetest'></span>

<h3>Description</h3>

<p>Normalize test data using output from the <code>normalize()</code> of the training data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalizetest(Xtst, Xn)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="normalizetest_+3A_xtst">Xtst</code></td>
<td>
<p>a matrix with the test data with observations down the rows and variables in the columns.</p>
</td></tr>
<tr><td><code id="normalizetest_+3A_xn">Xn</code></td>
<td>
<p>List with the output from normalize(Xtr) of the training data.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can e.g. be used for the test data in the <code><a href="#topic+predict.ASDA">predict.ASDA</a></code> function.
</p>


<h3>Value</h3>

<p><code>normalizetest</code> returns the normalized test data <code>Xtst</code>
</p>


<h3>Author(s)</h3>

<p>Line Clemmensen
</p>


<h3>References</h3>

<p>Clemmensen, L., Hastie, T. and Ersboell, K. (2008)
&quot;Sparse discriminant analysis&quot;, Technical report, IMM, Technical University of Denmark
</p>


<h3>See Also</h3>

<p><code><a href="#topic+normalize">normalize</a></code>, <code><a href="#topic+predict.ASDA">predict.ASDA</a></code>, <code><a href="#topic+ASDA">ASDA</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Data
Xtr&lt;-matrix(sample(seq(3),12,replace=TRUE),nrow=3)
Xtst&lt;-matrix(sample(seq(3),12,replace=TRUE),nrow=3)

## Normalize training data
Nm&lt;-normalize(Xtr)

## Normalize test data
Xtst&lt;-normalizetest(Xtst,Nm)

</code></pre>

<hr>
<h2 id='nullSp'>Finding null space of linear operator</h2><span id='topic+nullSp'></span>

<h3>Description</h3>

<p>Finds the null space of a linear operator A in <code class="reqn">R^{n \times m}</code>.
The null space is given as a matrix, where the columns form an orthonormal basis
for the nullspace. This function emulates the null function in matlab, it
works exactly the same, but the basis vectors may be different, i.e. rotated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nullSp(A)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nullSp_+3A_a">A</code></td>
<td>
<p>m by n matrix</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used by other functions and should only be called explicitly for
debugging purposes.
</p>


<h3>Value</h3>

<p><code>nullSp</code> returns a matrix whose columns span the nullspace of A.
</p>


<h3>See Also</h3>

<p>Alternative <code><a href="MASS.html#topic+Null">Null</a></code> function in MASS package.
</p>

<hr>
<h2 id='ordASDA'>Ordinal Accelerated Sparse Discriminant Analysis</h2><span id='topic+ordASDA'></span><span id='topic+ordASDA.default'></span>

<h3>Description</h3>

<p>Applies accelerated proximal gradient algorithm to
the optimal scoring formulation of sparse discriminant analysis proposed
by Clemmensen et al. 2011. The problem is further casted to a binary
classification problem as described in &quot;Learning to Classify Ordinal Data:
The Data Replication Method&quot; by Cardoso and da Costa to handle the ordinal labels.
This function serves as a wrapper for the <code><a href="#topic+ASDA">ASDA</a></code> function, where the
appropriate data augmentation is performed. Since the problem is casted into
a binary classication problem, only a single discriminant vector comes from the
result. The first *p* entries correspond to the variables/coefficients for
the predictors, while the following K-1 entries correspond to biases for the
found hyperplane, to separate the classes. The resulting object is of class ordASDA
and has an accompanying predict function. The paper by Cardoso and dat Costa can
be found here: (http://www.jmlr.org/papers/volume8/cardoso07a/cardoso07a.pdf).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ordASDA(Xt, ...)

## Default S3 method:
ordASDA(
  Xt,
  Yt,
  s = 1,
  Om,
  gam = 0.001,
  lam = 1e-06,
  method = "SDAAP",
  control,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ordASDA_+3A_xt">Xt</code></td>
<td>
<p>n by p data matrix, (can also be a data.frame that can be coerced to a matrix)</p>
</td></tr>
<tr><td><code id="ordASDA_+3A_...">...</code></td>
<td>
<p>Additional arguments for <code><a href="#topic+ASDA">ASDA</a></code> and <code><a href="MASS.html#topic+lda">lda</a></code>
function in package MASS.</p>
</td></tr>
<tr><td><code id="ordASDA_+3A_yt">Yt</code></td>
<td>
<p>vector of length n, equal to the number of samples. The classes should be
1,2,...,K where K is the number of classes. Yt needs to be a numeric vector.</p>
</td></tr>
<tr><td><code id="ordASDA_+3A_s">s</code></td>
<td>
<p>We need to find a hyperplane that separates all classes with different biases.
For each new bias we define a binary classification problem, where a maximum of
s ordinal classes or contained in each of the two classes. A higher value of s means
that more data will be copied in the data augmentation step. BY default s is 1.</p>
</td></tr>
<tr><td><code id="ordASDA_+3A_om">Om</code></td>
<td>
<p>p by p parameter matrix Omega in generalized elastic net penalty, where
p is the number of variables.</p>
</td></tr>
<tr><td><code id="ordASDA_+3A_gam">gam</code></td>
<td>
<p>Regularization parameter for elastic net penalty, must be greater than zero.</p>
</td></tr>
<tr><td><code id="ordASDA_+3A_lam">lam</code></td>
<td>
<p>Regularization parameter for l1 penalty, must be greater than zero.</p>
</td></tr>
<tr><td><code id="ordASDA_+3A_method">method</code></td>
<td>
<p>String to select method, now either SDAD or SDAAP, see ?ASDA for more info.</p>
</td></tr>
<tr><td><code id="ordASDA_+3A_control">control</code></td>
<td>
<p>List of control arguments further passed to ASDA. See <code><a href="#topic+ASDA">ASDA</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ordASDA</code> returns an object of <code><a href="base.html#topic+class">class</a></code> &quot;<code>ordASDA</code>&quot; including a list
with the same components as an ASDA objects and:
</p>

<dl>
<dt><code>h</code></dt><dd><p>Scalar value for biases.</p>
</dd>
<dt><code>K</code></dt><dd><p>Number of classes.</p>
</dd>
</dl>

<p><code>NULL</code>
</p>


<h3>Note</h3>

<p>Remember to normalize the data.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ASDA">ASDA</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    set.seed(123)

    # You can play around with these values to generate some 2D data to test one
    numClasses &lt;- 5
    sigma &lt;- matrix(c(1,-0.2,-0.2,1),2,2)
    mu &lt;- c(0,0)
    numObsPerClass &lt;- 5

    # Generate the data, can access with train$X and train$Y
    train &lt;- accSDA::genDat(numClasses,numObsPerClass,mu,sigma)
    test &lt;- accSDA::genDat(numClasses,numObsPerClass*2,mu,sigma)

    # Visualize it, only using the first variable gives very good separation
    plot(train$X[,1],train$X[,2],col = factor(train$Y),asp=1,main="Training Data")

    # Train the ordinal based model
    res &lt;- accSDA::ordASDA(train$X,train$Y,s=2,h=1, gam=1e-6, lam=1e-3)
    vals &lt;- predict(object = res,newdata = test$X) # Takes a while to run ~ 10 seconds
    sum(vals==test$Y)/length(vals) # Get accuracy on test set
    #plot(test$X[,1],test$X[,2],col = factor(test$Y),asp=1,
    #      main="Test Data with correct labels")
    #plot(test$X[,1],test$X[,2],col = factor(vals),asp=1,
    #    main="Test Data with predictions from ordinal classifier")

</code></pre>

<hr>
<h2 id='predict.ASDA'>Predict method for sparse discriminant analysis</h2><span id='topic+predict.ASDA'></span>

<h3>Description</h3>

<p>Predicted values based on fit from the function <code><a href="#topic+ASDA">ASDA</a></code>. This
function is used to classify new observations based on their explanatory variables/features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ASDA'
predict(object, newdata = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.ASDA_+3A_object">object</code></td>
<td>
<p>Object of class ASDA. This object is returned from the function <code><a href="#topic+ASDA">ASDA</a></code>.</p>
</td></tr>
<tr><td><code id="predict.ASDA_+3A_newdata">newdata</code></td>
<td>
<p>A matrix of new observations to classify.</p>
</td></tr>
<tr><td><code id="predict.ASDA_+3A_...">...</code></td>
<td>
<p>Arguments passed to <code><a href="MASS.html#topic+predict.lda">predict.lda</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components:
</p>

<dl>
<dt><code>class</code></dt><dd><p>The classification (a factor)</p>
</dd>
<dt><code>posterior</code></dt><dd><p>posterior probabilities for the classes</p>
</dd>
<dt><code>x</code></dt><dd><p>the scores</p>
</dd>
</dl>



<h3>Note</h3>

<p>The input matrix newdata should be normalized w.r.t. the normalization
of the training data
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SDAAP">SDAAP</a></code>, <code><a href="#topic+SDAP">SDAP</a></code> and <code><a href="#topic+SDAD">SDAD</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    # Prepare training and test set
    train &lt;- c(1:40,51:90,101:140)
    Xtrain &lt;- iris[train,1:4]
    nX &lt;- normalize(Xtrain)
    Xtrain &lt;- nX$Xc
    Ytrain &lt;- iris[train,5]
    Xtest &lt;- iris[-train,1:4]
    Xtest &lt;- normalizetest(Xtest,nX)
    Ytest &lt;- iris[-train,5]

    # Define parameters for SDAD
    Om &lt;- diag(4)+0.1*matrix(1,4,4) #elNet coef mat
    gam &lt;- 0.01
    lam &lt;- 0.01
    method &lt;- "SDAD"
    q &lt;- 2
    control &lt;- list(PGsteps = 100,
                    PGtol = c(1e-5,1e-5),
                    mu = 1,
                    maxits = 100,
                    tol = 1e-3,
                    quiet = FALSE)

    # Run the algorithm
    res &lt;- ASDA(Xt = Xtrain,
                Yt = Ytrain,
                Om = Om,
                gam = gam ,
                lam = lam,
                q = q,
                method = method,
                control = control)

    # Do the predictions on the test set
    preds &lt;- predict(object = res, newdata = Xtest)
</code></pre>

<hr>
<h2 id='predict.ordASDA'>Predict method for ordinal sparse discriminant analysis</h2><span id='topic+predict.ordASDA'></span>

<h3>Description</h3>

<p>Predicted values based on fit from the function <code><a href="#topic+ordASDA">ordASDA</a></code>. This
function is used to classify new observations based on their explanatory variables/features.
There is no need to normalize the data, the data is normalized based on the normalization
data from the ordASDA object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ordASDA'
predict(object, newdata = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.ordASDA_+3A_object">object</code></td>
<td>
<p>Object of class ordASDA. This object is returned from the function <code><a href="#topic+ordASDA">ordASDA</a></code>.</p>
</td></tr>
<tr><td><code id="predict.ordASDA_+3A_newdata">newdata</code></td>
<td>
<p>A matrix of new observations to classify.</p>
</td></tr>
<tr><td><code id="predict.ordASDA_+3A_...">...</code></td>
<td>
<p>Arguments passed to <code><a href="MASS.html#topic+predict.lda">predict.lda</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of predictions.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ordASDA">ordASDA</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    set.seed(123)

    # You can play around with these values to generate some 2D data to test one
    numClasses &lt;- 5
    sigma &lt;- matrix(c(1,-0.2,-0.2,1),2,2)
    mu &lt;- c(0,0)
    numObsPerClass &lt;- 5

    # Generate the data, can access with train$X and train$Y
    train &lt;- accSDA::genDat(numClasses,numObsPerClass,mu,sigma)
    test &lt;- accSDA::genDat(numClasses,numObsPerClass*2,mu,sigma)

    # Visualize it, only using the first variable gives very good separation
    plot(train$X[,1],train$X[,2],col = factor(train$Y),asp=1,main="Training Data")

    # Train the ordinal based model
    res &lt;- accSDA::ordASDA(train$X,train$Y,s=2,h=1, gam=1e-6, lam=1e-3)
    vals &lt;- predict(object = res,newdata = test$X) # Takes a while to run ~ 10 seconds
    sum(vals==test$Y)/length(vals) # Get accuracy on test set
    #plot(test$X[,1],test$X[,2],col = factor(test$Y),asp=1,
    #      main="Test Data with correct labels")
    #plot(test$X[,1],test$X[,2],col = factor(vals),asp=1,
    #     main="Test Data with predictions from ordinal classifier")
</code></pre>

<hr>
<h2 id='print.ASDA'>Print method for ASDA object</h2><span id='topic+print.ASDA'></span>

<h3>Description</h3>

<p>Prints a summary of the output from the <code><a href="#topic+ASDA">ASDA</a></code> function. The
output summarizes the discriminant analysis in human readable format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ASDA'
print(x, digits = max(3, getOption("digits") - 3), numshow = 5, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.ASDA_+3A_x">x</code></td>
<td>
<p>Object of class ASDA. This object is returned from the function <code><a href="#topic+ASDA">ASDA</a></code>.</p>
</td></tr>
<tr><td><code id="print.ASDA_+3A_digits">digits</code></td>
<td>
<p>Number of digits to show in printed numbers.</p>
</td></tr>
<tr><td><code id="print.ASDA_+3A_numshow">numshow</code></td>
<td>
<p>Number of best ranked variables w.r.t. to their absolute coefficients.</p>
</td></tr>
<tr><td><code id="print.ASDA_+3A_...">...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An invisible copy of <code>x</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ASDA">ASDA</a></code>, <code><a href="#topic+predict.ASDA">predict.ASDA</a></code> and <code><a href="#topic+SDAD">SDAD</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    # Prepare training and test set
    train &lt;- c(1:40,51:90,101:140)
    Xtrain &lt;- iris[train,1:4]
    nX &lt;- normalize(Xtrain)
    Xtrain &lt;- nX$Xc
    Ytrain &lt;- iris[train,5]
    Xtest &lt;- iris[-train,1:4]
    Xtest &lt;- normalizetest(Xtest,nX)
    Ytest &lt;- iris[-train,5]

    # Run the algorithm
    resDef &lt;- ASDA(Xtrain,Ytrain)

    # Print
    print(resDef)
</code></pre>

<hr>
<h2 id='prox_EN'>Accelerated Proximal Gradient on l1 regularized quadratic program</h2><span id='topic+prox_EN'></span>

<h3>Description</h3>

<p>Applies accelerated proximal gradient algorithm to the l1-regularized quadratic program
</p>
<p style="text-align: center;"><code class="reqn">f(\mathbf{x}) + g(\mathbf{x}) = \frac{1}{2}\mathbf{x}^TA\mathbf{x} - d^T\mathbf{x} + \lambda |\mathbf{x}|_1</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>prox_EN(A, d, x0, lam, alpha, maxits, tol)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prox_EN_+3A_a">A</code></td>
<td>
<p>p by p positive definite coefficient matrix
</p>
<p style="text-align: center;"><code class="reqn">A = (\gamma Om + X^T X/n)</code>
</p>
<p>.</p>
</td></tr>
<tr><td><code id="prox_EN_+3A_d">d</code></td>
<td>
<p>nx1 dimensional column vector.</p>
</td></tr>
<tr><td><code id="prox_EN_+3A_lam">lam</code></td>
<td>
<p>Regularization parameter for l1 penalty, must be greater than zero.</p>
</td></tr>
<tr><td><code id="prox_EN_+3A_alpha">alpha</code></td>
<td>
<p>Step length.</p>
</td></tr>
<tr><td><code id="prox_EN_+3A_maxits">maxits</code></td>
<td>
<p>Number of iterations to run</p>
</td></tr>
<tr><td><code id="prox_EN_+3A_tol">tol</code></td>
<td>
<p>Stopping tolerance for proximal gradient algorithm.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used by other functions and should only be called explicitly for
debugging purposes.
</p>


<h3>Value</h3>

<p><code>prox_EN</code> returns an object of <code><a href="base.html#topic+class">class</a></code> &quot;<code>prox_EN</code>&quot; including a list
with the following named components
</p>

<dl>
<dt><code>call</code></dt><dd><p>The matched call.</p>
</dd>
<dt><code>x</code></dt><dd><p>Found solution.</p>
</dd>
<dt><code>k</code></dt><dd><p>Number of iterations used.</p>
</dd>
</dl>



<h3>See Also</h3>

<p>Used by: <code><a href="#topic+SDAP">SDAP</a></code> and the <code>SDAPcv</code> cross-validation version.
</p>

<hr>
<h2 id='prox_ENbt'>Accelerated Proximal Gradient on l1 regularized quadratic program with backtracking</h2><span id='topic+prox_ENbt'></span>

<h3>Description</h3>

<p>Applies accelerated proximal gradient (with backtracking) algorithm to the l1-regularized quadratic program
</p>
<p style="text-align: center;"><code class="reqn">f(\mathbf{x}) + g(\mathbf{x}) = \frac{1}{2}\mathbf{x}^TA\mathbf{x} - d^T\mathbf{x} + \lambda |\mathbf{x}|_1</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>prox_ENbt(A, Xt, Om, gamma, d, x0, lam, L, eta, maxits, tol)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prox_ENbt_+3A_a">A</code></td>
<td>
<p>p by p positive definite coefficient matrix
</p>
<p style="text-align: center;"><code class="reqn">A = (\gamma Om + X^T X/n)</code>
</p>
<p>.</p>
</td></tr>
<tr><td><code id="prox_ENbt_+3A_xt">Xt</code></td>
<td>
<p>Same as X above, we need it to make calculations faster.</p>
</td></tr>
<tr><td><code id="prox_ENbt_+3A_om">Om</code></td>
<td>
<p>Same reason as for the above parameter.</p>
</td></tr>
<tr><td><code id="prox_ENbt_+3A_gamma">gamma</code></td>
<td>
<p>l2 regularizing parameter.</p>
</td></tr>
<tr><td><code id="prox_ENbt_+3A_d">d</code></td>
<td>
<p>nx1 dimensional column vector.</p>
</td></tr>
<tr><td><code id="prox_ENbt_+3A_lam">lam</code></td>
<td>
<p>Regularization parameter for l1 penalty, must be greater than zero.</p>
</td></tr>
<tr><td><code id="prox_ENbt_+3A_l">L</code></td>
<td>
<p>Initial value of backtracking Lipshitz constant.</p>
</td></tr>
<tr><td><code id="prox_ENbt_+3A_eta">eta</code></td>
<td>
<p>Backtracking scaling parameter.</p>
</td></tr>
<tr><td><code id="prox_ENbt_+3A_maxits">maxits</code></td>
<td>
<p>Number of iterations to run</p>
</td></tr>
<tr><td><code id="prox_ENbt_+3A_tol">tol</code></td>
<td>
<p>Stopping tolerance for proximal gradient algorithm.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used by other functions and should only be called explicitly for
debugging purposes.
</p>


<h3>Value</h3>

<p><code>prox_ENbt</code> returns an object of <code><a href="base.html#topic+class">class</a></code> &quot;<code>prox_ENbt</code>&quot; including a list
with the following named components
</p>

<dl>
<dt><code>call</code></dt><dd><p>The matched call.</p>
</dd>
<dt><code>x</code></dt><dd><p>Found solution.</p>
</dd>
<dt><code>k</code></dt><dd><p>Number of iterations used.</p>
</dd>
</dl>



<h3>See Also</h3>

<p>Used by: <code><a href="#topic+SDAP">SDAP</a></code> and the <code>SDAPcv</code> cross-validation version.
</p>

<hr>
<h2 id='SDAAP'>Sparse Discriminant Analysis solved via Accelerated Proximal Gradient</h2><span id='topic+SDAAP'></span><span id='topic+SDAAP.default'></span>

<h3>Description</h3>

<p>Applies accelerated proximal gradient algorithm to
the optimal scoring formulation of sparse discriminant analysis proposed
by Clemmensen et al. 2011.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SDAAP(Xt, ...)

## Default S3 method:
SDAAP(
  Xt,
  Yt,
  Om,
  gam,
  lam,
  q,
  PGsteps,
  PGtol,
  maxits,
  tol,
  selector = rep(1, dim(Xt)[2]),
  initTheta,
  bt = FALSE,
  L,
  eta,
  rankRed = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SDAAP_+3A_xt">Xt</code></td>
<td>
<p>n by p data matrix, (not a data frame, but a matrix)</p>
</td></tr>
<tr><td><code id="SDAAP_+3A_yt">Yt</code></td>
<td>
<p>n by K matrix of indicator variables (Yij = 1 if i in class j).
This will later be changed to handle factor variables as well.
Each observation belongs in a single class, so for a given row/observation,
only one element is 1 and the rest is 0.</p>
</td></tr>
<tr><td><code id="SDAAP_+3A_om">Om</code></td>
<td>
<p>p by p parameter matrix Omega in generalized elastic net penalty.</p>
</td></tr>
<tr><td><code id="SDAAP_+3A_gam">gam</code></td>
<td>
<p>Regularization parameter for elastic net penalty.</p>
</td></tr>
<tr><td><code id="SDAAP_+3A_lam">lam</code></td>
<td>
<p>Regularization parameter for l1 penalty, must be greater than zero.</p>
</td></tr>
<tr><td><code id="SDAAP_+3A_q">q</code></td>
<td>
<p>Desired number of discriminant vectors.</p>
</td></tr>
<tr><td><code id="SDAAP_+3A_pgsteps">PGsteps</code></td>
<td>
<p>Maximum number if inner proximal gradient algorithm for finding beta.</p>
</td></tr>
<tr><td><code id="SDAAP_+3A_pgtol">PGtol</code></td>
<td>
<p>Stopping tolerance for inner APG method.</p>
</td></tr>
<tr><td><code id="SDAAP_+3A_maxits">maxits</code></td>
<td>
<p>Number of iterations to run</p>
</td></tr>
<tr><td><code id="SDAAP_+3A_tol">tol</code></td>
<td>
<p>Stopping tolerance for proximal gradient algorithm.</p>
</td></tr>
<tr><td><code id="SDAAP_+3A_selector">selector</code></td>
<td>
<p>Vector to choose which parameters in the discriminant vector will be used to calculate the
regularization terms. The size of the vector must be *p* the number of predictors. The
default value is a vector of all ones. This is currently only used for ordinal classification.</p>
</td></tr>
<tr><td><code id="SDAAP_+3A_inittheta">initTheta</code></td>
<td>
<p>Option to set the initial theta vector, by default it is a vector of all ones
for the first theta.</p>
</td></tr>
<tr><td><code id="SDAAP_+3A_bt">bt</code></td>
<td>
<p>Boolean to indicate whether backtracking should be used, default false.</p>
</td></tr>
<tr><td><code id="SDAAP_+3A_l">L</code></td>
<td>
<p>Initial estimate for Lipshitz constant used for backtracking.</p>
</td></tr>
<tr><td><code id="SDAAP_+3A_eta">eta</code></td>
<td>
<p>Scalar for Lipshitz constant.</p>
</td></tr>
<tr><td><code id="SDAAP_+3A_rankred">rankRed</code></td>
<td>
<p>Boolean indicating whether Om is in factorized form, such that R^t*R = mO</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>SDAAP</code> returns an object of <code><a href="base.html#topic+class">class</a></code> &quot;<code>SDAAP</code>&quot; including a list
with the following named components:
</p>

<dl>
<dt><code>call</code></dt><dd><p>The matched call.</p>
</dd>
<dt><code>B</code></dt><dd><p>p by q matrix of discriminant vectors.</p>
</dd>
<dt><code>Q</code></dt><dd><p>K by q matrix of scoring vectors.</p>
</dd>
<dt><code>subits</code></dt><dd><p>Total number of iterations in proximal gradient subroutine.</p>
</dd>
<dt><code>totalits</code></dt><dd><p>Number coordinate descent iterations for all discriminant vectors</p>
</dd>
</dl>

<p><code>NULL</code>
</p>


<h3>See Also</h3>

<p><code>SDAAPcv</code>, <code><a href="#topic+SDAP">SDAP</a></code> and <code><a href="#topic+SDAD">SDAD</a></code>
</p>

<hr>
<h2 id='SDAD'>Sparse Discriminant Analysis solved via ADMM</h2><span id='topic+SDAD'></span><span id='topic+SDAD.default'></span>

<h3>Description</h3>

<p>Applies alternating direction methods of multipliers algorithm to
the optimal scoring formulation of sparse discriminant analysis proposed
by Clemmensen et al. 2011.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SDAD(Xt, ...)

## Default S3 method:
SDAD(
  Xt,
  Yt,
  Om,
  gam,
  lam,
  mu,
  q,
  PGsteps,
  PGtol,
  maxits,
  tol,
  selector = rep(1, dim(Xt)[2]),
  initTheta,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SDAD_+3A_xt">Xt</code></td>
<td>
<p>n by p data matrix, (not a data frame, but a matrix)</p>
</td></tr>
<tr><td><code id="SDAD_+3A_yt">Yt</code></td>
<td>
<p>n by K matrix of indicator variables (Yij = 1 if i in class j).
This will later be changed to handle factor variables as well.
Each observation belongs in a single class, so for a given row/observation,
only one element is 1 and the rest is 0.</p>
</td></tr>
<tr><td><code id="SDAD_+3A_om">Om</code></td>
<td>
<p>p by p parameter matrix Omega in generalized elastic net penalty.</p>
</td></tr>
<tr><td><code id="SDAD_+3A_gam">gam</code></td>
<td>
<p>Regularization parameter for elastic net penalty.</p>
</td></tr>
<tr><td><code id="SDAD_+3A_lam">lam</code></td>
<td>
<p>Regularization parameter for l1 penalty, must be greater than zero.</p>
</td></tr>
<tr><td><code id="SDAD_+3A_mu">mu</code></td>
<td>
<p>Penalty parameter for augmented Lagrangian term, must be greater than zero.</p>
</td></tr>
<tr><td><code id="SDAD_+3A_q">q</code></td>
<td>
<p>Desired number of discriminant vectors.</p>
</td></tr>
<tr><td><code id="SDAD_+3A_pgsteps">PGsteps</code></td>
<td>
<p>Maximum number if inner proximal gradient algorithm for finding beta.</p>
</td></tr>
<tr><td><code id="SDAD_+3A_pgtol">PGtol</code></td>
<td>
<p>Two stopping tolerances for inner ADMM method, first is absolute tolerance, second is relative.</p>
</td></tr>
<tr><td><code id="SDAD_+3A_maxits">maxits</code></td>
<td>
<p>Number of iterations to run</p>
</td></tr>
<tr><td><code id="SDAD_+3A_tol">tol</code></td>
<td>
<p>Stopping tolerance for proximal gradient algorithm.</p>
</td></tr>
<tr><td><code id="SDAD_+3A_selector">selector</code></td>
<td>
<p>Vector to choose which parameters in the discriminant vector will be used to calculate the
regularization terms. The size of the vector must be *p* the number of predictors. The
default value is a vector of all ones. This is currently only used for ordinal classification.</p>
</td></tr>
<tr><td><code id="SDAD_+3A_inittheta">initTheta</code></td>
<td>
<p>Initial first theta, default value is a vector of ones.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>SDAD</code> returns an object of <code><a href="base.html#topic+class">class</a></code> &quot;<code>SDAD</code>&quot; including a list
with the following named components: (More will be added later to handle the predict function)
</p>

<dl>
<dt><code>call</code></dt><dd><p>The matched call.</p>
</dd>
<dt><code>B</code></dt><dd><p>p by q matrix of discriminant vectors.</p>
</dd>
<dt><code>Q</code></dt><dd><p>K by q matrix of scoring vectors.</p>
</dd>
<dt><code>subits</code></dt><dd><p>Total number of iterations in proximal gradient subroutine.</p>
</dd>
<dt><code>totalits</code></dt><dd><p>Number coordinate descent iterations for all discriminant vectors</p>
</dd>
</dl>

<p><code>NULL</code>
</p>


<h3>See Also</h3>

<p><code>SDADcv</code>, <code><a href="#topic+SDAAP">SDAAP</a></code> and <code><a href="#topic+SDAP">SDAP</a></code>
</p>

<hr>
<h2 id='SDAP'>Sparse Discriminant Analysis solved via Proximal Gradient</h2><span id='topic+SDAP'></span><span id='topic+SDAP.default'></span>

<h3>Description</h3>

<p>Applies proximal gradient algorithm to
the optimal scoring formulation of sparse discriminant analysis proposed
by Clemmensen et al. 2011.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SDAP(Xt, ...)

## Default S3 method:
SDAP(
  Xt,
  Yt,
  Om,
  gam,
  lam,
  q,
  PGsteps,
  PGtol,
  maxits,
  tol,
  initTheta,
  bt = FALSE,
  L,
  eta,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SDAP_+3A_xt">Xt</code></td>
<td>
<p>n by p data matrix, (not a data frame, but a matrix)</p>
</td></tr>
<tr><td><code id="SDAP_+3A_yt">Yt</code></td>
<td>
<p>n by K matrix of indicator variables (Yij = 1 if i in class j).
This will later be changed to handle factor variables as well.
Each observation belongs in a single class, so for a given row/observation,
only one element is 1 and the rest is 0.</p>
</td></tr>
<tr><td><code id="SDAP_+3A_om">Om</code></td>
<td>
<p>p by p parameter matrix Omega in generalized elastic net penalty.</p>
</td></tr>
<tr><td><code id="SDAP_+3A_gam">gam</code></td>
<td>
<p>Regularization parameter for elastic net penalty.</p>
</td></tr>
<tr><td><code id="SDAP_+3A_lam">lam</code></td>
<td>
<p>Regularization parameter for l1 penalty, must be greater than zero.</p>
</td></tr>
<tr><td><code id="SDAP_+3A_q">q</code></td>
<td>
<p>Desired number of discriminant vectors.</p>
</td></tr>
<tr><td><code id="SDAP_+3A_pgsteps">PGsteps</code></td>
<td>
<p>Maximum number if inner proximal gradient algorithm for finding beta.</p>
</td></tr>
<tr><td><code id="SDAP_+3A_pgtol">PGtol</code></td>
<td>
<p>Stopping tolerance for inner APG method.</p>
</td></tr>
<tr><td><code id="SDAP_+3A_maxits">maxits</code></td>
<td>
<p>Number of iterations to run</p>
</td></tr>
<tr><td><code id="SDAP_+3A_tol">tol</code></td>
<td>
<p>Stopping tolerance for proximal gradient algorithm.</p>
</td></tr>
<tr><td><code id="SDAP_+3A_inittheta">initTheta</code></td>
<td>
<p>Initial first theta, default value is a vector of ones.</p>
</td></tr>
<tr><td><code id="SDAP_+3A_bt">bt</code></td>
<td>
<p>Boolean to indicate whether backtracking should be used, default false.</p>
</td></tr>
<tr><td><code id="SDAP_+3A_l">L</code></td>
<td>
<p>Initial estimate for Lipshitz constant used for backtracking.</p>
</td></tr>
<tr><td><code id="SDAP_+3A_eta">eta</code></td>
<td>
<p>Scalar for Lipshitz constant.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>SDAP</code> returns an object of <code><a href="base.html#topic+class">class</a></code> &quot;<code>SDAP</code>&quot; including a list
with the following named components: (More will be added later to handle the predict function)
</p>

<dl>
<dt><code>call</code></dt><dd><p>The matched call.</p>
</dd>
<dt><code>B</code></dt><dd><p>p by q matrix of discriminant vectors.</p>
</dd>
<dt><code>Q</code></dt><dd><p>K by q matrix of scoring vectors.</p>
</dd>
<dt><code>subits</code></dt><dd><p>Total number of iterations in proximal gradient subroutine.</p>
</dd>
<dt><code>totalits</code></dt><dd><p>Number coordinate descent iterations for all discriminant vectors</p>
</dd>
</dl>

<p><code>NULL</code>
</p>


<h3>See Also</h3>

<p><code>SDAPcv</code>, <code><a href="#topic+SDAAP">SDAAP</a></code> and <code><a href="#topic+SDAD">SDAD</a></code>
</p>

<hr>
<h2 id='SZVD'>Sparse Zero Variance Discriminant Analysis</h2><span id='topic+SZVD'></span><span id='topic+SZVD.default'></span>

<h3>Description</h3>

<p>Applies SZVD heuristic for sparse zero-variance discriminant
analysis to given training set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SZVD(train, ...)

## Default S3 method:
SZVD(
  train,
  gamma,
  D,
  penalty = TRUE,
  scaling = TRUE,
  tol = list(abs = 1e-04, rel = 1e-04),
  maxits = 2000,
  beta = 1,
  quiet = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SZVD_+3A_train">train</code></td>
<td>
<p>Data matrix where first column is the response class.</p>
</td></tr>
<tr><td><code id="SZVD_+3A_...">...</code></td>
<td>
<p>Parameters passed to SZVD.default.</p>
</td></tr>
<tr><td><code id="SZVD_+3A_gamma">gamma</code></td>
<td>
<p>Set of regularization parameters controlling l1-penalty.</p>
</td></tr>
<tr><td><code id="SZVD_+3A_d">D</code></td>
<td>
<p>dictionary/basis matrix.</p>
</td></tr>
<tr><td><code id="SZVD_+3A_penalty">penalty</code></td>
<td>
<p>Controls whether to apply reweighting of l1-penalty (using sigma = within-class std devs).</p>
</td></tr>
<tr><td><code id="SZVD_+3A_scaling">scaling</code></td>
<td>
<p>Logical indicating whether to scale data such that each
feature has variance 1.</p>
</td></tr>
<tr><td><code id="SZVD_+3A_tol">tol</code></td>
<td>
<p>Stopping tolerances for ADMM algorithm,
must include tol$rel and tol$abs.</p>
</td></tr>
<tr><td><code id="SZVD_+3A_maxits">maxits</code></td>
<td>
<p>Maximum number of iterations used in the ADMM algorithm.</p>
</td></tr>
<tr><td><code id="SZVD_+3A_beta">beta</code></td>
<td>
<p>penalty term controlling the splitting constraint.</p>
</td></tr>
<tr><td><code id="SZVD_+3A_quiet">quiet</code></td>
<td>
<p>Print intermediate outpur or not.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function will currently solve as a standalone function in accSDA for time comparison.
A wrapper function like ASDA will be created to use the functionality of plots and such.
Maybe call it ASZDA. For that purpose the individual ZVD function will need to be implemented.
</p>


<h3>Value</h3>

<p><code>SZVD</code> returns an object of <code><a href="base.html#topic+class">class</a></code> &quot;<code>SZVD</code>&quot; including a list
with the following named components:
</p>

<dl>
<dt><code>DVs</code></dt><dd><p>Discriminant vectors.</p>
</dd>
<dt><code>its</code></dt><dd><p>Number of iterations required to find DVs.</p>
</dd>
<dt><code>pen_scal</code></dt><dd><p>Weights used in reweighted l1-penalty.</p>
</dd>
<dt><code>N</code></dt><dd><p>Basis for the null-space of the sample within-class covariance.</p>
</dd>
<dt><code>means</code></dt><dd><p>Training class-means.</p>
</dd>
<dt><code>mus</code></dt><dd><p>Training meand and variance scaling/centering terms.</p>
</dd>
<dt><code>w0</code></dt><dd><p>unpenalized zero-variance discriminants (initial solutions) plus B and W, etc.</p>
</dd>
</dl>

<p><code>NULL</code>
</p>


<h3>See Also</h3>

<p>Used by: <code><a href="#topic+SZVDcv">SZVDcv</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(123)
  P &lt;- 300 # Number of variables
  N &lt;- 50 # Number of samples per class

  # Mean for classes, they are zero everywhere except the first 3 coordinates
  m1 &lt;- rep(0,P)
  m1[1] &lt;- 3

  m2 &lt;- rep(0,P)
  m2[2] &lt;- 3

  m3 &lt;- rep(0,P)
  m3[3] &lt;- 3

  # Sample dummy data
  Xtrain &lt;- rbind(MASS::mvrnorm(n=N,mu = m1, Sigma = diag(P)),
                 MASS::mvrnorm(n=N,mu = m2, Sigma = diag(P)),
                MASS::mvrnorm(n=N,mu = m3, Sigma = diag(P)))


  # Generate the labels
  Ytrain &lt;- rep(1:3,each=N)

  # Normalize the data
  Xt &lt;- accSDA::normalize(Xtrain)
  Xtrain &lt;- Xt$Xc

  # Train the classifier and increase the sparsity parameter from the default
  # so we penalize more for non-sparse solutions.
  res &lt;- accSDA::SZVD(cbind(Ytrain,Xtrain),beta=2.5,
                     maxits=1000,tol = list(abs = 1e-04, rel = 1e-04))
</code></pre>

<hr>
<h2 id='SZVD_ADMM'>Alternating Direction Method of Multipliers for SZVD</h2><span id='topic+SZVD_ADMM'></span>

<h3>Description</h3>

<p>Iteratively solves the problem
</p>
<p style="text-align: center;"><code class="reqn">\min(-1/2*x^TB^Tx + \gamma p(y): ||x||_2 \leq 1, DNx = y)</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>SZVD_ADMM(B, N, D, sols0, pen_scal, gamma, beta, tol, maxits, quiet = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SZVD_ADMM_+3A_b">B</code></td>
<td>
<p>Between class covariance matrix for objective (in space defined by N).</p>
</td></tr>
<tr><td><code id="SZVD_ADMM_+3A_n">N</code></td>
<td>
<p>basis matrix for null space of covariance matrix W.</p>
</td></tr>
<tr><td><code id="SZVD_ADMM_+3A_d">D</code></td>
<td>
<p>penalty dictionary/basis.</p>
</td></tr>
<tr><td><code id="SZVD_ADMM_+3A_sols0">sols0</code></td>
<td>
<p>initial solutions sols0$x, sols0$y, sols0$z</p>
</td></tr>
<tr><td><code id="SZVD_ADMM_+3A_pen_scal">pen_scal</code></td>
<td>
<p>penalty scaling term.</p>
</td></tr>
<tr><td><code id="SZVD_ADMM_+3A_gamma">gamma</code></td>
<td>
<p>l1 regularization parameter</p>
</td></tr>
<tr><td><code id="SZVD_ADMM_+3A_beta">beta</code></td>
<td>
<p>penalty term controlling the splitting constraint.</p>
</td></tr>
<tr><td><code id="SZVD_ADMM_+3A_tol">tol</code></td>
<td>
<p>tol$abs = absolute error, tol$rel = relative error to be
achieved to declare convergence of the algorithm.</p>
</td></tr>
<tr><td><code id="SZVD_ADMM_+3A_maxits">maxits</code></td>
<td>
<p>maximum number of iterations of the algorithm to run.</p>
</td></tr>
<tr><td><code id="SZVD_ADMM_+3A_quiet">quiet</code></td>
<td>
<p>toggles between displaying intermediate statistics.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used by other functions and should only be called explicitly for
debugging purposes.
</p>


<h3>Value</h3>

<p><code>SZVD_ADMM</code> returns an object of <code><a href="base.html#topic+class">class</a></code> &quot;<code>SZVD_ADMM</code>&quot; including a list
with the following named components
</p>

<dl>
<dt><code>x,y,z</code></dt><dd><p>Iterates at termination.</p>
</dd>
<dt><code>its</code></dt><dd><p>Number of iterations required to converge.</p>
</dd>
<dt><code>errtol</code></dt><dd><p>Stopping error bound at termination</p>
</dd>
</dl>



<h3>See Also</h3>

<p>Used by: <code><a href="#topic+SZVDcv">SZVDcv</a></code>.
</p>

<hr>
<h2 id='SZVD_kFold_cv'>Cross-validation of sparse zero variance discriminant analysis</h2><span id='topic+SZVD_kFold_cv'></span><span id='topic+SZVD_kFold_cv.default'></span>

<h3>Description</h3>

<p>Applies alternating direction methods of multipliers to solve sparse
zero variance discriminant analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SZVD_kFold_cv(X, ...)

## Default S3 method:
SZVD_kFold_cv(
  X,
  Y,
  folds,
  gams,
  beta,
  D,
  q,
  maxits,
  tol,
  ztol,
  feat,
  penalty,
  quiet,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SZVD_kFold_cv_+3A_x">X</code></td>
<td>
<p>n by p data matrix, variables should be scaled to by sd</p>
</td></tr>
<tr><td><code id="SZVD_kFold_cv_+3A_...">...</code></td>
<td>
<p>Parameters passed to SZVD.default.</p>
</td></tr>
<tr><td><code id="SZVD_kFold_cv_+3A_y">Y</code></td>
<td>
<p>n by K indicator matrix.</p>
</td></tr>
<tr><td><code id="SZVD_kFold_cv_+3A_folds">folds</code></td>
<td>
<p>number of folds to use in K-fold cross-validation.</p>
</td></tr>
<tr><td><code id="SZVD_kFold_cv_+3A_gams">gams</code></td>
<td>
<p>Number of regularly spaced regularization parameters to try in [0,1]*max_gamma.
See details for how max_gamma is computed in the function.</p>
</td></tr>
<tr><td><code id="SZVD_kFold_cv_+3A_beta">beta</code></td>
<td>
<p>Augmented Lagrangian parameter. Must be greater than zero.</p>
</td></tr>
<tr><td><code id="SZVD_kFold_cv_+3A_d">D</code></td>
<td>
<p>Penalty dictionary basis matrix.</p>
</td></tr>
<tr><td><code id="SZVD_kFold_cv_+3A_q">q</code></td>
<td>
<p>Desired number of discriminant vectors.</p>
</td></tr>
<tr><td><code id="SZVD_kFold_cv_+3A_maxits">maxits</code></td>
<td>
<p>Number of iterations to run ADMM algorithm.</p>
</td></tr>
<tr><td><code id="SZVD_kFold_cv_+3A_tol">tol</code></td>
<td>
<p>Stopping tolerances for ADMM, must have tol$rel and tol$abs.</p>
</td></tr>
<tr><td><code id="SZVD_kFold_cv_+3A_ztol">ztol</code></td>
<td>
<p>Rounding tolerance for truncating entries to 0.</p>
</td></tr>
<tr><td><code id="SZVD_kFold_cv_+3A_feat">feat</code></td>
<td>
<p>Maximum fraction of nonzero features desired in validation scheme.</p>
</td></tr>
<tr><td><code id="SZVD_kFold_cv_+3A_penalty">penalty</code></td>
<td>
<p>Controls whether to apply reweighting of l1-penalty (using sigma = within-class std devs)</p>
</td></tr>
<tr><td><code id="SZVD_kFold_cv_+3A_quiet">quiet</code></td>
<td>
<p>toggles between displaying intermediate statistics.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Add how max_gamma is calculated from the ZVD solution.
This function might require a wrapper similar to ASDA.
</p>


<h3>Value</h3>

<p><code>SZVDcv</code> returns an object of <code><a href="base.html#topic+class">class</a></code> &quot;<code>SZVDcv</code>&quot;
including a list with the named components <code>DVs</code> and <code>gambest</code>.
Where <code>DVs</code> are the discriminant vectors for the best l1 regularization
parameter and <code>gambest</code> is the best regularization parameter found
in the cross-validation.
</p>
<p><code>NULL</code>
</p>


<h3>See Also</h3>

<p>Used by: <code><a href="#topic+SZVDcv">SZVDcv</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  P &lt;- 150 # Number of variables
  N &lt;- 20 # Number of samples per class

  # Mean for classes, they are zero everywhere except the first 3 coordinates
  m1 &lt;- rep(0,P)
  m1[1] &lt;- 3

  m2 &lt;- rep(0,P)
  m2[2] &lt;- 3

  m3 &lt;- rep(0,P)
  m3[3] &lt;- 3

  # Sample dummy data
  Xtrain &lt;- rbind(MASS::mvrnorm(n=N,mu = m1, Sigma = diag(P)),
                 MASS::mvrnorm(n=N,mu = m2, Sigma = diag(P)),
                MASS::mvrnorm(n=N,mu = m3, Sigma = diag(P)))


  # Generate the labels
  Ytrain &lt;- cbind(c(rep(1,N),rep(0,2*N)),
                  c(rep(0,N),rep(1,N),rep(0,N)),
                  c(rep(0,2*N),rep(1,N)))

  # Normalize the data
  Xt &lt;- accSDA::normalize(Xtrain)
  Xtrain &lt;- Xt$Xc

  # Train the classifier and increase the sparsity parameter from the default
  # so we penalize more for non-sparse solutions.
  res &lt;- accSDA::SZVD_kFold_cv(Xtrain,Ytrain,folds=2,gams=2,beta=2.5,q=1, D=diag(P),
                              maxits=50,tol=list(abs=1e-2,rel=1e-2),
                              ztol=1e-3,feat=0.3,quiet=FALSE,penalty=TRUE)
</code></pre>

<hr>
<h2 id='SZVDcv'>Cross-validation of sparse zero variance discriminant analysis</h2><span id='topic+SZVDcv'></span><span id='topic+SZVDcv.default'></span>

<h3>Description</h3>

<p>Applies alternating direction methods of multipliers to solve sparse
zero variance discriminant analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SZVDcv(Atrain, ...)

## Default S3 method:
SZVDcv(
  Atrain,
  Aval,
  k,
  num_gammas,
  g_mults,
  D,
  sparsity_pen,
  scaling,
  penalty,
  beta,
  tol,
  ztol,
  maxits,
  quiet,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SZVDcv_+3A_atrain">Atrain</code></td>
<td>
<p>Training data set.</p>
</td></tr>
<tr><td><code id="SZVDcv_+3A_...">...</code></td>
<td>
<p>Parameters passed to SZVD.default.</p>
</td></tr>
<tr><td><code id="SZVDcv_+3A_aval">Aval</code></td>
<td>
<p>Validation set.</p>
</td></tr>
<tr><td><code id="SZVDcv_+3A_k">k</code></td>
<td>
<p>Number of classes within training and validation sets.</p>
</td></tr>
<tr><td><code id="SZVDcv_+3A_num_gammas">num_gammas</code></td>
<td>
<p>Number of gammas to train on.</p>
</td></tr>
<tr><td><code id="SZVDcv_+3A_g_mults">g_mults</code></td>
<td>
<p>Parameters defining range of gammas to train, g_max*(c_min, c_max).
Note that it is an array/vector with two elements.</p>
</td></tr>
<tr><td><code id="SZVDcv_+3A_d">D</code></td>
<td>
<p>Penalty dictionary basis matrix.</p>
</td></tr>
<tr><td><code id="SZVDcv_+3A_sparsity_pen">sparsity_pen</code></td>
<td>
<p>weight defining validation criteria as weighted sum of misclassification error and
cardinality of discriminant vectors.</p>
</td></tr>
<tr><td><code id="SZVDcv_+3A_scaling">scaling</code></td>
<td>
<p>Whether to rescale data so each feature has variance 1.</p>
</td></tr>
<tr><td><code id="SZVDcv_+3A_penalty">penalty</code></td>
<td>
<p>Controls whether to apply reweighting of l1-penalty (using sigma = within-class std devs)</p>
</td></tr>
<tr><td><code id="SZVDcv_+3A_beta">beta</code></td>
<td>
<p>Parameter for augmented Lagrangian term in the ADMM algorithm.</p>
</td></tr>
<tr><td><code id="SZVDcv_+3A_tol">tol</code></td>
<td>
<p>Stopping tolerances for the ADMM algorithm, must have tol$rel and tol$abs.</p>
</td></tr>
<tr><td><code id="SZVDcv_+3A_ztol">ztol</code></td>
<td>
<p>Threshold for truncating values in DVs to zero.</p>
</td></tr>
<tr><td><code id="SZVDcv_+3A_maxits">maxits</code></td>
<td>
<p>Maximum number of iterations used in the ADMM algorithm.</p>
</td></tr>
<tr><td><code id="SZVDcv_+3A_quiet">quiet</code></td>
<td>
<p>Controls display of intermediate results.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function might require a wrapper similar to ASDA.
</p>


<h3>Value</h3>

<p><code>SZVDcv</code> returns an object of <code><a href="base.html#topic+class">class</a></code> &quot;<code>SZVDcv</code>&quot;
including a list with the following named components:
</p>

<dl>
<dt><code>DVs</code></dt><dd><p>Discriminant vectors for the best choice of gamma.</p>
</dd>
<dt><code>all_DVs</code></dt><dd><p>Discriminant vectors for all choices of gamma.</p>
</dd>
<dt><code>l0_DVs</code></dt><dd><p>Discriminant vectors for gamma minimizing cardinality.</p>
</dd>
<dt><code>mc_DVs</code></dt><dd><p>Discriminant vector minimizing misclassification.</p>
</dd>
<dt><code>gamma</code></dt><dd><p>Choice of gamma minimizing validation criterion.</p>
</dd>
<dt><code>gammas</code></dt><dd><p>Set of all gammas trained on.</p>
</dd>
<dt><code>max_g</code></dt><dd><p>Maximum value of gamma guaranteed to yield a nontrivial solution.</p>
</dd>
<dt><code>ind</code></dt><dd><p>Index of best gamma.</p>
</dd>
<dt><code>w0</code></dt><dd><p>unpenalized zero-variance discriminants (initial solutions) plus B and W, etc. from ZVD</p>
</dd>
</dl>

<p><code>NULL</code>
</p>


<h3>See Also</h3>

<p>Non CV version: <code><a href="#topic+SZVD">SZVD</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  P &lt;- 300 # Number of variables
  N &lt;- 50 # Number of samples per class

  # Mean for classes, they are zero everywhere except the first 3 coordinates
  m1 &lt;- rep(0,P)
  m1[1] &lt;- 3

  m2 &lt;- rep(0,P)
  m2[2] &lt;- 3

  m3 &lt;- rep(0,P)
  m3[3] &lt;- 3

  # Sample dummy data
  Xtrain &lt;- rbind(MASS::mvrnorm(n=N,mu = m1, Sigma = diag(P)),
                 MASS::mvrnorm(n=N,mu = m2, Sigma = diag(P)),
                MASS::mvrnorm(n=N,mu = m3, Sigma = diag(P)))
 Xval &lt;- rbind(MASS::mvrnorm(n=N,mu = m1, Sigma = diag(P)),
                 MASS::mvrnorm(n=N,mu = m2, Sigma = diag(P)),
                MASS::mvrnorm(n=N,mu = m3, Sigma = diag(P)))

  # Generate the labels
  Ytrain &lt;- rep(1:3,each=N)
  Yval &lt;- rep(1:3,each=N)


  # Train the classifier and increase the sparsity parameter from the default
  # so we penalize more for non-sparse solutions.

  res &lt;- accSDA::SZVDcv(cbind(Ytrain,Xtrain),cbind(Yval,Xval),num_gammas=4,
                        g_mults = c(0,1),beta=2.5,
                        D=diag(P), maxits=100,tol=list(abs=1e-3,rel=1e-3), k = 3,
                        ztol=1e-4,sparsity_pen=0.3,quiet=FALSE,penalty=TRUE,scaling=TRUE)
</code></pre>

<hr>
<h2 id='test_ZVD'>Classify test data using nearest centroid classification and
discriminant vectors learned from the training set.</h2><span id='topic+test_ZVD'></span>

<h3>Description</h3>

<p>This function is used in SZVDcv and is only meant for internal
use at this stage. Will potentially be released in future versions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test_ZVD(w, test, classMeans, mus, scaling, ztol)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="test_ZVD_+3A_w">w</code></td>
<td>
<p>Matrix with columns equal to discriminant vectors.</p>
</td></tr>
<tr><td><code id="test_ZVD_+3A_test">test</code></td>
<td>
<p>matrix containing test set.</p>
</td></tr>
<tr><td><code id="test_ZVD_+3A_classmeans">classMeans</code></td>
<td>
<p>Means of each class in the training set,
(used for computing centroids for classification).</p>
</td></tr>
<tr><td><code id="test_ZVD_+3A_mus">mus</code></td>
<td>
<p>means/standard devs of the training set,
(used for centering/normalizing the test data appropriately).</p>
</td></tr>
<tr><td><code id="test_ZVD_+3A_scaling">scaling</code></td>
<td>
<p>Logical indicating whether scaling should be done.
on the test set.</p>
</td></tr>
<tr><td><code id="test_ZVD_+3A_ztol">ztol</code></td>
<td>
<p>Threshold for setting values in DVs to zero.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used by other functions and should only be called explicitly for
debugging purposes. Potential release in the future.
This function should potentially be made internal for the release.
</p>


<h3>Value</h3>

<p><code>test_ZVD</code> returns an object of <code><a href="base.html#topic+class">class</a></code> &quot;<code>test_ZVD</code>&quot; including a list
with the following named components
</p>

<dl>
<dt><code>stats</code></dt><dd><p>list containing number of misclassified observations,
l0 and l1 norms of discriminants.</p>
</dd>
<dt><code>pred_labs</code></dt><dd><p>predicted class labels according to nearest centroid
and the discriminants.</p>
</dd>
</dl>



<h3>See Also</h3>

<p>Used by: <code><a href="#topic+SZVDcv">SZVDcv</a></code>.
</p>

<hr>
<h2 id='vec_shrink'>Softmax for SZVD ADMM iterations</h2><span id='topic+vec_shrink'></span>

<h3>Description</h3>

<p>Applies softmax the soft thresholding shrinkage operator to v with tolerance a.
That is, output is the vector with entries with absolute value v_i - a if
|v_i| &gt; a and zero otherwise, with sign pattern matching that of v.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vec_shrink(v, a)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vec_shrink_+3A_v">v</code></td>
<td>
<p>Vector to be thresholded.</p>
</td></tr>
<tr><td><code id="vec_shrink_+3A_a">a</code></td>
<td>
<p>Vector of tolerances.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used by other functions and should only be called explicitly for
debugging purposes.
</p>


<h3>Value</h3>

<p>thresholded v vector.
</p>

<dl>
<dt><code>x,y,z</code></dt><dd><p>Iterates at termination.</p>
</dd>
<dt><code>its</code></dt><dd><p>Number of iterations required to converge.</p>
</dd>
<dt><code>errtol</code></dt><dd><p>Stopping error bound at termination</p>
</dd>
</dl>



<h3>See Also</h3>

<p>Used by: <code><a href="#topic+SZVD_ADMM">SZVD_ADMM</a></code>.
</p>

<hr>
<h2 id='ZVD'>Zero Variance Discriminant Analysis</h2><span id='topic+ZVD'></span><span id='topic+ZVD.default'></span>

<h3>Description</h3>

<p>Implements the ZVD algorithm to solve dicriminant vectors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ZVD(A, ...)

## Default S3 method:
ZVD(A, scaling = FALSE, get_DVs = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ZVD_+3A_a">A</code></td>
<td>
<p>Matrix, where first column corresponds to class labels.</p>
</td></tr>
<tr><td><code id="ZVD_+3A_...">...</code></td>
<td>
<p>Parameters passed to ZVD.default.</p>
</td></tr>
<tr><td><code id="ZVD_+3A_scaling">scaling</code></td>
<td>
<p>Logical whether to rescale data so each feature has variance 1.</p>
</td></tr>
<tr><td><code id="ZVD_+3A_get_dvs">get_DVs</code></td>
<td>
<p>Logical whether to obtain unpenalized zero-variance discriminant vectors.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function should potentially be made internal for the release.
</p>


<h3>Value</h3>

<p><code>SZVDcv</code> returns an object of <code><a href="base.html#topic+class">class</a></code> &quot;<code>ZVD</code>&quot;
including a list with the following named components:
</p>

<dl>
<dt><code>dvs</code></dt><dd><p>discriminant vectors (optional).</p>
</dd>
<dt><code>B</code></dt><dd><p>sample between-class covariance.</p>
</dd>
<dt><code>W</code></dt><dd><p>sample within-class covariance.</p>
</dd>
<dt><code>N</code></dt><dd><p>basis for the null space of the sample within-class covariance.</p>
</dd>
<dt><code>mu</code></dt><dd><p>training mean and variance scaling/centering terms</p>
</dd>
<dt><code>means</code></dt><dd><p>vectors of sample class-means.</p>
</dd>
<dt><code>k</code></dt><dd><p>number of classes in given data set.</p>
</dd>
<dt><code>labels</code></dt><dd><p>list of classes.</p>
</dd>
<dt><code>obs</code></dt><dd><p>matrix of data observations.</p>
</dd>
<dt><code>class_obs</code></dt><dd><p>Matrices of observations of each class.</p>
</dd>
</dl>

<p><code>NULL</code>
</p>


<h3>See Also</h3>

<p>Used by: <code><a href="#topic+SZVDcv">SZVDcv</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  # Generate Gaussian data on three classes with bunch of redundant variables

  P &lt;- 300 # Number of variables
  N &lt;- 50 # Number of samples per class

  # Mean for classes, they are zero everywhere except the first 3 coordinates
  m1 &lt;- rep(0,P)
  m1[1] &lt;- 3

  m2 &lt;- rep(0,P)
  m2[2] &lt;- 3

  m3 &lt;- rep(0,P)
  m3[3] &lt;- 3

  # Sample dummy data
  Xtrain &lt;- rbind(MASS::mvrnorm(n=N,mu = m1, Sigma = diag(P)),
              MASS::mvrnorm(n=N,mu = m2, Sigma = diag(P)),
              MASS::mvrnorm(n=N,mu = m3, Sigma = diag(P)))


  # Generate the labels
  Ytrain &lt;- rep(1:3,each=N)

  # Normalize the data
  Xt &lt;- accSDA::normalize(Xtrain)
  Xtrain &lt;- Xt$Xc

  # Train the classifier and increase the sparsity parameter from the default
  # so we penalize more for non-sparse solutions.
  res &lt;- accSDA::ZVD(cbind(Ytrain,Xtrain))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
