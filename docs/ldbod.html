<!DOCTYPE html><html><head><title>Help for package ldbod</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ldbod}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ldbod'><p>Local Density-Based Outlier Detection using Subsampling with Approximate Nearest Neighbor Search</p></a></li>
<li><a href='#ldbod.ref'><p>Local Density-Based Outlier Detection using Reference Data with Approximate Nearest Neighbor Search</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Local Density-Based Outlier Detection</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.2</td>
</tr>
<tr>
<td>Author:</td>
<td>Kristopher Williams</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Kristopher Williams &lt;kristopher.williams83@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Flexible procedures to compute local density-based outlier scores for ranking outliers.
  Both exact and approximate nearest neighbor search can be implemented, while also accommodating
  multiple neighborhood sizes and four different local density-based methods. It allows for
  referencing a random subsample of the input data or a user specified reference data set
  to compute outlier scores against, so both unsupervised and semi-supervised outlier
  detection can be implemented.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, RANN, mnormt</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/kwilliams83/ldbod">https://github.com/kwilliams83/ldbod</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2017-05-26 03:52:35 UTC; kwilliams</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2017-05-26 06:04:25 UTC</td>
</tr>
</table>
<hr>
<h2 id='ldbod'>Local Density-Based Outlier Detection using Subsampling with Approximate Nearest Neighbor Search</h2><span id='topic+ldbod'></span>

<h3>Description</h3>

<p>This function computes local density-based outlier scores for input data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ldbod(X, k = c(10, 20), nsub = nrow(X), method = c("lof", "ldf", "rkof",
  "lpdf"), ldf.param = c(h = 1, c = 0.1), rkof.param = c(alpha = 1, C = 1,
  sig2 = 1), lpdf.param = c(cov.type = "full", sigma2 = 1e-05, tmax = 1, v =
  1), treetype = "kd", searchtype = "standard", eps = 0,
  scale.data = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ldbod_+3A_x">X</code></td>
<td>
<p>An n x p data matrix to compute outlier scores</p>
</td></tr>
<tr><td><code id="ldbod_+3A_k">k</code></td>
<td>
<p>A vector of neighborhood sizes, k must be less than nsub</p>
</td></tr>
<tr><td><code id="ldbod_+3A_nsub">nsub</code></td>
<td>
<p>Subsample size, nsub must be greater than k.  Usually nsub = 0.10*n or larger is recommended. Default is nsub = n</p>
</td></tr>
<tr><td><code id="ldbod_+3A_method">method</code></td>
<td>
<p>Character vector specifying the local density-based method(s) to compute. User can specify more than
one method.  By default all methods are computed</p>
</td></tr>
<tr><td><code id="ldbod_+3A_ldf.param">ldf.param</code></td>
<td>
<p>Vector of parameters for method LDF. h is the positive bandwidth parameter and c is a positive scaling constant.  Default values are h=1 and c=0.1</p>
</td></tr>
<tr><td><code id="ldbod_+3A_rkof.param">rkof.param</code></td>
<td>
<p>Vector of parameters for method RKOF. C is the postive bandwidth paramter, alpha is a sensitiveity parameter in the interval [0,1],
and  sig2 is the variance parameter.  Default values are alpha=1, C=1, sig2=1</p>
</td></tr>
<tr><td><code id="ldbod_+3A_lpdf.param">lpdf.param</code></td>
<td>
<p>Vector of paramters for method LPDF.  cov.type is the covariance parameterization type,
which users can specifiy as either 'full' or 'diag'.  sigma2 is the positive regularization parameter, tmax is the maximum number of updates, and
v is the degrees of freedom for the multivariate t distribution.  Default values are cov.type = 'full',tmax=1, sigma2=1e-5, and v=1.</p>
</td></tr>
<tr><td><code id="ldbod_+3A_treetype">treetype</code></td>
<td>
<p>Character vector specifiying tree method.  Either 'kd' or 'bd' tree may be specified.  Default is 'kd'.
Refer to documentation for RANN package.</p>
</td></tr>
<tr><td><code id="ldbod_+3A_searchtype">searchtype</code></td>
<td>
<p>Character vector specifiying kNN search type. Default value is &quot;standard&quot;. Refer to documentation for RANN package.</p>
</td></tr>
<tr><td><code id="ldbod_+3A_eps">eps</code></td>
<td>
<p>Error bound.  Default is 0.0 which implies exact nearest neighgour search.  Refer to documentation for RANN package.</p>
</td></tr>
<tr><td><code id="ldbod_+3A_scale.data">scale.data</code></td>
<td>
<p>Logical value indicating to scale each feature of X using standard noramlization with mean 0 and standard deviation of 1</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the local density-based outlier scores for input data, X, referencing a random subsample of X. The subsampled
data set is constructed by randomly drawning nsub samples from X without replacement.
</p>
<p>Four different methods can be implemented LOF, LDF, RKOF, and LPDF.  Each method specified returns densities and relative densities.
Methods LDF and RKOF uses guassian kernels, and method LDPF uses multivarite t distribution. Outlier scores returned are positive
except for lpde and lpdr which are log scaled densities (natural log). Score lpdr has shown to be highly sensitive to k.
</p>
<p>All kNN computations are carried out using the nn2() function from the RANN package. Multivariate t densities are
computed using the dmt() function from the mnormt package.  Refer to specific packages for more details.  Note: all
neighborhoods are strickly of size k; therefore, the algorithms for LOF, LDF, and RKOF are not exact implementations, but
algorithms are similiar for most situation and are equivalent when distance to k-th nearest neighbor is unique.  If there are many
duplicate data points, then implementation of algorithms could lead to dramatically different (positive or negative) results than those that allow
neighborhood sizes larger than k, especially if k is relatively small.  Removing duplicates is recommended before computing
outlier scores unless there is good reason to keep them.
</p>
<p>The algorithm can be used to compute an ensemble of outlier scores by using multiple k values
and/or iterating over multiple subsamples.
</p>


<h3>Value</h3>

<p>A list of length 9 with the elements:
</p>
<p>lrd &ndash;An n x length(k) matrix where each column vector represents the local reachabiility denity (LRD) outlier scores for each specifed k value.  Smaller values indicate a point in more outlying.
</p>
<p>lof   &ndash;An n x length(k) matrix where each column vector represents the local outlier factor (LOF) outlier scores for each specifed k value. Larger values indicate a point in more outlying.
</p>
<p>lde   &ndash;An n x length(k) matrix where each column vector represents the local density estimate (LDE) outlier scores for each specifed k value. Smaller values indicate a point in more outlying.
</p>
<p>ldf   &ndash;An n x length(k) matrix where each column vector represents the local density factor (LDF) outlier scores for each specifed k value. Larger values indicate a point in more outlying.
</p>
<p>kde   &ndash;An n x length(k) matrix where each column vector represents the kernel density estimate (KDE) outlier scores for each specifed k value. Smaller values indicate a point in more outlying.
</p>
<p>rkof   &ndash;An n x length(k) matrix where each column vector represents the robust kernel density factor (RKOF) outlier scores for each specifed k value. Larger values indicate a point in more outlying.
</p>
<p>lpde   &ndash;An n x length(k) matrix where each column vector represents the local parametric density estimate (LPDE) outlier scores for each specifed k value on log scale. Smaller values indicate a point in more outlying.
</p>
<p>lpdf  &ndash;An n x length(k) matrix where each column vector represents the local parametric density factor (LPDF) outlier scores for each specifed k value. Smaller values indicate a point in more outlying.
</p>
<p>lpdr   &ndash;An n x length(k) matrix where each column vector represents the local parametric density ratio (LPDR) outlier scores for each specifed k value. Smaller values indicate a point in more outlying.
LPDR is typically used to detect groups of outliers.
</p>
<p>If a method is not specified then returns NULL
</p>


<h3>References</h3>

<p>M. M. Breunig, H-P. Kriegel, R.T. Ng, and J. Sander (2000). LOF: Identifying density-based local outliers.  In Proc. of ACM
International Conference on Knowledge Discovery and Data Mining, 93-104.
</p>
<p>L. J. Latecki, A. Lazarevic, and D. Pokrajac (2007). Outlier Detection with kernel density functions.  In Proc. of Machine Learning and Data
Mining in Pattern Recognition, 61-75
</p>
<p>J. Gao, W. Hu, Z. Zhang, X. Zhang, and O. Wu (2011). RKOF: Robust kernel-based local outlier detection. In Proc. of Advances in Knowledge Discovery and
Data Mining, 270-283.
</p>
<p>K. T. Williams (2016).  Local parametric density-based outlier deteciton and ensemble learning with application to malware detection. PhD Dissertation.
The University of Texas at San Antonio.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 500 x 2 data matrix
X &lt;- matrix(rnorm(1000),500,2)

# five outliers
outliers &lt;- matrix(c(rnorm(2,20),rnorm(2,-12),rnorm(2,-8),rnorm(2,-5),rnorm(2,9)),5,2)
 X &lt;- rbind(X,outliers)

# compute outlier scores without subsampling for all methods using neighborhood size of 50
scores &lt;- ldbod(X, k=50)

head(scores$lrd); head(scores$rkof)

# plot data and highlight top 5 outliers retured by lof
plot(X)
top5outliers &lt;- X[order(scores$lof,decreasing=TRUE)[1:5],]
points(top5outliers,col=2)

# plot data and highlight top 5 outliers retured by outlier score lpde
plot(X)
top5outliers &lt;- X[order(scores$lpde,decreasing=FALSE)[1:5],]
points(top5outliers,col=2)

# compute outlier scores for k= 10,20 with 10% subsampling for methods 'lof' and 'lpdf'
scores &lt;- ldbod(X, k = c(10,20), nsub = 0.10*nrow(X), method = c('lof','lpdf'))

# plot data and highlight top 5 outliers retuned by lof for k=20
plot(X)
top5outliers &lt;- X[order(scores$lof[,2],decreasing=TRUE)[1:5],]
points(top5outliers,col=2)

</code></pre>

<hr>
<h2 id='ldbod.ref'>Local Density-Based Outlier Detection using Reference Data with Approximate Nearest Neighbor Search</h2><span id='topic+ldbod.ref'></span>

<h3>Description</h3>

<p>This function computes local density-based outlier scores for input data and user specified reference set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ldbod.ref(X, Y, k = c(10, 20), method = c("lof", "ldf", "rkof", "lpdf"),
  ldf.param = c(h = 1, c = 0.1), rkof.param = c(alpha = 1, C = 1, sig2 = 1),
  lpdf.param = c(cov.type = "full", sigma2 = 1e-05, tmax = 1, v = 1),
  treetype = "kd", searchtype = "standard", eps = 0, scale.data = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ldbod.ref_+3A_x">X</code></td>
<td>
<p>An n x p data matrix to compute outlier scores</p>
</td></tr>
<tr><td><code id="ldbod.ref_+3A_y">Y</code></td>
<td>
<p>An m x p reference data matrix.</p>
</td></tr>
<tr><td><code id="ldbod.ref_+3A_k">k</code></td>
<td>
<p>A vector of neighborhood sizes, k must be less than m.</p>
</td></tr>
<tr><td><code id="ldbod.ref_+3A_method">method</code></td>
<td>
<p>Character vector specifying the local density-based method(s) to compute. User can specify more than
one method.  By default all methods are computed</p>
</td></tr>
<tr><td><code id="ldbod.ref_+3A_ldf.param">ldf.param</code></td>
<td>
<p>Vector of parameters for method LDF. h is the positive bandwidth parameter and c is a positive scaling constant.  Default values are h=1 and c=0.1</p>
</td></tr>
<tr><td><code id="ldbod.ref_+3A_rkof.param">rkof.param</code></td>
<td>
<p>Vector of parameters for method RKOF. C is the postive bandwidth paramter, alpha is a sensitiveity parameter in the interval [0,1],
and  sig2 is the variance parameter.  Default values are alpha=1, C=1, sig2=1</p>
</td></tr>
<tr><td><code id="ldbod.ref_+3A_lpdf.param">lpdf.param</code></td>
<td>
<p>Vector of paramters for method LPDF.  cov.type is the covariance parameterization type,
which users can specifiy as either 'full' or 'diag'.  sigma2 is the positive regularization parameter, tmax is the maximum number of updates, and
v is the degrees of freedom for the multivariate t distribution.  Default values are cov.type = 'full',tmax=1, sigma2=1e-5, and v=1.</p>
</td></tr>
<tr><td><code id="ldbod.ref_+3A_treetype">treetype</code></td>
<td>
<p>Character vector specifiying tree method.  Either 'kd' or 'bd' tree may be specified.  Default is 'kd'.
Refer to documentation for RANN package.</p>
</td></tr>
<tr><td><code id="ldbod.ref_+3A_searchtype">searchtype</code></td>
<td>
<p>Character vector specifiying kNN search type. Default value is &quot;standard&quot;. Refer to documentation for RANN package.</p>
</td></tr>
<tr><td><code id="ldbod.ref_+3A_eps">eps</code></td>
<td>
<p>Error bound.  Default is 0.0 which implies exact nearest neighgour search.  Refer to documentation for RANN package.</p>
</td></tr>
<tr><td><code id="ldbod.ref_+3A_scale.data">scale.data</code></td>
<td>
<p>Logical value indicating to scale each feature of X using standard noramlization based on mean and standard deviation for features of Y.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes local density-based outlier scores for input data, X, referencing data Y.  For semi-supervised outlier detection Y would be a set of &quot;normal&quot;
reference points; otherwise, Y can be any other set of reference points of interest. This allows users the flexibility to reference other data sets besides X or
a subset of X.
Four different methods can be implemented LOF, LDF, RKOF, and LPDF.  Each method specified returns densities and relative densities.
Methods LDF and RKOF uses guassian kernels, and method LDPF uses multivarite t distribution.
Outlier scores returned are non-negative except for lpde and lpdr which are log scaled densities (natural log). Note: Outlier score
lpdr is strictly designed for unsupervised outlier detection and should not be used in the semi-supervised setting.
Refer to references for
more details about each method.
</p>
<p>All kNN computations are carried out using the nn2() function from the RANN package. Multivariate t densities are
computed using the dmt() function from the mnormt package.  Refer to specific packages for more details.  Note: all
neighborhoods are strickly of size k; therefore, the algorithms for LOF, LDF, and RKOF are not exact implementations, but
algorithms are similiar for most situation and are equivalent when distance to k-th nearest neighbor is unique.  If there are many
duplicate data points in Y, then implementation of algorithms could lead to dramatically different (positive or negative) results than those that allow
neighborhood sizes larger than k, especially if k is relatively small.  Removing duplicates is recommended before computing
outlier scores unless there is good reason to keep them.
</p>
<p>The algorithm can be used to compute an ensemble of unsupervised outlier scores by using multiple k values
and/or multiple iterations of reference data.
</p>


<h3>Value</h3>

<p>A list of length 9 with the elements:
</p>
<p>lrd &ndash;An n x length(k) matrix where each column vector represents outlier scores for each specifed k value.  Smaller values indicate a point in more outlying.
</p>
<p>lof   &ndash;An n x length(k) matrix where each column vector represents outlier scores for each specifed k value. Larger values indicate a point in more outlying.
</p>
<p>lde   &ndash;An n x length(k) matrix where each column vector represents outlier scores for each specifed k value. Smaller values indicate a point in more outlying.
</p>
<p>ldf   &ndash;An n x length(k) matrix where each column vector represents outlier scores for each specifed k value. Larger values indicate a point in more outlying.
</p>
<p>kde   &ndash;An n x length(k) matrix where each column vector represents outlier scores for each specifed k value. Smaller values indicate a point in more outlying.
</p>
<p>rkof   &ndash;An n x length(k) matrix where each column vector represents outlier scores for each specifed k value. Larger values indicate a point in more outlying.
</p>
<p>lpde   &ndash;An n x length(k) matrix where each column vector represents outlier scores for each specifed k value. Smaller values indicate a point in more outlying.
</p>
<p>lpdf  &ndash;An n x length(k) matrix where each column vector represents outlier scores for each specifed k value. Smaller values indicate a point in more outlying.
</p>
<p>lpdr   &ndash;An n x length(k) matrix where each column vector represents outlier scores for each specifed k value. Smaller values indicate a point in more outlying.
</p>
<p>If a method is not specified then returns NULL
</p>


<h3>References</h3>

<p>M. M. Breunig, H-P. Kriegel, R.T. Ng, and J. Sander (2000). LOF: Identifying density-based local outliers.  In Proc. of ACM
International Conference on Knowledge Discovery and Data Mining, 93-104.
</p>
<p>L. J. Latecki, A. Lazarevic, and D. Pokrajac (2007). Outlier Detection with kernel density functions.  In Proc. of Machine Learning and Data
Mining in Pattern Recognition, 61-75
</p>
<p>J. Gao, W. Hu, Z. Zhang, X. Zhang, and O. Wu (2011). RKOF: Robust kernel-based local outlier detection. In Proc. of Advances in Knowledge Discovery and
Data Mining, 270-283.
</p>
<p>K. T. Williams (2016).  Local parametric density-based outlier deteciton and ensemble learning with application to malware detection. PhD Dissertation.
The University of Texas at San Antonio.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># 500 x 2 data matrix
X &lt;- matrix(rnorm(1000),500,2)
Y &lt;- X
# five outliers
outliers &lt;- matrix(c(rnorm(2,20),rnorm(2,-12),rnorm(2,-8),rnorm(2,-5),rnorm(2,9)),5,2)
 X &lt;- rbind(X,outliers)

# compute outlier scores referencing Y for all methods using a neighborhood size of 50
scores &lt;- ldbod.ref(X,Y, k=50)

head(scores$lrd); head(scores$rkof)

# plot data and highlight top 5 outliers retured by lof
plot(X)
top5outliers &lt;- X[order(scores$lof,decreasing=TRUE)[1:5],]
points(top5outliers,col=2)

# plot data and highlight top 5 outliers retured by outlier score lpde
plot(X)
top5outliers &lt;- X[order(scores$lpde,decreasing=FALSE)[1:5],]
points(top5outliers,col=2)


# compute outlier scores for k= 10,20 referencing Y for methods 'lof' and 'lpdf'
scores &lt;- ldbod.ref(X,Y, k = c(10,20), method = c('lof','lpdf'))

# plot data and highlight top 5 outliers retuned by lof for k=20
plot(X)
top5outliers &lt;- X[order(scores$lof[,2],decreasing=TRUE)[1:5],]
points(top5outliers,col=2)


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
